[
{"library": "torch", "item_id": "torch.nn.parameter.Parameter", "item_type": "class", "code": "classtorch.nn.parameter.Parameter", "description": "A kind of Tensor that is to be considered a module parameter. Parameters are Tensor subclasses, that have a very special property when used with Module s - when they\u2019re assigned as Module attributes they are automatically added to the list of its parameters, and will appear e.g. in parameters() iterator. Assigning a Tensor doesn\u2019t have such effect. This is because one might want to cache some temporary state, like last hidden state of the RNN, in the model. If there was no such class as Parameter, these temporaries would get registered too.  Parameters  data (Tensor) \u2013 parameter tensor. requires_grad (bool, optional) \u2013 if the parameter requires gradient. See Excluding subgraphs from backward for more details. Default: True    ", "parameters": ["data (Tensor) : parameter tensor.", "requires_grad (bool, optional) : if the parameter requires gradient. SeeExcluding subgraphs from backward for more details. Default: True"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.LPPool1d", "item_type": "class", "code": "classtorch.nn.LPPool1d(norm_type:float,kernel_size:Union[T,Tuple[T,...]],stride:Optional[Union[T,Tuple[T,...]]]=None,ceil_mode:bool=False)", "description": "Applies a 1D power-average pooling over an input signal composed of several input planes. On each window, the function computed is:  f(X)=\u2211x\u2208Xxppf(X) = \\sqrt[p]{\\sum_{x \\in X} x^{p}}  f(X)=p\u200bx\u2208X\u2211\u200bxp\u200b   At p = \u221e\\infty\u221e  , one gets Max Pooling At p = 1, one gets Sum Pooling (which is proportional to Average Pooling)   Note If the sum to the power of p is zero, the gradient of this function is not defined. This implementation will set the gradient to zero in this case.   Parameters  kernel_size \u2013 a single int, the size of the window stride \u2013 a single int, the stride of the window. Default value is kernel_size ceil_mode \u2013 when True, will use ceil instead of floor to compute the output shape     Shape: Input: (N,C,Lin)(N, C, L_{in})(N,C,Lin\u200b)   Output: (N,C,Lout)(N, C, L_{out})(N,C,Lout\u200b)  , where  Lout=\u230aLin\u2212kernel_sizestride+1\u230bL_{out} = \\left\\lfloor\\frac{L_{in} - \\text{kernel\\_size}}{\\text{stride}} + 1\\right\\rfloor  Lout\u200b=\u230astrideLin\u200b\u2212kernel_size\u200b+1\u230b     Examples::&gt;&gt;&gt; # power-2 pool of window of length 3, with stride 2. &gt;&gt;&gt; m = nn.LPPool1d(2, 3, stride=2) &gt;&gt;&gt; input = torch.randn(20, 16, 50) &gt;&gt;&gt; output = m(input)     ", "parameters": ["kernel_size : a single int, the size of the window", "stride : a single int, the stride of the window. Default value is kernel_size", "ceil_mode : when True, will use ceil instead of floor to compute the output shape"], "returns": [], "example": "NA", "shape": "\nInput: (N,C,Lin)(N, C, L_{in})(N,C,Lin\u200b)\n\n\nOutput: (N,C,Lout)(N, C, L_{out})(N,C,Lout\u200b)\n\n, where\n\nLout=\u230aLin\u2212kernel_sizestride+1\u230bL_{out} = \\left\\lfloor\\frac{L_{in} - \\text{kernel\\_size}}{\\text{stride}} + 1\\right\\rfloor\n\nLout\u200b=\u230astrideLin\u200b\u2212kernel_size\u200b+1\u230b\n\n\n\n"},
{"library": "torch", "item_id": "torch.nn.FractionalMaxPool2d", "item_type": "class", "code": "classtorch.nn.FractionalMaxPool2d(kernel_size:Union[T,Tuple[T,T]],output_size:Optional[Union[T,Tuple[T,T]]]=None,output_ratio:Optional[Union[T,Tuple[T,T]]]=None,return_indices:bool=False,_random_samples=None)", "description": "Applies a 2D fractional max pooling over an input signal composed of several input planes. Fractional MaxPooling is described in detail in the paper Fractional MaxPooling by Ben Graham The max-pooling operation is applied in kH\u00d7kWkH \\times kWkH\u00d7kW   regions by a stochastic step size determined by the target output size. The number of output features is equal to the number of input planes.  Parameters  kernel_size \u2013 the size of the window to take a max over. Can be a single number k (for a square kernel of k x k) or a tuple (kh, kw) output_size \u2013 the target output size of the image of the form oH x oW. Can be a tuple (oH, oW) or a single number oH for a square image oH x oH output_ratio \u2013 If one wants to have an output size as a ratio of the input size, this option can be given. This has to be a number or tuple in the range (0, 1) return_indices \u2013 if True, will return the indices along with the outputs. Useful to pass to nn.MaxUnpool2d(). Default: False    Examples &gt;&gt;&gt; # pool of square window of size=3, and target output size 13x12 &gt;&gt;&gt; m = nn.FractionalMaxPool2d(3, output_size=(13, 12)) &gt;&gt;&gt; # pool of square window and target output size being half of input image size &gt;&gt;&gt; m = nn.FractionalMaxPool2d(3, output_ratio=(0.5, 0.5)) &gt;&gt;&gt; input = torch.randn(20, 16, 50, 32) &gt;&gt;&gt; output = m(input)   ", "parameters": ["kernel_size : the size of the window to take a max over.Can be a single number k (for a square kernel of k x k) or a tuple (kh, kw)", "output_size : the target output size of the image of the form oH x oW.Can be a tuple (oH, oW) or a single number oH for a square image oH x oH", "output_ratio : If one wants to have an output size as a ratio of the input size, this option can be given.This has to be a number or tuple in the range (0, 1)", "return_indices : if True, will return the indices along with the outputs.Useful to pass to nn.MaxUnpool2d(). Default: False"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.AvgPool3d", "item_type": "class", "code": "classtorch.nn.AvgPool3d(kernel_size:Union[T,Tuple[T,T,T]],stride:Optional[Union[T,Tuple[T,T,T]]]=None,padding:Union[T,Tuple[T,T,T]]=0,ceil_mode:bool=False,count_include_pad:bool=True,divisor_override=None)", "description": "Applies a 3D average pooling over an input signal composed of several input planes. In the simplest case, the output value of the layer with input size (N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W)  , output (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\u200b,Hout\u200b,Wout\u200b)   and kernel_size (kD,kH,kW)(kD, kH, kW)(kD,kH,kW)   can be precisely described as:  out(Ni,Cj,d,h,w)=\u2211k=0kD\u22121\u2211m=0kH\u22121\u2211n=0kW\u22121input(Ni,Cj,stride[0]\u00d7d+k,stride[1]\u00d7h+m,stride[2]\u00d7w+n)kD\u00d7kH\u00d7kW\\begin{aligned}     \\text{out}(N_i, C_j, d, h, w) ={} &amp; \\sum_{k=0}^{kD-1} \\sum_{m=0}^{kH-1} \\sum_{n=0}^{kW-1} \\\\                                       &amp; \\frac{\\text{input}(N_i, C_j, \\text{stride}[0] \\times d + k,                                               \\text{stride}[1] \\times h + m, \\text{stride}[2] \\times w + n)}                                              {kD \\times kH \\times kW} \\end{aligned}  out(Ni\u200b,Cj\u200b,d,h,w)=\u200bk=0\u2211kD\u22121\u200bm=0\u2211kH\u22121\u200bn=0\u2211kW\u22121\u200bkD\u00d7kH\u00d7kWinput(Ni\u200b,Cj\u200b,stride[0]\u00d7d+k,stride[1]\u00d7h+m,stride[2]\u00d7w+n)\u200b\u200b  If padding is non-zero, then the input is implicitly zero-padded on all three sides for padding number of points. The parameters kernel_size, stride can either be:   a single int \u2013 in which case the same value is used for the depth, height and width dimension a tuple of three ints \u2013 in which case, the first int is used for the depth dimension, the second int for the height dimension and the third int for the width dimension    Parameters  kernel_size \u2013 the size of the window stride \u2013 the stride of the window. Default value is kernel_size padding \u2013 implicit zero padding to be added on all three sides ceil_mode \u2013 when True, will use ceil instead of floor to compute the output shape count_include_pad \u2013 when True, will include the zero-padding in the averaging calculation divisor_override \u2013 if specified, it will be used as divisor, otherwise kernel_size will be used     Shape: Input: (N,C,Din,Hin,Win)(N, C, D_{in}, H_{in}, W_{in})(N,C,Din\u200b,Hin\u200b,Win\u200b)   Output: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\u200b,Hout\u200b,Wout\u200b)  , where  Dout=\u230aDin+2\u00d7padding[0]\u2212kernel_size[0]stride[0]+1\u230bD_{out} = \\left\\lfloor\\frac{D_{in} + 2 \\times \\text{padding}[0] -       \\text{kernel\\_size}[0]}{\\text{stride}[0]} + 1\\right\\rfloor  Dout\u200b=\u230astride[0]Din\u200b+2\u00d7padding[0]\u2212kernel_size[0]\u200b+1\u230b   Hout=\u230aHin+2\u00d7padding[1]\u2212kernel_size[1]stride[1]+1\u230bH_{out} = \\left\\lfloor\\frac{H_{in} + 2 \\times \\text{padding}[1] -       \\text{kernel\\_size}[1]}{\\text{stride}[1]} + 1\\right\\rfloor  Hout\u200b=\u230astride[1]Hin\u200b+2\u00d7padding[1]\u2212kernel_size[1]\u200b+1\u230b   Wout=\u230aWin+2\u00d7padding[2]\u2212kernel_size[2]stride[2]+1\u230bW_{out} = \\left\\lfloor\\frac{W_{in} + 2 \\times \\text{padding}[2] -       \\text{kernel\\_size}[2]}{\\text{stride}[2]} + 1\\right\\rfloor  Wout\u200b=\u230astride[2]Win\u200b+2\u00d7padding[2]\u2212kernel_size[2]\u200b+1\u230b      Examples: &gt;&gt;&gt; # pool of square window of size=3, stride=2 &gt;&gt;&gt; m = nn.AvgPool3d(3, stride=2) &gt;&gt;&gt; # pool of non-square window &gt;&gt;&gt; m = nn.AvgPool3d((3, 2, 2), stride=(2, 1, 2)) &gt;&gt;&gt; input = torch.randn(20, 16, 50,44, 31) &gt;&gt;&gt; output = m(input)   ", "parameters": ["kernel_size : the size of the window", "stride : the stride of the window. Default value is kernel_size", "padding : implicit zero padding to be added on all three sides", "ceil_mode : when True, will use ceil instead of floor to compute the output shape", "count_include_pad : when True, will include the zero-padding in the averaging calculation", "divisor_override : if specified, it will be used as divisor, otherwise kernel_size will be used"], "returns": [], "example": "NA", "shape": "\nInput: (N,C,Din,Hin,Win)(N, C, D_{in}, H_{in}, W_{in})(N,C,Din\u200b,Hin\u200b,Win\u200b)\n\n\nOutput: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\u200b,Hout\u200b,Wout\u200b)\n\n, where\n\nDout=\u230aDin+2\u00d7padding[0]\u2212kernel_size[0]stride[0]+1\u230bD_{out} = \\left\\lfloor\\frac{D_{in} + 2 \\times \\text{padding}[0] -\n      \\text{kernel\\_size}[0]}{\\text{stride}[0]} + 1\\right\\rfloor\n\nDout\u200b=\u230astride[0]Din\u200b+2\u00d7padding[0]\u2212kernel_size[0]\u200b+1\u230b\n\n\nHout=\u230aHin+2\u00d7padding[1]\u2212kernel_size[1]stride[1]+1\u230bH_{out} = \\left\\lfloor\\frac{H_{in} + 2 \\times \\text{padding}[1] -\n      \\text{kernel\\_size}[1]}{\\text{stride}[1]} + 1\\right\\rfloor\n\nHout\u200b=\u230astride[1]Hin\u200b+2\u00d7padding[1]\u2212kernel_size[1]\u200b+1\u230b\n\n\nWout=\u230aWin+2\u00d7padding[2]\u2212kernel_size[2]stride[2]+1\u230bW_{out} = \\left\\lfloor\\frac{W_{in} + 2 \\times \\text{padding}[2] -\n      \\text{kernel\\_size}[2]}{\\text{stride}[2]} + 1\\right\\rfloor\n\nWout\u200b=\u230astride[2]Win\u200b+2\u00d7padding[2]\u2212kernel_size[2]\u200b+1\u230b\n\n\n\n"},
{"library": "torch", "item_id": "torch.nn.AdaptiveAvgPool3d", "item_type": "class", "code": "classtorch.nn.AdaptiveAvgPool3d(output_size:Union[T,Tuple[T,...]])", "description": "Applies a 3D adaptive average pooling over an input signal composed of several input planes. The output is of size D x H x W, for any input size. The number of output features is equal to the number of input planes.  Parameters output_size \u2013 the target output size of the form D x H x W. Can be a tuple (D, H, W) or a single number D for a cube D x D x D. D, H and W can be either a int, or None which means the size will be the same as that of the input.   Examples &gt;&gt;&gt; # target output size of 5x7x9 &gt;&gt;&gt; m = nn.AdaptiveAvgPool3d((5,7,9)) &gt;&gt;&gt; input = torch.randn(1, 64, 8, 9, 10) &gt;&gt;&gt; output = m(input) &gt;&gt;&gt; # target output size of 7x7x7 (cube) &gt;&gt;&gt; m = nn.AdaptiveAvgPool3d(7) &gt;&gt;&gt; input = torch.randn(1, 64, 10, 9, 8) &gt;&gt;&gt; output = m(input) &gt;&gt;&gt; # target output size of 7x9x8 &gt;&gt;&gt; m = nn.AdaptiveAvgPool3d((7, None, None)) &gt;&gt;&gt; input = torch.randn(1, 64, 10, 9, 8) &gt;&gt;&gt; output = m(input)   ", "parameters": ["output_size : the target output size of the form D x H x W.Can be a tuple (D, H, W) or a single number D for a cube D x D x D.D, H and W can be either a int, or None which means the size willbe the same as that of the input."], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.AdaptiveAvgPool1d", "item_type": "class", "code": "classtorch.nn.AdaptiveAvgPool1d(output_size:Union[T,Tuple[T,...]])", "description": "Applies a 1D adaptive average pooling over an input signal composed of several input planes. The output size is H, for any input size. The number of output features is equal to the number of input planes.  Parameters output_size \u2013 the target output size H   Examples &gt;&gt;&gt; # target output size of 5 &gt;&gt;&gt; m = nn.AdaptiveAvgPool1d(5) &gt;&gt;&gt; input = torch.randn(1, 64, 8) &gt;&gt;&gt; output = m(input)   ", "parameters": ["output_size : the target output size H"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.AvgPool2d", "item_type": "class", "code": "classtorch.nn.AvgPool2d(kernel_size:Union[T,Tuple[T,T]],stride:Optional[Union[T,Tuple[T,T]]]=None,padding:Union[T,Tuple[T,T]]=0,ceil_mode:bool=False,count_include_pad:bool=True,divisor_override:bool=None)", "description": "Applies a 2D average pooling over an input signal composed of several input planes. In the simplest case, the output value of the layer with input size (N,C,H,W)(N, C, H, W)(N,C,H,W)  , output (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)   and kernel_size (kH,kW)(kH, kW)(kH,kW)   can be precisely described as:  out(Ni,Cj,h,w)=1kH\u2217kW\u2211m=0kH\u22121\u2211n=0kW\u22121input(Ni,Cj,stride[0]\u00d7h+m,stride[1]\u00d7w+n)out(N_i, C_j, h, w)  = \\frac{1}{kH * kW} \\sum_{m=0}^{kH-1} \\sum_{n=0}^{kW-1}                        input(N_i, C_j, stride[0] \\times h + m, stride[1] \\times w + n)out(Ni\u200b,Cj\u200b,h,w)=kH\u2217kW1\u200bm=0\u2211kH\u22121\u200bn=0\u2211kW\u22121\u200binput(Ni\u200b,Cj\u200b,stride[0]\u00d7h+m,stride[1]\u00d7w+n)  If padding is non-zero, then the input is implicitly zero-padded on both sides for padding number of points. The parameters kernel_size, stride, padding can either be:   a single int \u2013 in which case the same value is used for the height and width dimension a tuple of two ints \u2013 in which case, the first int is used for the height dimension, and the second int for the width dimension    Parameters  kernel_size \u2013 the size of the window stride \u2013 the stride of the window. Default value is kernel_size padding \u2013 implicit zero padding to be added on both sides ceil_mode \u2013 when True, will use ceil instead of floor to compute the output shape count_include_pad \u2013 when True, will include the zero-padding in the averaging calculation divisor_override \u2013 if specified, it will be used as divisor, otherwise kernel_size will be used     Shape: Input: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)   Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)  , where  Hout=\u230aHin+2\u00d7padding[0]\u2212kernel_size[0]stride[0]+1\u230bH_{out} = \\left\\lfloor\\frac{H_{in}  + 2 \\times \\text{padding}[0] -   \\text{kernel\\_size}[0]}{\\text{stride}[0]} + 1\\right\\rfloor  Hout\u200b=\u230astride[0]Hin\u200b+2\u00d7padding[0]\u2212kernel_size[0]\u200b+1\u230b   Wout=\u230aWin+2\u00d7padding[1]\u2212kernel_size[1]stride[1]+1\u230bW_{out} = \\left\\lfloor\\frac{W_{in}  + 2 \\times \\text{padding}[1] -   \\text{kernel\\_size}[1]}{\\text{stride}[1]} + 1\\right\\rfloor  Wout\u200b=\u230astride[1]Win\u200b+2\u00d7padding[1]\u2212kernel_size[1]\u200b+1\u230b      Examples: &gt;&gt;&gt; # pool of square window of size=3, stride=2 &gt;&gt;&gt; m = nn.AvgPool2d(3, stride=2) &gt;&gt;&gt; # pool of non-square window &gt;&gt;&gt; m = nn.AvgPool2d((3, 2), stride=(2, 1)) &gt;&gt;&gt; input = torch.randn(20, 16, 50, 32) &gt;&gt;&gt; output = m(input)   ", "parameters": ["kernel_size : the size of the window", "stride : the stride of the window. Default value is kernel_size", "padding : implicit zero padding to be added on both sides", "ceil_mode : when True, will use ceil instead of floor to compute the output shape", "count_include_pad : when True, will include the zero-padding in the averaging calculation", "divisor_override : if specified, it will be used as divisor, otherwise kernel_size will be used"], "returns": [], "example": "NA", "shape": "\nInput: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)\n\n\nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)\n\n, where\n\nHout=\u230aHin+2\u00d7padding[0]\u2212kernel_size[0]stride[0]+1\u230bH_{out} = \\left\\lfloor\\frac{H_{in}  + 2 \\times \\text{padding}[0] -\n  \\text{kernel\\_size}[0]}{\\text{stride}[0]} + 1\\right\\rfloor\n\nHout\u200b=\u230astride[0]Hin\u200b+2\u00d7padding[0]\u2212kernel_size[0]\u200b+1\u230b\n\n\nWout=\u230aWin+2\u00d7padding[1]\u2212kernel_size[1]stride[1]+1\u230bW_{out} = \\left\\lfloor\\frac{W_{in}  + 2 \\times \\text{padding}[1] -\n  \\text{kernel\\_size}[1]}{\\text{stride}[1]} + 1\\right\\rfloor\n\nWout\u200b=\u230astride[1]Win\u200b+2\u00d7padding[1]\u2212kernel_size[1]\u200b+1\u230b\n\n\n\n"},
{"library": "torch", "item_id": "torch.nn.AdaptiveAvgPool2d", "item_type": "class", "code": "classtorch.nn.AdaptiveAvgPool2d(output_size:Union[T,Tuple[T,...]])", "description": "Applies a 2D adaptive average pooling over an input signal composed of several input planes. The output is of size H x W, for any input size. The number of output features is equal to the number of input planes.  Parameters output_size \u2013 the target output size of the image of the form H x W. Can be a tuple (H, W) or a single H for a square image H x H. H and W can be either a int, or None which means the size will be the same as that of the input.   Examples &gt;&gt;&gt; # target output size of 5x7 &gt;&gt;&gt; m = nn.AdaptiveAvgPool2d((5,7)) &gt;&gt;&gt; input = torch.randn(1, 64, 8, 9) &gt;&gt;&gt; output = m(input) &gt;&gt;&gt; # target output size of 7x7 (square) &gt;&gt;&gt; m = nn.AdaptiveAvgPool2d(7) &gt;&gt;&gt; input = torch.randn(1, 64, 10, 9) &gt;&gt;&gt; output = m(input) &gt;&gt;&gt; # target output size of 10x7 &gt;&gt;&gt; m = nn.AdaptiveAvgPool2d((None, 7)) &gt;&gt;&gt; input = torch.randn(1, 64, 10, 9) &gt;&gt;&gt; output = m(input)   ", "parameters": ["output_size : the target output size of the image of the form H x W.Can be a tuple (H, W) or a single H for a square image H x H.H and W can be either a int, or None which means the size willbe the same as that of the input."], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.AdaptiveMaxPool3d", "item_type": "class", "code": "classtorch.nn.AdaptiveMaxPool3d(output_size:Union[T,Tuple[T,...]],return_indices:bool=False)", "description": "Applies a 3D adaptive max pooling over an input signal composed of several input planes. The output is of size D x H x W, for any input size. The number of output features is equal to the number of input planes.  Parameters  output_size \u2013 the target output size of the image of the form D x H x W. Can be a tuple (D, H, W) or a single D for a cube D x D x D. D, H and W can be either a int, or None which means the size will be the same as that of the input. return_indices \u2013 if True, will return the indices along with the outputs. Useful to pass to nn.MaxUnpool3d. Default: False    Examples &gt;&gt;&gt; # target output size of 5x7x9 &gt;&gt;&gt; m = nn.AdaptiveMaxPool3d((5,7,9)) &gt;&gt;&gt; input = torch.randn(1, 64, 8, 9, 10) &gt;&gt;&gt; output = m(input) &gt;&gt;&gt; # target output size of 7x7x7 (cube) &gt;&gt;&gt; m = nn.AdaptiveMaxPool3d(7) &gt;&gt;&gt; input = torch.randn(1, 64, 10, 9, 8) &gt;&gt;&gt; output = m(input) &gt;&gt;&gt; # target output size of 7x9x8 &gt;&gt;&gt; m = nn.AdaptiveMaxPool3d((7, None, None)) &gt;&gt;&gt; input = torch.randn(1, 64, 10, 9, 8) &gt;&gt;&gt; output = m(input)   ", "parameters": ["output_size : the target output size of the image of the form D x H x W.Can be a tuple (D, H, W) or a single D for a cube D x D x D.D, H and W can be either a int, or None which means the size willbe the same as that of the input.", "return_indices : if True, will return the indices along with the outputs.Useful to pass to nn.MaxUnpool3d. Default: False"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.AvgPool1d", "item_type": "class", "code": "classtorch.nn.AvgPool1d(kernel_size:Union[T,Tuple[T]],stride:Union[T,Tuple[T]]=None,padding:Union[T,Tuple[T]]=0,ceil_mode:bool=False,count_include_pad:bool=True)", "description": "Applies a 1D average pooling over an input signal composed of several input planes. In the simplest case, the output value of the layer with input size (N,C,L)(N, C, L)(N,C,L)  , output (N,C,Lout)(N, C, L_{out})(N,C,Lout\u200b)   and kernel_size kkk   can be precisely described as:  out(Ni,Cj,l)=1k\u2211m=0k\u22121input(Ni,Cj,stride\u00d7l+m)\\text{out}(N_i, C_j, l) = \\frac{1}{k} \\sum_{m=0}^{k-1}                        \\text{input}(N_i, C_j, \\text{stride} \\times l + m)out(Ni\u200b,Cj\u200b,l)=k1\u200bm=0\u2211k\u22121\u200binput(Ni\u200b,Cj\u200b,stride\u00d7l+m)  If padding is non-zero, then the input is implicitly zero-padded on both sides for padding number of points. The parameters kernel_size, stride, padding can each be an int or a one-element tuple.  Parameters  kernel_size \u2013 the size of the window stride \u2013 the stride of the window. Default value is kernel_size padding \u2013 implicit zero padding to be added on both sides ceil_mode \u2013 when True, will use ceil instead of floor to compute the output shape count_include_pad \u2013 when True, will include the zero-padding in the averaging calculation     Shape: Input: (N,C,Lin)(N, C, L_{in})(N,C,Lin\u200b)   Output: (N,C,Lout)(N, C, L_{out})(N,C,Lout\u200b)  , where  Lout=\u230aLin+2\u00d7padding\u2212kernel_sizestride+1\u230bL_{out} = \\left\\lfloor \\frac{L_{in} + 2 \\times \\text{padding} - \\text{kernel\\_size}}{\\text{stride}} + 1\\right\\rfloor  Lout\u200b=\u230astrideLin\u200b+2\u00d7padding\u2212kernel_size\u200b+1\u230b      Examples: &gt;&gt;&gt; # pool with window of size=3, stride=2 &gt;&gt;&gt; m = nn.AvgPool1d(3, stride=2) &gt;&gt;&gt; m(torch.tensor([[[1.,2,3,4,5,6,7]]])) tensor([[[ 2.,  4.,  6.]]])   ", "parameters": ["kernel_size : the size of the window", "stride : the stride of the window. Default value is kernel_size", "padding : implicit zero padding to be added on both sides", "ceil_mode : when True, will use ceil instead of floor to compute the output shape", "count_include_pad : when True, will include the zero-padding in the averaging calculation"], "returns": [], "example": "NA", "shape": "\nInput: (N,C,Lin)(N, C, L_{in})(N,C,Lin\u200b)\n\n\nOutput: (N,C,Lout)(N, C, L_{out})(N,C,Lout\u200b)\n\n, where\n\nLout=\u230aLin+2\u00d7padding\u2212kernel_sizestride+1\u230bL_{out} = \\left\\lfloor \\frac{L_{in} +\n2 \\times \\text{padding} - \\text{kernel\\_size}}{\\text{stride}} + 1\\right\\rfloor\n\nLout\u200b=\u230astrideLin\u200b+2\u00d7padding\u2212kernel_size\u200b+1\u230b\n\n\n\n"},
{"library": "torch", "item_id": "torch.nn.AdaptiveMaxPool2d", "item_type": "class", "code": "classtorch.nn.AdaptiveMaxPool2d(output_size:Union[T,Tuple[T,...]],return_indices:bool=False)", "description": "Applies a 2D adaptive max pooling over an input signal composed of several input planes. The output is of size H x W, for any input size. The number of output features is equal to the number of input planes.  Parameters  output_size \u2013 the target output size of the image of the form H x W. Can be a tuple (H, W) or a single H for a square image H x H. H and W can be either a int, or None which means the size will be the same as that of the input. return_indices \u2013 if True, will return the indices along with the outputs. Useful to pass to nn.MaxUnpool2d. Default: False    Examples &gt;&gt;&gt; # target output size of 5x7 &gt;&gt;&gt; m = nn.AdaptiveMaxPool2d((5,7)) &gt;&gt;&gt; input = torch.randn(1, 64, 8, 9) &gt;&gt;&gt; output = m(input) &gt;&gt;&gt; # target output size of 7x7 (square) &gt;&gt;&gt; m = nn.AdaptiveMaxPool2d(7) &gt;&gt;&gt; input = torch.randn(1, 64, 10, 9) &gt;&gt;&gt; output = m(input) &gt;&gt;&gt; # target output size of 10x7 &gt;&gt;&gt; m = nn.AdaptiveMaxPool2d((None, 7)) &gt;&gt;&gt; input = torch.randn(1, 64, 10, 9) &gt;&gt;&gt; output = m(input)   ", "parameters": ["output_size : the target output size of the image of the form H x W.Can be a tuple (H, W) or a single H for a square image H x H.H and W can be either a int, or None which means the size willbe the same as that of the input.", "return_indices : if True, will return the indices along with the outputs.Useful to pass to nn.MaxUnpool2d. Default: False"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.AdaptiveMaxPool1d", "item_type": "class", "code": "classtorch.nn.AdaptiveMaxPool1d(output_size:Union[T,Tuple[T,...]],return_indices:bool=False)", "description": "Applies a 1D adaptive max pooling over an input signal composed of several input planes. The output size is H, for any input size. The number of output features is equal to the number of input planes.  Parameters  output_size \u2013 the target output size H return_indices \u2013 if True, will return the indices along with the outputs. Useful to pass to nn.MaxUnpool1d. Default: False    Examples &gt;&gt;&gt; # target output size of 5 &gt;&gt;&gt; m = nn.AdaptiveMaxPool1d(5) &gt;&gt;&gt; input = torch.randn(1, 64, 8) &gt;&gt;&gt; output = m(input)   ", "parameters": ["output_size : the target output size H", "return_indices : if True, will return the indices along with the outputs.Useful to pass to nn.MaxUnpool1d. Default: False"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.LPPool2d", "item_type": "class", "code": "classtorch.nn.LPPool2d(norm_type:float,kernel_size:Union[T,Tuple[T,...]],stride:Optional[Union[T,Tuple[T,...]]]=None,ceil_mode:bool=False)", "description": "Applies a 2D power-average pooling over an input signal composed of several input planes. On each window, the function computed is:  f(X)=\u2211x\u2208Xxppf(X) = \\sqrt[p]{\\sum_{x \\in X} x^{p}}  f(X)=p\u200bx\u2208X\u2211\u200bxp\u200b   At p = \u221e\\infty\u221e  , one gets Max Pooling At p = 1, one gets Sum Pooling (which is proportional to average pooling)  The parameters kernel_size, stride can either be:   a single int \u2013 in which case the same value is used for the height and width dimension a tuple of two ints \u2013 in which case, the first int is used for the height dimension, and the second int for the width dimension    Note If the sum to the power of p is zero, the gradient of this function is not defined. This implementation will set the gradient to zero in this case.   Parameters  kernel_size \u2013 the size of the window stride \u2013 the stride of the window. Default value is kernel_size ceil_mode \u2013 when True, will use ceil instead of floor to compute the output shape     Shape: Input: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)   Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)  , where  Hout=\u230aHin\u2212kernel_size[0]stride[0]+1\u230bH_{out} = \\left\\lfloor\\frac{H_{in} - \\text{kernel\\_size}[0]}{\\text{stride}[0]} + 1\\right\\rfloor  Hout\u200b=\u230astride[0]Hin\u200b\u2212kernel_size[0]\u200b+1\u230b   Wout=\u230aWin\u2212kernel_size[1]stride[1]+1\u230bW_{out} = \\left\\lfloor\\frac{W_{in} - \\text{kernel\\_size}[1]}{\\text{stride}[1]} + 1\\right\\rfloor  Wout\u200b=\u230astride[1]Win\u200b\u2212kernel_size[1]\u200b+1\u230b      Examples: &gt;&gt;&gt; # power-2 pool of square window of size=3, stride=2 &gt;&gt;&gt; m = nn.LPPool2d(2, 3, stride=2) &gt;&gt;&gt; # pool of non-square window of power 1.2 &gt;&gt;&gt; m = nn.LPPool2d(1.2, (3, 2), stride=(2, 1)) &gt;&gt;&gt; input = torch.randn(20, 16, 50, 32) &gt;&gt;&gt; output = m(input)   ", "parameters": ["kernel_size : the size of the window", "stride : the stride of the window. Default value is kernel_size", "ceil_mode : when True, will use ceil instead of floor to compute the output shape"], "returns": [], "example": "NA", "shape": "\nInput: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)\n\n\nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)\n\n, where\n\nHout=\u230aHin\u2212kernel_size[0]stride[0]+1\u230bH_{out} = \\left\\lfloor\\frac{H_{in} - \\text{kernel\\_size}[0]}{\\text{stride}[0]} + 1\\right\\rfloor\n\nHout\u200b=\u230astride[0]Hin\u200b\u2212kernel_size[0]\u200b+1\u230b\n\n\nWout=\u230aWin\u2212kernel_size[1]stride[1]+1\u230bW_{out} = \\left\\lfloor\\frac{W_{in} - \\text{kernel\\_size}[1]}{\\text{stride}[1]} + 1\\right\\rfloor\n\nWout\u200b=\u230astride[1]Win\u200b\u2212kernel_size[1]\u200b+1\u230b\n\n\n\n"},
{"library": "torch", "item_id": "torch.nn.MaxUnpool3d", "item_type": "class", "code": "classtorch.nn.MaxUnpool3d(kernel_size:Union[T,Tuple[T,T,T]],stride:Optional[Union[T,Tuple[T,T,T]]]=None,padding:Union[T,Tuple[T,T,T]]=0)", "description": "Computes a partial inverse of MaxPool3d. MaxPool3d is not fully invertible, since the non-maximal values are lost. MaxUnpool3d takes in as input the output of MaxPool3d including the indices of the maximal values and computes a partial inverse in which all non-maximal values are set to zero.  Note MaxPool3d can map several input sizes to the same output sizes. Hence, the inversion process can get ambiguous. To accommodate this, you can provide the needed output size as an additional argument output_size in the forward call. See the Inputs section below.   Parameters  kernel_size (int or tuple) \u2013 Size of the max pooling window. stride (int or tuple) \u2013 Stride of the max pooling window. It is set to kernel_size by default. padding (int or tuple) \u2013 Padding that was added to the input     Inputs: input: the input Tensor to invert indices: the indices given out by MaxPool3d output_size (optional): the targeted output size   Shape: Input: (N,C,Din,Hin,Win)(N, C, D_{in}, H_{in}, W_{in})(N,C,Din\u200b,Hin\u200b,Win\u200b)   Output: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\u200b,Hout\u200b,Wout\u200b)  , where  Dout=(Din\u22121)\u00d7stride[0]\u22122\u00d7padding[0]+kernel_size[0]D_{out} = (D_{in} - 1) \\times \\text{stride[0]} - 2 \\times \\text{padding[0]} + \\text{kernel\\_size[0]}  Dout\u200b=(Din\u200b\u22121)\u00d7stride[0]\u22122\u00d7padding[0]+kernel_size[0]   Hout=(Hin\u22121)\u00d7stride[1]\u22122\u00d7padding[1]+kernel_size[1]H_{out} = (H_{in} - 1) \\times \\text{stride[1]} - 2 \\times \\text{padding[1]} + \\text{kernel\\_size[1]}  Hout\u200b=(Hin\u200b\u22121)\u00d7stride[1]\u22122\u00d7padding[1]+kernel_size[1]   Wout=(Win\u22121)\u00d7stride[2]\u22122\u00d7padding[2]+kernel_size[2]W_{out} = (W_{in} - 1) \\times \\text{stride[2]} - 2 \\times \\text{padding[2]} + \\text{kernel\\_size[2]}  Wout\u200b=(Win\u200b\u22121)\u00d7stride[2]\u22122\u00d7padding[2]+kernel_size[2]  or as given by output_size in the call operator     Example: &gt;&gt;&gt; # pool of square window of size=3, stride=2 &gt;&gt;&gt; pool = nn.MaxPool3d(3, stride=2, return_indices=True) &gt;&gt;&gt; unpool = nn.MaxUnpool3d(3, stride=2) &gt;&gt;&gt; output, indices = pool(torch.randn(20, 16, 51, 33, 15)) &gt;&gt;&gt; unpooled_output = unpool(output, indices) &gt;&gt;&gt; unpooled_output.size() torch.Size([20, 16, 51, 33, 15])   ", "parameters": ["kernel_size (int or tuple) : Size of the max pooling window.", "stride (int or tuple) : Stride of the max pooling window.It is set to kernel_size by default.", "padding (int or tuple) : Padding that was added to the input"], "returns": [], "example": "&gt;&gt;&gt; # pool of square window of size=3, stride=2\n&gt;&gt;&gt; pool = nn.MaxPool3d(3, stride=2, return_indices=True)\n&gt;&gt;&gt; unpool = nn.MaxUnpool3d(3, stride=2)\n&gt;&gt;&gt; output, indices = pool(torch.randn(20, 16, 51, 33, 15))\n&gt;&gt;&gt; unpooled_output = unpool(output, indices)\n&gt;&gt;&gt; unpooled_output.size()\ntorch.Size([20, 16, 51, 33, 15])\n\n", "shape": "\ninput: the input Tensor to invert\nindices: the indices given out by MaxPool3d\noutput_size (optional): the targeted output size\n\n"},
{"library": "torch", "item_id": "torch.nn.MaxUnpool2d", "item_type": "class", "code": "classtorch.nn.MaxUnpool2d(kernel_size:Union[T,Tuple[T,T]],stride:Optional[Union[T,Tuple[T,T]]]=None,padding:Union[T,Tuple[T,T]]=0)", "description": "Computes a partial inverse of MaxPool2d. MaxPool2d is not fully invertible, since the non-maximal values are lost. MaxUnpool2d takes in as input the output of MaxPool2d including the indices of the maximal values and computes a partial inverse in which all non-maximal values are set to zero.  Note MaxPool2d can map several input sizes to the same output sizes. Hence, the inversion process can get ambiguous. To accommodate this, you can provide the needed output size as an additional argument output_size in the forward call. See the Inputs and Example below.   Parameters  kernel_size (int or tuple) \u2013 Size of the max pooling window. stride (int or tuple) \u2013 Stride of the max pooling window. It is set to kernel_size by default. padding (int or tuple) \u2013 Padding that was added to the input     Inputs: input: the input Tensor to invert indices: the indices given out by MaxPool2d output_size (optional): the targeted output size   Shape: Input: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)   Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)  , where  Hout=(Hin\u22121)\u00d7stride[0]\u22122\u00d7padding[0]+kernel_size[0]H_{out} = (H_{in} - 1) \\times \\text{stride[0]} - 2 \\times \\text{padding[0]} + \\text{kernel\\_size[0]}  Hout\u200b=(Hin\u200b\u22121)\u00d7stride[0]\u22122\u00d7padding[0]+kernel_size[0]   Wout=(Win\u22121)\u00d7stride[1]\u22122\u00d7padding[1]+kernel_size[1]W_{out} = (W_{in} - 1) \\times \\text{stride[1]} - 2 \\times \\text{padding[1]} + \\text{kernel\\_size[1]}  Wout\u200b=(Win\u200b\u22121)\u00d7stride[1]\u22122\u00d7padding[1]+kernel_size[1]  or as given by output_size in the call operator     Example: &gt;&gt;&gt; pool = nn.MaxPool2d(2, stride=2, return_indices=True) &gt;&gt;&gt; unpool = nn.MaxUnpool2d(2, stride=2) &gt;&gt;&gt; input = torch.tensor([[[[ 1.,  2,  3,  4],                             [ 5,  6,  7,  8],                             [ 9, 10, 11, 12],                             [13, 14, 15, 16]]]]) &gt;&gt;&gt; output, indices = pool(input) &gt;&gt;&gt; unpool(output, indices) tensor([[[[  0.,   0.,   0.,   0.],           [  0.,   6.,   0.,   8.],           [  0.,   0.,   0.,   0.],           [  0.,  14.,   0.,  16.]]]])  &gt;&gt;&gt; # specify a different output size than input size &gt;&gt;&gt; unpool(output, indices, output_size=torch.Size([1, 1, 5, 5])) tensor([[[[  0.,   0.,   0.,   0.,   0.],           [  6.,   0.,   8.,   0.,   0.],           [  0.,   0.,   0.,  14.,   0.],           [ 16.,   0.,   0.,   0.,   0.],           [  0.,   0.,   0.,   0.,   0.]]]])   ", "parameters": ["kernel_size (int or tuple) : Size of the max pooling window.", "stride (int or tuple) : Stride of the max pooling window.It is set to kernel_size by default.", "padding (int or tuple) : Padding that was added to the input"], "returns": [], "example": "&gt;&gt;&gt; pool = nn.MaxPool2d(2, stride=2, return_indices=True)\n&gt;&gt;&gt; unpool = nn.MaxUnpool2d(2, stride=2)\n&gt;&gt;&gt; input = torch.tensor([[[[ 1.,  2,  3,  4],\n                            [ 5,  6,  7,  8],\n                            [ 9, 10, 11, 12],\n                            [13, 14, 15, 16]]]])\n&gt;&gt;&gt; output, indices = pool(input)\n&gt;&gt;&gt; unpool(output, indices)\ntensor([[[[  0.,   0.,   0.,   0.],\n          [  0.,   6.,   0.,   8.],\n          [  0.,   0.,   0.,   0.],\n          [  0.,  14.,   0.,  16.]]]])\n\n&gt;&gt;&gt; # specify a different output size than input size\n&gt;&gt;&gt; unpool(output, indices, output_size=torch.Size([1, 1, 5, 5]))\ntensor([[[[  0.,   0.,   0.,   0.,   0.],\n          [  6.,   0.,   8.,   0.,   0.],\n          [  0.,   0.,   0.,  14.,   0.],\n          [ 16.,   0.,   0.,   0.,   0.],\n          [  0.,   0.,   0.,   0.,   0.]]]])\n\n", "shape": "\ninput: the input Tensor to invert\nindices: the indices given out by MaxPool2d\noutput_size (optional): the targeted output size\n\n"},
{"library": "torch", "item_id": "torch.nn.TransformerDecoderLayer.forward", "item_type": "method", "code": "forward(tgt:torch.Tensor,memory:torch.Tensor,tgt_mask:Optional[torch.Tensor]=None,memory_mask:Optional[torch.Tensor]=None,tgt_key_padding_mask:Optional[torch.Tensor]=None,memory_key_padding_mask:Optional[torch.Tensor]=None)\u2192torch.Tensor", "description": "Pass the inputs (and mask) through the decoder layer.  Parameters  tgt \u2013 the sequence to the decoder layer (required). memory \u2013 the sequence from the last layer of the encoder (required). tgt_mask \u2013 the mask for the tgt sequence (optional). memory_mask \u2013 the mask for the memory sequence (optional). tgt_key_padding_mask \u2013 the mask for the tgt keys per batch (optional). memory_key_padding_mask \u2013 the mask for the memory keys per batch (optional).     Shape:see the docs in Transformer class.   ", "parameters": ["tgt : the sequence to the decoder layer (required).", "memory : the sequence from the last layer of the encoder (required).", "tgt_mask : the mask for the tgt sequence (optional).", "memory_mask : the mask for the memory sequence (optional).", "tgt_key_padding_mask : the mask for the tgt keys per batch (optional).", "memory_key_padding_mask : the mask for the memory keys per batch (optional)."], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.TransformerDecoderLayer", "item_type": "class", "code": "classtorch.nn.TransformerDecoderLayer(d_model,nhead,dim_feedforward=2048,dropout=0.1,activation='relu')", "description": "TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network. This standard decoder layer is based on the paper \u201cAttention Is All You Need\u201d. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users may modify or implement in a different way during application.  Parameters  d_model \u2013 the number of expected features in the input (required). nhead \u2013 the number of heads in the multiheadattention models (required). dim_feedforward \u2013 the dimension of the feedforward network model (default=2048). dropout \u2013 the dropout value (default=0.1). activation \u2013 the activation function of intermediate layer, relu or gelu (default=relu).     Examples::&gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8) &gt;&gt;&gt; memory = torch.rand(10, 32, 512) &gt;&gt;&gt; tgt = torch.rand(20, 32, 512) &gt;&gt;&gt; out = decoder_layer(tgt, memory)       forward(tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: Optional[torch.Tensor] = None, memory_mask: Optional[torch.Tensor] = None, tgt_key_padding_mask: Optional[torch.Tensor] = None, memory_key_padding_mask: Optional[torch.Tensor] = None) \u2192 torch.Tensor  Pass the inputs (and mask) through the decoder layer.  Parameters  tgt \u2013 the sequence to the decoder layer (required). memory \u2013 the sequence from the last layer of the encoder (required). tgt_mask \u2013 the mask for the tgt sequence (optional). memory_mask \u2013 the mask for the memory sequence (optional). tgt_key_padding_mask \u2013 the mask for the tgt keys per batch (optional). memory_key_padding_mask \u2013 the mask for the memory keys per batch (optional).     Shape:see the docs in Transformer class.     ", "parameters": ["d_model : the number of expected features in the input (required).", "nhead : the number of heads in the multiheadattention models (required).", "dim_feedforward : the dimension of the feedforward network model (default=2048).", "dropout : the dropout value (default=0.1).", "activation : the activation function of intermediate layer, relu or gelu (default=relu).", "tgt : the sequence to the decoder layer (required).", "memory : the sequence from the last layer of the encoder (required).", "tgt_mask : the mask for the tgt sequence (optional).", "memory_mask : the mask for the memory sequence (optional).", "tgt_key_padding_mask : the mask for the tgt keys per batch (optional).", "memory_key_padding_mask : the mask for the memory keys per batch (optional)."], "returns": [], "example": "NA", "shape": "&gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n&gt;&gt;&gt; memory = torch.rand(10, 32, 512)\n&gt;&gt;&gt; tgt = torch.rand(20, 32, 512)\n&gt;&gt;&gt; out = decoder_layer(tgt, memory)\n\n\n"},
{"library": "torch", "item_id": "torch.nn.MaxUnpool1d", "item_type": "class", "code": "classtorch.nn.MaxUnpool1d(kernel_size:Union[T,Tuple[T]],stride:Optional[Union[T,Tuple[T]]]=None,padding:Union[T,Tuple[T]]=0)", "description": "Computes a partial inverse of MaxPool1d. MaxPool1d is not fully invertible, since the non-maximal values are lost. MaxUnpool1d takes in as input the output of MaxPool1d including the indices of the maximal values and computes a partial inverse in which all non-maximal values are set to zero.  Note MaxPool1d can map several input sizes to the same output sizes. Hence, the inversion process can get ambiguous. To accommodate this, you can provide the needed output size as an additional argument output_size in the forward call. See the Inputs and Example below.   Parameters  kernel_size (int or tuple) \u2013 Size of the max pooling window. stride (int or tuple) \u2013 Stride of the max pooling window. It is set to kernel_size by default. padding (int or tuple) \u2013 Padding that was added to the input     Inputs: input: the input Tensor to invert indices: the indices given out by MaxPool1d output_size (optional): the targeted output size   Shape: Input: (N,C,Hin)(N, C, H_{in})(N,C,Hin\u200b)   Output: (N,C,Hout)(N, C, H_{out})(N,C,Hout\u200b)  , where  Hout=(Hin\u22121)\u00d7stride[0]\u22122\u00d7padding[0]+kernel_size[0]H_{out} = (H_{in} - 1) \\times \\text{stride}[0] - 2 \\times \\text{padding}[0] + \\text{kernel\\_size}[0]  Hout\u200b=(Hin\u200b\u22121)\u00d7stride[0]\u22122\u00d7padding[0]+kernel_size[0]  or as given by output_size in the call operator     Example: &gt;&gt;&gt; pool = nn.MaxPool1d(2, stride=2, return_indices=True) &gt;&gt;&gt; unpool = nn.MaxUnpool1d(2, stride=2) &gt;&gt;&gt; input = torch.tensor([[[1., 2, 3, 4, 5, 6, 7, 8]]]) &gt;&gt;&gt; output, indices = pool(input) &gt;&gt;&gt; unpool(output, indices) tensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.]]])  &gt;&gt;&gt; # Example showcasing the use of output_size &gt;&gt;&gt; input = torch.tensor([[[1., 2, 3, 4, 5, 6, 7, 8, 9]]]) &gt;&gt;&gt; output, indices = pool(input) &gt;&gt;&gt; unpool(output, indices, output_size=input.size()) tensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.,  0.]]])  &gt;&gt;&gt; unpool(output, indices) tensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.]]])   ", "parameters": ["kernel_size (int or tuple) : Size of the max pooling window.", "stride (int or tuple) : Stride of the max pooling window.It is set to kernel_size by default.", "padding (int or tuple) : Padding that was added to the input"], "returns": [], "example": "&gt;&gt;&gt; pool = nn.MaxPool1d(2, stride=2, return_indices=True)\n&gt;&gt;&gt; unpool = nn.MaxUnpool1d(2, stride=2)\n&gt;&gt;&gt; input = torch.tensor([[[1., 2, 3, 4, 5, 6, 7, 8]]])\n&gt;&gt;&gt; output, indices = pool(input)\n&gt;&gt;&gt; unpool(output, indices)\ntensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.]]])\n\n&gt;&gt;&gt; # Example showcasing the use of output_size\n&gt;&gt;&gt; input = torch.tensor([[[1., 2, 3, 4, 5, 6, 7, 8, 9]]])\n&gt;&gt;&gt; output, indices = pool(input)\n&gt;&gt;&gt; unpool(output, indices, output_size=input.size())\ntensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.,  0.]]])\n\n&gt;&gt;&gt; unpool(output, indices)\ntensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.]]])\n\n", "shape": "\ninput: the input Tensor to invert\nindices: the indices given out by MaxPool1d\noutput_size (optional): the targeted output size\n\n"},
{"library": "torch", "item_id": "torch.promote_types", "item_type": "function", "code": "torch.promote_types(type1,type2)\u2192dtype", "description": "Returns the torch.dtype with the smallest size and scalar kind that is not smaller nor of lower kind than either type1 or type2. See type promotion documentation for more information on the type promotion logic.  Parameters  type1 (torch.dtype) \u2013  type2 (torch.dtype) \u2013     Example: &gt;&gt;&gt; torch.promote_types(torch.int32, torch.float32)) torch.float32 &gt;&gt;&gt; torch.promote_types(torch.uint8, torch.long) torch.long   ", "parameters": ["type1 (torch.dtype) : ", "type2 (torch.dtype) : "], "returns": [], "example": " torch.promote_types(torch.int32, torch.float32))\ntorch.float32\n torch.promote_types(torch.uint8, torch.long)\ntorch.long\n\n", "shape": ""},
{"library": "torch"},
{"library": "torch"},
{"library": "torch"},
{"library": "torch"},
{"library": "torch", "item_id": "torch.nn.utils.rnn.pad_sequence", "item_type": "function", "code": "torch.nn.utils.rnn.pad_sequence(sequences,batch_first=False,padding_value=0.0)", "description": "Pad a list of variable length Tensors with padding_value pad_sequence stacks a list of Tensors along a new dimension, and pads them to equal length. For example, if the input is list of sequences with size L x * and if batch_first is False, and T x B x * otherwise. B is batch size. It is equal to the number of elements in sequences. T is length of the longest sequence. L is length of the sequence. * is any number of trailing dimensions, including none. Example &gt;&gt;&gt; from torch.nn.utils.rnn import pad_sequence &gt;&gt;&gt; a = torch.ones(25, 300) &gt;&gt;&gt; b = torch.ones(22, 300) &gt;&gt;&gt; c = torch.ones(15, 300) &gt;&gt;&gt; pad_sequence([a, b, c]).size() torch.Size([25, 3, 300])    Note This function returns a Tensor of size T x B x * or B x T x * where T is the length of the longest sequence. This function assumes trailing dimensions and type of all the Tensors in sequences are same.   Parameters  sequences (list[Tensor]) \u2013 list of variable length sequences. batch_first (bool, optional) \u2013 output will be in B x T x * if True, or in T x B x * otherwise padding_value (float, optional) \u2013 value for padded elements. Default: 0.   Returns Tensor of size T x B x * if batch_first is False. Tensor of size B x T x * otherwise   ", "parameters": ["sequences (list[Tensor]) : list of variable length sequences.", "batch_first (bool, optional) : output will be in B x T x * if True, or inT x B x * otherwise", "padding_value (float, optional) : value for padded elements. Default: 0."], "returns": "Tensor of size T x B x * if batch_first is False.Tensor of size B x T x * otherwise", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.rnn.pack_sequence", "item_type": "function", "code": "torch.nn.utils.rnn.pack_sequence(sequences,enforce_sorted=True)", "description": "Packs a list of variable length Tensors sequences should be a list of Tensors of size L x *, where L is the length of a sequence and * is any number of trailing dimensions, including zero. For unsorted sequences, use enforce_sorted = False. If enforce_sorted is True, the sequences should be sorted in the order of decreasing length. enforce_sorted = True is only necessary for ONNX export. Example &gt;&gt;&gt; from torch.nn.utils.rnn import pack_sequence &gt;&gt;&gt; a = torch.tensor([1,2,3]) &gt;&gt;&gt; b = torch.tensor([4,5]) &gt;&gt;&gt; c = torch.tensor([6]) &gt;&gt;&gt; pack_sequence([a, b, c]) PackedSequence(data=tensor([ 1,  4,  6,  2,  5,  3]), batch_sizes=tensor([ 3,  2,  1]))    Parameters  sequences (list[Tensor]) \u2013 A list of sequences of decreasing length. enforce_sorted (bool, optional) \u2013 if True, checks that the input contains sequences sorted by length in a decreasing order. If False, this condition is not checked. Default: True.   Returns a PackedSequence object   ", "parameters": ["sequences (list[Tensor]) : A list of sequences of decreasing length.", "enforce_sorted (bool, optional) : if True, checks that the inputcontains sequences sorted by length in a decreasing order. IfFalse, this condition is not checked. Default: True."], "returns": "a PackedSequence object", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Flatten.add_module", "item_type": "method", "code": "add_module(name:str,module:Optional[Module])\u2192None", "description": "Adds a child module to the current module. The module can be accessed as an attribute using the given name.  Parameters  name (string) \u2013 name of the child module. The child module can be accessed from this module using the given name module (Module) \u2013 child module to be added to the module.    ", "parameters": ["name (string) : name of the child module. The child module can beaccessed from this module using the given name", "module (Module) : child module to be added to the module."], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Flatten.apply", "item_type": "method", "code": "apply(fn:Callable[Module,None])\u2192T", "description": "Applies fn recursively to every submodule (as returned by .children()) as well as self. Typical use includes initializing the parameters of a model (see also torch.nn.init).  Parameters fn (Module -&gt; None) \u2013 function to be applied to each submodule  Returns self  Return type Module   Example: &gt;&gt;&gt; @torch.no_grad() &gt;&gt;&gt; def init_weights(m): &gt;&gt;&gt;     print(m) &gt;&gt;&gt;     if type(m) == nn.Linear: &gt;&gt;&gt;         m.weight.fill_(1.0) &gt;&gt;&gt;         print(m.weight) &gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) &gt;&gt;&gt; net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1.,  1.],         [ 1.,  1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1.,  1.],         [ 1.,  1.]]) Sequential(   (0): Linear(in_features=2, out_features=2, bias=True)   (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential(   (0): Linear(in_features=2, out_features=2, bias=True)   (1): Linear(in_features=2, out_features=2, bias=True) )   ", "parameters": ["fn (Module -&gt; None) : function to be applied to each submodule", "self", "Module"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.remove_spectral_norm", "item_type": "function", "code": "torch.nn.utils.remove_spectral_norm(module,name='weight')", "description": "Removes the spectral normalization reparameterization from a module.  Parameters  module (Module) \u2013 containing module name (str, optional) \u2013 name of weight parameter    Example &gt;&gt;&gt; m = spectral_norm(nn.Linear(40, 10)) &gt;&gt;&gt; remove_spectral_norm(m)   ", "parameters": ["module (Module) : containing module", "name (str, optional) : name of weight parameter"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.rnn.PackedSequence.batch_sizes", "item_type": "method", "code": "propertybatch_sizes", "description": "Alias for field number 1 ", "parameters": [], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.rnn.PackedSequence.count", "item_type": "method", "code": "count(value)\u2192integer--returnnumberofoccurrencesofvalue", "description": "", "parameters": [], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.rnn.PackedSequence.data", "item_type": "method", "code": "propertydata", "description": "Alias for field number 0 ", "parameters": [], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.rnn.PackedSequence.index", "item_type": "method", "code": "index(value[,start[,stop]])\u2192integer--returnfirstindexofvalue.", "description": "Raises ValueError if the value is not present. ", "parameters": [], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.rnn.PackedSequence.is_cuda", "item_type": "method", "code": "propertyis_cuda", "description": "Returns true if self.data stored on a gpu ", "parameters": [], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.rnn.PackedSequence.is_pinned", "item_type": "method", "code": "is_pinned()", "description": "Returns true if self.data stored on in pinned memory ", "parameters": [], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.rnn.PackedSequence.sorted_indices", "item_type": "method", "code": "propertysorted_indices", "description": "Alias for field number 2 ", "parameters": [], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.rnn.pack_padded_sequence", "item_type": "function", "code": "torch.nn.utils.rnn.pack_padded_sequence(input,lengths,batch_first=False,enforce_sorted=True)", "description": "Packs a Tensor containing padded sequences of variable length. input can be of size T x B x * where T is the length of the longest sequence (equal to lengths[0]), B is the batch size, and * is any number of dimensions (including 0). If batch_first is True, B x T x * input is expected. For unsorted sequences, use enforce_sorted = False. If enforce_sorted is True, the sequences should be sorted by length in a decreasing order, i.e. input[:,0] should be the longest sequence, and input[:,B-1] the shortest one. enforce_sorted = True is only necessary for ONNX export.  Note This function accepts any input that has at least two dimensions. You can apply it to pack the labels, and use the output of the RNN with them to compute the loss directly. A Tensor can be retrieved from a PackedSequence object by accessing its .data attribute.   Parameters  input (Tensor) \u2013 padded batch of variable length sequences. lengths (Tensor) \u2013 list of sequences lengths of each batch element. batch_first (bool, optional) \u2013 if True, the input is expected in B x T x * format. enforce_sorted (bool, optional) \u2013 if True, the input is expected to contain sequences sorted by length in a decreasing order. If False, the input will get sorted unconditionally. Default: True.   Returns a PackedSequence object   ", "parameters": ["input (Tensor) : padded batch of variable length sequences.", "lengths (Tensor) : list of sequences lengths of each batch element.", "batch_first (bool, optional) : if True, the input is expected in B x T x *format.", "enforce_sorted (bool, optional) : if True, the input is expected tocontain sequences sorted by length in a decreasing order. IfFalse, the input will get sorted unconditionally. Default: True."], "returns": "a PackedSequence object", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.spectral_norm", "item_type": "function", "code": "torch.nn.utils.spectral_norm(module,name='weight',n_power_iterations=1,eps=1e-12,dim=None)", "description": "Applies spectral normalization to a parameter in the given module.  WSN=W\u03c3(W),\u03c3(W)=max\u2061h:h\u22600\u2225Wh\u22252\u2225h\u22252\\mathbf{W}_{SN} = \\dfrac{\\mathbf{W}}{\\sigma(\\mathbf{W})}, \\sigma(\\mathbf{W}) = \\max_{\\mathbf{h}: \\mathbf{h} \\ne 0} \\dfrac{\\|\\mathbf{W} \\mathbf{h}\\|_2}{\\|\\mathbf{h}\\|_2}  WSN\u200b=\u03c3(W)W\u200b,\u03c3(W)=h:h\ue020\u200b=0max\u200b\u2225h\u22252\u200b\u2225Wh\u22252\u200b\u200b  Spectral normalization stabilizes the training of discriminators (critics) in Generative Adversarial Networks (GANs) by rescaling the weight tensor with spectral norm \u03c3\\sigma\u03c3   of the weight matrix calculated using power iteration method. If the dimension of the weight tensor is greater than 2, it is reshaped to 2D in power iteration method to get spectral norm. This is implemented via a hook that calculates spectral norm and rescales weight before every forward() call. See Spectral Normalization for Generative Adversarial Networks .  Parameters  module (nn.Module) \u2013 containing module name (str, optional) \u2013 name of weight parameter n_power_iterations (int, optional) \u2013 number of power iterations to calculate spectral norm eps (float, optional) \u2013 epsilon for numerical stability in calculating norms dim (int, optional) \u2013 dimension corresponding to number of outputs, the default is 0, except for modules that are instances of ConvTranspose{1,2,3}d, when it is 1   Returns The original module with the spectral norm hook   Example: &gt;&gt;&gt; m = spectral_norm(nn.Linear(20, 40)) &gt;&gt;&gt; m Linear(in_features=20, out_features=40, bias=True) &gt;&gt;&gt; m.weight_u.size() torch.Size([40])   ", "parameters": ["module (nn.Module) : containing module", "name (str, optional) : name of weight parameter", "n_power_iterations (int, optional) : number of power iterations tocalculate spectral norm", "eps (float, optional) : epsilon for numerical stability incalculating norms", "dim (int, optional) : dimension corresponding to number of outputs,the default is 0, except for modules that are instances ofConvTranspose{1,2,3}d, when it is 1"], "returns": "The original module with the spectral norm hook", "example": "&gt;&gt;&gt; m = spectral_norm(nn.Linear(20, 40))\n&gt;&gt;&gt; m\nLinear(in_features=20, out_features=40, bias=True)\n&gt;&gt;&gt; m.weight_u.size()\ntorch.Size([40])\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.rnn.pad_packed_sequence", "item_type": "function", "code": "torch.nn.utils.rnn.pad_packed_sequence(sequence,batch_first=False,padding_value=0.0,total_length=None)", "description": "Pads a packed batch of variable length sequences. It is an inverse operation to pack_padded_sequence(). The returned Tensor\u2019s data will be of size T x B x *, where T is the length of the longest sequence and B is the batch size. If batch_first is True, the data will be transposed into B x T x * format. Example &gt;&gt;&gt; from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence &gt;&gt;&gt; seq = torch.tensor([[1,2,0], [3,0,0], [4,5,6]]) &gt;&gt;&gt; lens = [2, 1, 3] &gt;&gt;&gt; packed = pack_padded_sequence(seq, lens, batch_first=True, enforce_sorted=False) &gt;&gt;&gt; packed PackedSequence(data=tensor([4, 1, 3, 5, 2, 6]), batch_sizes=tensor([3, 2, 1]),                sorted_indices=tensor([2, 0, 1]), unsorted_indices=tensor([1, 2, 0])) &gt;&gt;&gt; seq_unpacked, lens_unpacked = pad_packed_sequence(packed, batch_first=True) &gt;&gt;&gt; seq_unpacked tensor([[1, 2, 0],         [3, 0, 0],         [4, 5, 6]]) &gt;&gt;&gt; lens_unpacked tensor([2, 1, 3])    Note total_length is useful to implement the pack sequence -&gt; recurrent network -&gt; unpack sequence pattern in a Module wrapped in DataParallel. See this FAQ section for details.   Parameters  sequence (PackedSequence) \u2013 batch to pad batch_first (bool, optional) \u2013 if True, the output will be in B x T x * format. padding_value (float, optional) \u2013 values for padded elements. total_length (int, optional) \u2013 if not None, the output will be padded to have length total_length. This method will throw ValueError if total_length is less than the max sequence length in sequence.   Returns Tuple of Tensor containing the padded sequence, and a Tensor containing the list of lengths of each sequence in the batch. Batch elements will be re-ordered as they were ordered originally when the batch was passed to pack_padded_sequence or pack_sequence.   ", "parameters": ["sequence (PackedSequence) : batch to pad", "batch_first (bool, optional) : if True, the output will be in B x T x *format.", "padding_value (float, optional) : values for padded elements.", "total_length (int, optional) : if not None, the output will be padded tohave length total_length. This method will throw ValueErrorif total_length is less than the max sequence length insequence."], "returns": "Tuple of Tensor containing the padded sequence, and a Tensorcontaining the list of lengths of each sequence in the batch.Batch elements will be re-ordered as they were ordered originally whenthe batch was passed to pack_padded_sequence or pack_sequence.", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.is_pruned", "item_type": "function", "code": "torch.nn.utils.prune.is_pruned(module)", "description": "Check whether module is pruned by looking for forward_pre_hooks in its modules that inherit from the BasePruningMethod.  Parameters module (nn.Module) \u2013 object that is either pruned or unpruned  Returns binary answer to whether module is pruned.   Examples &gt;&gt;&gt; m = nn.Linear(5, 7) &gt;&gt;&gt; print(prune.is_pruned(m)) False &gt;&gt;&gt; prune.random_unstructured(m, name='weight', amount=0.2) &gt;&gt;&gt; print(prune.is_pruned(m)) True   ", "parameters": ["module (nn.Module) : object that is either pruned or unpruned", "binary answer to whether module is pruned."], "returns": "binary answer to whether module is pruned.", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Flatten.bfloat16", "item_type": "method", "code": "bfloat16()\u2192T", "description": "Casts all floating point parameters and buffers to bfloat16 datatype.  Returns self  Return type Module   ", "parameters": ["self", "Module"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Flatten.buffers", "item_type": "method", "code": "buffers(recurse:bool=True)\u2192Iterator[torch.Tensor]", "description": "Returns an iterator over module buffers.  Parameters recurse (bool) \u2013 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.  Yields torch.Tensor \u2013 module buffer   Example: &gt;&gt;&gt; for buf in model.buffers(): &gt;&gt;&gt;     print(type(buf), buf.size()) &lt;class 'torch.Tensor'&gt; (20L,) &lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)   ", "parameters": ["recurse (bool) : if True, then yields buffers of this moduleand all submodules. Otherwise, yields only buffers thatare direct members of this module.", "torch.Tensor : module buffer"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.weight_norm", "item_type": "function", "code": "torch.nn.utils.weight_norm(module,name='weight',dim=0)", "description": "Applies weight normalization to a parameter in the given module.  w=gv\u2225v\u2225\\mathbf{w} = g \\dfrac{\\mathbf{v}}{\\|\\mathbf{v}\\|}  w=g\u2225v\u2225v\u200b  Weight normalization is a reparameterization that decouples the magnitude of a weight tensor from its direction. This replaces the parameter specified by name (e.g. 'weight') with two parameters: one specifying the magnitude (e.g. 'weight_g') and one specifying the direction (e.g. 'weight_v'). Weight normalization is implemented via a hook that recomputes the weight tensor from the magnitude and direction before every forward() call. By default, with dim=0, the norm is computed independently per output channel/plane. To compute a norm over the entire weight tensor, use dim=None. See https://arxiv.org/abs/1602.07868  Parameters  module (Module) \u2013 containing module name (str, optional) \u2013 name of weight parameter dim (int, optional) \u2013 dimension over which to compute the norm   Returns The original module with the weight norm hook   Example: &gt;&gt;&gt; m = weight_norm(nn.Linear(20, 40), name='weight') &gt;&gt;&gt; m Linear(in_features=20, out_features=40, bias=True) &gt;&gt;&gt; m.weight_g.size() torch.Size([40, 1]) &gt;&gt;&gt; m.weight_v.size() torch.Size([40, 20])   ", "parameters": ["module (Module) : containing module", "name (str, optional) : name of weight parameter", "dim (int, optional) : dimension over which to compute the norm"], "returns": "The original module with the weight norm hook", "example": "&gt;&gt;&gt; m = weight_norm(nn.Linear(20, 40), name='weight')\n&gt;&gt;&gt; m\nLinear(in_features=20, out_features=40, bias=True)\n&gt;&gt;&gt; m.weight_g.size()\ntorch.Size([40, 1])\n&gt;&gt;&gt; m.weight_v.size()\ntorch.Size([40, 20])\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.remove_weight_norm", "item_type": "function", "code": "torch.nn.utils.remove_weight_norm(module,name='weight')", "description": "Removes the weight normalization reparameterization from a module.  Parameters  module (Module) \u2013 containing module name (str, optional) \u2013 name of weight parameter    Example &gt;&gt;&gt; m = weight_norm(nn.Linear(20, 40)) &gt;&gt;&gt; remove_weight_norm(m)   ", "parameters": ["module (Module) : containing module", "name (str, optional) : name of weight parameter"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.rnn.PackedSequence.to", "item_type": "method", "code": "to(*args,**kwargs)", "description": "Performs dtype and/or device conversion on self.data. It has similar signature as torch.Tensor.to(), except optional arguments like non_blocking and copy should be passed as kwargs, not args, or they will not apply to the index tensors.  Note If the self.data Tensor already has the correct torch.dtype and torch.device, then self is returned. Otherwise, returns a copy with the desired configuration.  ", "parameters": [], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.rnn.PackedSequence.unsorted_indices", "item_type": "method", "code": "propertyunsorted_indices", "description": "Alias for field number 3 ", "parameters": [], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.rnn.PackedSequence", "item_type": "class", "code": "classtorch.nn.utils.rnn.PackedSequence", "description": "Holds the data and list of batch_sizes of a packed sequence. All RNN modules accept packed sequences as inputs.  Note Instances of this class should never be created manually. They are meant to be instantiated by functions like pack_padded_sequence(). Batch sizes represent the number elements at each sequence step in the batch, not the varying sequence lengths passed to pack_padded_sequence().  For instance, given data abc and x the PackedSequence would contain data axbc with batch_sizes=[2,1,1].   Variables  ~PackedSequence.data (Tensor) \u2013 Tensor containing packed sequence ~PackedSequence.batch_sizes (Tensor) \u2013 Tensor of integers holding information about the batch size at each sequence step ~PackedSequence.sorted_indices (Tensor, optional) \u2013 Tensor of integers holding how this PackedSequence is constructed from sequences. ~PackedSequence.unsorted_indices (Tensor, optional) \u2013 Tensor of integers holding how this to recover the original sequences with correct order.     Note data can be on arbitrary device and of arbitrary dtype. sorted_indices and unsorted_indices must be torch.int64 tensors on the same device as data. However, batch_sizes should always be a CPU torch.int64 tensor. This invariant is maintained throughout PackedSequence class, and all functions that construct a :class:PackedSequence in PyTorch (i.e., they only pass in tensors conforming to this constraint).    property batch_sizes Alias for field number 1     count(value) \u2192 integer -- return number of occurrences of value     property data Alias for field number 0     index(value[, start[, stop]]) \u2192 integer -- return first index of value. Raises ValueError if the value is not present.     property is_cuda Returns true if self.data stored on a gpu     is_pinned()  Returns true if self.data stored on in pinned memory     property sorted_indices Alias for field number 2     to(*args, **kwargs)  Performs dtype and/or device conversion on self.data. It has similar signature as torch.Tensor.to(), except optional arguments like non_blocking and copy should be passed as kwargs, not args, or they will not apply to the index tensors.  Note If the self.data Tensor already has the correct torch.dtype and torch.device, then self is returned. Otherwise, returns a copy with the desired configuration.      property unsorted_indices Alias for field number 3   ", "parameters": [], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Flatten.children", "item_type": "method", "code": "children()\u2192Iterator[torch.nn.modules.module.Module]", "description": "Returns an iterator over immediate children modules.  Yields Module \u2013 a child module   ", "parameters": ["Module : a child module"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Flatten.cpu", "item_type": "method", "code": "cpu()\u2192T", "description": "Moves all model parameters and buffers to the CPU.  Returns self  Return type Module   ", "parameters": ["self", "Module"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Flatten.cuda", "item_type": "method", "code": "cuda(device:Union[int,torch.device,None]=None)\u2192T", "description": "Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.  Parameters device (int, optional) \u2013 if specified, all parameters will be copied to that device  Returns self  Return type Module   ", "parameters": ["device (int, optional) : if specified, all parameters will becopied to that device", "self", "Module"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.remove", "item_type": "function", "code": "torch.nn.utils.prune.remove(module,name)", "description": "Removes the pruning reparameterization from a module and the pruning method from the forward hook. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!   Parameters  module (nn.Module) \u2013 module containing the tensor to prune name (str) \u2013 parameter name within module on which pruning will act.    Examples &gt;&gt;&gt; m = random_unstructured(nn.Linear(5, 7), name='weight', amount=0.2) &gt;&gt;&gt; m = remove(m, name='weight')   ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "name (str) : parameter name within module on which pruningwill act."], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.custom_from_mask", "item_type": "function", "code": "torch.nn.utils.prune.custom_from_mask(module,name,mask)", "description": "Prunes tensor corresponding to parameter called name in module by applying the pre-computed mask in mask. Modifies module in place (and also return the modified module) by: 1) adding a named buffer called name+'_mask' corresponding to the binary mask applied to the parameter name by the pruning method. 2) replacing the parameter name by its pruned version, while the original (unpruned) parameter is stored in a new parameter named name+'_orig'.  Parameters  module (nn.Module) \u2013 module containing the tensor to prune name (str) \u2013 parameter name within module on which pruning will act. mask (Tensor) \u2013 binary mask to be applied to the parameter.   Returns modified (i.e. pruned) version of the input module  Return type module (nn.Module)   Examples &gt;&gt;&gt; m = prune.custom_from_mask(         nn.Linear(5, 3), name='bias', mask=torch.Tensor([0, 1, 0])     ) &gt;&gt;&gt; print(m.bias_mask) tensor([0., 1., 0.])   ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "name (str) : parameter name within module on which pruningwill act.", "mask (Tensor) : binary mask to be applied to the parameter."], "returns": "modified (i.e. pruned) version of the input module", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Flatten.double", "item_type": "method", "code": "double()\u2192T", "description": "Casts all floating point parameters and buffers to double datatype.  Returns self  Return type Module   ", "parameters": ["self", "Module"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Flatten.eval", "item_type": "method", "code": "eval()\u2192T", "description": "Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc. This is equivalent with self.train(False).  Returns self  Return type Module   ", "parameters": ["self", "Module"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.ln_structured", "item_type": "function", "code": "torch.nn.utils.prune.ln_structured(module,name,amount,n,dim)", "description": "Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) channels along the specified dim with the lowest L``n``-norm. Modifies module in place (and also return the modified module) by: 1) adding a named buffer called name+'_mask' corresponding to the binary mask applied to the parameter name by the pruning method. 2) replacing the parameter name by its pruned version, while the original (unpruned) parameter is stored in a new parameter named name+'_orig'.  Parameters  module (nn.Module) \u2013 module containing the tensor to prune name (str) \u2013 parameter name within module on which pruning will act. amount (int or float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune. n (int, float, inf, -inf, 'fro', 'nuc') \u2013 See documentation of valid entries for argument p in torch.norm(). dim (int) \u2013 index of the dim along which we define channels to prune.   Returns modified (i.e. pruned) version of the input module  Return type module (nn.Module)   Examples &gt;&gt;&gt; m = prune.ln_structured(        nn.Conv2d(5, 3, 2), 'weight', amount=0.3, dim=1, n=float('-inf')     )   ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "name (str) : parameter name within module on which pruningwill act.", "amount (int or float) : quantity of parameters to prune.If float, should be between 0.0 and 1.0 and represent thefraction of parameters to prune. If int, it represents theabsolute number of parameters to prune.", "n (int, float, inf, -inf, 'fro', 'nuc') : See documentation of validentries for argument p in torch.norm().", "dim (int) : index of the dim along which we define channels to prune."], "returns": "modified (i.e. pruned) version of the input module", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.random_structured", "item_type": "function", "code": "torch.nn.utils.prune.random_structured(module,name,amount,dim)", "description": "Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) channels along the specified dim selected at random. Modifies module in place (and also return the modified module) by: 1) adding a named buffer called name+'_mask' corresponding to the binary mask applied to the parameter name by the pruning method. 2) replacing the parameter name by its pruned version, while the original (unpruned) parameter is stored in a new parameter named name+'_orig'.  Parameters  module (nn.Module) \u2013 module containing the tensor to prune name (str) \u2013 parameter name within module on which pruning will act. amount (int or float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune. dim (int) \u2013 index of the dim along which we define channels to prune.   Returns modified (i.e. pruned) version of the input module  Return type module (nn.Module)   Examples &gt;&gt;&gt; m = prune.random_structured(         nn.Linear(5, 3), 'weight', amount=3, dim=1     ) &gt;&gt;&gt; columns_pruned = int(sum(torch.sum(m.weight, dim=0) == 0)) &gt;&gt;&gt; print(columns_pruned) 3   ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "name (str) : parameter name within module on which pruningwill act.", "amount (int or float) : quantity of parameters to prune.If float, should be between 0.0 and 1.0 and represent thefraction of parameters to prune. If int, it represents theabsolute number of parameters to prune.", "dim (int) : index of the dim along which we define channels to prune."], "returns": "modified (i.e. pruned) version of the input module", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.l1_unstructured", "item_type": "function", "code": "torch.nn.utils.prune.l1_unstructured(module,name,amount)", "description": "Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) units with the lowest L1-norm. Modifies module in place (and also return the modified module) by: 1) adding a named buffer called name+'_mask' corresponding to the binary mask applied to the parameter name by the pruning method. 2) replacing the parameter name by its pruned version, while the original (unpruned) parameter is stored in a new parameter named name+'_orig'.  Parameters  module (nn.Module) \u2013 module containing the tensor to prune name (str) \u2013 parameter name within module on which pruning will act. amount (int or float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune.   Returns modified (i.e. pruned) version of the input module  Return type module (nn.Module)   Examples &gt;&gt;&gt; m = prune.l1_unstructured(nn.Linear(2, 3), 'weight', amount=0.2) &gt;&gt;&gt; m.state_dict().keys() odict_keys(['bias', 'weight_orig', 'weight_mask'])   ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "name (str) : parameter name within module on which pruningwill act.", "amount (int or float) : quantity of parameters to prune.If float, should be between 0.0 and 1.0 and represent thefraction of parameters to prune. If int, it represents theabsolute number of parameters to prune."], "returns": "modified (i.e. pruned) version of the input module", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.global_unstructured", "item_type": "function", "code": "torch.nn.utils.prune.global_unstructured(parameters,pruning_method,**kwargs)", "description": "Globally prunes tensors corresponding to all parameters in parameters by applying the specified pruning_method. Modifies modules in place by: 1) adding a named buffer called name+'_mask' corresponding to the binary mask applied to the parameter name by the pruning method. 2) replacing the parameter name by its pruned version, while the original (unpruned) parameter is stored in a new parameter named name+'_orig'.  Parameters  parameters (Iterable of (module, name) tuples) \u2013 parameters of the model to prune in a global fashion, i.e. by aggregating all weights prior to deciding which ones to prune. module must be of type nn.Module, and name must be a string. pruning_method (function) \u2013 a valid pruning function from this module, or a custom one implemented by the user that satisfies the implementation guidelines and has PRUNING_TYPE='unstructured'. kwargs \u2013 other keyword arguments such as: amount (int or float): quantity of parameters to prune across the specified parameters. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune.   Raises TypeError \u2013 if PRUNING_TYPE != 'unstructured'    Note Since global structured pruning doesn\u2019t make much sense unless the norm is normalized by the size of the parameter, we now limit the scope of global pruning to unstructured methods.  Examples &gt;&gt;&gt; net = nn.Sequential(OrderedDict([         ('first', nn.Linear(10, 4)),         ('second', nn.Linear(4, 1)),     ])) &gt;&gt;&gt; parameters_to_prune = (         (net.first, 'weight'),         (net.second, 'weight'),     ) &gt;&gt;&gt; prune.global_unstructured(         parameters_to_prune,         pruning_method=prune.L1Unstructured,         amount=10,     ) &gt;&gt;&gt; print(sum(torch.nn.utils.parameters_to_vector(net.buffers()) == 0)) tensor(10, dtype=torch.uint8)   ", "parameters": ["parameters (Iterable of (module, name) tuples) : parameters ofthe model to prune in a global fashion, i.e. by aggregating allweights prior to deciding which ones to prune. module must be oftype nn.Module, and name must be a string.", "pruning_method (function) : a valid pruning function from this module,or a custom one implemented by the user that satisfies theimplementation guidelines and has PRUNING_TYPE='unstructured'.", "kwargs : other keyword arguments such as:amount (int or float): quantity of parameters to prune across thespecified parameters.If float, should be between 0.0 and 1.0 and represent thefraction of parameters to prune. If int, it represents theabsolute number of parameters to prune."], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.identity", "item_type": "function", "code": "torch.nn.utils.prune.identity(module,name)", "description": "Applies pruning reparametrization to the tensor corresponding to the parameter called name in module without actually pruning any units. Modifies module in place (and also return the modified module) by: 1) adding a named buffer called name+'_mask' corresponding to the binary mask applied to the parameter name by the pruning method. 2) replacing the parameter name by its pruned version, while the original (unpruned) parameter is stored in a new parameter named name+'_orig'.  Note The mask is a tensor of ones.   Parameters  module (nn.Module) \u2013 module containing the tensor to prune. name (str) \u2013 parameter name within module on which pruning will act.   Returns modified (i.e. pruned) version of the input module  Return type module (nn.Module)   Examples &gt;&gt;&gt; m = prune.identity(nn.Linear(2, 3), 'bias') &gt;&gt;&gt; print(m.bias_mask) tensor([1., 1., 1.])   ", "parameters": ["module (nn.Module) : module containing the tensor to prune.", "name (str) : parameter name within module on which pruningwill act."], "returns": "modified (i.e. pruned) version of the input module", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Flatten.extra_repr", "item_type": "method", "code": "extra_repr()\u2192str", "description": "Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. ", "parameters": [], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Flatten.float", "item_type": "method", "code": "float()\u2192T", "description": "Casts all floating point parameters and buffers to float datatype.  Returns self  Return type Module   ", "parameters": ["self", "Module"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Flatten.half", "item_type": "method", "code": "half()\u2192T", "description": "Casts all floating point parameters and buffers to half datatype.  Returns self  Return type Module   ", "parameters": ["self", "Module"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.LnStructured.apply", "item_type": "method", "code": "classmethodapply(module,name,amount,n,dim)", "description": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters  module (nn.Module) \u2013 module containing the tensor to prune name (str) \u2013 parameter name within module on which pruning will act. amount (int or float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune. n (int, float, inf, -inf, 'fro', 'nuc') \u2013 See documentation of valid entries for argument p in torch.norm(). dim (int) \u2013 index of the dim along which we define channels to prune.    ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "name (str) : parameter name within module on which pruningwill act.", "amount (int or float) : quantity of parameters to prune.If float, should be between 0.0 and 1.0 and represent thefraction of parameters to prune. If int, it represents theabsolute number of parameters to prune.", "n (int, float, inf, -inf, 'fro', 'nuc') : See documentation of validentries for argument p in torch.norm().", "dim (int) : index of the dim along which we define channels toprune."], "returns": "pruned version of the input tensor", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.LnStructured.apply_mask", "item_type": "method", "code": "apply_mask(module)", "description": "Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters module (nn.Module) \u2013 module containing the tensor to prune  Returns pruned version of the input tensor  Return type pruned_tensor (torch.Tensor)   ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)"], "returns": "pruned version of the input tensor", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.CustomFromMask.apply", "item_type": "method", "code": "classmethodapply(module,name,mask)", "description": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters  module (nn.Module) \u2013 module containing the tensor to prune name (str) \u2013 parameter name within module on which pruning will act.    ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "name (str) : parameter name within module on which pruningwill act."], "returns": "pruned version of the input tensor", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.CustomFromMask.apply_mask", "item_type": "method", "code": "apply_mask(module)", "description": "Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters module (nn.Module) \u2013 module containing the tensor to prune  Returns pruned version of the input tensor  Return type pruned_tensor (torch.Tensor)   ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)"], "returns": "pruned version of the input tensor", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.CustomFromMask.prune", "item_type": "method", "code": "prune(t,default_mask=None)", "description": "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters  t (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). default_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns pruned version of tensor t.   ", "parameters": ["t (torch.Tensor) : tensor to prune (of same dimensions asdefault_mask).", "default_mask (torch.Tensor, optional) : mask from previous pruningiteration, if any. To be considered when determining whatportion of the tensor that pruning should act on. If None,default to a mask of ones."], "returns": "pruned version of the input tensor", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.CustomFromMask.remove", "item_type": "method", "code": "remove(module)", "description": "Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!  ", "parameters": [], "returns": "pruned version of the input tensor", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.RandomStructured.apply", "item_type": "method", "code": "classmethodapply(module,name,amount,dim=-1)", "description": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters  module (nn.Module) \u2013 module containing the tensor to prune name (str) \u2013 parameter name within module on which pruning will act. amount (int or float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune. dim (int, optional) \u2013 index of the dim along which we define channels to prune. Default: -1.    ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "name (str) : parameter name within module on which pruningwill act.", "amount (int or float) : quantity of parameters to prune.If float, should be between 0.0 and 1.0 and represent thefraction of parameters to prune. If int, it represents theabsolute number of parameters to prune.", "dim (int, optional) : index of the dim along which we definechannels to prune. Default: -1."], "returns": "pruned version of the input tensor", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.RandomStructured.apply_mask", "item_type": "method", "code": "apply_mask(module)", "description": "Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters module (nn.Module) \u2013 module containing the tensor to prune  Returns pruned version of the input tensor  Return type pruned_tensor (torch.Tensor)   ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)"], "returns": "pruned version of the input tensor", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.RandomStructured.compute_mask", "item_type": "method", "code": "compute_mask(t,default_mask)", "description": "Computes and returns a mask for the input tensor t. Starting from a base default_mask (which should be a mask of ones if the tensor has not been pruned yet), generate a random mask to apply on top of the default_mask by randomly zeroing out channels along the specified dim of the tensor.  Parameters  t (torch.Tensor) \u2013 tensor representing the parameter to prune default_mask (torch.Tensor) \u2013 Base mask from previous pruning iterations, that need to be respected after the new mask is applied. Same dims as t.   Returns mask to apply to t, of same dims as t  Return type mask (torch.Tensor)  Raises IndexError \u2013 if self.dim &gt;= len(t.shape)   ", "parameters": ["t (torch.Tensor) : tensor representing the parameter to prune", "default_mask (torch.Tensor) : Base mask from previous pruningiterations, that need to be respected after the new mask isapplied. Same dims as t."], "returns": "pruned version of the input tensor", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Flatten.load_state_dict", "item_type": "method", "code": "load_state_dict(state_dict:Dict[str,torch.Tensor],strict:bool=True)", "description": "Copies parameters and buffers from state_dict into this module and its descendants. If strict is True, then the keys of state_dict must exactly match the keys returned by this module\u2019s state_dict() function.  Parameters  state_dict (dict) \u2013 a dict containing parameters and persistent buffers. strict (bool, optional) \u2013 whether to strictly enforce that the keys in state_dict match the keys returned by this module\u2019s state_dict() function. Default: True   Returns  missing_keys is a list of str containing the missing keys unexpected_keys is a list of str containing the unexpected keys    Return type NamedTuple with missing_keys and unexpected_keys fields   ", "parameters": ["state_dict (dict) : a dict containing parameters andpersistent buffers.", "strict (bool, optional) : whether to strictly enforce that the keysin state_dict match the keys returned by this module\u2019sstate_dict() function. Default: True", "missing_keys is a list of str containing the missing keys", "unexpected_keys is a list of str containing the unexpected keys"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Flatten.modules", "item_type": "method", "code": "modules()\u2192Iterator[torch.nn.modules.module.Module]", "description": "Returns an iterator over all modules in the network.  Yields Module \u2013 a module in the network    Note Duplicate modules are returned only once. In the following example, l will be returned only once.  Example: &gt;&gt;&gt; l = nn.Linear(2, 2) &gt;&gt;&gt; net = nn.Sequential(l, l) &gt;&gt;&gt; for idx, m in enumerate(net.modules()):         print(idx, '-&gt;', m)  0 -&gt; Sequential(   (0): Linear(in_features=2, out_features=2, bias=True)   (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -&gt; Linear(in_features=2, out_features=2, bias=True)   ", "parameters": ["Module : a module in the network"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.L1Unstructured.apply", "item_type": "method", "code": "classmethodapply(module,name,amount)", "description": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters  module (nn.Module) \u2013 module containing the tensor to prune name (str) \u2013 parameter name within module on which pruning will act. amount (int or float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune.    ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "name (str) : parameter name within module on which pruningwill act.", "amount (int or float) : quantity of parameters to prune.If float, should be between 0.0 and 1.0 and represent thefraction of parameters to prune. If int, it represents theabsolute number of parameters to prune."], "returns": "pruned version of the input tensor", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.L1Unstructured.apply_mask", "item_type": "method", "code": "apply_mask(module)", "description": "Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters module (nn.Module) \u2013 module containing the tensor to prune  Returns pruned version of the input tensor  Return type pruned_tensor (torch.Tensor)   ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)"], "returns": "pruned version of the input tensor", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.L1Unstructured.prune", "item_type": "method", "code": "prune(t,default_mask=None)", "description": "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters  t (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). default_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns pruned version of tensor t.   ", "parameters": ["t (torch.Tensor) : tensor to prune (of same dimensions asdefault_mask).", "default_mask (torch.Tensor, optional) : mask from previous pruningiteration, if any. To be considered when determining whatportion of the tensor that pruning should act on. If None,default to a mask of ones."], "returns": "pruned version of the input tensor", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.random_unstructured", "item_type": "function", "code": "torch.nn.utils.prune.random_unstructured(module,name,amount)", "description": "Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) units selected at random. Modifies module in place (and also return the modified module) by: 1) adding a named buffer called name+'_mask' corresponding to the binary mask applied to the parameter name by the pruning method. 2) replacing the parameter name by its pruned version, while the original (unpruned) parameter is stored in a new parameter named name+'_orig'.  Parameters  module (nn.Module) \u2013 module containing the tensor to prune name (str) \u2013 parameter name within module on which pruning will act. amount (int or float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune.   Returns modified (i.e. pruned) version of the input module  Return type module (nn.Module)   Examples &gt;&gt;&gt; m = prune.random_unstructured(nn.Linear(2, 3), 'weight', amount=1) &gt;&gt;&gt; torch.sum(m.weight_mask == 0) tensor(1)   ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "name (str) : parameter name within module on which pruningwill act.", "amount (int or float) : quantity of parameters to prune.If float, should be between 0.0 and 1.0 and represent thefraction of parameters to prune. If int, it represents theabsolute number of parameters to prune."], "returns": "modified (i.e. pruned) version of the input module", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.LnStructured.compute_mask", "item_type": "method", "code": "compute_mask(t,default_mask)", "description": "Computes and returns a mask for the input tensor t. Starting from a base default_mask (which should be a mask of ones if the tensor has not been pruned yet), generate a mask to apply on top of the default_mask by zeroing out the channels along the specified dim with the lowest Ln-norm.  Parameters  t (torch.Tensor) \u2013 tensor representing the parameter to prune default_mask (torch.Tensor) \u2013 Base mask from previous pruning iterations, that need to be respected after the new mask is applied.  Same dims as t.   Returns mask to apply to t, of same dims as t  Return type mask (torch.Tensor)  Raises IndexError \u2013 if self.dim &gt;= len(t.shape)   ", "parameters": ["t (torch.Tensor) : tensor representing the parameter to prune", "default_mask (torch.Tensor) : Base mask from previous pruningiterations, that need to be respected after the new mask isapplied.  Same dims as t."], "returns": "pruned version of the input tensor", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.LnStructured.prune", "item_type": "method", "code": "prune(t,default_mask=None)", "description": "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters  t (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). default_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns pruned version of tensor t.   ", "parameters": ["t (torch.Tensor) : tensor to prune (of same dimensions asdefault_mask).", "default_mask (torch.Tensor, optional) : mask from previous pruningiteration, if any. To be considered when determining whatportion of the tensor that pruning should act on. If None,default to a mask of ones."], "returns": "pruned version of the input tensor", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.RandomUnstructured.apply", "item_type": "method", "code": "classmethodapply(module,name,amount)", "description": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters  module (nn.Module) \u2013 module containing the tensor to prune name (str) \u2013 parameter name within module on which pruning will act. amount (int or float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune.    ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "name (str) : parameter name within module on which pruningwill act.", "amount (int or float) : quantity of parameters to prune.If float, should be between 0.0 and 1.0 and represent thefraction of parameters to prune. If int, it represents theabsolute number of parameters to prune."], "returns": "pruned version of the input tensor", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.RandomUnstructured.apply_mask", "item_type": "method", "code": "apply_mask(module)", "description": "Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters module (nn.Module) \u2013 module containing the tensor to prune  Returns pruned version of the input tensor  Return type pruned_tensor (torch.Tensor)   ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)"], "returns": "pruned version of the input tensor", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.RandomUnstructured.prune", "item_type": "method", "code": "prune(t,default_mask=None)", "description": "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters  t (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). default_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns pruned version of tensor t.   ", "parameters": ["t (torch.Tensor) : tensor to prune (of same dimensions asdefault_mask).", "default_mask (torch.Tensor, optional) : mask from previous pruningiteration, if any. To be considered when determining whatportion of the tensor that pruning should act on. If None,default to a mask of ones."], "returns": "pruned version of the input tensor", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.CustomFromMask", "item_type": "class", "code": "classtorch.nn.utils.prune.CustomFromMask(mask)", "description": "  classmethod apply(module, name, mask)  Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters  module (nn.Module) \u2013 module containing the tensor to prune name (str) \u2013 parameter name within module on which pruning will act.        apply_mask(module) Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters module (nn.Module) \u2013 module containing the tensor to prune  Returns pruned version of the input tensor  Return type pruned_tensor (torch.Tensor)       prune(t, default_mask=None) Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters  t (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). default_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns pruned version of tensor t.       remove(module) Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!    ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "name (str) : parameter name within module on which pruningwill act.", "t (torch.Tensor) : tensor to prune (of same dimensions asdefault_mask).", "default_mask (torch.Tensor, optional) : mask from previous pruningiteration, if any. To be considered when determining whatportion of the tensor that pruning should act on. If None,default to a mask of ones."], "returns": "pruned version of the input tensor", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.PruningContainer.add_pruning_method", "item_type": "method", "code": "add_pruning_method(method)", "description": "Adds a child pruning method to the container.  Parameters method (subclass of BasePruningMethod) \u2013 child pruning method to be added to the container.   ", "parameters": ["method (subclass of BasePruningMethod) : child pruning methodto be added to the container."], "returns": "pruned version of the input tensor", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.PruningContainer.apply", "item_type": "method", "code": "classmethodapply(module,name,*args,**kwargs)", "description": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters  module (nn.Module) \u2013 module containing the tensor to prune name (str) \u2013 parameter name within module on which pruning will act. args \u2013 arguments passed on to a subclass of BasePruningMethod kwargs \u2013 keyword arguments passed on to a subclass of a BasePruningMethod    ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "name (str) : parameter name within module on which pruningwill act.", "args : arguments passed on to a subclass ofBasePruningMethod", "kwargs : keyword arguments passed on to a subclass of aBasePruningMethod"], "returns": "pruned version of the input tensor", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.Identity.apply", "item_type": "method", "code": "classmethodapply(module,name)", "description": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters  module (nn.Module) \u2013 module containing the tensor to prune name (str) \u2013 parameter name within module on which pruning will act.    ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "name (str) : parameter name within module on which pruningwill act."], "returns": "pruned version of the input tensor", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.Identity.apply_mask", "item_type": "method", "code": "apply_mask(module)", "description": "Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters module (nn.Module) \u2013 module containing the tensor to prune  Returns pruned version of the input tensor  Return type pruned_tensor (torch.Tensor)   ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)"], "returns": "pruned version of the input tensor", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.Identity.prune", "item_type": "method", "code": "prune(t,default_mask=None)", "description": "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters  t (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). default_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns pruned version of tensor t.   ", "parameters": ["t (torch.Tensor) : tensor to prune (of same dimensions asdefault_mask).", "default_mask (torch.Tensor, optional) : mask from previous pruningiteration, if any. To be considered when determining whatportion of the tensor that pruning should act on. If None,default to a mask of ones."], "returns": "pruned version of the input tensor", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.Identity.remove", "item_type": "method", "code": "remove(module)", "description": "Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!  ", "parameters": [], "returns": "pruned version of the input tensor", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.RandomStructured.prune", "item_type": "method", "code": "prune(t,default_mask=None)", "description": "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters  t (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). default_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns pruned version of tensor t.   ", "parameters": ["t (torch.Tensor) : tensor to prune (of same dimensions asdefault_mask).", "default_mask (torch.Tensor, optional) : mask from previous pruningiteration, if any. To be considered when determining whatportion of the tensor that pruning should act on. If None,default to a mask of ones."], "returns": "pruned version of the input tensor", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.RandomStructured.remove", "item_type": "method", "code": "remove(module)", "description": "Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!  ", "parameters": [], "returns": "pruned version of the input tensor", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.RandomStructured", "item_type": "class", "code": "classtorch.nn.utils.prune.RandomStructured(amount,dim=-1)", "description": "Prune entire (currently unpruned) channels in a tensor at random.  Parameters  amount (int or float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune. dim (int, optional) \u2013 index of the dim along which we define channels to prune. Default: -1.      classmethod apply(module, name, amount, dim=-1)  Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters  module (nn.Module) \u2013 module containing the tensor to prune name (str) \u2013 parameter name within module on which pruning will act. amount (int or float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune. dim (int, optional) \u2013 index of the dim along which we define channels to prune. Default: -1.        apply_mask(module) Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters module (nn.Module) \u2013 module containing the tensor to prune  Returns pruned version of the input tensor  Return type pruned_tensor (torch.Tensor)       compute_mask(t, default_mask)  Computes and returns a mask for the input tensor t. Starting from a base default_mask (which should be a mask of ones if the tensor has not been pruned yet), generate a random mask to apply on top of the default_mask by randomly zeroing out channels along the specified dim of the tensor.  Parameters  t (torch.Tensor) \u2013 tensor representing the parameter to prune default_mask (torch.Tensor) \u2013 Base mask from previous pruning iterations, that need to be respected after the new mask is applied. Same dims as t.   Returns mask to apply to t, of same dims as t  Return type mask (torch.Tensor)  Raises IndexError \u2013 if self.dim &gt;= len(t.shape)       prune(t, default_mask=None) Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters  t (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). default_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns pruned version of tensor t.       remove(module) Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!    ", "parameters": ["amount (int or float) : quantity of parameters to prune.If float, should be between 0.0 and 1.0 and represent thefraction of parameters to prune. If int, it represents theabsolute number of parameters to prune.", "dim (int, optional) : index of the dim along which we definechannels to prune. Default: -1.", "module (nn.Module) : module containing the tensor to prune", "name (str) : parameter name within module on which pruningwill act.", "amount (int or float) : quantity of parameters to prune.If float, should be between 0.0 and 1.0 and represent thefraction of parameters to prune. If int, it represents theabsolute number of parameters to prune.", "dim (int, optional) : index of the dim along which we definechannels to prune. Default: -1.", "t (torch.Tensor) : tensor representing the parameter to prune", "default_mask (torch.Tensor) : Base mask from previous pruningiterations, that need to be respected after the new mask isapplied. Same dims as t.", "t (torch.Tensor) : tensor to prune (of same dimensions asdefault_mask).", "default_mask (torch.Tensor, optional) : mask from previous pruningiteration, if any. To be considered when determining whatportion of the tensor that pruning should act on. If None,default to a mask of ones."], "returns": "pruned version of the input tensor", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Flatten.named_buffers", "item_type": "method", "code": "named_buffers(prefix:str='',recurse:bool=True)\u2192Iterator[Tuple[str,torch.Tensor]]", "description": "Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.  Parameters  prefix (str) \u2013 prefix to prepend to all buffer names. recurse (bool) \u2013 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.   Yields (string, torch.Tensor) \u2013 Tuple containing the name and buffer   Example: &gt;&gt;&gt; for name, buf in self.named_buffers(): &gt;&gt;&gt;    if name in ['running_var']: &gt;&gt;&gt;        print(buf.size())   ", "parameters": ["prefix (str) : prefix to prepend to all buffer names.", "recurse (bool) : if True, then yields buffers of this moduleand all submodules. Otherwise, yields only buffers thatare direct members of this module."], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Flatten.named_children", "item_type": "method", "code": "named_children()\u2192Iterator[Tuple[str,torch.nn.modules.module.Module]]", "description": "Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.  Yields (string, Module) \u2013 Tuple containing a name and child module   Example: &gt;&gt;&gt; for name, module in model.named_children(): &gt;&gt;&gt;     if name in ['conv4', 'conv5']: &gt;&gt;&gt;         print(module)   ", "parameters": ["(string, Module) : Tuple containing a name and child module"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.L1Unstructured.remove", "item_type": "method", "code": "remove(module)", "description": "Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!  ", "parameters": [], "returns": "pruned version of the input tensor", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.L1Unstructured", "item_type": "class", "code": "classtorch.nn.utils.prune.L1Unstructured(amount)", "description": "Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm.  Parameters amount (int or float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune.     classmethod apply(module, name, amount)  Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters  module (nn.Module) \u2013 module containing the tensor to prune name (str) \u2013 parameter name within module on which pruning will act. amount (int or float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune.        apply_mask(module) Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters module (nn.Module) \u2013 module containing the tensor to prune  Returns pruned version of the input tensor  Return type pruned_tensor (torch.Tensor)       prune(t, default_mask=None) Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters  t (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). default_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns pruned version of tensor t.       remove(module) Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!    ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "name (str) : parameter name within module on which pruningwill act.", "amount (int or float) : quantity of parameters to prune.If float, should be between 0.0 and 1.0 and represent thefraction of parameters to prune. If int, it represents theabsolute number of parameters to prune.", "t (torch.Tensor) : tensor to prune (of same dimensions asdefault_mask).", "default_mask (torch.Tensor, optional) : mask from previous pruningiteration, if any. To be considered when determining whatportion of the tensor that pruning should act on. If None,default to a mask of ones."], "returns": "pruned version of the input tensor", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.LnStructured.remove", "item_type": "method", "code": "remove(module)", "description": "Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!  ", "parameters": [], "returns": "pruned version of the input tensor", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.LnStructured", "item_type": "class", "code": "classtorch.nn.utils.prune.LnStructured(amount,n,dim=-1)", "description": "Prune entire (currently unpruned) channels in a tensor based on their Ln-norm.  Parameters  amount (int or float) \u2013 quantity of channels to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune. n (int, float, inf, -inf, 'fro', 'nuc') \u2013 See documentation of valid entries for argument p in torch.norm(). dim (int, optional) \u2013 index of the dim along which we define channels to prune. Default: -1.      classmethod apply(module, name, amount, n, dim)  Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters  module (nn.Module) \u2013 module containing the tensor to prune name (str) \u2013 parameter name within module on which pruning will act. amount (int or float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune. n (int, float, inf, -inf, 'fro', 'nuc') \u2013 See documentation of valid entries for argument p in torch.norm(). dim (int) \u2013 index of the dim along which we define channels to prune.        apply_mask(module) Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters module (nn.Module) \u2013 module containing the tensor to prune  Returns pruned version of the input tensor  Return type pruned_tensor (torch.Tensor)       compute_mask(t, default_mask)  Computes and returns a mask for the input tensor t. Starting from a base default_mask (which should be a mask of ones if the tensor has not been pruned yet), generate a mask to apply on top of the default_mask by zeroing out the channels along the specified dim with the lowest Ln-norm.  Parameters  t (torch.Tensor) \u2013 tensor representing the parameter to prune default_mask (torch.Tensor) \u2013 Base mask from previous pruning iterations, that need to be respected after the new mask is applied.  Same dims as t.   Returns mask to apply to t, of same dims as t  Return type mask (torch.Tensor)  Raises IndexError \u2013 if self.dim &gt;= len(t.shape)       prune(t, default_mask=None) Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters  t (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). default_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns pruned version of tensor t.       remove(module) Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!    ", "parameters": ["amount (int or float) : quantity of channels to prune.If float, should be between 0.0 and 1.0 and represent thefraction of parameters to prune. If int, it represents theabsolute number of parameters to prune.", "n (int, float, inf, -inf, 'fro', 'nuc') : See documentation of validentries for argument p in torch.norm().", "dim (int, optional) : index of the dim along which we definechannels to prune. Default: -1.", "module (nn.Module) : module containing the tensor to prune", "name (str) : parameter name within module on which pruningwill act.", "amount (int or float) : quantity of parameters to prune.If float, should be between 0.0 and 1.0 and represent thefraction of parameters to prune. If int, it represents theabsolute number of parameters to prune.", "n (int, float, inf, -inf, 'fro', 'nuc') : See documentation of validentries for argument p in torch.norm().", "dim (int) : index of the dim along which we define channels toprune.", "t (torch.Tensor) : tensor representing the parameter to prune", "default_mask (torch.Tensor) : Base mask from previous pruningiterations, that need to be respected after the new mask isapplied.  Same dims as t.", "t (torch.Tensor) : tensor to prune (of same dimensions asdefault_mask).", "default_mask (torch.Tensor, optional) : mask from previous pruningiteration, if any. To be considered when determining whatportion of the tensor that pruning should act on. If None,default to a mask of ones."], "returns": "pruned version of the input tensor", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.RandomUnstructured.remove", "item_type": "method", "code": "remove(module)", "description": "Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!  ", "parameters": [], "returns": "pruned version of the input tensor", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.RandomUnstructured", "item_type": "class", "code": "classtorch.nn.utils.prune.RandomUnstructured(amount)", "description": "Prune (currently unpruned) units in a tensor at random.  Parameters  name (str) \u2013 parameter name within module on which pruning will act. amount (int or float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune.      classmethod apply(module, name, amount)  Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters  module (nn.Module) \u2013 module containing the tensor to prune name (str) \u2013 parameter name within module on which pruning will act. amount (int or float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune.        apply_mask(module) Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters module (nn.Module) \u2013 module containing the tensor to prune  Returns pruned version of the input tensor  Return type pruned_tensor (torch.Tensor)       prune(t, default_mask=None) Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters  t (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). default_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns pruned version of tensor t.       remove(module) Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!    ", "parameters": ["name (str) : parameter name within module on which pruningwill act.", "amount (int or float) : quantity of parameters to prune.If float, should be between 0.0 and 1.0 and represent thefraction of parameters to prune. If int, it represents theabsolute number of parameters to prune.", "module (nn.Module) : module containing the tensor to prune", "name (str) : parameter name within module on which pruningwill act.", "amount (int or float) : quantity of parameters to prune.If float, should be between 0.0 and 1.0 and represent thefraction of parameters to prune. If int, it represents theabsolute number of parameters to prune.", "t (torch.Tensor) : tensor to prune (of same dimensions asdefault_mask).", "default_mask (torch.Tensor, optional) : mask from previous pruningiteration, if any. To be considered when determining whatportion of the tensor that pruning should act on. If None,default to a mask of ones."], "returns": "pruned version of the input tensor", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.PruningContainer.apply_mask", "item_type": "method", "code": "apply_mask(module)", "description": "Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters module (nn.Module) \u2013 module containing the tensor to prune  Returns pruned version of the input tensor  Return type pruned_tensor (torch.Tensor)   ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)"], "returns": "pruned version of the input tensor", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.PruningContainer.compute_mask", "item_type": "method", "code": "compute_mask(t,default_mask)", "description": "Applies the latest method by computing the new partial masks and returning its combination with the default_mask. The new partial mask should be computed on the entries or channels that were not zeroed out by the default_mask. Which portions of the tensor t the new mask will be calculated from depends on the PRUNING_TYPE (handled by the type handler):  for \u2018unstructured\u2019, the mask will be computed from the raveled list of nonmasked entries; for \u2018structured\u2019, the mask will be computed from the nonmasked channels in the tensor; for \u2018global\u2019, the mask will be computed across all entries.   Parameters  t (torch.Tensor) \u2013 tensor representing the parameter to prune (of same dimensions as default_mask). default_mask (torch.Tensor) \u2013 mask from previous pruning iteration.   Returns new mask that combines the effects of the default_mask and the new mask from the current pruning method (of same dimensions as default_mask and t).  Return type mask (torch.Tensor)   ", "parameters": ["t (torch.Tensor) : tensor representing the parameter to prune(of same dimensions as default_mask).", "default_mask (torch.Tensor) : mask from previous pruning iteration."], "returns": "pruned version of the input tensor", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.PruningContainer.prune", "item_type": "method", "code": "prune(t,default_mask=None)", "description": "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters  t (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). default_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns pruned version of tensor t.   ", "parameters": ["t (torch.Tensor) : tensor to prune (of same dimensions asdefault_mask).", "default_mask (torch.Tensor, optional) : mask from previous pruningiteration, if any. To be considered when determining whatportion of the tensor that pruning should act on. If None,default to a mask of ones."], "returns": "pruned version of the input tensor", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.Identity", "item_type": "class", "code": "classtorch.nn.utils.prune.Identity", "description": "Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones.   classmethod apply(module, name)  Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters  module (nn.Module) \u2013 module containing the tensor to prune name (str) \u2013 parameter name within module on which pruning will act.        apply_mask(module) Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters module (nn.Module) \u2013 module containing the tensor to prune  Returns pruned version of the input tensor  Return type pruned_tensor (torch.Tensor)       prune(t, default_mask=None) Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters  t (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). default_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns pruned version of tensor t.       remove(module) Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!    ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "name (str) : parameter name within module on which pruningwill act.", "t (torch.Tensor) : tensor to prune (of same dimensions asdefault_mask).", "default_mask (torch.Tensor, optional) : mask from previous pruningiteration, if any. To be considered when determining whatportion of the tensor that pruning should act on. If None,default to a mask of ones."], "returns": "pruned version of the input tensor", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.vector_to_parameters", "item_type": "function", "code": "torch.nn.utils.vector_to_parameters(vec,parameters)", "description": "Convert one vector to the parameters  Parameters  vec (Tensor) \u2013 a single vector represents the parameters of a model. parameters (Iterable[Tensor]) \u2013 an iterator of Tensors that are the parameters of a model.    ", "parameters": ["vec (Tensor) : a single vector represents the parameters of a model.", "parameters (Iterable[Tensor]) : an iterator of Tensors that are theparameters of a model."], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.BasePruningMethod.apply", "item_type": "method", "code": "classmethodapply(module,name,*args,**kwargs)", "description": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters  module (nn.Module) \u2013 module containing the tensor to prune name (str) \u2013 parameter name within module on which pruning will act. args \u2013 arguments passed on to a subclass of BasePruningMethod kwargs \u2013 keyword arguments passed on to a subclass of a BasePruningMethod    ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "name (str) : parameter name within module on which pruningwill act.", "args : arguments passed on to a subclass ofBasePruningMethod", "kwargs : keyword arguments passed on to a subclass of aBasePruningMethod"], "returns": "pruned version of the input tensor", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.BasePruningMethod.apply_mask", "item_type": "method", "code": "apply_mask(module)", "description": "Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters module (nn.Module) \u2013 module containing the tensor to prune  Returns pruned version of the input tensor  Return type pruned_tensor (torch.Tensor)   ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)"], "returns": "pruned version of the input tensor", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.BasePruningMethod.compute_mask", "item_type": "method", "code": "abstractcompute_mask(t,default_mask)", "description": "Computes and returns a mask for the input tensor t. Starting from a base default_mask (which should be a mask of ones if the tensor has not been pruned yet), generate a random mask to apply on top of the default_mask according to the specific pruning method recipe.  Parameters  t (torch.Tensor) \u2013 tensor representing the parameter to prune default_mask (torch.Tensor) \u2013 Base mask from previous pruning that need to be respected after the new mask is (iterations,) \u2013  Same dims as t. (applied.) \u2013    Returns mask to apply to t, of same dims as t  Return type mask (torch.Tensor)   ", "parameters": ["t (torch.Tensor) : tensor representing the parameter to prune", "default_mask (torch.Tensor) : Base mask from previous pruning", "that need to be respected after the new mask is (iterations,) : ", "Same dims as t. (applied.) : "], "returns": "pruned version of the input tensor", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.clip_grad_norm_", "item_type": "function", "code": "torch.nn.utils.clip_grad_norm_(parameters,max_norm,norm_type=2)", "description": "Clips gradient norm of an iterable of parameters. The norm is computed over all gradients together, as if they were concatenated into a single vector. Gradients are modified in-place.  Parameters  parameters (Iterable[Tensor] or Tensor) \u2013 an iterable of Tensors or a single Tensor that will have gradients normalized max_norm (float or int) \u2013 max norm of the gradients norm_type (float or int) \u2013 type of the used p-norm. Can be 'inf' for infinity norm.   Returns Total norm of the parameters (viewed as a single vector).   ", "parameters": ["parameters (Iterable[Tensor] or Tensor) : an iterable of Tensors or asingle Tensor that will have gradients normalized", "max_norm (float or int) : max norm of the gradients", "norm_type (float or int) : type of the used p-norm. Can be 'inf' forinfinity norm."], "returns": "Total norm of the parameters (viewed as a single vector).", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Flatten.named_modules", "item_type": "method", "code": "named_modules(memo:Optional[Set[Module]]=None,prefix:str='')", "description": "Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.  Yields (string, Module) \u2013 Tuple of name and module    Note Duplicate modules are returned only once. In the following example, l will be returned only once.  Example: &gt;&gt;&gt; l = nn.Linear(2, 2) &gt;&gt;&gt; net = nn.Sequential(l, l) &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):         print(idx, '-&gt;', m)  0 -&gt; ('', Sequential(   (0): Linear(in_features=2, out_features=2, bias=True)   (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -&gt; ('0', Linear(in_features=2, out_features=2, bias=True))   ", "parameters": ["(string, Module) : Tuple of name and module"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.parameters_to_vector", "item_type": "function", "code": "torch.nn.utils.parameters_to_vector(parameters)", "description": "Convert parameters to one vector  Parameters parameters (Iterable[Tensor]) \u2013 an iterator of Tensors that are the parameters of a model.  Returns The parameters represented by a single vector   ", "parameters": ["parameters (Iterable[Tensor]) : an iterator of Tensors that are theparameters of a model.", "The parameters represented by a single vector"], "returns": "The parameters represented by a single vector", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.clip_grad_value_", "item_type": "function", "code": "torch.nn.utils.clip_grad_value_(parameters,clip_value)", "description": "Clips gradient of an iterable of parameters at specified value. Gradients are modified in-place.  Parameters  parameters (Iterable[Tensor] or Tensor) \u2013 an iterable of Tensors or a single Tensor that will have gradients normalized clip_value (float or int) \u2013 maximum allowed value of the gradients. The gradients are clipped in the range [-clip_value,clip_value]\\left[\\text{-clip\\_value}, \\text{clip\\_value}\\right][-clip_value,clip_value]      ", "parameters": ["parameters (Iterable[Tensor] or Tensor) : an iterable of Tensors or asingle Tensor that will have gradients normalized", "clip_value (float or int) : maximum allowed value of the gradients.The gradients are clipped in the range[-clip_value,clip_value]\\left[\\text{-clip\\_value}, \\text{clip\\_value}\\right][-clip_value,clip_value]"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.parallel.DistributedDataParallel.no_sync", "item_type": "method", "code": "no_sync()", "description": "A context manager to disable gradient synchronizations across DDP processes. Within this context, gradients will be accumulated on module variables, which will later be synchronized in the first forward-backward pass exiting the context. Example: &gt;&gt;&gt; ddp = torch.nn.DistributedDataParallel(model, pg) &gt;&gt;&gt; with ddp.no_sync(): ...   for input in inputs: ...     ddp(input).backward()  # no synchronization, accumulate grads ... ddp(another_input).backward()  # synchronize grads   ", "parameters": [], "returns": [], "example": " ddp = torch.nn.DistributedDataParallel(model, pg)\n with ddp.no_sync():\n...   for input in inputs:\n...     ddp(input).backward()  # no synchronization, accumulate grads\n... ddp(another_input).backward()  # synchronize grads\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.parallel.DistributedDataParallel", "item_type": "class", "code": "classtorch.nn.parallel.DistributedDataParallel(module,device_ids=None,output_device=None,dim=0,broadcast_buffers=True,process_group=None,bucket_cap_mb=25,find_unused_parameters=False,check_reduction=False)", "description": "Implements distributed data parallelism that is based on torch.distributed package at the module level. This container parallelizes the application of the given module by splitting the input across the specified devices by chunking in the batch dimension. The module is replicated on each machine and each device, and each such replica handles a portion of the input. During the backwards pass, gradients from each node are averaged. The batch size should be larger than the number of GPUs used locally. See also: Basics and Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel. The same constraints on input as in torch.nn.DataParallel apply. Creation of this class requires that torch.distributed to be already initialized, by calling torch.distributed.init_process_group(). DistributedDataParallel is proven to be significantly faster than torch.nn.DataParallel for single-node multi-GPU data parallel training. Here is how to use it: on each host with N GPUs, you should spawn up N processes, while ensuring that each process individually works on a single GPU from 0 to N-1. Therefore, it is your job to ensure that your training script operates on a single given GPU by calling: &gt;&gt;&gt; torch.cuda.set_device(i)   where i is from 0 to N-1. In each process, you should refer the following to construct this module: &gt;&gt;&gt; torch.distributed.init_process_group(backend='nccl', world_size=4, init_method='...') &gt;&gt;&gt; model = DistributedDataParallel(model, device_ids=[i], output_device=i)   In order to spawn up multiple processes per node, you can use either torch.distributed.launch or torch.multiprocessing.spawn  Note Please refer to PyTorch Distributed Overview for a brief introduction to all features related to distributed training.   Note nccl backend is currently the fastest and highly recommended backend to be used with Multi-Process Single-GPU distributed training and this applies to both single-node and multi-node distributed training   Note This module also supports mixed-precision distributed training. This means that your model can have different types of parameters such as mixed types of fp16 and fp32, the gradient reduction on these mixed types of parameters will just work fine. Also note that nccl backend is currently the fastest and highly recommended backend for fp16/fp32 mixed-precision training.   Note If you use torch.save on one process to checkpoint the module, and torch.load on some other processes to recover it, make sure that map_location is configured properly for every process. Without map_location, torch.load would recover the module to devices where the module was saved from.   Warning This module works only with the gloo and nccl backends.   Warning Constructor, forward method, and differentiation of the output (or a function of the output of this module) is a distributed synchronization point. Take that into account in case different processes might be executing different code.   Warning This module assumes all parameters are registered in the model by the time it is created. No parameters should be added nor removed later. Same applies to buffers.   Warning This module assumes all parameters are registered in the model of each distributed processes are in the same order. The module itself will conduct gradient all-reduction following the reverse order of the registered parameters of the model. In other words, it is users\u2019 responsibility to ensure that each distributed process has the exact same model and thus the exact same parameter registration order.   Warning This module allows parameters with non-rowmajor-contiguous strides. For example, your model may contain some parameters whose torch.memory_format is torch.contiguous_format and others whose format is torch.channels_last.  However, corresponding parameters in different processes must have the same strides.   Warning This module doesn\u2019t work with torch.autograd.grad() (i.e. it will only work if gradients are to be accumulated in .grad attributes of parameters).   Warning If you plan on using this module with a nccl backend or a gloo backend (that uses Infiniband), together with a DataLoader that uses multiple workers, please change the multiprocessing start method to forkserver (Python 3 only) or spawn. Unfortunately Gloo (that uses Infiniband) and NCCL2 are not fork safe, and you will likely experience deadlocks if you don\u2019t change this setting.   Warning Forward and backward hooks defined on module and its submodules won\u2019t be invoked anymore, unless the hooks are initialized in the forward() method.   Warning You should never try to change your model\u2019s parameters after wrapping up your model with DistributedDataParallel. In other words, when wrapping up your model with DistributedDataParallel, the constructor of DistributedDataParallel will register the additional gradient reduction functions on all the parameters of the model itself at the time of construction. If you change the model\u2019s parameters after the DistributedDataParallel construction, this is not supported and unexpected behaviors can happen, since some parameters\u2019 gradient reduction functions might not get called.   Note Parameters are never broadcast between processes. The module performs an all-reduce step on gradients and assumes that they will be modified by the optimizer in all processes in the same way. Buffers (e.g. BatchNorm stats) are broadcast from the module in process of rank 0, to all other replicas in the system in every iteration.   Note If you are using DistributedDataParallel in conjunction with the Distributed RPC Framework, you should always use torch.distributed.autograd.backward() to compute gradients and torch.distributed.optim.DistributedOptimizer for optimizing parameters.   Example::&gt;&gt;&gt; import torch.distributed.autograd as dist_autograd &gt;&gt;&gt; from torch.nn.parallel import DistributedDataParallel as DDP &gt;&gt;&gt; from torch import optim &gt;&gt;&gt; from torch.distributed.optim import DistributedOptimizer &gt;&gt;&gt; from torch.distributed.rpc import RRef &gt;&gt;&gt; &gt;&gt;&gt; t1 = torch.rand((3, 3), requires_grad=True) &gt;&gt;&gt; t2 = torch.rand((3, 3), requires_grad=True) &gt;&gt;&gt; rref = rpc.remote(\"worker1\", torch.add, args=(t1, t2)) &gt;&gt;&gt; ddp_model = DDP(my_model) &gt;&gt;&gt; &gt;&gt;&gt; # Setup optimizer &gt;&gt;&gt; optimizer_params = [rref] &gt;&gt;&gt; for param in ddp_model.parameters(): &gt;&gt;&gt;     optimizer_params.append(RRef(param)) &gt;&gt;&gt; &gt;&gt;&gt; dist_optim = DistributedOptimizer( &gt;&gt;&gt;     optim.SGD, &gt;&gt;&gt;     optimizer_params, &gt;&gt;&gt;     lr=0.05, &gt;&gt;&gt; ) &gt;&gt;&gt; &gt;&gt;&gt; with dist_autograd.context() as context_id: &gt;&gt;&gt;     pred = ddp_model(rref.to_here()) &gt;&gt;&gt;     loss = loss_func(pred, loss) &gt;&gt;&gt;     dist_autograd.backward(context_id, loss) &gt;&gt;&gt;     dist_optim.step()      Warning Using DistributedDataParallel in conjuction with the Distributed RPC Framework is experimental and subject to change.   Parameters  module (Module) \u2013 module to be parallelized device_ids (list of python:int or torch.device) \u2013 CUDA devices. This should only be provided when the input module resides on a single CUDA device. For single-device modules, the i``th :attr:`module` replica is placed on ``device_ids[i]. For multi-device modules and CPU modules, device_ids must be None or an empty list, and input data for the forward pass must be placed on the correct device. (default: all devices for single-device modules) output_device (int or torch.device) \u2013 device location of output for single-device CUDA modules. For multi-device modules and CPU modules, it must be None, and the module itself dictates the output location. (default: device_ids[0] for single-device modules) broadcast_buffers (bool) \u2013 flag that enables syncing (broadcasting) buffers of the module at beginning of the forward function. (default: True) process_group \u2013 the process group to be used for distributed data all-reduction. If None, the default process group, which is created by `torch.distributed.init_process_group`, will be used. (default: None) bucket_cap_mb \u2013 DistributedDataParallel will bucket parameters into multiple buckets so that gradient reduction of each bucket can potentially overlap with backward computation. bucket_cap_mb controls the bucket size in MegaBytes (MB) (default: 25) find_unused_parameters (bool) \u2013 Traverse the autograd graph of all tensors contained in the return value of the wrapped module\u2019s forward function. Parameters that don\u2019t receive gradients as part of this graph are preemptively marked as being ready to be reduced. Note that all forward outputs that are derived from module parameters must participate in calculating loss and later the gradient computation. If they don\u2019t, this wrapper will hang waiting for autograd to produce gradients for those parameters. Any outputs derived from module parameters that are otherwise unused can be detached from the autograd graph using torch.Tensor.detach. (default: False) check_reduction \u2013 when setting to True, it enables DistributedDataParallel to automatically check if the previous iteration\u2019s backward reductions were successfully issued at the beginning of every iteration\u2019s forward function. You normally don\u2019t need this option enabled unless you are observing weird behaviors such as different ranks are getting different gradients, which should not happen if DistributedDataParallel is correctly used. (default: False)   Variables ~DistributedDataParallel.module (Module) \u2013 the module to be parallelized   Example: &gt;&gt;&gt; torch.distributed.init_process_group(backend='nccl', world_size=4, init_method='...') &gt;&gt;&gt; net = torch.nn.DistributedDataParallel(model, pg)     no_sync()  A context manager to disable gradient synchronizations across DDP processes. Within this context, gradients will be accumulated on module variables, which will later be synchronized in the first forward-backward pass exiting the context. Example: &gt;&gt;&gt; ddp = torch.nn.DistributedDataParallel(model, pg) &gt;&gt;&gt; with ddp.no_sync(): ...   for input in inputs: ...     ddp(input).backward()  # no synchronization, accumulate grads ... ddp(another_input).backward()  # synchronize grads     ", "parameters": ["module (Module) : module to be parallelized", "device_ids (list of python:int or torch.device) : CUDA devices. This shouldonly be provided when the input module resides on a singleCUDA device. For single-device modules, the i``th:attr:`module` replica is placed on ``device_ids[i]. Formulti-device modules and CPU modules, device_ids must be Noneor an empty list, and input data for the forward pass must beplaced on the correct device. (default: all devices forsingle-device modules)", "output_device (int or torch.device) : device location of output forsingle-device CUDA modules. For multi-device modules andCPU modules, it must be None, and the module itselfdictates the output location. (default: device_ids[0] forsingle-device modules)", "broadcast_buffers (bool) : flag that enables syncing (broadcasting) buffers ofthe module at beginning of the forward function.(default: True)", "process_group : the process group to be used for distributed dataall-reduction. If None, the default process group, whichis created by `torch.distributed.init_process_group`,will be used. (default: None)", "bucket_cap_mb : DistributedDataParallel will bucket parameters intomultiple buckets so that gradient reduction of eachbucket can potentially overlap with backward computation.bucket_cap_mb controls the bucket size in MegaBytes (MB)(default: 25)", "find_unused_parameters (bool) : Traverse the autograd graph of all tensorscontained in the return value of the wrappedmodule\u2019s forward function.Parameters that don\u2019t receive gradients aspart of this graph are preemptively markedas being ready to be reduced. Note that allforward outputs that are derived frommodule parameters must participate incalculating loss and later the gradientcomputation. If they don\u2019t, this wrapper willhang waiting for autograd to produce gradientsfor those parameters. Any outputs derived frommodule parameters that are otherwise unused canbe detached from the autograd graph usingtorch.Tensor.detach. (default: False)", "check_reduction : when setting to True, it enables DistributedDataParallelto automatically check if the previous iteration\u2019sbackward reductions were successfully issued at thebeginning of every iteration\u2019s forward function.You normally don\u2019t need this option enabled unless youare observing weird behaviors such as different ranksare getting different gradients, which should nothappen if DistributedDataParallel is correctly used.(default: False)"], "returns": [], "example": "&gt;&gt;&gt; ddp = torch.nn.DistributedDataParallel(model, pg)\n&gt;&gt;&gt; with ddp.no_sync():\n...   for input in inputs:\n...     ddp(input).backward()  # no synchronization, accumulate grads\n... ddp(another_input).backward()  # synchronize grads\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.PruningContainer.remove", "item_type": "method", "code": "remove(module)", "description": "Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!  ", "parameters": [], "returns": "pruned version of the input tensor", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.PruningContainer", "item_type": "class", "code": "classtorch.nn.utils.prune.PruningContainer(*args)", "description": "Container holding a sequence of pruning methods for iterative pruning. Keeps track of the order in which pruning methods are applied and handles combining successive pruning calls. Accepts as argument an instance of a BasePruningMethod or an iterable of them.   add_pruning_method(method)  Adds a child pruning method to the container.  Parameters method (subclass of BasePruningMethod) \u2013 child pruning method to be added to the container.       classmethod apply(module, name, *args, **kwargs) Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters  module (nn.Module) \u2013 module containing the tensor to prune name (str) \u2013 parameter name within module on which pruning will act. args \u2013 arguments passed on to a subclass of BasePruningMethod kwargs \u2013 keyword arguments passed on to a subclass of a BasePruningMethod        apply_mask(module) Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters module (nn.Module) \u2013 module containing the tensor to prune  Returns pruned version of the input tensor  Return type pruned_tensor (torch.Tensor)       compute_mask(t, default_mask)  Applies the latest method by computing the new partial masks and returning its combination with the default_mask. The new partial mask should be computed on the entries or channels that were not zeroed out by the default_mask. Which portions of the tensor t the new mask will be calculated from depends on the PRUNING_TYPE (handled by the type handler):  for \u2018unstructured\u2019, the mask will be computed from the raveled list of nonmasked entries; for \u2018structured\u2019, the mask will be computed from the nonmasked channels in the tensor; for \u2018global\u2019, the mask will be computed across all entries.   Parameters  t (torch.Tensor) \u2013 tensor representing the parameter to prune (of same dimensions as default_mask). default_mask (torch.Tensor) \u2013 mask from previous pruning iteration.   Returns new mask that combines the effects of the default_mask and the new mask from the current pruning method (of same dimensions as default_mask and t).  Return type mask (torch.Tensor)       prune(t, default_mask=None) Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters  t (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). default_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns pruned version of tensor t.       remove(module) Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!    ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "name (str) : parameter name within module on which pruningwill act.", "args : arguments passed on to a subclass ofBasePruningMethod", "kwargs : keyword arguments passed on to a subclass of aBasePruningMethod", "t (torch.Tensor) : tensor representing the parameter to prune(of same dimensions as default_mask).", "default_mask (torch.Tensor) : mask from previous pruning iteration.", "t (torch.Tensor) : tensor to prune (of same dimensions asdefault_mask).", "default_mask (torch.Tensor, optional) : mask from previous pruningiteration, if any. To be considered when determining whatportion of the tensor that pruning should act on. If None,default to a mask of ones."], "returns": "pruned version of the input tensor", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.DataParallel", "item_type": "class", "code": "classtorch.nn.DataParallel(module,device_ids=None,output_device=None,dim=0)", "description": "Implements data parallelism at the module level. This container parallelizes the application of the given module by splitting the input across the specified devices by chunking in the batch dimension (other objects will be copied once per device). In the forward pass, the module is replicated on each device, and each replica handles a portion of the input. During the backwards pass, gradients from each replica are summed into the original module. The batch size should be larger than the number of GPUs used.  Warning It is recommended to use DistributedDataParallel, instead of this class, to do multi-GPU training, even if there is only a single node. See: Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel and Distributed Data Parallel.  Arbitrary positional and keyword inputs are allowed to be passed into DataParallel but some types are specially handled. tensors will be scattered on dim specified (default 0). tuple, list and dict types will be shallow copied. The other types will be shared among different threads and can be corrupted if written to in the model\u2019s forward pass. The parallelized module must have its parameters and buffers on device_ids[0] before running this DataParallel module.  Warning In each forward, module is replicated on each device, so any updates to the running module in forward will be lost. For example, if module has a counter attribute that is incremented in each forward, it will always stay at the initial value because the update is done on the replicas which are destroyed after forward. However, DataParallel guarantees that the replica on device[0] will have its parameters and buffers sharing storage with the base parallelized module. So in-place updates to the parameters or buffers on device[0] will be recorded. E.g., BatchNorm2d and spectral_norm() rely on this behavior to update the buffers.   Warning Forward and backward hooks defined on module and its submodules will be invoked len(device_ids) times, each with inputs located on a particular device. Particularly, the hooks are only guaranteed to be executed in correct order with respect to operations on corresponding devices. For example, it is not guaranteed that hooks set via register_forward_pre_hook() be executed before all len(device_ids) forward() calls, but that each such hook be executed before the corresponding forward() call of that device.   Warning When module returns a scalar (i.e., 0-dimensional tensor) in forward(), this wrapper will return a vector of length equal to number of devices used in data parallelism, containing the result from each device.   Note There is a subtlety in using the pack sequence -&gt; recurrent network -&gt; unpack sequence pattern in a Module wrapped in DataParallel. See My recurrent network doesn\u2019t work with data parallelism section in FAQ for details.   Parameters  module (Module) \u2013 module to be parallelized device_ids (list of python:int or torch.device) \u2013 CUDA devices (default: all devices) output_device (int or torch.device) \u2013 device location of output (default: device_ids[0])   Variables ~DataParallel.module (Module) \u2013 the module to be parallelized   Example: &gt;&gt;&gt; net = torch.nn.DataParallel(model, device_ids=[0, 1, 2]) &gt;&gt;&gt; output = net(input_var)  # input_var can be on any device, including CPU   ", "parameters": ["module (Module) : module to be parallelized", "device_ids (list of python:int or torch.device) : CUDA devices (default: all devices)", "output_device (int or torch.device) : device location of output (default: device_ids[0])"], "returns": [], "example": " net = torch.nn.DataParallel(model, device_ids=[0, 1, 2])\n output = net(input_var)  # input_var can be on any device, including CPU\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.UpsamplingBilinear2d", "item_type": "class", "code": "classtorch.nn.UpsamplingBilinear2d(size:Optional[Union[T,Tuple[T,T]]]=None,scale_factor:Optional[Union[T,Tuple[T,T]]]=None)", "description": "Applies a 2D bilinear upsampling to an input signal composed of several input channels. To specify the scale, it takes either the size or the scale_factor as it\u2019s constructor argument. When size is given, it is the output size of the image (h, w).  Parameters  size (int or Tuple[int, int], optional) \u2013 output spatial sizes scale_factor (float or Tuple[float, float], optional) \u2013 multiplier for spatial size.     Warning This class is deprecated in favor of interpolate(). It is equivalent to nn.functional.interpolate(..., mode='bilinear', align_corners=True).   Shape: Input: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)   Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)   where     Hout=\u230aHin\u00d7scale_factor\u230bH_{out} = \\left\\lfloor H_{in} \\times \\text{scale\\_factor} \\right\\rfloor  Hout\u200b=\u230aHin\u200b\u00d7scale_factor\u230b   Wout=\u230aWin\u00d7scale_factor\u230bW_{out} = \\left\\lfloor W_{in} \\times \\text{scale\\_factor} \\right\\rfloor  Wout\u200b=\u230aWin\u200b\u00d7scale_factor\u230b  Examples: &gt;&gt;&gt; input = torch.arange(1, 5, dtype=torch.float32).view(1, 1, 2, 2) &gt;&gt;&gt; input tensor([[[[ 1.,  2.],           [ 3.,  4.]]]])  &gt;&gt;&gt; m = nn.UpsamplingBilinear2d(scale_factor=2) &gt;&gt;&gt; m(input) tensor([[[[ 1.0000,  1.3333,  1.6667,  2.0000],           [ 1.6667,  2.0000,  2.3333,  2.6667],           [ 2.3333,  2.6667,  3.0000,  3.3333],           [ 3.0000,  3.3333,  3.6667,  4.0000]]]])   ", "parameters": ["size (int or Tuple[int, int], optional) : output spatial sizes", "scale_factor (float or Tuple[float, float], optional) : multiplier forspatial size."], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.BasePruningMethod.prune", "item_type": "method", "code": "prune(t,default_mask=None)", "description": "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters  t (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). default_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns pruned version of tensor t.   ", "parameters": ["t (torch.Tensor) : tensor to prune (of same dimensions asdefault_mask).", "default_mask (torch.Tensor, optional) : mask from previous pruningiteration, if any. To be considered when determining whatportion of the tensor that pruning should act on. If None,default to a mask of ones."], "returns": "pruned version of the input tensor", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.BasePruningMethod.remove", "item_type": "method", "code": "remove(module)", "description": "Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!  ", "parameters": [], "returns": "pruned version of the input tensor", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.utils.prune.BasePruningMethod", "item_type": "class", "code": "classtorch.nn.utils.prune.BasePruningMethod", "description": "Abstract base class for creation of new pruning techniques. Provides a skeleton for customization requiring the overriding of methods such as compute_mask() and apply().   classmethod apply(module, name, *args, **kwargs)  Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters  module (nn.Module) \u2013 module containing the tensor to prune name (str) \u2013 parameter name within module on which pruning will act. args \u2013 arguments passed on to a subclass of BasePruningMethod kwargs \u2013 keyword arguments passed on to a subclass of a BasePruningMethod        apply_mask(module)  Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters module (nn.Module) \u2013 module containing the tensor to prune  Returns pruned version of the input tensor  Return type pruned_tensor (torch.Tensor)       abstract compute_mask(t, default_mask)  Computes and returns a mask for the input tensor t. Starting from a base default_mask (which should be a mask of ones if the tensor has not been pruned yet), generate a random mask to apply on top of the default_mask according to the specific pruning method recipe.  Parameters  t (torch.Tensor) \u2013 tensor representing the parameter to prune default_mask (torch.Tensor) \u2013 Base mask from previous pruning that need to be respected after the new mask is (iterations,) \u2013  Same dims as t. (applied.) \u2013    Returns mask to apply to t, of same dims as t  Return type mask (torch.Tensor)       prune(t, default_mask=None)  Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters  t (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). default_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns pruned version of tensor t.       remove(module)  Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!    ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "name (str) : parameter name within module on which pruningwill act.", "args : arguments passed on to a subclass ofBasePruningMethod", "kwargs : keyword arguments passed on to a subclass of aBasePruningMethod", "t (torch.Tensor) : tensor representing the parameter to prune", "default_mask (torch.Tensor) : Base mask from previous pruning", "that need to be respected after the new mask is (iterations,) : ", "Same dims as t. (applied.) : ", "t (torch.Tensor) : tensor to prune (of same dimensions asdefault_mask).", "default_mask (torch.Tensor, optional) : mask from previous pruningiteration, if any. To be considered when determining whatportion of the tensor that pruning should act on. If None,default to a mask of ones."], "returns": "pruned version of the input tensor", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Flatten.named_parameters", "item_type": "method", "code": "named_parameters(prefix:str='',recurse:bool=True)\u2192Iterator[Tuple[str,torch.Tensor]]", "description": "Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.  Parameters  prefix (str) \u2013 prefix to prepend to all parameter names. recurse (bool) \u2013 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.   Yields (string, Parameter) \u2013 Tuple containing the name and parameter   Example: &gt;&gt;&gt; for name, param in self.named_parameters(): &gt;&gt;&gt;    if name in ['bias']: &gt;&gt;&gt;        print(param.size())   ", "parameters": ["prefix (str) : prefix to prepend to all parameter names.", "recurse (bool) : if True, then yields parameters of this moduleand all submodules. Otherwise, yields only parameters thatare direct members of this module."], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Flatten.parameters", "item_type": "method", "code": "parameters(recurse:bool=True)\u2192Iterator[torch.nn.parameter.Parameter]", "description": "Returns an iterator over module parameters. This is typically passed to an optimizer.  Parameters recurse (bool) \u2013 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.  Yields Parameter \u2013 module parameter   Example: &gt;&gt;&gt; for param in model.parameters(): &gt;&gt;&gt;     print(type(param), param.size()) &lt;class 'torch.Tensor'&gt; (20L,) &lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)   ", "parameters": ["recurse (bool) : if True, then yields parameters of this moduleand all submodules. Otherwise, yields only parameters thatare direct members of this module.", "Parameter : module parameter"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Upsample", "item_type": "class", "code": "classtorch.nn.Upsample(size:Optional[Union[T,Tuple[T,...]]]=None,scale_factor:Optional[Union[T,Tuple[T,...]]]=None,mode:str='nearest',align_corners:Optional[bool]=None)", "description": "Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data. The input data is assumed to be of the form minibatch x channels x [optional depth] x [optional height] x width. Hence, for spatial inputs, we expect a 4D Tensor and for volumetric inputs, we expect a 5D Tensor. The algorithms available for upsampling are nearest neighbor and linear, bilinear, bicubic and trilinear for 3D, 4D and 5D input Tensor, respectively. One can either give a scale_factor or the target output size to calculate the output size. (You cannot give both, as it is ambiguous)  Parameters  size (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int], optional) \u2013 output spatial sizes scale_factor (float or Tuple[float] or Tuple[float, float] or Tuple[float, float, float], optional) \u2013 multiplier for spatial size. Has to match input size if it is a tuple. mode (str, optional) \u2013 the upsampling algorithm: one of 'nearest', 'linear', 'bilinear', 'bicubic' and 'trilinear'. Default: 'nearest' align_corners (bool, optional) \u2013 if True, the corner pixels of the input and output tensors are aligned, and thus preserving the values at those pixels. This only has effect when mode is 'linear', 'bilinear', or 'trilinear'. Default: False     Shape: Input: (N,C,Win)(N, C, W_{in})(N,C,Win\u200b)  , (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)   or (N,C,Din,Hin,Win)(N, C, D_{in}, H_{in}, W_{in})(N,C,Din\u200b,Hin\u200b,Win\u200b)   Output: (N,C,Wout)(N, C, W_{out})(N,C,Wout\u200b)  , (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)   or (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\u200b,Hout\u200b,Wout\u200b)  , where     Dout=\u230aDin\u00d7scale_factor\u230bD_{out} = \\left\\lfloor D_{in} \\times \\text{scale\\_factor} \\right\\rfloor  Dout\u200b=\u230aDin\u200b\u00d7scale_factor\u230b   Hout=\u230aHin\u00d7scale_factor\u230bH_{out} = \\left\\lfloor H_{in} \\times \\text{scale\\_factor} \\right\\rfloor  Hout\u200b=\u230aHin\u200b\u00d7scale_factor\u230b   Wout=\u230aWin\u00d7scale_factor\u230bW_{out} = \\left\\lfloor W_{in} \\times \\text{scale\\_factor} \\right\\rfloor  Wout\u200b=\u230aWin\u200b\u00d7scale_factor\u230b   Warning With align_corners = True, the linearly interpolating modes (linear, bilinear, bicubic, and trilinear) don\u2019t proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is align_corners = False. See below for concrete examples on how this affects the outputs.   Note If you want downsampling/general resizing, you should use interpolate().  Examples: &gt;&gt;&gt; input = torch.arange(1, 5, dtype=torch.float32).view(1, 1, 2, 2) &gt;&gt;&gt; input tensor([[[[ 1.,  2.],           [ 3.,  4.]]]])  &gt;&gt;&gt; m = nn.Upsample(scale_factor=2, mode='nearest') &gt;&gt;&gt; m(input) tensor([[[[ 1.,  1.,  2.,  2.],           [ 1.,  1.,  2.,  2.],           [ 3.,  3.,  4.,  4.],           [ 3.,  3.,  4.,  4.]]]])  &gt;&gt;&gt; m = nn.Upsample(scale_factor=2, mode='bilinear')  # align_corners=False &gt;&gt;&gt; m(input) tensor([[[[ 1.0000,  1.2500,  1.7500,  2.0000],           [ 1.5000,  1.7500,  2.2500,  2.5000],           [ 2.5000,  2.7500,  3.2500,  3.5000],           [ 3.0000,  3.2500,  3.7500,  4.0000]]]])  &gt;&gt;&gt; m = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True) &gt;&gt;&gt; m(input) tensor([[[[ 1.0000,  1.3333,  1.6667,  2.0000],           [ 1.6667,  2.0000,  2.3333,  2.6667],           [ 2.3333,  2.6667,  3.0000,  3.3333],           [ 3.0000,  3.3333,  3.6667,  4.0000]]]])  &gt;&gt;&gt; # Try scaling the same data in a larger tensor &gt;&gt;&gt; &gt;&gt;&gt; input_3x3 = torch.zeros(3, 3).view(1, 1, 3, 3) &gt;&gt;&gt; input_3x3[:, :, :2, :2].copy_(input) tensor([[[[ 1.,  2.],           [ 3.,  4.]]]]) &gt;&gt;&gt; input_3x3 tensor([[[[ 1.,  2.,  0.],           [ 3.,  4.,  0.],           [ 0.,  0.,  0.]]]])  &gt;&gt;&gt; m = nn.Upsample(scale_factor=2, mode='bilinear')  # align_corners=False &gt;&gt;&gt; # Notice that values in top left corner are the same with the small input (except at boundary) &gt;&gt;&gt; m(input_3x3) tensor([[[[ 1.0000,  1.2500,  1.7500,  1.5000,  0.5000,  0.0000],           [ 1.5000,  1.7500,  2.2500,  1.8750,  0.6250,  0.0000],           [ 2.5000,  2.7500,  3.2500,  2.6250,  0.8750,  0.0000],           [ 2.2500,  2.4375,  2.8125,  2.2500,  0.7500,  0.0000],           [ 0.7500,  0.8125,  0.9375,  0.7500,  0.2500,  0.0000],           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]])  &gt;&gt;&gt; m = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True) &gt;&gt;&gt; # Notice that values in top left corner are now changed &gt;&gt;&gt; m(input_3x3) tensor([[[[ 1.0000,  1.4000,  1.8000,  1.6000,  0.8000,  0.0000],           [ 1.8000,  2.2000,  2.6000,  2.2400,  1.1200,  0.0000],           [ 2.6000,  3.0000,  3.4000,  2.8800,  1.4400,  0.0000],           [ 2.4000,  2.7200,  3.0400,  2.5600,  1.2800,  0.0000],           [ 1.2000,  1.3600,  1.5200,  1.2800,  0.6400,  0.0000],           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]])   ", "parameters": ["size (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int], optional) : output spatial sizes", "scale_factor (float or Tuple[float] or Tuple[float, float] or Tuple[float, float, float], optional) : multiplier for spatial size. Has to match input size if it is a tuple.", "mode (str, optional) : the upsampling algorithm: one of 'nearest','linear', 'bilinear', 'bicubic' and 'trilinear'.Default: 'nearest'", "align_corners (bool, optional) : if True, the corner pixels of the inputand output tensors are aligned, and thus preserving the values atthose pixels. This only has effect when mode is'linear', 'bilinear', or 'trilinear'. Default: False"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.UpsamplingNearest2d", "item_type": "class", "code": "classtorch.nn.UpsamplingNearest2d(size:Optional[Union[T,Tuple[T,T]]]=None,scale_factor:Optional[Union[T,Tuple[T,T]]]=None)", "description": "Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels. To specify the scale, it takes either the size or the scale_factor as it\u2019s constructor argument. When size is given, it is the output size of the image (h, w).  Parameters  size (int or Tuple[int, int], optional) \u2013 output spatial sizes scale_factor (float or Tuple[float, float], optional) \u2013 multiplier for spatial size.     Warning This class is deprecated in favor of interpolate().   Shape: Input: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)   Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)   where     Hout=\u230aHin\u00d7scale_factor\u230bH_{out} = \\left\\lfloor H_{in} \\times \\text{scale\\_factor} \\right\\rfloor  Hout\u200b=\u230aHin\u200b\u00d7scale_factor\u230b   Wout=\u230aWin\u00d7scale_factor\u230bW_{out} = \\left\\lfloor W_{in} \\times \\text{scale\\_factor} \\right\\rfloor  Wout\u200b=\u230aWin\u200b\u00d7scale_factor\u230b  Examples: &gt;&gt;&gt; input = torch.arange(1, 5, dtype=torch.float32).view(1, 1, 2, 2) &gt;&gt;&gt; input tensor([[[[ 1.,  2.],           [ 3.,  4.]]]])  &gt;&gt;&gt; m = nn.UpsamplingNearest2d(scale_factor=2) &gt;&gt;&gt; m(input) tensor([[[[ 1.,  1.,  2.,  2.],           [ 1.,  1.,  2.,  2.],           [ 3.,  3.,  4.,  4.],           [ 3.,  3.,  4.,  4.]]]])   ", "parameters": ["size (int or Tuple[int, int], optional) : output spatial sizes", "scale_factor (float or Tuple[float, float], optional) : multiplier forspatial size."], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.MultiMarginLoss", "item_type": "class", "code": "classtorch.nn.MultiMarginLoss(p:int=1,margin:float=1.0,weight:Optional[torch.Tensor]=None,size_average=None,reduce=None,reduction:str='mean')", "description": "Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between input xxx   (a 2D mini-batch Tensor) and output yyy   (which is a 1D tensor of target class indices, 0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121  ): For each mini-batch sample, the loss in terms of the 1D input xxx   and scalar output yyy   is:  loss(x,y)=\u2211imax\u2061(0,margin\u2212x[y]+x[i]))px.size(0)\\text{loss}(x, y) = \\frac{\\sum_i \\max(0, \\text{margin} - x[y] + x[i]))^p}{\\text{x.size}(0)}  loss(x,y)=x.size(0)\u2211i\u200bmax(0,margin\u2212x[y]+x[i]))p\u200b  where x\u2208{0,\u2005\u200a\u22ef\u2009,\u2005\u200ax.size(0)\u22121}x \\in \\left\\{0, \\; \\cdots , \\; \\text{x.size}(0) - 1\\right\\}x\u2208{0,\u22ef,x.size(0)\u22121}   and i\u2260yi \\neq yi\ue020\u200b=y  . Optionally, you can give non-equal weighting on the classes by passing a 1D weight tensor into the constructor. The loss function then becomes:  loss(x,y)=\u2211imax\u2061(0,w[y]\u2217(margin\u2212x[y]+x[i]))p)x.size(0)\\text{loss}(x, y) = \\frac{\\sum_i \\max(0, w[y] * (\\text{margin} - x[y] + x[i]))^p)}{\\text{x.size}(0)}  loss(x,y)=x.size(0)\u2211i\u200bmax(0,w[y]\u2217(margin\u2212x[y]+x[i]))p)\u200b   Parameters  p (int, optional) \u2013 Has a default value of 111  . 111   and 222   are the only supported values. margin (float, optional) \u2013 Has a default value of 111  . weight (Tensor, optional) \u2013 a manual rescaling weight given to each class. If given, it has to be a Tensor of size C. Otherwise, it is treated as if having all ones. size_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'    ", "parameters": ["p (int, optional) : Has a default value of 111. 111 and 222are the only supported values.", "margin (float, optional) : Has a default value of 111.", "weight (Tensor, optional) : a manual rescaling weight given to eachclass. If given, it has to be a Tensor of size C. Otherwise, it istreated as if having all ones.", "size_average (bool, optional) : Deprecated (see reduction). By default,the losses are averaged over each loss element in the batch. Note that forsome losses, there are multiple elements per sample. If the field size_averageis set to False, the losses are instead summed for each minibatch. Ignoredwhen reduce is False. Default: True", "reduce (bool, optional) : Deprecated (see reduction). By default, thelosses are averaged or summed over observations for each minibatch dependingon size_average. When reduce is False, returns a loss perbatch element instead and ignores size_average. Default: True", "reduction (string, optional) : Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.CosineEmbeddingLoss", "item_type": "class", "code": "classtorch.nn.CosineEmbeddingLoss(margin:float=0.0,size_average=None,reduce=None,reduction:str='mean')", "description": "Creates a criterion that measures the loss given input tensors x1x_1x1\u200b  , x2x_2x2\u200b   and a Tensor label yyy   with values 1 or -1. This is used for measuring whether two inputs are similar or dissimilar, using the cosine distance, and is typically used for learning nonlinear embeddings or semi-supervised learning. The loss function for each sample is:  loss(x,y)={1\u2212cos\u2061(x1,x2),if\u00a0y=1max\u2061(0,cos\u2061(x1,x2)\u2212margin),if\u00a0y=\u22121\\text{loss}(x, y) = \\begin{cases} 1 - \\cos(x_1, x_2), &amp; \\text{if } y = 1 \\\\ \\max(0, \\cos(x_1, x_2) - \\text{margin}), &amp; \\text{if } y = -1 \\end{cases}  loss(x,y)={1\u2212cos(x1\u200b,x2\u200b),max(0,cos(x1\u200b,x2\u200b)\u2212margin),\u200bif\u00a0y=1if\u00a0y=\u22121\u200b   Parameters  margin (float, optional) \u2013 Should be a number from \u22121-1\u22121   to 111  , 000   to 0.50.50.5   is suggested. If margin is missing, the default value is 000  . size_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'    ", "parameters": ["margin (float, optional) : Should be a number from \u22121-1\u22121 to 111,000 to 0.50.50.5 is suggested. If margin is missing, thedefault value is 000.", "size_average (bool, optional) : Deprecated (see reduction). By default,the losses are averaged over each loss element in the batch. Note that forsome losses, there are multiple elements per sample. If the field size_averageis set to False, the losses are instead summed for each minibatch. Ignoredwhen reduce is False. Default: True", "reduce (bool, optional) : Deprecated (see reduction). By default, thelosses are averaged or summed over observations for each minibatch dependingon size_average. When reduce is False, returns a loss perbatch element instead and ignores size_average. Default: True", "reduction (string, optional) : Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.TripletMarginLoss", "item_type": "class", "code": "classtorch.nn.TripletMarginLoss(margin:float=1.0,p:float=2.0,eps:float=1e-06,swap:bool=False,size_average=None,reduce=None,reduction:str='mean')", "description": "Creates a criterion that measures the triplet loss given an input tensors x1x1x1  , x2x2x2  , x3x3x3   and a margin with a value greater than 000  . This is used for measuring a relative similarity between samples. A triplet is composed by a, p and n (i.e., anchor, positive examples and negative examples respectively). The shapes of all input tensors should be (N,D)(N, D)(N,D)  . The distance swap is described in detail in the paper Learning shallow convolutional feature descriptors with triplet losses by V. Balntas, E. Riba et al. The loss function for each sample in the mini-batch is:  L(a,p,n)=max\u2061{d(ai,pi)\u2212d(ai,ni)+margin,0}L(a, p, n) = \\max \\{d(a_i, p_i) - d(a_i, n_i) + {\\rm margin}, 0\\}  L(a,p,n)=max{d(ai\u200b,pi\u200b)\u2212d(ai\u200b,ni\u200b)+margin,0}  where  d(xi,yi)=\u2225xi\u2212yi\u2225pd(x_i, y_i) = \\left\\lVert {\\bf x}_i - {\\bf y}_i \\right\\rVert_p  d(xi\u200b,yi\u200b)=\u2225xi\u200b\u2212yi\u200b\u2225p\u200b   Parameters  margin (float, optional) \u2013 Default: 111  . p (int, optional) \u2013 The norm degree for pairwise distance. Default: 222  . swap (bool, optional) \u2013 The distance swap is described in detail in the paper Learning shallow convolutional feature descriptors with triplet losses by V. Balntas, E. Riba et al. Default: False. size_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'     Shape: Input: (N,D)(N, D)(N,D)   where DDD   is the vector dimension. Output: scalar. If reduction is 'none', then (N)(N)(N)  .    &gt;&gt;&gt; triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2) &gt;&gt;&gt; anchor = torch.randn(100, 128, requires_grad=True) &gt;&gt;&gt; positive = torch.randn(100, 128, requires_grad=True) &gt;&gt;&gt; negative = torch.randn(100, 128, requires_grad=True) &gt;&gt;&gt; output = triplet_loss(anchor, positive, negative) &gt;&gt;&gt; output.backward()   ", "parameters": ["margin (float, optional) : Default: 111.", "p (int, optional) : The norm degree for pairwise distance. Default: 222.", "swap (bool, optional) : The distance swap is described in detail in the paperLearning shallow convolutional feature descriptors with triplet losses byV. Balntas, E. Riba et al. Default: False.", "size_average (bool, optional) : Deprecated (see reduction). By default,the losses are averaged over each loss element in the batch. Note that forsome losses, there are multiple elements per sample. If the field size_averageis set to False, the losses are instead summed for each minibatch. Ignoredwhen reduce is False. Default: True", "reduce (bool, optional) : Deprecated (see reduction). By default, thelosses are averaged or summed over observations for each minibatch dependingon size_average. When reduce is False, returns a loss perbatch element instead and ignores size_average. Default: True", "reduction (string, optional) : Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.PixelShuffle", "item_type": "class", "code": "classtorch.nn.PixelShuffle(upscale_factor:int)", "description": "Rearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W)   to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r)  . This is useful for implementing efficient sub-pixel convolution with a stride of 1/r1/r1/r  . Look at the paper: Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network by Shi et. al (2016) for more details.  Parameters upscale_factor (int) \u2013 factor to increase spatial resolution by    Shape: Input: (N,L,Hin,Win)(N, L, H_{in}, W_{in})(N,L,Hin\u200b,Win\u200b)   where L=C\u00d7upscale_factor2L=C \\times \\text{upscale\\_factor}^2L=C\u00d7upscale_factor2   Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)   where Hout=Hin\u00d7upscale_factorH_{out} = H_{in} \\times \\text{upscale\\_factor}Hout\u200b=Hin\u200b\u00d7upscale_factor   and Wout=Win\u00d7upscale_factorW_{out} = W_{in} \\times \\text{upscale\\_factor}Wout\u200b=Win\u200b\u00d7upscale_factor      Examples: &gt;&gt;&gt; pixel_shuffle = nn.PixelShuffle(3) &gt;&gt;&gt; input = torch.randn(1, 9, 4, 4) &gt;&gt;&gt; output = pixel_shuffle(input) &gt;&gt;&gt; print(output.size()) torch.Size([1, 1, 12, 12])   ", "parameters": ["upscale_factor (int) : factor to increase spatial resolution by"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Flatten.register_backward_hook", "item_type": "method", "code": "register_backward_hook(hook:Callable[[Module,Union[Tuple[torch.Tensor,...],torch.Tensor],Union[Tuple[torch.Tensor,...],torch.Tensor]],Union[None,torch.Tensor]])\u2192torch.utils.hooks.RemovableHandle", "description": "Registers a backward hook on the module.  Warning The current implementation will not have the presented behavior for complex Module that perform many operations. In some failure cases, grad_input and grad_output will only contain the gradients for a subset of the inputs and outputs. For such Module, you should use torch.Tensor.register_hook() directly on a specific input or output to get the required gradients.  The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature: hook(module, grad_input, grad_output) -&gt; Tensor or None   The grad_input and grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of grad_input in subsequent computations. grad_input will only correspond to the inputs given as positional arguments.  Returns a handle that can be used to remove the added hook by calling handle.remove()  Return type torch.utils.hooks.RemovableHandle   ", "parameters": ["a handle that can be used to remove the added hook by callinghandle.remove()", "torch.utils.hooks.RemovableHandle"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Flatten.register_buffer", "item_type": "method", "code": "register_buffer(name:str,tensor:Optional[torch.Tensor],persistent:bool=True)\u2192None", "description": "Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm\u2019s running_mean is not a parameter, but is part of the module\u2019s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting persistent to False. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module\u2019s state_dict. Buffers can be accessed as attributes using given names.  Parameters  name (string) \u2013 name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor) \u2013 buffer to be registered. persistent (bool) \u2013 whether the buffer is part of this module\u2019s state_dict.    Example: &gt;&gt;&gt; self.register_buffer('running_mean', torch.zeros(num_features))   ", "parameters": ["name (string) : name of the buffer. The buffer can be accessedfrom this module using the given name", "tensor (Tensor) : buffer to be registered.", "persistent (bool) : whether the buffer is part of this module\u2019sstate_dict."], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.MultiLabelSoftMarginLoss", "item_type": "class", "code": "classtorch.nn.MultiLabelSoftMarginLoss(weight:Optional[torch.Tensor]=None,size_average=None,reduce=None,reduction:str='mean')", "description": "Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between input xxx   and target yyy   of size (N,C)(N, C)(N,C)  . For each sample in the minibatch:  loss(x,y)=\u22121C\u2217\u2211iy[i]\u2217log\u2061((1+exp\u2061(\u2212x[i]))\u22121)+(1\u2212y[i])\u2217log\u2061(exp\u2061(\u2212x[i])(1+exp\u2061(\u2212x[i])))loss(x, y) = - \\frac{1}{C} * \\sum_i y[i] * \\log((1 + \\exp(-x[i]))^{-1})                  + (1-y[i]) * \\log\\left(\\frac{\\exp(-x[i])}{(1 + \\exp(-x[i]))}\\right)  loss(x,y)=\u2212C1\u200b\u2217i\u2211\u200by[i]\u2217log((1+exp(\u2212x[i]))\u22121)+(1\u2212y[i])\u2217log((1+exp(\u2212x[i]))exp(\u2212x[i])\u200b)  where i\u2208{0,\u2005\u200a\u22ef\u2009,\u2005\u200ax.nElement()\u22121}i \\in \\left\\{0, \\; \\cdots , \\; \\text{x.nElement}() - 1\\right\\}i\u2208{0,\u22ef,x.nElement()\u22121}  , y[i]\u2208{0,\u2005\u200a1}y[i] \\in \\left\\{0, \\; 1\\right\\}y[i]\u2208{0,1}  .  Parameters  weight (Tensor, optional) \u2013 a manual rescaling weight given to each class. If given, it has to be a Tensor of size C. Otherwise, it is treated as if having all ones. size_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'     Shape: Input: (N,C)(N, C)(N,C)   where N is the batch size and C is the number of classes. Target: (N,C)(N, C)(N,C)  , label targets padded by -1 ensuring same shape as the input. Output: scalar. If reduction is 'none', then (N)(N)(N)  .    ", "parameters": ["weight (Tensor, optional) : a manual rescaling weight given to eachclass. If given, it has to be a Tensor of size C. Otherwise, it istreated as if having all ones.", "size_average (bool, optional) : Deprecated (see reduction). By default,the losses are averaged over each loss element in the batch. Note that forsome losses, there are multiple elements per sample. If the field size_averageis set to False, the losses are instead summed for each minibatch. Ignoredwhen reduce is False. Default: True", "reduce (bool, optional) : Deprecated (see reduction). By default, thelosses are averaged or summed over observations for each minibatch dependingon size_average. When reduce is False, returns a loss perbatch element instead and ignores size_average. Default: True", "reduction (string, optional) : Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.SoftMarginLoss", "item_type": "class", "code": "classtorch.nn.SoftMarginLoss(size_average=None,reduce=None,reduction:str='mean')", "description": "Creates a criterion that optimizes a two-class classification logistic loss between input tensor xxx   and target tensor yyy   (containing 1 or -1).  loss(x,y)=\u2211ilog\u2061(1+exp\u2061(\u2212y[i]\u2217x[i]))x.nelement()\\text{loss}(x, y) = \\sum_i \\frac{\\log(1 + \\exp(-y[i]*x[i]))}{\\text{x.nelement}()}  loss(x,y)=i\u2211\u200bx.nelement()log(1+exp(\u2212y[i]\u2217x[i]))\u200b   Parameters  size_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'     Shape: Input: (\u2217)(*)(\u2217)   where \u2217*\u2217   means, any number of additional dimensions Target: (\u2217)(*)(\u2217)  , same shape as the input Output: scalar. If reduction is 'none', then same shape as the input    ", "parameters": ["size_average (bool, optional) : Deprecated (see reduction). By default,the losses are averaged over each loss element in the batch. Note that forsome losses, there are multiple elements per sample. If the field size_averageis set to False, the losses are instead summed for each minibatch. Ignoredwhen reduce is False. Default: True", "reduce (bool, optional) : Deprecated (see reduction). By default, thelosses are averaged or summed over observations for each minibatch dependingon size_average. When reduce is False, returns a loss perbatch element instead and ignores size_average. Default: True", "reduction (string, optional) : Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Flatten.register_forward_hook", "item_type": "method", "code": "register_forward_hook(hook:Callable[...,None])\u2192torch.utils.hooks.RemovableHandle", "description": "Registers a forward hook on the module. The hook will be called every time after forward() has computed an output. It should have the following signature: hook(module, input, output) -&gt; None or modified output   The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after forward() is called.  Returns a handle that can be used to remove the added hook by calling handle.remove()  Return type torch.utils.hooks.RemovableHandle   ", "parameters": ["a handle that can be used to remove the added hook by callinghandle.remove()", "torch.utils.hooks.RemovableHandle"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Flatten.register_forward_pre_hook", "item_type": "method", "code": "register_forward_pre_hook(hook:Callable[...,None])\u2192torch.utils.hooks.RemovableHandle", "description": "Registers a forward pre-hook on the module. The hook will be called every time before forward() is invoked. It should have the following signature: hook(module, input) -&gt; None or modified input   The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple).  Returns a handle that can be used to remove the added hook by calling handle.remove()  Return type torch.utils.hooks.RemovableHandle   ", "parameters": ["a handle that can be used to remove the added hook by callinghandle.remove()", "torch.utils.hooks.RemovableHandle"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.SmoothL1Loss", "item_type": "class", "code": "classtorch.nn.SmoothL1Loss(size_average=None,reduce=None,reduction:str='mean')", "description": "Creates a criterion that uses a squared term if the absolute element-wise error falls below 1 and an L1 term otherwise. It is less sensitive to outliers than the MSELoss and in some cases prevents exploding gradients (e.g. see Fast R-CNN paper by Ross Girshick). Also known as the Huber loss:  loss(x,y)=1n\u2211izi\\text{loss}(x, y) = \\frac{1}{n} \\sum_{i} z_{i}  loss(x,y)=n1\u200bi\u2211\u200bzi\u200b  where ziz_{i}zi\u200b   is given by:  zi={0.5(xi\u2212yi)2,if\u00a0\u2223xi\u2212yi\u2223&lt;1\u2223xi\u2212yi\u2223\u22120.5,otherwise\u00a0z_{i} = \\begin{cases} 0.5 (x_i - y_i)^2, &amp; \\text{if } |x_i - y_i| &lt; 1 \\\\ |x_i - y_i| - 0.5, &amp; \\text{otherwise } \\end{cases}  zi\u200b={0.5(xi\u200b\u2212yi\u200b)2,\u2223xi\u200b\u2212yi\u200b\u2223\u22120.5,\u200bif\u00a0\u2223xi\u200b\u2212yi\u200b\u2223&lt;1otherwise\u00a0\u200b  xxx   and yyy   arbitrary shapes with a total of nnn   elements each the sum operation still operates over all the elements, and divides by nnn  . The division by nnn   can be avoided if sets reduction = 'sum'.  Parameters  size_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'     Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where \u2217*\u2217   means, any number of additional dimensions Target: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input Output: scalar. If reduction is 'none', then (N,\u2217)(N, *)(N,\u2217)  , same shape as the input    ", "parameters": ["size_average (bool, optional) : Deprecated (see reduction). By default,the losses are averaged over each loss element in the batch. Note that forsome losses, there are multiple elements per sample. If the field size_averageis set to False, the losses are instead summed for each minibatch. Ignoredwhen reduce is False. Default: True", "reduce (bool, optional) : Deprecated (see reduction). By default, thelosses are averaged or summed over observations for each minibatch dependingon size_average. When reduce is False, returns a loss perbatch element instead and ignores size_average. Default: True", "reduction (string, optional) : Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.MarginRankingLoss", "item_type": "class", "code": "classtorch.nn.MarginRankingLoss(margin:float=0.0,size_average=None,reduce=None,reduction:str='mean')", "description": "Creates a criterion that measures the loss given inputs x1x1x1  , x2x2x2  , two 1D mini-batch Tensors, and a label 1D mini-batch tensor yyy   (containing 1 or -1). If y=1y = 1y=1   then it assumed the first input should be ranked higher (have a larger value) than the second input, and vice-versa for y=\u22121y = -1y=\u22121  . The loss function for each pair of samples in the mini-batch is:  loss(x1,x2,y)=max\u2061(0,\u2212y\u2217(x1\u2212x2)+margin)\\text{loss}(x1, x2, y) = \\max(0, -y * (x1 - x2) + \\text{margin})  loss(x1,x2,y)=max(0,\u2212y\u2217(x1\u2212x2)+margin)   Parameters  margin (float, optional) \u2013 Has a default value of 000  . size_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'     Shape: Input: (N,D)(N, D)(N,D)   where N is the batch size and D is the size of a sample. Target: (N)(N)(N)   Output: scalar. If reduction is 'none', then (N)(N)(N)  .    ", "parameters": ["margin (float, optional) : Has a default value of 000.", "size_average (bool, optional) : Deprecated (see reduction). By default,the losses are averaged over each loss element in the batch. Note that forsome losses, there are multiple elements per sample. If the field size_averageis set to False, the losses are instead summed for each minibatch. Ignoredwhen reduce is False. Default: True", "reduce (bool, optional) : Deprecated (see reduction). By default, thelosses are averaged or summed over observations for each minibatch dependingon size_average. When reduce is False, returns a loss perbatch element instead and ignores size_average. Default: True", "reduction (string, optional) : Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.BCEWithLogitsLoss", "item_type": "class", "code": "classtorch.nn.BCEWithLogitsLoss(weight:Optional[torch.Tensor]=None,size_average=None,reduce=None,reduction:str='mean',pos_weight:Optional[torch.Tensor]=None)", "description": "This loss combines a Sigmoid layer and the BCELoss in one single class. This version is more numerically stable than using a plain Sigmoid followed by a BCELoss as, by combining the operations into one layer, we take advantage of the log-sum-exp trick for numerical stability. The unreduced (i.e. with reduction set to 'none') loss can be described as:  \u2113(x,y)=L={l1,\u2026,lN}\u22a4,ln=\u2212wn[yn\u22c5log\u2061\u03c3(xn)+(1\u2212yn)\u22c5log\u2061(1\u2212\u03c3(xn))],\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad l_n = - w_n \\left[ y_n \\cdot \\log \\sigma(x_n) + (1 - y_n) \\cdot \\log (1 - \\sigma(x_n)) \\right],  \u2113(x,y)=L={l1\u200b,\u2026,lN\u200b}\u22a4,ln\u200b=\u2212wn\u200b[yn\u200b\u22c5log\u03c3(xn\u200b)+(1\u2212yn\u200b)\u22c5log(1\u2212\u03c3(xn\u200b))],  where NNN   is the batch size. If reduction is not 'none' (default 'mean'), then  \u2113(x,y)={mean\u2061(L),if\u00a0reduction=\u2019mean\u2019;sum\u2061(L),if\u00a0reduction=\u2019sum\u2019.\\ell(x, y) = \\begin{cases}     \\operatorname{mean}(L), &amp; \\text{if reduction} = \\text{'mean';}\\\\     \\operatorname{sum}(L),  &amp; \\text{if reduction} = \\text{'sum'.} \\end{cases}  \u2113(x,y)={mean(L),sum(L),\u200bif\u00a0reduction=\u2019mean\u2019;if\u00a0reduction=\u2019sum\u2019.\u200b  This is used for measuring the error of a reconstruction in for example an auto-encoder. Note that the targets t[i] should be numbers between 0 and 1. It\u2019s possible to trade off recall and precision by adding weights to positive examples. In the case of multi-label classification the loss can be described as:  \u2113c(x,y)=Lc={l1,c,\u2026,lN,c}\u22a4,ln,c=\u2212wn,c[pcyn,c\u22c5log\u2061\u03c3(xn,c)+(1\u2212yn,c)\u22c5log\u2061(1\u2212\u03c3(xn,c))],\\ell_c(x, y) = L_c = \\{l_{1,c},\\dots,l_{N,c}\\}^\\top, \\quad l_{n,c} = - w_{n,c} \\left[ p_c y_{n,c} \\cdot \\log \\sigma(x_{n,c}) + (1 - y_{n,c}) \\cdot \\log (1 - \\sigma(x_{n,c})) \\right],  \u2113c\u200b(x,y)=Lc\u200b={l1,c\u200b,\u2026,lN,c\u200b}\u22a4,ln,c\u200b=\u2212wn,c\u200b[pc\u200byn,c\u200b\u22c5log\u03c3(xn,c\u200b)+(1\u2212yn,c\u200b)\u22c5log(1\u2212\u03c3(xn,c\u200b))],  where ccc   is the class number (c&gt;1c &gt; 1c&gt;1   for multi-label binary classification, c=1c = 1c=1   for single-label binary classification), nnn   is the number of the sample in the batch and pcp_cpc\u200b   is the weight of the positive answer for the class ccc  . pc&gt;1p_c &gt; 1pc\u200b&gt;1   increases the recall, pc&lt;1p_c &lt; 1pc\u200b&lt;1   increases the precision. For example, if a dataset contains 100 positive and 300 negative examples of a single class, then pos_weight for the class should be equal to 300100=3\\frac{300}{100}=3100300\u200b=3  . The loss would act as if the dataset contains 3\u00d7100=3003\\times 100=3003\u00d7100=300   positive examples. Examples: &gt;&gt;&gt; target = torch.ones([10, 64], dtype=torch.float32)  # 64 classes, batch size = 10 &gt;&gt;&gt; output = torch.full([10, 64], 1.5)  # A prediction (logit) &gt;&gt;&gt; pos_weight = torch.ones([64])  # All weights are equal to 1 &gt;&gt;&gt; criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight) &gt;&gt;&gt; criterion(output, target)  # -log(sigmoid(1.5)) tensor(0.2014)    Parameters  weight (Tensor, optional) \u2013 a manual rescaling weight given to the loss of each batch element. If given, has to be a Tensor of size nbatch. size_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean' pos_weight (Tensor, optional) \u2013 a weight of positive examples. Must be a vector with length equal to the number of classes.     Shape:  Input: (N,\u2217)(N, *)(N,\u2217)   where \u2217*\u2217   means, any number of additional dimensions Target: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input Output: scalar. If reduction is 'none', then (N,\u2217)(N, *)(N,\u2217)  , same shape as input.   Examples: &gt;&gt;&gt; loss = nn.BCEWithLogitsLoss() &gt;&gt;&gt; input = torch.randn(3, requires_grad=True) &gt;&gt;&gt; target = torch.empty(3).random_(2) &gt;&gt;&gt; output = loss(input, target) &gt;&gt;&gt; output.backward()     ", "parameters": ["weight (Tensor, optional) : a manual rescaling weight given to the lossof each batch element. If given, has to be a Tensor of size nbatch.", "size_average (bool, optional) : Deprecated (see reduction). By default,the losses are averaged over each loss element in the batch. Note that forsome losses, there are multiple elements per sample. If the field size_averageis set to False, the losses are instead summed for each minibatch. Ignoredwhen reduce is False. Default: True", "reduce (bool, optional) : Deprecated (see reduction). By default, thelosses are averaged or summed over observations for each minibatch dependingon size_average. When reduce is False, returns a loss perbatch element instead and ignores size_average. Default: True", "reduction (string, optional) : Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'", "pos_weight (Tensor, optional) : a weight of positive examples.Must be a vector with length equal to the number of classes."], "returns": [], "example": "NA", "shape": "\n\nInput: (N,\u2217)(N, *)(N,\u2217)\n\n where \u2217*\u2217\n\n means, any number of additional dimensions\nTarget: (N,\u2217)(N, *)(N,\u2217)\n\n, same shape as the input\nOutput: scalar. If reduction is 'none', then (N,\u2217)(N, *)(N,\u2217)\n\n, same\nshape as input.\n\n\nExamples:\n&gt;&gt;&gt; loss = nn.BCEWithLogitsLoss()\n&gt;&gt;&gt; input = torch.randn(3, requires_grad=True)\n&gt;&gt;&gt; target = torch.empty(3).random_(2)\n&gt;&gt;&gt; output = loss(input, target)\n&gt;&gt;&gt; output.backward()\n\n\n"},
{"library": "torch", "item_id": "torch.nn.HingeEmbeddingLoss", "item_type": "class", "code": "classtorch.nn.HingeEmbeddingLoss(margin:float=1.0,size_average=None,reduce=None,reduction:str='mean')", "description": "Measures the loss given an input tensor xxx   and a labels tensor yyy   (containing 1 or -1). This is usually used for measuring whether two inputs are similar or dissimilar, e.g. using the L1 pairwise distance as xxx  , and is typically used for learning nonlinear embeddings or semi-supervised learning. The loss function for nnn  -th sample in the mini-batch is  ln={xn,if\u2005\u200ayn=1,max\u2061{0,\u0394\u2212xn},if\u2005\u200ayn=\u22121,l_n = \\begin{cases}     x_n, &amp; \\text{if}\\; y_n = 1,\\\\     \\max \\{0, \\Delta - x_n\\}, &amp; \\text{if}\\; y_n = -1, \\end{cases}  ln\u200b={xn\u200b,max{0,\u0394\u2212xn\u200b},\u200bifyn\u200b=1,ifyn\u200b=\u22121,\u200b  and the total loss functions is  \u2113(x,y)={mean\u2061(L),if\u00a0reduction=\u2019mean\u2019;sum\u2061(L),if\u00a0reduction=\u2019sum\u2019.\\ell(x, y) = \\begin{cases}     \\operatorname{mean}(L), &amp; \\text{if reduction} = \\text{'mean';}\\\\     \\operatorname{sum}(L),  &amp; \\text{if reduction} = \\text{'sum'.} \\end{cases}  \u2113(x,y)={mean(L),sum(L),\u200bif\u00a0reduction=\u2019mean\u2019;if\u00a0reduction=\u2019sum\u2019.\u200b  where L={l1,\u2026,lN}\u22a4L = \\{l_1,\\dots,l_N\\}^\\topL={l1\u200b,\u2026,lN\u200b}\u22a4  .  Parameters  margin (float, optional) \u2013 Has a default value of 1. size_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'     Shape: Input: (\u2217)(*)(\u2217)   where \u2217*\u2217   means, any number of dimensions. The sum operation operates over all the elements. Target: (\u2217)(*)(\u2217)  , same shape as the input Output: scalar. If reduction is 'none', then same shape as the input    ", "parameters": ["margin (float, optional) : Has a default value of 1.", "size_average (bool, optional) : Deprecated (see reduction). By default,the losses are averaged over each loss element in the batch. Note that forsome losses, there are multiple elements per sample. If the field size_averageis set to False, the losses are instead summed for each minibatch. Ignoredwhen reduce is False. Default: True", "reduce (bool, optional) : Deprecated (see reduction). By default, thelosses are averaged or summed over observations for each minibatch dependingon size_average. When reduce is False, returns a loss perbatch element instead and ignores size_average. Default: True", "reduction (string, optional) : Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.BCELoss", "item_type": "class", "code": "classtorch.nn.BCELoss(weight:Optional[torch.Tensor]=None,size_average=None,reduce=None,reduction:str='mean')", "description": "Creates a criterion that measures the Binary Cross Entropy between the target and the output: The unreduced (i.e. with reduction set to 'none') loss can be described as:  \u2113(x,y)=L={l1,\u2026,lN}\u22a4,ln=\u2212wn[yn\u22c5log\u2061xn+(1\u2212yn)\u22c5log\u2061(1\u2212xn)],\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad l_n = - w_n \\left[ y_n \\cdot \\log x_n + (1 - y_n) \\cdot \\log (1 - x_n) \\right],  \u2113(x,y)=L={l1\u200b,\u2026,lN\u200b}\u22a4,ln\u200b=\u2212wn\u200b[yn\u200b\u22c5logxn\u200b+(1\u2212yn\u200b)\u22c5log(1\u2212xn\u200b)],  where NNN   is the batch size. If reduction is not 'none' (default 'mean'), then  \u2113(x,y)={mean\u2061(L),if\u00a0reduction=\u2019mean\u2019;sum\u2061(L),if\u00a0reduction=\u2019sum\u2019.\\ell(x, y) = \\begin{cases}     \\operatorname{mean}(L), &amp; \\text{if reduction} = \\text{'mean';}\\\\     \\operatorname{sum}(L),  &amp; \\text{if reduction} = \\text{'sum'.} \\end{cases}  \u2113(x,y)={mean(L),sum(L),\u200bif\u00a0reduction=\u2019mean\u2019;if\u00a0reduction=\u2019sum\u2019.\u200b  This is used for measuring the error of a reconstruction in for example an auto-encoder. Note that the targets yyy   should be numbers between 0 and 1. Notice that if xnx_nxn\u200b   is either 0 or 1, one of the log terms would be mathematically undefined in the above loss equation. PyTorch chooses to set log\u2061(0)=\u2212\u221e\\log (0) = -\\inftylog(0)=\u2212\u221e  , since lim\u2061x\u21920log\u2061(x)=\u2212\u221e\\lim_{x\\to 0} \\log (x) = -\\inftylimx\u21920\u200blog(x)=\u2212\u221e  . However, an infinite term in the loss equation is not desirable for several reasons. For one, if either yn=0y_n = 0yn\u200b=0   or (1\u2212yn)=0(1 - y_n) = 0(1\u2212yn\u200b)=0  , then we would be multiplying 0 with infinity. Secondly, if we have an infinite loss value, then we would also have an infinite term in our gradient, since lim\u2061x\u21920ddxlog\u2061(x)=\u221e\\lim_{x\\to 0} \\frac{d}{dx} \\log (x) = \\inftylimx\u21920\u200bdxd\u200blog(x)=\u221e  . This would make BCELoss\u2019s backward method nonlinear with respect to xnx_nxn\u200b  , and using it for things like linear regression would not be straight-forward. Our solution is that BCELoss clamps its log function outputs to be greater than or equal to -100. This way, we can always have a finite loss value and a linear backward method.  Parameters  weight (Tensor, optional) \u2013 a manual rescaling weight given to the loss of each batch element. If given, has to be a Tensor of size nbatch. size_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'     Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where \u2217*\u2217   means, any number of additional dimensions Target: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input Output: scalar. If reduction is 'none', then (N,\u2217)(N, *)(N,\u2217)  , same shape as input.    Examples: &gt;&gt;&gt; m = nn.Sigmoid() &gt;&gt;&gt; loss = nn.BCELoss() &gt;&gt;&gt; input = torch.randn(3, requires_grad=True) &gt;&gt;&gt; target = torch.empty(3).random_(2) &gt;&gt;&gt; output = loss(m(input), target) &gt;&gt;&gt; output.backward()   ", "parameters": ["weight (Tensor, optional) : a manual rescaling weight given to the lossof each batch element. If given, has to be a Tensor of size nbatch.", "size_average (bool, optional) : Deprecated (see reduction). By default,the losses are averaged over each loss element in the batch. Note that forsome losses, there are multiple elements per sample. If the field size_averageis set to False, the losses are instead summed for each minibatch. Ignoredwhen reduce is False. Default: True", "reduce (bool, optional) : Deprecated (see reduction). By default, thelosses are averaged or summed over observations for each minibatch dependingon size_average. When reduce is False, returns a loss perbatch element instead and ignores size_average. Default: True", "reduction (string, optional) : Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.MultiLabelMarginLoss", "item_type": "class", "code": "classtorch.nn.MultiLabelMarginLoss(size_average=None,reduce=None,reduction:str='mean')", "description": "Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between input xxx   (a 2D mini-batch Tensor) and output yyy   (which is a 2D Tensor of target class indices). For each sample in the mini-batch:  loss(x,y)=\u2211ijmax\u2061(0,1\u2212(x[y[j]]\u2212x[i]))x.size(0)\\text{loss}(x, y) = \\sum_{ij}\\frac{\\max(0, 1 - (x[y[j]] - x[i]))}{\\text{x.size}(0)}  loss(x,y)=ij\u2211\u200bx.size(0)max(0,1\u2212(x[y[j]]\u2212x[i]))\u200b  where x\u2208{0,\u2005\u200a\u22ef\u2009,\u2005\u200ax.size(0)\u22121}x \\in \\left\\{0, \\; \\cdots , \\; \\text{x.size}(0) - 1\\right\\}x\u2208{0,\u22ef,x.size(0)\u22121}  , y\u2208{0,\u2005\u200a\u22ef\u2009,\u2005\u200ay.size(0)\u22121}y \\in \\left\\{0, \\; \\cdots , \\; \\text{y.size}(0) - 1\\right\\}y\u2208{0,\u22ef,y.size(0)\u22121}  , 0\u2264y[j]\u2264x.size(0)\u221210 \\leq y[j] \\leq \\text{x.size}(0)-10\u2264y[j]\u2264x.size(0)\u22121  , and i\u2260y[j]i \\neq y[j]i\ue020\u200b=y[j]   for all iii   and jjj  . yyy   and xxx   must have the same size. The criterion only considers a contiguous block of non-negative targets that starts at the front. This allows for different samples to have variable amounts of target classes.  Parameters  size_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'     Shape: Input: (C)(C)(C)   or (N,C)(N, C)(N,C)   where N is the batch size and C is the number of classes. Target: (C)(C)(C)   or (N,C)(N, C)(N,C)  , label targets padded by -1 ensuring same shape as the input. Output: scalar. If reduction is 'none', then (N)(N)(N)  .    Examples: &gt;&gt;&gt; loss = nn.MultiLabelMarginLoss() &gt;&gt;&gt; x = torch.FloatTensor([[0.1, 0.2, 0.4, 0.8]]) &gt;&gt;&gt; # for target y, only consider labels 3 and 0, not after label -1 &gt;&gt;&gt; y = torch.LongTensor([[3, 0, -1, 1]]) &gt;&gt;&gt; loss(x, y) &gt;&gt;&gt; # 0.25 * ((1-(0.1-0.2)) + (1-(0.1-0.4)) + (1-(0.8-0.2)) + (1-(0.8-0.4))) tensor(0.8500)   ", "parameters": ["size_average (bool, optional) : Deprecated (see reduction). By default,the losses are averaged over each loss element in the batch. Note that forsome losses, there are multiple elements per sample. If the field size_averageis set to False, the losses are instead summed for each minibatch. Ignoredwhen reduce is False. Default: True", "reduce (bool, optional) : Deprecated (see reduction). By default, thelosses are averaged or summed over observations for each minibatch dependingon size_average. When reduce is False, returns a loss perbatch element instead and ignores size_average. Default: True", "reduction (string, optional) : Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Flatten.register_parameter", "item_type": "method", "code": "register_parameter(name:str,param:Optional[torch.nn.parameter.Parameter])\u2192None", "description": "Adds a parameter to the module. The parameter can be accessed as an attribute using given name.  Parameters  name (string) \u2013 name of the parameter. The parameter can be accessed from this module using the given name param (Parameter) \u2013 parameter to be added to the module.    ", "parameters": ["name (string) : name of the parameter. The parameter can be accessedfrom this module using the given name", "param (Parameter) : parameter to be added to the module."], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Flatten.requires_grad_", "item_type": "method", "code": "requires_grad_(requires_grad:bool=True)\u2192T", "description": "Change if autograd should record operations on parameters in this module. This method sets the parameters\u2019 requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).  Parameters requires_grad (bool) \u2013 whether autograd should record operations on parameters in this module. Default: True.  Returns self  Return type Module   ", "parameters": ["requires_grad (bool) : whether autograd should record operations onparameters in this module. Default: True.", "self", "Module"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.KLDivLoss", "item_type": "class", "code": "classtorch.nn.KLDivLoss(size_average=None,reduce=None,reduction:str='mean',log_target:bool=False)", "description": "The Kullback-Leibler divergence Loss KL divergence is a useful distance measure for continuous distributions and is often useful when performing direct regression over the space of (discretely sampled) continuous output distributions. As with NLLLoss, the input given is expected to contain log-probabilities and is not restricted to a 2D Tensor. The targets are interpreted as probabilities by default, but could be considered as log-probabilities with log_target set to True. This criterion expects a target Tensor of the same size as the input Tensor. The unreduced (i.e. with reduction set to 'none') loss can be described as:  l(x,y)=L={l1,\u2026,lN},ln=yn\u22c5(log\u2061yn\u2212xn)l(x,y) = L = \\{ l_1,\\dots,l_N \\}, \\quad l_n = y_n \\cdot \\left( \\log y_n - x_n \\right)  l(x,y)=L={l1\u200b,\u2026,lN\u200b},ln\u200b=yn\u200b\u22c5(logyn\u200b\u2212xn\u200b)  where the index NNN   spans all dimensions of input and LLL   has the same shape as input. If reduction is not 'none' (default 'mean'), then:  \u2113(x,y)={mean\u2061(L),if\u00a0reduction=\u2019mean\u2019;sum\u2061(L),if\u00a0reduction=\u2019sum\u2019.\\ell(x, y) = \\begin{cases}     \\operatorname{mean}(L), &amp; \\text{if reduction} = \\text{'mean';} \\\\     \\operatorname{sum}(L),  &amp; \\text{if reduction} = \\text{'sum'.} \\end{cases}  \u2113(x,y)={mean(L),sum(L),\u200bif\u00a0reduction=\u2019mean\u2019;if\u00a0reduction=\u2019sum\u2019.\u200b  In default reduction mode 'mean', the losses are averaged for each minibatch over observations as well as over dimensions. 'batchmean' mode gives the correct KL divergence where losses are averaged over batch dimension only. 'mean' mode\u2019s behavior will be changed to the same as 'batchmean' in the next major release.  Parameters  size_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'batchmean' | 'sum' | 'mean'. 'none': no reduction will be applied. 'batchmean': the sum of the output will be divided by batchsize. 'sum': the output will be summed. 'mean': the output will be divided by the number of elements in the output. Default: 'mean' log_target (bool, optional) \u2013 Specifies whether target is passed in the log space. Default: False     Note size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction.   Note reduction = 'mean' doesn\u2019t return the true kl divergence value, please use reduction = 'batchmean' which aligns with KL math definition. In the next major release, 'mean' will be changed to be the same as 'batchmean'.   Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where \u2217*\u2217   means, any number of additional dimensions Target: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input Output: scalar by default. If :attr:reduction is 'none', then (N,\u2217)(N, *)(N,\u2217)  , the same shape as the input    ", "parameters": ["size_average (bool, optional) : Deprecated (see reduction). By default,the losses are averaged over each loss element in the batch. Note that forsome losses, there are multiple elements per sample. If the field size_averageis set to False, the losses are instead summed for each minibatch. Ignoredwhen reduce is False. Default: True", "reduce (bool, optional) : Deprecated (see reduction). By default, thelosses are averaged or summed over observations for each minibatch dependingon size_average. When reduce is False, returns a loss perbatch element instead and ignores size_average. Default: True", "reduction (string, optional) : Specifies the reduction to apply to the output:'none' | 'batchmean' | 'sum' | 'mean'.'none': no reduction will be applied.'batchmean': the sum of the output will be divided by batchsize.'sum': the output will be summed.'mean': the output will be divided by the number of elements in the output.Default: 'mean'", "log_target (bool, optional) : Specifies whether target is passed in the log space.Default: False"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.PoissonNLLLoss", "item_type": "class", "code": "classtorch.nn.PoissonNLLLoss(log_input:bool=True,full:bool=False,size_average=None,eps:float=1e-08,reduce=None,reduction:str='mean')", "description": "Negative log likelihood loss with Poisson distribution of target. The loss can be described as:  target\u223cPoisson(input)loss(input,target)=input\u2212target\u2217log\u2061(input)+log\u2061(target!)\\text{target} \\sim \\mathrm{Poisson}(\\text{input})  \\text{loss}(\\text{input}, \\text{target}) = \\text{input} - \\text{target} * \\log(\\text{input})                             + \\log(\\text{target!})target\u223cPoisson(input)loss(input,target)=input\u2212target\u2217log(input)+log(target!)  The last term can be omitted or approximated with Stirling formula. The approximation is used for target values more than 1. For targets less or equal to 1 zeros are added to the loss.  Parameters  log_input (bool, optional) \u2013 if True the loss is computed as exp\u2061(input)\u2212target\u2217input\\exp(\\text{input}) - \\text{target}*\\text{input}exp(input)\u2212target\u2217input  , if False the loss is input\u2212target\u2217log\u2061(input+eps)\\text{input} - \\text{target}*\\log(\\text{input}+\\text{eps})input\u2212target\u2217log(input+eps)  . full (bool, optional) \u2013 whether to compute full loss, i. e. to add the Stirling approximation term  target\u2217log\u2061(target)\u2212target+0.5\u2217log\u2061(2\u03c0target).\\text{target}*\\log(\\text{target}) - \\text{target} + 0.5 * \\log(2\\pi\\text{target}).  target\u2217log(target)\u2212target+0.5\u2217log(2\u03c0target).   size_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True eps (float, optional) \u2013 Small value to avoid evaluation of log\u2061(0)\\log(0)log(0)   when log_input = False. Default: 1e-8 reduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'    Examples: &gt;&gt;&gt; loss = nn.PoissonNLLLoss() &gt;&gt;&gt; log_input = torch.randn(5, 2, requires_grad=True) &gt;&gt;&gt; target = torch.randn(5, 2) &gt;&gt;&gt; output = loss(log_input, target) &gt;&gt;&gt; output.backward()    Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where \u2217*\u2217   means, any number of additional dimensions Target: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input Output: scalar by default. If reduction is 'none', then (N,\u2217)(N, *)(N,\u2217)  , the same shape as the input    ", "parameters": ["log_input (bool, optional) : if True the loss is computed asexp\u2061(input)\u2212target\u2217input\\exp(\\text{input}) - \\text{target}*\\text{input}exp(input)\u2212target\u2217input, if False the loss isinput\u2212target\u2217log\u2061(input+eps)\\text{input} - \\text{target}*\\log(\\text{input}+\\text{eps})input\u2212target\u2217log(input+eps).", "full (bool, optional) : whether to compute full loss, i. e. to add theStirling approximation termtarget\u2217log\u2061(target)\u2212target+0.5\u2217log\u2061(2\u03c0target).\\text{target}*\\log(\\text{target}) - \\text{target} + 0.5 * \\log(2\\pi\\text{target}).target\u2217log(target)\u2212target+0.5\u2217log(2\u03c0target).", "size_average (bool, optional) : Deprecated (see reduction). By default,the losses are averaged over each loss element in the batch. Note that forsome losses, there are multiple elements per sample. If the field size_averageis set to False, the losses are instead summed for each minibatch. Ignoredwhen reduce is False. Default: True", "eps (float, optional) : Small value to avoid evaluation of log\u2061(0)\\log(0)log(0) whenlog_input = False. Default: 1e-8", "reduce (bool, optional) : Deprecated (see reduction). By default, thelosses are averaged or summed over observations for each minibatch dependingon size_average. When reduce is False, returns a loss perbatch element instead and ignores size_average. Default: True", "reduction (string, optional) : Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Flatten.state_dict", "item_type": "method", "code": "state_dict(destination=None,prefix='',keep_vars=False)", "description": "Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names.  Returns a dictionary containing a whole state of the module  Return type dict   Example: &gt;&gt;&gt; module.state_dict().keys() ['bias', 'weight']   ", "parameters": ["a dictionary containing a whole state of the module", "dict"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Flatten.to", "item_type": "method", "code": "to(*args,**kwargs)", "description": "Moves and/or casts the parameters and buffers. This can be called as   to(device=None, dtype=None, non_blocking=False)     to(dtype, non_blocking=False)     to(tensor, non_blocking=False)     to(memory_format=torch.channels_last)   Its signature is similar to torch.Tensor.to(), but only accepts floating point desired dtype s. In addition, this method will only cast the floating point parameters and buffers to dtype (if given). The integral parameters and buffers will be moved device, if that is given, but with dtypes unchanged. When non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples.  Note This method modifies the module in-place.   Parameters  device (torch.device) \u2013 the desired device of the parameters and buffers in this module dtype (torch.dtype) \u2013 the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor) \u2013 Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (torch.memory_format) \u2013 the desired memory format for 4D parameters and buffers in this module (keyword only argument)   Returns self  Return type Module   Example: &gt;&gt;&gt; linear = nn.Linear(2, 2) &gt;&gt;&gt; linear.weight Parameter containing: tensor([[ 0.1913, -0.3420],         [-0.5113, -0.2325]]) &gt;&gt;&gt; linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) &gt;&gt;&gt; linear.weight Parameter containing: tensor([[ 0.1913, -0.3420],         [-0.5113, -0.2325]], dtype=torch.float64) &gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\") &gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) &gt;&gt;&gt; linear.weight Parameter containing: tensor([[ 0.1914, -0.3420],         [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') &gt;&gt;&gt; cpu = torch.device(\"cpu\") &gt;&gt;&gt; linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) &gt;&gt;&gt; linear.weight Parameter containing: tensor([[ 0.1914, -0.3420],         [-0.5112, -0.2324]], dtype=torch.float16)   ", "parameters": ["device (torch.device) : the desired device of the parametersand buffers in this module", "dtype (torch.dtype) : the desired floating point type ofthe floating point parameters and buffers in this module", "tensor (torch.Tensor) : Tensor whose dtype and device are the desireddtype and device for all parameters and buffers in this module", "memory_format (torch.memory_format) : the desired memoryformat for 4D parameters and buffers in this module (keywordonly argument)"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.CTCLoss", "item_type": "class", "code": "classtorch.nn.CTCLoss(blank:int=0,reduction:str='mean',zero_infinity:bool=False)", "description": "The Connectionist Temporal Classification loss. Calculates loss between a continuous (unsegmented) time series and a target sequence. CTCLoss sums over the probability of possible alignments of input to target, producing a loss value which is differentiable with respect to each input node. The alignment of input to target is assumed to be \u201cmany-to-one\u201d, which limits the length of the target sequence such that it must be \u2264\\leq\u2264   the input length.  Parameters  blank (int, optional) \u2013 blank label. Default 000  . reduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the output losses will be divided by the target lengths and then the mean over the batch is taken. Default: 'mean' zero_infinity (bool, optional) \u2013 Whether to zero infinite losses and the associated gradients. Default: False Infinite losses mainly occur when the inputs are too short to be aligned to the targets.     Shape: Log_probs: Tensor of size (T,N,C)(T, N, C)(T,N,C)  , where T=input\u00a0lengthT = \\text{input length}T=input\u00a0length  , N=batch\u00a0sizeN = \\text{batch size}N=batch\u00a0size  , and C=number\u00a0of\u00a0classes\u00a0(including\u00a0blank)C = \\text{number of classes (including blank)}C=number\u00a0of\u00a0classes\u00a0(including\u00a0blank)  . The logarithmized probabilities of the outputs (e.g. obtained with torch.nn.functional.log_softmax()). Targets: Tensor of size (N,S)(N, S)(N,S)   or (sum\u2061(target_lengths))(\\operatorname{sum}(\\text{target\\_lengths}))(sum(target_lengths))  , where N=batch\u00a0sizeN = \\text{batch size}N=batch\u00a0size   and S=max\u00a0target\u00a0length,\u00a0if\u00a0shape\u00a0is\u00a0(N,S)S = \\text{max target length, if shape is } (N, S)S=max\u00a0target\u00a0length,\u00a0if\u00a0shape\u00a0is\u00a0(N,S)  . It represent the target sequences. Each element in the target sequence is a class index. And the target index cannot be blank (default=0). In the (N,S)(N, S)(N,S)   form, targets are padded to the length of the longest sequence, and stacked. In the (sum\u2061(target_lengths))(\\operatorname{sum}(\\text{target\\_lengths}))(sum(target_lengths))   form, the targets are assumed to be un-padded and concatenated within 1 dimension. Input_lengths: Tuple or tensor of size (N)(N)(N)  , where N=batch\u00a0sizeN = \\text{batch size}N=batch\u00a0size  . It represent the lengths of the inputs (must each be \u2264T\\leq T\u2264T  ). And the lengths are specified for each sequence to achieve masking under the assumption that sequences are padded to equal lengths. Target_lengths: Tuple or tensor of size (N)(N)(N)  , where N=batch\u00a0sizeN = \\text{batch size}N=batch\u00a0size  . It represent lengths of the targets. Lengths are specified for each sequence to achieve masking under the assumption that sequences are padded to equal lengths. If target shape is (N,S)(N,S)(N,S)  , target_lengths are effectively the stop index sns_nsn\u200b   for each target sequence, such that target_n = targets[n,0:s_n] for each target in a batch. Lengths must each be \u2264S\\leq S\u2264S   If the targets are given as a 1d tensor that is the concatenation of individual targets, the target_lengths must add up to the total length of the tensor. Output: scalar. If reduction is 'none', then (N)(N)(N)  , where N=batch\u00a0sizeN = \\text{batch size}N=batch\u00a0size  .    Examples: &gt;&gt;&gt; # Target are to be padded &gt;&gt;&gt; T = 50      # Input sequence length &gt;&gt;&gt; C = 20      # Number of classes (including blank) &gt;&gt;&gt; N = 16      # Batch size &gt;&gt;&gt; S = 30      # Target sequence length of longest target in batch (padding length) &gt;&gt;&gt; S_min = 10  # Minimum target length, for demonstration purposes &gt;&gt;&gt; &gt;&gt;&gt; # Initialize random batch of input vectors, for *size = (T,N,C) &gt;&gt;&gt; input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_() &gt;&gt;&gt; &gt;&gt;&gt; # Initialize random batch of targets (0 = blank, 1:C = classes) &gt;&gt;&gt; target = torch.randint(low=1, high=C, size=(N, S), dtype=torch.long) &gt;&gt;&gt; &gt;&gt;&gt; input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long) &gt;&gt;&gt; target_lengths = torch.randint(low=S_min, high=S, size=(N,), dtype=torch.long) &gt;&gt;&gt; ctc_loss = nn.CTCLoss() &gt;&gt;&gt; loss = ctc_loss(input, target, input_lengths, target_lengths) &gt;&gt;&gt; loss.backward() &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; # Target are to be un-padded &gt;&gt;&gt; T = 50      # Input sequence length &gt;&gt;&gt; C = 20      # Number of classes (including blank) &gt;&gt;&gt; N = 16      # Batch size &gt;&gt;&gt; &gt;&gt;&gt; # Initialize random batch of input vectors, for *size = (T,N,C) &gt;&gt;&gt; input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_() &gt;&gt;&gt; input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long) &gt;&gt;&gt; &gt;&gt;&gt; # Initialize random batch of targets (0 = blank, 1:C = classes) &gt;&gt;&gt; target_lengths = torch.randint(low=1, high=T, size=(N,), dtype=torch.long) &gt;&gt;&gt; target = torch.randint(low=1, high=C, size=(sum(target_lengths),), dtype=torch.long) &gt;&gt;&gt; ctc_loss = nn.CTCLoss() &gt;&gt;&gt; loss = ctc_loss(input, target, input_lengths, target_lengths) &gt;&gt;&gt; loss.backward()    Reference:A. Graves et al.: Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks: https://www.cs.toronto.edu/~graves/icml_2006.pdf    Note In order to use CuDNN, the following must be satisfied: targets must be in concatenated format, all input_lengths must be T.  blank=0blank=0blank=0  , target_lengths \u2264256\\leq 256\u2264256  , the integer arguments must be of dtype torch.int32. The regular implementation uses the (more common in PyTorch) torch.long dtype.   Note In some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. Please see the notes on Reproducibility for background.  ", "parameters": ["blank (int, optional) : blank label. Default 000.", "reduction (string, optional) : Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the output losses will be divided by the target lengths andthen the mean over the batch is taken. Default: 'mean'", "zero_infinity (bool, optional) : Whether to zero infinite losses and the associated gradients.Default: FalseInfinite losses mainly occur when the inputs are too shortto be aligned to the targets."], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.CrossEntropyLoss", "item_type": "class", "code": "classtorch.nn.CrossEntropyLoss(weight:Optional[torch.Tensor]=None,size_average=None,ignore_index:int=-100,reduce=None,reduction:str='mean')", "description": "This criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class. It is useful when training a classification problem with C classes. If provided, the optional argument weight should be a 1D Tensor assigning weight to each of the classes. This is particularly useful when you have an unbalanced training set. The input is expected to contain raw, unnormalized scores for each class. input has to be a Tensor of size either (minibatch,C)(minibatch, C)(minibatch,C)   or (minibatch,C,d1,d2,...,dK)(minibatch, C, d_1, d_2, ..., d_K)(minibatch,C,d1\u200b,d2\u200b,...,dK\u200b)   with K\u22651K \\geq 1K\u22651   for the K-dimensional case (described later). This criterion expects a class index in the range [0,C\u22121][0, C-1][0,C\u22121]   as the target for each value of a 1D tensor of size minibatch; if ignore_index is specified, this criterion also accepts this class index (this index may not necessarily be in the class range). The loss can be described as:  loss(x,class)=\u2212log\u2061(exp\u2061(x[class])\u2211jexp\u2061(x[j]))=\u2212x[class]+log\u2061(\u2211jexp\u2061(x[j]))\\text{loss}(x, class) = -\\log\\left(\\frac{\\exp(x[class])}{\\sum_j \\exp(x[j])}\\right)                = -x[class] + \\log\\left(\\sum_j \\exp(x[j])\\right)  loss(x,class)=\u2212log(\u2211j\u200bexp(x[j])exp(x[class])\u200b)=\u2212x[class]+log(j\u2211\u200bexp(x[j]))  or in the case of the weight argument being specified:  loss(x,class)=weight[class](\u2212x[class]+log\u2061(\u2211jexp\u2061(x[j])))\\text{loss}(x, class) = weight[class] \\left(-x[class] + \\log\\left(\\sum_j \\exp(x[j])\\right)\\right)  loss(x,class)=weight[class](\u2212x[class]+log(j\u2211\u200bexp(x[j])))  The losses are averaged across observations for each minibatch. If the weight argument is specified then this is a weighted average:  loss=\u2211i=1Nloss(i,class[i])\u2211i=1Nweight[class[i]]\\text{loss} = \\frac{\\sum^{N}_{i=1} loss(i, class[i])}{\\sum^{N}_{i=1} weight[class[i]]}  loss=\u2211i=1N\u200bweight[class[i]]\u2211i=1N\u200bloss(i,class[i])\u200b  Can also be used for higher dimension inputs, such as 2D images, by providing an input of size (minibatch,C,d1,d2,...,dK)(minibatch, C, d_1, d_2, ..., d_K)(minibatch,C,d1\u200b,d2\u200b,...,dK\u200b)   with K\u22651K \\geq 1K\u22651  , where KKK   is the number of dimensions, and a target of appropriate shape (see below).  Parameters  weight (Tensor, optional) \u2013 a manual rescaling weight given to each class. If given, has to be a Tensor of size C size_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True ignore_index (int, optional) \u2013 Specifies a target value that is ignored and does not contribute to the input gradient. When size_average is True, the loss is averaged over non-ignored targets. reduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the weighted mean of the output is taken, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'     Shape: Input: (N,C)(N, C)(N,C)   where C = number of classes, or (N,C,d1,d2,...,dK)(N, C, d_1, d_2, ..., d_K)(N,C,d1\u200b,d2\u200b,...,dK\u200b)   with K\u22651K \\geq 1K\u22651   in the case of K-dimensional loss. Target: (N)(N)(N)   where each value is 0\u2264targets[i]\u2264C\u221210 \\leq \\text{targets}[i] \\leq C-10\u2264targets[i]\u2264C\u22121  , or (N,d1,d2,...,dK)(N, d_1, d_2, ..., d_K)(N,d1\u200b,d2\u200b,...,dK\u200b)   with K\u22651K \\geq 1K\u22651   in the case of K-dimensional loss. Output: scalar. If reduction is 'none', then the same size as the target: (N)(N)(N)  , or (N,d1,d2,...,dK)(N, d_1, d_2, ..., d_K)(N,d1\u200b,d2\u200b,...,dK\u200b)   with K\u22651K \\geq 1K\u22651   in the case of K-dimensional loss.    Examples: &gt;&gt;&gt; loss = nn.CrossEntropyLoss() &gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True) &gt;&gt;&gt; target = torch.empty(3, dtype=torch.long).random_(5) &gt;&gt;&gt; output = loss(input, target) &gt;&gt;&gt; output.backward()   ", "parameters": ["weight (Tensor, optional) : a manual rescaling weight given to each class.If given, has to be a Tensor of size C", "size_average (bool, optional) : Deprecated (see reduction). By default,the losses are averaged over each loss element in the batch. Note that forsome losses, there are multiple elements per sample. If the field size_averageis set to False, the losses are instead summed for each minibatch. Ignoredwhen reduce is False. Default: True", "ignore_index (int, optional) : Specifies a target value that is ignoredand does not contribute to the input gradient. When size_average isTrue, the loss is averaged over non-ignored targets.", "reduce (bool, optional) : Deprecated (see reduction). By default, thelosses are averaged or summed over observations for each minibatch dependingon size_average. When reduce is False, returns a loss perbatch element instead and ignores size_average. Default: True", "reduction (string, optional) : Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction willbe applied, 'mean': the weighted mean of the output is taken,'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and inthe meantime, specifying either of those two args will overridereduction. Default: 'mean'"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.PairwiseDistance", "item_type": "class", "code": "classtorch.nn.PairwiseDistance(p:float=2.0,eps:float=1e-06,keepdim:bool=False)", "description": "Computes the batchwise pairwise distance between vectors v1v_1v1\u200b  , v2v_2v2\u200b   using the p-norm:  \u2225x\u2225p=(\u2211i=1n\u2223xi\u2223p)1/p.\\Vert x \\Vert _p = \\left( \\sum_{i=1}^n  \\vert x_i \\vert ^ p \\right) ^ {1/p}.  \u2225x\u2225p\u200b=(i=1\u2211n\u200b\u2223xi\u200b\u2223p)1/p.   Parameters  p (real) \u2013 the norm degree. Default: 2 eps (float, optional) \u2013 Small value to avoid division by zero. Default: 1e-6 keepdim (bool, optional) \u2013 Determines whether or not to keep the vector dimension. Default: False     Shape: Input1: (N,D)(N, D)(N,D)   where D = vector dimension Input2: (N,D)(N, D)(N,D)  , same shape as the Input1 Output: (N)(N)(N)  . If keepdim is True, then (N,1)(N, 1)(N,1)  .   Examples::&gt;&gt;&gt; pdist = nn.PairwiseDistance(p=2) &gt;&gt;&gt; input1 = torch.randn(100, 128) &gt;&gt;&gt; input2 = torch.randn(100, 128) &gt;&gt;&gt; output = pdist(input1, input2)     ", "parameters": ["p (real) : the norm degree. Default: 2", "eps (float, optional) : Small value to avoid division by zero.Default: 1e-6", "keepdim (bool, optional) : Determines whether or not to keep the vector dimension.Default: False"], "returns": [], "example": "NA", "shape": "\nInput1: (N,D)(N, D)(N,D)\n\n where D = vector dimension\nInput2: (N,D)(N, D)(N,D)\n\n, same shape as the Input1\nOutput: (N)(N)(N)\n\n. If keepdim is True, then (N,1)(N, 1)(N,1)\n\n.\n\n"},
{"library": "torch", "item_id": "torch.nn.NLLLoss", "item_type": "class", "code": "classtorch.nn.NLLLoss(weight:Optional[torch.Tensor]=None,size_average=None,ignore_index:int=-100,reduce=None,reduction:str='mean')", "description": "The negative log likelihood loss. It is useful to train a classification problem with C classes. If provided, the optional argument weight should be a 1D Tensor assigning weight to each of the classes. This is particularly useful when you have an unbalanced training set. The input given through a forward call is expected to contain log-probabilities of each class. input has to be a Tensor of size either (minibatch,C)(minibatch, C)(minibatch,C)   or (minibatch,C,d1,d2,...,dK)(minibatch, C, d_1, d_2, ..., d_K)(minibatch,C,d1\u200b,d2\u200b,...,dK\u200b)   with K\u22651K \\geq 1K\u22651   for the K-dimensional case (described later). Obtaining log-probabilities in a neural network is easily achieved by adding a  LogSoftmax  layer in the last layer of your network. You may use CrossEntropyLoss instead, if you prefer not to add an extra layer. The target that this loss expects should be a class index in the range [0,C\u22121][0, C-1][0,C\u22121]   where C = number of classes; if ignore_index is specified, this loss also accepts this class index (this index may not necessarily be in the class range). The unreduced (i.e. with reduction set to 'none') loss can be described as:  \u2113(x,y)=L={l1,\u2026,lN}\u22a4,ln=\u2212wynxn,yn,wc=weight[c]\u22c51{c=\u0338ignore_index},\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad l_n = - w_{y_n} x_{n,y_n}, \\quad w_{c} = \\text{weight}[c] \\cdot \\mathbb{1}\\{c \\not= \\text{ignore\\_index}\\},  \u2113(x,y)=L={l1\u200b,\u2026,lN\u200b}\u22a4,ln\u200b=\u2212wyn\u200b\u200bxn,yn\u200b\u200b,wc\u200b=weight[c]\u22c51{c\ue020\u200b=ignore_index},  where xxx   is the input, yyy   is the target, www   is the weight, and NNN   is the batch size. If reduction is not 'none' (default 'mean'), then  \u2113(x,y)={\u2211n=1N1\u2211n=1Nwynln,if\u00a0reduction=\u2019mean\u2019;\u2211n=1Nln,if\u00a0reduction=\u2019sum\u2019.\\ell(x, y) = \\begin{cases}     \\sum_{n=1}^N \\frac{1}{\\sum_{n=1}^N w_{y_n}} l_n, &amp;     \\text{if reduction} = \\text{'mean';}\\\\     \\sum_{n=1}^N l_n,  &amp;     \\text{if reduction} = \\text{'sum'.} \\end{cases}  \u2113(x,y)={\u2211n=1N\u200b\u2211n=1N\u200bwyn\u200b\u200b1\u200bln\u200b,\u2211n=1N\u200bln\u200b,\u200bif\u00a0reduction=\u2019mean\u2019;if\u00a0reduction=\u2019sum\u2019.\u200b  Can also be used for higher dimension inputs, such as 2D images, by providing an input of size (minibatch,C,d1,d2,...,dK)(minibatch, C, d_1, d_2, ..., d_K)(minibatch,C,d1\u200b,d2\u200b,...,dK\u200b)   with K\u22651K \\geq 1K\u22651  , where KKK   is the number of dimensions, and a target of appropriate shape (see below). In the case of images, it computes NLL loss per-pixel.  Parameters  weight (Tensor, optional) \u2013 a manual rescaling weight given to each class. If given, it has to be a Tensor of size C. Otherwise, it is treated as if having all ones. size_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True ignore_index (int, optional) \u2013 Specifies a target value that is ignored and does not contribute to the input gradient. When size_average is True, the loss is averaged over non-ignored targets. reduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the weighted mean of the output is taken, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'     Shape: Input: (N,C)(N, C)(N,C)   where C = number of classes, or (N,C,d1,d2,...,dK)(N, C, d_1, d_2, ..., d_K)(N,C,d1\u200b,d2\u200b,...,dK\u200b)   with K\u22651K \\geq 1K\u22651   in the case of K-dimensional loss. Target: (N)(N)(N)   where each value is 0\u2264targets[i]\u2264C\u221210 \\leq \\text{targets}[i] \\leq C-10\u2264targets[i]\u2264C\u22121  , or (N,d1,d2,...,dK)(N, d_1, d_2, ..., d_K)(N,d1\u200b,d2\u200b,...,dK\u200b)   with K\u22651K \\geq 1K\u22651   in the case of K-dimensional loss. Output: scalar. If reduction is 'none', then the same size as the target: (N)(N)(N)  , or (N,d1,d2,...,dK)(N, d_1, d_2, ..., d_K)(N,d1\u200b,d2\u200b,...,dK\u200b)   with K\u22651K \\geq 1K\u22651   in the case of K-dimensional loss.    Examples: &gt;&gt;&gt; m = nn.LogSoftmax(dim=1) &gt;&gt;&gt; loss = nn.NLLLoss() &gt;&gt;&gt; # input is of size N x C = 3 x 5 &gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True) &gt;&gt;&gt; # each element in target has to have 0 &lt;= value &lt; C &gt;&gt;&gt; target = torch.tensor([1, 0, 4]) &gt;&gt;&gt; output = loss(m(input), target) &gt;&gt;&gt; output.backward() &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; # 2D loss example (used, for example, with image inputs) &gt;&gt;&gt; N, C = 5, 4 &gt;&gt;&gt; loss = nn.NLLLoss() &gt;&gt;&gt; # input is of size N x C x height x width &gt;&gt;&gt; data = torch.randn(N, 16, 10, 10) &gt;&gt;&gt; conv = nn.Conv2d(16, C, (3, 3)) &gt;&gt;&gt; m = nn.LogSoftmax(dim=1) &gt;&gt;&gt; # each element in target has to have 0 &lt;= value &lt; C &gt;&gt;&gt; target = torch.empty(N, 8, 8, dtype=torch.long).random_(0, C) &gt;&gt;&gt; output = loss(m(conv(data)), target) &gt;&gt;&gt; output.backward()   ", "parameters": ["weight (Tensor, optional) : a manual rescaling weight given to eachclass. If given, it has to be a Tensor of size C. Otherwise, it istreated as if having all ones.", "size_average (bool, optional) : Deprecated (see reduction). By default,the losses are averaged over each loss element in the batch. Note that forsome losses, there are multiple elements per sample. If the field size_averageis set to False, the losses are instead summed for each minibatch. Ignoredwhen reduce is False. Default: True", "ignore_index (int, optional) : Specifies a target value that is ignoredand does not contribute to the input gradient. Whensize_average is True, the loss is averaged overnon-ignored targets.", "reduce (bool, optional) : Deprecated (see reduction). By default, thelosses are averaged or summed over observations for each minibatch dependingon size_average. When reduce is False, returns a loss perbatch element instead and ignores size_average. Default: True", "reduction (string, optional) : Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction willbe applied, 'mean': the weighted mean of the output is taken,'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and inthe meantime, specifying either of those two args will overridereduction. Default: 'mean'"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.MSELoss", "item_type": "class", "code": "classtorch.nn.MSELoss(size_average=None,reduce=None,reduction:str='mean')", "description": "Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the input xxx   and target yyy  . The unreduced (i.e. with reduction set to 'none') loss can be described as:  \u2113(x,y)=L={l1,\u2026,lN}\u22a4,ln=(xn\u2212yn)2,\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad l_n = \\left( x_n - y_n \\right)^2,  \u2113(x,y)=L={l1\u200b,\u2026,lN\u200b}\u22a4,ln\u200b=(xn\u200b\u2212yn\u200b)2,  where NNN   is the batch size. If reduction is not 'none' (default 'mean'), then:  \u2113(x,y)={mean\u2061(L),if\u00a0reduction=\u2019mean\u2019;sum\u2061(L),if\u00a0reduction=\u2019sum\u2019.\\ell(x, y) = \\begin{cases}     \\operatorname{mean}(L), &amp;  \\text{if reduction} = \\text{'mean';}\\\\     \\operatorname{sum}(L),  &amp;  \\text{if reduction} = \\text{'sum'.} \\end{cases}  \u2113(x,y)={mean(L),sum(L),\u200bif\u00a0reduction=\u2019mean\u2019;if\u00a0reduction=\u2019sum\u2019.\u200b  xxx   and yyy   are tensors of arbitrary shapes with a total of nnn   elements each. The mean operation still operates over all the elements, and divides by nnn  . The division by nnn   can be avoided if one sets reduction = 'sum'.  Parameters  size_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'     Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where \u2217*\u2217   means, any number of additional dimensions Target: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input    Examples: &gt;&gt;&gt; loss = nn.MSELoss() &gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True) &gt;&gt;&gt; target = torch.randn(3, 5) &gt;&gt;&gt; output = loss(input, target) &gt;&gt;&gt; output.backward()   ", "parameters": ["size_average (bool, optional) : Deprecated (see reduction). By default,the losses are averaged over each loss element in the batch. Note that forsome losses, there are multiple elements per sample. If the field size_averageis set to False, the losses are instead summed for each minibatch. Ignoredwhen reduce is False. Default: True", "reduce (bool, optional) : Deprecated (see reduction). By default, thelosses are averaged or summed over observations for each minibatch dependingon size_average. When reduce is False, returns a loss perbatch element instead and ignores size_average. Default: True", "reduction (string, optional) : Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.L1Loss", "item_type": "class", "code": "classtorch.nn.L1Loss(size_average=None,reduce=None,reduction:str='mean')", "description": "Creates a criterion that measures the mean absolute error (MAE) between each element in the input xxx   and target yyy  . The unreduced (i.e. with reduction set to 'none') loss can be described as:  \u2113(x,y)=L={l1,\u2026,lN}\u22a4,ln=\u2223xn\u2212yn\u2223,\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad l_n = \\left| x_n - y_n \\right|,  \u2113(x,y)=L={l1\u200b,\u2026,lN\u200b}\u22a4,ln\u200b=\u2223xn\u200b\u2212yn\u200b\u2223,  where NNN   is the batch size. If reduction is not 'none' (default 'mean'), then:  \u2113(x,y)={mean\u2061(L),if\u00a0reduction=\u2019mean\u2019;sum\u2061(L),if\u00a0reduction=\u2019sum\u2019.\\ell(x, y) = \\begin{cases}     \\operatorname{mean}(L), &amp; \\text{if reduction} = \\text{'mean';}\\\\     \\operatorname{sum}(L),  &amp; \\text{if reduction} = \\text{'sum'.} \\end{cases}  \u2113(x,y)={mean(L),sum(L),\u200bif\u00a0reduction=\u2019mean\u2019;if\u00a0reduction=\u2019sum\u2019.\u200b  xxx   and yyy   are tensors of arbitrary shapes with a total of nnn   elements each. The sum operation still operates over all the elements, and divides by nnn  . The division by nnn   can be avoided if one sets reduction = 'sum'.  Parameters  size_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'     Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where \u2217*\u2217   means, any number of additional dimensions Target: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input Output: scalar. If reduction is 'none', then (N,\u2217)(N, *)(N,\u2217)  , same shape as the input    Examples: &gt;&gt;&gt; loss = nn.L1Loss() &gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True) &gt;&gt;&gt; target = torch.randn(3, 5) &gt;&gt;&gt; output = loss(input, target) &gt;&gt;&gt; output.backward()   ", "parameters": ["size_average (bool, optional) : Deprecated (see reduction). By default,the losses are averaged over each loss element in the batch. Note that forsome losses, there are multiple elements per sample. If the field size_averageis set to False, the losses are instead summed for each minibatch. Ignoredwhen reduce is False. Default: True", "reduce (bool, optional) : Deprecated (see reduction). By default, thelosses are averaged or summed over observations for each minibatch dependingon size_average. When reduce is False, returns a loss perbatch element instead and ignores size_average. Default: True", "reduction (string, optional) : Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Flatten.train", "item_type": "method", "code": "train(mode:bool=True)\u2192T", "description": "Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc.  Parameters mode (bool) \u2013 whether to set training mode (True) or evaluation mode (False). Default: True.  Returns self  Return type Module   ", "parameters": ["mode (bool) : whether to set training mode (True) or evaluationmode (False). Default: True.", "self", "Module"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Flatten.type", "item_type": "method", "code": "type(dst_type:Union[torch.dtype,str])\u2192T", "description": "Casts all parameters and buffers to dst_type.  Parameters dst_type (type or string) \u2013 the desired type  Returns self  Return type Module   ", "parameters": ["dst_type (type or string) : the desired type", "self", "Module"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.CosineSimilarity", "item_type": "class", "code": "classtorch.nn.CosineSimilarity(dim:int=1,eps:float=1e-08)", "description": "Returns cosine similarity between x1x_1x1\u200b   and x2x_2x2\u200b  , computed along dim.  similarity=x1\u22c5x2max\u2061(\u2225x1\u22252\u22c5\u2225x2\u22252,\u03f5).\\text{similarity} = \\dfrac{x_1 \\cdot x_2}{\\max(\\Vert x_1 \\Vert _2 \\cdot \\Vert x_2 \\Vert _2, \\epsilon)}.  similarity=max(\u2225x1\u200b\u22252\u200b\u22c5\u2225x2\u200b\u22252\u200b,\u03f5)x1\u200b\u22c5x2\u200b\u200b.   Parameters  dim (int, optional) \u2013 Dimension where cosine similarity is computed. Default: 1 eps (float, optional) \u2013 Small value to avoid division by zero. Default: 1e-8     Shape: Input1: (\u22171,D,\u22172)(\\ast_1, D, \\ast_2)(\u22171\u200b,D,\u22172\u200b)   where D is at position dim Input2: (\u22171,D,\u22172)(\\ast_1, D, \\ast_2)(\u22171\u200b,D,\u22172\u200b)  , same shape as the Input1 Output: (\u22171,\u22172)(\\ast_1, \\ast_2)(\u22171\u200b,\u22172\u200b)     Examples::&gt;&gt;&gt; input1 = torch.randn(100, 128) &gt;&gt;&gt; input2 = torch.randn(100, 128) &gt;&gt;&gt; cos = nn.CosineSimilarity(dim=1, eps=1e-6) &gt;&gt;&gt; output = cos(input1, input2)     ", "parameters": ["dim (int, optional) : Dimension where cosine similarity is computed. Default: 1", "eps (float, optional) : Small value to avoid division by zero.Default: 1e-8"], "returns": [], "example": "NA", "shape": "\nInput1: (\u22171,D,\u22172)(\\ast_1, D, \\ast_2)(\u22171\u200b,D,\u22172\u200b)\n\n where D is at position dim\nInput2: (\u22171,D,\u22172)(\\ast_1, D, \\ast_2)(\u22171\u200b,D,\u22172\u200b)\n\n, same shape as the Input1\nOutput: (\u22171,\u22172)(\\ast_1, \\ast_2)(\u22171\u200b,\u22172\u200b)\n\n\n\n"},
{"library": "torch", "item_id": "torch.nn.Flatten.zero_grad", "item_type": "method", "code": "zero_grad()\u2192None", "description": "Sets gradients of all model parameters to zero. ", "parameters": [], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.EmbeddingBag.from_pretrained", "item_type": "method", "code": "classmethodfrom_pretrained(embeddings:torch.Tensor,freeze:bool=True,max_norm:Optional[float]=None,norm_type:float=2.0,scale_grad_by_freq:bool=False,mode:str='mean',sparse:bool=False,include_last_offset:bool=False)\u2192torch.nn.modules.sparse.EmbeddingBag", "description": "Creates EmbeddingBag instance from given 2-dimensional FloatTensor.  Parameters  embeddings (Tensor) \u2013 FloatTensor containing weights for the EmbeddingBag. First dimension is being passed to EmbeddingBag as \u2018num_embeddings\u2019, second as \u2018embedding_dim\u2019. freeze (boolean, optional) \u2013 If True, the tensor does not get updated in the learning process. Equivalent to embeddingbag.weight.requires_grad = False. Default: True max_norm (float, optional) \u2013 See module initialization documentation. Default: None norm_type (float, optional) \u2013 See module initialization documentation. Default 2. scale_grad_by_freq (boolean, optional) \u2013 See module initialization documentation. Default False. mode (string, optional) \u2013 See module initialization documentation. Default: \"mean\" sparse (bool, optional) \u2013 See module initialization documentation. Default: False. include_last_offset (bool, optional) \u2013 See module initialization documentation. Default: False.    Examples: &gt;&gt;&gt; # FloatTensor containing pretrained weights &gt;&gt;&gt; weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]]) &gt;&gt;&gt; embeddingbag = nn.EmbeddingBag.from_pretrained(weight) &gt;&gt;&gt; # Get embeddings for index 1 &gt;&gt;&gt; input = torch.LongTensor([[1, 0]]) &gt;&gt;&gt; embeddingbag(input) tensor([[ 2.5000,  3.7000,  4.6500]])   ", "parameters": ["embeddings (Tensor) : FloatTensor containing weights for the EmbeddingBag.First dimension is being passed to EmbeddingBag as \u2018num_embeddings\u2019, second as \u2018embedding_dim\u2019.", "freeze (boolean, optional) : If True, the tensor does not get updated in the learning process.Equivalent to embeddingbag.weight.requires_grad = False. Default: True", "max_norm (float, optional) : See module initialization documentation. Default: None", "norm_type (float, optional) : See module initialization documentation. Default 2.", "scale_grad_by_freq (boolean, optional) : See module initialization documentation. Default False.", "mode (string, optional) : See module initialization documentation. Default: \"mean\"", "sparse (bool, optional) : See module initialization documentation. Default: False.", "include_last_offset (bool, optional) : See module initialization documentation. Default: False."], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Embedding.from_pretrained", "item_type": "method", "code": "classmethodfrom_pretrained(embeddings,freeze=True,padding_idx=None,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,sparse=False)", "description": "Creates Embedding instance from given 2-dimensional FloatTensor.  Parameters  embeddings (Tensor) \u2013 FloatTensor containing weights for the Embedding. First dimension is being passed to Embedding as num_embeddings, second as embedding_dim. freeze (boolean, optional) \u2013 If True, the tensor does not get updated in the learning process. Equivalent to embedding.weight.requires_grad = False. Default: True padding_idx (int, optional) \u2013 See module initialization documentation. max_norm (float, optional) \u2013 See module initialization documentation. norm_type (float, optional) \u2013 See module initialization documentation. Default 2. scale_grad_by_freq (boolean, optional) \u2013 See module initialization documentation. Default False. sparse (bool, optional) \u2013 See module initialization documentation.    Examples: &gt;&gt;&gt; # FloatTensor containing pretrained weights &gt;&gt;&gt; weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]]) &gt;&gt;&gt; embedding = nn.Embedding.from_pretrained(weight) &gt;&gt;&gt; # Get embeddings for index 1 &gt;&gt;&gt; input = torch.LongTensor([1]) &gt;&gt;&gt; embedding(input) tensor([[ 4.0000,  5.1000,  6.3000]])   ", "parameters": ["embeddings (Tensor) : FloatTensor containing weights for the Embedding.First dimension is being passed to Embedding as num_embeddings, second as embedding_dim.", "freeze (boolean, optional) : If True, the tensor does not get updated in the learning process.Equivalent to embedding.weight.requires_grad = False. Default: True", "padding_idx (int, optional) : See module initialization documentation.", "max_norm (float, optional) : See module initialization documentation.", "norm_type (float, optional) : See module initialization documentation. Default 2.", "scale_grad_by_freq (boolean, optional) : See module initialization documentation. Default False.", "sparse (bool, optional) : See module initialization documentation."], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Embedding", "item_type": "class", "code": "classtorch.nn.Embedding(num_embeddings:int,embedding_dim:int,padding_idx:Optional[int]=None,max_norm:Optional[float]=None,norm_type:float=2.0,scale_grad_by_freq:bool=False,sparse:bool=False,_weight:Optional[torch.Tensor]=None)", "description": "A simple lookup table that stores embeddings of a fixed dictionary and size. This module is often used to store word embeddings and retrieve them using indices. The input to the module is a list of indices, and the output is the corresponding word embeddings.  Parameters  num_embeddings (int) \u2013 size of the dictionary of embeddings embedding_dim (int) \u2013 the size of each embedding vector padding_idx (int, optional) \u2013 If given, pads the output with the embedding vector at padding_idx (initialized to zeros) whenever it encounters the index. max_norm (float, optional) \u2013 If given, each embedding vector with norm larger than max_norm is renormalized to have norm max_norm. norm_type (float, optional) \u2013 The p of the p-norm to compute for the max_norm option. Default 2. scale_grad_by_freq (boolean, optional) \u2013 If given, this will scale gradients by the inverse of frequency of the words in the mini-batch. Default False. sparse (bool, optional) \u2013 If True, gradient w.r.t. weight matrix will be a sparse tensor. See Notes for more details regarding sparse gradients.   Variables ~Embedding.weight (Tensor) \u2013 the learnable weights of the module of shape (num_embeddings, embedding_dim) initialized from N(0,1)\\mathcal{N}(0, 1)N(0,1)      Shape: Input: (\u2217)(*)(\u2217)  , LongTensor of arbitrary shape containing the indices to extract Output: (\u2217,H)(*, H)(\u2217,H)  , where * is the input shape and H=embedding_dimH=\\text{embedding\\_dim}H=embedding_dim       Note Keep in mind that only a limited number of optimizers support sparse gradients: currently it\u2019s optim.SGD (CUDA and CPU), optim.SparseAdam (CUDA and CPU) and optim.Adagrad (CPU)   Note With padding_idx set, the embedding vector at padding_idx is initialized to all zeros. However, note that this vector can be modified afterwards, e.g., using a customized initialization method, and thus changing the vector used to pad the output. The gradient for this vector from Embedding is always zero.  Examples: &gt;&gt;&gt; # an Embedding module containing 10 tensors of size 3 &gt;&gt;&gt; embedding = nn.Embedding(10, 3) &gt;&gt;&gt; # a batch of 2 samples of 4 indices each &gt;&gt;&gt; input = torch.LongTensor([[1,2,4,5],[4,3,2,9]]) &gt;&gt;&gt; embedding(input) tensor([[[-0.0251, -1.6902,  0.7172],          [-0.6431,  0.0748,  0.6969],          [ 1.4970,  1.3448, -0.9685],          [-0.3677, -2.7265, -0.1685]],          [[ 1.4970,  1.3448, -0.9685],          [ 0.4362, -0.4004,  0.9400],          [-0.6431,  0.0748,  0.6969],          [ 0.9124, -2.3616,  1.1151]]])   &gt;&gt;&gt; # example with padding_idx &gt;&gt;&gt; embedding = nn.Embedding(10, 3, padding_idx=0) &gt;&gt;&gt; input = torch.LongTensor([[0,2,0,5]]) &gt;&gt;&gt; embedding(input) tensor([[[ 0.0000,  0.0000,  0.0000],          [ 0.1535, -2.0309,  0.9315],          [ 0.0000,  0.0000,  0.0000],          [-0.1655,  0.9897,  0.0635]]])     classmethod from_pretrained(embeddings, freeze=True, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False)  Creates Embedding instance from given 2-dimensional FloatTensor.  Parameters  embeddings (Tensor) \u2013 FloatTensor containing weights for the Embedding. First dimension is being passed to Embedding as num_embeddings, second as embedding_dim. freeze (boolean, optional) \u2013 If True, the tensor does not get updated in the learning process. Equivalent to embedding.weight.requires_grad = False. Default: True padding_idx (int, optional) \u2013 See module initialization documentation. max_norm (float, optional) \u2013 See module initialization documentation. norm_type (float, optional) \u2013 See module initialization documentation. Default 2. scale_grad_by_freq (boolean, optional) \u2013 See module initialization documentation. Default False. sparse (bool, optional) \u2013 See module initialization documentation.    Examples: &gt;&gt;&gt; # FloatTensor containing pretrained weights &gt;&gt;&gt; weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]]) &gt;&gt;&gt; embedding = nn.Embedding.from_pretrained(weight) &gt;&gt;&gt; # Get embeddings for index 1 &gt;&gt;&gt; input = torch.LongTensor([1]) &gt;&gt;&gt; embedding(input) tensor([[ 4.0000,  5.1000,  6.3000]])     ", "parameters": ["num_embeddings (int) : size of the dictionary of embeddings", "embedding_dim (int) : the size of each embedding vector", "padding_idx (int, optional) : If given, pads the output with the embedding vector at padding_idx(initialized to zeros) whenever it encounters the index.", "max_norm (float, optional) : If given, each embedding vector with norm larger than max_normis renormalized to have norm max_norm.", "norm_type (float, optional) : The p of the p-norm to compute for the max_norm option. Default 2.", "scale_grad_by_freq (boolean, optional) : If given, this will scale gradients by the inverse of frequency ofthe words in the mini-batch. Default False.", "sparse (bool, optional) : If True, gradient w.r.t. weight matrix will be a sparse tensor.See Notes for more details regarding sparse gradients.", "embeddings (Tensor) : FloatTensor containing weights for the Embedding.First dimension is being passed to Embedding as num_embeddings, second as embedding_dim.", "freeze (boolean, optional) : If True, the tensor does not get updated in the learning process.Equivalent to embedding.weight.requires_grad = False. Default: True", "padding_idx (int, optional) : See module initialization documentation.", "max_norm (float, optional) : See module initialization documentation.", "norm_type (float, optional) : See module initialization documentation. Default 2.", "scale_grad_by_freq (boolean, optional) : See module initialization documentation. Default False.", "sparse (bool, optional) : See module initialization documentation."], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.AlphaDropout", "item_type": "class", "code": "classtorch.nn.AlphaDropout(p:float=0.5,inplace:bool=False)", "description": "Applies Alpha Dropout over the input. Alpha Dropout is a type of Dropout that maintains the self-normalizing property. For an input with zero mean and unit standard deviation, the output of Alpha Dropout maintains the original mean and standard deviation of the input. Alpha Dropout goes hand-in-hand with SELU activation function, which ensures that the outputs have zero mean and unit standard deviation. During training, it randomly masks some of the elements of the input tensor with probability p using samples from a bernoulli distribution. The elements to masked are randomized on every forward call, and scaled and shifted to maintain zero mean and unit standard deviation. During evaluation the module simply computes an identity function. More details can be found in the paper Self-Normalizing Neural Networks .  Parameters  p (float) \u2013 probability of an element to be dropped. Default: 0.5 inplace (bool, optional) \u2013 If set to True, will do this operation in-place     Shape: Input: (\u2217)(*)(\u2217)  . Input can be of any shape Output: (\u2217)(*)(\u2217)  . Output is of the same shape as input    Examples: &gt;&gt;&gt; m = nn.AlphaDropout(p=0.2) &gt;&gt;&gt; input = torch.randn(20, 16) &gt;&gt;&gt; output = m(input)   ", "parameters": ["p (float) : probability of an element to be dropped. Default: 0.5", "inplace (bool, optional) : If set to True, will do this operationin-place"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Dropout2d", "item_type": "class", "code": "classtorch.nn.Dropout2d(p:float=0.5,inplace:bool=False)", "description": "Randomly zero out entire channels (a channel is a 2D feature map, e.g., the jjj  -th channel of the iii  -th sample in the batched input is a 2D tensor input[i,j]\\text{input}[i, j]input[i,j]  ). Each channel will be zeroed out independently on every forward call with probability p using samples from a Bernoulli distribution. Usually the input comes from nn.Conv2d modules. As described in the paper Efficient Object Localization Using Convolutional Networks , if adjacent pixels within feature maps are strongly correlated (as is normally the case in early convolution layers) then i.i.d. dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease. In this case, nn.Dropout2d() will help promote independence between feature maps and should be used instead.  Parameters  p (float, optional) \u2013 probability of an element to be zero-ed. inplace (bool, optional) \u2013 If set to True, will do this operation in-place     Shape: Input: (N,C,H,W)(N, C, H, W)(N,C,H,W)   Output: (N,C,H,W)(N, C, H, W)(N,C,H,W)   (same shape as input)    Examples: &gt;&gt;&gt; m = nn.Dropout2d(p=0.2) &gt;&gt;&gt; input = torch.randn(20, 16, 32, 32) &gt;&gt;&gt; output = m(input)   ", "parameters": ["p (float, optional) : probability of an element to be zero-ed.", "inplace (bool, optional) : If set to True, will do this operationin-place"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Dropout", "item_type": "class", "code": "classtorch.nn.Dropout(p:float=0.5,inplace:bool=False)", "description": "During training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution. Each channel will be zeroed out independently on every forward call. This has proven to be an effective technique for regularization and preventing the co-adaptation of neurons as described in the paper Improving neural networks by preventing co-adaptation of feature detectors . Furthermore, the outputs are scaled by a factor of 11\u2212p\\frac{1}{1-p}1\u2212p1\u200b   during training. This means that during evaluation the module simply computes an identity function.  Parameters  p \u2013 probability of an element to be zeroed. Default: 0.5 inplace \u2013 If set to True, will do this operation in-place. Default: False     Shape: Input: (\u2217)(*)(\u2217)  . Input can be of any shape Output: (\u2217)(*)(\u2217)  . Output is of the same shape as input    Examples: &gt;&gt;&gt; m = nn.Dropout(p=0.2) &gt;&gt;&gt; input = torch.randn(20, 16) &gt;&gt;&gt; output = m(input)   ", "parameters": ["p : probability of an element to be zeroed. Default: 0.5", "inplace : If set to True, will do this operation in-place. Default: False"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Bilinear", "item_type": "class", "code": "classtorch.nn.Bilinear(in1_features:int,in2_features:int,out_features:int,bias:bool=True)", "description": "Applies a bilinear transformation to the incoming data: y=x1TAx2+by = x_1^T A x_2 + by=x1T\u200bAx2\u200b+b    Parameters  in1_features \u2013 size of each first input sample in2_features \u2013 size of each second input sample out_features \u2013 size of each output sample bias \u2013 If set to False, the layer will not learn an additive bias. Default: True     Shape: Input1: (N,\u2217,Hin1)(N, *, H_{in1})(N,\u2217,Hin1\u200b)   where Hin1=in1_featuresH_{in1}=\\text{in1\\_features}Hin1\u200b=in1_features   and \u2217*\u2217   means any number of additional dimensions. All but the last dimension of the inputs should be the same. Input2: (N,\u2217,Hin2)(N, *, H_{in2})(N,\u2217,Hin2\u200b)   where Hin2=in2_featuresH_{in2}=\\text{in2\\_features}Hin2\u200b=in2_features  . Output: (N,\u2217,Hout)(N, *, H_{out})(N,\u2217,Hout\u200b)   where Hout=out_featuresH_{out}=\\text{out\\_features}Hout\u200b=out_features   and all but the last dimension are the same shape as the input.     Variables  ~Bilinear.weight \u2013 the learnable weights of the module of shape (out_features,in1_features,in2_features)(\\text{out\\_features}, \\text{in1\\_features}, \\text{in2\\_features})(out_features,in1_features,in2_features)  . The values are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b)  , where k=1in1_featuresk = \\frac{1}{\\text{in1\\_features}}k=in1_features1\u200b   ~Bilinear.bias \u2013 the learnable bias of the module of shape (out_features)(\\text{out\\_features})(out_features)  . If bias is True, the values are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b)  , where k=1in1_featuresk = \\frac{1}{\\text{in1\\_features}}k=in1_features1\u200b      Examples: &gt;&gt;&gt; m = nn.Bilinear(20, 30, 40) &gt;&gt;&gt; input1 = torch.randn(128, 20) &gt;&gt;&gt; input2 = torch.randn(128, 30) &gt;&gt;&gt; output = m(input1, input2) &gt;&gt;&gt; print(output.size()) torch.Size([128, 40])   ", "parameters": ["in1_features : size of each first input sample", "in2_features : size of each second input sample", "out_features : size of each output sample", "bias : If set to False, the layer will not learn an additive bias.Default: True", "~Bilinear.weight : the learnable weights of the module of shape(out_features,in1_features,in2_features)(\\text{out\\_features}, \\text{in1\\_features}, \\text{in2\\_features})(out_features,in1_features,in2_features).The values are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b), wherek=1in1_featuresk = \\frac{1}{\\text{in1\\_features}}k=in1_features1\u200b", "~Bilinear.bias : the learnable bias of the module of shape (out_features)(\\text{out\\_features})(out_features).If bias is True, the values are initialized fromU(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b), wherek=1in1_featuresk = \\frac{1}{\\text{in1\\_features}}k=in1_features1\u200b"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Dropout3d", "item_type": "class", "code": "classtorch.nn.Dropout3d(p:float=0.5,inplace:bool=False)", "description": "Randomly zero out entire channels (a channel is a 3D feature map, e.g., the jjj  -th channel of the iii  -th sample in the batched input is a 3D tensor input[i,j]\\text{input}[i, j]input[i,j]  ). Each channel will be zeroed out independently on every forward call with probability p using samples from a Bernoulli distribution. Usually the input comes from nn.Conv3d modules. As described in the paper Efficient Object Localization Using Convolutional Networks , if adjacent pixels within feature maps are strongly correlated (as is normally the case in early convolution layers) then i.i.d. dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease. In this case, nn.Dropout3d() will help promote independence between feature maps and should be used instead.  Parameters  p (float, optional) \u2013 probability of an element to be zeroed. inplace (bool, optional) \u2013 If set to True, will do this operation in-place     Shape: Input: (N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W)   Output: (N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W)   (same shape as input)    Examples: &gt;&gt;&gt; m = nn.Dropout3d(p=0.2) &gt;&gt;&gt; input = torch.randn(20, 16, 4, 32, 32) &gt;&gt;&gt; output = m(input)   ", "parameters": ["p (float, optional) : probability of an element to be zeroed.", "inplace (bool, optional) : If set to True, will do this operationin-place"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Flatten", "item_type": "class", "code": "classtorch.nn.Flatten(start_dim:int=1,end_dim:int=-1)", "description": "Flattens a contiguous range of dims into a tensor. For use with Sequential. :param start_dim: first dim to flatten (default = 1). :param end_dim: last dim to flatten (default = -1).  Shape: Input: (N,\u2217dims)(N, *dims)(N,\u2217dims)   Output: (N,\u220f\u2217dims)(N, \\prod *dims)(N,\u220f\u2217dims)   (for the default case).   Examples::&gt;&gt;&gt; m = nn.Sequential( &gt;&gt;&gt;     nn.Conv2d(1, 32, 5, 1, 1), &gt;&gt;&gt;     nn.Flatten() &gt;&gt;&gt; )       add_module(name: str, module: Optional[Module]) \u2192 None Adds a child module to the current module. The module can be accessed as an attribute using the given name.  Parameters  name (string) \u2013 name of the child module. The child module can be accessed from this module using the given name module (Module) \u2013 child module to be added to the module.        apply(fn: Callable[Module, None]) \u2192 T Applies fn recursively to every submodule (as returned by .children()) as well as self. Typical use includes initializing the parameters of a model (see also torch.nn.init).  Parameters fn (Module -&gt; None) \u2013 function to be applied to each submodule  Returns self  Return type Module   Example: &gt;&gt;&gt; @torch.no_grad() &gt;&gt;&gt; def init_weights(m): &gt;&gt;&gt;     print(m) &gt;&gt;&gt;     if type(m) == nn.Linear: &gt;&gt;&gt;         m.weight.fill_(1.0) &gt;&gt;&gt;         print(m.weight) &gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) &gt;&gt;&gt; net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1.,  1.],         [ 1.,  1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1.,  1.],         [ 1.,  1.]]) Sequential(   (0): Linear(in_features=2, out_features=2, bias=True)   (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential(   (0): Linear(in_features=2, out_features=2, bias=True)   (1): Linear(in_features=2, out_features=2, bias=True) )       bfloat16() \u2192 T Casts all floating point parameters and buffers to bfloat16 datatype.  Returns self  Return type Module       buffers(recurse: bool = True) \u2192 Iterator[torch.Tensor] Returns an iterator over module buffers.  Parameters recurse (bool) \u2013 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.  Yields torch.Tensor \u2013 module buffer   Example: &gt;&gt;&gt; for buf in model.buffers(): &gt;&gt;&gt;     print(type(buf), buf.size()) &lt;class 'torch.Tensor'&gt; (20L,) &lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)       children() \u2192 Iterator[torch.nn.modules.module.Module] Returns an iterator over immediate children modules.  Yields Module \u2013 a child module       cpu() \u2192 T Moves all model parameters and buffers to the CPU.  Returns self  Return type Module       cuda(device: Union[int, torch.device, None] = None) \u2192 T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.  Parameters device (int, optional) \u2013 if specified, all parameters will be copied to that device  Returns self  Return type Module       double() \u2192 T Casts all floating point parameters and buffers to double datatype.  Returns self  Return type Module       eval() \u2192 T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc. This is equivalent with self.train(False).  Returns self  Return type Module       extra_repr() \u2192 str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.     float() \u2192 T Casts all floating point parameters and buffers to float datatype.  Returns self  Return type Module       half() \u2192 T Casts all floating point parameters and buffers to half datatype.  Returns self  Return type Module       load_state_dict(state_dict: Dict[str, torch.Tensor], strict: bool = True) Copies parameters and buffers from state_dict into this module and its descendants. If strict is True, then the keys of state_dict must exactly match the keys returned by this module\u2019s state_dict() function.  Parameters  state_dict (dict) \u2013 a dict containing parameters and persistent buffers. strict (bool, optional) \u2013 whether to strictly enforce that the keys in state_dict match the keys returned by this module\u2019s state_dict() function. Default: True   Returns  missing_keys is a list of str containing the missing keys unexpected_keys is a list of str containing the unexpected keys    Return type NamedTuple with missing_keys and unexpected_keys fields       modules() \u2192 Iterator[torch.nn.modules.module.Module] Returns an iterator over all modules in the network.  Yields Module \u2013 a module in the network    Note Duplicate modules are returned only once. In the following example, l will be returned only once.  Example: &gt;&gt;&gt; l = nn.Linear(2, 2) &gt;&gt;&gt; net = nn.Sequential(l, l) &gt;&gt;&gt; for idx, m in enumerate(net.modules()):         print(idx, '-&gt;', m)  0 -&gt; Sequential(   (0): Linear(in_features=2, out_features=2, bias=True)   (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -&gt; Linear(in_features=2, out_features=2, bias=True)       named_buffers(prefix: str = '', recurse: bool = True) \u2192 Iterator[Tuple[str, torch.Tensor]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.  Parameters  prefix (str) \u2013 prefix to prepend to all buffer names. recurse (bool) \u2013 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.   Yields (string, torch.Tensor) \u2013 Tuple containing the name and buffer   Example: &gt;&gt;&gt; for name, buf in self.named_buffers(): &gt;&gt;&gt;    if name in ['running_var']: &gt;&gt;&gt;        print(buf.size())       named_children() \u2192 Iterator[Tuple[str, torch.nn.modules.module.Module]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.  Yields (string, Module) \u2013 Tuple containing a name and child module   Example: &gt;&gt;&gt; for name, module in model.named_children(): &gt;&gt;&gt;     if name in ['conv4', 'conv5']: &gt;&gt;&gt;         print(module)       named_modules(memo: Optional[Set[Module]] = None, prefix: str = '') Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.  Yields (string, Module) \u2013 Tuple of name and module    Note Duplicate modules are returned only once. In the following example, l will be returned only once.  Example: &gt;&gt;&gt; l = nn.Linear(2, 2) &gt;&gt;&gt; net = nn.Sequential(l, l) &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):         print(idx, '-&gt;', m)  0 -&gt; ('', Sequential(   (0): Linear(in_features=2, out_features=2, bias=True)   (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -&gt; ('0', Linear(in_features=2, out_features=2, bias=True))       named_parameters(prefix: str = '', recurse: bool = True) \u2192 Iterator[Tuple[str, torch.Tensor]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.  Parameters  prefix (str) \u2013 prefix to prepend to all parameter names. recurse (bool) \u2013 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.   Yields (string, Parameter) \u2013 Tuple containing the name and parameter   Example: &gt;&gt;&gt; for name, param in self.named_parameters(): &gt;&gt;&gt;    if name in ['bias']: &gt;&gt;&gt;        print(param.size())       parameters(recurse: bool = True) \u2192 Iterator[torch.nn.parameter.Parameter] Returns an iterator over module parameters. This is typically passed to an optimizer.  Parameters recurse (bool) \u2013 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.  Yields Parameter \u2013 module parameter   Example: &gt;&gt;&gt; for param in model.parameters(): &gt;&gt;&gt;     print(type(param), param.size()) &lt;class 'torch.Tensor'&gt; (20L,) &lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)       register_backward_hook(hook: Callable[[Module, Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[None, torch.Tensor]]) \u2192 torch.utils.hooks.RemovableHandle Registers a backward hook on the module.  Warning The current implementation will not have the presented behavior for complex Module that perform many operations. In some failure cases, grad_input and grad_output will only contain the gradients for a subset of the inputs and outputs. For such Module, you should use torch.Tensor.register_hook() directly on a specific input or output to get the required gradients.  The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature: hook(module, grad_input, grad_output) -&gt; Tensor or None   The grad_input and grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of grad_input in subsequent computations. grad_input will only correspond to the inputs given as positional arguments.  Returns a handle that can be used to remove the added hook by calling handle.remove()  Return type torch.utils.hooks.RemovableHandle       register_buffer(name: str, tensor: Optional[torch.Tensor], persistent: bool = True) \u2192 None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm\u2019s running_mean is not a parameter, but is part of the module\u2019s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting persistent to False. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module\u2019s state_dict. Buffers can be accessed as attributes using given names.  Parameters  name (string) \u2013 name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor) \u2013 buffer to be registered. persistent (bool) \u2013 whether the buffer is part of this module\u2019s state_dict.    Example: &gt;&gt;&gt; self.register_buffer('running_mean', torch.zeros(num_features))       register_forward_hook(hook: Callable[..., None]) \u2192 torch.utils.hooks.RemovableHandle Registers a forward hook on the module. The hook will be called every time after forward() has computed an output. It should have the following signature: hook(module, input, output) -&gt; None or modified output   The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after forward() is called.  Returns a handle that can be used to remove the added hook by calling handle.remove()  Return type torch.utils.hooks.RemovableHandle       register_forward_pre_hook(hook: Callable[..., None]) \u2192 torch.utils.hooks.RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before forward() is invoked. It should have the following signature: hook(module, input) -&gt; None or modified input   The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple).  Returns a handle that can be used to remove the added hook by calling handle.remove()  Return type torch.utils.hooks.RemovableHandle       register_parameter(name: str, param: Optional[torch.nn.parameter.Parameter]) \u2192 None Adds a parameter to the module. The parameter can be accessed as an attribute using given name.  Parameters  name (string) \u2013 name of the parameter. The parameter can be accessed from this module using the given name param (Parameter) \u2013 parameter to be added to the module.        requires_grad_(requires_grad: bool = True) \u2192 T Change if autograd should record operations on parameters in this module. This method sets the parameters\u2019 requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).  Parameters requires_grad (bool) \u2013 whether autograd should record operations on parameters in this module. Default: True.  Returns self  Return type Module       state_dict(destination=None, prefix='', keep_vars=False) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names.  Returns a dictionary containing a whole state of the module  Return type dict   Example: &gt;&gt;&gt; module.state_dict().keys() ['bias', 'weight']       to(*args, **kwargs) Moves and/or casts the parameters and buffers. This can be called as   to(device=None, dtype=None, non_blocking=False)     to(dtype, non_blocking=False)     to(tensor, non_blocking=False)     to(memory_format=torch.channels_last)   Its signature is similar to torch.Tensor.to(), but only accepts floating point desired dtype s. In addition, this method will only cast the floating point parameters and buffers to dtype (if given). The integral parameters and buffers will be moved device, if that is given, but with dtypes unchanged. When non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples.  Note This method modifies the module in-place.   Parameters  device (torch.device) \u2013 the desired device of the parameters and buffers in this module dtype (torch.dtype) \u2013 the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor) \u2013 Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (torch.memory_format) \u2013 the desired memory format for 4D parameters and buffers in this module (keyword only argument)   Returns self  Return type Module   Example: &gt;&gt;&gt; linear = nn.Linear(2, 2) &gt;&gt;&gt; linear.weight Parameter containing: tensor([[ 0.1913, -0.3420],         [-0.5113, -0.2325]]) &gt;&gt;&gt; linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) &gt;&gt;&gt; linear.weight Parameter containing: tensor([[ 0.1913, -0.3420],         [-0.5113, -0.2325]], dtype=torch.float64) &gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\") &gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) &gt;&gt;&gt; linear.weight Parameter containing: tensor([[ 0.1914, -0.3420],         [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') &gt;&gt;&gt; cpu = torch.device(\"cpu\") &gt;&gt;&gt; linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) &gt;&gt;&gt; linear.weight Parameter containing: tensor([[ 0.1914, -0.3420],         [-0.5112, -0.2324]], dtype=torch.float16)       train(mode: bool = True) \u2192 T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc.  Parameters mode (bool) \u2013 whether to set training mode (True) or evaluation mode (False). Default: True.  Returns self  Return type Module       type(dst_type: Union[torch.dtype, str]) \u2192 T Casts all parameters and buffers to dst_type.  Parameters dst_type (type or string) \u2013 the desired type  Returns self  Return type Module       zero_grad() \u2192 None Sets gradients of all model parameters to zero.   ", "parameters": ["name (string) : name of the child module. The child module can beaccessed from this module using the given name", "module (Module) : child module to be added to the module.", "state_dict (dict) : a dict containing parameters andpersistent buffers.", "strict (bool, optional) : whether to strictly enforce that the keysin state_dict match the keys returned by this module\u2019sstate_dict() function. Default: True", "missing_keys is a list of str containing the missing keys", "unexpected_keys is a list of str containing the unexpected keys", "prefix (str) : prefix to prepend to all buffer names.", "recurse (bool) : if True, then yields buffers of this moduleand all submodules. Otherwise, yields only buffers thatare direct members of this module.", "prefix (str) : prefix to prepend to all parameter names.", "recurse (bool) : if True, then yields parameters of this moduleand all submodules. Otherwise, yields only parameters thatare direct members of this module.", "name (string) : name of the buffer. The buffer can be accessedfrom this module using the given name", "tensor (Tensor) : buffer to be registered.", "persistent (bool) : whether the buffer is part of this module\u2019sstate_dict.", "name (string) : name of the parameter. The parameter can be accessedfrom this module using the given name", "param (Parameter) : parameter to be added to the module.", "device (torch.device) : the desired device of the parametersand buffers in this module", "dtype (torch.dtype) : the desired floating point type ofthe floating point parameters and buffers in this module", "tensor (torch.Tensor) : Tensor whose dtype and device are the desireddtype and device for all parameters and buffers in this module", "memory_format (torch.memory_format) : the desired memoryformat for 4D parameters and buffers in this module (keywordonly argument)"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": "\nInput: (N,\u2217dims)(N, *dims)(N,\u2217dims)\n\n\nOutput: (N,\u220f\u2217dims)(N, \\prod *dims)(N,\u220f\u2217dims)\n\n (for the default case).\n\n"},
{"library": "torch", "item_id": "torch.nn.EmbeddingBag", "item_type": "class", "code": "classtorch.nn.EmbeddingBag(num_embeddings:int,embedding_dim:int,max_norm:Optional[float]=None,norm_type:float=2.0,scale_grad_by_freq:bool=False,mode:str='mean',sparse:bool=False,_weight:Optional[torch.Tensor]=None,include_last_offset:bool=False)", "description": "Computes sums or means of \u2018bags\u2019 of embeddings, without instantiating the intermediate embeddings. For bags of constant length and no per_sample_weights, this class   with mode=\"sum\" is equivalent to Embedding followed by torch.sum(dim=0), with mode=\"mean\" is equivalent to Embedding followed by torch.mean(dim=0), with mode=\"max\" is equivalent to Embedding followed by torch.max(dim=0).   However, EmbeddingBag is much more time and memory efficient than using a chain of these operations. EmbeddingBag also supports per-sample weights as an argument to the forward pass. This scales the output of the Embedding before performing a weighted reduction as specified by mode. If per_sample_weights` is passed, the only supported mode is \"sum\", which computes a weighted sum according to per_sample_weights.  Parameters  num_embeddings (int) \u2013 size of the dictionary of embeddings embedding_dim (int) \u2013 the size of each embedding vector max_norm (float, optional) \u2013 If given, each embedding vector with norm larger than max_norm is renormalized to have norm max_norm. norm_type (float, optional) \u2013 The p of the p-norm to compute for the max_norm option. Default 2. scale_grad_by_freq (boolean, optional) \u2013 if given, this will scale gradients by the inverse of frequency of the words in the mini-batch. Default False. Note: this option is not supported when mode=\"max\". mode (string, optional) \u2013 \"sum\", \"mean\" or \"max\". Specifies the way to reduce the bag. \"sum\" computes the weighted sum, taking per_sample_weights into consideration. \"mean\" computes the average of the values in the bag, \"max\" computes the max value over each bag. Default: \"mean\" sparse (bool, optional) \u2013 if True, gradient w.r.t. weight matrix will be a sparse tensor. See Notes for more details regarding sparse gradients. Note: this option is not supported when mode=\"max\". include_last_offset (bool, optional) \u2013 if True, offsets has one additional element, where the last element is equivalent to the size of indices. This matches the CSR format. Note: this option is currently only supported when mode=\"sum\".   Variables ~EmbeddingBag.weight (Tensor) \u2013 the learnable weights of the module of shape (num_embeddings, embedding_dim) initialized from N(0,1)\\mathcal{N}(0, 1)N(0,1)  .    Inputs: input (LongTensor), offsets (LongTensor, optional), andper_index_weights (Tensor, optional)  If input is 2D of shape (B, N), it will be treated as B bags (sequences) each of fixed length N, and this will return B values aggregated in a way depending on the mode. offsets is ignored and required to be None in this case.  If input is 1D of shape (N), it will be treated as a concatenation of multiple bags (sequences). offsets is required to be a 1D tensor containing the starting index positions of each bag in input. Therefore, for offsets of shape (B), input will be viewed as having B bags. Empty bags (i.e., having 0-length) will have returned vectors filled by zeros.    per_sample_weights (Tensor, optional): a tensor of float / double weights, or Noneto indicate all weights should be taken to be 1. If specified, per_sample_weights must have exactly the same shape as input and is treated as having the same offsets, if those are not None. Only supported for mode='sum'.     Output shape: (B, embedding_dim) Examples: &gt;&gt;&gt; # an Embedding module containing 10 tensors of size 3 &gt;&gt;&gt; embedding_sum = nn.EmbeddingBag(10, 3, mode='sum') &gt;&gt;&gt; # a batch of 2 samples of 4 indices each &gt;&gt;&gt; input = torch.LongTensor([1,2,4,5,4,3,2,9]) &gt;&gt;&gt; offsets = torch.LongTensor([0,4]) &gt;&gt;&gt; embedding_sum(input, offsets) tensor([[-0.8861, -5.4350, -0.0523],         [ 1.1306, -2.5798, -1.0044]])     classmethod from_pretrained(embeddings: torch.Tensor, freeze: bool = True, max_norm: Optional[float] = None, norm_type: float = 2.0, scale_grad_by_freq: bool = False, mode: str = 'mean', sparse: bool = False, include_last_offset: bool = False) \u2192 torch.nn.modules.sparse.EmbeddingBag  Creates EmbeddingBag instance from given 2-dimensional FloatTensor.  Parameters  embeddings (Tensor) \u2013 FloatTensor containing weights for the EmbeddingBag. First dimension is being passed to EmbeddingBag as \u2018num_embeddings\u2019, second as \u2018embedding_dim\u2019. freeze (boolean, optional) \u2013 If True, the tensor does not get updated in the learning process. Equivalent to embeddingbag.weight.requires_grad = False. Default: True max_norm (float, optional) \u2013 See module initialization documentation. Default: None norm_type (float, optional) \u2013 See module initialization documentation. Default 2. scale_grad_by_freq (boolean, optional) \u2013 See module initialization documentation. Default False. mode (string, optional) \u2013 See module initialization documentation. Default: \"mean\" sparse (bool, optional) \u2013 See module initialization documentation. Default: False. include_last_offset (bool, optional) \u2013 See module initialization documentation. Default: False.    Examples: &gt;&gt;&gt; # FloatTensor containing pretrained weights &gt;&gt;&gt; weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]]) &gt;&gt;&gt; embeddingbag = nn.EmbeddingBag.from_pretrained(weight) &gt;&gt;&gt; # Get embeddings for index 1 &gt;&gt;&gt; input = torch.LongTensor([[1, 0]]) &gt;&gt;&gt; embeddingbag(input) tensor([[ 2.5000,  3.7000,  4.6500]])     ", "parameters": ["num_embeddings (int) : size of the dictionary of embeddings", "embedding_dim (int) : the size of each embedding vector", "max_norm (float, optional) : If given, each embedding vector with norm larger than max_normis renormalized to have norm max_norm.", "norm_type (float, optional) : The p of the p-norm to compute for the max_norm option. Default 2.", "scale_grad_by_freq (boolean, optional) : if given, this will scale gradients by the inverse of frequency ofthe words in the mini-batch. Default False.Note: this option is not supported when mode=\"max\".", "mode (string, optional) : \"sum\", \"mean\" or \"max\". Specifies the way to reduce the bag.\"sum\" computes the weighted sum, taking per_sample_weightsinto consideration. \"mean\" computes the average of the valuesin the bag, \"max\" computes the max value over each bag.Default: \"mean\"", "sparse (bool, optional) : if True, gradient w.r.t. weight matrix will be a sparse tensor. SeeNotes for more details regarding sparse gradients. Note: this option is notsupported when mode=\"max\".", "include_last_offset (bool, optional) : if True, offsets has one additional element, where the last elementis equivalent to the size of indices. This matches the CSR format. Note:this option is currently only supported when mode=\"sum\".", "embeddings (Tensor) : FloatTensor containing weights for the EmbeddingBag.First dimension is being passed to EmbeddingBag as \u2018num_embeddings\u2019, second as \u2018embedding_dim\u2019.", "freeze (boolean, optional) : If True, the tensor does not get updated in the learning process.Equivalent to embeddingbag.weight.requires_grad = False. Default: True", "max_norm (float, optional) : See module initialization documentation. Default: None", "norm_type (float, optional) : See module initialization documentation. Default 2.", "scale_grad_by_freq (boolean, optional) : See module initialization documentation. Default False.", "mode (string, optional) : See module initialization documentation. Default: \"mean\"", "sparse (bool, optional) : See module initialization documentation. Default: False.", "include_last_offset (bool, optional) : See module initialization documentation. Default: False."], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.LSTMCell", "item_type": "class", "code": "classtorch.nn.LSTMCell(input_size:int,hidden_size:int,bias:bool=True)", "description": "A long short-term memory (LSTM) cell.  i=\u03c3(Wiix+bii+Whih+bhi)f=\u03c3(Wifx+bif+Whfh+bhf)g=tanh\u2061(Wigx+big+Whgh+bhg)o=\u03c3(Wiox+bio+Whoh+bho)c\u2032=f\u2217c+i\u2217gh\u2032=o\u2217tanh\u2061(c\u2032)\\begin{array}{ll} i = \\sigma(W_{ii} x + b_{ii} + W_{hi} h + b_{hi}) \\\\ f = \\sigma(W_{if} x + b_{if} + W_{hf} h + b_{hf}) \\\\ g = \\tanh(W_{ig} x + b_{ig} + W_{hg} h + b_{hg}) \\\\ o = \\sigma(W_{io} x + b_{io} + W_{ho} h + b_{ho}) \\\\ c' = f * c + i * g \\\\ h' = o * \\tanh(c') \\\\ \\end{array}i=\u03c3(Wii\u200bx+bii\u200b+Whi\u200bh+bhi\u200b)f=\u03c3(Wif\u200bx+bif\u200b+Whf\u200bh+bhf\u200b)g=tanh(Wig\u200bx+big\u200b+Whg\u200bh+bhg\u200b)o=\u03c3(Wio\u200bx+bio\u200b+Who\u200bh+bho\u200b)c\u2032=f\u2217c+i\u2217gh\u2032=o\u2217tanh(c\u2032)\u200b  where \u03c3\\sigma\u03c3   is the sigmoid function, and \u2217*\u2217   is the Hadamard product.  Parameters  input_size \u2013 The number of expected features in the input x hidden_size \u2013 The number of features in the hidden state h bias \u2013 If False, then the layer does not use bias weights b_ih and b_hh. Default: True     Inputs: input, (h_0, c_0) input of shape (batch, input_size): tensor containing input features h_0 of shape (batch, hidden_size): tensor containing the initial hidden state for each element in the batch. c_0 of shape (batch, hidden_size): tensor containing the initial cell state for each element in the batch. If (h_0, c_0) is not provided, both h_0 and c_0 default to zero.    Outputs: (h_1, c_1) h_1 of shape (batch, hidden_size): tensor containing the next hidden state for each element in the batch c_1 of shape (batch, hidden_size): tensor containing the next cell state for each element in the batch     Variables  ~LSTMCell.weight_ih \u2013 the learnable input-hidden weights, of shape (4*hidden_size, input_size) ~LSTMCell.weight_hh \u2013 the learnable hidden-hidden weights, of shape (4*hidden_size, hidden_size) ~LSTMCell.bias_ih \u2013 the learnable input-hidden bias, of shape (4*hidden_size) ~LSTMCell.bias_hh \u2013 the learnable hidden-hidden bias, of shape (4*hidden_size)     Note All the weights and biases are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b)   where k=1hidden_sizek = \\frac{1}{\\text{hidden\\_size}}k=hidden_size1\u200b    Examples: &gt;&gt;&gt; rnn = nn.LSTMCell(10, 20) &gt;&gt;&gt; input = torch.randn(6, 3, 10) &gt;&gt;&gt; hx = torch.randn(3, 20) &gt;&gt;&gt; cx = torch.randn(3, 20) &gt;&gt;&gt; output = [] &gt;&gt;&gt; for i in range(6):         hx, cx = rnn(input[i], (hx, cx))         output.append(hx)   ", "parameters": ["input_size : The number of expected features in the input x", "hidden_size : The number of features in the hidden state h", "bias : If False, then the layer does not use bias weights b_ih andb_hh. Default: True", "~LSTMCell.weight_ih : the learnable input-hidden weights, of shape(4*hidden_size, input_size)", "~LSTMCell.weight_hh : the learnable hidden-hidden weights, of shape(4*hidden_size, hidden_size)", "~LSTMCell.bias_ih : the learnable input-hidden bias, of shape (4*hidden_size)", "~LSTMCell.bias_hh : the learnable hidden-hidden bias, of shape (4*hidden_size)"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.GRUCell", "item_type": "class", "code": "classtorch.nn.GRUCell(input_size:int,hidden_size:int,bias:bool=True)", "description": "A gated recurrent unit (GRU) cell  r=\u03c3(Wirx+bir+Whrh+bhr)z=\u03c3(Wizx+biz+Whzh+bhz)n=tanh\u2061(Winx+bin+r\u2217(Whnh+bhn))h\u2032=(1\u2212z)\u2217n+z\u2217h\\begin{array}{ll} r = \\sigma(W_{ir} x + b_{ir} + W_{hr} h + b_{hr}) \\\\ z = \\sigma(W_{iz} x + b_{iz} + W_{hz} h + b_{hz}) \\\\ n = \\tanh(W_{in} x + b_{in} + r * (W_{hn} h + b_{hn})) \\\\ h' = (1 - z) * n + z * h \\end{array}r=\u03c3(Wir\u200bx+bir\u200b+Whr\u200bh+bhr\u200b)z=\u03c3(Wiz\u200bx+biz\u200b+Whz\u200bh+bhz\u200b)n=tanh(Win\u200bx+bin\u200b+r\u2217(Whn\u200bh+bhn\u200b))h\u2032=(1\u2212z)\u2217n+z\u2217h\u200b  where \u03c3\\sigma\u03c3   is the sigmoid function, and \u2217*\u2217   is the Hadamard product.  Parameters  input_size \u2013 The number of expected features in the input x hidden_size \u2013 The number of features in the hidden state h bias \u2013 If False, then the layer does not use bias weights b_ih and b_hh. Default: True     Inputs: input, hidden input of shape (batch, input_size): tensor containing input features hidden of shape (batch, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided.   Outputs: h\u2019 h\u2019 of shape (batch, hidden_size): tensor containing the next hidden state for each element in the batch   Shape: Input1: (N,Hin)(N, H_{in})(N,Hin\u200b)   tensor containing input features where HinH_{in}Hin\u200b   = input_size Input2: (N,Hout)(N, H_{out})(N,Hout\u200b)   tensor containing the initial hidden state for each element in the batch where HoutH_{out}Hout\u200b   = hidden_size Defaults to zero if not provided. Output: (N,Hout)(N, H_{out})(N,Hout\u200b)   tensor containing the next hidden state for each element in the batch     Variables  ~GRUCell.weight_ih \u2013 the learnable input-hidden weights, of shape (3*hidden_size, input_size) ~GRUCell.weight_hh \u2013 the learnable hidden-hidden weights, of shape (3*hidden_size, hidden_size) ~GRUCell.bias_ih \u2013 the learnable input-hidden bias, of shape (3*hidden_size) ~GRUCell.bias_hh \u2013 the learnable hidden-hidden bias, of shape (3*hidden_size)     Note All the weights and biases are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b)   where k=1hidden_sizek = \\frac{1}{\\text{hidden\\_size}}k=hidden_size1\u200b    Examples: &gt;&gt;&gt; rnn = nn.GRUCell(10, 20) &gt;&gt;&gt; input = torch.randn(6, 3, 10) &gt;&gt;&gt; hx = torch.randn(3, 20) &gt;&gt;&gt; output = [] &gt;&gt;&gt; for i in range(6):         hx = rnn(input[i], hx)         output.append(hx)   ", "parameters": ["input_size : The number of expected features in the input x", "hidden_size : The number of features in the hidden state h", "bias : If False, then the layer does not use bias weights b_ih andb_hh. Default: True", "~GRUCell.weight_ih : the learnable input-hidden weights, of shape(3*hidden_size, input_size)", "~GRUCell.weight_hh : the learnable hidden-hidden weights, of shape(3*hidden_size, hidden_size)", "~GRUCell.bias_ih : the learnable input-hidden bias, of shape (3*hidden_size)", "~GRUCell.bias_hh : the learnable hidden-hidden bias, of shape (3*hidden_size)"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.TransformerEncoderLayer.forward", "item_type": "method", "code": "forward(src:torch.Tensor,src_mask:Optional[torch.Tensor]=None,src_key_padding_mask:Optional[torch.Tensor]=None)\u2192torch.Tensor", "description": "Pass the input through the encoder layer.  Parameters  src \u2013 the sequence to the encoder layer (required). src_mask \u2013 the mask for the src sequence (optional). src_key_padding_mask \u2013 the mask for the src keys per batch (optional).     Shape:see the docs in Transformer class.   ", "parameters": ["src : the sequence to the encoder layer (required).", "src_mask : the mask for the src sequence (optional).", "src_key_padding_mask : the mask for the src keys per batch (optional)."], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.TransformerEncoderLayer", "item_type": "class", "code": "classtorch.nn.TransformerEncoderLayer(d_model,nhead,dim_feedforward=2048,dropout=0.1,activation='relu')", "description": "TransformerEncoderLayer is made up of self-attn and feedforward network. This standard encoder layer is based on the paper \u201cAttention Is All You Need\u201d. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users may modify or implement in a different way during application.  Parameters  d_model \u2013 the number of expected features in the input (required). nhead \u2013 the number of heads in the multiheadattention models (required). dim_feedforward \u2013 the dimension of the feedforward network model (default=2048). dropout \u2013 the dropout value (default=0.1). activation \u2013 the activation function of intermediate layer, relu or gelu (default=relu).     Examples::&gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8) &gt;&gt;&gt; src = torch.rand(10, 32, 512) &gt;&gt;&gt; out = encoder_layer(src)       forward(src: torch.Tensor, src_mask: Optional[torch.Tensor] = None, src_key_padding_mask: Optional[torch.Tensor] = None) \u2192 torch.Tensor  Pass the input through the encoder layer.  Parameters  src \u2013 the sequence to the encoder layer (required). src_mask \u2013 the mask for the src sequence (optional). src_key_padding_mask \u2013 the mask for the src keys per batch (optional).     Shape:see the docs in Transformer class.     ", "parameters": ["d_model : the number of expected features in the input (required).", "nhead : the number of heads in the multiheadattention models (required).", "dim_feedforward : the dimension of the feedforward network model (default=2048).", "dropout : the dropout value (default=0.1).", "activation : the activation function of intermediate layer, relu or gelu (default=relu).", "src : the sequence to the encoder layer (required).", "src_mask : the mask for the src sequence (optional).", "src_key_padding_mask : the mask for the src keys per batch (optional)."], "returns": [], "example": "NA", "shape": "&gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n&gt;&gt;&gt; src = torch.rand(10, 32, 512)\n&gt;&gt;&gt; out = encoder_layer(src)\n\n\n"},
{"library": "torch", "item_id": "torch.nn.Transformer.forward", "item_type": "method", "code": "forward(src:torch.Tensor,tgt:torch.Tensor,src_mask:Optional[torch.Tensor]=None,tgt_mask:Optional[torch.Tensor]=None,memory_mask:Optional[torch.Tensor]=None,src_key_padding_mask:Optional[torch.Tensor]=None,tgt_key_padding_mask:Optional[torch.Tensor]=None,memory_key_padding_mask:Optional[torch.Tensor]=None)\u2192torch.Tensor", "description": "Take in and process masked source/target sequences.  Parameters  src \u2013 the sequence to the encoder (required). tgt \u2013 the sequence to the decoder (required). src_mask \u2013 the additive mask for the src sequence (optional). tgt_mask \u2013 the additive mask for the tgt sequence (optional). memory_mask \u2013 the additive mask for the encoder output (optional). src_key_padding_mask \u2013 the ByteTensor mask for src keys per batch (optional). tgt_key_padding_mask \u2013 the ByteTensor mask for tgt keys per batch (optional). memory_key_padding_mask \u2013 the ByteTensor mask for memory keys per batch (optional).     Shape: src: (S,N,E)(S, N, E)(S,N,E)  . tgt: (T,N,E)(T, N, E)(T,N,E)  . src_mask: (S,S)(S, S)(S,S)  . tgt_mask: (T,T)(T, T)(T,T)  . memory_mask: (T,S)(T, S)(T,S)  . src_key_padding_mask: (N,S)(N, S)(N,S)  . tgt_key_padding_mask: (N,T)(N, T)(N,T)  . memory_key_padding_mask: (N,S)(N, S)(N,S)  .  Note: [src/tgt/memory]_mask ensures that position i is allowed to attend the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend while the zero positions will be unchanged. If a BoolTensor is provided, positions with True are not allowed to attend while False values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight. [src/tgt/memory]_key_padding_mask provides specified elements in the key to be ignored by the attention. If a ByteTensor is provided, the non-zero positions will be ignored while the zero positions will be unchanged. If a BoolTensor is provided, the positions with the value of True will be ignored while the position with the value of False will be unchanged.  output: (T,N,E)(T, N, E)(T,N,E)  .  Note: Due to the multi-head attention architecture in the transformer model, the output sequence length of a transformer is same as the input sequence (i.e. target) length of the decode. where S is the source sequence length, T is the target sequence length, N is the batch size, E is the feature number   Examples &gt;&gt;&gt; output = transformer_model(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)   ", "parameters": ["src : the sequence to the encoder (required).", "tgt : the sequence to the decoder (required).", "src_mask : the additive mask for the src sequence (optional).", "tgt_mask : the additive mask for the tgt sequence (optional).", "memory_mask : the additive mask for the encoder output (optional).", "src_key_padding_mask : the ByteTensor mask for src keys per batch (optional).", "tgt_key_padding_mask : the ByteTensor mask for tgt keys per batch (optional).", "memory_key_padding_mask : the ByteTensor mask for memory keys per batch (optional)."], "returns": [], "example": "NA", "shape": "\nsrc: (S,N,E)(S, N, E)(S,N,E)\n\n.\ntgt: (T,N,E)(T, N, E)(T,N,E)\n\n.\nsrc_mask: (S,S)(S, S)(S,S)\n\n.\ntgt_mask: (T,T)(T, T)(T,T)\n\n.\nmemory_mask: (T,S)(T, S)(T,S)\n\n.\nsrc_key_padding_mask: (N,S)(N, S)(N,S)\n\n.\ntgt_key_padding_mask: (N,T)(N, T)(N,T)\n\n.\nmemory_key_padding_mask: (N,S)(N, S)(N,S)\n\n.\n\nNote: [src/tgt/memory]_mask ensures that position i is allowed to attend the unmasked\npositions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\nwhile the zero positions will be unchanged. If a BoolTensor is provided, positions with True\nare not allowed to attend while False values will be unchanged. If a FloatTensor\nis provided, it will be added to the attention weight.\n[src/tgt/memory]_key_padding_mask provides specified elements in the key to be ignored by\nthe attention. If a ByteTensor is provided, the non-zero positions will be ignored while the zero\npositions will be unchanged. If a BoolTensor is provided, the positions with the\nvalue of True will be ignored while the position with the value of False will be unchanged.\n\noutput: (T,N,E)(T, N, E)(T,N,E)\n\n.\n\nNote: Due to the multi-head attention architecture in the transformer model,\nthe output sequence length of a transformer is same as the input sequence\n(i.e. target) length of the decode.\nwhere S is the source sequence length, T is the target sequence length, N is the\nbatch size, E is the feature number\n"},
{"library": "torch", "item_id": "torch.nn.TransformerDecoder.forward", "item_type": "method", "code": "forward(tgt:torch.Tensor,memory:torch.Tensor,tgt_mask:Optional[torch.Tensor]=None,memory_mask:Optional[torch.Tensor]=None,tgt_key_padding_mask:Optional[torch.Tensor]=None,memory_key_padding_mask:Optional[torch.Tensor]=None)\u2192torch.Tensor", "description": "Pass the inputs (and mask) through the decoder layer in turn.  Parameters  tgt \u2013 the sequence to the decoder (required). memory \u2013 the sequence from the last layer of the encoder (required). tgt_mask \u2013 the mask for the tgt sequence (optional). memory_mask \u2013 the mask for the memory sequence (optional). tgt_key_padding_mask \u2013 the mask for the tgt keys per batch (optional). memory_key_padding_mask \u2013 the mask for the memory keys per batch (optional).     Shape:see the docs in Transformer class.   ", "parameters": ["tgt : the sequence to the decoder (required).", "memory : the sequence from the last layer of the encoder (required).", "tgt_mask : the mask for the tgt sequence (optional).", "memory_mask : the mask for the memory sequence (optional).", "tgt_key_padding_mask : the mask for the tgt keys per batch (optional).", "memory_key_padding_mask : the mask for the memory keys per batch (optional)."], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.TransformerDecoder", "item_type": "class", "code": "classtorch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)", "description": "TransformerDecoder is a stack of N decoder layers  Parameters  decoder_layer \u2013 an instance of the TransformerDecoderLayer() class (required). num_layers \u2013 the number of sub-decoder-layers in the decoder (required). norm \u2013 the layer normalization component (optional).     Examples::&gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8) &gt;&gt;&gt; transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=6) &gt;&gt;&gt; memory = torch.rand(10, 32, 512) &gt;&gt;&gt; tgt = torch.rand(20, 32, 512) &gt;&gt;&gt; out = transformer_decoder(tgt, memory)       forward(tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: Optional[torch.Tensor] = None, memory_mask: Optional[torch.Tensor] = None, tgt_key_padding_mask: Optional[torch.Tensor] = None, memory_key_padding_mask: Optional[torch.Tensor] = None) \u2192 torch.Tensor  Pass the inputs (and mask) through the decoder layer in turn.  Parameters  tgt \u2013 the sequence to the decoder (required). memory \u2013 the sequence from the last layer of the encoder (required). tgt_mask \u2013 the mask for the tgt sequence (optional). memory_mask \u2013 the mask for the memory sequence (optional). tgt_key_padding_mask \u2013 the mask for the tgt keys per batch (optional). memory_key_padding_mask \u2013 the mask for the memory keys per batch (optional).     Shape:see the docs in Transformer class.     ", "parameters": ["decoder_layer : an instance of the TransformerDecoderLayer() class (required).", "num_layers : the number of sub-decoder-layers in the decoder (required).", "norm : the layer normalization component (optional).", "tgt : the sequence to the decoder (required).", "memory : the sequence from the last layer of the encoder (required).", "tgt_mask : the mask for the tgt sequence (optional).", "memory_mask : the mask for the memory sequence (optional).", "tgt_key_padding_mask : the mask for the tgt keys per batch (optional).", "memory_key_padding_mask : the mask for the memory keys per batch (optional)."], "returns": [], "example": "NA", "shape": "&gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n&gt;&gt;&gt; transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n&gt;&gt;&gt; memory = torch.rand(10, 32, 512)\n&gt;&gt;&gt; tgt = torch.rand(20, 32, 512)\n&gt;&gt;&gt; out = transformer_decoder(tgt, memory)\n\n\n"},
{"library": "torch", "item_id": "torch.nn.TransformerEncoder.forward", "item_type": "method", "code": "forward(src:torch.Tensor,mask:Optional[torch.Tensor]=None,src_key_padding_mask:Optional[torch.Tensor]=None)\u2192torch.Tensor", "description": "Pass the input through the encoder layers in turn.  Parameters  src \u2013 the sequence to the encoder (required). mask \u2013 the mask for the src sequence (optional). src_key_padding_mask \u2013 the mask for the src keys per batch (optional).     Shape:see the docs in Transformer class.   ", "parameters": ["src : the sequence to the encoder (required).", "mask : the mask for the src sequence (optional).", "src_key_padding_mask : the mask for the src keys per batch (optional)."], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.TransformerEncoder", "item_type": "class", "code": "classtorch.nn.TransformerEncoder(encoder_layer,num_layers,norm=None)", "description": "TransformerEncoder is a stack of N encoder layers  Parameters  encoder_layer \u2013 an instance of the TransformerEncoderLayer() class (required). num_layers \u2013 the number of sub-encoder-layers in the encoder (required). norm \u2013 the layer normalization component (optional).     Examples::&gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8) &gt;&gt;&gt; transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6) &gt;&gt;&gt; src = torch.rand(10, 32, 512) &gt;&gt;&gt; out = transformer_encoder(src)       forward(src: torch.Tensor, mask: Optional[torch.Tensor] = None, src_key_padding_mask: Optional[torch.Tensor] = None) \u2192 torch.Tensor  Pass the input through the encoder layers in turn.  Parameters  src \u2013 the sequence to the encoder (required). mask \u2013 the mask for the src sequence (optional). src_key_padding_mask \u2013 the mask for the src keys per batch (optional).     Shape:see the docs in Transformer class.     ", "parameters": ["encoder_layer : an instance of the TransformerEncoderLayer() class (required).", "num_layers : the number of sub-encoder-layers in the encoder (required).", "norm : the layer normalization component (optional).", "src : the sequence to the encoder (required).", "mask : the mask for the src sequence (optional).", "src_key_padding_mask : the mask for the src keys per batch (optional)."], "returns": [], "example": "NA", "shape": "&gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n&gt;&gt;&gt; transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n&gt;&gt;&gt; src = torch.rand(10, 32, 512)\n&gt;&gt;&gt; out = transformer_encoder(src)\n\n\n"},
{"library": "torch", "item_id": "torch.nn.Linear", "item_type": "class", "code": "classtorch.nn.Linear(in_features:int,out_features:int,bias:bool=True)", "description": "Applies a linear transformation to the incoming data: y=xAT+by = xA^T + by=xAT+b    Parameters  in_features \u2013 size of each input sample out_features \u2013 size of each output sample bias \u2013 If set to False, the layer will not learn an additive bias. Default: True     Shape: Input: (N,\u2217,Hin)(N, *, H_{in})(N,\u2217,Hin\u200b)   where \u2217*\u2217   means any number of additional dimensions and Hin=in_featuresH_{in} = \\text{in\\_features}Hin\u200b=in_features   Output: (N,\u2217,Hout)(N, *, H_{out})(N,\u2217,Hout\u200b)   where all but the last dimension are the same shape as the input and Hout=out_featuresH_{out} = \\text{out\\_features}Hout\u200b=out_features  .     Variables  ~Linear.weight \u2013 the learnable weights of the module of shape (out_features,in_features)(\\text{out\\_features}, \\text{in\\_features})(out_features,in_features)  . The values are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b)  , where k=1in_featuresk = \\frac{1}{\\text{in\\_features}}k=in_features1\u200b   ~Linear.bias \u2013 the learnable bias of the module of shape (out_features)(\\text{out\\_features})(out_features)  . If bias is True, the values are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b)   where k=1in_featuresk = \\frac{1}{\\text{in\\_features}}k=in_features1\u200b      Examples: &gt;&gt;&gt; m = nn.Linear(20, 30) &gt;&gt;&gt; input = torch.randn(128, 20) &gt;&gt;&gt; output = m(input) &gt;&gt;&gt; print(output.size()) torch.Size([128, 30])   ", "parameters": ["in_features : size of each input sample", "out_features : size of each output sample", "bias : If set to False, the layer will not learn an additive bias.Default: True", "~Linear.weight : the learnable weights of the module of shape(out_features,in_features)(\\text{out\\_features}, \\text{in\\_features})(out_features,in_features). The values areinitialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b), wherek=1in_featuresk = \\frac{1}{\\text{in\\_features}}k=in_features1\u200b", "~Linear.bias : the learnable bias of the module of shape (out_features)(\\text{out\\_features})(out_features).If bias is True, the values are initialized fromU(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b) wherek=1in_featuresk = \\frac{1}{\\text{in\\_features}}k=in_features1\u200b"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Identity", "item_type": "class", "code": "classtorch.nn.Identity(*args,**kwargs)", "description": "A placeholder identity operator that is argument-insensitive.  Parameters  args \u2013 any argument (unused) kwargs \u2013 any keyword argument (unused)    Examples: &gt;&gt;&gt; m = nn.Identity(54, unused_argument1=0.1, unused_argument2=False) &gt;&gt;&gt; input = torch.randn(128, 20) &gt;&gt;&gt; output = m(input) &gt;&gt;&gt; print(output.size()) torch.Size([128, 20])   ", "parameters": ["args : any argument (unused)", "kwargs : any keyword argument (unused)"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Transformer.generate_square_subsequent_mask", "item_type": "method", "code": "generate_square_subsequent_mask(sz:int)\u2192torch.Tensor", "description": "Generate a square mask for the sequence. The masked positions are filled with float(\u2018-inf\u2019). Unmasked positions are filled with float(0.0). ", "parameters": [], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Transformer", "item_type": "class", "code": "classtorch.nn.Transformer(d_model:int=512,nhead:int=8,num_encoder_layers:int=6,num_decoder_layers:int=6,dim_feedforward:int=2048,dropout:float=0.1,activation:str='relu',custom_encoder:Optional[Any]=None,custom_decoder:Optional[Any]=None)", "description": "A transformer model. User is able to modify the attributes as needed. The architecture is based on the paper \u201cAttention Is All You Need\u201d. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users can build the BERT(https://arxiv.org/abs/1810.04805) model with corresponding parameters.  Parameters  d_model \u2013 the number of expected features in the encoder/decoder inputs (default=512). nhead \u2013 the number of heads in the multiheadattention models (default=8). num_encoder_layers \u2013 the number of sub-encoder-layers in the encoder (default=6). num_decoder_layers \u2013 the number of sub-decoder-layers in the decoder (default=6). dim_feedforward \u2013 the dimension of the feedforward network model (default=2048). dropout \u2013 the dropout value (default=0.1). activation \u2013 the activation function of encoder/decoder intermediate layer, relu or gelu (default=relu). custom_encoder \u2013 custom encoder (default=None). custom_decoder \u2013 custom decoder (default=None).     Examples::&gt;&gt;&gt; transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12) &gt;&gt;&gt; src = torch.rand((10, 32, 512)) &gt;&gt;&gt; tgt = torch.rand((20, 32, 512)) &gt;&gt;&gt; out = transformer_model(src, tgt)     Note: A full example to apply nn.Transformer module for the word language model is available in https://github.com/pytorch/examples/tree/master/word_language_model   forward(src: torch.Tensor, tgt: torch.Tensor, src_mask: Optional[torch.Tensor] = None, tgt_mask: Optional[torch.Tensor] = None, memory_mask: Optional[torch.Tensor] = None, src_key_padding_mask: Optional[torch.Tensor] = None, tgt_key_padding_mask: Optional[torch.Tensor] = None, memory_key_padding_mask: Optional[torch.Tensor] = None) \u2192 torch.Tensor  Take in and process masked source/target sequences.  Parameters  src \u2013 the sequence to the encoder (required). tgt \u2013 the sequence to the decoder (required). src_mask \u2013 the additive mask for the src sequence (optional). tgt_mask \u2013 the additive mask for the tgt sequence (optional). memory_mask \u2013 the additive mask for the encoder output (optional). src_key_padding_mask \u2013 the ByteTensor mask for src keys per batch (optional). tgt_key_padding_mask \u2013 the ByteTensor mask for tgt keys per batch (optional). memory_key_padding_mask \u2013 the ByteTensor mask for memory keys per batch (optional).     Shape: src: (S,N,E)(S, N, E)(S,N,E)  . tgt: (T,N,E)(T, N, E)(T,N,E)  . src_mask: (S,S)(S, S)(S,S)  . tgt_mask: (T,T)(T, T)(T,T)  . memory_mask: (T,S)(T, S)(T,S)  . src_key_padding_mask: (N,S)(N, S)(N,S)  . tgt_key_padding_mask: (N,T)(N, T)(N,T)  . memory_key_padding_mask: (N,S)(N, S)(N,S)  .  Note: [src/tgt/memory]_mask ensures that position i is allowed to attend the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend while the zero positions will be unchanged. If a BoolTensor is provided, positions with True are not allowed to attend while False values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight. [src/tgt/memory]_key_padding_mask provides specified elements in the key to be ignored by the attention. If a ByteTensor is provided, the non-zero positions will be ignored while the zero positions will be unchanged. If a BoolTensor is provided, the positions with the value of True will be ignored while the position with the value of False will be unchanged.  output: (T,N,E)(T, N, E)(T,N,E)  .  Note: Due to the multi-head attention architecture in the transformer model, the output sequence length of a transformer is same as the input sequence (i.e. target) length of the decode. where S is the source sequence length, T is the target sequence length, N is the batch size, E is the feature number   Examples &gt;&gt;&gt; output = transformer_model(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)       generate_square_subsequent_mask(sz: int) \u2192 torch.Tensor  Generate a square mask for the sequence. The masked positions are filled with float(\u2018-inf\u2019). Unmasked positions are filled with float(0.0).   ", "parameters": ["d_model : the number of expected features in the encoder/decoder inputs (default=512).", "nhead : the number of heads in the multiheadattention models (default=8).", "num_encoder_layers : the number of sub-encoder-layers in the encoder (default=6).", "num_decoder_layers : the number of sub-decoder-layers in the decoder (default=6).", "dim_feedforward : the dimension of the feedforward network model (default=2048).", "dropout : the dropout value (default=0.1).", "activation : the activation function of encoder/decoder intermediate layer, relu or gelu (default=relu).", "custom_encoder : custom encoder (default=None).", "custom_decoder : custom decoder (default=None).", "src : the sequence to the encoder (required).", "tgt : the sequence to the decoder (required).", "src_mask : the additive mask for the src sequence (optional).", "tgt_mask : the additive mask for the tgt sequence (optional).", "memory_mask : the additive mask for the encoder output (optional).", "src_key_padding_mask : the ByteTensor mask for src keys per batch (optional).", "tgt_key_padding_mask : the ByteTensor mask for tgt keys per batch (optional).", "memory_key_padding_mask : the ByteTensor mask for memory keys per batch (optional)."], "returns": [], "example": "NA", "shape": "&gt;&gt;&gt; transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)\n&gt;&gt;&gt; src = torch.rand((10, 32, 512))\n&gt;&gt;&gt; tgt = torch.rand((20, 32, 512))\n&gt;&gt;&gt; out = transformer_model(src, tgt)\n\n\n"},
{"library": "torch", "item_id": "torch.nn.RNNCell", "item_type": "class", "code": "classtorch.nn.RNNCell(input_size:int,hidden_size:int,bias:bool=True,nonlinearity:str='tanh')", "description": "An Elman RNN cell with tanh or ReLU non-linearity.  h\u2032=tanh\u2061(Wihx+bih+Whhh+bhh)h' = \\tanh(W_{ih} x + b_{ih}  +  W_{hh} h + b_{hh})h\u2032=tanh(Wih\u200bx+bih\u200b+Whh\u200bh+bhh\u200b)  If nonlinearity is \u2018relu\u2019, then ReLU is used in place of tanh.  Parameters  input_size \u2013 The number of expected features in the input x hidden_size \u2013 The number of features in the hidden state h bias \u2013 If False, then the layer does not use bias weights b_ih and b_hh. Default: True nonlinearity \u2013 The non-linearity to use. Can be either 'tanh' or 'relu'. Default: 'tanh'     Inputs: input, hidden input of shape (batch, input_size): tensor containing input features hidden of shape (batch, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided.   Outputs: h\u2019 h\u2019 of shape (batch, hidden_size): tensor containing the next hidden state for each element in the batch   Shape: Input1: (N,Hin)(N, H_{in})(N,Hin\u200b)   tensor containing input features where HinH_{in}Hin\u200b   = input_size Input2: (N,Hout)(N, H_{out})(N,Hout\u200b)   tensor containing the initial hidden state for each element in the batch where HoutH_{out}Hout\u200b   = hidden_size Defaults to zero if not provided. Output: (N,Hout)(N, H_{out})(N,Hout\u200b)   tensor containing the next hidden state for each element in the batch     Variables  ~RNNCell.weight_ih \u2013 the learnable input-hidden weights, of shape (hidden_size, input_size) ~RNNCell.weight_hh \u2013 the learnable hidden-hidden weights, of shape (hidden_size, hidden_size) ~RNNCell.bias_ih \u2013 the learnable input-hidden bias, of shape (hidden_size) ~RNNCell.bias_hh \u2013 the learnable hidden-hidden bias, of shape (hidden_size)     Note All the weights and biases are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b)   where k=1hidden_sizek = \\frac{1}{\\text{hidden\\_size}}k=hidden_size1\u200b    Examples: &gt;&gt;&gt; rnn = nn.RNNCell(10, 20) &gt;&gt;&gt; input = torch.randn(6, 3, 10) &gt;&gt;&gt; hx = torch.randn(3, 20) &gt;&gt;&gt; output = [] &gt;&gt;&gt; for i in range(6):         hx = rnn(input[i], hx)         output.append(hx)   ", "parameters": ["input_size : The number of expected features in the input x", "hidden_size : The number of features in the hidden state h", "bias : If False, then the layer does not use bias weights b_ih and b_hh.Default: True", "nonlinearity : The non-linearity to use. Can be either 'tanh' or 'relu'. Default: 'tanh'", "~RNNCell.weight_ih : the learnable input-hidden weights, of shape(hidden_size, input_size)", "~RNNCell.weight_hh : the learnable hidden-hidden weights, of shape(hidden_size, hidden_size)", "~RNNCell.bias_ih : the learnable input-hidden bias, of shape (hidden_size)", "~RNNCell.bias_hh : the learnable hidden-hidden bias, of shape (hidden_size)"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.GRU", "item_type": "class", "code": "classtorch.nn.GRU(*args,**kwargs)", "description": "Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence. For each element in the input sequence, each layer computes the following function:  rt=\u03c3(Wirxt+bir+Whrh(t\u22121)+bhr)zt=\u03c3(Wizxt+biz+Whzh(t\u22121)+bhz)nt=tanh\u2061(Winxt+bin+rt\u2217(Whnh(t\u22121)+bhn))ht=(1\u2212zt)\u2217nt+zt\u2217h(t\u22121)\\begin{array}{ll}     r_t = \\sigma(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\\\     z_t = \\sigma(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\\\     n_t = \\tanh(W_{in} x_t + b_{in} + r_t * (W_{hn} h_{(t-1)}+ b_{hn})) \\\\     h_t = (1 - z_t) * n_t + z_t * h_{(t-1)} \\end{array}  rt\u200b=\u03c3(Wir\u200bxt\u200b+bir\u200b+Whr\u200bh(t\u22121)\u200b+bhr\u200b)zt\u200b=\u03c3(Wiz\u200bxt\u200b+biz\u200b+Whz\u200bh(t\u22121)\u200b+bhz\u200b)nt\u200b=tanh(Win\u200bxt\u200b+bin\u200b+rt\u200b\u2217(Whn\u200bh(t\u22121)\u200b+bhn\u200b))ht\u200b=(1\u2212zt\u200b)\u2217nt\u200b+zt\u200b\u2217h(t\u22121)\u200b\u200b  where hth_tht\u200b   is the hidden state at time t, xtx_txt\u200b   is the input at time t, h(t\u22121)h_{(t-1)}h(t\u22121)\u200b   is the hidden state of the layer at time t-1 or the initial hidden state at time 0, and rtr_trt\u200b  , ztz_tzt\u200b  , ntn_tnt\u200b   are the reset, update, and new gates, respectively. \u03c3\\sigma\u03c3   is the sigmoid function, and \u2217*\u2217   is the Hadamard product. In a multilayer GRU, the input xt(l)x^{(l)}_txt(l)\u200b   of the lll   -th layer (l&gt;=2l &gt;= 2l&gt;=2  ) is the hidden state ht(l\u22121)h^{(l-1)}_tht(l\u22121)\u200b   of the previous layer multiplied by dropout \u03b4t(l\u22121)\\delta^{(l-1)}_t\u03b4t(l\u22121)\u200b   where each \u03b4t(l\u22121)\\delta^{(l-1)}_t\u03b4t(l\u22121)\u200b   is a Bernoulli random variable which is 000   with probability dropout.  Parameters  input_size \u2013 The number of expected features in the input x hidden_size \u2013 The number of features in the hidden state h num_layers \u2013 Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two GRUs together to form a stacked GRU, with the second GRU taking in outputs of the first GRU and computing the final results. Default: 1 bias \u2013 If False, then the layer does not use bias weights b_ih and b_hh. Default: True batch_first \u2013 If True, then the input and output tensors are provided as (batch, seq, feature). Default: False dropout \u2013 If non-zero, introduces a Dropout layer on the outputs of each GRU layer except the last layer, with dropout probability equal to dropout. Default: 0 bidirectional \u2013 If True, becomes a bidirectional GRU. Default: False     Inputs: input, h_0 input of shape (seq_len, batch, input_size): tensor containing the features of the input sequence. The input can also be a packed variable length sequence. See torch.nn.utils.rnn.pack_padded_sequence() for details. h_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided. If the RNN is bidirectional, num_directions should be 2, else it should be 1.   Outputs: output, h_n output of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features h_t from the last layer of the GRU, for each t. If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence. For the unpacked case, the directions can be separated using output.view(seq_len, batch, num_directions, hidden_size), with forward and backward being direction 0 and 1 respectively. Similarly, the directions can be separated in the packed case.  h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len Like output, the layers can be separated using h_n.view(num_layers, num_directions, batch, hidden_size).    Shape: Input1: (L,N,Hin)(L, N, H_{in})(L,N,Hin\u200b)   tensor containing input features where Hin=input_sizeH_{in}=\\text{input\\_size}Hin\u200b=input_size   and L represents a sequence length. Input2: (S,N,Hout)(S, N, H_{out})(S,N,Hout\u200b)   tensor containing the initial hidden state for each element in the batch. Hout=hidden_sizeH_{out}=\\text{hidden\\_size}Hout\u200b=hidden_size   Defaults to zero if not provided. where S=num_layers\u2217num_directionsS=\\text{num\\_layers} * \\text{num\\_directions}S=num_layers\u2217num_directions   If the RNN is bidirectional, num_directions should be 2, else it should be 1. Output1: (L,N,Hall)(L, N, H_{all})(L,N,Hall\u200b)   where Hall=num_directions\u2217hidden_sizeH_{all}=\\text{num\\_directions} * \\text{hidden\\_size}Hall\u200b=num_directions\u2217hidden_size   Output2: (S,N,Hout)(S, N, H_{out})(S,N,Hout\u200b)   tensor containing the next hidden state for each element in the batch     Variables  ~GRU.weight_ih_l[k] \u2013 the learnable input-hidden weights of the kth\\text{k}^{th}kth   layer (W_ir|W_iz|W_in), of shape (3*hidden_size, input_size) for k = 0. Otherwise, the shape is (3*hidden_size, num_directions * hidden_size) ~GRU.weight_hh_l[k] \u2013 the learnable hidden-hidden weights of the kth\\text{k}^{th}kth   layer (W_hr|W_hz|W_hn), of shape (3*hidden_size, hidden_size) ~GRU.bias_ih_l[k] \u2013 the learnable input-hidden bias of the kth\\text{k}^{th}kth   layer (b_ir|b_iz|b_in), of shape (3*hidden_size) ~GRU.bias_hh_l[k] \u2013 the learnable hidden-hidden bias of the kth\\text{k}^{th}kth   layer (b_hr|b_hz|b_hn), of shape (3*hidden_size)     Note All the weights and biases are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b)   where k=1hidden_sizek = \\frac{1}{\\text{hidden\\_size}}k=hidden_size1\u200b     Orphan    Note If the following conditions are satisfied: 1) cudnn is enabled, 2) input data is on the GPU 3) input data has dtype torch.float16 4) V100 GPU is used, 5) input data is not in PackedSequence format persistent algorithm can be selected to improve performance.  Examples: &gt;&gt;&gt; rnn = nn.GRU(10, 20, 2) &gt;&gt;&gt; input = torch.randn(5, 3, 10) &gt;&gt;&gt; h0 = torch.randn(2, 3, 20) &gt;&gt;&gt; output, hn = rnn(input, h0)   ", "parameters": ["input_size : The number of expected features in the input x", "hidden_size : The number of features in the hidden state h", "num_layers : Number of recurrent layers. E.g., setting num_layers=2would mean stacking two GRUs together to form a stacked GRU,with the second GRU taking in outputs of the first GRU andcomputing the final results. Default: 1", "bias : If False, then the layer does not use bias weights b_ih and b_hh.Default: True", "batch_first : If True, then the input and output tensors are providedas (batch, seq, feature). Default: False", "dropout : If non-zero, introduces a Dropout layer on the outputs of eachGRU layer except the last layer, with dropout probability equal todropout. Default: 0", "bidirectional : If True, becomes a bidirectional GRU. Default: False", "~GRU.weight_ih_l[k] : the learnable input-hidden weights of the kth\\text{k}^{th}kth layer(W_ir|W_iz|W_in), of shape (3*hidden_size, input_size) for k = 0.Otherwise, the shape is (3*hidden_size, num_directions * hidden_size)", "~GRU.weight_hh_l[k] : the learnable hidden-hidden weights of the kth\\text{k}^{th}kth layer(W_hr|W_hz|W_hn), of shape (3*hidden_size, hidden_size)", "~GRU.bias_ih_l[k] : the learnable input-hidden bias of the kth\\text{k}^{th}kth layer(b_ir|b_iz|b_in), of shape (3*hidden_size)", "~GRU.bias_hh_l[k] : the learnable hidden-hidden bias of the kth\\text{k}^{th}kth layer(b_hr|b_hz|b_hn), of shape (3*hidden_size)"], "returns": [], "example": "NA", "shape": "\ninput of shape (seq_len, batch, input_size): tensor containing the features\nof the input sequence. The input can also be a packed variable length\nsequence. See torch.nn.utils.rnn.pack_padded_sequence()\nfor details.\nh_0 of shape (num_layers * num_directions, batch, hidden_size): tensor\ncontaining the initial hidden state for each element in the batch.\nDefaults to zero if not provided. If the RNN is bidirectional,\nnum_directions should be 2, else it should be 1.\n\n"},
{"library": "torch", "item_id": "torch.nn.LSTM", "item_type": "class", "code": "classtorch.nn.LSTM(*args,**kwargs)", "description": "Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence. For each element in the input sequence, each layer computes the following function:  it=\u03c3(Wiixt+bii+Whiht\u22121+bhi)ft=\u03c3(Wifxt+bif+Whfht\u22121+bhf)gt=tanh\u2061(Wigxt+big+Whght\u22121+bhg)ot=\u03c3(Wioxt+bio+Whoht\u22121+bho)ct=ft\u2299ct\u22121+it\u2299gtht=ot\u2299tanh\u2061(ct)\\begin{array}{ll} \\\\     i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\\\     f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\\\     g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}) \\\\     o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\\\     c_t = f_t \\odot c_{t-1} + i_t \\odot g_t \\\\     h_t = o_t \\odot \\tanh(c_t) \\\\ \\end{array}  it\u200b=\u03c3(Wii\u200bxt\u200b+bii\u200b+Whi\u200bht\u22121\u200b+bhi\u200b)ft\u200b=\u03c3(Wif\u200bxt\u200b+bif\u200b+Whf\u200bht\u22121\u200b+bhf\u200b)gt\u200b=tanh(Wig\u200bxt\u200b+big\u200b+Whg\u200bht\u22121\u200b+bhg\u200b)ot\u200b=\u03c3(Wio\u200bxt\u200b+bio\u200b+Who\u200bht\u22121\u200b+bho\u200b)ct\u200b=ft\u200b\u2299ct\u22121\u200b+it\u200b\u2299gt\u200bht\u200b=ot\u200b\u2299tanh(ct\u200b)\u200b  where hth_tht\u200b   is the hidden state at time t, ctc_tct\u200b   is the cell state at time t, xtx_txt\u200b   is the input at time t, ht\u22121h_{t-1}ht\u22121\u200b   is the hidden state of the layer at time t-1 or the initial hidden state at time 0, and iti_tit\u200b  , ftf_tft\u200b  , gtg_tgt\u200b  , oto_tot\u200b   are the input, forget, cell, and output gates, respectively. \u03c3\\sigma\u03c3   is the sigmoid function, and \u2299\\odot\u2299   is the Hadamard product. In a multilayer LSTM, the input xt(l)x^{(l)}_txt(l)\u200b   of the lll   -th layer (l&gt;=2l &gt;= 2l&gt;=2  ) is the hidden state ht(l\u22121)h^{(l-1)}_tht(l\u22121)\u200b   of the previous layer multiplied by dropout \u03b4t(l\u22121)\\delta^{(l-1)}_t\u03b4t(l\u22121)\u200b   where each \u03b4t(l\u22121)\\delta^{(l-1)}_t\u03b4t(l\u22121)\u200b   is a Bernoulli random variable which is 000   with probability dropout.  Parameters  input_size \u2013 The number of expected features in the input x hidden_size \u2013 The number of features in the hidden state h num_layers \u2013 Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results. Default: 1 bias \u2013 If False, then the layer does not use bias weights b_ih and b_hh. Default: True batch_first \u2013 If True, then the input and output tensors are provided as (batch, seq, feature). Default: False dropout \u2013 If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to dropout. Default: 0 bidirectional \u2013 If True, becomes a bidirectional LSTM. Default: False     Inputs: input, (h_0, c_0) input of shape (seq_len, batch, input_size): tensor containing the features of the input sequence. The input can also be a packed variable length sequence. See torch.nn.utils.rnn.pack_padded_sequence() or torch.nn.utils.rnn.pack_sequence() for details. h_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. If the LSTM is bidirectional, num_directions should be 2, else it should be 1. c_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial cell state for each element in the batch. If (h_0, c_0) is not provided, both h_0 and c_0 default to zero.    Outputs: output, (h_n, c_n) output of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features (h_t) from the last layer of the LSTM, for each t. If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence. For the unpacked case, the directions can be separated using output.view(seq_len, batch, num_directions, hidden_size), with forward and backward being direction 0 and 1 respectively. Similarly, the directions can be separated in the packed case.  h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len. Like output, the layers can be separated using h_n.view(num_layers, num_directions, batch, hidden_size) and similarly for c_n.  c_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the cell state for t = seq_len.     Variables  ~LSTM.weight_ih_l[k] \u2013 the learnable input-hidden weights of the kth\\text{k}^{th}kth   layer (W_ii|W_if|W_ig|W_io), of shape (4*hidden_size, input_size) for k = 0. Otherwise, the shape is (4*hidden_size, num_directions * hidden_size) ~LSTM.weight_hh_l[k] \u2013 the learnable hidden-hidden weights of the kth\\text{k}^{th}kth   layer (W_hi|W_hf|W_hg|W_ho), of shape (4*hidden_size, hidden_size) ~LSTM.bias_ih_l[k] \u2013 the learnable input-hidden bias of the kth\\text{k}^{th}kth   layer (b_ii|b_if|b_ig|b_io), of shape (4*hidden_size) ~LSTM.bias_hh_l[k] \u2013 the learnable hidden-hidden bias of the kth\\text{k}^{th}kth   layer (b_hi|b_hf|b_hg|b_ho), of shape (4*hidden_size)     Note All the weights and biases are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b)   where k=1hidden_sizek = \\frac{1}{\\text{hidden\\_size}}k=hidden_size1\u200b     Orphan    Note If the following conditions are satisfied: 1) cudnn is enabled, 2) input data is on the GPU 3) input data has dtype torch.float16 4) V100 GPU is used, 5) input data is not in PackedSequence format persistent algorithm can be selected to improve performance.  Examples: &gt;&gt;&gt; rnn = nn.LSTM(10, 20, 2) &gt;&gt;&gt; input = torch.randn(5, 3, 10) &gt;&gt;&gt; h0 = torch.randn(2, 3, 20) &gt;&gt;&gt; c0 = torch.randn(2, 3, 20) &gt;&gt;&gt; output, (hn, cn) = rnn(input, (h0, c0))   ", "parameters": ["input_size : The number of expected features in the input x", "hidden_size : The number of features in the hidden state h", "num_layers : Number of recurrent layers. E.g., setting num_layers=2would mean stacking two LSTMs together to form a stacked LSTM,with the second LSTM taking in outputs of the first LSTM andcomputing the final results. Default: 1", "bias : If False, then the layer does not use bias weights b_ih and b_hh.Default: True", "batch_first : If True, then the input and output tensors are providedas (batch, seq, feature). Default: False", "dropout : If non-zero, introduces a Dropout layer on the outputs of eachLSTM layer except the last layer, with dropout probability equal todropout. Default: 0", "bidirectional : If True, becomes a bidirectional LSTM. Default: False", "~LSTM.weight_ih_l[k] : the learnable input-hidden weights of the kth\\text{k}^{th}kth layer(W_ii|W_if|W_ig|W_io), of shape (4*hidden_size, input_size) for k = 0.Otherwise, the shape is (4*hidden_size, num_directions * hidden_size)", "~LSTM.weight_hh_l[k] : the learnable hidden-hidden weights of the kth\\text{k}^{th}kth layer(W_hi|W_hf|W_hg|W_ho), of shape (4*hidden_size, hidden_size)", "~LSTM.bias_ih_l[k] : the learnable input-hidden bias of the kth\\text{k}^{th}kth layer(b_ii|b_if|b_ig|b_io), of shape (4*hidden_size)", "~LSTM.bias_hh_l[k] : the learnable hidden-hidden bias of the kth\\text{k}^{th}kth layer(b_hi|b_hf|b_hg|b_ho), of shape (4*hidden_size)"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.RNN", "item_type": "class", "code": "classtorch.nn.RNN(*args,**kwargs)", "description": "Applies a multi-layer Elman RNN with tanh\u2061\\tanhtanh   or ReLU\\text{ReLU}ReLU   non-linearity to an input sequence. For each element in the input sequence, each layer computes the following function:  ht=tanh\u2061(Wihxt+bih+Whhh(t\u22121)+bhh)h_t = \\tanh(W_{ih} x_t + b_{ih} + W_{hh} h_{(t-1)} + b_{hh})  ht\u200b=tanh(Wih\u200bxt\u200b+bih\u200b+Whh\u200bh(t\u22121)\u200b+bhh\u200b)  where hth_tht\u200b   is the hidden state at time t, xtx_txt\u200b   is the input at time t, and h(t\u22121)h_{(t-1)}h(t\u22121)\u200b   is the hidden state of the previous layer at time t-1 or the initial hidden state at time 0. If nonlinearity is 'relu', then ReLU\\text{ReLU}ReLU   is used instead of tanh\u2061\\tanhtanh  .  Parameters  input_size \u2013 The number of expected features in the input x hidden_size \u2013 The number of features in the hidden state h num_layers \u2013 Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two RNNs together to form a stacked RNN, with the second RNN taking in outputs of the first RNN and computing the final results. Default: 1 nonlinearity \u2013 The non-linearity to use. Can be either 'tanh' or 'relu'. Default: 'tanh' bias \u2013 If False, then the layer does not use bias weights b_ih and b_hh. Default: True batch_first \u2013 If True, then the input and output tensors are provided as (batch, seq, feature). Default: False dropout \u2013 If non-zero, introduces a Dropout layer on the outputs of each RNN layer except the last layer, with dropout probability equal to dropout. Default: 0 bidirectional \u2013 If True, becomes a bidirectional RNN. Default: False     Inputs: input, h_0 input of shape (seq_len, batch, input_size): tensor containing the features of the input sequence. The input can also be a packed variable length sequence. See torch.nn.utils.rnn.pack_padded_sequence() or torch.nn.utils.rnn.pack_sequence() for details. h_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided. If the RNN is bidirectional, num_directions should be 2, else it should be 1.   Outputs: output, h_n output of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features (h_t) from the last layer of the RNN, for each t.  If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence. For the unpacked case, the directions can be separated using output.view(seq_len, batch, num_directions, hidden_size), with forward and backward being direction 0 and 1 respectively. Similarly, the directions can be separated in the packed case.  h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len. Like output, the layers can be separated using h_n.view(num_layers, num_directions, batch, hidden_size).    Shape: Input1: (L,N,Hin)(L, N, H_{in})(L,N,Hin\u200b)   tensor containing input features where Hin=input_sizeH_{in}=\\text{input\\_size}Hin\u200b=input_size   and L represents a sequence length. Input2: (S,N,Hout)(S, N, H_{out})(S,N,Hout\u200b)   tensor containing the initial hidden state for each element in the batch. Hout=hidden_sizeH_{out}=\\text{hidden\\_size}Hout\u200b=hidden_size   Defaults to zero if not provided. where S=num_layers\u2217num_directionsS=\\text{num\\_layers} * \\text{num\\_directions}S=num_layers\u2217num_directions   If the RNN is bidirectional, num_directions should be 2, else it should be 1. Output1: (L,N,Hall)(L, N, H_{all})(L,N,Hall\u200b)   where Hall=num_directions\u2217hidden_sizeH_{all}=\\text{num\\_directions} * \\text{hidden\\_size}Hall\u200b=num_directions\u2217hidden_size   Output2: (S,N,Hout)(S, N, H_{out})(S,N,Hout\u200b)   tensor containing the next hidden state for each element in the batch     Variables  ~RNN.weight_ih_l[k] \u2013 the learnable input-hidden weights of the k-th layer, of shape (hidden_size, input_size) for k = 0. Otherwise, the shape is (hidden_size, num_directions * hidden_size) ~RNN.weight_hh_l[k] \u2013 the learnable hidden-hidden weights of the k-th layer, of shape (hidden_size, hidden_size) ~RNN.bias_ih_l[k] \u2013 the learnable input-hidden bias of the k-th layer, of shape (hidden_size) ~RNN.bias_hh_l[k] \u2013 the learnable hidden-hidden bias of the k-th layer, of shape (hidden_size)     Note All the weights and biases are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b)   where k=1hidden_sizek = \\frac{1}{\\text{hidden\\_size}}k=hidden_size1\u200b     Orphan    Note If the following conditions are satisfied: 1) cudnn is enabled, 2) input data is on the GPU 3) input data has dtype torch.float16 4) V100 GPU is used, 5) input data is not in PackedSequence format persistent algorithm can be selected to improve performance.  Examples: &gt;&gt;&gt; rnn = nn.RNN(10, 20, 2) &gt;&gt;&gt; input = torch.randn(5, 3, 10) &gt;&gt;&gt; h0 = torch.randn(2, 3, 20) &gt;&gt;&gt; output, hn = rnn(input, h0)   ", "parameters": ["input_size : The number of expected features in the input x", "hidden_size : The number of features in the hidden state h", "num_layers : Number of recurrent layers. E.g., setting num_layers=2would mean stacking two RNNs together to form a stacked RNN,with the second RNN taking in outputs of the first RNN andcomputing the final results. Default: 1", "nonlinearity : The non-linearity to use. Can be either 'tanh' or 'relu'. Default: 'tanh'", "bias : If False, then the layer does not use bias weights b_ih and b_hh.Default: True", "batch_first : If True, then the input and output tensors are providedas (batch, seq, feature). Default: False", "dropout : If non-zero, introduces a Dropout layer on the outputs of eachRNN layer except the last layer, with dropout probability equal todropout. Default: 0", "bidirectional : If True, becomes a bidirectional RNN. Default: False", "~RNN.weight_ih_l[k] : the learnable input-hidden weights of the k-th layer,of shape (hidden_size, input_size) for k = 0. Otherwise, the shape is(hidden_size, num_directions * hidden_size)", "~RNN.weight_hh_l[k] : the learnable hidden-hidden weights of the k-th layer,of shape (hidden_size, hidden_size)", "~RNN.bias_ih_l[k] : the learnable input-hidden bias of the k-th layer,of shape (hidden_size)", "~RNN.bias_hh_l[k] : the learnable hidden-hidden bias of the k-th layer,of shape (hidden_size)"], "returns": [], "example": "NA", "shape": "\ninput of shape (seq_len, batch, input_size): tensor containing the features\nof the input sequence. The input can also be a packed variable length\nsequence. See torch.nn.utils.rnn.pack_padded_sequence()\nor torch.nn.utils.rnn.pack_sequence()\nfor details.\nh_0 of shape (num_layers * num_directions, batch, hidden_size): tensor\ncontaining the initial hidden state for each element in the batch.\nDefaults to zero if not provided. If the RNN is bidirectional,\nnum_directions should be 2, else it should be 1.\n\n"},
{"library": "torch", "item_id": "torch.nn.RNNBase.flatten_parameters", "item_type": "method", "code": "flatten_parameters()\u2192None", "description": "Resets parameter data pointer so that they can use faster code paths. Right now, this works only if the module is on the GPU and cuDNN is enabled. Otherwise, it\u2019s a no-op. ", "parameters": [], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.RNNBase", "item_type": "class", "code": "classtorch.nn.RNNBase(mode:str,input_size:int,hidden_size:int,num_layers:int=1,bias:bool=True,batch_first:bool=False,dropout:float=0.0,bidirectional:bool=False)", "description": "  flatten_parameters() \u2192 None  Resets parameter data pointer so that they can use faster code paths. Right now, this works only if the module is on the GPU and cuDNN is enabled. Otherwise, it\u2019s a no-op.   ", "parameters": [], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.LocalResponseNorm", "item_type": "class", "code": "classtorch.nn.LocalResponseNorm(size:int,alpha:float=0.0001,beta:float=0.75,k:float=1.0)", "description": "Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension. Applies normalization across channels.  bc=ac(k+\u03b1n\u2211c\u2032=max\u2061(0,c\u2212n/2)min\u2061(N\u22121,c+n/2)ac\u20322)\u2212\u03b2b_{c} = a_{c}\\left(k + \\frac{\\alpha}{n} \\sum_{c'=\\max(0, c-n/2)}^{\\min(N-1,c+n/2)}a_{c'}^2\\right)^{-\\beta}  bc\u200b=ac\u200b\u239d\u239c\u239b\u200bk+n\u03b1\u200bc\u2032=max(0,c\u2212n/2)\u2211min(N\u22121,c+n/2)\u200bac\u20322\u200b\u23a0\u239f\u239e\u200b\u2212\u03b2   Parameters  size \u2013 amount of neighbouring channels used for normalization alpha \u2013 multiplicative factor. Default: 0.0001 beta \u2013 exponent. Default: 0.75 k \u2013 additive factor. Default: 1     Shape: Input: (N,C,\u2217)(N, C, *)(N,C,\u2217)   Output: (N,C,\u2217)(N, C, *)(N,C,\u2217)   (same shape as input)    Examples: &gt;&gt;&gt; lrn = nn.LocalResponseNorm(2) &gt;&gt;&gt; signal_2d = torch.randn(32, 5, 24, 24) &gt;&gt;&gt; signal_4d = torch.randn(16, 5, 7, 7, 7, 7) &gt;&gt;&gt; output_2d = lrn(signal_2d) &gt;&gt;&gt; output_4d = lrn(signal_4d)   ", "parameters": ["size : amount of neighbouring channels used for normalization", "alpha : multiplicative factor. Default: 0.0001", "beta : exponent. Default: 0.75", "k : additive factor. Default: 1"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.LayerNorm", "item_type": "class", "code": "classtorch.nn.LayerNorm(normalized_shape:Union[int,List[int],torch.Size],eps:float=1e-05,elementwise_affine:bool=True)", "description": "Applies Layer Normalization over a mini-batch of inputs as described in the paper Layer Normalization  y=x\u2212E[x]Var[x]+\u03f5\u2217\u03b3+\u03b2y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta  y=Var[x]+\u03f5\u200bx\u2212E[x]\u200b\u2217\u03b3+\u03b2  The mean and standard-deviation are calculated separately over the last certain number dimensions which have to be of the shape specified by normalized_shape. \u03b3\\gamma\u03b3   and \u03b2\\beta\u03b2   are learnable affine transform parameters of normalized_shape if elementwise_affine is True. The standard-deviation is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False).  Note Unlike Batch Normalization and Instance Normalization, which applies scalar scale and bias for each entire channel/plane with the affine option, Layer Normalization applies per-element scale and bias with elementwise_affine.  This layer uses statistics computed from input data in both training and evaluation modes.  Parameters  normalized_shape (int or list or torch.Size) \u2013 input shape from an expected input of size  [\u2217\u00d7normalized_shape[0]\u00d7normalized_shape[1]\u00d7\u2026\u00d7normalized_shape[\u22121]][* \\times \\text{normalized\\_shape}[0] \\times \\text{normalized\\_shape}[1]     \\times \\ldots \\times \\text{normalized\\_shape}[-1]]  [\u2217\u00d7normalized_shape[0]\u00d7normalized_shape[1]\u00d7\u2026\u00d7normalized_shape[\u22121]]  If a single integer is used, it is treated as a singleton list, and this module will normalize over the last dimension which is expected to be of that specific size.  eps \u2013 a value added to the denominator for numerical stability. Default: 1e-5 elementwise_affine \u2013 a boolean value that when set to True, this module has learnable per-element affine parameters initialized to ones (for weights) and zeros (for biases). Default: True.     Shape: Input: (N,\u2217)(N, *)(N,\u2217)   Output: (N,\u2217)(N, *)(N,\u2217)   (same shape as input)    Examples: &gt;&gt;&gt; input = torch.randn(20, 5, 10, 10) &gt;&gt;&gt; # With Learnable Parameters &gt;&gt;&gt; m = nn.LayerNorm(input.size()[1:]) &gt;&gt;&gt; # Without Learnable Parameters &gt;&gt;&gt; m = nn.LayerNorm(input.size()[1:], elementwise_affine=False) &gt;&gt;&gt; # Normalize over last two dimensions &gt;&gt;&gt; m = nn.LayerNorm([10, 10]) &gt;&gt;&gt; # Normalize over last dimension of size 10 &gt;&gt;&gt; m = nn.LayerNorm(10) &gt;&gt;&gt; # Activating the module &gt;&gt;&gt; output = m(input)   ", "parameters": ["normalized_shape (int or list or torch.Size) : input shape from an expected inputof size[\u2217\u00d7normalized_shape[0]\u00d7normalized_shape[1]\u00d7\u2026\u00d7normalized_shape[\u22121]][* \\times \\text{normalized\\_shape}[0] \\times \\text{normalized\\_shape}[1]    \\times \\ldots \\times \\text{normalized\\_shape}[-1]][\u2217\u00d7normalized_shape[0]\u00d7normalized_shape[1]\u00d7\u2026\u00d7normalized_shape[\u22121]]If a single integer is used, it is treated as a singleton list, and this module willnormalize over the last dimension which is expected to be of that specific size.", "eps : a value added to the denominator for numerical stability. Default: 1e-5", "elementwise_affine : a boolean value that when set to True, this modulehas learnable per-element affine parameters initialized to ones (for weights)and zeros (for biases). Default: True."], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.InstanceNorm3d", "item_type": "class", "code": "classtorch.nn.InstanceNorm3d(num_features:int,eps:float=1e-05,momentum:float=0.1,affine:bool=False,track_running_stats:bool=False)", "description": "Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization.  y=x\u2212E[x]Var[x]+\u03f5\u2217\u03b3+\u03b2y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\betay=Var[x]+\u03f5\u200bx\u2212E[x]\u200b\u2217\u03b3+\u03b2  The mean and standard-deviation are calculated per-dimension separately for each object in a mini-batch. \u03b3\\gamma\u03b3   and \u03b2\\beta\u03b2   are learnable parameter vectors of size C (where C is the input size) if affine is True. The standard-deviation is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False). By default, this layer uses instance statistics computed from input data in both training and evaluation modes. If track_running_stats is set to True, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1.  Note This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is x^new=(1\u2212momentum)\u00d7x^+momemtum\u00d7xt\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momemtum} \\times x_tx^new\u200b=(1\u2212momentum)\u00d7x^+momemtum\u00d7xt\u200b  , where x^\\hat{x}x^   is the estimated statistic and xtx_txt\u200b   is the new observed value.   Note InstanceNorm3d and LayerNorm are very similar, but have some subtle differences. InstanceNorm3d is applied on each channel of channeled data like 3D models with RGB color, but LayerNorm is usually applied on entire sample and often in NLP tasks. Additionally, LayerNorm applies elementwise affine transform, while InstanceNorm3d usually don\u2019t apply affine transform.   Parameters  num_features \u2013 CCC   from an expected input of size (N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W)   eps \u2013 a value added to the denominator for numerical stability. Default: 1e-5 momentum \u2013 the value used for the running_mean and running_var computation. Default: 0.1 affine \u2013 a boolean value that when set to True, this module has learnable affine parameters, initialized the same way as done for batch normalization. Default: False. track_running_stats \u2013 a boolean value that when set to True, this module tracks the running mean and variance, and when set to False, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: False     Shape: Input: (N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W)   Output: (N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W)   (same shape as input)    Examples: &gt;&gt;&gt; # Without Learnable Parameters &gt;&gt;&gt; m = nn.InstanceNorm3d(100) &gt;&gt;&gt; # With Learnable Parameters &gt;&gt;&gt; m = nn.InstanceNorm3d(100, affine=True) &gt;&gt;&gt; input = torch.randn(20, 100, 35, 45, 10) &gt;&gt;&gt; output = m(input)   ", "parameters": ["num_features : CCC from an expected input of size(N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W)", "eps : a value added to the denominator for numerical stability. Default: 1e-5", "momentum : the value used for the running_mean and running_var computation. Default: 0.1", "affine : a boolean value that when set to True, this module haslearnable affine parameters, initialized the same way as done for batch normalization.Default: False.", "track_running_stats : a boolean value that when set to True, thismodule tracks the running mean and variance, and when set to False,this module does not track such statistics and always uses batchstatistics in both training and eval modes. Default: False"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.InstanceNorm1d", "item_type": "class", "code": "classtorch.nn.InstanceNorm1d(num_features:int,eps:float=1e-05,momentum:float=0.1,affine:bool=False,track_running_stats:bool=False)", "description": "Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization.  y=x\u2212E[x]Var[x]+\u03f5\u2217\u03b3+\u03b2y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\betay=Var[x]+\u03f5\u200bx\u2212E[x]\u200b\u2217\u03b3+\u03b2  The mean and standard-deviation are calculated per-dimension separately for each object in a mini-batch. \u03b3\\gamma\u03b3   and \u03b2\\beta\u03b2   are learnable parameter vectors of size C (where C is the input size) if affine is True. The standard-deviation is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False). By default, this layer uses instance statistics computed from input data in both training and evaluation modes. If track_running_stats is set to True, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1.  Note This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is x^new=(1\u2212momentum)\u00d7x^+momemtum\u00d7xt\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momemtum} \\times x_tx^new\u200b=(1\u2212momentum)\u00d7x^+momemtum\u00d7xt\u200b  , where x^\\hat{x}x^   is the estimated statistic and xtx_txt\u200b   is the new observed value.   Note InstanceNorm1d and LayerNorm are very similar, but have some subtle differences. InstanceNorm1d is applied on each channel of channeled data like multidimensional time series, but LayerNorm is usually applied on entire sample and often in NLP tasks. Additionally, LayerNorm applies elementwise affine transform, while InstanceNorm1d usually don\u2019t apply affine transform.   Parameters  num_features \u2013 CCC   from an expected input of size (N,C,L)(N, C, L)(N,C,L)   or LLL   from input of size (N,L)(N, L)(N,L)   eps \u2013 a value added to the denominator for numerical stability. Default: 1e-5 momentum \u2013 the value used for the running_mean and running_var computation. Default: 0.1 affine \u2013 a boolean value that when set to True, this module has learnable affine parameters, initialized the same way as done for batch normalization. Default: False. track_running_stats \u2013 a boolean value that when set to True, this module tracks the running mean and variance, and when set to False, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: False     Shape: Input: (N,C,L)(N, C, L)(N,C,L)   Output: (N,C,L)(N, C, L)(N,C,L)   (same shape as input)    Examples: &gt;&gt;&gt; # Without Learnable Parameters &gt;&gt;&gt; m = nn.InstanceNorm1d(100) &gt;&gt;&gt; # With Learnable Parameters &gt;&gt;&gt; m = nn.InstanceNorm1d(100, affine=True) &gt;&gt;&gt; input = torch.randn(20, 100, 40) &gt;&gt;&gt; output = m(input)   ", "parameters": ["num_features : CCC from an expected input of size(N,C,L)(N, C, L)(N,C,L) or LLL from input of size (N,L)(N, L)(N,L)", "eps : a value added to the denominator for numerical stability. Default: 1e-5", "momentum : the value used for the running_mean and running_var computation. Default: 0.1", "affine : a boolean value that when set to True, this module haslearnable affine parameters, initialized the same way as done for batch normalization.Default: False.", "track_running_stats : a boolean value that when set to True, thismodule tracks the running mean and variance, and when set to False,this module does not track such statistics and always uses batchstatistics in both training and eval modes. Default: False"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.InstanceNorm2d", "item_type": "class", "code": "classtorch.nn.InstanceNorm2d(num_features:int,eps:float=1e-05,momentum:float=0.1,affine:bool=False,track_running_stats:bool=False)", "description": "Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization.  y=x\u2212E[x]Var[x]+\u03f5\u2217\u03b3+\u03b2y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\betay=Var[x]+\u03f5\u200bx\u2212E[x]\u200b\u2217\u03b3+\u03b2  The mean and standard-deviation are calculated per-dimension separately for each object in a mini-batch. \u03b3\\gamma\u03b3   and \u03b2\\beta\u03b2   are learnable parameter vectors of size C (where C is the input size) if affine is True. The standard-deviation is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False). By default, this layer uses instance statistics computed from input data in both training and evaluation modes. If track_running_stats is set to True, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1.  Note This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is x^new=(1\u2212momentum)\u00d7x^+momemtum\u00d7xt\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momemtum} \\times x_tx^new\u200b=(1\u2212momentum)\u00d7x^+momemtum\u00d7xt\u200b  , where x^\\hat{x}x^   is the estimated statistic and xtx_txt\u200b   is the new observed value.   Note InstanceNorm2d and LayerNorm are very similar, but have some subtle differences. InstanceNorm2d is applied on each channel of channeled data like RGB images, but LayerNorm is usually applied on entire sample and often in NLP tasks. Additionally, LayerNorm applies elementwise affine transform, while InstanceNorm2d usually don\u2019t apply affine transform.   Parameters  num_features \u2013 CCC   from an expected input of size (N,C,H,W)(N, C, H, W)(N,C,H,W)   eps \u2013 a value added to the denominator for numerical stability. Default: 1e-5 momentum \u2013 the value used for the running_mean and running_var computation. Default: 0.1 affine \u2013 a boolean value that when set to True, this module has learnable affine parameters, initialized the same way as done for batch normalization. Default: False. track_running_stats \u2013 a boolean value that when set to True, this module tracks the running mean and variance, and when set to False, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: False     Shape: Input: (N,C,H,W)(N, C, H, W)(N,C,H,W)   Output: (N,C,H,W)(N, C, H, W)(N,C,H,W)   (same shape as input)    Examples: &gt;&gt;&gt; # Without Learnable Parameters &gt;&gt;&gt; m = nn.InstanceNorm2d(100) &gt;&gt;&gt; # With Learnable Parameters &gt;&gt;&gt; m = nn.InstanceNorm2d(100, affine=True) &gt;&gt;&gt; input = torch.randn(20, 100, 35, 45) &gt;&gt;&gt; output = m(input)   ", "parameters": ["num_features : CCC from an expected input of size(N,C,H,W)(N, C, H, W)(N,C,H,W)", "eps : a value added to the denominator for numerical stability. Default: 1e-5", "momentum : the value used for the running_mean and running_var computation. Default: 0.1", "affine : a boolean value that when set to True, this module haslearnable affine parameters, initialized the same way as done for batch normalization.Default: False.", "track_running_stats : a boolean value that when set to True, thismodule tracks the running mean and variance, and when set to False,this module does not track such statistics and always uses batchstatistics in both training and eval modes. Default: False"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.GroupNorm", "item_type": "class", "code": "classtorch.nn.GroupNorm(num_groups:int,num_channels:int,eps:float=1e-05,affine:bool=True)", "description": "Applies Group Normalization over a mini-batch of inputs as described in the paper Group Normalization  y=x\u2212E[x]Var[x]+\u03f5\u2217\u03b3+\u03b2y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta  y=Var[x]+\u03f5\u200bx\u2212E[x]\u200b\u2217\u03b3+\u03b2  The input channels are separated into num_groups groups, each containing num_channels / num_groups channels. The mean and standard-deviation are calculated separately over the each group. \u03b3\\gamma\u03b3   and \u03b2\\beta\u03b2   are learnable per-channel affine transform parameter vectors of size num_channels if affine is True. The standard-deviation is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False). This layer uses statistics computed from input data in both training and evaluation modes.  Parameters  num_groups (int) \u2013 number of groups to separate the channels into num_channels (int) \u2013 number of channels expected in input eps \u2013 a value added to the denominator for numerical stability. Default: 1e-5 affine \u2013 a boolean value that when set to True, this module has learnable per-channel affine parameters initialized to ones (for weights) and zeros (for biases). Default: True.     Shape: Input: (N,C,\u2217)(N, C, *)(N,C,\u2217)   where C=num_channelsC=\\text{num\\_channels}C=num_channels   Output: (N,C,\u2217)(N, C, *)(N,C,\u2217)   (same shape as input)    Examples: &gt;&gt;&gt; input = torch.randn(20, 6, 10, 10) &gt;&gt;&gt; # Separate 6 channels into 3 groups &gt;&gt;&gt; m = nn.GroupNorm(3, 6) &gt;&gt;&gt; # Separate 6 channels into 6 groups (equivalent with InstanceNorm) &gt;&gt;&gt; m = nn.GroupNorm(6, 6) &gt;&gt;&gt; # Put all 6 channels into a single group (equivalent with LayerNorm) &gt;&gt;&gt; m = nn.GroupNorm(1, 6) &gt;&gt;&gt; # Activating the module &gt;&gt;&gt; output = m(input)   ", "parameters": ["num_groups (int) : number of groups to separate the channels into", "num_channels (int) : number of channels expected in input", "eps : a value added to the denominator for numerical stability. Default: 1e-5", "affine : a boolean value that when set to True, this modulehas learnable per-channel affine parameters initialized to ones (for weights)and zeros (for biases). Default: True."], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.SyncBatchNorm.convert_sync_batchnorm", "item_type": "method", "code": "classmethodconvert_sync_batchnorm(module,process_group=None)", "description": "Helper function to convert all BatchNorm*D layers in the model to torch.nn.SyncBatchNorm layers.  Parameters  module (nn.Module) \u2013 module containing one or more attr:BatchNorm*D layers process_group (optional) \u2013 process group to scope synchronization, default is the whole world   Returns The original module with the converted torch.nn.SyncBatchNorm layers. If the original module is a BatchNorm*D layer, a new torch.nn.SyncBatchNorm layer object will be returned instead.   Example: &gt;&gt;&gt; # Network with nn.BatchNorm layer &gt;&gt;&gt; module = torch.nn.Sequential( &gt;&gt;&gt;            torch.nn.Linear(20, 100), &gt;&gt;&gt;            torch.nn.BatchNorm1d(100), &gt;&gt;&gt;          ).cuda() &gt;&gt;&gt; # creating process group (optional) &gt;&gt;&gt; # process_ids is a list of int identifying rank ids. &gt;&gt;&gt; process_group = torch.distributed.new_group(process_ids) &gt;&gt;&gt; sync_bn_module = torch.nn.SyncBatchNorm.convert_sync_batchnorm(module, process_group)   ", "parameters": ["module (nn.Module) : module containing one or more attr:BatchNorm*D layers", "process_group (optional) : process group to scope synchronization,default is the whole world"], "returns": "The original module with the converted torch.nn.SyncBatchNormlayers. If the original module is a BatchNorm*D layer,a new torch.nn.SyncBatchNorm layer object will be returnedinstead.", "example": " # Network with nn.BatchNorm layer\n module = torch.nn.Sequential(\n            torch.nn.Linear(20, 100),\n            torch.nn.BatchNorm1d(100),\n          ).cuda()\n # creating process group (optional)\n # process_ids is a list of int identifying rank ids.\n process_group = torch.distributed.new_group(process_ids)\n sync_bn_module = torch.nn.SyncBatchNorm.convert_sync_batchnorm(module, process_group)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.BatchNorm3d", "item_type": "class", "code": "classtorch.nn.BatchNorm3d(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)", "description": "Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .  y=x\u2212E[x]Var[x]+\u03f5\u2217\u03b3+\u03b2y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\betay=Var[x]+\u03f5\u200bx\u2212E[x]\u200b\u2217\u03b3+\u03b2  The mean and standard-deviation are calculated per-dimension over the mini-batches and \u03b3\\gamma\u03b3   and \u03b2\\beta\u03b2   are learnable parameter vectors of size C (where C is the input size). By default, the elements of \u03b3\\gamma\u03b3   are set to 1 and the elements of \u03b2\\beta\u03b2   are set to 0. The standard-deviation is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False). Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1. If track_running_stats is set to False, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well.  Note This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is x^new=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_tx^new\u200b=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\u200b  , where x^\\hat{x}x^   is the estimated statistic and xtx_txt\u200b   is the new observed value.  Because the Batch Normalization is done over the C dimension, computing statistics on (N, D, H, W) slices, it\u2019s common terminology to call this Volumetric Batch Normalization or Spatio-temporal Batch Normalization.  Parameters  num_features \u2013 CCC   from an expected input of size (N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W)   eps \u2013 a value added to the denominator for numerical stability. Default: 1e-5 momentum \u2013 the value used for the running_mean and running_var computation. Can be set to None for cumulative moving average (i.e. simple average). Default: 0.1 affine \u2013 a boolean value that when set to True, this module has learnable affine parameters. Default: True track_running_stats \u2013 a boolean value that when set to True, this module tracks the running mean and variance, and when set to False, this module does not track such statistics and uses batch statistics instead in both training and eval modes if the running mean and variance are None. Default: True     Shape: Input: (N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W)   Output: (N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W)   (same shape as input)    Examples: &gt;&gt;&gt; # With Learnable Parameters &gt;&gt;&gt; m = nn.BatchNorm3d(100) &gt;&gt;&gt; # Without Learnable Parameters &gt;&gt;&gt; m = nn.BatchNorm3d(100, affine=False) &gt;&gt;&gt; input = torch.randn(20, 100, 35, 45, 10) &gt;&gt;&gt; output = m(input)   ", "parameters": ["num_features : CCC from an expected input of size(N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W)", "eps : a value added to the denominator for numerical stability.Default: 1e-5", "momentum : the value used for the running_mean and running_varcomputation. Can be set to None for cumulative moving average(i.e. simple average). Default: 0.1", "affine : a boolean value that when set to True, this module haslearnable affine parameters. Default: True", "track_running_stats : a boolean value that when set to True, thismodule tracks the running mean and variance, and when set to False,this module does not track such statistics and uses batch statistics insteadin both training and eval modes if the running mean and variance are None. Default: True"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.BatchNorm1d", "item_type": "class", "code": "classtorch.nn.BatchNorm1d(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)", "description": "Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .  y=x\u2212E[x]Var[x]+\u03f5\u2217\u03b3+\u03b2y = \\frac{x - \\mathrm{E}[x]}{\\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\betay=Var[x]+\u03f5\u200bx\u2212E[x]\u200b\u2217\u03b3+\u03b2  The mean and standard-deviation are calculated per-dimension over the mini-batches and \u03b3\\gamma\u03b3   and \u03b2\\beta\u03b2   are learnable parameter vectors of size C (where C is the input size). By default, the elements of \u03b3\\gamma\u03b3   are set to 1 and the elements of \u03b2\\beta\u03b2   are set to 0. The standard-deviation is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False). Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1. If track_running_stats is set to False, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well.  Note This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is x^new=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_tx^new\u200b=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\u200b  , where x^\\hat{x}x^   is the estimated statistic and xtx_txt\u200b   is the new observed value.  Because the Batch Normalization is done over the C dimension, computing statistics on (N, L) slices, it\u2019s common terminology to call this Temporal Batch Normalization.  Parameters  num_features \u2013 CCC   from an expected input of size (N,C,L)(N, C, L)(N,C,L)   or LLL   from input of size (N,L)(N, L)(N,L)   eps \u2013 a value added to the denominator for numerical stability. Default: 1e-5 momentum \u2013 the value used for the running_mean and running_var computation. Can be set to None for cumulative moving average (i.e. simple average). Default: 0.1 affine \u2013 a boolean value that when set to True, this module has learnable affine parameters. Default: True track_running_stats \u2013 a boolean value that when set to True, this module tracks the running mean and variance, and when set to False, this module does not track such statistics and uses batch statistics instead in both training and eval modes if the running mean and variance are None. Default: True     Shape: Input: (N,C)(N, C)(N,C)   or (N,C,L)(N, C, L)(N,C,L)   Output: (N,C)(N, C)(N,C)   or (N,C,L)(N, C, L)(N,C,L)   (same shape as input)    Examples: &gt;&gt;&gt; # With Learnable Parameters &gt;&gt;&gt; m = nn.BatchNorm1d(100) &gt;&gt;&gt; # Without Learnable Parameters &gt;&gt;&gt; m = nn.BatchNorm1d(100, affine=False) &gt;&gt;&gt; input = torch.randn(20, 100) &gt;&gt;&gt; output = m(input)   ", "parameters": ["num_features : CCC from an expected input of size(N,C,L)(N, C, L)(N,C,L) or LLL from input of size (N,L)(N, L)(N,L)", "eps : a value added to the denominator for numerical stability.Default: 1e-5", "momentum : the value used for the running_mean and running_varcomputation. Can be set to None for cumulative moving average(i.e. simple average). Default: 0.1", "affine : a boolean value that when set to True, this module haslearnable affine parameters. Default: True", "track_running_stats : a boolean value that when set to True, thismodule tracks the running mean and variance, and when set to False,this module does not track such statistics and uses batch statistics insteadin both training and eval modes if the running mean and variance are None. Default: True"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.AdaptiveLogSoftmaxWithLoss.log_prob", "item_type": "method", "code": "log_prob(input:torch.Tensor)\u2192torch.Tensor", "description": "Computes log probabilities for all n_classes\\texttt{n\\_classes}n_classes    Parameters input (Tensor) \u2013 a minibatch of examples  Returns log-probabilities of for each class ccc   in range 0&lt;=c&lt;=n_classes0 &lt;= c &lt;= \\texttt{n\\_classes}0&lt;=c&lt;=n_classes  , where n_classes\\texttt{n\\_classes}n_classes   is a parameter passed to AdaptiveLogSoftmaxWithLoss constructor.    Shape: Input: (N,in_features)(N, \\texttt{in\\_features})(N,in_features)   Output: (N,n_classes)(N, \\texttt{n\\_classes})(N,n_classes)      ", "parameters": ["input (Tensor) : a minibatch of examples", "log-probabilities of for each class cccin range 0&lt;=c&lt;=n_classes0 &lt;= c &lt;= \\texttt{n\\_classes}0&lt;=c&lt;=n_classes, where n_classes\\texttt{n\\_classes}n_classes is aparameter passed to AdaptiveLogSoftmaxWithLoss constructor."], "returns": "output is a Tensor of size N containing computed targetlog probabilities for each exampleloss is a Scalar representing the computed negativelog likelihood loss", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.AdaptiveLogSoftmaxWithLoss.predict", "item_type": "method", "code": "predict(input:torch.Tensor)\u2192torch.Tensor", "description": "This is equivalent to self.log_pob(input).argmax(dim=1), but is more efficient in some cases.  Parameters input (Tensor) \u2013 a minibatch of examples  Returns a class with the highest probability for each example  Return type output (Tensor)    Shape: Input: (N,in_features)(N, \\texttt{in\\_features})(N,in_features)   Output: (N)(N)(N)      ", "parameters": ["input (Tensor) : a minibatch of examples", "a class with the highest probability for each example", "output (Tensor)"], "returns": "output is a Tensor of size N containing computed targetlog probabilities for each exampleloss is a Scalar representing the computed negativelog likelihood loss", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.AdaptiveLogSoftmaxWithLoss", "item_type": "class", "code": "classtorch.nn.AdaptiveLogSoftmaxWithLoss(in_features:int,n_classes:int,cutoffs:Sequence[int],div_value:float=4.0,head_bias:bool=False)", "description": "Efficient softmax approximation as described in Efficient softmax approximation for GPUs by Edouard Grave, Armand Joulin, Moustapha Ciss\u00e9, David Grangier, and Herv\u00e9 J\u00e9gou. Adaptive softmax is an approximate strategy for training models with large output spaces. It is most effective when the label distribution is highly imbalanced, for example in natural language modelling, where the word frequency distribution approximately follows the Zipf\u2019s law. Adaptive softmax partitions the labels into several clusters, according to their frequency. These clusters may contain different number of targets each. Additionally, clusters containing less frequent labels assign lower dimensional embeddings to those labels, which speeds up the computation. For each minibatch, only clusters for which at least one target is present are evaluated. The idea is that the clusters which are accessed frequently (like the first one, containing most frequent labels), should also be cheap to compute \u2013 that is, contain a small number of assigned labels. We highly recommend taking a look at the original paper for more details.  cutoffs should be an ordered Sequence of integers sorted in the increasing order. It controls number of clusters and the partitioning of targets into clusters. For example setting cutoffs = [10, 100, 1000] means that first 10 targets will be assigned to the \u2018head\u2019 of the adaptive softmax, targets 11, 12, \u2026, 100 will be assigned to the first cluster, and targets 101, 102, \u2026, 1000 will be assigned to the second cluster, while targets 1001, 1002, \u2026, n_classes - 1 will be assigned to the last, third cluster. div_value is used to compute the size of each additional cluster, which is given as \u230ain_featuresdiv_valueidx\u230b\\left\\lfloor\\frac{\\texttt{in\\_features}}{\\texttt{div\\_value}^{idx}}\\right\\rfloor\u230adiv_valueidxin_features\u200b\u230b  , where idxidxidx   is the cluster index (with clusters for less frequent words having larger indices, and indices starting from 111  ). head_bias if set to True, adds a bias term to the \u2018head\u2019 of the adaptive softmax. See paper for details. Set to False in the official implementation.   Warning Labels passed as inputs to this module should be sorted according to their frequency. This means that the most frequent label should be represented by the index 0, and the least frequent label should be represented by the index n_classes - 1.   Note This module returns a NamedTuple with output and loss fields. See further documentation for details.   Note To compute log-probabilities for all classes, the log_prob method can be used.   Parameters  in_features (int) \u2013 Number of features in the input tensor n_classes (int) \u2013 Number of classes in the dataset cutoffs (Sequence) \u2013 Cutoffs used to assign targets to their buckets div_value (float, optional) \u2013 value used as an exponent to compute sizes of the clusters. Default: 4.0 head_bias (bool, optional) \u2013 If True, adds a bias term to the \u2018head\u2019 of the adaptive softmax. Default: False   Returns  output is a Tensor of size N containing computed target log probabilities for each example loss is a Scalar representing the computed negative log likelihood loss    Return type NamedTuple with output and loss fields    Shape: input: (N,in_features)(N, \\texttt{in\\_features})(N,in_features)   target: (N)(N)(N)   where each value satisfies 0&lt;=target[i]&lt;=n_classes0 &lt;= \\texttt{target[i]} &lt;= \\texttt{n\\_classes}0&lt;=target[i]&lt;=n_classes   output1: (N)(N)(N)   output2: Scalar      log_prob(input: torch.Tensor) \u2192 torch.Tensor  Computes log probabilities for all n_classes\\texttt{n\\_classes}n_classes    Parameters input (Tensor) \u2013 a minibatch of examples  Returns log-probabilities of for each class ccc   in range 0&lt;=c&lt;=n_classes0 &lt;= c &lt;= \\texttt{n\\_classes}0&lt;=c&lt;=n_classes  , where n_classes\\texttt{n\\_classes}n_classes   is a parameter passed to AdaptiveLogSoftmaxWithLoss constructor.    Shape: Input: (N,in_features)(N, \\texttt{in\\_features})(N,in_features)   Output: (N,n_classes)(N, \\texttt{n\\_classes})(N,n_classes)          predict(input: torch.Tensor) \u2192 torch.Tensor  This is equivalent to self.log_pob(input).argmax(dim=1), but is more efficient in some cases.  Parameters input (Tensor) \u2013 a minibatch of examples  Returns a class with the highest probability for each example  Return type output (Tensor)    Shape: Input: (N,in_features)(N, \\texttt{in\\_features})(N,in_features)   Output: (N)(N)(N)        ", "parameters": ["in_features (int) : Number of features in the input tensor", "n_classes (int) : Number of classes in the dataset", "cutoffs (Sequence) : Cutoffs used to assign targets to their buckets", "div_value (float, optional) : value used as an exponent to compute sizesof the clusters. Default: 4.0", "head_bias (bool, optional) : If True, adds a bias term to the \u2018head\u2019 of theadaptive softmax. Default: False", "output is a Tensor of size N containing computed targetlog probabilities for each example", "loss is a Scalar representing the computed negativelog likelihood loss"], "returns": "output is a Tensor of size N containing computed targetlog probabilities for each exampleloss is a Scalar representing the computed negativelog likelihood loss", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.BatchNorm2d", "item_type": "class", "code": "classtorch.nn.BatchNorm2d(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)", "description": "Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .  y=x\u2212E[x]Var[x]+\u03f5\u2217\u03b3+\u03b2y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\betay=Var[x]+\u03f5\u200bx\u2212E[x]\u200b\u2217\u03b3+\u03b2  The mean and standard-deviation are calculated per-dimension over the mini-batches and \u03b3\\gamma\u03b3   and \u03b2\\beta\u03b2   are learnable parameter vectors of size C (where C is the input size). By default, the elements of \u03b3\\gamma\u03b3   are set to 1 and the elements of \u03b2\\beta\u03b2   are set to 0. The standard-deviation is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False). Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1. If track_running_stats is set to False, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well.  Note This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is x^new=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_tx^new\u200b=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\u200b  , where x^\\hat{x}x^   is the estimated statistic and xtx_txt\u200b   is the new observed value.  Because the Batch Normalization is done over the C dimension, computing statistics on (N, H, W) slices, it\u2019s common terminology to call this Spatial Batch Normalization.  Parameters  num_features \u2013 CCC   from an expected input of size (N,C,H,W)(N, C, H, W)(N,C,H,W)   eps \u2013 a value added to the denominator for numerical stability. Default: 1e-5 momentum \u2013 the value used for the running_mean and running_var computation. Can be set to None for cumulative moving average (i.e. simple average). Default: 0.1 affine \u2013 a boolean value that when set to True, this module has learnable affine parameters. Default: True track_running_stats \u2013 a boolean value that when set to True, this module tracks the running mean and variance, and when set to False, this module does not track such statistics and uses batch statistics instead in both training and eval modes if the running mean and variance are None. Default: True     Shape: Input: (N,C,H,W)(N, C, H, W)(N,C,H,W)   Output: (N,C,H,W)(N, C, H, W)(N,C,H,W)   (same shape as input)    Examples: &gt;&gt;&gt; # With Learnable Parameters &gt;&gt;&gt; m = nn.BatchNorm2d(100) &gt;&gt;&gt; # Without Learnable Parameters &gt;&gt;&gt; m = nn.BatchNorm2d(100, affine=False) &gt;&gt;&gt; input = torch.randn(20, 100, 35, 45) &gt;&gt;&gt; output = m(input)   ", "parameters": ["num_features : CCC from an expected input of size(N,C,H,W)(N, C, H, W)(N,C,H,W)", "eps : a value added to the denominator for numerical stability.Default: 1e-5", "momentum : the value used for the running_mean and running_varcomputation. Can be set to None for cumulative moving average(i.e. simple average). Default: 0.1", "affine : a boolean value that when set to True, this module haslearnable affine parameters. Default: True", "track_running_stats : a boolean value that when set to True, thismodule tracks the running mean and variance, and when set to False,this module does not track such statistics and uses batch statistics insteadin both training and eval modes if the running mean and variance are None. Default: True"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.SyncBatchNorm", "item_type": "class", "code": "classtorch.nn.SyncBatchNorm(num_features:int,eps:float=1e-05,momentum:float=0.1,affine:bool=True,track_running_stats:bool=True,process_group:Optional[Any]=None)", "description": "Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .  y=x\u2212E[x]Var[x]+\u03f5\u2217\u03b3+\u03b2y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\betay=Var[x]+\u03f5\u200bx\u2212E[x]\u200b\u2217\u03b3+\u03b2  The mean and standard-deviation are calculated per-dimension over all mini-batches of the same process groups. \u03b3\\gamma\u03b3   and \u03b2\\beta\u03b2   are learnable parameter vectors of size C (where C is the input size). By default, the elements of \u03b3\\gamma\u03b3   are sampled from U(0,1)\\mathcal{U}(0, 1)U(0,1)   and the elements of \u03b2\\beta\u03b2   are set to 0. The standard-deviation is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False). Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1. If track_running_stats is set to False, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well.  Note This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is x^new=(1\u2212momentum)\u00d7x^+momemtum\u00d7xt\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momemtum} \\times x_tx^new\u200b=(1\u2212momentum)\u00d7x^+momemtum\u00d7xt\u200b  , where x^\\hat{x}x^   is the estimated statistic and xtx_txt\u200b   is the new observed value.  Because the Batch Normalization is done for each channel in the C dimension, computing statistics on (N, +) slices, it\u2019s common terminology to call this Volumetric Batch Normalization or Spatio-temporal Batch Normalization. Currently SyncBatchNorm only supports DistributedDataParallel (DDP) with single GPU per process. Use torch.nn.SyncBatchNorm.convert_sync_batchnorm() to convert BatchNorm*D layer to SyncBatchNorm before wrapping Network with DDP.  Parameters  num_features \u2013 CCC   from an expected input of size (N,C,+)(N, C, +)(N,C,+)   eps \u2013 a value added to the denominator for numerical stability. Default: 1e-5 momentum \u2013 the value used for the running_mean and running_var computation. Can be set to None for cumulative moving average (i.e. simple average). Default: 0.1 affine \u2013 a boolean value that when set to True, this module has learnable affine parameters. Default: True track_running_stats \u2013 a boolean value that when set to True, this module tracks the running mean and variance, and when set to False, this module does not track such statistics and uses batch statistics instead in both training and eval modes if the running mean and variance are None. Default: True process_group \u2013 synchronization of stats happen within each process group individually. Default behavior is synchronization across the whole world     Shape: Input: (N,C,+)(N, C, +)(N,C,+)   Output: (N,C,+)(N, C, +)(N,C,+)   (same shape as input)    Examples: &gt;&gt;&gt; # With Learnable Parameters &gt;&gt;&gt; m = nn.SyncBatchNorm(100) &gt;&gt;&gt; # creating process group (optional) &gt;&gt;&gt; # process_ids is a list of int identifying rank ids. &gt;&gt;&gt; process_group = torch.distributed.new_group(process_ids) &gt;&gt;&gt; # Without Learnable Parameters &gt;&gt;&gt; m = nn.BatchNorm3d(100, affine=False, process_group=process_group) &gt;&gt;&gt; input = torch.randn(20, 100, 35, 45, 10) &gt;&gt;&gt; output = m(input)  &gt;&gt;&gt; # network is nn.BatchNorm layer &gt;&gt;&gt; sync_bn_network = nn.SyncBatchNorm.convert_sync_batchnorm(network, process_group) &gt;&gt;&gt; # only single gpu per process is currently supported &gt;&gt;&gt; ddp_sync_bn_network = torch.nn.parallel.DistributedDataParallel( &gt;&gt;&gt;                         sync_bn_network, &gt;&gt;&gt;                         device_ids=[args.local_rank], &gt;&gt;&gt;                         output_device=args.local_rank)     classmethod convert_sync_batchnorm(module, process_group=None)  Helper function to convert all BatchNorm*D layers in the model to torch.nn.SyncBatchNorm layers.  Parameters  module (nn.Module) \u2013 module containing one or more attr:BatchNorm*D layers process_group (optional) \u2013 process group to scope synchronization, default is the whole world   Returns The original module with the converted torch.nn.SyncBatchNorm layers. If the original module is a BatchNorm*D layer, a new torch.nn.SyncBatchNorm layer object will be returned instead.   Example: &gt;&gt;&gt; # Network with nn.BatchNorm layer &gt;&gt;&gt; module = torch.nn.Sequential( &gt;&gt;&gt;            torch.nn.Linear(20, 100), &gt;&gt;&gt;            torch.nn.BatchNorm1d(100), &gt;&gt;&gt;          ).cuda() &gt;&gt;&gt; # creating process group (optional) &gt;&gt;&gt; # process_ids is a list of int identifying rank ids. &gt;&gt;&gt; process_group = torch.distributed.new_group(process_ids) &gt;&gt;&gt; sync_bn_module = torch.nn.SyncBatchNorm.convert_sync_batchnorm(module, process_group)     ", "parameters": ["num_features : CCC from an expected input of size(N,C,+)(N, C, +)(N,C,+)", "eps : a value added to the denominator for numerical stability.Default: 1e-5", "momentum : the value used for the running_mean and running_varcomputation. Can be set to None for cumulative moving average(i.e. simple average). Default: 0.1", "affine : a boolean value that when set to True, this module haslearnable affine parameters. Default: True", "track_running_stats : a boolean value that when set to True, thismodule tracks the running mean and variance, and when set to False,this module does not track such statistics and uses batch statistics insteadin both training and eval modes if the running mean and variance are None. Default: True", "process_group : synchronization of stats happen within each process groupindividually. Default behavior is synchronization across the wholeworld", "module (nn.Module) : module containing one or more attr:BatchNorm*D layers", "process_group (optional) : process group to scope synchronization,default is the whole world"], "returns": "The original module with the converted torch.nn.SyncBatchNormlayers. If the original module is a BatchNorm*D layer,a new torch.nn.SyncBatchNorm layer object will be returnedinstead.", "example": " # Network with nn.BatchNorm layer\n module = torch.nn.Sequential(\n            torch.nn.Linear(20, 100),\n            torch.nn.BatchNorm1d(100),\n          ).cuda()\n # creating process group (optional)\n # process_ids is a list of int identifying rank ids.\n process_group = torch.distributed.new_group(process_ids)\n sync_bn_module = torch.nn.SyncBatchNorm.convert_sync_batchnorm(module, process_group)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Softmax2d", "item_type": "class", "code": "classtorch.nn.Softmax2d", "description": "Applies SoftMax over features to each spatial location. When given an image of Channels x Height x Width, it will apply Softmax to each location (Channels,hi,wj)(Channels, h_i, w_j)(Channels,hi\u200b,wj\u200b)    Shape: Input: (N,C,H,W)(N, C, H, W)(N,C,H,W)   Output: (N,C,H,W)(N, C, H, W)(N,C,H,W)   (same shape as input)     Returns a Tensor of the same dimension and shape as the input with values in the range [0, 1]   Examples: &gt;&gt;&gt; m = nn.Softmax2d() &gt;&gt;&gt; # you softmax over the 2nd dimension &gt;&gt;&gt; input = torch.randn(2, 3, 12, 13) &gt;&gt;&gt; output = m(input)   ", "parameters": [], "returns": "a Tensor of the same dimension and shape as the input withvalues in the range [0, 1]", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.LogSoftmax", "item_type": "class", "code": "classtorch.nn.LogSoftmax(dim:Optional[int]=None)", "description": "Applies the log\u2061(Softmax(x))\\log(\\text{Softmax}(x))log(Softmax(x))   function to an n-dimensional input Tensor. The LogSoftmax formulation can be simplified as:  LogSoftmax(xi)=log\u2061(exp\u2061(xi)\u2211jexp\u2061(xj))\\text{LogSoftmax}(x_{i}) = \\log\\left(\\frac{\\exp(x_i) }{ \\sum_j \\exp(x_j)} \\right)  LogSoftmax(xi\u200b)=log(\u2211j\u200bexp(xj\u200b)exp(xi\u200b)\u200b)   Shape: Input: (\u2217)(*)(\u2217)   where * means, any number of additional dimensions Output: (\u2217)(*)(\u2217)  , same shape as the input     Parameters dim (int) \u2013 A dimension along which LogSoftmax will be computed.  Returns a Tensor of the same dimension and shape as the input with values in the range [-inf, 0)   Examples: &gt;&gt;&gt; m = nn.LogSoftmax() &gt;&gt;&gt; input = torch.randn(2, 3) &gt;&gt;&gt; output = m(input)   ", "parameters": ["dim (int) : A dimension along which LogSoftmax will be computed.", "a Tensor of the same dimension and shape as the input withvalues in the range [-inf, 0)"], "returns": "a Tensor of the same dimension and shape as the input withvalues in the range [-inf, 0)", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Softmin", "item_type": "class", "code": "classtorch.nn.Softmin(dim:Optional[int]=None)", "description": "Applies the Softmin function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0, 1] and sum to 1. Softmin is defined as:  Softmin(xi)=exp\u2061(\u2212xi)\u2211jexp\u2061(\u2212xj)\\text{Softmin}(x_{i}) = \\frac{\\exp(-x_i)}{\\sum_j \\exp(-x_j)}  Softmin(xi\u200b)=\u2211j\u200bexp(\u2212xj\u200b)exp(\u2212xi\u200b)\u200b   Shape: Input: (\u2217)(*)(\u2217)   where * means, any number of additional dimensions Output: (\u2217)(*)(\u2217)  , same shape as the input     Parameters dim (int) \u2013 A dimension along which Softmin will be computed (so every slice along dim will sum to 1).  Returns a Tensor of the same dimension and shape as the input, with values in the range [0, 1]   Examples: &gt;&gt;&gt; m = nn.Softmin() &gt;&gt;&gt; input = torch.randn(2, 3) &gt;&gt;&gt; output = m(input)   ", "parameters": ["dim (int) : A dimension along which Softmin will be computed (so every slicealong dim will sum to 1).", "a Tensor of the same dimension and shape as the input, withvalues in the range [0, 1]"], "returns": "a Tensor of the same dimension and shape as the input, withvalues in the range [0, 1]", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Threshold", "item_type": "class", "code": "classtorch.nn.Threshold(threshold:float,value:float,inplace:bool=False)", "description": "Thresholds each element of the input Tensor. Threshold is defined as:  y={x,\u00a0if\u00a0x&gt;thresholdvalue,\u00a0otherwise\u00a0y = \\begin{cases} x, &amp;\\text{ if } x &gt; \\text{threshold} \\\\ \\text{value}, &amp;\\text{ otherwise } \\end{cases}  y={x,value,\u200b\u00a0if\u00a0x&gt;threshold\u00a0otherwise\u00a0\u200b   Parameters  threshold \u2013 The value to threshold at value \u2013 The value to replace with inplace \u2013 can optionally do the operation in-place. Default: False     Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input    Examples: &gt;&gt;&gt; m = nn.Threshold(0.1, 20) &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; output = m(input)   ", "parameters": ["threshold : The value to threshold at", "value : The value to replace with", "inplace : can optionally do the operation in-place. Default: False"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Softmax", "item_type": "class", "code": "classtorch.nn.Softmax(dim:Optional[int]=None)", "description": "Applies the Softmax function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0,1] and sum to 1. Softmax is defined as:  Softmax(xi)=exp\u2061(xi)\u2211jexp\u2061(xj)\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}  Softmax(xi\u200b)=\u2211j\u200bexp(xj\u200b)exp(xi\u200b)\u200b  When the input Tensor is a sparse tensor then the unspecifed values are treated as -inf.  Shape: Input: (\u2217)(*)(\u2217)   where * means, any number of additional dimensions Output: (\u2217)(*)(\u2217)  , same shape as the input     Returns a Tensor of the same dimension and shape as the input with values in the range [0, 1]  Parameters dim (int) \u2013 A dimension along which Softmax will be computed (so every slice along dim will sum to 1).    Note This module doesn\u2019t work directly with NLLLoss, which expects the Log to be computed between the Softmax and itself. Use LogSoftmax instead (it\u2019s faster and has better numerical properties).  Examples: &gt;&gt;&gt; m = nn.Softmax(dim=1) &gt;&gt;&gt; input = torch.randn(2, 3) &gt;&gt;&gt; output = m(input)   ", "parameters": ["a Tensor of the same dimension and shape as the input withvalues in the range [0, 1]", "dim (int) : A dimension along which Softmax will be computed (so every slicealong dim will sum to 1)."], "returns": "a Tensor of the same dimension and shape as the input withvalues in the range [0, 1]", "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Tanhshrink", "item_type": "class", "code": "classtorch.nn.Tanhshrink", "description": "Applies the element-wise function:  Tanhshrink(x)=x\u2212tanh\u2061(x)\\text{Tanhshrink}(x) = x - \\tanh(x)  Tanhshrink(x)=x\u2212tanh(x)   Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input     Examples: &gt;&gt;&gt; m = nn.Tanhshrink() &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; output = m(input)   ", "parameters": [], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Tanh", "item_type": "class", "code": "classtorch.nn.Tanh", "description": "Applies the element-wise function:  Tanh(x)=tanh\u2061(x)=exp\u2061(x)\u2212exp\u2061(\u2212x)exp\u2061(x)+exp\u2061(\u2212x)\\text{Tanh}(x) = \\tanh(x) = \\frac{\\exp(x) - \\exp(-x)} {\\exp(x) + \\exp(-x)}  Tanh(x)=tanh(x)=exp(x)+exp(\u2212x)exp(x)\u2212exp(\u2212x)\u200b   Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input     Examples: &gt;&gt;&gt; m = nn.Tanh() &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; output = m(input)   ", "parameters": [], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Softsign", "item_type": "class", "code": "classtorch.nn.Softsign", "description": "Applies the element-wise function:  SoftSign(x)=x1+\u2223x\u2223\\text{SoftSign}(x) = \\frac{x}{ 1 + |x|}  SoftSign(x)=1+\u2223x\u2223x\u200b   Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input     Examples: &gt;&gt;&gt; m = nn.Softsign() &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; output = m(input)   ", "parameters": [], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Softplus", "item_type": "class", "code": "classtorch.nn.Softplus(beta:int=1,threshold:int=20)", "description": "Applies the element-wise function:  Softplus(x)=1\u03b2\u2217log\u2061(1+exp\u2061(\u03b2\u2217x))\\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x))  Softplus(x)=\u03b21\u200b\u2217log(1+exp(\u03b2\u2217x))  SoftPlus is a smooth approximation to the ReLU function and can be used to constrain the output of a machine to always be positive. For numerical stability the implementation reverts to the linear function when input\u00d7\u03b2&gt;thresholdinput \\times \\beta &gt; thresholdinput\u00d7\u03b2&gt;threshold  .  Parameters  beta \u2013 the \u03b2\\beta\u03b2   value for the Softplus formulation. Default: 1 threshold \u2013 values above this revert to a linear function. Default: 20     Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input     Examples: &gt;&gt;&gt; m = nn.Softplus() &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; output = m(input)   ", "parameters": ["beta : the \u03b2\\beta\u03b2 value for the Softplus formulation. Default: 1", "threshold : values above this revert to a linear function. Default: 20"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Softshrink", "item_type": "class", "code": "classtorch.nn.Softshrink(lambd:float=0.5)", "description": "Applies the soft shrinkage function elementwise:  SoftShrinkage(x)={x\u2212\u03bb,\u00a0if\u00a0x&gt;\u03bbx+\u03bb,\u00a0if\u00a0x&lt;\u2212\u03bb0,\u00a0otherwise\u00a0\\text{SoftShrinkage}(x) = \\begin{cases} x - \\lambda, &amp; \\text{ if } x &gt; \\lambda \\\\ x + \\lambda, &amp; \\text{ if } x &lt; -\\lambda \\\\ 0, &amp; \\text{ otherwise } \\end{cases}  SoftShrinkage(x)=\u23a9\u23aa\u23aa\u23a8\u23aa\u23aa\u23a7\u200bx\u2212\u03bb,x+\u03bb,0,\u200b\u00a0if\u00a0x&gt;\u03bb\u00a0if\u00a0x&lt;\u2212\u03bb\u00a0otherwise\u00a0\u200b   Parameters lambd \u2013 the \u03bb\\lambda\u03bb   (must be no less than zero) value for the Softshrink formulation. Default: 0.5    Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input     Examples: &gt;&gt;&gt; m = nn.Softshrink() &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; output = m(input)   ", "parameters": ["lambd : the \u03bb\\lambda\u03bb (must be no less than zero) value for the Softshrink formulation. Default: 0.5"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Sigmoid", "item_type": "class", "code": "classtorch.nn.Sigmoid", "description": "Applies the element-wise function:  Sigmoid(x)=\u03c3(x)=11+exp\u2061(\u2212x)\\text{Sigmoid}(x) = \\sigma(x) = \\frac{1}{1 + \\exp(-x)}  Sigmoid(x)=\u03c3(x)=1+exp(\u2212x)1\u200b   Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input     Examples: &gt;&gt;&gt; m = nn.Sigmoid() &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; output = m(input)   ", "parameters": [], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.SiLU", "item_type": "class", "code": "classtorch.nn.SiLU(inplace:bool=False)", "description": "Applies the silu function, element-wise.  silu(x)=x\u2217\u03c3(x),where\u00a0\u03c3(x)\u00a0is\u00a0the\u00a0logistic\u00a0sigmoid.\\text{silu}(x) = x * \\sigma(x), \\text{where } \\sigma(x) \\text{ is the logistic sigmoid.}  silu(x)=x\u2217\u03c3(x),where\u00a0\u03c3(x)\u00a0is\u00a0the\u00a0logistic\u00a0sigmoid.   Note See Gaussian Error Linear Units (GELUs) where the SiLU (Sigmoid Linear Unit) was originally coined, and see Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning and Swish: a Self-Gated Activation Function where the SiLU was experimented with later.   Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input    Examples: &gt;&gt;&gt; m = nn.SiLU() &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; output = m(input)   ", "parameters": [], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.CELU", "item_type": "class", "code": "classtorch.nn.CELU(alpha:float=1.0,inplace:bool=False)", "description": "Applies the element-wise function:  CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))  CELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x/\u03b1)\u22121))  More details can be found in the paper Continuously Differentiable Exponential Linear Units .  Parameters  alpha \u2013 the \u03b1\\alpha\u03b1   value for the CELU formulation. Default: 1.0 inplace \u2013 can optionally do the operation in-place. Default: False     Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input     Examples: &gt;&gt;&gt; m = nn.CELU() &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; output = m(input)   ", "parameters": ["alpha : the \u03b1\\alpha\u03b1 value for the CELU formulation. Default: 1.0", "inplace : can optionally do the operation in-place. Default: False"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.GELU", "item_type": "class", "code": "classtorch.nn.GELU", "description": "Applies the Gaussian Error Linear Units function:  GELU(x)=x\u2217\u03a6(x)\\text{GELU}(x) = x * \\Phi(x)  GELU(x)=x\u2217\u03a6(x)  where \u03a6(x)\\Phi(x)\u03a6(x)   is the Cumulative Distribution Function for Gaussian Distribution.  Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input     Examples: &gt;&gt;&gt; m = nn.GELU() &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; output = m(input)   ", "parameters": [], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.SELU", "item_type": "class", "code": "classtorch.nn.SELU(inplace:bool=False)", "description": "Applied element-wise, as:  SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = \\text{scale} * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))  SELU(x)=scale\u2217(max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)))  with \u03b1=1.6732632423543772848170429916717\\alpha = 1.6732632423543772848170429916717\u03b1=1.6732632423543772848170429916717   and scale=1.0507009873554804934193349852946\\text{scale} = 1.0507009873554804934193349852946scale=1.0507009873554804934193349852946  . More details can be found in the paper Self-Normalizing Neural Networks .  Parameters inplace (bool, optional) \u2013 can optionally do the operation in-place. Default: False    Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input     Examples: &gt;&gt;&gt; m = nn.SELU() &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; output = m(input)   ", "parameters": ["inplace (bool, optional) : can optionally do the operation in-place. Default: False"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.RReLU", "item_type": "class", "code": "classtorch.nn.RReLU(lower:float=0.125,upper:float=0.3333333333333333,inplace:bool=False)", "description": "Applies the randomized leaky rectified liner unit function, element-wise, as described in the paper: Empirical Evaluation of Rectified Activations in Convolutional Network. The function is defined as:  RReLU(x)={xif\u00a0x\u22650ax\u00a0otherwise\u00a0\\text{RReLU}(x) = \\begin{cases}     x &amp; \\text{if } x \\geq 0 \\\\     ax &amp; \\text{ otherwise } \\end{cases}  RReLU(x)={xax\u200bif\u00a0x\u22650\u00a0otherwise\u00a0\u200b  where aaa   is randomly sampled from uniform distribution U(lower,upper)\\mathcal{U}(\\text{lower}, \\text{upper})U(lower,upper)  .  See: https://arxiv.org/pdf/1505.00853.pdf   Parameters  lower \u2013 lower bound of the uniform distribution. Default: 18\\frac{1}{8}81\u200b   upper \u2013 upper bound of the uniform distribution. Default: 13\\frac{1}{3}31\u200b   inplace \u2013 can optionally do the operation in-place. Default: False     Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input    Examples: &gt;&gt;&gt; m = nn.RReLU(0.1, 0.3) &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; output = m(input)   ", "parameters": ["lower : lower bound of the uniform distribution. Default: 18\\frac{1}{8}81\u200b", "upper : upper bound of the uniform distribution. Default: 13\\frac{1}{3}31\u200b", "inplace : can optionally do the operation in-place. Default: False"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.ReLU6", "item_type": "class", "code": "classtorch.nn.ReLU6(inplace:bool=False)", "description": "Applies the element-wise function:  ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)  ReLU6(x)=min(max(0,x),6)   Parameters inplace \u2013 can optionally do the operation in-place. Default: False    Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input     Examples: &gt;&gt;&gt; m = nn.ReLU6() &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; output = m(input)   ", "parameters": ["inplace : can optionally do the operation in-place. Default: False"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.LogSigmoid", "item_type": "class", "code": "classtorch.nn.LogSigmoid", "description": "Applies the element-wise function:  LogSigmoid(x)=log\u2061(11+exp\u2061(\u2212x))\\text{LogSigmoid}(x) = \\log\\left(\\frac{ 1 }{ 1 + \\exp(-x)}\\right)  LogSigmoid(x)=log(1+exp(\u2212x)1\u200b)   Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input     Examples: &gt;&gt;&gt; m = nn.LogSigmoid() &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; output = m(input)   ", "parameters": [], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Hardswish", "item_type": "class", "code": "classtorch.nn.Hardswish", "description": "Applies the hardswish function, element-wise, as described in the paper: Searching for MobileNetV3.  Hardswish(x)={0if\u00a0x\u2264\u22123,xif\u00a0x\u2265+3,x\u22c5(x+3)/6otherwise\\text{Hardswish}(x) = \\begin{cases}     0 &amp; \\text{if~} x \\le -3, \\\\     x &amp; \\text{if~} x \\ge +3, \\\\     x \\cdot (x + 3) /6 &amp; \\text{otherwise} \\end{cases}  Hardswish(x)=\u23a9\u23aa\u23aa\u23a8\u23aa\u23aa\u23a7\u200b0xx\u22c5(x+3)/6\u200bif\u00a0x\u2264\u22123,if\u00a0x\u2265+3,otherwise\u200b   Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input    Examples: &gt;&gt;&gt; m = nn.Hardswish() &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; output = m(input)   ", "parameters": [], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.LeakyReLU", "item_type": "class", "code": "classtorch.nn.LeakyReLU(negative_slope:float=0.01,inplace:bool=False)", "description": "Applies the element-wise function:  LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)  LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)  or  LeakyRELU(x)={x,\u00a0if\u00a0x\u22650negative_slope\u00d7x,\u00a0otherwise\u00a0\\text{LeakyRELU}(x) = \\begin{cases} x, &amp; \\text{ if } x \\geq 0 \\\\ \\text{negative\\_slope} \\times x, &amp; \\text{ otherwise } \\end{cases}  LeakyRELU(x)={x,negative_slope\u00d7x,\u200b\u00a0if\u00a0x\u22650\u00a0otherwise\u00a0\u200b   Parameters  negative_slope \u2013 Controls the angle of the negative slope. Default: 1e-2 inplace \u2013 can optionally do the operation in-place. Default: False     Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input     Examples: &gt;&gt;&gt; m = nn.LeakyReLU(0.1) &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; output = m(input)   ", "parameters": ["negative_slope : Controls the angle of the negative slope. Default: 1e-2", "inplace : can optionally do the operation in-place. Default: False"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.ReLU", "item_type": "class", "code": "classtorch.nn.ReLU(inplace:bool=False)", "description": "Applies the rectified linear unit function element-wise: ReLU(x)=(x)+=max\u2061(0,x)\\text{ReLU}(x) = (x)^+ = \\max(0, x)ReLU(x)=(x)+=max(0,x)    Parameters inplace \u2013 can optionally do the operation in-place. Default: False    Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input     Examples:   &gt;&gt;&gt; m = nn.ReLU()   &gt;&gt;&gt; input = torch.randn(2)   &gt;&gt;&gt; output = m(input)   An implementation of CReLU - https://arxiv.org/abs/1603.05201    &gt;&gt;&gt; m = nn.ReLU()   &gt;&gt;&gt; input = torch.randn(2).unsqueeze(0)   &gt;&gt;&gt; output = torch.cat((m(input),m(-input)))   ", "parameters": ["inplace : can optionally do the operation in-place. Default: False"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.MultiheadAttention.forward", "item_type": "method", "code": "forward(query,key,value,key_padding_mask=None,need_weights=True,attn_mask=None)", "description": " Parameters  key, value (query,) \u2013 map a query and a set of key-value pairs to an output. See \u201cAttention Is All You Need\u201d for more details. key_padding_mask \u2013 if provided, specified padding elements in the key will be ignored by the attention. When given a binary mask and a value is True, the corresponding value on the attention layer will be ignored. When given a byte mask and a value is non-zero, the corresponding value on the attention layer will be ignored need_weights \u2013 output attn_output_weights. attn_mask \u2013 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all the batches while a 3D mask allows to specify a different mask for the entries of each batch.     Shape: Inputs: query: (L,N,E)(L, N, E)(L,N,E)   where L is the target sequence length, N is the batch size, E is the embedding dimension. key: (S,N,E)(S, N, E)(S,N,E)  , where S is the source sequence length, N is the batch size, E is the embedding dimension. value: (S,N,E)(S, N, E)(S,N,E)   where S is the source sequence length, N is the batch size, E is the embedding dimension. key_padding_mask: (N,S)(N, S)(N,S)   where N is the batch size, S is the source sequence length. If a ByteTensor is provided, the non-zero positions will be ignored while the position with the zero positions will be unchanged. If a BoolTensor is provided, the positions with the value of True will be ignored while the position with the value of False will be unchanged. attn_mask: 2D mask (L,S)(L, S)(L,S)   where L is the target sequence length, S is the source sequence length. 3D mask (N\u2217numheads,L,S)(N*num_heads, L, S)(N\u2217numh\u200beads,L,S)   where N is the batch size, L is the target sequence length, S is the source sequence length. attn_mask ensure that position i is allowed to attend the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend while the zero positions will be unchanged. If a BoolTensor is provided, positions with True is not allowed to attend while False values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight. Outputs: attn_output: (L,N,E)(L, N, E)(L,N,E)   where L is the target sequence length, N is the batch size, E is the embedding dimension. attn_output_weights: (N,L,S)(N, L, S)(N,L,S)   where N is the batch size, L is the target sequence length, S is the source sequence length.    ", "parameters": ["key, value (query,) : map a query and a set of key-value pairs to an output.See \u201cAttention Is All You Need\u201d for more details.", "key_padding_mask : if provided, specified padding elements in the key willbe ignored by the attention. When given a binary mask and a value is True,the corresponding value on the attention layer will be ignored. When givena byte mask and a value is non-zero, the corresponding value on the attentionlayer will be ignored", "need_weights : output attn_output_weights.", "attn_mask : 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for allthe batches while a 3D mask allows to specify a different mask for the entries of each batch."], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Hardtanh", "item_type": "class", "code": "classtorch.nn.Hardtanh(min_val:float=-1.0,max_val:float=1.0,inplace:bool=False,min_value:Optional[float]=None,max_value:Optional[float]=None)", "description": "Applies the HardTanh function element-wise HardTanh is defined as:  HardTanh(x)={1\u00a0if\u00a0x&gt;1\u22121\u00a0if\u00a0x&lt;\u22121x\u00a0otherwise\u00a0\\text{HardTanh}(x) = \\begin{cases}     1 &amp; \\text{ if } x &gt; 1 \\\\     -1 &amp; \\text{ if } x &lt; -1 \\\\     x &amp; \\text{ otherwise } \\\\ \\end{cases}  HardTanh(x)=\u23a9\u23aa\u23aa\u23a8\u23aa\u23aa\u23a7\u200b1\u22121x\u200b\u00a0if\u00a0x&gt;1\u00a0if\u00a0x&lt;\u22121\u00a0otherwise\u00a0\u200b  The range of the linear region [\u22121,1][-1, 1][\u22121,1]   can be adjusted using min_val and max_val.  Parameters  min_val \u2013 minimum value of the linear region range. Default: -1 max_val \u2013 maximum value of the linear region range. Default: 1 inplace \u2013 can optionally do the operation in-place. Default: False    Keyword arguments min_value and max_value have been deprecated in favor of min_val and max_val.  Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input     Examples: &gt;&gt;&gt; m = nn.Hardtanh(-2, 2) &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; output = m(input)   ", "parameters": ["min_val : minimum value of the linear region range. Default: -1", "max_val : maximum value of the linear region range. Default: 1", "inplace : can optionally do the operation in-place. Default: False"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.PReLU", "item_type": "class", "code": "classtorch.nn.PReLU(num_parameters:int=1,init:float=0.25)", "description": "Applies the element-wise function:  PReLU(x)=max\u2061(0,x)+a\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + a * \\min(0,x)  PReLU(x)=max(0,x)+a\u2217min(0,x)  or  PReLU(x)={x,\u00a0if\u00a0x\u22650ax,\u00a0otherwise\u00a0\\text{PReLU}(x) = \\begin{cases} x, &amp; \\text{ if } x \\geq 0 \\\\ ax, &amp; \\text{ otherwise } \\end{cases}  PReLU(x)={x,ax,\u200b\u00a0if\u00a0x\u22650\u00a0otherwise\u00a0\u200b  Here aaa   is a learnable parameter. When called without arguments, nn.PReLU() uses a single parameter aaa   across all input channels. If called with nn.PReLU(nChannels), a separate aaa   is used for each input channel.  Note weight decay should not be used when learning aaa   for good performance.   Note Channel dim is the 2nd dim of input. When input has dims &lt; 2, then there is no channel dim and the number of channels = 1.   Parameters  num_parameters (int) \u2013 number of aaa   to learn. Although it takes an int as input, there is only two values are legitimate: 1, or the number of channels at input. Default: 1 init (float) \u2013 the initial value of aaa  . Default: 0.25     Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input     Variables ~PReLU.weight (Tensor) \u2013 the learnable weights of shape (num_parameters).    Examples: &gt;&gt;&gt; m = nn.PReLU() &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; output = m(input)   ", "parameters": ["num_parameters (int) : number of aaa to learn.Although it takes an int as input, there is only two values are legitimate:1, or the number of channels at input. Default: 1", "init (float) : the initial value of aaa. Default: 0.25"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.MultiheadAttention", "item_type": "class", "code": "classtorch.nn.MultiheadAttention(embed_dim,num_heads,dropout=0.0,bias=True,add_bias_kv=False,add_zero_attn=False,kdim=None,vdim=None)", "description": "Allows the model to jointly attend to information from different representation subspaces. See reference: Attention Is All You Need  MultiHead(Q,K,V)=Concat(head1,\u2026,headh)WOwhereheadi=Attention(QWiQ,KWiK,VWiV)\\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O \\text{where} head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)  MultiHead(Q,K,V)=Concat(head1\u200b,\u2026,headh\u200b)WOwhereheadi\u200b=Attention(QWiQ\u200b,KWiK\u200b,VWiV\u200b)   Parameters  embed_dim \u2013 total dimension of the model. num_heads \u2013 parallel attention heads. dropout \u2013 a Dropout layer on attn_output_weights. Default: 0.0. bias \u2013 add bias as module parameter. Default: True. add_bias_kv \u2013 add bias to the key and value sequences at dim=0. add_zero_attn \u2013 add a new batch of zeros to the key and value sequences at dim=1. kdim \u2013 total number of features in key. Default: None. vdim \u2013 total number of features in value. Default: None. Note \u2013 if kdim and vdim are None, they will be set to embed_dim such that key, and value have the same number of features. (query,) \u2013     Examples: &gt;&gt;&gt; multihead_attn = nn.MultiheadAttention(embed_dim, num_heads) &gt;&gt;&gt; attn_output, attn_output_weights = multihead_attn(query, key, value)     forward(query, key, value, key_padding_mask=None, need_weights=True, attn_mask=None)   Parameters  key, value (query,) \u2013 map a query and a set of key-value pairs to an output. See \u201cAttention Is All You Need\u201d for more details. key_padding_mask \u2013 if provided, specified padding elements in the key will be ignored by the attention. When given a binary mask and a value is True, the corresponding value on the attention layer will be ignored. When given a byte mask and a value is non-zero, the corresponding value on the attention layer will be ignored need_weights \u2013 output attn_output_weights. attn_mask \u2013 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all the batches while a 3D mask allows to specify a different mask for the entries of each batch.     Shape: Inputs: query: (L,N,E)(L, N, E)(L,N,E)   where L is the target sequence length, N is the batch size, E is the embedding dimension. key: (S,N,E)(S, N, E)(S,N,E)  , where S is the source sequence length, N is the batch size, E is the embedding dimension. value: (S,N,E)(S, N, E)(S,N,E)   where S is the source sequence length, N is the batch size, E is the embedding dimension. key_padding_mask: (N,S)(N, S)(N,S)   where N is the batch size, S is the source sequence length. If a ByteTensor is provided, the non-zero positions will be ignored while the position with the zero positions will be unchanged. If a BoolTensor is provided, the positions with the value of True will be ignored while the position with the value of False will be unchanged. attn_mask: 2D mask (L,S)(L, S)(L,S)   where L is the target sequence length, S is the source sequence length. 3D mask (N\u2217numheads,L,S)(N*num_heads, L, S)(N\u2217numh\u200beads,L,S)   where N is the batch size, L is the target sequence length, S is the source sequence length. attn_mask ensure that position i is allowed to attend the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend while the zero positions will be unchanged. If a BoolTensor is provided, positions with True is not allowed to attend while False values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight. Outputs: attn_output: (L,N,E)(L, N, E)(L,N,E)   where L is the target sequence length, N is the batch size, E is the embedding dimension. attn_output_weights: (N,L,S)(N, L, S)(N,L,S)   where N is the batch size, L is the target sequence length, S is the source sequence length.      ", "parameters": ["embed_dim : total dimension of the model.", "num_heads : parallel attention heads.", "dropout : a Dropout layer on attn_output_weights. Default: 0.0.", "bias : add bias as module parameter. Default: True.", "add_bias_kv : add bias to the key and value sequences at dim=0.", "add_zero_attn : add a new batch of zeros to the key andvalue sequences at dim=1.", "kdim : total number of features in key. Default: None.", "vdim : total number of features in value. Default: None.", "Note : if kdim and vdim are None, they will be set to embed_dim such that", "key, and value have the same number of features. (query,) : ", "key, value (query,) : map a query and a set of key-value pairs to an output.See \u201cAttention Is All You Need\u201d for more details.", "key_padding_mask : if provided, specified padding elements in the key willbe ignored by the attention. When given a binary mask and a value is True,the corresponding value on the attention layer will be ignored. When givena byte mask and a value is non-zero, the corresponding value on the attentionlayer will be ignored", "need_weights : output attn_output_weights.", "attn_mask : 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for allthe batches while a 3D mask allows to specify a different mask for the entries of each batch."], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.ELU", "item_type": "class", "code": "classtorch.nn.ELU(alpha:float=1.0,inplace:bool=False)", "description": "Applies the element-wise function:  ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))  ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121))   Parameters  alpha \u2013 the \u03b1\\alpha\u03b1   value for the ELU formulation. Default: 1.0 inplace \u2013 can optionally do the operation in-place. Default: False     Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input     Examples: &gt;&gt;&gt; m = nn.ELU() &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; output = m(input)   ", "parameters": ["alpha : the \u03b1\\alpha\u03b1 value for the ELU formulation. Default: 1.0", "inplace : can optionally do the operation in-place. Default: False"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.ConstantPad2d", "item_type": "class", "code": "classtorch.nn.ConstantPad2d(padding:Union[T,Tuple[T,T,T,T]],value:float)", "description": "Pads the input tensor boundaries with a constant value. For N-dimensional padding, use torch.nn.functional.pad().  Parameters padding (int, tuple) \u2013 the size of the padding. If is int, uses the same padding in all boundaries. If a 4-tuple, uses (padding_left\\text{padding\\_left}padding_left  , padding_right\\text{padding\\_right}padding_right  , padding_top\\text{padding\\_top}padding_top  , padding_bottom\\text{padding\\_bottom}padding_bottom  )    Shape: Input: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)   Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)   where Hout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}Hout\u200b=Hin\u200b+padding_top+padding_bottom   Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout\u200b=Win\u200b+padding_left+padding_right       Examples: &gt;&gt;&gt; m = nn.ConstantPad2d(2, 3.5) &gt;&gt;&gt; input = torch.randn(1, 2, 2) &gt;&gt;&gt; input tensor([[[ 1.6585,  0.4320],          [-0.8701, -0.4649]]]) &gt;&gt;&gt; m(input) tensor([[[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],          [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],          [ 3.5000,  3.5000,  1.6585,  0.4320,  3.5000,  3.5000],          [ 3.5000,  3.5000, -0.8701, -0.4649,  3.5000,  3.5000],          [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],          [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000]]]) &gt;&gt;&gt; # using different paddings for different sides &gt;&gt;&gt; m = nn.ConstantPad2d((3, 0, 2, 1), 3.5) &gt;&gt;&gt; m(input) tensor([[[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000],          [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000],          [ 3.5000,  3.5000,  3.5000,  1.6585,  0.4320],          [ 3.5000,  3.5000,  3.5000, -0.8701, -0.4649],          [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000]]])   ", "parameters": ["padding (int, tuple) : the size of the padding. If is int, uses the samepadding in all boundaries. If a 4-tuple, uses (padding_left\\text{padding\\_left}padding_left,padding_right\\text{padding\\_right}padding_right, padding_top\\text{padding\\_top}padding_top, padding_bottom\\text{padding\\_bottom}padding_bottom)"], "returns": [], "example": "NA", "shape": "\nInput: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)\n\n\nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)\n\n where\nHout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}Hout\u200b=Hin\u200b+padding_top+padding_bottom\n\n\nWout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout\u200b=Win\u200b+padding_left+padding_right\n\n\n\n\n"},
{"library": "torch", "item_id": "torch.nn.Hardshrink", "item_type": "class", "code": "classtorch.nn.Hardshrink(lambd:float=0.5)", "description": "Applies the hard shrinkage function element-wise:  HardShrink(x)={x,\u00a0if\u00a0x&gt;\u03bbx,\u00a0if\u00a0x&lt;\u2212\u03bb0,\u00a0otherwise\u00a0\\text{HardShrink}(x) = \\begin{cases} x, &amp; \\text{ if } x &gt; \\lambda \\\\ x, &amp; \\text{ if } x &lt; -\\lambda \\\\ 0, &amp; \\text{ otherwise } \\end{cases}  HardShrink(x)=\u23a9\u23aa\u23aa\u23a8\u23aa\u23aa\u23a7\u200bx,x,0,\u200b\u00a0if\u00a0x&gt;\u03bb\u00a0if\u00a0x&lt;\u2212\u03bb\u00a0otherwise\u00a0\u200b   Parameters lambd \u2013 the \u03bb\\lambda\u03bb   value for the Hardshrink formulation. Default: 0.5    Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input     Examples: &gt;&gt;&gt; m = nn.Hardshrink() &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; output = m(input)   ", "parameters": ["lambd : the \u03bb\\lambda\u03bb value for the Hardshrink formulation. Default: 0.5"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.ZeroPad2d", "item_type": "class", "code": "classtorch.nn.ZeroPad2d(padding:Union[T,Tuple[T,T,T,T]])", "description": "Pads the input tensor boundaries with zero. For N-dimensional padding, use torch.nn.functional.pad().  Parameters padding (int, tuple) \u2013 the size of the padding. If is int, uses the same padding in all boundaries. If a 4-tuple, uses (padding_left\\text{padding\\_left}padding_left  , padding_right\\text{padding\\_right}padding_right  , padding_top\\text{padding\\_top}padding_top  , padding_bottom\\text{padding\\_bottom}padding_bottom  )    Shape: Input: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)   Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)   where Hout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}Hout\u200b=Hin\u200b+padding_top+padding_bottom   Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout\u200b=Win\u200b+padding_left+padding_right       Examples: &gt;&gt;&gt; m = nn.ZeroPad2d(2) &gt;&gt;&gt; input = torch.randn(1, 1, 3, 3) &gt;&gt;&gt; input tensor([[[[-0.1678, -0.4418,  1.9466],           [ 0.9604, -0.4219, -0.5241],           [-0.9162, -0.5436, -0.6446]]]]) &gt;&gt;&gt; m(input) tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],           [ 0.0000,  0.0000, -0.1678, -0.4418,  1.9466,  0.0000,  0.0000],           [ 0.0000,  0.0000,  0.9604, -0.4219, -0.5241,  0.0000,  0.0000],           [ 0.0000,  0.0000, -0.9162, -0.5436, -0.6446,  0.0000,  0.0000],           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]]) &gt;&gt;&gt; # using different paddings for different sides &gt;&gt;&gt; m = nn.ZeroPad2d((1, 1, 2, 0)) &gt;&gt;&gt; m(input) tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],           [ 0.0000, -0.1678, -0.4418,  1.9466,  0.0000],           [ 0.0000,  0.9604, -0.4219, -0.5241,  0.0000],           [ 0.0000, -0.9162, -0.5436, -0.6446,  0.0000]]]])   ", "parameters": ["padding (int, tuple) : the size of the padding. If is int, uses the samepadding in all boundaries. If a 4-tuple, uses (padding_left\\text{padding\\_left}padding_left,padding_right\\text{padding\\_right}padding_right, padding_top\\text{padding\\_top}padding_top, padding_bottom\\text{padding\\_bottom}padding_bottom)"], "returns": [], "example": "NA", "shape": "\nInput: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)\n\n\nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)\n\n where\nHout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}Hout\u200b=Hin\u200b+padding_top+padding_bottom\n\n\nWout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout\u200b=Win\u200b+padding_left+padding_right\n\n\n\n\n"},
{"library": "torch", "item_id": "torch.nn.ConstantPad3d", "item_type": "class", "code": "classtorch.nn.ConstantPad3d(padding:Union[T,Tuple[T,T,T,T,T,T]],value:float)", "description": "Pads the input tensor boundaries with a constant value. For N-dimensional padding, use torch.nn.functional.pad().  Parameters padding (int, tuple) \u2013 the size of the padding. If is int, uses the same padding in all boundaries. If a 6-tuple, uses (padding_left\\text{padding\\_left}padding_left  , padding_right\\text{padding\\_right}padding_right  , padding_top\\text{padding\\_top}padding_top  , padding_bottom\\text{padding\\_bottom}padding_bottom  , padding_front\\text{padding\\_front}padding_front  , padding_back\\text{padding\\_back}padding_back  )    Shape: Input: (N,C,Din,Hin,Win)(N, C, D_{in}, H_{in}, W_{in})(N,C,Din\u200b,Hin\u200b,Win\u200b)   Output: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\u200b,Hout\u200b,Wout\u200b)   where Dout=Din+padding_front+padding_backD_{out} = D_{in} + \\text{padding\\_front} + \\text{padding\\_back}Dout\u200b=Din\u200b+padding_front+padding_back   Hout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}Hout\u200b=Hin\u200b+padding_top+padding_bottom   Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout\u200b=Win\u200b+padding_left+padding_right       Examples: &gt;&gt;&gt; m = nn.ConstantPad3d(3, 3.5) &gt;&gt;&gt; input = torch.randn(16, 3, 10, 20, 30) &gt;&gt;&gt; output = m(input) &gt;&gt;&gt; # using different paddings for different sides &gt;&gt;&gt; m = nn.ConstantPad3d((3, 3, 6, 6, 0, 1), 3.5) &gt;&gt;&gt; output = m(input)   ", "parameters": ["padding (int, tuple) : the size of the padding. If is int, uses the samepadding in all boundaries. If a 6-tuple, uses(padding_left\\text{padding\\_left}padding_left, padding_right\\text{padding\\_right}padding_right,padding_top\\text{padding\\_top}padding_top, padding_bottom\\text{padding\\_bottom}padding_bottom,padding_front\\text{padding\\_front}padding_front, padding_back\\text{padding\\_back}padding_back)"], "returns": [], "example": "NA", "shape": "\nInput: (N,C,Din,Hin,Win)(N, C, D_{in}, H_{in}, W_{in})(N,C,Din\u200b,Hin\u200b,Win\u200b)\n\n\nOutput: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\u200b,Hout\u200b,Wout\u200b)\n\n where\nDout=Din+padding_front+padding_backD_{out} = D_{in} + \\text{padding\\_front} + \\text{padding\\_back}Dout\u200b=Din\u200b+padding_front+padding_back\n\n\nHout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}Hout\u200b=Hin\u200b+padding_top+padding_bottom\n\n\nWout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout\u200b=Win\u200b+padding_left+padding_right\n\n\n\n\n"},
{"library": "torch", "item_id": "torch.nn.ConstantPad1d", "item_type": "class", "code": "classtorch.nn.ConstantPad1d(padding:Union[T,Tuple[T,T]],value:float)", "description": "Pads the input tensor boundaries with a constant value. For N-dimensional padding, use torch.nn.functional.pad().  Parameters padding (int, tuple) \u2013 the size of the padding. If is int, uses the same padding in both boundaries. If a 2-tuple, uses (padding_left\\text{padding\\_left}padding_left  , padding_right\\text{padding\\_right}padding_right  )    Shape: Input: (N,C,Win)(N, C, W_{in})(N,C,Win\u200b)   Output: (N,C,Wout)(N, C, W_{out})(N,C,Wout\u200b)   where Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout\u200b=Win\u200b+padding_left+padding_right       Examples: &gt;&gt;&gt; m = nn.ConstantPad1d(2, 3.5) &gt;&gt;&gt; input = torch.randn(1, 2, 4) &gt;&gt;&gt; input tensor([[[-1.0491, -0.7152, -0.0749,  0.8530],          [-1.3287,  1.8966,  0.1466, -0.2771]]]) &gt;&gt;&gt; m(input) tensor([[[ 3.5000,  3.5000, -1.0491, -0.7152, -0.0749,  0.8530,  3.5000,            3.5000],          [ 3.5000,  3.5000, -1.3287,  1.8966,  0.1466, -0.2771,  3.5000,            3.5000]]]) &gt;&gt;&gt; m = nn.ConstantPad1d(2, 3.5) &gt;&gt;&gt; input = torch.randn(1, 2, 3) &gt;&gt;&gt; input tensor([[[ 1.6616,  1.4523, -1.1255],          [-3.6372,  0.1182, -1.8652]]]) &gt;&gt;&gt; m(input) tensor([[[ 3.5000,  3.5000,  1.6616,  1.4523, -1.1255,  3.5000,  3.5000],          [ 3.5000,  3.5000, -3.6372,  0.1182, -1.8652,  3.5000,  3.5000]]]) &gt;&gt;&gt; # using different paddings for different sides &gt;&gt;&gt; m = nn.ConstantPad1d((3, 1), 3.5) &gt;&gt;&gt; m(input) tensor([[[ 3.5000,  3.5000,  3.5000,  1.6616,  1.4523, -1.1255,  3.5000],          [ 3.5000,  3.5000,  3.5000, -3.6372,  0.1182, -1.8652,  3.5000]]])   ", "parameters": ["padding (int, tuple) : the size of the padding. If is int, uses the samepadding in both boundaries. If a 2-tuple, uses(padding_left\\text{padding\\_left}padding_left, padding_right\\text{padding\\_right}padding_right)"], "returns": [], "example": "NA", "shape": "\nInput: (N,C,Win)(N, C, W_{in})(N,C,Win\u200b)\n\n\nOutput: (N,C,Wout)(N, C, W_{out})(N,C,Wout\u200b)\n\n where\nWout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout\u200b=Win\u200b+padding_left+padding_right\n\n\n\n\n"},
{"library": "torch", "item_id": "torch.nn.Hardsigmoid", "item_type": "class", "code": "classtorch.nn.Hardsigmoid", "description": "Applies the element-wise function:  Hardsigmoid(x)={0if\u00a0x\u2264\u22123,1if\u00a0x\u2265+3,x/6+1/2otherwise\\text{Hardsigmoid}(x) = \\begin{cases}     0 &amp; \\text{if~} x \\le -3, \\\\     1 &amp; \\text{if~} x \\ge +3, \\\\     x / 6 + 1 / 2 &amp; \\text{otherwise} \\end{cases}  Hardsigmoid(x)=\u23a9\u23aa\u23aa\u23a8\u23aa\u23aa\u23a7\u200b01x/6+1/2\u200bif\u00a0x\u2264\u22123,if\u00a0x\u2265+3,otherwise\u200b   Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input    Examples: &gt;&gt;&gt; m = nn.Hardsigmoid() &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; output = m(input)   ", "parameters": [], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.ReplicationPad3d", "item_type": "class", "code": "classtorch.nn.ReplicationPad3d(padding:Union[T,Tuple[T,T,T,T,T,T]])", "description": "Pads the input tensor using replication of the input boundary. For N-dimensional padding, use torch.nn.functional.pad().  Parameters padding (int, tuple) \u2013 the size of the padding. If is int, uses the same padding in all boundaries. If a 6-tuple, uses (padding_left\\text{padding\\_left}padding_left  , padding_right\\text{padding\\_right}padding_right  , padding_top\\text{padding\\_top}padding_top  , padding_bottom\\text{padding\\_bottom}padding_bottom  , padding_front\\text{padding\\_front}padding_front  , padding_back\\text{padding\\_back}padding_back  )    Shape: Input: (N,C,Din,Hin,Win)(N, C, D_{in}, H_{in}, W_{in})(N,C,Din\u200b,Hin\u200b,Win\u200b)   Output: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\u200b,Hout\u200b,Wout\u200b)   where Dout=Din+padding_front+padding_backD_{out} = D_{in} + \\text{padding\\_front} + \\text{padding\\_back}Dout\u200b=Din\u200b+padding_front+padding_back   Hout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}Hout\u200b=Hin\u200b+padding_top+padding_bottom   Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout\u200b=Win\u200b+padding_left+padding_right       Examples: &gt;&gt;&gt; m = nn.ReplicationPad3d(3) &gt;&gt;&gt; input = torch.randn(16, 3, 8, 320, 480) &gt;&gt;&gt; output = m(input) &gt;&gt;&gt; # using different paddings for different sides &gt;&gt;&gt; m = nn.ReplicationPad3d((3, 3, 6, 6, 1, 1)) &gt;&gt;&gt; output = m(input)   ", "parameters": ["padding (int, tuple) : the size of the padding. If is int, uses the samepadding in all boundaries. If a 6-tuple, uses(padding_left\\text{padding\\_left}padding_left, padding_right\\text{padding\\_right}padding_right,padding_top\\text{padding\\_top}padding_top, padding_bottom\\text{padding\\_bottom}padding_bottom,padding_front\\text{padding\\_front}padding_front, padding_back\\text{padding\\_back}padding_back)"], "returns": [], "example": "NA", "shape": "\nInput: (N,C,Din,Hin,Win)(N, C, D_{in}, H_{in}, W_{in})(N,C,Din\u200b,Hin\u200b,Win\u200b)\n\n\nOutput: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\u200b,Hout\u200b,Wout\u200b)\n\n where\nDout=Din+padding_front+padding_backD_{out} = D_{in} + \\text{padding\\_front} + \\text{padding\\_back}Dout\u200b=Din\u200b+padding_front+padding_back\n\n\nHout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}Hout\u200b=Hin\u200b+padding_top+padding_bottom\n\n\nWout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout\u200b=Win\u200b+padding_left+padding_right\n\n\n\n\n"},
{"library": "torch", "item_id": "torch.nn.ReplicationPad2d", "item_type": "class", "code": "classtorch.nn.ReplicationPad2d(padding:Union[T,Tuple[T,T,T,T]])", "description": "Pads the input tensor using replication of the input boundary. For N-dimensional padding, use torch.nn.functional.pad().  Parameters padding (int, tuple) \u2013 the size of the padding. If is int, uses the same padding in all boundaries. If a 4-tuple, uses (padding_left\\text{padding\\_left}padding_left  , padding_right\\text{padding\\_right}padding_right  , padding_top\\text{padding\\_top}padding_top  , padding_bottom\\text{padding\\_bottom}padding_bottom  )    Shape: Input: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)   Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)   where Hout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}Hout\u200b=Hin\u200b+padding_top+padding_bottom   Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout\u200b=Win\u200b+padding_left+padding_right       Examples: &gt;&gt;&gt; m = nn.ReplicationPad2d(2) &gt;&gt;&gt; input = torch.arange(9, dtype=torch.float).reshape(1, 1, 3, 3) &gt;&gt;&gt; input tensor([[[[0., 1., 2.],           [3., 4., 5.],           [6., 7., 8.]]]]) &gt;&gt;&gt; m(input) tensor([[[[0., 0., 0., 1., 2., 2., 2.],           [0., 0., 0., 1., 2., 2., 2.],           [0., 0., 0., 1., 2., 2., 2.],           [3., 3., 3., 4., 5., 5., 5.],           [6., 6., 6., 7., 8., 8., 8.],           [6., 6., 6., 7., 8., 8., 8.],           [6., 6., 6., 7., 8., 8., 8.]]]]) &gt;&gt;&gt; # using different paddings for different sides &gt;&gt;&gt; m = nn.ReplicationPad2d((1, 1, 2, 0)) &gt;&gt;&gt; m(input) tensor([[[[0., 0., 1., 2., 2.],           [0., 0., 1., 2., 2.],           [0., 0., 1., 2., 2.],           [3., 3., 4., 5., 5.],           [6., 6., 7., 8., 8.]]]])   ", "parameters": ["padding (int, tuple) : the size of the padding. If is int, uses the samepadding in all boundaries. If a 4-tuple, uses (padding_left\\text{padding\\_left}padding_left,padding_right\\text{padding\\_right}padding_right, padding_top\\text{padding\\_top}padding_top, padding_bottom\\text{padding\\_bottom}padding_bottom)"], "returns": [], "example": "NA", "shape": "\nInput: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)\n\n\nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)\n\n where\nHout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}Hout\u200b=Hin\u200b+padding_top+padding_bottom\n\n\nWout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout\u200b=Win\u200b+padding_left+padding_right\n\n\n\n\n"},
{"library": "torch", "item_id": "torch.nn.ReplicationPad1d", "item_type": "class", "code": "classtorch.nn.ReplicationPad1d(padding:Union[T,Tuple[T,T]])", "description": "Pads the input tensor using replication of the input boundary. For N-dimensional padding, use torch.nn.functional.pad().  Parameters padding (int, tuple) \u2013 the size of the padding. If is int, uses the same padding in all boundaries. If a 2-tuple, uses (padding_left\\text{padding\\_left}padding_left  , padding_right\\text{padding\\_right}padding_right  )    Shape: Input: (N,C,Win)(N, C, W_{in})(N,C,Win\u200b)   Output: (N,C,Wout)(N, C, W_{out})(N,C,Wout\u200b)   where Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout\u200b=Win\u200b+padding_left+padding_right       Examples: &gt;&gt;&gt; m = nn.ReplicationPad1d(2) &gt;&gt;&gt; input = torch.arange(8, dtype=torch.float).reshape(1, 2, 4) &gt;&gt;&gt; input tensor([[[0., 1., 2., 3.],          [4., 5., 6., 7.]]]) &gt;&gt;&gt; m(input) tensor([[[0., 0., 0., 1., 2., 3., 3., 3.],          [4., 4., 4., 5., 6., 7., 7., 7.]]]) &gt;&gt;&gt; # using different paddings for different sides &gt;&gt;&gt; m = nn.ReplicationPad1d((3, 1)) &gt;&gt;&gt; m(input) tensor([[[0., 0., 0., 0., 1., 2., 3., 3.],          [4., 4., 4., 4., 5., 6., 7., 7.]]])   ", "parameters": ["padding (int, tuple) : the size of the padding. If is int, uses the samepadding in all boundaries. If a 2-tuple, uses(padding_left\\text{padding\\_left}padding_left, padding_right\\text{padding\\_right}padding_right)"], "returns": [], "example": "NA", "shape": "\nInput: (N,C,Win)(N, C, W_{in})(N,C,Win\u200b)\n\n\nOutput: (N,C,Wout)(N, C, W_{out})(N,C,Wout\u200b)\n\n where\nWout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout\u200b=Win\u200b+padding_left+padding_right\n\n\n\n\n"},
{"library": "torch", "item_id": "torch.nn.ReflectionPad2d", "item_type": "class", "code": "classtorch.nn.ReflectionPad2d(padding:Union[T,Tuple[T,T,T,T]])", "description": "Pads the input tensor using the reflection of the input boundary. For N-dimensional padding, use torch.nn.functional.pad().  Parameters padding (int, tuple) \u2013 the size of the padding. If is int, uses the same padding in all boundaries. If a 4-tuple, uses (padding_left\\text{padding\\_left}padding_left  , padding_right\\text{padding\\_right}padding_right  , padding_top\\text{padding\\_top}padding_top  , padding_bottom\\text{padding\\_bottom}padding_bottom  )    Shape: Input: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)   Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)   where Hout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}Hout\u200b=Hin\u200b+padding_top+padding_bottom   Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout\u200b=Win\u200b+padding_left+padding_right       Examples: &gt;&gt;&gt; m = nn.ReflectionPad2d(2) &gt;&gt;&gt; input = torch.arange(9, dtype=torch.float).reshape(1, 1, 3, 3) &gt;&gt;&gt; input tensor([[[[0., 1., 2.],           [3., 4., 5.],           [6., 7., 8.]]]]) &gt;&gt;&gt; m(input) tensor([[[[8., 7., 6., 7., 8., 7., 6.],           [5., 4., 3., 4., 5., 4., 3.],           [2., 1., 0., 1., 2., 1., 0.],           [5., 4., 3., 4., 5., 4., 3.],           [8., 7., 6., 7., 8., 7., 6.],           [5., 4., 3., 4., 5., 4., 3.],           [2., 1., 0., 1., 2., 1., 0.]]]]) &gt;&gt;&gt; # using different paddings for different sides &gt;&gt;&gt; m = nn.ReflectionPad2d((1, 1, 2, 0)) &gt;&gt;&gt; m(input) tensor([[[[7., 6., 7., 8., 7.],           [4., 3., 4., 5., 4.],           [1., 0., 1., 2., 1.],           [4., 3., 4., 5., 4.],           [7., 6., 7., 8., 7.]]]])   ", "parameters": ["padding (int, tuple) : the size of the padding. If is int, uses the samepadding in all boundaries. If a 4-tuple, uses (padding_left\\text{padding\\_left}padding_left,padding_right\\text{padding\\_right}padding_right, padding_top\\text{padding\\_top}padding_top, padding_bottom\\text{padding\\_bottom}padding_bottom)"], "returns": [], "example": "NA", "shape": "\nInput: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)\n\n\nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)\n\n where\nHout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}Hout\u200b=Hin\u200b+padding_top+padding_bottom\n\n\nWout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout\u200b=Win\u200b+padding_left+padding_right\n\n\n\n\n"},
{"library": "torch", "item_id": "torch.nn.MaxPool3d", "item_type": "class", "code": "classtorch.nn.MaxPool3d(kernel_size:Union[T,Tuple[T,...]],stride:Optional[Union[T,Tuple[T,...]]]=None,padding:Union[T,Tuple[T,...]]=0,dilation:Union[T,Tuple[T,...]]=1,return_indices:bool=False,ceil_mode:bool=False)", "description": "Applies a 3D max pooling over an input signal composed of several input planes. In the simplest case, the output value of the layer with input size (N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W)  , output (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\u200b,Hout\u200b,Wout\u200b)   and kernel_size (kD,kH,kW)(kD, kH, kW)(kD,kH,kW)   can be precisely described as:  out(Ni,Cj,d,h,w)=max\u2061k=0,\u2026,kD\u22121max\u2061m=0,\u2026,kH\u22121max\u2061n=0,\u2026,kW\u22121input(Ni,Cj,stride[0]\u00d7d+k,stride[1]\u00d7h+m,stride[2]\u00d7w+n)\\begin{aligned}     \\text{out}(N_i, C_j, d, h, w) ={} &amp; \\max_{k=0, \\ldots, kD-1} \\max_{m=0, \\ldots, kH-1} \\max_{n=0, \\ldots, kW-1} \\\\                                       &amp; \\text{input}(N_i, C_j, \\text{stride[0]} \\times d + k,                                                      \\text{stride[1]} \\times h + m, \\text{stride[2]} \\times w + n) \\end{aligned}  out(Ni\u200b,Cj\u200b,d,h,w)=\u200bk=0,\u2026,kD\u22121max\u200bm=0,\u2026,kH\u22121max\u200bn=0,\u2026,kW\u22121max\u200binput(Ni\u200b,Cj\u200b,stride[0]\u00d7d+k,stride[1]\u00d7h+m,stride[2]\u00d7w+n)\u200b  If padding is non-zero, then the input is implicitly zero-padded on both sides for padding number of points. dilation controls the spacing between the kernel points. It is harder to describe, but this link has a nice visualization of what dilation does. The parameters kernel_size, stride, padding, dilation can either be:   a single int \u2013 in which case the same value is used for the depth, height and width dimension a tuple of three ints \u2013 in which case, the first int is used for the depth dimension, the second int for the height dimension and the third int for the width dimension    Parameters  kernel_size \u2013 the size of the window to take a max over stride \u2013 the stride of the window. Default value is kernel_size padding \u2013 implicit zero padding to be added on all three sides dilation \u2013 a parameter that controls the stride of elements in the window return_indices \u2013 if True, will return the max indices along with the outputs. Useful for torch.nn.MaxUnpool3d later ceil_mode \u2013 when True, will use ceil instead of floor to compute the output shape     Shape: Input: (N,C,Din,Hin,Win)(N, C, D_{in}, H_{in}, W_{in})(N,C,Din\u200b,Hin\u200b,Win\u200b)   Output: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\u200b,Hout\u200b,Wout\u200b)  , where  Dout=\u230aDin+2\u00d7padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121stride[0]+1\u230bD_{out} = \\left\\lfloor\\frac{D_{in} + 2 \\times \\text{padding}[0] - \\text{dilation}[0] \\times   (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor  Dout\u200b=\u230astride[0]Din\u200b+2\u00d7padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121\u200b+1\u230b   Hout=\u230aHin+2\u00d7padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121stride[1]+1\u230bH_{out} = \\left\\lfloor\\frac{H_{in} + 2 \\times \\text{padding}[1] - \\text{dilation}[1] \\times   (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor  Hout\u200b=\u230astride[1]Hin\u200b+2\u00d7padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121\u200b+1\u230b   Wout=\u230aWin+2\u00d7padding[2]\u2212dilation[2]\u00d7(kernel_size[2]\u22121)\u22121stride[2]+1\u230bW_{out} = \\left\\lfloor\\frac{W_{in} + 2 \\times \\text{padding}[2] - \\text{dilation}[2] \\times   (\\text{kernel\\_size}[2] - 1) - 1}{\\text{stride}[2]} + 1\\right\\rfloor  Wout\u200b=\u230astride[2]Win\u200b+2\u00d7padding[2]\u2212dilation[2]\u00d7(kernel_size[2]\u22121)\u22121\u200b+1\u230b      Examples: &gt;&gt;&gt; # pool of square window of size=3, stride=2 &gt;&gt;&gt; m = nn.MaxPool3d(3, stride=2) &gt;&gt;&gt; # pool of non-square window &gt;&gt;&gt; m = nn.MaxPool3d((3, 2, 2), stride=(2, 1, 2)) &gt;&gt;&gt; input = torch.randn(20, 16, 50,44, 31) &gt;&gt;&gt; output = m(input)   ", "parameters": ["kernel_size : the size of the window to take a max over", "stride : the stride of the window. Default value is kernel_size", "padding : implicit zero padding to be added on all three sides", "dilation : a parameter that controls the stride of elements in the window", "return_indices : if True, will return the max indices along with the outputs.Useful for torch.nn.MaxUnpool3d later", "ceil_mode : when True, will use ceil instead of floor to compute the output shape"], "returns": [], "example": "NA", "shape": "\nInput: (N,C,Din,Hin,Win)(N, C, D_{in}, H_{in}, W_{in})(N,C,Din\u200b,Hin\u200b,Win\u200b)\n\n\nOutput: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\u200b,Hout\u200b,Wout\u200b)\n\n, where\n\nDout=\u230aDin+2\u00d7padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121stride[0]+1\u230bD_{out} = \\left\\lfloor\\frac{D_{in} + 2 \\times \\text{padding}[0] - \\text{dilation}[0] \\times\n  (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor\n\nDout\u200b=\u230astride[0]Din\u200b+2\u00d7padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121\u200b+1\u230b\n\n\nHout=\u230aHin+2\u00d7padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121stride[1]+1\u230bH_{out} = \\left\\lfloor\\frac{H_{in} + 2 \\times \\text{padding}[1] - \\text{dilation}[1] \\times\n  (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor\n\nHout\u200b=\u230astride[1]Hin\u200b+2\u00d7padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121\u200b+1\u230b\n\n\nWout=\u230aWin+2\u00d7padding[2]\u2212dilation[2]\u00d7(kernel_size[2]\u22121)\u22121stride[2]+1\u230bW_{out} = \\left\\lfloor\\frac{W_{in} + 2 \\times \\text{padding}[2] - \\text{dilation}[2] \\times\n  (\\text{kernel\\_size}[2] - 1) - 1}{\\text{stride}[2]} + 1\\right\\rfloor\n\nWout\u200b=\u230astride[2]Win\u200b+2\u00d7padding[2]\u2212dilation[2]\u00d7(kernel_size[2]\u22121)\u22121\u200b+1\u230b\n\n\n\n"},
{"library": "torch", "item_id": "torch.nn.ReflectionPad1d", "item_type": "class", "code": "classtorch.nn.ReflectionPad1d(padding:Union[T,Tuple[T,T]])", "description": "Pads the input tensor using the reflection of the input boundary. For N-dimensional padding, use torch.nn.functional.pad().  Parameters padding (int, tuple) \u2013 the size of the padding. If is int, uses the same padding in all boundaries. If a 2-tuple, uses (padding_left\\text{padding\\_left}padding_left  , padding_right\\text{padding\\_right}padding_right  )    Shape: Input: (N,C,Win)(N, C, W_{in})(N,C,Win\u200b)   Output: (N,C,Wout)(N, C, W_{out})(N,C,Wout\u200b)   where Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout\u200b=Win\u200b+padding_left+padding_right       Examples: &gt;&gt;&gt; m = nn.ReflectionPad1d(2) &gt;&gt;&gt; input = torch.arange(8, dtype=torch.float).reshape(1, 2, 4) &gt;&gt;&gt; input tensor([[[0., 1., 2., 3.],          [4., 5., 6., 7.]]]) &gt;&gt;&gt; m(input) tensor([[[2., 1., 0., 1., 2., 3., 2., 1.],          [6., 5., 4., 5., 6., 7., 6., 5.]]]) &gt;&gt;&gt; # using different paddings for different sides &gt;&gt;&gt; m = nn.ReflectionPad1d((3, 1)) &gt;&gt;&gt; m(input) tensor([[[3., 2., 1., 0., 1., 2., 3., 2.],          [7., 6., 5., 4., 5., 6., 7., 6.]]])   ", "parameters": ["padding (int, tuple) : the size of the padding. If is int, uses the samepadding in all boundaries. If a 2-tuple, uses(padding_left\\text{padding\\_left}padding_left, padding_right\\text{padding\\_right}padding_right)"], "returns": [], "example": "NA", "shape": "\nInput: (N,C,Win)(N, C, W_{in})(N,C,Win\u200b)\n\n\nOutput: (N,C,Wout)(N, C, W_{out})(N,C,Wout\u200b)\n\n where\nWout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout\u200b=Win\u200b+padding_left+padding_right\n\n\n\n\n"},
{"library": "torch", "item_id": "torch.nn.MaxPool2d", "item_type": "class", "code": "classtorch.nn.MaxPool2d(kernel_size:Union[T,Tuple[T,...]],stride:Optional[Union[T,Tuple[T,...]]]=None,padding:Union[T,Tuple[T,...]]=0,dilation:Union[T,Tuple[T,...]]=1,return_indices:bool=False,ceil_mode:bool=False)", "description": "Applies a 2D max pooling over an input signal composed of several input planes. In the simplest case, the output value of the layer with input size (N,C,H,W)(N, C, H, W)(N,C,H,W)  , output (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)   and kernel_size (kH,kW)(kH, kW)(kH,kW)   can be precisely described as:  out(Ni,Cj,h,w)=max\u2061m=0,\u2026,kH\u22121max\u2061n=0,\u2026,kW\u22121input(Ni,Cj,stride[0]\u00d7h+m,stride[1]\u00d7w+n)\\begin{aligned}     out(N_i, C_j, h, w) ={} &amp; \\max_{m=0, \\ldots, kH-1} \\max_{n=0, \\ldots, kW-1} \\\\                             &amp; \\text{input}(N_i, C_j, \\text{stride[0]} \\times h + m,                                            \\text{stride[1]} \\times w + n) \\end{aligned}  out(Ni\u200b,Cj\u200b,h,w)=\u200bm=0,\u2026,kH\u22121max\u200bn=0,\u2026,kW\u22121max\u200binput(Ni\u200b,Cj\u200b,stride[0]\u00d7h+m,stride[1]\u00d7w+n)\u200b  If padding is non-zero, then the input is implicitly zero-padded on both sides for padding number of points. dilation controls the spacing between the kernel points. It is harder to describe, but this link has a nice visualization of what dilation does. The parameters kernel_size, stride, padding, dilation can either be:   a single int \u2013 in which case the same value is used for the height and width dimension a tuple of two ints \u2013 in which case, the first int is used for the height dimension, and the second int for the width dimension    Parameters  kernel_size \u2013 the size of the window to take a max over stride \u2013 the stride of the window. Default value is kernel_size padding \u2013 implicit zero padding to be added on both sides dilation \u2013 a parameter that controls the stride of elements in the window return_indices \u2013 if True, will return the max indices along with the outputs. Useful for torch.nn.MaxUnpool2d later ceil_mode \u2013 when True, will use ceil instead of floor to compute the output shape     Shape: Input: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)   Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)  , where  Hout=\u230aHin+2\u2217padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121stride[0]+1\u230bH_{out} = \\left\\lfloor\\frac{H_{in} + 2 * \\text{padding[0]} - \\text{dilation[0]}       \\times (\\text{kernel\\_size[0]} - 1) - 1}{\\text{stride[0]}} + 1\\right\\rfloor  Hout\u200b=\u230astride[0]Hin\u200b+2\u2217padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121\u200b+1\u230b   Wout=\u230aWin+2\u2217padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121stride[1]+1\u230bW_{out} = \\left\\lfloor\\frac{W_{in} + 2 * \\text{padding[1]} - \\text{dilation[1]}       \\times (\\text{kernel\\_size[1]} - 1) - 1}{\\text{stride[1]}} + 1\\right\\rfloor  Wout\u200b=\u230astride[1]Win\u200b+2\u2217padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121\u200b+1\u230b      Examples: &gt;&gt;&gt; # pool of square window of size=3, stride=2 &gt;&gt;&gt; m = nn.MaxPool2d(3, stride=2) &gt;&gt;&gt; # pool of non-square window &gt;&gt;&gt; m = nn.MaxPool2d((3, 2), stride=(2, 1)) &gt;&gt;&gt; input = torch.randn(20, 16, 50, 32) &gt;&gt;&gt; output = m(input)   ", "parameters": ["kernel_size : the size of the window to take a max over", "stride : the stride of the window. Default value is kernel_size", "padding : implicit zero padding to be added on both sides", "dilation : a parameter that controls the stride of elements in the window", "return_indices : if True, will return the max indices along with the outputs.Useful for torch.nn.MaxUnpool2d later", "ceil_mode : when True, will use ceil instead of floor to compute the output shape"], "returns": [], "example": "NA", "shape": "\nInput: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)\n\n\nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)\n\n, where\n\nHout=\u230aHin+2\u2217padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121stride[0]+1\u230bH_{out} = \\left\\lfloor\\frac{H_{in} + 2 * \\text{padding[0]} - \\text{dilation[0]}\n      \\times (\\text{kernel\\_size[0]} - 1) - 1}{\\text{stride[0]}} + 1\\right\\rfloor\n\nHout\u200b=\u230astride[0]Hin\u200b+2\u2217padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121\u200b+1\u230b\n\n\nWout=\u230aWin+2\u2217padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121stride[1]+1\u230bW_{out} = \\left\\lfloor\\frac{W_{in} + 2 * \\text{padding[1]} - \\text{dilation[1]}\n      \\times (\\text{kernel\\_size[1]} - 1) - 1}{\\text{stride[1]}} + 1\\right\\rfloor\n\nWout\u200b=\u230astride[1]Win\u200b+2\u2217padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121\u200b+1\u230b\n\n\n\n"},
{"library": "torch", "item_id": "torch.nn.MaxPool1d", "item_type": "class", "code": "classtorch.nn.MaxPool1d(kernel_size:Union[T,Tuple[T,...]],stride:Optional[Union[T,Tuple[T,...]]]=None,padding:Union[T,Tuple[T,...]]=0,dilation:Union[T,Tuple[T,...]]=1,return_indices:bool=False,ceil_mode:bool=False)", "description": "Applies a 1D max pooling over an input signal composed of several input planes. In the simplest case, the output value of the layer with input size (N,C,L)(N, C, L)(N,C,L)   and output (N,C,Lout)(N, C, L_{out})(N,C,Lout\u200b)   can be precisely described as:  out(Ni,Cj,k)=max\u2061m=0,\u2026,kernel_size\u22121input(Ni,Cj,stride\u00d7k+m)out(N_i, C_j, k) = \\max_{m=0, \\ldots, \\text{kernel\\_size} - 1}         input(N_i, C_j, stride \\times k + m)  out(Ni\u200b,Cj\u200b,k)=m=0,\u2026,kernel_size\u22121max\u200binput(Ni\u200b,Cj\u200b,stride\u00d7k+m)  If padding is non-zero, then the input is implicitly zero-padded on both sides for padding number of points. dilation controls the spacing between the kernel points. It is harder to describe, but this link has a nice visualization of what dilation does.  Parameters  kernel_size \u2013 the size of the window to take a max over stride \u2013 the stride of the window. Default value is kernel_size padding \u2013 implicit zero padding to be added on both sides dilation \u2013 a parameter that controls the stride of elements in the window return_indices \u2013 if True, will return the max indices along with the outputs. Useful for torch.nn.MaxUnpool1d later ceil_mode \u2013 when True, will use ceil instead of floor to compute the output shape     Shape: Input: (N,C,Lin)(N, C, L_{in})(N,C,Lin\u200b)   Output: (N,C,Lout)(N, C, L_{out})(N,C,Lout\u200b)  , where  Lout=\u230aLin+2\u00d7padding\u2212dilation\u00d7(kernel_size\u22121)\u22121stride+1\u230bL_{out} = \\left\\lfloor \\frac{L_{in} + 2 \\times \\text{padding} - \\text{dilation}       \\times (\\text{kernel\\_size} - 1) - 1}{\\text{stride}} + 1\\right\\rfloor  Lout\u200b=\u230astrideLin\u200b+2\u00d7padding\u2212dilation\u00d7(kernel_size\u22121)\u22121\u200b+1\u230b      Examples: &gt;&gt;&gt; # pool of size=3, stride=2 &gt;&gt;&gt; m = nn.MaxPool1d(3, stride=2) &gt;&gt;&gt; input = torch.randn(20, 16, 50) &gt;&gt;&gt; output = m(input)   ", "parameters": ["kernel_size : the size of the window to take a max over", "stride : the stride of the window. Default value is kernel_size", "padding : implicit zero padding to be added on both sides", "dilation : a parameter that controls the stride of elements in the window", "return_indices : if True, will return the max indices along with the outputs.Useful for torch.nn.MaxUnpool1d later", "ceil_mode : when True, will use ceil instead of floor to compute the output shape"], "returns": [], "example": "NA", "shape": "\nInput: (N,C,Lin)(N, C, L_{in})(N,C,Lin\u200b)\n\n\nOutput: (N,C,Lout)(N, C, L_{out})(N,C,Lout\u200b)\n\n, where\n\nLout=\u230aLin+2\u00d7padding\u2212dilation\u00d7(kernel_size\u22121)\u22121stride+1\u230bL_{out} = \\left\\lfloor \\frac{L_{in} + 2 \\times \\text{padding} - \\text{dilation}\n      \\times (\\text{kernel\\_size} - 1) - 1}{\\text{stride}} + 1\\right\\rfloor\n\nLout\u200b=\u230astrideLin\u200b+2\u00d7padding\u2212dilation\u00d7(kernel_size\u22121)\u22121\u200b+1\u230b\n\n\n\n"},
{"library": "torch", "item_id": "torch.nn.Fold", "item_type": "class", "code": "classtorch.nn.Fold(output_size:Union[T,Tuple[T,...]],kernel_size:Union[T,Tuple[T,...]],dilation:Union[T,Tuple[T,...]]=1,padding:Union[T,Tuple[T,...]]=0,stride:Union[T,Tuple[T,...]]=1)", "description": "Combines an array of sliding local blocks into a large containing tensor. Consider a batched input tensor containing sliding local blocks, e.g., patches of images, of shape (N,C\u00d7\u220f(kernel_size),L)(N, C \\times  \\prod(\\text{kernel\\_size}), L)(N,C\u00d7\u220f(kernel_size),L)  , where NNN   is batch dimension, C\u00d7\u220f(kernel_size)C \\times \\prod(\\text{kernel\\_size})C\u00d7\u220f(kernel_size)   is the number of values within a block (a block has \u220f(kernel_size)\\prod(\\text{kernel\\_size})\u220f(kernel_size)   spatial locations each containing a CCC  -channeled vector), and LLL   is the total number of blocks. (This is exactly the same specification as the output shape of Unfold.) This operation combines these local blocks into the large output tensor of shape (N,C,output_size[0],output_size[1],\u2026\u2009)(N, C, \\text{output\\_size}[0], \\text{output\\_size}[1], \\dots)(N,C,output_size[0],output_size[1],\u2026)   by summing the overlapping values. Similar to Unfold, the arguments must satisfy  L=\u220fd\u230aoutput_size[d]+2\u00d7padding[d]\u2212dilation[d]\u00d7(kernel_size[d]\u22121)\u22121stride[d]+1\u230b,L = \\prod_d \\left\\lfloor\\frac{\\text{output\\_size}[d] + 2 \\times \\text{padding}[d] %     - \\text{dilation}[d] \\times (\\text{kernel\\_size}[d] - 1) - 1}{\\text{stride}[d]} + 1\\right\\rfloor,  L=d\u220f\u200b\u230astride[d]output_size[d]+2\u00d7padding[d]\u2212dilation[d]\u00d7(kernel_size[d]\u22121)\u22121\u200b+1\u230b,  where ddd   is over all spatial dimensions.  output_size describes the spatial shape of the large containing tensor of the sliding local blocks. It is useful to resolve the ambiguity when multiple input shapes map to same number of sliding blocks, e.g., with stride &gt; 0.  The padding, stride and dilation arguments specify how the sliding blocks are retrieved.  stride controls the stride for the sliding blocks. padding controls the amount of implicit zero-paddings on both sides for padding number of points for each dimension before reshaping. dilation controls the spacing between the kernel points; also known as the \u00e0 trous algorithm. It is harder to describe, but this link has a nice visualization of what dilation does.   Parameters  output_size (int or tuple) \u2013 the shape of the spatial dimensions of the output (i.e., output.sizes()[2:]) kernel_size (int or tuple) \u2013 the size of the sliding blocks stride (int or tuple) \u2013 the stride of the sliding blocks in the input spatial dimensions. Default: 1 padding (int or tuple, optional) \u2013 implicit zero padding to be added on both sides of input. Default: 0 dilation (int or tuple, optional) \u2013 a parameter that controls the stride of elements within the neighborhood. Default: 1     If output_size, kernel_size, dilation, padding or stride is an int or a tuple of length 1 then their values will be replicated across all spatial dimensions. For the case of two output spatial dimensions this operation is sometimes called col2im.   Note Fold calculates each combined value in the resulting large tensor by summing all values from all containing blocks. Unfold extracts the values in the local blocks by copying from the large tensor. So, if the blocks overlap, they are not inverses of each other. In general, folding and unfolding operations are related as follows. Consider Fold and Unfold instances created with the same parameters: &gt;&gt;&gt; fold_params = dict(kernel_size=..., dilation=..., padding=..., stride=...) &gt;&gt;&gt; fold = nn.Fold(output_size=..., **fold_params) &gt;&gt;&gt; unfold = nn.Unfold(**fold_params)   Then for any (supported) input tensor the following equality holds: fold(unfold(input)) == divisor * input   where divisor is a tensor that depends only on the shape and dtype of the input: &gt;&gt;&gt; input_ones = torch.ones(input.shape, dtype=input.dtype) &gt;&gt;&gt; divisor = fold(unfold(input_ones))   When the divisor tensor contains no zero elements, then fold and unfold operations are inverses of each other (up to constant divisor).   Warning Currently, only 4-D output tensors (batched image-like tensors) are supported.   Shape: Input: (N,C\u00d7\u220f(kernel_size),L)(N, C \\times \\prod(\\text{kernel\\_size}), L)(N,C\u00d7\u220f(kernel_size),L)   Output: (N,C,output_size[0],output_size[1],\u2026\u2009)(N, C, \\text{output\\_size}[0], \\text{output\\_size}[1], \\dots)(N,C,output_size[0],output_size[1],\u2026)   as described above    Examples: &gt;&gt;&gt; fold = nn.Fold(output_size=(4, 5), kernel_size=(2, 2)) &gt;&gt;&gt; input = torch.randn(1, 3 * 2 * 2, 12) &gt;&gt;&gt; output = fold(input) &gt;&gt;&gt; output.size() torch.Size([1, 3, 4, 5])   ", "parameters": ["output_size (int or tuple) : the shape of the spatial dimensions of theoutput (i.e., output.sizes()[2:])", "kernel_size (int or tuple) : the size of the sliding blocks", "stride (int or tuple) : the stride of the sliding blocks in the inputspatial dimensions. Default: 1", "padding (int or tuple, optional) : implicit zero padding to be added onboth sides of input. Default: 0", "dilation (int or tuple, optional) : a parameter that controls thestride of elements within theneighborhood. Default: 1"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.ConvTranspose2d", "item_type": "class", "code": "classtorch.nn.ConvTranspose2d(in_channels:int,out_channels:int,kernel_size:Union[T,Tuple[T,T]],stride:Union[T,Tuple[T,T]]=1,padding:Union[T,Tuple[T,T]]=0,output_padding:Union[T,Tuple[T,T]]=0,groups:int=1,bias:bool=True,dilation:int=1,padding_mode:str='zeros')", "description": "Applies a 2D transposed convolution operator over an input image composed of several input planes. This module can be seen as the gradient of Conv2d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation).  stride controls the stride for the cross-correlation. padding controls the amount of implicit zero-paddings on both sides for dilation * (kernel_size - 1) - padding number of points. See note below for details. output_padding controls the additional size added to one side of the output shape. See note below for details. dilation controls the spacing between the kernel points; also known as the \u00e0 trous algorithm. It is harder to describe, but this link has a nice visualization of what dilation does. groups controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups. For example,   At groups=1, all inputs are convolved to all outputs. At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels, and producing half the output channels, and both subsequently concatenated. At groups= in_channels, each input channel is convolved with its own set of filters (of size \u230aout_channelsin_channels\u230b\\left\\lfloor\\frac{out\\_channels}{in\\_channels}\\right\\rfloor\u230ain_channelsout_channels\u200b\u230b  ).     The parameters kernel_size, stride, padding, output_padding can either be:   a single int \u2013 in which case the same value is used for the height and width dimensions a tuple of two ints \u2013 in which case, the first int is used for the height dimension, and the second int for the width dimension    Note Depending of the size of your kernel, several (of the last) columns of the input might be lost, because it is a valid cross-correlation, and not a full cross-correlation. It is up to the user to add proper padding.   Note The padding argument effectively adds dilation * (kernel_size - 1) - padding amount of zero padding to both sizes of the input. This is set so that when a Conv2d and a ConvTranspose2d are initialized with same parameters, they are inverses of each other in regard to the input and output shapes. However, when stride &gt; 1, Conv2d maps multiple input shapes to the same output shape. output_padding is provided to resolve this ambiguity by effectively increasing the calculated output shape on one side. Note that output_padding is only used to find output shape, but does not actually add zero-padding to output.   Note In some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. Please see the notes on Reproducibility for background.   Parameters  in_channels (int) \u2013 Number of channels in the input image out_channels (int) \u2013 Number of channels produced by the convolution kernel_size (int or tuple) \u2013 Size of the convolving kernel stride (int or tuple, optional) \u2013 Stride of the convolution. Default: 1 padding (int or tuple, optional) \u2013 dilation * (kernel_size - 1) - padding zero-padding will be added to both sides of each dimension in the input. Default: 0 output_padding (int or tuple, optional) \u2013 Additional size added to one side of each dimension in the output shape. Default: 0 groups (int, optional) \u2013 Number of blocked connections from input channels to output channels. Default: 1 bias (bool, optional) \u2013 If True, adds a learnable bias to the output. Default: True dilation (int or tuple, optional) \u2013 Spacing between kernel elements. Default: 1     Shape: Input: (N,Cin,Hin,Win)(N, C_{in}, H_{in}, W_{in})(N,Cin\u200b,Hin\u200b,Win\u200b)   Output: (N,Cout,Hout,Wout)(N, C_{out}, H_{out}, W_{out})(N,Cout\u200b,Hout\u200b,Wout\u200b)   where   Hout=(Hin\u22121)\u00d7stride[0]\u22122\u00d7padding[0]+dilation[0]\u00d7(kernel_size[0]\u22121)+output_padding[0]+1H_{out} = (H_{in} - 1) \\times \\text{stride}[0] - 2 \\times \\text{padding}[0] + \\text{dilation}[0]           \\times (\\text{kernel\\_size}[0] - 1) + \\text{output\\_padding}[0] + 1  Hout\u200b=(Hin\u200b\u22121)\u00d7stride[0]\u22122\u00d7padding[0]+dilation[0]\u00d7(kernel_size[0]\u22121)+output_padding[0]+1   Wout=(Win\u22121)\u00d7stride[1]\u22122\u00d7padding[1]+dilation[1]\u00d7(kernel_size[1]\u22121)+output_padding[1]+1W_{out} = (W_{in} - 1) \\times \\text{stride}[1] - 2 \\times \\text{padding}[1] + \\text{dilation}[1]           \\times (\\text{kernel\\_size}[1] - 1) + \\text{output\\_padding}[1] + 1  Wout\u200b=(Win\u200b\u22121)\u00d7stride[1]\u22122\u00d7padding[1]+dilation[1]\u00d7(kernel_size[1]\u22121)+output_padding[1]+1     Variables  ~ConvTranspose2d.weight (Tensor) \u2013 the learnable weights of the module of shape (in_channels,out_channelsgroups,(\\text{in\\_channels}, \\frac{\\text{out\\_channels}}{\\text{groups}},(in_channels,groupsout_channels\u200b,   kernel_size[0],kernel_size[1])\\text{kernel\\_size[0]}, \\text{kernel\\_size[1]})kernel_size[0],kernel_size[1])  . The values of these weights are sampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b)   where k=groupsCout\u2217\u220fi=01kernel_size[i]k = \\frac{groups}{C_\\text{out} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}k=Cout\u200b\u2217\u220fi=01\u200bkernel_size[i]groups\u200b   ~ConvTranspose2d.bias (Tensor) \u2013 the learnable bias of the module of shape (out_channels) If bias is True, then the values of these weights are sampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b)   where k=groupsCout\u2217\u220fi=01kernel_size[i]k = \\frac{groups}{C_\\text{out} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}k=Cout\u200b\u2217\u220fi=01\u200bkernel_size[i]groups\u200b      Examples: &gt;&gt;&gt; # With square kernels and equal stride &gt;&gt;&gt; m = nn.ConvTranspose2d(16, 33, 3, stride=2) &gt;&gt;&gt; # non-square kernels and unequal stride and with padding &gt;&gt;&gt; m = nn.ConvTranspose2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2)) &gt;&gt;&gt; input = torch.randn(20, 16, 50, 100) &gt;&gt;&gt; output = m(input) &gt;&gt;&gt; # exact output size can be also specified as an argument &gt;&gt;&gt; input = torch.randn(1, 16, 12, 12) &gt;&gt;&gt; downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1) &gt;&gt;&gt; upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1) &gt;&gt;&gt; h = downsample(input) &gt;&gt;&gt; h.size() torch.Size([1, 16, 6, 6]) &gt;&gt;&gt; output = upsample(h, output_size=input.size()) &gt;&gt;&gt; output.size() torch.Size([1, 16, 12, 12])   ", "parameters": ["in_channels (int) : Number of channels in the input image", "out_channels (int) : Number of channels produced by the convolution", "kernel_size (int or tuple) : Size of the convolving kernel", "stride (int or tuple, optional) : Stride of the convolution. Default: 1", "padding (int or tuple, optional) : dilation * (kernel_size - 1) - padding zero-paddingwill be added to both sides of each dimension in the input. Default: 0", "output_padding (int or tuple, optional) : Additional size added to one sideof each dimension in the output shape. Default: 0", "groups (int, optional) : Number of blocked connections from input channels to output channels. Default: 1", "bias (bool, optional) : If True, adds a learnable bias to the output. Default: True", "dilation (int or tuple, optional) : Spacing between kernel elements. Default: 1", "~ConvTranspose2d.weight (Tensor) : the learnable weights of the module of shape(in_channels,out_channelsgroups,(\\text{in\\_channels}, \\frac{\\text{out\\_channels}}{\\text{groups}},(in_channels,groupsout_channels\u200b,kernel_size[0],kernel_size[1])\\text{kernel\\_size[0]}, \\text{kernel\\_size[1]})kernel_size[0],kernel_size[1]).The values of these weights are sampled fromU(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b) wherek=groupsCout\u2217\u220fi=01kernel_size[i]k = \\frac{groups}{C_\\text{out} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}k=Cout\u200b\u2217\u220fi=01\u200bkernel_size[i]groups\u200b", "~ConvTranspose2d.bias (Tensor) : the learnable bias of the module of shape (out_channels)If bias is True, then the values of these weights aresampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b) wherek=groupsCout\u2217\u220fi=01kernel_size[i]k = \\frac{groups}{C_\\text{out} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}k=Cout\u200b\u2217\u220fi=01\u200bkernel_size[i]groups\u200b"], "returns": [], "example": "NA", "shape": "\nInput: (N,Cin,Hin,Win)(N, C_{in}, H_{in}, W_{in})(N,Cin\u200b,Hin\u200b,Win\u200b)\n\n\nOutput: (N,Cout,Hout,Wout)(N, C_{out}, H_{out}, W_{out})(N,Cout\u200b,Hout\u200b,Wout\u200b)\n\n where\n\n\nHout=(Hin\u22121)\u00d7stride[0]\u22122\u00d7padding[0]+dilation[0]\u00d7(kernel_size[0]\u22121)+output_padding[0]+1H_{out} = (H_{in} - 1) \\times \\text{stride}[0] - 2 \\times \\text{padding}[0] + \\text{dilation}[0]\n          \\times (\\text{kernel\\_size}[0] - 1) + \\text{output\\_padding}[0] + 1\n\nHout\u200b=(Hin\u200b\u22121)\u00d7stride[0]\u22122\u00d7padding[0]+dilation[0]\u00d7(kernel_size[0]\u22121)+output_padding[0]+1\n\n\nWout=(Win\u22121)\u00d7stride[1]\u22122\u00d7padding[1]+dilation[1]\u00d7(kernel_size[1]\u22121)+output_padding[1]+1W_{out} = (W_{in} - 1) \\times \\text{stride}[1] - 2 \\times \\text{padding}[1] + \\text{dilation}[1]\n          \\times (\\text{kernel\\_size}[1] - 1) + \\text{output\\_padding}[1] + 1\n\nWout\u200b=(Win\u200b\u22121)\u00d7stride[1]\u22122\u00d7padding[1]+dilation[1]\u00d7(kernel_size[1]\u22121)+output_padding[1]+1\n\n"},
{"library": "torch", "item_id": "torch.nn.Unfold", "item_type": "class", "code": "classtorch.nn.Unfold(kernel_size:Union[T,Tuple[T,...]],dilation:Union[T,Tuple[T,...]]=1,padding:Union[T,Tuple[T,...]]=0,stride:Union[T,Tuple[T,...]]=1)", "description": "Extracts sliding local blocks from a batched input tensor. Consider a batched input tensor of shape (N,C,\u2217)(N, C, *)(N,C,\u2217)  , where NNN   is the batch dimension, CCC   is the channel dimension, and \u2217*\u2217   represent arbitrary spatial dimensions. This operation flattens each sliding kernel_size-sized block within the spatial dimensions of input into a column (i.e., last dimension) of a 3-D output tensor of shape (N,C\u00d7\u220f(kernel_size),L)(N, C \\times \\prod(\\text{kernel\\_size}), L)(N,C\u00d7\u220f(kernel_size),L)  , where C\u00d7\u220f(kernel_size)C \\times \\prod(\\text{kernel\\_size})C\u00d7\u220f(kernel_size)   is the total number of values within each block (a block has \u220f(kernel_size)\\prod(\\text{kernel\\_size})\u220f(kernel_size)   spatial locations each containing a CCC  -channeled vector), and LLL   is the total number of such blocks:  L=\u220fd\u230aspatial_size[d]+2\u00d7padding[d]\u2212dilation[d]\u00d7(kernel_size[d]\u22121)\u22121stride[d]+1\u230b,L = \\prod_d \\left\\lfloor\\frac{\\text{spatial\\_size}[d] + 2 \\times \\text{padding}[d] %     - \\text{dilation}[d] \\times (\\text{kernel\\_size}[d] - 1) - 1}{\\text{stride}[d]} + 1\\right\\rfloor,  L=d\u220f\u200b\u230astride[d]spatial_size[d]+2\u00d7padding[d]\u2212dilation[d]\u00d7(kernel_size[d]\u22121)\u22121\u200b+1\u230b,  where spatial_size\\text{spatial\\_size}spatial_size   is formed by the spatial dimensions of input (\u2217*\u2217   above), and ddd   is over all spatial dimensions. Therefore, indexing output at the last dimension (column dimension) gives all values within a certain block. The padding, stride and dilation arguments specify how the sliding blocks are retrieved.  stride controls the stride for the sliding blocks. padding controls the amount of implicit zero-paddings on both sides for padding number of points for each dimension before reshaping. dilation controls the spacing between the kernel points; also known as the \u00e0 trous algorithm. It is harder to describe, but this link has a nice visualization of what dilation does.   Parameters  kernel_size (int or tuple) \u2013 the size of the sliding blocks stride (int or tuple, optional) \u2013 the stride of the sliding blocks in the input spatial dimensions. Default: 1 padding (int or tuple, optional) \u2013 implicit zero padding to be added on both sides of input. Default: 0 dilation (int or tuple, optional) \u2013 a parameter that controls the stride of elements within the neighborhood. Default: 1     If kernel_size, dilation, padding or stride is an int or a tuple of length 1, their values will be replicated across all spatial dimensions. For the case of two input spatial dimensions this operation is sometimes called im2col.   Note Fold calculates each combined value in the resulting large tensor by summing all values from all containing blocks. Unfold extracts the values in the local blocks by copying from the large tensor. So, if the blocks overlap, they are not inverses of each other. In general, folding and unfolding operations are related as follows. Consider Fold and Unfold instances created with the same parameters: &gt;&gt;&gt; fold_params = dict(kernel_size=..., dilation=..., padding=..., stride=...) &gt;&gt;&gt; fold = nn.Fold(output_size=..., **fold_params) &gt;&gt;&gt; unfold = nn.Unfold(**fold_params)   Then for any (supported) input tensor the following equality holds: fold(unfold(input)) == divisor * input   where divisor is a tensor that depends only on the shape and dtype of the input: &gt;&gt;&gt; input_ones = torch.ones(input.shape, dtype=input.dtype) &gt;&gt;&gt; divisor = fold(unfold(input_ones))   When the divisor tensor contains no zero elements, then fold and unfold operations are inverses of each other (up to constant divisor).   Warning Currently, only 4-D input tensors (batched image-like tensors) are supported.   Shape: Input: (N,C,\u2217)(N, C, *)(N,C,\u2217)   Output: (N,C\u00d7\u220f(kernel_size),L)(N, C \\times \\prod(\\text{kernel\\_size}), L)(N,C\u00d7\u220f(kernel_size),L)   as described above    Examples: &gt;&gt;&gt; unfold = nn.Unfold(kernel_size=(2, 3)) &gt;&gt;&gt; input = torch.randn(2, 5, 3, 4) &gt;&gt;&gt; output = unfold(input) &gt;&gt;&gt; # each patch contains 30 values (2x3=6 vectors, each of 5 channels) &gt;&gt;&gt; # 4 blocks (2x3 kernels) in total in the 3x4 input &gt;&gt;&gt; output.size() torch.Size([2, 30, 4])  &gt;&gt;&gt; # Convolution is equivalent with Unfold + Matrix Multiplication + Fold (or view to output shape) &gt;&gt;&gt; inp = torch.randn(1, 3, 10, 12) &gt;&gt;&gt; w = torch.randn(2, 3, 4, 5) &gt;&gt;&gt; inp_unf = torch.nn.functional.unfold(inp, (4, 5)) &gt;&gt;&gt; out_unf = inp_unf.transpose(1, 2).matmul(w.view(w.size(0), -1).t()).transpose(1, 2) &gt;&gt;&gt; out = torch.nn.functional.fold(out_unf, (7, 8), (1, 1)) &gt;&gt;&gt; # or equivalently (and avoiding a copy), &gt;&gt;&gt; # out = out_unf.view(1, 2, 7, 8) &gt;&gt;&gt; (torch.nn.functional.conv2d(inp, w) - out).abs().max() tensor(1.9073e-06)   ", "parameters": ["kernel_size (int or tuple) : the size of the sliding blocks", "stride (int or tuple, optional) : the stride of the sliding blocks in the inputspatial dimensions. Default: 1", "padding (int or tuple, optional) : implicit zero padding to be added onboth sides of input. Default: 0", "dilation (int or tuple, optional) : a parameter that controls thestride of elements within theneighborhood. Default: 1"], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.ConvTranspose1d", "item_type": "class", "code": "classtorch.nn.ConvTranspose1d(in_channels:int,out_channels:int,kernel_size:Union[T,Tuple[T]],stride:Union[T,Tuple[T]]=1,padding:Union[T,Tuple[T]]=0,output_padding:Union[T,Tuple[T]]=0,groups:int=1,bias:bool=True,dilation:Union[T,Tuple[T]]=1,padding_mode:str='zeros')", "description": "Applies a 1D transposed convolution operator over an input image composed of several input planes. This module can be seen as the gradient of Conv1d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation).  stride controls the stride for the cross-correlation. padding controls the amount of implicit zero-paddings on both sides for dilation * (kernel_size - 1) - padding number of points. See note below for details. output_padding controls the additional size added to one side of the output shape. See note below for details. dilation controls the spacing between the kernel points; also known as the \u00e0 trous algorithm. It is harder to describe, but this link has a nice visualization of what dilation does. groups controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups. For example,   At groups=1, all inputs are convolved to all outputs. At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels, and producing half the output channels, and both subsequently concatenated. At groups= in_channels, each input channel is convolved with its own set of filters (of size \u230aout_channelsin_channels\u230b\\left\\lfloor\\frac{out\\_channels}{in\\_channels}\\right\\rfloor\u230ain_channelsout_channels\u200b\u230b  ).      Note Depending of the size of your kernel, several (of the last) columns of the input might be lost, because it is a valid cross-correlation, and not a full cross-correlation. It is up to the user to add proper padding.   Note The padding argument effectively adds dilation * (kernel_size - 1) - padding amount of zero padding to both sizes of the input. This is set so that when a Conv1d and a ConvTranspose1d are initialized with same parameters, they are inverses of each other in regard to the input and output shapes. However, when stride &gt; 1, Conv1d maps multiple input shapes to the same output shape. output_padding is provided to resolve this ambiguity by effectively increasing the calculated output shape on one side. Note that output_padding is only used to find output shape, but does not actually add zero-padding to output.   Note In some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. Please see the notes on Reproducibility for background.   Parameters  in_channels (int) \u2013 Number of channels in the input image out_channels (int) \u2013 Number of channels produced by the convolution kernel_size (int or tuple) \u2013 Size of the convolving kernel stride (int or tuple, optional) \u2013 Stride of the convolution. Default: 1 padding (int or tuple, optional) \u2013 dilation * (kernel_size - 1) - padding zero-padding will be added to both sides of the input. Default: 0 output_padding (int or tuple, optional) \u2013 Additional size added to one side of the output shape. Default: 0 groups (int, optional) \u2013 Number of blocked connections from input channels to output channels. Default: 1 bias (bool, optional) \u2013 If True, adds a learnable bias to the output. Default: True dilation (int or tuple, optional) \u2013 Spacing between kernel elements. Default: 1     Shape: Input: (N,Cin,Lin)(N, C_{in}, L_{in})(N,Cin\u200b,Lin\u200b)   Output: (N,Cout,Lout)(N, C_{out}, L_{out})(N,Cout\u200b,Lout\u200b)   where  Lout=(Lin\u22121)\u00d7stride\u22122\u00d7padding+dilation\u00d7(kernel_size\u22121)+output_padding+1L_{out} = (L_{in} - 1) \\times \\text{stride} - 2 \\times \\text{padding} + \\text{dilation}           \\times (\\text{kernel\\_size} - 1) + \\text{output\\_padding} + 1  Lout\u200b=(Lin\u200b\u22121)\u00d7stride\u22122\u00d7padding+dilation\u00d7(kernel_size\u22121)+output_padding+1       Variables  ~ConvTranspose1d.weight (Tensor) \u2013 the learnable weights of the module of shape (in_channels,out_channelsgroups,(\\text{in\\_channels}, \\frac{\\text{out\\_channels}}{\\text{groups}},(in_channels,groupsout_channels\u200b,   kernel_size)\\text{kernel\\_size})kernel_size)  . The values of these weights are sampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b)   where k=groupsCout\u2217kernel_sizek = \\frac{groups}{C_\\text{out} * \\text{kernel\\_size}}k=Cout\u200b\u2217kernel_sizegroups\u200b   ~ConvTranspose1d.bias (Tensor) \u2013 the learnable bias of the module of shape (out_channels). If bias is True, then the values of these weights are sampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b)   where k=groupsCout\u2217kernel_sizek = \\frac{groups}{C_\\text{out} * \\text{kernel\\_size}}k=Cout\u200b\u2217kernel_sizegroups\u200b      ", "parameters": ["in_channels (int) : Number of channels in the input image", "out_channels (int) : Number of channels produced by the convolution", "kernel_size (int or tuple) : Size of the convolving kernel", "stride (int or tuple, optional) : Stride of the convolution. Default: 1", "padding (int or tuple, optional) : dilation * (kernel_size - 1) - padding zero-paddingwill be added to both sides of the input. Default: 0", "output_padding (int or tuple, optional) : Additional size added to one sideof the output shape. Default: 0", "groups (int, optional) : Number of blocked connections from input channels to output channels. Default: 1", "bias (bool, optional) : If True, adds a learnable bias to the output. Default: True", "dilation (int or tuple, optional) : Spacing between kernel elements. Default: 1", "~ConvTranspose1d.weight (Tensor) : the learnable weights of the module of shape(in_channels,out_channelsgroups,(\\text{in\\_channels}, \\frac{\\text{out\\_channels}}{\\text{groups}},(in_channels,groupsout_channels\u200b,kernel_size)\\text{kernel\\_size})kernel_size).The values of these weights are sampled fromU(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b) wherek=groupsCout\u2217kernel_sizek = \\frac{groups}{C_\\text{out} * \\text{kernel\\_size}}k=Cout\u200b\u2217kernel_sizegroups\u200b", "~ConvTranspose1d.bias (Tensor) : the learnable bias of the module of shape (out_channels).If bias is True, then the values of these weights aresampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b) wherek=groupsCout\u2217kernel_sizek = \\frac{groups}{C_\\text{out} * \\text{kernel\\_size}}k=Cout\u200b\u2217kernel_sizegroups\u200b"], "returns": [], "example": "NA", "shape": "\nInput: (N,Cin,Lin)(N, C_{in}, L_{in})(N,Cin\u200b,Lin\u200b)\n\n\nOutput: (N,Cout,Lout)(N, C_{out}, L_{out})(N,Cout\u200b,Lout\u200b)\n\n where\n\nLout=(Lin\u22121)\u00d7stride\u22122\u00d7padding+dilation\u00d7(kernel_size\u22121)+output_padding+1L_{out} = (L_{in} - 1) \\times \\text{stride} - 2 \\times \\text{padding} + \\text{dilation}\n          \\times (\\text{kernel\\_size} - 1) + \\text{output\\_padding} + 1\n\nLout\u200b=(Lin\u200b\u22121)\u00d7stride\u22122\u00d7padding+dilation\u00d7(kernel_size\u22121)+output_padding+1\n\n\n\n"},
{"library": "torch", "item_id": "torch.nn.Conv1d", "item_type": "class", "code": "classtorch.nn.Conv1d(in_channels:int,out_channels:int,kernel_size:Union[T,Tuple[T]],stride:Union[T,Tuple[T]]=1,padding:Union[T,Tuple[T]]=0,dilation:Union[T,Tuple[T]]=1,groups:int=1,bias:bool=True,padding_mode:str='zeros')", "description": "Applies a 1D convolution over an input signal composed of several input planes. In the simplest case, the output value of the layer with input size (N,Cin,L)(N, C_{\\text{in}}, L)(N,Cin\u200b,L)   and output (N,Cout,Lout)(N, C_{\\text{out}}, L_{\\text{out}})(N,Cout\u200b,Lout\u200b)   can be precisely described as:  out(Ni,Coutj)=bias(Coutj)+\u2211k=0Cin\u22121weight(Coutj,k)\u22c6input(Ni,k)\\text{out}(N_i, C_{\\text{out}_j}) = \\text{bias}(C_{\\text{out}_j}) + \\sum_{k = 0}^{C_{in} - 1} \\text{weight}(C_{\\text{out}_j}, k) \\star \\text{input}(N_i, k)  out(Ni\u200b,Coutj\u200b\u200b)=bias(Coutj\u200b\u200b)+k=0\u2211Cin\u200b\u22121\u200bweight(Coutj\u200b\u200b,k)\u22c6input(Ni\u200b,k)  where \u22c6\\star\u22c6   is the valid cross-correlation operator, NNN   is a batch size, CCC   denotes a number of channels, LLL   is a length of signal sequence.  stride controls the stride for the cross-correlation, a single number or a one-element tuple. padding controls the amount of implicit zero-paddings on both sides for padding number of points. dilation controls the spacing between the kernel points; also known as the \u00e0 trous algorithm. It is harder to describe, but this link has a nice visualization of what dilation does. groups controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups. For example,   At groups=1, all inputs are convolved to all outputs. At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels, and producing half the output channels, and both subsequently concatenated. At groups= in_channels, each input channel is convolved with its own set of filters, of size \u230aout_channelsin_channels\u230b\\left\\lfloor\\frac{out\\_channels}{in\\_channels}\\right\\rfloor\u230ain_channelsout_channels\u200b\u230b  .      Note Depending of the size of your kernel, several (of the last) columns of the input might be lost, because it is a valid cross-correlation, and not a full cross-correlation. It is up to the user to add proper padding.   Note When groups == in_channels and out_channels == K * in_channels, where K is a positive integer, this operation is also termed in literature as depthwise convolution. In other words, for an input of size (N,Cin,Lin)(N, C_{in}, L_{in})(N,Cin\u200b,Lin\u200b)  , a depthwise convolution with a depthwise multiplier K, can be constructed by arguments (Cin=Cin,Cout=Cin\u00d7K,...,groups=Cin)(C_\\text{in}=C_{in}, C_\\text{out}=C_{in} \\times K, ..., \\text{groups}=C_{in})(Cin\u200b=Cin\u200b,Cout\u200b=Cin\u200b\u00d7K,...,groups=Cin\u200b)  .   Note In some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. Please see the notes on Reproducibility for background.   Parameters  in_channels (int) \u2013 Number of channels in the input image out_channels (int) \u2013 Number of channels produced by the convolution kernel_size (int or tuple) \u2013 Size of the convolving kernel stride (int or tuple, optional) \u2013 Stride of the convolution. Default: 1 padding (int or tuple, optional) \u2013 Zero-padding added to both sides of the input. Default: 0 padding_mode (string, optional) \u2013 'zeros', 'reflect', 'replicate' or 'circular'. Default: 'zeros' dilation (int or tuple, optional) \u2013 Spacing between kernel elements. Default: 1 groups (int, optional) \u2013 Number of blocked connections from input channels to output channels. Default: 1 bias (bool, optional) \u2013 If True, adds a learnable bias to the output. Default: True     Shape: Input: (N,Cin,Lin)(N, C_{in}, L_{in})(N,Cin\u200b,Lin\u200b)   Output: (N,Cout,Lout)(N, C_{out}, L_{out})(N,Cout\u200b,Lout\u200b)   where  Lout=\u230aLin+2\u00d7padding\u2212dilation\u00d7(kernel_size\u22121)\u22121stride+1\u230bL_{out} = \\left\\lfloor\\frac{L_{in} + 2 \\times \\text{padding} - \\text{dilation}           \\times (\\text{kernel\\_size} - 1) - 1}{\\text{stride}} + 1\\right\\rfloor  Lout\u200b=\u230astrideLin\u200b+2\u00d7padding\u2212dilation\u00d7(kernel_size\u22121)\u22121\u200b+1\u230b       Variables  ~Conv1d.weight (Tensor) \u2013 the learnable weights of the module of shape (out_channels,in_channelsgroups,kernel_size)(\\text{out\\_channels}, \\frac{\\text{in\\_channels}}{\\text{groups}}, \\text{kernel\\_size})(out_channels,groupsin_channels\u200b,kernel_size)  . The values of these weights are sampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b)   where k=groupsCin\u2217kernel_sizek = \\frac{groups}{C_\\text{in} * \\text{kernel\\_size}}k=Cin\u200b\u2217kernel_sizegroups\u200b   ~Conv1d.bias (Tensor) \u2013 the learnable bias of the module of shape (out_channels). If bias is True, then the values of these weights are sampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b)   where k=groupsCin\u2217kernel_sizek = \\frac{groups}{C_\\text{in} * \\text{kernel\\_size}}k=Cin\u200b\u2217kernel_sizegroups\u200b      Examples: &gt;&gt;&gt; m = nn.Conv1d(16, 33, 3, stride=2) &gt;&gt;&gt; input = torch.randn(20, 16, 50) &gt;&gt;&gt; output = m(input)   ", "parameters": ["in_channels (int) : Number of channels in the input image", "out_channels (int) : Number of channels produced by the convolution", "kernel_size (int or tuple) : Size of the convolving kernel", "stride (int or tuple, optional) : Stride of the convolution. Default: 1", "padding (int or tuple, optional) : Zero-padding added to both sides ofthe input. Default: 0", "padding_mode (string, optional) : 'zeros', 'reflect','replicate' or 'circular'. Default: 'zeros'", "dilation (int or tuple, optional) : Spacing between kernelelements. Default: 1", "groups (int, optional) : Number of blocked connections from inputchannels to output channels. Default: 1", "bias (bool, optional) : If True, adds a learnable bias to theoutput. Default: True", "~Conv1d.weight (Tensor) : the learnable weights of the module of shape(out_channels,in_channelsgroups,kernel_size)(\\text{out\\_channels},\\frac{\\text{in\\_channels}}{\\text{groups}}, \\text{kernel\\_size})(out_channels,groupsin_channels\u200b,kernel_size).The values of these weights are sampled fromU(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b) wherek=groupsCin\u2217kernel_sizek = \\frac{groups}{C_\\text{in} * \\text{kernel\\_size}}k=Cin\u200b\u2217kernel_sizegroups\u200b", "~Conv1d.bias (Tensor) : the learnable bias of the module of shape(out_channels). If bias is True, then the values of these weights aresampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b) wherek=groupsCin\u2217kernel_sizek = \\frac{groups}{C_\\text{in} * \\text{kernel\\_size}}k=Cin\u200b\u2217kernel_sizegroups\u200b"], "returns": [], "example": "NA", "shape": "\nInput: (N,Cin,Lin)(N, C_{in}, L_{in})(N,Cin\u200b,Lin\u200b)\n\n\nOutput: (N,Cout,Lout)(N, C_{out}, L_{out})(N,Cout\u200b,Lout\u200b)\n\n where\n\nLout=\u230aLin+2\u00d7padding\u2212dilation\u00d7(kernel_size\u22121)\u22121stride+1\u230bL_{out} = \\left\\lfloor\\frac{L_{in} + 2 \\times \\text{padding} - \\text{dilation}\n          \\times (\\text{kernel\\_size} - 1) - 1}{\\text{stride}} + 1\\right\\rfloor\n\nLout\u200b=\u230astrideLin\u200b+2\u00d7padding\u2212dilation\u00d7(kernel_size\u22121)\u22121\u200b+1\u230b\n\n\n\n"},
{"library": "torch", "item_id": "torch.nn.Conv2d", "item_type": "class", "code": "classtorch.nn.Conv2d(in_channels:int,out_channels:int,kernel_size:Union[T,Tuple[T,T]],stride:Union[T,Tuple[T,T]]=1,padding:Union[T,Tuple[T,T]]=0,dilation:Union[T,Tuple[T,T]]=1,groups:int=1,bias:bool=True,padding_mode:str='zeros')", "description": "Applies a 2D convolution over an input signal composed of several input planes. In the simplest case, the output value of the layer with input size (N,Cin,H,W)(N, C_{\\text{in}}, H, W)(N,Cin\u200b,H,W)   and output (N,Cout,Hout,Wout)(N, C_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})(N,Cout\u200b,Hout\u200b,Wout\u200b)   can be precisely described as:  out(Ni,Coutj)=bias(Coutj)+\u2211k=0Cin\u22121weight(Coutj,k)\u22c6input(Ni,k)\\text{out}(N_i, C_{\\text{out}_j}) = \\text{bias}(C_{\\text{out}_j}) + \\sum_{k = 0}^{C_{\\text{in}} - 1} \\text{weight}(C_{\\text{out}_j}, k) \\star \\text{input}(N_i, k)  out(Ni\u200b,Coutj\u200b\u200b)=bias(Coutj\u200b\u200b)+k=0\u2211Cin\u200b\u22121\u200bweight(Coutj\u200b\u200b,k)\u22c6input(Ni\u200b,k)  where \u22c6\\star\u22c6   is the valid 2D cross-correlation operator, NNN   is a batch size, CCC   denotes a number of channels, HHH   is a height of input planes in pixels, and WWW   is width in pixels.  stride controls the stride for the cross-correlation, a single number or a tuple. padding controls the amount of implicit zero-paddings on both sides for padding number of points for each dimension. dilation controls the spacing between the kernel points; also known as the \u00e0 trous algorithm. It is harder to describe, but this link has a nice visualization of what dilation does. groups controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups. For example,   At groups=1, all inputs are convolved to all outputs. At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels, and producing half the output channels, and both subsequently concatenated. At groups= in_channels, each input channel is convolved with its own set of filters, of size: \u230aout_channelsin_channels\u230b\\left\\lfloor\\frac{out\\_channels}{in\\_channels}\\right\\rfloor\u230ain_channelsout_channels\u200b\u230b  .     The parameters kernel_size, stride, padding, dilation can either be:   a single int \u2013 in which case the same value is used for the height and width dimension a tuple of two ints \u2013 in which case, the first int is used for the height dimension, and the second int for the width dimension    Note Depending of the size of your kernel, several (of the last) columns of the input might be lost, because it is a valid cross-correlation, and not a full cross-correlation. It is up to the user to add proper padding.   Note When groups == in_channels and out_channels == K * in_channels, where K is a positive integer, this operation is also termed in literature as depthwise convolution. In other words, for an input of size (N,Cin,Hin,Win)(N, C_{in}, H_{in}, W_{in})(N,Cin\u200b,Hin\u200b,Win\u200b)  , a depthwise convolution with a depthwise multiplier K, can be constructed by arguments (in_channels=Cin,out_channels=Cin\u00d7K,...,groups=Cin)(in\\_channels=C_{in}, out\\_channels=C_{in} \\times K, ..., groups=C_{in})(in_channels=Cin\u200b,out_channels=Cin\u200b\u00d7K,...,groups=Cin\u200b)  .   Note In some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. Please see the notes on Reproducibility for background.   Parameters  in_channels (int) \u2013 Number of channels in the input image out_channels (int) \u2013 Number of channels produced by the convolution kernel_size (int or tuple) \u2013 Size of the convolving kernel stride (int or tuple, optional) \u2013 Stride of the convolution. Default: 1 padding (int or tuple, optional) \u2013 Zero-padding added to both sides of the input. Default: 0 padding_mode (string, optional) \u2013 'zeros', 'reflect', 'replicate' or 'circular'. Default: 'zeros' dilation (int or tuple, optional) \u2013 Spacing between kernel elements. Default: 1 groups (int, optional) \u2013 Number of blocked connections from input channels to output channels. Default: 1 bias (bool, optional) \u2013 If True, adds a learnable bias to the output. Default: True     Shape: Input: (N,Cin,Hin,Win)(N, C_{in}, H_{in}, W_{in})(N,Cin\u200b,Hin\u200b,Win\u200b)   Output: (N,Cout,Hout,Wout)(N, C_{out}, H_{out}, W_{out})(N,Cout\u200b,Hout\u200b,Wout\u200b)   where  Hout=\u230aHin+2\u00d7padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121stride[0]+1\u230bH_{out} = \\left\\lfloor\\frac{H_{in}  + 2 \\times \\text{padding}[0] - \\text{dilation}[0]           \\times (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor  Hout\u200b=\u230astride[0]Hin\u200b+2\u00d7padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121\u200b+1\u230b   Wout=\u230aWin+2\u00d7padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121stride[1]+1\u230bW_{out} = \\left\\lfloor\\frac{W_{in}  + 2 \\times \\text{padding}[1] - \\text{dilation}[1]           \\times (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor  Wout\u200b=\u230astride[1]Win\u200b+2\u00d7padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121\u200b+1\u230b       Variables  ~Conv2d.weight (Tensor) \u2013 the learnable weights of the module of shape (out_channels,in_channelsgroups,(\\text{out\\_channels}, \\frac{\\text{in\\_channels}}{\\text{groups}},(out_channels,groupsin_channels\u200b,   kernel_size[0],kernel_size[1])\\text{kernel\\_size[0]}, \\text{kernel\\_size[1]})kernel_size[0],kernel_size[1])  . The values of these weights are sampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b)   where k=groupsCin\u2217\u220fi=01kernel_size[i]k = \\frac{groups}{C_\\text{in} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}k=Cin\u200b\u2217\u220fi=01\u200bkernel_size[i]groups\u200b   ~Conv2d.bias (Tensor) \u2013 the learnable bias of the module of shape (out_channels). If bias is True, then the values of these weights are sampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b)   where k=groupsCin\u2217\u220fi=01kernel_size[i]k = \\frac{groups}{C_\\text{in} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}k=Cin\u200b\u2217\u220fi=01\u200bkernel_size[i]groups\u200b      Examples &gt;&gt;&gt; # With square kernels and equal stride &gt;&gt;&gt; m = nn.Conv2d(16, 33, 3, stride=2) &gt;&gt;&gt; # non-square kernels and unequal stride and with padding &gt;&gt;&gt; m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2)) &gt;&gt;&gt; # non-square kernels and unequal stride and with padding and dilation &gt;&gt;&gt; m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1)) &gt;&gt;&gt; input = torch.randn(20, 16, 50, 100) &gt;&gt;&gt; output = m(input)   ", "parameters": ["in_channels (int) : Number of channels in the input image", "out_channels (int) : Number of channels produced by the convolution", "kernel_size (int or tuple) : Size of the convolving kernel", "stride (int or tuple, optional) : Stride of the convolution. Default: 1", "padding (int or tuple, optional) : Zero-padding added to both sides ofthe input. Default: 0", "padding_mode (string, optional) : 'zeros', 'reflect','replicate' or 'circular'. Default: 'zeros'", "dilation (int or tuple, optional) : Spacing between kernel elements. Default: 1", "groups (int, optional) : Number of blocked connections from inputchannels to output channels. Default: 1", "bias (bool, optional) : If True, adds a learnable bias to theoutput. Default: True", "~Conv2d.weight (Tensor) : the learnable weights of the module of shape(out_channels,in_channelsgroups,(\\text{out\\_channels}, \\frac{\\text{in\\_channels}}{\\text{groups}},(out_channels,groupsin_channels\u200b,kernel_size[0],kernel_size[1])\\text{kernel\\_size[0]}, \\text{kernel\\_size[1]})kernel_size[0],kernel_size[1]).The values of these weights are sampled fromU(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b) wherek=groupsCin\u2217\u220fi=01kernel_size[i]k = \\frac{groups}{C_\\text{in} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}k=Cin\u200b\u2217\u220fi=01\u200bkernel_size[i]groups\u200b", "~Conv2d.bias (Tensor) : the learnable bias of the module of shape(out_channels). If bias is True,then the values of these weights aresampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b) wherek=groupsCin\u2217\u220fi=01kernel_size[i]k = \\frac{groups}{C_\\text{in} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}k=Cin\u200b\u2217\u220fi=01\u200bkernel_size[i]groups\u200b"], "returns": [], "example": "NA", "shape": "\nInput: (N,Cin,Hin,Win)(N, C_{in}, H_{in}, W_{in})(N,Cin\u200b,Hin\u200b,Win\u200b)\n\n\nOutput: (N,Cout,Hout,Wout)(N, C_{out}, H_{out}, W_{out})(N,Cout\u200b,Hout\u200b,Wout\u200b)\n\n where\n\nHout=\u230aHin+2\u00d7padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121stride[0]+1\u230bH_{out} = \\left\\lfloor\\frac{H_{in}  + 2 \\times \\text{padding}[0] - \\text{dilation}[0]\n          \\times (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor\n\nHout\u200b=\u230astride[0]Hin\u200b+2\u00d7padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121\u200b+1\u230b\n\n\nWout=\u230aWin+2\u00d7padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121stride[1]+1\u230bW_{out} = \\left\\lfloor\\frac{W_{in}  + 2 \\times \\text{padding}[1] - \\text{dilation}[1]\n          \\times (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor\n\nWout\u200b=\u230astride[1]Win\u200b+2\u00d7padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121\u200b+1\u230b\n\n\n\n"},
{"library": "torch", "item_id": "torch.nn.Conv3d", "item_type": "class", "code": "classtorch.nn.Conv3d(in_channels:int,out_channels:int,kernel_size:Union[T,Tuple[T,T,T]],stride:Union[T,Tuple[T,T,T]]=1,padding:Union[T,Tuple[T,T,T]]=0,dilation:Union[T,Tuple[T,T,T]]=1,groups:int=1,bias:bool=True,padding_mode:str='zeros')", "description": "Applies a 3D convolution over an input signal composed of several input planes. In the simplest case, the output value of the layer with input size (N,Cin,D,H,W)(N, C_{in}, D, H, W)(N,Cin\u200b,D,H,W)   and output (N,Cout,Dout,Hout,Wout)(N, C_{out}, D_{out}, H_{out}, W_{out})(N,Cout\u200b,Dout\u200b,Hout\u200b,Wout\u200b)   can be precisely described as:  out(Ni,Coutj)=bias(Coutj)+\u2211k=0Cin\u22121weight(Coutj,k)\u22c6input(Ni,k)out(N_i, C_{out_j}) = bias(C_{out_j}) +                         \\sum_{k = 0}^{C_{in} - 1} weight(C_{out_j}, k) \\star input(N_i, k)  out(Ni\u200b,Coutj\u200b\u200b)=bias(Coutj\u200b\u200b)+k=0\u2211Cin\u200b\u22121\u200bweight(Coutj\u200b\u200b,k)\u22c6input(Ni\u200b,k)  where \u22c6\\star\u22c6   is the valid 3D cross-correlation operator  stride controls the stride for the cross-correlation. padding controls the amount of implicit zero-paddings on both sides for padding number of points for each dimension. dilation controls the spacing between the kernel points; also known as the \u00e0 trous algorithm. It is harder to describe, but this link has a nice visualization of what dilation does. groups controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups. For example,   At groups=1, all inputs are convolved to all outputs. At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels, and producing half the output channels, and both subsequently concatenated. At groups= in_channels, each input channel is convolved with its own set of filters, of size \u230aout_channelsin_channels\u230b\\left\\lfloor\\frac{out\\_channels}{in\\_channels}\\right\\rfloor\u230ain_channelsout_channels\u200b\u230b  .     The parameters kernel_size, stride, padding, dilation can either be:   a single int \u2013 in which case the same value is used for the depth, height and width dimension a tuple of three ints \u2013 in which case, the first int is used for the depth dimension, the second int for the height dimension and the third int for the width dimension    Note Depending of the size of your kernel, several (of the last) columns of the input might be lost, because it is a valid cross-correlation, and not a full cross-correlation. It is up to the user to add proper padding.   Note When groups == in_channels and out_channels == K * in_channels, where K is a positive integer, this operation is also termed in literature as depthwise convolution. In other words, for an input of size (N,Cin,Din,Hin,Win)(N, C_{in}, D_{in}, H_{in}, W_{in})(N,Cin\u200b,Din\u200b,Hin\u200b,Win\u200b)  , a depthwise convolution with a depthwise multiplier K, can be constructed by arguments (in_channels=Cin,out_channels=Cin\u00d7K,...,groups=Cin)(in\\_channels=C_{in}, out\\_channels=C_{in} \\times K, ..., groups=C_{in})(in_channels=Cin\u200b,out_channels=Cin\u200b\u00d7K,...,groups=Cin\u200b)  .   Note In some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. Please see the notes on Reproducibility for background.   Parameters  in_channels (int) \u2013 Number of channels in the input image out_channels (int) \u2013 Number of channels produced by the convolution kernel_size (int or tuple) \u2013 Size of the convolving kernel stride (int or tuple, optional) \u2013 Stride of the convolution. Default: 1 padding (int or tuple, optional) \u2013 Zero-padding added to all three sides of the input. Default: 0 padding_mode (string, optional) \u2013 'zeros', 'reflect', 'replicate' or 'circular'. Default: 'zeros' dilation (int or tuple, optional) \u2013 Spacing between kernel elements. Default: 1 groups (int, optional) \u2013 Number of blocked connections from input channels to output channels. Default: 1 bias (bool, optional) \u2013 If True, adds a learnable bias to the output. Default: True     Shape: Input: (N,Cin,Din,Hin,Win)(N, C_{in}, D_{in}, H_{in}, W_{in})(N,Cin\u200b,Din\u200b,Hin\u200b,Win\u200b)   Output: (N,Cout,Dout,Hout,Wout)(N, C_{out}, D_{out}, H_{out}, W_{out})(N,Cout\u200b,Dout\u200b,Hout\u200b,Wout\u200b)   where  Dout=\u230aDin+2\u00d7padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121stride[0]+1\u230bD_{out} = \\left\\lfloor\\frac{D_{in} + 2 \\times \\text{padding}[0] - \\text{dilation}[0]       \\times (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor  Dout\u200b=\u230astride[0]Din\u200b+2\u00d7padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121\u200b+1\u230b   Hout=\u230aHin+2\u00d7padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121stride[1]+1\u230bH_{out} = \\left\\lfloor\\frac{H_{in} + 2 \\times \\text{padding}[1] - \\text{dilation}[1]       \\times (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor  Hout\u200b=\u230astride[1]Hin\u200b+2\u00d7padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121\u200b+1\u230b   Wout=\u230aWin+2\u00d7padding[2]\u2212dilation[2]\u00d7(kernel_size[2]\u22121)\u22121stride[2]+1\u230bW_{out} = \\left\\lfloor\\frac{W_{in} + 2 \\times \\text{padding}[2] - \\text{dilation}[2]       \\times (\\text{kernel\\_size}[2] - 1) - 1}{\\text{stride}[2]} + 1\\right\\rfloor  Wout\u200b=\u230astride[2]Win\u200b+2\u00d7padding[2]\u2212dilation[2]\u00d7(kernel_size[2]\u22121)\u22121\u200b+1\u230b       Variables  ~Conv3d.weight (Tensor) \u2013 the learnable weights of the module of shape (out_channels,in_channelsgroups,(\\text{out\\_channels}, \\frac{\\text{in\\_channels}}{\\text{groups}},(out_channels,groupsin_channels\u200b,   kernel_size[0],kernel_size[1],kernel_size[2])\\text{kernel\\_size[0]}, \\text{kernel\\_size[1]}, \\text{kernel\\_size[2]})kernel_size[0],kernel_size[1],kernel_size[2])  . The values of these weights are sampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b)   where k=groupsCin\u2217\u220fi=02kernel_size[i]k = \\frac{groups}{C_\\text{in} * \\prod_{i=0}^{2}\\text{kernel\\_size}[i]}k=Cin\u200b\u2217\u220fi=02\u200bkernel_size[i]groups\u200b   ~Conv3d.bias (Tensor) \u2013 the learnable bias of the module of shape (out_channels). If bias is True, then the values of these weights are sampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b)   where k=groupsCin\u2217\u220fi=02kernel_size[i]k = \\frac{groups}{C_\\text{in} * \\prod_{i=0}^{2}\\text{kernel\\_size}[i]}k=Cin\u200b\u2217\u220fi=02\u200bkernel_size[i]groups\u200b      Examples: &gt;&gt;&gt; # With square kernels and equal stride &gt;&gt;&gt; m = nn.Conv3d(16, 33, 3, stride=2) &gt;&gt;&gt; # non-square kernels and unequal stride and with padding &gt;&gt;&gt; m = nn.Conv3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(4, 2, 0)) &gt;&gt;&gt; input = torch.randn(20, 16, 10, 50, 100) &gt;&gt;&gt; output = m(input)   ", "parameters": ["in_channels (int) : Number of channels in the input image", "out_channels (int) : Number of channels produced by the convolution", "kernel_size (int or tuple) : Size of the convolving kernel", "stride (int or tuple, optional) : Stride of the convolution. Default: 1", "padding (int or tuple, optional) : Zero-padding added to all three sides of the input. Default: 0", "padding_mode (string, optional) : 'zeros', 'reflect', 'replicate' or 'circular'. Default: 'zeros'", "dilation (int or tuple, optional) : Spacing between kernel elements. Default: 1", "groups (int, optional) : Number of blocked connections from input channels to output channels. Default: 1", "bias (bool, optional) : If True, adds a learnable bias to the output. Default: True", "~Conv3d.weight (Tensor) : the learnable weights of the module of shape(out_channels,in_channelsgroups,(\\text{out\\_channels}, \\frac{\\text{in\\_channels}}{\\text{groups}},(out_channels,groupsin_channels\u200b,kernel_size[0],kernel_size[1],kernel_size[2])\\text{kernel\\_size[0]}, \\text{kernel\\_size[1]}, \\text{kernel\\_size[2]})kernel_size[0],kernel_size[1],kernel_size[2]).The values of these weights are sampled fromU(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b) wherek=groupsCin\u2217\u220fi=02kernel_size[i]k = \\frac{groups}{C_\\text{in} * \\prod_{i=0}^{2}\\text{kernel\\_size}[i]}k=Cin\u200b\u2217\u220fi=02\u200bkernel_size[i]groups\u200b", "~Conv3d.bias (Tensor) : the learnable bias of the module of shape (out_channels). If bias is True,then the values of these weights aresampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b) wherek=groupsCin\u2217\u220fi=02kernel_size[i]k = \\frac{groups}{C_\\text{in} * \\prod_{i=0}^{2}\\text{kernel\\_size}[i]}k=Cin\u200b\u2217\u220fi=02\u200bkernel_size[i]groups\u200b"], "returns": [], "example": "NA", "shape": "\nInput: (N,Cin,Din,Hin,Win)(N, C_{in}, D_{in}, H_{in}, W_{in})(N,Cin\u200b,Din\u200b,Hin\u200b,Win\u200b)\n\n\nOutput: (N,Cout,Dout,Hout,Wout)(N, C_{out}, D_{out}, H_{out}, W_{out})(N,Cout\u200b,Dout\u200b,Hout\u200b,Wout\u200b)\n\n where\n\nDout=\u230aDin+2\u00d7padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121stride[0]+1\u230bD_{out} = \\left\\lfloor\\frac{D_{in} + 2 \\times \\text{padding}[0] - \\text{dilation}[0]\n      \\times (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor\n\nDout\u200b=\u230astride[0]Din\u200b+2\u00d7padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121\u200b+1\u230b\n\n\nHout=\u230aHin+2\u00d7padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121stride[1]+1\u230bH_{out} = \\left\\lfloor\\frac{H_{in} + 2 \\times \\text{padding}[1] - \\text{dilation}[1]\n      \\times (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor\n\nHout\u200b=\u230astride[1]Hin\u200b+2\u00d7padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121\u200b+1\u230b\n\n\nWout=\u230aWin+2\u00d7padding[2]\u2212dilation[2]\u00d7(kernel_size[2]\u22121)\u22121stride[2]+1\u230bW_{out} = \\left\\lfloor\\frac{W_{in} + 2 \\times \\text{padding}[2] - \\text{dilation}[2]\n      \\times (\\text{kernel\\_size}[2] - 1) - 1}{\\text{stride}[2]} + 1\\right\\rfloor\n\nWout\u200b=\u230astride[2]Win\u200b+2\u00d7padding[2]\u2212dilation[2]\u00d7(kernel_size[2]\u22121)\u22121\u200b+1\u230b\n\n\n\n"},
{"library": "torch", "item_id": "torch.nn.ConvTranspose3d", "item_type": "class", "code": "classtorch.nn.ConvTranspose3d(in_channels:int,out_channels:int,kernel_size:Union[T,Tuple[T,T,T]],stride:Union[T,Tuple[T,T,T]]=1,padding:Union[T,Tuple[T,T,T]]=0,output_padding:Union[T,Tuple[T,T,T]]=0,groups:int=1,bias:bool=True,dilation:Union[T,Tuple[T,T,T]]=1,padding_mode:str='zeros')", "description": "Applies a 3D transposed convolution operator over an input image composed of several input planes. The transposed convolution operator multiplies each input value element-wise by a learnable kernel, and sums over the outputs from all input feature planes. This module can be seen as the gradient of Conv3d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation).  stride controls the stride for the cross-correlation. padding controls the amount of implicit zero-paddings on both sides for dilation * (kernel_size - 1) - padding number of points. See note below for details. output_padding controls the additional size added to one side of the output shape. See note below for details. dilation controls the spacing between the kernel points; also known as the \u00e0 trous algorithm. It is harder to describe, but this link has a nice visualization of what dilation does. groups controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups. For example,   At groups=1, all inputs are convolved to all outputs. At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels, and producing half the output channels, and both subsequently concatenated. At groups= in_channels, each input channel is convolved with its own set of filters (of size \u230aout_channelsin_channels\u230b\\left\\lfloor\\frac{out\\_channels}{in\\_channels}\\right\\rfloor\u230ain_channelsout_channels\u200b\u230b  ).     The parameters kernel_size, stride, padding, output_padding can either be:   a single int \u2013 in which case the same value is used for the depth, height and width dimensions a tuple of three ints \u2013 in which case, the first int is used for the depth dimension, the second int for the height dimension and the third int for the width dimension    Note Depending of the size of your kernel, several (of the last) columns of the input might be lost, because it is a valid cross-correlation, and not a full cross-correlation. It is up to the user to add proper padding.   Note The padding argument effectively adds dilation * (kernel_size - 1) - padding amount of zero padding to both sizes of the input. This is set so that when a Conv3d and a ConvTranspose3d are initialized with same parameters, they are inverses of each other in regard to the input and output shapes. However, when stride &gt; 1, Conv3d maps multiple input shapes to the same output shape. output_padding is provided to resolve this ambiguity by effectively increasing the calculated output shape on one side. Note that output_padding is only used to find output shape, but does not actually add zero-padding to output.   Note In some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. Please see the notes on Reproducibility for background.   Parameters  in_channels (int) \u2013 Number of channels in the input image out_channels (int) \u2013 Number of channels produced by the convolution kernel_size (int or tuple) \u2013 Size of the convolving kernel stride (int or tuple, optional) \u2013 Stride of the convolution. Default: 1 padding (int or tuple, optional) \u2013 dilation * (kernel_size - 1) - padding zero-padding will be added to both sides of each dimension in the input. Default: 0 output_padding (int or tuple, optional) \u2013 Additional size added to one side of each dimension in the output shape. Default: 0 groups (int, optional) \u2013 Number of blocked connections from input channels to output channels. Default: 1 bias (bool, optional) \u2013 If True, adds a learnable bias to the output. Default: True dilation (int or tuple, optional) \u2013 Spacing between kernel elements. Default: 1     Shape: Input: (N,Cin,Din,Hin,Win)(N, C_{in}, D_{in}, H_{in}, W_{in})(N,Cin\u200b,Din\u200b,Hin\u200b,Win\u200b)   Output: (N,Cout,Dout,Hout,Wout)(N, C_{out}, D_{out}, H_{out}, W_{out})(N,Cout\u200b,Dout\u200b,Hout\u200b,Wout\u200b)   where   Dout=(Din\u22121)\u00d7stride[0]\u22122\u00d7padding[0]+dilation[0]\u00d7(kernel_size[0]\u22121)+output_padding[0]+1D_{out} = (D_{in} - 1) \\times \\text{stride}[0] - 2 \\times \\text{padding}[0] + \\text{dilation}[0]           \\times (\\text{kernel\\_size}[0] - 1) + \\text{output\\_padding}[0] + 1  Dout\u200b=(Din\u200b\u22121)\u00d7stride[0]\u22122\u00d7padding[0]+dilation[0]\u00d7(kernel_size[0]\u22121)+output_padding[0]+1   Hout=(Hin\u22121)\u00d7stride[1]\u22122\u00d7padding[1]+dilation[1]\u00d7(kernel_size[1]\u22121)+output_padding[1]+1H_{out} = (H_{in} - 1) \\times \\text{stride}[1] - 2 \\times \\text{padding}[1] + \\text{dilation}[1]           \\times (\\text{kernel\\_size}[1] - 1) + \\text{output\\_padding}[1] + 1  Hout\u200b=(Hin\u200b\u22121)\u00d7stride[1]\u22122\u00d7padding[1]+dilation[1]\u00d7(kernel_size[1]\u22121)+output_padding[1]+1   Wout=(Win\u22121)\u00d7stride[2]\u22122\u00d7padding[2]+dilation[2]\u00d7(kernel_size[2]\u22121)+output_padding[2]+1W_{out} = (W_{in} - 1) \\times \\text{stride}[2] - 2 \\times \\text{padding}[2] + \\text{dilation}[2]           \\times (\\text{kernel\\_size}[2] - 1) + \\text{output\\_padding}[2] + 1  Wout\u200b=(Win\u200b\u22121)\u00d7stride[2]\u22122\u00d7padding[2]+dilation[2]\u00d7(kernel_size[2]\u22121)+output_padding[2]+1     Variables  ~ConvTranspose3d.weight (Tensor) \u2013 the learnable weights of the module of shape (in_channels,out_channelsgroups,(\\text{in\\_channels}, \\frac{\\text{out\\_channels}}{\\text{groups}},(in_channels,groupsout_channels\u200b,   kernel_size[0],kernel_size[1],kernel_size[2])\\text{kernel\\_size[0]}, \\text{kernel\\_size[1]}, \\text{kernel\\_size[2]})kernel_size[0],kernel_size[1],kernel_size[2])  . The values of these weights are sampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b)   where k=groupsCout\u2217\u220fi=02kernel_size[i]k = \\frac{groups}{C_\\text{out} * \\prod_{i=0}^{2}\\text{kernel\\_size}[i]}k=Cout\u200b\u2217\u220fi=02\u200bkernel_size[i]groups\u200b   ~ConvTranspose3d.bias (Tensor) \u2013 the learnable bias of the module of shape (out_channels) If bias is True, then the values of these weights are sampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b)   where k=groupsCout\u2217\u220fi=02kernel_size[i]k = \\frac{groups}{C_\\text{out} * \\prod_{i=0}^{2}\\text{kernel\\_size}[i]}k=Cout\u200b\u2217\u220fi=02\u200bkernel_size[i]groups\u200b      Examples: &gt;&gt;&gt; # With square kernels and equal stride &gt;&gt;&gt; m = nn.ConvTranspose3d(16, 33, 3, stride=2) &gt;&gt;&gt; # non-square kernels and unequal stride and with padding &gt;&gt;&gt; m = nn.ConvTranspose3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(0, 4, 2)) &gt;&gt;&gt; input = torch.randn(20, 16, 10, 50, 100) &gt;&gt;&gt; output = m(input)   ", "parameters": ["in_channels (int) : Number of channels in the input image", "out_channels (int) : Number of channels produced by the convolution", "kernel_size (int or tuple) : Size of the convolving kernel", "stride (int or tuple, optional) : Stride of the convolution. Default: 1", "padding (int or tuple, optional) : dilation * (kernel_size - 1) - padding zero-paddingwill be added to both sides of each dimension in the input. Default: 0", "output_padding (int or tuple, optional) : Additional size added to one sideof each dimension in the output shape. Default: 0", "groups (int, optional) : Number of blocked connections from input channels to output channels. Default: 1", "bias (bool, optional) : If True, adds a learnable bias to the output. Default: True", "dilation (int or tuple, optional) : Spacing between kernel elements. Default: 1", "~ConvTranspose3d.weight (Tensor) : the learnable weights of the module of shape(in_channels,out_channelsgroups,(\\text{in\\_channels}, \\frac{\\text{out\\_channels}}{\\text{groups}},(in_channels,groupsout_channels\u200b,kernel_size[0],kernel_size[1],kernel_size[2])\\text{kernel\\_size[0]}, \\text{kernel\\_size[1]}, \\text{kernel\\_size[2]})kernel_size[0],kernel_size[1],kernel_size[2]).The values of these weights are sampled fromU(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b) wherek=groupsCout\u2217\u220fi=02kernel_size[i]k = \\frac{groups}{C_\\text{out} * \\prod_{i=0}^{2}\\text{kernel\\_size}[i]}k=Cout\u200b\u2217\u220fi=02\u200bkernel_size[i]groups\u200b", "~ConvTranspose3d.bias (Tensor) : the learnable bias of the module of shape (out_channels)If bias is True, then the values of these weights aresampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b) wherek=groupsCout\u2217\u220fi=02kernel_size[i]k = \\frac{groups}{C_\\text{out} * \\prod_{i=0}^{2}\\text{kernel\\_size}[i]}k=Cout\u200b\u2217\u220fi=02\u200bkernel_size[i]groups\u200b"], "returns": [], "example": "NA", "shape": "\nInput: (N,Cin,Din,Hin,Win)(N, C_{in}, D_{in}, H_{in}, W_{in})(N,Cin\u200b,Din\u200b,Hin\u200b,Win\u200b)\n\n\nOutput: (N,Cout,Dout,Hout,Wout)(N, C_{out}, D_{out}, H_{out}, W_{out})(N,Cout\u200b,Dout\u200b,Hout\u200b,Wout\u200b)\n\n where\n\n\nDout=(Din\u22121)\u00d7stride[0]\u22122\u00d7padding[0]+dilation[0]\u00d7(kernel_size[0]\u22121)+output_padding[0]+1D_{out} = (D_{in} - 1) \\times \\text{stride}[0] - 2 \\times \\text{padding}[0] + \\text{dilation}[0]\n          \\times (\\text{kernel\\_size}[0] - 1) + \\text{output\\_padding}[0] + 1\n\nDout\u200b=(Din\u200b\u22121)\u00d7stride[0]\u22122\u00d7padding[0]+dilation[0]\u00d7(kernel_size[0]\u22121)+output_padding[0]+1\n\n\nHout=(Hin\u22121)\u00d7stride[1]\u22122\u00d7padding[1]+dilation[1]\u00d7(kernel_size[1]\u22121)+output_padding[1]+1H_{out} = (H_{in} - 1) \\times \\text{stride}[1] - 2 \\times \\text{padding}[1] + \\text{dilation}[1]\n          \\times (\\text{kernel\\_size}[1] - 1) + \\text{output\\_padding}[1] + 1\n\nHout\u200b=(Hin\u200b\u22121)\u00d7stride[1]\u22122\u00d7padding[1]+dilation[1]\u00d7(kernel_size[1]\u22121)+output_padding[1]+1\n\n\nWout=(Win\u22121)\u00d7stride[2]\u22122\u00d7padding[2]+dilation[2]\u00d7(kernel_size[2]\u22121)+output_padding[2]+1W_{out} = (W_{in} - 1) \\times \\text{stride}[2] - 2 \\times \\text{padding}[2] + \\text{dilation}[2]\n          \\times (\\text{kernel\\_size}[2] - 1) + \\text{output\\_padding}[2] + 1\n\nWout\u200b=(Win\u200b\u22121)\u00d7stride[2]\u22122\u00d7padding[2]+dilation[2]\u00d7(kernel_size[2]\u22121)+output_padding[2]+1\n\n"},
{"library": "torch", "item_id": "torch.nn.ModuleList.append", "item_type": "method", "code": "append(module:torch.nn.modules.module.Module)\u2192T", "description": "Appends a given module to the end of the list.  Parameters module (nn.Module) \u2013 module to append   ", "parameters": ["module (nn.Module) : module to append"], "returns": [], "example": "class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])\n\n    def forward(self, x):\n        # ModuleList can act as an iterable, or be indexed using ints\n        for i, l in enumerate(self.linears):\n            x = self.linears[i // 2](x) + l(x)\n        return x\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.ModuleList.extend", "item_type": "method", "code": "extend(modules:Iterable[torch.nn.modules.module.Module])\u2192T", "description": "Appends modules from a Python iterable to the end of the list.  Parameters modules (iterable) \u2013 iterable of modules to append   ", "parameters": ["modules (iterable) : iterable of modules to append"], "returns": [], "example": "class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])\n\n    def forward(self, x):\n        # ModuleList can act as an iterable, or be indexed using ints\n        for i, l in enumerate(self.linears):\n            x = self.linears[i // 2](x) + l(x)\n        return x\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.ModuleList.insert", "item_type": "method", "code": "insert(index:int,module:torch.nn.modules.module.Module)\u2192None", "description": "Insert a given module before a given index in the list.  Parameters  index (int) \u2013 index to insert. module (nn.Module) \u2013 module to insert    ", "parameters": ["index (int) : index to insert.", "module (nn.Module) : module to insert"], "returns": [], "example": "class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])\n\n    def forward(self, x):\n        # ModuleList can act as an iterable, or be indexed using ints\n        for i, l in enumerate(self.linears):\n            x = self.linears[i // 2](x) + l(x)\n        return x\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.ModuleList", "item_type": "class", "code": "classtorch.nn.ModuleList(modules:Optional[Iterable[torch.nn.modules.module.Module]]=None)", "description": "Holds submodules in a list. ModuleList can be indexed like a regular Python list, but modules it contains are properly registered, and will be visible by all Module methods.  Parameters modules (iterable, optional) \u2013 an iterable of modules to add   Example: class MyModule(nn.Module):     def __init__(self):         super(MyModule, self).__init__()         self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])      def forward(self, x):         # ModuleList can act as an iterable, or be indexed using ints         for i, l in enumerate(self.linears):             x = self.linears[i // 2](x) + l(x)         return x     append(module: torch.nn.modules.module.Module) \u2192 T  Appends a given module to the end of the list.  Parameters module (nn.Module) \u2013 module to append       extend(modules: Iterable[torch.nn.modules.module.Module]) \u2192 T  Appends modules from a Python iterable to the end of the list.  Parameters modules (iterable) \u2013 iterable of modules to append       insert(index: int, module: torch.nn.modules.module.Module) \u2192 None  Insert a given module before a given index in the list.  Parameters  index (int) \u2013 index to insert. module (nn.Module) \u2013 module to insert      ", "parameters": ["index (int) : index to insert.", "module (nn.Module) : module to insert"], "returns": [], "example": "class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])\n\n    def forward(self, x):\n        # ModuleList can act as an iterable, or be indexed using ints\n        for i, l in enumerate(self.linears):\n            x = self.linears[i // 2](x) + l(x)\n        return x\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.ModuleDict.clear", "item_type": "method", "code": "clear()\u2192None", "description": "Remove all items from the ModuleDict. ", "parameters": [], "returns": [], "example": "class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.choices = nn.ModuleDict({\n                'conv': nn.Conv2d(10, 10, 3),\n                'pool': nn.MaxPool2d(3)\n        })\n        self.activations = nn.ModuleDict([\n                ['lrelu', nn.LeakyReLU()],\n                ['prelu', nn.PReLU()]\n        ])\n\n    def forward(self, x, choice, act):\n        x = self.choices[choice](x)\n        x = self.activations[act](x)\n        return x\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.ModuleDict.items", "item_type": "method", "code": "items()\u2192Iterable[Tuple[str,torch.nn.modules.module.Module]]", "description": "Return an iterable of the ModuleDict key/value pairs. ", "parameters": [], "returns": [], "example": "class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.choices = nn.ModuleDict({\n                'conv': nn.Conv2d(10, 10, 3),\n                'pool': nn.MaxPool2d(3)\n        })\n        self.activations = nn.ModuleDict([\n                ['lrelu', nn.LeakyReLU()],\n                ['prelu', nn.PReLU()]\n        ])\n\n    def forward(self, x, choice, act):\n        x = self.choices[choice](x)\n        x = self.activations[act](x)\n        return x\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.ParameterDict.clear", "item_type": "method", "code": "clear()\u2192None", "description": "Remove all items from the ParameterDict. ", "parameters": [], "returns": [], "example": "class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.params = nn.ParameterDict({\n                'left': nn.Parameter(torch.randn(5, 10)),\n                'right': nn.Parameter(torch.randn(5, 10))\n        })\n\n    def forward(self, x, choice):\n        x = self.params[choice].mm(x)\n        return x\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.ParameterDict.items", "item_type": "method", "code": "items()\u2192Iterable[Tuple[str,Parameter]]", "description": "Return an iterable of the ParameterDict key/value pairs. ", "parameters": [], "returns": [], "example": "class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.params = nn.ParameterDict({\n                'left': nn.Parameter(torch.randn(5, 10)),\n                'right': nn.Parameter(torch.randn(5, 10))\n        })\n\n    def forward(self, x, choice):\n        x = self.params[choice].mm(x)\n        return x\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.ParameterDict.keys", "item_type": "method", "code": "keys()\u2192Iterable[str]", "description": "Return an iterable of the ParameterDict keys. ", "parameters": [], "returns": [], "example": "class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.params = nn.ParameterDict({\n                'left': nn.Parameter(torch.randn(5, 10)),\n                'right': nn.Parameter(torch.randn(5, 10))\n        })\n\n    def forward(self, x, choice):\n        x = self.params[choice].mm(x)\n        return x\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.ParameterDict.pop", "item_type": "method", "code": "pop(key:str)\u2192Parameter", "description": "Remove key from the ParameterDict and return its parameter.  Parameters key (string) \u2013 key to pop from the ParameterDict   ", "parameters": ["key (string) : key to pop from the ParameterDict"], "returns": [], "example": "class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.params = nn.ParameterDict({\n                'left': nn.Parameter(torch.randn(5, 10)),\n                'right': nn.Parameter(torch.randn(5, 10))\n        })\n\n    def forward(self, x, choice):\n        x = self.params[choice].mm(x)\n        return x\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.ParameterDict.update", "item_type": "method", "code": "update(parameters:Mapping[str,Parameter])\u2192None", "description": "Update the ParameterDict with the key-value pairs from a mapping or an iterable, overwriting existing keys.  Note If parameters is an OrderedDict, a ParameterDict, or an iterable of key-value pairs, the order of new elements in it is preserved.   Parameters parameters (iterable) \u2013 a mapping (dictionary) from string to Parameter, or an iterable of key-value pairs of type (string, Parameter)   ", "parameters": ["parameters (iterable) : a mapping (dictionary) from string toParameter, or an iterable ofkey-value pairs of type (string, Parameter)"], "returns": [], "example": "class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.params = nn.ParameterDict({\n                'left': nn.Parameter(torch.randn(5, 10)),\n                'right': nn.Parameter(torch.randn(5, 10))\n        })\n\n    def forward(self, x, choice):\n        x = self.params[choice].mm(x)\n        return x\n\n", "shape": ""},
{"library": "torch"},
{"library": "torch"},
{"library": "torch"},
{"library": "torch"},
{"library": "torch", "item_id": "torch.nn.Module.add_module", "item_type": "method", "code": "add_module(name:str,module:Optional[Module])\u2192None", "description": "Adds a child module to the current module. The module can be accessed as an attribute using the given name.  Parameters  name (string) \u2013 name of the child module. The child module can be accessed from this module using the given name module (Module) \u2013 child module to be added to the module.    ", "parameters": ["name (string) : name of the child module. The child module can beaccessed from this module using the given name", "module (Module) : child module to be added to the module."], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Module.apply", "item_type": "method", "code": "apply(fn:Callable[Module,None])\u2192T", "description": "Applies fn recursively to every submodule (as returned by .children()) as well as self. Typical use includes initializing the parameters of a model (see also torch.nn.init).  Parameters fn (Module -&gt; None) \u2013 function to be applied to each submodule  Returns self  Return type Module   Example: &gt;&gt;&gt; @torch.no_grad() &gt;&gt;&gt; def init_weights(m): &gt;&gt;&gt;     print(m) &gt;&gt;&gt;     if type(m) == nn.Linear: &gt;&gt;&gt;         m.weight.fill_(1.0) &gt;&gt;&gt;         print(m.weight) &gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) &gt;&gt;&gt; net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1.,  1.],         [ 1.,  1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1.,  1.],         [ 1.,  1.]]) Sequential(   (0): Linear(in_features=2, out_features=2, bias=True)   (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential(   (0): Linear(in_features=2, out_features=2, bias=True)   (1): Linear(in_features=2, out_features=2, bias=True) )   ", "parameters": ["fn (Module -&gt; None) : function to be applied to each submodule", "self", "Module"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.ParameterList.append", "item_type": "method", "code": "append(parameter:Parameter)\u2192T", "description": "Appends a given parameter at the end of the list.  Parameters parameter (nn.Parameter) \u2013 parameter to append   ", "parameters": ["parameter (nn.Parameter) : parameter to append"], "returns": [], "example": "class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.params = nn.ParameterList([nn.Parameter(torch.randn(10, 10)) for i in range(10)])\n\n    def forward(self, x):\n        # ParameterList can act as an iterable, or be indexed using ints\n        for i, p in enumerate(self.params):\n            x = self.params[i // 2].mm(x) + p.mm(x)\n        return x\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Sequential", "item_type": "class", "code": "classtorch.nn.Sequential(*args:Any)", "description": "A sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an ordered dict of modules can also be passed in. To make it easier to understand, here is a small example: # Example of using Sequential model = nn.Sequential(           nn.Conv2d(1,20,5),           nn.ReLU(),           nn.Conv2d(20,64,5),           nn.ReLU()         )  # Example of using Sequential with OrderedDict model = nn.Sequential(OrderedDict([           ('conv1', nn.Conv2d(1,20,5)),           ('relu1', nn.ReLU()),           ('conv2', nn.Conv2d(20,64,5)),           ('relu2', nn.ReLU())         ]))   ", "parameters": [], "returns": [], "example": "NA", "shape": ""},
{"library": "torch", "item_id": "torch.nn.ModuleDict.keys", "item_type": "method", "code": "keys()\u2192Iterable[str]", "description": "Return an iterable of the ModuleDict keys. ", "parameters": [], "returns": [], "example": "class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.choices = nn.ModuleDict({\n                'conv': nn.Conv2d(10, 10, 3),\n                'pool': nn.MaxPool2d(3)\n        })\n        self.activations = nn.ModuleDict([\n                ['lrelu', nn.LeakyReLU()],\n                ['prelu', nn.PReLU()]\n        ])\n\n    def forward(self, x, choice, act):\n        x = self.choices[choice](x)\n        x = self.activations[act](x)\n        return x\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.ModuleDict.pop", "item_type": "method", "code": "pop(key:str)\u2192torch.nn.modules.module.Module", "description": "Remove key from the ModuleDict and return its module.  Parameters key (string) \u2013 key to pop from the ModuleDict   ", "parameters": ["key (string) : key to pop from the ModuleDict"], "returns": [], "example": "class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.choices = nn.ModuleDict({\n                'conv': nn.Conv2d(10, 10, 3),\n                'pool': nn.MaxPool2d(3)\n        })\n        self.activations = nn.ModuleDict([\n                ['lrelu', nn.LeakyReLU()],\n                ['prelu', nn.PReLU()]\n        ])\n\n    def forward(self, x, choice, act):\n        x = self.choices[choice](x)\n        x = self.activations[act](x)\n        return x\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.ModuleDict.update", "item_type": "method", "code": "update(modules:Mapping[str,torch.nn.modules.module.Module])\u2192None", "description": "Update the ModuleDict with the key-value pairs from a mapping or an iterable, overwriting existing keys.  Note If modules is an OrderedDict, a ModuleDict, or an iterable of key-value pairs, the order of new elements in it is preserved.   Parameters modules (iterable) \u2013 a mapping (dictionary) from string to Module, or an iterable of key-value pairs of type (string, Module)   ", "parameters": ["modules (iterable) : a mapping (dictionary) from string to Module,or an iterable of key-value pairs of type (string, Module)"], "returns": [], "example": "class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.choices = nn.ModuleDict({\n                'conv': nn.Conv2d(10, 10, 3),\n                'pool': nn.MaxPool2d(3)\n        })\n        self.activations = nn.ModuleDict([\n                ['lrelu', nn.LeakyReLU()],\n                ['prelu', nn.PReLU()]\n        ])\n\n    def forward(self, x, choice, act):\n        x = self.choices[choice](x)\n        x = self.activations[act](x)\n        return x\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.ParameterDict.values", "item_type": "method", "code": "values()\u2192Iterable[Parameter]", "description": "Return an iterable of the ParameterDict values. ", "parameters": [], "returns": [], "example": "class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.params = nn.ParameterDict({\n                'left': nn.Parameter(torch.randn(5, 10)),\n                'right': nn.Parameter(torch.randn(5, 10))\n        })\n\n    def forward(self, x, choice):\n        x = self.params[choice].mm(x)\n        return x\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.ParameterDict", "item_type": "class", "code": "classtorch.nn.ParameterDict(parameters:Optional[Mapping[str,Parameter]]=None)", "description": "Holds parameters in a dictionary. ParameterDict can be indexed like a regular Python dictionary, but parameters it contains are properly registered, and will be visible by all Module methods. ParameterDict is an ordered dictionary that respects  the order of insertion, and in update(), the order of the merged OrderedDict or another ParameterDict (the argument to update()).  Note that update() with other unordered mapping types (e.g., Python\u2019s plain dict) does not preserve the order of the merged mapping.  Parameters parameters (iterable, optional) \u2013 a mapping (dictionary) of (string : Parameter) or an iterable of key-value pairs of type (string, Parameter)   Example: class MyModule(nn.Module):     def __init__(self):         super(MyModule, self).__init__()         self.params = nn.ParameterDict({                 'left': nn.Parameter(torch.randn(5, 10)),                 'right': nn.Parameter(torch.randn(5, 10))         })      def forward(self, x, choice):         x = self.params[choice].mm(x)         return x     clear() \u2192 None  Remove all items from the ParameterDict.     items() \u2192 Iterable[Tuple[str, Parameter]]  Return an iterable of the ParameterDict key/value pairs.     keys() \u2192 Iterable[str]  Return an iterable of the ParameterDict keys.     pop(key: str) \u2192 Parameter  Remove key from the ParameterDict and return its parameter.  Parameters key (string) \u2013 key to pop from the ParameterDict       update(parameters: Mapping[str, Parameter]) \u2192 None  Update the ParameterDict with the key-value pairs from a mapping or an iterable, overwriting existing keys.  Note If parameters is an OrderedDict, a ParameterDict, or an iterable of key-value pairs, the order of new elements in it is preserved.   Parameters parameters (iterable) \u2013 a mapping (dictionary) from string to Parameter, or an iterable of key-value pairs of type (string, Parameter)       values() \u2192 Iterable[Parameter]  Return an iterable of the ParameterDict values.   ", "parameters": ["parameters (iterable, optional) : a mapping (dictionary) of(string : Parameter) or an iterable of key-value pairsof type (string, Parameter)", "key (string) : key to pop from the ParameterDict", "parameters (iterable) : a mapping (dictionary) from string toParameter, or an iterable ofkey-value pairs of type (string, Parameter)"], "returns": [], "example": "class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.params = nn.ParameterDict({\n                'left': nn.Parameter(torch.randn(5, 10)),\n                'right': nn.Parameter(torch.randn(5, 10))\n        })\n\n    def forward(self, x, choice):\n        x = self.params[choice].mm(x)\n        return x\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Module.bfloat16", "item_type": "method", "code": "bfloat16()\u2192T", "description": "Casts all floating point parameters and buffers to bfloat16 datatype.  Returns self  Return type Module   ", "parameters": ["self", "Module"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Module.buffers", "item_type": "method", "code": "buffers(recurse:bool=True)\u2192Iterator[torch.Tensor]", "description": "Returns an iterator over module buffers.  Parameters recurse (bool) \u2013 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.  Yields torch.Tensor \u2013 module buffer   Example: &gt;&gt;&gt; for buf in model.buffers(): &gt;&gt;&gt;     print(type(buf), buf.size()) &lt;class 'torch.Tensor'&gt; (20L,) &lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)   ", "parameters": ["recurse (bool) : if True, then yields buffers of this moduleand all submodules. Otherwise, yields only buffers thatare direct members of this module.", "torch.Tensor : module buffer"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.ParameterList.extend", "item_type": "method", "code": "extend(parameters:Iterable[Parameter])\u2192T", "description": "Appends parameters from a Python iterable to the end of the list.  Parameters parameters (iterable) \u2013 iterable of parameters to append   ", "parameters": ["parameters (iterable) : iterable of parameters to append"], "returns": [], "example": "class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.params = nn.ParameterList([nn.Parameter(torch.randn(10, 10)) for i in range(10)])\n\n    def forward(self, x):\n        # ParameterList can act as an iterable, or be indexed using ints\n        for i, p in enumerate(self.params):\n            x = self.params[i // 2].mm(x) + p.mm(x)\n        return x\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.ParameterList", "item_type": "class", "code": "classtorch.nn.ParameterList(parameters:Optional[Iterable[Parameter]]=None)", "description": "Holds parameters in a list. ParameterList can be indexed like a regular Python list, but parameters it contains are properly registered, and will be visible by all Module methods.  Parameters parameters (iterable, optional) \u2013 an iterable of Parameter to add   Example: class MyModule(nn.Module):     def __init__(self):         super(MyModule, self).__init__()         self.params = nn.ParameterList([nn.Parameter(torch.randn(10, 10)) for i in range(10)])      def forward(self, x):         # ParameterList can act as an iterable, or be indexed using ints         for i, p in enumerate(self.params):             x = self.params[i // 2].mm(x) + p.mm(x)         return x     append(parameter: Parameter) \u2192 T  Appends a given parameter at the end of the list.  Parameters parameter (nn.Parameter) \u2013 parameter to append       extend(parameters: Iterable[Parameter]) \u2192 T  Appends parameters from a Python iterable to the end of the list.  Parameters parameters (iterable) \u2013 iterable of parameters to append     ", "parameters": ["parameters (iterable, optional) : an iterable of Parameter to add", "parameter (nn.Parameter) : parameter to append", "parameters (iterable) : iterable of parameters to append"], "returns": [], "example": "class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.params = nn.ParameterList([nn.Parameter(torch.randn(10, 10)) for i in range(10)])\n\n    def forward(self, x):\n        # ParameterList can act as an iterable, or be indexed using ints\n        for i, p in enumerate(self.params):\n            x = self.params[i // 2].mm(x) + p.mm(x)\n        return x\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.ModuleDict.values", "item_type": "method", "code": "values()\u2192Iterable[torch.nn.modules.module.Module]", "description": "Return an iterable of the ModuleDict values. ", "parameters": [], "returns": [], "example": "class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.choices = nn.ModuleDict({\n                'conv': nn.Conv2d(10, 10, 3),\n                'pool': nn.MaxPool2d(3)\n        })\n        self.activations = nn.ModuleDict([\n                ['lrelu', nn.LeakyReLU()],\n                ['prelu', nn.PReLU()]\n        ])\n\n    def forward(self, x, choice, act):\n        x = self.choices[choice](x)\n        x = self.activations[act](x)\n        return x\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Module.children", "item_type": "method", "code": "children()\u2192Iterator[torch.nn.modules.module.Module]", "description": "Returns an iterator over immediate children modules.  Yields Module \u2013 a child module   ", "parameters": ["Module : a child module"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Module.cpu", "item_type": "method", "code": "cpu()\u2192T", "description": "Moves all model parameters and buffers to the CPU.  Returns self  Return type Module   ", "parameters": ["self", "Module"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Module.cuda", "item_type": "method", "code": "cuda(device:Union[int,torch.device,None]=None)\u2192T", "description": "Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.  Parameters device (int, optional) \u2013 if specified, all parameters will be copied to that device  Returns self  Return type Module   ", "parameters": ["device (int, optional) : if specified, all parameters will becopied to that device", "self", "Module"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.ModuleDict", "item_type": "class", "code": "classtorch.nn.ModuleDict(modules:Optional[Mapping[str,torch.nn.modules.module.Module]]=None)", "description": "Holds submodules in a dictionary. ModuleDict can be indexed like a regular Python dictionary, but modules it contains are properly registered, and will be visible by all Module methods. ModuleDict is an ordered dictionary that respects  the order of insertion, and in update(), the order of the merged OrderedDict, dict (started from Python 3.6) or another ModuleDict (the argument to update()).  Note that update() with other unordered mapping types (e.g., Python\u2019s plain dict before Python verision 3.6) does not preserve the order of the merged mapping.  Parameters modules (iterable, optional) \u2013 a mapping (dictionary) of (string: module) or an iterable of key-value pairs of type (string, module)   Example: class MyModule(nn.Module):     def __init__(self):         super(MyModule, self).__init__()         self.choices = nn.ModuleDict({                 'conv': nn.Conv2d(10, 10, 3),                 'pool': nn.MaxPool2d(3)         })         self.activations = nn.ModuleDict([                 ['lrelu', nn.LeakyReLU()],                 ['prelu', nn.PReLU()]         ])      def forward(self, x, choice, act):         x = self.choices[choice](x)         x = self.activations[act](x)         return x     clear() \u2192 None  Remove all items from the ModuleDict.     items() \u2192 Iterable[Tuple[str, torch.nn.modules.module.Module]]  Return an iterable of the ModuleDict key/value pairs.     keys() \u2192 Iterable[str]  Return an iterable of the ModuleDict keys.     pop(key: str) \u2192 torch.nn.modules.module.Module  Remove key from the ModuleDict and return its module.  Parameters key (string) \u2013 key to pop from the ModuleDict       update(modules: Mapping[str, torch.nn.modules.module.Module]) \u2192 None  Update the ModuleDict with the key-value pairs from a mapping or an iterable, overwriting existing keys.  Note If modules is an OrderedDict, a ModuleDict, or an iterable of key-value pairs, the order of new elements in it is preserved.   Parameters modules (iterable) \u2013 a mapping (dictionary) from string to Module, or an iterable of key-value pairs of type (string, Module)       values() \u2192 Iterable[torch.nn.modules.module.Module]  Return an iterable of the ModuleDict values.   ", "parameters": ["modules (iterable, optional) : a mapping (dictionary) of (string: module)or an iterable of key-value pairs of type (string, module)", "key (string) : key to pop from the ModuleDict", "modules (iterable) : a mapping (dictionary) from string to Module,or an iterable of key-value pairs of type (string, Module)"], "returns": [], "example": "class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.choices = nn.ModuleDict({\n                'conv': nn.Conv2d(10, 10, 3),\n                'pool': nn.MaxPool2d(3)\n        })\n        self.activations = nn.ModuleDict([\n                ['lrelu', nn.LeakyReLU()],\n                ['prelu', nn.PReLU()]\n        ])\n\n    def forward(self, x, choice, act):\n        x = self.choices[choice](x)\n        x = self.activations[act](x)\n        return x\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Module.double", "item_type": "method", "code": "double()\u2192T", "description": "Casts all floating point parameters and buffers to double datatype.  Returns self  Return type Module   ", "parameters": ["self", "Module"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Module.eval", "item_type": "method", "code": "eval()\u2192T", "description": "Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc. This is equivalent with self.train(False).  Returns self  Return type Module   ", "parameters": ["self", "Module"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Module.extra_repr", "item_type": "method", "code": "extra_repr()\u2192str", "description": "Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. ", "parameters": [], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Module.float", "item_type": "method", "code": "float()\u2192T", "description": "Casts all floating point parameters and buffers to float datatype.  Returns self  Return type Module   ", "parameters": ["self", "Module"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Module.half", "item_type": "method", "code": "half()\u2192T", "description": "Casts all floating point parameters and buffers to half datatype.  Returns self  Return type Module   ", "parameters": ["self", "Module"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Module.load_state_dict", "item_type": "method", "code": "load_state_dict(state_dict:Dict[str,torch.Tensor],strict:bool=True)", "description": "Copies parameters and buffers from state_dict into this module and its descendants. If strict is True, then the keys of state_dict must exactly match the keys returned by this module\u2019s state_dict() function.  Parameters  state_dict (dict) \u2013 a dict containing parameters and persistent buffers. strict (bool, optional) \u2013 whether to strictly enforce that the keys in state_dict match the keys returned by this module\u2019s state_dict() function. Default: True   Returns  missing_keys is a list of str containing the missing keys unexpected_keys is a list of str containing the unexpected keys    Return type NamedTuple with missing_keys and unexpected_keys fields   ", "parameters": ["state_dict (dict) : a dict containing parameters andpersistent buffers.", "strict (bool, optional) : whether to strictly enforce that the keysin state_dict match the keys returned by this module\u2019sstate_dict() function. Default: True", "missing_keys is a list of str containing the missing keys", "unexpected_keys is a list of str containing the unexpected keys"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Module.modules", "item_type": "method", "code": "modules()\u2192Iterator[torch.nn.modules.module.Module]", "description": "Returns an iterator over all modules in the network.  Yields Module \u2013 a module in the network    Note Duplicate modules are returned only once. In the following example, l will be returned only once.  Example: &gt;&gt;&gt; l = nn.Linear(2, 2) &gt;&gt;&gt; net = nn.Sequential(l, l) &gt;&gt;&gt; for idx, m in enumerate(net.modules()):         print(idx, '-&gt;', m)  0 -&gt; Sequential(   (0): Linear(in_features=2, out_features=2, bias=True)   (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -&gt; Linear(in_features=2, out_features=2, bias=True)   ", "parameters": ["Module : a module in the network"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Module.named_buffers", "item_type": "method", "code": "named_buffers(prefix:str='',recurse:bool=True)\u2192Iterator[Tuple[str,torch.Tensor]]", "description": "Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.  Parameters  prefix (str) \u2013 prefix to prepend to all buffer names. recurse (bool) \u2013 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.   Yields (string, torch.Tensor) \u2013 Tuple containing the name and buffer   Example: &gt;&gt;&gt; for name, buf in self.named_buffers(): &gt;&gt;&gt;    if name in ['running_var']: &gt;&gt;&gt;        print(buf.size())   ", "parameters": ["prefix (str) : prefix to prepend to all buffer names.", "recurse (bool) : if True, then yields buffers of this moduleand all submodules. Otherwise, yields only buffers thatare direct members of this module."], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Module.named_children", "item_type": "method", "code": "named_children()\u2192Iterator[Tuple[str,torch.nn.modules.module.Module]]", "description": "Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.  Yields (string, Module) \u2013 Tuple containing a name and child module   Example: &gt;&gt;&gt; for name, module in model.named_children(): &gt;&gt;&gt;     if name in ['conv4', 'conv5']: &gt;&gt;&gt;         print(module)   ", "parameters": ["(string, Module) : Tuple containing a name and child module"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Module.named_modules", "item_type": "method", "code": "named_modules(memo:Optional[Set[Module]]=None,prefix:str='')", "description": "Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.  Yields (string, Module) \u2013 Tuple of name and module    Note Duplicate modules are returned only once. In the following example, l will be returned only once.  Example: &gt;&gt;&gt; l = nn.Linear(2, 2) &gt;&gt;&gt; net = nn.Sequential(l, l) &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):         print(idx, '-&gt;', m)  0 -&gt; ('', Sequential(   (0): Linear(in_features=2, out_features=2, bias=True)   (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -&gt; ('0', Linear(in_features=2, out_features=2, bias=True))   ", "parameters": ["(string, Module) : Tuple of name and module"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Module.named_parameters", "item_type": "method", "code": "named_parameters(prefix:str='',recurse:bool=True)\u2192Iterator[Tuple[str,torch.Tensor]]", "description": "Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.  Parameters  prefix (str) \u2013 prefix to prepend to all parameter names. recurse (bool) \u2013 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.   Yields (string, Parameter) \u2013 Tuple containing the name and parameter   Example: &gt;&gt;&gt; for name, param in self.named_parameters(): &gt;&gt;&gt;    if name in ['bias']: &gt;&gt;&gt;        print(param.size())   ", "parameters": ["prefix (str) : prefix to prepend to all parameter names.", "recurse (bool) : if True, then yields parameters of this moduleand all submodules. Otherwise, yields only parameters thatare direct members of this module."], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Module.parameters", "item_type": "method", "code": "parameters(recurse:bool=True)\u2192Iterator[torch.nn.parameter.Parameter]", "description": "Returns an iterator over module parameters. This is typically passed to an optimizer.  Parameters recurse (bool) \u2013 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.  Yields Parameter \u2013 module parameter   Example: &gt;&gt;&gt; for param in model.parameters(): &gt;&gt;&gt;     print(type(param), param.size()) &lt;class 'torch.Tensor'&gt; (20L,) &lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)   ", "parameters": ["recurse (bool) : if True, then yields parameters of this moduleand all submodules. Otherwise, yields only parameters thatare direct members of this module.", "Parameter : module parameter"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Module.register_backward_hook", "item_type": "method", "code": "register_backward_hook(hook:Callable[[Module,Union[Tuple[torch.Tensor,...],torch.Tensor],Union[Tuple[torch.Tensor,...],torch.Tensor]],Union[None,torch.Tensor]])\u2192torch.utils.hooks.RemovableHandle", "description": "Registers a backward hook on the module.  Warning The current implementation will not have the presented behavior for complex Module that perform many operations. In some failure cases, grad_input and grad_output will only contain the gradients for a subset of the inputs and outputs. For such Module, you should use torch.Tensor.register_hook() directly on a specific input or output to get the required gradients.  The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature: hook(module, grad_input, grad_output) -&gt; Tensor or None   The grad_input and grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of grad_input in subsequent computations. grad_input will only correspond to the inputs given as positional arguments.  Returns a handle that can be used to remove the added hook by calling handle.remove()  Return type torch.utils.hooks.RemovableHandle   ", "parameters": ["a handle that can be used to remove the added hook by callinghandle.remove()", "torch.utils.hooks.RemovableHandle"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Module.register_buffer", "item_type": "method", "code": "register_buffer(name:str,tensor:Optional[torch.Tensor],persistent:bool=True)\u2192None", "description": "Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm\u2019s running_mean is not a parameter, but is part of the module\u2019s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting persistent to False. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module\u2019s state_dict. Buffers can be accessed as attributes using given names.  Parameters  name (string) \u2013 name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor) \u2013 buffer to be registered. persistent (bool) \u2013 whether the buffer is part of this module\u2019s state_dict.    Example: &gt;&gt;&gt; self.register_buffer('running_mean', torch.zeros(num_features))   ", "parameters": ["name (string) : name of the buffer. The buffer can be accessedfrom this module using the given name", "tensor (Tensor) : buffer to be registered.", "persistent (bool) : whether the buffer is part of this module\u2019sstate_dict."], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Module.register_forward_hook", "item_type": "method", "code": "register_forward_hook(hook:Callable[...,None])\u2192torch.utils.hooks.RemovableHandle", "description": "Registers a forward hook on the module. The hook will be called every time after forward() has computed an output. It should have the following signature: hook(module, input, output) -&gt; None or modified output   The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after forward() is called.  Returns a handle that can be used to remove the added hook by calling handle.remove()  Return type torch.utils.hooks.RemovableHandle   ", "parameters": ["a handle that can be used to remove the added hook by callinghandle.remove()", "torch.utils.hooks.RemovableHandle"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Module.register_forward_pre_hook", "item_type": "method", "code": "register_forward_pre_hook(hook:Callable[...,None])\u2192torch.utils.hooks.RemovableHandle", "description": "Registers a forward pre-hook on the module. The hook will be called every time before forward() is invoked. It should have the following signature: hook(module, input) -&gt; None or modified input   The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple).  Returns a handle that can be used to remove the added hook by calling handle.remove()  Return type torch.utils.hooks.RemovableHandle   ", "parameters": ["a handle that can be used to remove the added hook by callinghandle.remove()", "torch.utils.hooks.RemovableHandle"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Module.register_parameter", "item_type": "method", "code": "register_parameter(name:str,param:Optional[torch.nn.parameter.Parameter])\u2192None", "description": "Adds a parameter to the module. The parameter can be accessed as an attribute using given name.  Parameters  name (string) \u2013 name of the parameter. The parameter can be accessed from this module using the given name param (Parameter) \u2013 parameter to be added to the module.    ", "parameters": ["name (string) : name of the parameter. The parameter can be accessedfrom this module using the given name", "param (Parameter) : parameter to be added to the module."], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Module.requires_grad_", "item_type": "method", "code": "requires_grad_(requires_grad:bool=True)\u2192T", "description": "Change if autograd should record operations on parameters in this module. This method sets the parameters\u2019 requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).  Parameters requires_grad (bool) \u2013 whether autograd should record operations on parameters in this module. Default: True.  Returns self  Return type Module   ", "parameters": ["requires_grad (bool) : whether autograd should record operations onparameters in this module. Default: True.", "self", "Module"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Module.state_dict", "item_type": "method", "code": "state_dict(destination=None,prefix='',keep_vars=False)", "description": "Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names.  Returns a dictionary containing a whole state of the module  Return type dict   Example: &gt;&gt;&gt; module.state_dict().keys() ['bias', 'weight']   ", "parameters": ["a dictionary containing a whole state of the module", "dict"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Module.to", "item_type": "method", "code": "to(*args,**kwargs)", "description": "Moves and/or casts the parameters and buffers. This can be called as   to(device=None, dtype=None, non_blocking=False)      to(dtype, non_blocking=False)      to(tensor, non_blocking=False)      to(memory_format=torch.channels_last)    Its signature is similar to torch.Tensor.to(), but only accepts floating point desired dtype s. In addition, this method will only cast the floating point parameters and buffers to dtype (if given). The integral parameters and buffers will be moved device, if that is given, but with dtypes unchanged. When non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples.  Note This method modifies the module in-place.   Parameters  device (torch.device) \u2013 the desired device of the parameters and buffers in this module dtype (torch.dtype) \u2013 the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor) \u2013 Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (torch.memory_format) \u2013 the desired memory format for 4D parameters and buffers in this module (keyword only argument)   Returns self  Return type Module   Example: &gt;&gt;&gt; linear = nn.Linear(2, 2) &gt;&gt;&gt; linear.weight Parameter containing: tensor([[ 0.1913, -0.3420],         [-0.5113, -0.2325]]) &gt;&gt;&gt; linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) &gt;&gt;&gt; linear.weight Parameter containing: tensor([[ 0.1913, -0.3420],         [-0.5113, -0.2325]], dtype=torch.float64) &gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\") &gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) &gt;&gt;&gt; linear.weight Parameter containing: tensor([[ 0.1914, -0.3420],         [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') &gt;&gt;&gt; cpu = torch.device(\"cpu\") &gt;&gt;&gt; linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) &gt;&gt;&gt; linear.weight Parameter containing: tensor([[ 0.1914, -0.3420],         [-0.5112, -0.2324]], dtype=torch.float16)   ", "parameters": ["device (torch.device) : the desired device of the parametersand buffers in this module", "dtype (torch.dtype) : the desired floating point type ofthe floating point parameters and buffers in this module", "tensor (torch.Tensor) : Tensor whose dtype and device are the desireddtype and device for all parameters and buffers in this module", "memory_format (torch.memory_format) : the desired memoryformat for 4D parameters and buffers in this module (keywordonly argument)"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Module.train", "item_type": "method", "code": "train(mode:bool=True)\u2192T", "description": "Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc.  Parameters mode (bool) \u2013 whether to set training mode (True) or evaluation mode (False). Default: True.  Returns self  Return type Module   ", "parameters": ["mode (bool) : whether to set training mode (True) or evaluationmode (False). Default: True.", "self", "Module"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Module.type", "item_type": "method", "code": "type(dst_type:Union[torch.dtype,str])\u2192T", "description": "Casts all parameters and buffers to dst_type.  Parameters dst_type (type or string) \u2013 the desired type  Returns self  Return type Module   ", "parameters": ["dst_type (type or string) : the desired type", "self", "Module"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Module.zero_grad", "item_type": "method", "code": "zero_grad()\u2192None", "description": "Sets gradients of all model parameters to zero. ", "parameters": [], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Module", "item_type": "class", "code": "classtorch.nn.Module", "description": "Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes: import torch.nn as nn import torch.nn.functional as F  class Model(nn.Module):     def __init__(self):         super(Model, self).__init__()         self.conv1 = nn.Conv2d(1, 20, 5)         self.conv2 = nn.Conv2d(20, 20, 5)      def forward(self, x):         x = F.relu(self.conv1(x))         return F.relu(self.conv2(x))   Submodules assigned in this way will be registered, and will have their parameters converted too when you call to(), etc.  Variables training (bool) \u2013 Boolean represents whether this module is in training or evaluation mode.     add_module(name: str, module: Optional[Module]) \u2192 None  Adds a child module to the current module. The module can be accessed as an attribute using the given name.  Parameters  name (string) \u2013 name of the child module. The child module can be accessed from this module using the given name module (Module) \u2013 child module to be added to the module.        apply(fn: Callable[Module, None]) \u2192 T  Applies fn recursively to every submodule (as returned by .children()) as well as self. Typical use includes initializing the parameters of a model (see also torch.nn.init).  Parameters fn (Module -&gt; None) \u2013 function to be applied to each submodule  Returns self  Return type Module   Example: &gt;&gt;&gt; @torch.no_grad() &gt;&gt;&gt; def init_weights(m): &gt;&gt;&gt;     print(m) &gt;&gt;&gt;     if type(m) == nn.Linear: &gt;&gt;&gt;         m.weight.fill_(1.0) &gt;&gt;&gt;         print(m.weight) &gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) &gt;&gt;&gt; net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1.,  1.],         [ 1.,  1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1.,  1.],         [ 1.,  1.]]) Sequential(   (0): Linear(in_features=2, out_features=2, bias=True)   (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential(   (0): Linear(in_features=2, out_features=2, bias=True)   (1): Linear(in_features=2, out_features=2, bias=True) )       bfloat16() \u2192 T  Casts all floating point parameters and buffers to bfloat16 datatype.  Returns self  Return type Module       buffers(recurse: bool = True) \u2192 Iterator[torch.Tensor]  Returns an iterator over module buffers.  Parameters recurse (bool) \u2013 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.  Yields torch.Tensor \u2013 module buffer   Example: &gt;&gt;&gt; for buf in model.buffers(): &gt;&gt;&gt;     print(type(buf), buf.size()) &lt;class 'torch.Tensor'&gt; (20L,) &lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)       children() \u2192 Iterator[torch.nn.modules.module.Module]  Returns an iterator over immediate children modules.  Yields Module \u2013 a child module       cpu() \u2192 T  Moves all model parameters and buffers to the CPU.  Returns self  Return type Module       cuda(device: Union[int, torch.device, None] = None) \u2192 T  Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.  Parameters device (int, optional) \u2013 if specified, all parameters will be copied to that device  Returns self  Return type Module       double() \u2192 T  Casts all floating point parameters and buffers to double datatype.  Returns self  Return type Module       dump_patches: bool = False This allows better BC support for load_state_dict(). In state_dict(), the version number will be saved as in the attribute _metadata of the returned state dict, and thus pickled. _metadata is a dictionary with keys that follow the naming convention of state dict. See _load_from_state_dict on how to use this information in loading. If new parameters/buffers are added/removed from a module, this number shall be bumped, and the module\u2019s _load_from_state_dict method can compare the version number and do appropriate changes if the state dict is from before the change.     eval() \u2192 T  Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc. This is equivalent with self.train(False).  Returns self  Return type Module       extra_repr() \u2192 str  Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.     float() \u2192 T  Casts all floating point parameters and buffers to float datatype.  Returns self  Return type Module       half() \u2192 T  Casts all floating point parameters and buffers to half datatype.  Returns self  Return type Module       load_state_dict(state_dict: Dict[str, torch.Tensor], strict: bool = True)  Copies parameters and buffers from state_dict into this module and its descendants. If strict is True, then the keys of state_dict must exactly match the keys returned by this module\u2019s state_dict() function.  Parameters  state_dict (dict) \u2013 a dict containing parameters and persistent buffers. strict (bool, optional) \u2013 whether to strictly enforce that the keys in state_dict match the keys returned by this module\u2019s state_dict() function. Default: True   Returns  missing_keys is a list of str containing the missing keys unexpected_keys is a list of str containing the unexpected keys    Return type NamedTuple with missing_keys and unexpected_keys fields       modules() \u2192 Iterator[torch.nn.modules.module.Module]  Returns an iterator over all modules in the network.  Yields Module \u2013 a module in the network    Note Duplicate modules are returned only once. In the following example, l will be returned only once.  Example: &gt;&gt;&gt; l = nn.Linear(2, 2) &gt;&gt;&gt; net = nn.Sequential(l, l) &gt;&gt;&gt; for idx, m in enumerate(net.modules()):         print(idx, '-&gt;', m)  0 -&gt; Sequential(   (0): Linear(in_features=2, out_features=2, bias=True)   (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -&gt; Linear(in_features=2, out_features=2, bias=True)       named_buffers(prefix: str = '', recurse: bool = True) \u2192 Iterator[Tuple[str, torch.Tensor]]  Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.  Parameters  prefix (str) \u2013 prefix to prepend to all buffer names. recurse (bool) \u2013 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.   Yields (string, torch.Tensor) \u2013 Tuple containing the name and buffer   Example: &gt;&gt;&gt; for name, buf in self.named_buffers(): &gt;&gt;&gt;    if name in ['running_var']: &gt;&gt;&gt;        print(buf.size())       named_children() \u2192 Iterator[Tuple[str, torch.nn.modules.module.Module]]  Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.  Yields (string, Module) \u2013 Tuple containing a name and child module   Example: &gt;&gt;&gt; for name, module in model.named_children(): &gt;&gt;&gt;     if name in ['conv4', 'conv5']: &gt;&gt;&gt;         print(module)       named_modules(memo: Optional[Set[Module]] = None, prefix: str = '')  Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.  Yields (string, Module) \u2013 Tuple of name and module    Note Duplicate modules are returned only once. In the following example, l will be returned only once.  Example: &gt;&gt;&gt; l = nn.Linear(2, 2) &gt;&gt;&gt; net = nn.Sequential(l, l) &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):         print(idx, '-&gt;', m)  0 -&gt; ('', Sequential(   (0): Linear(in_features=2, out_features=2, bias=True)   (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -&gt; ('0', Linear(in_features=2, out_features=2, bias=True))       named_parameters(prefix: str = '', recurse: bool = True) \u2192 Iterator[Tuple[str, torch.Tensor]]  Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.  Parameters  prefix (str) \u2013 prefix to prepend to all parameter names. recurse (bool) \u2013 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.   Yields (string, Parameter) \u2013 Tuple containing the name and parameter   Example: &gt;&gt;&gt; for name, param in self.named_parameters(): &gt;&gt;&gt;    if name in ['bias']: &gt;&gt;&gt;        print(param.size())       parameters(recurse: bool = True) \u2192 Iterator[torch.nn.parameter.Parameter]  Returns an iterator over module parameters. This is typically passed to an optimizer.  Parameters recurse (bool) \u2013 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.  Yields Parameter \u2013 module parameter   Example: &gt;&gt;&gt; for param in model.parameters(): &gt;&gt;&gt;     print(type(param), param.size()) &lt;class 'torch.Tensor'&gt; (20L,) &lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)       register_backward_hook(hook: Callable[[Module, Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[None, torch.Tensor]]) \u2192 torch.utils.hooks.RemovableHandle  Registers a backward hook on the module.  Warning The current implementation will not have the presented behavior for complex Module that perform many operations. In some failure cases, grad_input and grad_output will only contain the gradients for a subset of the inputs and outputs. For such Module, you should use torch.Tensor.register_hook() directly on a specific input or output to get the required gradients.  The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature: hook(module, grad_input, grad_output) -&gt; Tensor or None   The grad_input and grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of grad_input in subsequent computations. grad_input will only correspond to the inputs given as positional arguments.  Returns a handle that can be used to remove the added hook by calling handle.remove()  Return type torch.utils.hooks.RemovableHandle       register_buffer(name: str, tensor: Optional[torch.Tensor], persistent: bool = True) \u2192 None  Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm\u2019s running_mean is not a parameter, but is part of the module\u2019s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting persistent to False. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module\u2019s state_dict. Buffers can be accessed as attributes using given names.  Parameters  name (string) \u2013 name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor) \u2013 buffer to be registered. persistent (bool) \u2013 whether the buffer is part of this module\u2019s state_dict.    Example: &gt;&gt;&gt; self.register_buffer('running_mean', torch.zeros(num_features))       register_forward_hook(hook: Callable[..., None]) \u2192 torch.utils.hooks.RemovableHandle  Registers a forward hook on the module. The hook will be called every time after forward() has computed an output. It should have the following signature: hook(module, input, output) -&gt; None or modified output   The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after forward() is called.  Returns a handle that can be used to remove the added hook by calling handle.remove()  Return type torch.utils.hooks.RemovableHandle       register_forward_pre_hook(hook: Callable[..., None]) \u2192 torch.utils.hooks.RemovableHandle  Registers a forward pre-hook on the module. The hook will be called every time before forward() is invoked. It should have the following signature: hook(module, input) -&gt; None or modified input   The input contains only the positional arguments given to the module. Keyword arguments won\u2019t be passed to the hooks and only to the forward. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple).  Returns a handle that can be used to remove the added hook by calling handle.remove()  Return type torch.utils.hooks.RemovableHandle       register_parameter(name: str, param: Optional[torch.nn.parameter.Parameter]) \u2192 None  Adds a parameter to the module. The parameter can be accessed as an attribute using given name.  Parameters  name (string) \u2013 name of the parameter. The parameter can be accessed from this module using the given name param (Parameter) \u2013 parameter to be added to the module.        requires_grad_(requires_grad: bool = True) \u2192 T  Change if autograd should record operations on parameters in this module. This method sets the parameters\u2019 requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).  Parameters requires_grad (bool) \u2013 whether autograd should record operations on parameters in this module. Default: True.  Returns self  Return type Module       state_dict(destination=None, prefix='', keep_vars=False)  Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names.  Returns a dictionary containing a whole state of the module  Return type dict   Example: &gt;&gt;&gt; module.state_dict().keys() ['bias', 'weight']       to(*args, **kwargs)  Moves and/or casts the parameters and buffers. This can be called as   to(device=None, dtype=None, non_blocking=False)      to(dtype, non_blocking=False)      to(tensor, non_blocking=False)      to(memory_format=torch.channels_last)    Its signature is similar to torch.Tensor.to(), but only accepts floating point desired dtype s. In addition, this method will only cast the floating point parameters and buffers to dtype (if given). The integral parameters and buffers will be moved device, if that is given, but with dtypes unchanged. When non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples.  Note This method modifies the module in-place.   Parameters  device (torch.device) \u2013 the desired device of the parameters and buffers in this module dtype (torch.dtype) \u2013 the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor) \u2013 Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (torch.memory_format) \u2013 the desired memory format for 4D parameters and buffers in this module (keyword only argument)   Returns self  Return type Module   Example: &gt;&gt;&gt; linear = nn.Linear(2, 2) &gt;&gt;&gt; linear.weight Parameter containing: tensor([[ 0.1913, -0.3420],         [-0.5113, -0.2325]]) &gt;&gt;&gt; linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) &gt;&gt;&gt; linear.weight Parameter containing: tensor([[ 0.1913, -0.3420],         [-0.5113, -0.2325]], dtype=torch.float64) &gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\") &gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) &gt;&gt;&gt; linear.weight Parameter containing: tensor([[ 0.1914, -0.3420],         [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') &gt;&gt;&gt; cpu = torch.device(\"cpu\") &gt;&gt;&gt; linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) &gt;&gt;&gt; linear.weight Parameter containing: tensor([[ 0.1914, -0.3420],         [-0.5112, -0.2324]], dtype=torch.float16)       train(mode: bool = True) \u2192 T  Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc.  Parameters mode (bool) \u2013 whether to set training mode (True) or evaluation mode (False). Default: True.  Returns self  Return type Module       type(dst_type: Union[torch.dtype, str]) \u2192 T  Casts all parameters and buffers to dst_type.  Parameters dst_type (type or string) \u2013 the desired type  Returns self  Return type Module       zero_grad() \u2192 None  Sets gradients of all model parameters to zero.   ", "parameters": ["name (string) : name of the child module. The child module can beaccessed from this module using the given name", "module (Module) : child module to be added to the module.", "state_dict (dict) : a dict containing parameters andpersistent buffers.", "strict (bool, optional) : whether to strictly enforce that the keysin state_dict match the keys returned by this module\u2019sstate_dict() function. Default: True", "missing_keys is a list of str containing the missing keys", "unexpected_keys is a list of str containing the unexpected keys", "prefix (str) : prefix to prepend to all buffer names.", "recurse (bool) : if True, then yields buffers of this moduleand all submodules. Otherwise, yields only buffers thatare direct members of this module.", "prefix (str) : prefix to prepend to all parameter names.", "recurse (bool) : if True, then yields parameters of this moduleand all submodules. Otherwise, yields only parameters thatare direct members of this module.", "name (string) : name of the buffer. The buffer can be accessedfrom this module using the given name", "tensor (Tensor) : buffer to be registered.", "persistent (bool) : whether the buffer is part of this module\u2019sstate_dict.", "name (string) : name of the parameter. The parameter can be accessedfrom this module using the given name", "param (Parameter) : parameter to be added to the module.", "device (torch.device) : the desired device of the parametersand buffers in this module", "dtype (torch.dtype) : the desired floating point type ofthe floating point parameters and buffers in this module", "tensor (torch.Tensor) : Tensor whose dtype and device are the desireddtype and device for all parameters and buffers in this module", "memory_format (torch.memory_format) : the desired memoryformat for 4D parameters and buffers in this module (keywordonly argument)"], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""},
{"library": "torch", "item_id": "torch.nn.Module.dump_patches", "item_type": "attribute", "code": "dump_patches:bool=False", "description": "This allows better BC support for load_state_dict(). In state_dict(), the version number will be saved as in the attribute _metadata of the returned state dict, and thus pickled. _metadata is a dictionary with keys that follow the naming convention of state dict. See _load_from_state_dict on how to use this information in loading. If new parameters/buffers are added/removed from a module, this number shall be bumped, and the module\u2019s _load_from_state_dict method can compare the version number and do appropriate changes if the state dict is from before the change. ", "parameters": [], "returns": "self", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": ""}
]