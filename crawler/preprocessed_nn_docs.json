[{"id": "torch.nn.parameter.Parameter", "type": "class", "code": "torch.nn.parameter.Parameter", "example": "NA", "summary": "A kind of Tensor that is to be considered a module parameter", "returns": [], "shape": "", "code-info": {"name": "torch.nn.parameter.Parameter", "parameters": []}},
{"id": "torch.nn.LPPool1d", "type": "class", "code": "torch.nn.LPPool1d(norm_type:float,kernel_size:Union[T,Tuple[T,...]],stride:Optional[Union[T,Tuple[T,...]]]=None,ceil_mode:bool=False)", "example": "NA", "summary": "Applies a 1D power-average pooling over an input signal composed of several input planes", "returns": [], "shape": "\nInput: (N,C,Lin)(N, C, L_{in})(N,C,Lin\u200b)\n\n\nOutput: (N,C,Lout)(N, C, L_{out})(N,C,Lout\u200b)\n\n, where\n\nLout=\u230aLin\u2212kernel_sizestride+1\u230bL_{out} = \\left\\lfloor\\frac{L_{in} - \\text{kernel\\_size}}{\\text{stride}} + 1\\right\\rfloor\n\nLout\u200b=\u230astrideLin\u200b\u2212kernel_size\u200b+1\u230b\n\n\n\n", "code-info": {"name": "torch.nn.LPPool1d", "parameters": [{"name": "norm_type", "type": "float", "is_optional": false, "description": ""}, {"name": "kernel_size", "type": "Union[T,Tuple[T,...]]", "is_optional": false, "description": "a single int, the size of the window"}, {"name": "stride", "type": "Optional[Union[T,Tuple[T,...]]]", "default_value": "None", "is_optional": false, "description": "a single int, the stride of the window. Default value is kernel_size"}, {"name": "ceil_mode", "type": "bool", "default_value": "False", "is_optional": false, "description": "when True, will use ceil instead of floor to compute the output shape"}]}},
{"id": "torch.nn.FractionalMaxPool2d", "type": "class", "code": "torch.nn.FractionalMaxPool2d(kernel_size:Union[T,Tuple[T,T]],output_size:Optional[Union[T,Tuple[T,T]]]=None,output_ratio:Optional[Union[T,Tuple[T,T]]]=None,return_indices:bool=False,_random_samples=None)", "example": "NA", "summary": "Applies a 2D fractional max pooling over an input signal composed of several input planes", "returns": [], "shape": "", "code-info": {"name": "torch.nn.FractionalMaxPool2d", "parameters": [{"name": "kernel_size", "type": "Union[T,Tuple[T,T]]", "is_optional": false, "description": "the size of the window to take a max over.Can be a single number k (for a square kernel of k x k) or a tuple (kh, kw)"}, {"name": "output_size", "type": "Optional[Union[T,Tuple[T,T]]]", "default_value": "None", "is_optional": false, "description": "the target output size of the image of the form oH x oW.Can be a tuple (oH, oW) or a single number oH for a square image oH x oH"}, {"name": "output_ratio", "type": "Optional[Union[T,Tuple[T,T]]]", "default_value": "None", "is_optional": false, "description": "If one wants to have an output size as a ratio of the input size, this option can be given.This has to be a number or tuple in the range (0, 1)"}, {"name": "return_indices", "type": "bool", "default_value": "False", "is_optional": false, "description": "if True, will return the indices along with the outputs.Useful to pass to nn.MaxUnpool2d(). Default: False"}]}},
{"id": "torch.nn.AvgPool3d", "type": "class", "code": "torch.nn.AvgPool3d(kernel_size:Union[T,Tuple[T,T,T]],stride:Optional[Union[T,Tuple[T,T,T]]]=None,padding:Union[T,Tuple[T,T,T]]=0,ceil_mode:bool=False,count_include_pad:bool=True,divisor_override=None)", "example": "NA", "summary": "Applies a 3D average pooling over an input signal composed of several input planes", "returns": [], "shape": "\nInput: (N,C,Din,Hin,Win)(N, C, D_{in}, H_{in}, W_{in})(N,C,Din\u200b,Hin\u200b,Win\u200b)\n\n\nOutput: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\u200b,Hout\u200b,Wout\u200b)\n\n, where\n\nDout=\u230aDin+2\u00d7padding[0]\u2212kernel_size[0]stride[0]+1\u230bD_{out} = \\left\\lfloor\\frac{D_{in} + 2 \\times \\text{padding}[0] -\n      \\text{kernel\\_size}[0]}{\\text{stride}[0]} + 1\\right\\rfloor\n\nDout\u200b=\u230astride[0]Din\u200b+2\u00d7padding[0]\u2212kernel_size[0]\u200b+1\u230b\n\n\nHout=\u230aHin+2\u00d7padding[1]\u2212kernel_size[1]stride[1]+1\u230bH_{out} = \\left\\lfloor\\frac{H_{in} + 2 \\times \\text{padding}[1] -\n      \\text{kernel\\_size}[1]}{\\text{stride}[1]} + 1\\right\\rfloor\n\nHout\u200b=\u230astride[1]Hin\u200b+2\u00d7padding[1]\u2212kernel_size[1]\u200b+1\u230b\n\n\nWout=\u230aWin+2\u00d7padding[2]\u2212kernel_size[2]stride[2]+1\u230bW_{out} = \\left\\lfloor\\frac{W_{in} + 2 \\times \\text{padding}[2] -\n      \\text{kernel\\_size}[2]}{\\text{stride}[2]} + 1\\right\\rfloor\n\nWout\u200b=\u230astride[2]Win\u200b+2\u00d7padding[2]\u2212kernel_size[2]\u200b+1\u230b\n\n\n\n", "code-info": {"name": "torch.nn.AvgPool3d", "parameters": [{"name": "kernel_size", "type": "Union[T,Tuple[T,T,T]]", "is_optional": false, "description": "the size of the window"}, {"name": "stride", "type": "Optional[Union[T,Tuple[T,T,T]]]", "default_value": "None", "is_optional": false, "description": "the stride of the window. Default value is kernel_size"}, {"name": "padding", "type": "Union[T,Tuple[T,T,T]]", "default_value": "0", "is_optional": false, "description": "implicit zero padding to be added on all three sides"}, {"name": "ceil_mode", "type": "bool", "default_value": "False", "is_optional": false, "description": "when True, will use ceil instead of floor to compute the output shape"}, {"name": "count_include_pad", "type": "bool", "default_value": "True", "is_optional": false, "description": "when True, will include the zero-padding in the averaging calculation"}]}},
{"id": "torch.nn.AdaptiveAvgPool3d", "type": "class", "code": "torch.nn.AdaptiveAvgPool3d(output_size:Union[T,Tuple[T,...]])", "example": "NA", "summary": "Applies a 3D adaptive average pooling over an input signal composed of several input planes", "returns": [], "shape": "", "code-info": {"name": "torch.nn.AdaptiveAvgPool3d", "parameters": [{"name": "output_size", "type": "Union[T,Tuple[T,...]]", "is_optional": false, "description": "the target output size of the form D x H x W.Can be a tuple (D, H, W) or a single number D for a cube D x D x D.D, H and W can be either a int, or None which means the size willbe the same as that of the input."}]}},
{"id": "torch.nn.AdaptiveAvgPool1d", "type": "class", "code": "torch.nn.AdaptiveAvgPool1d(output_size:Union[T,Tuple[T,...]])", "example": "NA", "summary": "Applies a 1D adaptive average pooling over an input signal composed of several input planes", "returns": [], "shape": "", "code-info": {"name": "torch.nn.AdaptiveAvgPool1d", "parameters": [{"name": "output_size", "type": "Union[T,Tuple[T,...]]", "is_optional": false, "description": "the target output size H"}]}},
{"id": "torch.nn.AvgPool2d", "type": "class", "code": "torch.nn.AvgPool2d(kernel_size:Union[T,Tuple[T,T]],stride:Optional[Union[T,Tuple[T,T]]]=None,padding:Union[T,Tuple[T,T]]=0,ceil_mode:bool=False,count_include_pad:bool=True,divisor_override:bool=None)", "example": "NA", "summary": "Applies a 2D average pooling over an input signal composed of several input planes", "returns": [], "shape": "\nInput: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)\n\n\nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)\n\n, where\n\nHout=\u230aHin+2\u00d7padding[0]\u2212kernel_size[0]stride[0]+1\u230bH_{out} = \\left\\lfloor\\frac{H_{in}  + 2 \\times \\text{padding}[0] -\n  \\text{kernel\\_size}[0]}{\\text{stride}[0]} + 1\\right\\rfloor\n\nHout\u200b=\u230astride[0]Hin\u200b+2\u00d7padding[0]\u2212kernel_size[0]\u200b+1\u230b\n\n\nWout=\u230aWin+2\u00d7padding[1]\u2212kernel_size[1]stride[1]+1\u230bW_{out} = \\left\\lfloor\\frac{W_{in}  + 2 \\times \\text{padding}[1] -\n  \\text{kernel\\_size}[1]}{\\text{stride}[1]} + 1\\right\\rfloor\n\nWout\u200b=\u230astride[1]Win\u200b+2\u00d7padding[1]\u2212kernel_size[1]\u200b+1\u230b\n\n\n\n", "code-info": {"name": "torch.nn.AvgPool2d", "parameters": [{"name": "kernel_size", "type": "Union[T,Tuple[T,T]]", "is_optional": false, "description": "the size of the window"}, {"name": "stride", "type": "Optional[Union[T,Tuple[T,T]]]", "default_value": "None", "is_optional": false, "description": "the stride of the window. Default value is kernel_size"}, {"name": "padding", "type": "Union[T,Tuple[T,T]]", "default_value": "0", "is_optional": false, "description": "implicit zero padding to be added on both sides"}, {"name": "ceil_mode", "type": "bool", "default_value": "False", "is_optional": false, "description": "when True, will use ceil instead of floor to compute the output shape"}, {"name": "count_include_pad", "type": "bool", "default_value": "True", "is_optional": false, "description": "when True, will include the zero-padding in the averaging calculation"}, {"name": "divisor_override", "type": "bool", "default_value": "None", "is_optional": false, "description": "if specified, it will be used as divisor, otherwise kernel_size will be used"}]}},
{"id": "torch.nn.AdaptiveAvgPool2d", "type": "class", "code": "torch.nn.AdaptiveAvgPool2d(output_size:Union[T,Tuple[T,...]])", "example": "NA", "summary": "Applies a 2D adaptive average pooling over an input signal composed of several input planes", "returns": [], "shape": "", "code-info": {"name": "torch.nn.AdaptiveAvgPool2d", "parameters": [{"name": "output_size", "type": "Union[T,Tuple[T,...]]", "is_optional": false, "description": "the target output size of the image of the form H x W.Can be a tuple (H, W) or a single H for a square image H x H.H and W can be either a int, or None which means the size willbe the same as that of the input."}]}},
{"id": "torch.nn.AdaptiveMaxPool3d", "type": "class", "code": "torch.nn.AdaptiveMaxPool3d(output_size:Union[T,Tuple[T,...]],return_indices:bool=False)", "example": "NA", "summary": "Applies a 3D adaptive max pooling over an input signal composed of several input planes", "returns": [], "shape": "", "code-info": {"name": "torch.nn.AdaptiveMaxPool3d", "parameters": [{"name": "output_size", "type": "Union[T,Tuple[T,...]]", "is_optional": false, "description": "the target output size of the image of the form D x H x W.Can be a tuple (D, H, W) or a single D for a cube D x D x D.D, H and W can be either a int, or None which means the size willbe the same as that of the input."}, {"name": "return_indices", "type": "bool", "default_value": "False", "is_optional": false, "description": "if True, will return the indices along with the outputs.Useful to pass to nn.MaxUnpool3d. Default: False"}]}},
{"id": "torch.nn.AvgPool1d", "type": "class", "code": "torch.nn.AvgPool1d(kernel_size:Union[T,Tuple[T]],stride:Union[T,Tuple[T]]=None,padding:Union[T,Tuple[T]]=0,ceil_mode:bool=False,count_include_pad:bool=True)", "example": "NA", "summary": "Applies a 1D average pooling over an input signal composed of several input planes", "returns": [], "shape": "\nInput: (N,C,Lin)(N, C, L_{in})(N,C,Lin\u200b)\n\n\nOutput: (N,C,Lout)(N, C, L_{out})(N,C,Lout\u200b)\n\n, where\n\nLout=\u230aLin+2\u00d7padding\u2212kernel_sizestride+1\u230bL_{out} = \\left\\lfloor \\frac{L_{in} +\n2 \\times \\text{padding} - \\text{kernel\\_size}}{\\text{stride}} + 1\\right\\rfloor\n\nLout\u200b=\u230astrideLin\u200b+2\u00d7padding\u2212kernel_size\u200b+1\u230b\n\n\n\n", "code-info": {"name": "torch.nn.AvgPool1d", "parameters": [{"name": "kernel_size", "type": "Union[T,Tuple[T]]", "is_optional": false, "description": "the size of the window"}, {"name": "stride", "type": "Union[T,Tuple[T]]", "default_value": "None", "is_optional": false, "description": "the stride of the window. Default value is kernel_size"}, {"name": "padding", "type": "Union[T,Tuple[T]]", "default_value": "0", "is_optional": false, "description": "implicit zero padding to be added on both sides"}, {"name": "ceil_mode", "type": "bool", "default_value": "False", "is_optional": false, "description": "when True, will use ceil instead of floor to compute the output shape"}, {"name": "count_include_pad", "type": "bool", "default_value": "True", "is_optional": false, "description": "when True, will include the zero-padding in the averaging calculation"}]}},
{"id": "torch.nn.AdaptiveMaxPool2d", "type": "class", "code": "torch.nn.AdaptiveMaxPool2d(output_size:Union[T,Tuple[T,...]],return_indices:bool=False)", "example": "NA", "summary": "Applies a 2D adaptive max pooling over an input signal composed of several input planes", "returns": [], "shape": "", "code-info": {"name": "torch.nn.AdaptiveMaxPool2d", "parameters": [{"name": "output_size", "type": "Union[T,Tuple[T,...]]", "is_optional": false, "description": "the target output size of the image of the form H x W.Can be a tuple (H, W) or a single H for a square image H x H.H and W can be either a int, or None which means the size willbe the same as that of the input."}, {"name": "return_indices", "type": "bool", "default_value": "False", "is_optional": false, "description": "if True, will return the indices along with the outputs.Useful to pass to nn.MaxUnpool2d. Default: False"}]}},
{"id": "torch.nn.AdaptiveMaxPool1d", "type": "class", "code": "torch.nn.AdaptiveMaxPool1d(output_size:Union[T,Tuple[T,...]],return_indices:bool=False)", "example": "NA", "summary": "Applies a 1D adaptive max pooling over an input signal composed of several input planes", "returns": [], "shape": "", "code-info": {"name": "torch.nn.AdaptiveMaxPool1d", "parameters": [{"name": "output_size", "type": "Union[T,Tuple[T,...]]", "is_optional": false, "description": "the target output size H"}, {"name": "return_indices", "type": "bool", "default_value": "False", "is_optional": false, "description": "if True, will return the indices along with the outputs.Useful to pass to nn.MaxUnpool1d. Default: False"}]}},
{"id": "torch.nn.LPPool2d", "type": "class", "code": "torch.nn.LPPool2d(norm_type:float,kernel_size:Union[T,Tuple[T,...]],stride:Optional[Union[T,Tuple[T,...]]]=None,ceil_mode:bool=False)", "example": "NA", "summary": "Applies a 2D power-average pooling over an input signal composed of several input planes", "returns": [], "shape": "\nInput: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)\n\n\nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)\n\n, where\n\nHout=\u230aHin\u2212kernel_size[0]stride[0]+1\u230bH_{out} = \\left\\lfloor\\frac{H_{in} - \\text{kernel\\_size}[0]}{\\text{stride}[0]} + 1\\right\\rfloor\n\nHout\u200b=\u230astride[0]Hin\u200b\u2212kernel_size[0]\u200b+1\u230b\n\n\nWout=\u230aWin\u2212kernel_size[1]stride[1]+1\u230bW_{out} = \\left\\lfloor\\frac{W_{in} - \\text{kernel\\_size}[1]}{\\text{stride}[1]} + 1\\right\\rfloor\n\nWout\u200b=\u230astride[1]Win\u200b\u2212kernel_size[1]\u200b+1\u230b\n\n\n\n", "code-info": {"name": "torch.nn.LPPool2d", "parameters": [{"name": "norm_type", "type": "float", "is_optional": false, "description": ""}, {"name": "kernel_size", "type": "Union[T,Tuple[T,...]]", "is_optional": false, "description": "the size of the window"}, {"name": "stride", "type": "Optional[Union[T,Tuple[T,...]]]", "default_value": "None", "is_optional": false, "description": "the stride of the window. Default value is kernel_size"}, {"name": "ceil_mode", "type": "bool", "default_value": "False", "is_optional": false, "description": "when True, will use ceil instead of floor to compute the output shape"}]}},
{"id": "torch.nn.MaxUnpool3d", "type": "class", "code": "torch.nn.MaxUnpool3d(kernel_size:Union[T,Tuple[T,T,T]],stride:Optional[Union[T,Tuple[T,T,T]]]=None,padding:Union[T,Tuple[T,T,T]]=0)", "example": "  # pool of square window of size=3, stride=2  pool = nn.MaxPool3d(3, stride=2, return_indices=True)  unpool = nn.MaxUnpool3d(3, stride=2)  output, indices = pool(torch.randn(20, 16, 51, 33, 15))  unpooled_output = unpool(output, indices)  unpooled_output.size() torch.Size([20, 16, 51, 33, 15])   ", "summary": "Computes a partial inverse of MaxPool3d", "returns": [], "shape": "\ninput: the input Tensor to invert\nindices: the indices given out by MaxPool3d\noutput_size (optional): the targeted output size\n\n", "code-info": {"name": "torch.nn.MaxUnpool3d", "parameters": [{"name": "kernel_size", "type": "Union[T,Tuple[T,T,T]]", "is_optional": false, "description": "Size of the max pooling window."}, {"name": "stride", "type": "Optional[Union[T,Tuple[T,T,T]]]", "default_value": "None", "is_optional": false, "description": "Stride of the max pooling window.It is set to kernel_size by default."}, {"name": "padding", "type": "Union[T,Tuple[T,T,T]]", "default_value": "0", "is_optional": false, "description": "Padding that was added to the input"}]}},
{"id": "torch.nn.MaxUnpool2d", "type": "class", "code": "torch.nn.MaxUnpool2d(kernel_size:Union[T,Tuple[T,T]],stride:Optional[Union[T,Tuple[T,T]]]=None,padding:Union[T,Tuple[T,T]]=0)", "example": "  pool = nn.MaxPool2d(2, stride=2, return_indices=True)  unpool = nn.MaxUnpool2d(2, stride=2)  input = torch.tensor([[[[ 1.,  2,  3,  4],                             [ 5,  6,  7,  8],                             [ 9, 10, 11, 12],                             [13, 14, 15, 16]]]])  output, indices = pool(input)  unpool(output, indices) tensor([[[[  0.,   0.,   0.,   0.],           [  0.,   6.,   0.,   8.],           [  0.,   0.,   0.,   0.],           [  0.,  14.,   0.,  16.]]]])   # specify a different output size than input size  unpool(output, indices, output_size=torch.Size([1, 1, 5, 5])) tensor([[[[  0.,   0.,   0.,   0.,   0.],           [  6.,   0.,   8.,   0.,   0.],           [  0.,   0.,   0.,  14.,   0.],           [ 16.,   0.,   0.,   0.,   0.],           [  0.,   0.,   0.,   0.,   0.]]]])   ", "summary": "Computes a partial inverse of MaxPool2d", "returns": [], "shape": "\ninput: the input Tensor to invert\nindices: the indices given out by MaxPool2d\noutput_size (optional): the targeted output size\n\n", "code-info": {"name": "torch.nn.MaxUnpool2d", "parameters": [{"name": "kernel_size", "type": "Union[T,Tuple[T,T]]", "is_optional": false, "description": "Size of the max pooling window."}, {"name": "stride", "type": "Optional[Union[T,Tuple[T,T]]]", "default_value": "None", "is_optional": false, "description": "Stride of the max pooling window.It is set to kernel_size by default."}, {"name": "padding", "type": "Union[T,Tuple[T,T]]", "default_value": "0", "is_optional": false, "description": "Padding that was added to the input"}]}},
{"id": "torch.nn.TransformerDecoderLayer.forward", "type": "method", "code": "torch.nn.TransformerDecoderLayer.forward(tgt:torch.Tensor,memory:torch.Tensor,tgt_mask:Optional[torch.Tensor]=None,memory_mask:Optional[torch.Tensor]=None,tgt_key_padding_mask:Optional[torch.Tensor]=None,memory_key_padding_mask:Optional[torch.Tensor]=None)", "example": "NA", "summary": "Pass the inputs (and mask) through the decoder layer", "returns": [], "shape": "", "code-info": {"name": "torch.nn.TransformerDecoderLayer.forward", "parameters": [{"name": "tgt", "type": "torch.Tensor", "is_optional": false, "description": "the sequence to the decoder layer (required)."}, {"name": "memory", "type": "torch.Tensor", "is_optional": false, "description": "the sequence from the last layer of the encoder (required)."}, {"name": "tgt_mask", "type": "Optional[torch.Tensor]", "default_value": "None", "is_optional": true, "description": "the mask for the tgt sequence (optional)."}, {"name": "memory_mask", "type": "Optional[torch.Tensor]", "default_value": "None", "is_optional": true, "description": "the mask for the memory sequence (optional)."}, {"name": "tgt_key_padding_mask", "type": "Optional[torch.Tensor]", "default_value": "None", "is_optional": true, "description": "the mask for the tgt keys per batch (optional)."}, {"name": "memory_key_padding_mask", "type": "Optional[torch.Tensor]", "default_value": "None", "is_optional": true, "description": "the mask for the memory keys per batch (optional)."}]}},
{"id": "torch.nn.TransformerDecoderLayer", "type": "class", "code": "torch.nn.TransformerDecoderLayer(d_model,nhead,dim_feedforward=2048,dropout=0.1,activation='relu')", "example": "NA", "summary": "TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network", "returns": [], "shape": "&gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n&gt;&gt;&gt; memory = torch.rand(10, 32, 512)\n&gt;&gt;&gt; tgt = torch.rand(20, 32, 512)\n&gt;&gt;&gt; out = decoder_layer(tgt, memory)\n\n\n", "code-info": {"name": "torch.nn.TransformerDecoderLayer", "parameters": []}},
{"id": "torch.nn.MaxUnpool1d", "type": "class", "code": "torch.nn.MaxUnpool1d(kernel_size:Union[T,Tuple[T]],stride:Optional[Union[T,Tuple[T]]]=None,padding:Union[T,Tuple[T]]=0)", "example": "  pool = nn.MaxPool1d(2, stride=2, return_indices=True)  unpool = nn.MaxUnpool1d(2, stride=2)  input = torch.tensor([[[1., 2, 3, 4, 5, 6, 7, 8]]])  output, indices = pool(input)  unpool(output, indices) tensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.]]])   # Example showcasing the use of output_size  input = torch.tensor([[[1., 2, 3, 4, 5, 6, 7, 8, 9]]])  output, indices = pool(input)  unpool(output, indices, output_size=input.size()) tensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.,  0.]]])   unpool(output, indices) tensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.]]])   ", "summary": "Computes a partial inverse of MaxPool1d", "returns": [], "shape": "\ninput: the input Tensor to invert\nindices: the indices given out by MaxPool1d\noutput_size (optional): the targeted output size\n\n", "code-info": {"name": "torch.nn.MaxUnpool1d", "parameters": [{"name": "kernel_size", "type": "Union[T,Tuple[T]]", "is_optional": false, "description": "Size of the max pooling window."}, {"name": "stride", "type": "Optional[Union[T,Tuple[T]]]", "default_value": "None", "is_optional": false, "description": "Stride of the max pooling window.It is set to kernel_size by default."}, {"name": "padding", "type": "Union[T,Tuple[T]]", "default_value": "0", "is_optional": false, "description": "Padding that was added to the input"}]}},
{"id": "torch.promote_types", "type": "function", "code": "torch.promote_types(type1,type2)", "example": "  torch.promote_types(torch.int32, torch.float32)) torch.float32  torch.promote_types(torch.uint8, torch.long) torch.long   ", "summary": "Returns the torch.dtype with the smallest size and scalar kind that is not smaller nor of lower kind than either type1 or type2", "returns": [], "shape": "", "code-info": {"name": "torch.promote_types", "parameters": []}},
{"id": "torch.nn.utils.rnn.pad_sequence", "type": "function", "code": "torch.nn.utils.rnn.pad_sequence(sequences,batch_first=False,padding_value=0.0)", "example": "NA", "summary": "Pad a list of variable length Tensors with padding_value pad_sequence stacks a list of Tensors along a new dimension, and pads them to equal length", "returns": "Tensor of size T x B x * if batch_first is False.Tensor of size B x T x * otherwise", "shape": "", "code-info": {"name": "torch.nn.utils.rnn.pad_sequence", "parameters": []}},
{"id": "torch.nn.utils.rnn.pack_sequence", "type": "function", "code": "torch.nn.utils.rnn.pack_sequence(sequences,enforce_sorted=True)", "example": "NA", "summary": "Packs a list of variable length Tensors sequences should be a list of Tensors of size L x *, where L is the length of a sequence and * is any number of trailing dimensions, including zero", "returns": "a PackedSequence object", "shape": "", "code-info": {"name": "torch.nn.utils.rnn.pack_sequence", "parameters": []}},
{"id": "torch.nn.Flatten.add_module", "type": "method", "code": "torch.nn.Flatten.add_module(name:str,module:Optional[Module])", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "summary": "Adds a child module to the current module", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Flatten.add_module", "parameters": [{"name": "name", "type": "str", "is_optional": false, "description": "name of the child module. The child module can beaccessed from this module using the given name"}, {"name": "module", "type": "Optional[Module]", "is_optional": false, "description": "child module to be added to the module."}]}},
{"id": "torch.nn.Flatten.apply", "type": "method", "code": "torch.nn.Flatten.apply(fn:Callable[Module,None])", "example": "  @torch.no_grad()  def init_weights(m):      print(m)      if type(m) == nn.Linear:          m.weight.fill_(1.0)          print(m.weight)  net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))  net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1.,  1.],         [ 1.,  1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1.,  1.],         [ 1.,  1.]]) Sequential(   (0): Linear(in_features=2, out_features=2, bias=True)   (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential(   (0): Linear(in_features=2, out_features=2, bias=True)   (1): Linear(in_features=2, out_features=2, bias=True) )   ", "summary": "Applies fn recursively to every submodule (as returned by .children()) as well as self", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Flatten.apply", "parameters": [{"name": "fn", "type": "Callable[Module,None]", "is_optional": false, "description": "function to be applied to each submodule"}]}},
{"id": "torch.nn.utils.remove_spectral_norm", "type": "function", "code": "torch.nn.utils.remove_spectral_norm(module,name='weight')", "example": "NA", "summary": "Removes the spectral normalization reparameterization from a module", "returns": [], "shape": "", "code-info": {"name": "torch.nn.utils.remove_spectral_norm", "parameters": []}},
{"id": "torch.nn.utils.rnn.PackedSequence.batch_sizes", "type": "method", "code": "torch.nn.utils.rnn.PackedSequence.batch_sizes", "example": "NA", "summary": "Alias for field number 1 ", "returns": [], "shape": "", "code-info": {"name": "torch.nn.utils.rnn.PackedSequence.batch_sizes", "parameters": []}},
{"id": "torch.nn.utils.rnn.PackedSequence.count", "type": "method", "code": "torch.nn.utils.rnn.PackedSequence.count(value)", "example": "NA", "summary": "", "returns": [], "shape": "", "code-info": {"name": "torch.nn.utils.rnn.PackedSequence.count", "parameters": []}},
{"id": "torch.nn.utils.rnn.PackedSequence.data", "type": "method", "code": "torch.nn.utils.rnn.PackedSequence.data", "example": "NA", "summary": "Alias for field number 0 ", "returns": [], "shape": "", "code-info": {"name": "torch.nn.utils.rnn.PackedSequence.data", "parameters": []}},
{"id": "torch.nn.utils.rnn.PackedSequence.index", "type": "method", "code": "torch.nn.utils.rnn.PackedSequence.index(value[,start[,stop]])", "example": "NA", "summary": "Raises ValueError if the value is not present", "returns": [], "shape": "", "code-info": {"name": "torch.nn.utils.rnn.PackedSequence.index", "parameters": []}},
{"id": "torch.nn.utils.rnn.PackedSequence.is_cuda", "type": "method", "code": "torch.nn.utils.rnn.PackedSequence.is_cuda", "example": "NA", "summary": "Returns true if self.data stored on a gpu ", "returns": [], "shape": "", "code-info": {"name": "torch.nn.utils.rnn.PackedSequence.is_cuda", "parameters": []}},
{"id": "torch.nn.utils.rnn.PackedSequence.is_pinned", "type": "method", "code": "torch.nn.utils.rnn.PackedSequence.is_pinned()", "example": "NA", "summary": "Returns true if self.data stored on in pinned memory ", "returns": [], "shape": "", "code-info": {"name": "torch.nn.utils.rnn.PackedSequence.is_pinned", "parameters": []}},
{"id": "torch.nn.utils.rnn.PackedSequence.sorted_indices", "type": "method", "code": "torch.nn.utils.rnn.PackedSequence.sorted_indices", "example": "NA", "summary": "Alias for field number 2 ", "returns": [], "shape": "", "code-info": {"name": "torch.nn.utils.rnn.PackedSequence.sorted_indices", "parameters": []}},
{"id": "torch.nn.utils.rnn.pack_padded_sequence", "type": "function", "code": "torch.nn.utils.rnn.pack_padded_sequence(input,lengths,batch_first=False,enforce_sorted=True)", "example": "NA", "summary": "Packs a Tensor containing padded sequences of variable length", "returns": "a PackedSequence object", "shape": "", "code-info": {"name": "torch.nn.utils.rnn.pack_padded_sequence", "parameters": []}},
{"id": "torch.nn.utils.spectral_norm", "type": "function", "code": "torch.nn.utils.spectral_norm(module,name='weight',n_power_iterations=1,eps=1e-12,dim=None)", "example": "  m = spectral_norm(nn.Linear(20, 40))  m Linear(in_features=20, out_features=40, bias=True)  m.weight_u.size() torch.Size([40])   ", "summary": "Applies spectral normalization to a parameter in the given module", "returns": "The original module with the spectral norm hook", "shape": "", "code-info": {"name": "torch.nn.utils.spectral_norm", "parameters": []}},
{"id": "torch.nn.utils.rnn.pad_packed_sequence", "type": "function", "code": "torch.nn.utils.rnn.pad_packed_sequence(sequence,batch_first=False,padding_value=0.0,total_length=None)", "example": "NA", "summary": "Pads a packed batch of variable length sequences", "returns": "Tuple of Tensor containing the padded sequence, and a Tensorcontaining the list of lengths of each sequence in the batch.Batch elements will be re-ordered as they were ordered originally whenthe batch was passed to pack_padded_sequence or pack_sequence.", "shape": "", "code-info": {"name": "torch.nn.utils.rnn.pad_packed_sequence", "parameters": []}},
{"id": "torch.nn.utils.prune.is_pruned", "type": "function", "code": "torch.nn.utils.prune.is_pruned(module)", "example": "NA", "summary": "Check whether module is pruned by looking for forward_pre_hooks in its modules that inherit from the BasePruningMethod", "returns": "binary answer to whether module is pruned.", "shape": "", "code-info": {"name": "torch.nn.utils.prune.is_pruned", "parameters": []}},
{"id": "torch.nn.Flatten.bfloat16", "type": "method", "code": "torch.nn.Flatten.bfloat16()", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "summary": "Casts all floating point parameters and buffers to bfloat16 datatype", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Flatten.bfloat16", "parameters": []}},
{"id": "torch.nn.Flatten.buffers", "type": "method", "code": "torch.nn.Flatten.buffers(recurse:bool=True)", "example": "  for buf in model.buffers():      print(type(buf), buf.size()) &lt;class 'torch.Tensor' (20L,) &lt;class 'torch.Tensor' (20L, 1L, 5L, 5L)   ", "summary": "Returns an iterator over module buffers", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Flatten.buffers", "parameters": [{"name": "recurse", "type": "bool", "default_value": "True", "is_optional": false, "description": "if True, then yields buffers of this moduleand all submodules. Otherwise, yields only buffers thatare direct members of this module."}]}},
{"id": "torch.nn.utils.weight_norm", "type": "function", "code": "torch.nn.utils.weight_norm(module,name='weight',dim=0)", "example": "  m = weight_norm(nn.Linear(20, 40), name='weight')  m Linear(in_features=20, out_features=40, bias=True)  m.weight_g.size() torch.Size([40, 1])  m.weight_v.size() torch.Size([40, 20])   ", "summary": "Applies weight normalization to a parameter in the given module", "returns": "The original module with the weight norm hook", "shape": "", "code-info": {"name": "torch.nn.utils.weight_norm", "parameters": []}},
{"id": "torch.nn.utils.remove_weight_norm", "type": "function", "code": "torch.nn.utils.remove_weight_norm(module,name='weight')", "example": "NA", "summary": "Removes the weight normalization reparameterization from a module", "returns": [], "shape": "", "code-info": {"name": "torch.nn.utils.remove_weight_norm", "parameters": []}},
{"id": "torch.nn.utils.rnn.PackedSequence.to", "type": "method", "code": "torch.nn.utils.rnn.PackedSequence.to(*args,**kwargs)", "example": "NA", "summary": "Performs dtype and/or device conversion on self.data", "returns": [], "shape": "", "code-info": {"name": "torch.nn.utils.rnn.PackedSequence.to", "parameters": []}},
{"id": "torch.nn.utils.rnn.PackedSequence.unsorted_indices", "type": "method", "code": "torch.nn.utils.rnn.PackedSequence.unsorted_indices", "example": "NA", "summary": "Alias for field number 3 ", "returns": [], "shape": "", "code-info": {"name": "torch.nn.utils.rnn.PackedSequence.unsorted_indices", "parameters": []}},
{"id": "torch.nn.utils.rnn.PackedSequence", "type": "class", "code": "torch.nn.utils.rnn.PackedSequence", "example": "NA", "summary": "Holds the data and list of batch_sizes of a packed sequence", "returns": [], "shape": "", "code-info": {"name": "torch.nn.utils.rnn.PackedSequence", "parameters": []}},
{"id": "torch.nn.Flatten.children", "type": "method", "code": "torch.nn.Flatten.children()", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "summary": "Returns an iterator over immediate children modules", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Flatten.children", "parameters": []}},
{"id": "torch.nn.Flatten.cpu", "type": "method", "code": "torch.nn.Flatten.cpu()", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "summary": "Moves all model parameters and buffers to the CPU", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Flatten.cpu", "parameters": []}},
{"id": "torch.nn.Flatten.cuda", "type": "method", "code": "torch.nn.Flatten.cuda(device:Union[int,torch.device,None]=None)", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "summary": "Moves all model parameters and buffers to the GPU", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Flatten.cuda", "parameters": [{"name": "device", "type": "Union[int,torch.device,None]", "default_value": "None", "is_optional": true, "description": "if specified, all parameters will becopied to that device"}]}},
{"id": "torch.nn.utils.prune.remove", "type": "function", "code": "torch.nn.utils.prune.remove(module,name)", "example": "NA", "summary": "Removes the pruning reparameterization from a module and the pruning method from the forward hook", "returns": [], "shape": "", "code-info": {"name": "torch.nn.utils.prune.remove", "parameters": []}},
{"id": "torch.nn.utils.prune.custom_from_mask", "type": "function", "code": "torch.nn.utils.prune.custom_from_mask(module,name,mask)", "example": "NA", "summary": "Prunes tensor corresponding to parameter called name in module by applying the pre-computed mask in mask", "returns": "modified (i.e. pruned) version of the input module", "shape": "", "code-info": {"name": "torch.nn.utils.prune.custom_from_mask", "parameters": []}},
{"id": "torch.nn.Flatten.double", "type": "method", "code": "torch.nn.Flatten.double()", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "summary": "Casts all floating point parameters and buffers to double datatype", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Flatten.double", "parameters": []}},
{"id": "torch.nn.Flatten.eval", "type": "method", "code": "torch.nn.Flatten.eval()", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "summary": "Sets the module in evaluation mode", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Flatten.eval", "parameters": []}},
{"id": "torch.nn.utils.prune.ln_structured", "type": "function", "code": "torch.nn.utils.prune.ln_structured(module,name,amount,n,dim)", "example": "NA", "summary": "Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) channels along the specified dim with the lowest L``n``-norm", "returns": "modified (i.e. pruned) version of the input module", "shape": "", "code-info": {"name": "torch.nn.utils.prune.ln_structured", "parameters": []}},
{"id": "torch.nn.utils.prune.random_structured", "type": "function", "code": "torch.nn.utils.prune.random_structured(module,name,amount,dim)", "example": "NA", "summary": "Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) channels along the specified dim selected at random", "returns": "modified (i.e. pruned) version of the input module", "shape": "", "code-info": {"name": "torch.nn.utils.prune.random_structured", "parameters": []}},
{"id": "torch.nn.utils.prune.l1_unstructured", "type": "function", "code": "torch.nn.utils.prune.l1_unstructured(module,name,amount)", "example": "NA", "summary": "Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) units with the lowest L1-norm", "returns": "modified (i.e. pruned) version of the input module", "shape": "", "code-info": {"name": "torch.nn.utils.prune.l1_unstructured", "parameters": []}},
{"id": "torch.nn.utils.prune.global_unstructured", "type": "function", "code": "torch.nn.utils.prune.global_unstructured(parameters,pruning_method,**kwargs)", "example": "NA", "summary": "Globally prunes tensors corresponding to all parameters in parameters by applying the specified pruning_method", "returns": [], "shape": "", "code-info": {"name": "torch.nn.utils.prune.global_unstructured", "parameters": []}},
{"id": "torch.nn.utils.prune.identity", "type": "function", "code": "torch.nn.utils.prune.identity(module,name)", "example": "NA", "summary": "Applies pruning reparametrization to the tensor corresponding to the parameter called name in module without actually pruning any units", "returns": "modified (i.e. pruned) version of the input module", "shape": "", "code-info": {"name": "torch.nn.utils.prune.identity", "parameters": []}},
{"id": "torch.nn.Flatten.extra_repr", "type": "method", "code": "torch.nn.Flatten.extra_repr()", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "summary": "Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Flatten.extra_repr", "parameters": []}},
{"id": "torch.nn.Flatten.float", "type": "method", "code": "torch.nn.Flatten.float()", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "summary": "Casts all floating point parameters and buffers to float datatype", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Flatten.float", "parameters": []}},
{"id": "torch.nn.Flatten.half", "type": "method", "code": "torch.nn.Flatten.half()", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "summary": "Casts all floating point parameters and buffers to half datatype", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Flatten.half", "parameters": []}},
{"id": "torch.nn.utils.prune.LnStructured.apply", "type": "method", "code": "torch.nn.utils.prune.LnStructured.apply(module,name,amount,n,dim)", "example": "NA", "summary": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask", "returns": "pruned version of the input tensor", "shape": "", "code-info": {"name": "torch.nn.utils.prune.LnStructured.apply", "parameters": []}},
{"id": "torch.nn.utils.prune.LnStructured.apply_mask", "type": "method", "code": "torch.nn.utils.prune.LnStructured.apply_mask(module)", "example": "NA", "summary": "Simply handles the multiplication between the parameter being pruned and the generated mask", "returns": "pruned version of the input tensor", "shape": "", "code-info": {"name": "torch.nn.utils.prune.LnStructured.apply_mask", "parameters": []}},
{"id": "torch.nn.utils.prune.CustomFromMask.apply", "type": "method", "code": "torch.nn.utils.prune.CustomFromMask.apply(module,name,mask)", "example": "NA", "summary": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask", "returns": "pruned version of the input tensor", "shape": "", "code-info": {"name": "torch.nn.utils.prune.CustomFromMask.apply", "parameters": []}},
{"id": "torch.nn.utils.prune.CustomFromMask.apply_mask", "type": "method", "code": "torch.nn.utils.prune.CustomFromMask.apply_mask(module)", "example": "NA", "summary": "Simply handles the multiplication between the parameter being pruned and the generated mask", "returns": "pruned version of the input tensor", "shape": "", "code-info": {"name": "torch.nn.utils.prune.CustomFromMask.apply_mask", "parameters": []}},
{"id": "torch.nn.utils.prune.CustomFromMask.prune", "type": "method", "code": "torch.nn.utils.prune.CustomFromMask.prune(t,default_mask=None)", "example": "NA", "summary": "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask()", "returns": "pruned version of the input tensor", "shape": "", "code-info": {"name": "torch.nn.utils.prune.CustomFromMask.prune", "parameters": []}},
{"id": "torch.nn.utils.prune.CustomFromMask.remove", "type": "method", "code": "torch.nn.utils.prune.CustomFromMask.remove(module)", "example": "NA", "summary": "Removes the pruning reparameterization from a module", "returns": "pruned version of the input tensor", "shape": "", "code-info": {"name": "torch.nn.utils.prune.CustomFromMask.remove", "parameters": []}},
{"id": "torch.nn.utils.prune.RandomStructured.apply", "type": "method", "code": "torch.nn.utils.prune.RandomStructured.apply(module,name,amount,dim=-1)", "example": "NA", "summary": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask", "returns": "pruned version of the input tensor", "shape": "", "code-info": {"name": "torch.nn.utils.prune.RandomStructured.apply", "parameters": []}},
{"id": "torch.nn.utils.prune.RandomStructured.apply_mask", "type": "method", "code": "torch.nn.utils.prune.RandomStructured.apply_mask(module)", "example": "NA", "summary": "Simply handles the multiplication between the parameter being pruned and the generated mask", "returns": "pruned version of the input tensor", "shape": "", "code-info": {"name": "torch.nn.utils.prune.RandomStructured.apply_mask", "parameters": []}},
{"id": "torch.nn.utils.prune.RandomStructured.compute_mask", "type": "method", "code": "torch.nn.utils.prune.RandomStructured.compute_mask(t,default_mask)", "example": "NA", "summary": "Computes and returns a mask for the input tensor t", "returns": "pruned version of the input tensor", "shape": "", "code-info": {"name": "torch.nn.utils.prune.RandomStructured.compute_mask", "parameters": []}},
{"id": "torch.nn.Flatten.load_state_dict", "type": "method", "code": "torch.nn.Flatten.load_state_dict(state_dict:Dict[str,torch.Tensor],strict:bool=True)", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "summary": "Copies parameters and buffers from state_dict into this module and its descendants", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Flatten.load_state_dict", "parameters": [{"name": "state_dict", "type": "Dict[str,torch.Tensor]", "is_optional": false, "description": "a dict containing parameters andpersistent buffers."}, {"name": "strict", "type": "bool", "default_value": "True", "is_optional": true, "description": "whether to strictly enforce that the keysin state_dict match the keys returned by this module\u2019sstate_dict() function. Default: True"}]}},
{"id": "torch.nn.Flatten.modules", "type": "method", "code": "torch.nn.Flatten.modules()", "example": "  l = nn.Linear(2, 2)  net = nn.Sequential(l, l)  for idx, m in enumerate(net.modules()):         print(idx, '-', m)  0 - Sequential(   (0): Linear(in_features=2, out_features=2, bias=True)   (1): Linear(in_features=2, out_features=2, bias=True) ) 1 - Linear(in_features=2, out_features=2, bias=True)   ", "summary": "Returns an iterator over all modules in the network", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Flatten.modules", "parameters": []}},
{"id": "torch.nn.utils.prune.L1Unstructured.apply", "type": "method", "code": "torch.nn.utils.prune.L1Unstructured.apply(module,name,amount)", "example": "NA", "summary": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask", "returns": "pruned version of the input tensor", "shape": "", "code-info": {"name": "torch.nn.utils.prune.L1Unstructured.apply", "parameters": []}},
{"id": "torch.nn.utils.prune.L1Unstructured.apply_mask", "type": "method", "code": "torch.nn.utils.prune.L1Unstructured.apply_mask(module)", "example": "NA", "summary": "Simply handles the multiplication between the parameter being pruned and the generated mask", "returns": "pruned version of the input tensor", "shape": "", "code-info": {"name": "torch.nn.utils.prune.L1Unstructured.apply_mask", "parameters": []}},
{"id": "torch.nn.utils.prune.L1Unstructured.prune", "type": "method", "code": "torch.nn.utils.prune.L1Unstructured.prune(t,default_mask=None)", "example": "NA", "summary": "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask()", "returns": "pruned version of the input tensor", "shape": "", "code-info": {"name": "torch.nn.utils.prune.L1Unstructured.prune", "parameters": []}},
{"id": "torch.nn.utils.prune.random_unstructured", "type": "function", "code": "torch.nn.utils.prune.random_unstructured(module,name,amount)", "example": "NA", "summary": "Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) units selected at random", "returns": "modified (i.e. pruned) version of the input module", "shape": "", "code-info": {"name": "torch.nn.utils.prune.random_unstructured", "parameters": []}},
{"id": "torch.nn.utils.prune.LnStructured.compute_mask", "type": "method", "code": "torch.nn.utils.prune.LnStructured.compute_mask(t,default_mask)", "example": "NA", "summary": "Computes and returns a mask for the input tensor t", "returns": "pruned version of the input tensor", "shape": "", "code-info": {"name": "torch.nn.utils.prune.LnStructured.compute_mask", "parameters": []}},
{"id": "torch.nn.utils.prune.LnStructured.prune", "type": "method", "code": "torch.nn.utils.prune.LnStructured.prune(t,default_mask=None)", "example": "NA", "summary": "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask()", "returns": "pruned version of the input tensor", "shape": "", "code-info": {"name": "torch.nn.utils.prune.LnStructured.prune", "parameters": []}},
{"id": "torch.nn.utils.prune.RandomUnstructured.apply", "type": "method", "code": "torch.nn.utils.prune.RandomUnstructured.apply(module,name,amount)", "example": "NA", "summary": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask", "returns": "pruned version of the input tensor", "shape": "", "code-info": {"name": "torch.nn.utils.prune.RandomUnstructured.apply", "parameters": []}},
{"id": "torch.nn.utils.prune.RandomUnstructured.apply_mask", "type": "method", "code": "torch.nn.utils.prune.RandomUnstructured.apply_mask(module)", "example": "NA", "summary": "Simply handles the multiplication between the parameter being pruned and the generated mask", "returns": "pruned version of the input tensor", "shape": "", "code-info": {"name": "torch.nn.utils.prune.RandomUnstructured.apply_mask", "parameters": []}},
{"id": "torch.nn.utils.prune.RandomUnstructured.prune", "type": "method", "code": "torch.nn.utils.prune.RandomUnstructured.prune(t,default_mask=None)", "example": "NA", "summary": "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask()", "returns": "pruned version of the input tensor", "shape": "", "code-info": {"name": "torch.nn.utils.prune.RandomUnstructured.prune", "parameters": []}},
{"id": "torch.nn.utils.prune.CustomFromMask", "type": "class", "code": "torch.nn.utils.prune.CustomFromMask(mask)", "example": "NA", "summary": "  classmethod apply(module, name, mask)  Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask", "returns": "pruned version of the input tensor", "shape": "", "code-info": {"name": "torch.nn.utils.prune.CustomFromMask", "parameters": []}},
{"id": "torch.nn.utils.prune.PruningContainer.add_pruning_method", "type": "method", "code": "torch.nn.utils.prune.PruningContainer.add_pruning_method(method)", "example": "NA", "summary": "Adds a child pruning method to the container", "returns": "pruned version of the input tensor", "shape": "", "code-info": {"name": "torch.nn.utils.prune.PruningContainer.add_pruning_method", "parameters": []}},
{"id": "torch.nn.utils.prune.PruningContainer.apply", "type": "method", "code": "torch.nn.utils.prune.PruningContainer.apply(module,name,*args,**kwargs)", "example": "NA", "summary": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask", "returns": "pruned version of the input tensor", "shape": "", "code-info": {"name": "torch.nn.utils.prune.PruningContainer.apply", "parameters": []}},
{"id": "torch.nn.utils.prune.Identity.apply", "type": "method", "code": "torch.nn.utils.prune.Identity.apply(module,name)", "example": "NA", "summary": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask", "returns": "pruned version of the input tensor", "shape": "", "code-info": {"name": "torch.nn.utils.prune.Identity.apply", "parameters": []}},
{"id": "torch.nn.utils.prune.Identity.apply_mask", "type": "method", "code": "torch.nn.utils.prune.Identity.apply_mask(module)", "example": "NA", "summary": "Simply handles the multiplication between the parameter being pruned and the generated mask", "returns": "pruned version of the input tensor", "shape": "", "code-info": {"name": "torch.nn.utils.prune.Identity.apply_mask", "parameters": []}},
{"id": "torch.nn.utils.prune.Identity.prune", "type": "method", "code": "torch.nn.utils.prune.Identity.prune(t,default_mask=None)", "example": "NA", "summary": "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask()", "returns": "pruned version of the input tensor", "shape": "", "code-info": {"name": "torch.nn.utils.prune.Identity.prune", "parameters": []}},
{"id": "torch.nn.utils.prune.Identity.remove", "type": "method", "code": "torch.nn.utils.prune.Identity.remove(module)", "example": "NA", "summary": "Removes the pruning reparameterization from a module", "returns": "pruned version of the input tensor", "shape": "", "code-info": {"name": "torch.nn.utils.prune.Identity.remove", "parameters": []}},
{"id": "torch.nn.utils.prune.RandomStructured.prune", "type": "method", "code": "torch.nn.utils.prune.RandomStructured.prune(t,default_mask=None)", "example": "NA", "summary": "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask()", "returns": "pruned version of the input tensor", "shape": "", "code-info": {"name": "torch.nn.utils.prune.RandomStructured.prune", "parameters": []}},
{"id": "torch.nn.utils.prune.RandomStructured.remove", "type": "method", "code": "torch.nn.utils.prune.RandomStructured.remove(module)", "example": "NA", "summary": "Removes the pruning reparameterization from a module", "returns": "pruned version of the input tensor", "shape": "", "code-info": {"name": "torch.nn.utils.prune.RandomStructured.remove", "parameters": []}},
{"id": "torch.nn.utils.prune.RandomStructured", "type": "class", "code": "torch.nn.utils.prune.RandomStructured(amount,dim=-1)", "example": "NA", "summary": "Prune entire (currently unpruned) channels in a tensor at random", "returns": "pruned version of the input tensor", "shape": "", "code-info": {"name": "torch.nn.utils.prune.RandomStructured", "parameters": []}},
{"id": "torch.nn.Flatten.named_buffers", "type": "method", "code": "torch.nn.Flatten.named_buffers(prefix:str='',recurse:bool=True)", "example": "  for name, buf in self.named_buffers():     if name in ['running_var']:         print(buf.size())   ", "summary": "Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Flatten.named_buffers", "parameters": [{"name": "prefix", "type": "str", "default_value": "''", "is_optional": false, "description": "prefix to prepend to all buffer names."}, {"name": "recurse", "type": "bool", "default_value": "True", "is_optional": false, "description": "if True, then yields buffers of this moduleand all submodules. Otherwise, yields only buffers thatare direct members of this module."}]}},
{"id": "torch.nn.Flatten.named_children", "type": "method", "code": "torch.nn.Flatten.named_children()", "example": "  for name, module in model.named_children():      if name in ['conv4', 'conv5']:          print(module)   ", "summary": "Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Flatten.named_children", "parameters": []}},
{"id": "torch.nn.utils.prune.L1Unstructured.remove", "type": "method", "code": "torch.nn.utils.prune.L1Unstructured.remove(module)", "example": "NA", "summary": "Removes the pruning reparameterization from a module", "returns": "pruned version of the input tensor", "shape": "", "code-info": {"name": "torch.nn.utils.prune.L1Unstructured.remove", "parameters": []}},
{"id": "torch.nn.utils.prune.L1Unstructured", "type": "class", "code": "torch.nn.utils.prune.L1Unstructured(amount)", "example": "NA", "summary": "Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm", "returns": "pruned version of the input tensor", "shape": "", "code-info": {"name": "torch.nn.utils.prune.L1Unstructured", "parameters": []}},
{"id": "torch.nn.utils.prune.LnStructured.remove", "type": "method", "code": "torch.nn.utils.prune.LnStructured.remove(module)", "example": "NA", "summary": "Removes the pruning reparameterization from a module", "returns": "pruned version of the input tensor", "shape": "", "code-info": {"name": "torch.nn.utils.prune.LnStructured.remove", "parameters": []}},
{"id": "torch.nn.utils.prune.LnStructured", "type": "class", "code": "torch.nn.utils.prune.LnStructured(amount,n,dim=-1)", "example": "NA", "summary": "Prune entire (currently unpruned) channels in a tensor based on their Ln-norm", "returns": "pruned version of the input tensor", "shape": "", "code-info": {"name": "torch.nn.utils.prune.LnStructured", "parameters": []}},
{"id": "torch.nn.utils.prune.RandomUnstructured.remove", "type": "method", "code": "torch.nn.utils.prune.RandomUnstructured.remove(module)", "example": "NA", "summary": "Removes the pruning reparameterization from a module", "returns": "pruned version of the input tensor", "shape": "", "code-info": {"name": "torch.nn.utils.prune.RandomUnstructured.remove", "parameters": []}},
{"id": "torch.nn.utils.prune.RandomUnstructured", "type": "class", "code": "torch.nn.utils.prune.RandomUnstructured(amount)", "example": "NA", "summary": "Prune (currently unpruned) units in a tensor at random", "returns": "pruned version of the input tensor", "shape": "", "code-info": {"name": "torch.nn.utils.prune.RandomUnstructured", "parameters": []}},
{"id": "torch.nn.utils.prune.PruningContainer.apply_mask", "type": "method", "code": "torch.nn.utils.prune.PruningContainer.apply_mask(module)", "example": "NA", "summary": "Simply handles the multiplication between the parameter being pruned and the generated mask", "returns": "pruned version of the input tensor", "shape": "", "code-info": {"name": "torch.nn.utils.prune.PruningContainer.apply_mask", "parameters": []}},
{"id": "torch.nn.utils.prune.PruningContainer.compute_mask", "type": "method", "code": "torch.nn.utils.prune.PruningContainer.compute_mask(t,default_mask)", "example": "NA", "summary": "Applies the latest method by computing the new partial masks and returning its combination with the default_mask", "returns": "pruned version of the input tensor", "shape": "", "code-info": {"name": "torch.nn.utils.prune.PruningContainer.compute_mask", "parameters": []}},
{"id": "torch.nn.utils.prune.PruningContainer.prune", "type": "method", "code": "torch.nn.utils.prune.PruningContainer.prune(t,default_mask=None)", "example": "NA", "summary": "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask()", "returns": "pruned version of the input tensor", "shape": "", "code-info": {"name": "torch.nn.utils.prune.PruningContainer.prune", "parameters": []}},
{"id": "torch.nn.utils.prune.Identity", "type": "class", "code": "torch.nn.utils.prune.Identity", "example": "NA", "summary": "Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones", "returns": "pruned version of the input tensor", "shape": "", "code-info": {"name": "torch.nn.utils.prune.Identity", "parameters": []}},
{"id": "torch.nn.utils.vector_to_parameters", "type": "function", "code": "torch.nn.utils.vector_to_parameters(vec,parameters)", "example": "NA", "summary": "Convert one vector to the parameters  Parameters  vec (Tensor) \u2013 a single vector represents the parameters of a model", "returns": [], "shape": "", "code-info": {"name": "torch.nn.utils.vector_to_parameters", "parameters": []}},
{"id": "torch.nn.utils.prune.BasePruningMethod.apply", "type": "method", "code": "torch.nn.utils.prune.BasePruningMethod.apply(module,name,*args,**kwargs)", "example": "NA", "summary": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask", "returns": "pruned version of the input tensor", "shape": "", "code-info": {"name": "torch.nn.utils.prune.BasePruningMethod.apply", "parameters": []}},
{"id": "torch.nn.utils.prune.BasePruningMethod.apply_mask", "type": "method", "code": "torch.nn.utils.prune.BasePruningMethod.apply_mask(module)", "example": "NA", "summary": "Simply handles the multiplication between the parameter being pruned and the generated mask", "returns": "pruned version of the input tensor", "shape": "", "code-info": {"name": "torch.nn.utils.prune.BasePruningMethod.apply_mask", "parameters": []}},
{"id": "torch.nn.utils.prune.BasePruningMethod.compute_mask", "type": "method", "code": "torch.nn.utils.prune.BasePruningMethod.compute_mask(t,default_mask)", "example": "NA", "summary": "Computes and returns a mask for the input tensor t", "returns": "pruned version of the input tensor", "shape": "", "code-info": {"name": "torch.nn.utils.prune.BasePruningMethod.compute_mask", "parameters": []}},
{"id": "torch.nn.utils.clip_grad_norm_", "type": "function", "code": "torch.nn.utils.clip_grad_norm_(parameters,max_norm,norm_type=2)", "example": "NA", "summary": "Clips gradient norm of an iterable of parameters", "returns": "Total norm of the parameters (viewed as a single vector).", "shape": "", "code-info": {"name": "torch.nn.utils.clip_grad_norm_", "parameters": []}},
{"id": "torch.nn.Flatten.named_modules", "type": "method", "code": "torch.nn.Flatten.named_modules(memo:Optional[Set[Module]]=None,prefix:str='')", "example": "  l = nn.Linear(2, 2)  net = nn.Sequential(l, l)  for idx, m in enumerate(net.named_modules()):         print(idx, '-', m)  0 - ('', Sequential(   (0): Linear(in_features=2, out_features=2, bias=True)   (1): Linear(in_features=2, out_features=2, bias=True) )) 1 - ('0', Linear(in_features=2, out_features=2, bias=True))   ", "summary": "Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Flatten.named_modules", "parameters": [{"name": "memo", "type": "Optional[Set[Module]]", "default_value": "None", "is_optional": false, "description": ""}, {"name": "prefix", "type": "str", "default_value": "''", "is_optional": false, "description": ""}]}},
{"id": "torch.nn.utils.parameters_to_vector", "type": "function", "code": "torch.nn.utils.parameters_to_vector(parameters)", "example": "NA", "summary": "Convert parameters to one vector  Parameters parameters (Iterable[Tensor]) \u2013 an iterator of Tensors that are the parameters of a model", "returns": "The parameters represented by a single vector", "shape": "", "code-info": {"name": "torch.nn.utils.parameters_to_vector", "parameters": []}},
{"id": "torch.nn.utils.clip_grad_value_", "type": "function", "code": "torch.nn.utils.clip_grad_value_(parameters,clip_value)", "example": "NA", "summary": "Clips gradient of an iterable of parameters at specified value", "returns": [], "shape": "", "code-info": {"name": "torch.nn.utils.clip_grad_value_", "parameters": []}},
{"id": "torch.nn.parallel.DistributedDataParallel.no_sync", "type": "method", "code": "torch.nn.parallel.DistributedDataParallel.no_sync()", "example": "  ddp = torch.nn.DistributedDataParallel(model, pg)  with ddp.no_sync(): ...   for input in inputs: ...     ddp(input).backward()  # no synchronization, accumulate grads ... ddp(another_input).backward()  # synchronize grads   ", "summary": "A context manager to disable gradient synchronizations across DDP processes", "returns": [], "shape": "", "code-info": {"name": "torch.nn.parallel.DistributedDataParallel.no_sync", "parameters": []}},
{"id": "torch.nn.parallel.DistributedDataParallel", "type": "class", "code": "torch.nn.parallel.DistributedDataParallel(module,device_ids=None,output_device=None,dim=0,broadcast_buffers=True,process_group=None,bucket_cap_mb=25,find_unused_parameters=False,check_reduction=False)", "example": ": import torch.distributed.autograd as dist_autograd  from torch.nn.parallel import DistributedDataParallel as DDP  from torch import optim  from torch.distributed.optim import DistributedOptimizer  from torch.distributed.rpc import RRef   t1 = torch.rand((3, 3), requires_grad=True)  t2 = torch.rand((3, 3), requires_grad=True)  rref = rpc.remote(\"worker1\", torch.add, args=(t1, t2))  ddp_model = DDP(my_model)   # Setup optimizer  optimizer_params = [rref]  for param in ddp_model.parameters():      optimizer_params.append(RRef(param))   dist_optim = DistributedOptimizer(      optim.SGD,      optimizer_params,      lr=0.05,  )   with dist_autograd.context() as context_id:      pred = ddp_model(rref.to_here())      loss = loss_func(pred, loss)      dist_autograd.backward(context_id, loss)      dist_optim.step()      Warning Using DistributedDataParallel in conjuction with the Distributed RPC Framework is experimental and subject to change.   Parameters  module (Module) \u2013 module to be parallelized device_ids (list of python:int or torch.device) \u2013 CUDA devices. This should only be provided when the input module resides on a single CUDA device. For single-device modules, the i``th :attr:`module` replica is placed on ``device_ids[i]. For multi-device modules and CPU modules, device_ids must be None or an empty list, and input data for the forward pass must be placed on the correct device. (default: all devices for single-device modules) output_device (int or torch.device) \u2013 device location of output for single-device CUDA modules. For multi-device modules and CPU modules, it must be None, and the module itself dictates the output location. (default: device_ids[0] for single-device modules) broadcast_buffers (bool) \u2013 flag that enables syncing (broadcasting) buffers of the module at beginning of the forward function. (default: True) process_group \u2013 the process group to be used for distributed data all-reduction. If None, the default process group, which is created by `torch.distributed.init_process_group`, will be used. (default: None) bucket_cap_mb \u2013 DistributedDataParallel will bucket parameters into multiple buckets so that gradient reduction of each bucket can potentially overlap with backward computation. bucket_cap_mb controls the bucket size in MegaBytes (MB) (default: 25) find_unused_parameters (bool) \u2013 Traverse the autograd graph of all tensors contained in the return value of the wrapped module\u2019s forward function. Parameters that don\u2019t receive gradients as part of this graph are preemptively marked as being ready to be reduced. Note that all forward outputs that are derived from module parameters must participate in calculating loss and later the gradient computation. If they don\u2019t, this wrapper will hang waiting for autograd to produce gradients for those parameters. Any outputs derived from module parameters that are otherwise unused can be detached from the autograd graph using torch.Tensor.detach. (default: False) check_reduction \u2013 when setting to True, it enables DistributedDataParallel to automatically check if the previous iteration\u2019s backward reductions were successfully issued at the beginning of every iteration\u2019s forward function. You normally don\u2019t need this option enabled unless you are observing weird behaviors such as different ranks are getting different gradients, which should not happen if DistributedDataParallel is correctly used. (default: False)   Variables ~DistributedDataParallel.module (Module) \u2013 the module to be parallelized   ", "summary": "Implements distributed data parallelism that is based on torch.distributed package at the module level", "returns": [], "shape": "", "code-info": {"name": "torch.nn.parallel.DistributedDataParallel", "parameters": []}},
{"id": "torch.nn.utils.prune.PruningContainer.remove", "type": "method", "code": "torch.nn.utils.prune.PruningContainer.remove(module)", "example": "NA", "summary": "Removes the pruning reparameterization from a module", "returns": "pruned version of the input tensor", "shape": "", "code-info": {"name": "torch.nn.utils.prune.PruningContainer.remove", "parameters": []}},
{"id": "torch.nn.utils.prune.PruningContainer", "type": "class", "code": "torch.nn.utils.prune.PruningContainer(*args)", "example": "NA", "summary": "Container holding a sequence of pruning methods for iterative pruning", "returns": "pruned version of the input tensor", "shape": "", "code-info": {"name": "torch.nn.utils.prune.PruningContainer", "parameters": []}},
{"id": "torch.nn.DataParallel", "type": "class", "code": "torch.nn.DataParallel(module,device_ids=None,output_device=None,dim=0)", "example": "  net = torch.nn.DataParallel(model, device_ids=[0, 1, 2])  output = net(input_var)  # input_var can be on any device, including CPU   ", "summary": "Implements data parallelism at the module level", "returns": [], "shape": "", "code-info": {"name": "torch.nn.DataParallel", "parameters": []}},
{"id": "torch.nn.UpsamplingBilinear2d", "type": "class", "code": "torch.nn.UpsamplingBilinear2d(size:Optional[Union[T,Tuple[T,T]]]=None,scale_factor:Optional[Union[T,Tuple[T,T]]]=None)", "example": "NA", "summary": "Applies a 2D bilinear upsampling to an input signal composed of several input channels", "returns": [], "shape": "", "code-info": {"name": "torch.nn.UpsamplingBilinear2d", "parameters": [{"name": "size", "type": "Optional[Union[T,Tuple[T,T]]]", "default_value": "None", "is_optional": true, "description": "output spatial sizes"}, {"name": "scale_factor", "type": "Optional[Union[T,Tuple[T,T]]]", "default_value": "None", "is_optional": true, "description": "multiplier forspatial size."}]}},
{"id": "torch.nn.utils.prune.BasePruningMethod.prune", "type": "method", "code": "torch.nn.utils.prune.BasePruningMethod.prune(t,default_mask=None)", "example": "NA", "summary": "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask()", "returns": "pruned version of the input tensor", "shape": "", "code-info": {"name": "torch.nn.utils.prune.BasePruningMethod.prune", "parameters": []}},
{"id": "torch.nn.utils.prune.BasePruningMethod.remove", "type": "method", "code": "torch.nn.utils.prune.BasePruningMethod.remove(module)", "example": "NA", "summary": "Removes the pruning reparameterization from a module", "returns": "pruned version of the input tensor", "shape": "", "code-info": {"name": "torch.nn.utils.prune.BasePruningMethod.remove", "parameters": []}},
{"id": "torch.nn.utils.prune.BasePruningMethod", "type": "class", "code": "torch.nn.utils.prune.BasePruningMethod", "example": "NA", "summary": "Abstract base class for creation of new pruning techniques", "returns": "pruned version of the input tensor", "shape": "", "code-info": {"name": "torch.nn.utils.prune.BasePruningMethod", "parameters": []}},
{"id": "torch.nn.Flatten.named_parameters", "type": "method", "code": "torch.nn.Flatten.named_parameters(prefix:str='',recurse:bool=True)", "example": "  for name, param in self.named_parameters():     if name in ['bias']:         print(param.size())   ", "summary": "Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Flatten.named_parameters", "parameters": [{"name": "prefix", "type": "str", "default_value": "''", "is_optional": false, "description": "prefix to prepend to all parameter names."}, {"name": "recurse", "type": "bool", "default_value": "True", "is_optional": false, "description": "if True, then yields parameters of this moduleand all submodules. Otherwise, yields only parameters thatare direct members of this module."}]}},
{"id": "torch.nn.Flatten.parameters", "type": "method", "code": "torch.nn.Flatten.parameters(recurse:bool=True)", "example": "  for param in model.parameters():      print(type(param), param.size()) &lt;class 'torch.Tensor' (20L,) &lt;class 'torch.Tensor' (20L, 1L, 5L, 5L)   ", "summary": "Returns an iterator over module parameters", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Flatten.parameters", "parameters": [{"name": "recurse", "type": "bool", "default_value": "True", "is_optional": false, "description": "if True, then yields parameters of this moduleand all submodules. Otherwise, yields only parameters thatare direct members of this module."}]}},
{"id": "torch.nn.Upsample", "type": "class", "code": "torch.nn.Upsample(size:Optional[Union[T,Tuple[T,...]]]=None,scale_factor:Optional[Union[T,Tuple[T,...]]]=None,mode:str='nearest',align_corners:Optional[bool]=None)", "example": "NA", "summary": "Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data", "returns": [], "shape": "", "code-info": {"name": "torch.nn.Upsample", "parameters": [{"name": "size", "type": "Optional[Union[T,Tuple[T,...]]]", "default_value": "None", "is_optional": true, "description": "output spatial sizes"}, {"name": "scale_factor", "type": "Optional[Union[T,Tuple[T,...]]]", "default_value": "None", "is_optional": true, "description": "multiplier for spatial size. Has to match input size if it is a tuple."}, {"name": "mode", "type": "str", "default_value": "'nearest'", "is_optional": true, "description": "the upsampling algorithm: one of 'nearest','linear', 'bilinear', 'bicubic' and 'trilinear'.Default: 'nearest'"}, {"name": "align_corners", "type": "Optional[bool]", "default_value": "False", "is_optional": true, "description": "if True, the corner pixels of the inputand output tensors are aligned, and thus preserving the values atthose pixels. This only has effect when mode is'linear', 'bilinear', or 'trilinear'. Default: False"}]}},
{"id": "torch.nn.UpsamplingNearest2d", "type": "class", "code": "torch.nn.UpsamplingNearest2d(size:Optional[Union[T,Tuple[T,T]]]=None,scale_factor:Optional[Union[T,Tuple[T,T]]]=None)", "example": "NA", "summary": "Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels", "returns": [], "shape": "", "code-info": {"name": "torch.nn.UpsamplingNearest2d", "parameters": [{"name": "size", "type": "Optional[Union[T,Tuple[T,T]]]", "default_value": "None", "is_optional": true, "description": "output spatial sizes"}, {"name": "scale_factor", "type": "Optional[Union[T,Tuple[T,T]]]", "default_value": "None", "is_optional": true, "description": "multiplier forspatial size."}]}},
{"id": "torch.nn.MultiMarginLoss", "type": "class", "code": "torch.nn.MultiMarginLoss(p:int=1,margin:float=1.0,weight:Optional[torch.Tensor]=None,size_average=None,reduce=None,reduction:str='mean')", "example": "NA", "summary": "Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between input xxx   (a 2D mini-batch Tensor) and output yyy   (which is a 1D tensor of target class indices, 0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121  ): For each mini-batch sample, the loss in terms of the 1D input xxx   and scalar output yyy   is:  loss(x,y)=\u2211imax\u2061(0,margin\u2212x[y]+x[i]))px.size(0)\\text{loss}(x, y) = \\frac{\\sum_i \\max(0, \\text{margin} - x[y] + x[i]))^p}{\\text{x.size}(0)}  loss(x,y)=x.size(0)\u2211i\u200bmax(0,margin\u2212x[y]+x[i]))p\u200b  where x\u2208{0,\u2005\u200a\u22ef\u2009,\u2005\u200ax.size(0)\u22121}x \\in \\left\\{0, \\; \\cdots , \\; \\text{x.size}(0) - 1\\right\\}x\u2208{0,\u22ef,x.size(0)\u22121}   and i\u2260yi \\neq yi\ue020\u200b=y  ", "returns": [], "shape": "", "code-info": {"name": "torch.nn.MultiMarginLoss", "parameters": [{"name": "p", "type": "int", "default_value": "1", "is_optional": true, "description": "Has a default value of 111. 111 and 222are the only supported values."}, {"name": "margin", "type": "float", "default_value": "1.0", "is_optional": true, "description": "Has a default value of 111."}, {"name": "weight", "type": "Optional[torch.Tensor]", "default_value": "None", "is_optional": true, "description": "a manual rescaling weight given to eachclass. If given, it has to be a Tensor of size C. Otherwise, it istreated as if having all ones."}, {"name": "reduction", "type": "str", "default_value": "'mean'", "is_optional": true, "description": "Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"}]}},
{"id": "torch.nn.CosineEmbeddingLoss", "type": "class", "code": "torch.nn.CosineEmbeddingLoss(margin:float=0.0,size_average=None,reduce=None,reduction:str='mean')", "example": "NA", "summary": "Creates a criterion that measures the loss given input tensors x1x_1x1\u200b  , x2x_2x2\u200b   and a Tensor label yyy   with values 1 or -1", "returns": [], "shape": "", "code-info": {"name": "torch.nn.CosineEmbeddingLoss", "parameters": [{"name": "margin", "type": "float", "default_value": "0.0", "is_optional": true, "description": "Should be a number from \u22121-1\u22121 to 111,000 to 0.50.50.5 is suggested. If margin is missing, thedefault value is 000."}, {"name": "reduction", "type": "str", "default_value": "'mean'", "is_optional": true, "description": "Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"}]}},
{"id": "torch.nn.TripletMarginLoss", "type": "class", "code": "torch.nn.TripletMarginLoss(margin:float=1.0,p:float=2.0,eps:float=1e-06,swap:bool=False,size_average=None,reduce=None,reduction:str='mean')", "example": "NA", "summary": "Creates a criterion that measures the triplet loss given an input tensors x1x1x1  , x2x2x2  , x3x3x3   and a margin with a value greater than 000  ", "returns": [], "shape": "", "code-info": {"name": "torch.nn.TripletMarginLoss", "parameters": [{"name": "margin", "type": "float", "default_value": "111", "is_optional": true, "description": "Default: 111."}, {"name": "p", "type": "float", "default_value": "222", "is_optional": true, "description": "The norm degree for pairwise distance. Default: 222."}, {"name": "eps", "type": "float", "default_value": "1e-06", "is_optional": false, "description": ""}, {"name": "swap", "type": "bool", "default_value": "False", "is_optional": true, "description": "The distance swap is described in detail in the paperLearning shallow convolutional feature descriptors with triplet losses byV. Balntas, E. Riba et al. Default: False."}, {"name": "reduction", "type": "str", "default_value": "'mean'", "is_optional": true, "description": "Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"}]}},
{"id": "torch.nn.PixelShuffle", "type": "class", "code": "torch.nn.PixelShuffle(upscale_factor:int)", "example": "NA", "summary": "Rearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W)   to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r)  ", "returns": [], "shape": "", "code-info": {"name": "torch.nn.PixelShuffle", "parameters": [{"name": "upscale_factor", "type": "int", "is_optional": false, "description": "factor to increase spatial resolution by"}]}},
{"id": "torch.nn.Flatten.register_backward_hook", "type": "method", "code": "torch.nn.Flatten.register_backward_hook(hook:Callable[[Module,Union[Tuple[torch.Tensor,...],torch.Tensor],Union[Tuple[torch.Tensor,...],torch.Tensor]],Union[None,torch.Tensor]])", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "summary": "Registers a backward hook on the module", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Flatten.register_backward_hook", "parameters": [{"name": "hook", "type": "Callable[[Module,Union[Tuple[torch.Tensor,...],torch.Tensor],Union[Tuple[torch.Tensor,...],torch.Tensor]],Union[None,torch.Tensor]]", "is_optional": false, "description": ""}]}},
{"id": "torch.nn.Flatten.register_buffer", "type": "method", "code": "torch.nn.Flatten.register_buffer(name:str,tensor:Optional[torch.Tensor],persistent:bool=True)", "example": "  self.register_buffer('running_mean', torch.zeros(num_features))   ", "summary": "Adds a buffer to the module", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Flatten.register_buffer", "parameters": [{"name": "name", "type": "str", "is_optional": false, "description": "name of the buffer. The buffer can be accessedfrom this module using the given name"}, {"name": "tensor", "type": "Optional[torch.Tensor]", "is_optional": false, "description": "buffer to be registered."}, {"name": "persistent", "type": "bool", "default_value": "True", "is_optional": false, "description": "whether the buffer is part of this module\u2019sstate_dict."}]}},
{"id": "torch.nn.MultiLabelSoftMarginLoss", "type": "class", "code": "torch.nn.MultiLabelSoftMarginLoss(weight:Optional[torch.Tensor]=None,size_average=None,reduce=None,reduction:str='mean')", "example": "NA", "summary": "Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between input xxx   and target yyy   of size (N,C)(N, C)(N,C)  ", "returns": [], "shape": "", "code-info": {"name": "torch.nn.MultiLabelSoftMarginLoss", "parameters": [{"name": "weight", "type": "Optional[torch.Tensor]", "default_value": "None", "is_optional": true, "description": "a manual rescaling weight given to eachclass. If given, it has to be a Tensor of size C. Otherwise, it istreated as if having all ones."}, {"name": "reduction", "type": "str", "default_value": "'mean'", "is_optional": true, "description": "Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"}]}},
{"id": "torch.nn.SoftMarginLoss", "type": "class", "code": "torch.nn.SoftMarginLoss(size_average=None,reduce=None,reduction:str='mean')", "example": "NA", "summary": "Creates a criterion that optimizes a two-class classification logistic loss between input tensor xxx   and target tensor yyy   (containing 1 or -1)", "returns": [], "shape": "", "code-info": {"name": "torch.nn.SoftMarginLoss", "parameters": [{"name": "reduction", "type": "str", "default_value": "'mean'", "is_optional": true, "description": "Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"}]}},
{"id": "torch.nn.Flatten.register_forward_hook", "type": "method", "code": "torch.nn.Flatten.register_forward_hook(hook:Callable[...,None])", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "summary": "Registers a forward hook on the module", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Flatten.register_forward_hook", "parameters": [{"name": "hook", "type": "Callable[...,None]", "is_optional": false, "description": ""}]}},
{"id": "torch.nn.Flatten.register_forward_pre_hook", "type": "method", "code": "torch.nn.Flatten.register_forward_pre_hook(hook:Callable[...,None])", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "summary": "Registers a forward pre-hook on the module", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Flatten.register_forward_pre_hook", "parameters": [{"name": "hook", "type": "Callable[...,None]", "is_optional": false, "description": ""}]}},
{"id": "torch.nn.SmoothL1Loss", "type": "class", "code": "torch.nn.SmoothL1Loss(size_average=None,reduce=None,reduction:str='mean')", "example": "NA", "summary": "Creates a criterion that uses a squared term if the absolute element-wise error falls below 1 and an L1 term otherwise", "returns": [], "shape": "", "code-info": {"name": "torch.nn.SmoothL1Loss", "parameters": [{"name": "reduction", "type": "str", "default_value": "'mean'", "is_optional": true, "description": "Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"}]}},
{"id": "torch.nn.MarginRankingLoss", "type": "class", "code": "torch.nn.MarginRankingLoss(margin:float=0.0,size_average=None,reduce=None,reduction:str='mean')", "example": "NA", "summary": "Creates a criterion that measures the loss given inputs x1x1x1  , x2x2x2  , two 1D mini-batch Tensors, and a label 1D mini-batch tensor yyy   (containing 1 or -1)", "returns": [], "shape": "", "code-info": {"name": "torch.nn.MarginRankingLoss", "parameters": [{"name": "margin", "type": "float", "default_value": "0.0", "is_optional": true, "description": "Has a default value of 000."}, {"name": "reduction", "type": "str", "default_value": "'mean'", "is_optional": true, "description": "Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"}]}},
{"id": "torch.nn.BCEWithLogitsLoss", "type": "class", "code": "torch.nn.BCEWithLogitsLoss(weight:Optional[torch.Tensor]=None,size_average=None,reduce=None,reduction:str='mean',pos_weight:Optional[torch.Tensor]=None)", "example": "NA", "summary": "This loss combines a Sigmoid layer and the BCELoss in one single class", "returns": [], "shape": "\n\nInput: (N,\u2217)(N, *)(N,\u2217)\n\n where \u2217*\u2217\n\n means, any number of additional dimensions\nTarget: (N,\u2217)(N, *)(N,\u2217)\n\n, same shape as the input\nOutput: scalar. If reduction is 'none', then (N,\u2217)(N, *)(N,\u2217)\n\n, same\nshape as input.\n\n\nExamples:\n&gt;&gt;&gt; loss = nn.BCEWithLogitsLoss()\n&gt;&gt;&gt; input = torch.randn(3, requires_grad=True)\n&gt;&gt;&gt; target = torch.empty(3).random_(2)\n&gt;&gt;&gt; output = loss(input, target)\n&gt;&gt;&gt; output.backward()\n\n\n", "code-info": {"name": "torch.nn.BCEWithLogitsLoss", "parameters": [{"name": "weight", "type": "Optional[torch.Tensor]", "default_value": "None", "is_optional": true, "description": "a manual rescaling weight given to the lossof each batch element. If given, has to be a Tensor of size nbatch."}, {"name": "reduction", "type": "str", "default_value": "'mean'", "is_optional": true, "description": "Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"}, {"name": "pos_weight", "type": "Optional[torch.Tensor]", "default_value": "None", "is_optional": true, "description": "a weight of positive examples.Must be a vector with length equal to the number of classes."}]}},
{"id": "torch.nn.HingeEmbeddingLoss", "type": "class", "code": "torch.nn.HingeEmbeddingLoss(margin:float=1.0,size_average=None,reduce=None,reduction:str='mean')", "example": "NA", "summary": "Measures the loss given an input tensor xxx   and a labels tensor yyy   (containing 1 or -1)", "returns": [], "shape": "", "code-info": {"name": "torch.nn.HingeEmbeddingLoss", "parameters": [{"name": "margin", "type": "float", "default_value": "1.0", "is_optional": true, "description": "Has a default value of 1."}, {"name": "reduction", "type": "str", "default_value": "'mean'", "is_optional": true, "description": "Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"}]}},
{"id": "torch.nn.BCELoss", "type": "class", "code": "torch.nn.BCELoss(weight:Optional[torch.Tensor]=None,size_average=None,reduce=None,reduction:str='mean')", "example": "NA", "summary": "Creates a criterion that measures the Binary Cross Entropy between the target and the output: The unreduced (i.e", "returns": [], "shape": "", "code-info": {"name": "torch.nn.BCELoss", "parameters": [{"name": "weight", "type": "Optional[torch.Tensor]", "default_value": "None", "is_optional": true, "description": "a manual rescaling weight given to the lossof each batch element. If given, has to be a Tensor of size nbatch."}, {"name": "reduction", "type": "str", "default_value": "'mean'", "is_optional": true, "description": "Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"}]}},
{"id": "torch.nn.MultiLabelMarginLoss", "type": "class", "code": "torch.nn.MultiLabelMarginLoss(size_average=None,reduce=None,reduction:str='mean')", "example": "NA", "summary": "Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between input xxx   (a 2D mini-batch Tensor) and output yyy   (which is a 2D Tensor of target class indices)", "returns": [], "shape": "", "code-info": {"name": "torch.nn.MultiLabelMarginLoss", "parameters": [{"name": "reduction", "type": "str", "default_value": "'mean'", "is_optional": true, "description": "Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"}]}},
{"id": "torch.nn.Flatten.register_parameter", "type": "method", "code": "torch.nn.Flatten.register_parameter(name:str,param:Optional[torch.nn.parameter.Parameter])", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "summary": "Adds a parameter to the module", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Flatten.register_parameter", "parameters": [{"name": "name", "type": "str", "is_optional": false, "description": "name of the parameter. The parameter can be accessedfrom this module using the given name"}, {"name": "param", "type": "Optional[torch.nn.parameter.Parameter]", "is_optional": false, "description": "parameter to be added to the module."}]}},
{"id": "torch.nn.Flatten.requires_grad_", "type": "method", "code": "torch.nn.Flatten.requires_grad_(requires_grad:bool=True)", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "summary": "Change if autograd should record operations on parameters in this module", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Flatten.requires_grad_", "parameters": [{"name": "requires_grad", "type": "bool", "default_value": "True", "is_optional": false, "description": "whether autograd should record operations onparameters in this module. Default: True."}]}},
{"id": "torch.nn.KLDivLoss", "type": "class", "code": "torch.nn.KLDivLoss(size_average=None,reduce=None,reduction:str='mean',log_target:bool=False)", "example": "NA", "summary": "The Kullback-Leibler divergence Loss KL divergence is a useful distance measure for continuous distributions and is often useful when performing direct regression over the space of (discretely sampled) continuous output distributions", "returns": [], "shape": "", "code-info": {"name": "torch.nn.KLDivLoss", "parameters": [{"name": "reduction", "type": "str", "default_value": "'mean'", "is_optional": true, "description": "Specifies the reduction to apply to the output:'none' | 'batchmean' | 'sum' | 'mean'.'none': no reduction will be applied.'batchmean': the sum of the output will be divided by batchsize.'sum': the output will be summed.'mean': the output will be divided by the number of elements in the output.Default: 'mean'"}, {"name": "log_target", "type": "bool", "default_value": "False", "is_optional": true, "description": "Specifies whether target is passed in the log space.Default: False"}]}},
{"id": "torch.nn.PoissonNLLLoss", "type": "class", "code": "torch.nn.PoissonNLLLoss(log_input:bool=True,full:bool=False,size_average=None,eps:float=1e-08,reduce=None,reduction:str='mean')", "example": "NA", "summary": "Negative log likelihood loss with Poisson distribution of target", "returns": [], "shape": "", "code-info": {"name": "torch.nn.PoissonNLLLoss", "parameters": [{"name": "log_input", "type": "bool", "default_value": "True", "is_optional": true, "description": "if True the loss is computed asexp\u2061(input)\u2212target\u2217input\\exp(\\text{input}) - \\text{target}*\\text{input}exp(input)\u2212target\u2217input, if False the loss isinput\u2212target\u2217log\u2061(input+eps)\\text{input} - \\text{target}*\\log(\\text{input}+\\text{eps})input\u2212target\u2217log(input+eps)."}, {"name": "full", "type": "bool", "default_value": "False", "is_optional": true, "description": "whether to compute full loss, i. e. to add theStirling approximation termtarget\u2217log\u2061(target)\u2212target+0.5\u2217log\u2061(2\u03c0target).\\text{target}*\\log(\\text{target}) - \\text{target} + 0.5 * \\log(2\\pi\\text{target}).target\u2217log(target)\u2212target+0.5\u2217log(2\u03c0target)."}, {"name": "eps", "type": "float", "default_value": "1e-8", "is_optional": true, "description": "Small value to avoid evaluation of log\u2061(0)\\log(0)log(0) whenlog_input = False. Default: 1e-8"}, {"name": "reduction", "type": "str", "default_value": "'mean'", "is_optional": true, "description": "Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"}]}},
{"id": "torch.nn.Flatten.state_dict", "type": "method", "code": "torch.nn.Flatten.state_dict(destination=None,prefix='',keep_vars=False)", "example": "  module.state_dict().keys() ['bias', 'weight']   ", "summary": "Returns a dictionary containing a whole state of the module", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Flatten.state_dict", "parameters": []}},
{"id": "torch.nn.Flatten.to", "type": "method", "code": "torch.nn.Flatten.to(*args,**kwargs)", "example": "  linear = nn.Linear(2, 2)  linear.weight Parameter containing: tensor([[ 0.1913, -0.3420],         [-0.5113, -0.2325]])  linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True)  linear.weight Parameter containing: tensor([[ 0.1913, -0.3420],         [-0.5113, -0.2325]], dtype=torch.float64)  gpu1 = torch.device(\"cuda:1\")  linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True)  linear.weight Parameter containing: tensor([[ 0.1914, -0.3420],         [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')  cpu = torch.device(\"cpu\")  linear.to(cpu) Linear(in_features=2, out_features=2, bias=True)  linear.weight Parameter containing: tensor([[ 0.1914, -0.3420],         [-0.5112, -0.2324]], dtype=torch.float16)   ", "summary": "Moves and/or casts the parameters and buffers", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Flatten.to", "parameters": []}},
{"id": "torch.nn.CTCLoss", "type": "class", "code": "torch.nn.CTCLoss(blank:int=0,reduction:str='mean',zero_infinity:bool=False)", "example": "NA", "summary": "The Connectionist Temporal Classification loss", "returns": [], "shape": "", "code-info": {"name": "torch.nn.CTCLoss", "parameters": [{"name": "blank", "type": "int", "default_value": "0", "is_optional": true, "description": "blank label. Default 000."}, {"name": "reduction", "type": "str", "default_value": "'mean'", "is_optional": true, "description": "Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the output losses will be divided by the target lengths andthen the mean over the batch is taken. Default: 'mean'"}, {"name": "zero_infinity", "type": "bool", "default_value": "FalseInfinite", "is_optional": true, "description": "Whether to zero infinite losses and the associated gradients.Default: FalseInfinite losses mainly occur when the inputs are too shortto be aligned to the targets."}]}},
{"id": "torch.nn.CrossEntropyLoss", "type": "class", "code": "torch.nn.CrossEntropyLoss(weight:Optional[torch.Tensor]=None,size_average=None,ignore_index:int=-100,reduce=None,reduction:str='mean')", "example": "NA", "summary": "This criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class", "returns": [], "shape": "", "code-info": {"name": "torch.nn.CrossEntropyLoss", "parameters": [{"name": "weight", "type": "Optional[torch.Tensor]", "default_value": "None", "is_optional": true, "description": "a manual rescaling weight given to each class.If given, has to be a Tensor of size C"}, {"name": "ignore_index", "type": "int", "default_value": "-100", "is_optional": true, "description": "Specifies a target value that is ignoredand does not contribute to the input gradient. When size_average isTrue, the loss is averaged over non-ignored targets."}, {"name": "reduction", "type": "str", "default_value": "'mean'", "is_optional": true, "description": "Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction willbe applied, 'mean': the weighted mean of the output is taken,'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and inthe meantime, specifying either of those two args will overridereduction. Default: 'mean'"}]}},
{"id": "torch.nn.PairwiseDistance", "type": "class", "code": "torch.nn.PairwiseDistance(p:float=2.0,eps:float=1e-06,keepdim:bool=False)", "example": "NA", "summary": "Computes the batchwise pairwise distance between vectors v1v_1v1\u200b  , v2v_2v2\u200b   using the p-norm:  \u2225x\u2225p=(\u2211i=1n\u2223xi\u2223p)1/p.\\Vert x \\Vert _p = \\left( \\sum_{i=1}^n  \\vert x_i \\vert ^ p \\right) ^ {1/p}", "returns": [], "shape": "\nInput1: (N,D)(N, D)(N,D)\n\n where D = vector dimension\nInput2: (N,D)(N, D)(N,D)\n\n, same shape as the Input1\nOutput: (N)(N)(N)\n\n. If keepdim is True, then (N,1)(N, 1)(N,1)\n\n.\n\n", "code-info": {"name": "torch.nn.PairwiseDistance", "parameters": [{"name": "p", "type": "float", "default_value": "2", "is_optional": false, "description": "the norm degree. Default: 2"}, {"name": "eps", "type": "float", "default_value": "1e-6", "is_optional": true, "description": "Small value to avoid division by zero.Default: 1e-6"}, {"name": "keepdim", "type": "bool", "default_value": "False", "is_optional": true, "description": "Determines whether or not to keep the vector dimension.Default: False"}]}},
{"id": "torch.nn.NLLLoss", "type": "class", "code": "torch.nn.NLLLoss(weight:Optional[torch.Tensor]=None,size_average=None,ignore_index:int=-100,reduce=None,reduction:str='mean')", "example": "NA", "summary": "The negative log likelihood loss", "returns": [], "shape": "", "code-info": {"name": "torch.nn.NLLLoss", "parameters": [{"name": "weight", "type": "Optional[torch.Tensor]", "default_value": "None", "is_optional": true, "description": "a manual rescaling weight given to eachclass. If given, it has to be a Tensor of size C. Otherwise, it istreated as if having all ones."}, {"name": "ignore_index", "type": "int", "default_value": "-100", "is_optional": true, "description": "Specifies a target value that is ignoredand does not contribute to the input gradient. Whensize_average is True, the loss is averaged overnon-ignored targets."}, {"name": "reduction", "type": "str", "default_value": "'mean'", "is_optional": true, "description": "Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction willbe applied, 'mean': the weighted mean of the output is taken,'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and inthe meantime, specifying either of those two args will overridereduction. Default: 'mean'"}]}},
{"id": "torch.nn.MSELoss", "type": "class", "code": "torch.nn.MSELoss(size_average=None,reduce=None,reduction:str='mean')", "example": "NA", "summary": "Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the input xxx   and target yyy  ", "returns": [], "shape": "", "code-info": {"name": "torch.nn.MSELoss", "parameters": [{"name": "reduction", "type": "str", "default_value": "'mean'", "is_optional": true, "description": "Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"}]}},
{"id": "torch.nn.L1Loss", "type": "class", "code": "torch.nn.L1Loss(size_average=None,reduce=None,reduction:str='mean')", "example": "NA", "summary": "Creates a criterion that measures the mean absolute error (MAE) between each element in the input xxx   and target yyy  ", "returns": [], "shape": "", "code-info": {"name": "torch.nn.L1Loss", "parameters": [{"name": "reduction", "type": "str", "default_value": "'mean'", "is_optional": true, "description": "Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"}]}},
{"id": "torch.nn.Flatten.train", "type": "method", "code": "torch.nn.Flatten.train(mode:bool=True)", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "summary": "Sets the module in training mode", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Flatten.train", "parameters": [{"name": "mode", "type": "bool", "default_value": "True", "is_optional": false, "description": "whether to set training mode (True) or evaluationmode (False). Default: True."}]}},
{"id": "torch.nn.Flatten.type", "type": "method", "code": "torch.nn.Flatten.type(dst_type:Union[torch.dtype,str])", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "summary": "Casts all parameters and buffers to dst_type", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Flatten.type", "parameters": [{"name": "dst_type", "type": "Union[torch.dtype,str]", "is_optional": false, "description": "the desired type"}]}},
{"id": "torch.nn.CosineSimilarity", "type": "class", "code": "torch.nn.CosineSimilarity(dim:int=1,eps:float=1e-08)", "example": "NA", "summary": "Returns cosine similarity between x1x_1x1\u200b   and x2x_2x2\u200b  , computed along dim", "returns": [], "shape": "\nInput1: (\u22171,D,\u22172)(\\ast_1, D, \\ast_2)(\u22171\u200b,D,\u22172\u200b)\n\n where D is at position dim\nInput2: (\u22171,D,\u22172)(\\ast_1, D, \\ast_2)(\u22171\u200b,D,\u22172\u200b)\n\n, same shape as the Input1\nOutput: (\u22171,\u22172)(\\ast_1, \\ast_2)(\u22171\u200b,\u22172\u200b)\n\n\n\n", "code-info": {"name": "torch.nn.CosineSimilarity", "parameters": [{"name": "dim", "type": "int", "default_value": "1", "is_optional": true, "description": "Dimension where cosine similarity is computed. Default: 1"}, {"name": "eps", "type": "float", "default_value": "1e-8", "is_optional": true, "description": "Small value to avoid division by zero.Default: 1e-8"}]}},
{"id": "torch.nn.Flatten.zero_grad", "type": "method", "code": "torch.nn.Flatten.zero_grad()", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "summary": "Sets gradients of all model parameters to zero", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Flatten.zero_grad", "parameters": []}},
{"id": "torch.nn.EmbeddingBag.from_pretrained", "type": "method", "code": "torch.nn.EmbeddingBag.from_pretrained(embeddings:torch.Tensor,freeze:bool=True,max_norm:Optional[float]=None,norm_type:float=2.0,scale_grad_by_freq:bool=False,mode:str='mean',sparse:bool=False,include_last_offset:bool=False)", "example": "NA", "summary": "Creates EmbeddingBag instance from given 2-dimensional FloatTensor", "returns": [], "shape": "", "code-info": {"name": "torch.nn.EmbeddingBag.from_pretrained", "parameters": [{"name": "embeddings", "type": "torch.Tensor", "is_optional": false, "description": "FloatTensor containing weights for the EmbeddingBag.First dimension is being passed to EmbeddingBag as \u2018num_embeddings\u2019, second as \u2018embedding_dim\u2019."}, {"name": "freeze", "type": "bool", "default_value": "True", "is_optional": true, "description": "If True, the tensor does not get updated in the learning process.Equivalent to embeddingbag.weight.requires_grad = False. Default: True"}, {"name": "max_norm", "type": "Optional[float]", "default_value": "None", "is_optional": true, "description": "See module initialization documentation. Default: None"}, {"name": "norm_type", "type": "float", "default_value": "2.0", "is_optional": true, "description": "See module initialization documentation. Default 2."}, {"name": "scale_grad_by_freq", "type": "bool", "default_value": "False", "is_optional": true, "description": "See module initialization documentation. Default False."}, {"name": "mode", "type": "str", "default_value": "\"mean\"", "is_optional": true, "description": "See module initialization documentation. Default: \"mean\""}, {"name": "sparse", "type": "bool", "default_value": "False", "is_optional": true, "description": "See module initialization documentation. Default: False."}, {"name": "include_last_offset", "type": "bool", "default_value": "False", "is_optional": true, "description": "See module initialization documentation. Default: False."}]}},
{"id": "torch.nn.Embedding.from_pretrained", "type": "method", "code": "torch.nn.Embedding.from_pretrained(embeddings,freeze=True,padding_idx=None,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,sparse=False)", "example": "NA", "summary": "Creates Embedding instance from given 2-dimensional FloatTensor", "returns": [], "shape": "", "code-info": {"name": "torch.nn.Embedding.from_pretrained", "parameters": []}},
{"id": "torch.nn.Embedding", "type": "class", "code": "torch.nn.Embedding(num_embeddings:int,embedding_dim:int,padding_idx:Optional[int]=None,max_norm:Optional[float]=None,norm_type:float=2.0,scale_grad_by_freq:bool=False,sparse:bool=False,_weight:Optional[torch.Tensor]=None)", "example": "NA", "summary": "A simple lookup table that stores embeddings of a fixed dictionary and size", "returns": [], "shape": "", "code-info": {"name": "torch.nn.Embedding", "parameters": [{"name": "num_embeddings", "type": "int", "is_optional": false, "description": "size of the dictionary of embeddings"}, {"name": "embedding_dim", "type": "int", "is_optional": false, "description": "the size of each embedding vector"}, {"name": "padding_idx", "type": "Optional[int]", "default_value": "None", "is_optional": true, "description": "See module initialization documentation."}, {"name": "max_norm", "type": "Optional[float]", "default_value": "None", "is_optional": true, "description": "See module initialization documentation."}, {"name": "norm_type", "type": "float", "default_value": "2.0", "is_optional": true, "description": "See module initialization documentation. Default 2."}, {"name": "scale_grad_by_freq", "type": "bool", "default_value": "False", "is_optional": true, "description": "See module initialization documentation. Default False."}, {"name": "sparse", "type": "bool", "default_value": "False", "is_optional": true, "description": "See module initialization documentation."}, {"name": "_weight", "type": "Optional[torch.Tensor]", "default_value": "None", "is_optional": false, "description": ""}]}},
{"id": "torch.nn.AlphaDropout", "type": "class", "code": "torch.nn.AlphaDropout(p:float=0.5,inplace:bool=False)", "example": "NA", "summary": "Applies Alpha Dropout over the input", "returns": [], "shape": "", "code-info": {"name": "torch.nn.AlphaDropout", "parameters": [{"name": "p", "type": "float", "default_value": "05", "is_optional": false, "description": "probability of an element to be dropped. Default: 0.5"}, {"name": "inplace", "type": "bool", "default_value": "False", "is_optional": true, "description": "If set to True, will do this operationin-place"}]}},
{"id": "torch.nn.Dropout2d", "type": "class", "code": "torch.nn.Dropout2d(p:float=0.5,inplace:bool=False)", "example": "NA", "summary": "Randomly zero out entire channels (a channel is a 2D feature map, e.g., the jjj  -th channel of the iii  -th sample in the batched input is a 2D tensor input[i,j]\\text{input}[i, j]input[i,j]  )", "returns": [], "shape": "", "code-info": {"name": "torch.nn.Dropout2d", "parameters": [{"name": "p", "type": "float", "default_value": "0.5", "is_optional": true, "description": "probability of an element to be zero-ed."}, {"name": "inplace", "type": "bool", "default_value": "False", "is_optional": true, "description": "If set to True, will do this operationin-place"}]}},
{"id": "torch.nn.Dropout", "type": "class", "code": "torch.nn.Dropout(p:float=0.5,inplace:bool=False)", "example": "NA", "summary": "During training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution", "returns": [], "shape": "", "code-info": {"name": "torch.nn.Dropout", "parameters": [{"name": "p", "type": "float", "default_value": "05", "is_optional": false, "description": "probability of an element to be zeroed. Default: 0.5"}, {"name": "inplace", "type": "bool", "default_value": "False", "is_optional": false, "description": "If set to True, will do this operation in-place. Default: False"}]}},
{"id": "torch.nn.Bilinear", "type": "class", "code": "torch.nn.Bilinear(in1_features:int,in2_features:int,out_features:int,bias:bool=True)", "example": "NA", "summary": "Applies a bilinear transformation to the incoming data: y=x1TAx2+by = x_1^T A x_2 + by=x1T\u200bAx2\u200b+b    Parameters  in1_features \u2013 size of each first input sample in2_features \u2013 size of each second input sample out_features \u2013 size of each output sample bias \u2013 If set to False, the layer will not learn an additive bias", "returns": [], "shape": "", "code-info": {"name": "torch.nn.Bilinear", "parameters": [{"name": "in1_features", "type": "int", "is_optional": false, "description": "size of each first input sample"}, {"name": "in2_features", "type": "int", "is_optional": false, "description": "size of each second input sample"}, {"name": "out_features", "type": "int", "is_optional": false, "description": "size of each output sample"}, {"name": "bias", "type": "bool", "default_value": "True", "is_optional": false, "description": "If set to False, the layer will not learn an additive bias.Default: True"}]}},
{"id": "torch.nn.Dropout3d", "type": "class", "code": "torch.nn.Dropout3d(p:float=0.5,inplace:bool=False)", "example": "NA", "summary": "Randomly zero out entire channels (a channel is a 3D feature map, e.g., the jjj  -th channel of the iii  -th sample in the batched input is a 3D tensor input[i,j]\\text{input}[i, j]input[i,j]  )", "returns": [], "shape": "", "code-info": {"name": "torch.nn.Dropout3d", "parameters": [{"name": "p", "type": "float", "default_value": "0.5", "is_optional": true, "description": "probability of an element to be zeroed."}, {"name": "inplace", "type": "bool", "default_value": "False", "is_optional": true, "description": "If set to True, will do this operationin-place"}]}},
{"id": "torch.nn.Flatten", "type": "class", "code": "torch.nn.Flatten(start_dim:int=1,end_dim:int=-1)", "example": "  @torch.no_grad()  def init_weights(m):      print(m)      if type(m) == nn.Linear:          m.weight.fill_(1.0)          print(m.weight)  net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))  net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1.,  1.],         [ 1.,  1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1.,  1.],         [ 1.,  1.]]) Sequential(   (0): Linear(in_features=2, out_features=2, bias=True)   (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential(   (0): Linear(in_features=2, out_features=2, bias=True)   (1): Linear(in_features=2, out_features=2, bias=True) )       bfloat16() \u2192 T Casts all floating point parameters and buffers to bfloat16 datatype.  Returns self  Return type Module       buffers(recurse: bool = True) \u2192 Iterator[torch.Tensor] Returns an iterator over module buffers.  Parameters recurse (bool) \u2013 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.  Yields torch.Tensor \u2013 module buffer   ", "summary": "Flattens a contiguous range of dims into a tensor", "returns": "self", "shape": "\nInput: (N,\u2217dims)(N, *dims)(N,\u2217dims)\n\n\nOutput: (N,\u220f\u2217dims)(N, \\prod *dims)(N,\u220f\u2217dims)\n\n (for the default case).\n\n", "code-info": {"name": "torch.nn.Flatten", "parameters": [{"name": "start_dim", "type": "int", "default_value": "1", "is_optional": false, "description": ""}, {"name": "end_dim", "type": "int", "default_value": "-1", "is_optional": false, "description": ""}]}},
{"id": "torch.nn.EmbeddingBag", "type": "class", "code": "torch.nn.EmbeddingBag(num_embeddings:int,embedding_dim:int,max_norm:Optional[float]=None,norm_type:float=2.0,scale_grad_by_freq:bool=False,mode:str='mean',sparse:bool=False,_weight:Optional[torch.Tensor]=None,include_last_offset:bool=False)", "example": "NA", "summary": "Computes sums or means of \u2018bags\u2019 of embeddings, without instantiating the intermediate embeddings", "returns": [], "shape": "", "code-info": {"name": "torch.nn.EmbeddingBag", "parameters": [{"name": "num_embeddings", "type": "int", "is_optional": false, "description": "size of the dictionary of embeddings"}, {"name": "embedding_dim", "type": "int", "is_optional": false, "description": "the size of each embedding vector"}, {"name": "max_norm", "type": "Optional[float]", "default_value": "None", "is_optional": true, "description": "See module initialization documentation. Default: None"}, {"name": "norm_type", "type": "float", "default_value": "2.0", "is_optional": true, "description": "See module initialization documentation. Default 2."}, {"name": "scale_grad_by_freq", "type": "bool", "default_value": "False", "is_optional": true, "description": "See module initialization documentation. Default False."}, {"name": "mode", "type": "str", "default_value": "\"mean\"", "is_optional": true, "description": "See module initialization documentation. Default: \"mean\""}, {"name": "sparse", "type": "bool", "default_value": "False", "is_optional": true, "description": "See module initialization documentation. Default: False."}, {"name": "_weight", "type": "Optional[torch.Tensor]", "default_value": "None", "is_optional": false, "description": ""}, {"name": "include_last_offset", "type": "bool", "default_value": "False", "is_optional": true, "description": "See module initialization documentation. Default: False."}]}},
{"id": "torch.nn.LSTMCell", "type": "class", "code": "torch.nn.LSTMCell(input_size:int,hidden_size:int,bias:bool=True)", "example": "NA", "summary": "A long short-term memory (LSTM) cell", "returns": [], "shape": "", "code-info": {"name": "torch.nn.LSTMCell", "parameters": [{"name": "input_size", "type": "int", "is_optional": false, "description": "The number of expected features in the input x"}, {"name": "hidden_size", "type": "int", "is_optional": false, "description": "The number of features in the hidden state h"}, {"name": "bias", "type": "bool", "default_value": "True", "is_optional": false, "description": "If False, then the layer does not use bias weights b_ih andb_hh. Default: True"}]}},
{"id": "torch.nn.GRUCell", "type": "class", "code": "torch.nn.GRUCell(input_size:int,hidden_size:int,bias:bool=True)", "example": "NA", "summary": "A gated recurrent unit (GRU) cell  r=\u03c3(Wirx+bir+Whrh+bhr)z=\u03c3(Wizx+biz+Whzh+bhz)n=tanh\u2061(Winx+bin+r\u2217(Whnh+bhn))h\u2032=(1\u2212z)\u2217n+z\u2217h\\begin{array}{ll} r = \\sigma(W_{ir} x + b_{ir} + W_{hr} h + b_{hr}) \\\\ z = \\sigma(W_{iz} x + b_{iz} + W_{hz} h + b_{hz}) \\\\ n = \\tanh(W_{in} x + b_{in} + r * (W_{hn} h + b_{hn})) \\\\ h' = (1 - z) * n + z * h \\end{array}r=\u03c3(Wir\u200bx+bir\u200b+Whr\u200bh+bhr\u200b)z=\u03c3(Wiz\u200bx+biz\u200b+Whz\u200bh+bhz\u200b)n=tanh(Win\u200bx+bin\u200b+r\u2217(Whn\u200bh+bhn\u200b))h\u2032=(1\u2212z)\u2217n+z\u2217h\u200b  where \u03c3\\sigma\u03c3   is the sigmoid function, and \u2217*\u2217   is the Hadamard product", "returns": [], "shape": "", "code-info": {"name": "torch.nn.GRUCell", "parameters": [{"name": "input_size", "type": "int", "is_optional": false, "description": "The number of expected features in the input x"}, {"name": "hidden_size", "type": "int", "is_optional": false, "description": "The number of features in the hidden state h"}, {"name": "bias", "type": "bool", "default_value": "True", "is_optional": false, "description": "If False, then the layer does not use bias weights b_ih andb_hh. Default: True"}]}},
{"id": "torch.nn.TransformerEncoderLayer.forward", "type": "method", "code": "torch.nn.TransformerEncoderLayer.forward(src:torch.Tensor,src_mask:Optional[torch.Tensor]=None,src_key_padding_mask:Optional[torch.Tensor]=None)", "example": "NA", "summary": "Pass the input through the encoder layer", "returns": [], "shape": "", "code-info": {"name": "torch.nn.TransformerEncoderLayer.forward", "parameters": [{"name": "src", "type": "torch.Tensor", "is_optional": false, "description": "the sequence to the encoder layer (required)."}, {"name": "src_mask", "type": "Optional[torch.Tensor]", "default_value": "None", "is_optional": true, "description": "the mask for the src sequence (optional)."}, {"name": "src_key_padding_mask", "type": "Optional[torch.Tensor]", "default_value": "None", "is_optional": true, "description": "the mask for the src keys per batch (optional)."}]}},
{"id": "torch.nn.TransformerEncoderLayer", "type": "class", "code": "torch.nn.TransformerEncoderLayer(d_model,nhead,dim_feedforward=2048,dropout=0.1,activation='relu')", "example": "NA", "summary": "TransformerEncoderLayer is made up of self-attn and feedforward network", "returns": [], "shape": "&gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n&gt;&gt;&gt; src = torch.rand(10, 32, 512)\n&gt;&gt;&gt; out = encoder_layer(src)\n\n\n", "code-info": {"name": "torch.nn.TransformerEncoderLayer", "parameters": []}},
{"id": "torch.nn.Transformer.forward", "type": "method", "code": "torch.nn.Transformer.forward(src:torch.Tensor,tgt:torch.Tensor,src_mask:Optional[torch.Tensor]=None,tgt_mask:Optional[torch.Tensor]=None,memory_mask:Optional[torch.Tensor]=None,src_key_padding_mask:Optional[torch.Tensor]=None,tgt_key_padding_mask:Optional[torch.Tensor]=None,memory_key_padding_mask:Optional[torch.Tensor]=None)", "example": "NA", "summary": "Take in and process masked source/target sequences", "returns": [], "shape": "\nsrc: (S,N,E)(S, N, E)(S,N,E)\n\n.\ntgt: (T,N,E)(T, N, E)(T,N,E)\n\n.\nsrc_mask: (S,S)(S, S)(S,S)\n\n.\ntgt_mask: (T,T)(T, T)(T,T)\n\n.\nmemory_mask: (T,S)(T, S)(T,S)\n\n.\nsrc_key_padding_mask: (N,S)(N, S)(N,S)\n\n.\ntgt_key_padding_mask: (N,T)(N, T)(N,T)\n\n.\nmemory_key_padding_mask: (N,S)(N, S)(N,S)\n\n.\n\nNote: [src/tgt/memory]_mask ensures that position i is allowed to attend the unmasked\npositions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\nwhile the zero positions will be unchanged. If a BoolTensor is provided, positions with True\nare not allowed to attend while False values will be unchanged. If a FloatTensor\nis provided, it will be added to the attention weight.\n[src/tgt/memory]_key_padding_mask provides specified elements in the key to be ignored by\nthe attention. If a ByteTensor is provided, the non-zero positions will be ignored while the zero\npositions will be unchanged. If a BoolTensor is provided, the positions with the\nvalue of True will be ignored while the position with the value of False will be unchanged.\n\noutput: (T,N,E)(T, N, E)(T,N,E)\n\n.\n\nNote: Due to the multi-head attention architecture in the transformer model,\nthe output sequence length of a transformer is same as the input sequence\n(i.e. target) length of the decode.\nwhere S is the source sequence length, T is the target sequence length, N is the\nbatch size, E is the feature number\n", "code-info": {"name": "torch.nn.Transformer.forward", "parameters": [{"name": "src", "type": "torch.Tensor", "is_optional": false, "description": "the sequence to the encoder (required)."}, {"name": "tgt", "type": "torch.Tensor", "is_optional": false, "description": "the sequence to the decoder (required)."}, {"name": "src_mask", "type": "Optional[torch.Tensor]", "default_value": "None", "is_optional": true, "description": "the additive mask for the src sequence (optional)."}, {"name": "tgt_mask", "type": "Optional[torch.Tensor]", "default_value": "None", "is_optional": true, "description": "the additive mask for the tgt sequence (optional)."}, {"name": "memory_mask", "type": "Optional[torch.Tensor]", "default_value": "None", "is_optional": true, "description": "the additive mask for the encoder output (optional)."}, {"name": "src_key_padding_mask", "type": "Optional[torch.Tensor]", "default_value": "None", "is_optional": true, "description": "the ByteTensor mask for src keys per batch (optional)."}, {"name": "tgt_key_padding_mask", "type": "Optional[torch.Tensor]", "default_value": "None", "is_optional": true, "description": "the ByteTensor mask for tgt keys per batch (optional)."}, {"name": "memory_key_padding_mask", "type": "Optional[torch.Tensor]", "default_value": "None", "is_optional": true, "description": "the ByteTensor mask for memory keys per batch (optional)."}]}},
{"id": "torch.nn.TransformerDecoder.forward", "type": "method", "code": "torch.nn.TransformerDecoder.forward(tgt:torch.Tensor,memory:torch.Tensor,tgt_mask:Optional[torch.Tensor]=None,memory_mask:Optional[torch.Tensor]=None,tgt_key_padding_mask:Optional[torch.Tensor]=None,memory_key_padding_mask:Optional[torch.Tensor]=None)", "example": "NA", "summary": "Pass the inputs (and mask) through the decoder layer in turn", "returns": [], "shape": "", "code-info": {"name": "torch.nn.TransformerDecoder.forward", "parameters": [{"name": "tgt", "type": "torch.Tensor", "is_optional": false, "description": "the sequence to the decoder (required)."}, {"name": "memory", "type": "torch.Tensor", "is_optional": false, "description": "the sequence from the last layer of the encoder (required)."}, {"name": "tgt_mask", "type": "Optional[torch.Tensor]", "default_value": "None", "is_optional": true, "description": "the mask for the tgt sequence (optional)."}, {"name": "memory_mask", "type": "Optional[torch.Tensor]", "default_value": "None", "is_optional": true, "description": "the mask for the memory sequence (optional)."}, {"name": "tgt_key_padding_mask", "type": "Optional[torch.Tensor]", "default_value": "None", "is_optional": true, "description": "the mask for the tgt keys per batch (optional)."}, {"name": "memory_key_padding_mask", "type": "Optional[torch.Tensor]", "default_value": "None", "is_optional": true, "description": "the mask for the memory keys per batch (optional)."}]}},
{"id": "torch.nn.TransformerDecoder", "type": "class", "code": "torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)", "example": "NA", "summary": "TransformerDecoder is a stack of N decoder layers  Parameters  decoder_layer \u2013 an instance of the TransformerDecoderLayer() class (required)", "returns": [], "shape": "&gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n&gt;&gt;&gt; transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n&gt;&gt;&gt; memory = torch.rand(10, 32, 512)\n&gt;&gt;&gt; tgt = torch.rand(20, 32, 512)\n&gt;&gt;&gt; out = transformer_decoder(tgt, memory)\n\n\n", "code-info": {"name": "torch.nn.TransformerDecoder", "parameters": []}},
{"id": "torch.nn.TransformerEncoder.forward", "type": "method", "code": "torch.nn.TransformerEncoder.forward(src:torch.Tensor,mask:Optional[torch.Tensor]=None,src_key_padding_mask:Optional[torch.Tensor]=None)", "example": "NA", "summary": "Pass the input through the encoder layers in turn", "returns": [], "shape": "", "code-info": {"name": "torch.nn.TransformerEncoder.forward", "parameters": [{"name": "src", "type": "torch.Tensor", "is_optional": false, "description": "the sequence to the encoder (required)."}, {"name": "mask", "type": "Optional[torch.Tensor]", "default_value": "None", "is_optional": true, "description": "the mask for the src sequence (optional)."}, {"name": "src_key_padding_mask", "type": "Optional[torch.Tensor]", "default_value": "None", "is_optional": true, "description": "the mask for the src keys per batch (optional)."}]}},
{"id": "torch.nn.TransformerEncoder", "type": "class", "code": "torch.nn.TransformerEncoder(encoder_layer,num_layers,norm=None)", "example": "NA", "summary": "TransformerEncoder is a stack of N encoder layers  Parameters  encoder_layer \u2013 an instance of the TransformerEncoderLayer() class (required)", "returns": [], "shape": "&gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n&gt;&gt;&gt; transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n&gt;&gt;&gt; src = torch.rand(10, 32, 512)\n&gt;&gt;&gt; out = transformer_encoder(src)\n\n\n", "code-info": {"name": "torch.nn.TransformerEncoder", "parameters": []}},
{"id": "torch.nn.Linear", "type": "class", "code": "torch.nn.Linear(in_features:int,out_features:int,bias:bool=True)", "example": "NA", "summary": "Applies a linear transformation to the incoming data: y=xAT+by = xA^T + by=xAT+b    Parameters  in_features \u2013 size of each input sample out_features \u2013 size of each output sample bias \u2013 If set to False, the layer will not learn an additive bias", "returns": [], "shape": "", "code-info": {"name": "torch.nn.Linear", "parameters": [{"name": "in_features", "type": "int", "is_optional": false, "description": "size of each input sample"}, {"name": "out_features", "type": "int", "is_optional": false, "description": "size of each output sample"}, {"name": "bias", "type": "bool", "default_value": "True", "is_optional": false, "description": "If set to False, the layer will not learn an additive bias.Default: True"}]}},
{"id": "torch.nn.Identity", "type": "class", "code": "torch.nn.Identity(*args,**kwargs)", "example": "NA", "summary": "A placeholder identity operator that is argument-insensitive", "returns": [], "shape": "", "code-info": {"name": "torch.nn.Identity", "parameters": []}},
{"id": "torch.nn.Transformer.generate_square_subsequent_mask", "type": "method", "code": "torch.nn.Transformer.generate_square_subsequent_mask(sz:int)", "example": "NA", "summary": "Generate a square mask for the sequence", "returns": [], "shape": "", "code-info": {"name": "torch.nn.Transformer.generate_square_subsequent_mask", "parameters": [{"name": "sz", "type": "int", "is_optional": false, "description": ""}]}},
{"id": "torch.nn.Transformer", "type": "class", "code": "torch.nn.Transformer(d_model:int=512,nhead:int=8,num_encoder_layers:int=6,num_decoder_layers:int=6,dim_feedforward:int=2048,dropout:float=0.1,activation:str='relu',custom_encoder:Optional[Any]=None,custom_decoder:Optional[Any]=None)", "example": "NA", "summary": "A transformer model", "returns": [], "shape": "&gt;&gt;&gt; transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)\n&gt;&gt;&gt; src = torch.rand((10, 32, 512))\n&gt;&gt;&gt; tgt = torch.rand((20, 32, 512))\n&gt;&gt;&gt; out = transformer_model(src, tgt)\n\n\n", "code-info": {"name": "torch.nn.Transformer", "parameters": [{"name": "d_model", "type": "int", "default_value": "512", "is_optional": false, "description": "the number of expected features in the encoder/decoder inputs (default=512)."}, {"name": "nhead", "type": "int", "default_value": "8", "is_optional": false, "description": "the number of heads in the multiheadattention models (default=8)."}, {"name": "num_encoder_layers", "type": "int", "default_value": "6", "is_optional": false, "description": "the number of sub-encoder-layers in the encoder (default=6)."}, {"name": "num_decoder_layers", "type": "int", "default_value": "6", "is_optional": false, "description": "the number of sub-decoder-layers in the decoder (default=6)."}, {"name": "dim_feedforward", "type": "int", "default_value": "2048", "is_optional": false, "description": "the dimension of the feedforward network model (default=2048)."}, {"name": "dropout", "type": "float", "default_value": "0.1", "is_optional": false, "description": "the dropout value (default=0.1)."}, {"name": "activation", "type": "str", "default_value": "'relu'", "is_optional": false, "description": "the activation function of encoder/decoder intermediate layer, relu or gelu (default=relu)."}, {"name": "custom_encoder", "type": "Optional[Any]", "default_value": "None", "is_optional": false, "description": "custom encoder (default=None)."}, {"name": "custom_decoder", "type": "Optional[Any]", "default_value": "None", "is_optional": false, "description": "custom decoder (default=None)."}]}},
{"id": "torch.nn.RNNCell", "type": "class", "code": "torch.nn.RNNCell(input_size:int,hidden_size:int,bias:bool=True,nonlinearity:str='tanh')", "example": "NA", "summary": "An Elman RNN cell with tanh or ReLU non-linearity", "returns": [], "shape": "", "code-info": {"name": "torch.nn.RNNCell", "parameters": [{"name": "input_size", "type": "int", "is_optional": false, "description": "The number of expected features in the input x"}, {"name": "hidden_size", "type": "int", "is_optional": false, "description": "The number of features in the hidden state h"}, {"name": "bias", "type": "bool", "default_value": "True", "is_optional": false, "description": "If False, then the layer does not use bias weights b_ih and b_hh.Default: True"}, {"name": "nonlinearity", "type": "str", "default_value": "'tanh'", "is_optional": false, "description": "The non-linearity to use. Can be either 'tanh' or 'relu'. Default: 'tanh'"}]}},
{"id": "torch.nn.GRU", "type": "class", "code": "torch.nn.GRU(*args,**kwargs)", "example": "NA", "summary": "Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence", "returns": [], "shape": "\ninput of shape (seq_len, batch, input_size): tensor containing the features\nof the input sequence. The input can also be a packed variable length\nsequence. See torch.nn.utils.rnn.pack_padded_sequence()\nfor details.\nh_0 of shape (num_layers * num_directions, batch, hidden_size): tensor\ncontaining the initial hidden state for each element in the batch.\nDefaults to zero if not provided. If the RNN is bidirectional,\nnum_directions should be 2, else it should be 1.\n\n", "code-info": {"name": "torch.nn.GRU", "parameters": []}},
{"id": "torch.nn.LSTM", "type": "class", "code": "torch.nn.LSTM(*args,**kwargs)", "example": "NA", "summary": "Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence", "returns": [], "shape": "", "code-info": {"name": "torch.nn.LSTM", "parameters": []}},
{"id": "torch.nn.RNN", "type": "class", "code": "torch.nn.RNN(*args,**kwargs)", "example": "NA", "summary": "Applies a multi-layer Elman RNN with tanh\u2061\\tanhtanh   or ReLU\\text{ReLU}ReLU   non-linearity to an input sequence", "returns": [], "shape": "\ninput of shape (seq_len, batch, input_size): tensor containing the features\nof the input sequence. The input can also be a packed variable length\nsequence. See torch.nn.utils.rnn.pack_padded_sequence()\nor torch.nn.utils.rnn.pack_sequence()\nfor details.\nh_0 of shape (num_layers * num_directions, batch, hidden_size): tensor\ncontaining the initial hidden state for each element in the batch.\nDefaults to zero if not provided. If the RNN is bidirectional,\nnum_directions should be 2, else it should be 1.\n\n", "code-info": {"name": "torch.nn.RNN", "parameters": []}},
{"id": "torch.nn.RNNBase.flatten_parameters", "type": "method", "code": "torch.nn.RNNBase.flatten_parameters()", "example": "NA", "summary": "Resets parameter data pointer so that they can use faster code paths", "returns": [], "shape": "", "code-info": {"name": "torch.nn.RNNBase.flatten_parameters", "parameters": []}},
{"id": "torch.nn.RNNBase", "type": "class", "code": "torch.nn.RNNBase(mode:str,input_size:int,hidden_size:int,num_layers:int=1,bias:bool=True,batch_first:bool=False,dropout:float=0.0,bidirectional:bool=False)", "example": "NA", "summary": "  flatten_parameters() \u2192 None  Resets parameter data pointer so that they can use faster code paths", "returns": [], "shape": "", "code-info": {"name": "torch.nn.RNNBase", "parameters": [{"name": "mode", "type": "str", "is_optional": false, "description": ""}, {"name": "input_size", "type": "int", "is_optional": false, "description": ""}, {"name": "hidden_size", "type": "int", "is_optional": false, "description": ""}, {"name": "num_layers", "type": "int", "default_value": "1", "is_optional": false, "description": ""}, {"name": "bias", "type": "bool", "default_value": "True", "is_optional": false, "description": ""}, {"name": "batch_first", "type": "bool", "default_value": "False", "is_optional": false, "description": ""}, {"name": "dropout", "type": "float", "default_value": "0.0", "is_optional": false, "description": ""}, {"name": "bidirectional", "type": "bool", "default_value": "False", "is_optional": false, "description": ""}]}},
{"id": "torch.nn.LocalResponseNorm", "type": "class", "code": "torch.nn.LocalResponseNorm(size:int,alpha:float=0.0001,beta:float=0.75,k:float=1.0)", "example": "NA", "summary": "Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension", "returns": [], "shape": "", "code-info": {"name": "torch.nn.LocalResponseNorm", "parameters": [{"name": "size", "type": "int", "is_optional": false, "description": "amount of neighbouring channels used for normalization"}, {"name": "alpha", "type": "float", "default_value": "00001", "is_optional": false, "description": "multiplicative factor. Default: 0.0001"}, {"name": "beta", "type": "float", "default_value": "075", "is_optional": false, "description": "exponent. Default: 0.75"}, {"name": "k", "type": "float", "default_value": "1", "is_optional": false, "description": "additive factor. Default: 1"}]}},
{"id": "torch.nn.LayerNorm", "type": "class", "code": "torch.nn.LayerNorm(normalized_shape:Union[int,List[int],torch.Size],eps:float=1e-05,elementwise_affine:bool=True)", "example": "NA", "summary": "Applies Layer Normalization over a mini-batch of inputs as described in the paper Layer Normalization  y=x\u2212E[x]Var[x]+\u03f5\u2217\u03b3+\u03b2y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta  y=Var[x]+\u03f5\u200bx\u2212E[x]\u200b\u2217\u03b3+\u03b2  The mean and standard-deviation are calculated separately over the last certain number dimensions which have to be of the shape specified by normalized_shape", "returns": [], "shape": "", "code-info": {"name": "torch.nn.LayerNorm", "parameters": [{"name": "normalized_shape", "type": "Union[int,List[int],torch.Size]", "is_optional": false, "description": "input shape from an expected inputof size[\u2217\u00d7normalized_shape[0]\u00d7normalized_shape[1]\u00d7\u2026\u00d7normalized_shape[\u22121]][* \\times \\text{normalized\\_shape}[0] \\times \\text{normalized\\_shape}[1]    \\times \\ldots \\times \\text{normalized\\_shape}[-1]][\u2217\u00d7normalized_shape[0]\u00d7normalized_shape[1]\u00d7\u2026\u00d7normalized_shape[\u22121]]If a single integer is used, it is treated as a singleton list, and this module willnormalize over the last dimension which is expected to be of that specific size."}, {"name": "eps", "type": "float", "default_value": "1e-5", "is_optional": false, "description": "a value added to the denominator for numerical stability. Default: 1e-5"}, {"name": "elementwise_affine", "type": "bool", "default_value": "True", "is_optional": false, "description": "a boolean value that when set to True, this modulehas learnable per-element affine parameters initialized to ones (for weights)and zeros (for biases). Default: True."}]}},
{"id": "torch.nn.InstanceNorm3d", "type": "class", "code": "torch.nn.InstanceNorm3d(num_features:int,eps:float=1e-05,momentum:float=0.1,affine:bool=False,track_running_stats:bool=False)", "example": "NA", "summary": "Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization", "returns": [], "shape": "", "code-info": {"name": "torch.nn.InstanceNorm3d", "parameters": [{"name": "num_features", "type": "int", "is_optional": false, "description": "CCC from an expected input of size(N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W)"}, {"name": "eps", "type": "float", "default_value": "1e-5", "is_optional": false, "description": "a value added to the denominator for numerical stability. Default: 1e-5"}, {"name": "momentum", "type": "float", "default_value": "01", "is_optional": false, "description": "the value used for the running_mean and running_var computation. Default: 0.1"}, {"name": "affine", "type": "bool", "default_value": "False", "is_optional": false, "description": "a boolean value that when set to True, this module haslearnable affine parameters, initialized the same way as done for batch normalization.Default: False."}, {"name": "track_running_stats", "type": "bool", "default_value": "False", "is_optional": false, "description": "a boolean value that when set to True, thismodule tracks the running mean and variance, and when set to False,this module does not track such statistics and always uses batchstatistics in both training and eval modes. Default: False"}]}},
{"id": "torch.nn.InstanceNorm1d", "type": "class", "code": "torch.nn.InstanceNorm1d(num_features:int,eps:float=1e-05,momentum:float=0.1,affine:bool=False,track_running_stats:bool=False)", "example": "NA", "summary": "Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization", "returns": [], "shape": "", "code-info": {"name": "torch.nn.InstanceNorm1d", "parameters": [{"name": "num_features", "type": "int", "is_optional": false, "description": "CCC from an expected input of size(N,C,L)(N, C, L)(N,C,L) or LLL from input of size (N,L)(N, L)(N,L)"}, {"name": "eps", "type": "float", "default_value": "1e-5", "is_optional": false, "description": "a value added to the denominator for numerical stability. Default: 1e-5"}, {"name": "momentum", "type": "float", "default_value": "01", "is_optional": false, "description": "the value used for the running_mean and running_var computation. Default: 0.1"}, {"name": "affine", "type": "bool", "default_value": "False", "is_optional": false, "description": "a boolean value that when set to True, this module haslearnable affine parameters, initialized the same way as done for batch normalization.Default: False."}, {"name": "track_running_stats", "type": "bool", "default_value": "False", "is_optional": false, "description": "a boolean value that when set to True, thismodule tracks the running mean and variance, and when set to False,this module does not track such statistics and always uses batchstatistics in both training and eval modes. Default: False"}]}},
{"id": "torch.nn.InstanceNorm2d", "type": "class", "code": "torch.nn.InstanceNorm2d(num_features:int,eps:float=1e-05,momentum:float=0.1,affine:bool=False,track_running_stats:bool=False)", "example": "NA", "summary": "Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization", "returns": [], "shape": "", "code-info": {"name": "torch.nn.InstanceNorm2d", "parameters": [{"name": "num_features", "type": "int", "is_optional": false, "description": "CCC from an expected input of size(N,C,H,W)(N, C, H, W)(N,C,H,W)"}, {"name": "eps", "type": "float", "default_value": "1e-5", "is_optional": false, "description": "a value added to the denominator for numerical stability. Default: 1e-5"}, {"name": "momentum", "type": "float", "default_value": "01", "is_optional": false, "description": "the value used for the running_mean and running_var computation. Default: 0.1"}, {"name": "affine", "type": "bool", "default_value": "False", "is_optional": false, "description": "a boolean value that when set to True, this module haslearnable affine parameters, initialized the same way as done for batch normalization.Default: False."}, {"name": "track_running_stats", "type": "bool", "default_value": "False", "is_optional": false, "description": "a boolean value that when set to True, thismodule tracks the running mean and variance, and when set to False,this module does not track such statistics and always uses batchstatistics in both training and eval modes. Default: False"}]}},
{"id": "torch.nn.GroupNorm", "type": "class", "code": "torch.nn.GroupNorm(num_groups:int,num_channels:int,eps:float=1e-05,affine:bool=True)", "example": "NA", "summary": "Applies Group Normalization over a mini-batch of inputs as described in the paper Group Normalization  y=x\u2212E[x]Var[x]+\u03f5\u2217\u03b3+\u03b2y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta  y=Var[x]+\u03f5\u200bx\u2212E[x]\u200b\u2217\u03b3+\u03b2  The input channels are separated into num_groups groups, each containing num_channels / num_groups channels", "returns": [], "shape": "", "code-info": {"name": "torch.nn.GroupNorm", "parameters": [{"name": "num_groups", "type": "int", "is_optional": false, "description": "number of groups to separate the channels into"}, {"name": "num_channels", "type": "int", "is_optional": false, "description": "number of channels expected in input"}, {"name": "eps", "type": "float", "default_value": "1e-5", "is_optional": false, "description": "a value added to the denominator for numerical stability. Default: 1e-5"}, {"name": "affine", "type": "bool", "default_value": "True", "is_optional": false, "description": "a boolean value that when set to True, this modulehas learnable per-channel affine parameters initialized to ones (for weights)and zeros (for biases). Default: True."}]}},
{"id": "torch.nn.SyncBatchNorm.convert_sync_batchnorm", "type": "method", "code": "torch.nn.SyncBatchNorm.convert_sync_batchnorm(module,process_group=None)", "example": "  # Network with nn.BatchNorm layer  module = torch.nn.Sequential(             torch.nn.Linear(20, 100),             torch.nn.BatchNorm1d(100),           ).cuda()  # creating process group (optional)  # process_ids is a list of int identifying rank ids.  process_group = torch.distributed.new_group(process_ids)  sync_bn_module = torch.nn.SyncBatchNorm.convert_sync_batchnorm(module, process_group)   ", "summary": "Helper function to convert all BatchNorm*D layers in the model to torch.nn.SyncBatchNorm layers", "returns": "The original module with the converted torch.nn.SyncBatchNormlayers. If the original module is a BatchNorm*D layer,a new torch.nn.SyncBatchNorm layer object will be returnedinstead.", "shape": "", "code-info": {"name": "torch.nn.SyncBatchNorm.convert_sync_batchnorm", "parameters": []}},
{"id": "torch.nn.BatchNorm3d", "type": "class", "code": "torch.nn.BatchNorm3d(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)", "example": "NA", "summary": "Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift ", "returns": [], "shape": "", "code-info": {"name": "torch.nn.BatchNorm3d", "parameters": []}},
{"id": "torch.nn.BatchNorm1d", "type": "class", "code": "torch.nn.BatchNorm1d(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)", "example": "NA", "summary": "Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift ", "returns": [], "shape": "", "code-info": {"name": "torch.nn.BatchNorm1d", "parameters": []}},
{"id": "torch.nn.AdaptiveLogSoftmaxWithLoss.log_prob", "type": "method", "code": "torch.nn.AdaptiveLogSoftmaxWithLoss.log_prob(input:torch.Tensor)", "example": "NA", "summary": "Computes log probabilities for all n_classes\\texttt{n\\_classes}n_classes    Parameters input (Tensor) \u2013 a minibatch of examples  Returns log-probabilities of for each class ccc   in range 0&lt;=c&lt;=n_classes0 &lt;= c &lt;= \\texttt{n\\_classes}0&lt;=c&lt;=n_classes  , where n_classes\\texttt{n\\_classes}n_classes   is a parameter passed to AdaptiveLogSoftmaxWithLoss constructor", "returns": "output is a Tensor of size N containing computed targetlog probabilities for each exampleloss is a Scalar representing the computed negativelog likelihood loss", "shape": "", "code-info": {"name": "torch.nn.AdaptiveLogSoftmaxWithLoss.log_prob", "parameters": [{"name": "input", "type": "torch.Tensor", "is_optional": false, "description": "a minibatch of examples"}]}},
{"id": "torch.nn.AdaptiveLogSoftmaxWithLoss.predict", "type": "method", "code": "torch.nn.AdaptiveLogSoftmaxWithLoss.predict(input:torch.Tensor)", "example": "NA", "summary": "This is equivalent to self.log_pob(input).argmax(dim=1), but is more efficient in some cases", "returns": "output is a Tensor of size N containing computed targetlog probabilities for each exampleloss is a Scalar representing the computed negativelog likelihood loss", "shape": "", "code-info": {"name": "torch.nn.AdaptiveLogSoftmaxWithLoss.predict", "parameters": [{"name": "input", "type": "torch.Tensor", "is_optional": false, "description": "a minibatch of examples"}]}},
{"id": "torch.nn.AdaptiveLogSoftmaxWithLoss", "type": "class", "code": "torch.nn.AdaptiveLogSoftmaxWithLoss(in_features:int,n_classes:int,cutoffs:Sequence[int],div_value:float=4.0,head_bias:bool=False)", "example": "NA", "summary": "Efficient softmax approximation as described in Efficient softmax approximation for GPUs by Edouard Grave, Armand Joulin, Moustapha Ciss\u00e9, David Grangier, and Herv\u00e9 J\u00e9gou", "returns": "output is a Tensor of size N containing computed targetlog probabilities for each exampleloss is a Scalar representing the computed negativelog likelihood loss", "shape": "", "code-info": {"name": "torch.nn.AdaptiveLogSoftmaxWithLoss", "parameters": [{"name": "in_features", "type": "int", "is_optional": false, "description": "Number of features in the input tensor"}, {"name": "n_classes", "type": "int", "is_optional": false, "description": "Number of classes in the dataset"}, {"name": "cutoffs", "type": "Sequence[int]", "is_optional": false, "description": "Cutoffs used to assign targets to their buckets"}, {"name": "div_value", "type": "float", "default_value": "40", "is_optional": true, "description": "value used as an exponent to compute sizesof the clusters. Default: 4.0"}, {"name": "head_bias", "type": "bool", "default_value": "False", "is_optional": true, "description": "If True, adds a bias term to the \u2018head\u2019 of theadaptive softmax. Default: False"}]}},
{"id": "torch.nn.BatchNorm2d", "type": "class", "code": "torch.nn.BatchNorm2d(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)", "example": "NA", "summary": "Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift ", "returns": [], "shape": "", "code-info": {"name": "torch.nn.BatchNorm2d", "parameters": []}},
{"id": "torch.nn.SyncBatchNorm", "type": "class", "code": "torch.nn.SyncBatchNorm(num_features:int,eps:float=1e-05,momentum:float=0.1,affine:bool=True,track_running_stats:bool=True,process_group:Optional[Any]=None)", "example": "  # Network with nn.BatchNorm layer  module = torch.nn.Sequential(             torch.nn.Linear(20, 100),             torch.nn.BatchNorm1d(100),           ).cuda()  # creating process group (optional)  # process_ids is a list of int identifying rank ids.  process_group = torch.distributed.new_group(process_ids)  sync_bn_module = torch.nn.SyncBatchNorm.convert_sync_batchnorm(module, process_group)     ", "summary": "Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift ", "returns": "The original module with the converted torch.nn.SyncBatchNormlayers. If the original module is a BatchNorm*D layer,a new torch.nn.SyncBatchNorm layer object will be returnedinstead.", "shape": "", "code-info": {"name": "torch.nn.SyncBatchNorm", "parameters": [{"name": "num_features", "type": "int", "is_optional": false, "description": "CCC from an expected input of size(N,C,+)(N, C, +)(N,C,+)"}, {"name": "eps", "type": "float", "default_value": "1e-5", "is_optional": false, "description": "a value added to the denominator for numerical stability.Default: 1e-5"}, {"name": "momentum", "type": "float", "default_value": "01", "is_optional": false, "description": "the value used for the running_mean and running_varcomputation. Can be set to None for cumulative moving average(i.e. simple average). Default: 0.1"}, {"name": "affine", "type": "bool", "default_value": "True", "is_optional": false, "description": "a boolean value that when set to True, this module haslearnable affine parameters. Default: True"}, {"name": "track_running_stats", "type": "bool", "default_value": "True", "is_optional": false, "description": "a boolean value that when set to True, thismodule tracks the running mean and variance, and when set to False,this module does not track such statistics and uses batch statistics insteadin both training and eval modes if the running mean and variance are None. Default: True"}, {"name": "process_group", "type": "Optional[Any]", "default_value": "None", "is_optional": true, "description": "process group to scope synchronization,default is the whole world"}]}},
{"id": "torch.nn.Softmax2d", "type": "class", "code": "torch.nn.Softmax2d", "example": "NA", "summary": "Applies SoftMax over features to each spatial location", "returns": "a Tensor of the same dimension and shape as the input withvalues in the range [0, 1]", "shape": "", "code-info": {"name": "torch.nn.Softmax2d", "parameters": []}},
{"id": "torch.nn.LogSoftmax", "type": "class", "code": "torch.nn.LogSoftmax(dim:Optional[int]=None)", "example": "NA", "summary": "Applies the log\u2061(Softmax(x))\\log(\\text{Softmax}(x))log(Softmax(x))   function to an n-dimensional input Tensor", "returns": "a Tensor of the same dimension and shape as the input withvalues in the range [-inf, 0)", "shape": "", "code-info": {"name": "torch.nn.LogSoftmax", "parameters": [{"name": "dim", "type": "Optional[int]", "default_value": "None", "is_optional": false, "description": "A dimension along which LogSoftmax will be computed."}]}},
{"id": "torch.nn.Softmin", "type": "class", "code": "torch.nn.Softmin(dim:Optional[int]=None)", "example": "NA", "summary": "Applies the Softmin function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0, 1] and sum to 1", "returns": "a Tensor of the same dimension and shape as the input, withvalues in the range [0, 1]", "shape": "", "code-info": {"name": "torch.nn.Softmin", "parameters": [{"name": "dim", "type": "Optional[int]", "default_value": "None", "is_optional": false, "description": "A dimension along which Softmin will be computed (so every slicealong dim will sum to 1)."}]}},
{"id": "torch.nn.Threshold", "type": "class", "code": "torch.nn.Threshold(threshold:float,value:float,inplace:bool=False)", "example": "NA", "summary": "Thresholds each element of the input Tensor", "returns": [], "shape": "", "code-info": {"name": "torch.nn.Threshold", "parameters": [{"name": "threshold", "type": "float", "is_optional": false, "description": "The value to threshold at"}, {"name": "value", "type": "float", "is_optional": false, "description": "The value to replace with"}, {"name": "inplace", "type": "bool", "default_value": "False", "is_optional": true, "description": "can optionally do the operation in-place. Default: False"}]}},
{"id": "torch.nn.Softmax", "type": "class", "code": "torch.nn.Softmax(dim:Optional[int]=None)", "example": "NA", "summary": "Applies the Softmax function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0,1] and sum to 1", "returns": "a Tensor of the same dimension and shape as the input withvalues in the range [0, 1]", "shape": "", "code-info": {"name": "torch.nn.Softmax", "parameters": [{"name": "dim", "type": "Optional[int]", "default_value": "None", "is_optional": false, "description": "A dimension along which Softmax will be computed (so every slicealong dim will sum to 1)."}]}},
{"id": "torch.nn.Tanhshrink", "type": "class", "code": "torch.nn.Tanhshrink", "example": "NA", "summary": "Applies the element-wise function:  Tanhshrink(x)=x\u2212tanh\u2061(x)\\text{Tanhshrink}(x) = x - \\tanh(x)  Tanhshrink(x)=x\u2212tanh(x)   Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input     Examples: &gt;&gt;&gt; m = nn.Tanhshrink() &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; output = m(input)   ", "returns": [], "shape": "", "code-info": {"name": "torch.nn.Tanhshrink", "parameters": []}},
{"id": "torch.nn.Tanh", "type": "class", "code": "torch.nn.Tanh", "example": "NA", "summary": "Applies the element-wise function:  Tanh(x)=tanh\u2061(x)=exp\u2061(x)\u2212exp\u2061(\u2212x)exp\u2061(x)+exp\u2061(\u2212x)\\text{Tanh}(x) = \\tanh(x) = \\frac{\\exp(x) - \\exp(-x)} {\\exp(x) + \\exp(-x)}  Tanh(x)=tanh(x)=exp(x)+exp(\u2212x)exp(x)\u2212exp(\u2212x)\u200b   Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input     Examples: &gt;&gt;&gt; m = nn.Tanh() &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; output = m(input)   ", "returns": [], "shape": "", "code-info": {"name": "torch.nn.Tanh", "parameters": []}},
{"id": "torch.nn.Softsign", "type": "class", "code": "torch.nn.Softsign", "example": "NA", "summary": "Applies the element-wise function:  SoftSign(x)=x1+\u2223x\u2223\\text{SoftSign}(x) = \\frac{x}{ 1 + |x|}  SoftSign(x)=1+\u2223x\u2223x\u200b   Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input     Examples: &gt;&gt;&gt; m = nn.Softsign() &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; output = m(input)   ", "returns": [], "shape": "", "code-info": {"name": "torch.nn.Softsign", "parameters": []}},
{"id": "torch.nn.Softplus", "type": "class", "code": "torch.nn.Softplus(beta:int=1,threshold:int=20)", "example": "NA", "summary": "Applies the element-wise function:  Softplus(x)=1\u03b2\u2217log\u2061(1+exp\u2061(\u03b2\u2217x))\\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x))  Softplus(x)=\u03b21\u200b\u2217log(1+exp(\u03b2\u2217x))  SoftPlus is a smooth approximation to the ReLU function and can be used to constrain the output of a machine to always be positive", "returns": [], "shape": "", "code-info": {"name": "torch.nn.Softplus", "parameters": [{"name": "beta", "type": "int", "default_value": "1", "is_optional": false, "description": "the \u03b2\\beta\u03b2 value for the Softplus formulation. Default: 1"}, {"name": "threshold", "type": "int", "default_value": "20", "is_optional": false, "description": "values above this revert to a linear function. Default: 20"}]}},
{"id": "torch.nn.Softshrink", "type": "class", "code": "torch.nn.Softshrink(lambd:float=0.5)", "example": "NA", "summary": "Applies the soft shrinkage function elementwise:  SoftShrinkage(x)={x\u2212\u03bb,\u00a0if\u00a0x&gt;\u03bbx+\u03bb,\u00a0if\u00a0x&lt;\u2212\u03bb0,\u00a0otherwise\u00a0\\text{SoftShrinkage}(x) = \\begin{cases} x - \\lambda, &amp; \\text{ if } x &gt; \\lambda \\\\ x + \\lambda, &amp; \\text{ if } x &lt; -\\lambda \\\\ 0, &amp; \\text{ otherwise } \\end{cases}  SoftShrinkage(x)=\u23a9\u23aa\u23aa\u23a8\u23aa\u23aa\u23a7\u200bx\u2212\u03bb,x+\u03bb,0,\u200b\u00a0if\u00a0x&gt;\u03bb\u00a0if\u00a0x&lt;\u2212\u03bb\u00a0otherwise\u00a0\u200b   Parameters lambd \u2013 the \u03bb\\lambda\u03bb   (must be no less than zero) value for the Softshrink formulation", "returns": [], "shape": "", "code-info": {"name": "torch.nn.Softshrink", "parameters": [{"name": "lambd", "type": "float", "default_value": "05", "is_optional": false, "description": "the \u03bb\\lambda\u03bb (must be no less than zero) value for the Softshrink formulation. Default: 0.5"}]}},
{"id": "torch.nn.Sigmoid", "type": "class", "code": "torch.nn.Sigmoid", "example": "NA", "summary": "Applies the element-wise function:  Sigmoid(x)=\u03c3(x)=11+exp\u2061(\u2212x)\\text{Sigmoid}(x) = \\sigma(x) = \\frac{1}{1 + \\exp(-x)}  Sigmoid(x)=\u03c3(x)=1+exp(\u2212x)1\u200b   Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input     Examples: &gt;&gt;&gt; m = nn.Sigmoid() &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; output = m(input)   ", "returns": [], "shape": "", "code-info": {"name": "torch.nn.Sigmoid", "parameters": []}},
{"id": "torch.nn.SiLU", "type": "class", "code": "torch.nn.SiLU(inplace:bool=False)", "example": "NA", "summary": "Applies the silu function, element-wise", "returns": [], "shape": "", "code-info": {"name": "torch.nn.SiLU", "parameters": [{"name": "inplace", "type": "bool", "default_value": "False", "is_optional": false, "description": ""}]}},
{"id": "torch.nn.CELU", "type": "class", "code": "torch.nn.CELU(alpha:float=1.0,inplace:bool=False)", "example": "NA", "summary": "Applies the element-wise function:  CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))  CELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x/\u03b1)\u22121))  More details can be found in the paper Continuously Differentiable Exponential Linear Units ", "returns": [], "shape": "", "code-info": {"name": "torch.nn.CELU", "parameters": [{"name": "alpha", "type": "float", "default_value": "10", "is_optional": false, "description": "the \u03b1\\alpha\u03b1 value for the CELU formulation. Default: 1.0"}, {"name": "inplace", "type": "bool", "default_value": "False", "is_optional": true, "description": "can optionally do the operation in-place. Default: False"}]}},
{"id": "torch.nn.GELU", "type": "class", "code": "torch.nn.GELU", "example": "NA", "summary": "Applies the Gaussian Error Linear Units function:  GELU(x)=x\u2217\u03a6(x)\\text{GELU}(x) = x * \\Phi(x)  GELU(x)=x\u2217\u03a6(x)  where \u03a6(x)\\Phi(x)\u03a6(x)   is the Cumulative Distribution Function for Gaussian Distribution", "returns": [], "shape": "", "code-info": {"name": "torch.nn.GELU", "parameters": []}},
{"id": "torch.nn.SELU", "type": "class", "code": "torch.nn.SELU(inplace:bool=False)", "example": "NA", "summary": "Applied element-wise, as:  SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = \\text{scale} * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))  SELU(x)=scale\u2217(max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)))  with \u03b1=1.6732632423543772848170429916717\\alpha = 1.6732632423543772848170429916717\u03b1=1.6732632423543772848170429916717   and scale=1.0507009873554804934193349852946\\text{scale} = 1.0507009873554804934193349852946scale=1.0507009873554804934193349852946  ", "returns": [], "shape": "", "code-info": {"name": "torch.nn.SELU", "parameters": [{"name": "inplace", "type": "bool", "default_value": "False", "is_optional": true, "description": "can optionally do the operation in-place. Default: False"}]}},
{"id": "torch.nn.RReLU", "type": "class", "code": "torch.nn.RReLU(lower:float=0.125,upper:float=0.3333333333333333,inplace:bool=False)", "example": "NA", "summary": "Applies the randomized leaky rectified liner unit function, element-wise, as described in the paper: Empirical Evaluation of Rectified Activations in Convolutional Network", "returns": [], "shape": "", "code-info": {"name": "torch.nn.RReLU", "parameters": [{"name": "lower", "type": "float", "default_value": "18\\frac{1}{8}81\u200b", "is_optional": false, "description": "lower bound of the uniform distribution. Default: 18\\frac{1}{8}81\u200b"}, {"name": "upper", "type": "float", "default_value": "13\\frac{1}{3}31\u200b", "is_optional": false, "description": "upper bound of the uniform distribution. Default: 13\\frac{1}{3}31\u200b"}, {"name": "inplace", "type": "bool", "default_value": "False", "is_optional": true, "description": "can optionally do the operation in-place. Default: False"}]}},
{"id": "torch.nn.ReLU6", "type": "class", "code": "torch.nn.ReLU6(inplace:bool=False)", "example": "NA", "summary": "Applies the element-wise function:  ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)  ReLU6(x)=min(max(0,x),6)   Parameters inplace \u2013 can optionally do the operation in-place", "returns": [], "shape": "", "code-info": {"name": "torch.nn.ReLU6", "parameters": [{"name": "inplace", "type": "bool", "default_value": "False", "is_optional": true, "description": "can optionally do the operation in-place. Default: False"}]}},
{"id": "torch.nn.LogSigmoid", "type": "class", "code": "torch.nn.LogSigmoid", "example": "NA", "summary": "Applies the element-wise function:  LogSigmoid(x)=log\u2061(11+exp\u2061(\u2212x))\\text{LogSigmoid}(x) = \\log\\left(\\frac{ 1 }{ 1 + \\exp(-x)}\\right)  LogSigmoid(x)=log(1+exp(\u2212x)1\u200b)   Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input     Examples: &gt;&gt;&gt; m = nn.LogSigmoid() &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; output = m(input)   ", "returns": [], "shape": "", "code-info": {"name": "torch.nn.LogSigmoid", "parameters": []}},
{"id": "torch.nn.Hardswish", "type": "class", "code": "torch.nn.Hardswish", "example": "NA", "summary": "Applies the hardswish function, element-wise, as described in the paper: Searching for MobileNetV3", "returns": [], "shape": "", "code-info": {"name": "torch.nn.Hardswish", "parameters": []}},
{"id": "torch.nn.LeakyReLU", "type": "class", "code": "torch.nn.LeakyReLU(negative_slope:float=0.01,inplace:bool=False)", "example": "NA", "summary": "Applies the element-wise function:  LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)  LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)  or  LeakyRELU(x)={x,\u00a0if\u00a0x\u22650negative_slope\u00d7x,\u00a0otherwise\u00a0\\text{LeakyRELU}(x) = \\begin{cases} x, &amp; \\text{ if } x \\geq 0 \\\\ \\text{negative\\_slope} \\times x, &amp; \\text{ otherwise } \\end{cases}  LeakyRELU(x)={x,negative_slope\u00d7x,\u200b\u00a0if\u00a0x\u22650\u00a0otherwise\u00a0\u200b   Parameters  negative_slope \u2013 Controls the angle of the negative slope", "returns": [], "shape": "", "code-info": {"name": "torch.nn.LeakyReLU", "parameters": [{"name": "negative_slope", "type": "float", "default_value": "1e-2", "is_optional": false, "description": "Controls the angle of the negative slope. Default: 1e-2"}, {"name": "inplace", "type": "bool", "default_value": "False", "is_optional": true, "description": "can optionally do the operation in-place. Default: False"}]}},
{"id": "torch.nn.ReLU", "type": "class", "code": "torch.nn.ReLU(inplace:bool=False)", "example": "NA", "summary": "Applies the rectified linear unit function element-wise: ReLU(x)=(x)+=max\u2061(0,x)\\text{ReLU}(x) = (x)^+ = \\max(0, x)ReLU(x)=(x)+=max(0,x)    Parameters inplace \u2013 can optionally do the operation in-place", "returns": [], "shape": "", "code-info": {"name": "torch.nn.ReLU", "parameters": [{"name": "inplace", "type": "bool", "default_value": "False", "is_optional": true, "description": "can optionally do the operation in-place. Default: False"}]}},
{"id": "torch.nn.MultiheadAttention.forward", "type": "method", "code": "torch.nn.MultiheadAttention.forward(query,key,value,key_padding_mask=None,need_weights=True,attn_mask=None)", "example": "NA", "summary": " Parameters  key, value (query,) \u2013 map a query and a set of key-value pairs to an output", "returns": [], "shape": "", "code-info": {"name": "torch.nn.MultiheadAttention.forward", "parameters": []}},
{"id": "torch.nn.Hardtanh", "type": "class", "code": "torch.nn.Hardtanh(min_val:float=-1.0,max_val:float=1.0,inplace:bool=False,min_value:Optional[float]=None,max_value:Optional[float]=None)", "example": "NA", "summary": "Applies the HardTanh function element-wise HardTanh is defined as:  HardTanh(x)={1\u00a0if\u00a0x&gt;1\u22121\u00a0if\u00a0x&lt;\u22121x\u00a0otherwise\u00a0\\text{HardTanh}(x) = \\begin{cases}     1 &amp; \\text{ if } x &gt; 1 \\\\     -1 &amp; \\text{ if } x &lt; -1 \\\\     x &amp; \\text{ otherwise } \\\\ \\end{cases}  HardTanh(x)=\u23a9\u23aa\u23aa\u23a8\u23aa\u23aa\u23a7\u200b1\u22121x\u200b\u00a0if\u00a0x&gt;1\u00a0if\u00a0x&lt;\u22121\u00a0otherwise\u00a0\u200b  The range of the linear region [\u22121,1][-1, 1][\u22121,1]   can be adjusted using min_val and max_val", "returns": [], "shape": "", "code-info": {"name": "torch.nn.Hardtanh", "parameters": [{"name": "min_val", "type": "float", "default_value": "-1", "is_optional": false, "description": "minimum value of the linear region range. Default: -1"}, {"name": "max_val", "type": "float", "default_value": "1", "is_optional": false, "description": "maximum value of the linear region range. Default: 1"}, {"name": "inplace", "type": "bool", "default_value": "False", "is_optional": true, "description": "can optionally do the operation in-place. Default: False"}, {"name": "min_value", "type": "Optional[float]", "default_value": "None", "is_optional": false, "description": ""}, {"name": "max_value", "type": "Optional[float]", "default_value": "None", "is_optional": false, "description": ""}]}},
{"id": "torch.nn.PReLU", "type": "class", "code": "torch.nn.PReLU(num_parameters:int=1,init:float=0.25)", "example": "NA", "summary": "Applies the element-wise function:  PReLU(x)=max\u2061(0,x)+a\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + a * \\min(0,x)  PReLU(x)=max(0,x)+a\u2217min(0,x)  or  PReLU(x)={x,\u00a0if\u00a0x\u22650ax,\u00a0otherwise\u00a0\\text{PReLU}(x) = \\begin{cases} x, &amp; \\text{ if } x \\geq 0 \\\\ ax, &amp; \\text{ otherwise } \\end{cases}  PReLU(x)={x,ax,\u200b\u00a0if\u00a0x\u22650\u00a0otherwise\u00a0\u200b  Here aaa   is a learnable parameter", "returns": [], "shape": "", "code-info": {"name": "torch.nn.PReLU", "parameters": [{"name": "num_parameters", "type": "int", "default_value": "1", "is_optional": false, "description": "number of aaa to learn.Although it takes an int as input, there is only two values are legitimate:1, or the number of channels at input. Default: 1"}, {"name": "init", "type": "float", "default_value": "025", "is_optional": false, "description": "the initial value of aaa. Default: 0.25"}]}},
{"id": "torch.nn.MultiheadAttention", "type": "class", "code": "torch.nn.MultiheadAttention(embed_dim,num_heads,dropout=0.0,bias=True,add_bias_kv=False,add_zero_attn=False,kdim=None,vdim=None)", "example": "NA", "summary": "Allows the model to jointly attend to information from different representation subspaces", "returns": [], "shape": "", "code-info": {"name": "torch.nn.MultiheadAttention", "parameters": []}},
{"id": "torch.nn.ELU", "type": "class", "code": "torch.nn.ELU(alpha:float=1.0,inplace:bool=False)", "example": "NA", "summary": "Applies the element-wise function:  ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))  ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121))   Parameters  alpha \u2013 the \u03b1\\alpha\u03b1   value for the ELU formulation", "returns": [], "shape": "", "code-info": {"name": "torch.nn.ELU", "parameters": [{"name": "alpha", "type": "float", "default_value": "10", "is_optional": false, "description": "the \u03b1\\alpha\u03b1 value for the ELU formulation. Default: 1.0"}, {"name": "inplace", "type": "bool", "default_value": "False", "is_optional": true, "description": "can optionally do the operation in-place. Default: False"}]}},
{"id": "torch.nn.ConstantPad2d", "type": "class", "code": "torch.nn.ConstantPad2d(padding:Union[T,Tuple[T,T,T,T]],value:float)", "example": "NA", "summary": "Pads the input tensor boundaries with a constant value", "returns": [], "shape": "\nInput: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)\n\n\nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)\n\n where\nHout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}Hout\u200b=Hin\u200b+padding_top+padding_bottom\n\n\nWout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout\u200b=Win\u200b+padding_left+padding_right\n\n\n\n\n", "code-info": {"name": "torch.nn.ConstantPad2d", "parameters": [{"name": "padding", "type": "Union[T,Tuple[T,T,T,T]]", "is_optional": false, "description": "the size of the padding. If is int, uses the samepadding in all boundaries. If a 4-tuple, uses (padding_left\\text{padding\\_left}padding_left,padding_right\\text{padding\\_right}padding_right, padding_top\\text{padding\\_top}padding_top, padding_bottom\\text{padding\\_bottom}padding_bottom)"}, {"name": "value", "type": "float", "is_optional": false, "description": ""}]}},
{"id": "torch.nn.Hardshrink", "type": "class", "code": "torch.nn.Hardshrink(lambd:float=0.5)", "example": "NA", "summary": "Applies the hard shrinkage function element-wise:  HardShrink(x)={x,\u00a0if\u00a0x&gt;\u03bbx,\u00a0if\u00a0x&lt;\u2212\u03bb0,\u00a0otherwise\u00a0\\text{HardShrink}(x) = \\begin{cases} x, &amp; \\text{ if } x &gt; \\lambda \\\\ x, &amp; \\text{ if } x &lt; -\\lambda \\\\ 0, &amp; \\text{ otherwise } \\end{cases}  HardShrink(x)=\u23a9\u23aa\u23aa\u23a8\u23aa\u23aa\u23a7\u200bx,x,0,\u200b\u00a0if\u00a0x&gt;\u03bb\u00a0if\u00a0x&lt;\u2212\u03bb\u00a0otherwise\u00a0\u200b   Parameters lambd \u2013 the \u03bb\\lambda\u03bb   value for the Hardshrink formulation", "returns": [], "shape": "", "code-info": {"name": "torch.nn.Hardshrink", "parameters": [{"name": "lambd", "type": "float", "default_value": "05", "is_optional": false, "description": "the \u03bb\\lambda\u03bb value for the Hardshrink formulation. Default: 0.5"}]}},
{"id": "torch.nn.ZeroPad2d", "type": "class", "code": "torch.nn.ZeroPad2d(padding:Union[T,Tuple[T,T,T,T]])", "example": "NA", "summary": "Pads the input tensor boundaries with zero", "returns": [], "shape": "\nInput: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)\n\n\nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)\n\n where\nHout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}Hout\u200b=Hin\u200b+padding_top+padding_bottom\n\n\nWout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout\u200b=Win\u200b+padding_left+padding_right\n\n\n\n\n", "code-info": {"name": "torch.nn.ZeroPad2d", "parameters": [{"name": "padding", "type": "Union[T,Tuple[T,T,T,T]]", "is_optional": false, "description": "the size of the padding. If is int, uses the samepadding in all boundaries. If a 4-tuple, uses (padding_left\\text{padding\\_left}padding_left,padding_right\\text{padding\\_right}padding_right, padding_top\\text{padding\\_top}padding_top, padding_bottom\\text{padding\\_bottom}padding_bottom)"}]}},
{"id": "torch.nn.ConstantPad3d", "type": "class", "code": "torch.nn.ConstantPad3d(padding:Union[T,Tuple[T,T,T,T,T,T]],value:float)", "example": "NA", "summary": "Pads the input tensor boundaries with a constant value", "returns": [], "shape": "\nInput: (N,C,Din,Hin,Win)(N, C, D_{in}, H_{in}, W_{in})(N,C,Din\u200b,Hin\u200b,Win\u200b)\n\n\nOutput: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\u200b,Hout\u200b,Wout\u200b)\n\n where\nDout=Din+padding_front+padding_backD_{out} = D_{in} + \\text{padding\\_front} + \\text{padding\\_back}Dout\u200b=Din\u200b+padding_front+padding_back\n\n\nHout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}Hout\u200b=Hin\u200b+padding_top+padding_bottom\n\n\nWout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout\u200b=Win\u200b+padding_left+padding_right\n\n\n\n\n", "code-info": {"name": "torch.nn.ConstantPad3d", "parameters": [{"name": "padding", "type": "Union[T,Tuple[T,T,T,T,T,T]]", "is_optional": false, "description": "the size of the padding. If is int, uses the samepadding in all boundaries. If a 6-tuple, uses(padding_left\\text{padding\\_left}padding_left, padding_right\\text{padding\\_right}padding_right,padding_top\\text{padding\\_top}padding_top, padding_bottom\\text{padding\\_bottom}padding_bottom,padding_front\\text{padding\\_front}padding_front, padding_back\\text{padding\\_back}padding_back)"}, {"name": "value", "type": "float", "is_optional": false, "description": ""}]}},
{"id": "torch.nn.ConstantPad1d", "type": "class", "code": "torch.nn.ConstantPad1d(padding:Union[T,Tuple[T,T]],value:float)", "example": "NA", "summary": "Pads the input tensor boundaries with a constant value", "returns": [], "shape": "\nInput: (N,C,Win)(N, C, W_{in})(N,C,Win\u200b)\n\n\nOutput: (N,C,Wout)(N, C, W_{out})(N,C,Wout\u200b)\n\n where\nWout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout\u200b=Win\u200b+padding_left+padding_right\n\n\n\n\n", "code-info": {"name": "torch.nn.ConstantPad1d", "parameters": [{"name": "padding", "type": "Union[T,Tuple[T,T]]", "is_optional": false, "description": "the size of the padding. If is int, uses the samepadding in both boundaries. If a 2-tuple, uses(padding_left\\text{padding\\_left}padding_left, padding_right\\text{padding\\_right}padding_right)"}, {"name": "value", "type": "float", "is_optional": false, "description": ""}]}},
{"id": "torch.nn.Hardsigmoid", "type": "class", "code": "torch.nn.Hardsigmoid", "example": "NA", "summary": "Applies the element-wise function:  Hardsigmoid(x)={0if\u00a0x\u2264\u22123,1if\u00a0x\u2265+3,x/6+1/2otherwise\\text{Hardsigmoid}(x) = \\begin{cases}     0 &amp; \\text{if~} x \\le -3, \\\\     1 &amp; \\text{if~} x \\ge +3, \\\\     x / 6 + 1 / 2 &amp; \\text{otherwise} \\end{cases}  Hardsigmoid(x)=\u23a9\u23aa\u23aa\u23a8\u23aa\u23aa\u23a7\u200b01x/6+1/2\u200bif\u00a0x\u2264\u22123,if\u00a0x\u2265+3,otherwise\u200b   Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input    Examples: &gt;&gt;&gt; m = nn.Hardsigmoid() &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; output = m(input)   ", "returns": [], "shape": "", "code-info": {"name": "torch.nn.Hardsigmoid", "parameters": []}},
{"id": "torch.nn.ReplicationPad3d", "type": "class", "code": "torch.nn.ReplicationPad3d(padding:Union[T,Tuple[T,T,T,T,T,T]])", "example": "NA", "summary": "Pads the input tensor using replication of the input boundary", "returns": [], "shape": "\nInput: (N,C,Din,Hin,Win)(N, C, D_{in}, H_{in}, W_{in})(N,C,Din\u200b,Hin\u200b,Win\u200b)\n\n\nOutput: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\u200b,Hout\u200b,Wout\u200b)\n\n where\nDout=Din+padding_front+padding_backD_{out} = D_{in} + \\text{padding\\_front} + \\text{padding\\_back}Dout\u200b=Din\u200b+padding_front+padding_back\n\n\nHout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}Hout\u200b=Hin\u200b+padding_top+padding_bottom\n\n\nWout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout\u200b=Win\u200b+padding_left+padding_right\n\n\n\n\n", "code-info": {"name": "torch.nn.ReplicationPad3d", "parameters": [{"name": "padding", "type": "Union[T,Tuple[T,T,T,T,T,T]]", "is_optional": false, "description": "the size of the padding. If is int, uses the samepadding in all boundaries. If a 6-tuple, uses(padding_left\\text{padding\\_left}padding_left, padding_right\\text{padding\\_right}padding_right,padding_top\\text{padding\\_top}padding_top, padding_bottom\\text{padding\\_bottom}padding_bottom,padding_front\\text{padding\\_front}padding_front, padding_back\\text{padding\\_back}padding_back)"}]}},
{"id": "torch.nn.ReplicationPad2d", "type": "class", "code": "torch.nn.ReplicationPad2d(padding:Union[T,Tuple[T,T,T,T]])", "example": "NA", "summary": "Pads the input tensor using replication of the input boundary", "returns": [], "shape": "\nInput: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)\n\n\nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)\n\n where\nHout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}Hout\u200b=Hin\u200b+padding_top+padding_bottom\n\n\nWout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout\u200b=Win\u200b+padding_left+padding_right\n\n\n\n\n", "code-info": {"name": "torch.nn.ReplicationPad2d", "parameters": [{"name": "padding", "type": "Union[T,Tuple[T,T,T,T]]", "is_optional": false, "description": "the size of the padding. If is int, uses the samepadding in all boundaries. If a 4-tuple, uses (padding_left\\text{padding\\_left}padding_left,padding_right\\text{padding\\_right}padding_right, padding_top\\text{padding\\_top}padding_top, padding_bottom\\text{padding\\_bottom}padding_bottom)"}]}},
{"id": "torch.nn.ReplicationPad1d", "type": "class", "code": "torch.nn.ReplicationPad1d(padding:Union[T,Tuple[T,T]])", "example": "NA", "summary": "Pads the input tensor using replication of the input boundary", "returns": [], "shape": "\nInput: (N,C,Win)(N, C, W_{in})(N,C,Win\u200b)\n\n\nOutput: (N,C,Wout)(N, C, W_{out})(N,C,Wout\u200b)\n\n where\nWout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout\u200b=Win\u200b+padding_left+padding_right\n\n\n\n\n", "code-info": {"name": "torch.nn.ReplicationPad1d", "parameters": [{"name": "padding", "type": "Union[T,Tuple[T,T]]", "is_optional": false, "description": "the size of the padding. If is int, uses the samepadding in all boundaries. If a 2-tuple, uses(padding_left\\text{padding\\_left}padding_left, padding_right\\text{padding\\_right}padding_right)"}]}},
{"id": "torch.nn.ReflectionPad2d", "type": "class", "code": "torch.nn.ReflectionPad2d(padding:Union[T,Tuple[T,T,T,T]])", "example": "NA", "summary": "Pads the input tensor using the reflection of the input boundary", "returns": [], "shape": "\nInput: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)\n\n\nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)\n\n where\nHout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}Hout\u200b=Hin\u200b+padding_top+padding_bottom\n\n\nWout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout\u200b=Win\u200b+padding_left+padding_right\n\n\n\n\n", "code-info": {"name": "torch.nn.ReflectionPad2d", "parameters": [{"name": "padding", "type": "Union[T,Tuple[T,T,T,T]]", "is_optional": false, "description": "the size of the padding. If is int, uses the samepadding in all boundaries. If a 4-tuple, uses (padding_left\\text{padding\\_left}padding_left,padding_right\\text{padding\\_right}padding_right, padding_top\\text{padding\\_top}padding_top, padding_bottom\\text{padding\\_bottom}padding_bottom)"}]}},
{"id": "torch.nn.MaxPool3d", "type": "class", "code": "torch.nn.MaxPool3d(kernel_size:Union[T,Tuple[T,...]],stride:Optional[Union[T,Tuple[T,...]]]=None,padding:Union[T,Tuple[T,...]]=0,dilation:Union[T,Tuple[T,...]]=1,return_indices:bool=False,ceil_mode:bool=False)", "example": "NA", "summary": "Applies a 3D max pooling over an input signal composed of several input planes", "returns": [], "shape": "\nInput: (N,C,Din,Hin,Win)(N, C, D_{in}, H_{in}, W_{in})(N,C,Din\u200b,Hin\u200b,Win\u200b)\n\n\nOutput: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\u200b,Hout\u200b,Wout\u200b)\n\n, where\n\nDout=\u230aDin+2\u00d7padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121stride[0]+1\u230bD_{out} = \\left\\lfloor\\frac{D_{in} + 2 \\times \\text{padding}[0] - \\text{dilation}[0] \\times\n  (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor\n\nDout\u200b=\u230astride[0]Din\u200b+2\u00d7padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121\u200b+1\u230b\n\n\nHout=\u230aHin+2\u00d7padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121stride[1]+1\u230bH_{out} = \\left\\lfloor\\frac{H_{in} + 2 \\times \\text{padding}[1] - \\text{dilation}[1] \\times\n  (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor\n\nHout\u200b=\u230astride[1]Hin\u200b+2\u00d7padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121\u200b+1\u230b\n\n\nWout=\u230aWin+2\u00d7padding[2]\u2212dilation[2]\u00d7(kernel_size[2]\u22121)\u22121stride[2]+1\u230bW_{out} = \\left\\lfloor\\frac{W_{in} + 2 \\times \\text{padding}[2] - \\text{dilation}[2] \\times\n  (\\text{kernel\\_size}[2] - 1) - 1}{\\text{stride}[2]} + 1\\right\\rfloor\n\nWout\u200b=\u230astride[2]Win\u200b+2\u00d7padding[2]\u2212dilation[2]\u00d7(kernel_size[2]\u22121)\u22121\u200b+1\u230b\n\n\n\n", "code-info": {"name": "torch.nn.MaxPool3d", "parameters": [{"name": "kernel_size", "type": "Union[T,Tuple[T,...]]", "is_optional": false, "description": "the size of the window to take a max over"}, {"name": "stride", "type": "Optional[Union[T,Tuple[T,...]]]", "default_value": "None", "is_optional": false, "description": "the stride of the window. Default value is kernel_size"}, {"name": "padding", "type": "Union[T,Tuple[T,...]]", "default_value": "0", "is_optional": false, "description": "implicit zero padding to be added on all three sides"}, {"name": "dilation", "type": "Union[T,Tuple[T,...]]", "default_value": "1", "is_optional": false, "description": "a parameter that controls the stride of elements in the window"}, {"name": "return_indices", "type": "bool", "default_value": "False", "is_optional": false, "description": "if True, will return the max indices along with the outputs.Useful for torch.nn.MaxUnpool3d later"}, {"name": "ceil_mode", "type": "bool", "default_value": "False", "is_optional": false, "description": "when True, will use ceil instead of floor to compute the output shape"}]}},
{"id": "torch.nn.ReflectionPad1d", "type": "class", "code": "torch.nn.ReflectionPad1d(padding:Union[T,Tuple[T,T]])", "example": "NA", "summary": "Pads the input tensor using the reflection of the input boundary", "returns": [], "shape": "\nInput: (N,C,Win)(N, C, W_{in})(N,C,Win\u200b)\n\n\nOutput: (N,C,Wout)(N, C, W_{out})(N,C,Wout\u200b)\n\n where\nWout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout\u200b=Win\u200b+padding_left+padding_right\n\n\n\n\n", "code-info": {"name": "torch.nn.ReflectionPad1d", "parameters": [{"name": "padding", "type": "Union[T,Tuple[T,T]]", "is_optional": false, "description": "the size of the padding. If is int, uses the samepadding in all boundaries. If a 2-tuple, uses(padding_left\\text{padding\\_left}padding_left, padding_right\\text{padding\\_right}padding_right)"}]}},
{"id": "torch.nn.MaxPool2d", "type": "class", "code": "torch.nn.MaxPool2d(kernel_size:Union[T,Tuple[T,...]],stride:Optional[Union[T,Tuple[T,...]]]=None,padding:Union[T,Tuple[T,...]]=0,dilation:Union[T,Tuple[T,...]]=1,return_indices:bool=False,ceil_mode:bool=False)", "example": "NA", "summary": "Applies a 2D max pooling over an input signal composed of several input planes", "returns": [], "shape": "\nInput: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)\n\n\nOutput: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)\n\n, where\n\nHout=\u230aHin+2\u2217padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121stride[0]+1\u230bH_{out} = \\left\\lfloor\\frac{H_{in} + 2 * \\text{padding[0]} - \\text{dilation[0]}\n      \\times (\\text{kernel\\_size[0]} - 1) - 1}{\\text{stride[0]}} + 1\\right\\rfloor\n\nHout\u200b=\u230astride[0]Hin\u200b+2\u2217padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121\u200b+1\u230b\n\n\nWout=\u230aWin+2\u2217padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121stride[1]+1\u230bW_{out} = \\left\\lfloor\\frac{W_{in} + 2 * \\text{padding[1]} - \\text{dilation[1]}\n      \\times (\\text{kernel\\_size[1]} - 1) - 1}{\\text{stride[1]}} + 1\\right\\rfloor\n\nWout\u200b=\u230astride[1]Win\u200b+2\u2217padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121\u200b+1\u230b\n\n\n\n", "code-info": {"name": "torch.nn.MaxPool2d", "parameters": [{"name": "kernel_size", "type": "Union[T,Tuple[T,...]]", "is_optional": false, "description": "the size of the window to take a max over"}, {"name": "stride", "type": "Optional[Union[T,Tuple[T,...]]]", "default_value": "None", "is_optional": false, "description": "the stride of the window. Default value is kernel_size"}, {"name": "padding", "type": "Union[T,Tuple[T,...]]", "default_value": "0", "is_optional": false, "description": "implicit zero padding to be added on both sides"}, {"name": "dilation", "type": "Union[T,Tuple[T,...]]", "default_value": "1", "is_optional": false, "description": "a parameter that controls the stride of elements in the window"}, {"name": "return_indices", "type": "bool", "default_value": "False", "is_optional": false, "description": "if True, will return the max indices along with the outputs.Useful for torch.nn.MaxUnpool2d later"}, {"name": "ceil_mode", "type": "bool", "default_value": "False", "is_optional": false, "description": "when True, will use ceil instead of floor to compute the output shape"}]}},
{"id": "torch.nn.MaxPool1d", "type": "class", "code": "torch.nn.MaxPool1d(kernel_size:Union[T,Tuple[T,...]],stride:Optional[Union[T,Tuple[T,...]]]=None,padding:Union[T,Tuple[T,...]]=0,dilation:Union[T,Tuple[T,...]]=1,return_indices:bool=False,ceil_mode:bool=False)", "example": "NA", "summary": "Applies a 1D max pooling over an input signal composed of several input planes", "returns": [], "shape": "\nInput: (N,C,Lin)(N, C, L_{in})(N,C,Lin\u200b)\n\n\nOutput: (N,C,Lout)(N, C, L_{out})(N,C,Lout\u200b)\n\n, where\n\nLout=\u230aLin+2\u00d7padding\u2212dilation\u00d7(kernel_size\u22121)\u22121stride+1\u230bL_{out} = \\left\\lfloor \\frac{L_{in} + 2 \\times \\text{padding} - \\text{dilation}\n      \\times (\\text{kernel\\_size} - 1) - 1}{\\text{stride}} + 1\\right\\rfloor\n\nLout\u200b=\u230astrideLin\u200b+2\u00d7padding\u2212dilation\u00d7(kernel_size\u22121)\u22121\u200b+1\u230b\n\n\n\n", "code-info": {"name": "torch.nn.MaxPool1d", "parameters": [{"name": "kernel_size", "type": "Union[T,Tuple[T,...]]", "is_optional": false, "description": "the size of the window to take a max over"}, {"name": "stride", "type": "Optional[Union[T,Tuple[T,...]]]", "default_value": "None", "is_optional": false, "description": "the stride of the window. Default value is kernel_size"}, {"name": "padding", "type": "Union[T,Tuple[T,...]]", "default_value": "0", "is_optional": false, "description": "implicit zero padding to be added on both sides"}, {"name": "dilation", "type": "Union[T,Tuple[T,...]]", "default_value": "1", "is_optional": false, "description": "a parameter that controls the stride of elements in the window"}, {"name": "return_indices", "type": "bool", "default_value": "False", "is_optional": false, "description": "if True, will return the max indices along with the outputs.Useful for torch.nn.MaxUnpool1d later"}, {"name": "ceil_mode", "type": "bool", "default_value": "False", "is_optional": false, "description": "when True, will use ceil instead of floor to compute the output shape"}]}},
{"id": "torch.nn.Fold", "type": "class", "code": "torch.nn.Fold(output_size:Union[T,Tuple[T,...]],kernel_size:Union[T,Tuple[T,...]],dilation:Union[T,Tuple[T,...]]=1,padding:Union[T,Tuple[T,...]]=0,stride:Union[T,Tuple[T,...]]=1)", "example": "NA", "summary": "Combines an array of sliding local blocks into a large containing tensor", "returns": [], "shape": "", "code-info": {"name": "torch.nn.Fold", "parameters": [{"name": "output_size", "type": "Union[T,Tuple[T,...]]", "is_optional": false, "description": "the shape of the spatial dimensions of theoutput (i.e., output.sizes()[2:])"}, {"name": "kernel_size", "type": "Union[T,Tuple[T,...]]", "is_optional": false, "description": "the size of the sliding blocks"}, {"name": "dilation", "type": "Union[T,Tuple[T,...]]", "default_value": "1", "is_optional": true, "description": "a parameter that controls thestride of elements within theneighborhood. Default: 1"}, {"name": "padding", "type": "Union[T,Tuple[T,...]]", "default_value": "0", "is_optional": true, "description": "implicit zero padding to be added onboth sides of input. Default: 0"}, {"name": "stride", "type": "Union[T,Tuple[T,...]]", "default_value": "1", "is_optional": false, "description": "the stride of the sliding blocks in the inputspatial dimensions. Default: 1"}]}},
{"id": "torch.nn.ConvTranspose2d", "type": "class", "code": "torch.nn.ConvTranspose2d(in_channels:int,out_channels:int,kernel_size:Union[T,Tuple[T,T]],stride:Union[T,Tuple[T,T]]=1,padding:Union[T,Tuple[T,T]]=0,output_padding:Union[T,Tuple[T,T]]=0,groups:int=1,bias:bool=True,dilation:int=1,padding_mode:str='zeros')", "example": "NA", "summary": "Applies a 2D transposed convolution operator over an input image composed of several input planes", "returns": [], "shape": "\nInput: (N,Cin,Hin,Win)(N, C_{in}, H_{in}, W_{in})(N,Cin\u200b,Hin\u200b,Win\u200b)\n\n\nOutput: (N,Cout,Hout,Wout)(N, C_{out}, H_{out}, W_{out})(N,Cout\u200b,Hout\u200b,Wout\u200b)\n\n where\n\n\nHout=(Hin\u22121)\u00d7stride[0]\u22122\u00d7padding[0]+dilation[0]\u00d7(kernel_size[0]\u22121)+output_padding[0]+1H_{out} = (H_{in} - 1) \\times \\text{stride}[0] - 2 \\times \\text{padding}[0] + \\text{dilation}[0]\n          \\times (\\text{kernel\\_size}[0] - 1) + \\text{output\\_padding}[0] + 1\n\nHout\u200b=(Hin\u200b\u22121)\u00d7stride[0]\u22122\u00d7padding[0]+dilation[0]\u00d7(kernel_size[0]\u22121)+output_padding[0]+1\n\n\nWout=(Win\u22121)\u00d7stride[1]\u22122\u00d7padding[1]+dilation[1]\u00d7(kernel_size[1]\u22121)+output_padding[1]+1W_{out} = (W_{in} - 1) \\times \\text{stride}[1] - 2 \\times \\text{padding}[1] + \\text{dilation}[1]\n          \\times (\\text{kernel\\_size}[1] - 1) + \\text{output\\_padding}[1] + 1\n\nWout\u200b=(Win\u200b\u22121)\u00d7stride[1]\u22122\u00d7padding[1]+dilation[1]\u00d7(kernel_size[1]\u22121)+output_padding[1]+1\n\n", "code-info": {"name": "torch.nn.ConvTranspose2d", "parameters": [{"name": "in_channels", "type": "int", "is_optional": false, "description": "Number of channels in the input image"}, {"name": "out_channels", "type": "int", "is_optional": false, "description": "Number of channels produced by the convolution"}, {"name": "kernel_size", "type": "Union[T,Tuple[T,T]]", "is_optional": false, "description": "Size of the convolving kernel"}, {"name": "stride", "type": "Union[T,Tuple[T,T]]", "default_value": "1", "is_optional": true, "description": "Stride of the convolution. Default: 1"}, {"name": "padding", "type": "Union[T,Tuple[T,T]]", "default_value": "0", "is_optional": true, "description": "dilation * (kernel_size - 1) - padding zero-paddingwill be added to both sides of each dimension in the input. Default: 0"}, {"name": "output_padding", "type": "Union[T,Tuple[T,T]]", "default_value": "0", "is_optional": true, "description": "Additional size added to one sideof each dimension in the output shape. Default: 0"}, {"name": "groups", "type": "int", "default_value": "1", "is_optional": true, "description": "Number of blocked connections from input channels to output channels. Default: 1"}, {"name": "bias", "type": "bool", "default_value": "True", "is_optional": true, "description": "If True, adds a learnable bias to the output. Default: True"}, {"name": "dilation", "type": "int", "default_value": "1", "is_optional": true, "description": "Spacing between kernel elements. Default: 1"}, {"name": "padding_mode", "type": "str", "default_value": "'zeros'", "is_optional": false, "description": ""}]}},
{"id": "torch.nn.Unfold", "type": "class", "code": "torch.nn.Unfold(kernel_size:Union[T,Tuple[T,...]],dilation:Union[T,Tuple[T,...]]=1,padding:Union[T,Tuple[T,...]]=0,stride:Union[T,Tuple[T,...]]=1)", "example": "NA", "summary": "Extracts sliding local blocks from a batched input tensor", "returns": [], "shape": "", "code-info": {"name": "torch.nn.Unfold", "parameters": [{"name": "kernel_size", "type": "Union[T,Tuple[T,...]]", "is_optional": false, "description": "the size of the sliding blocks"}, {"name": "dilation", "type": "Union[T,Tuple[T,...]]", "default_value": "1", "is_optional": true, "description": "a parameter that controls thestride of elements within theneighborhood. Default: 1"}, {"name": "padding", "type": "Union[T,Tuple[T,...]]", "default_value": "0", "is_optional": true, "description": "implicit zero padding to be added onboth sides of input. Default: 0"}, {"name": "stride", "type": "Union[T,Tuple[T,...]]", "default_value": "1", "is_optional": true, "description": "the stride of the sliding blocks in the inputspatial dimensions. Default: 1"}]}},
{"id": "torch.nn.ConvTranspose1d", "type": "class", "code": "torch.nn.ConvTranspose1d(in_channels:int,out_channels:int,kernel_size:Union[T,Tuple[T]],stride:Union[T,Tuple[T]]=1,padding:Union[T,Tuple[T]]=0,output_padding:Union[T,Tuple[T]]=0,groups:int=1,bias:bool=True,dilation:Union[T,Tuple[T]]=1,padding_mode:str='zeros')", "example": "NA", "summary": "Applies a 1D transposed convolution operator over an input image composed of several input planes", "returns": [], "shape": "\nInput: (N,Cin,Lin)(N, C_{in}, L_{in})(N,Cin\u200b,Lin\u200b)\n\n\nOutput: (N,Cout,Lout)(N, C_{out}, L_{out})(N,Cout\u200b,Lout\u200b)\n\n where\n\nLout=(Lin\u22121)\u00d7stride\u22122\u00d7padding+dilation\u00d7(kernel_size\u22121)+output_padding+1L_{out} = (L_{in} - 1) \\times \\text{stride} - 2 \\times \\text{padding} + \\text{dilation}\n          \\times (\\text{kernel\\_size} - 1) + \\text{output\\_padding} + 1\n\nLout\u200b=(Lin\u200b\u22121)\u00d7stride\u22122\u00d7padding+dilation\u00d7(kernel_size\u22121)+output_padding+1\n\n\n\n", "code-info": {"name": "torch.nn.ConvTranspose1d", "parameters": [{"name": "in_channels", "type": "int", "is_optional": false, "description": "Number of channels in the input image"}, {"name": "out_channels", "type": "int", "is_optional": false, "description": "Number of channels produced by the convolution"}, {"name": "kernel_size", "type": "Union[T,Tuple[T]]", "is_optional": false, "description": "Size of the convolving kernel"}, {"name": "stride", "type": "Union[T,Tuple[T]]", "default_value": "1", "is_optional": true, "description": "Stride of the convolution. Default: 1"}, {"name": "padding", "type": "Union[T,Tuple[T]]", "default_value": "0", "is_optional": true, "description": "dilation * (kernel_size - 1) - padding zero-paddingwill be added to both sides of the input. Default: 0"}, {"name": "output_padding", "type": "Union[T,Tuple[T]]", "default_value": "0", "is_optional": true, "description": "Additional size added to one sideof the output shape. Default: 0"}, {"name": "groups", "type": "int", "default_value": "1", "is_optional": true, "description": "Number of blocked connections from input channels to output channels. Default: 1"}, {"name": "bias", "type": "bool", "default_value": "True", "is_optional": true, "description": "If True, adds a learnable bias to the output. Default: True"}, {"name": "dilation", "type": "Union[T,Tuple[T]]", "default_value": "1", "is_optional": true, "description": "Spacing between kernel elements. Default: 1"}, {"name": "padding_mode", "type": "str", "default_value": "'zeros'", "is_optional": false, "description": ""}]}},
{"id": "torch.nn.Conv1d", "type": "class", "code": "torch.nn.Conv1d(in_channels:int,out_channels:int,kernel_size:Union[T,Tuple[T]],stride:Union[T,Tuple[T]]=1,padding:Union[T,Tuple[T]]=0,dilation:Union[T,Tuple[T]]=1,groups:int=1,bias:bool=True,padding_mode:str='zeros')", "example": "NA", "summary": "Applies a 1D convolution over an input signal composed of several input planes", "returns": [], "shape": "\nInput: (N,Cin,Lin)(N, C_{in}, L_{in})(N,Cin\u200b,Lin\u200b)\n\n\nOutput: (N,Cout,Lout)(N, C_{out}, L_{out})(N,Cout\u200b,Lout\u200b)\n\n where\n\nLout=\u230aLin+2\u00d7padding\u2212dilation\u00d7(kernel_size\u22121)\u22121stride+1\u230bL_{out} = \\left\\lfloor\\frac{L_{in} + 2 \\times \\text{padding} - \\text{dilation}\n          \\times (\\text{kernel\\_size} - 1) - 1}{\\text{stride}} + 1\\right\\rfloor\n\nLout\u200b=\u230astrideLin\u200b+2\u00d7padding\u2212dilation\u00d7(kernel_size\u22121)\u22121\u200b+1\u230b\n\n\n\n", "code-info": {"name": "torch.nn.Conv1d", "parameters": [{"name": "in_channels", "type": "int", "is_optional": false, "description": "Number of channels in the input image"}, {"name": "out_channels", "type": "int", "is_optional": false, "description": "Number of channels produced by the convolution"}, {"name": "kernel_size", "type": "Union[T,Tuple[T]]", "is_optional": false, "description": "Size of the convolving kernel"}, {"name": "stride", "type": "Union[T,Tuple[T]]", "default_value": "1", "is_optional": true, "description": "Stride of the convolution. Default: 1"}, {"name": "padding", "type": "Union[T,Tuple[T]]", "default_value": "0", "is_optional": true, "description": "Zero-padding added to both sides ofthe input. Default: 0"}, {"name": "dilation", "type": "Union[T,Tuple[T]]", "default_value": "1", "is_optional": true, "description": "Spacing between kernelelements. Default: 1"}, {"name": "groups", "type": "int", "default_value": "1", "is_optional": true, "description": "Number of blocked connections from inputchannels to output channels. Default: 1"}, {"name": "bias", "type": "bool", "default_value": "True", "is_optional": true, "description": "If True, adds a learnable bias to theoutput. Default: True"}, {"name": "padding_mode", "type": "str", "default_value": "'zeros'", "is_optional": true, "description": "'zeros', 'reflect','replicate' or 'circular'. Default: 'zeros'"}]}},
{"id": "torch.nn.Conv2d", "type": "class", "code": "torch.nn.Conv2d(in_channels:int,out_channels:int,kernel_size:Union[T,Tuple[T,T]],stride:Union[T,Tuple[T,T]]=1,padding:Union[T,Tuple[T,T]]=0,dilation:Union[T,Tuple[T,T]]=1,groups:int=1,bias:bool=True,padding_mode:str='zeros')", "example": "NA", "summary": "Applies a 2D convolution over an input signal composed of several input planes", "returns": [], "shape": "\nInput: (N,Cin,Hin,Win)(N, C_{in}, H_{in}, W_{in})(N,Cin\u200b,Hin\u200b,Win\u200b)\n\n\nOutput: (N,Cout,Hout,Wout)(N, C_{out}, H_{out}, W_{out})(N,Cout\u200b,Hout\u200b,Wout\u200b)\n\n where\n\nHout=\u230aHin+2\u00d7padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121stride[0]+1\u230bH_{out} = \\left\\lfloor\\frac{H_{in}  + 2 \\times \\text{padding}[0] - \\text{dilation}[0]\n          \\times (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor\n\nHout\u200b=\u230astride[0]Hin\u200b+2\u00d7padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121\u200b+1\u230b\n\n\nWout=\u230aWin+2\u00d7padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121stride[1]+1\u230bW_{out} = \\left\\lfloor\\frac{W_{in}  + 2 \\times \\text{padding}[1] - \\text{dilation}[1]\n          \\times (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor\n\nWout\u200b=\u230astride[1]Win\u200b+2\u00d7padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121\u200b+1\u230b\n\n\n\n", "code-info": {"name": "torch.nn.Conv2d", "parameters": [{"name": "in_channels", "type": "int", "is_optional": false, "description": "Number of channels in the input image"}, {"name": "out_channels", "type": "int", "is_optional": false, "description": "Number of channels produced by the convolution"}, {"name": "kernel_size", "type": "Union[T,Tuple[T,T]]", "is_optional": false, "description": "Size of the convolving kernel"}, {"name": "stride", "type": "Union[T,Tuple[T,T]]", "default_value": "1", "is_optional": true, "description": "Stride of the convolution. Default: 1"}, {"name": "padding", "type": "Union[T,Tuple[T,T]]", "default_value": "0", "is_optional": true, "description": "Zero-padding added to both sides ofthe input. Default: 0"}, {"name": "dilation", "type": "Union[T,Tuple[T,T]]", "default_value": "1", "is_optional": true, "description": "Spacing between kernel elements. Default: 1"}, {"name": "groups", "type": "int", "default_value": "1", "is_optional": true, "description": "Number of blocked connections from inputchannels to output channels. Default: 1"}, {"name": "bias", "type": "bool", "default_value": "True", "is_optional": true, "description": "If True, adds a learnable bias to theoutput. Default: True"}, {"name": "padding_mode", "type": "str", "default_value": "'zeros'", "is_optional": true, "description": "'zeros', 'reflect','replicate' or 'circular'. Default: 'zeros'"}]}},
{"id": "torch.nn.Conv3d", "type": "class", "code": "torch.nn.Conv3d(in_channels:int,out_channels:int,kernel_size:Union[T,Tuple[T,T,T]],stride:Union[T,Tuple[T,T,T]]=1,padding:Union[T,Tuple[T,T,T]]=0,dilation:Union[T,Tuple[T,T,T]]=1,groups:int=1,bias:bool=True,padding_mode:str='zeros')", "example": "NA", "summary": "Applies a 3D convolution over an input signal composed of several input planes", "returns": [], "shape": "\nInput: (N,Cin,Din,Hin,Win)(N, C_{in}, D_{in}, H_{in}, W_{in})(N,Cin\u200b,Din\u200b,Hin\u200b,Win\u200b)\n\n\nOutput: (N,Cout,Dout,Hout,Wout)(N, C_{out}, D_{out}, H_{out}, W_{out})(N,Cout\u200b,Dout\u200b,Hout\u200b,Wout\u200b)\n\n where\n\nDout=\u230aDin+2\u00d7padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121stride[0]+1\u230bD_{out} = \\left\\lfloor\\frac{D_{in} + 2 \\times \\text{padding}[0] - \\text{dilation}[0]\n      \\times (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor\n\nDout\u200b=\u230astride[0]Din\u200b+2\u00d7padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121\u200b+1\u230b\n\n\nHout=\u230aHin+2\u00d7padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121stride[1]+1\u230bH_{out} = \\left\\lfloor\\frac{H_{in} + 2 \\times \\text{padding}[1] - \\text{dilation}[1]\n      \\times (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor\n\nHout\u200b=\u230astride[1]Hin\u200b+2\u00d7padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121\u200b+1\u230b\n\n\nWout=\u230aWin+2\u00d7padding[2]\u2212dilation[2]\u00d7(kernel_size[2]\u22121)\u22121stride[2]+1\u230bW_{out} = \\left\\lfloor\\frac{W_{in} + 2 \\times \\text{padding}[2] - \\text{dilation}[2]\n      \\times (\\text{kernel\\_size}[2] - 1) - 1}{\\text{stride}[2]} + 1\\right\\rfloor\n\nWout\u200b=\u230astride[2]Win\u200b+2\u00d7padding[2]\u2212dilation[2]\u00d7(kernel_size[2]\u22121)\u22121\u200b+1\u230b\n\n\n\n", "code-info": {"name": "torch.nn.Conv3d", "parameters": [{"name": "in_channels", "type": "int", "is_optional": false, "description": "Number of channels in the input image"}, {"name": "out_channels", "type": "int", "is_optional": false, "description": "Number of channels produced by the convolution"}, {"name": "kernel_size", "type": "Union[T,Tuple[T,T,T]]", "is_optional": false, "description": "Size of the convolving kernel"}, {"name": "stride", "type": "Union[T,Tuple[T,T,T]]", "default_value": "1", "is_optional": true, "description": "Stride of the convolution. Default: 1"}, {"name": "padding", "type": "Union[T,Tuple[T,T,T]]", "default_value": "0", "is_optional": true, "description": "Zero-padding added to all three sides of the input. Default: 0"}, {"name": "dilation", "type": "Union[T,Tuple[T,T,T]]", "default_value": "1", "is_optional": true, "description": "Spacing between kernel elements. Default: 1"}, {"name": "groups", "type": "int", "default_value": "1", "is_optional": true, "description": "Number of blocked connections from input channels to output channels. Default: 1"}, {"name": "bias", "type": "bool", "default_value": "True", "is_optional": true, "description": "If True, adds a learnable bias to the output. Default: True"}, {"name": "padding_mode", "type": "str", "default_value": "'zeros'", "is_optional": true, "description": "'zeros', 'reflect', 'replicate' or 'circular'. Default: 'zeros'"}]}},
{"id": "torch.nn.ConvTranspose3d", "type": "class", "code": "torch.nn.ConvTranspose3d(in_channels:int,out_channels:int,kernel_size:Union[T,Tuple[T,T,T]],stride:Union[T,Tuple[T,T,T]]=1,padding:Union[T,Tuple[T,T,T]]=0,output_padding:Union[T,Tuple[T,T,T]]=0,groups:int=1,bias:bool=True,dilation:Union[T,Tuple[T,T,T]]=1,padding_mode:str='zeros')", "example": "NA", "summary": "Applies a 3D transposed convolution operator over an input image composed of several input planes", "returns": [], "shape": "\nInput: (N,Cin,Din,Hin,Win)(N, C_{in}, D_{in}, H_{in}, W_{in})(N,Cin\u200b,Din\u200b,Hin\u200b,Win\u200b)\n\n\nOutput: (N,Cout,Dout,Hout,Wout)(N, C_{out}, D_{out}, H_{out}, W_{out})(N,Cout\u200b,Dout\u200b,Hout\u200b,Wout\u200b)\n\n where\n\n\nDout=(Din\u22121)\u00d7stride[0]\u22122\u00d7padding[0]+dilation[0]\u00d7(kernel_size[0]\u22121)+output_padding[0]+1D_{out} = (D_{in} - 1) \\times \\text{stride}[0] - 2 \\times \\text{padding}[0] + \\text{dilation}[0]\n          \\times (\\text{kernel\\_size}[0] - 1) + \\text{output\\_padding}[0] + 1\n\nDout\u200b=(Din\u200b\u22121)\u00d7stride[0]\u22122\u00d7padding[0]+dilation[0]\u00d7(kernel_size[0]\u22121)+output_padding[0]+1\n\n\nHout=(Hin\u22121)\u00d7stride[1]\u22122\u00d7padding[1]+dilation[1]\u00d7(kernel_size[1]\u22121)+output_padding[1]+1H_{out} = (H_{in} - 1) \\times \\text{stride}[1] - 2 \\times \\text{padding}[1] + \\text{dilation}[1]\n          \\times (\\text{kernel\\_size}[1] - 1) + \\text{output\\_padding}[1] + 1\n\nHout\u200b=(Hin\u200b\u22121)\u00d7stride[1]\u22122\u00d7padding[1]+dilation[1]\u00d7(kernel_size[1]\u22121)+output_padding[1]+1\n\n\nWout=(Win\u22121)\u00d7stride[2]\u22122\u00d7padding[2]+dilation[2]\u00d7(kernel_size[2]\u22121)+output_padding[2]+1W_{out} = (W_{in} - 1) \\times \\text{stride}[2] - 2 \\times \\text{padding}[2] + \\text{dilation}[2]\n          \\times (\\text{kernel\\_size}[2] - 1) + \\text{output\\_padding}[2] + 1\n\nWout\u200b=(Win\u200b\u22121)\u00d7stride[2]\u22122\u00d7padding[2]+dilation[2]\u00d7(kernel_size[2]\u22121)+output_padding[2]+1\n\n", "code-info": {"name": "torch.nn.ConvTranspose3d", "parameters": [{"name": "in_channels", "type": "int", "is_optional": false, "description": "Number of channels in the input image"}, {"name": "out_channels", "type": "int", "is_optional": false, "description": "Number of channels produced by the convolution"}, {"name": "kernel_size", "type": "Union[T,Tuple[T,T,T]]", "is_optional": false, "description": "Size of the convolving kernel"}, {"name": "stride", "type": "Union[T,Tuple[T,T,T]]", "default_value": "1", "is_optional": true, "description": "Stride of the convolution. Default: 1"}, {"name": "padding", "type": "Union[T,Tuple[T,T,T]]", "default_value": "0", "is_optional": true, "description": "dilation * (kernel_size - 1) - padding zero-paddingwill be added to both sides of each dimension in the input. Default: 0"}, {"name": "output_padding", "type": "Union[T,Tuple[T,T,T]]", "default_value": "0", "is_optional": true, "description": "Additional size added to one sideof each dimension in the output shape. Default: 0"}, {"name": "groups", "type": "int", "default_value": "1", "is_optional": true, "description": "Number of blocked connections from input channels to output channels. Default: 1"}, {"name": "bias", "type": "bool", "default_value": "True", "is_optional": true, "description": "If True, adds a learnable bias to the output. Default: True"}, {"name": "dilation", "type": "Union[T,Tuple[T,T,T]]", "default_value": "1", "is_optional": true, "description": "Spacing between kernel elements. Default: 1"}, {"name": "padding_mode", "type": "str", "default_value": "'zeros'", "is_optional": false, "description": ""}]}},
{"id": "torch.nn.ModuleList.append", "type": "method", "code": "torch.nn.ModuleList.append(module:torch.nn.modules.module.Module)", "example": "class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])\n\n    def forward(self, x):\n        # ModuleList can act as an iterable, or be indexed using ints\n        for i, l in enumerate(self.linears):\n            x = self.linears[i // 2](x) + l(x)\n        return x\n\n", "summary": "Appends a given module to the end of the list", "returns": [], "shape": "", "code-info": {"name": "torch.nn.ModuleList.append", "parameters": [{"name": "module", "type": "torch.nn.modules.module.Module", "is_optional": false, "description": "module to append"}]}},
{"id": "torch.nn.ModuleList.extend", "type": "method", "code": "torch.nn.ModuleList.extend(modules:Iterable[torch.nn.modules.module.Module])", "example": "class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])\n\n    def forward(self, x):\n        # ModuleList can act as an iterable, or be indexed using ints\n        for i, l in enumerate(self.linears):\n            x = self.linears[i // 2](x) + l(x)\n        return x\n\n", "summary": "Appends modules from a Python iterable to the end of the list", "returns": [], "shape": "", "code-info": {"name": "torch.nn.ModuleList.extend", "parameters": [{"name": "modules", "type": "Iterable[torch.nn.modules.module.Module]", "is_optional": false, "description": "iterable of modules to append"}]}},
{"id": "torch.nn.ModuleList.insert", "type": "method", "code": "torch.nn.ModuleList.insert(index:int,module:torch.nn.modules.module.Module)", "example": "class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])\n\n    def forward(self, x):\n        # ModuleList can act as an iterable, or be indexed using ints\n        for i, l in enumerate(self.linears):\n            x = self.linears[i // 2](x) + l(x)\n        return x\n\n", "summary": "Insert a given module before a given index in the list", "returns": [], "shape": "", "code-info": {"name": "torch.nn.ModuleList.insert", "parameters": [{"name": "index", "type": "int", "is_optional": false, "description": "index to insert."}, {"name": "module", "type": "torch.nn.modules.module.Module", "is_optional": false, "description": "module to insert"}]}},
{"id": "torch.nn.ModuleList", "type": "class", "code": "torch.nn.ModuleList(modules:Optional[Iterable[torch.nn.modules.module.Module]]=None)", "example": " class MyModule(nn.Module):     def __init__(self):         super(MyModule, self).__init__()         self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])      def forward(self, x):         # ModuleList can act as an iterable, or be indexed using ints         for i, l in enumerate(self.linears):             x = self.linears[i // 2](x) + l(x)         return x     append(module: torch.nn.modules.module.Module) \u2192 T  Appends a given module to the end of the list.  Parameters module (nn.Module) \u2013 module to append       extend(modules: Iterable[torch.nn.modules.module.Module]) \u2192 T  Appends modules from a Python iterable to the end of the list.  Parameters modules (iterable) \u2013 iterable of modules to append       insert(index: int, module: torch.nn.modules.module.Module) \u2192 None  Insert a given module before a given index in the list.  Parameters  index (int) \u2013 index to insert. module (nn.Module) \u2013 module to insert      ", "summary": "Holds submodules in a list", "returns": [], "shape": "", "code-info": {"name": "torch.nn.ModuleList", "parameters": [{"name": "modules", "type": "Optional[Iterable[torch.nn.modules.module.Module]]", "default_value": "None", "is_optional": false, "description": ""}]}},
{"id": "torch.nn.ModuleDict.clear", "type": "method", "code": "torch.nn.ModuleDict.clear()", "example": "class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.choices = nn.ModuleDict({\n                'conv': nn.Conv2d(10, 10, 3),\n                'pool': nn.MaxPool2d(3)\n        })\n        self.activations = nn.ModuleDict([\n                ['lrelu', nn.LeakyReLU()],\n                ['prelu', nn.PReLU()]\n        ])\n\n    def forward(self, x, choice, act):\n        x = self.choices[choice](x)\n        x = self.activations[act](x)\n        return x\n\n", "summary": "Remove all items from the ModuleDict", "returns": [], "shape": "", "code-info": {"name": "torch.nn.ModuleDict.clear", "parameters": []}},
{"id": "torch.nn.ModuleDict.items", "type": "method", "code": "torch.nn.ModuleDict.items()", "example": "class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.choices = nn.ModuleDict({\n                'conv': nn.Conv2d(10, 10, 3),\n                'pool': nn.MaxPool2d(3)\n        })\n        self.activations = nn.ModuleDict([\n                ['lrelu', nn.LeakyReLU()],\n                ['prelu', nn.PReLU()]\n        ])\n\n    def forward(self, x, choice, act):\n        x = self.choices[choice](x)\n        x = self.activations[act](x)\n        return x\n\n", "summary": "Return an iterable of the ModuleDict key/value pairs", "returns": [], "shape": "", "code-info": {"name": "torch.nn.ModuleDict.items", "parameters": []}},
{"id": "torch.nn.ParameterDict.clear", "type": "method", "code": "torch.nn.ParameterDict.clear()", "example": "class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.params = nn.ParameterDict({\n                'left': nn.Parameter(torch.randn(5, 10)),\n                'right': nn.Parameter(torch.randn(5, 10))\n        })\n\n    def forward(self, x, choice):\n        x = self.params[choice].mm(x)\n        return x\n\n", "summary": "Remove all items from the ParameterDict", "returns": [], "shape": "", "code-info": {"name": "torch.nn.ParameterDict.clear", "parameters": []}},
{"id": "torch.nn.ParameterDict.items", "type": "method", "code": "torch.nn.ParameterDict.items()", "example": "class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.params = nn.ParameterDict({\n                'left': nn.Parameter(torch.randn(5, 10)),\n                'right': nn.Parameter(torch.randn(5, 10))\n        })\n\n    def forward(self, x, choice):\n        x = self.params[choice].mm(x)\n        return x\n\n", "summary": "Return an iterable of the ParameterDict key/value pairs", "returns": [], "shape": "", "code-info": {"name": "torch.nn.ParameterDict.items", "parameters": []}},
{"id": "torch.nn.ParameterDict.keys", "type": "method", "code": "torch.nn.ParameterDict.keys()", "example": "class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.params = nn.ParameterDict({\n                'left': nn.Parameter(torch.randn(5, 10)),\n                'right': nn.Parameter(torch.randn(5, 10))\n        })\n\n    def forward(self, x, choice):\n        x = self.params[choice].mm(x)\n        return x\n\n", "summary": "Return an iterable of the ParameterDict keys", "returns": [], "shape": "", "code-info": {"name": "torch.nn.ParameterDict.keys", "parameters": []}},
{"id": "torch.nn.ParameterDict.pop", "type": "method", "code": "torch.nn.ParameterDict.pop(key:str)", "example": "class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.params = nn.ParameterDict({\n                'left': nn.Parameter(torch.randn(5, 10)),\n                'right': nn.Parameter(torch.randn(5, 10))\n        })\n\n    def forward(self, x, choice):\n        x = self.params[choice].mm(x)\n        return x\n\n", "summary": "Remove key from the ParameterDict and return its parameter", "returns": [], "shape": "", "code-info": {"name": "torch.nn.ParameterDict.pop", "parameters": [{"name": "key", "type": "str", "is_optional": false, "description": "key to pop from the ParameterDict"}]}},
{"id": "torch.nn.ParameterDict.update", "type": "method", "code": "torch.nn.ParameterDict.update(parameters:Mapping[str,Parameter])", "example": "class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.params = nn.ParameterDict({\n                'left': nn.Parameter(torch.randn(5, 10)),\n                'right': nn.Parameter(torch.randn(5, 10))\n        })\n\n    def forward(self, x, choice):\n        x = self.params[choice].mm(x)\n        return x\n\n", "summary": "Update the ParameterDict with the key-value pairs from a mapping or an iterable, overwriting existing keys", "returns": [], "shape": "", "code-info": {"name": "torch.nn.ParameterDict.update", "parameters": [{"name": "parameters", "type": "Mapping[str,Parameter]", "is_optional": false, "description": "a mapping (dictionary) from string toParameter, or an iterable ofkey-value pairs of type (string, Parameter)"}]}},
{"id": "torch.nn.Module.add_module", "type": "method", "code": "torch.nn.Module.add_module(name:str,module:Optional[Module])", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "summary": "Adds a child module to the current module", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Module.add_module", "parameters": [{"name": "name", "type": "str", "is_optional": false, "description": "name of the child module. The child module can beaccessed from this module using the given name"}, {"name": "module", "type": "Optional[Module]", "is_optional": false, "description": "child module to be added to the module."}]}},
{"id": "torch.nn.Module.apply", "type": "method", "code": "torch.nn.Module.apply(fn:Callable[Module,None])", "example": "  @torch.no_grad()  def init_weights(m):      print(m)      if type(m) == nn.Linear:          m.weight.fill_(1.0)          print(m.weight)  net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))  net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1.,  1.],         [ 1.,  1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1.,  1.],         [ 1.,  1.]]) Sequential(   (0): Linear(in_features=2, out_features=2, bias=True)   (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential(   (0): Linear(in_features=2, out_features=2, bias=True)   (1): Linear(in_features=2, out_features=2, bias=True) )   ", "summary": "Applies fn recursively to every submodule (as returned by .children()) as well as self", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Module.apply", "parameters": [{"name": "fn", "type": "Callable[Module,None]", "is_optional": false, "description": "function to be applied to each submodule"}]}},
{"id": "torch.nn.ParameterList.append", "type": "method", "code": "torch.nn.ParameterList.append(parameter:Parameter)", "example": "class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.params = nn.ParameterList([nn.Parameter(torch.randn(10, 10)) for i in range(10)])\n\n    def forward(self, x):\n        # ParameterList can act as an iterable, or be indexed using ints\n        for i, p in enumerate(self.params):\n            x = self.params[i // 2].mm(x) + p.mm(x)\n        return x\n\n", "summary": "Appends a given parameter at the end of the list", "returns": [], "shape": "", "code-info": {"name": "torch.nn.ParameterList.append", "parameters": [{"name": "parameter", "type": "Parameter", "is_optional": false, "description": "parameter to append"}]}},
{"id": "torch.nn.Sequential", "type": "class", "code": "torch.nn.Sequential(*args:Any)", "example": "NA", "summary": "A sequential container", "returns": [], "shape": "", "code-info": {"name": "torch.nn.Sequential", "parameters": [{"name": "*args", "type": "Any", "is_optional": false, "description": ""}]}},
{"id": "torch.nn.ModuleDict.keys", "type": "method", "code": "torch.nn.ModuleDict.keys()", "example": "class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.choices = nn.ModuleDict({\n                'conv': nn.Conv2d(10, 10, 3),\n                'pool': nn.MaxPool2d(3)\n        })\n        self.activations = nn.ModuleDict([\n                ['lrelu', nn.LeakyReLU()],\n                ['prelu', nn.PReLU()]\n        ])\n\n    def forward(self, x, choice, act):\n        x = self.choices[choice](x)\n        x = self.activations[act](x)\n        return x\n\n", "summary": "Return an iterable of the ModuleDict keys", "returns": [], "shape": "", "code-info": {"name": "torch.nn.ModuleDict.keys", "parameters": []}},
{"id": "torch.nn.ModuleDict.pop", "type": "method", "code": "torch.nn.ModuleDict.pop(key:str)", "example": "class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.choices = nn.ModuleDict({\n                'conv': nn.Conv2d(10, 10, 3),\n                'pool': nn.MaxPool2d(3)\n        })\n        self.activations = nn.ModuleDict([\n                ['lrelu', nn.LeakyReLU()],\n                ['prelu', nn.PReLU()]\n        ])\n\n    def forward(self, x, choice, act):\n        x = self.choices[choice](x)\n        x = self.activations[act](x)\n        return x\n\n", "summary": "Remove key from the ModuleDict and return its module", "returns": [], "shape": "", "code-info": {"name": "torch.nn.ModuleDict.pop", "parameters": [{"name": "key", "type": "str", "is_optional": false, "description": "key to pop from the ModuleDict"}]}},
{"id": "torch.nn.ModuleDict.update", "type": "method", "code": "torch.nn.ModuleDict.update(modules:Mapping[str,torch.nn.modules.module.Module])", "example": "class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.choices = nn.ModuleDict({\n                'conv': nn.Conv2d(10, 10, 3),\n                'pool': nn.MaxPool2d(3)\n        })\n        self.activations = nn.ModuleDict([\n                ['lrelu', nn.LeakyReLU()],\n                ['prelu', nn.PReLU()]\n        ])\n\n    def forward(self, x, choice, act):\n        x = self.choices[choice](x)\n        x = self.activations[act](x)\n        return x\n\n", "summary": "Update the ModuleDict with the key-value pairs from a mapping or an iterable, overwriting existing keys", "returns": [], "shape": "", "code-info": {"name": "torch.nn.ModuleDict.update", "parameters": [{"name": "modules", "type": "Mapping[str,torch.nn.modules.module.Module]", "is_optional": false, "description": "a mapping (dictionary) from string to Module,or an iterable of key-value pairs of type (string, Module)"}]}},
{"id": "torch.nn.ParameterDict.values", "type": "method", "code": "torch.nn.ParameterDict.values()", "example": "class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.params = nn.ParameterDict({\n                'left': nn.Parameter(torch.randn(5, 10)),\n                'right': nn.Parameter(torch.randn(5, 10))\n        })\n\n    def forward(self, x, choice):\n        x = self.params[choice].mm(x)\n        return x\n\n", "summary": "Return an iterable of the ParameterDict values", "returns": [], "shape": "", "code-info": {"name": "torch.nn.ParameterDict.values", "parameters": []}},
{"id": "torch.nn.ParameterDict", "type": "class", "code": "torch.nn.ParameterDict(parameters:Optional[Mapping[str,Parameter]]=None)", "example": " class MyModule(nn.Module):     def __init__(self):         super(MyModule, self).__init__()         self.params = nn.ParameterDict({                 'left': nn.Parameter(torch.randn(5, 10)),                 'right': nn.Parameter(torch.randn(5, 10))         })      def forward(self, x, choice):         x = self.params[choice].mm(x)         return x     clear() \u2192 None  Remove all items from the ParameterDict.     items() \u2192 Iterable[Tuple[str, Parameter]]  Return an iterable of the ParameterDict key/value pairs.     keys() \u2192 Iterable[str]  Return an iterable of the ParameterDict keys.     pop(key: str) \u2192 Parameter  Remove key from the ParameterDict and return its parameter.  Parameters key (string) \u2013 key to pop from the ParameterDict       update(parameters: Mapping[str, Parameter]) \u2192 None  Update the ParameterDict with the key-value pairs from a mapping or an iterable, overwriting existing keys.  Note If parameters is an OrderedDict, a ParameterDict, or an iterable of key-value pairs, the order of new elements in it is preserved.   Parameters parameters (iterable) \u2013 a mapping (dictionary) from string to Parameter, or an iterable of key-value pairs of type (string, Parameter)       values() \u2192 Iterable[Parameter]  Return an iterable of the ParameterDict values.   ", "summary": "Holds parameters in a dictionary", "returns": [], "shape": "", "code-info": {"name": "torch.nn.ParameterDict", "parameters": [{"name": "parameters", "type": "Optional[Mapping[str,Parameter]]", "default_value": "None", "is_optional": true, "description": "a mapping (dictionary) from string toParameter, or an iterable ofkey-value pairs of type (string, Parameter)"}]}},
{"id": "torch.nn.Module.bfloat16", "type": "method", "code": "torch.nn.Module.bfloat16()", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "summary": "Casts all floating point parameters and buffers to bfloat16 datatype", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Module.bfloat16", "parameters": []}},
{"id": "torch.nn.Module.buffers", "type": "method", "code": "torch.nn.Module.buffers(recurse:bool=True)", "example": "  for buf in model.buffers():      print(type(buf), buf.size()) &lt;class 'torch.Tensor' (20L,) &lt;class 'torch.Tensor' (20L, 1L, 5L, 5L)   ", "summary": "Returns an iterator over module buffers", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Module.buffers", "parameters": [{"name": "recurse", "type": "bool", "default_value": "True", "is_optional": false, "description": "if True, then yields buffers of this moduleand all submodules. Otherwise, yields only buffers thatare direct members of this module."}]}},
{"id": "torch.nn.ParameterList.extend", "type": "method", "code": "torch.nn.ParameterList.extend(parameters:Iterable[Parameter])", "example": "class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.params = nn.ParameterList([nn.Parameter(torch.randn(10, 10)) for i in range(10)])\n\n    def forward(self, x):\n        # ParameterList can act as an iterable, or be indexed using ints\n        for i, p in enumerate(self.params):\n            x = self.params[i // 2].mm(x) + p.mm(x)\n        return x\n\n", "summary": "Appends parameters from a Python iterable to the end of the list", "returns": [], "shape": "", "code-info": {"name": "torch.nn.ParameterList.extend", "parameters": [{"name": "parameters", "type": "Iterable[Parameter]", "is_optional": false, "description": "iterable of parameters to append"}]}},
{"id": "torch.nn.ParameterList", "type": "class", "code": "torch.nn.ParameterList(parameters:Optional[Iterable[Parameter]]=None)", "example": " class MyModule(nn.Module):     def __init__(self):         super(MyModule, self).__init__()         self.params = nn.ParameterList([nn.Parameter(torch.randn(10, 10)) for i in range(10)])      def forward(self, x):         # ParameterList can act as an iterable, or be indexed using ints         for i, p in enumerate(self.params):             x = self.params[i // 2].mm(x) + p.mm(x)         return x     append(parameter: Parameter) \u2192 T  Appends a given parameter at the end of the list.  Parameters parameter (nn.Parameter) \u2013 parameter to append       extend(parameters: Iterable[Parameter]) \u2192 T  Appends parameters from a Python iterable to the end of the list.  Parameters parameters (iterable) \u2013 iterable of parameters to append     ", "summary": "Holds parameters in a list", "returns": [], "shape": "", "code-info": {"name": "torch.nn.ParameterList", "parameters": [{"name": "parameters", "type": "Optional[Iterable[Parameter]]", "default_value": "None", "is_optional": true, "description": "iterable of parameters to append"}]}},
{"id": "torch.nn.ModuleDict.values", "type": "method", "code": "torch.nn.ModuleDict.values()", "example": "class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.choices = nn.ModuleDict({\n                'conv': nn.Conv2d(10, 10, 3),\n                'pool': nn.MaxPool2d(3)\n        })\n        self.activations = nn.ModuleDict([\n                ['lrelu', nn.LeakyReLU()],\n                ['prelu', nn.PReLU()]\n        ])\n\n    def forward(self, x, choice, act):\n        x = self.choices[choice](x)\n        x = self.activations[act](x)\n        return x\n\n", "summary": "Return an iterable of the ModuleDict values", "returns": [], "shape": "", "code-info": {"name": "torch.nn.ModuleDict.values", "parameters": []}},
{"id": "torch.nn.Module.children", "type": "method", "code": "torch.nn.Module.children()", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "summary": "Returns an iterator over immediate children modules", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Module.children", "parameters": []}},
{"id": "torch.nn.Module.cpu", "type": "method", "code": "torch.nn.Module.cpu()", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "summary": "Moves all model parameters and buffers to the CPU", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Module.cpu", "parameters": []}},
{"id": "torch.nn.Module.cuda", "type": "method", "code": "torch.nn.Module.cuda(device:Union[int,torch.device,None]=None)", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "summary": "Moves all model parameters and buffers to the GPU", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Module.cuda", "parameters": [{"name": "device", "type": "Union[int,torch.device,None]", "default_value": "None", "is_optional": true, "description": "if specified, all parameters will becopied to that device"}]}},
{"id": "torch.nn.ModuleDict", "type": "class", "code": "torch.nn.ModuleDict(modules:Optional[Mapping[str,torch.nn.modules.module.Module]]=None)", "example": " class MyModule(nn.Module):     def __init__(self):         super(MyModule, self).__init__()         self.choices = nn.ModuleDict({                 'conv': nn.Conv2d(10, 10, 3),                 'pool': nn.MaxPool2d(3)         })         self.activations = nn.ModuleDict([                 ['lrelu', nn.LeakyReLU()],                 ['prelu', nn.PReLU()]         ])      def forward(self, x, choice, act):         x = self.choices[choice](x)         x = self.activations[act](x)         return x     clear() \u2192 None  Remove all items from the ModuleDict.     items() \u2192 Iterable[Tuple[str, torch.nn.modules.module.Module]]  Return an iterable of the ModuleDict key/value pairs.     keys() \u2192 Iterable[str]  Return an iterable of the ModuleDict keys.     pop(key: str) \u2192 torch.nn.modules.module.Module  Remove key from the ModuleDict and return its module.  Parameters key (string) \u2013 key to pop from the ModuleDict       update(modules: Mapping[str, torch.nn.modules.module.Module]) \u2192 None  Update the ModuleDict with the key-value pairs from a mapping or an iterable, overwriting existing keys.  Note If modules is an OrderedDict, a ModuleDict, or an iterable of key-value pairs, the order of new elements in it is preserved.   Parameters modules (iterable) \u2013 a mapping (dictionary) from string to Module, or an iterable of key-value pairs of type (string, Module)       values() \u2192 Iterable[torch.nn.modules.module.Module]  Return an iterable of the ModuleDict values.   ", "summary": "Holds submodules in a dictionary", "returns": [], "shape": "", "code-info": {"name": "torch.nn.ModuleDict", "parameters": [{"name": "modules", "type": "Optional[Mapping[str,torch.nn.modules.module.Module]]", "default_value": "None", "is_optional": true, "description": "a mapping (dictionary) from string to Module,or an iterable of key-value pairs of type (string, Module)"}]}},
{"id": "torch.nn.Module.double", "type": "method", "code": "torch.nn.Module.double()", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "summary": "Casts all floating point parameters and buffers to double datatype", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Module.double", "parameters": []}},
{"id": "torch.nn.Module.eval", "type": "method", "code": "torch.nn.Module.eval()", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "summary": "Sets the module in evaluation mode", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Module.eval", "parameters": []}},
{"id": "torch.nn.Module.extra_repr", "type": "method", "code": "torch.nn.Module.extra_repr()", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "summary": "Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Module.extra_repr", "parameters": []}},
{"id": "torch.nn.Module.float", "type": "method", "code": "torch.nn.Module.float()", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "summary": "Casts all floating point parameters and buffers to float datatype", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Module.float", "parameters": []}},
{"id": "torch.nn.Module.half", "type": "method", "code": "torch.nn.Module.half()", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "summary": "Casts all floating point parameters and buffers to half datatype", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Module.half", "parameters": []}},
{"id": "torch.nn.Module.load_state_dict", "type": "method", "code": "torch.nn.Module.load_state_dict(state_dict:Dict[str,torch.Tensor],strict:bool=True)", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "summary": "Copies parameters and buffers from state_dict into this module and its descendants", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Module.load_state_dict", "parameters": [{"name": "state_dict", "type": "Dict[str,torch.Tensor]", "is_optional": false, "description": "a dict containing parameters andpersistent buffers."}, {"name": "strict", "type": "bool", "default_value": "True", "is_optional": true, "description": "whether to strictly enforce that the keysin state_dict match the keys returned by this module\u2019sstate_dict() function. Default: True"}]}},
{"id": "torch.nn.Module.modules", "type": "method", "code": "torch.nn.Module.modules()", "example": "  l = nn.Linear(2, 2)  net = nn.Sequential(l, l)  for idx, m in enumerate(net.modules()):         print(idx, '-', m)  0 - Sequential(   (0): Linear(in_features=2, out_features=2, bias=True)   (1): Linear(in_features=2, out_features=2, bias=True) ) 1 - Linear(in_features=2, out_features=2, bias=True)   ", "summary": "Returns an iterator over all modules in the network", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Module.modules", "parameters": []}},
{"id": "torch.nn.Module.named_buffers", "type": "method", "code": "torch.nn.Module.named_buffers(prefix:str='',recurse:bool=True)", "example": "  for name, buf in self.named_buffers():     if name in ['running_var']:         print(buf.size())   ", "summary": "Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Module.named_buffers", "parameters": [{"name": "prefix", "type": "str", "default_value": "''", "is_optional": false, "description": "prefix to prepend to all buffer names."}, {"name": "recurse", "type": "bool", "default_value": "True", "is_optional": false, "description": "if True, then yields buffers of this moduleand all submodules. Otherwise, yields only buffers thatare direct members of this module."}]}},
{"id": "torch.nn.Module.named_children", "type": "method", "code": "torch.nn.Module.named_children()", "example": "  for name, module in model.named_children():      if name in ['conv4', 'conv5']:          print(module)   ", "summary": "Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Module.named_children", "parameters": []}},
{"id": "torch.nn.Module.named_modules", "type": "method", "code": "torch.nn.Module.named_modules(memo:Optional[Set[Module]]=None,prefix:str='')", "example": "  l = nn.Linear(2, 2)  net = nn.Sequential(l, l)  for idx, m in enumerate(net.named_modules()):         print(idx, '-', m)  0 - ('', Sequential(   (0): Linear(in_features=2, out_features=2, bias=True)   (1): Linear(in_features=2, out_features=2, bias=True) )) 1 - ('0', Linear(in_features=2, out_features=2, bias=True))   ", "summary": "Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Module.named_modules", "parameters": [{"name": "memo", "type": "Optional[Set[Module]]", "default_value": "None", "is_optional": false, "description": ""}, {"name": "prefix", "type": "str", "default_value": "''", "is_optional": false, "description": ""}]}},
{"id": "torch.nn.Module.named_parameters", "type": "method", "code": "torch.nn.Module.named_parameters(prefix:str='',recurse:bool=True)", "example": "  for name, param in self.named_parameters():     if name in ['bias']:         print(param.size())   ", "summary": "Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Module.named_parameters", "parameters": [{"name": "prefix", "type": "str", "default_value": "''", "is_optional": false, "description": "prefix to prepend to all parameter names."}, {"name": "recurse", "type": "bool", "default_value": "True", "is_optional": false, "description": "if True, then yields parameters of this moduleand all submodules. Otherwise, yields only parameters thatare direct members of this module."}]}},
{"id": "torch.nn.Module.parameters", "type": "method", "code": "torch.nn.Module.parameters(recurse:bool=True)", "example": "  for param in model.parameters():      print(type(param), param.size()) &lt;class 'torch.Tensor' (20L,) &lt;class 'torch.Tensor' (20L, 1L, 5L, 5L)   ", "summary": "Returns an iterator over module parameters", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Module.parameters", "parameters": [{"name": "recurse", "type": "bool", "default_value": "True", "is_optional": false, "description": "if True, then yields parameters of this moduleand all submodules. Otherwise, yields only parameters thatare direct members of this module."}]}},
{"id": "torch.nn.Module.register_backward_hook", "type": "method", "code": "torch.nn.Module.register_backward_hook(hook:Callable[[Module,Union[Tuple[torch.Tensor,...],torch.Tensor],Union[Tuple[torch.Tensor,...],torch.Tensor]],Union[None,torch.Tensor]])", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "summary": "Registers a backward hook on the module", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Module.register_backward_hook", "parameters": [{"name": "hook", "type": "Callable[[Module,Union[Tuple[torch.Tensor,...],torch.Tensor],Union[Tuple[torch.Tensor,...],torch.Tensor]],Union[None,torch.Tensor]]", "is_optional": false, "description": ""}]}},
{"id": "torch.nn.Module.register_buffer", "type": "method", "code": "torch.nn.Module.register_buffer(name:str,tensor:Optional[torch.Tensor],persistent:bool=True)", "example": "  self.register_buffer('running_mean', torch.zeros(num_features))   ", "summary": "Adds a buffer to the module", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Module.register_buffer", "parameters": [{"name": "name", "type": "str", "is_optional": false, "description": "name of the buffer. The buffer can be accessedfrom this module using the given name"}, {"name": "tensor", "type": "Optional[torch.Tensor]", "is_optional": false, "description": "buffer to be registered."}, {"name": "persistent", "type": "bool", "default_value": "True", "is_optional": false, "description": "whether the buffer is part of this module\u2019sstate_dict."}]}},
{"id": "torch.nn.Module.register_forward_hook", "type": "method", "code": "torch.nn.Module.register_forward_hook(hook:Callable[...,None])", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "summary": "Registers a forward hook on the module", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Module.register_forward_hook", "parameters": [{"name": "hook", "type": "Callable[...,None]", "is_optional": false, "description": ""}]}},
{"id": "torch.nn.Module.register_forward_pre_hook", "type": "method", "code": "torch.nn.Module.register_forward_pre_hook(hook:Callable[...,None])", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "summary": "Registers a forward pre-hook on the module", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Module.register_forward_pre_hook", "parameters": [{"name": "hook", "type": "Callable[...,None]", "is_optional": false, "description": ""}]}},
{"id": "torch.nn.Module.register_parameter", "type": "method", "code": "torch.nn.Module.register_parameter(name:str,param:Optional[torch.nn.parameter.Parameter])", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "summary": "Adds a parameter to the module", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Module.register_parameter", "parameters": [{"name": "name", "type": "str", "is_optional": false, "description": "name of the parameter. The parameter can be accessedfrom this module using the given name"}, {"name": "param", "type": "Optional[torch.nn.parameter.Parameter]", "is_optional": false, "description": "parameter to be added to the module."}]}},
{"id": "torch.nn.Module.requires_grad_", "type": "method", "code": "torch.nn.Module.requires_grad_(requires_grad:bool=True)", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "summary": "Change if autograd should record operations on parameters in this module", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Module.requires_grad_", "parameters": [{"name": "requires_grad", "type": "bool", "default_value": "True", "is_optional": false, "description": "whether autograd should record operations onparameters in this module. Default: True."}]}},
{"id": "torch.nn.Module.state_dict", "type": "method", "code": "torch.nn.Module.state_dict(destination=None,prefix='',keep_vars=False)", "example": "  module.state_dict().keys() ['bias', 'weight']   ", "summary": "Returns a dictionary containing a whole state of the module", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Module.state_dict", "parameters": []}},
{"id": "torch.nn.Module.to", "type": "method", "code": "torch.nn.Module.to(*args,**kwargs)", "example": "  linear = nn.Linear(2, 2)  linear.weight Parameter containing: tensor([[ 0.1913, -0.3420],         [-0.5113, -0.2325]])  linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True)  linear.weight Parameter containing: tensor([[ 0.1913, -0.3420],         [-0.5113, -0.2325]], dtype=torch.float64)  gpu1 = torch.device(\"cuda:1\")  linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True)  linear.weight Parameter containing: tensor([[ 0.1914, -0.3420],         [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')  cpu = torch.device(\"cpu\")  linear.to(cpu) Linear(in_features=2, out_features=2, bias=True)  linear.weight Parameter containing: tensor([[ 0.1914, -0.3420],         [-0.5112, -0.2324]], dtype=torch.float16)   ", "summary": "Moves and/or casts the parameters and buffers", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Module.to", "parameters": []}},
{"id": "torch.nn.Module.train", "type": "method", "code": "torch.nn.Module.train(mode:bool=True)", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "summary": "Sets the module in training mode", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Module.train", "parameters": [{"name": "mode", "type": "bool", "default_value": "True", "is_optional": false, "description": "whether to set training mode (True) or evaluationmode (False). Default: True."}]}},
{"id": "torch.nn.Module.type", "type": "method", "code": "torch.nn.Module.type(dst_type:Union[torch.dtype,str])", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "summary": "Casts all parameters and buffers to dst_type", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Module.type", "parameters": [{"name": "dst_type", "type": "Union[torch.dtype,str]", "is_optional": false, "description": "the desired type"}]}},
{"id": "torch.nn.Module.zero_grad", "type": "method", "code": "torch.nn.Module.zero_grad()", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "summary": "Sets gradients of all model parameters to zero", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Module.zero_grad", "parameters": []}},
{"id": "torch.nn.Module", "type": "class", "code": "torch.nn.Module", "example": "  @torch.no_grad()  def init_weights(m):      print(m)      if type(m) == nn.Linear:          m.weight.fill_(1.0)          print(m.weight)  net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))  net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1.,  1.],         [ 1.,  1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1.,  1.],         [ 1.,  1.]]) Sequential(   (0): Linear(in_features=2, out_features=2, bias=True)   (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential(   (0): Linear(in_features=2, out_features=2, bias=True)   (1): Linear(in_features=2, out_features=2, bias=True) )       bfloat16() \u2192 T  Casts all floating point parameters and buffers to bfloat16 datatype.  Returns self  Return type Module       buffers(recurse: bool = True) \u2192 Iterator[torch.Tensor]  Returns an iterator over module buffers.  Parameters recurse (bool) \u2013 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.  Yields torch.Tensor \u2013 module buffer   ", "summary": "Base class for all neural network modules", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Module", "parameters": []}},
{"id": "torch.nn.Module.dump_patches", "type": "attribute", "code": "torch.nn.Module.dump_patches", "example": "&gt;&gt;&gt; linear = nn.Linear(2, 2)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n&gt;&gt;&gt; linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n&gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\")\n&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n&gt;&gt;&gt; cpu = torch.device(\"cpu\")\n&gt;&gt;&gt; linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n&gt;&gt;&gt; linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "summary": "This allows better BC support for load_state_dict()", "returns": "self", "shape": "", "code-info": {"name": "torch.nn.Module.dump_patches", "parameters": []}}]
