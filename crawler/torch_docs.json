[
{"library": "torch", "item_id": "torchvision.get_image_backend", "item_type": "function", "code": "torchvision.get_image_backend()", "description": "Gets the name of the package used to load images ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.set_image_backend", "item_type": "function", "code": "torchvision.set_image_backend(backend)", "description": "Specifies the package used to load images.  Parameters backend (string) \u2013 Name of the image backend. one of {\u2018PIL\u2019, \u2018accimage\u2019}. The accimage package uses the Intel IPP library. It is generally faster than PIL, but does not support as many operations.   ", "parameters": ["backend (string) : Name of the image backend. one of {\u2018PIL\u2019, \u2018accimage\u2019}.The accimage package uses the Intel IPP library. It isgenerally faster than PIL, but does not support as many operations."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.set_video_backend", "item_type": "function", "code": "torchvision.set_video_backend(backend)", "description": "Specifies the package used to decode videos.  Parameters backend (string) \u2013 Name of the video backend. one of {\u2018pyav\u2019, \u2018video_reader\u2019}. The pyav package uses the 3rd party PyAv library. It is a Pythonic  binding for the FFmpeg libraries.   The video_reader package includes a native c++ implementation ontop of FFMPEG libraries, and a python API of TorchScript custom operator. It is generally decoding faster than pyav, but perhaps is less robust.      ", "parameters": ["backend (string) : Name of the video backend. one of {\u2018pyav\u2019, \u2018video_reader\u2019}.The pyav package uses the 3rd party PyAv library. It is a Pythonicbinding for the FFmpeg libraries.The video_reader package includes a native c++ implementation ontop of FFMPEG libraries, and a python API of TorchScript custom operator.It is generally decoding faster than pyav, but perhaps is less robust.", "top of FFMPEG libraries, and a python API of TorchScript custom operator.It is generally decoding faster than pyav, but perhaps is less robust."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.ops.nms", "item_type": "function", "code": "torchvision.ops.nms(boxes,scores,iou_threshold)", "description": "Performs non-maximum suppression (NMS) on the boxes according to their intersection-over-union (IoU). NMS iteratively removes lower scoring boxes which have an IoU greater than iou_threshold with another (higher scoring) box.  Parameters  boxes (Tensor[N, 4])) \u2013 boxes to perform NMS on. They are expected to be in (x1, y1, x2, y2) format scores (Tensor[N]) \u2013 scores for each one of the boxes iou_threshold (python:float) \u2013 discards all overlapping boxes with IoU &gt; iou_threshold   Returns keep \u2013 int64 tensor with the indices of the elements that have been kept by NMS, sorted in decreasing order of scores  Return type Tensor   ", "parameters": ["boxes (Tensor[N, 4])) : boxes to perform NMS on. Theyare expected to be in (x1, y1, x2, y2) format", "scores (Tensor[N]) : scores for each one of the boxes", "iou_threshold (python:float) : discards all overlappingboxes with IoU &gt; iou_threshold"], "returns": "keep \u2013 int64 tensor with the indicesof the elements that have been keptby NMS, sorted in decreasing order of scores", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.ops.roi_align", "item_type": "function", "code": "torchvision.ops.roi_align(input,boxes,output_size,spatial_scale=1.0,sampling_ratio=-1)", "description": "Performs Region of Interest (RoI) Align operator described in Mask R-CNN  Parameters  input (Tensor[N, C, H, W]) \u2013 input tensor boxes (Tensor[K, 5] or List[Tensor[L, 4]]) \u2013 the box coordinates in (x1, y1, x2, y2) format where the regions will be taken from. If a single Tensor is passed, then the first column should contain the batch index. If a list of Tensors is passed, then each Tensor will correspond to the boxes for an element i in a batch output_size (python:int or Tuple[python:int, python:int]) \u2013 the size of the output after the cropping is performed, as (height, width) spatial_scale (python:float) \u2013 a scaling factor that maps the input coordinates to the box coordinates. Default: 1.0 sampling_ratio (python:int) \u2013 number of sampling points in the interpolation grid used to compute the output value of each pooled output bin. If &gt; 0, then exactly sampling_ratio x sampling_ratio grid points are used. If &lt;= 0, then an adaptive number of grid points are used (computed as ceil(roi_width / pooled_w), and likewise for height). Default: -1   Returns output (Tensor[K, C, output_size[0], output_size[1]])   ", "parameters": ["input (Tensor[N, C, H, W]) : input tensor", "boxes (Tensor[K, 5] or List[Tensor[L, 4]]) : the box coordinates in (x1, y1, x2, y2)format where the regions will be taken from. If a single Tensor is passed,then the first column should contain the batch index. If a list of Tensorsis passed, then each Tensor will correspond to the boxes for an element iin a batch", "output_size (python:int or Tuple[python:int, python:int]) : the size of the output after the croppingis performed, as (height, width)", "spatial_scale (python:float) : a scaling factor that maps the input coordinates tothe box coordinates. Default: 1.0", "sampling_ratio (python:int) : number of sampling points in the interpolation gridused to compute the output value of each pooled output bin. If &gt; 0,then exactly sampling_ratio x sampling_ratio grid points are used. If&lt;= 0, then an adaptive number of grid points are used (computed asceil(roi_width / pooled_w), and likewise for height). Default: -1"], "returns": "output (Tensor[K, C, output_size[0], output_size[1]])", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.ops.roi_pool", "item_type": "function", "code": "torchvision.ops.roi_pool(input,boxes,output_size,spatial_scale=1.0)", "description": "Performs Region of Interest (RoI) Pool operator described in Fast R-CNN  Parameters  input (Tensor[N, C, H, W]) \u2013 input tensor boxes (Tensor[K, 5] or List[Tensor[L, 4]]) \u2013 the box coordinates in (x1, y1, x2, y2) format where the regions will be taken from. If a single Tensor is passed, then the first column should contain the batch index. If a list of Tensors is passed, then each Tensor will correspond to the boxes for an element i in a batch output_size (python:int or Tuple[python:int, python:int]) \u2013 the size of the output after the cropping is performed, as (height, width) spatial_scale (python:float) \u2013 a scaling factor that maps the input coordinates to the box coordinates. Default: 1.0   Returns output (Tensor[K, C, output_size[0], output_size[1]])   ", "parameters": ["input (Tensor[N, C, H, W]) : input tensor", "boxes (Tensor[K, 5] or List[Tensor[L, 4]]) : the box coordinates in (x1, y1, x2, y2)format where the regions will be taken from. If a single Tensor is passed,then the first column should contain the batch index. If a list of Tensorsis passed, then each Tensor will correspond to the boxes for an element iin a batch", "output_size (python:int or Tuple[python:int, python:int]) : the size of the output after the croppingis performed, as (height, width)", "spatial_scale (python:float) : a scaling factor that maps the input coordinates tothe box coordinates. Default: 1.0"], "returns": "output (Tensor[K, C, output_size[0], output_size[1]])", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.ops.RoIAlign", "item_type": "class", "code": "classtorchvision.ops.RoIAlign(output_size,spatial_scale,sampling_ratio)", "description": "See roi_align ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.ops.RoIPool", "item_type": "class", "code": "classtorchvision.ops.RoIPool(output_size,spatial_scale)", "description": "See roi_pool ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.utils.make_grid", "item_type": "function", "code": "torchvision.utils.make_grid(tensor,nrow=8,padding=2,normalize=False,range=None,scale_each=False,pad_value=0)", "description": "Make a grid of images.  Parameters  tensor (Tensor or list) \u2013 4D mini-batch Tensor of shape (B x C x H x W) or a list of images all of the same size. nrow (python:int, optional) \u2013 Number of images displayed in each row of the grid. The final grid size is (B / nrow, nrow). Default: 8. padding (python:int, optional) \u2013 amount of padding. Default: 2. normalize (bool, optional) \u2013 If True, shift the image to the range (0, 1), by the min and max values specified by range. Default: False. range (tuple, optional) \u2013 tuple (min, max) where min and max are numbers, then these numbers are used to normalize the image. By default, min and max are computed from the tensor. scale_each (bool, optional) \u2013 If True, scale each image in the batch of images separately rather than the (min, max) over all images. Default: False. pad_value (python:float, optional) \u2013 Value for the padded pixels. Default: 0.    Example See this notebook here ", "parameters": ["tensor (Tensor or list) : 4D mini-batch Tensor of shape (B x C x H x W)or a list of images all of the same size.", "nrow (python:int, optional) : Number of images displayed in each row of the grid.The final grid size is (B / nrow, nrow). Default: 8.", "padding (python:int, optional) : amount of padding. Default: 2.", "normalize (bool, optional) : If True, shift the image to the range (0, 1),by the min and max values specified by range. Default: False.", "range (tuple, optional) : tuple (min, max) where min and max are numbers,then these numbers are used to normalize the image. By default, min and maxare computed from the tensor.", "scale_each (bool, optional) : If True, scale each image in the batch ofimages separately rather than the (min, max) over all images. Default: False.", "pad_value (python:float, optional) : Value for the padded pixels. Default: 0."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.utils.save_image", "item_type": "function", "code": "torchvision.utils.save_image(tensor,fp,nrow=8,padding=2,normalize=False,range=None,scale_each=False,pad_value=0,format=None)", "description": "Save a given Tensor into an image file.  Parameters  tensor (Tensor or list) \u2013 Image to be saved. If given a mini-batch tensor, saves the tensor as a grid of images by calling make_grid. - A filename (fp) \u2013  format (Optional) \u2013 If omitted, the format to use is determined from the filename extension. If a file object was used instead of a filename, this parameter should always be used. **kwargs \u2013 Other arguments are documented in make_grid.    ", "parameters": ["tensor (Tensor or list) : Image to be saved. If given a mini-batch tensor,saves the tensor as a grid of images by calling make_grid.", "- A filename (fp) : ", "format (Optional) : If omitted, the format to use is determined from the filename extension.If a file object was used instead of a filename, this parameter should always be used.", "**kwargs : Other arguments are documented in make_grid."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.datasets.CocoCaptions.__getitem__", "item_type": "method", "code": "__getitem__(index)", "description": " Parameters index (python:int) \u2013 Index  Returns Tuple (image, target). target is a list of captions for the image.  Return type tuple   ", "parameters": ["index (python:int) : Index", "Tuple (image, target). target is a list of captions for the image.", "tuple"], "returns": "Tuple (image, target). target is a list of captions for the image.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.models.alexnet", "item_type": "function", "code": "torchvision.models.alexnet(pretrained=False,progress=True,**kwargs)", "description": "AlexNet model architecture from the \u201cOne weird trick\u2026\u201d paper.  Parameters  pretrained (bool) \u2013 If True, returns a model pre-trained on ImageNet progress (bool) \u2013 If True, displays a progress bar of the download to stderr    ", "parameters": ["pretrained (bool) : If True, returns a model pre-trained on ImageNet", "progress (bool) : If True, displays a progress bar of the download to stderr"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.datasets.CocoDetection.__getitem__", "item_type": "method", "code": "__getitem__(index)", "description": " Parameters index (python:int) \u2013 Index  Returns Tuple (image, target). target is the object returned by coco.loadAnns.  Return type tuple   ", "parameters": ["index (python:int) : Index", "Tuple (image, target). target is the object returned by coco.loadAnns.", "tuple"], "returns": "Tuple (image, target). target is the object returned by coco.loadAnns.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.datasets.LSUN.__getitem__", "item_type": "method", "code": "__getitem__(index)", "description": " Parameters index (python:int) \u2013 Index  Returns Tuple (image, target) where target is the index of the target category.  Return type tuple   ", "parameters": ["index (python:int) : Index", "Tuple (image, target) where target is the index of the target category.", "tuple"], "returns": "Tuple (image, target) where target is the index of the target category.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.datasets.ImageFolder.__getitem__", "item_type": "method", "code": "__getitem__(index)", "description": " Parameters index (python:int) \u2013 Index  Returns (sample, target) where target is class_index of the target class.  Return type tuple   ", "parameters": ["index (python:int) : Index", "(sample, target) where target is class_index of the target class.", "tuple"], "returns": "(sample, target) where target is class_index of the target class.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.datasets.DatasetFolder.__getitem__", "item_type": "method", "code": "__getitem__(index)", "description": " Parameters index (python:int) \u2013 Index  Returns (sample, target) where target is class_index of the target class.  Return type tuple   ", "parameters": ["index (python:int) : Index", "(sample, target) where target is class_index of the target class.", "tuple"], "returns": "(sample, target) where target is class_index of the target class.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.datasets.CIFAR10.__getitem__", "item_type": "method", "code": "__getitem__(index)", "description": " Parameters index (python:int) \u2013 Index  Returns (image, target) where target is index of the target class.  Return type tuple   ", "parameters": ["index (python:int) : Index", "(image, target) where target is index of the target class.", "tuple"], "returns": "(image, target) where target is index of the target class.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.datasets.STL10.__getitem__", "item_type": "method", "code": "__getitem__(index)", "description": " Parameters index (python:int) \u2013 Index  Returns (image, target) where target is index of the target class.  Return type tuple   ", "parameters": ["index (python:int) : Index", "(image, target) where target is index of the target class.", "tuple"], "returns": "(image, target) where target is index of the target class.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.datasets.SVHN.__getitem__", "item_type": "method", "code": "__getitem__(index)", "description": " Parameters index (python:int) \u2013 Index  Returns (image, target) where target is index of the target class.  Return type tuple   ", "parameters": ["index (python:int) : Index", "(image, target) where target is index of the target class.", "tuple"], "returns": "(image, target) where target is index of the target class.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.datasets.PhotoTour.__getitem__", "item_type": "method", "code": "__getitem__(index)", "description": " Parameters index (python:int) \u2013 Index  Returns (data1, data2, matches)  Return type tuple   ", "parameters": ["index (python:int) : Index", "(data1, data2, matches)", "tuple"], "returns": "(data1, data2, matches)", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.datasets.SBU.__getitem__", "item_type": "method", "code": "__getitem__(index)", "description": " Parameters index (python:int) \u2013 Index  Returns (image, target) where target is a caption for the photo.  Return type tuple   ", "parameters": ["index (python:int) : Index", "(image, target) where target is a caption for the photo.", "tuple"], "returns": "(image, target) where target is a caption for the photo.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.models.vgg11", "item_type": "function", "code": "torchvision.models.vgg11(pretrained=False,progress=True,**kwargs)", "description": "VGG 11-layer model (configuration \u201cA\u201d) from \u201cVery Deep Convolutional Networks For Large-Scale Image Recognition\u201d  Parameters  pretrained (bool) \u2013 If True, returns a model pre-trained on ImageNet progress (bool) \u2013 If True, displays a progress bar of the download to stderr    ", "parameters": ["pretrained (bool) : If True, returns a model pre-trained on ImageNet", "progress (bool) : If True, displays a progress bar of the download to stderr"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.models.vgg11_bn", "item_type": "function", "code": "torchvision.models.vgg11_bn(pretrained=False,progress=True,**kwargs)", "description": "VGG 11-layer model (configuration \u201cA\u201d) with batch normalization \u201cVery Deep Convolutional Networks For Large-Scale Image Recognition\u201d  Parameters  pretrained (bool) \u2013 If True, returns a model pre-trained on ImageNet progress (bool) \u2013 If True, displays a progress bar of the download to stderr    ", "parameters": ["pretrained (bool) : If True, returns a model pre-trained on ImageNet", "progress (bool) : If True, displays a progress bar of the download to stderr"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.models.vgg13", "item_type": "function", "code": "torchvision.models.vgg13(pretrained=False,progress=True,**kwargs)", "description": "VGG 13-layer model (configuration \u201cB\u201d) \u201cVery Deep Convolutional Networks For Large-Scale Image Recognition\u201d  Parameters  pretrained (bool) \u2013 If True, returns a model pre-trained on ImageNet progress (bool) \u2013 If True, displays a progress bar of the download to stderr    ", "parameters": ["pretrained (bool) : If True, returns a model pre-trained on ImageNet", "progress (bool) : If True, displays a progress bar of the download to stderr"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.models.vgg13_bn", "item_type": "function", "code": "torchvision.models.vgg13_bn(pretrained=False,progress=True,**kwargs)", "description": "VGG 13-layer model (configuration \u201cB\u201d) with batch normalization \u201cVery Deep Convolutional Networks For Large-Scale Image Recognition\u201d  Parameters  pretrained (bool) \u2013 If True, returns a model pre-trained on ImageNet progress (bool) \u2013 If True, displays a progress bar of the download to stderr    ", "parameters": ["pretrained (bool) : If True, returns a model pre-trained on ImageNet", "progress (bool) : If True, displays a progress bar of the download to stderr"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.models.vgg16", "item_type": "function", "code": "torchvision.models.vgg16(pretrained=False,progress=True,**kwargs)", "description": "VGG 16-layer model (configuration \u201cD\u201d) \u201cVery Deep Convolutional Networks For Large-Scale Image Recognition\u201d  Parameters  pretrained (bool) \u2013 If True, returns a model pre-trained on ImageNet progress (bool) \u2013 If True, displays a progress bar of the download to stderr    ", "parameters": ["pretrained (bool) : If True, returns a model pre-trained on ImageNet", "progress (bool) : If True, displays a progress bar of the download to stderr"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.models.vgg16_bn", "item_type": "function", "code": "torchvision.models.vgg16_bn(pretrained=False,progress=True,**kwargs)", "description": "VGG 16-layer model (configuration \u201cD\u201d) with batch normalization \u201cVery Deep Convolutional Networks For Large-Scale Image Recognition\u201d  Parameters  pretrained (bool) \u2013 If True, returns a model pre-trained on ImageNet progress (bool) \u2013 If True, displays a progress bar of the download to stderr    ", "parameters": ["pretrained (bool) : If True, returns a model pre-trained on ImageNet", "progress (bool) : If True, displays a progress bar of the download to stderr"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.models.vgg19", "item_type": "function", "code": "torchvision.models.vgg19(pretrained=False,progress=True,**kwargs)", "description": "VGG 19-layer model (configuration \u201cE\u201d) \u201cVery Deep Convolutional Networks For Large-Scale Image Recognition\u201d  Parameters  pretrained (bool) \u2013 If True, returns a model pre-trained on ImageNet progress (bool) \u2013 If True, displays a progress bar of the download to stderr    ", "parameters": ["pretrained (bool) : If True, returns a model pre-trained on ImageNet", "progress (bool) : If True, displays a progress bar of the download to stderr"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.models.vgg19_bn", "item_type": "function", "code": "torchvision.models.vgg19_bn(pretrained=False,progress=True,**kwargs)", "description": "VGG 19-layer model (configuration \u2018E\u2019) with batch normalization \u201cVery Deep Convolutional Networks For Large-Scale Image Recognition\u201d  Parameters  pretrained (bool) \u2013 If True, returns a model pre-trained on ImageNet progress (bool) \u2013 If True, displays a progress bar of the download to stderr    ", "parameters": ["pretrained (bool) : If True, returns a model pre-trained on ImageNet", "progress (bool) : If True, displays a progress bar of the download to stderr"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.models.resnet18", "item_type": "function", "code": "torchvision.models.resnet18(pretrained=False,progress=True,**kwargs)", "description": "ResNet-18 model from \u201cDeep Residual Learning for Image Recognition\u201d  Parameters  pretrained (bool) \u2013 If True, returns a model pre-trained on ImageNet progress (bool) \u2013 If True, displays a progress bar of the download to stderr    ", "parameters": ["pretrained (bool) : If True, returns a model pre-trained on ImageNet", "progress (bool) : If True, displays a progress bar of the download to stderr"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.io.read_video", "item_type": "function", "code": "torchvision.io.read_video(filename,start_pts=0,end_pts=None,pts_unit='pts')", "description": "Reads a video from a file, returning both the video frames as well as the audio frames  Parameters  filename (str) \u2013 path to the video file start_pts (python:int if pts_unit = 'pts', optional) \u2013 float / Fraction if pts_unit = \u2018sec\u2019, optional the start presentation time of the video end_pts (python:int if pts_unit = 'pts', optional) \u2013 float / Fraction if pts_unit = \u2018sec\u2019, optional the end presentation time pts_unit (str, optional) \u2013 unit in which start_pts and end_pts values will be interpreted, either \u2018pts\u2019 or \u2018sec\u2019. Defaults to \u2018pts\u2019.   Returns  vframes (Tensor[T, H, W, C]) \u2013 the T video frames aframes (Tensor[K, L]) \u2013 the audio frames, where K is the number of channels and L is the number of points info (Dict) \u2013 metadata for the video and audio. Can contain the fields video_fps (float) and audio_fps (int)     ", "parameters": ["filename (str) : path to the video file", "start_pts (python:int if pts_unit = 'pts', optional) : float / Fraction if pts_unit = \u2018sec\u2019, optionalthe start presentation time of the video", "end_pts (python:int if pts_unit = 'pts', optional) : float / Fraction if pts_unit = \u2018sec\u2019, optionalthe end presentation time", "pts_unit (str, optional) : unit in which start_pts and end_pts values will be interpreted, either \u2018pts\u2019 or \u2018sec\u2019. Defaults to \u2018pts\u2019.", "vframes (Tensor[T, H, W, C]) : the T video frames", "aframes (Tensor[K, L]) : the audio frames, where K is the number of channels and L is thenumber of points", "info (Dict) : metadata for the video and audio. Can contain the fields video_fps (float)and audio_fps (int)"], "returns": "vframes (Tensor[T, H, W, C]) \u2013 the T video framesaframes (Tensor[K, L]) \u2013 the audio frames, where K is the number of channels and L is thenumber of pointsinfo (Dict) \u2013 metadata for the video and audio. Can contain the fields video_fps (float)and audio_fps (int)", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.io.read_video_timestamps", "item_type": "function", "code": "torchvision.io.read_video_timestamps(filename,pts_unit='pts')", "description": "List the video frames timestamps. Note that the function decodes the whole video frame-by-frame.  Parameters  filename (str) \u2013 path to the video file pts_unit (str, optional) \u2013 unit in which timestamp values will be returned either \u2018pts\u2019 or \u2018sec\u2019. Defaults to \u2018pts\u2019.   Returns  pts (List[int] if pts_unit = \u2018pts\u2019) \u2013 List[Fraction] if pts_unit = \u2018sec\u2019 presentation timestamps for each one of the frames in the video. video_fps (int) \u2013 the frame rate for the video     ", "parameters": ["filename (str) : path to the video file", "pts_unit (str, optional) : unit in which timestamp values will be returned either \u2018pts\u2019 or \u2018sec\u2019. Defaults to \u2018pts\u2019.", "pts (List[int] if pts_unit = \u2018pts\u2019) : List[Fraction] if pts_unit = \u2018sec\u2019presentation timestamps for each one of the frames in the video.", "video_fps (int) : the frame rate for the video"], "returns": "pts (List[int] if pts_unit = \u2018pts\u2019) \u2013 List[Fraction] if pts_unit = \u2018sec\u2019presentation timestamps for each one of the frames in the video.video_fps (int) \u2013 the frame rate for the video", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.io.write_video", "item_type": "function", "code": "torchvision.io.write_video(filename,video_array,fps,video_codec='libx264',options=None)", "description": "Writes a 4d tensor in [T, H, W, C] format in a video file  Parameters  filename (str) \u2013 path where the video will be saved video_array (Tensor[T, H, W, C]) \u2013 tensor containing the individual frames, as a uint8 tensor in [T, H, W, C] format fps (Number) \u2013 frames per second    ", "parameters": ["filename (str) : path where the video will be saved", "video_array (Tensor[T, H, W, C]) : tensor containing the individual frames, as a uint8 tensor in [T, H, W, C] format", "fps (Number) : frames per second"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.functional.adjust_brightness", "item_type": "function", "code": "torchvision.transforms.functional.adjust_brightness(img,brightness_factor)", "description": "Adjust brightness of an Image.  Parameters  img (PIL Image) \u2013 PIL Image to be adjusted. brightness_factor (python:float) \u2013 How much to adjust the brightness. Can be any non negative number. 0 gives a black image, 1 gives the original image while 2 increases the brightness by a factor of 2.   Returns Brightness adjusted image.  Return type PIL Image   ", "parameters": ["img (PIL Image) : PIL Image to be adjusted.", "brightness_factor (python:float) : How much to adjust the brightness. Can beany non negative number. 0 gives a black image, 1 gives theoriginal image while 2 increases the brightness by a factor of 2."], "returns": "Brightness adjusted image.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.functional.adjust_contrast", "item_type": "function", "code": "torchvision.transforms.functional.adjust_contrast(img,contrast_factor)", "description": "Adjust contrast of an Image.  Parameters  img (PIL Image) \u2013 PIL Image to be adjusted. contrast_factor (python:float) \u2013 How much to adjust the contrast. Can be any non negative number. 0 gives a solid gray image, 1 gives the original image while 2 increases the contrast by a factor of 2.   Returns Contrast adjusted image.  Return type PIL Image   ", "parameters": ["img (PIL Image) : PIL Image to be adjusted.", "contrast_factor (python:float) : How much to adjust the contrast. Can be anynon negative number. 0 gives a solid gray image, 1 gives theoriginal image while 2 increases the contrast by a factor of 2."], "returns": "Contrast adjusted image.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.rename", "item_type": "method", "code": "rename(*names,**rename_map)", "description": "Renames dimension names of self. There are two main usages: self.rename(**rename_map) returns a view on tensor that has dims renamed as specified in the mapping rename_map. self.rename(*names) returns a view on tensor, renaming all dimensions positionally using names. Use self.rename(None) to drop names on a tensor. One cannot specify both positional args names and keyword args rename_map. Examples: &gt;&gt;&gt; imgs = torch.rand(2, 3, 5, 7, names=('N', 'C', 'H', 'W')) &gt;&gt;&gt; renamed_imgs = imgs.rename(N='batch', C='channels') &gt;&gt;&gt; renamed_imgs.names ('batch', 'channels', 'H', 'W')  &gt;&gt;&gt; renamed_imgs = imgs.rename(None) &gt;&gt;&gt; renamed_imgs.names (None,)  &gt;&gt;&gt; renamed_imgs = imgs.rename('batch', 'channel', 'height', 'width') &gt;&gt;&gt; renamed_imgs.names ('batch', 'channel', 'height', 'width')    Warning The named tensor API is experimental and subject to change.  ", "parameters": [], "returns": null, "example": " imgs = torch.rand(2, 3, 5, 7, names=('N', 'C', 'H', 'W'))\n renamed_imgs = imgs.rename(N='batch', C='channels')\n renamed_imgs.names\n('batch', 'channels', 'H', 'W')\n\n renamed_imgs = imgs.rename(None)\n renamed_imgs.names\n(None,)\n\n renamed_imgs = imgs.rename('batch', 'channel', 'height', 'width')\n renamed_imgs.names\n('batch', 'channel', 'height', 'width')\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.rename_", "item_type": "method", "code": "rename_(*names,**rename_map)", "description": "In-place version of rename(). ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.torch.finfo", "item_type": "class", "code": "classtorch.finfo", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.torch.iinfo", "item_type": "class", "code": "classtorch.iinfo", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.utils.dlpack.from_dlpack", "item_type": "function", "code": "torch.utils.dlpack.from_dlpack(dlpack)\u2192Tensor", "description": "Decodes a DLPack to a tensor.  Parameters dlpack \u2013 a PyCapsule object with the dltensor   The tensor will share the memory with the object represented in the dlpack. Note that each dlpack can only be consumed once. ", "parameters": ["dlpack : a PyCapsule object with the dltensor"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.utils.dlpack.to_dlpack", "item_type": "function", "code": "torch.utils.dlpack.to_dlpack(tensor)\u2192PyCapsule", "description": "Returns a DLPack representing the tensor.  Parameters tensor \u2013 a tensor to be exported   The dlpack shares the tensors memory. Note that each dlpack can only be consumed once. ", "parameters": ["tensor : a tensor to be exported"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.datasets.Flickr8k.__getitem__", "item_type": "method", "code": "__getitem__(index)", "description": " Parameters index (python:int) \u2013 Index  Returns Tuple (image, target). target is a list of captions for the image.  Return type tuple   ", "parameters": ["index (python:int) : Index", "Tuple (image, target). target is a list of captions for the image.", "tuple"], "returns": "Tuple (image, target). target is a list of captions for the image.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.datasets.Flickr30k.__getitem__", "item_type": "method", "code": "__getitem__(index)", "description": " Parameters index (python:int) \u2013 Index  Returns Tuple (image, target). target is a list of captions for the image.  Return type tuple   ", "parameters": ["index (python:int) : Index", "Tuple (image, target). target is a list of captions for the image.", "tuple"], "returns": "Tuple (image, target). target is a list of captions for the image.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.datasets.VOCSegmentation.__getitem__", "item_type": "method", "code": "__getitem__(index)", "description": " Parameters index (python:int) \u2013 Index  Returns (image, target) where target is the image segmentation.  Return type tuple   ", "parameters": ["index (python:int) : Index", "(image, target) where target is the image segmentation.", "tuple"], "returns": "(image, target) where target is the image segmentation.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.datasets.VOCDetection.__getitem__", "item_type": "method", "code": "__getitem__(index)", "description": " Parameters index (python:int) \u2013 Index  Returns (image, target) where target is a dictionary of the XML tree.  Return type tuple   ", "parameters": ["index (python:int) : Index", "(image, target) where target is a dictionary of the XML tree.", "tuple"], "returns": "(image, target) where target is a dictionary of the XML tree.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.datasets.Cityscapes.__getitem__", "item_type": "method", "code": "__getitem__(index)", "description": " Parameters index (python:int) \u2013 Index  Returns (image, target) where target is a tuple of all target types if target_type is a list with more than one item. Otherwise target is a json object if target_type=\u201dpolygon\u201d, else the image segmentation.  Return type tuple   ", "parameters": ["index (python:int) : Index", "(image, target) where target is a tuple of all target types if target_type is a list with morethan one item. Otherwise target is a json object if target_type=\u201dpolygon\u201d, else the image segmentation.", "tuple"], "returns": "(image, target) where target is a tuple of all target types if target_type is a list with morethan one item. Otherwise target is a json object if target_type=\u201dpolygon\u201d, else the image segmentation.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.datasets.USPS.__getitem__", "item_type": "method", "code": "__getitem__(index)", "description": " Parameters index (python:int) \u2013 Index  Returns (image, target) where target is index of the target class.  Return type tuple   ", "parameters": ["index (python:int) : Index", "(image, target) where target is index of the target class.", "tuple"], "returns": "(image, target) where target is index of the target class.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.utils.cpp_extension.CppExtension", "item_type": "function", "code": "torch.utils.cpp_extension.CppExtension(name,sources,*args,**kwargs)", "description": "Creates a setuptools.Extension for C++. Convenience method that creates a setuptools.Extension with the bare minimum (but often sufficient) arguments to build a C++ extension. All arguments are forwarded to the setuptools.Extension constructor. Example &gt;&gt;&gt; from setuptools import setup &gt;&gt;&gt; from torch.utils.cpp_extension import BuildExtension, CppExtension &gt;&gt;&gt; setup(         name='extension',         ext_modules=[             CppExtension(                 name='extension',                 sources=['extension.cpp'],                 extra_compile_args=['-g']),         ],         cmdclass={             'build_ext': BuildExtension         })   ", "parameters": [], "returns": null, "example": " from setuptools import setup\n from torch.utils.cpp_extension import BuildExtension, CppExtension\n setup(\n        name='extension',\n        ext_modules=[\n            CppExtension(\n                name='extension',\n                sources=['extension.cpp'],\n                extra_compile_args=['-g']),\n        ],\n        cmdclass={\n            'build_ext': BuildExtension\n        })\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.utils.cpp_extension.CUDAExtension", "item_type": "function", "code": "torch.utils.cpp_extension.CUDAExtension(name,sources,*args,**kwargs)", "description": "Creates a setuptools.Extension for CUDA/C++. Convenience method that creates a setuptools.Extension with the bare minimum (but often sufficient) arguments to build a CUDA/C++ extension. This includes the CUDA include path, library path and runtime library. All arguments are forwarded to the setuptools.Extension constructor. Example &gt;&gt;&gt; from setuptools import setup &gt;&gt;&gt; from torch.utils.cpp_extension import BuildExtension, CUDAExtension &gt;&gt;&gt; setup(         name='cuda_extension',         ext_modules=[             CUDAExtension(                     name='cuda_extension',                     sources=['extension.cpp', 'extension_kernel.cu'],                     extra_compile_args={'cxx': ['-g'],                                         'nvcc': ['-O2']})         ],         cmdclass={             'build_ext': BuildExtension         })   ", "parameters": [], "returns": null, "example": " from setuptools import setup\n from torch.utils.cpp_extension import BuildExtension, CUDAExtension\n setup(\n        name='cuda_extension',\n        ext_modules=[\n            CUDAExtension(\n                    name='cuda_extension',\n                    sources=['extension.cpp', 'extension_kernel.cu'],\n                    extra_compile_args={'cxx': ['-g'],\n                                        'nvcc': ['-O2']})\n        ],\n        cmdclass={\n            'build_ext': BuildExtension\n        })\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.utils.cpp_extension.BuildExtension", "item_type": "function", "code": "torch.utils.cpp_extension.BuildExtension(*args,**kwargs)", "description": "A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the minimum required compiler flags (e.g. -std=c++11) as well as mixed C++/CUDA compilation (and support for CUDA files in general). When using BuildExtension, it is allowed to supply a dictionary for extra_compile_args (rather than the usual list) that maps from languages (cxx or nvcc) to a list of additional compiler flags to supply to the compiler. This makes it possible to supply different flags to the C++ and CUDA compiler during mixed compilation. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.utils.cpp_extension.load", "item_type": "function", "code": "torch.utils.cpp_extension.load(name,sources,extra_cflags=None,extra_cuda_cflags=None,extra_ldflags=None,extra_include_paths=None,build_directory=None,verbose=False,with_cuda=None,is_python_module=True)", "description": "Loads a PyTorch C++ extension just-in-time (JIT). To load an extension, a Ninja build file is emitted, which is used to compile the given sources into a dynamic library. This library is subsequently loaded into the current Python process as a module and returned from this function, ready for use. By default, the directory to which the build file is emitted and the resulting library compiled to is &lt;tmp&gt;/torch_extensions/&lt;name&gt;, where &lt;tmp&gt; is the temporary folder on the current platform and &lt;name&gt; the name of the extension. This location can be overridden in two ways. First, if the TORCH_EXTENSIONS_DIR environment variable is set, it replaces &lt;tmp&gt;/torch_extensions and all extensions will be compiled into subfolders of this directory. Second, if the build_directory argument to this function is supplied, it overrides the entire path, i.e. the library will be compiled into that folder directly. To compile the sources, the default system compiler (c++) is used, which can be overridden by setting the CXX environment variable. To pass additional arguments to the compilation process, extra_cflags or extra_ldflags can be provided. For example, to compile your extension with optimizations, pass extra_cflags=['-O3']. You can also use extra_cflags to pass further include directories. CUDA support with mixed compilation is provided. Simply pass CUDA source files (.cu or .cuh) along with other sources. Such files will be detected and compiled with nvcc rather than the C++ compiler. This includes passing the CUDA lib64 directory as a library directory, and linking cudart. You can pass additional flags to nvcc via extra_cuda_cflags, just like with extra_cflags for C++. Various heuristics for finding the CUDA install directory are used, which usually work fine. If not, setting the CUDA_HOME environment variable is the safest option.  Parameters  name \u2013 The name of the extension to build. This MUST be the same as the name of the pybind11 module! sources \u2013 A list of relative or absolute paths to C++ source files. extra_cflags \u2013 optional list of compiler flags to forward to the build. extra_cuda_cflags \u2013 optional list of compiler flags to forward to nvcc when building CUDA sources. extra_ldflags \u2013 optional list of linker flags to forward to the build. extra_include_paths \u2013 optional list of include directories to forward to the build. build_directory \u2013 optional path to use as build workspace. verbose \u2013 If True, turns on verbose logging of load steps. with_cuda \u2013 Determines whether CUDA headers and libraries are added to the build. If set to None (default), this value is automatically determined based on the existence of .cu or .cuh in sources. Set it to True` to force CUDA headers and libraries to be included. is_python_module \u2013 If True (default), imports the produced shared library as a Python module. If False, loads it into the process as a plain dynamic library.   Returns If is_python_module is True, returns the loaded PyTorch extension as a Python module. If is_python_module is False returns nothing (the shared library is loaded into the process as a side effect).   Example &gt;&gt;&gt; from torch.utils.cpp_extension import load &gt;&gt;&gt; module = load(         name='extension',         sources=['extension.cpp', 'extension_kernel.cu'],         extra_cflags=['-O2'],         verbose=True)   ", "parameters": ["name : The name of the extension to build. This MUST be the same as thename of the pybind11 module!", "sources : A list of relative or absolute paths to C++ source files.", "extra_cflags : optional list of compiler flags to forward to the build.", "extra_cuda_cflags : optional list of compiler flags to forward to nvccwhen building CUDA sources.", "extra_ldflags : optional list of linker flags to forward to the build.", "extra_include_paths : optional list of include directories to forwardto the build.", "build_directory : optional path to use as build workspace.", "verbose : If True, turns on verbose logging of load steps.", "with_cuda : Determines whether CUDA headers and libraries are added tothe build. If set to None (default), this value isautomatically determined based on the existence of .cu or.cuh in sources. Set it to True` to force CUDA headersand libraries to be included.", "is_python_module : If True (default), imports the produced sharedlibrary as a Python module. If False, loads it into the processas a plain dynamic library."], "returns": "If is_python_module is True, returns the loaded PyTorchextension as a Python module. If is_python_module is Falsereturns nothing (the shared library is loaded into the process as a sideeffect).", "example": " from torch.utils.cpp_extension import load\n module = load(\n        name='extension',\n        sources=['extension.cpp', 'extension_kernel.cu'],\n        extra_cflags=['-O2'],\n        verbose=True)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.models.resnet34", "item_type": "function", "code": "torchvision.models.resnet34(pretrained=False,progress=True,**kwargs)", "description": "ResNet-34 model from \u201cDeep Residual Learning for Image Recognition\u201d  Parameters  pretrained (bool) \u2013 If True, returns a model pre-trained on ImageNet progress (bool) \u2013 If True, displays a progress bar of the download to stderr    ", "parameters": ["pretrained (bool) : If True, returns a model pre-trained on ImageNet", "progress (bool) : If True, displays a progress bar of the download to stderr"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.models.resnet50", "item_type": "function", "code": "torchvision.models.resnet50(pretrained=False,progress=True,**kwargs)", "description": "ResNet-50 model from \u201cDeep Residual Learning for Image Recognition\u201d  Parameters  pretrained (bool) \u2013 If True, returns a model pre-trained on ImageNet progress (bool) \u2013 If True, displays a progress bar of the download to stderr    ", "parameters": ["pretrained (bool) : If True, returns a model pre-trained on ImageNet", "progress (bool) : If True, displays a progress bar of the download to stderr"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.models.resnet101", "item_type": "function", "code": "torchvision.models.resnet101(pretrained=False,progress=True,**kwargs)", "description": "ResNet-101 model from \u201cDeep Residual Learning for Image Recognition\u201d  Parameters  pretrained (bool) \u2013 If True, returns a model pre-trained on ImageNet progress (bool) \u2013 If True, displays a progress bar of the download to stderr    ", "parameters": ["pretrained (bool) : If True, returns a model pre-trained on ImageNet", "progress (bool) : If True, displays a progress bar of the download to stderr"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.models.resnet152", "item_type": "function", "code": "torchvision.models.resnet152(pretrained=False,progress=True,**kwargs)", "description": "ResNet-152 model from \u201cDeep Residual Learning for Image Recognition\u201d  Parameters  pretrained (bool) \u2013 If True, returns a model pre-trained on ImageNet progress (bool) \u2013 If True, displays a progress bar of the download to stderr    ", "parameters": ["pretrained (bool) : If True, returns a model pre-trained on ImageNet", "progress (bool) : If True, displays a progress bar of the download to stderr"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.models.squeezenet1_0", "item_type": "function", "code": "torchvision.models.squeezenet1_0(pretrained=False,progress=True,**kwargs)", "description": "SqueezeNet model architecture from the \u201cSqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB model size\u201d paper.  Parameters  pretrained (bool) \u2013 If True, returns a model pre-trained on ImageNet progress (bool) \u2013 If True, displays a progress bar of the download to stderr    ", "parameters": ["pretrained (bool) : If True, returns a model pre-trained on ImageNet", "progress (bool) : If True, displays a progress bar of the download to stderr"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.models.squeezenet1_1", "item_type": "function", "code": "torchvision.models.squeezenet1_1(pretrained=False,progress=True,**kwargs)", "description": "SqueezeNet 1.1 model from the official SqueezeNet repo. SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters than SqueezeNet 1.0, without sacrificing accuracy.  Parameters  pretrained (bool) \u2013 If True, returns a model pre-trained on ImageNet progress (bool) \u2013 If True, displays a progress bar of the download to stderr    ", "parameters": ["pretrained (bool) : If True, returns a model pre-trained on ImageNet", "progress (bool) : If True, displays a progress bar of the download to stderr"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.functional.adjust_gamma", "item_type": "function", "code": "torchvision.transforms.functional.adjust_gamma(img,gamma,gain=1)", "description": "Perform gamma correction on an image. Also known as Power Law Transform. Intensities in RGB mode are adjusted based on the following equation:  Iout=255\u00d7gain\u00d7(Iin255)\u03b3I_{\\text{out}} = 255 \\times \\text{gain} \\times \\left(\\frac{I_{\\text{in}}}{255}\\right)^{\\gamma}  Iout\u200b=255\u00d7gain\u00d7(255Iin\u200b\u200b)\u03b3  See Gamma Correction for more details.  Parameters  img (PIL Image) \u2013 PIL Image to be adjusted. gamma (python:float) \u2013 Non negative real number, same as \u03b3\\gamma\u03b3   in the equation. gamma larger than 1 make the shadows darker, while gamma smaller than 1 make dark regions lighter. gain (python:float) \u2013 The constant multiplier.    ", "parameters": ["img (PIL Image) : PIL Image to be adjusted.", "gamma (python:float) : Non negative real number, same as \u03b3\\gamma\u03b3 in the equation.gamma larger than 1 make the shadows darker,while gamma smaller than 1 make dark regions lighter.", "gain (python:float) : The constant multiplier."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.functional.adjust_hue", "item_type": "function", "code": "torchvision.transforms.functional.adjust_hue(img,hue_factor)", "description": "Adjust hue of an image. The image hue is adjusted by converting the image to HSV and cyclically shifting the intensities in the hue channel (H). The image is then converted back to original image mode. hue_factor is the amount of shift in H channel and must be in the interval [-0.5, 0.5]. See Hue for more details.  Parameters  img (PIL Image) \u2013 PIL Image to be adjusted. hue_factor (python:float) \u2013 How much to shift the hue channel. Should be in [-0.5, 0.5]. 0.5 and -0.5 give complete reversal of hue channel in HSV space in positive and negative direction respectively. 0 means no shift. Therefore, both -0.5 and 0.5 will give an image with complementary colors while 0 gives the original image.   Returns Hue adjusted image.  Return type PIL Image   ", "parameters": ["img (PIL Image) : PIL Image to be adjusted.", "hue_factor (python:float) : How much to shift the hue channel. Should be in[-0.5, 0.5]. 0.5 and -0.5 give complete reversal of hue channel inHSV space in positive and negative direction respectively.0 means no shift. Therefore, both -0.5 and 0.5 will give an imagewith complementary colors while 0 gives the original image."], "returns": "Hue adjusted image.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.functional.adjust_saturation", "item_type": "function", "code": "torchvision.transforms.functional.adjust_saturation(img,saturation_factor)", "description": "Adjust color saturation of an image.  Parameters  img (PIL Image) \u2013 PIL Image to be adjusted. saturation_factor (python:float) \u2013 How much to adjust the saturation. 0 will give a black and white image, 1 will give the original image while 2 will enhance the saturation by a factor of 2.   Returns Saturation adjusted image.  Return type PIL Image   ", "parameters": ["img (PIL Image) : PIL Image to be adjusted.", "saturation_factor (python:float) : How much to adjust the saturation. 0 willgive a black and white image, 1 will give the original image while2 will enhance the saturation by a factor of 2."], "returns": "Saturation adjusted image.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.functional.affine", "item_type": "function", "code": "torchvision.transforms.functional.affine(img,angle,translate,scale,shear,resample=0,fillcolor=None)", "description": "Apply affine transformation on the image keeping image center invariant  Parameters  img (PIL Image) \u2013 PIL Image to be rotated. angle (python:float or python:int) \u2013 rotation angle in degrees between -180 and 180, clockwise direction. translate (list or tuple of python:integers) \u2013 horizontal and vertical translations (post-rotation translation) scale (python:float) \u2013 overall scale shear (python:float or tuple or list) \u2013 shear angle value in degrees between -180 to 180, clockwise direction. a tuple of list is specified, the first value corresponds to a shear parallel to the x axis, while (If) \u2013  second value corresponds to a shear parallel to the y axis. (the) \u2013  resample (PIL.Image.NEAREST or PIL.Image.BILINEAR or PIL.Image.BICUBIC, optional) \u2013 An optional resampling filter. See filters for more information. If omitted, or if the image has mode \u201c1\u201d or \u201cP\u201d, it is set to PIL.Image.NEAREST. fillcolor (python:int) \u2013 Optional fill color for the area outside the transform in the output image. (Pillow&gt;=5.0.0)    ", "parameters": ["img (PIL Image) : PIL Image to be rotated.", "angle (python:float or python:int) : rotation angle in degrees between -180 and 180, clockwise direction.", "translate (list or tuple of python:integers) : horizontal and vertical translations (post-rotation translation)", "scale (python:float) : overall scale", "shear (python:float or tuple or list) : shear angle value in degrees between -180 to 180, clockwise direction.", "a tuple of list is specified, the first value corresponds to a shear parallel to the x axis, while (If) : ", "second value corresponds to a shear parallel to the y axis. (the) : ", "resample (PIL.Image.NEAREST or PIL.Image.BILINEAR or PIL.Image.BICUBIC, optional) : An optional resampling filter.See filters for more information.If omitted, or if the image has mode \u201c1\u201d or \u201cP\u201d, it is set to PIL.Image.NEAREST.", "fillcolor (python:int) : Optional fill color for the area outside the transform in the output image. (Pillow&gt;=5.0.0)"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.utils.checkpoint.checkpoint", "item_type": "function", "code": "torch.utils.checkpoint.checkpoint(function,*args,**kwargs)", "description": "Checkpoint a model or part of the model Checkpointing works by trading compute for memory. Rather than storing all intermediate activations of the entire computation graph for computing backward, the checkpointed part does not save intermediate activations, and instead recomputes them in backward pass. It can be applied on any part of a model. Specifically, in the forward pass, function will run in torch.no_grad() manner, i.e., not storing the intermediate activations. Instead, the forward pass saves the inputs tuple and the function parameter. In the backwards pass, the saved inputs and function is retreived, and the forward pass is computed on function again, now tracking the intermediate activations, and then the gradients are calculated using these activation values.  Warning Checkpointing doesn\u2019t work with torch.autograd.grad(), but only with torch.autograd.backward().   Warning If function invocation during backward does anything different than the one during forward, e.g., due to some global variable, the checkpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be detected.   Parameters  function \u2013 describes what to run in the forward pass of the model or part of the model. It should also know how to handle the inputs passed as the tuple. For example, in LSTM, if user passes (activation, hidden), function should correctly use the first input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring the RNG state during each checkpoint. args \u2013 tuple containing inputs to the function   Returns Output of running function on *args   ", "parameters": ["function : describes what to run in the forward pass of the model orpart of the model. It should also know how to handle the inputspassed as the tuple. For example, in LSTM, if user passes(activation, hidden), function should correctly use thefirst input as activation and the second input as hidden", "preserve_rng_state (bool, optional, default=True) : Omit stashing and restoringthe RNG state during each checkpoint.", "args : tuple containing inputs to the function"], "returns": "Output of running function on *args", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.utils.checkpoint.checkpoint_sequential", "item_type": "function", "code": "torch.utils.checkpoint.checkpoint_sequential(functions,segments,*inputs,**kwargs)", "description": "A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order (sequentially). Therefore, we can divide such a model in various segments and checkpoint each segment. All segments except the last will run in torch.no_grad() manner, i.e., not storing the intermediate activations. The inputs of each checkpointed segment will be saved for re-running the segment in the backward pass. See checkpoint() on how checkpointing works.  Warning Checkpointing doesn\u2019t work with torch.autograd.grad(), but only with torch.autograd.backward().   Parameters  functions \u2013 A torch.nn.Sequential or the list of modules or functions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model inputs \u2013 tuple of Tensors that are inputs to functions preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring the RNG state during each checkpoint.   Returns Output of running functions sequentially on *inputs   Example &gt;&gt;&gt; model = nn.Sequential(...) &gt;&gt;&gt; input_var = checkpoint_sequential(model, chunks, input_var)   ", "parameters": ["functions : A torch.nn.Sequential or the list of modules orfunctions (comprising the model) to run sequentially.", "segments : Number of chunks to create in the model", "inputs : tuple of Tensors that are inputs to functions", "preserve_rng_state (bool, optional, default=True) : Omit stashing and restoringthe RNG state during each checkpoint."], "returns": "Output of running functions sequentially on *inputs", "example": " model = nn.Sequential(...)\n input_var = checkpoint_sequential(model, chunks, input_var)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.utils.tensorboard.writer.SummaryWriter.__init__", "item_type": "method", "code": "__init__(log_dir=None,comment='',purge_step=None,max_queue=10,flush_secs=120,filename_suffix='')", "description": "Creates a SummaryWriter that will write out events and summaries to the event file.  Parameters  log_dir (string) \u2013 Save directory location. Default is runs/CURRENT_DATETIME_HOSTNAME, which changes after each run. Use hierarchical folder structure to compare between runs easily. e.g. pass in \u2018runs/exp1\u2019, \u2018runs/exp2\u2019, etc. for each new experiment to compare across them. comment (string) \u2013 Comment log_dir suffix appended to the default log_dir. If log_dir is assigned, this argument has no effect. purge_step (python:int) \u2013 When logging crashes at step T+XT+XT+X   and restarts at step TTT  , any events whose global_step larger or equal to TTT   will be purged and hidden from TensorBoard. Note that crashed and resumed experiments should have the same log_dir. max_queue (python:int) \u2013 Size of the queue for pending events and summaries before one of the \u2018add\u2019 calls forces a flush to disk. Default is ten items. flush_secs (python:int) \u2013 How often, in seconds, to flush the pending events and summaries to disk. Default is every two minutes. filename_suffix (string) \u2013 Suffix added to all event filenames in the log_dir directory. More details on filename construction in tensorboard.summary.writer.event_file_writer.EventFileWriter.    Examples: from torch.utils.tensorboard import SummaryWriter  # create a summary writer with automatically generated folder name. writer = SummaryWriter() # folder location: runs/May04_22-14-54_s-MacBook-Pro.local/  # create a summary writer using the specified folder name. writer = SummaryWriter(\"my_experiment\") # folder location: my_experiment  # create a summary writer with comment appended. writer = SummaryWriter(comment=\"LR_0.1_BATCH_16\") # folder location: runs/May04_22-14-54_s-MacBook-Pro.localLR_0.1_BATCH_16/   ", "parameters": ["log_dir (string) : Save directory location. Default isruns/CURRENT_DATETIME_HOSTNAME, which changes after each run.Use hierarchical folder structure to comparebetween runs easily. e.g. pass in \u2018runs/exp1\u2019, \u2018runs/exp2\u2019, etc.for each new experiment to compare across them.", "comment (string) : Comment log_dir suffix appended to the defaultlog_dir. If log_dir is assigned, this argument has no effect.", "purge_step (python:int) : When logging crashes at step T+XT+XT+X and restarts at step TTT,any events whose global_step larger or equal to TTT will bepurged and hidden from TensorBoard.Note that crashed and resumed experiments should have the same log_dir.", "max_queue (python:int) : Size of the queue for pending events andsummaries before one of the \u2018add\u2019 calls forces a flush to disk.Default is ten items.", "flush_secs (python:int) : How often, in seconds, to flush thepending events and summaries to disk. Default is every two minutes.", "filename_suffix (string) : Suffix added to all event filenames inthe log_dir directory. More details on filename construction intensorboard.summary.writer.event_file_writer.EventFileWriter."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.utils.model_zoo.load_url", "item_type": "function", "code": "torch.utils.model_zoo.load_url(url,model_dir=None,map_location=None,progress=True,check_hash=False)", "description": "Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically decompressed. If the object is already present in model_dir, it\u2019s deserialized and returned. The default value of model_dir is $TORCH_HOME/checkpoints where environment variable $TORCH_HOME defaults to $XDG_CACHE_HOME/torch. $XDG_CACHE_HOME follows the X Design Group specification of the Linux filesytem layout, with a default value ~/.cache if not set.  Parameters  url (string) \u2013 URL of the object to download model_dir (string, optional) \u2013 directory in which to save the object map_location (optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress (bool, optional) \u2013 whether or not to display a progress bar to stderr. Default: True check_hash (bool, optional) \u2013 If True, the filename part of the URL should follow the naming convention filename-&lt;sha256&gt;.ext where &lt;sha256&gt; is the first eight or more digits of the SHA256 hash of the contents of the file. The hash is used to ensure unique names and to verify the contents of the file. Default: False    Example &gt;&gt;&gt; state_dict = torch.hub.load_state_dict_from_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth')   ", "parameters": ["url (string) : URL of the object to download", "model_dir (string, optional) : directory in which to save the object", "map_location (optional) : a function or a dict specifying how to remap storage locations (see torch.load)", "progress (bool, optional) : whether or not to display a progress bar to stderr.Default: True", "check_hash (bool, optional) : If True, the filename part of the URL should follow the naming conventionfilename-&lt;sha256&gt;.ext where &lt;sha256&gt; is the first eight or moredigits of the SHA256 hash of the contents of the file. The hash is used toensure unique names and to verify the contents of the file.Default: False"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.utils.data.get_worker_info", "item_type": "function", "code": "torch.utils.data.get_worker_info()", "description": "Returns the information about the current DataLoader iterator worker process. When called in a worker, this returns an object guaranteed to have the following attributes:  id: the current worker id. num_workers: the total number of workers. seed: the random seed set for the current worker. This value is determined by main process RNG and the worker id. See DataLoader\u2019s documentation for more details. dataset: the copy of the dataset object in this process. Note that this will be a different object in a different process than the one in the main process.  When called in the main process, this returns None.  Note When used in a worker_init_fn passed over to DataLoader, this method can be useful to set up each worker process differently, for instance, using worker_id to configure the dataset object to only read a specific fraction of a sharded dataset, or use seed to seed other libraries used in dataset code (e.g., NumPy).  ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.utils.data.random_split", "item_type": "function", "code": "torch.utils.data.random_split(dataset,lengths)", "description": "Randomly split a dataset into non-overlapping new datasets of given lengths.  Parameters  dataset (Dataset) \u2013 Dataset to be split lengths (sequence) \u2013 lengths of splits to be produced    ", "parameters": ["dataset (Dataset) : Dataset to be split", "lengths (sequence) : lengths of splits to be produced"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.FloatStorage.bfloat16", "item_type": "method", "code": "bfloat16()", "description": "Casts this storage to bfloat16 type ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.FloatStorage.bool", "item_type": "method", "code": "bool()", "description": "Casts this storage to bool type ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.FloatStorage.byte", "item_type": "method", "code": "byte()", "description": "Casts this storage to byte type ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.FloatStorage.char", "item_type": "method", "code": "char()", "description": "Casts this storage to char type ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.FloatStorage.clone", "item_type": "method", "code": "clone()", "description": "Returns a copy of this storage ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.FloatStorage.copy_", "item_type": "method", "code": "copy_()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.FloatStorage.cpu", "item_type": "method", "code": "cpu()", "description": "Returns a CPU copy of this storage if it\u2019s not already on the CPU ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.FloatStorage.cuda", "item_type": "method", "code": "cuda(device=None,non_blocking=False,**kwargs)", "description": "Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then no copy is performed and the original object is returned.  Parameters  device (python:int) \u2013 The destination GPU id. Defaults to the current device. non_blocking (bool) \u2013 If True and the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, the argument has no effect. **kwargs \u2013 For compatibility, may contain the key async in place of the non_blocking argument.    ", "parameters": ["device (python:int) : The destination GPU id. Defaults to the current device.", "non_blocking (bool) : If True and the source is in pinned memory,the copy will be asynchronous with respect to the host. Otherwise,the argument has no effect.", "**kwargs : For compatibility, may contain the key async in place ofthe non_blocking argument."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.FloatStorage.data_ptr", "item_type": "method", "code": "data_ptr()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.FloatStorage.double", "item_type": "method", "code": "double()", "description": "Casts this storage to double type ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.refine_names", "item_type": "method", "code": "refine_names(*names)", "description": "Refines the dimension names of self according to names. Refining is a special case of renaming that \u201clifts\u201d unnamed dimensions. A None dim can be refined to have any name; a named dim can only be refined to have the same name. Because named tensors can coexist with unnamed tensors, refining names gives a nice way to write named-tensor-aware code that works with both named and unnamed tensors. names may contain up to one Ellipsis (...). The Ellipsis is expanded greedily; it is expanded in-place to fill names to the same length as self.dim() using names from the corresponding indices of self.names. Python 2 does not support Ellipsis but one may use a string literal instead ('...').  Parameters names (iterable of str) \u2013 The desired names of the output tensor. May contain up to one Ellipsis.   Examples: &gt;&gt;&gt; imgs = torch.randn(32, 3, 128, 128) &gt;&gt;&gt; named_imgs = imgs.refine_names('N', 'C', 'H', 'W') &gt;&gt;&gt; named_imgs.names ('N', 'C', 'H', 'W')  &gt;&gt;&gt; tensor = torch.randn(2, 3, 5, 7, 11) &gt;&gt;&gt; tensor = tensor.refine_names('A', ..., 'B', 'C') &gt;&gt;&gt; tensor.names ('A', None, None, 'B', 'C')    Warning The named tensor API is experimental and subject to change.  ", "parameters": ["names (iterable of str) : The desired names of the output tensor. Maycontain up to one Ellipsis."], "returns": null, "example": " imgs = torch.randn(32, 3, 128, 128)\n named_imgs = imgs.refine_names('N', 'C', 'H', 'W')\n named_imgs.names\n('N', 'C', 'H', 'W')\n\n tensor = torch.randn(2, 3, 5, 7, 11)\n tensor = tensor.refine_names('A', ..., 'B', 'C')\n tensor.names\n('A', None, None, 'B', 'C')\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.align_as", "item_type": "method", "code": "align_as(other)\u2192Tensor", "description": "Permutes the dimensions of the self tensor to match the dimension order in the other tensor, adding size-one dims for any new names. This operation is useful for explicit broadcasting by names (see examples). All of the dims of self must be named in order to use this method. The resulting tensor is a view on the original tensor. All dimension names of self must be present in other.names. other may contain named dimensions that are not in self.names; the output tensor has a size-one dimension for each of those new names. To align a tensor to a specific order, use align_to(). Examples: # Example 1: Applying a mask &gt;&gt;&gt; mask = torch.randint(2, [127, 128], dtype=torch.bool).refine_names('W', 'H') &gt;&gt;&gt; imgs = torch.randn(32, 128, 127, 3, names=('N', 'H', 'W', 'C')) &gt;&gt;&gt; imgs.masked_fill_(mask.align_as(imgs), 0)   # Example 2: Applying a per-channel-scale def scale_channels(input, scale):     scale = scale.refine_names('C')     return input * scale.align_as(input)  &gt;&gt;&gt; num_channels = 3 &gt;&gt;&gt; scale = torch.randn(num_channels, names=('C',)) &gt;&gt;&gt; imgs = torch.rand(32, 128, 128, num_channels, names=('N', 'H', 'W', 'C')) &gt;&gt;&gt; more_imgs = torch.rand(32, num_channels, 128, 128, names=('N', 'C', 'H', 'W')) &gt;&gt;&gt; videos = torch.randn(3, num_channels, 128, 128, 128, names=('N', 'C', 'H', 'W', 'D'))  # scale_channels is agnostic to the dimension order of the input &gt;&gt;&gt; scale_channels(imgs, scale) &gt;&gt;&gt; scale_channels(more_imgs, scale) &gt;&gt;&gt; scale_channels(videos, scale)    Warning The named tensor API is experimental and subject to change.  ", "parameters": [], "returns": null, "example": "# Example 1: Applying a mask\n mask = torch.randint(2, [127, 128], dtype=torch.bool).refine_names('W', 'H')\n imgs = torch.randn(32, 128, 127, 3, names=('N', 'H', 'W', 'C'))\n imgs.masked_fill_(mask.align_as(imgs), 0)\n\n\n# Example 2: Applying a per-channel-scale\ndef scale_channels(input, scale):\n    scale = scale.refine_names('C')\n    return input * scale.align_as(input)\n\n num_channels = 3\n scale = torch.randn(num_channels, names=('C',))\n imgs = torch.rand(32, 128, 128, num_channels, names=('N', 'H', 'W', 'C'))\n more_imgs = torch.rand(32, num_channels, 128, 128, names=('N', 'C', 'H', 'W'))\n videos = torch.randn(3, num_channels, 128, 128, 128, names=('N', 'C', 'H', 'W', 'D'))\n\n# scale_channels is agnostic to the dimension order of the input\n scale_channels(imgs, scale)\n scale_channels(more_imgs, scale)\n scale_channels(videos, scale)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.align_to", "item_type": "method", "code": "align_to(*names)", "description": "Permutes the dimensions of the self tensor to match the order specified in names, adding size-one dims for any new names. All of the dims of self must be named in order to use this method. The resulting tensor is a view on the original tensor. All dimension names of self must be present in names. names may contain additional names that are not in self.names; the output tensor has a size-one dimension for each of those new names. names may contain up to one Ellipsis (...). The Ellipsis is expanded to be equal to all dimension names of self that are not mentioned in names, in the order that they appear in self. Python 2 does not support Ellipsis but one may use a string literal instead ('...').  Parameters names (iterable of str) \u2013 The desired dimension ordering of the output tensor. May contain up to one Ellipsis that is expanded to all unmentioned dim names of self.   Examples: &gt;&gt;&gt; tensor = torch.randn(2, 2, 2, 2, 2, 2) &gt;&gt;&gt; named_tensor = tensor.refine_names('A', 'B', 'C', 'D', 'E', 'F')  # Move the F and E dims to the front while keeping the rest in order &gt;&gt;&gt; named_tensor.align_to('F', 'E', ...)    Warning The named tensor API is experimental and subject to change.  ", "parameters": ["names (iterable of str) : The desired dimension ordering of theoutput tensor. May contain up to one Ellipsis that is expandedto all unmentioned dim names of self."], "returns": null, "example": " tensor = torch.randn(2, 2, 2, 2, 2, 2)\n named_tensor = tensor.refine_names('A', 'B', 'C', 'D', 'E', 'F')\n\n# Move the F and E dims to the front while keeping the rest in order\n named_tensor.align_to('F', 'E', ...)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.unflatten", "item_type": "method", "code": "unflatten(dim,namedshape)", "description": "Unflattens the named dimension dim, viewing it in the shape specified by namedshape.  Parameters namedshape \u2013 (iterable of (name, size) tuples).   Examples: &gt;&gt;&gt; flat_imgs = torch.rand(32, 3 * 128 * 128, names=('N', 'features')) &gt;&gt;&gt; imgs = flat_imgs.unflatten('features', (('C', 3), ('H', 128), ('W', 128))) &gt;&gt;&gt; imgs.names, images.shape (('N', 'C', 'H', 'W'), torch.Size([32, 3, 128, 128]))    Warning The named tensor API is experimental and subject to change.  ", "parameters": ["namedshape : (iterable of (name, size) tuples)."], "returns": null, "example": " flat_imgs = torch.rand(32, 3 * 128 * 128, names=('N', 'features'))\n imgs = flat_imgs.unflatten('features', (('C', 3), ('H', 128), ('W', 128)))\n imgs.names, images.shape\n(('N', 'C', 'H', 'W'), torch.Size([32, 3, 128, 128]))\n\n", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "method", "code": "flatten(dims,out_dim)\u2192Tensor", "description": "Flattens dims into a single dimension with name out_dim. All of dims must be consecutive in order in the self tensor, but not necessary contiguous in memory. Examples: &gt;&gt;&gt; imgs = torch.randn(32, 3, 128, 128, names=('N', 'C', 'H', 'W')) &gt;&gt;&gt; flat_imgs = imgs.flatten(['C', 'H', 'W'], 'features') &gt;&gt;&gt; flat_imgs.names, flat_imgs.shape (('N', 'features'), torch.Size([32, 49152]))    Warning The named tensor API is experimental and subject to change.  ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.datasets.MNIST", "item_type": "class", "code": "classtorchvision.datasets.MNIST(root,train=True,transform=None,target_transform=None,download=False)", "description": "MNIST Dataset.  Parameters  root (string) \u2013 Root directory of dataset where MNIST/processed/training.pt and  MNIST/processed/test.pt exist. train (bool, optional) \u2013 If True, creates dataset from training.pt, otherwise from test.pt. download (bool, optional) \u2013 If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again. transform (callable, optional) \u2013 A function/transform that  takes in an PIL image and returns a transformed version. E.g, transforms.RandomCrop target_transform (callable, optional) \u2013 A function/transform that takes in the target and transforms it.    ", "parameters": ["root (string) : Root directory of dataset where MNIST/processed/training.ptand  MNIST/processed/test.pt exist.", "train (bool, optional) : If True, creates dataset from training.pt,otherwise from test.pt.", "download (bool, optional) : If true, downloads the dataset from the internet andputs it in root directory. If dataset is already downloaded, it is notdownloaded again.", "transform (callable, optional) : A function/transform that  takes in an PIL imageand returns a transformed version. E.g, transforms.RandomCrop", "target_transform (callable, optional) : A function/transform that takes in thetarget and transforms it."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.datasets.FashionMNIST", "item_type": "class", "code": "classtorchvision.datasets.FashionMNIST(root,train=True,transform=None,target_transform=None,download=False)", "description": "Fashion-MNIST Dataset.  Parameters  root (string) \u2013 Root directory of dataset where Fashion-MNIST/processed/training.pt and  Fashion-MNIST/processed/test.pt exist. train (bool, optional) \u2013 If True, creates dataset from training.pt, otherwise from test.pt. download (bool, optional) \u2013 If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again. transform (callable, optional) \u2013 A function/transform that  takes in an PIL image and returns a transformed version. E.g, transforms.RandomCrop target_transform (callable, optional) \u2013 A function/transform that takes in the target and transforms it.    ", "parameters": ["root (string) : Root directory of dataset where Fashion-MNIST/processed/training.ptand  Fashion-MNIST/processed/test.pt exist.", "train (bool, optional) : If True, creates dataset from training.pt,otherwise from test.pt.", "download (bool, optional) : If true, downloads the dataset from the internet andputs it in root directory. If dataset is already downloaded, it is notdownloaded again.", "transform (callable, optional) : A function/transform that  takes in an PIL imageand returns a transformed version. E.g, transforms.RandomCrop", "target_transform (callable, optional) : A function/transform that takes in thetarget and transforms it."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.datasets.KMNIST", "item_type": "class", "code": "classtorchvision.datasets.KMNIST(root,train=True,transform=None,target_transform=None,download=False)", "description": "Kuzushiji-MNIST Dataset.  Parameters  root (string) \u2013 Root directory of dataset where KMNIST/processed/training.pt and  KMNIST/processed/test.pt exist. train (bool, optional) \u2013 If True, creates dataset from training.pt, otherwise from test.pt. download (bool, optional) \u2013 If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again. transform (callable, optional) \u2013 A function/transform that  takes in an PIL image and returns a transformed version. E.g, transforms.RandomCrop target_transform (callable, optional) \u2013 A function/transform that takes in the target and transforms it.    ", "parameters": ["root (string) : Root directory of dataset where KMNIST/processed/training.ptand  KMNIST/processed/test.pt exist.", "train (bool, optional) : If True, creates dataset from training.pt,otherwise from test.pt.", "download (bool, optional) : If true, downloads the dataset from the internet andputs it in root directory. If dataset is already downloaded, it is notdownloaded again.", "transform (callable, optional) : A function/transform that  takes in an PIL imageand returns a transformed version. E.g, transforms.RandomCrop", "target_transform (callable, optional) : A function/transform that takes in thetarget and transforms it."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.datasets.EMNIST", "item_type": "class", "code": "classtorchvision.datasets.EMNIST(root,split,**kwargs)", "description": "EMNIST Dataset.  Parameters  root (string) \u2013 Root directory of dataset where EMNIST/processed/training.pt and  EMNIST/processed/test.pt exist. split (string) \u2013 The dataset has 6 different splits: byclass, bymerge, balanced, letters, digits and mnist. This argument specifies which one to use. train (bool, optional) \u2013 If True, creates dataset from training.pt, otherwise from test.pt. download (bool, optional) \u2013 If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again. transform (callable, optional) \u2013 A function/transform that  takes in an PIL image and returns a transformed version. E.g, transforms.RandomCrop target_transform (callable, optional) \u2013 A function/transform that takes in the target and transforms it.    ", "parameters": ["root (string) : Root directory of dataset where EMNIST/processed/training.ptand  EMNIST/processed/test.pt exist.", "split (string) : The dataset has 6 different splits: byclass, bymerge,balanced, letters, digits and mnist. This argument specifieswhich one to use.", "train (bool, optional) : If True, creates dataset from training.pt,otherwise from test.pt.", "download (bool, optional) : If true, downloads the dataset from the internet andputs it in root directory. If dataset is already downloaded, it is notdownloaded again.", "transform (callable, optional) : A function/transform that  takes in an PIL imageand returns a transformed version. E.g, transforms.RandomCrop", "target_transform (callable, optional) : A function/transform that takes in thetarget and transforms it."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.utils.cpp_extension.load_inline", "item_type": "function", "code": "torch.utils.cpp_extension.load_inline(name,cpp_sources,cuda_sources=None,functions=None,extra_cflags=None,extra_cuda_cflags=None,extra_ldflags=None,extra_include_paths=None,build_directory=None,verbose=False,with_cuda=None,is_python_module=True,with_pytorch_error_handling=True)", "description": "Loads a PyTorch C++ extension just-in-time (JIT) from string sources. This function behaves exactly like load(), but takes its sources as strings rather than filenames. These strings are stored to files in the build directory, after which the behavior of load_inline() is identical to load(). See the tests for good examples of using this function. Sources may omit two required parts of a typical non-inline C++ extension: the necessary header includes, as well as the (pybind11) binding code. More precisely, strings passed to cpp_sources are first concatenated into a single .cpp file. This file is then prepended with #include &lt;torch/extension.h&gt;. Furthermore, if the functions argument is supplied, bindings will be automatically generated for each function specified. functions can either be a list of function names, or a dictionary mapping from function names to docstrings. If a list is given, the name of each function is used as its docstring. The sources in cuda_sources are concatenated into a separate .cu file and  prepended with torch/types.h, cuda.h and cuda_runtime.h includes. The .cpp and .cu files are compiled separately, but ultimately linked into a single library. Note that no bindings are generated for functions in cuda_sources per  se. To bind to a CUDA kernel, you must create a C++ function that calls it, and either declare or define this C++ function in one of the cpp_sources (and include its name in functions). See load() for a description of arguments omitted below.  Parameters  cpp_sources \u2013 A string, or list of strings, containing C++ source code. cuda_sources \u2013 A string, or list of strings, containing CUDA source code. functions \u2013 A list of function names for which to generate function bindings. If a dictionary is given, it should map function names to docstrings (which are otherwise just the function names). with_cuda \u2013 Determines whether CUDA headers and libraries are added to the build. If set to None (default), this value is automatically determined based on whether cuda_sources is provided. Set it to True to force CUDA headers and libraries to be included. with_pytorch_error_handling \u2013 Determines whether pytorch error and warning macros are handled by pytorch instead of pybind. To do this, each function foo is called via an intermediary _safe_foo function. This redirection might cause issues in obscure cases of cpp. This flag should be set to False when this redirect causes issues.    Example &gt;&gt;&gt; from torch.utils.cpp_extension import load_inline &gt;&gt;&gt; source = ''' at::Tensor sin_add(at::Tensor x, at::Tensor y) {   return x.sin() + y.sin(); } ''' &gt;&gt;&gt; module = load_inline(name='inline_extension',                          cpp_sources= ,                          functions=['sin_add'])   ", "parameters": ["cpp_sources : A string, or list of strings, containing C++ source code.", "cuda_sources : A string, or list of strings, containing CUDA source code.", "functions : A list of function names for which to generate functionbindings. If a dictionary is given, it should map function names todocstrings (which are otherwise just the function names).", "with_cuda : Determines whether CUDA headers and libraries are added tothe build. If set to None (default), this value isautomatically determined based on whether cuda_sources isprovided. Set it to True to force CUDA headersand libraries to be included.", "with_pytorch_error_handling : Determines whether pytorch error andwarning macros are handled by pytorch instead of pybind. To dothis, each function foo is called via an intermediary _safe_foofunction. This redirection might cause issues in obscure casesof cpp. This flag should be set to False when this redirectcauses issues."], "returns": null, "example": " from torch.utils.cpp_extension import load_inline\n source = '''\nat::Tensor sin_add(at::Tensor x, at::Tensor y) {\n  return x.sin() + y.sin();\n}\n'''\n module = load_inline(name='inline_extension',\n                         cpp_sources=[source],\n                         functions=['sin_add'])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.utils.cpp_extension.include_paths", "item_type": "function", "code": "torch.utils.cpp_extension.include_paths(cuda=False)", "description": "Get the include paths required to build a C++ or CUDA extension.  Parameters cuda \u2013 If True, includes CUDA-specific include paths.  Returns A list of include path strings.   ", "parameters": ["cuda : If True, includes CUDA-specific include paths.", "A list of include path strings."], "returns": "A list of include path strings.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.utils.cpp_extension.check_compiler_abi_compatibility", "item_type": "function", "code": "torch.utils.cpp_extension.check_compiler_abi_compatibility(compiler)", "description": "Verifies that the given compiler is ABI-compatible with PyTorch.  Parameters compiler (str) \u2013 The compiler executable name to check (e.g. g++). Must be executable in a shell process.  Returns False if the compiler is (likely) ABI-incompatible with PyTorch, else True.   ", "parameters": ["compiler (str) : The compiler executable name to check (e.g. g++).Must be executable in a shell process.", "False if the compiler is (likely) ABI-incompatible with PyTorch,else True."], "returns": "False if the compiler is (likely) ABI-incompatible with PyTorch,else True.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.models.densenet121", "item_type": "function", "code": "torchvision.models.densenet121(pretrained=False,progress=True,**kwargs)", "description": "Densenet-121 model from \u201cDensely Connected Convolutional Networks\u201d  Parameters  pretrained (bool) \u2013 If True, returns a model pre-trained on ImageNet progress (bool) \u2013 If True, displays a progress bar of the download to stderr memory_efficient (bool) \u2013 but slower. Default: False. See \u201cpaper\u201d    ", "parameters": ["pretrained (bool) : If True, returns a model pre-trained on ImageNet", "progress (bool) : If True, displays a progress bar of the download to stderr", "memory_efficient (bool) : but slower. Default: False. See \u201cpaper\u201d"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.models.densenet169", "item_type": "function", "code": "torchvision.models.densenet169(pretrained=False,progress=True,**kwargs)", "description": "Densenet-169 model from \u201cDensely Connected Convolutional Networks\u201d  Parameters  pretrained (bool) \u2013 If True, returns a model pre-trained on ImageNet progress (bool) \u2013 If True, displays a progress bar of the download to stderr memory_efficient (bool) \u2013 but slower. Default: False. See \u201cpaper\u201d     ", "parameters": ["pretrained (bool) : If True, returns a model pre-trained on ImageNet", "progress (bool) : If True, displays a progress bar of the download to stderr", "memory_efficient (bool) : but slower. Default: False. See \u201cpaper\u201d"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.models.densenet161", "item_type": "function", "code": "torchvision.models.densenet161(pretrained=False,progress=True,**kwargs)", "description": "Densenet-161 model from \u201cDensely Connected Convolutional Networks\u201d  Parameters  pretrained (bool) \u2013 If True, returns a model pre-trained on ImageNet progress (bool) \u2013 If True, displays a progress bar of the download to stderr memory_efficient (bool) \u2013 but slower. Default: False. See \u201cpaper\u201d     ", "parameters": ["pretrained (bool) : If True, returns a model pre-trained on ImageNet", "progress (bool) : If True, displays a progress bar of the download to stderr", "memory_efficient (bool) : but slower. Default: False. See \u201cpaper\u201d"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.models.densenet201", "item_type": "function", "code": "torchvision.models.densenet201(pretrained=False,progress=True,**kwargs)", "description": "Densenet-201 model from \u201cDensely Connected Convolutional Networks\u201d  Parameters  pretrained (bool) \u2013 If True, returns a model pre-trained on ImageNet progress (bool) \u2013 If True, displays a progress bar of the download to stderr memory_efficient (bool) \u2013 but slower. Default: False. See \u201cpaper\u201d     ", "parameters": ["pretrained (bool) : If True, returns a model pre-trained on ImageNet", "progress (bool) : If True, displays a progress bar of the download to stderr", "memory_efficient (bool) : but slower. Default: False. See \u201cpaper\u201d"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.models.inception_v3", "item_type": "function", "code": "torchvision.models.inception_v3(pretrained=False,progress=True,**kwargs)", "description": "Inception v3 model architecture from \u201cRethinking the Inception Architecture for Computer Vision\u201d.  Note Important: In contrast to the other models the inception_v3 expects tensors with a size of N x 3 x 299 x 299, so ensure your images are sized accordingly.   Parameters  pretrained (bool) \u2013 If True, returns a model pre-trained on ImageNet progress (bool) \u2013 If True, displays a progress bar of the download to stderr aux_logits (bool) \u2013 If True, add an auxiliary branch that can improve training. Default: True transform_input (bool) \u2013 If True, preprocesses the input according to the method with which it was trained on ImageNet. Default: False    ", "parameters": ["pretrained (bool) : If True, returns a model pre-trained on ImageNet", "progress (bool) : If True, displays a progress bar of the download to stderr", "aux_logits (bool) : If True, add an auxiliary branch that can improve training.Default: True", "transform_input (bool) : If True, preprocesses the input according to the method with which itwas trained on ImageNet. Default: False"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.models.googlenet", "item_type": "function", "code": "torchvision.models.googlenet(pretrained=False,progress=True,**kwargs)", "description": "GoogLeNet (Inception v1) model architecture from \u201cGoing Deeper with Convolutions\u201d.  Parameters  pretrained (bool) \u2013 If True, returns a model pre-trained on ImageNet progress (bool) \u2013 If True, displays a progress bar of the download to stderr aux_logits (bool) \u2013 If True, adds two auxiliary branches that can improve training. Default: False when pretrained is True otherwise True transform_input (bool) \u2013 If True, preprocesses the input according to the method with which it was trained on ImageNet. Default: False    ", "parameters": ["pretrained (bool) : If True, returns a model pre-trained on ImageNet", "progress (bool) : If True, displays a progress bar of the download to stderr", "aux_logits (bool) : If True, adds two auxiliary branches that can improve training.Default: False when pretrained is True otherwise True", "transform_input (bool) : If True, preprocesses the input according to the method with which itwas trained on ImageNet. Default: False"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.models.shufflenet_v2_x0_5", "item_type": "function", "code": "torchvision.models.shufflenet_v2_x0_5(pretrained=False,progress=True,**kwargs)", "description": "Constructs a ShuffleNetV2 with 0.5x output channels, as described in \u201cShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design\u201d.  Parameters  pretrained (bool) \u2013 If True, returns a model pre-trained on ImageNet progress (bool) \u2013 If True, displays a progress bar of the download to stderr    ", "parameters": ["pretrained (bool) : If True, returns a model pre-trained on ImageNet", "progress (bool) : If True, displays a progress bar of the download to stderr"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.functional.center_crop", "item_type": "function", "code": "torchvision.transforms.functional.center_crop(img,output_size)", "description": "Crop the given PIL Image and resize it to desired size.  Parameters  img (PIL Image) \u2013 Image to be cropped. (0,0) denotes the top left corner of the image. output_size (sequence or python:int) \u2013 (height, width) of the crop box. If int, it is used for both directions   Returns Cropped image.  Return type PIL Image   ", "parameters": ["img (PIL Image) : Image to be cropped. (0,0) denotes the top left corner of the image.", "output_size (sequence or python:int) : (height, width) of the crop box. If int,it is used for both directions"], "returns": "Cropped image.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.functional.crop", "item_type": "function", "code": "torchvision.transforms.functional.crop(img,top,left,height,width)", "description": "Crop the given PIL Image. :param img: Image to be cropped. (0,0) denotes the top left corner of the image. :type img: PIL Image :param top: Vertical component of the top left corner of the crop box. :type top: int :param left: Horizontal component of the top left corner of the crop box. :type left: int :param height: Height of the crop box. :type height: int :param width: Width of the crop box. :type width: int  Returns Cropped image.  Return type PIL Image   ", "parameters": [], "returns": "Cropped image.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.functional.erase", "item_type": "function", "code": "torchvision.transforms.functional.erase(img,i,j,h,w,v,inplace=False)", "description": "Erase the input Tensor Image with given value.  Parameters  img (Tensor Image) \u2013 Tensor image of size (C, H, W) to be erased i (python:int) \u2013 i in (i,j) i.e coordinates of the upper left corner. j (python:int) \u2013 j in (i,j) i.e coordinates of the upper left corner. h (python:int) \u2013 Height of the erased region. w (python:int) \u2013 Width of the erased region. v \u2013 Erasing value. inplace (bool, optional) \u2013 For in-place operations. By default is set False.   Returns Erased image.  Return type Tensor Image   ", "parameters": ["img (Tensor Image) : Tensor image of size (C, H, W) to be erased", "i (python:int) : i in (i,j) i.e coordinates of the upper left corner.", "j (python:int) : j in (i,j) i.e coordinates of the upper left corner.", "h (python:int) : Height of the erased region.", "w (python:int) : Width of the erased region.", "v : Erasing value.", "inplace (bool, optional) : For in-place operations. By default is set False."], "returns": "Erased image.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.functional.five_crop", "item_type": "function", "code": "torchvision.transforms.functional.five_crop(img,size)", "description": "Crop the given PIL Image into four corners and the central crop.  Note This transform returns a tuple of images and there may be a mismatch in the number of inputs and targets your Dataset returns.   Parameters size (sequence or python:int) \u2013 Desired output size of the crop. If size is an int instead of sequence like (h, w), a square crop (size, size) is made.  Returns  tuple (tl, tr, bl, br, center)Corresponding top left, top right, bottom left, bottom right and center crop.     Return type tuple   ", "parameters": ["size (sequence or python:int) : Desired output size of the crop. If size is anint instead of sequence like (h, w), a square crop (size, size) ismade.", "tuple (tl, tr, bl, br, center)Corresponding top left, top right, bottom left, bottom right and center crop.", "Corresponding top left, top right, bottom left, bottom right and center crop.", "tuple"], "returns": "tuple (tl, tr, bl, br, center)Corresponding top left, top right, bottom left, bottom right and center crop.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.functional.hflip", "item_type": "function", "code": "torchvision.transforms.functional.hflip(img)", "description": "Horizontally flip the given PIL Image.  Parameters img (PIL Image) \u2013 Image to be flipped.  Returns Horizontall flipped image.  Return type PIL Image   ", "parameters": ["img (PIL Image) : Image to be flipped.", "Horizontall flipped image.", "PIL Image"], "returns": "Horizontall flipped image.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.functional.normalize", "item_type": "function", "code": "torchvision.transforms.functional.normalize(tensor,mean,std,inplace=False)", "description": "Normalize a tensor image with mean and standard deviation.  Note This transform acts out of place by default, i.e., it does not mutates the input tensor.  See Normalize for more details.  Parameters  tensor (Tensor) \u2013 Tensor image of size (C, H, W) to be normalized. mean (sequence) \u2013 Sequence of means for each channel. std (sequence) \u2013 Sequence of standard deviations for each channel. inplace (bool,optional) \u2013 Bool to make this operation inplace.   Returns Normalized Tensor image.  Return type Tensor   ", "parameters": ["tensor (Tensor) : Tensor image of size (C, H, W) to be normalized.", "mean (sequence) : Sequence of means for each channel.", "std (sequence) : Sequence of standard deviations for each channel.", "inplace (bool,optional) : Bool to make this operation inplace."], "returns": "Normalized Tensor image.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.functional.pad", "item_type": "function", "code": "torchvision.transforms.functional.pad(img,padding,fill=0,padding_mode='constant')", "description": "Pad the given PIL Image on all sides with specified padding mode and fill value.  Parameters  img (PIL Image) \u2013 Image to be padded. padding (python:int or tuple) \u2013 Padding on each border. If a single int is provided this is used to pad all borders. If tuple of length 2 is provided this is the padding on left/right and top/bottom respectively. If a tuple of length 4 is provided this is the padding for the left, top, right and bottom borders respectively. fill \u2013 Pixel fill value for constant fill. Default is 0. If a tuple of length 3, it is used to fill R, G, B channels respectively. This value is only used when the padding_mode is constant padding_mode \u2013 Type of padding. Should be: constant, edge, reflect or symmetric. Default is constant.  constant: pads with a constant value, this value is specified with fill edge: pads with the last value on the edge of the image reflect: pads with reflection of image (without repeating the last value on the edge)  padding [1, 2, 3, 4] with 2 elements on both sides in reflect mode will result in [3, 2, 1, 2, 3, 4, 3, 2]   symmetric: pads with reflection of image (repeating the last value on the edge)  padding [1, 2, 3, 4] with 2 elements on both sides in symmetric mode will result in [2, 1, 1, 2, 3, 4, 4, 3]       Returns Padded image.  Return type PIL Image   ", "parameters": ["img (PIL Image) : Image to be padded.", "padding (python:int or tuple) : Padding on each border. If a single int is provided thisis used to pad all borders. If tuple of length 2 is provided this is the paddingon left/right and top/bottom respectively. If a tuple of length 4 is providedthis is the padding for the left, top, right and bottom bordersrespectively.", "fill : Pixel fill value for constant fill. Default is 0. If a tuple oflength 3, it is used to fill R, G, B channels respectively.This value is only used when the padding_mode is constant", "padding_mode : Type of padding. Should be: constant, edge, reflect or symmetric. Default is constant.constant: pads with a constant value, this value is specified with filledge: pads with the last value on the edge of the imagereflect: pads with reflection of image (without repeating the last value on the edge)padding [1, 2, 3, 4] with 2 elements on both sides in reflect modewill result in [3, 2, 1, 2, 3, 4, 3, 2]symmetric: pads with reflection of image (repeating the last value on the edge)padding [1, 2, 3, 4] with 2 elements on both sides in symmetric modewill result in [2, 1, 1, 2, 3, 4, 4, 3]", "constant: pads with a constant value, this value is specified with fill", "edge: pads with the last value on the edge of the image", "reflect: pads with reflection of image (without repeating the last value on the edge)padding [1, 2, 3, 4] with 2 elements on both sides in reflect modewill result in [3, 2, 1, 2, 3, 4, 3, 2]", "symmetric: pads with reflection of image (repeating the last value on the edge)padding [1, 2, 3, 4] with 2 elements on both sides in symmetric modewill result in [2, 1, 1, 2, 3, 4, 4, 3]"], "returns": "Padded image.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.utils.tensorboard.writer.SummaryWriter.add_scalar", "item_type": "method", "code": "add_scalar(tag,scalar_value,global_step=None,walltime=None)", "description": "Add scalar data to summary.  Parameters  tag (string) \u2013 Data identifier scalar_value (python:float or string/blobname) \u2013 Value to save global_step (python:int) \u2013 Global step value to record walltime (python:float) \u2013 Optional override default walltime (time.time()) with seconds after epoch of event    Examples: from torch.utils.tensorboard import SummaryWriter writer = SummaryWriter() x = range(100) for i in x:     writer.add_scalar('y=2x', i * 2, i) writer.close()   Expected result:  ", "parameters": ["tag (string) : Data identifier", "scalar_value (python:float or string/blobname) : Value to save", "global_step (python:int) : Global step value to record", "walltime (python:float) : Optional override default walltime (time.time())with seconds after epoch of event"], "returns": null, "example": "from torch.utils.tensorboard import SummaryWriter\nwriter = SummaryWriter()\nx = range(100)\nfor i in x:\n    writer.add_scalar('y=2x', i * 2, i)\nwriter.close()\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.utils.tensorboard.writer.SummaryWriter.add_scalars", "item_type": "method", "code": "add_scalars(main_tag,tag_scalar_dict,global_step=None,walltime=None)", "description": "Adds many scalar data to summary. Note that this function also keeps logged scalars in memory. In extreme case it explodes your RAM.  Parameters  main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (python:int) \u2013 Global step value to record walltime (python:float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event    Examples: from torch.utils.tensorboard import SummaryWriter writer = SummaryWriter() r = 5 for i in range(100):     writer.add_scalars('run_14h', {'xsinx':i*np.sin(i/r),                                     'xcosx':i*np.cos(i/r),                                     'tanx': np.tan(i/r)}, i) writer.close() # This call adds three values to the same scalar plot with the tag # 'run_14h' in TensorBoard's scalar section.   Expected result:  ", "parameters": ["main_tag (string) : The parent name for the tags", "tag_scalar_dict (dict) : Key-value pair storing the tag and corresponding values", "global_step (python:int) : Global step value to record", "walltime (python:float) : Optional override default walltime (time.time())seconds after epoch of event"], "returns": null, "example": "from torch.utils.tensorboard import SummaryWriter\nwriter = SummaryWriter()\nr = 5\nfor i in range(100):\n    writer.add_scalars('run_14h', {'xsinx':i*np.sin(i/r),\n                                    'xcosx':i*np.cos(i/r),\n                                    'tanx': np.tan(i/r)}, i)\nwriter.close()\n# This call adds three values to the same scalar plot with the tag\n# 'run_14h' in TensorBoard's scalar section.\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.utils.tensorboard.writer.SummaryWriter.add_histogram", "item_type": "method", "code": "add_histogram(tag,values,global_step=None,bins='tensorflow',walltime=None,max_bins=None)", "description": "Add histogram to summary.  Parameters  tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (python:int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find other options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (python:float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event    Examples: from torch.utils.tensorboard import SummaryWriter import numpy as np writer = SummaryWriter() for i in range(10):     x = np.random.random(1000)     writer.add_histogram('distribution centers', x + i, i) writer.close()   Expected result:  ", "parameters": ["tag (string) : Data identifier", "values (torch.Tensor, numpy.array, or string/blobname) : Values to build histogram", "global_step (python:int) : Global step value to record", "bins (string) : One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can findother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html", "walltime (python:float) : Optional override default walltime (time.time())seconds after epoch of event"], "returns": null, "example": "from torch.utils.tensorboard import SummaryWriter\nimport numpy as np\nwriter = SummaryWriter()\nfor i in range(10):\n    x = np.random.random(1000)\n    writer.add_histogram('distribution centers', x + i, i)\nwriter.close()\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.utils.tensorboard.writer.SummaryWriter.add_image", "item_type": "method", "code": "add_image(tag,img_tensor,global_step=None,walltime=None,dataformats='CHW')", "description": "Add image data to summary. Note that this requires the pillow package.  Parameters  tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (python:int) \u2013 Global step value to record walltime (python:float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event     Shape:img_tensor: Default is (3,H,W)(3, H, W)(3,H,W)  . You can use torchvision.utils.make_grid() to convert a batch of tensor into 3xHxW format or call add_images and let us do the job. Tensor with (1,H,W)(1, H, W)(1,H,W)  , (H,W)(H, W)(H,W)  , (H,W,3)(H, W, 3)(H,W,3)   is also suitible as long as corresponding dataformats argument is passed. e.g. CHW, HWC, HW.   Examples: from torch.utils.tensorboard import SummaryWriter import numpy as np img = np.zeros((3, 100, 100)) img[0] = np.arange(0, 10000).reshape(100, 100) / 10000 img[1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000  img_HWC = np.zeros((100, 100, 3)) img_HWC[:, :, 0] = np.arange(0, 10000).reshape(100, 100) / 10000 img_HWC[:, :, 1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000  writer = SummaryWriter() writer.add_image('my_image', img, 0)  # If you have non-default dimension setting, set the dataformats argument. writer.add_image('my_image_HWC', img_HWC, 0, dataformats='HWC') writer.close()   Expected result:  ", "parameters": ["tag (string) : Data identifier", "img_tensor (torch.Tensor, numpy.array, or string/blobname) : Image data", "global_step (python:int) : Global step value to record", "walltime (python:float) : Optional override default walltime (time.time())seconds after epoch of event"], "returns": null, "example": "from torch.utils.tensorboard import SummaryWriter\nimport numpy as np\nimg = np.zeros((3, 100, 100))\nimg[0] = np.arange(0, 10000).reshape(100, 100) / 10000\nimg[1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000\n\nimg_HWC = np.zeros((100, 100, 3))\nimg_HWC[:, :, 0] = np.arange(0, 10000).reshape(100, 100) / 10000\nimg_HWC[:, :, 1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000\n\nwriter = SummaryWriter()\nwriter.add_image('my_image', img, 0)\n\n# If you have non-default dimension setting, set the dataformats argument.\nwriter.add_image('my_image_HWC', img_HWC, 0, dataformats='HWC')\nwriter.close()\n\n", "shape": "img_tensor: Default is (3,H,W)(3, H, W)(3,H,W)  . You can use torchvision.utils.make_grid() to convert a batch of tensor into 3xHxW format or call add_images and let us do the job. Tensor with (1,H,W)(1, H, W)(1,H,W)  , (H,W)(H, W)(H,W)  , (H,W,3)(H, W, 3)(H,W,3)   is also suitible as long as corresponding dataformats argument is passed. e.g. CHW, HWC, HW. "},
{"library": "torch", "item_id": "torch.utils.data.DataLoader", "item_type": "class", "code": "classtorch.utils.data.DataLoader(dataset,batch_size=1,shuffle=False,sampler=None,batch_sampler=None,num_workers=0,collate_fn=None,pin_memory=False,drop_last=False,timeout=0,worker_init_fn=None,multiprocessing_context=None)", "description": "Data loader. Combines a dataset and a sampler, and provides an iterable over the given dataset. The DataLoader supports both map-style and iterable-style datasets with single- or multi-process loading, customizing loading order and optional automatic batching (collation) and memory pinning. See torch.utils.data documentation page for more details.  Parameters  dataset (Dataset) \u2013 dataset from which to load the data. batch_size (python:int, optional) \u2013 how many samples per batch to load (default: 1). shuffle (bool, optional) \u2013 set to True to have the data reshuffled at every epoch (default: False). sampler (Sampler, optional) \u2013 defines the strategy to draw samples from the dataset. If specified, shuffle must be False. batch_sampler (Sampler, optional) \u2013 like sampler, but returns a batch of indices at a time. Mutually exclusive with batch_size, shuffle, sampler, and drop_last. num_workers (python:int, optional) \u2013 how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. (default: 0) collate_fn (callable, optional) \u2013 merges a list of samples to form a mini-batch of Tensor(s).  Used when using batched loading from a map-style dataset. pin_memory (bool, optional) \u2013 If True, the data loader will copy Tensors into CUDA pinned memory before returning them.  If your data elements are a custom type, or your collate_fn returns a batch that is a custom type, see the example below. drop_last (bool, optional) \u2013 set to True to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If False and the size of dataset is not divisible by the batch size, then the last batch will be smaller. (default: False) timeout (numeric, optional) \u2013 if positive, the timeout value for collecting a batch from workers. Should always be non-negative. (default: 0) worker_init_fn (callable, optional) \u2013 If not None, this will be called on each worker subprocess with the worker id (an int in [0, num_workers - 1]) as input, after seeding and before data loading. (default: None)     Warning If the spawn start method is used, worker_init_fn cannot be an unpicklable object, e.g., a lambda function. See Multiprocessing best practices on more details related to multiprocessing in PyTorch.   Note len(dataloader) heuristic is based on the length of the sampler used. When dataset is an IterableDataset, len(dataset) (if implemented) is returned instead, regardless of multi-process loading configurations, because PyTorch trust user dataset code in correctly handling multi-process loading to avoid duplicate data. See Dataset Types for more details on these two types of datasets and how IterableDataset interacts with Multi-process data loading.  ", "parameters": ["dataset (Dataset) : dataset from which to load the data.", "batch_size (python:int, optional) : how many samples per batch to load(default: 1).", "shuffle (bool, optional) : set to True to have the data reshuffledat every epoch (default: False).", "sampler (Sampler, optional) : defines the strategy to draw samples fromthe dataset. If specified, shuffle must be False.", "batch_sampler (Sampler, optional) : like sampler, but returns a batch ofindices at a time. Mutually exclusive with batch_size,shuffle, sampler, and drop_last.", "num_workers (python:int, optional) : how many subprocesses to use for dataloading. 0 means that the data will be loaded in the main process.(default: 0)", "collate_fn (callable, optional) : merges a list of samples to form amini-batch of Tensor(s).  Used when using batched loading from amap-style dataset.", "pin_memory (bool, optional) : If True, the data loader will copy Tensorsinto CUDA pinned memory before returning them.  If your data elementsare a custom type, or your collate_fn returns a batch that is a custom type,see the example below.", "drop_last (bool, optional) : set to True to drop the last incomplete batch,if the dataset size is not divisible by the batch size. If False andthe size of dataset is not divisible by the batch size, then the last batchwill be smaller. (default: False)", "timeout (numeric, optional) : if positive, the timeout value for collecting a batchfrom workers. Should always be non-negative. (default: 0)", "worker_init_fn (callable, optional) : If not None, this will be called on eachworker subprocess with the worker id (an int in [0, num_workers - 1]) asinput, after seeding and before data loading. (default: None)"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.utils.data.Dataset", "item_type": "class", "code": "classtorch.utils.data.Dataset", "description": "An abstract class representing a Dataset. All datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite __getitem__(), supporting fetching a data sample for a given key. Subclasses could also optionally overwrite __len__(), which is expected to return the size of the dataset by many Sampler implementations and the default options of DataLoader.  Note DataLoader by default constructs a index sampler that yields integral indices.  To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided.  ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.utils.data.IterableDataset", "item_type": "class", "code": "classtorch.utils.data.IterableDataset", "description": "An iterable Dataset. All datasets that represent an iterable of data samples should subclass it. Such form of datasets is particularly useful when data come from a stream. All subclasses should overwrite __iter__(), which would return an iterator of samples in this dataset. When a subclass is used with DataLoader, each item in the dataset will be yielded from the DataLoader iterator. When num_workers &gt; 0, each worker process will have a different copy of the dataset object, so it is often desired to configure each copy independently to avoid having duplicate data returned from the workers. get_worker_info(), when called in a worker process, returns information about the worker. It can be used in either the dataset\u2019s __iter__() method or the DataLoader \u2018s worker_init_fn option to modify each copy\u2019s behavior. Example 1: splitting workload across all workers in __iter__(): &gt;&gt;&gt; class MyIterableDataset(torch.utils.data.IterableDataset): ...     def __init__(self, start, end): ...         super(MyIterableDataset).__init__() ...         assert end &gt; start, \"this example code only works with end &gt;= start\" ...         self.start = start ...         self.end = end ... ...     def __iter__(self): ...         worker_info = torch.utils.data.get_worker_info() ...         if worker_info is None:  # single-process data loading, return the full iterator ...             iter_start = self.start ...             iter_end = self.end ...         else:  # in a worker process ...             # split workload ...             per_worker = int(math.ceil((self.end - self.start) / float(worker_info.num_workers))) ...             worker_id = worker_info.id ...             iter_start = self.start + worker_id * per_worker ...             iter_end = min(iter_start + per_worker, self.end) ...         return iter(range(iter_start, iter_end)) ... &gt;&gt;&gt; # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6]. &gt;&gt;&gt; ds = MyIterableDataset(start=3, end=7)  &gt;&gt;&gt; # Single-process loading &gt;&gt;&gt; print(list(torch.utils.data.DataLoader(ds, num_workers=0))) [3, 4, 5, 6]  &gt;&gt;&gt; # Mult-process loading with two worker processes &gt;&gt;&gt; # Worker 0 fetched [3, 4].  Worker 1 fetched [5, 6]. &gt;&gt;&gt; print(list(torch.utils.data.DataLoader(ds, num_workers=2))) [3, 5, 4, 6]  &gt;&gt;&gt; # With even more workers &gt;&gt;&gt; print(list(torch.utils.data.DataLoader(ds, num_workers=20))) [3, 4, 5, 6]   Example 2: splitting workload across all workers using worker_init_fn: &gt;&gt;&gt; class MyIterableDataset(torch.utils.data.IterableDataset): ...     def __init__(self, start, end): ...         super(MyIterableDataset).__init__() ...         assert end &gt; start, \"this example code only works with end &gt;= start\" ...         self.start = start ...         self.end = end ... ...     def __iter__(self): ...         return iter(range(self.start, self.end)) ... &gt;&gt;&gt; # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6]. &gt;&gt;&gt; ds = MyIterableDataset(start=3, end=7)  &gt;&gt;&gt; # Single-process loading &gt;&gt;&gt; print(list(torch.utils.data.DataLoader(ds, num_workers=0))) [3, 4, 5, 6] &gt;&gt;&gt; &gt;&gt;&gt; # Directly doing multi-process loading yields duplicate data &gt;&gt;&gt; print(list(torch.utils.data.DataLoader(ds, num_workers=2))) [3, 3, 4, 4, 5, 5, 6, 6]  &gt;&gt;&gt; # Define a `worker_init_fn` that configures each dataset copy differently &gt;&gt;&gt; def worker_init_fn(worker_id): ...     worker_info = torch.utils.data.get_worker_info() ...     dataset = worker_info.dataset  # the dataset copy in this worker process ...     overall_start = dataset.start ...     overall_end = dataset.end ...     # configure the dataset to only process the split workload ...     per_worker = int(math.ceil((overall_end - overall_start) / float(worker_info.num_workers))) ...     worker_id = worker_info.id ...     dataset.start = overall_start + worker_id * per_worker ...     dataset.end = min(dataset.start + per_worker, overall_end) ...  &gt;&gt;&gt; # Mult-process loading with the custom `worker_init_fn` &gt;&gt;&gt; # Worker 0 fetched [3, 4].  Worker 1 fetched [5, 6]. &gt;&gt;&gt; print(list(torch.utils.data.DataLoader(ds, num_workers=2, worker_init_fn=worker_init_fn))) [3, 5, 4, 6]  &gt;&gt;&gt; # With even more workers &gt;&gt;&gt; print(list(torch.utils.data.DataLoader(ds, num_workers=20, worker_init_fn=worker_init_fn))) [3, 4, 5, 6]   ", "parameters": [], "returns": null, "example": " class MyIterableDataset(torch.utils.data.IterableDataset):\n...     def __init__(self, start, end):\n...         super(MyIterableDataset).__init__()\n...         assert end  start, \"this example code only works with end = start\"\n...         self.start = start\n...         self.end = end\n...\n...     def __iter__(self):\n...         worker_info = torch.utils.data.get_worker_info()\n...         if worker_info is None:  # single-process data loading, return the full iterator\n...             iter_start = self.start\n...             iter_end = self.end\n...         else:  # in a worker process\n...             # split workload\n...             per_worker = int(math.ceil((self.end - self.start) / float(worker_info.num_workers)))\n...             worker_id = worker_info.id\n...             iter_start = self.start + worker_id * per_worker\n...             iter_end = min(iter_start + per_worker, self.end)\n...         return iter(range(iter_start, iter_end))\n...\n # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6].\n ds = MyIterableDataset(start=3, end=7)\n\n # Single-process loading\n print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n[3, 4, 5, 6]\n\n # Mult-process loading with two worker processes\n # Worker 0 fetched [3, 4].  Worker 1 fetched [5, 6].\n print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n[3, 5, 4, 6]\n\n # With even more workers\n print(list(torch.utils.data.DataLoader(ds, num_workers=20)))\n[3, 4, 5, 6]\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.random.fork_rng", "item_type": "function", "code": "torch.random.fork_rng(devices=None,enabled=True,_caller='fork_rng',_devices_kw='devices')", "description": "Forks the RNG, so that when you return, the RNG is reset to the state that it was previously in.  Parameters  devices (iterable of CUDA IDs) \u2013 CUDA devices for which to fork the RNG.  CPU RNG state is always forked.  By default, fork_rng() operates on all devices, but will emit a warning if your machine has a lot of devices, since this function will run very slowly in that case. If you explicitly specify devices, this warning will be suppressed enabled (bool) \u2013 if False, the RNG is not forked.  This is a convenience argument for easily disabling the context manager without having to delete it and unindent your Python code under it.    ", "parameters": ["devices (iterable of CUDA IDs) : CUDA devices for which to forkthe RNG.  CPU RNG state is always forked.  By default, fork_rng() operateson all devices, but will emit a warning if your machine has a lotof devices, since this function will run very slowly in that case.If you explicitly specify devices, this warning will be suppressed", "enabled (bool) : if False, the RNG is not forked.  This is a convenienceargument for easily disabling the context manager without havingto delete it and unindent your Python code under it."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.random.get_rng_state", "item_type": "function", "code": "torch.random.get_rng_state()", "description": "Returns the random number generator state as a torch.ByteTensor. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.random.initial_seed", "item_type": "function", "code": "torch.random.initial_seed()", "description": "Returns the initial seed for generating random numbers as a Python long. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.random.manual_seed", "item_type": "function", "code": "torch.random.manual_seed(seed)", "description": "Sets the seed for generating random numbers. Returns a torch.Generator object.  Parameters seed (python:int) \u2013 The desired seed.   ", "parameters": ["seed (python:int) : The desired seed."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.random.seed", "item_type": "function", "code": "torch.random.seed()", "description": "Sets the seed for generating random numbers to a non-deterministic random number. Returns a 64 bit number used to seed the RNG. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.random.set_rng_state", "item_type": "function", "code": "torch.random.set_rng_state(new_state)", "description": "Sets the random number generator state.  Parameters new_state (torch.ByteTensor) \u2013 The desired state   ", "parameters": ["new_state (torch.ByteTensor) : The desired state"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.random.get_rng_state()", "description": "Returns the random number generator state as a torch.ByteTensor. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributed.rpc.init_rpc", "item_type": "function", "code": "torch.distributed.rpc.init_rpc(name,backend=BackendType.PROCESS_GROUP,rank=-1,world_size=None,rpc_backend_options=None)", "description": "Initializes RPC primitives such as the local RPC agent and distributed autograd. Initializes the local RPC agent which immediately makes the current process ready to send and receive RPCs. This method also properly initializes a default process group backend that uses gloo for collective communication.  Parameters  backend (Enum) \u2013 type of RPC backend implementation. Currently, process group backend is the only available backend implementation. (default: RpcBackend.PROCESS_GROUP). name (str) \u2013 a globally unique name of this node. (e.g., Trainer3, ParameterServer2, Master, Worker1) Name can only contain number, alphabet, underscore, and/or dash, and must be shorter than 128 characters. rank (python:int) \u2013 a globally unique id/rank of this node. world_size (python:int) \u2013 The number of workers in the group. rpc_backend_options (RpcBackendOptions) \u2013 The options passed to RpcAgent consturctor.    ", "parameters": ["backend (Enum) : type of RPC backend implementation.Currently, process group backend is the onlyavailable backend implementation. (default:RpcBackend.PROCESS_GROUP).", "name (str) : a globally unique name of this node. (e.g.,Trainer3, ParameterServer2, Master,Worker1) Name can only contain number, alphabet,underscore, and/or dash, and must be shorter than128 characters.", "rank (python:int) : a globally unique id/rank of this node.", "world_size (python:int) : The number of workers in the group.", "rpc_backend_options (RpcBackendOptions) : The options passed to RpcAgentconsturctor."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributed.rpc.rpc_sync", "item_type": "function", "code": "torch.distributed.rpc.rpc_sync(to,func,args=None,kwargs=None)", "description": "Make a blocking RPC call to run function func on worker to. RPC messages are sent and received in parallel to execution of Python code. This method is thread-safe.  Parameters  to (str or WorkerInfo) \u2013 id or name of the destination worker. func (callable) \u2013 any callable function. builtin functions (like torch.add()) can be sent over RPC more efficiently. args (tuple) \u2013 the argument tuple for the func invocation. kwargs (dict) \u2013 is a dictionary of keyword arguments for the func invocation.   Returns Returns the result of running func on args and kwargs.   Example: On worker 0: &gt;&gt;&gt; import torch.distributed.rpc as rpc &gt;&gt;&gt; rpc.init_rpc(\"worker0\", rank=0, world_size=2) &gt;&gt;&gt; ret = rpc.rpc_sync(\"worker1\", torch.add, args=(torch.ones(2), 3)) &gt;&gt;&gt; rpc.shutdown()  On worker 1: &gt;&gt;&gt; import torch.distributed.rpc as rpc &gt;&gt;&gt; rpc.init_rpc(\"worker1\", rank=1, world_size=2) &gt;&gt;&gt; rpc.shutdown()   ", "parameters": ["to (str or WorkerInfo) : id or name of the destination worker.", "func (callable) : any callable function. builtin functions (liketorch.add()) can be sent over RPC more efficiently.", "args (tuple) : the argument tuple for the func invocation.", "kwargs (dict) : is a dictionary of keyword arguments for the funcinvocation."], "returns": "Returns the result of running func on args and kwargs.", "example": "On worker 0:\n import torch.distributed.rpc as rpc\n rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n ret = rpc.rpc_sync(\"worker1\", torch.add, args=(torch.ones(2), 3))\n rpc.shutdown()\n\nOn worker 1:\n import torch.distributed.rpc as rpc\n rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n rpc.shutdown()\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributed.rpc.rpc_async", "item_type": "function", "code": "torch.distributed.rpc.rpc_async(to,func,args=None,kwargs=None)", "description": "Make a non-blocking RPC call to run function func on worker to. RPC messages are sent and received in parallel to execution of Python code. This method is thread-safe. This method will immediately return a torch.distributed.FutureMessage that can be awaited on.  Parameters  to (str or WorkerInfo) \u2013 id or name of the destination worker. func (callable) \u2013 any callable function. builtin functions (like torch.add()) can be sent over RPC more efficiently. args (tuple) \u2013 the argument tuple for the func invocation. kwargs (dict) \u2013 is a dictionary of keyword arguments for the func invocation.   Returns Returns a torch.distributed.FutureMessage object that can be waited on. When completed, the return value of func on args and kwargs can be retrieved from the FutureMessage object.   Example: On worker 0: &gt;&gt;&gt; import torch.distributed.rpc as rpc &gt;&gt;&gt; rpc.init_rpc(\"worker0\", rank=0, world_size=2) &gt;&gt;&gt; fut1 = rpc.rpc_async(\"worker1\", torch.add, args=(torch.ones(2), 3)) &gt;&gt;&gt; fut2 = rpc.rpc_async(\"worker1\", min, args=(1, 2)) &gt;&gt;&gt; result = fut1.wait() + fut2.wait() &gt;&gt;&gt; rpc.shutdown()  On worker 1: &gt;&gt;&gt; import torch.distributed.rpc as rpc &gt;&gt;&gt; rpc.init_rpc(\"worker1\", rank=1, world_size=2) &gt;&gt;&gt; rpc.shutdown()   ", "parameters": ["to (str or WorkerInfo) : id or name of the destination worker.", "func (callable) : any callable function. builtin functions (liketorch.add()) can be sent over RPC more efficiently.", "args (tuple) : the argument tuple for the func invocation.", "kwargs (dict) : is a dictionary of keyword arguments for the funcinvocation."], "returns": "Returns a torch.distributed.FutureMessage object that can be waitedon. When completed, the return value of func on args andkwargs can be retrieved from the FutureMessage object.", "example": "On worker 0:\n import torch.distributed.rpc as rpc\n rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n fut1 = rpc.rpc_async(\"worker1\", torch.add, args=(torch.ones(2), 3))\n fut2 = rpc.rpc_async(\"worker1\", min, args=(1, 2))\n result = fut1.wait() + fut2.wait()\n rpc.shutdown()\n\nOn worker 1:\n import torch.distributed.rpc as rpc\n rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n rpc.shutdown()\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.FloatStorage.element_size", "item_type": "method", "code": "element_size()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.FloatStorage.fill_", "item_type": "method", "code": "fill_()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.FloatStorage.float", "item_type": "method", "code": "float()", "description": "Casts this storage to float type ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.FloatStorage.from_buffer", "item_type": "method", "code": "staticfrom_buffer()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.FloatStorage.from_file", "item_type": "method", "code": "staticfrom_file(filename,shared=False,size=0)\u2192Storage", "description": "If shared is True, then memory is shared between all processes. All changes are written to the file. If shared is False, then the changes on the storage do not affect the file. size is the number of elements in the storage. If shared is False, then the file must contain at least size * sizeof(Type) bytes (Type is the type of storage). If shared is True the file will be created if needed.  Parameters  filename (str) \u2013 file name to map shared (bool) \u2013 whether to share memory size (python:int) \u2013 number of elements in the storage    ", "parameters": ["filename (str) : file name to map", "shared (bool) : whether to share memory", "size (python:int) : number of elements in the storage"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.FloatStorage.half", "item_type": "method", "code": "half()", "description": "Casts this storage to half type ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.FloatStorage.int", "item_type": "method", "code": "int()", "description": "Casts this storage to int type ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.FloatStorage.is_pinned", "item_type": "method", "code": "is_pinned()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.FloatStorage.is_shared", "item_type": "method", "code": "is_shared()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.FloatStorage.long", "item_type": "method", "code": "long()", "description": "Casts this storage to long type ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.FloatStorage.new", "item_type": "method", "code": "new()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.FloatStorage.pin_memory", "item_type": "method", "code": "pin_memory()", "description": "Copies the storage to pinned memory, if it\u2019s not already pinned. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.quantization.quantize", "item_type": "function", "code": "torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)", "description": "Converts a float model to quantized model. First it will prepare the model for calibration or training, then it calls run_fn which will run the calibration step or training step, after that we will call convert which will convert the model to a quantized model.  Parameters  model \u2013 input model run_fn \u2013 a function for evaluating the prepared model, can be a function that simply runs the prepared model or a training loop run_args \u2013 positional arguments for run_fn inplace \u2013 carry out model transformations in-place, the original module is mutated mapping \u2013 correspondence between original module types and quantized counterparts   Returns Quantized model.   ", "parameters": ["model : input model", "run_fn : a function for evaluating the prepared model, can be afunction that simply runs the prepared model or a training loop", "run_args : positional arguments for run_fn", "inplace : carry out model transformations in-place, the original module is mutated", "mapping : correspondence between original module types and quantized counterparts"], "returns": "Quantized model.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.optim.Optimizer.add_param_group", "item_type": "method", "code": "add_param_group(param_group)", "description": "Add a param group to the Optimizer s param_groups. This can be useful when fine tuning a pre-trained network as frozen layers can be made trainable and added to the Optimizer as training progresses.  Parameters  param_group (dict) \u2013 Specifies what Tensors should be optimized along with group optimization options. (specific) \u2013     ", "parameters": ["param_group (dict) : Specifies what Tensors should be optimized along with group", "optimization options. (specific) : "], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.onnx.export", "item_type": "function", "code": "torch.onnx.export(model,args,f,export_params=True,verbose=False,training=False,input_names=None,output_names=None,aten=False,export_raw_ir=False,operator_export_type=None,opset_version=None,_retain_param_name=True,do_constant_folding=False,example_outputs=None,strip_doc_string=True,dynamic_axes=None,keep_initializers_as_inputs=None)", "description": "Export a model into ONNX format.  This exporter runs your model once in order to get a trace of its execution to be exported; at the moment, it supports a limited set of dynamic models (e.g., RNNs.)  Parameters  model (torch.nn.Module) \u2013 the model to be exported. args (tuple of arguments) \u2013 the inputs to the model, e.g., such that model(*args) is a valid invocation of the model.  Any non-Tensor arguments will be hard-coded into the exported model; any Tensor arguments will become inputs of the exported model, in the order they occur in args.  If args is a Tensor, this is equivalent to having called it with a 1-ary tuple of that Tensor. (Note: passing keyword arguments to the model is not currently supported.  Give us a shout if you need it.) f \u2013 a file-like object (has to implement fileno that returns a file descriptor) or a string containing a file name.  A binary Protobuf will be written to this file. export_params (bool, default True) \u2013 if specified, all parameters will be exported.  Set this to False if you want to export an untrained model. In this case, the exported model will first take all of its parameters as arguments, the ordering as specified by model.state_dict().values() verbose (bool, default False) \u2013 if specified, we will print out a debug description of the trace being exported. training (bool, default False) \u2013 export the model in training mode.  At the moment, ONNX is oriented towards exporting models for inference only, so you will generally not need to set this to True. input_names (list of strings, default empty list) \u2013 names to assign to the input nodes of the graph, in order output_names (list of strings, default empty list) \u2013 names to assign to the output nodes of the graph, in order aten (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the model in aten mode. If using aten mode, all the ops original exported by the functions in symbolic_opset&lt;version&gt;.py are exported as ATen ops. export_raw_ir (bool, default False) \u2013 [DEPRECATED. use operator_export_type] export the internal IR directly instead of converting it to ONNX ops. operator_export_type (enum, default OperatorExportTypes.ONNX) \u2013 OperatorExportTypes.ONNX: all ops are exported as regular ONNX ops. OperatorExportTypes.ONNX_ATEN: all ops are exported as ATen ops. OperatorExportTypes.ONNX_ATEN_FALLBACK: if symbolic is missing, fall back on ATen op. OperatorExportTypes.RAW: export raw ir. opset_version (python:int, default is 9) \u2013 by default we export the model to the opset version of the onnx submodule. Since ONNX\u2019s latest opset may evolve before next stable release, by default we export to one stable opset version. Right now, supported stable opset version is 9. The opset_version must be _onnx_master_opset or in _onnx_stable_opsets which are defined in torch/onnx/symbolic_helper.py do_constant_folding (bool, default False) \u2013 If True, the constant-folding optimization is applied to the model during export. Constant-folding optimization will replace some of the ops that have all constant inputs, with pre-computed constant nodes. example_outputs (tuple of Tensors, default None) \u2013 example_outputs must be provided when exporting a ScriptModule or TorchScript Function. strip_doc_string (bool, default True) \u2013 if True, strips the field \u201cdoc_string\u201d from the exported model, which information about the stack trace. example_outputs \u2013 example outputs of the model that is being exported. dynamic_axes (dict&lt;string, dict&lt;python:int, string&gt;&gt; or dict&lt;string, list(python:int)&gt;, default empty dict) \u2013 a dictionary to specify dynamic axes of input/output, such that: - KEY:  input and/or output names - VALUE: index of dynamic axes for given key and potentially the name to be used for exported dynamic axes. In general the value is defined according to one of the following ways or a combination of both: (1). A list of integers specifiying the dynamic axes of provided input. In this scenario automated names will be generated and applied to dynamic axes of provided input/output during export. OR (2). An inner dictionary that specifies a mapping FROM the index of dynamic axis in corresponding input/output TO the name that is desired to be applied on such axis of such input/output during export. Example. if we have the following shape for inputs and outputs: shape(input_1) = ('b', 3, 'w', 'h') and shape(input_2) = ('b', 4) and shape(output)  = ('b', 'd', 5)    Then dynamic axes can be defined either as: (a). ONLY INDICES:dynamic_axes = {\u2018input_1\u2019:[0, 2, 3], \u2018input_2\u2019:[0], \u2018output\u2019:[0, 1]} where automatic names will be generated for exported dynamic axes  (b). INDICES WITH CORRESPONDING NAMES:dynamic_axes = {\u2018input_1\u2019:{0:\u2019batch\u2019, 1:\u2019width\u2019, 2:\u2019height\u2019}, \u2018input_2\u2019:{0:\u2019batch\u2019}, \u2018output\u2019:{0:\u2019batch\u2019, 1:\u2019detections\u2019} where provided names will be applied to exported dynamic axes  (c). MIXED MODE OF (a) and (b)dynamic_axes = {\u2018input_1\u2019:[0, 2, 3], \u2018input_2\u2019:{0:\u2019batch\u2019}, \u2018output\u2019:[0,1]}      keep_initializers_as_inputs (bool, default None) \u2013 If True, all the initializers (typically corresponding to parameters) in the exported graph will also be added as inputs to the graph. If False, then initializers are not added as inputs to the graph, and only the non-parameter inputs are added as inputs. This may allow for better optimizations (such as constant folding etc.) by backends/runtimes that execute these graphs. If unspecified (default None), then the behavior is chosen automatically as follows. If operator_export_type is OperatorExportTypes.ONNX, the behavior is equivalent to setting this argument to False. For other values of operator_export_type, the behavior is equivalent to setting this argument to True. Note that for ONNX opset version &lt; 9, initializers MUST be part of graph inputs. Therefore, if opset_version argument is set to a 8 or lower, this argument will be ignored.    ", "parameters": ["model (torch.nn.Module) : the model to be exported.", "args (tuple of arguments) : the inputs tothe model, e.g., such that model(*args) is a validinvocation of the model.  Any non-Tensor arguments willbe hard-coded into the exported model; any Tensor argumentswill become inputs of the exported model, in the order theyoccur in args.  If args is a Tensor, this is equivalentto having called it with a 1-ary tuple of that Tensor.(Note: passing keyword arguments to the model is not currentlysupported.  Give us a shout if you need it.)", "f : a file-like object (has to implement fileno that returns a file descriptor)or a string containing a file name.  A binary Protobuf will be writtento this file.", "export_params (bool, default True) : if specified, all parameters willbe exported.  Set this to False if you want to export an untrained model.In this case, the exported model will first take all of its parametersas arguments, the ordering as specified by model.state_dict().values()", "verbose (bool, default False) : if specified, we will print out a debugdescription of the trace being exported.", "training (bool, default False) : export the model in training mode.  Atthe moment, ONNX is oriented towards exporting models for inferenceonly, so you will generally not need to set this to True.", "input_names (list of strings, default empty list) : names to assign to theinput nodes of the graph, in order", "output_names (list of strings, default empty list) : names to assign to theoutput nodes of the graph, in order", "aten (bool, default False) : [DEPRECATED. use operator_export_type] export themodel in aten mode. If using aten mode, all the ops original exportedby the functions in symbolic_opset&lt;version&gt;.py are exported as ATen ops.", "export_raw_ir (bool, default False) : [DEPRECATED. use operator_export_type]export the internal IR directly instead of converting it to ONNX ops.", "operator_export_type (enum, default OperatorExportTypes.ONNX) : OperatorExportTypes.ONNX: all ops are exported as regular ONNX ops.OperatorExportTypes.ONNX_ATEN: all ops are exported as ATen ops.OperatorExportTypes.ONNX_ATEN_FALLBACK: if symbolic is missing, fall back on ATen op.OperatorExportTypes.RAW: export raw ir.", "opset_version (python:int, default is 9) : by default we export the model to theopset version of the onnx submodule. Since ONNX\u2019s latest opset mayevolve before next stable release, by default we export to one stableopset version. Right now, supported stable opset version is 9.The opset_version must be _onnx_master_opset or in _onnx_stable_opsetswhich are defined in torch/onnx/symbolic_helper.py", "do_constant_folding (bool, default False) : If True, the constant-foldingoptimization is applied to the model during export. Constant-foldingoptimization will replace some of the ops that have all constantinputs, with pre-computed constant nodes.", "example_outputs (tuple of Tensors, default None) : example_outputs must be providedwhen exporting a ScriptModule or TorchScript Function.", "strip_doc_string (bool, default True) : if True, strips the field\u201cdoc_string\u201d from the exported model, which information about the stacktrace.", "example_outputs : example outputs of the model that is being exported.", "dynamic_axes (dict&lt;string, dict&lt;python:int, string&gt;&gt; or dict&lt;string, list(python:int)&gt;, default empty dict) : a dictionary to specify dynamic axes of input/output, such that:- KEY:  input and/or output names- VALUE: index of dynamic axes for given key and potentially the name to be used forexported dynamic axes. In general the value is defined according to one of the followingways or a combination of both:(1). A list of integers specifiying the dynamic axes of provided input. In this scenarioautomated names will be generated and applied to dynamic axes of provided input/outputduring export.OR (2). An inner dictionary that specifies a mapping FROM the index of dynamic axis incorresponding input/output TO the name that is desired to be applied on such axis ofsuch input/output during export.Example. if we have the following shape for inputs and outputs:shape(input_1) = ('b', 3, 'w', 'h')and shape(input_2) = ('b', 4)and shape(output)  = ('b', 'd', 5)Then dynamic axes can be defined either as:(a). ONLY INDICES:dynamic_axes = {\u2018input_1\u2019:[0, 2, 3], \u2018input_2\u2019:[0], \u2018output\u2019:[0, 1]}where automatic names will be generated for exported dynamic axes(b). INDICES WITH CORRESPONDING NAMES:dynamic_axes = {\u2018input_1\u2019:{0:\u2019batch\u2019, 1:\u2019width\u2019, 2:\u2019height\u2019},\u2018input_2\u2019:{0:\u2019batch\u2019},\u2018output\u2019:{0:\u2019batch\u2019, 1:\u2019detections\u2019}where provided names will be applied to exported dynamic axes(c). MIXED MODE OF (a) and (b)dynamic_axes = {\u2018input_1\u2019:[0, 2, 3], \u2018input_2\u2019:{0:\u2019batch\u2019}, \u2018output\u2019:[0,1]}", "keep_initializers_as_inputs (bool, default None) : If True, all the initializers(typically corresponding to parameters) in the exported graph will also beadded as inputs to the graph. If False, then initializers are not added asinputs to the graph, and only the non-parameter inputs are added as inputs.This may allow for better optimizations (such as constant folding etc.) bybackends/runtimes that execute these graphs. If unspecified (default None),then the behavior is chosen automatically as follows. If operator_export_typeis OperatorExportTypes.ONNX, the behavior is equivalent to setting thisargument to False. For other values of operator_export_type, the behavior isequivalent to setting this argument to True. Note that for ONNX opset version &lt; 9,initializers MUST be part of graph inputs. Therefore, if opset_version argument isset to a 8 or lower, this argument will be ignored."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.datasets.QMNIST", "item_type": "class", "code": "classtorchvision.datasets.QMNIST(root,what=None,compat=True,train=True,**kwargs)", "description": "QMNIST Dataset.  Parameters  root (string) \u2013 Root directory of dataset whose ``processed\u2019\u2019 subdir contains torch binary files with the datasets. what (string,optional) \u2013 Can be \u2018train\u2019, \u2018test\u2019, \u2018test10k\u2019, \u2018test50k\u2019, or \u2018nist\u2019 for respectively the mnist compatible training set, the 60k qmnist testing set, the 10k qmnist examples that match the mnist testing set, the 50k remaining qmnist testing examples, or all the nist digits. The default is to select \u2018train\u2019 or \u2018test\u2019 according to the compatibility argument \u2018train\u2019. compat (bool,optional) \u2013 A boolean that says whether the target for each example is class number (for compatibility with the MNIST dataloader) or a torch vector containing the full qmnist information. Default=True. download (bool, optional) \u2013 If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again. transform (callable, optional) \u2013 A function/transform that takes in an PIL image and returns a transformed version. E.g, transforms.RandomCrop target_transform (callable, optional) \u2013 A function/transform that takes in the target and transforms it. train (bool,optional,compatibility) \u2013 When argument \u2018what\u2019 is not specified, this boolean decides whether to load the training set ot the testing set.  Default: True.    ", "parameters": ["root (string) : Root directory of dataset whose ``processed\u2019\u2019subdir contains torch binary files with the datasets.", "what (string,optional) : Can be \u2018train\u2019, \u2018test\u2019, \u2018test10k\u2019,\u2018test50k\u2019, or \u2018nist\u2019 for respectively the mnist compatibletraining set, the 60k qmnist testing set, the 10k qmnistexamples that match the mnist testing set, the 50kremaining qmnist testing examples, or all the nistdigits. The default is to select \u2018train\u2019 or \u2018test\u2019according to the compatibility argument \u2018train\u2019.", "compat (bool,optional) : A boolean that says whether the targetfor each example is class number (for compatibility withthe MNIST dataloader) or a torch vector containing thefull qmnist information. Default=True.", "download (bool, optional) : If true, downloads the dataset fromthe internet and puts it in root directory. If dataset isalready downloaded, it is not downloaded again.", "transform (callable, optional) : A function/transform thattakes in an PIL image and returns a transformedversion. E.g, transforms.RandomCrop", "target_transform (callable, optional) : A function/transformthat takes in the target and transforms it.", "train (bool,optional,compatibility) : When argument \u2018what\u2019 isnot specified, this boolean decides whether to load thetraining set ot the testing set.  Default: True."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.datasets.FakeData", "item_type": "class", "code": "classtorchvision.datasets.FakeData(size=1000,image_size=(3,224,224),num_classes=10,transform=None,target_transform=None,random_offset=0)", "description": "A fake dataset that returns randomly generated images and returns them as PIL images  Parameters  size (python:int, optional) \u2013 Size of the dataset. Default: 1000 images image_size (tuple, optional) \u2013 Size if the returned images. Default: (3, 224, 224) num_classes (python:int, optional) \u2013 Number of classes in the datset. Default: 10 transform (callable, optional) \u2013 A function/transform that  takes in an PIL image and returns a transformed version. E.g, transforms.RandomCrop target_transform (callable, optional) \u2013 A function/transform that takes in the target and transforms it. random_offset (python:int) \u2013 Offsets the index-based random seed used to generate each image. Default: 0    ", "parameters": ["size (python:int, optional) : Size of the dataset. Default: 1000 images", "image_size (tuple, optional) : Size if the returned images. Default: (3, 224, 224)", "num_classes (python:int, optional) : Number of classes in the datset. Default: 10", "transform (callable, optional) : A function/transform that  takes in an PIL imageand returns a transformed version. E.g, transforms.RandomCrop", "target_transform (callable, optional) : A function/transform that takes in thetarget and transforms it.", "random_offset (python:int) : Offsets the index-based random seed used togenerate each image. Default: 0"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.datasets.CocoCaptions", "item_type": "class", "code": "classtorchvision.datasets.CocoCaptions(root,annFile,transform=None,target_transform=None,transforms=None)", "description": "MS Coco Captions Dataset.  Parameters  root (string) \u2013 Root directory where images are downloaded to. annFile (string) \u2013 Path to json annotation file. transform (callable, optional) \u2013 A function/transform that  takes in an PIL image and returns a transformed version. E.g, transforms.ToTensor target_transform (callable, optional) \u2013 A function/transform that takes in the target and transforms it. transforms (callable, optional) \u2013 A function/transform that takes input sample and its target as entry and returns a transformed version.    Example import torchvision.datasets as dset import torchvision.transforms as transforms cap = dset.CocoCaptions(root = 'dir where images are',                         annFile = 'json annotation file',                         transform=transforms.ToTensor())  print('Number of samples: ', len(cap)) img, target = cap[3] # load 4th sample  print(\"Image Size: \", img.size()) print(target)   Output: Number of samples: 82783 Image Size: (3L, 427L, 640L) [u'A plane emitting smoke stream flying over a mountain.', u'A plane darts across a bright blue sky behind a mountain covered in snow', u'A plane leaves a contrail above the snowy mountain top.', u'A mountain that has a plane flying overheard in the distance.', u'A mountain view with a plume of smoke in the background']     __getitem__(index)   Parameters index (python:int) \u2013 Index  Returns Tuple (image, target). target is a list of captions for the image.  Return type tuple     ", "parameters": ["root (string) : Root directory where images are downloaded to.", "annFile (string) : Path to json annotation file.", "transform (callable, optional) : A function/transform that  takes in an PIL imageand returns a transformed version. E.g, transforms.ToTensor", "target_transform (callable, optional) : A function/transform that takes in thetarget and transforms it.", "transforms (callable, optional) : A function/transform that takes input sample and its target as entryand returns a transformed version."], "returns": "Tuple (image, target). target is a list of captions for the image.", "example": "import torchvision.datasets as dset\nimport torchvision.transforms as transforms\ncap = dset.CocoCaptions(root = 'dir where images are',\n                        annFile = 'json annotation file',\n                        transform=transforms.ToTensor())\n\nprint('Number of samples: ', len(cap))\nimg, target = cap[3] # load 4th sample\n\nprint(\"Image Size: \", img.size())\nprint(target)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.datasets.CocoDetection", "item_type": "class", "code": "classtorchvision.datasets.CocoDetection(root,annFile,transform=None,target_transform=None,transforms=None)", "description": "MS Coco Detection Dataset.  Parameters  root (string) \u2013 Root directory where images are downloaded to. annFile (string) \u2013 Path to json annotation file. transform (callable, optional) \u2013 A function/transform that  takes in an PIL image and returns a transformed version. E.g, transforms.ToTensor target_transform (callable, optional) \u2013 A function/transform that takes in the target and transforms it. transforms (callable, optional) \u2013 A function/transform that takes input sample and its target as entry and returns a transformed version.      __getitem__(index)   Parameters index (python:int) \u2013 Index  Returns Tuple (image, target). target is the object returned by coco.loadAnns.  Return type tuple     ", "parameters": ["root (string) : Root directory where images are downloaded to.", "annFile (string) : Path to json annotation file.", "transform (callable, optional) : A function/transform that  takes in an PIL imageand returns a transformed version. E.g, transforms.ToTensor", "target_transform (callable, optional) : A function/transform that takes in thetarget and transforms it.", "transforms (callable, optional) : A function/transform that takes input sample and its target as entryand returns a transformed version."], "returns": "Tuple (image, target). target is the object returned by coco.loadAnns.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.sparse.addmm", "item_type": "function", "code": "torch.sparse.addmm(mat,mat1,mat2,beta=1,alpha=1)", "description": "This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. mat1 need to have sparse_dim = 2. Note that the gradients of mat1 is a coalesced sparse tensor.  Parameters  mat (Tensor) \u2013 a dense matrix to be added mat1 (SparseTensor) \u2013 a sparse matrix to be multiplied mat2 (Tensor) \u2013 a dense matrix be multiplied beta (Number, optional) \u2013 multiplier for mat (\u03b2\\beta\u03b2  ) alpha (Number, optional) \u2013 multiplier for mat1@mat2mat1 @ mat2mat1@mat2   (\u03b1\\alpha\u03b1  )    ", "parameters": ["mat (Tensor) : a dense matrix to be added", "mat1 (SparseTensor) : a sparse matrix to be multiplied", "mat2 (Tensor) : a dense matrix be multiplied", "beta (Number, optional) : multiplier for mat (\u03b2\\beta\u03b2)", "alpha (Number, optional) : multiplier for mat1@mat2mat1 @ mat2mat1@mat2 (\u03b1\\alpha\u03b1)"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.sparse.mm", "item_type": "function", "code": "torch.sparse.mm(mat1,mat2)", "description": "Performs a matrix multiplication of the sparse matrix mat1 and dense matrix mat2. Similar to torch.mm(), If mat1 is a (n\u00d7m)(n \\times m)(n\u00d7m)   tensor, mat2 is a (m\u00d7p)(m \\times p)(m\u00d7p)   tensor, out will be a (n\u00d7p)(n \\times p)(n\u00d7p)   dense tensor. mat1 need to have sparse_dim = 2. This function also supports backward for both matrices. Note that the gradients of mat1 is a coalesced sparse tensor.  Parameters  mat1 (SparseTensor) \u2013 the first sparse matrix to be multiplied mat2 (Tensor) \u2013 the second dense matrix to be multiplied    Example: &gt;&gt;&gt; a = torch.randn(2, 3).to_sparse().requires_grad_(True) &gt;&gt;&gt; a tensor(indices=tensor([[0, 0, 0, 1, 1, 1],                        [0, 1, 2, 0, 1, 2]]),        values=tensor([ 1.5901,  0.0183, -0.6146,  1.8061, -0.0112,  0.6302]),        size=(2, 3), nnz=6, layout=torch.sparse_coo, requires_grad=True)  &gt;&gt;&gt; b = torch.randn(3, 2, requires_grad=True) &gt;&gt;&gt; b tensor([[-0.6479,  0.7874],         [-1.2056,  0.5641],         [-1.1716, -0.9923]], requires_grad=True)  &gt;&gt;&gt; y = torch.sparse.mm(a, b) &gt;&gt;&gt; y tensor([[-0.3323,  1.8723],         [-1.8951,  0.7904]], grad_fn=&lt;SparseAddmmBackward&gt;) &gt;&gt;&gt; y.sum().backward() &gt;&gt;&gt; a.grad tensor(indices=tensor([[0, 0, 0, 1, 1, 1],                        [0, 1, 2, 0, 1, 2]]),        values=tensor([ 0.1394, -0.6415, -2.1639,  0.1394, -0.6415, -2.1639]),        size=(2, 3), nnz=6, layout=torch.sparse_coo)   ", "parameters": ["mat1 (SparseTensor) : the first sparse matrix to be multiplied", "mat2 (Tensor) : the second dense matrix to be multiplied"], "returns": null, "example": " a = torch.randn(2, 3).to_sparse().requires_grad_(True)\n a\ntensor(indices=tensor([[0, 0, 0, 1, 1, 1],\n                       [0, 1, 2, 0, 1, 2]]),\n       values=tensor([ 1.5901,  0.0183, -0.6146,  1.8061, -0.0112,  0.6302]),\n       size=(2, 3), nnz=6, layout=torch.sparse_coo, requires_grad=True)\n\n b = torch.randn(3, 2, requires_grad=True)\n b\ntensor([[-0.6479,  0.7874],\n        [-1.2056,  0.5641],\n        [-1.1716, -0.9923]], requires_grad=True)\n\n y = torch.sparse.mm(a, b)\n y\ntensor([[-0.3323,  1.8723],\n        [-1.8951,  0.7904]], grad_fn=&lt;SparseAddmmBackward)\n y.sum().backward()\n a.grad\ntensor(indices=tensor([[0, 0, 0, 1, 1, 1],\n                       [0, 1, 2, 0, 1, 2]]),\n       values=tensor([ 0.1394, -0.6415, -2.1639,  0.1394, -0.6415, -2.1639]),\n       size=(2, 3), nnz=6, layout=torch.sparse_coo)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.sparse.sum", "item_type": "function", "code": "torch.sparse.sum(input,dim=None,dtype=None)", "description": "Returns the sum of each row of SparseTensor input in the given dimensions dim. If dim is a list of dimensions, reduce over all of them. When sum over all sparse_dim, this method returns a Tensor instead of SparseTensor. All summed dim are squeezed (see torch.squeeze()), resulting an output tensor having dim fewer dimensions than input. During backward, only gradients at nnz locations of input will propagate back. Note that the gradients of input is coalesced.  Parameters  input (Tensor) \u2013 the input SparseTensor dim (python:int or tuple of python:ints) \u2013 a dimension or a list of dimensions to reduce. Default: reduce over all dims. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor. Default: dtype of input.    Example: &gt;&gt;&gt; nnz = 3 &gt;&gt;&gt; dims = [5, 5, 2, 3] &gt;&gt;&gt; I = torch.cat([torch.randint(0, dims[0], size=(nnz,)),                    torch.randint(0, dims[1], size=(nnz,))], 0).reshape(2, nnz) &gt;&gt;&gt; V = torch.randn(nnz, dims[2], dims[3]) &gt;&gt;&gt; size = torch.Size(dims) &gt;&gt;&gt; S = torch.sparse_coo_tensor(I, V, size) &gt;&gt;&gt; S tensor(indices=tensor([[2, 0, 3],                        [2, 4, 1]]),        values=tensor([[[-0.6438, -1.6467,  1.4004],                        [ 0.3411,  0.0918, -0.2312]],                        [[ 0.5348,  0.0634, -2.0494],                        [-0.7125, -1.0646,  2.1844]],                        [[ 0.1276,  0.1874, -0.6334],                        [-1.9682, -0.5340,  0.7483]]]),        size=(5, 5, 2, 3), nnz=3, layout=torch.sparse_coo)  # when sum over only part of sparse_dims, return a SparseTensor &gt;&gt;&gt; torch.sparse.sum(S, [1, 3]) tensor(indices=tensor([[0, 2, 3]]),        values=tensor([[-1.4512,  0.4073],                       [-0.8901,  0.2017],                       [-0.3183, -1.7539]]),        size=(5, 2), nnz=3, layout=torch.sparse_coo)  # when sum over all sparse dim, return a dense Tensor # with summed dims squeezed &gt;&gt;&gt; torch.sparse.sum(S, [0, 1, 3]) tensor([-2.6596, -1.1450])   ", "parameters": ["input (Tensor) : the input SparseTensor", "dim (python:int or tuple of python:ints) : a dimension or a list of dimensions to reduce. Default: reduceover all dims.", "dtype (torch.dtype, optional) : the desired data type of returned Tensor.Default: dtype of input."], "returns": null, "example": " nnz = 3\n dims = [5, 5, 2, 3]\n I = torch.cat([torch.randint(0, dims[0], size=(nnz,)),\n                   torch.randint(0, dims[1], size=(nnz,))], 0).reshape(2, nnz)\n V = torch.randn(nnz, dims[2], dims[3])\n size = torch.Size(dims)\n S = torch.sparse_coo_tensor(I, V, size)\n S\ntensor(indices=tensor([[2, 0, 3],\n                       [2, 4, 1]]),\n       values=tensor([[[-0.6438, -1.6467,  1.4004],\n                       [ 0.3411,  0.0918, -0.2312]],\n\n                      [[ 0.5348,  0.0634, -2.0494],\n                       [-0.7125, -1.0646,  2.1844]],\n\n                      [[ 0.1276,  0.1874, -0.6334],\n                       [-1.9682, -0.5340,  0.7483]]]),\n       size=(5, 5, 2, 3), nnz=3, layout=torch.sparse_coo)\n\n# when sum over only part of sparse_dims, return a SparseTensor\n torch.sparse.sum(S, [1, 3])\ntensor(indices=tensor([[0, 2, 3]]),\n       values=tensor([[-1.4512,  0.4073],\n                      [-0.8901,  0.2017],\n                      [-0.3183, -1.7539]]),\n       size=(5, 2), nnz=3, layout=torch.sparse_coo)\n\n# when sum over all sparse dim, return a dense Tensor\n# with summed dims squeezed\n torch.sparse.sum(S, [0, 1, 3])\ntensor([-2.6596, -1.1450])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.utils.cpp_extension.verify_ninja_availability", "item_type": "function", "code": "torch.utils.cpp_extension.verify_ninja_availability()", "description": "Returns True if the ninja build system is available on the system. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.models.shufflenet_v2_x1_0", "item_type": "function", "code": "torchvision.models.shufflenet_v2_x1_0(pretrained=False,progress=True,**kwargs)", "description": "Constructs a ShuffleNetV2 with 1.0x output channels, as described in \u201cShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design\u201d.  Parameters  pretrained (bool) \u2013 If True, returns a model pre-trained on ImageNet progress (bool) \u2013 If True, displays a progress bar of the download to stderr    ", "parameters": ["pretrained (bool) : If True, returns a model pre-trained on ImageNet", "progress (bool) : If True, displays a progress bar of the download to stderr"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.models.shufflenet_v2_x1_5", "item_type": "function", "code": "torchvision.models.shufflenet_v2_x1_5(pretrained=False,progress=True,**kwargs)", "description": "Constructs a ShuffleNetV2 with 1.5x output channels, as described in \u201cShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design\u201d.  Parameters  pretrained (bool) \u2013 If True, returns a model pre-trained on ImageNet progress (bool) \u2013 If True, displays a progress bar of the download to stderr    ", "parameters": ["pretrained (bool) : If True, returns a model pre-trained on ImageNet", "progress (bool) : If True, displays a progress bar of the download to stderr"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.models.shufflenet_v2_x2_0", "item_type": "function", "code": "torchvision.models.shufflenet_v2_x2_0(pretrained=False,progress=True,**kwargs)", "description": "Constructs a ShuffleNetV2 with 2.0x output channels, as described in \u201cShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design\u201d.  Parameters  pretrained (bool) \u2013 If True, returns a model pre-trained on ImageNet progress (bool) \u2013 If True, displays a progress bar of the download to stderr    ", "parameters": ["pretrained (bool) : If True, returns a model pre-trained on ImageNet", "progress (bool) : If True, displays a progress bar of the download to stderr"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.models.mobilenet_v2", "item_type": "function", "code": "torchvision.models.mobilenet_v2(pretrained=False,progress=True,**kwargs)", "description": "Constructs a MobileNetV2 architecture from \u201cMobileNetV2: Inverted Residuals and Linear Bottlenecks\u201d.  Parameters  pretrained (bool) \u2013 If True, returns a model pre-trained on ImageNet progress (bool) \u2013 If True, displays a progress bar of the download to stderr    ", "parameters": ["pretrained (bool) : If True, returns a model pre-trained on ImageNet", "progress (bool) : If True, displays a progress bar of the download to stderr"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.models.resnext50_32x4d", "item_type": "function", "code": "torchvision.models.resnext50_32x4d(pretrained=False,progress=True,**kwargs)", "description": "ResNeXt-50 32x4d model from \u201cAggregated Residual Transformation for Deep Neural Networks\u201d  Parameters  pretrained (bool) \u2013 If True, returns a model pre-trained on ImageNet progress (bool) \u2013 If True, displays a progress bar of the download to stderr    ", "parameters": ["pretrained (bool) : If True, returns a model pre-trained on ImageNet", "progress (bool) : If True, displays a progress bar of the download to stderr"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.models.resnext101_32x8d", "item_type": "function", "code": "torchvision.models.resnext101_32x8d(pretrained=False,progress=True,**kwargs)", "description": "ResNeXt-101 32x8d model from \u201cAggregated Residual Transformation for Deep Neural Networks\u201d  Parameters  pretrained (bool) \u2013 If True, returns a model pre-trained on ImageNet progress (bool) \u2013 If True, displays a progress bar of the download to stderr    ", "parameters": ["pretrained (bool) : If True, returns a model pre-trained on ImageNet", "progress (bool) : If True, displays a progress bar of the download to stderr"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.models.wide_resnet50_2", "item_type": "function", "code": "torchvision.models.wide_resnet50_2(pretrained=False,progress=True,**kwargs)", "description": "Wide ResNet-50-2 model from \u201cWide Residual Networks\u201d The model is the same as ResNet except for the bottleneck number of channels which is twice larger in every block. The number of channels in outer 1x1 convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048 channels, and in Wide ResNet-50-2 has 2048-1024-2048.  Parameters  pretrained (bool) \u2013 If True, returns a model pre-trained on ImageNet progress (bool) \u2013 If True, displays a progress bar of the download to stderr    ", "parameters": ["pretrained (bool) : If True, returns a model pre-trained on ImageNet", "progress (bool) : If True, displays a progress bar of the download to stderr"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.models.wide_resnet101_2", "item_type": "function", "code": "torchvision.models.wide_resnet101_2(pretrained=False,progress=True,**kwargs)", "description": "Wide ResNet-101-2 model from \u201cWide Residual Networks\u201d The model is the same as ResNet except for the bottleneck number of channels which is twice larger in every block. The number of channels in outer 1x1 convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048 channels, and in Wide ResNet-50-2 has 2048-1024-2048.  Parameters  pretrained (bool) \u2013 If True, returns a model pre-trained on ImageNet progress (bool) \u2013 If True, displays a progress bar of the download to stderr    ", "parameters": ["pretrained (bool) : If True, returns a model pre-trained on ImageNet", "progress (bool) : If True, displays a progress bar of the download to stderr"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.functional.perspective", "item_type": "function", "code": "torchvision.transforms.functional.perspective(img,startpoints,endpoints,interpolation=3)", "description": "Perform perspective transform of the given PIL Image.  Parameters  img (PIL Image) \u2013 Image to be transformed. startpoints \u2013 List containing [top-left, top-right, bottom-right, bottom-left] of the orignal image endpoints \u2013 List containing [top-left, top-right, bottom-right, bottom-left] of the transformed image interpolation \u2013 Default- Image.BICUBIC   Returns Perspectively transformed Image.  Return type PIL Image   ", "parameters": ["img (PIL Image) : Image to be transformed.", "startpoints : List containing [top-left, top-right, bottom-right, bottom-left] of the orignal image", "endpoints : List containing [top-left, top-right, bottom-right, bottom-left] of the transformed image", "interpolation : Default- Image.BICUBIC"], "returns": "Perspectively transformed Image.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.functional.resize", "item_type": "function", "code": "torchvision.transforms.functional.resize(img,size,interpolation=2)", "description": "Resize the input PIL Image to the given size.  Parameters  img (PIL Image) \u2013 Image to be resized. size (sequence or python:int) \u2013 Desired output size. If size is a sequence like (h, w), the output size will be matched to this. If size is an int, the smaller edge of the image will be matched to this number maintaing the aspect ratio. i.e, if height &gt; width, then image will be rescaled to (size\u00d7heightwidth,size)\\left(\\text{size} \\times \\frac{\\text{height}}{\\text{width}}, \\text{size}\\right)(size\u00d7widthheight\u200b,size)   interpolation (python:int, optional) \u2013 Desired interpolation. Default is PIL.Image.BILINEAR   Returns Resized image.  Return type PIL Image   ", "parameters": ["img (PIL Image) : Image to be resized.", "size (sequence or python:int) : Desired output size. If size is a sequence like(h, w), the output size will be matched to this. If size is an int,the smaller edge of the image will be matched to this number maintaingthe aspect ratio. i.e, if height &gt; width, then image will be rescaled to(size\u00d7heightwidth,size)\\left(\\text{size} \\times \\frac{\\text{height}}{\\text{width}}, \\text{size}\\right)(size\u00d7widthheight\u200b,size)", "interpolation (python:int, optional) : Desired interpolation. Default isPIL.Image.BILINEAR"], "returns": "Resized image.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.functional.resized_crop", "item_type": "function", "code": "torchvision.transforms.functional.resized_crop(img,top,left,height,width,size,interpolation=2)", "description": "Crop the given PIL Image and resize it to desired size. Notably used in RandomResizedCrop.  Parameters  img (PIL Image) \u2013 Image to be cropped. (0,0) denotes the top left corner of the image. top (python:int) \u2013 Vertical component of the top left corner of the crop box. left (python:int) \u2013 Horizontal component of the top left corner of the crop box. height (python:int) \u2013 Height of the crop box. width (python:int) \u2013 Width of the crop box. size (sequence or python:int) \u2013 Desired output size. Same semantics as resize. interpolation (python:int, optional) \u2013 Desired interpolation. Default is PIL.Image.BILINEAR.   Returns Cropped image.  Return type PIL Image   ", "parameters": ["img (PIL Image) : Image to be cropped. (0,0) denotes the top left corner of the image.", "top (python:int) : Vertical component of the top left corner of the crop box.", "left (python:int) : Horizontal component of the top left corner of the crop box.", "height (python:int) : Height of the crop box.", "width (python:int) : Width of the crop box.", "size (sequence or python:int) : Desired output size. Same semantics as resize.", "interpolation (python:int, optional) : Desired interpolation. Default isPIL.Image.BILINEAR."], "returns": "Cropped image.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.functional.rotate", "item_type": "function", "code": "torchvision.transforms.functional.rotate(img,angle,resample=False,expand=False,center=None,fill=0)", "description": "Rotate the image by angle.  Parameters  img (PIL Image) \u2013 PIL Image to be rotated. angle (python:float or python:int) \u2013 In degrees degrees counter clockwise order. resample (PIL.Image.NEAREST or PIL.Image.BILINEAR or PIL.Image.BICUBIC, optional) \u2013 An optional resampling filter. See filters for more information. If omitted, or if the image has mode \u201c1\u201d or \u201cP\u201d, it is set to PIL.Image.NEAREST. expand (bool, optional) \u2013 Optional expansion flag. If true, expands the output image to make it large enough to hold the entire rotated image. If false or omitted, make the output image the same size as the input image. Note that the expand flag assumes rotation around the center and no translation. center (2-tuple, optional) \u2013 Optional center of rotation. Origin is the upper left corner. Default is the center of the image. fill (3-tuple or python:int) \u2013 RGB pixel fill value for area outside the rotated image. If int, it is used for all channels respectively.    ", "parameters": ["img (PIL Image) : PIL Image to be rotated.", "angle (python:float or python:int) : In degrees degrees counter clockwise order.", "resample (PIL.Image.NEAREST or PIL.Image.BILINEAR or PIL.Image.BICUBIC, optional) : An optional resampling filter. See filters for more information.If omitted, or if the image has mode \u201c1\u201d or \u201cP\u201d, it is set to PIL.Image.NEAREST.", "expand (bool, optional) : Optional expansion flag.If true, expands the output image to make it large enough to hold the entire rotated image.If false or omitted, make the output image the same size as the input image.Note that the expand flag assumes rotation around the center and no translation.", "center (2-tuple, optional) : Optional center of rotation.Origin is the upper left corner.Default is the center of the image.", "fill (3-tuple or python:int) : RGB pixel fill value for area outside the rotated image.If int, it is used for all channels respectively."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.functional.ten_crop", "item_type": "function", "code": "torchvision.transforms.functional.ten_crop(img,size,vertical_flip=False)", "description": " Crop the given PIL Image into four corners and the central crop plus theflipped version of these (horizontal flipping is used by default).    Note This transform returns a tuple of images and there may be a mismatch in the number of inputs and targets your Dataset returns.   Parameters  size (sequence or python:int) \u2013 Desired output size of the crop. If size is an int instead of sequence like (h, w), a square crop (size, size) is made. vertical_flip (bool) \u2013 Use vertical flipping instead of horizontal   Returns  tuple (tl, tr, bl, br, center, tl_flip, tr_flip, bl_flip, br_flip, center_flip)Corresponding top left, top right, bottom left, bottom right and center crop and same for the flipped image.     Return type tuple   ", "parameters": ["size (sequence or python:int) : Desired output size of the crop. If size is anint instead of sequence like (h, w), a square crop (size, size) ismade.", "vertical_flip (bool) : Use vertical flipping instead of horizontal"], "returns": "tuple (tl, tr, bl, br, center, tl_flip, tr_flip, bl_flip, br_flip, center_flip)Corresponding top left, top right, bottom left, bottom right and center cropand same for the flipped image.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.functional.to_grayscale", "item_type": "function", "code": "torchvision.transforms.functional.to_grayscale(img,num_output_channels=1)", "description": "Convert image to grayscale version of image.  Parameters img (PIL Image) \u2013 Image to be converted to grayscale.  Returns  Grayscale version of the image.if num_output_channels = 1 : returned image is single channel if num_output_channels = 3 : returned image is 3 channel with r = g = b     Return type PIL Image   ", "parameters": ["img (PIL Image) : Image to be converted to grayscale.", "Grayscale version of the image.if num_output_channels = 1 : returned image is single channelif num_output_channels = 3 : returned image is 3 channel with r = g = b", "if num_output_channels = 1 : returned image is single channelif num_output_channels = 3 : returned image is 3 channel with r = g = b", "PIL Image"], "returns": "Grayscale version of the image.if num_output_channels = 1 : returned image is single channelif num_output_channels = 3 : returned image is 3 channel with r = g = b", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.init.calculate_gain", "item_type": "function", "code": "torch.nn.init.calculate_gain(nonlinearity,param=None)", "description": "Return the recommended gain value for the given nonlinearity function. The values are as follows:       nonlinearity gain    Linear / Identity 111    Conv{1,2,3}D 111    Sigmoid 111    Tanh 53\\frac{5}{3}35\u200b    ReLU 2\\sqrt{2}2\u200b    Leaky Relu 21+negative_slope2\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}1+negative_slope22\u200b\u200b       Parameters  nonlinearity \u2013 the non-linear function (nn.functional name) param \u2013 optional parameter for the non-linear function    Examples &gt;&gt;&gt; gain = nn.init.calculate_gain('leaky_relu', 0.2)  # leaky_relu with negative_slope=0.2   ", "parameters": ["nonlinearity : the non-linear function (nn.functional name)", "param : optional parameter for the non-linear function"], "returns": null, "example": " gain = nn.init.calculate_gain('leaky_relu', 0.2)  # leaky_relu with negative_slope=0.2\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.init.uniform_", "item_type": "function", "code": "torch.nn.init.uniform_(tensor,a=0.0,b=1.0)", "description": "Fills the input Tensor with values drawn from the uniform distribution U(a,b)\\mathcal{U}(a, b)U(a,b)  .  Parameters  tensor \u2013 an n-dimensional torch.Tensor a \u2013 the lower bound of the uniform distribution b \u2013 the upper bound of the uniform distribution    Examples &gt;&gt;&gt; w = torch.empty(3, 5) &gt;&gt;&gt; nn.init.uniform_(w)   ", "parameters": ["tensor : an n-dimensional torch.Tensor", "a : the lower bound of the uniform distribution", "b : the upper bound of the uniform distribution"], "returns": null, "example": " w = torch.empty(3, 5)\n nn.init.uniform_(w)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.utils.tensorboard.writer.SummaryWriter.add_images", "item_type": "method", "code": "add_images(tag,img_tensor,global_step=None,walltime=None,dataformats='NCHW')", "description": "Add batched image data to summary. Note that this requires the pillow package.  Parameters  tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (python:int) \u2013 Global step value to record walltime (python:float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event dataformats (string) \u2013 Image data format specification of the form NCHW, NHWC, CHW, HWC, HW, WH, etc.     Shape:img_tensor: Default is (N,3,H,W)(N, 3, H, W)(N,3,H,W)  . If dataformats is specified, other shape will be accepted. e.g. NCHW or NHWC.   Examples: from torch.utils.tensorboard import SummaryWriter import numpy as np  img_batch = np.zeros((16, 3, 100, 100)) for i in range(16):     img_batch[i, 0] = np.arange(0, 10000).reshape(100, 100) / 10000 / 16 * i     img_batch[i, 1] = (1 - np.arange(0, 10000).reshape(100, 100) / 10000) / 16 * i  writer = SummaryWriter() writer.add_images('my_image_batch', img_batch, 0) writer.close()   Expected result:  ", "parameters": ["tag (string) : Data identifier", "img_tensor (torch.Tensor, numpy.array, or string/blobname) : Image data", "global_step (python:int) : Global step value to record", "walltime (python:float) : Optional override default walltime (time.time())seconds after epoch of event", "dataformats (string) : Image data format specification of the formNCHW, NHWC, CHW, HWC, HW, WH, etc."], "returns": null, "example": "from torch.utils.tensorboard import SummaryWriter\nimport numpy as np\n\nimg_batch = np.zeros((16, 3, 100, 100))\nfor i in range(16):\n    img_batch[i, 0] = np.arange(0, 10000).reshape(100, 100) / 10000 / 16 * i\n    img_batch[i, 1] = (1 - np.arange(0, 10000).reshape(100, 100) / 10000) / 16 * i\n\nwriter = SummaryWriter()\nwriter.add_images('my_image_batch', img_batch, 0)\nwriter.close()\n\n", "shape": "img_tensor: Default is (N,3,H,W)(N, 3, H, W)(N,3,H,W)  . If dataformats is specified, other shape will be accepted. e.g. NCHW or NHWC. "},
{"library": "torch", "item_id": "torch.utils.tensorboard.writer.SummaryWriter.add_figure", "item_type": "method", "code": "add_figure(tag,figure,global_step=None,close=True,walltime=None)", "description": "Render matplotlib figure into an image and add it to summary. Note that this requires the matplotlib package.  Parameters  tag (string) \u2013 Data identifier figure (matplotlib.pyplot.figure) \u2013 Figure or a list of figures global_step (python:int) \u2013 Global step value to record close (bool) \u2013 Flag to automatically close the figure walltime (python:float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event    ", "parameters": ["tag (string) : Data identifier", "figure (matplotlib.pyplot.figure) : Figure or a list of figures", "global_step (python:int) : Global step value to record", "close (bool) : Flag to automatically close the figure", "walltime (python:float) : Optional override default walltime (time.time())seconds after epoch of event"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.utils.tensorboard.writer.SummaryWriter.add_video", "item_type": "method", "code": "add_video(tag,vid_tensor,global_step=None,fps=4,walltime=None)", "description": "Add video data to summary. Note that this requires the moviepy package.  Parameters  tag (string) \u2013 Data identifier vid_tensor (torch.Tensor) \u2013 Video data global_step (python:int) \u2013 Global step value to record fps (python:float or python:int) \u2013 Frames per second walltime (python:float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event     Shape:vid_tensor: (N,T,C,H,W)(N, T, C, H, W)(N,T,C,H,W)  . The values should lie in [0, 255] for type uint8 or [0, 1] for type float.   ", "parameters": ["tag (string) : Data identifier", "vid_tensor (torch.Tensor) : Video data", "global_step (python:int) : Global step value to record", "fps (python:float or python:int) : Frames per second", "walltime (python:float) : Optional override default walltime (time.time())seconds after epoch of event"], "returns": null, "example": "NA", "shape": "vid_tensor: (N,T,C,H,W)(N, T, C, H, W)(N,T,C,H,W)  . The values should lie in [0, 255] for type uint8 or [0, 1] for type float. "},
{"library": "torch", "item_id": "torch.utils.tensorboard.writer.SummaryWriter.add_audio", "item_type": "method", "code": "add_audio(tag,snd_tensor,global_step=None,sample_rate=44100,walltime=None)", "description": "Add audio data to summary.  Parameters  tag (string) \u2013 Data identifier snd_tensor (torch.Tensor) \u2013 Sound data global_step (python:int) \u2013 Global step value to record sample_rate (python:int) \u2013 sample rate in Hz walltime (python:float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event     Shape:snd_tensor: (1,L)(1, L)(1,L)  . The values should lie between [-1, 1].   ", "parameters": ["tag (string) : Data identifier", "snd_tensor (torch.Tensor) : Sound data", "global_step (python:int) : Global step value to record", "sample_rate (python:int) : sample rate in Hz", "walltime (python:float) : Optional override default walltime (time.time())seconds after epoch of event"], "returns": null, "example": "NA", "shape": "snd_tensor: (1,L)(1, L)(1,L)  . The values should lie between [-1, 1]. "},
{"library": "torch", "item_id": "torch.jit.script", "item_type": "function", "code": "torch.jit.script(obj)", "description": "Scripting a function or nn.Module will inspect the source code, compile it as TorchScript code using the TorchScript compiler, and return a ScriptModule or ScriptFunction. TorchScript itself is a subset of the Python language, so not all features in Python work, but we provide enough functionality to compute on tensors and do control-dependent operations. For a complete guide, see the TorchScript Language Reference. torch.jit.script can be used as a function for modules and functions, and as a decorator @torch.jit.script for TorchScript Classes and functions.  Scripting a functionThe @torch.jit.script decorator will construct a ScriptFunction by compiling the body of the function. Example (scripting a function): import torch  @torch.jit.script def foo(x, y):     if x.max() &gt; y.max():         r = x     else:         r = y     return r  print(type(foo))  # torch.jit.ScriptFuncion  # See the compiled graph as Python code print(foo.code)  # Call the function using the TorchScript interpreter foo(torch.ones(2, 2), torch.ones(2, 2))    Scripting an nn.ModuleScripting an nn.Module by default will compile the forward method and recursively compile any methods, submodules, and functions called by forward. If a nn.Module only uses features supported in TorchScript, no changes to the original module code should be necessary. script will construct ScriptModule that has copies of the attributes, parameters, and methods of the original module. Example (scripting a simple module with a Parameter): import torch  class MyModule(torch.nn.Module):     def __init__(self, N, M):         super(MyModule, self).__init__()         # This parameter will be copied to the new ScriptModule         self.weight = torch.nn.Parameter(torch.rand(N, M))          # When this submodule is used, it will be compiled         self.linear = torch.nn.Linear(N, M)      def forward(self, input):         output = self.weight.mv(input)          # This calls the `forward` method of the `nn.Linear` module, which will         # cause the `self.linear` submodule to be compiled to a `ScriptModule` here         output = self.linear(output)         return output  scripted_module = torch.jit.script(MyModule(2, 3))   Example (scripting a module with traced submodules): import torch import torch.nn as nn import torch.nn.functional as F  class MyModule(nn.Module):     def __init__(self):         super(MyModule, self).__init__()         # torch.jit.trace produces a ScriptModule's conv1 and conv2         self.conv1 = torch.jit.trace(nn.Conv2d(1, 20, 5), torch.rand(1, 1, 16, 16))         self.conv2 = torch.jit.trace(nn.Conv2d(20, 20, 5), torch.rand(1, 20, 16, 16))      def forward(self, input):       input = F.relu(self.conv1(input))       input = F.relu(self.conv2(input))       return input  scripted_module = torch.jit.script(MyModule())   To compile a method other than forward (and recursively compile anything it calls), add the @torch.jit.export decorator to the method. To opt out of compilation use @torch.jit.ignore. Example (an exported and ignored method in a module): import torch import torch.nn as nn  class MyModule(nn.Module):     def __init__(self):         super(MyModule, self).__init__()      @torch.jit.export     def some_entry_point(self, input):         return input + 10      @torch.jit.ignore     def python_only_fn(self, input):         # This function won't be compiled, so any         # Python APIs can be used         import pdb         pdb.set_trace()      def forward(self, input):         if self.training:             self.python_only_fn(input)         return input * 99  scripted_module = torch.jit.script(MyModule()) print(scripted_module.some_entry_point(torch.randn(2, 2))) print(scripted_module(torch.randn(2, 2)))     ", "parameters": [], "returns": null, "example": "import torch\n\n@torch.jit.script\ndef foo(x, y):\n    if x.max()  y.max():\n        r = x\n    else:\n        r = y\n    return r\n\nprint(type(foo))  # torch.jit.ScriptFuncion\n\n# See the compiled graph as Python code\nprint(foo.code)\n\n# Call the function using the TorchScript interpreter\nfoo(torch.ones(2, 2), torch.ones(2, 2))\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.utils.data.TensorDataset", "item_type": "class", "code": "classtorch.utils.data.TensorDataset(*tensors)", "description": "Dataset wrapping tensors. Each sample will be retrieved by indexing tensors along the first dimension.  Parameters *tensors (Tensor) \u2013 tensors that have the same size of the first dimension.   ", "parameters": ["*tensors (Tensor) : tensors that have the same size of the first dimension."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.utils.data.ConcatDataset", "item_type": "class", "code": "classtorch.utils.data.ConcatDataset(datasets)", "description": "Dataset as a concatenation of multiple datasets. This class is useful to assemble different existing datasets.  Parameters datasets (sequence) \u2013 List of datasets to be concatenated   ", "parameters": ["datasets (sequence) : List of datasets to be concatenated"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.utils.data.ChainDataset", "item_type": "class", "code": "classtorch.utils.data.ChainDataset(datasets)", "description": "Dataset for chainning multiple IterableDataset s. This class is useful to assemble different existing dataset streams. The chainning operation is done on-the-fly, so concatenating large-scale datasets with this class will be efficient.  Parameters datasets (iterable of IterableDataset) \u2013 datasets to be chained together   ", "parameters": ["datasets (iterable of IterableDataset) : datasets to be chained together"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.utils.data.Subset", "item_type": "class", "code": "classtorch.utils.data.Subset(dataset,indices)", "description": "Subset of a dataset at specified indices.  Parameters  dataset (Dataset) \u2013 The whole Dataset indices (sequence) \u2013 Indices in the whole set selected for subset    ", "parameters": ["dataset (Dataset) : The whole Dataset", "indices (sequence) : Indices in the whole set selected for subset"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.utils.data.Sampler", "item_type": "class", "code": "classtorch.utils.data.Sampler(data_source)", "description": "Base class for all Samplers. Every Sampler subclass has to provide an __iter__() method, providing a way to iterate over indices of dataset elements, and a __len__() method that returns the length of the returned iterators.  Note The __len__() method isn\u2019t strictly required by DataLoader, but is expected in any calculation involving the length of a DataLoader.  ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.utils.data.SequentialSampler", "item_type": "class", "code": "classtorch.utils.data.SequentialSampler(data_source)", "description": "Samples elements sequentially, always in the same order.  Parameters data_source (Dataset) \u2013 dataset to sample from   ", "parameters": ["data_source (Dataset) : dataset to sample from"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.utils.data.RandomSampler", "item_type": "class", "code": "classtorch.utils.data.RandomSampler(data_source,replacement=False,num_samples=None)", "description": "Samples elements randomly. If without replacement, then sample from a shuffled dataset. If with replacement, then user can specify num_samples to draw.  Parameters  data_source (Dataset) \u2013 dataset to sample from replacement (bool) \u2013 samples are drawn with replacement if True, default=``False`` num_samples (python:int) \u2013 number of samples to draw, default=`len(dataset)`. This argument is supposed to be specified only when replacement is True.    ", "parameters": ["data_source (Dataset) : dataset to sample from", "replacement (bool) : samples are drawn with replacement if True, default=``False``", "num_samples (python:int) : number of samples to draw, default=`len(dataset)`. This argumentis supposed to be specified only when replacement is True."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.utils.data.SubsetRandomSampler", "item_type": "class", "code": "classtorch.utils.data.SubsetRandomSampler(indices)", "description": "Samples elements randomly from a given list of indices, without replacement.  Parameters indices (sequence) \u2013 a sequence of indices   ", "parameters": ["indices (sequence) : a sequence of indices"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.random.set_rng_state(new_state)", "description": "Sets the random number generator state.  Parameters new_state (torch.ByteTensor) \u2013 The desired state   ", "parameters": ["new_state (torch.ByteTensor) : The desired state"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.random.manual_seed(seed)", "description": "Sets the seed for generating random numbers. Returns a torch.Generator object.  Parameters seed (python:int) \u2013 The desired seed.   ", "parameters": ["seed (python:int) : The desired seed."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.random.seed()", "description": "Sets the seed for generating random numbers to a non-deterministic random number. Returns a 64 bit number used to seed the RNG. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.random.initial_seed()", "description": "Returns the initial seed for generating random numbers as a Python long. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.random.fork_rng(devices=None,enabled=True,_caller='fork_rng',_devices_kw='devices')", "description": "Forks the RNG, so that when you return, the RNG is reset to the state that it was previously in.  Parameters  devices (iterable of CUDA IDs) \u2013 CUDA devices for which to fork the RNG.  CPU RNG state is always forked.  By default, fork_rng() operates on all devices, but will emit a warning if your machine has a lot of devices, since this function will run very slowly in that case. If you explicitly specify devices, this warning will be suppressed enabled (bool) \u2013 if False, the RNG is not forked.  This is a convenience argument for easily disabling the context manager without having to delete it and unindent your Python code under it.    ", "parameters": ["devices (iterable of CUDA IDs) : CUDA devices for which to forkthe RNG.  CPU RNG state is always forked.  By default, fork_rng() operateson all devices, but will emit a warning if your machine has a lotof devices, since this function will run very slowly in that case.If you explicitly specify devices, this warning will be suppressed", "enabled (bool) : if False, the RNG is not forked.  This is a convenienceargument for easily disabling the context manager without havingto delete it and unindent your Python code under it."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributed.rpc.remote", "item_type": "function", "code": "torch.distributed.rpc.remote(to,func,args=None,kwargs=None)", "description": "Make a remote call to run func on worker to and return an RRef to the result value immediately. Worker to will be the owner of the returned RRef, and the worker calling remote is a user. The owner manages the global reference count of its RRef, and the owner RRef is only destructed when globally there are no living references to it.  Parameters  to (str or WorkerInfo) \u2013 id or name of the destination worker. func (callable) \u2013 builtin functions (like torch.add()). args (tuple) \u2013 the argument tuple for the func invocation. kwargs (dict) \u2013 is a dictionary of keyword arguments for the func invocation.   Returns A user RRef instance to the result value. Use the blocking API torch.distributed.rpc.RRef.to_here() to retrieve the result value locally.   Example: On worker 0: &gt;&gt;&gt; import torch.distributed.rpc as rpc &gt;&gt;&gt; rpc.init_rpc(\"worker0\", rank=0, world_size=2) &gt;&gt;&gt; rref1 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 3)) &gt;&gt;&gt; rref2 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 1)) &gt;&gt;&gt; x = rref1.to_here() + rref2.to_here() &gt;&gt;&gt; rpc.shutdown()  On worker 1: &gt;&gt;&gt; import torch.distributed.rpc as rpc &gt;&gt;&gt; rpc.init_rpc(\"worker1\", rank=1, world_size=2) &gt;&gt;&gt; rpc.shutdown()   ", "parameters": ["to (str or WorkerInfo) : id or name of the destination worker.", "func (callable) : builtin functions (like torch.add()).", "args (tuple) : the argument tuple for the func invocation.", "kwargs (dict) : is a dictionary of keyword arguments for the funcinvocation."], "returns": "A user RRef instance to the resultvalue. Use the blocking API torch.distributed.rpc.RRef.to_here()to retrieve the result value locally.", "example": "On worker 0:\n import torch.distributed.rpc as rpc\n rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n rref1 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 3))\n rref2 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 1))\n x = rref1.to_here() + rref2.to_here()\n rpc.shutdown()\n\nOn worker 1:\n import torch.distributed.rpc as rpc\n rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n rpc.shutdown()\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributed.rpc.get_worker_info", "item_type": "function", "code": "torch.distributed.rpc.get_worker_info(worker_name=None)", "description": "Get WorkerInfo of a given worker name. Use this WorkerInfo to avoid passing an expensive string on every invocation.  Parameters worker_name (str) \u2013 the string name of a worker. If None, return the the id of the current worker. (default None)  Returns WorkerInfo instance for the given worker_name or WorkerInfo of the current worker if worker_name is None.   ", "parameters": ["worker_name (str) : the string name of a worker. If None, return thethe id of the current worker. (default None)", "WorkerInfo instance for the givenworker_name or WorkerInfo of thecurrent worker if worker_name is None."], "returns": "WorkerInfo instance for the givenworker_name or WorkerInfo of thecurrent worker if worker_name is None.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributed.rpc.shutdown", "item_type": "function", "code": "torch.distributed.rpc.shutdown(graceful=True)", "description": "Perform a shutdown of the RPC agent, and then destroy the RPC agent. This stops the local agent from  accepting outstanding requests, and shuts down the RPC framework by terminating all RPC threads. If graceful=True, then this will block until all local and remote RPC processes reach this method and wait for all outstanding work to complete. Otherwise, if graceful=False, then this is a local shutdown, and it does not wait for other RPC processes to reach this method.  Parameters graceful (bool) \u2013 Whether to do a graceful shutdown or not. If True, this will block until all local and remote RPC processes have reached this method and wait for all outstanding work to complete.   Example: On worker 0: &gt;&gt;&gt; import torch.distributed.rpc as rpc &gt;&gt;&gt; rpc.init_rpc(\"worker0\", rank=0, world_size=2) &gt;&gt;&gt; # do some work &gt;&gt;&gt; result = rpc.rpc_sync(\"worker1\", torch.add, args=(torch.ones(1), 1)) &gt;&gt;&gt; # ready to shutdown &gt;&gt;&gt; rpc.shutdown()  On worker 1: &gt;&gt;&gt; import torch.distributed.rpc as rpc &gt;&gt;&gt; rpc.init_rpc(\"worker1\", rank=1, world_size=2) &gt;&gt;&gt; # wait for worker 0 to finish work, and then shutdown. &gt;&gt;&gt; rpc.shutdown()   ", "parameters": ["graceful (bool) : Whether to do a graceful shutdown or not. If True,this will block until all local and remote RPCprocesses have reached this method and wait for alloutstanding work to complete."], "returns": null, "example": "On worker 0:\n import torch.distributed.rpc as rpc\n rpc.init_rpc(\"worker0\", rank=0, world_size=2)\n # do some work\n result = rpc.rpc_sync(\"worker1\", torch.add, args=(torch.ones(1), 1))\n # ready to shutdown\n rpc.shutdown()\n\nOn worker 1:\n import torch.distributed.rpc as rpc\n rpc.init_rpc(\"worker1\", rank=1, world_size=2)\n # wait for worker 0 to finish work, and then shutdown.\n rpc.shutdown()\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.FloatStorage.resize_", "item_type": "method", "code": "resize_()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.FloatStorage.share_memory_", "item_type": "method", "code": "share_memory_()", "description": "Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA storages, which do not need to be moved for sharing across processes. Storages in shared memory cannot be resized. Returns: self ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.FloatStorage.short", "item_type": "method", "code": "short()", "description": "Casts this storage to short type ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.FloatStorage.size", "item_type": "method", "code": "size()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.FloatStorage.tolist", "item_type": "method", "code": "tolist()", "description": "Returns a list containing the elements of this storage ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.FloatStorage.type", "item_type": "method", "code": "type(dtype=None,non_blocking=False,**kwargs)", "description": "Returns the type if dtype is not provided, else casts this object to the specified type. If this is already of the correct type, no copy is performed and the original object is returned.  Parameters  dtype (python:type or string) \u2013 The desired type non_blocking (bool) \u2013 If True, and the source is in pinned memory and destination is on the GPU or vice versa, the copy is performed asynchronously with respect to the host. Otherwise, the argument has no effect. **kwargs \u2013 For compatibility, may contain the key async in place of the non_blocking argument. The async arg is deprecated.    ", "parameters": ["dtype (python:type or string) : The desired type", "non_blocking (bool) : If True, and the source is in pinned memoryand destination is on the GPU or vice versa, the copy is performedasynchronously with respect to the host. Otherwise, the argumenthas no effect.", "**kwargs : For compatibility, may contain the key async in place ofthe non_blocking argument. The async arg is deprecated."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.quantization.quantize_dynamic", "item_type": "function", "code": "torch.quantization.quantize_dynamic(model,qconfig_spec=None,dtype=torch.qint8,mapping=None,inplace=False)", "description": "Converts a float model to dynamic (i.e. weights-only) quantized model. Replaces specified modules with dynamic weight-only quantized versions and output the quantized model. For simplest usage provide dtype argument that can be float16 or qint8. Weight-only quantization by default is performed for layers with large weights size - i.e. Linear and RNN variants. Fine grained control is possible with qconfig and mapping that act similarly to quantize(). If qconfig is provided, the dtype argument is ignored.  Parameters  module \u2013 input model qconfig_spec \u2013 Either:  A dictionary that maps from name or type of submodule to quantization configuration, qconfig applies to all submodules of a given module unless qconfig for the submodules are specified (when the submodule already has qconfig attribute). Entries in the dictionary need to be QConfigDynamic instances. A set of types and/or submodule names to apply dynamic quantization to, in which case the dtype argument is used to specifiy the bit-width   inplace \u2013 carry out model transformations in-place, the original module is mutated mapping \u2013 maps type of a submodule to a type of corresponding dynamically quantized version with which the submodule needs to be replaced    ", "parameters": ["module : input model", "qconfig_spec : Either:A dictionary that maps from name or type of submodule to quantizationconfiguration, qconfig applies to all submodules of a givenmodule unless qconfig for the submodules are specified (when thesubmodule already has qconfig attribute). Entries in the dictionaryneed to be QConfigDynamic instances.A set of types and/or submodule names to apply dynamic quantization to,in which case the dtype argument is used to specifiy the bit-width", "A dictionary that maps from name or type of submodule to quantizationconfiguration, qconfig applies to all submodules of a givenmodule unless qconfig for the submodules are specified (when thesubmodule already has qconfig attribute). Entries in the dictionaryneed to be QConfigDynamic instances.", "A set of types and/or submodule names to apply dynamic quantization to,in which case the dtype argument is used to specifiy the bit-width", "inplace : carry out model transformations in-place, the original module is mutated", "mapping : maps type of a submodule to a type of corresponding dynamically quantized versionwith which the submodule needs to be replaced"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.quantization.quantize_qat", "item_type": "function", "code": "torch.quantization.quantize_qat(model,run_fn,run_args,inplace=False)", "description": "Do quantization aware training and output a quantized model  Parameters  model \u2013 input model run_fn \u2013 a function for evaluating the prepared model, can be a function that simply runs the prepared model or a training loop run_args \u2013 positional arguments for run_fn   Returns Quantized model.   ", "parameters": ["model : input model", "run_fn : a function for evaluating the prepared model, can be afunction that simply runs the prepared model or a trainingloop", "run_args : positional arguments for run_fn"], "returns": "Quantized model.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.quantization.prepare", "item_type": "function", "code": "torch.quantization.prepare(model,qconfig_dict=None,inplace=False)", "description": "Prepares a copy of the model for quantization calibration or quantization-aware training. Quantization configuration can be passed as an qconfig_dict or assigned preemptively to individual submodules in .qconfig attribute. The model will be attached with observer or fake quant modules, and qconfig will be propagated.  Parameters  model \u2013 input model to be modified in-place qconfig_dict \u2013 dictionary that maps from name or type of submodule to quantization configuration, qconfig applies to all submodules of a given module unless qconfig for the submodules are specified (when the submodule already has qconfig attribute) inplace \u2013 carry out model transformations in-place, the original module is mutated    ", "parameters": ["model : input model to be modified in-place", "qconfig_dict : dictionary that maps from name or type of submodule to quantizationconfiguration, qconfig applies to all submodules of a givenmodule unless qconfig for the submodules are specified (when thesubmodule already has qconfig attribute)", "inplace : carry out model transformations in-place, the original module is mutated"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.quantization.prepare_qat", "item_type": "function", "code": "torch.quantization.prepare_qat(model,mapping=None,inplace=False)", "description": "Prepares a copy of the model for quantization calibration or quantization-aware training and convers it to quantized version. Quantization configuration can be passed as an qconfig_dict or assigned preemptively to individual submodules in .qconfig attribute.  Parameters  model \u2013 input model to be modified in-place mapping \u2013 dictionary that maps float modules to quantized modules to be replaced. inplace \u2013 carry out model transformations in-place, the original module is mutated    ", "parameters": ["model : input model to be modified in-place", "mapping : dictionary that maps float modules to quantized modules to bereplaced.", "inplace : carry out model transformations in-place, the original moduleis mutated"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.quantization.convert", "item_type": "function", "code": "torch.quantization.convert(module,mapping=None,inplace=False)", "description": "Converts the float module with observers (where we can get quantization parameters) to a quantized module.  Parameters  module \u2013 calibrated module with observers mapping \u2013 a dictionary that maps from float module type to quantized module type, can be overwrritten to allow swapping user defined Modules inplace \u2013 carry out model transformations in-place, the original module is mutated    ", "parameters": ["module : calibrated module with observers", "mapping : a dictionary that maps from float module type to quantizedmodule type, can be overwrritten to allow swapping user definedModules", "inplace : carry out model transformations in-place, the original moduleis mutated"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.quantization.fuse_modules", "item_type": "function", "code": "torch.quantization.fuse_modules(model,modules_to_fuse,inplace=False,fuser_func=&lt;functionfuse_known_modules&gt;)", "description": "Fuses a list of modules into a single module Fuses only the following sequence of modules:  conv, bn conv, bn, relu conv, relu linear, relu  All other sequences are left unchanged. For these sequences, replaces the first item in the list with the fused module, replacing the rest of the modules with identity.  Parameters  model \u2013 Model containing the modules to be fused modules_to_fuse \u2013 list of list of module names to fuse. Can also be a list of strings if there is only a single list of modules to fuse. inplace \u2013 bool specifying if fusion happens in place on the model, by default a new model is returned fuser_func \u2013 Function that takes in a list of modules and outputs a list of fused modules of the same length. For example, fuser_func([convModule, BNModule]) returns the list [ConvBNModule, nn.Identity()] Defaults to torch.quantization.fuse_known_modules   Returns model with fused modules. A new copy is created if inplace=True.   Examples: &gt;&gt;&gt; m = myModel() &gt;&gt;&gt; # m is a module containing  the sub-modules below &gt;&gt;&gt; modules_to_fuse = [ ['conv1', 'bn1', 'relu1'], ['submodule.conv', 'submodule.relu']] &gt;&gt;&gt; fused_m = torch.quantization.fuse_modules(m, modules_to_fuse) &gt;&gt;&gt; output = fused_m(input)  &gt;&gt;&gt; m = myModel() &gt;&gt;&gt; # Alternately provide a single list of modules to fuse &gt;&gt;&gt; modules_to_fuse = ['conv1', 'bn1', 'relu1'] &gt;&gt;&gt; fused_m = torch.quantization.fuse_modules(m, modules_to_fuse) &gt;&gt;&gt; output = fused_m(input)   ", "parameters": ["model : Model containing the modules to be fused", "modules_to_fuse : list of list of module names to fuse. Can also be a listof strings if there is only a single list of modules to fuse.", "inplace : bool specifying if fusion happens in place on the model, by defaulta new model is returned", "fuser_func : Function that takes in a list of modules and outputs a list of fused modulesof the same length. For example,fuser_func([convModule, BNModule]) returns the list [ConvBNModule, nn.Identity()]Defaults to torch.quantization.fuse_known_modules"], "returns": "model with fused modules. A new copy is created if inplace=True.", "example": " m = myModel()\n # m is a module containing  the sub-modules below\n modules_to_fuse = [ ['conv1', 'bn1', 'relu1'], ['submodule.conv', 'submodule.relu']]\n fused_m = torch.quantization.fuse_modules(m, modules_to_fuse)\n output = fused_m(input)\n\n m = myModel()\n # Alternately provide a single list of modules to fuse\n modules_to_fuse = ['conv1', 'bn1', 'relu1']\n fused_m = torch.quantization.fuse_modules(m, modules_to_fuse)\n output = fused_m(input)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.quantization.add_quant_dequant", "item_type": "function", "code": "torch.quantization.add_quant_dequant(module)", "description": "Wrap the leaf child module in QuantWrapper if it has a valid qconfig Note that this function will modify the children of module inplace and it can return a new module which wraps the input module as well.  Parameters  module \u2013 input module with qconfig attributes for all the leaf modules we want to quantize (that) \u2013    Returns Either the inplace modified module with submodules wrapped in QuantWrapper based on qconfig or a new QuantWrapper module which wraps the input module, the latter case only happens when the input module is a leaf module and we want to quantize it.   ", "parameters": ["module : input module with qconfig attributes for all the leaf modules", "we want to quantize (that) : "], "returns": "Either the inplace modified module with submodules wrapped inQuantWrapper based on qconfig or a new QuantWrapper module whichwraps the input module, the latter case only happens when the inputmodule is a leaf module and we want to quantize it.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.optim.Optimizer.load_state_dict", "item_type": "method", "code": "load_state_dict(state_dict)", "description": "Loads the optimizer state.  Parameters state_dict (dict) \u2013 optimizer state. Should be an object returned from a call to state_dict().   ", "parameters": ["state_dict (dict) : optimizer state. Should be an object returnedfrom a call to state_dict()."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.optim.Optimizer.state_dict", "item_type": "method", "code": "state_dict()", "description": "Returns the state of the optimizer as a dict. It contains two entries:   state - a dict holding current optimization state. Its contentdiffers between optimizer classes.    param_groups - a dict containing all parameter groups  ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.optim.Optimizer.step", "item_type": "method", "code": "step(closure)", "description": "Performs a single optimization step (parameter update).  Parameters closure (callable) \u2013 A closure that reevaluates the model and returns the loss. Optional for most optimizers.   ", "parameters": ["closure (callable) : A closure that reevaluates the model andreturns the loss. Optional for most optimizers."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.optim.Optimizer.zero_grad", "item_type": "method", "code": "zero_grad()", "description": "Clears the gradients of all optimized torch.Tensor s. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.optim.Adadelta.step", "item_type": "method", "code": "step(closure=None)", "description": "Performs a single optimization step.  Parameters closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.   ", "parameters": ["closure (callable, optional) : A closure that reevaluates the modeland returns the loss."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.optim.Adagrad.step", "item_type": "method", "code": "step(closure=None)", "description": "Performs a single optimization step.  Parameters closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.   ", "parameters": ["closure (callable, optional) : A closure that reevaluates the modeland returns the loss."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.optim.Adam.step", "item_type": "method", "code": "step(closure=None)", "description": "Performs a single optimization step.  Parameters closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.   ", "parameters": ["closure (callable, optional) : A closure that reevaluates the modeland returns the loss."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.optim.AdamW.step", "item_type": "method", "code": "step(closure=None)", "description": "Performs a single optimization step.  Parameters closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.   ", "parameters": ["closure (callable, optional) : A closure that reevaluates the modeland returns the loss."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.onnx.register_custom_op_symbolic", "item_type": "function", "code": "torch.onnx.register_custom_op_symbolic(symbolic_name,symbolic_fn,opset_version)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.onnx.operators.shape_as_tensor", "item_type": "function", "code": "torch.onnx.operators.shape_as_tensor(x)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.onnx.set_training", "item_type": "function", "code": "torch.onnx.set_training(model,mode)", "description": "A context manager to temporarily set the training mode of \u2018model\u2019 to \u2018mode\u2019, resetting it when we exit the with-block.  A no-op if mode is None. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.onnx.is_in_onnx_export", "item_type": "function", "code": "torch.onnx.is_in_onnx_export()", "description": "Check whether it\u2019s in the middle of the ONNX export. This function returns True in the middle of torch.onnx.export(). torch.onnx.export should be executed with single thread. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.datasets.LSUN", "item_type": "class", "code": "classtorchvision.datasets.LSUN(root,classes='train',transform=None,target_transform=None)", "description": "LSUN dataset.  Parameters  root (string) \u2013 Root directory for the database files. classes (string or list) \u2013 One of {\u2018train\u2019, \u2018val\u2019, \u2018test\u2019} or a list of categories to load. e,g. [\u2018bedroom_train\u2019, \u2018church_train\u2019]. transform (callable, optional) \u2013 A function/transform that  takes in an PIL image and returns a transformed version. E.g, transforms.RandomCrop target_transform (callable, optional) \u2013 A function/transform that takes in the target and transforms it.      __getitem__(index)   Parameters index (python:int) \u2013 Index  Returns Tuple (image, target) where target is the index of the target category.  Return type tuple     ", "parameters": ["root (string) : Root directory for the database files.", "classes (string or list) : One of {\u2018train\u2019, \u2018val\u2019, \u2018test\u2019} or a list ofcategories to load. e,g. [\u2018bedroom_train\u2019, \u2018church_train\u2019].", "transform (callable, optional) : A function/transform that  takes in an PIL imageand returns a transformed version. E.g, transforms.RandomCrop", "target_transform (callable, optional) : A function/transform that takes in thetarget and transforms it."], "returns": "Tuple (image, target) where target is the index of the target category.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.datasets.ImageFolder", "item_type": "class", "code": "classtorchvision.datasets.ImageFolder(root,transform=None,target_transform=None,loader=&lt;functiondefault_loader&gt;,is_valid_file=None)", "description": "A generic data loader where the images are arranged in this way: root/dog/xxx.png root/dog/xxy.png root/dog/xxz.png  root/cat/123.png root/cat/nsdf3.png root/cat/asd932_.png    Parameters  root (string) \u2013 Root directory path. transform (callable, optional) \u2013 A function/transform that  takes in an PIL image and returns a transformed version. E.g, transforms.RandomCrop target_transform (callable, optional) \u2013 A function/transform that takes in the target and transforms it. loader (callable, optional) \u2013 A function to load an image given its path. is_valid_file \u2013 A function that takes path of an Image file and check if the file is a valid file (used to check of corrupt files)      __getitem__(index)  Parameters index (python:int) \u2013 Index  Returns (sample, target) where target is class_index of the target class.  Return type tuple     ", "parameters": ["root (string) : Root directory path.", "transform (callable, optional) : A function/transform that  takes in an PIL imageand returns a transformed version. E.g, transforms.RandomCrop", "target_transform (callable, optional) : A function/transform that takes in thetarget and transforms it.", "loader (callable, optional) : A function to load an image given its path.", "is_valid_file : A function that takes path of an Image fileand check if the file is a valid file (used to check of corrupt files)"], "returns": "(sample, target) where target is class_index of the target class.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.datasets.DatasetFolder", "item_type": "class", "code": "classtorchvision.datasets.DatasetFolder(root,loader,extensions=None,transform=None,target_transform=None,is_valid_file=None)", "description": "A generic data loader where the samples are arranged in this way: root/class_x/xxx.ext root/class_x/xxy.ext root/class_x/xxz.ext  root/class_y/123.ext root/class_y/nsdf3.ext root/class_y/asd932_.ext    Parameters  root (string) \u2013 Root directory path. loader (callable) \u2013 A function to load a sample given its path. extensions (tuple[string]) \u2013 A list of allowed extensions. both extensions and is_valid_file should not be passed. transform (callable, optional) \u2013 A function/transform that takes in a sample and returns a transformed version. E.g, transforms.RandomCrop for images. target_transform (callable, optional) \u2013 A function/transform that takes in the target and transforms it. is_valid_file \u2013 A function that takes path of a file and check if the file is a valid file (used to check of corrupt files) both extensions and is_valid_file should not be passed.      __getitem__(index)   Parameters index (python:int) \u2013 Index  Returns (sample, target) where target is class_index of the target class.  Return type tuple     ", "parameters": ["root (string) : Root directory path.", "loader (callable) : A function to load a sample given its path.", "extensions (tuple[string]) : A list of allowed extensions.both extensions and is_valid_file should not be passed.", "transform (callable, optional) : A function/transform that takes ina sample and returns a transformed version.E.g, transforms.RandomCrop for images.", "target_transform (callable, optional) : A function/transform that takesin the target and transforms it.", "is_valid_file : A function that takes path of a fileand check if the file is a valid file (used to check of corrupt files)both extensions and is_valid_file should not be passed."], "returns": "(sample, target) where target is class_index of the target class.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.datasets.ImageNet", "item_type": "class", "code": "classtorchvision.datasets.ImageNet(root,split='train',download=None,**kwargs)", "description": "ImageNet 2012 Classification Dataset.  Parameters  root (string) \u2013 Root directory of the ImageNet Dataset. split (string, optional) \u2013 The dataset split, supports train, or val. transform (callable, optional) \u2013 A function/transform that  takes in an PIL image and returns a transformed version. E.g, transforms.RandomCrop target_transform (callable, optional) \u2013 A function/transform that takes in the target and transforms it. loader \u2013 A function to load an image given its path.    ", "parameters": ["root (string) : Root directory of the ImageNet Dataset.", "split (string, optional) : The dataset split, supports train, or val.", "transform (callable, optional) : A function/transform that  takes in an PIL imageand returns a transformed version. E.g, transforms.RandomCrop", "target_transform (callable, optional) : A function/transform that takes in thetarget and transforms it.", "loader : A function to load an image given its path."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.sparse.FloatTensor.add", "item_type": "method", "code": "add()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.sparse.FloatTensor.add_", "item_type": "method", "code": "add_()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.sparse.FloatTensor.clone", "item_type": "method", "code": "clone()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.sparse.FloatTensor.dim", "item_type": "method", "code": "dim()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.sparse.FloatTensor.div", "item_type": "method", "code": "div()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.sparse.FloatTensor.div_", "item_type": "method", "code": "div_()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.sparse.FloatTensor.get_device", "item_type": "method", "code": "get_device()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.sparse.FloatTensor.hspmm", "item_type": "method", "code": "hspmm()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.sparse.FloatTensor.mm", "item_type": "method", "code": "mm()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.sparse.FloatTensor.mul", "item_type": "method", "code": "mul()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.sparse.FloatTensor.mul_", "item_type": "method", "code": "mul_()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.models.mnasnet0_5", "item_type": "function", "code": "torchvision.models.mnasnet0_5(pretrained=False,progress=True,**kwargs)", "description": "MNASNet with depth multiplier of 0.5 from \u201cMnasNet: Platform-Aware Neural Architecture Search for Mobile\u201d. :param pretrained: If True, returns a model pre-trained on ImageNet :type pretrained: bool :param progress: If True, displays a progress bar of the download to stderr :type progress: bool ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.models.mnasnet0_75", "item_type": "function", "code": "torchvision.models.mnasnet0_75(pretrained=False,progress=True,**kwargs)", "description": "MNASNet with depth multiplier of 0.75 from \u201cMnasNet: Platform-Aware Neural Architecture Search for Mobile\u201d. :param pretrained: If True, returns a model pre-trained on ImageNet :type pretrained: bool :param progress: If True, displays a progress bar of the download to stderr :type progress: bool ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.models.mnasnet1_0", "item_type": "function", "code": "torchvision.models.mnasnet1_0(pretrained=False,progress=True,**kwargs)", "description": "MNASNet with depth multiplier of 1.0 from \u201cMnasNet: Platform-Aware Neural Architecture Search for Mobile\u201d. :param pretrained: If True, returns a model pre-trained on ImageNet :type pretrained: bool :param progress: If True, displays a progress bar of the download to stderr :type progress: bool ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.models.mnasnet1_3", "item_type": "function", "code": "torchvision.models.mnasnet1_3(pretrained=False,progress=True,**kwargs)", "description": "MNASNet with depth multiplier of 1.3 from \u201cMnasNet: Platform-Aware Neural Architecture Search for Mobile\u201d. :param pretrained: If True, returns a model pre-trained on ImageNet :type pretrained: bool :param progress: If True, displays a progress bar of the download to stderr :type progress: bool ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.models.segmentation.fcn_resnet50", "item_type": "function", "code": "torchvision.models.segmentation.fcn_resnet50(pretrained=False,progress=True,num_classes=21,aux_loss=None,**kwargs)", "description": "Constructs a Fully-Convolutional Network model with a ResNet-50 backbone.  Parameters  pretrained (bool) \u2013 If True, returns a model pre-trained on COCO train2017 which contains the same classes as Pascal VOC progress (bool) \u2013 If True, displays a progress bar of the download to stderr    ", "parameters": ["pretrained (bool) : If True, returns a model pre-trained on COCO train2017 whichcontains the same classes as Pascal VOC", "progress (bool) : If True, displays a progress bar of the download to stderr"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.models.segmentation.fcn_resnet101", "item_type": "function", "code": "torchvision.models.segmentation.fcn_resnet101(pretrained=False,progress=True,num_classes=21,aux_loss=None,**kwargs)", "description": "Constructs a Fully-Convolutional Network model with a ResNet-101 backbone.  Parameters  pretrained (bool) \u2013 If True, returns a model pre-trained on COCO train2017 which contains the same classes as Pascal VOC progress (bool) \u2013 If True, displays a progress bar of the download to stderr    ", "parameters": ["pretrained (bool) : If True, returns a model pre-trained on COCO train2017 whichcontains the same classes as Pascal VOC", "progress (bool) : If True, displays a progress bar of the download to stderr"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.models.segmentation.deeplabv3_resnet50", "item_type": "function", "code": "torchvision.models.segmentation.deeplabv3_resnet50(pretrained=False,progress=True,num_classes=21,aux_loss=None,**kwargs)", "description": "Constructs a DeepLabV3 model with a ResNet-50 backbone.  Parameters  pretrained (bool) \u2013 If True, returns a model pre-trained on COCO train2017 which contains the same classes as Pascal VOC progress (bool) \u2013 If True, displays a progress bar of the download to stderr    ", "parameters": ["pretrained (bool) : If True, returns a model pre-trained on COCO train2017 whichcontains the same classes as Pascal VOC", "progress (bool) : If True, displays a progress bar of the download to stderr"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.functional.to_pil_image", "item_type": "function", "code": "torchvision.transforms.functional.to_pil_image(pic,mode=None)", "description": "Convert a tensor or an ndarray to PIL Image. See ToPILImage for more details.  Parameters  pic (Tensor or numpy.ndarray) \u2013 Image to be converted to PIL Image. mode (PIL.Image mode) \u2013 color space and pixel depth of input data (optional).     Returns Image converted to PIL Image.  Return type PIL Image   ", "parameters": ["pic (Tensor or numpy.ndarray) : Image to be converted to PIL Image.", "mode (PIL.Image mode) : color space and pixel depth of input data (optional)."], "returns": "Image converted to PIL Image.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.functional.to_tensor", "item_type": "function", "code": "torchvision.transforms.functional.to_tensor(pic)", "description": "Convert a PIL Image or numpy.ndarray to tensor. See ToTensor for more details.  Parameters pic (PIL Image or numpy.ndarray) \u2013 Image to be converted to tensor.  Returns Converted image.  Return type Tensor   ", "parameters": ["pic (PIL Image or numpy.ndarray) : Image to be converted to tensor.", "Converted image.", "Tensor"], "returns": "Converted image.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.functional.vflip", "item_type": "function", "code": "torchvision.transforms.functional.vflip(img)", "description": "Vertically flip the given PIL Image.  Parameters img (PIL Image) \u2013 Image to be flipped.  Returns Vertically flipped image.  Return type PIL Image   ", "parameters": ["img (PIL Image) : Image to be flipped.", "Vertically flipped image.", "PIL Image"], "returns": "Vertically flipped image.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.Normalize.__call__", "item_type": "method", "code": "__call__(tensor)", "description": " Parameters tensor (Tensor) \u2013 Tensor image of size (C, H, W) to be normalized.  Returns Normalized Tensor image.  Return type Tensor   ", "parameters": ["tensor (Tensor) : Tensor image of size (C, H, W) to be normalized.", "Normalized Tensor image.", "Tensor"], "returns": "Normalized Tensor image.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.ToPILImage.__call__", "item_type": "method", "code": "__call__(pic)", "description": " Parameters pic (Tensor or numpy.ndarray) \u2013 Image to be converted to PIL Image.  Returns Image converted to PIL Image.  Return type PIL Image   ", "parameters": ["pic (Tensor or numpy.ndarray) : Image to be converted to PIL Image.", "Image converted to PIL Image.", "PIL Image"], "returns": "Image converted to PIL Image.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.hub.list", "item_type": "function", "code": "torch.hub.list(github,force_reload=False)", "description": "List all entrypoints available in github hubconf.  Parameters  github (string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional tag/branch. The default branch is master if not specified. Example: \u2018pytorch/vision[:hub]\u2019 force_reload (bool, optional) \u2013 whether to discard the existing cache and force a fresh download. Default is False.   Returns a list of available entrypoint names  Return type entrypoints   Example &gt;&gt;&gt; entrypoints = torch.hub.list('pytorch/vision', force_reload=True)   ", "parameters": ["github (string) : a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optionaltag/branch. The default branch is master if not specified.Example: \u2018pytorch/vision[:hub]\u2019", "force_reload (bool, optional) : whether to discard the existing cache and force a fresh download.Default is False."], "returns": "a list of available entrypoint names", "example": " entrypoints = torch.hub.list('pytorch/vision', force_reload=True)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.hub.help", "item_type": "function", "code": "torch.hub.help(github,model,force_reload=False)", "description": "Show the docstring of entrypoint model.  Parameters  github (string) \u2013 a string with format &lt;repo_owner/repo_name[:tag_name]&gt; with an optional tag/branch. The default branch is master if not specified. Example: \u2018pytorch/vision[:hub]\u2019 model (string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py force_reload (bool, optional) \u2013 whether to discard the existing cache and force a fresh download. Default is False.    Example &gt;&gt;&gt; print(torch.hub.help('pytorch/vision', 'resnet18', force_reload=True))   ", "parameters": ["github (string) : a string with format &lt;repo_owner/repo_name[:tag_name]&gt; with an optionaltag/branch. The default branch is master if not specified.Example: \u2018pytorch/vision[:hub]\u2019", "model (string) : a string of entrypoint name defined in repo\u2019s hubconf.py", "force_reload (bool, optional) : whether to discard the existing cache and force a fresh download.Default is False."], "returns": null, "example": " print(torch.hub.help('pytorch/vision', 'resnet18', force_reload=True))\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.hub.load", "item_type": "function", "code": "torch.hub.load(github,model,*args,**kwargs)", "description": "Load a model from a github repo, with pretrained weights.  Parameters  github (string) \u2013 a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optional tag/branch. The default branch is master if not specified. Example: \u2018pytorch/vision[:hub]\u2019 model (string) \u2013 a string of entrypoint name defined in repo\u2019s hubconf.py *args (optional) \u2013 the corresponding args for callable model. force_reload (bool, optional) \u2013 whether to force a fresh download of github repo unconditionally. Default is False. verbose (bool, optional) \u2013 If False, mute messages about hitting local caches. Note that the message about first download is cannot be muted. Default is True. **kwargs (optional) \u2013 the corresponding kwargs for callable model.   Returns a single model with corresponding pretrained weights.   Example &gt;&gt;&gt; model = torch.hub.load('pytorch/vision', 'resnet50', pretrained=True)   ", "parameters": ["github (string) : a string with format \u201crepo_owner/repo_name[:tag_name]\u201d with an optionaltag/branch. The default branch is master if not specified.Example: \u2018pytorch/vision[:hub]\u2019", "model (string) : a string of entrypoint name defined in repo\u2019s hubconf.py", "*args (optional) : the corresponding args for callable model.", "force_reload (bool, optional) : whether to force a fresh download of github repo unconditionally.Default is False.", "verbose (bool, optional) : If False, mute messages about hitting local caches. Note that the messageabout first download is cannot be muted.Default is True.", "**kwargs (optional) : the corresponding kwargs for callable model."], "returns": "a single model with corresponding pretrained weights.", "example": " model = torch.hub.load('pytorch/vision', 'resnet50', pretrained=True)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.kl.kl_divergence", "item_type": "function", "code": "torch.distributions.kl.kl_divergence(p,q)", "description": "Compute Kullback-Leibler divergence KL(p\u2225q)KL(p \\| q)KL(p\u2225q)   between two distributions.  KL(p\u2225q)=\u222bp(x)log\u2061p(x)q(x)\u2009dxKL(p \\| q) = \\int p(x) \\log\\frac {p(x)} {q(x)} \\,dxKL(p\u2225q)=\u222bp(x)logq(x)p(x)\u200bdx   Parameters  p (Distribution) \u2013 A Distribution object. q (Distribution) \u2013 A Distribution object.   Returns A batch of KL divergences of shape batch_shape.  Return type Tensor  Raises NotImplementedError \u2013 If the distribution types have not been registered via     register_kl().   ", "parameters": ["p (Distribution) : A Distribution object.", "q (Distribution) : A Distribution object."], "returns": "A batch of KL divergences of shape batch_shape.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.init.normal_", "item_type": "function", "code": "torch.nn.init.normal_(tensor,mean=0.0,std=1.0)", "description": "Fills the input Tensor with values drawn from the normal distribution N(mean,std2)\\mathcal{N}(\\text{mean}, \\text{std}^2)N(mean,std2)  .  Parameters  tensor \u2013 an n-dimensional torch.Tensor mean \u2013 the mean of the normal distribution std \u2013 the standard deviation of the normal distribution    Examples &gt;&gt;&gt; w = torch.empty(3, 5) &gt;&gt;&gt; nn.init.normal_(w)   ", "parameters": ["tensor : an n-dimensional torch.Tensor", "mean : the mean of the normal distribution", "std : the standard deviation of the normal distribution"], "returns": null, "example": " w = torch.empty(3, 5)\n nn.init.normal_(w)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.init.constant_", "item_type": "function", "code": "torch.nn.init.constant_(tensor,val)", "description": "Fills the input Tensor with the value val\\text{val}val  .  Parameters  tensor \u2013 an n-dimensional torch.Tensor val \u2013 the value to fill the tensor with    Examples &gt;&gt;&gt; w = torch.empty(3, 5) &gt;&gt;&gt; nn.init.constant_(w, 0.3)   ", "parameters": ["tensor : an n-dimensional torch.Tensor", "val : the value to fill the tensor with"], "returns": null, "example": " w = torch.empty(3, 5)\n nn.init.constant_(w, 0.3)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.init.ones_", "item_type": "function", "code": "torch.nn.init.ones_(tensor)", "description": "Fills the input Tensor with the scalar value 1.  Parameters tensor \u2013 an n-dimensional torch.Tensor   Examples &gt;&gt;&gt; w = torch.empty(3, 5) &gt;&gt;&gt; nn.init.ones_(w)   ", "parameters": ["tensor : an n-dimensional torch.Tensor"], "returns": null, "example": " w = torch.empty(3, 5)\n nn.init.ones_(w)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.init.zeros_", "item_type": "function", "code": "torch.nn.init.zeros_(tensor)", "description": "Fills the input Tensor with the scalar value 0.  Parameters tensor \u2013 an n-dimensional torch.Tensor   Examples &gt;&gt;&gt; w = torch.empty(3, 5) &gt;&gt;&gt; nn.init.zeros_(w)   ", "parameters": ["tensor : an n-dimensional torch.Tensor"], "returns": null, "example": " w = torch.empty(3, 5)\n nn.init.zeros_(w)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.init.eye_", "item_type": "function", "code": "torch.nn.init.eye_(tensor)", "description": "Fills the 2-dimensional input Tensor with the identity matrix. Preserves the identity of the inputs in Linear layers, where as many inputs are preserved as possible.  Parameters tensor \u2013 a 2-dimensional torch.Tensor   Examples &gt;&gt;&gt; w = torch.empty(3, 5) &gt;&gt;&gt; nn.init.eye_(w)   ", "parameters": ["tensor : a 2-dimensional torch.Tensor"], "returns": null, "example": " w = torch.empty(3, 5)\n nn.init.eye_(w)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.init.dirac_", "item_type": "function", "code": "torch.nn.init.dirac_(tensor)", "description": "Fills the {3, 4, 5}-dimensional input Tensor with the Dirac delta function. Preserves the identity of the inputs in Convolutional layers, where as many input channels are preserved as possible.  Parameters tensor \u2013 a {3, 4, 5}-dimensional torch.Tensor   Examples &gt;&gt;&gt; w = torch.empty(3, 16, 5, 5) &gt;&gt;&gt; nn.init.dirac_(w)   ", "parameters": ["tensor : a {3, 4, 5}-dimensional torch.Tensor"], "returns": null, "example": " w = torch.empty(3, 16, 5, 5)\n nn.init.dirac_(w)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.init.xavier_uniform_", "item_type": "function", "code": "torch.nn.init.xavier_uniform_(tensor,gain=1.0)", "description": "Fills the input Tensor with values according to the method described in Understanding the difficulty of training deep feedforward neural networks - Glorot, X. &amp; Bengio, Y. (2010), using a uniform distribution. The resulting tensor will have values sampled from U(\u2212a,a)\\mathcal{U}(-a, a)U(\u2212a,a)   where  a=gain\u00d76fan_in+fan_outa = \\text{gain} \\times \\sqrt{\\frac{6}{\\text{fan\\_in} + \\text{fan\\_out}}}  a=gain\u00d7fan_in+fan_out6\u200b\u200b  Also known as Glorot initialization.  Parameters  tensor \u2013 an n-dimensional torch.Tensor gain \u2013 an optional scaling factor    Examples &gt;&gt;&gt; w = torch.empty(3, 5) &gt;&gt;&gt; nn.init.xavier_uniform_(w, gain=nn.init.calculate_gain('relu'))   ", "parameters": ["tensor : an n-dimensional torch.Tensor", "gain : an optional scaling factor"], "returns": null, "example": " w = torch.empty(3, 5)\n nn.init.xavier_uniform_(w, gain=nn.init.calculate_gain('relu'))\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.utils.tensorboard.writer.SummaryWriter.add_text", "item_type": "method", "code": "add_text(tag,text_string,global_step=None,walltime=None)", "description": "Add text data to summary.  Parameters  tag (string) \u2013 Data identifier text_string (string) \u2013 String to save global_step (python:int) \u2013 Global step value to record walltime (python:float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event    Examples: writer.add_text('lstm', 'This is an lstm', 0) writer.add_text('rnn', 'This is an rnn', 10)   ", "parameters": ["tag (string) : Data identifier", "text_string (string) : String to save", "global_step (python:int) : Global step value to record", "walltime (python:float) : Optional override default walltime (time.time())seconds after epoch of event"], "returns": null, "example": "writer.add_text('lstm', 'This is an lstm', 0)\nwriter.add_text('rnn', 'This is an rnn', 10)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.utils.tensorboard.writer.SummaryWriter.add_graph", "item_type": "method", "code": "add_graph(model,input_to_model=None,verbose=False)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.utils.tensorboard.writer.SummaryWriter.add_embedding", "item_type": "method", "code": "add_embedding(mat,metadata=None,label_img=None,global_step=None,tag='default',metadata_header=None)", "description": "Add embedding projector data to summary.  Parameters  mat (torch.Tensor or numpy.array) \u2013 A matrix which each row is the feature vector of the data point metadata (list) \u2013 A list of labels, each element will be convert to string label_img (torch.Tensor) \u2013 Images correspond to each data point global_step (python:int) \u2013 Global step value to record tag (string) \u2013 Name for the embedding     Shape:mat: (N,D)(N, D)(N,D)  , where N is number of data and D is feature dimension label_img: (N,C,H,W)(N, C, H, W)(N,C,H,W)     Examples: import keyword import torch meta = [] while len(meta)&lt;100:     meta = meta+keyword.kwlist # get some strings meta = meta[:100]  for i, v in enumerate(meta):     meta[i] = v+str(i)  label_img = torch.rand(100, 3, 10, 32) for i in range(100):     label_img[i]*=i/100.0  writer.add_embedding(torch.randn(100, 5), metadata=meta, label_img=label_img) writer.add_embedding(torch.randn(100, 5), label_img=label_img) writer.add_embedding(torch.randn(100, 5), metadata=meta)   ", "parameters": ["mat (torch.Tensor or numpy.array) : A matrix which each row is the feature vector of the data point", "metadata (list) : A list of labels, each element will be convert to string", "label_img (torch.Tensor) : Images correspond to each data point", "global_step (python:int) : Global step value to record", "tag (string) : Name for the embedding"], "returns": null, "example": "import keyword\nimport torch\nmeta = []\nwhile len(meta)&lt;100:\n    meta = meta+keyword.kwlist # get some strings\nmeta = meta[:100]\n\nfor i, v in enumerate(meta):\n    meta[i] = v+str(i)\n\nlabel_img = torch.rand(100, 3, 10, 32)\nfor i in range(100):\n    label_img[i]*=i/100.0\n\nwriter.add_embedding(torch.randn(100, 5), metadata=meta, label_img=label_img)\nwriter.add_embedding(torch.randn(100, 5), label_img=label_img)\nwriter.add_embedding(torch.randn(100, 5), metadata=meta)\n\n", "shape": "mat: (N,D)(N, D)(N,D)  , where N is number of data and D is feature dimension label_img: (N,C,H,W)(N, C, H, W)(N,C,H,W)   "},
{"library": "torch", "item_id": "torch.utils.tensorboard.writer.SummaryWriter.add_pr_curve", "item_type": "method", "code": "add_pr_curve(tag,labels,predictions,global_step=None,num_thresholds=127,weights=None,walltime=None)", "description": "Adds precision recall curve. Plotting a precision-recall curve lets you understand your model\u2019s performance under different threshold settings. With this function, you provide the ground truth labeling (T/F) and prediction confidence (usually the output of your model) for each target. The TensorBoard UI will let you choose the threshold interactively.  Parameters  tag (string) \u2013 Data identifier labels (torch.Tensor, numpy.array, or string/blobname) \u2013 Ground truth data. Binary label for each element. predictions (torch.Tensor, numpy.array, or string/blobname) \u2013 The probability that an element be classified as true. Value should in [0, 1] global_step (python:int) \u2013 Global step value to record num_thresholds (python:int) \u2013 Number of thresholds used to draw the curve. walltime (python:float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event    Examples: from torch.utils.tensorboard import SummaryWriter import numpy as np labels = np.random.randint(2, size=100)  # binary label predictions = np.random.rand(100) writer = SummaryWriter() writer.add_pr_curve('pr_curve', labels, predictions, 0) writer.close()   ", "parameters": ["tag (string) : Data identifier", "labels (torch.Tensor, numpy.array, or string/blobname) : Ground truth data. Binary label for each element.", "predictions (torch.Tensor, numpy.array, or string/blobname) : The probability that an element be classified as true.Value should in [0, 1]", "global_step (python:int) : Global step value to record", "num_thresholds (python:int) : Number of thresholds used to draw the curve.", "walltime (python:float) : Optional override default walltime (time.time())seconds after epoch of event"], "returns": null, "example": "from torch.utils.tensorboard import SummaryWriter\nimport numpy as np\nlabels = np.random.randint(2, size=100)  # binary label\npredictions = np.random.rand(100)\nwriter = SummaryWriter()\nwriter.add_pr_curve('pr_curve', labels, predictions, 0)\nwriter.close()\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.utils.tensorboard.writer.SummaryWriter.add_custom_scalars", "item_type": "method", "code": "add_custom_scalars(layout)", "description": "Create special chart by collecting charts tags in \u2018scalars\u2019. Note that this function can only be called once for each SummaryWriter() object. Because it only provides metadata to tensorboard, the function can be called before or after the training loop.  Parameters layout (dict) \u2013 {categoryName: charts}, where charts is also a dictionary {chartName: ListOfProperties}. The first element in ListOfProperties is the chart\u2019s type (one of Multiline or Margin) and the second element should be a list containing the tags you have used in add_scalar function, which will be collected into the new chart.   Examples: layout = {'Taiwan':{'twse':['Multiline',['twse/0050', 'twse/2330']]},              'USA':{ 'dow':['Margin',   ['dow/aaa', 'dow/bbb', 'dow/ccc']],                   'nasdaq':['Margin',   ['nasdaq/aaa', 'nasdaq/bbb', 'nasdaq/ccc']]}}  writer.add_custom_scalars(layout)   ", "parameters": ["layout (dict) : {categoryName: charts}, where charts is also a dictionary{chartName: ListOfProperties}. The first element in ListOfProperties is the chart\u2019s type(one of Multiline or Margin) and the second element should be a list containing the tagsyou have used in add_scalar function, which will be collected into the new chart."], "returns": null, "example": "layout = {'Taiwan':{'twse':['Multiline',['twse/0050', 'twse/2330']]},\n             'USA':{ 'dow':['Margin',   ['dow/aaa', 'dow/bbb', 'dow/ccc']],\n                  'nasdaq':['Margin',   ['nasdaq/aaa', 'nasdaq/bbb', 'nasdaq/ccc']]}}\n\nwriter.add_custom_scalars(layout)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.utils.tensorboard.writer.SummaryWriter.add_mesh", "item_type": "method", "code": "add_mesh(tag,vertices,colors=None,faces=None,config_dict=None,global_step=None,walltime=None)", "description": "Add meshes or 3D point clouds to TensorBoard. The visualization is based on Three.js, so it allows users to interact with the rendered object. Besides the basic definitions such as vertices, faces, users can further provide camera parameter, lighting condition, etc. Please check https://threejs.org/docs/index.html#manual/en/introduction/Creating-a-scene for advanced usage.  Parameters  tag (string) \u2013 Data identifier vertices (torch.Tensor) \u2013 List of the 3D coordinates of vertices. colors (torch.Tensor) \u2013 Colors for each vertex faces (torch.Tensor) \u2013 Indices of vertices within each triangle. (Optional) config_dict \u2013 Dictionary with ThreeJS classes names and configuration. global_step (python:int) \u2013 Global step value to record walltime (python:float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event     Shape:vertices: (B,N,3)(B, N, 3)(B,N,3)  . (batch, number_of_vertices, channels) colors: (B,N,3)(B, N, 3)(B,N,3)  . The values should lie in [0, 255] for type uint8 or [0, 1] for type float. faces: (B,N,3)(B, N, 3)(B,N,3)  . The values should lie in [0, number_of_vertices] for type uint8.   Examples: from torch.utils.tensorboard import SummaryWriter vertices_tensor = torch.as_tensor([     [1, 1, 1],     [-1, -1, 1],     [1, -1, -1],     [-1, 1, -1], ], dtype=torch.float).unsqueeze(0) colors_tensor = torch.as_tensor([     [255, 0, 0],     [0, 255, 0],     [0, 0, 255],     [255, 0, 255], ], dtype=torch.int).unsqueeze(0) faces_tensor = torch.as_tensor([     [0, 2, 3],     [0, 3, 1],     [0, 1, 2],     [1, 3, 2], ], dtype=torch.int).unsqueeze(0)  writer = SummaryWriter() writer.add_mesh('my_mesh', vertices=vertices_tensor, colors=colors_tensor, faces=faces_tensor)  writer.close()   ", "parameters": ["tag (string) : Data identifier", "vertices (torch.Tensor) : List of the 3D coordinates of vertices.", "colors (torch.Tensor) : Colors for each vertex", "faces (torch.Tensor) : Indices of vertices within each triangle. (Optional)", "config_dict : Dictionary with ThreeJS classes names and configuration.", "global_step (python:int) : Global step value to record", "walltime (python:float) : Optional override default walltime (time.time())seconds after epoch of event"], "returns": null, "example": "from torch.utils.tensorboard import SummaryWriter\nvertices_tensor = torch.as_tensor([\n    [1, 1, 1],\n    [-1, -1, 1],\n    [1, -1, -1],\n    [-1, 1, -1],\n], dtype=torch.float).unsqueeze(0)\ncolors_tensor = torch.as_tensor([\n    [255, 0, 0],\n    [0, 255, 0],\n    [0, 0, 255],\n    [255, 0, 255],\n], dtype=torch.int).unsqueeze(0)\nfaces_tensor = torch.as_tensor([\n    [0, 2, 3],\n    [0, 3, 1],\n    [0, 1, 2],\n    [1, 3, 2],\n], dtype=torch.int).unsqueeze(0)\n\nwriter = SummaryWriter()\nwriter.add_mesh('my_mesh', vertices=vertices_tensor, colors=colors_tensor, faces=faces_tensor)\n\nwriter.close()\n\n", "shape": "vertices: (B,N,3)(B, N, 3)(B,N,3)  . (batch, number_of_vertices, channels) colors: (B,N,3)(B, N, 3)(B,N,3)  . The values should lie in [0, 255] for type uint8 or [0, 1] for type float. faces: (B,N,3)(B, N, 3)(B,N,3)  . The values should lie in [0, number_of_vertices] for type uint8. "},
{"library": "torch", "item_id": "torch.distributed.init_process_group", "item_type": "function", "code": "torch.distributed.init_process_group(backend,init_method=None,timeout=datetime.timedelta(0,1800),world_size=-1,rank=-1,store=None,group_name='')", "description": "Initializes the default distributed process group, and this will also initialize the distributed package.  There are 2 main ways to initialize a process group: Specify store, rank, and world_size explicitly. Specify init_method (a URL string) which indicates where/how to discover peers. Optionally specify rank and world_size, or encode all required parameters in the URL and omit them.    If neither is specified, init_method is assumed to be \u201cenv://\u201d.  Parameters  backend (str or Backend) \u2013 The backend to use. Depending on build-time configurations, valid values include mpi, gloo, and nccl. This field should be given as a lowercase string (e.g., \"gloo\"), which can also be accessed via Backend attributes (e.g., Backend.GLOO). If using multiple processes per machine with nccl backend, each process must have exclusive access to every GPU it uses, as sharing GPUs between processes can result in deadlocks. init_method (str, optional) \u2013 URL specifying how to initialize the process group. Default is \u201cenv://\u201d if no init_method or store is specified. Mutually exclusive with store. world_size (python:int, optional) \u2013 Number of processes participating in the job. Required if store is specified. rank (python:int, optional) \u2013 Rank of the current process. Required if store is specified. store (Store, optional) \u2013 Key/value store accessible to all workers, used to exchange connection/address information. Mutually exclusive with init_method. timeout (timedelta, optional) \u2013 Timeout for operations executed against the process group. Default value equals 30 minutes. This is applicable for the gloo backend. For nccl, this is applicable only if the environment variable NCCL_BLOCKING_WAIT is set to 1. group_name (str, optional, deprecated) \u2013 Group name.    To enable backend == Backend.MPI, PyTorch needs to built from source on a system that supports MPI. The same applies to NCCL as well. ", "parameters": ["backend (str or Backend) : The backend to use. Depending onbuild-time configurations, valid values include mpi, gloo,and nccl. This field should be given as a lowercase string(e.g., \"gloo\"), which can also be accessed viaBackend attributes (e.g., Backend.GLOO). If usingmultiple processes per machine with nccl backend, each processmust have exclusive access to every GPU it uses, as sharing GPUsbetween processes can result in deadlocks.", "init_method (str, optional) : URL specifying how to initialize theprocess group. Default is \u201cenv://\u201d if noinit_method or store is specified.Mutually exclusive with store.", "world_size (python:int, optional) : Number of processes participating inthe job. Required if store is specified.", "rank (python:int, optional) : Rank of the current process.Required if store is specified.", "store (Store, optional) : Key/value store accessible to all workers, usedto exchange connection/address information.Mutually exclusive with init_method.", "timeout (timedelta, optional) : Timeout for operations executed againstthe process group. Default value equals 30 minutes.This is applicable for the gloo backend. For nccl, this isapplicable only if the environment variable NCCL_BLOCKING_WAITis set to 1.", "group_name (str, optional, deprecated) : Group name."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.current_blas_handle", "item_type": "function", "code": "torch.cuda.current_blas_handle()", "description": "Returns cublasHandle_t pointer to current cuBLAS handle ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.current_device", "item_type": "function", "code": "torch.cuda.current_device()", "description": "Returns the index of a currently selected device. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.current_stream", "item_type": "function", "code": "torch.cuda.current_stream(device=None)", "description": "Returns the currently selected Stream for a given device.  Parameters device (torch.device or python:int, optional) \u2013 selected device. Returns the currently selected Stream for the current device, given by current_device(), if device is None (default).   ", "parameters": ["device (torch.device or python:int, optional) : selected device. Returnsthe currently selected Stream for the current device, givenby current_device(), if device is None(default)."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.default_stream", "item_type": "function", "code": "torch.cuda.default_stream(device=None)", "description": "Returns the default Stream for a given device.  Parameters device (torch.device or python:int, optional) \u2013 selected device. Returns the default Stream for the current device, given by current_device(), if device is None (default).   ", "parameters": ["device (torch.device or python:int, optional) : selected device. Returnsthe default Stream for the current device, given bycurrent_device(), if device is None(default)."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.jit.trace", "item_type": "function", "code": "torch.jit.trace(func,example_inputs,optimize=None,check_trace=True,check_inputs=None,check_tolerance=1e-5)", "description": "Trace a function and return an executable  or ScriptFunction that will be optimized using just-in-time compilation. Tracing is ideal for code that operates only on Tensors and lists, dictionaries, and tuples of Tensors. Using torch.jit.trace and torch.jit.trace_module, you can turn an existing module or Python function into a TorchScript ScriptFunction or ScriptModule. You must provide example inputs, and we run the function, recording the operations performed on all the tensors.  The resulting recording of a standalone function produces ScriptFunction. The resulting recording of forward function of nn.Module or nn.Module produces ScriptModule.  This module also contains any parameters that the original module had as well.  Warning Tracing only correctly records functions and modules which are not data dependent (e.g., do not have conditionals on data in tensors) and do not have any untracked external dependencies (e.g., perform input/output or access global variables). Tracing only records operations done when the given function is run on the given tensors. Therefore, the returned ScriptModule will always run the same traced graph on any input. This has some important implications when your module is expected to run different sets of operations, depending on the input and/or the module state. For example,  Tracing will not record any control-flow like if-statements or loops. When this control-flow is constant across your module, this is fine and it often inlines the control-flow decisions. But sometimes the control-flow is actually part of the model itself. For instance, a recurrent network is a loop over the (possibly dynamic) length of an input sequence. In the returned ScriptModule, operations that have different behaviors in training and eval modes will always behave as if it is in the mode it was in during tracing, no matter which mode the ScriptModule is in.  In cases like these, tracing would not be appropriate and scripting is a better choice. If you trace such models, you may silently get incorrect results on subsequent invocations of the model. The tracer will try to emit warnings when doing something that may cause an incorrect trace to be produced.   Parameters  func (callable or torch.nn.Module) \u2013 A Python function or torch.nn.Module that will be run with example_inputs. arguments and returns to func must be tensors or (possibly nested) tuples that contain tensors. When a module is passed to torch.jit.trace, only the forward method is run and traced (see torch.jit.trace for details). example_inputs (tuple) \u2013 A tuple of example inputs that will be passed to the function while tracing. The resulting trace can be run with inputs of different types and shapes assuming the traced operations support those types and shapes. example_inputs may also be a single Tensor in which case it is automatically wrapped in a tuple.   Keyword Arguments  check_trace (bool, optional) \u2013 Check if the same inputs run through traced code produce the same outputs. Default: True. You might want to disable this if, for example, your network contains non- deterministic ops or if you are sure that the network is correct despite a checker failure. check_inputs (list of tuples, optional) \u2013 A list of tuples of input arguments that should be used to check the trace against what is expected. Each tuple is equivalent to a set of input arguments that would be specified in example_inputs. For best results, pass in a set of checking inputs representative of the space of shapes and types of inputs you expect the network to see. If not specified, the original example_inputs are used for checking check_tolerance (python:float, optional) \u2013 Floating-point comparison tolerance to use in the checker procedure. This can be used to relax the checker strictness in the event that results diverge numerically for a known reason, such as operator fusion.   Returns If callable is nn.Module or forward of nn.Module, trace returns a ScriptModule object with a single forward method containing the traced code. The returned ScriptModule will have the same set of sub-modules and parameters as the original nn.Module. If callable is a standalone function, trace returns ScriptFunction   Example (tracing a function): import torch  def foo(x, y):     return 2 * x + y  # Run `foo` with the provided inputs and record the tensor operations traced_foo = torch.jit.trace(foo, (torch.rand(3), torch.rand(3)))  # `traced_foo` can now be run with the TorchScript interpreter or saved # and loaded in a Python-free environment   Example (tracing an existing module): import torch import torch.nn as nn  class Net(nn.Module):     def __init__(self):         super(Net, self).__init__()         self.conv = nn.Conv2d(1, 1, 3)      def forward(self, x):         return self.conv(x)  n = Net() example_weight = torch.rand(1, 1, 3, 3) example_forward_input = torch.rand(1, 1, 3, 3)  # Trace a specific method and construct `ScriptModule` with # a single `forward` method module = torch.jit.trace(n.forward, example_forward_input)  # Trace a module (implicitly traces `forward`) and construct a # `ScriptModule` with a single `forward` method module = torch.jit.trace(n, example_forward_input)   ", "parameters": ["func (callable or torch.nn.Module) : A Python function or torch.nn.Modulethat will be run with example_inputs.arguments and returns to func must be tensorsor (possibly nested) tuples thatcontain tensors. When a module is passed totorch.jit.trace, only theforward method is run and traced(see torch.jit.trace for details).", "example_inputs (tuple) : A tuple of example inputs that will be passed to the functionwhile tracing. The resulting trace can be run withinputs of different types and shapes assuming the traced operationssupport those types and shapes. example_inputs may also be a singleTensor in which case it is automatically wrapped in a tuple.", "check_trace (bool, optional) : Check if the same inputs run throughtraced code produce the same outputs. Default: True. You might wantto disable this if, for example, your network contains non-deterministic ops or if you are sure that the network is correct despitea checker failure.", "check_inputs (list of tuples, optional) : A list of tuples of input arguments that should be usedto check the trace against what is expected. Each tupleis equivalent to a set of input arguments that wouldbe specified in example_inputs. For best results, pass in aset of checking inputs representative of the space ofshapes and types of inputs you expect the network to see.If not specified, the original example_inputs are used for checking", "check_tolerance (python:float, optional) : Floating-point comparison tolerance to use in the checker procedure.This can be used to relax the checker strictness in the event thatresults diverge numerically for a known reason, such as operator fusion."], "returns": "If callable is nn.Module or forward of nn.Module, trace returnsa ScriptModule object with a single forward method containing the traced code.The returned ScriptModule will have the same set of sub-modules and parameters as theoriginal nn.Module.If callable is a standalone function, trace returns ScriptFunction", "example": "import torch\n\ndef foo(x, y):\n    return 2 * x + y\n\n# Run `foo` with the provided inputs and record the tensor operations\ntraced_foo = torch.jit.trace(foo, (torch.rand(3), torch.rand(3)))\n\n# `traced_foo` can now be run with the TorchScript interpreter or saved\n# and loaded in a Python-free environment\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.jit.trace_module", "item_type": "function", "code": "torch.jit.trace_module(mod,inputs,optimize=None,check_trace=True,check_inputs=None,check_tolerance=1e-5)", "description": "Trace a module and return an executable ScriptModule that will be optimized using just-in-time compilation. When a module is passed to torch.jit.trace, only the forward method is run and traced. With trace_module, you can specify a dictionary of method names to example inputs to trace (see the example_inputs) argument below. See torch.jit.trace for more information on tracing.  Parameters  mod (torch.nn.Module) \u2013 A torch.nn.Module containing methods whose names are specified in example_inputs. The given methods will be compiled as a part of a single ScriptModule. example_inputs (dict) \u2013 A dict containing sample inputs indexed by method names in mod. The inputs will be passed to methods whose names correspond to inputs\u2019 keys while tracing. { 'forward' : example_forward_input, 'method2': example_method2_input}   Keyword Arguments  check_trace (bool, optional) \u2013 Check if the same inputs run through traced code produce the same outputs. Default: True. You might want to disable this if, for example, your network contains non- deterministic ops or if you are sure that the network is correct despite a checker failure. check_inputs (list of dicts, optional) \u2013 A list of dicts of input arguments that should be used to check the trace against what is expected. Each tuple is equivalent to a set of input arguments that would be specified in example_inputs. For best results, pass in a set of checking inputs representative of the space of shapes and types of inputs you expect the network to see. If not specified, the original example_inputs are used for checking check_tolerance (python:float, optional) \u2013 Floating-point comparison tolerance to use in the checker procedure. This can be used to relax the checker strictness in the event that results diverge numerically for a known reason, such as operator fusion.   Returns A ScriptModule object with a single forward method containing the traced code. When func is a torch.nn.Module, the returned ScriptModule will have the same set of sub-modules and parameters as func.   Example (tracing a module with multiple methods): import torch import torch.nn as nn  class Net(nn.Module):     def __init__(self):         super(Net, self).__init__()         self.conv = nn.Conv2d(1, 1, 3)      def forward(self, x):         return self.conv(x)      def weighted_kernel_sum(self, weight):         return weight * self.conv.weight   n = Net() example_weight = torch.rand(1, 1, 3, 3) example_forward_input = torch.rand(1, 1, 3, 3)  # Trace a specific method and construct `ScriptModule` with # a single `forward` method module = torch.jit.trace(n.forward, example_forward_input)  # Trace a module (implicitly traces `forward`) and construct a # `ScriptModule` with a single `forward` method module = torch.jit.trace(n, example_forward_input)  # Trace specific methods on a module (specified in `inputs`), constructs # a `ScriptModule` with `forward` and `weighted_kernel_sum` methods inputs = {'forward' : example_forward_input, 'weighted_kernel_sum' : example_weight} module = torch.jit.trace_module(n, inputs)   ", "parameters": ["mod (torch.nn.Module) : A torch.nn.Module containing methods whose names arespecified in example_inputs. The given methods will be compiledas a part of a single ScriptModule.", "example_inputs (dict) : A dict containing sample inputs indexed by method names in mod.The inputs will be passed to methods whose names correspond to inputs\u2019keys while tracing.{ 'forward' : example_forward_input, 'method2': example_method2_input}", "check_trace (bool, optional) : Check if the same inputs run throughtraced code produce the same outputs. Default: True. You might wantto disable this if, for example, your network contains non-deterministic ops or if you are sure that the network is correct despitea checker failure.", "check_inputs (list of dicts, optional) : A list of dicts of input arguments that should be usedto check the trace against what is expected. Each tupleis equivalent to a set of input arguments that wouldbe specified in example_inputs. For best results, pass in aset of checking inputs representative of the space ofshapes and types of inputs you expect the network to see.If not specified, the original example_inputs are used for checking", "check_tolerance (python:float, optional) : Floating-point comparison tolerance to use in the checker procedure.This can be used to relax the checker strictness in the event thatresults diverge numerically for a known reason, such as operator fusion."], "returns": "A ScriptModule object with a single forward method containing the traced code.When func is a torch.nn.Module, the returned ScriptModule will have the same set ofsub-modules and parameters as func.", "example": "import torch\nimport torch.nn as nn\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv = nn.Conv2d(1, 1, 3)\n\n    def forward(self, x):\n        return self.conv(x)\n\n    def weighted_kernel_sum(self, weight):\n        return weight * self.conv.weight\n\n\nn = Net()\nexample_weight = torch.rand(1, 1, 3, 3)\nexample_forward_input = torch.rand(1, 1, 3, 3)\n\n# Trace a specific method and construct `ScriptModule` with\n# a single `forward` method\nmodule = torch.jit.trace(n.forward, example_forward_input)\n\n# Trace a module (implicitly traces `forward`) and construct a\n# `ScriptModule` with a single `forward` method\nmodule = torch.jit.trace(n, example_forward_input)\n\n# Trace specific methods on a module (specified in `inputs`), constructs\n# a `ScriptModule` with `forward` and `weighted_kernel_sum` methods\ninputs = {'forward' : example_forward_input, 'weighted_kernel_sum' : example_weight}\nmodule = torch.jit.trace_module(n, inputs)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.autograd.backward", "item_type": "function", "code": "torch.autograd.backward(tensors,grad_tensors=None,retain_graph=None,create_graph=False,grad_variables=None)", "description": "Computes the sum of gradients of given tensors w.r.t. graph leaves. The graph is differentiated using the chain rule. If any of tensors are non-scalar (i.e. their data has more than one element) and require gradient, then the Jacobian-vector product would be computed, in this case the function additionally requires specifying grad_tensors. It should be a sequence of matching length, that contains the \u201cvector\u201d in the Jacobian-vector product, usually the gradient of the differentiated function w.r.t. corresponding tensors (None is an acceptable value for all tensors that don\u2019t need gradient tensors). This function accumulates gradients in the leaves - you might need to zero them before calling it.  Parameters  tensors (sequence of Tensor) \u2013 Tensors of which the derivative will be computed. grad_tensors (sequence of (Tensor or None)) \u2013 The \u201cvector\u201d in the Jacobian-vector product, usually gradients w.r.t. each element of corresponding tensors. None values can be specified for scalar Tensors or ones that don\u2019t require grad. If a None value would be acceptable for all grad_tensors, then this argument is optional. retain_graph (bool, optional) \u2013 If False, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Defaults to the value of create_graph. create_graph (bool, optional) \u2013 If True, graph of the derivative will be constructed, allowing to compute higher order derivative products. Defaults to False.    ", "parameters": ["tensors (sequence of Tensor) : Tensors of which the derivative will becomputed.", "grad_tensors (sequence of (Tensor or None)) : The \u201cvector\u201d in the Jacobian-vectorproduct, usually gradients w.r.t. each element of corresponding tensors.None values can be specified for scalar Tensors or ones that don\u2019t requiregrad. If a None value would be acceptable for all grad_tensors, then thisargument is optional.", "retain_graph (bool, optional) : If False, the graph used to compute the gradwill be freed. Note that in nearly all cases setting this option to Trueis not needed and often can be worked around in a much more efficientway. Defaults to the value of create_graph.", "create_graph (bool, optional) : If True, graph of the derivative willbe constructed, allowing to compute higher order derivative products.Defaults to False."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.autograd.grad", "item_type": "function", "code": "torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)", "description": "Computes and returns the sum of gradients of outputs w.r.t. the inputs. grad_outputs should be a sequence of length matching output containing the \u201cvector\u201d in Jacobian-vector product, usually the pre-computed gradients w.r.t. each of the outputs. If an output doesn\u2019t require_grad, then the gradient can be None). If only_inputs is True, the function will only return a list of gradients w.r.t the specified inputs. If it\u2019s False, then gradient w.r.t. all remaining leaves will still be computed, and will be accumulated into their .grad attribute.  Parameters  outputs (sequence of Tensor) \u2013 outputs of the differentiated function. inputs (sequence of Tensor) \u2013 Inputs w.r.t. which the gradient will be returned (and not accumulated into .grad). grad_outputs (sequence of Tensor) \u2013 The \u201cvector\u201d in the Jacobian-vector product. Usually gradients w.r.t. each output. None values can be specified for scalar Tensors or ones that don\u2019t require grad. If a None value would be acceptable for all grad_tensors, then this argument is optional. Default: None. retain_graph (bool, optional) \u2013 If False, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Defaults to the value of create_graph. create_graph (bool, optional) \u2013 If True, graph of the derivative will be constructed, allowing to compute higher order derivative products. Default: False. allow_unused (bool, optional) \u2013 If False, specifying inputs that were not used when computing outputs (and therefore their grad is always zero) is an error. Defaults to False.    ", "parameters": ["outputs (sequence of Tensor) : outputs of the differentiated function.", "inputs (sequence of Tensor) : Inputs w.r.t. which the gradient will bereturned (and not accumulated into .grad).", "grad_outputs (sequence of Tensor) : The \u201cvector\u201d in the Jacobian-vector product.Usually gradients w.r.t. each output. None values can be specified for scalarTensors or ones that don\u2019t require grad. If a None value would be acceptablefor all grad_tensors, then this argument is optional. Default: None.", "retain_graph (bool, optional) : If False, the graph used to compute the gradwill be freed. Note that in nearly all cases setting this option to Trueis not needed and often can be worked around in a much more efficientway. Defaults to the value of create_graph.", "create_graph (bool, optional) : If True, graph of the derivative willbe constructed, allowing to compute higher order derivative products.Default: False.", "allow_unused (bool, optional) : If False, specifying inputs that were notused when computing outputs (and therefore their grad is always zero)is an error. Defaults to False."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.utils.data.WeightedRandomSampler", "item_type": "class", "code": "classtorch.utils.data.WeightedRandomSampler(weights,num_samples,replacement=True)", "description": "Samples elements from [0,..,len(weights)-1] with given probabilities (weights).  Parameters  weights (sequence) \u2013 a sequence of weights, not necessary summing up to one num_samples (python:int) \u2013 number of samples to draw replacement (bool) \u2013 if True, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a sample index is drawn for a row, it cannot be drawn again for that row.    Example &gt;&gt;&gt; list(WeightedRandomSampler([0.1, 0.9, 0.4, 0.7, 3.0, 0.6], 5, replacement=True)) [0, 0, 0, 1, 0] &gt;&gt;&gt; list(WeightedRandomSampler([0.9, 0.4, 0.05, 0.2, 0.3, 0.1], 5, replacement=False)) [0, 1, 4, 3, 2]   ", "parameters": ["weights (sequence) : a sequence of weights, not necessary summing up to one", "num_samples (python:int) : number of samples to draw", "replacement (bool) : if True, samples are drawn with replacement.If not, they are drawn without replacement, which means that when asample index is drawn for a row, it cannot be drawn again for that row."], "returns": null, "example": " list(WeightedRandomSampler([0.1, 0.9, 0.4, 0.7, 3.0, 0.6], 5, replacement=True))\n[0, 0, 0, 1, 0]\n list(WeightedRandomSampler([0.9, 0.4, 0.05, 0.2, 0.3, 0.1], 5, replacement=False))\n[0, 1, 4, 3, 2]\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.utils.data.BatchSampler", "item_type": "class", "code": "classtorch.utils.data.BatchSampler(sampler,batch_size,drop_last)", "description": "Wraps another sampler to yield a mini-batch of indices.  Parameters  sampler (Sampler) \u2013 Base sampler. batch_size (python:int) \u2013 Size of mini-batch. drop_last (bool) \u2013 If True, the sampler will drop the last batch if its size would be less than batch_size    Example &gt;&gt;&gt; list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=False)) [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]] &gt;&gt;&gt; list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=True)) [[0, 1, 2], [3, 4, 5], [6, 7, 8]]   ", "parameters": ["sampler (Sampler) : Base sampler.", "batch_size (python:int) : Size of mini-batch.", "drop_last (bool) : If True, the sampler will drop the last batch ifits size would be less than batch_size"], "returns": null, "example": " list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=False))\n[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\n list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=True))\n[[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.utils.data.distributed.DistributedSampler", "item_type": "class", "code": "classtorch.utils.data.distributed.DistributedSampler(dataset,num_replicas=None,rank=None,shuffle=True)", "description": "Sampler that restricts data loading to a subset of the dataset. It is especially useful in conjunction with torch.nn.parallel.DistributedDataParallel. In such case, each process can pass a DistributedSampler instance as a DataLoader sampler, and load a subset of the original dataset that is exclusive to it.  Note Dataset is assumed to be of constant size.   Parameters  dataset \u2013 Dataset used for sampling. num_replicas (optional) \u2013 Number of processes participating in distributed training. rank (optional) \u2013 Rank of the current process within num_replicas. shuffle (optional) \u2013 If true (default), sampler will shuffle the indices    ", "parameters": ["dataset : Dataset used for sampling.", "num_replicas (optional) : Number of processes participating indistributed training.", "rank (optional) : Rank of the current process within num_replicas.", "shuffle (optional) : If true (default), sampler will shuffle the indices"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.torch.dtype", "item_type": "class", "code": "classtorch.dtype", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.torch.device", "item_type": "class", "code": "classtorch.device", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.torch.layout", "item_type": "class", "code": "classtorch.layout", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributed.autograd.backward", "item_type": "function", "code": "torch.distributed.autograd.backward(roots:List[Tensor])\u2192None", "description": "Kicks off the distributed backward pass using the provided roots. This currently implements the FAST mode algorithm which assumes all RPC messages sent in the same distributed autograd context across workers would be part of the autograd graph during the backward pass. We use the provided roots to discover the autograd graph and compute appropriate dependencies. This method blocks until the entire autograd computation is done. We accumulate the gradients in the appropriate torch.distributed.autograd.context on each of the nodes. The autograd context used is the current autograd context of this node when torch.distributed.autograd.backward() is called. If there is no valid autograd context, we throw an error. You can retrieve the accumulated gradients using the get_gradients() API.  Parameters roots (list) \u2013 Tensors which represent the roots of the autograd computation. All the tensors should be scalars.   Example: &gt;&gt; import torch.distributed.autograd as dist_autograd &gt;&gt; with dist_autograd.context() as context_id: &gt;&gt;      pred = model.forward() &gt;&gt;      loss = loss_func(pred, loss) &gt;&gt;      dist_autograd.backward(loss)   ", "parameters": ["roots (list) : Tensors which represent the roots of the autogradcomputation. All the tensors should be scalars."], "returns": null, "example": " import torch.distributed.autograd as dist_autograd\n with dist_autograd.context() as context_id:\n      pred = model.forward()\n      loss = loss_func(pred, loss)\n      dist_autograd.backward(loss)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributed.autograd.get_gradients", "item_type": "function", "code": "torch.distributed.autograd.get_gradients(context_id:int)\u2192Dict[Tensor,Tensor]", "description": "Retrieves a map from Tensor to the appropriate gradient for that Tensor accumulated in the provided context_id as part of the distributed autograd backward pass.  Parameters context_id (python:int) \u2013 The autograd context id for which we should retrieve the gradients.  Returns A map where the key is the Tensor and the value is the associated gradient for that Tensor.   Example: &gt;&gt; import torch.distributed.autograd as dist_autograd &gt;&gt; with dist_autograd.context() as context_id: &gt;&gt;      t1 = torch.rand((3, 3), requires_grad=True) &gt;&gt;      t2 = torch.rand((3, 3), requires_grad=True) &gt;&gt;      loss = t1 + t2 &gt;&gt;      dist_autograd.backward([loss.sum()]) &gt;&gt;      grads = dist_autograd.get_gradients(context_id) &gt;&gt;      print (grads[t1]) &gt;&gt;      print (grads[t2])   ", "parameters": ["context_id (python:int) : The autograd context id for which we should retrieve thegradients.", "A map where the key is the Tensor and the value is the associated gradient for that Tensor."], "returns": "A map where the key is the Tensor and the value is the associated gradient for that Tensor.", "example": " import torch.distributed.autograd as dist_autograd\n with dist_autograd.context() as context_id:\n      t1 = torch.rand((3, 3), requires_grad=True)\n      t2 = torch.rand((3, 3), requires_grad=True)\n      loss = t1 + t2\n      dist_autograd.backward([loss.sum()])\n      grads = dist_autograd.get_gradients(context_id)\n      print (grads[t1])\n      print (grads[t2])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributed.rpc.RRef.is_owner", "item_type": "method", "code": "is_owner(self:torch.distributed.rpc.RRef)\u2192bool", "description": "Returns whether or not the current node is the owner of this RRef. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributed.rpc.RRef.local_value", "item_type": "method", "code": "local_value(self:torch.distributed.rpc.RRef)\u2192object", "description": "If the current node is the owner, returns a reference to the local value. Otherwise, throws an exception. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributed.rpc.RRef.owner", "item_type": "method", "code": "owner(self:torch.distributed.rpc.RRef)\u2192torch.distributed.rpc.WorkerInfo", "description": "Returns worker information of the node that owns this RRef. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributed.rpc.RRef.to_here", "item_type": "method", "code": "to_here(self:torch.distributed.rpc.RRef)\u2192object", "description": "Blocking call that copies the value of the RRef from the owner to the local node and returns it. If the current node is the owner, returns a reference to the local value. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributed.optim.DistributedOptimizer.step", "item_type": "method", "code": "step()", "description": "Performs a single optimization step. This will call torch.optim.Optimizer.step() on each worker containing parameters to be optimized, and will block until all workers return. The current distributed autograd context will be used globally. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributed.rpc.RRef", "item_type": "class", "code": "classtorch.distributed.rpc.RRef", "description": "A class encapsulating a reference to a value of some type on a remote worker. This handle will keep the referenced remote value alive on the worker.   is_owner(self: torch.distributed.rpc.RRef) \u2192 bool Returns whether or not the current node is the owner of this RRef.     local_value(self: torch.distributed.rpc.RRef) \u2192 object If the current node is the owner, returns a reference to the local value. Otherwise, throws an exception.     owner(self: torch.distributed.rpc.RRef) \u2192 torch.distributed.rpc.WorkerInfo Returns worker information of the node that owns this RRef.     to_here(self: torch.distributed.rpc.RRef) \u2192 object Blocking call that copies the value of the RRef from the owner to the local node and returns it. If the current node is the owner, returns a reference to the local value.   ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.FloatStorage", "item_type": "class", "code": "classtorch.FloatStorage", "description": "  bfloat16() Casts this storage to bfloat16 type     bool() Casts this storage to bool type     byte() Casts this storage to byte type     char() Casts this storage to char type     clone() Returns a copy of this storage     copy_()     cpu() Returns a CPU copy of this storage if it\u2019s not already on the CPU     cuda(device=None, non_blocking=False, **kwargs) Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then no copy is performed and the original object is returned.  Parameters  device (python:int) \u2013 The destination GPU id. Defaults to the current device. non_blocking (bool) \u2013 If True and the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, the argument has no effect. **kwargs \u2013 For compatibility, may contain the key async in place of the non_blocking argument.        data_ptr()     device     double() Casts this storage to double type     dtype     element_size()     fill_()     float() Casts this storage to float type     static from_buffer()     static from_file(filename, shared=False, size=0) \u2192 Storage If shared is True, then memory is shared between all processes. All changes are written to the file. If shared is False, then the changes on the storage do not affect the file. size is the number of elements in the storage. If shared is False, then the file must contain at least size * sizeof(Type) bytes (Type is the type of storage). If shared is True the file will be created if needed.  Parameters  filename (str) \u2013 file name to map shared (bool) \u2013 whether to share memory size (python:int) \u2013 number of elements in the storage        half() Casts this storage to half type     int() Casts this storage to int type     is_cuda = False     is_pinned()     is_shared()     is_sparse = False     long() Casts this storage to long type     new()     pin_memory() Copies the storage to pinned memory, if it\u2019s not already pinned.     resize_()     share_memory_() Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA storages, which do not need to be moved for sharing across processes. Storages in shared memory cannot be resized. Returns: self     short() Casts this storage to short type     size()     tolist() Returns a list containing the elements of this storage     type(dtype=None, non_blocking=False, **kwargs) Returns the type if dtype is not provided, else casts this object to the specified type. If this is already of the correct type, no copy is performed and the original object is returned.  Parameters  dtype (python:type or string) \u2013 The desired type non_blocking (bool) \u2013 If True, and the source is in pinned memory and destination is on the GPU or vice versa, the copy is performed asynchronously with respect to the host. Otherwise, the argument has no effect. **kwargs \u2013 For compatibility, may contain the key async in place of the non_blocking argument. The async arg is deprecated.      ", "parameters": ["device (python:int) : The destination GPU id. Defaults to the current device.", "non_blocking (bool) : If True and the source is in pinned memory,the copy will be asynchronous with respect to the host. Otherwise,the argument has no effect.", "**kwargs : For compatibility, may contain the key async in place ofthe non_blocking argument.", "filename (str) : file name to map", "shared (bool) : whether to share memory", "size (python:int) : number of elements in the storage", "dtype (python:type or string) : The desired type", "non_blocking (bool) : If True, and the source is in pinned memoryand destination is on the GPU or vice versa, the copy is performedasynchronously with respect to the host. Otherwise, the argumenthas no effect.", "**kwargs : For compatibility, may contain the key async in place ofthe non_blocking argument. The async arg is deprecated."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.FloatStorage.device", "item_type": "attribute", "code": "device", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.FloatStorage.dtype", "item_type": "attribute", "code": "dtype", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.FloatStorage.is_cuda", "item_type": "attribute", "code": "is_cuda=False", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.FloatStorage.is_sparse", "item_type": "attribute", "code": "is_sparse=False", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.quantization.add_observer_", "item_type": "function", "code": "torch.quantization.add_observer_(module)", "description": "Add observer for the leaf child of the module. This function insert observer module to all leaf child module that has a valid qconfig attribute.  Parameters module \u2013 input module with qconfig attributes for all the leaf modules that we want to quantize  Returns None, module is modified inplace with added observer modules and forward_hooks   ", "parameters": ["module : input module with qconfig attributes for all the leaf modules that we want to quantize", "None, module is modified inplace with added observer modules and forward_hooks"], "returns": "None, module is modified inplace with added observer modules and forward_hooks", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.quantization.swap_module", "item_type": "function", "code": "torch.quantization.swap_module(mod,mapping)", "description": "Swaps the module if it has a quantized counterpart and it has an observer attached.  Parameters  mod \u2013 input module mapping \u2013 a dictionary that maps from nn module to nnq module   Returns The corresponding quantized module of mod   ", "parameters": ["mod : input module", "mapping : a dictionary that maps from nn module to nnq module"], "returns": "The corresponding quantized module of mod", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.quantization.propagate_qconfig_", "item_type": "function", "code": "torch.quantization.propagate_qconfig_(module,qconfig_dict=None)", "description": "Propagate qconfig through the module hierarchy and assign qconfig attribute on each leaf module  Parameters  module \u2013 input module qconfig_dict \u2013 dictionary that maps from name or type of submodule to quantization configuration, qconfig applies to all submodules of a given module unless qconfig for the submodules are specified (when the submodule already has qconfig attribute)   Returns None, module is modified inplace with qconfig attached   ", "parameters": ["module : input module", "qconfig_dict : dictionary that maps from name or type of submodule toquantization configuration, qconfig applies to all submodules of agiven module unless qconfig for the submodules are specified (whenthe submodule already has qconfig attribute)"], "returns": "None, module is modified inplace with qconfig attached", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.quantization.default_eval_fn", "item_type": "function", "code": "torch.quantization.default_eval_fn(model,calib_data)", "description": "Default evaluation function takes a torch.utils.data.Dataset or a list of input Tensors and run the model on the dataset ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.quantization.get_observer_dict", "item_type": "function", "code": "torch.quantization.get_observer_dict(mod,target_dict,prefix='')", "description": "Traverse the modules and save all observers into dict. This is mainly used for quantization accuracy debug :param mod: the top module we want to save all observers :param prefix: the prefix for the current module :param target_dict: the dictionary used to save all the observers ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.quantized.functional.relu", "item_type": "function", "code": "torch.nn.quantized.functional.relu(input,inplace=False)\u2192Tensor", "description": "Applies the rectified linear unit function element-wise. See ReLU for more details.  Parameters  input \u2013 quantized input inplace \u2013 perform the computation inplace    ", "parameters": ["input : quantized input", "inplace : perform the computation inplace"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.quantized.functional.linear", "item_type": "function", "code": "torch.nn.quantized.functional.linear(input,weight,bias=None,scale=None,zero_point=None)", "description": "Applies a linear transformation to the incoming quantized data: y=xAT+by = xA^T + by=xAT+b  . See Linear  Note Current implementation packs weights on every call, which has penalty on performance. If you want to avoid the overhead, use Linear.   Parameters  input (Tensor) \u2013 Quantized input of type torch.quint8 weight (Tensor) \u2013 Quantized weight of type torch.qint8 bias (Tensor) \u2013 None or fp32 bias of type torch.float scale (double) \u2013 output scale. If None, derived from the input scale zero_point (python:long) \u2013 output zero point. If None, derived from the input zero_point     Shape: Input: (N,\u2217,in_features)(N, *, in\\_features)(N,\u2217,in_features)   where * means any number of additional dimensions Weight: (out_features,in_features)(out\\_features, in\\_features)(out_features,in_features)   Bias: (out_features)(out\\_features)(out_features)   Output: (N,\u2217,out_features)(N, *, out\\_features)(N,\u2217,out_features)      ", "parameters": ["input (Tensor) : Quantized input of type torch.quint8", "weight (Tensor) : Quantized weight of type torch.qint8", "bias (Tensor) : None or fp32 bias of type torch.float", "scale (double) : output scale. If None, derived from the input scale", "zero_point (python:long) : output zero point. If None, derived from the input zero_point"], "returns": null, "example": "NA", "shape": " Input: (N,\u2217,in_features)(N, *, in\\_features)(N,\u2217,in_features)   where * means any number of additional dimensions Weight: (out_features,in_features)(out\\_features, in\\_features)(out_features,in_features)   Bias: (out_features)(out\\_features)(out_features)   Output: (N,\u2217,out_features)(N, *, out\\_features)(N,\u2217,out_features)    "},
{"library": "torch", "item_id": "torch.optim.SparseAdam.step", "item_type": "method", "code": "step(closure=None)", "description": "Performs a single optimization step.  Parameters closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.   ", "parameters": ["closure (callable, optional) : A closure that reevaluates the modeland returns the loss."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.optim.Adamax.step", "item_type": "method", "code": "step(closure=None)", "description": "Performs a single optimization step.  Parameters closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.   ", "parameters": ["closure (callable, optional) : A closure that reevaluates the modeland returns the loss."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.optim.ASGD.step", "item_type": "method", "code": "step(closure=None)", "description": "Performs a single optimization step.  Parameters closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.   ", "parameters": ["closure (callable, optional) : A closure that reevaluates the modeland returns the loss."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.optim.LBFGS.step", "item_type": "method", "code": "step(closure)", "description": "Performs a single optimization step.  Parameters closure (callable) \u2013 A closure that reevaluates the model and returns the loss.   ", "parameters": ["closure (callable) : A closure that reevaluates the modeland returns the loss."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.optim.RMSprop.step", "item_type": "method", "code": "step(closure=None)", "description": "Performs a single optimization step.  Parameters closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.   ", "parameters": ["closure (callable, optional) : A closure that reevaluates the modeland returns the loss."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.optim.Rprop.step", "item_type": "method", "code": "step(closure=None)", "description": "Performs a single optimization step.  Parameters closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.   ", "parameters": ["closure (callable, optional) : A closure that reevaluates the modeland returns the loss."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.optim.SGD.step", "item_type": "method", "code": "step(closure=None)", "description": "Performs a single optimization step.  Parameters closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.   ", "parameters": ["closure (callable, optional) : A closure that reevaluates the modeland returns the loss."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.optim.lr_scheduler.LambdaLR.load_state_dict", "item_type": "method", "code": "load_state_dict(state_dict)", "description": "Loads the schedulers state.  Parameters state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict().   ", "parameters": ["state_dict (dict) : scheduler state. Should be an object returnedfrom a call to state_dict()."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.datasets.CIFAR10", "item_type": "class", "code": "classtorchvision.datasets.CIFAR10(root,train=True,transform=None,target_transform=None,download=False)", "description": "CIFAR10 Dataset.  Parameters  root (string) \u2013 Root directory of dataset where directory cifar-10-batches-py exists or will be saved to if download is set to True. train (bool, optional) \u2013 If True, creates dataset from training set, otherwise creates from test set. transform (callable, optional) \u2013 A function/transform that takes in an PIL image and returns a transformed version. E.g, transforms.RandomCrop target_transform (callable, optional) \u2013 A function/transform that takes in the target and transforms it. download (bool, optional) \u2013 If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.      __getitem__(index)   Parameters index (python:int) \u2013 Index  Returns (image, target) where target is index of the target class.  Return type tuple     ", "parameters": ["root (string) : Root directory of dataset where directorycifar-10-batches-py exists or will be saved to if download is set to True.", "train (bool, optional) : If True, creates dataset from training set, otherwisecreates from test set.", "transform (callable, optional) : A function/transform that takes in an PIL imageand returns a transformed version. E.g, transforms.RandomCrop", "target_transform (callable, optional) : A function/transform that takes in thetarget and transforms it.", "download (bool, optional) : If true, downloads the dataset from the internet andputs it in root directory. If dataset is already downloaded, it is notdownloaded again."], "returns": "(image, target) where target is index of the target class.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.datasets.CIFAR100", "item_type": "class", "code": "classtorchvision.datasets.CIFAR100(root,train=True,transform=None,target_transform=None,download=False)", "description": "CIFAR100 Dataset. This is a subclass of the CIFAR10 Dataset. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.datasets.STL10", "item_type": "class", "code": "classtorchvision.datasets.STL10(root,split='train',folds=None,transform=None,target_transform=None,download=False)", "description": "STL10 Dataset.  Parameters  root (string) \u2013 Root directory of dataset where directory stl10_binary exists. split (string) \u2013 One of {\u2018train\u2019, \u2018test\u2019, \u2018unlabeled\u2019, \u2018train+unlabeled\u2019}. Accordingly dataset is selected. folds (python:int, optional) \u2013 One of {0-9} or None. For training, loads one of the 10 pre-defined folds of 1k samples for the  standard evaluation procedure. If no value is passed, loads the 5k samples.   transform (callable, optional) \u2013 A function/transform that  takes in an PIL image and returns a transformed version. E.g, transforms.RandomCrop target_transform (callable, optional) \u2013 A function/transform that takes in the target and transforms it. download (bool, optional) \u2013 If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.      __getitem__(index)   Parameters index (python:int) \u2013 Index  Returns (image, target) where target is index of the target class.  Return type tuple     ", "parameters": ["root (string) : Root directory of dataset where directorystl10_binary exists.", "split (string) : One of {\u2018train\u2019, \u2018test\u2019, \u2018unlabeled\u2019, \u2018train+unlabeled\u2019}.Accordingly dataset is selected.", "folds (python:int, optional) : One of {0-9} or None.For training, loads one of the 10 pre-defined folds of 1k samples for thestandard evaluation procedure. If no value is passed, loads the 5k samples.", "transform (callable, optional) : A function/transform that  takes in an PIL imageand returns a transformed version. E.g, transforms.RandomCrop", "target_transform (callable, optional) : A function/transform that takes in thetarget and transforms it.", "download (bool, optional) : If true, downloads the dataset from the internet andputs it in root directory. If dataset is already downloaded, it is notdownloaded again."], "returns": "(image, target) where target is index of the target class.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.datasets.SVHN", "item_type": "class", "code": "classtorchvision.datasets.SVHN(root,split='train',transform=None,target_transform=None,download=False)", "description": "SVHN Dataset. Note: The SVHN dataset assigns the label 10 to the digit 0. However, in this Dataset, we assign the label 0 to the digit 0 to be compatible with PyTorch loss functions which expect the class labels to be in the range [0, C-1]  Parameters  root (string) \u2013 Root directory of dataset where directory SVHN exists. split (string) \u2013 One of {\u2018train\u2019, \u2018test\u2019, \u2018extra\u2019}. Accordingly dataset is selected. \u2018extra\u2019 is Extra training set. transform (callable, optional) \u2013 A function/transform that  takes in an PIL image and returns a transformed version. E.g, transforms.RandomCrop target_transform (callable, optional) \u2013 A function/transform that takes in the target and transforms it. download (bool, optional) \u2013 If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.      __getitem__(index)   Parameters index (python:int) \u2013 Index  Returns (image, target) where target is index of the target class.  Return type tuple     ", "parameters": ["root (string) : Root directory of dataset where directorySVHN exists.", "split (string) : One of {\u2018train\u2019, \u2018test\u2019, \u2018extra\u2019}.Accordingly dataset is selected. \u2018extra\u2019 is Extra training set.", "transform (callable, optional) : A function/transform that  takes in an PIL imageand returns a transformed version. E.g, transforms.RandomCrop", "target_transform (callable, optional) : A function/transform that takes in thetarget and transforms it.", "download (bool, optional) : If true, downloads the dataset from the internet andputs it in root directory. If dataset is already downloaded, it is notdownloaded again."], "returns": "(image, target) where target is index of the target class.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.datasets.PhotoTour", "item_type": "class", "code": "classtorchvision.datasets.PhotoTour(root,name,train=True,transform=None,download=False)", "description": "Learning Local Image Descriptors Data Dataset.  Parameters  root (string) \u2013 Root directory where images are. name (string) \u2013 Name of the dataset to load. transform (callable, optional) \u2013 A function/transform that  takes in an PIL image and returns a transformed version. download (bool, optional) \u2013 If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.      __getitem__(index)   Parameters index (python:int) \u2013 Index  Returns (data1, data2, matches)  Return type tuple     ", "parameters": ["root (string) : Root directory where images are.", "name (string) : Name of the dataset to load.", "transform (callable, optional) : A function/transform that  takes in an PIL imageand returns a transformed version.", "download (bool, optional) : If true, downloads the dataset from the internet andputs it in root directory. If dataset is already downloaded, it is notdownloaded again."], "returns": "(data1, data2, matches)", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "bernoulli_(p=0.5,*,generator=None)\u2192Tensor", "description": "Fills each location of self with an independent sample from Bernoulli(p)\\text{Bernoulli}(\\texttt{p})Bernoulli(p)  . self can have integral dtype. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.sparse.FloatTensor.narrow_copy", "item_type": "method", "code": "narrow_copy()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.sparse.FloatTensor.resizeAs_", "item_type": "method", "code": "resizeAs_()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.sparse.FloatTensor.size", "item_type": "method", "code": "size()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.sparse.FloatTensor.spadd", "item_type": "method", "code": "spadd()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.sparse.FloatTensor.spmm", "item_type": "method", "code": "spmm()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.sparse.FloatTensor.sspaddmm", "item_type": "method", "code": "sspaddmm()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.sparse.FloatTensor.sspmm", "item_type": "method", "code": "sspmm()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.sparse.FloatTensor.sub", "item_type": "method", "code": "sub()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.sparse.FloatTensor.sub_", "item_type": "method", "code": "sub_()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.sparse.FloatTensor.t_", "item_type": "method", "code": "t_()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.sparse.FloatTensor.to_dense", "item_type": "method", "code": "to_dense()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.sparse.FloatTensor.transpose", "item_type": "method", "code": "transpose()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.conv1d", "item_type": "function", "code": "torch.nn.functional.conv1d(input,weight,bias=None,stride=1,padding=0,dilation=1,groups=1)\u2192Tensor", "description": "Applies a 1D convolution over an input signal composed of several input planes. See Conv1d for details and output shape.  Note In some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. Please see the notes on Reproducibility for background.   Parameters  input \u2013 input tensor of shape (minibatch,in_channels,iW)(\\text{minibatch} , \\text{in\\_channels} , iW)(minibatch,in_channels,iW)   weight \u2013 filters of shape (out_channels,in_channelsgroups,kW)(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kW)(out_channels,groupsin_channels\u200b,kW)   bias \u2013 optional bias of shape (out_channels)(\\text{out\\_channels})(out_channels)  . Default: None stride \u2013 the stride of the convolving kernel. Can be a single number or a one-element tuple (sW,). Default: 1 padding \u2013 implicit paddings on both sides of the input. Can be a single number or a one-element tuple (padW,). Default: 0 dilation \u2013 the spacing between kernel elements. Can be a single number or a one-element tuple (dW,). Default: 1 groups \u2013 split input into groups, in_channels\\text{in\\_channels}in_channels   should be divisible by the number of groups. Default: 1    Examples: &gt;&gt;&gt; filters = torch.randn(33, 16, 3) &gt;&gt;&gt; inputs = torch.randn(20, 16, 50) &gt;&gt;&gt; F.conv1d(inputs, filters)   ", "parameters": ["input : input tensor of shape (minibatch,in_channels,iW)(\\text{minibatch} , \\text{in\\_channels} , iW)(minibatch,in_channels,iW)", "weight : filters of shape (out_channels,in_channelsgroups,kW)(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kW)(out_channels,groupsin_channels\u200b,kW)", "bias : optional bias of shape (out_channels)(\\text{out\\_channels})(out_channels). Default: None", "stride : the stride of the convolving kernel. Can be a single number ora one-element tuple (sW,). Default: 1", "padding : implicit paddings on both sides of the input. Can be asingle number or a one-element tuple (padW,). Default: 0", "dilation : the spacing between kernel elements. Can be a single number ora one-element tuple (dW,). Default: 1", "groups : split input into groups, in_channels\\text{in\\_channels}in_channels should be divisible bythe number of groups. Default: 1"], "returns": null, "example": " filters = torch.randn(33, 16, 3)\n inputs = torch.randn(20, 16, 50)\n F.conv1d(inputs, filters)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.models.segmentation.deeplabv3_resnet101", "item_type": "function", "code": "torchvision.models.segmentation.deeplabv3_resnet101(pretrained=False,progress=True,num_classes=21,aux_loss=None,**kwargs)", "description": "Constructs a DeepLabV3 model with a ResNet-101 backbone.  Parameters  pretrained (bool) \u2013 If True, returns a model pre-trained on COCO train2017 which contains the same classes as Pascal VOC progress (bool) \u2013 If True, displays a progress bar of the download to stderr    ", "parameters": ["pretrained (bool) : If True, returns a model pre-trained on COCO train2017 whichcontains the same classes as Pascal VOC", "progress (bool) : If True, displays a progress bar of the download to stderr"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.models.detection.fasterrcnn_resnet50_fpn", "item_type": "function", "code": "torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False,progress=True,num_classes=91,pretrained_backbone=True,**kwargs)", "description": "Constructs a Faster R-CNN model with a ResNet-50-FPN backbone. The input to the model is expected to be a list of tensors, each of shape [C, H, W], one for each image, and should be in 0-1 range. Different images can have different sizes. The behavior of the model changes depending if it is in training or evaluation mode. During training, the model expects both the input tensors, as well as a targets (list of dictionary), containing:   boxes (FloatTensor[N, 4]): the ground-truth boxes in [x1, y1, x2, y2] format, with values between 0 and H and 0 and W labels (Int64Tensor[N]): the class label for each ground-truth box   The model returns a Dict[Tensor] during training, containing the classification and regression losses for both the RPN and the R-CNN. During inference, the model requires only the input tensors, and returns the post-processed predictions as a List[Dict[Tensor]], one for each input image. The fields of the Dict are as follows:   boxes (FloatTensor[N, 4]): the predicted boxes in [x1, y1, x2, y2] format, with values between 0 and H and 0 and W labels (Int64Tensor[N]): the predicted labels for each image scores (Tensor[N]): the scores or each prediction   Example: &gt;&gt;&gt; model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True) &gt;&gt;&gt; # For training &gt;&gt;&gt; images, boxes = torch.rand(4, 3, 600, 1200), torch.rand(4, 11, 4) &gt;&gt;&gt; labels = torch.randint(1, 91, (4, 11)) &gt;&gt;&gt; images = list(image for image in images) &gt;&gt;&gt; targets = [] &gt;&gt;&gt; for i in range(len(images)): &gt;&gt;&gt;     d = {} &gt;&gt;&gt;     d['boxes'] = boxes[i] &gt;&gt;&gt;     d['labels'] = labels[i] &gt;&gt;&gt;     targets.append(d) &gt;&gt;&gt; output = model(images, targets) &gt;&gt;&gt; # For inference &gt;&gt;&gt; model.eval() &gt;&gt;&gt; x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)] &gt;&gt;&gt; predictions = model(x)    Parameters  pretrained (bool) \u2013 If True, returns a model pre-trained on COCO train2017 progress (bool) \u2013 If True, displays a progress bar of the download to stderr    ", "parameters": ["pretrained (bool) : If True, returns a model pre-trained on COCO train2017", "progress (bool) : If True, displays a progress bar of the download to stderr"], "returns": null, "example": " model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n # For training\n images, boxes = torch.rand(4, 3, 600, 1200), torch.rand(4, 11, 4)\n labels = torch.randint(1, 91, (4, 11))\n images = list(image for image in images)\n targets = []\n for i in range(len(images)):\n     d = {}\n     d['boxes'] = boxes[i]\n     d['labels'] = labels[i]\n     targets.append(d)\n output = model(images, targets)\n # For inference\n model.eval()\n x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n predictions = model(x)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.models.detection.maskrcnn_resnet50_fpn", "item_type": "function", "code": "torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=False,progress=True,num_classes=91,pretrained_backbone=True,**kwargs)", "description": "Constructs a Mask R-CNN model with a ResNet-50-FPN backbone. The input to the model is expected to be a list of tensors, each of shape [C, H, W], one for each image, and should be in 0-1 range. Different images can have different sizes. The behavior of the model changes depending if it is in training or evaluation mode. During training, the model expects both the input tensors, as well as a targets (list of dictionary), containing:   boxes (FloatTensor[N, 4]): the ground-truth boxes in [x1, y1, x2, y2] format, with values between 0 and H and 0 and W labels (Int64Tensor[N]): the class label for each ground-truth box masks (UInt8Tensor[N, H, W]): the segmentation binary masks for each instance   The model returns a Dict[Tensor] during training, containing the classification and regression losses for both the RPN and the R-CNN, and the mask loss. During inference, the model requires only the input tensors, and returns the post-processed predictions as a List[Dict[Tensor]], one for each input image. The fields of the Dict are as follows:   boxes (FloatTensor[N, 4]): the predicted boxes in [x1, y1, x2, y2] format, with values between 0 and H and 0 and W labels (Int64Tensor[N]): the predicted labels for each image scores (Tensor[N]): the scores or each prediction masks (UInt8Tensor[N, 1, H, W]): the predicted masks for each instance, in 0-1 range. In order to obtain the final segmentation masks, the soft masks can be thresholded, generally with a value of 0.5 (mask &gt;= 0.5)   Example: &gt;&gt;&gt; model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True) &gt;&gt;&gt; model.eval() &gt;&gt;&gt; x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)] &gt;&gt;&gt; predictions = model(x)    Parameters  pretrained (bool) \u2013 If True, returns a model pre-trained on COCO train2017 progress (bool) \u2013 If True, displays a progress bar of the download to stderr    ", "parameters": ["pretrained (bool) : If True, returns a model pre-trained on COCO train2017", "progress (bool) : If True, displays a progress bar of the download to stderr"], "returns": null, "example": " model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n model.eval()\n x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n predictions = model(x)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.ToTensor.__call__", "item_type": "method", "code": "__call__(pic)", "description": " Parameters pic (PIL Image or numpy.ndarray) \u2013 Image to be converted to tensor.  Returns Converted image.  Return type Tensor   ", "parameters": ["pic (PIL Image or numpy.ndarray) : Image to be converted to tensor.", "Converted image.", "Tensor"], "returns": "Converted image.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.Compose", "item_type": "class", "code": "classtorchvision.transforms.Compose(transforms)", "description": "Composes several transforms together.  Parameters transforms (list of Transform objects) \u2013 list of transforms to compose.   Example &gt;&gt;&gt; transforms.Compose([ &gt;&gt;&gt;     transforms.CenterCrop(10), &gt;&gt;&gt;     transforms.ToTensor(), &gt;&gt;&gt; ])   ", "parameters": ["transforms (list of Transform objects) : list of transforms to compose."], "returns": null, "example": " transforms.Compose([\n     transforms.CenterCrop(10),\n     transforms.ToTensor(),\n ])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.CenterCrop", "item_type": "class", "code": "classtorchvision.transforms.CenterCrop(size)", "description": "Crops the given PIL Image at the center.  Parameters size (sequence or python:int) \u2013 Desired output size of the crop. If size is an int instead of sequence like (h, w), a square crop (size, size) is made.   ", "parameters": ["size (sequence or python:int) : Desired output size of the crop. If size is anint instead of sequence like (h, w), a square crop (size, size) ismade."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.ColorJitter", "item_type": "class", "code": "classtorchvision.transforms.ColorJitter(brightness=0,contrast=0,saturation=0,hue=0)", "description": "Randomly change the brightness, contrast and saturation of an image.  Parameters  brightness (python:float or tuple of python:float (min, max)) \u2013 How much to jitter brightness. brightness_factor is chosen uniformly from [max(0, 1 - brightness), 1 + brightness] or the given [min, max]. Should be non negative numbers. contrast (python:float or tuple of python:float (min, max)) \u2013 How much to jitter contrast. contrast_factor is chosen uniformly from [max(0, 1 - contrast), 1 + contrast] or the given [min, max]. Should be non negative numbers. saturation (python:float or tuple of python:float (min, max)) \u2013 How much to jitter saturation. saturation_factor is chosen uniformly from [max(0, 1 - saturation), 1 + saturation] or the given [min, max]. Should be non negative numbers. hue (python:float or tuple of python:float (min, max)) \u2013 How much to jitter hue. hue_factor is chosen uniformly from [-hue, hue] or the given [min, max]. Should have 0&lt;= hue &lt;= 0.5 or -0.5 &lt;= min &lt;= max &lt;= 0.5.    ", "parameters": ["brightness (python:float or tuple of python:float (min, max)) : How much to jitter brightness.brightness_factor is chosen uniformly from [max(0, 1 - brightness), 1 + brightness]or the given [min, max]. Should be non negative numbers.", "contrast (python:float or tuple of python:float (min, max)) : How much to jitter contrast.contrast_factor is chosen uniformly from [max(0, 1 - contrast), 1 + contrast]or the given [min, max]. Should be non negative numbers.", "saturation (python:float or tuple of python:float (min, max)) : How much to jitter saturation.saturation_factor is chosen uniformly from [max(0, 1 - saturation), 1 + saturation]or the given [min, max]. Should be non negative numbers.", "hue (python:float or tuple of python:float (min, max)) : How much to jitter hue.hue_factor is chosen uniformly from [-hue, hue] or the given [min, max].Should have 0&lt;= hue &lt;= 0.5 or -0.5 &lt;= min &lt;= max &lt;= 0.5."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.hub.download_url_to_file", "item_type": "function", "code": "torch.hub.download_url_to_file(url,dst,hash_prefix=None,progress=True)", "description": "Download object at the given URL to a local path.  Parameters  url (string) \u2013 URL of the object to download dst (string) \u2013 Full path where object will be saved, e.g. /tmp/temporary_file hash_prefix (string, optional) \u2013 If not None, the SHA256 downloaded file should start with hash_prefix. Default: None progress (bool, optional) \u2013 whether or not to display a progress bar to stderr Default: True    Example &gt;&gt;&gt; torch.hub.download_url_to_file('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth', '/tmp/temporary_file')   ", "parameters": ["url (string) : URL of the object to download", "dst (string) : Full path where object will be saved, e.g. /tmp/temporary_file", "hash_prefix (string, optional) : If not None, the SHA256 downloaded file should start with hash_prefix.Default: None", "progress (bool, optional) : whether or not to display a progress bar to stderrDefault: True"], "returns": null, "example": " torch.hub.download_url_to_file('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth', '/tmp/temporary_file')\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.hub.load_state_dict_from_url", "item_type": "function", "code": "torch.hub.load_state_dict_from_url(url,model_dir=None,map_location=None,progress=True,check_hash=False)", "description": "Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically decompressed. If the object is already present in model_dir, it\u2019s deserialized and returned. The default value of model_dir is $TORCH_HOME/checkpoints where environment variable $TORCH_HOME defaults to $XDG_CACHE_HOME/torch. $XDG_CACHE_HOME follows the X Design Group specification of the Linux filesytem layout, with a default value ~/.cache if not set.  Parameters  url (string) \u2013 URL of the object to download model_dir (string, optional) \u2013 directory in which to save the object map_location (optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress (bool, optional) \u2013 whether or not to display a progress bar to stderr. Default: True check_hash (bool, optional) \u2013 If True, the filename part of the URL should follow the naming convention filename-&lt;sha256&gt;.ext where &lt;sha256&gt; is the first eight or more digits of the SHA256 hash of the contents of the file. The hash is used to ensure unique names and to verify the contents of the file. Default: False    Example &gt;&gt;&gt; state_dict = torch.hub.load_state_dict_from_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth')   ", "parameters": ["url (string) : URL of the object to download", "model_dir (string, optional) : directory in which to save the object", "map_location (optional) : a function or a dict specifying how to remap storage locations (see torch.load)", "progress (bool, optional) : whether or not to display a progress bar to stderr.Default: True", "check_hash (bool, optional) : If True, the filename part of the URL should follow the naming conventionfilename-&lt;sha256&gt;.ext where &lt;sha256&gt; is the first eight or moredigits of the SHA256 hash of the contents of the file. The hash is used toensure unique names and to verify the contents of the file.Default: False"], "returns": null, "example": " state_dict = torch.hub.load_state_dict_from_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth')\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.hub.set_dir", "item_type": "function", "code": "torch.hub.set_dir(d)", "description": "Optionally set hub_dir to a local dir to save downloaded models &amp; weights. If set_dir is not called, default path is $TORCH_HOME/hub where environment variable $TORCH_HOME defaults to $XDG_CACHE_HOME/torch. $XDG_CACHE_HOME follows the X Design Group specification of the Linux filesytem layout, with a default value ~/.cache if the environment variable is not set.  Parameters d (string) \u2013 path to a local folder to save downloaded models &amp; weights.   ", "parameters": ["d (string) : path to a local folder to save downloaded models &amp; weights."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.kl.register_kl", "item_type": "function", "code": "torch.distributions.kl.register_kl(type_p,type_q)", "description": "Decorator to register a pairwise function with kl_divergence(). Usage: @register_kl(Normal, Normal) def kl_normal_normal(p, q):     # insert implementation here   Lookup returns the most specific (type,type) match ordered by subclass. If the match is ambiguous, a RuntimeWarning is raised. For example to resolve the ambiguous situation: @register_kl(BaseP, DerivedQ) def kl_version1(p, q): ... @register_kl(DerivedP, BaseQ) def kl_version2(p, q): ...   you should register a third most-specific implementation, e.g.: register_kl(DerivedP, DerivedQ)(kl_version1)  # Break the tie.    Parameters  type_p (python:type) \u2013 A subclass of Distribution. type_q (python:type) \u2013 A subclass of Distribution.    ", "parameters": ["type_p (python:type) : A subclass of Distribution.", "type_q (python:type) : A subclass of Distribution."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.distribution.Distribution.arg_constraints", "item_type": "method", "code": "propertyarg_constraints", "description": "Returns a dictionary from argument names to Constraint objects that should be satisfied by each argument of this distribution. Args that are not tensors need not appear in this dict. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.distribution.Distribution.batch_shape", "item_type": "method", "code": "propertybatch_shape", "description": "Returns the shape over which parameters are batched. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.distribution.Distribution.cdf", "item_type": "method", "code": "cdf(value)", "description": "Returns the cumulative density/mass function evaluated at value.  Parameters value (Tensor) \u2013    ", "parameters": ["value (Tensor) : "], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.distribution.Distribution.entropy", "item_type": "method", "code": "entropy()", "description": "Returns entropy of distribution, batched over batch_shape.  Returns Tensor of shape batch_shape.   ", "parameters": [], "returns": "Tensor of shape batch_shape.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.distribution.Distribution.enumerate_support", "item_type": "method", "code": "enumerate_support(expand=True)", "description": "Returns tensor containing all values supported by a discrete distribution. The result will enumerate over dimension 0, so the shape of the result will be (cardinality,) + batch_shape + event_shape (where event_shape = () for univariate distributions). Note that this enumerates over all batched tensors in lock-step [[0, 0], [1, 1], \u2026]. With expand=False, enumeration happens along dim 0, but with the remaining batch dimensions being singleton dimensions, [[0], [1], ... To iterate over the full Cartesian product use itertools.product(m.enumerate_support()).  Parameters expand (bool) \u2013 whether to expand the support over the batch dims to match the distribution\u2019s batch_shape.  Returns Tensor iterating over dimension 0.   ", "parameters": ["expand (bool) : whether to expand the support over thebatch dims to match the distribution\u2019s batch_shape.", "Tensor iterating over dimension 0."], "returns": "Tensor iterating over dimension 0.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.distribution.Distribution.event_shape", "item_type": "method", "code": "propertyevent_shape", "description": "Returns the shape of a single sample (without batching). ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.distribution.Distribution.expand", "item_type": "method", "code": "expand(batch_shape,_instance=None)", "description": "Returns a new distribution instance (or populates an existing instance provided by a derived class) with batch dimensions expanded to batch_shape. This method calls expand on the distribution\u2019s parameters. As such, this does not allocate new memory for the expanded distribution instance. Additionally, this does not repeat any args checking or parameter broadcasting in __init__.py, when an instance is first created.  Parameters  batch_shape (torch.Size) \u2013 the desired expanded size. _instance \u2013 new instance provided by subclasses that need to override .expand.   Returns New distribution instance with batch dimensions expanded to batch_size.   ", "parameters": ["batch_shape (torch.Size) : the desired expanded size.", "_instance : new instance provided by subclasses thatneed to override .expand."], "returns": "New distribution instance with batch dimensions expanded tobatch_size.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.init.xavier_normal_", "item_type": "function", "code": "torch.nn.init.xavier_normal_(tensor,gain=1.0)", "description": "Fills the input Tensor with values according to the method described in Understanding the difficulty of training deep feedforward neural networks - Glorot, X. &amp; Bengio, Y. (2010), using a normal distribution. The resulting tensor will have values sampled from N(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)   where  std=gain\u00d72fan_in+fan_out\\text{std} = \\text{gain} \\times \\sqrt{\\frac{2}{\\text{fan\\_in} + \\text{fan\\_out}}}  std=gain\u00d7fan_in+fan_out2\u200b\u200b  Also known as Glorot initialization.  Parameters  tensor \u2013 an n-dimensional torch.Tensor gain \u2013 an optional scaling factor    Examples &gt;&gt;&gt; w = torch.empty(3, 5) &gt;&gt;&gt; nn.init.xavier_normal_(w)   ", "parameters": ["tensor : an n-dimensional torch.Tensor", "gain : an optional scaling factor"], "returns": null, "example": " w = torch.empty(3, 5)\n nn.init.xavier_normal_(w)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.init.kaiming_uniform_", "item_type": "function", "code": "torch.nn.init.kaiming_uniform_(tensor,a=0,mode='fan_in',nonlinearity='leaky_relu')", "description": "Fills the input Tensor with values according to the method described in Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification - He, K. et al. (2015), using a uniform distribution. The resulting tensor will have values sampled from U(\u2212bound,bound)\\mathcal{U}(-\\text{bound}, \\text{bound})U(\u2212bound,bound)   where  bound=gain\u00d73fan_mode\\text{bound} = \\text{gain} \\times \\sqrt{\\frac{3}{\\text{fan\\_mode}}}  bound=gain\u00d7fan_mode3\u200b\u200b  Also known as He initialization.  Parameters  tensor \u2013 an n-dimensional torch.Tensor a \u2013 the negative slope of the rectifier used after this layer (only with 'leaky_relu') (used) \u2013  mode \u2013 either 'fan_in' (default) or 'fan_out'. Choosing 'fan_in' preserves the magnitude of the variance of the weights in the forward pass. Choosing 'fan_out' preserves the magnitudes in the backwards pass. nonlinearity \u2013 the non-linear function (nn.functional name), recommended to use only with 'relu' or 'leaky_relu' (default).    Examples &gt;&gt;&gt; w = torch.empty(3, 5) &gt;&gt;&gt; nn.init.kaiming_uniform_(w, mode='fan_in', nonlinearity='relu')   ", "parameters": ["tensor : an n-dimensional torch.Tensor", "a : the negative slope of the rectifier used after this layer (only", "with 'leaky_relu') (used) : ", "mode : either 'fan_in' (default) or 'fan_out'. Choosing 'fan_in'preserves the magnitude of the variance of the weights in theforward pass. Choosing 'fan_out' preserves the magnitudes in thebackwards pass.", "nonlinearity : the non-linear function (nn.functional name),recommended to use only with 'relu' or 'leaky_relu' (default)."], "returns": null, "example": " w = torch.empty(3, 5)\n nn.init.kaiming_uniform_(w, mode='fan_in', nonlinearity='relu')\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.init.kaiming_normal_", "item_type": "function", "code": "torch.nn.init.kaiming_normal_(tensor,a=0,mode='fan_in',nonlinearity='leaky_relu')", "description": "Fills the input Tensor with values according to the method described in Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification - He, K. et al. (2015), using a normal distribution. The resulting tensor will have values sampled from N(0,std2)\\mathcal{N}(0, \\text{std}^2)N(0,std2)   where  std=gainfan_mode\\text{std} = \\frac{\\text{gain}}{\\sqrt{\\text{fan\\_mode}}}  std=fan_mode\u200bgain\u200b  Also known as He initialization.  Parameters  tensor \u2013 an n-dimensional torch.Tensor a \u2013 the negative slope of the rectifier used after this layer (only with 'leaky_relu') (used) \u2013  mode \u2013 either 'fan_in' (default) or 'fan_out'. Choosing 'fan_in' preserves the magnitude of the variance of the weights in the forward pass. Choosing 'fan_out' preserves the magnitudes in the backwards pass. nonlinearity \u2013 the non-linear function (nn.functional name), recommended to use only with 'relu' or 'leaky_relu' (default).    Examples &gt;&gt;&gt; w = torch.empty(3, 5) &gt;&gt;&gt; nn.init.kaiming_normal_(w, mode='fan_out', nonlinearity='relu')   ", "parameters": ["tensor : an n-dimensional torch.Tensor", "a : the negative slope of the rectifier used after this layer (only", "with 'leaky_relu') (used) : ", "mode : either 'fan_in' (default) or 'fan_out'. Choosing 'fan_in'preserves the magnitude of the variance of the weights in theforward pass. Choosing 'fan_out' preserves the magnitudes in thebackwards pass.", "nonlinearity : the non-linear function (nn.functional name),recommended to use only with 'relu' or 'leaky_relu' (default)."], "returns": null, "example": " w = torch.empty(3, 5)\n nn.init.kaiming_normal_(w, mode='fan_out', nonlinearity='relu')\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.init.orthogonal_", "item_type": "function", "code": "torch.nn.init.orthogonal_(tensor,gain=1)", "description": "Fills the input Tensor with a (semi) orthogonal matrix, as described in Exact solutions to the nonlinear dynamics of learning in deep linear neural networks - Saxe, A. et al. (2013). The input tensor must have at least 2 dimensions, and for tensors with more than 2 dimensions the trailing dimensions are flattened.  Parameters  tensor \u2013 an n-dimensional torch.Tensor, where n\u22652n \\geq 2n\u22652   gain \u2013 optional scaling factor    Examples &gt;&gt;&gt; w = torch.empty(3, 5) &gt;&gt;&gt; nn.init.orthogonal_(w)   ", "parameters": ["tensor : an n-dimensional torch.Tensor, where n\u22652n \\geq 2n\u22652", "gain : optional scaling factor"], "returns": null, "example": " w = torch.empty(3, 5)\n nn.init.orthogonal_(w)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.init.sparse_", "item_type": "function", "code": "torch.nn.init.sparse_(tensor,sparsity,std=0.01)", "description": "Fills the 2D input Tensor as a sparse matrix, where the non-zero elements will be drawn from the normal distribution N(0,0.01)\\mathcal{N}(0, 0.01)N(0,0.01)  , as described in Deep learning via Hessian-free optimization - Martens, J. (2010).  Parameters  tensor \u2013 an n-dimensional torch.Tensor sparsity \u2013 The fraction of elements in each column to be set to zero std \u2013 the standard deviation of the normal distribution used to generate the non-zero values    Examples &gt;&gt;&gt; w = torch.empty(3, 5) &gt;&gt;&gt; nn.init.sparse_(w, sparsity=0.1)   ", "parameters": ["tensor : an n-dimensional torch.Tensor", "sparsity : The fraction of elements in each column to be set to zero", "std : the standard deviation of the normal distribution used to generatethe non-zero values"], "returns": null, "example": " w = torch.empty(3, 5)\n nn.init.sparse_(w, sparsity=0.1)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.utils.tensorboard.writer.SummaryWriter.add_hparams", "item_type": "method", "code": "add_hparams(hparam_dict=None,metric_dict=None)", "description": "Add a set of hyperparameters to be compared in TensorBoard.  Parameters  hparam_dict (dict) \u2013 Each key-value pair in the dictionary is the name of the hyper parameter and it\u2019s corresponding value. metric_dict (dict) \u2013 Each key-value pair in the dictionary is the name of the metric and it\u2019s corresponding value. Note that the key used here should be unique in the tensorboard record. Otherwise the value you added by add_scalar will be displayed in hparam plugin. In most cases, this is unwanted.    Examples: from torch.utils.tensorboard import SummaryWriter with SummaryWriter() as w:     for i in range(5):         w.add_hparams({'lr': 0.1*i, 'bsize': i},                       {'hparam/accuracy': 10*i, 'hparam/loss': 10*i})   Expected result:  ", "parameters": ["hparam_dict (dict) : Each key-value pair in the dictionary is thename of the hyper parameter and it\u2019s corresponding value.", "metric_dict (dict) : Each key-value pair in the dictionary is thename of the metric and it\u2019s corresponding value. Note that the key usedhere should be unique in the tensorboard record. Otherwise the valueyou added by add_scalar will be displayed in hparam plugin. In mostcases, this is unwanted."], "returns": null, "example": "from torch.utils.tensorboard import SummaryWriter\nwith SummaryWriter() as w:\n    for i in range(5):\n        w.add_hparams({'lr': 0.1*i, 'bsize': i},\n                      {'hparam/accuracy': 10*i, 'hparam/loss': 10*i})\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.utils.tensorboard.writer.SummaryWriter.flush", "item_type": "method", "code": "flush()", "description": "Flushes the event file to disk. Call this method to make sure that all pending events have been written to disk. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.utils.tensorboard.writer.SummaryWriter.close", "item_type": "method", "code": "close()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.utils.tensorboard.writer.SummaryWriter", "item_type": "class", "code": "classtorch.utils.tensorboard.writer.SummaryWriter(log_dir=None,comment='',purge_step=None,max_queue=10,flush_secs=120,filename_suffix='')", "description": "Writes entries directly to event files in the log_dir to be consumed by TensorBoard. The SummaryWriter class provides a high-level API to create an event file in a given directory and add summaries and events to it. The class updates the file contents asynchronously. This allows a training program to call methods to add data to the file directly from the training loop, without slowing down training.   __init__(log_dir=None, comment='', purge_step=None, max_queue=10, flush_secs=120, filename_suffix='')  Creates a SummaryWriter that will write out events and summaries to the event file.  Parameters  log_dir (string) \u2013 Save directory location. Default is runs/CURRENT_DATETIME_HOSTNAME, which changes after each run. Use hierarchical folder structure to compare between runs easily. e.g. pass in \u2018runs/exp1\u2019, \u2018runs/exp2\u2019, etc. for each new experiment to compare across them. comment (string) \u2013 Comment log_dir suffix appended to the default log_dir. If log_dir is assigned, this argument has no effect. purge_step (python:int) \u2013 When logging crashes at step T+XT+XT+X   and restarts at step TTT  , any events whose global_step larger or equal to TTT   will be purged and hidden from TensorBoard. Note that crashed and resumed experiments should have the same log_dir. max_queue (python:int) \u2013 Size of the queue for pending events and summaries before one of the \u2018add\u2019 calls forces a flush to disk. Default is ten items. flush_secs (python:int) \u2013 How often, in seconds, to flush the pending events and summaries to disk. Default is every two minutes. filename_suffix (string) \u2013 Suffix added to all event filenames in the log_dir directory. More details on filename construction in tensorboard.summary.writer.event_file_writer.EventFileWriter.    Examples: from torch.utils.tensorboard import SummaryWriter  # create a summary writer with automatically generated folder name. writer = SummaryWriter() # folder location: runs/May04_22-14-54_s-MacBook-Pro.local/  # create a summary writer using the specified folder name. writer = SummaryWriter(\"my_experiment\") # folder location: my_experiment  # create a summary writer with comment appended. writer = SummaryWriter(comment=\"LR_0.1_BATCH_16\") # folder location: runs/May04_22-14-54_s-MacBook-Pro.localLR_0.1_BATCH_16/       add_scalar(tag, scalar_value, global_step=None, walltime=None)  Add scalar data to summary.  Parameters  tag (string) \u2013 Data identifier scalar_value (python:float or string/blobname) \u2013 Value to save global_step (python:int) \u2013 Global step value to record walltime (python:float) \u2013 Optional override default walltime (time.time()) with seconds after epoch of event    Examples: from torch.utils.tensorboard import SummaryWriter writer = SummaryWriter() x = range(100) for i in x:     writer.add_scalar('y=2x', i * 2, i) writer.close()   Expected result:      add_scalars(main_tag, tag_scalar_dict, global_step=None, walltime=None)  Adds many scalar data to summary. Note that this function also keeps logged scalars in memory. In extreme case it explodes your RAM.  Parameters  main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (python:int) \u2013 Global step value to record walltime (python:float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event    Examples: from torch.utils.tensorboard import SummaryWriter writer = SummaryWriter() r = 5 for i in range(100):     writer.add_scalars('run_14h', {'xsinx':i*np.sin(i/r),                                     'xcosx':i*np.cos(i/r),                                     'tanx': np.tan(i/r)}, i) writer.close() # This call adds three values to the same scalar plot with the tag # 'run_14h' in TensorBoard's scalar section.   Expected result:      add_histogram(tag, values, global_step=None, bins='tensorflow', walltime=None, max_bins=None)  Add histogram to summary.  Parameters  tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (python:int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find other options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (python:float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event    Examples: from torch.utils.tensorboard import SummaryWriter import numpy as np writer = SummaryWriter() for i in range(10):     x = np.random.random(1000)     writer.add_histogram('distribution centers', x + i, i) writer.close()   Expected result:      add_image(tag, img_tensor, global_step=None, walltime=None, dataformats='CHW')  Add image data to summary. Note that this requires the pillow package.  Parameters  tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (python:int) \u2013 Global step value to record walltime (python:float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event     Shape:img_tensor: Default is (3,H,W)(3, H, W)(3,H,W)  . You can use torchvision.utils.make_grid() to convert a batch of tensor into 3xHxW format or call add_images and let us do the job. Tensor with (1,H,W)(1, H, W)(1,H,W)  , (H,W)(H, W)(H,W)  , (H,W,3)(H, W, 3)(H,W,3)   is also suitible as long as corresponding dataformats argument is passed. e.g. CHW, HWC, HW.   Examples: from torch.utils.tensorboard import SummaryWriter import numpy as np img = np.zeros((3, 100, 100)) img[0] = np.arange(0, 10000).reshape(100, 100) / 10000 img[1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000  img_HWC = np.zeros((100, 100, 3)) img_HWC[:, :, 0] = np.arange(0, 10000).reshape(100, 100) / 10000 img_HWC[:, :, 1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000  writer = SummaryWriter() writer.add_image('my_image', img, 0)  # If you have non-default dimension setting, set the dataformats argument. writer.add_image('my_image_HWC', img_HWC, 0, dataformats='HWC') writer.close()   Expected result:      add_images(tag, img_tensor, global_step=None, walltime=None, dataformats='NCHW')  Add batched image data to summary. Note that this requires the pillow package.  Parameters  tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (python:int) \u2013 Global step value to record walltime (python:float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event dataformats (string) \u2013 Image data format specification of the form NCHW, NHWC, CHW, HWC, HW, WH, etc.     Shape:img_tensor: Default is (N,3,H,W)(N, 3, H, W)(N,3,H,W)  . If dataformats is specified, other shape will be accepted. e.g. NCHW or NHWC.   Examples: from torch.utils.tensorboard import SummaryWriter import numpy as np  img_batch = np.zeros((16, 3, 100, 100)) for i in range(16):     img_batch[i, 0] = np.arange(0, 10000).reshape(100, 100) / 10000 / 16 * i     img_batch[i, 1] = (1 - np.arange(0, 10000).reshape(100, 100) / 10000) / 16 * i  writer = SummaryWriter() writer.add_images('my_image_batch', img_batch, 0) writer.close()   Expected result:      add_figure(tag, figure, global_step=None, close=True, walltime=None)  Render matplotlib figure into an image and add it to summary. Note that this requires the matplotlib package.  Parameters  tag (string) \u2013 Data identifier figure (matplotlib.pyplot.figure) \u2013 Figure or a list of figures global_step (python:int) \u2013 Global step value to record close (bool) \u2013 Flag to automatically close the figure walltime (python:float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event        add_video(tag, vid_tensor, global_step=None, fps=4, walltime=None)  Add video data to summary. Note that this requires the moviepy package.  Parameters  tag (string) \u2013 Data identifier vid_tensor (torch.Tensor) \u2013 Video data global_step (python:int) \u2013 Global step value to record fps (python:float or python:int) \u2013 Frames per second walltime (python:float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event     Shape:vid_tensor: (N,T,C,H,W)(N, T, C, H, W)(N,T,C,H,W)  . The values should lie in [0, 255] for type uint8 or [0, 1] for type float.       add_audio(tag, snd_tensor, global_step=None, sample_rate=44100, walltime=None)  Add audio data to summary.  Parameters  tag (string) \u2013 Data identifier snd_tensor (torch.Tensor) \u2013 Sound data global_step (python:int) \u2013 Global step value to record sample_rate (python:int) \u2013 sample rate in Hz walltime (python:float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event     Shape:snd_tensor: (1,L)(1, L)(1,L)  . The values should lie between [-1, 1].       add_text(tag, text_string, global_step=None, walltime=None)  Add text data to summary.  Parameters  tag (string) \u2013 Data identifier text_string (string) \u2013 String to save global_step (python:int) \u2013 Global step value to record walltime (python:float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event    Examples: writer.add_text('lstm', 'This is an lstm', 0) writer.add_text('rnn', 'This is an rnn', 10)       add_graph(model, input_to_model=None, verbose=False)      add_embedding(mat, metadata=None, label_img=None, global_step=None, tag='default', metadata_header=None)  Add embedding projector data to summary.  Parameters  mat (torch.Tensor or numpy.array) \u2013 A matrix which each row is the feature vector of the data point metadata (list) \u2013 A list of labels, each element will be convert to string label_img (torch.Tensor) \u2013 Images correspond to each data point global_step (python:int) \u2013 Global step value to record tag (string) \u2013 Name for the embedding     Shape:mat: (N,D)(N, D)(N,D)  , where N is number of data and D is feature dimension label_img: (N,C,H,W)(N, C, H, W)(N,C,H,W)     Examples: import keyword import torch meta = [] while len(meta)&lt;100:     meta = meta+keyword.kwlist # get some strings meta = meta[:100]  for i, v in enumerate(meta):     meta[i] = v+str(i)  label_img = torch.rand(100, 3, 10, 32) for i in range(100):     label_img[i]*=i/100.0  writer.add_embedding(torch.randn(100, 5), metadata=meta, label_img=label_img) writer.add_embedding(torch.randn(100, 5), label_img=label_img) writer.add_embedding(torch.randn(100, 5), metadata=meta)       add_pr_curve(tag, labels, predictions, global_step=None, num_thresholds=127, weights=None, walltime=None)  Adds precision recall curve. Plotting a precision-recall curve lets you understand your model\u2019s performance under different threshold settings. With this function, you provide the ground truth labeling (T/F) and prediction confidence (usually the output of your model) for each target. The TensorBoard UI will let you choose the threshold interactively.  Parameters  tag (string) \u2013 Data identifier labels (torch.Tensor, numpy.array, or string/blobname) \u2013 Ground truth data. Binary label for each element. predictions (torch.Tensor, numpy.array, or string/blobname) \u2013 The probability that an element be classified as true. Value should in [0, 1] global_step (python:int) \u2013 Global step value to record num_thresholds (python:int) \u2013 Number of thresholds used to draw the curve. walltime (python:float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event    Examples: from torch.utils.tensorboard import SummaryWriter import numpy as np labels = np.random.randint(2, size=100)  # binary label predictions = np.random.rand(100) writer = SummaryWriter() writer.add_pr_curve('pr_curve', labels, predictions, 0) writer.close()       add_custom_scalars(layout)  Create special chart by collecting charts tags in \u2018scalars\u2019. Note that this function can only be called once for each SummaryWriter() object. Because it only provides metadata to tensorboard, the function can be called before or after the training loop.  Parameters layout (dict) \u2013 {categoryName: charts}, where charts is also a dictionary {chartName: ListOfProperties}. The first element in ListOfProperties is the chart\u2019s type (one of Multiline or Margin) and the second element should be a list containing the tags you have used in add_scalar function, which will be collected into the new chart.   Examples: layout = {'Taiwan':{'twse':['Multiline',['twse/0050', 'twse/2330']]},              'USA':{ 'dow':['Margin',   ['dow/aaa', 'dow/bbb', 'dow/ccc']],                   'nasdaq':['Margin',   ['nasdaq/aaa', 'nasdaq/bbb', 'nasdaq/ccc']]}}  writer.add_custom_scalars(layout)       add_mesh(tag, vertices, colors=None, faces=None, config_dict=None, global_step=None, walltime=None)  Add meshes or 3D point clouds to TensorBoard. The visualization is based on Three.js, so it allows users to interact with the rendered object. Besides the basic definitions such as vertices, faces, users can further provide camera parameter, lighting condition, etc. Please check https://threejs.org/docs/index.html#manual/en/introduction/Creating-a-scene for advanced usage.  Parameters  tag (string) \u2013 Data identifier vertices (torch.Tensor) \u2013 List of the 3D coordinates of vertices. colors (torch.Tensor) \u2013 Colors for each vertex faces (torch.Tensor) \u2013 Indices of vertices within each triangle. (Optional) config_dict \u2013 Dictionary with ThreeJS classes names and configuration. global_step (python:int) \u2013 Global step value to record walltime (python:float) \u2013 Optional override default walltime (time.time()) seconds after epoch of event     Shape:vertices: (B,N,3)(B, N, 3)(B,N,3)  . (batch, number_of_vertices, channels) colors: (B,N,3)(B, N, 3)(B,N,3)  . The values should lie in [0, 255] for type uint8 or [0, 1] for type float. faces: (B,N,3)(B, N, 3)(B,N,3)  . The values should lie in [0, number_of_vertices] for type uint8.   Examples: from torch.utils.tensorboard import SummaryWriter vertices_tensor = torch.as_tensor([     [1, 1, 1],     [-1, -1, 1],     [1, -1, -1],     [-1, 1, -1], ], dtype=torch.float).unsqueeze(0) colors_tensor = torch.as_tensor([     [255, 0, 0],     [0, 255, 0],     [0, 0, 255],     [255, 0, 255], ], dtype=torch.int).unsqueeze(0) faces_tensor = torch.as_tensor([     [0, 2, 3],     [0, 3, 1],     [0, 1, 2],     [1, 3, 2], ], dtype=torch.int).unsqueeze(0)  writer = SummaryWriter() writer.add_mesh('my_mesh', vertices=vertices_tensor, colors=colors_tensor, faces=faces_tensor)  writer.close()       add_hparams(hparam_dict=None, metric_dict=None)  Add a set of hyperparameters to be compared in TensorBoard.  Parameters  hparam_dict (dict) \u2013 Each key-value pair in the dictionary is the name of the hyper parameter and it\u2019s corresponding value. metric_dict (dict) \u2013 Each key-value pair in the dictionary is the name of the metric and it\u2019s corresponding value. Note that the key used here should be unique in the tensorboard record. Otherwise the value you added by add_scalar will be displayed in hparam plugin. In most cases, this is unwanted.    Examples: from torch.utils.tensorboard import SummaryWriter with SummaryWriter() as w:     for i in range(5):         w.add_hparams({'lr': 0.1*i, 'bsize': i},                       {'hparam/accuracy': 10*i, 'hparam/loss': 10*i})   Expected result:      flush()  Flushes the event file to disk. Call this method to make sure that all pending events have been written to disk.     close()    ", "parameters": ["log_dir (string) : Save directory location. Default isruns/CURRENT_DATETIME_HOSTNAME, which changes after each run.Use hierarchical folder structure to comparebetween runs easily. e.g. pass in \u2018runs/exp1\u2019, \u2018runs/exp2\u2019, etc.for each new experiment to compare across them.", "comment (string) : Comment log_dir suffix appended to the defaultlog_dir. If log_dir is assigned, this argument has no effect.", "purge_step (python:int) : When logging crashes at step T+XT+XT+X and restarts at step TTT,any events whose global_step larger or equal to TTT will bepurged and hidden from TensorBoard.Note that crashed and resumed experiments should have the same log_dir.", "max_queue (python:int) : Size of the queue for pending events andsummaries before one of the \u2018add\u2019 calls forces a flush to disk.Default is ten items.", "flush_secs (python:int) : How often, in seconds, to flush thepending events and summaries to disk. Default is every two minutes.", "filename_suffix (string) : Suffix added to all event filenames inthe log_dir directory. More details on filename construction intensorboard.summary.writer.event_file_writer.EventFileWriter.", "tag (string) : Data identifier", "scalar_value (python:float or string/blobname) : Value to save", "global_step (python:int) : Global step value to record", "walltime (python:float) : Optional override default walltime (time.time())with seconds after epoch of event", "main_tag (string) : The parent name for the tags", "tag_scalar_dict (dict) : Key-value pair storing the tag and corresponding values", "global_step (python:int) : Global step value to record", "walltime (python:float) : Optional override default walltime (time.time())seconds after epoch of event", "tag (string) : Data identifier", "values (torch.Tensor, numpy.array, or string/blobname) : Values to build histogram", "global_step (python:int) : Global step value to record", "bins (string) : One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can findother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html", "walltime (python:float) : Optional override default walltime (time.time())seconds after epoch of event", "tag (string) : Data identifier", "img_tensor (torch.Tensor, numpy.array, or string/blobname) : Image data", "global_step (python:int) : Global step value to record", "walltime (python:float) : Optional override default walltime (time.time())seconds after epoch of event", "tag (string) : Data identifier", "img_tensor (torch.Tensor, numpy.array, or string/blobname) : Image data", "global_step (python:int) : Global step value to record", "walltime (python:float) : Optional override default walltime (time.time())seconds after epoch of event", "dataformats (string) : Image data format specification of the formNCHW, NHWC, CHW, HWC, HW, WH, etc.", "tag (string) : Data identifier", "figure (matplotlib.pyplot.figure) : Figure or a list of figures", "global_step (python:int) : Global step value to record", "close (bool) : Flag to automatically close the figure", "walltime (python:float) : Optional override default walltime (time.time())seconds after epoch of event", "tag (string) : Data identifier", "vid_tensor (torch.Tensor) : Video data", "global_step (python:int) : Global step value to record", "fps (python:float or python:int) : Frames per second", "walltime (python:float) : Optional override default walltime (time.time())seconds after epoch of event", "tag (string) : Data identifier", "snd_tensor (torch.Tensor) : Sound data", "global_step (python:int) : Global step value to record", "sample_rate (python:int) : sample rate in Hz", "walltime (python:float) : Optional override default walltime (time.time())seconds after epoch of event", "tag (string) : Data identifier", "text_string (string) : String to save", "global_step (python:int) : Global step value to record", "walltime (python:float) : Optional override default walltime (time.time())seconds after epoch of event", "mat (torch.Tensor or numpy.array) : A matrix which each row is the feature vector of the data point", "metadata (list) : A list of labels, each element will be convert to string", "label_img (torch.Tensor) : Images correspond to each data point", "global_step (python:int) : Global step value to record", "tag (string) : Name for the embedding", "tag (string) : Data identifier", "labels (torch.Tensor, numpy.array, or string/blobname) : Ground truth data. Binary label for each element.", "predictions (torch.Tensor, numpy.array, or string/blobname) : The probability that an element be classified as true.Value should in [0, 1]", "global_step (python:int) : Global step value to record", "num_thresholds (python:int) : Number of thresholds used to draw the curve.", "walltime (python:float) : Optional override default walltime (time.time())seconds after epoch of event", "tag (string) : Data identifier", "vertices (torch.Tensor) : List of the 3D coordinates of vertices.", "colors (torch.Tensor) : Colors for each vertex", "faces (torch.Tensor) : Indices of vertices within each triangle. (Optional)", "config_dict : Dictionary with ThreeJS classes names and configuration.", "global_step (python:int) : Global step value to record", "walltime (python:float) : Optional override default walltime (time.time())seconds after epoch of event", "hparam_dict (dict) : Each key-value pair in the dictionary is thename of the hyper parameter and it\u2019s corresponding value.", "metric_dict (dict) : Each key-value pair in the dictionary is thename of the metric and it\u2019s corresponding value. Note that the key usedhere should be unique in the tensorboard record. Otherwise the valueyou added by add_scalar will be displayed in hparam plugin. In mostcases, this is unwanted."], "returns": null, "example": "from torch.utils.tensorboard import SummaryWriter\n\n# create a summary writer with automatically generated folder name.\nwriter = SummaryWriter()\n# folder location: runs/May04_22-14-54_s-MacBook-Pro.local/\n\n# create a summary writer using the specified folder name.\nwriter = SummaryWriter(\"my_experiment\")\n# folder location: my_experiment\n\n# create a summary writer with comment appended.\nwriter = SummaryWriter(comment=\"LR_0.1_BATCH_16\")\n# folder location: runs/May04_22-14-54_s-MacBook-Pro.localLR_0.1_BATCH_16/\n\n", "shape": "img_tensor: Default is (3,H,W)(3, H, W)(3,H,W)  . You can use torchvision.utils.make_grid() to convert a batch of tensor into 3xHxW format or call add_images and let us do the job. Tensor with (1,H,W)(1, H, W)(1,H,W)  , (H,W)(H, W)(H,W)  , (H,W,3)(H, W, 3)(H,W,3)   is also suitible as long as corresponding dataformats argument is passed. e.g. CHW, HWC, HW. "},
{"library": "torch", "item_id": "torch.distributed.get_backend", "item_type": "function", "code": "torch.distributed.get_backend(group=&lt;objectobject&gt;)", "description": "Returns the backend of the given process group.  Parameters group (ProcessGroup, optional) \u2013 The process group to work on. The default is the general main process group. If another specific group is specified, the calling process must be part of group.  Returns The backend of the given process group as a lower case string.   ", "parameters": ["group (ProcessGroup, optional) : The process group to work on. Thedefault is the general main process group. If another specific groupis specified, the calling process must be part of group.", "The backend of the given process group as a lower case string."], "returns": "The backend of the given process group as a lower case string.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributed.get_rank", "item_type": "function", "code": "torch.distributed.get_rank(group=&lt;objectobject&gt;)", "description": "Returns the rank of current process group Rank is a unique identifier assigned to each process within a distributed process group. They are always consecutive integers ranging from 0 to world_size.  Parameters group (ProcessGroup, optional) \u2013 The process group to work on  Returns The rank of the process group -1, if not part of the group   ", "parameters": ["group (ProcessGroup, optional) : The process group to work on", "The rank of the process group-1, if not part of the group"], "returns": "The rank of the process group-1, if not part of the group", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributed.get_world_size", "item_type": "function", "code": "torch.distributed.get_world_size(group=&lt;objectobject&gt;)", "description": "Returns the number of processes in the current process group  Parameters group (ProcessGroup, optional) \u2013 The process group to work on  Returns The world size of the process group -1, if not part of the group   ", "parameters": ["group (ProcessGroup, optional) : The process group to work on", "The world size of the process group-1, if not part of the group"], "returns": "The world size of the process group-1, if not part of the group", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributed.is_initialized", "item_type": "function", "code": "torch.distributed.is_initialized()", "description": "Checking if the default process group has been initialized ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributed.is_mpi_available", "item_type": "function", "code": "torch.distributed.is_mpi_available()", "description": "Checks if the MPI backend is available. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributed.is_nccl_available", "item_type": "function", "code": "torch.distributed.is_nccl_available()", "description": "Checks if the NCCL backend is available. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributed.new_group", "item_type": "function", "code": "torch.distributed.new_group(ranks=None,timeout=datetime.timedelta(0,1800),backend=None)", "description": "Creates a new distributed group. This function requires that all processes in the main group (i.e. all processes that are part of the distributed job) enter this function, even if they are not going to be members of the group. Additionally, groups should be created in the same order in all processes.  Parameters  ranks (list[python:int]) \u2013 List of ranks of group members. timeout (timedelta, optional) \u2013 Timeout for operations executed against the process group. Default value equals 30 minutes. This is only applicable for the gloo backend. backend (str or Backend, optional) \u2013 The backend to use. Depending on build-time configurations, valid values are gloo and nccl. By default uses the same backend as the global group. This field should be given as a lowercase string (e.g., \"gloo\"), which can also be accessed via Backend attributes (e.g., Backend.GLOO).   Returns A handle of distributed group that can be given to collective calls.   ", "parameters": ["ranks (list[python:int]) : List of ranks of group members.", "timeout (timedelta, optional) : Timeout for operations executed againstthe process group. Default value equals 30 minutes.This is only applicable for the gloo backend.", "backend (str or Backend, optional) : The backend to use. Depending onbuild-time configurations, valid values are gloo and nccl.By default uses the same backend as the global group. This fieldshould be given as a lowercase string (e.g., \"gloo\"), which canalso be accessed via Backend attributes (e.g.,Backend.GLOO)."], "returns": "A handle of distributed group that can be given to collective calls.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributed.send", "item_type": "function", "code": "torch.distributed.send(tensor,dst,group=&lt;objectobject&gt;,tag=0)", "description": "Sends a tensor synchronously.  Parameters  tensor (Tensor) \u2013 Tensor to send. dst (python:int) \u2013 Destination rank. group (ProcessGroup, optional) \u2013 The process group to work on tag (python:int, optional) \u2013 Tag to match send with remote recv    ", "parameters": ["tensor (Tensor) : Tensor to send.", "dst (python:int) : Destination rank.", "group (ProcessGroup, optional) : The process group to work on", "tag (python:int, optional) : Tag to match send with remote recv"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributed.recv", "item_type": "function", "code": "torch.distributed.recv(tensor,src=None,group=&lt;objectobject&gt;,tag=0)", "description": "Receives a tensor synchronously.  Parameters  tensor (Tensor) \u2013 Tensor to fill with received data. src (python:int, optional) \u2013 Source rank. Will receive from any process if unspecified. group (ProcessGroup, optional) \u2013 The process group to work on tag (python:int, optional) \u2013 Tag to match recv with remote send   Returns Sender rank -1, if not part of the group   ", "parameters": ["tensor (Tensor) : Tensor to fill with received data.", "src (python:int, optional) : Source rank. Will receive from anyprocess if unspecified.", "group (ProcessGroup, optional) : The process group to work on", "tag (python:int, optional) : Tag to match recv with remote send"], "returns": "Sender rank-1, if not part of the group", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.device_count", "item_type": "function", "code": "torch.cuda.device_count()", "description": "Returns the number of GPUs available. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.get_device_capability", "item_type": "function", "code": "torch.cuda.get_device_capability(device=None)", "description": "Gets the cuda capability of a device.  Parameters device (torch.device or python:int, optional) \u2013 device for which to return the device capability. This function is a no-op if this argument is a negative integer. It uses the current device, given by current_device(), if device is None (default).  Returns the major and minor cuda capability of the device  Return type tuple(int, int)   ", "parameters": ["device (torch.device or python:int, optional) : device for which to return thedevice capability. This function is a no-op if this argument isa negative integer. It uses the current device, given bycurrent_device(), if device is None(default).", "the major and minor cuda capability of the device", "tuple(int, int)"], "returns": "the major and minor cuda capability of the device", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.get_device_name", "item_type": "function", "code": "torch.cuda.get_device_name(device=None)", "description": "Gets the name of a device.  Parameters device (torch.device or python:int, optional) \u2013 device for which to return the name. This function is a no-op if this argument is a negative integer. It uses the current device, given by current_device(), if device is None (default).   ", "parameters": ["device (torch.device or python:int, optional) : device for which to return thename. This function is a no-op if this argument is a negativeinteger. It uses the current device, given by current_device(),if device is None (default)."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.init", "item_type": "function", "code": "torch.cuda.init()", "description": "Initialize PyTorch\u2019s CUDA state.  You may need to call this explicitly if you are interacting with PyTorch via its C API, as Python bindings for CUDA functionality will not be until this initialization takes place.  Ordinary users should not need this, as all of PyTorch\u2019s CUDA methods automatically initialize CUDA state on-demand. Does nothing if the CUDA state is already initialized. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.ipc_collect", "item_type": "function", "code": "torch.cuda.ipc_collect()", "description": "Force collects GPU memory after it has been released by CUDA IPC.  Note Checks if any sent CUDA tensors could be cleaned from the memory. Force closes shared memory file used for reference counting if there is no active counters. Useful when the producer process stopped actively sending tensors and want to release unused memory.  ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.is_available", "item_type": "function", "code": "torch.cuda.is_available()", "description": "Returns a bool indicating if CUDA is currently available. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.is_initialized", "item_type": "function", "code": "torch.cuda.is_initialized()", "description": "Returns whether PyTorch\u2019s CUDA state has been initialized. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.set_device", "item_type": "function", "code": "torch.cuda.set_device(device)", "description": "Sets the current device. Usage of this function is discouraged in favor of device. In most cases it\u2019s better to use CUDA_VISIBLE_DEVICES environmental variable.  Parameters device (torch.device or python:int) \u2013 selected device. This function is a no-op if this argument is negative.   ", "parameters": ["device (torch.device or python:int) : selected device. This function is a no-opif this argument is negative."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.jit.save", "item_type": "function", "code": "torch.jit.save(m,f,_extra_files=ExtraFilesMap{})", "description": "Save an offline version of this module for use in a separate process. The saved module serializes all of the methods, submodules, parameters, and attributes of this module. It can be loaded into the C++ API using torch::jit::load(filename) or into the Python API with torch.jit.load. To be able to save a module, it must not make any calls to native Python functions. This means that all submodules must be subclasses of torch.jit.ScriptModule as well.  Danger All modules, no matter their device, are always loaded onto the CPU during loading. This is different from load\u2019s semantics and may change in the future.   Parameters  m \u2013 A ScriptModule to save. f \u2013 A file-like object (has to implement write and flush) or a string containing a file name. _extra_files \u2013 Map from filename to contents which will be stored as part of \u2018f\u2019.     Warning If you are using Python 2, torch.jit.save does NOT support StringIO.StringIO as a valid file-like object. This is because the write method should return the number of bytes written; StringIO.write() does not do this. Please use something like io.BytesIO instead.  Example: import torch import io  class MyModule(torch.nn.Module):     def forward(self, x):         return x + 10  m = torch.jit.script(MyModule())  # Save to file torch.jit.save(m, 'scriptmodule.pt') # This line is equivalent to the previous m.save(\"scriptmodule.pt\")  # Save to io.BytesIO buffer buffer = io.BytesIO() torch.jit.save(m, buffer)  # Save with extra files extra_files = torch._C.ExtraFilesMap() extra_files['foo.txt'] = 'bar' torch.jit.save(m, 'scriptmodule.pt', _extra_files=extra_files)   ", "parameters": ["m : A ScriptModule to save.", "f : A file-like object (has to implement write and flush) or a stringcontaining a file name.", "_extra_files : Map from filename to contents which will be stored as part of \u2018f\u2019."], "returns": null, "example": "import torch\nimport io\n\nclass MyModule(torch.nn.Module):\n    def forward(self, x):\n        return x + 10\n\nm = torch.jit.script(MyModule())\n\n# Save to file\ntorch.jit.save(m, 'scriptmodule.pt')\n# This line is equivalent to the previous\nm.save(\"scriptmodule.pt\")\n\n# Save to io.BytesIO buffer\nbuffer = io.BytesIO()\ntorch.jit.save(m, buffer)\n\n# Save with extra files\nextra_files = torch._C.ExtraFilesMap()\nextra_files['foo.txt'] = 'bar'\ntorch.jit.save(m, 'scriptmodule.pt', _extra_files=extra_files)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.jit.load", "item_type": "function", "code": "torch.jit.load(f,map_location=None,_extra_files=ExtraFilesMap{})", "description": "Load a ScriptModule or ScriptFunction previously saved with torch.jit.save All previously saved modules, no matter their device, are first loaded onto CPU, and then are moved to the devices they were saved from. If this fails (e.g. because the run time system doesn\u2019t have certain devices), an exception is raised.  Parameters  f \u2013 a file-like object (has to implement read, readline, tell, and seek), or a string containing a file name map_location (string or torch.device) \u2013 A simplified version of map_location in torch.save used to dynamically remap storages to an alternative set of devices. _extra_files (dictionary of filename to content) \u2013 The extra filenames given in the map would be loaded and their content would be stored in the provided map.   Returns A ScriptModule object.   Example: import torch import io  torch.jit.load('scriptmodule.pt')  # Load ScriptModule from io.BytesIO object with open('scriptmodule.pt', 'rb') as f:     buffer = io.BytesIO(f.read())  # Load all tensors to the original device torch.jit.load(buffer)  # Load all tensors onto CPU, using a device buffer.seek(0) torch.jit.load(buffer, map_location=torch.device('cpu'))  # Load all tensors onto CPU, using a string buffer.seek(0) torch.jit.load(buffer, map_location='cpu')  # Load with extra files. extra_files = torch._C.ExtraFilesMap() extra_files['foo.txt'] = 'bar' torch.jit.load('scriptmodule.pt', _extra_files=extra_files) print(extra_files['foo.txt'])   ", "parameters": ["f : a file-like object (has to implement read, readline, tell, and seek),or a string containing a file name", "map_location (string or torch.device) : A simplified version of map_location intorch.save used to dynamically remap storages to an alternative set of devices.", "_extra_files (dictionary of filename to content) : The extrafilenames given in the map would be loaded and their contentwould be stored in the provided map."], "returns": "A ScriptModule object.", "example": "import torch\nimport io\n\ntorch.jit.load('scriptmodule.pt')\n\n# Load ScriptModule from io.BytesIO object\nwith open('scriptmodule.pt', 'rb') as f:\n    buffer = io.BytesIO(f.read())\n\n# Load all tensors to the original device\ntorch.jit.load(buffer)\n\n# Load all tensors onto CPU, using a device\nbuffer.seek(0)\ntorch.jit.load(buffer, map_location=torch.device('cpu'))\n\n# Load all tensors onto CPU, using a string\nbuffer.seek(0)\ntorch.jit.load(buffer, map_location='cpu')\n\n# Load with extra files.\nextra_files = torch._C.ExtraFilesMap()\nextra_files['foo.txt'] = 'bar'\ntorch.jit.load('scriptmodule.pt', _extra_files=extra_files)\nprint(extra_files['foo.txt'])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.jit.export", "item_type": "function", "code": "torch.jit.export(fn)", "description": "This decorator indicates that a method on an nn.Module is used as an entry point into a ScriptModule and should be compiled. forward implicitly is assumed to be an entry point, so it does not need this decorator. Functions and methods called from forward are compiled as they are seen by the compiler, so they do not need this decorator either. Example (using @torch.jit.export on a method): import torch import torch.nn as nn  class MyModule(nn.Module):     def implicitly_compiled_method(self, x):         return x + 99      # `forward` is implicitly decorated with `@torch.jit.export`,     # so adding it here would have no effect     def forward(self, x):         return x + 10      @torch.jit.export     def another_forward(self, x):         # When the compiler sees this call, it will compile         # `implicitly_compiled_method`         return self.implicitly_compiled_method(x)      def unused_method(self, x):         return x - 20  # `m` will contain compiled methods: #     `forward` #     `another_forward` #     `implicitly_compiled_method` # `unused_method` will not be compiled since it was not called from # any compiled methods and wasn't decorated with `@torch.jit.export` m = torch.jit.script(MyModule())   ", "parameters": [], "returns": null, "example": "import torch\nimport torch.nn as nn\n\nclass MyModule(nn.Module):\n    def implicitly_compiled_method(self, x):\n        return x + 99\n\n    # `forward` is implicitly decorated with `@torch.jit.export`,\n    # so adding it here would have no effect\n    def forward(self, x):\n        return x + 10\n\n    @torch.jit.export\n    def another_forward(self, x):\n        # When the compiler sees this call, it will compile\n        # `implicitly_compiled_method`\n        return self.implicitly_compiled_method(x)\n\n    def unused_method(self, x):\n        return x - 20\n\n# `m` will contain compiled methods:\n#     `forward`\n#     `another_forward`\n#     `implicitly_compiled_method`\n# `unused_method` will not be compiled since it was not called from\n# any compiled methods and wasn't decorated with `@torch.jit.export`\nm = torch.jit.script(MyModule())\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.autograd.gradcheck", "item_type": "function", "code": "torch.autograd.gradcheck(func,inputs,eps=1e-06,atol=1e-05,rtol=0.001,raise_exception=True,check_sparse_nnz=False,nondet_tol=0.0)", "description": "Check gradients computed via small finite differences against analytical gradients w.r.t. tensors in inputs that are of floating point type and with requires_grad=True. The check between numerical and analytical gradients uses allclose().  Note The default values are designed for input of double precision. This check will likely fail if input is of less precision, e.g., FloatTensor.   Warning If any checked tensor in input has overlapping memory, i.e., different indices pointing to the same memory address (e.g., from torch.expand()), this check will likely fail because the numerical gradients computed by point perturbation at such indices will change values at all other indices that share the same memory address.   Parameters  func (function) \u2013 a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors inputs (tuple of Tensor or Tensor) \u2013 inputs to the function eps (python:float, optional) \u2013 perturbation for finite differences atol (python:float, optional) \u2013 absolute tolerance rtol (python:float, optional) \u2013 relative tolerance raise_exception (bool, optional) \u2013 indicating whether to raise an exception if the check fails. The exception gives more information about the exact nature of the failure. This is helpful when debugging gradchecks. check_sparse_nnz (bool, optional) \u2013 if True, gradcheck allows for SparseTensor input, and for any SparseTensor at input, gradcheck will perform check at nnz positions only. nondet_tol (python:float, optional) \u2013 tolerance for non-determinism. When running identical inputs through the differentiation, the results must either match exactly (default, 0.0) or be within this tolerance.   Returns True if all differences satisfy allclose condition   ", "parameters": ["func (function) : a Python function that takes Tensor inputs and returnsa Tensor or a tuple of Tensors", "inputs (tuple of Tensor or Tensor) : inputs to the function", "eps (python:float, optional) : perturbation for finite differences", "atol (python:float, optional) : absolute tolerance", "rtol (python:float, optional) : relative tolerance", "raise_exception (bool, optional) : indicating whether to raise an exception ifthe check fails. The exception gives more information about theexact nature of the failure. This is helpful when debugging gradchecks.", "check_sparse_nnz (bool, optional) : if True, gradcheck allows for SparseTensor input,and for any SparseTensor at input, gradcheck will perform check at nnz positions only.", "nondet_tol (python:float, optional) : tolerance for non-determinism. When runningidentical inputs through the differentiation, the results must either matchexactly (default, 0.0) or be within this tolerance."], "returns": "True if all differences satisfy allclose condition", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.autograd.gradgradcheck", "item_type": "function", "code": "torch.autograd.gradgradcheck(func,inputs,grad_outputs=None,eps=1e-06,atol=1e-05,rtol=0.001,gen_non_contig_grad_outputs=False,raise_exception=True,nondet_tol=0.0)", "description": "Check gradients of gradients computed via small finite differences against analytical gradients w.r.t. tensors in inputs and grad_outputs that are of floating point type and with requires_grad=True. This function checks that backpropagating through the gradients computed to the given grad_outputs are correct. The check between numerical and analytical gradients uses allclose().  Note The default values are designed for input and grad_outputs of double precision. This check will likely fail if they are of less precision, e.g., FloatTensor.   Warning If any checked tensor in input and grad_outputs has overlapping memory, i.e., different indices pointing to the same memory address (e.g., from torch.expand()), this check will likely fail because the numerical gradients computed by point perturbation at such indices will change values at all other indices that share the same memory address.   Parameters  func (function) \u2013 a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors inputs (tuple of Tensor or Tensor) \u2013 inputs to the function grad_outputs (tuple of Tensor or Tensor, optional) \u2013 The gradients with respect to the function\u2019s outputs. eps (python:float, optional) \u2013 perturbation for finite differences atol (python:float, optional) \u2013 absolute tolerance rtol (python:float, optional) \u2013 relative tolerance gen_non_contig_grad_outputs (bool, optional) \u2013 if grad_outputs is None and gen_non_contig_grad_outputs is True, the randomly generated gradient outputs are made to be noncontiguous raise_exception (bool, optional) \u2013 indicating whether to raise an exception if the check fails. The exception gives more information about the exact nature of the failure. This is helpful when debugging gradchecks. nondet_tol (python:float, optional) \u2013 tolerance for non-determinism. When running identical inputs through the differentiation, the results must either match exactly (default, 0.0) or be within this tolerance. Note that a small amount of nondeterminism in the gradient will lead to larger inaccuracies in the second derivative.   Returns True if all differences satisfy allclose condition   ", "parameters": ["func (function) : a Python function that takes Tensor inputs and returnsa Tensor or a tuple of Tensors", "inputs (tuple of Tensor or Tensor) : inputs to the function", "grad_outputs (tuple of Tensor or Tensor, optional) : The gradients withrespect to the function\u2019s outputs.", "eps (python:float, optional) : perturbation for finite differences", "atol (python:float, optional) : absolute tolerance", "rtol (python:float, optional) : relative tolerance", "gen_non_contig_grad_outputs (bool, optional) : if grad_outputs isNone and gen_non_contig_grad_outputs is True, therandomly generated gradient outputs are made to be noncontiguous", "raise_exception (bool, optional) : indicating whether to raise an exception ifthe check fails. The exception gives more information about theexact nature of the failure. This is helpful when debugging gradchecks.", "nondet_tol (python:float, optional) : tolerance for non-determinism. When runningidentical inputs through the differentiation, the results must either matchexactly (default, 0.0) or be within this tolerance. Note that a small amountof nondeterminism in the gradient will lead to larger inaccuracies inthe second derivative."], "returns": "True if all differences satisfy allclose condition", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.autograd.profiler.load_nvprof", "item_type": "function", "code": "torch.autograd.profiler.load_nvprof(path)", "description": "Opens an nvprof trace file and parses autograd annotations.  Parameters path (str) \u2013 path to nvprof trace   ", "parameters": ["path (str) : path to nvprof trace"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.backward", "item_type": "method", "code": "backward(gradient=None,retain_graph=None,create_graph=False)", "description": "Computes the gradient of current tensor w.r.t. graph leaves. The graph is differentiated using the chain rule. If the tensor is non-scalar (i.e. its data has more than one element) and requires gradient, the function additionally requires specifying gradient. It should be a tensor of matching type and location, that contains the gradient of the differentiated function w.r.t. self. This function accumulates gradients in the leaves - you might need to zero them before calling it.  Parameters  gradient (Tensor or None) \u2013 Gradient w.r.t. the tensor. If it is a tensor, it will be automatically converted to a Tensor that does not require grad unless create_graph is True. None values can be specified for scalar Tensors or ones that don\u2019t require grad. If a None value would be acceptable then this argument is optional. retain_graph (bool, optional) \u2013 If False, the graph used to compute the grads will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Defaults to the value of create_graph. create_graph (bool, optional) \u2013 If True, graph of the derivative will be constructed, allowing to compute higher order derivative products. Defaults to False.    ", "parameters": ["gradient (Tensor or None) : Gradient w.r.t. thetensor. If it is a tensor, it will be automatically convertedto a Tensor that does not require grad unless create_graph is True.None values can be specified for scalar Tensors or ones thatdon\u2019t require grad. If a None value would be acceptable thenthis argument is optional.", "retain_graph (bool, optional) : If False, the graph used to computethe grads will be freed. Note that in nearly all cases settingthis option to True is not needed and often can be worked aroundin a much more efficient way. Defaults to the value ofcreate_graph.", "create_graph (bool, optional) : If True, graph of the derivative willbe constructed, allowing to compute higher order derivativeproducts. Defaults to False."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributed.autograd.context", "item_type": "class", "code": "classtorch.distributed.autograd.context", "description": "Context object to wrap forward and backward passes when using distributed autograd. The context_id generated in the with statement  is required to uniquely identify a distributed backward pass on all workers. Each worker stores metadata associated with this context_id, which is required to correctly execute a distributed autograd pass. Example: &gt;&gt; import torch.distributed.autograd as dist_autograd &gt;&gt; with dist_autograd.context() as context_id: &gt;&gt;   t1 = torch.rand((3, 3), requires_grad=True) &gt;&gt;   t2 = torch.rand((3, 3), requires_grad=True) &gt;&gt;   loss = rpc.rpc_sync(\"worker1\", torch.add, args=(t1, t2)).sum() &gt;&gt;   dist_autograd.backward([loss])   ", "parameters": [], "returns": null, "example": " import torch.distributed.autograd as dist_autograd\n with dist_autograd.context() as context_id:\n   t1 = torch.rand((3, 3), requires_grad=True)\n   t2 = torch.rand((3, 3), requires_grad=True)\n   loss = rpc.rpc_sync(\"worker1\", torch.add, args=(t1, t2)).sum()\n   dist_autograd.backward([loss])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributed.optim.DistributedOptimizer", "item_type": "class", "code": "classtorch.distributed.optim.DistributedOptimizer(optimizer_class,params_rref,*args,**kwargs)", "description": "DistributedOptimizer takes remote references to parameters scattered across workers and applies the given optimizer locally for each parameter. This class uses get_gradients() in order to retrieve the gradients for specific parameters. Concurrent calls to step(), either from the same or different clients, will be serialized on each worker \u2013 as each worker\u2019s optimizer can only work on one set of gradients at a time. However, there is no guarantee that the full forward-backward-optimizer sequence will execute for one client at a time. This means that the gradients being applied may not correspond to the latest forward pass executed on a given worker. Also, there is no guaranteed ordering across workers.  Parameters  optimizer_class (optim.Optimizer) \u2013 the class of optimizer to instantiate on each worker. params_rref (list[RRef]) \u2013 list of RRefs to local or remote parameters to optimize. args \u2013 arguments to pass to the optimizer constructor on each worker. kwargs \u2013 arguments to pass to the optimizer constructor on each worker.    Example: &gt;&gt; import torch.distributed.autograd as dist_autograd &gt;&gt; import torch.distributed.rpc as rpc &gt;&gt; from torch import optim &gt;&gt; from torch.distributed.optim import DistributedOptimizer &gt;&gt; &gt;&gt; with dist_autograd.context() as context_id: &gt;&gt;   # Forward pass. &gt;&gt;   rref1 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 3)) &gt;&gt;   rref2 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 1)) &gt;&gt;   loss = rref1.to_here() + rref2.to_here() &gt;&gt; &gt;&gt;   # Backward pass. &gt;&gt;   dist_autograd.backward([loss.sum()]) &gt;&gt; &gt;&gt;   # Optimizer. &gt;&gt;   dist_optim = DistributedOptimizer( &gt;&gt;      optim.SGD, &gt;&gt;      [rref1, rref2], &gt;&gt;      lr=0.05, &gt;&gt;   ) &gt;&gt;   dist_optim.step()     step()  Performs a single optimization step. This will call torch.optim.Optimizer.step() on each worker containing parameters to be optimized, and will block until all workers return. The current distributed autograd context will be used globally.   ", "parameters": ["optimizer_class (optim.Optimizer) : the class of optimizer toinstantiate on each worker.", "params_rref (list[RRef]) : list of RRefs to local or remote parametersto optimize.", "args : arguments to pass to the optimizer constructor on each worker.", "kwargs : arguments to pass to the optimizer constructor on each worker."], "returns": null, "example": " import torch.distributed.autograd as dist_autograd\n import torch.distributed.rpc as rpc\n from torch import optim\n from torch.distributed.optim import DistributedOptimizer\n\n with dist_autograd.context() as context_id:\n   # Forward pass.\n   rref1 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 3))\n   rref2 = rpc.remote(\"worker1\", torch.add, args=(torch.ones(2), 1))\n   loss = rref1.to_here() + rref2.to_here()\n\n   # Backward pass.\n   dist_autograd.backward([loss.sum()])\n\n   # Optimizer.\n   dist_optim = DistributedOptimizer(\n      optim.SGD,\n      [rref1, rref2],\n      lr=0.05,\n   )\n   dist_optim.step()\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.quantized.functional.conv2d", "item_type": "function", "code": "torch.nn.quantized.functional.conv2d(input,weight,bias,stride=1,padding=0,dilation=1,groups=1,padding_mode='zeros',scale=1.0,zero_point=0,dtype=torch.quint8)", "description": "Applies a 2D convolution over a quantized 2D input composed of several input planes. See Conv2d for details and output shape.  Parameters  input \u2013 quantized input tensor of shape (minibatch,in_channels,iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iH , iW)(minibatch,in_channels,iH,iW)   weight \u2013 quantized filters of shape (out_channels,in_channelsgroups,kH,kW)(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kH , kW)(out_channels,groupsin_channels\u200b,kH,kW)   bias \u2013 non-quantized bias tensor of shape (out_channels)(\\text{out\\_channels})(out_channels)  . The tensor type must be torch.float. stride \u2013 the stride of the convolving kernel. Can be a single number or a tuple (sH, sW). Default: 1 padding \u2013 implicit paddings on both sides of the input. Can be a single number or a tuple (padH, padW). Default: 0 dilation \u2013 the spacing between kernel elements. Can be a single number or a tuple (dH, dW). Default: 1 groups \u2013 split input into groups, in_channels\\text{in\\_channels}in_channels   should be divisible by the number of groups. Default: 1 padding_mode \u2013 the padding mode to use. Only \u201czeros\u201d is supported for quantized convolution at the moment. Default: \u201czeros\u201d scale \u2013 quantization scale for the output. Default: 1.0 zero_point \u2013 quantization zero_point for the output. Default: 0 dtype \u2013 quantization data type to use. Default: torch.quint8    Examples: &gt;&gt;&gt; from torch.nn.quantized import functional as qF &gt;&gt;&gt; filters = torch.randn(8, 4, 3, 3, dtype=torch.float) &gt;&gt;&gt; inputs = torch.randn(1, 4, 5, 5, dtype=torch.float) &gt;&gt;&gt; bias = torch.randn(4, dtype=torch.float) &gt;&gt;&gt; &gt;&gt;&gt; scale, zero_point = 1.0, 0 &gt;&gt;&gt; dtype = torch.quint8 &gt;&gt;&gt; &gt;&gt;&gt; q_filters = torch.quantize_per_tensor(filters, scale, zero_point, dtype) &gt;&gt;&gt; q_inputs = torch.quantize_per_tensor(inputs, scale, zero_point, dtype) &gt;&gt;&gt; qF.conv2d(q_inputs, q_filters, bias, scale, zero_point, padding=1)   ", "parameters": ["input : quantized input tensor of shape (minibatch,in_channels,iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iH , iW)(minibatch,in_channels,iH,iW)", "weight : quantized filters of shape (out_channels,in_channelsgroups,kH,kW)(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kH , kW)(out_channels,groupsin_channels\u200b,kH,kW)", "bias : non-quantized bias tensor of shape (out_channels)(\\text{out\\_channels})(out_channels). The tensor type must be torch.float.", "stride : the stride of the convolving kernel. Can be a single number or atuple (sH, sW). Default: 1", "padding : implicit paddings on both sides of the input. Can be asingle number or a tuple (padH, padW). Default: 0", "dilation : the spacing between kernel elements. Can be a single number ora tuple (dH, dW). Default: 1", "groups : split input into groups, in_channels\\text{in\\_channels}in_channels should be divisible by thenumber of groups. Default: 1", "padding_mode : the padding mode to use. Only \u201czeros\u201d is supported for quantized convolution at the moment. Default: \u201czeros\u201d", "scale : quantization scale for the output. Default: 1.0", "zero_point : quantization zero_point for the output. Default: 0", "dtype : quantization data type to use. Default: torch.quint8"], "returns": null, "example": " from torch.nn.quantized import functional as qF\n filters = torch.randn(8, 4, 3, 3, dtype=torch.float)\n inputs = torch.randn(1, 4, 5, 5, dtype=torch.float)\n bias = torch.randn(4, dtype=torch.float)\n\n scale, zero_point = 1.0, 0\n dtype = torch.quint8\n\n q_filters = torch.quantize_per_tensor(filters, scale, zero_point, dtype)\n q_inputs = torch.quantize_per_tensor(inputs, scale, zero_point, dtype)\n qF.conv2d(q_inputs, q_filters, bias, scale, zero_point, padding=1)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.quantized.functional.conv3d", "item_type": "function", "code": "torch.nn.quantized.functional.conv3d(input,weight,bias,stride=1,padding=0,dilation=1,groups=1,padding_mode='zeros',scale=1.0,zero_point=0,dtype=torch.quint8)", "description": "Applies a 3D convolution over a quantized 3D input composed of several input planes. See Conv3d for details and output shape.  Parameters  input \u2013 quantized input tensor of shape (minibatch,in_channels,iD,iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iD , iH , iW)(minibatch,in_channels,iD,iH,iW)   weight \u2013 quantized filters of shape (out_channels,in_channelsgroups,kD,kH,kW)(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kD , kH , kW)(out_channels,groupsin_channels\u200b,kD,kH,kW)   bias \u2013 non-quantized bias tensor of shape (out_channels)(\\text{out\\_channels})(out_channels)  . The tensor type must be torch.float. stride \u2013 the stride of the convolving kernel. Can be a single number or a tuple (sD, sH, sW). Default: 1 padding \u2013 implicit paddings on both sides of the input. Can be a single number or a tuple (padD, padH, padW). Default: 0 dilation \u2013 the spacing between kernel elements. Can be a single number or a tuple (dD, dH, dW). Default: 1 groups \u2013 split input into groups, in_channels\\text{in\\_channels}in_channels   should be divisible by the number of groups. Default: 1 padding_mode \u2013 the padding mode to use. Only \u201czeros\u201d is supported for quantized convolution at the moment. Default: \u201czeros\u201d scale \u2013 quantization scale for the output. Default: 1.0 zero_point \u2013 quantization zero_point for the output. Default: 0 dtype \u2013 quantization data type to use. Default: torch.quint8    Examples: &gt;&gt;&gt; from torch.nn.quantized import functional as qF &gt;&gt;&gt; filters = torch.randn(8, 4, 3, 3, 3, dtype=torch.float) &gt;&gt;&gt; inputs = torch.randn(1, 4, 5, 5, 5, dtype=torch.float) &gt;&gt;&gt; bias = torch.randn(4, dtype=torch.float) &gt;&gt;&gt; &gt;&gt;&gt; scale, zero_point = 1.0, 0 &gt;&gt;&gt; dtype = torch.quint8 &gt;&gt;&gt; &gt;&gt;&gt; q_filters = torch.quantize_per_tensor(filters, scale, zero_point, dtype) &gt;&gt;&gt; q_inputs = torch.quantize_per_tensor(inputs, scale, zero_point, dtype) &gt;&gt;&gt; qF.conv3d(q_inputs, q_filters, bias, scale, zero_point, padding=1)   ", "parameters": ["input : quantized input tensor of shape(minibatch,in_channels,iD,iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iD , iH , iW)(minibatch,in_channels,iD,iH,iW)", "weight : quantized filters of shape(out_channels,in_channelsgroups,kD,kH,kW)(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kD , kH , kW)(out_channels,groupsin_channels\u200b,kD,kH,kW)", "bias : non-quantized bias tensor of shape(out_channels)(\\text{out\\_channels})(out_channels). The tensor type must be torch.float.", "stride : the stride of the convolving kernel. Can be a single number or atuple (sD, sH, sW). Default: 1", "padding : implicit paddings on both sides of the input. Can be asingle number or a tuple (padD, padH, padW). Default: 0", "dilation : the spacing between kernel elements. Can be a single number ora tuple (dD, dH, dW). Default: 1", "groups : split input into groups, in_channels\\text{in\\_channels}in_channels should bedivisible by the number of groups. Default: 1", "padding_mode : the padding mode to use. Only \u201czeros\u201d is supported forquantized convolution at the moment. Default: \u201czeros\u201d", "scale : quantization scale for the output. Default: 1.0", "zero_point : quantization zero_point for the output. Default: 0", "dtype : quantization data type to use. Default: torch.quint8"], "returns": null, "example": " from torch.nn.quantized import functional as qF\n filters = torch.randn(8, 4, 3, 3, 3, dtype=torch.float)\n inputs = torch.randn(1, 4, 5, 5, 5, dtype=torch.float)\n bias = torch.randn(4, dtype=torch.float)\n\n scale, zero_point = 1.0, 0\n dtype = torch.quint8\n\n q_filters = torch.quantize_per_tensor(filters, scale, zero_point, dtype)\n q_inputs = torch.quantize_per_tensor(inputs, scale, zero_point, dtype)\n qF.conv3d(q_inputs, q_filters, bias, scale, zero_point, padding=1)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.optim.lr_scheduler.LambdaLR.state_dict", "item_type": "method", "code": "state_dict()", "description": "Returns the state of the scheduler as a dict. It contains an entry for every variable in self.__dict__ which is not the optimizer. The learning rate lambda functions will only be saved if they are callable objects and not if they are functions or lambdas. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.optim.lr_scheduler.MultiplicativeLR.load_state_dict", "item_type": "method", "code": "load_state_dict(state_dict)", "description": "Loads the schedulers state.  Parameters state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict().   ", "parameters": ["state_dict (dict) : scheduler state. Should be an object returnedfrom a call to state_dict()."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.optim.lr_scheduler.MultiplicativeLR.state_dict", "item_type": "method", "code": "state_dict()", "description": "Returns the state of the scheduler as a dict. It contains an entry for every variable in self.__dict__ which is not the optimizer. The learning rate lambda functions will only be saved if they are callable objects and not if they are functions or lambdas. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.optim.lr_scheduler.CyclicLR.get_lr", "item_type": "method", "code": "get_lr()", "description": "Calculates the learning rate at batch index. This function treats self.last_epoch as the last batch index. If self.cycle_momentum is True, this function has a side effect of updating the optimizer\u2019s momentum. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.step", "item_type": "method", "code": "step(epoch=None)", "description": "Step could be called after every batch update Example &gt;&gt;&gt; scheduler = CosineAnnealingWarmRestarts(optimizer, T_0, T_mult) &gt;&gt;&gt; iters = len(dataloader) &gt;&gt;&gt; for epoch in range(20): &gt;&gt;&gt;     for i, sample in enumerate(dataloader): &gt;&gt;&gt;         inputs, labels = sample['inputs'], sample['labels'] &gt;&gt;&gt;         scheduler.step(epoch + i / iters) &gt;&gt;&gt;         optimizer.zero_grad() &gt;&gt;&gt;         outputs = net(inputs) &gt;&gt;&gt;         loss = criterion(outputs, labels) &gt;&gt;&gt;         loss.backward() &gt;&gt;&gt;         optimizer.step()   This function can be called in an interleaved way. Example &gt;&gt;&gt; scheduler = CosineAnnealingWarmRestarts(optimizer, T_0, T_mult) &gt;&gt;&gt; for epoch in range(20): &gt;&gt;&gt;     scheduler.step() &gt;&gt;&gt; scheduler.step(26) &gt;&gt;&gt; scheduler.step() # scheduler.step(27), instead of scheduler(20)   ", "parameters": [], "returns": null, "example": " scheduler = CosineAnnealingWarmRestarts(optimizer, T_0, T_mult)\n iters = len(dataloader)\n for epoch in range(20):\n     for i, sample in enumerate(dataloader):\n         inputs, labels = sample['inputs'], sample['labels']\n         scheduler.step(epoch + i / iters)\n         optimizer.zero_grad()\n         outputs = net(inputs)\n         loss = criterion(outputs, labels)\n         loss.backward()\n         optimizer.step()\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.optim.Optimizer", "item_type": "class", "code": "classtorch.optim.Optimizer(params,defaults)", "description": "Base class for all optimizers.  Warning Parameters need to be specified as collections that have a deterministic ordering that is consistent between runs. Examples of objects that don\u2019t satisfy those properties are sets and iterators over values of dictionaries.   Parameters  params (iterable) \u2013 an iterable of torch.Tensor s or dict s. Specifies what Tensors should be optimized. defaults \u2013 (dict): a dict containing default values of optimization options (used when a parameter group doesn\u2019t specify them).      add_param_group(param_group)  Add a param group to the Optimizer s param_groups. This can be useful when fine tuning a pre-trained network as frozen layers can be made trainable and added to the Optimizer as training progresses.  Parameters  param_group (dict) \u2013 Specifies what Tensors should be optimized along with group optimization options. (specific) \u2013         load_state_dict(state_dict)  Loads the optimizer state.  Parameters state_dict (dict) \u2013 optimizer state. Should be an object returned from a call to state_dict().       state_dict()  Returns the state of the optimizer as a dict. It contains two entries:   state - a dict holding current optimization state. Its contentdiffers between optimizer classes.    param_groups - a dict containing all parameter groups      step(closure)  Performs a single optimization step (parameter update).  Parameters closure (callable) \u2013 A closure that reevaluates the model and returns the loss. Optional for most optimizers.       zero_grad()  Clears the gradients of all optimized torch.Tensor s.   ", "parameters": ["params (iterable) : an iterable of torch.Tensor s ordict s. Specifies what Tensors should be optimized.", "defaults : (dict): a dict containing default values of optimizationoptions (used when a parameter group doesn\u2019t specify them).", "param_group (dict) : Specifies what Tensors should be optimized along with group", "optimization options. (specific) : "], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.optim.Adadelta", "item_type": "class", "code": "classtorch.optim.Adadelta(params,lr=1.0,rho=0.9,eps=1e-06,weight_decay=0)", "description": "Implements Adadelta algorithm. It has been proposed in ADADELTA: An Adaptive Learning Rate Method.  Parameters  params (iterable) \u2013 iterable of parameters to optimize or dicts defining parameter groups rho (python:float, optional) \u2013 coefficient used for computing a running average of squared gradients (default: 0.9) eps (python:float, optional) \u2013 term added to the denominator to improve numerical stability (default: 1e-6) lr (python:float, optional) \u2013 coefficient that scale delta before it is applied to the parameters (default: 1.0) weight_decay (python:float, optional) \u2013 weight decay (L2 penalty) (default: 0)      step(closure=None)  Performs a single optimization step.  Parameters closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.     ", "parameters": ["params (iterable) : iterable of parameters to optimize or dicts definingparameter groups", "rho (python:float, optional) : coefficient used for computing a running averageof squared gradients (default: 0.9)", "eps (python:float, optional) : term added to the denominator to improvenumerical stability (default: 1e-6)", "lr (python:float, optional) : coefficient that scale delta before it is appliedto the parameters (default: 1.0)", "weight_decay (python:float, optional) : weight decay (L2 penalty) (default: 0)"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.datasets.SBU", "item_type": "class", "code": "classtorchvision.datasets.SBU(root,transform=None,target_transform=None,download=True)", "description": "SBU Captioned Photo Dataset.  Parameters  root (string) \u2013 Root directory of dataset where tarball SBUCaptionedPhotoDataset.tar.gz exists. transform (callable, optional) \u2013 A function/transform that takes in a PIL image and returns a transformed version. E.g, transforms.RandomCrop target_transform (callable, optional) \u2013 A function/transform that takes in the target and transforms it. download (bool, optional) \u2013 If True, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.      __getitem__(index)   Parameters index (python:int) \u2013 Index  Returns (image, target) where target is a caption for the photo.  Return type tuple     ", "parameters": ["root (string) : Root directory of dataset where tarballSBUCaptionedPhotoDataset.tar.gz exists.", "transform (callable, optional) : A function/transform that takes in a PIL imageand returns a transformed version. E.g, transforms.RandomCrop", "target_transform (callable, optional) : A function/transform that takes in thetarget and transforms it.", "download (bool, optional) : If True, downloads the dataset from the internet andputs it in root directory. If dataset is already downloaded, it is notdownloaded again."], "returns": "(image, target) where target is a caption for the photo.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.datasets.Flickr8k", "item_type": "class", "code": "classtorchvision.datasets.Flickr8k(root,ann_file,transform=None,target_transform=None)", "description": "Flickr8k Entities Dataset.  Parameters  root (string) \u2013 Root directory where images are downloaded to. ann_file (string) \u2013 Path to annotation file. transform (callable, optional) \u2013 A function/transform that takes in a PIL image and returns a transformed version. E.g, transforms.ToTensor target_transform (callable, optional) \u2013 A function/transform that takes in the target and transforms it.      __getitem__(index)   Parameters index (python:int) \u2013 Index  Returns Tuple (image, target). target is a list of captions for the image.  Return type tuple     ", "parameters": ["root (string) : Root directory where images are downloaded to.", "ann_file (string) : Path to annotation file.", "transform (callable, optional) : A function/transform that takes in a PIL imageand returns a transformed version. E.g, transforms.ToTensor", "target_transform (callable, optional) : A function/transform that takes in thetarget and transforms it."], "returns": "Tuple (image, target). target is a list of captions for the image.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.datasets.Flickr30k", "item_type": "class", "code": "classtorchvision.datasets.Flickr30k(root,ann_file,transform=None,target_transform=None)", "description": "Flickr30k Entities Dataset.  Parameters  root (string) \u2013 Root directory where images are downloaded to. ann_file (string) \u2013 Path to annotation file. transform (callable, optional) \u2013 A function/transform that takes in a PIL image and returns a transformed version. E.g, transforms.ToTensor target_transform (callable, optional) \u2013 A function/transform that takes in the target and transforms it.      __getitem__(index)   Parameters index (python:int) \u2013 Index  Returns Tuple (image, target). target is a list of captions for the image.  Return type tuple     ", "parameters": ["root (string) : Root directory where images are downloaded to.", "ann_file (string) : Path to annotation file.", "transform (callable, optional) : A function/transform that takes in a PIL imageand returns a transformed version. E.g, transforms.ToTensor", "target_transform (callable, optional) : A function/transform that takes in thetarget and transforms it."], "returns": "Tuple (image, target). target is a list of captions for the image.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.datasets.VOCSegmentation", "item_type": "class", "code": "classtorchvision.datasets.VOCSegmentation(root,year='2012',image_set='train',download=False,transform=None,target_transform=None,transforms=None)", "description": "Pascal VOC Segmentation Dataset.  Parameters  root (string) \u2013 Root directory of the VOC Dataset. year (string, optional) \u2013 The dataset year, supports years 2007 to 2012. image_set (string, optional) \u2013 Select the image_set to use, train, trainval or val download (bool, optional) \u2013 If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again. transform (callable, optional) \u2013 A function/transform that  takes in an PIL image and returns a transformed version. E.g, transforms.RandomCrop target_transform (callable, optional) \u2013 A function/transform that takes in the target and transforms it. transforms (callable, optional) \u2013 A function/transform that takes input sample and its target as entry and returns a transformed version.      __getitem__(index)   Parameters index (python:int) \u2013 Index  Returns (image, target) where target is the image segmentation.  Return type tuple     ", "parameters": ["root (string) : Root directory of the VOC Dataset.", "year (string, optional) : The dataset year, supports years 2007 to 2012.", "image_set (string, optional) : Select the image_set to use, train, trainval or val", "download (bool, optional) : If true, downloads the dataset from the internet andputs it in root directory. If dataset is already downloaded, it is notdownloaded again.", "transform (callable, optional) : A function/transform that  takes in an PIL imageand returns a transformed version. E.g, transforms.RandomCrop", "target_transform (callable, optional) : A function/transform that takes in thetarget and transforms it.", "transforms (callable, optional) : A function/transform that takes input sample and its target as entryand returns a transformed version."], "returns": "(image, target) where target is the image segmentation.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.datasets.VOCDetection", "item_type": "class", "code": "classtorchvision.datasets.VOCDetection(root,year='2012',image_set='train',download=False,transform=None,target_transform=None,transforms=None)", "description": "Pascal VOC Detection Dataset.  Parameters  root (string) \u2013 Root directory of the VOC Dataset. year (string, optional) \u2013 The dataset year, supports years 2007 to 2012. image_set (string, optional) \u2013 Select the image_set to use, train, trainval or val download (bool, optional) \u2013 If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again. (default: alphabetic indexing of VOC\u2019s 20 classes). transform (callable, optional) \u2013 A function/transform that  takes in an PIL image and returns a transformed version. E.g, transforms.RandomCrop target_transform (callable, required) \u2013 A function/transform that takes in the target and transforms it. transforms (callable, optional) \u2013 A function/transform that takes input sample and its target as entry and returns a transformed version.      __getitem__(index)   Parameters index (python:int) \u2013 Index  Returns (image, target) where target is a dictionary of the XML tree.  Return type tuple     ", "parameters": ["root (string) : Root directory of the VOC Dataset.", "year (string, optional) : The dataset year, supports years 2007 to 2012.", "image_set (string, optional) : Select the image_set to use, train, trainval or val", "download (bool, optional) : If true, downloads the dataset from the internet andputs it in root directory. If dataset is already downloaded, it is notdownloaded again.(default: alphabetic indexing of VOC\u2019s 20 classes).", "transform (callable, optional) : A function/transform that  takes in an PIL imageand returns a transformed version. E.g, transforms.RandomCrop", "target_transform (callable, required) : A function/transform that takes in thetarget and transforms it.", "transforms (callable, optional) : A function/transform that takes input sample and its target as entryand returns a transformed version."], "returns": "(image, target) where target is a dictionary of the XML tree.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "bernoulli_(p_tensor,*,generator=None)\u2192Tensor", "description": "p_tensor should be a tensor containing probabilities to be used for drawing the binary random number. The ith\\text{i}^{th}ith   element of self tensor will be set to a value sampled from Bernoulli(p_tensor[i])\\text{Bernoulli}(\\texttt{p\\_tensor[i]})Bernoulli(p_tensor[i])  . self can have integral dtype, but p_tensor must have floating point dtype. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "to(dtype,non_blocking=False,copy=False)\u2192Tensor", "description": "Returns a Tensor with the specified dtype ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "to(device=None,dtype=None,non_blocking=False,copy=False)\u2192Tensor", "description": "Returns a Tensor with the specified device and (optional) dtype. If dtype is None it is inferred to be self.dtype. When non_blocking, tries to convert asynchronously with respect to the host if possible, e.g., converting a CPU Tensor with pinned memory to a CUDA Tensor. When copy is set, a new Tensor is created even when the Tensor already matches the desired conversion. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "to(other,non_blocking=False,copy=False)\u2192Tensor", "description": "Returns a Tensor with same torch.dtype and torch.device as the Tensor other. When non_blocking, tries to convert asynchronously with respect to the host if possible, e.g., converting a CPU Tensor with pinned memory to a CUDA Tensor. When copy is set, a new Tensor is created even when the Tensor already matches the desired conversion. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "all()\u2192bool", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "all(dim,keepdim=False,out=None)\u2192Tensor", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "any()\u2192bool", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "any(dim,keepdim=False,out=None)\u2192Tensor", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.new_tensor", "item_type": "method", "code": "new_tensor(data,dtype=None,device=None,requires_grad=False)\u2192Tensor", "description": "Returns a new Tensor with data as the tensor data. By default, the returned Tensor has the same torch.dtype and torch.device as this tensor.  Warning new_tensor() always copies data. If you have a Tensor data and want to avoid a copy, use torch.Tensor.requires_grad_() or torch.Tensor.detach(). If you have a numpy array and want to avoid a copy, use torch.from_numpy().   Warning When data is a tensor x, new_tensor() reads out \u2018the data\u2019 from whatever it is passed, and constructs a leaf variable. Therefore tensor.new_tensor(x) is equivalent to x.clone().detach() and tensor.new_tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True). The equivalents using clone() and detach() are recommended.   Parameters  data (array_like) \u2013 The returned Tensor copies data. dtype (torch.dtype, optional) \u2013 the desired type of returned tensor. Default: if None, same torch.dtype as this tensor. device (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, same torch.device as this tensor. requires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: &gt;&gt;&gt; tensor = torch.ones((2,), dtype=torch.int8) &gt;&gt;&gt; data = [[0, 1], [2, 3]] &gt;&gt;&gt; tensor.new_tensor(data) tensor([[ 0,  1],         [ 2,  3]], dtype=torch.int8)   ", "parameters": ["data (array_like) : The returned Tensor copies data.", "dtype (torch.dtype, optional) : the desired type of returned tensor.Default: if None, same torch.dtype as this tensor.", "device (torch.device, optional) : the desired device of returned tensor.Default: if None, same torch.device as this tensor.", "requires_grad (bool, optional) : If autograd should record operations on thereturned tensor. Default: False."], "returns": null, "example": " tensor = torch.ones((2,), dtype=torch.int8)\n data = [[0, 1], [2, 3]]\n tensor.new_tensor(data)\ntensor([[ 0,  1],\n        [ 2,  3]], dtype=torch.int8)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.sparse.FloatTensor.transpose_", "item_type": "method", "code": "transpose_()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.sparse.FloatTensor.zero_", "item_type": "method", "code": "zero_()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.sparse.FloatTensor.coalesce", "item_type": "method", "code": "coalesce()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.sparse.FloatTensor.is_coalesced", "item_type": "method", "code": "is_coalesced()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.sparse.FloatTensor._indices", "item_type": "method", "code": "_indices()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.sparse.FloatTensor._values", "item_type": "method", "code": "_values()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.sparse.FloatTensor._nnz", "item_type": "method", "code": "_nnz()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.sparse.FloatTensor", "item_type": "class", "code": "classtorch.sparse.FloatTensor", "description": "  add()     add_()     clone()     dim()     div()     div_()     get_device()     hspmm()     mm()     mul()     mul_()     narrow_copy()     resizeAs_()     size()     spadd()     spmm()     sspaddmm()     sspmm()     sub()     sub_()     t_()     to_dense()     transpose()     transpose_()     zero_()     coalesce()     is_coalesced()     _indices()     _values()     _nnz()   ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.conv2d", "item_type": "function", "code": "torch.nn.functional.conv2d(input,weight,bias=None,stride=1,padding=0,dilation=1,groups=1)\u2192Tensor", "description": "Applies a 2D convolution over an input image composed of several input planes. See Conv2d for details and output shape.  Note In some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. Please see the notes on Reproducibility for background.   Parameters  input \u2013 input tensor of shape (minibatch,in_channels,iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iH , iW)(minibatch,in_channels,iH,iW)   weight \u2013 filters of shape (out_channels,in_channelsgroups,kH,kW)(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kH , kW)(out_channels,groupsin_channels\u200b,kH,kW)   bias \u2013 optional bias tensor of shape (out_channels)(\\text{out\\_channels})(out_channels)  . Default: None stride \u2013 the stride of the convolving kernel. Can be a single number or a tuple (sH, sW). Default: 1 padding \u2013 implicit paddings on both sides of the input. Can be a single number or a tuple (padH, padW). Default: 0 dilation \u2013 the spacing between kernel elements. Can be a single number or a tuple (dH, dW). Default: 1 groups \u2013 split input into groups, in_channels\\text{in\\_channels}in_channels   should be divisible by the number of groups. Default: 1    Examples: &gt;&gt;&gt; # With square kernels and equal stride &gt;&gt;&gt; filters = torch.randn(8,4,3,3) &gt;&gt;&gt; inputs = torch.randn(1,4,5,5) &gt;&gt;&gt; F.conv2d(inputs, filters, padding=1)   ", "parameters": ["input : input tensor of shape (minibatch,in_channels,iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iH , iW)(minibatch,in_channels,iH,iW)", "weight : filters of shape (out_channels,in_channelsgroups,kH,kW)(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kH , kW)(out_channels,groupsin_channels\u200b,kH,kW)", "bias : optional bias tensor of shape (out_channels)(\\text{out\\_channels})(out_channels). Default: None", "stride : the stride of the convolving kernel. Can be a single number or atuple (sH, sW). Default: 1", "padding : implicit paddings on both sides of the input. Can be asingle number or a tuple (padH, padW). Default: 0", "dilation : the spacing between kernel elements. Can be a single number ora tuple (dH, dW). Default: 1", "groups : split input into groups, in_channels\\text{in\\_channels}in_channels should be divisible by thenumber of groups. Default: 1"], "returns": null, "example": " # With square kernels and equal stride\n filters = torch.randn(8,4,3,3)\n inputs = torch.randn(1,4,5,5)\n F.conv2d(inputs, filters, padding=1)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.conv3d", "item_type": "function", "code": "torch.nn.functional.conv3d(input,weight,bias=None,stride=1,padding=0,dilation=1,groups=1)\u2192Tensor", "description": "Applies a 3D convolution over an input image composed of several input planes. See Conv3d for details and output shape.  Note In some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. Please see the notes on Reproducibility for background.   Parameters  input \u2013 input tensor of shape (minibatch,in_channels,iT,iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iT , iH , iW)(minibatch,in_channels,iT,iH,iW)   weight \u2013 filters of shape (out_channels,in_channelsgroups,kT,kH,kW)(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kT , kH , kW)(out_channels,groupsin_channels\u200b,kT,kH,kW)   bias \u2013 optional bias tensor of shape (out_channels)(\\text{out\\_channels})(out_channels)  . Default: None stride \u2013 the stride of the convolving kernel. Can be a single number or a tuple (sT, sH, sW). Default: 1 padding \u2013 implicit paddings on both sides of the input. Can be a single number or a tuple (padT, padH, padW). Default: 0 dilation \u2013 the spacing between kernel elements. Can be a single number or a tuple (dT, dH, dW). Default: 1 groups \u2013 split input into groups, in_channels\\text{in\\_channels}in_channels   should be divisible by the number of groups. Default: 1    Examples: &gt;&gt;&gt; filters = torch.randn(33, 16, 3, 3, 3) &gt;&gt;&gt; inputs = torch.randn(20, 16, 50, 10, 20) &gt;&gt;&gt; F.conv3d(inputs, filters)   ", "parameters": ["input : input tensor of shape (minibatch,in_channels,iT,iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iT , iH , iW)(minibatch,in_channels,iT,iH,iW)", "weight : filters of shape (out_channels,in_channelsgroups,kT,kH,kW)(\\text{out\\_channels} , \\frac{\\text{in\\_channels}}{\\text{groups}} , kT , kH , kW)(out_channels,groupsin_channels\u200b,kT,kH,kW)", "bias : optional bias tensor of shape (out_channels)(\\text{out\\_channels})(out_channels). Default: None", "stride : the stride of the convolving kernel. Can be a single number or atuple (sT, sH, sW). Default: 1", "padding : implicit paddings on both sides of the input. Can be asingle number or a tuple (padT, padH, padW). Default: 0", "dilation : the spacing between kernel elements. Can be a single number ora tuple (dT, dH, dW). Default: 1", "groups : split input into groups, in_channels\\text{in\\_channels}in_channels should be divisible bythe number of groups. Default: 1"], "returns": null, "example": " filters = torch.randn(33, 16, 3, 3, 3)\n inputs = torch.randn(20, 16, 50, 10, 20)\n F.conv3d(inputs, filters)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.conv_transpose1d", "item_type": "function", "code": "torch.nn.functional.conv_transpose1d(input,weight,bias=None,stride=1,padding=0,output_padding=0,groups=1,dilation=1)\u2192Tensor", "description": "Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d. See ConvTranspose1d for details and output shape.  Note In some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. Please see the notes on Reproducibility for background.   Parameters  input \u2013 input tensor of shape (minibatch,in_channels,iW)(\\text{minibatch} , \\text{in\\_channels} , iW)(minibatch,in_channels,iW)   weight \u2013 filters of shape (in_channels,out_channelsgroups,kW)(\\text{in\\_channels} , \\frac{\\text{out\\_channels}}{\\text{groups}} , kW)(in_channels,groupsout_channels\u200b,kW)   bias \u2013 optional bias of shape (out_channels)(\\text{out\\_channels})(out_channels)  . Default: None stride \u2013 the stride of the convolving kernel. Can be a single number or a tuple (sW,). Default: 1 padding \u2013 dilation * (kernel_size - 1) - padding zero-padding will be added to both sides of each dimension in the input. Can be a single number or a tuple (padW,). Default: 0 output_padding \u2013 additional size added to one side of each dimension in the output shape. Can be a single number or a tuple (out_padW). Default: 0 groups \u2013 split input into groups, in_channels\\text{in\\_channels}in_channels   should be divisible by the number of groups. Default: 1 dilation \u2013 the spacing between kernel elements. Can be a single number or a tuple (dW,). Default: 1    Examples: &gt;&gt;&gt; inputs = torch.randn(20, 16, 50) &gt;&gt;&gt; weights = torch.randn(16, 33, 5) &gt;&gt;&gt; F.conv_transpose1d(inputs, weights)   ", "parameters": ["input : input tensor of shape (minibatch,in_channels,iW)(\\text{minibatch} , \\text{in\\_channels} , iW)(minibatch,in_channels,iW)", "weight : filters of shape (in_channels,out_channelsgroups,kW)(\\text{in\\_channels} , \\frac{\\text{out\\_channels}}{\\text{groups}} , kW)(in_channels,groupsout_channels\u200b,kW)", "bias : optional bias of shape (out_channels)(\\text{out\\_channels})(out_channels). Default: None", "stride : the stride of the convolving kernel. Can be a single number or atuple (sW,). Default: 1", "padding : dilation * (kernel_size - 1) - padding zero-padding will be added to bothsides of each dimension in the input. Can be a single number or a tuple(padW,). Default: 0", "output_padding : additional size added to one side of each dimension in theoutput shape. Can be a single number or a tuple (out_padW). Default: 0", "groups : split input into groups, in_channels\\text{in\\_channels}in_channels should be divisible by thenumber of groups. Default: 1", "dilation : the spacing between kernel elements. Can be a single number ora tuple (dW,). Default: 1"], "returns": null, "example": " inputs = torch.randn(20, 16, 50)\n weights = torch.randn(16, 33, 5)\n F.conv_transpose1d(inputs, weights)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.models.detection.keypointrcnn_resnet50_fpn", "item_type": "function", "code": "torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=False,progress=True,num_classes=2,num_keypoints=17,pretrained_backbone=True,**kwargs)", "description": "Constructs a Keypoint R-CNN model with a ResNet-50-FPN backbone. The input to the model is expected to be a list of tensors, each of shape [C, H, W], one for each image, and should be in 0-1 range. Different images can have different sizes. The behavior of the model changes depending if it is in training or evaluation mode. During training, the model expects both the input tensors, as well as a targets (list of dictionary), containing:   boxes (FloatTensor[N, 4]): the ground-truth boxes in [x1, y1, x2, y2] format, with values between 0 and H and 0 and W labels (Int64Tensor[N]): the class label for each ground-truth box keypoints (FloatTensor[N, K, 3]): the K keypoints location for each of the N instances, in the format [x, y, visibility], where visibility=0 means that the keypoint is not visible.   The model returns a Dict[Tensor] during training, containing the classification and regression losses for both the RPN and the R-CNN, and the keypoint loss. During inference, the model requires only the input tensors, and returns the post-processed predictions as a List[Dict[Tensor]], one for each input image. The fields of the Dict are as follows:   boxes (FloatTensor[N, 4]): the predicted boxes in [x1, y1, x2, y2] format, with values between 0 and H and 0 and W labels (Int64Tensor[N]): the predicted labels for each image scores (Tensor[N]): the scores or each prediction keypoints (FloatTensor[N, K, 3]): the locations of the predicted keypoints, in [x, y, v] format.   Example: &gt;&gt;&gt; model = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=True) &gt;&gt;&gt; model.eval() &gt;&gt;&gt; x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)] &gt;&gt;&gt; predictions = model(x)    Parameters  pretrained (bool) \u2013 If True, returns a model pre-trained on COCO train2017 progress (bool) \u2013 If True, displays a progress bar of the download to stderr    ", "parameters": ["pretrained (bool) : If True, returns a model pre-trained on COCO train2017", "progress (bool) : If True, displays a progress bar of the download to stderr"], "returns": null, "example": " model = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=True)\n model.eval()\n x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n predictions = model(x)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.models.video.r3d_18", "item_type": "function", "code": "torchvision.models.video.r3d_18(pretrained=False,progress=True,**kwargs)", "description": "Construct 18 layer Resnet3D model as in https://arxiv.org/abs/1711.11248  Parameters  pretrained (bool) \u2013 If True, returns a model pre-trained on Kinetics-400 progress (bool) \u2013 If True, displays a progress bar of the download to stderr   Returns R3D-18 network  Return type nn.Module   ", "parameters": ["pretrained (bool) : If True, returns a model pre-trained on Kinetics-400", "progress (bool) : If True, displays a progress bar of the download to stderr"], "returns": "R3D-18 network", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.models.video.mc3_18", "item_type": "function", "code": "torchvision.models.video.mc3_18(pretrained=False,progress=True,**kwargs)", "description": "Constructor for 18 layer Mixed Convolution network as in https://arxiv.org/abs/1711.11248  Parameters  pretrained (bool) \u2013 If True, returns a model pre-trained on Kinetics-400 progress (bool) \u2013 If True, displays a progress bar of the download to stderr   Returns MC3 Network definition  Return type nn.Module   ", "parameters": ["pretrained (bool) : If True, returns a model pre-trained on Kinetics-400", "progress (bool) : If True, displays a progress bar of the download to stderr"], "returns": "MC3 Network definition", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.models.video.r2plus1d_18", "item_type": "function", "code": "torchvision.models.video.r2plus1d_18(pretrained=False,progress=True,**kwargs)", "description": "Constructor for the 18 layer deep R(2+1)D network as in https://arxiv.org/abs/1711.11248  Parameters  pretrained (bool) \u2013 If True, returns a model pre-trained on Kinetics-400 progress (bool) \u2013 If True, displays a progress bar of the download to stderr   Returns R(2+1)D-18 network  Return type nn.Module   ", "parameters": ["pretrained (bool) : If True, returns a model pre-trained on Kinetics-400", "progress (bool) : If True, displays a progress bar of the download to stderr"], "returns": "R(2+1)D-18 network", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.FiveCrop", "item_type": "class", "code": "classtorchvision.transforms.FiveCrop(size)", "description": "Crop the given PIL Image into four corners and the central crop  Note This transform returns a tuple of images and there may be a mismatch in the number of inputs and targets your Dataset returns. See below for an example of how to deal with this.   Parameters size (sequence or python:int) \u2013 Desired output size of the crop. If size is an int instead of sequence like (h, w), a square crop of size (size, size) is made.   Example &gt;&gt;&gt; transform = Compose([ &gt;&gt;&gt;    FiveCrop(size), # this is a list of PIL Images &gt;&gt;&gt;    Lambda(lambda crops: torch.stack([ToTensor()(crop) for crop in crops])) # returns a 4D tensor &gt;&gt;&gt; ]) &gt;&gt;&gt; #In your test loop you can do the following: &gt;&gt;&gt; input, target = batch # input is a 5d tensor, target is 2d &gt;&gt;&gt; bs, ncrops, c, h, w = input.size() &gt;&gt;&gt; result = model(input.view(-1, c, h, w)) # fuse batch size and ncrops &gt;&gt;&gt; result_avg = result.view(bs, ncrops, -1).mean(1) # avg over crops   ", "parameters": ["size (sequence or python:int) : Desired output size of the crop. If size is an intinstead of sequence like (h, w), a square crop of size (size, size) is made."], "returns": null, "example": " transform = Compose([\n    FiveCrop(size), # this is a list of PIL Images\n    Lambda(lambda crops: torch.stack([ToTensor()(crop) for crop in crops])) # returns a 4D tensor\n ])\n #In your test loop you can do the following:\n input, target = batch # input is a 5d tensor, target is 2d\n bs, ncrops, c, h, w = input.size()\n result = model(input.view(-1, c, h, w)) # fuse batch size and ncrops\n result_avg = result.view(bs, ncrops, -1).mean(1) # avg over crops\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.Grayscale", "item_type": "class", "code": "classtorchvision.transforms.Grayscale(num_output_channels=1)", "description": "Convert image to grayscale.  Parameters num_output_channels (python:int) \u2013 (1 or 3) number of channels desired for output image  Returns Grayscale version of the input. - If num_output_channels == 1 : returned image is single channel - If num_output_channels == 3 : returned image is 3 channel with r == g == b  Return type PIL Image   ", "parameters": ["num_output_channels (python:int) : (1 or 3) number of channels desired for output image", "Grayscale version of the input.- If num_output_channels == 1 : returned image is single channel- If num_output_channels == 3 : returned image is 3 channel with r == g == b", "PIL Image"], "returns": "Grayscale version of the input.- If num_output_channels == 1 : returned image is single channel- If num_output_channels == 3 : returned image is 3 channel with r == g == b", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.Pad", "item_type": "class", "code": "classtorchvision.transforms.Pad(padding,fill=0,padding_mode='constant')", "description": "Pad the given PIL Image on all sides with the given \u201cpad\u201d value.  Parameters  padding (python:int or tuple) \u2013 Padding on each border. If a single int is provided this is used to pad all borders. If tuple of length 2 is provided this is the padding on left/right and top/bottom respectively. If a tuple of length 4 is provided this is the padding for the left, top, right and bottom borders respectively. fill (python:int or tuple) \u2013 Pixel fill value for constant fill. Default is 0. If a tuple of length 3, it is used to fill R, G, B channels respectively. This value is only used when the padding_mode is constant padding_mode (str) \u2013 Type of padding. Should be: constant, edge, reflect or symmetric. Default is constant.  constant: pads with a constant value, this value is specified with fill edge: pads with the last value at the edge of the image reflect: pads with reflection of image without repeating the last value on the edge  For example, padding [1, 2, 3, 4] with 2 elements on both sides in reflect mode will result in [3, 2, 1, 2, 3, 4, 3, 2]   symmetric: pads with reflection of image repeating the last value on the edge  For example, padding [1, 2, 3, 4] with 2 elements on both sides in symmetric mode will result in [2, 1, 1, 2, 3, 4, 4, 3]        ", "parameters": ["padding (python:int or tuple) : Padding on each border. If a single int is provided thisis used to pad all borders. If tuple of length 2 is provided this is the paddingon left/right and top/bottom respectively. If a tuple of length 4 is providedthis is the padding for the left, top, right and bottom bordersrespectively.", "fill (python:int or tuple) : Pixel fill value for constant fill. Default is 0. If a tuple oflength 3, it is used to fill R, G, B channels respectively.This value is only used when the padding_mode is constant", "padding_mode (str) : Type of padding. Should be: constant, edge, reflect or symmetric.Default is constant.constant: pads with a constant value, this value is specified with filledge: pads with the last value at the edge of the imagereflect: pads with reflection of image without repeating the last value on the edgeFor example, padding [1, 2, 3, 4] with 2 elements on both sides in reflect modewill result in [3, 2, 1, 2, 3, 4, 3, 2]symmetric: pads with reflection of image repeating the last value on the edgeFor example, padding [1, 2, 3, 4] with 2 elements on both sides in symmetric modewill result in [2, 1, 1, 2, 3, 4, 4, 3]", "constant: pads with a constant value, this value is specified with fill", "edge: pads with the last value at the edge of the image", "reflect: pads with reflection of image without repeating the last value on the edgeFor example, padding [1, 2, 3, 4] with 2 elements on both sides in reflect modewill result in [3, 2, 1, 2, 3, 4, 3, 2]", "symmetric: pads with reflection of image repeating the last value on the edgeFor example, padding [1, 2, 3, 4] with 2 elements on both sides in symmetric modewill result in [2, 1, 1, 2, 3, 4, 4, 3]"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.distribution.Distribution.icdf", "item_type": "method", "code": "icdf(value)", "description": "Returns the inverse cumulative density/mass function evaluated at value.  Parameters value (Tensor) \u2013    ", "parameters": ["value (Tensor) : "], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.distribution.Distribution.log_prob", "item_type": "method", "code": "log_prob(value)", "description": "Returns the log of the probability density/mass function evaluated at value.  Parameters value (Tensor) \u2013    ", "parameters": ["value (Tensor) : "], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.distribution.Distribution.mean", "item_type": "method", "code": "propertymean", "description": "Returns the mean of the distribution. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.distribution.Distribution.perplexity", "item_type": "method", "code": "perplexity()", "description": "Returns perplexity of distribution, batched over batch_shape.  Returns Tensor of shape batch_shape.   ", "parameters": [], "returns": "Tensor of shape batch_shape.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.distribution.Distribution.rsample", "item_type": "method", "code": "rsample(sample_shape=torch.Size([]))", "description": "Generates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.distribution.Distribution.sample", "item_type": "method", "code": "sample(sample_shape=torch.Size([]))", "description": "Generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.distribution.Distribution.sample_n", "item_type": "method", "code": "sample_n(n)", "description": "Generates n samples or n batches of samples if the distribution parameters are batched. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.distribution.Distribution.stddev", "item_type": "method", "code": "propertystddev", "description": "Returns the standard deviation of the distribution. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.distribution.Distribution.support", "item_type": "method", "code": "propertysupport", "description": "Returns a Constraint object representing this distribution\u2019s support. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.distribution.Distribution.variance", "item_type": "method", "code": "propertyvariance", "description": "Returns the variance of the distribution. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.exp_family.ExponentialFamily.entropy", "item_type": "method", "code": "entropy()", "description": "Method to compute the entropy using Bregman divergence of the log normalizer. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributed.isend", "item_type": "function", "code": "torch.distributed.isend(tensor,dst,group=&lt;objectobject&gt;,tag=0)", "description": "Sends a tensor asynchronously.  Parameters  tensor (Tensor) \u2013 Tensor to send. dst (python:int) \u2013 Destination rank. group (ProcessGroup, optional) \u2013 The process group to work on tag (python:int, optional) \u2013 Tag to match send with remote recv   Returns A distributed request object. None, if not part of the group   ", "parameters": ["tensor (Tensor) : Tensor to send.", "dst (python:int) : Destination rank.", "group (ProcessGroup, optional) : The process group to work on", "tag (python:int, optional) : Tag to match send with remote recv"], "returns": "A distributed request object.None, if not part of the group", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributed.irecv", "item_type": "function", "code": "torch.distributed.irecv(tensor,src,group=&lt;objectobject&gt;,tag=0)", "description": "Receives a tensor asynchronously.  Parameters  tensor (Tensor) \u2013 Tensor to fill with received data. src (python:int) \u2013 Source rank. group (ProcessGroup, optional) \u2013 The process group to work on tag (python:int, optional) \u2013 Tag to match recv with remote send   Returns A distributed request object. None, if not part of the group   ", "parameters": ["tensor (Tensor) : Tensor to fill with received data.", "src (python:int) : Source rank.", "group (ProcessGroup, optional) : The process group to work on", "tag (python:int, optional) : Tag to match recv with remote send"], "returns": "A distributed request object.None, if not part of the group", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributed.broadcast", "item_type": "function", "code": "torch.distributed.broadcast(tensor,src,group=&lt;objectobject&gt;,async_op=False)", "description": "Broadcasts the tensor to the whole group. tensor must have the same number of elements in all processes participating in the collective.  Parameters  tensor (Tensor) \u2013 Data to be sent if src is the rank of current process, and tensor to be used to save received data otherwise. src (python:int) \u2013 Source rank. group (ProcessGroup, optional) \u2013 The process group to work on async_op (bool, optional) \u2013 Whether this op should be an async op   Returns Async work handle, if async_op is set to True. None, if not async_op or if not part of the group   ", "parameters": ["tensor (Tensor) : Data to be sent if src is the rank of currentprocess, and tensor to be used to save received data otherwise.", "src (python:int) : Source rank.", "group (ProcessGroup, optional) : The process group to work on", "async_op (bool, optional) : Whether this op should be an async op"], "returns": "Async work handle, if async_op is set to True.None, if not async_op or if not part of the group", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributed.all_reduce", "item_type": "function", "code": "torch.distributed.all_reduce(tensor,op=ReduceOp.SUM,group=&lt;objectobject&gt;,async_op=False)", "description": "Reduces the tensor data across all machines in such a way that all get the final result. After the call tensor is going to be bitwise identical in all processes.  Parameters  tensor (Tensor) \u2013 Input and output of the collective. The function operates in-place. op (optional) \u2013 One of the values from torch.distributed.ReduceOp enum.  Specifies an operation used for element-wise reductions. group (ProcessGroup, optional) \u2013 The process group to work on async_op (bool, optional) \u2013 Whether this op should be an async op   Returns Async work handle, if async_op is set to True. None, if not async_op or if not part of the group   ", "parameters": ["tensor (Tensor) : Input and output of the collective. The functionoperates in-place.", "op (optional) : One of the values fromtorch.distributed.ReduceOpenum.  Specifies an operation used for element-wise reductions.", "group (ProcessGroup, optional) : The process group to work on", "async_op (bool, optional) : Whether this op should be an async op"], "returns": "Async work handle, if async_op is set to True.None, if not async_op or if not part of the group", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributed.reduce", "item_type": "function", "code": "torch.distributed.reduce(tensor,dst,op=ReduceOp.SUM,group=&lt;objectobject&gt;,async_op=False)", "description": "Reduces the tensor data across all machines. Only the process with rank dst is going to receive the final result.  Parameters  tensor (Tensor) \u2013 Input and output of the collective. The function operates in-place. dst (python:int) \u2013 Destination rank op (optional) \u2013 One of the values from torch.distributed.ReduceOp enum.  Specifies an operation used for element-wise reductions. group (ProcessGroup, optional) \u2013 The process group to work on async_op (bool, optional) \u2013 Whether this op should be an async op   Returns Async work handle, if async_op is set to True. None, if not async_op or if not part of the group   ", "parameters": ["tensor (Tensor) : Input and output of the collective. The functionoperates in-place.", "dst (python:int) : Destination rank", "op (optional) : One of the values fromtorch.distributed.ReduceOpenum.  Specifies an operation used for element-wise reductions.", "group (ProcessGroup, optional) : The process group to work on", "async_op (bool, optional) : Whether this op should be an async op"], "returns": "Async work handle, if async_op is set to True.None, if not async_op or if not part of the group", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributed.all_gather", "item_type": "function", "code": "torch.distributed.all_gather(tensor_list,tensor,group=&lt;objectobject&gt;,async_op=False)", "description": "Gathers tensors from the whole group in a list.  Parameters  tensor_list (list[Tensor]) \u2013 Output list. It should contain correctly-sized tensors to be used for output of the collective. tensor (Tensor) \u2013 Tensor to be broadcast from current process. group (ProcessGroup, optional) \u2013 The process group to work on async_op (bool, optional) \u2013 Whether this op should be an async op   Returns Async work handle, if async_op is set to True. None, if not async_op or if not part of the group   ", "parameters": ["tensor_list (list[Tensor]) : Output list. It should containcorrectly-sized tensors to be used for output of the collective.", "tensor (Tensor) : Tensor to be broadcast from current process.", "group (ProcessGroup, optional) : The process group to work on", "async_op (bool, optional) : Whether this op should be an async op"], "returns": "Async work handle, if async_op is set to True.None, if not async_op or if not part of the group", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.stream", "item_type": "function", "code": "torch.cuda.stream(stream)", "description": "Context-manager that selects a given stream. All CUDA kernels queued within its context will be enqueued on a selected stream.  Parameters stream (Stream) \u2013 selected stream. This manager is a no-op if it\u2019s None.    Note Streams are per-device. If the selected stream is not on the current device, this function will also change the current device to match the stream.  ", "parameters": ["stream (Stream) : selected stream. This manager is a no-op if it\u2019sNone."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.synchronize", "item_type": "function", "code": "torch.cuda.synchronize(device=None)", "description": "Waits for all kernels in all streams on a CUDA device to complete.  Parameters device (torch.device or python:int, optional) \u2013 device for which to synchronize. It uses the current device, given by current_device(), if device is None (default).   ", "parameters": ["device (torch.device or python:int, optional) : device for which to synchronize.It uses the current device, given by current_device(),if device is None (default)."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.get_rng_state", "item_type": "function", "code": "torch.cuda.get_rng_state(device='cuda')", "description": "Returns the random number generator state of the specified GPU as a ByteTensor.  Parameters device (torch.device or python:int, optional) \u2013 The device to return the RNG state of. Default: 'cuda' (i.e., torch.device('cuda'), the current CUDA device).    Warning This function eagerly initializes CUDA.  ", "parameters": ["device (torch.device or python:int, optional) : The device to return the RNG state of.Default: 'cuda' (i.e., torch.device('cuda'), the current CUDA device)."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.get_rng_state_all", "item_type": "function", "code": "torch.cuda.get_rng_state_all()", "description": "Returns a tuple of ByteTensor representing the random number states of all devices. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.set_rng_state", "item_type": "function", "code": "torch.cuda.set_rng_state(new_state,device='cuda')", "description": "Sets the random number generator state of the specified GPU.  Parameters  new_state (torch.ByteTensor) \u2013 The desired state device (torch.device or python:int, optional) \u2013 The device to set the RNG state. Default: 'cuda' (i.e., torch.device('cuda'), the current CUDA device).    ", "parameters": ["new_state (torch.ByteTensor) : The desired state", "device (torch.device or python:int, optional) : The device to set the RNG state.Default: 'cuda' (i.e., torch.device('cuda'), the current CUDA device)."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.set_rng_state_all", "item_type": "function", "code": "torch.cuda.set_rng_state_all(new_states)", "description": "Sets the random number generator state of all devices.  Parameters new_state (tuple of torch.ByteTensor) \u2013 The desired state for each device   ", "parameters": ["new_state (tuple of torch.ByteTensor) : The desired state for each device"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.manual_seed", "item_type": "function", "code": "torch.cuda.manual_seed(seed)", "description": "Sets the seed for generating random numbers for the current GPU. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored.  Parameters seed (python:int) \u2013 The desired seed.    Warning If you are working with a multi-GPU model, this function is insufficient to get determinism.  To seed all GPUs, use manual_seed_all().  ", "parameters": ["seed (python:int) : The desired seed."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.manual_seed_all", "item_type": "function", "code": "torch.cuda.manual_seed_all(seed)", "description": "Sets the seed for generating random numbers on all GPUs. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored.  Parameters seed (python:int) \u2013 The desired seed.   ", "parameters": ["seed (python:int) : The desired seed."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.jit.ignore", "item_type": "function", "code": "torch.jit.ignore(drop=False,**kwargs)", "description": "This decorator indicates to the compiler that a function or method should be ignored and left as a Python function. This allows you to leave code in your model that is not yet TorchScript compatible. Models with ignored functions cannot be exported; use torch.jit.unused instead. Example (using @torch.jit.ignore on a method): import torch import torch.nn as nn  class MyModule(nn.Module):     @torch.jit.ignore     def debugger(self, x):         import pdb         pdb.set_trace()      def forward(self, x):         x += 10         # The compiler would normally try to compile `debugger`,         # but since it is `@ignore`d, it will be left as a call         # to Python         self.debugger(x)         return x  m = torch.jit.script(MyModule())  # Error! The call `debugger` cannot be saved since it calls into Python m.save(\"m.pt\")   Example (using @torch.jit.ignore(drop=True) on a method): import torch import torch.nn as nn  class MyModule(nn.Module):     @torch.jit.ignore(drop=True)     def training_method(self, x):         import pdb         pdb.set_trace()      def forward(self, x):         if self.training:             self.training_method(x)         return x  m = torch.jit.script(MyModule())  # This is OK since `training_method` is not saved, the call is replaced # with a `raise`. m.save(\"m.pt\")   ", "parameters": [], "returns": null, "example": "import torch\nimport torch.nn as nn\n\nclass MyModule(nn.Module):\n    @torch.jit.ignore\n    def debugger(self, x):\n        import pdb\n        pdb.set_trace()\n\n    def forward(self, x):\n        x += 10\n        # The compiler would normally try to compile `debugger`,\n        # but since it is `@ignore`d, it will be left as a call\n        # to Python\n        self.debugger(x)\n        return x\n\nm = torch.jit.script(MyModule())\n\n# Error! The call `debugger` cannot be saved since it calls into Python\nm.save(\"m.pt\")\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.jit.unused", "item_type": "function", "code": "torch.jit.unused(fn)", "description": "This decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception. This allows you to leave code in your model that is not yet TorchScript compatible and still export your model.  Example (using @torch.jit.unused on a method): import torch import torch.nn as nn  class MyModule(nn.Module):     def __init__(self, use_memory_efficent):         super(MyModule, self).__init__()         self.use_memory_efficent = use_memory_efficent      @torch.jit.unused     def memory_efficient(self, x):         import pdb         pdb.set_trace()         return x + 10      def forward(self, x):         # Use not-yet-scriptable memory efficient mode         if self.use_memory_efficient:             return self.memory_efficient(x)         else:             return x + 10  m = torch.jit.script(MyModule(use_memory_efficent=False)) m.save(\"m.pt\")  m = torch.jit.script(MyModule(use_memory_efficient=True)) # exception raised m(torch.rand(100))    ", "parameters": [], "returns": null, "example": "import torch\nimport torch.nn as nn\n\nclass MyModule(nn.Module):\n    def __init__(self, use_memory_efficent):\n        super(MyModule, self).__init__()\n        self.use_memory_efficent = use_memory_efficent\n\n    @torch.jit.unused\n    def memory_efficient(self, x):\n        import pdb\n        pdb.set_trace()\n        return x + 10\n\n    def forward(self, x):\n        # Use not-yet-scriptable memory efficient mode\n        if self.use_memory_efficient:\n            return self.memory_efficient(x)\n        else:\n            return x + 10\n\nm = torch.jit.script(MyModule(use_memory_efficent=False))\nm.save(\"m.pt\")\n\nm = torch.jit.script(MyModule(use_memory_efficient=True))\n# exception raised\nm(torch.rand(100))\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.jit.is_scripting", "item_type": "function", "code": "torch.jit.is_scripting()", "description": "Function that returns True when in compilation and False otherwise. This is useful especially with the @unused decorator to leave code in your model that is not yet TorchScript compatible. .. testcode: import torch  @torch.jit.unused def unsupported_linear_op(x):     return x  def linear(x):    if not torch.jit.is_scripting():       return torch.linear(x)    else:       return unsupported_linear_op(x)   ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.jit.ScriptModule.code", "item_type": "method", "code": "propertycode", "description": "Returns a pretty-printed representation (as valid Python syntax) of the internal graph for the forward method. See Inspecting Code for details. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.jit.ScriptModule.graph", "item_type": "method", "code": "propertygraph", "description": "Returns a string representation of the internal graph for the forward method. See Interpreting Graphs for details. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.jit.ScriptModule.save", "item_type": "method", "code": "save(f,_extra_files=ExtraFilesMap{})", "description": "See torch.jit.save for details. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.detach", "item_type": "method", "code": "detach()", "description": "Returns a new Tensor, detached from the current graph. The result will never require gradient.  Note Returned Tensor shares the same storage with the original one. In-place modifications on either of them will be seen, and may trigger errors in correctness checks. IMPORTANT NOTE: Previously, in-place size / stride / storage changes (such as resize_ / resize_as_ / set_ / transpose_) to the returned tensor also update the original tensor. Now, these in-place changes will not update the original tensor anymore, and will instead trigger an error. For sparse tensors: In-place indices / values changes (such as zero_ / copy_ / add_) to the returned tensor will not update the original tensor anymore, and will instead trigger an error.  ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.detach_", "item_type": "method", "code": "detach_()", "description": "Detaches the Tensor from the graph that created it, making it a leaf. Views cannot be detached in-place. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.register_hook", "item_type": "method", "code": "register_hook(hook)", "description": "Registers a backward hook. The hook will be called every time a gradient with respect to the Tensor is computed. The hook should have the following signature: hook(grad) -&gt; Tensor or None   The hook should not modify its argument, but it can optionally return a new gradient which will be used in place of grad. This function returns a handle with a method handle.remove() that removes the hook from the module. Example: &gt;&gt;&gt; v = torch.tensor([0., 0., 0.], requires_grad=True) &gt;&gt;&gt; h = v.register_hook(lambda grad: grad * 2)  # double the gradient &gt;&gt;&gt; v.backward(torch.tensor([1., 2., 3.])) &gt;&gt;&gt; v.grad   2  4  6 [torch.FloatTensor of size (3,)]  &gt;&gt;&gt; h.remove()  # removes the hook   ", "parameters": [], "returns": null, "example": " v = torch.tensor([0., 0., 0.], requires_grad=True)\n h = v.register_hook(lambda grad: grad * 2)  # double the gradient\n v.backward(torch.tensor([1., 2., 3.]))\n v.grad\n\n 2\n 4\n 6\n[torch.FloatTensor of size (3,)]\n\n h.remove()  # removes the hook\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.retain_grad", "item_type": "method", "code": "retain_grad()", "description": "Enables .grad attribute for non-leaf Tensors. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.autograd.Function.backward", "item_type": "method", "code": "staticbackward(ctx,*grad_outputs)", "description": "Defines a formula for differentiating the operation. This function is to be overridden by all subclasses. It must accept a context ctx as the first argument, followed by as many outputs did forward() return, and it should return as many tensors, as there were inputs to forward(). Each argument is the gradient w.r.t the given output, and each returned value should be the gradient w.r.t. the corresponding input. The context can be used to retrieve tensors saved during the forward pass. It also has an attribute ctx.needs_input_grad as a tuple of booleans representing whether each input needs gradient. E.g., backward() will have ctx.needs_input_grad[0] = True if the first input to forward() needs gradient computated w.r.t. the output. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.autograd.Function.forward", "item_type": "method", "code": "staticforward(ctx,*args,**kwargs)", "description": "Performs the operation. This function is to be overridden by all subclasses. It must accept a context ctx as the first argument, followed by any number of arguments (tensors or other types). The context can be used to store tensors that can be then retrieved during the backward pass. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.autograd.function._ContextMethodMixin.mark_dirty", "item_type": "method", "code": "mark_dirty(*args)", "description": "Marks given tensors as modified in an in-place operation. This should be called at most once, only from inside the forward() method, and all arguments should be inputs. Every tensor that\u2019s been modified in-place in a call to forward() should be given to this function, to ensure correctness of our checks. It doesn\u2019t matter whether the function is called before or after modification. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.quantized.functional.max_pool2d", "item_type": "function", "code": "torch.nn.quantized.functional.max_pool2d(input,kernel_size,stride=None,padding=0,dilation=1,ceil_mode=False,return_indices=False)", "description": "Applies a 2D max pooling over a quantized input signal composed of several quantized input planes.  Note The input quantization parameters are propagated to the output.  See MaxPool2d for details. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.quantized.functional.adaptive_avg_pool2d", "item_type": "function", "code": "torch.nn.quantized.functional.adaptive_avg_pool2d(input,output_size)", "description": "Applies a 2D adaptive average pooling over a quantized input signal composed of several quantized input planes.  Note The input quantization paramteres propagate to the output.  See AdaptiveAvgPool2d for details and output shape.  Parameters output_size \u2013 the target output size (single integer or double-integer tuple)   ", "parameters": ["output_size : the target output size (single integer ordouble-integer tuple)"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.quantized.functional.avg_pool2d", "item_type": "function", "code": "torch.nn.quantized.functional.avg_pool2d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)", "description": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW   regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW   steps. The number of output features is equal to the number of input planes.  Note The input quantization parameters propagate to the output.  See AvgPool2d for details and output shape.  Parameters  input \u2013 quantized input tensor (minibatch,in_channels,iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iH , iW)(minibatch,in_channels,iH,iW)   kernel_size \u2013 size of the pooling region. Can be a single number or a tuple (kH, kW) stride \u2013 stride of the pooling operation. Can be a single number or a tuple (sH, sW). Default: kernel_size padding \u2013 implicit zero paddings on both sides of the input. Can be a single number or a tuple (padH, padW). Default: 0 ceil_mode \u2013 when True, will use ceil instead of floor in the formula to compute the output shape. Default: False count_include_pad \u2013 when True, will include the zero-padding in the averaging calculation. Default: True divisor_override \u2013 if specified, it will be used as divisor, otherwise size of the pooling region will be used. Default: None    ", "parameters": ["input : quantized input tensor (minibatch,in_channels,iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iH , iW)(minibatch,in_channels,iH,iW)", "kernel_size : size of the pooling region. Can be a single number or atuple (kH, kW)", "stride : stride of the pooling operation. Can be a single number or atuple (sH, sW). Default: kernel_size", "padding : implicit zero paddings on both sides of the input. Can be asingle number or a tuple (padH, padW). Default: 0", "ceil_mode : when True, will use ceil instead of floor in the formulato compute the output shape. Default: False", "count_include_pad : when True, will include the zero-padding in theaveraging calculation. Default: True", "divisor_override : if specified, it will be used as divisor, otherwisesize of the pooling region will be used. Default: None"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.quantized.functional.interpolate", "item_type": "function", "code": "torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode='nearest',align_corners=None)", "description": "Down/up samples the input to either the given size or the given scale_factor See torch.nn.functional.interpolate() for implementation details. The input dimensions are interpreted in the form: mini-batch x channels x [optional depth] x [optional height] x width.  Note The input quantization parameters propagate to the output.   Note Only 2D input is supported for quantized inputs   Note Only the following modes are supported for the quantized inputs:  bilinear nearest    Parameters  input (Tensor) \u2013 the input tensor size (python:int or Tuple[python:int] or Tuple[python:int, python:int] or Tuple[python:int, python:int, python:int]) \u2013 output spatial size. scale_factor (python:float or Tuple[python:float]) \u2013 multiplier for spatial size. Has to match input size if it is a tuple. mode (str) \u2013 algorithm used for upsampling: 'nearest' | 'bilinear' align_corners (bool, optional) \u2013 Geometrically, we consider the pixels of the input and output as squares rather than points. If set to True, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to False, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when scale_factor is kept the same. This only has an effect when mode is 'bilinear'. Default: False    ", "parameters": ["input (Tensor) : the input tensor", "size (python:int or Tuple[python:int] or Tuple[python:int, python:int] or Tuple[python:int, python:int, python:int]) : output spatial size.", "scale_factor (python:float or Tuple[python:float]) : multiplier for spatial size. Has to match input size if it is a tuple.", "mode (str) : algorithm used for upsampling:'nearest' | 'bilinear'", "align_corners (bool, optional) : Geometrically, we consider the pixels of theinput and output as squares rather than points.If set to True, the input and output tensors are aligned by thecenter points of their corner pixels, preserving the values at the corner pixels.If set to False, the input and output tensors are aligned by the cornerpoints of their corner pixels, and the interpolation uses edge value paddingfor out-of-boundary values, making this operation independent of input sizewhen scale_factor is kept the same. This only has an effect when modeis 'bilinear'.Default: False"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.quantized.functional.upsample", "item_type": "function", "code": "torch.nn.quantized.functional.upsample(input,size=None,scale_factor=None,mode='nearest',align_corners=None)", "description": "Upsamples the input to either the given size or the given scale_factor  Warning This function is deprecated in favor of torch.nn.quantized.functional.interpolate(). This is equivalent with nn.quantized.functional.interpolate(...).  See torch.nn.functional.interpolate() for implementation details. The input dimensions are interpreted in the form: mini-batch x channels x [optional depth] x [optional height] x width.  Note The input quantization parameters propagate to the output.   Note Only 2D input is supported for quantized inputs   Note Only the following modes are supported for the quantized inputs:  bilinear nearest    Parameters  input (Tensor) \u2013 quantized input tensor size (python:int or Tuple[python:int] or Tuple[python:int, python:int] or Tuple[python:int, python:int, python:int]) \u2013 output spatial size. scale_factor (python:float or Tuple[python:float]) \u2013 multiplier for spatial size. Has to be an integer. mode (string) \u2013 algorithm used for upsampling: 'nearest' | 'bilinear' align_corners (bool, optional) \u2013 Geometrically, we consider the pixels of the input and output as squares rather than points. If set to True, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to False, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when scale_factor is kept the same. This only has an effect when mode is 'bilinear'. Default: False     Warning With align_corners = True, the linearly interpolating modes (bilinear) don\u2019t proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is align_corners = False. See Upsample for concrete examples on how this affects the outputs.  ", "parameters": ["input (Tensor) : quantized input tensor", "size (python:int or Tuple[python:int] or Tuple[python:int, python:int] or Tuple[python:int, python:int, python:int]) : output spatial size.", "scale_factor (python:float or Tuple[python:float]) : multiplier for spatial size. Has to be an integer.", "mode (string) : algorithm used for upsampling:'nearest' | 'bilinear'", "align_corners (bool, optional) : Geometrically, we consider the pixels of theinput and output as squares rather than points.If set to True, the input and output tensors are aligned by thecenter points of their corner pixels, preserving the values at the corner pixels.If set to False, the input and output tensors are aligned by the cornerpoints of their corner pixels, and the interpolation uses edge value paddingfor out-of-boundary values, making this operation independent of input sizewhen scale_factor is kept the same. This only has an effect when modeis 'bilinear'.Default: False"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.optim.Adagrad", "item_type": "class", "code": "classtorch.optim.Adagrad(params,lr=0.01,lr_decay=0,weight_decay=0,initial_accumulator_value=0,eps=1e-10)", "description": "Implements Adagrad algorithm. It has been proposed in Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.  Parameters  params (iterable) \u2013 iterable of parameters to optimize or dicts defining parameter groups lr (python:float, optional) \u2013 learning rate (default: 1e-2) lr_decay (python:float, optional) \u2013 learning rate decay (default: 0) weight_decay (python:float, optional) \u2013 weight decay (L2 penalty) (default: 0) eps (python:float, optional) \u2013 term added to the denominator to improve numerical stability (default: 1e-10)      step(closure=None)  Performs a single optimization step.  Parameters closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.     ", "parameters": ["params (iterable) : iterable of parameters to optimize or dicts definingparameter groups", "lr (python:float, optional) : learning rate (default: 1e-2)", "lr_decay (python:float, optional) : learning rate decay (default: 0)", "weight_decay (python:float, optional) : weight decay (L2 penalty) (default: 0)", "eps (python:float, optional) : term added to the denominator to improvenumerical stability (default: 1e-10)"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.optim.Adam", "item_type": "class", "code": "classtorch.optim.Adam(params,lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0,amsgrad=False)", "description": "Implements Adam algorithm. It has been proposed in Adam: A Method for Stochastic Optimization.  Parameters  params (iterable) \u2013 iterable of parameters to optimize or dicts defining parameter groups lr (python:float, optional) \u2013 learning rate (default: 1e-3) betas (Tuple[python:float, python:float], optional) \u2013 coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999)) eps (python:float, optional) \u2013 term added to the denominator to improve numerical stability (default: 1e-8) weight_decay (python:float, optional) \u2013 weight decay (L2 penalty) (default: 0) amsgrad (boolean, optional) \u2013 whether to use the AMSGrad variant of this algorithm from the paper On the Convergence of Adam and Beyond (default: False)      step(closure=None)  Performs a single optimization step.  Parameters closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.     ", "parameters": ["params (iterable) : iterable of parameters to optimize or dicts definingparameter groups", "lr (python:float, optional) : learning rate (default: 1e-3)", "betas (Tuple[python:float, python:float], optional) : coefficients used for computingrunning averages of gradient and its square (default: (0.9, 0.999))", "eps (python:float, optional) : term added to the denominator to improvenumerical stability (default: 1e-8)", "weight_decay (python:float, optional) : weight decay (L2 penalty) (default: 0)", "amsgrad (boolean, optional) : whether to use the AMSGrad variant of thisalgorithm from the paper On the Convergence of Adam and Beyond(default: False)"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.optim.AdamW", "item_type": "class", "code": "classtorch.optim.AdamW(params,lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0.01,amsgrad=False)", "description": "Implements AdamW algorithm. The original Adam algorithm was proposed in Adam: A Method for Stochastic Optimization. The AdamW variant was proposed in Decoupled Weight Decay Regularization.  Parameters  params (iterable) \u2013 iterable of parameters to optimize or dicts defining parameter groups lr (python:float, optional) \u2013 learning rate (default: 1e-3) betas (Tuple[python:float, python:float], optional) \u2013 coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999)) eps (python:float, optional) \u2013 term added to the denominator to improve numerical stability (default: 1e-8) weight_decay (python:float, optional) \u2013 weight decay coefficient (default: 1e-2) amsgrad (boolean, optional) \u2013 whether to use the AMSGrad variant of this algorithm from the paper On the Convergence of Adam and Beyond (default: False)      step(closure=None)  Performs a single optimization step.  Parameters closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.     ", "parameters": ["params (iterable) : iterable of parameters to optimize or dicts definingparameter groups", "lr (python:float, optional) : learning rate (default: 1e-3)", "betas (Tuple[python:float, python:float], optional) : coefficients used for computingrunning averages of gradient and its square (default: (0.9, 0.999))", "eps (python:float, optional) : term added to the denominator to improvenumerical stability (default: 1e-8)", "weight_decay (python:float, optional) : weight decay coefficient (default: 1e-2)", "amsgrad (boolean, optional) : whether to use the AMSGrad variant of thisalgorithm from the paper On the Convergence of Adam and Beyond(default: False)"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.optim.SparseAdam", "item_type": "class", "code": "classtorch.optim.SparseAdam(params,lr=0.001,betas=(0.9,0.999),eps=1e-08)", "description": "Implements lazy version of Adam algorithm suitable for sparse tensors. In this variant, only moments that show up in the gradient get updated, and only those portions of the gradient get applied to the parameters.  Parameters  params (iterable) \u2013 iterable of parameters to optimize or dicts defining parameter groups lr (python:float, optional) \u2013 learning rate (default: 1e-3) betas (Tuple[python:float, python:float], optional) \u2013 coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999)) eps (python:float, optional) \u2013 term added to the denominator to improve numerical stability (default: 1e-8)      step(closure=None)  Performs a single optimization step.  Parameters closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.     ", "parameters": ["params (iterable) : iterable of parameters to optimize or dicts definingparameter groups", "lr (python:float, optional) : learning rate (default: 1e-3)", "betas (Tuple[python:float, python:float], optional) : coefficients used for computingrunning averages of gradient and its square (default: (0.9, 0.999))", "eps (python:float, optional) : term added to the denominator to improvenumerical stability (default: 1e-8)"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.datasets.Cityscapes", "item_type": "class", "code": "classtorchvision.datasets.Cityscapes(root,split='train',mode='fine',target_type='instance',transform=None,target_transform=None,transforms=None)", "description": "Cityscapes Dataset.  Parameters  root (string) \u2013 Root directory of dataset where directory leftImg8bit and gtFine or gtCoarse are located. split (string, optional) \u2013 The image split to use, train, test or val if mode=\u201dgtFine\u201d otherwise train, train_extra or val mode (string, optional) \u2013 The quality mode to use, gtFine or gtCoarse target_type (string or list, optional) \u2013 Type of target to use, instance, semantic, polygon or color. Can also be a list to output a tuple with all specified target types. transform (callable, optional) \u2013 A function/transform that takes in a PIL image and returns a transformed version. E.g, transforms.RandomCrop target_transform (callable, optional) \u2013 A function/transform that takes in the target and transforms it. transforms (callable, optional) \u2013 A function/transform that takes input sample and its target as entry and returns a transformed version.    Examples Get semantic segmentation target dataset = Cityscapes('./data/cityscapes', split='train', mode='fine',                      target_type='semantic')  img, smnt = dataset[0]   Get multiple targets dataset = Cityscapes('./data/cityscapes', split='train', mode='fine',                      target_type=['instance', 'color', 'polygon'])  img, (inst, col, poly) = dataset[0]   Validate on the \u201ccoarse\u201d set dataset = Cityscapes('./data/cityscapes', split='val', mode='coarse',                      target_type='semantic')  img, smnt = dataset[0]     __getitem__(index)   Parameters index (python:int) \u2013 Index  Returns (image, target) where target is a tuple of all target types if target_type is a list with more than one item. Otherwise target is a json object if target_type=\u201dpolygon\u201d, else the image segmentation.  Return type tuple     ", "parameters": ["root (string) : Root directory of dataset where directory leftImg8bitand gtFine or gtCoarse are located.", "split (string, optional) : The image split to use, train, test or val if mode=\u201dgtFine\u201dotherwise train, train_extra or val", "mode (string, optional) : The quality mode to use, gtFine or gtCoarse", "target_type (string or list, optional) : Type of target to use, instance, semantic, polygonor color. Can also be a list to output a tuple with all specified target types.", "transform (callable, optional) : A function/transform that takes in a PIL imageand returns a transformed version. E.g, transforms.RandomCrop", "target_transform (callable, optional) : A function/transform that takes in thetarget and transforms it.", "transforms (callable, optional) : A function/transform that takes input sample and its target as entryand returns a transformed version."], "returns": "(image, target) where target is a tuple of all target types if target_type is a list with morethan one item. Otherwise target is a json object if target_type=\u201dpolygon\u201d, else the image segmentation.", "example": "dataset = Cityscapes('./data/cityscapes', split='train', mode='fine',\n                     target_type='semantic')\n\nimg, smnt = dataset[0]\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.datasets.SBDataset", "item_type": "class", "code": "classtorchvision.datasets.SBDataset(root,image_set='train',mode='boundaries',download=False,transforms=None)", "description": "Semantic Boundaries Dataset The SBD currently contains annotations from 11355 images taken from the PASCAL VOC 2011 dataset.  Note Please note that the train and val splits included with this dataset are different from the splits in the PASCAL VOC dataset. In particular some \u201ctrain\u201d images might be part of VOC2012 val. If you are interested in testing on VOC 2012 val, then use image_set=\u2019train_noval\u2019, which excludes all val images.   Warning This class needs scipy to load target files from .mat format.   Parameters  root (string) \u2013 Root directory of the Semantic Boundaries Dataset image_set (string, optional) \u2013 Select the image_set to use, train, val or train_noval. Image set train_noval excludes VOC 2012 val images. mode (string, optional) \u2013 Select target type. Possible values \u2018boundaries\u2019 or \u2018segmentation\u2019. In case of \u2018boundaries\u2019, the target is an array of shape [num_classes, H, W], where num_classes=20. download (bool, optional) \u2013 If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again. transforms (callable, optional) \u2013 A function/transform that takes input sample and its target as entry and returns a transformed version. Input sample is PIL image and target is a numpy array if mode=\u2019boundaries\u2019 or PIL image if mode=\u2019segmentation\u2019.    ", "parameters": ["root (string) : Root directory of the Semantic Boundaries Dataset", "image_set (string, optional) : Select the image_set to use, train, val or train_noval.Image set train_noval excludes VOC 2012 val images.", "mode (string, optional) : Select target type. Possible values \u2018boundaries\u2019 or \u2018segmentation\u2019.In case of \u2018boundaries\u2019, the target is an array of shape [num_classes, H, W],where num_classes=20.", "download (bool, optional) : If true, downloads the dataset from the internet andputs it in root directory. If dataset is already downloaded, it is notdownloaded again.", "transforms (callable, optional) : A function/transform that takes input sample and its target as entryand returns a transformed version. Input sample is PIL image and target is a numpy arrayif mode=\u2019boundaries\u2019 or PIL image if mode=\u2019segmentation\u2019."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.datasets.USPS", "item_type": "class", "code": "classtorchvision.datasets.USPS(root,train=True,transform=None,target_transform=None,download=False)", "description": "USPS Dataset. The data-format is : [label [index:value ]*256 n] * num_lines, where label lies in [1, 10]. The value for each pixel lies in [-1, 1]. Here we transform the label into [0, 9] and make pixel values in [0, 255].  Parameters  root (string) \u2013 Root directory of dataset to store``USPS`` data files. train (bool, optional) \u2013 If True, creates dataset from usps.bz2, otherwise from usps.t.bz2. transform (callable, optional) \u2013 A function/transform that  takes in an PIL image and returns a transformed version. E.g, transforms.RandomCrop target_transform (callable, optional) \u2013 A function/transform that takes in the target and transforms it. download (bool, optional) \u2013 If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.      __getitem__(index)   Parameters index (python:int) \u2013 Index  Returns (image, target) where target is index of the target class.  Return type tuple     ", "parameters": ["root (string) : Root directory of dataset to store``USPS`` data files.", "train (bool, optional) : If True, creates dataset from usps.bz2,otherwise from usps.t.bz2.", "transform (callable, optional) : A function/transform that  takes in an PIL imageand returns a transformed version. E.g, transforms.RandomCrop", "target_transform (callable, optional) : A function/transform that takes in thetarget and transforms it.", "download (bool, optional) : If true, downloads the dataset from the internet andputs it in root directory. If dataset is already downloaded, it is notdownloaded again."], "returns": "(image, target) where target is index of the target class.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.new_full", "item_type": "method", "code": "new_full(size,fill_value,dtype=None,device=None,requires_grad=False)\u2192Tensor", "description": "Returns a Tensor of size size filled with fill_value. By default, the returned Tensor has the same torch.dtype and torch.device as this tensor.  Parameters  fill_value (scalar) \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired type of returned tensor. Default: if None, same torch.dtype as this tensor. device (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, same torch.device as this tensor. requires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: &gt;&gt;&gt; tensor = torch.ones((2,), dtype=torch.float64) &gt;&gt;&gt; tensor.new_full((3, 4), 3.141592) tensor([[ 3.1416,  3.1416,  3.1416,  3.1416],         [ 3.1416,  3.1416,  3.1416,  3.1416],         [ 3.1416,  3.1416,  3.1416,  3.1416]], dtype=torch.float64)   ", "parameters": ["fill_value (scalar) : the number to fill the output tensor with.", "dtype (torch.dtype, optional) : the desired type of returned tensor.Default: if None, same torch.dtype as this tensor.", "device (torch.device, optional) : the desired device of returned tensor.Default: if None, same torch.device as this tensor.", "requires_grad (bool, optional) : If autograd should record operations on thereturned tensor. Default: False."], "returns": null, "example": " tensor = torch.ones((2,), dtype=torch.float64)\n tensor.new_full((3, 4), 3.141592)\ntensor([[ 3.1416,  3.1416,  3.1416,  3.1416],\n        [ 3.1416,  3.1416,  3.1416,  3.1416],\n        [ 3.1416,  3.1416,  3.1416,  3.1416]], dtype=torch.float64)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.new_empty", "item_type": "method", "code": "new_empty(size,dtype=None,device=None,requires_grad=False)\u2192Tensor", "description": "Returns a Tensor of size size filled with uninitialized data. By default, the returned Tensor has the same torch.dtype and torch.device as this tensor.  Parameters  dtype (torch.dtype, optional) \u2013 the desired type of returned tensor. Default: if None, same torch.dtype as this tensor. device (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, same torch.device as this tensor. requires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: &gt;&gt;&gt; tensor = torch.ones(()) &gt;&gt;&gt; tensor.new_empty((2, 3)) tensor([[ 5.8182e-18,  4.5765e-41, -1.0545e+30],         [ 3.0949e-41,  4.4842e-44,  0.0000e+00]])   ", "parameters": ["dtype (torch.dtype, optional) : the desired type of returned tensor.Default: if None, same torch.dtype as this tensor.", "device (torch.device, optional) : the desired device of returned tensor.Default: if None, same torch.device as this tensor.", "requires_grad (bool, optional) : If autograd should record operations on thereturned tensor. Default: False."], "returns": null, "example": " tensor = torch.ones(())\n tensor.new_empty((2, 3))\ntensor([[ 5.8182e-18,  4.5765e-41, -1.0545e+30],\n        [ 3.0949e-41,  4.4842e-44,  0.0000e+00]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.new_ones", "item_type": "method", "code": "new_ones(size,dtype=None,device=None,requires_grad=False)\u2192Tensor", "description": "Returns a Tensor of size size filled with 1. By default, the returned Tensor has the same torch.dtype and torch.device as this tensor.  Parameters  size (python:int...) \u2013 a list, tuple, or torch.Size of integers defining the shape of the output tensor. dtype (torch.dtype, optional) \u2013 the desired type of returned tensor. Default: if None, same torch.dtype as this tensor. device (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, same torch.device as this tensor. requires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: &gt;&gt;&gt; tensor = torch.tensor((), dtype=torch.int32) &gt;&gt;&gt; tensor.new_ones((2, 3)) tensor([[ 1,  1,  1],         [ 1,  1,  1]], dtype=torch.int32)   ", "parameters": ["size (python:int...) : a list, tuple, or torch.Size of integers defining theshape of the output tensor.", "dtype (torch.dtype, optional) : the desired type of returned tensor.Default: if None, same torch.dtype as this tensor.", "device (torch.device, optional) : the desired device of returned tensor.Default: if None, same torch.device as this tensor.", "requires_grad (bool, optional) : If autograd should record operations on thereturned tensor. Default: False."], "returns": null, "example": " tensor = torch.tensor((), dtype=torch.int32)\n tensor.new_ones((2, 3))\ntensor([[ 1,  1,  1],\n        [ 1,  1,  1]], dtype=torch.int32)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.new_zeros", "item_type": "method", "code": "new_zeros(size,dtype=None,device=None,requires_grad=False)\u2192Tensor", "description": "Returns a Tensor of size size filled with 0. By default, the returned Tensor has the same torch.dtype and torch.device as this tensor.  Parameters  size (python:int...) \u2013 a list, tuple, or torch.Size of integers defining the shape of the output tensor. dtype (torch.dtype, optional) \u2013 the desired type of returned tensor. Default: if None, same torch.dtype as this tensor. device (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, same torch.device as this tensor. requires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: &gt;&gt;&gt; tensor = torch.tensor((), dtype=torch.float64) &gt;&gt;&gt; tensor.new_zeros((2, 3)) tensor([[ 0.,  0.,  0.],         [ 0.,  0.,  0.]], dtype=torch.float64)   ", "parameters": ["size (python:int...) : a list, tuple, or torch.Size of integers defining theshape of the output tensor.", "dtype (torch.dtype, optional) : the desired type of returned tensor.Default: if None, same torch.dtype as this tensor.", "device (torch.device, optional) : the desired device of returned tensor.Default: if None, same torch.device as this tensor.", "requires_grad (bool, optional) : If autograd should record operations on thereturned tensor. Default: False."], "returns": null, "example": " tensor = torch.tensor((), dtype=torch.float64)\n tensor.new_zeros((2, 3))\ntensor([[ 0.,  0.,  0.],\n        [ 0.,  0.,  0.]], dtype=torch.float64)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.abs", "item_type": "method", "code": "abs()\u2192Tensor", "description": "See torch.abs() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.abs_", "item_type": "method", "code": "abs_()\u2192Tensor", "description": "In-place version of abs() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.acos", "item_type": "method", "code": "acos()\u2192Tensor", "description": "See torch.acos() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.acos_", "item_type": "method", "code": "acos_()\u2192Tensor", "description": "In-place version of acos() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.add", "item_type": "method", "code": "add(value)\u2192Tensor", "description": "add(value=1, other) -&gt; Tensor See torch.add() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.conv_transpose2d", "item_type": "function", "code": "torch.nn.functional.conv_transpose2d(input,weight,bias=None,stride=1,padding=0,output_padding=0,groups=1,dilation=1)\u2192Tensor", "description": "Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d. See ConvTranspose2d for details and output shape.  Note In some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. Please see the notes on Reproducibility for background.   Parameters  input \u2013 input tensor of shape (minibatch,in_channels,iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iH , iW)(minibatch,in_channels,iH,iW)   weight \u2013 filters of shape (in_channels,out_channelsgroups,kH,kW)(\\text{in\\_channels} , \\frac{\\text{out\\_channels}}{\\text{groups}} , kH , kW)(in_channels,groupsout_channels\u200b,kH,kW)   bias \u2013 optional bias of shape (out_channels)(\\text{out\\_channels})(out_channels)  . Default: None stride \u2013 the stride of the convolving kernel. Can be a single number or a tuple (sH, sW). Default: 1 padding \u2013 dilation * (kernel_size - 1) - padding zero-padding will be added to both sides of each dimension in the input. Can be a single number or a tuple (padH, padW). Default: 0 output_padding \u2013 additional size added to one side of each dimension in the output shape. Can be a single number or a tuple (out_padH, out_padW). Default: 0 groups \u2013 split input into groups, in_channels\\text{in\\_channels}in_channels   should be divisible by the number of groups. Default: 1 dilation \u2013 the spacing between kernel elements. Can be a single number or a tuple (dH, dW). Default: 1    Examples: &gt;&gt;&gt; # With square kernels and equal stride &gt;&gt;&gt; inputs = torch.randn(1, 4, 5, 5) &gt;&gt;&gt; weights = torch.randn(4, 8, 3, 3) &gt;&gt;&gt; F.conv_transpose2d(inputs, weights, padding=1)   ", "parameters": ["input : input tensor of shape (minibatch,in_channels,iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iH , iW)(minibatch,in_channels,iH,iW)", "weight : filters of shape (in_channels,out_channelsgroups,kH,kW)(\\text{in\\_channels} , \\frac{\\text{out\\_channels}}{\\text{groups}} , kH , kW)(in_channels,groupsout_channels\u200b,kH,kW)", "bias : optional bias of shape (out_channels)(\\text{out\\_channels})(out_channels). Default: None", "stride : the stride of the convolving kernel. Can be a single number or atuple (sH, sW). Default: 1", "padding : dilation * (kernel_size - 1) - padding zero-padding will be added to bothsides of each dimension in the input. Can be a single number or a tuple(padH, padW). Default: 0", "output_padding : additional size added to one side of each dimension in theoutput shape. Can be a single number or a tuple (out_padH, out_padW).Default: 0", "groups : split input into groups, in_channels\\text{in\\_channels}in_channels should be divisible by thenumber of groups. Default: 1", "dilation : the spacing between kernel elements. Can be a single number ora tuple (dH, dW). Default: 1"], "returns": null, "example": " # With square kernels and equal stride\n inputs = torch.randn(1, 4, 5, 5)\n weights = torch.randn(4, 8, 3, 3)\n F.conv_transpose2d(inputs, weights, padding=1)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.conv_transpose3d", "item_type": "function", "code": "torch.nn.functional.conv_transpose3d(input,weight,bias=None,stride=1,padding=0,output_padding=0,groups=1,dilation=1)\u2192Tensor", "description": "Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d See ConvTranspose3d for details and output shape.  Note In some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. Please see the notes on Reproducibility for background.   Parameters  input \u2013 input tensor of shape (minibatch,in_channels,iT,iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iT , iH , iW)(minibatch,in_channels,iT,iH,iW)   weight \u2013 filters of shape (in_channels,out_channelsgroups,kT,kH,kW)(\\text{in\\_channels} , \\frac{\\text{out\\_channels}}{\\text{groups}} , kT , kH , kW)(in_channels,groupsout_channels\u200b,kT,kH,kW)   bias \u2013 optional bias of shape (out_channels)(\\text{out\\_channels})(out_channels)  . Default: None stride \u2013 the stride of the convolving kernel. Can be a single number or a tuple (sT, sH, sW). Default: 1 padding \u2013 dilation * (kernel_size - 1) - padding zero-padding will be added to both sides of each dimension in the input. Can be a single number or a tuple (padT, padH, padW). Default: 0 output_padding \u2013 additional size added to one side of each dimension in the output shape. Can be a single number or a tuple (out_padT, out_padH, out_padW). Default: 0 groups \u2013 split input into groups, in_channels\\text{in\\_channels}in_channels   should be divisible by the number of groups. Default: 1 dilation \u2013 the spacing between kernel elements. Can be a single number or a tuple (dT, dH, dW). Default: 1    Examples: &gt;&gt;&gt; inputs = torch.randn(20, 16, 50, 10, 20) &gt;&gt;&gt; weights = torch.randn(16, 33, 3, 3, 3) &gt;&gt;&gt; F.conv_transpose3d(inputs, weights)   ", "parameters": ["input : input tensor of shape (minibatch,in_channels,iT,iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iT , iH , iW)(minibatch,in_channels,iT,iH,iW)", "weight : filters of shape (in_channels,out_channelsgroups,kT,kH,kW)(\\text{in\\_channels} , \\frac{\\text{out\\_channels}}{\\text{groups}} , kT , kH , kW)(in_channels,groupsout_channels\u200b,kT,kH,kW)", "bias : optional bias of shape (out_channels)(\\text{out\\_channels})(out_channels). Default: None", "stride : the stride of the convolving kernel. Can be a single number or atuple (sT, sH, sW). Default: 1", "padding : dilation * (kernel_size - 1) - padding zero-padding will be added to bothsides of each dimension in the input. Can be a single number or a tuple(padT, padH, padW). Default: 0", "output_padding : additional size added to one side of each dimension in theoutput shape. Can be a single number or a tuple(out_padT, out_padH, out_padW). Default: 0", "groups : split input into groups, in_channels\\text{in\\_channels}in_channels should be divisible by thenumber of groups. Default: 1", "dilation : the spacing between kernel elements. Can be a single number ora tuple (dT, dH, dW). Default: 1"], "returns": null, "example": " inputs = torch.randn(20, 16, 50, 10, 20)\n weights = torch.randn(16, 33, 3, 3, 3)\n F.conv_transpose3d(inputs, weights)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.RandomAffine", "item_type": "class", "code": "classtorchvision.transforms.RandomAffine(degrees,translate=None,scale=None,shear=None,resample=False,fillcolor=0)", "description": "Random affine transformation of the image keeping center invariant  Parameters  degrees (sequence or python:float or python:int) \u2013 Range of degrees to select from. If degrees is a number instead of sequence like (min, max), the range of degrees will be (-degrees, +degrees). Set to 0 to deactivate rotations. translate (tuple, optional) \u2013 tuple of maximum absolute fraction for horizontal and vertical translations. For example translate=(a, b), then horizontal shift is randomly sampled in the range -img_width * a &lt; dx &lt; img_width * a and vertical shift is randomly sampled in the range -img_height * b &lt; dy &lt; img_height * b. Will not translate by default. scale (tuple, optional) \u2013 scaling factor interval, e.g (a, b), then scale is randomly sampled from the range a &lt;= scale &lt;= b. Will keep original scale by default. shear (sequence or python:float or python:int, optional) \u2013 Range of degrees to select from. If shear is a number, a shear parallel to the x axis in the range (-shear, +shear) will be apllied. Else if shear is a tuple or list of 2 values a shear parallel to the x axis in the range (shear[0], shear[1]) will be applied. Else if shear is a tuple or list of 4 values, a x-axis shear in (shear[0], shear[1]) and y-axis shear in (shear[2], shear[3]) will be applied. Will not apply shear by default resample ({PIL.Image.NEAREST, PIL.Image.BILINEAR, PIL.Image.BICUBIC}, optional) \u2013 An optional resampling filter. See filters for more information. If omitted, or if the image has mode \u201c1\u201d or \u201cP\u201d, it is set to PIL.Image.NEAREST. fillcolor (tuple or python:int) \u2013 Optional fill color (Tuple for RGB Image And int for grayscale) for the area outside the transform in the output image.(Pillow&gt;=5.0.0)    ", "parameters": ["degrees (sequence or python:float or python:int) : Range of degrees to select from.If degrees is a number instead of sequence like (min, max), the range of degreeswill be (-degrees, +degrees). Set to 0 to deactivate rotations.", "translate (tuple, optional) : tuple of maximum absolute fraction for horizontaland vertical translations. For example translate=(a, b), then horizontal shiftis randomly sampled in the range -img_width * a &lt; dx &lt; img_width * a and vertical shift israndomly sampled in the range -img_height * b &lt; dy &lt; img_height * b. Will not translate by default.", "scale (tuple, optional) : scaling factor interval, e.g (a, b), then scale israndomly sampled from the range a &lt;= scale &lt;= b. Will keep original scale by default.", "shear (sequence or python:float or python:int, optional) : Range of degrees to select from.If shear is a number, a shear parallel to the x axis in the range (-shear, +shear)will be apllied. Else if shear is a tuple or list of 2 values a shear parallel to the x axis in therange (shear[0], shear[1]) will be applied. Else if shear is a tuple or list of 4 values,a x-axis shear in (shear[0], shear[1]) and y-axis shear in (shear[2], shear[3]) will be applied.Will not apply shear by default", "resample ({PIL.Image.NEAREST, PIL.Image.BILINEAR, PIL.Image.BICUBIC}, optional) : An optional resampling filter. See filters for more information.If omitted, or if the image has mode \u201c1\u201d or \u201cP\u201d, it is set to PIL.Image.NEAREST.", "fillcolor (tuple or python:int) : Optional fill color (Tuple for RGB Image And int for grayscale) for the areaoutside the transform in the output image.(Pillow&gt;=5.0.0)"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.RandomApply", "item_type": "class", "code": "classtorchvision.transforms.RandomApply(transforms,p=0.5)", "description": "Apply randomly a list of transformations with a given probability  Parameters  transforms (list or tuple) \u2013 list of transformations p (python:float) \u2013 probability    ", "parameters": ["transforms (list or tuple) : list of transformations", "p (python:float) : probability"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.RandomChoice", "item_type": "class", "code": "classtorchvision.transforms.RandomChoice(transforms)", "description": "Apply single transformation randomly picked from a list ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.RandomCrop", "item_type": "class", "code": "classtorchvision.transforms.RandomCrop(size,padding=None,pad_if_needed=False,fill=0,padding_mode='constant')", "description": "Crop the given PIL Image at a random location.  Parameters  size (sequence or python:int) \u2013 Desired output size of the crop. If size is an int instead of sequence like (h, w), a square crop (size, size) is made. padding (python:int or sequence, optional) \u2013 Optional padding on each border of the image. Default is None, i.e no padding. If a sequence of length 4 is provided, it is used to pad left, top, right, bottom borders respectively. If a sequence of length 2 is provided, it is used to pad left/right, top/bottom borders, respectively. pad_if_needed (boolean) \u2013 It will pad the image if smaller than the desired size to avoid raising an exception. Since cropping is done after padding, the padding seems to be done at a random offset. fill \u2013 Pixel fill value for constant fill. Default is 0. If a tuple of length 3, it is used to fill R, G, B channels respectively. This value is only used when the padding_mode is constant padding_mode \u2013 Type of padding. Should be: constant, edge, reflect or symmetric. Default is constant.  constant: pads with a constant value, this value is specified with fill edge: pads with the last value on the edge of the image reflect: pads with reflection of image (without repeating the last value on the edge)  padding [1, 2, 3, 4] with 2 elements on both sides in reflect mode will result in [3, 2, 1, 2, 3, 4, 3, 2]   symmetric: pads with reflection of image (repeating the last value on the edge)  padding [1, 2, 3, 4] with 2 elements on both sides in symmetric mode will result in [2, 1, 1, 2, 3, 4, 4, 3]        ", "parameters": ["size (sequence or python:int) : Desired output size of the crop. If size is anint instead of sequence like (h, w), a square crop (size, size) ismade.", "padding (python:int or sequence, optional) : Optional padding on each borderof the image. Default is None, i.e no padding. If a sequence of length4 is provided, it is used to pad left, top, right, bottom bordersrespectively. If a sequence of length 2 is provided, it is used topad left/right, top/bottom borders, respectively.", "pad_if_needed (boolean) : It will pad the image if smaller than thedesired size to avoid raising an exception. Since cropping is doneafter padding, the padding seems to be done at a random offset.", "fill : Pixel fill value for constant fill. Default is 0. If a tuple oflength 3, it is used to fill R, G, B channels respectively.This value is only used when the padding_mode is constant", "padding_mode : Type of padding. Should be: constant, edge, reflect or symmetric. Default is constant.constant: pads with a constant value, this value is specified with filledge: pads with the last value on the edge of the imagereflect: pads with reflection of image (without repeating the last value on the edge)padding [1, 2, 3, 4] with 2 elements on both sides in reflect modewill result in [3, 2, 1, 2, 3, 4, 3, 2]symmetric: pads with reflection of image (repeating the last value on the edge)padding [1, 2, 3, 4] with 2 elements on both sides in symmetric modewill result in [2, 1, 1, 2, 3, 4, 4, 3]", "constant: pads with a constant value, this value is specified with fill", "edge: pads with the last value on the edge of the image", "reflect: pads with reflection of image (without repeating the last value on the edge)padding [1, 2, 3, 4] with 2 elements on both sides in reflect modewill result in [3, 2, 1, 2, 3, 4, 3, 2]", "symmetric: pads with reflection of image (repeating the last value on the edge)padding [1, 2, 3, 4] with 2 elements on both sides in symmetric modewill result in [2, 1, 1, 2, 3, 4, 4, 3]"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.bernoulli.Bernoulli.entropy", "item_type": "method", "code": "entropy()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.bernoulli.Bernoulli.enumerate_support", "item_type": "method", "code": "enumerate_support(expand=True)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.bernoulli.Bernoulli.expand", "item_type": "method", "code": "expand(batch_shape,_instance=None)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.bernoulli.Bernoulli.log_prob", "item_type": "method", "code": "log_prob(value)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.bernoulli.Bernoulli.mean", "item_type": "method", "code": "propertymean", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.bernoulli.Bernoulli.param_shape", "item_type": "method", "code": "propertyparam_shape", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.bernoulli.Bernoulli.sample", "item_type": "method", "code": "sample(sample_shape=torch.Size([]))", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.bernoulli.Bernoulli.variance", "item_type": "method", "code": "propertyvariance", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.beta.Beta.concentration0", "item_type": "method", "code": "propertyconcentration0", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.beta.Beta.concentration1", "item_type": "method", "code": "propertyconcentration1", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.beta.Beta.entropy", "item_type": "method", "code": "entropy()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.beta.Beta.expand", "item_type": "method", "code": "expand(batch_shape,_instance=None)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.beta.Beta.log_prob", "item_type": "method", "code": "log_prob(value)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributed.gather", "item_type": "function", "code": "torch.distributed.gather(tensor,gather_list=None,dst=0,group=&lt;objectobject&gt;,async_op=False)", "description": "Gathers a list of tensors in a single process.  Parameters  tensor (Tensor) \u2013 Input tensor. gather_list (list[Tensor], optional) \u2013 List of appropriately-sized tensors to use for gathered data (default is None, must be specified on the destination rank) dst (python:int, optional) \u2013 Destination rank (default is 0) group (ProcessGroup, optional) \u2013 The process group to work on async_op (bool, optional) \u2013 Whether this op should be an async op   Returns Async work handle, if async_op is set to True. None, if not async_op or if not part of the group   ", "parameters": ["tensor (Tensor) : Input tensor.", "gather_list (list[Tensor], optional) : List of appropriately-sizedtensors to use for gathered data (default is None, must be specifiedon the destination rank)", "dst (python:int, optional) : Destination rank (default is 0)", "group (ProcessGroup, optional) : The process group to work on", "async_op (bool, optional) : Whether this op should be an async op"], "returns": "Async work handle, if async_op is set to True.None, if not async_op or if not part of the group", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributed.scatter", "item_type": "function", "code": "torch.distributed.scatter(tensor,scatter_list=None,src=0,group=&lt;objectobject&gt;,async_op=False)", "description": "Scatters a list of tensors to all processes in a group. Each process will receive exactly one tensor and store its data in the tensor argument.  Parameters  tensor (Tensor) \u2013 Output tensor. scatter_list (list[Tensor]) \u2013 List of tensors to scatter (default is None, must be specified on the source rank) src (python:int) \u2013 Source rank (default is 0) group (ProcessGroup, optional) \u2013 The process group to work on async_op (bool, optional) \u2013 Whether this op should be an async op   Returns Async work handle, if async_op is set to True. None, if not async_op or if not part of the group   ", "parameters": ["tensor (Tensor) : Output tensor.", "scatter_list (list[Tensor]) : List of tensors to scatter (default isNone, must be specified on the source rank)", "src (python:int) : Source rank (default is 0)", "group (ProcessGroup, optional) : The process group to work on", "async_op (bool, optional) : Whether this op should be an async op"], "returns": "Async work handle, if async_op is set to True.None, if not async_op or if not part of the group", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributed.barrier", "item_type": "function", "code": "torch.distributed.barrier(group=&lt;objectobject&gt;,async_op=False)", "description": "Synchronizes all processes. This collective blocks processes until the whole group enters this function, if async_op is False, or if async work handle is called on wait().  Parameters  group (ProcessGroup, optional) \u2013 The process group to work on async_op (bool, optional) \u2013 Whether this op should be an async op   Returns Async work handle, if async_op is set to True. None, if not async_op or if not part of the group   ", "parameters": ["group (ProcessGroup, optional) : The process group to work on", "async_op (bool, optional) : Whether this op should be an async op"], "returns": "Async work handle, if async_op is set to True.None, if not async_op or if not part of the group", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributed.broadcast_multigpu", "item_type": "function", "code": "torch.distributed.broadcast_multigpu(tensor_list,src,group=&lt;objectobject&gt;,async_op=False,src_tensor=0)", "description": "Broadcasts the tensor to the whole group with multiple GPU tensors per node. tensor must have the same number of elements in all the GPUs from all processes participating in the collective. each tensor in the list must be on a different GPU Only nccl and gloo backend are currently supported tensors should only be GPU tensors  Parameters  tensor_list (List[Tensor]) \u2013 Tensors that participate in the collective operation. If src is the rank, then the specified src_tensor element of tensor_list (tensor_list[src_tensor]) will be broadcast to all other tensors (on different GPUs) in the src process and all tensors in tensor_list of other non-src processes. You also need to make sure that len(tensor_list) is the same for all the distributed processes calling this function. src (python:int) \u2013 Source rank. group (ProcessGroup, optional) \u2013 The process group to work on async_op (bool, optional) \u2013 Whether this op should be an async op src_tensor (python:int, optional) \u2013 Source tensor rank within tensor_list   Returns Async work handle, if async_op is set to True. None, if not async_op or if not part of the group   ", "parameters": ["tensor_list (List[Tensor]) : Tensors that participate in the collectiveoperation. If src is the rank, then the specified src_tensorelement of tensor_list (tensor_list[src_tensor]) will bebroadcast to all other tensors (on different GPUs) in the src processand all tensors in tensor_list of other non-src processes.You also need to make sure that len(tensor_list) is the samefor all the distributed processes calling this function.", "src (python:int) : Source rank.", "group (ProcessGroup, optional) : The process group to work on", "async_op (bool, optional) : Whether this op should be an async op", "src_tensor (python:int, optional) : Source tensor rank within tensor_list"], "returns": "Async work handle, if async_op is set to True.None, if not async_op or if not part of the group", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributed.all_reduce_multigpu", "item_type": "function", "code": "torch.distributed.all_reduce_multigpu(tensor_list,op=ReduceOp.SUM,group=&lt;objectobject&gt;,async_op=False)", "description": "Reduces the tensor data across all machines in such a way that all get the final result. This function reduces a number of tensors on every node, while each tensor resides on different GPUs. Therefore, the input tensor in the tensor list needs to be GPU tensors. Also, each tensor in the tensor list needs to reside on a different GPU. After the call, all tensor in tensor_list is going to be bitwise identical in all processes. Only nccl and gloo backend is currently supported tensors should only be GPU tensors  Parameters  list (tensor) \u2013 List of input and output tensors of the collective. The function operates in-place and requires that each tensor to be a GPU tensor on different GPUs. You also need to make sure that len(tensor_list) is the same for all the distributed processes calling this function. op (optional) \u2013 One of the values from torch.distributed.ReduceOp enum.  Specifies an operation used for element-wise reductions. group (ProcessGroup, optional) \u2013 The process group to work on async_op (bool, optional) \u2013 Whether this op should be an async op   Returns Async work handle, if async_op is set to True. None, if not async_op or if not part of the group   ", "parameters": ["list (tensor) : List of input and output tensors ofthe collective. The function operates in-place and requires thateach tensor to be a GPU tensor on different GPUs.You also need to make sure that len(tensor_list) is the same forall the distributed processes calling this function.", "op (optional) : One of the values fromtorch.distributed.ReduceOpenum.  Specifies an operation used for element-wise reductions.", "group (ProcessGroup, optional) : The process group to work on", "async_op (bool, optional) : Whether this op should be an async op"], "returns": "Async work handle, if async_op is set to True.None, if not async_op or if not part of the group", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributed.reduce_multigpu", "item_type": "function", "code": "torch.distributed.reduce_multigpu(tensor_list,dst,op=ReduceOp.SUM,group=&lt;objectobject&gt;,async_op=False,dst_tensor=0)", "description": "Reduces the tensor data on multiple GPUs across all machines. Each tensor in tensor_list should reside on a separate GPU Only the GPU of tensor_list[dst_tensor] on the process with rank dst is going to receive the final result. Only nccl backend is currently supported tensors should only be GPU tensors  Parameters  tensor_list (List[Tensor]) \u2013 Input and output GPU tensors of the collective. The function operates in-place. You also need to make sure that len(tensor_list) is the same for all the distributed processes calling this function. dst (python:int) \u2013 Destination rank op (optional) \u2013 One of the values from torch.distributed.ReduceOp enum.  Specifies an operation used for element-wise reductions. group (ProcessGroup, optional) \u2013 The process group to work on async_op (bool, optional) \u2013 Whether this op should be an async op dst_tensor (python:int, optional) \u2013 Destination tensor rank within tensor_list   Returns Async work handle, if async_op is set to True. None, otherwise   ", "parameters": ["tensor_list (List[Tensor]) : Input and output GPU tensors of thecollective. The function operates in-place.You also need to make sure that len(tensor_list) is the same forall the distributed processes calling this function.", "dst (python:int) : Destination rank", "op (optional) : One of the values fromtorch.distributed.ReduceOpenum.  Specifies an operation used for element-wise reductions.", "group (ProcessGroup, optional) : The process group to work on", "async_op (bool, optional) : Whether this op should be an async op", "dst_tensor (python:int, optional) : Destination tensor rank withintensor_list"], "returns": "Async work handle, if async_op is set to True.None, otherwise", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributed.all_gather_multigpu", "item_type": "function", "code": "torch.distributed.all_gather_multigpu(output_tensor_lists,input_tensor_list,group=&lt;objectobject&gt;,async_op=False)", "description": "Gathers tensors from the whole group in a list. Each tensor in tensor_list should reside on a separate GPU Only nccl backend is currently supported tensors should only be GPU tensors  Parameters  output_tensor_lists (List[List[Tensor]]) \u2013 Output lists. It should contain correctly-sized tensors on each GPU to be used for output of the collective, e.g. output_tensor_lists[i] contains the all_gather result that resides on the GPU of input_tensor_list[i]. Note that each element of output_tensor_lists has the size of world_size * len(input_tensor_list), since the function all gathers the result from every single GPU in the group. To interpret each element of output_tensor_lists[i], note that input_tensor_list[j] of rank k will be appear in output_tensor_lists[i][k * world_size + j] Also note that len(output_tensor_lists), and the size of each element in output_tensor_lists (each element is a list, therefore len(output_tensor_lists[i])) need to be the same for all the distributed processes calling this function.  input_tensor_list (List[Tensor]) \u2013 List of tensors(on different GPUs) to be broadcast from current process. Note that len(input_tensor_list) needs to be the same for all the distributed processes calling this function. group (ProcessGroup, optional) \u2013 The process group to work on async_op (bool, optional) \u2013 Whether this op should be an async op   Returns Async work handle, if async_op is set to True. None, if not async_op or if not part of the group   ", "parameters": ["output_tensor_lists (List[List[Tensor]]) : Output lists. It shouldcontain correctly-sized tensors on each GPU to be used for outputof the collective, e.g. output_tensor_lists[i] contains theall_gather result that resides on the GPU ofinput_tensor_list[i].Note that each element of output_tensor_lists has the size ofworld_size * len(input_tensor_list), since the function allgathers the result from every single GPU in the group. To interpreteach element of output_tensor_lists[i], note thatinput_tensor_list[j] of rank k will be appear inoutput_tensor_lists[i][k * world_size + j]Also note that len(output_tensor_lists), and the size of eachelement in output_tensor_lists (each element is a list,therefore len(output_tensor_lists[i])) need to be the samefor all the distributed processes calling this function.", "input_tensor_list (List[Tensor]) : List of tensors(on different GPUs) tobe broadcast from current process.Note that len(input_tensor_list) needs to be the same forall the distributed processes calling this function.", "group (ProcessGroup, optional) : The process group to work on", "async_op (bool, optional) : Whether this op should be an async op"], "returns": "Async work handle, if async_op is set to True.None, if not async_op or if not part of the group", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.seed", "item_type": "function", "code": "torch.cuda.seed()", "description": "Sets the seed for generating random numbers to a random number for the current GPU. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored.  Warning If you are working with a multi-GPU model, this function will only initialize the seed on one GPU.  To initialize all GPUs, use seed_all().  ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.seed_all", "item_type": "function", "code": "torch.cuda.seed_all()", "description": "Sets the seed for generating random numbers to a random number on all GPUs. It\u2019s safe to call this function if CUDA is not available; in that case, it is silently ignored. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.initial_seed", "item_type": "function", "code": "torch.cuda.initial_seed()", "description": "Returns the current random seed of the current GPU.  Warning This function eagerly initializes CUDA.  ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.comm.broadcast", "item_type": "function", "code": "torch.cuda.comm.broadcast(tensor,devices)", "description": "Broadcasts a tensor to a number of GPUs.  Parameters  tensor (Tensor) \u2013 tensor to broadcast. devices (Iterable) \u2013 an iterable of devices among which to broadcast. Note that it should be like (src, dst1, dst2, \u2026), the first element of which is the source device to broadcast from.   Returns A tuple containing copies of the tensor, placed on devices corresponding to indices from devices.   ", "parameters": ["tensor (Tensor) : tensor to broadcast.", "devices (Iterable) : an iterable of devices among which to broadcast.Note that it should be like (src, dst1, dst2, \u2026), the first elementof which is the source device to broadcast from."], "returns": "A tuple containing copies of the tensor, placed on devicescorresponding to indices from devices.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.comm.broadcast_coalesced", "item_type": "function", "code": "torch.cuda.comm.broadcast_coalesced(tensors,devices,buffer_size=10485760)", "description": "Broadcasts a sequence tensors to the specified GPUs. Small tensors are first coalesced into a buffer to reduce the number of synchronizations.  Parameters  tensors (sequence) \u2013 tensors to broadcast. devices (Iterable) \u2013 an iterable of devices among which to broadcast. Note that it should be like (src, dst1, dst2, \u2026), the first element of which is the source device to broadcast from. buffer_size (python:int) \u2013 maximum size of the buffer used for coalescing   Returns A tuple containing copies of the tensor, placed on devices corresponding to indices from devices.   ", "parameters": ["tensors (sequence) : tensors to broadcast.", "devices (Iterable) : an iterable of devices among which to broadcast.Note that it should be like (src, dst1, dst2, \u2026), the first elementof which is the source device to broadcast from.", "buffer_size (python:int) : maximum size of the buffer used for coalescing"], "returns": "A tuple containing copies of the tensor, placed on devicescorresponding to indices from devices.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.comm.reduce_add", "item_type": "function", "code": "torch.cuda.comm.reduce_add(inputs,destination=None)", "description": "Sums tensors from multiple GPUs. All inputs should have matching shapes.  Parameters  inputs (Iterable[Tensor]) \u2013 an iterable of tensors to add. destination (python:int, optional) \u2013 a device on which the output will be placed (default: current device).   Returns A tensor containing an elementwise sum of all inputs, placed on the destination device.   ", "parameters": ["inputs (Iterable[Tensor]) : an iterable of tensors to add.", "destination (python:int, optional) : a device on which the output will beplaced (default: current device)."], "returns": "A tensor containing an elementwise sum of all inputs, placed on thedestination device.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.comm.scatter", "item_type": "function", "code": "torch.cuda.comm.scatter(tensor,devices,chunk_sizes=None,dim=0,streams=None)", "description": "Scatters tensor across multiple GPUs.  Parameters  tensor (Tensor) \u2013 tensor to scatter. devices (Iterable[python:int]) \u2013 iterable of ints, specifying among which devices the tensor should be scattered. chunk_sizes (Iterable[python:int], optional) \u2013 sizes of chunks to be placed on each device. It should match devices in length and sum to tensor.size(dim). If not specified, the tensor will be divided into equal chunks. dim (python:int, optional) \u2013 A dimension along which to chunk the tensor.   Returns A tuple containing chunks of the tensor, spread across given devices.   ", "parameters": ["tensor (Tensor) : tensor to scatter.", "devices (Iterable[python:int]) : iterable of ints, specifying among whichdevices the tensor should be scattered.", "chunk_sizes (Iterable[python:int], optional) : sizes of chunks to be placed oneach device. It should match devices in length and sum totensor.size(dim). If not specified, the tensor will be dividedinto equal chunks.", "dim (python:int, optional) : A dimension along which to chunk the tensor."], "returns": "A tuple containing chunks of the tensor, spread across givendevices.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.comm.gather", "item_type": "function", "code": "torch.cuda.comm.gather(tensors,dim=0,destination=None)", "description": "Gathers tensors from multiple GPUs. Tensor sizes in all dimension different than dim have to match.  Parameters  tensors (Iterable[Tensor]) \u2013 iterable of tensors to gather. dim (python:int) \u2013 a dimension along which the tensors will be concatenated. destination (python:int, optional) \u2013 output device (-1 means CPU, default: current device)   Returns A tensor located on destination device, that is a result of concatenating tensors along dim.   ", "parameters": ["tensors (Iterable[Tensor]) : iterable of tensors to gather.", "dim (python:int) : a dimension along which the tensors will be concatenated.", "destination (python:int, optional) : output device (-1 means CPU, default:current device)"], "returns": "A tensor located on destination device, that is a result ofconcatenating tensors along dim.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.jit.ScriptModule", "item_type": "class", "code": "classtorch.jit.ScriptModule", "description": "  property code Returns a pretty-printed representation (as valid Python syntax) of the internal graph for the forward method. See Inspecting Code for details.     property graph Returns a string representation of the internal graph for the forward method. See Interpreting Graphs for details.     save(f, _extra_files=ExtraFilesMap{}) See torch.jit.save for details.   ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.jit.ScriptFunction", "item_type": "class", "code": "classtorch.jit.ScriptFunction", "description": "Functionally equivalent to a ScriptModule, but represents a single function and does not have any attributes or Parameters. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.autograd.function._ContextMethodMixin.mark_non_differentiable", "item_type": "method", "code": "mark_non_differentiable(*args)", "description": "Marks outputs as non-differentiable. This should be called at most once, only from inside the forward() method, and all arguments should be outputs. This will mark outputs as not requiring gradients, increasing the efficiency of backward computation. You still need to accept a gradient for each output in backward(), but it\u2019s always going to be a zero tensor with the same shape as the shape of a corresponding output. This is used e.g. for indices returned from a max Function. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.autograd.function._ContextMethodMixin.save_for_backward", "item_type": "method", "code": "save_for_backward(*tensors)", "description": "Saves given tensors for a future call to backward(). This should be called at most once, and only from inside the forward() method. Later, saved tensors can be accessed through the saved_tensors attribute. Before returning them to the user, a check is made to ensure they weren\u2019t used in any in-place operation that modified their content. Arguments can also be None. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.autograd.profiler.profile.export_chrome_trace", "item_type": "method", "code": "export_chrome_trace(path)", "description": "Exports an EventList as a Chrome tracing tools file. The checkpoint can be later loaded and inspected under chrome://tracing URL.  Parameters path (str) \u2013 Path where the trace will be written.   ", "parameters": ["path (str) : Path where the trace will be written."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.autograd.profiler.profile.key_averages", "item_type": "method", "code": "key_averages(group_by_input_shape=False)", "description": "Averages all function events over their keys. @param group_by_input_shapes The key would become (event name, input dimensions) rather than just event name. This is useful to see which dimensionality contributes to the runtime the most and may help with dimension specific optimizations or choosing best candidates for quantization (aka fitting a roof line)  Returns An EventList containing FunctionEventAvg objects.   ", "parameters": [], "returns": "An EventList containing FunctionEventAvg objects.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.autograd.profiler.profile.self_cpu_time_total", "item_type": "method", "code": "propertyself_cpu_time_total", "description": "Returns total time spent on CPU obtained as a sum of all self times across all the events. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.autograd.profiler.profile.table", "item_type": "method", "code": "table(sort_by=None,row_limit=100,header=None)", "description": "Prints an EventList as a nicely formatted table.  Parameters sort_by (str, optional) \u2013 Attribute used to sort entries. By default they are printed in the same order as they were registered. Valid keys include: cpu_time, cuda_time, cpu_time_total, cuda_time_total, count.  Returns A string containing the table.   ", "parameters": ["sort_by (str, optional) : Attribute used to sort entries. By defaultthey are printed in the same order as they were registered.Valid keys include: cpu_time, cuda_time, cpu_time_total,cuda_time_total, count.", "A string containing the table."], "returns": "A string containing the table.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.autograd.profiler.profile.total_average", "item_type": "method", "code": "total_average()", "description": "Averages all events.  Returns A FunctionEventAvg object.   ", "parameters": [], "returns": "A FunctionEventAvg object.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.autograd.no_grad", "item_type": "class", "code": "classtorch.autograd.no_grad", "description": "Context-manager that disabled gradient calculation. Disabling gradient calculation is useful for inference, when you are sure that you will not call Tensor.backward(). It will reduce memory consumption for computations that would otherwise have requires_grad=True. In this mode, the result of every computation will have requires_grad=False, even when the inputs have requires_grad=True. This mode has no effect when using enable_grad context manager . This context manager is thread local; it will not affect computation in other threads. Also functions as a decorator. Example: &gt;&gt;&gt; x = torch.tensor([1], requires_grad=True) &gt;&gt;&gt; with torch.no_grad(): ...   y = x * 2 &gt;&gt;&gt; y.requires_grad False &gt;&gt;&gt; @torch.no_grad() ... def doubler(x): ...     return x * 2 &gt;&gt;&gt; z = doubler(x) &gt;&gt;&gt; z.requires_grad False   ", "parameters": [], "returns": null, "example": " x = torch.tensor([1], requires_grad=True)\n with torch.no_grad():\n...   y = x * 2\n y.requires_grad\nFalse\n @torch.no_grad()\n... def doubler(x):\n...     return x * 2\n z = doubler(x)\n z.requires_grad\nFalse\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.quantized.functional.upsample_bilinear", "item_type": "function", "code": "torch.nn.quantized.functional.upsample_bilinear(input,size=None,scale_factor=None)", "description": "Upsamples the input, using bilinear upsampling.  Warning This function is deprecated in favor of torch.nn.quantized.functional.interpolate(). This is equivalent with nn.quantized.functional.interpolate(..., mode='bilinear', align_corners=True).   Note The input quantization parameters propagate to the output.   Note Only 2D inputs are supported   Parameters  input (Tensor) \u2013 quantized input size (python:int or Tuple[python:int, python:int]) \u2013 output spatial size. scale_factor (python:int or Tuple[python:int, python:int]) \u2013 multiplier for spatial size    ", "parameters": ["input (Tensor) : quantized input", "size (python:int or Tuple[python:int, python:int]) : output spatial size.", "scale_factor (python:int or Tuple[python:int, python:int]) : multiplier for spatial size"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.quantized.functional.upsample_nearest", "item_type": "function", "code": "torch.nn.quantized.functional.upsample_nearest(input,size=None,scale_factor=None)", "description": "Upsamples the input, using nearest neighbours\u2019 pixel values.  Warning This function is deprecated in favor of torch.nn.quantized.functional.interpolate(). This is equivalent with nn.quantized.functional.interpolate(..., mode='nearest').   Note The input quantization parameters propagate to the output.   Note Only 2D inputs are supported   Parameters  input (Tensor) \u2013 quantized input size (python:int or Tuple[python:int, python:int] or Tuple[python:int, python:int, python:int]) \u2013 output spatial size. scale_factor (python:int) \u2013 multiplier for spatial size. Has to be an integer.    ", "parameters": ["input (Tensor) : quantized input", "size (python:int or Tuple[python:int, python:int] or Tuple[python:int, python:int, python:int]) : output spatialsize.", "scale_factor (python:int) : multiplier for spatial size. Has to be an integer."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.intrinsic.qat.ConvBn2d.from_float", "item_type": "method", "code": "classmethodfrom_float(mod,qconfig=None)", "description": "Create a qat module from a float module or qparams_dict Args: mod a float module, either produced by torch.quantization utilities or directly from user ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.qat.Conv2d.from_float", "item_type": "method", "code": "classmethodfrom_float(mod,qconfig=None)", "description": "Create a qat module from a float module or qparams_dict Args: mod a float module, either produced by torch.quantization utilities or directly from user ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.qat.Linear.from_float", "item_type": "method", "code": "classmethodfrom_float(mod,qconfig=None)", "description": "Create a qat module from a float module or qparams_dict Args: mod a float module, either produced by torch.quantization utilities or directly from user ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.quantized.Conv2d.from_float", "item_type": "method", "code": "classmethodfrom_float(mod)", "description": "Creates a quantized module from a float module or qparams_dict.  Parameters mod (Module) \u2013 a float module, either produced by torch.quantization utilities or provided by the user   ", "parameters": ["mod (Module) : a float module, either produced by torch.quantizationutilities or provided by the user"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.quantized.Conv3d.from_float", "item_type": "method", "code": "classmethodfrom_float(mod)", "description": "Creates a quantized module from a float module or qparams_dict.  Parameters mod (Module) \u2013 a float module, either produced by torch.quantization utilities or provided by the user   ", "parameters": ["mod (Module) : a float module, either produced by torch.quantizationutilities or provided by the user"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.quantized.Linear.from_float", "item_type": "method", "code": "classmethodfrom_float(mod)", "description": "Create a quantized module from a float module or qparams_dict  Parameters mod (Module) \u2013 a float module, either produced by torch.quantization utilities or provided by the user   ", "parameters": ["mod (Module) : a float module, either produced by torch.quantizationutilities or provided by the user"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.optim.Adamax", "item_type": "class", "code": "classtorch.optim.Adamax(params,lr=0.002,betas=(0.9,0.999),eps=1e-08,weight_decay=0)", "description": "Implements Adamax algorithm (a variant of Adam based on infinity norm). It has been proposed in Adam: A Method for Stochastic Optimization.  Parameters  params (iterable) \u2013 iterable of parameters to optimize or dicts defining parameter groups lr (python:float, optional) \u2013 learning rate (default: 2e-3) betas (Tuple[python:float, python:float], optional) \u2013 coefficients used for computing running averages of gradient and its square eps (python:float, optional) \u2013 term added to the denominator to improve numerical stability (default: 1e-8) weight_decay (python:float, optional) \u2013 weight decay (L2 penalty) (default: 0)      step(closure=None)  Performs a single optimization step.  Parameters closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.     ", "parameters": ["params (iterable) : iterable of parameters to optimize or dicts definingparameter groups", "lr (python:float, optional) : learning rate (default: 2e-3)", "betas (Tuple[python:float, python:float], optional) : coefficients used for computingrunning averages of gradient and its square", "eps (python:float, optional) : term added to the denominator to improvenumerical stability (default: 1e-8)", "weight_decay (python:float, optional) : weight decay (L2 penalty) (default: 0)"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.optim.ASGD", "item_type": "class", "code": "classtorch.optim.ASGD(params,lr=0.01,lambd=0.0001,alpha=0.75,t0=1000000.0,weight_decay=0)", "description": "Implements Averaged Stochastic Gradient Descent. It has been proposed in Acceleration of stochastic approximation by averaging.  Parameters  params (iterable) \u2013 iterable of parameters to optimize or dicts defining parameter groups lr (python:float, optional) \u2013 learning rate (default: 1e-2) lambd (python:float, optional) \u2013 decay term (default: 1e-4) alpha (python:float, optional) \u2013 power for eta update (default: 0.75) t0 (python:float, optional) \u2013 point at which to start averaging (default: 1e6) weight_decay (python:float, optional) \u2013 weight decay (L2 penalty) (default: 0)      step(closure=None)  Performs a single optimization step.  Parameters closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.     ", "parameters": ["params (iterable) : iterable of parameters to optimize or dicts definingparameter groups", "lr (python:float, optional) : learning rate (default: 1e-2)", "lambd (python:float, optional) : decay term (default: 1e-4)", "alpha (python:float, optional) : power for eta update (default: 0.75)", "t0 (python:float, optional) : point at which to start averaging (default: 1e6)", "weight_decay (python:float, optional) : weight decay (L2 penalty) (default: 0)"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.optim.LBFGS", "item_type": "class", "code": "classtorch.optim.LBFGS(params,lr=1,max_iter=20,max_eval=None,tolerance_grad=1e-07,tolerance_change=1e-09,history_size=100,line_search_fn=None)", "description": "Implements L-BFGS algorithm, heavily inspired by minFunc &lt;https://www.cs.ubc.ca/~schmidtm/Software/minFunc.html&gt;.  Warning This optimizer doesn\u2019t support per-parameter options and parameter groups (there can be only one).   Warning Right now all parameters have to be on a single device. This will be improved in the future.   Note This is a very memory intensive optimizer (it requires additional param_bytes * (history_size + 1) bytes). If it doesn\u2019t fit in memory try reducing the history size, or use a different algorithm.   Parameters  lr (python:float) \u2013 learning rate (default: 1) max_iter (python:int) \u2013 maximal number of iterations per optimization step (default: 20) max_eval (python:int) \u2013 maximal number of function evaluations per optimization step (default: max_iter * 1.25). tolerance_grad (python:float) \u2013 termination tolerance on first order optimality (default: 1e-5). tolerance_change (python:float) \u2013 termination tolerance on function value/parameter changes (default: 1e-9). history_size (python:int) \u2013 update history size (default: 100). line_search_fn (str) \u2013 either \u2018strong_wolfe\u2019 or None (default: None).      step(closure)  Performs a single optimization step.  Parameters closure (callable) \u2013 A closure that reevaluates the model and returns the loss.     ", "parameters": ["lr (python:float) : learning rate (default: 1)", "max_iter (python:int) : maximal number of iterations per optimization step(default: 20)", "max_eval (python:int) : maximal number of function evaluations per optimizationstep (default: max_iter * 1.25).", "tolerance_grad (python:float) : termination tolerance on first order optimality(default: 1e-5).", "tolerance_change (python:float) : termination tolerance on functionvalue/parameter changes (default: 1e-9).", "history_size (python:int) : update history size (default: 100).", "line_search_fn (str) : either \u2018strong_wolfe\u2019 or None (default: None)."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.optim.RMSprop", "item_type": "class", "code": "classtorch.optim.RMSprop(params,lr=0.01,alpha=0.99,eps=1e-08,weight_decay=0,momentum=0,centered=False)", "description": "Implements RMSprop algorithm. Proposed by G. Hinton in his course. The centered version first appears in Generating Sequences With Recurrent Neural Networks. The implementation here takes the square root of the gradient average before adding epsilon (note that TensorFlow interchanges these two operations). The effective learning rate is thus \u03b1/(v+\u03f5)\\alpha/(\\sqrt{v} + \\epsilon)\u03b1/(v\u200b+\u03f5)   where \u03b1\\alpha\u03b1   is the scheduled learning rate and vvv   is the weighted moving average of the squared gradient.  Parameters  params (iterable) \u2013 iterable of parameters to optimize or dicts defining parameter groups lr (python:float, optional) \u2013 learning rate (default: 1e-2) momentum (python:float, optional) \u2013 momentum factor (default: 0) alpha (python:float, optional) \u2013 smoothing constant (default: 0.99) eps (python:float, optional) \u2013 term added to the denominator to improve numerical stability (default: 1e-8) centered (bool, optional) \u2013 if True, compute the centered RMSProp, the gradient is normalized by an estimation of its variance weight_decay (python:float, optional) \u2013 weight decay (L2 penalty) (default: 0)      step(closure=None)  Performs a single optimization step.  Parameters closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.     ", "parameters": ["params (iterable) : iterable of parameters to optimize or dicts definingparameter groups", "lr (python:float, optional) : learning rate (default: 1e-2)", "momentum (python:float, optional) : momentum factor (default: 0)", "alpha (python:float, optional) : smoothing constant (default: 0.99)", "eps (python:float, optional) : term added to the denominator to improvenumerical stability (default: 1e-8)", "centered (bool, optional) : if True, compute the centered RMSProp,the gradient is normalized by an estimation of its variance", "weight_decay (python:float, optional) : weight decay (L2 penalty) (default: 0)"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.optim.Rprop", "item_type": "class", "code": "classtorch.optim.Rprop(params,lr=0.01,etas=(0.5,1.2),step_sizes=(1e-06,50))", "description": "Implements the resilient backpropagation algorithm.  Parameters  params (iterable) \u2013 iterable of parameters to optimize or dicts defining parameter groups lr (python:float, optional) \u2013 learning rate (default: 1e-2) etas (Tuple[python:float, python:float], optional) \u2013 pair of (etaminus, etaplis), that are multiplicative increase and decrease factors (default: (0.5, 1.2)) step_sizes (Tuple[python:float, python:float], optional) \u2013 a pair of minimal and maximal allowed step sizes (default: (1e-6, 50))      step(closure=None)  Performs a single optimization step.  Parameters closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.     ", "parameters": ["params (iterable) : iterable of parameters to optimize or dicts definingparameter groups", "lr (python:float, optional) : learning rate (default: 1e-2)", "etas (Tuple[python:float, python:float], optional) : pair of (etaminus, etaplis), thatare multiplicative increase and decrease factors(default: (0.5, 1.2))", "step_sizes (Tuple[python:float, python:float], optional) : a pair of minimal andmaximal allowed step sizes (default: (1e-6, 50))"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.datasets.Kinetics400", "item_type": "class", "code": "classtorchvision.datasets.Kinetics400(root,frames_per_clip,step_between_clips=1,frame_rate=None,extensions=('avi',),transform=None,_precomputed_metadata=None,num_workers=1,_video_width=0,_video_height=0,_video_min_dimension=0,_audio_samples=0,_audio_channels=0)", "description": "Kinetics-400 dataset. Kinetics-400 is an action recognition video dataset. This dataset consider every video as a collection of video clips of fixed size, specified by frames_per_clip, where the step in frames between each clip is given by step_between_clips. To give an example, for 2 videos with 10 and 15 frames respectively, if frames_per_clip=5 and step_between_clips=5, the dataset size will be (2 + 3) = 5, where the first two elements will come from video 1, and the next three elements from video 2. Note that we drop clips which do not have exactly frames_per_clip elements, so not all frames in a video might be present. Internally, it uses a VideoClips object to handle clip creation.  Parameters  root (string) \u2013 Root directory of the Kinetics-400 Dataset. frames_per_clip (python:int) \u2013 number of frames in a clip step_between_clips (python:int) \u2013 number of frames between each clip transform (callable, optional) \u2013 A function/transform that  takes in a TxHxWxC video and returns a transformed version.   Returns the T video frames audio(Tensor[K, L]): the audio frames, where K is the number of channels  and L is the number of points  label (int): class of the video clip   Return type video (Tensor[T, H, W, C])   ", "parameters": ["root (string) : Root directory of the Kinetics-400 Dataset.", "frames_per_clip (python:int) : number of frames in a clip", "step_between_clips (python:int) : number of frames between each clip", "transform (callable, optional) : A function/transform that  takes in a TxHxWxC videoand returns a transformed version."], "returns": "the T video framesaudio(Tensor[K, L]): the audio frames, where K is the number of channelsand L is the number of pointslabel (int): class of the video clip", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.datasets.HMDB51", "item_type": "class", "code": "classtorchvision.datasets.HMDB51(root,annotation_path,frames_per_clip,step_between_clips=1,frame_rate=None,fold=1,train=True,transform=None,_precomputed_metadata=None,num_workers=1,_video_width=0,_video_height=0,_video_min_dimension=0,_audio_samples=0)", "description": "HMDB51 dataset. HMDB51 is an action recognition video dataset. This dataset consider every video as a collection of video clips of fixed size, specified by frames_per_clip, where the step in frames between each clip is given by step_between_clips. To give an example, for 2 videos with 10 and 15 frames respectively, if frames_per_clip=5 and step_between_clips=5, the dataset size will be (2 + 3) = 5, where the first two elements will come from video 1, and the next three elements from video 2. Note that we drop clips which do not have exactly frames_per_clip elements, so not all frames in a video might be present. Internally, it uses a VideoClips object to handle clip creation.  Parameters  root (string) \u2013 Root directory of the HMDB51 Dataset. annotation_path (str) \u2013 path to the folder containing the split files frames_per_clip (python:int) \u2013 number of frames in a clip. step_between_clips (python:int) \u2013 number of frames between each clip. fold (python:int, optional) \u2013 which fold to use. Should be between 1 and 3. train (bool, optional) \u2013 if True, creates a dataset from the train split, otherwise from the test split. transform (callable, optional) \u2013 A function/transform that  takes in a TxHxWxC video and returns a transformed version.   Returns the T video frames audio(Tensor[K, L]): the audio frames, where K is the number of channels  and L is the number of points  label (int): class of the video clip   Return type video (Tensor[T, H, W, C])   ", "parameters": ["root (string) : Root directory of the HMDB51 Dataset.", "annotation_path (str) : path to the folder containing the split files", "frames_per_clip (python:int) : number of frames in a clip.", "step_between_clips (python:int) : number of frames between each clip.", "fold (python:int, optional) : which fold to use. Should be between 1 and 3.", "train (bool, optional) : if True, creates a dataset from the train split,otherwise from the test split.", "transform (callable, optional) : A function/transform that  takes in a TxHxWxC videoand returns a transformed version."], "returns": "the T video framesaudio(Tensor[K, L]): the audio frames, where K is the number of channelsand L is the number of pointslabel (int): class of the video clip", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.datasets.UCF101", "item_type": "class", "code": "classtorchvision.datasets.UCF101(root,annotation_path,frames_per_clip,step_between_clips=1,frame_rate=None,fold=1,train=True,transform=None,_precomputed_metadata=None,num_workers=1,_video_width=0,_video_height=0,_video_min_dimension=0,_audio_samples=0)", "description": "UCF101 dataset. UCF101 is an action recognition video dataset. This dataset consider every video as a collection of video clips of fixed size, specified by frames_per_clip, where the step in frames between each clip is given by step_between_clips. To give an example, for 2 videos with 10 and 15 frames respectively, if frames_per_clip=5 and step_between_clips=5, the dataset size will be (2 + 3) = 5, where the first two elements will come from video 1, and the next three elements from video 2. Note that we drop clips which do not have exactly frames_per_clip elements, so not all frames in a video might be present. Internally, it uses a VideoClips object to handle clip creation.  Parameters  root (string) \u2013 Root directory of the UCF101 Dataset. annotation_path (str) \u2013 path to the folder containing the split files frames_per_clip (python:int) \u2013 number of frames in a clip. step_between_clips (python:int, optional) \u2013 number of frames between each clip. fold (python:int, optional) \u2013 which fold to use. Should be between 1 and 3. train (bool, optional) \u2013 if True, creates a dataset from the train split, otherwise from the test split. transform (callable, optional) \u2013 A function/transform that  takes in a TxHxWxC video and returns a transformed version.   Returns the T video frames audio(Tensor[K, L]): the audio frames, where K is the number of channels  and L is the number of points  label (int): class of the video clip   Return type video (Tensor[T, H, W, C])   ", "parameters": ["root (string) : Root directory of the UCF101 Dataset.", "annotation_path (str) : path to the folder containing the split files", "frames_per_clip (python:int) : number of frames in a clip.", "step_between_clips (python:int, optional) : number of frames between each clip.", "fold (python:int, optional) : which fold to use. Should be between 1 and 3.", "train (bool, optional) : if True, creates a dataset from the train split,otherwise from the test split.", "transform (callable, optional) : A function/transform that  takes in a TxHxWxC videoand returns a transformed version."], "returns": "the T video framesaudio(Tensor[K, L]): the audio frames, where K is the number of channelsand L is the number of pointslabel (int): class of the video clip", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.add_", "item_type": "method", "code": "add_(value)\u2192Tensor", "description": "add_(value=1, other) -&gt; Tensor In-place version of add() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.addbmm", "item_type": "method", "code": "addbmm(beta=1,alpha=1,batch1,batch2)\u2192Tensor", "description": "See torch.addbmm() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.addbmm_", "item_type": "method", "code": "addbmm_(beta=1,alpha=1,batch1,batch2)\u2192Tensor", "description": "In-place version of addbmm() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.addcdiv", "item_type": "method", "code": "addcdiv(value=1,tensor1,tensor2)\u2192Tensor", "description": "See torch.addcdiv() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.addcdiv_", "item_type": "method", "code": "addcdiv_(value=1,tensor1,tensor2)\u2192Tensor", "description": "In-place version of addcdiv() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.addcmul", "item_type": "method", "code": "addcmul(value=1,tensor1,tensor2)\u2192Tensor", "description": "See torch.addcmul() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.addcmul_", "item_type": "method", "code": "addcmul_(value=1,tensor1,tensor2)\u2192Tensor", "description": "In-place version of addcmul() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.addmm", "item_type": "method", "code": "addmm(beta=1,alpha=1,mat1,mat2)\u2192Tensor", "description": "See torch.addmm() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.addmm_", "item_type": "method", "code": "addmm_(beta=1,alpha=1,mat1,mat2)\u2192Tensor", "description": "In-place version of addmm() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.addmv", "item_type": "method", "code": "addmv(beta=1,alpha=1,mat,vec)\u2192Tensor", "description": "See torch.addmv() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.addmv_", "item_type": "method", "code": "addmv_(beta=1,alpha=1,mat,vec)\u2192Tensor", "description": "In-place version of addmv() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.unfold", "item_type": "function", "code": "torch.nn.functional.unfold(input,kernel_size,dilation=1,padding=0,stride=1)", "description": "Extracts sliding local blocks from an batched input tensor.  Warning Currently, only 4-D input tensors (batched image-like tensors) are supported.   Warning More than one element of the unfolded tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensor, please clone it first.  See torch.nn.Unfold for details ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.fold", "item_type": "function", "code": "torch.nn.functional.fold(input,output_size,kernel_size,dilation=1,padding=0,stride=1)", "description": "Combines an array of sliding local blocks into a large containing tensor.  Warning Currently, only 4-D output tensors (batched image-like tensors) are supported.  See torch.nn.Fold for details ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.avg_pool1d", "item_type": "function", "code": "torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\u2192Tensor", "description": "Applies a 1D average pooling over an input signal composed of several input planes. See AvgPool1d for details and output shape.  Parameters  input \u2013 input tensor of shape (minibatch,in_channels,iW)(\\text{minibatch} , \\text{in\\_channels} , iW)(minibatch,in_channels,iW)   kernel_size \u2013 the size of the window. Can be a single number or a tuple (kW,) stride \u2013 the stride of the window. Can be a single number or a tuple (sW,). Default: kernel_size padding \u2013 implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0 ceil_mode \u2013 when True, will use ceil instead of floor to compute the output shape. Default: False count_include_pad \u2013 when True, will include the zero-padding in the averaging calculation. Default: True    Examples: &gt;&gt;&gt; # pool of square window of size=3, stride=2 &gt;&gt;&gt; input = torch.tensor([[[1, 2, 3, 4, 5, 6, 7]]], dtype=torch.float32) &gt;&gt;&gt; F.avg_pool1d(input, kernel_size=3, stride=2) tensor([[[ 2.,  4.,  6.]]])   ", "parameters": ["input : input tensor of shape (minibatch,in_channels,iW)(\\text{minibatch} , \\text{in\\_channels} , iW)(minibatch,in_channels,iW)", "kernel_size : the size of the window. Can be a single number or atuple (kW,)", "stride : the stride of the window. Can be a single number or a tuple(sW,). Default: kernel_size", "padding : implicit zero paddings on both sides of the input. Can be asingle number or a tuple (padW,). Default: 0", "ceil_mode : when True, will use ceil instead of floor to compute theoutput shape. Default: False", "count_include_pad : when True, will include the zero-padding in theaveraging calculation. Default: True"], "returns": null, "example": " # pool of square window of size=3, stride=2\n input = torch.tensor([[[1, 2, 3, 4, 5, 6, 7]]], dtype=torch.float32)\n F.avg_pool1d(input, kernel_size=3, stride=2)\ntensor([[[ 2.,  4.,  6.]]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.avg_pool2d", "item_type": "function", "code": "torch.nn.functional.avg_pool2d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\u2192Tensor", "description": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW   regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW   steps. The number of output features is equal to the number of input planes. See AvgPool2d for details and output shape.  Parameters  input \u2013 input tensor (minibatch,in_channels,iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iH , iW)(minibatch,in_channels,iH,iW)   kernel_size \u2013 size of the pooling region. Can be a single number or a tuple (kH, kW) stride \u2013 stride of the pooling operation. Can be a single number or a tuple (sH, sW). Default: kernel_size padding \u2013 implicit zero paddings on both sides of the input. Can be a single number or a tuple (padH, padW). Default: 0 ceil_mode \u2013 when True, will use ceil instead of floor in the formula to compute the output shape. Default: False count_include_pad \u2013 when True, will include the zero-padding in the averaging calculation. Default: True divisor_override \u2013 if specified, it will be used as divisor, otherwise size of the pooling region will be used. Default: None    ", "parameters": ["input : input tensor (minibatch,in_channels,iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iH , iW)(minibatch,in_channels,iH,iW)", "kernel_size : size of the pooling region. Can be a single number or atuple (kH, kW)", "stride : stride of the pooling operation. Can be a single number or atuple (sH, sW). Default: kernel_size", "padding : implicit zero paddings on both sides of the input. Can be asingle number or a tuple (padH, padW). Default: 0", "ceil_mode : when True, will use ceil instead of floor in the formulato compute the output shape. Default: False", "count_include_pad : when True, will include the zero-padding in theaveraging calculation. Default: True", "divisor_override : if specified, it will be used as divisor, otherwisesize of the pooling region will be used. Default: None"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.avg_pool3d", "item_type": "function", "code": "torch.nn.functional.avg_pool3d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\u2192Tensor", "description": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW   regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW   steps. The number of output features is equal to \u230ainput\u00a0planessT\u230b\\lfloor\\frac{\\text{input planes}}{sT}\\rfloor\u230asTinput\u00a0planes\u200b\u230b  . See AvgPool3d for details and output shape.  Parameters  input \u2013 input tensor (minibatch,in_channels,iT\u00d7iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iT \\times iH , iW)(minibatch,in_channels,iT\u00d7iH,iW)   kernel_size \u2013 size of the pooling region. Can be a single number or a tuple (kT, kH, kW) stride \u2013 stride of the pooling operation. Can be a single number or a tuple (sT, sH, sW). Default: kernel_size padding \u2013 implicit zero paddings on both sides of the input. Can be a single number or a tuple (padT, padH, padW), Default: 0 ceil_mode \u2013 when True, will use ceil instead of floor in the formula to compute the output shape count_include_pad \u2013 when True, will include the zero-padding in the averaging calculation divisor_override \u2013 if specified, it will be used as divisor, otherwise size of the pooling region will be used. Default: None    ", "parameters": ["input : input tensor (minibatch,in_channels,iT\u00d7iH,iW)(\\text{minibatch} , \\text{in\\_channels} , iT \\times iH , iW)(minibatch,in_channels,iT\u00d7iH,iW)", "kernel_size : size of the pooling region. Can be a single number or atuple (kT, kH, kW)", "stride : stride of the pooling operation. Can be a single number or atuple (sT, sH, sW). Default: kernel_size", "padding : implicit zero paddings on both sides of the input. Can be asingle number or a tuple (padT, padH, padW), Default: 0", "ceil_mode : when True, will use ceil instead of floor in the formulato compute the output shape", "count_include_pad : when True, will include the zero-padding in theaveraging calculation", "divisor_override : if specified, it will be used as divisor, otherwisesize of the pooling region will be used. Default: None"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.RandomGrayscale", "item_type": "class", "code": "classtorchvision.transforms.RandomGrayscale(p=0.1)", "description": "Randomly convert image to grayscale with a probability of p (default 0.1).  Parameters p (python:float) \u2013 probability that image should be converted to grayscale.  Returns Grayscale version of the input image with probability p and unchanged with probability (1-p). - If input image is 1 channel: grayscale version is 1 channel - If input image is 3 channel: grayscale version is 3 channel with r == g == b  Return type PIL Image   ", "parameters": ["p (python:float) : probability that image should be converted to grayscale.", "Grayscale version of the input image with probability p and unchangedwith probability (1-p).- If input image is 1 channel: grayscale version is 1 channel- If input image is 3 channel: grayscale version is 3 channel with r == g == b", "PIL Image"], "returns": "Grayscale version of the input image with probability p and unchangedwith probability (1-p).- If input image is 1 channel: grayscale version is 1 channel- If input image is 3 channel: grayscale version is 3 channel with r == g == b", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.RandomHorizontalFlip", "item_type": "class", "code": "classtorchvision.transforms.RandomHorizontalFlip(p=0.5)", "description": "Horizontally flip the given PIL Image randomly with a given probability.  Parameters p (python:float) \u2013 probability of the image being flipped. Default value is 0.5   ", "parameters": ["p (python:float) : probability of the image being flipped. Default value is 0.5"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.RandomOrder", "item_type": "class", "code": "classtorchvision.transforms.RandomOrder(transforms)", "description": "Apply a list of transformations in a random order ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.RandomPerspective", "item_type": "class", "code": "classtorchvision.transforms.RandomPerspective(distortion_scale=0.5,p=0.5,interpolation=3)", "description": "Performs Perspective transformation of the given PIL Image randomly with a given probability.  Parameters  interpolation \u2013 Default- Image.BICUBIC p (python:float) \u2013 probability of the image being perspectively transformed. Default value is 0.5 distortion_scale (python:float) \u2013 it controls the degree of distortion and ranges from 0 to 1. Default value is 0.5.    ", "parameters": ["interpolation : Default- Image.BICUBIC", "p (python:float) : probability of the image being perspectively transformed. Default value is 0.5", "distortion_scale (python:float) : it controls the degree of distortion and ranges from 0 to 1. Default value is 0.5."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.RandomResizedCrop", "item_type": "class", "code": "classtorchvision.transforms.RandomResizedCrop(size,scale=(0.08,1.0),ratio=(0.75,1.3333333333333333),interpolation=2)", "description": "Crop the given PIL Image to random size and aspect ratio. A crop of random size (default: of 0.08 to 1.0) of the original size and a random aspect ratio (default: of 3/4 to 4/3) of the original aspect ratio is made. This crop is finally resized to given size. This is popularly used to train the Inception networks.  Parameters  size \u2013 expected output size of each edge scale \u2013 range of size of the origin size cropped ratio \u2013 range of aspect ratio of the origin aspect ratio cropped interpolation \u2013 Default: PIL.Image.BILINEAR    ", "parameters": ["size : expected output size of each edge", "scale : range of size of the origin size cropped", "ratio : range of aspect ratio of the origin aspect ratio cropped", "interpolation : Default: PIL.Image.BILINEAR"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.RandomRotation", "item_type": "class", "code": "classtorchvision.transforms.RandomRotation(degrees,resample=False,expand=False,center=None,fill=0)", "description": "Rotate the image by angle.  Parameters  degrees (sequence or python:float or python:int) \u2013 Range of degrees to select from. If degrees is a number instead of sequence like (min, max), the range of degrees will be (-degrees, +degrees). resample ({PIL.Image.NEAREST, PIL.Image.BILINEAR, PIL.Image.BICUBIC}, optional) \u2013 An optional resampling filter. See filters for more information. If omitted, or if the image has mode \u201c1\u201d or \u201cP\u201d, it is set to PIL.Image.NEAREST. expand (bool, optional) \u2013 Optional expansion flag. If true, expands the output to make it large enough to hold the entire rotated image. If false or omitted, make the output image the same size as the input image. Note that the expand flag assumes rotation around the center and no translation. center (2-tuple, optional) \u2013 Optional center of rotation. Origin is the upper left corner. Default is the center of the image. fill (3-tuple or python:int) \u2013 RGB pixel fill value for area outside the rotated image. If int, it is used for all channels respectively.    ", "parameters": ["degrees (sequence or python:float or python:int) : Range of degrees to select from.If degrees is a number instead of sequence like (min, max), the range of degreeswill be (-degrees, +degrees).", "resample ({PIL.Image.NEAREST, PIL.Image.BILINEAR, PIL.Image.BICUBIC}, optional) : An optional resampling filter. See filters for more information.If omitted, or if the image has mode \u201c1\u201d or \u201cP\u201d, it is set to PIL.Image.NEAREST.", "expand (bool, optional) : Optional expansion flag.If true, expands the output to make it large enough to hold the entire rotated image.If false or omitted, make the output image the same size as the input image.Note that the expand flag assumes rotation around the center and no translation.", "center (2-tuple, optional) : Optional center of rotation.Origin is the upper left corner.Default is the center of the image.", "fill (3-tuple or python:int) : RGB pixel fill value for area outside the rotated image.If int, it is used for all channels respectively."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.RandomSizedCrop", "item_type": "class", "code": "classtorchvision.transforms.RandomSizedCrop(*args,**kwargs)", "description": "Note: This transform is deprecated in favor of RandomResizedCrop. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.RandomVerticalFlip", "item_type": "class", "code": "classtorchvision.transforms.RandomVerticalFlip(p=0.5)", "description": "Vertically flip the given PIL Image randomly with a given probability.  Parameters p (python:float) \u2013 probability of the image being flipped. Default value is 0.5   ", "parameters": ["p (python:float) : probability of the image being flipped. Default value is 0.5"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.is_tensor", "item_type": "function", "code": "torch.is_tensor(obj)", "description": "Returns True if obj is a PyTorch tensor.  Parameters obj (Object) \u2013 Object to test   ", "parameters": ["obj (Object) : Object to test"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.beta.Beta.mean", "item_type": "method", "code": "propertymean", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.beta.Beta.rsample", "item_type": "method", "code": "rsample(sample_shape=())", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.beta.Beta.variance", "item_type": "method", "code": "propertyvariance", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.binomial.Binomial.enumerate_support", "item_type": "method", "code": "enumerate_support(expand=True)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.binomial.Binomial.expand", "item_type": "method", "code": "expand(batch_shape,_instance=None)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.binomial.Binomial.log_prob", "item_type": "method", "code": "log_prob(value)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.binomial.Binomial.mean", "item_type": "method", "code": "propertymean", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.binomial.Binomial.param_shape", "item_type": "method", "code": "propertyparam_shape", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.binomial.Binomial.sample", "item_type": "method", "code": "sample(sample_shape=torch.Size([]))", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.binomial.Binomial.support", "item_type": "method", "code": "propertysupport", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.binomial.Binomial.variance", "item_type": "method", "code": "propertyvariance", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.categorical.Categorical.entropy", "item_type": "method", "code": "entropy()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributed.Backend", "item_type": "class", "code": "classtorch.distributed.Backend", "description": "An enum-like class of available backends: GLOO, NCCL, and MPI. The values of this class are lowercase strings, e.g., \"gloo\". They can be accessed as attributes, e.g., Backend.NCCL. This class can be directly called to parse the string, e.g., Backend(backend_str) will check if backend_str is valid, and return the parsed lowercase string if so. It also accepts uppercase strings, e.g., Backend(\"GLOO\") returns \"gloo\".  Note The entry Backend.UNDEFINED is present but only used as initial value of some fields. Users should neither use it directly nor assume its existence.  ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributed.ReduceOp", "item_type": "class", "code": "classtorch.distributed.ReduceOp", "description": "An enum-like class for available reduction operations: SUM, PRODUCT, MIN, MAX, BAND, BOR, and BXOR. The values of this class can be accessed as attributes, e.g., ReduceOp.SUM. They are used in specifying strategies for reduction collectives, e.g., reduce(), all_reduce_multigpu(), etc. Members:  SUM PRODUCT MIN MAX BAND BOR BXOR  ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributed.reduce_op", "item_type": "class", "code": "classtorch.distributed.reduce_op", "description": "Deprecated enum-like class for reduction operations: SUM, PRODUCT, MIN, and MAX. ReduceOp is recommended to use instead. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.empty_cache", "item_type": "function", "code": "torch.cuda.empty_cache()", "description": "Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.  Note empty_cache() doesn\u2019t increase the amount of GPU memory available for PyTorch. However, it may help reduce fragmentation of GPU memory in certain cases. See Memory management for more details about GPU memory management.  ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.memory_stats", "item_type": "function", "code": "torch.cuda.memory_stats(device=None)", "description": "Returns a dictionary of CUDA memory allocator statistics for a given device. The return value of this function is a dictionary of statistics, each of which is a non-negative integer. Core statistics:  \"allocated.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": number of allocation requests received by the memory allocator. \"allocated_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": amount of allocated memory. \"segment.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": number of reserved segments from cudaMalloc(). \"reserved_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": amount of reserved memory. \"active.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": number of active memory blocks. \"active_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": amount of active memory. \"inactive_split.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": number of inactive, non-releasable memory blocks. \"inactive_split_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}\": amount of inactive, non-releasable memory.  For these core statistics, values are broken down as follows. Pool type:  all: combined statistics across all memory pools. large_pool: statistics for the large allocation pool (as of October 2019, for size &gt;= 1MB allocations). small_pool: statistics for the small allocation pool (as of October 2019, for size &lt; 1MB allocations).  Metric type:  current: current value of this metric. peak: maximum value of this metric. allocated: historical total increase in this metric. freed: historical total decrease in this metric.  In addition to the core statistics, we also provide some simple event counters:  \"num_alloc_retries\": number of failed cudaMalloc calls that result in a cache flush and retry. \"num_ooms\": number of out-of-memory errors thrown.   Parameters device (torch.device or python:int, optional) \u2013 selected device. Returns statistics for the current device, given by current_device(), if device is None (default).    Note See Memory management for more details about GPU memory management.  ", "parameters": ["device (torch.device or python:int, optional) : selected device. Returnsstatistics for the current device, given by current_device(),if device is None (default)."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.memory_summary", "item_type": "function", "code": "torch.cuda.memory_summary(device=None,abbreviated=False)", "description": "Returns a human-readable printout of the current memory allocator statistics for a given device. This can be useful to display periodically during training, or when handling out-of-memory exceptions.  Parameters  device (torch.device or python:int, optional) \u2013 selected device. Returns printout for the current device, given by current_device(), if device is None (default). abbreviated (bool, optional) \u2013 whether to return an abbreviated summary (default: False).     Note See Memory management for more details about GPU memory management.  ", "parameters": ["device (torch.device or python:int, optional) : selected device. Returnsprintout for the current device, given by current_device(),if device is None (default).", "abbreviated (bool, optional) : whether to return an abbreviated summary(default: False)."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.memory_snapshot", "item_type": "function", "code": "torch.cuda.memory_snapshot()", "description": "Returns a snapshot of the CUDA memory allocator state across all devices. Interpreting the output of this function requires familiarity with the memory allocator internals.  Note See Memory management for more details about GPU memory management.  ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.memory_allocated", "item_type": "function", "code": "torch.cuda.memory_allocated(device=None)", "description": "Returns the current GPU memory occupied by tensors in bytes for a given device.  Parameters device (torch.device or python:int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).    Note This is likely less than the amount shown in nvidia-smi since some unused memory can be held by the caching allocator and some context needs to be created on GPU. See Memory management for more details about GPU memory management.  ", "parameters": ["device (torch.device or python:int, optional) : selected device. Returnsstatistic for the current device, given by current_device(),if device is None (default)."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.max_memory_allocated", "item_type": "function", "code": "torch.cuda.max_memory_allocated(device=None)", "description": "Returns the maximum GPU memory occupied by tensors in bytes for a given device. By default, this returns the peak allocated memory since the beginning of this program. reset_peak_stats() can be used to reset the starting point in tracking this metric. For example, these two functions can measure the peak allocated memory usage of each iteration in a training loop.  Parameters device (torch.device or python:int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).    Note See Memory management for more details about GPU memory management.  ", "parameters": ["device (torch.device or python:int, optional) : selected device. Returnsstatistic for the current device, given by current_device(),if device is None (default)."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.reset_max_memory_allocated", "item_type": "function", "code": "torch.cuda.reset_max_memory_allocated(device=None)", "description": "Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device. See max_memory_allocated() for details.  Parameters device (torch.device or python:int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).    Warning This function now calls reset_peak_memory_stats(), which resets /all/ peak memory stats.   Note See Memory management for more details about GPU memory management.  ", "parameters": ["device (torch.device or python:int, optional) : selected device. Returnsstatistic for the current device, given by current_device(),if device is None (default)."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.autograd.enable_grad", "item_type": "class", "code": "classtorch.autograd.enable_grad", "description": "Context-manager that enables gradient calculation. Enables gradient calculation, if it has been disabled via no_grad or set_grad_enabled. This context manager is thread local; it will not affect computation in other threads. Also functions as a decorator. Example: &gt;&gt;&gt; x = torch.tensor([1], requires_grad=True) &gt;&gt;&gt; with torch.no_grad(): ...   with torch.enable_grad(): ...     y = x * 2 &gt;&gt;&gt; y.requires_grad True &gt;&gt;&gt; y.backward() &gt;&gt;&gt; x.grad &gt;&gt;&gt; @torch.enable_grad() ... def doubler(x): ...     return x * 2 &gt;&gt;&gt; with torch.no_grad(): ...     z = doubler(x) &gt;&gt;&gt; z.requires_grad True   ", "parameters": [], "returns": null, "example": " x = torch.tensor([1], requires_grad=True)\n with torch.no_grad():\n...   with torch.enable_grad():\n...     y = x * 2\n y.requires_grad\nTrue\n y.backward()\n x.grad\n @torch.enable_grad()\n... def doubler(x):\n...     return x * 2\n with torch.no_grad():\n...     z = doubler(x)\n z.requires_grad\nTrue\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.autograd.set_grad_enabled", "item_type": "class", "code": "classtorch.autograd.set_grad_enabled(mode)", "description": "Context-manager that sets gradient calculation to on or off. set_grad_enabled will enable or disable grads based on its argument mode. It can be used as a context-manager or as a function. When using enable_grad context manager, set_grad_enabled(False) has no effect. This context manager is thread local; it will not affect computation in other threads.  Parameters mode (bool) \u2013 Flag whether to enable grad (True), or disable (False). This can be used to conditionally enable gradients.   Example: &gt;&gt;&gt; x = torch.tensor([1], requires_grad=True) &gt;&gt;&gt; is_train = False &gt;&gt;&gt; with torch.set_grad_enabled(is_train): ...   y = x * 2 &gt;&gt;&gt; y.requires_grad False &gt;&gt;&gt; torch.set_grad_enabled(True) &gt;&gt;&gt; y = x * 2 &gt;&gt;&gt; y.requires_grad True &gt;&gt;&gt; torch.set_grad_enabled(False) &gt;&gt;&gt; y = x * 2 &gt;&gt;&gt; y.requires_grad False   ", "parameters": ["mode (bool) : Flag whether to enable grad (True), or disable(False). This can be used to conditionally enablegradients."], "returns": null, "example": " x = torch.tensor([1], requires_grad=True)\n is_train = False\n with torch.set_grad_enabled(is_train):\n...   y = x * 2\n y.requires_grad\nFalse\n torch.set_grad_enabled(True)\n y = x * 2\n y.requires_grad\nTrue\n torch.set_grad_enabled(False)\n y = x * 2\n y.requires_grad\nFalse\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.quantized.dynamic.Linear.from_float", "item_type": "method", "code": "classmethodfrom_float(mod)", "description": "Create a dynamic quantized module from a float module or qparams_dict  Parameters mod (Module) \u2013 a float module, either produced by torch.quantization utilities or provided by the user   ", "parameters": ["mod (Module) : a float module, either produced by torch.quantizationutilities or provided by the user"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.quantization.QConfig", "item_type": "class", "code": "classtorch.quantization.QConfig", "description": "Describes how to quantize a layer or a part of the network by providing settings (observer classes) for activations and weights respectively. Note that QConfig needs to contain observer classes (like MinMaxObserver) or a callable that returns instances on invocation, not the concrete observer instances themselves. Quantization preparation function will instantiate observers multiple times for each of the layers. Observer classes have usually reasonable default arguments, but they can be overwritten with with_args method (that behaves like functools.partial):  my_qconfig = QConfig(activation=MinMaxObserver.with_args(dtype=torch.qint8), weight=default_observer.with_args(dtype=torch.qint8))  ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.quantization.QConfigDynamic", "item_type": "class", "code": "classtorch.quantization.QConfigDynamic", "description": "Describes how to dynamically quantize a layer or a part of the network by providing settings (observer classe) for weights. It\u2019s like QConfig, but for dynamic quantization. Note that QConfigDynamic needs to contain observer classes (like MinMaxObserver) or a callable that returns instances on invocation, not the concrete observer instances themselves. Quantization function will instantiate observers multiple times for each of the layers. Observer classes have usually reasonable default arguments, but they can be overwritten with with_args method (that behaves like functools.partial):  my_qconfig = QConfigDynamic(weight=default_observer.with_args(dtype=torch.qint8))  ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.quantization.QuantStub", "item_type": "class", "code": "classtorch.quantization.QuantStub(qconfig=None)", "description": "Quantize stub module, before calibration, this is same as an observer, it will be swapped as nnq.Quantize in convert.  Parameters qconfig \u2013 quantization configuration for the tensor, if qconfig is not provided, we will get qconfig from parent modules   ", "parameters": ["qconfig : quantization configuration for the tensor,if qconfig is not provided, we will get qconfig from parent modules"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.quantization.DeQuantStub", "item_type": "class", "code": "classtorch.quantization.DeQuantStub", "description": "Dequantize stub module, before calibration, this is same as identity, this will be swapped as nnq.DeQuantize in convert. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.quantization.QuantWrapper", "item_type": "class", "code": "classtorch.quantization.QuantWrapper(module)", "description": "A wrapper class that wraps the input module, adds QuantStub and DeQuantStub and surround the call to module with call to quant and dequant modules. This is used by the quantization utility functions to add the quant and dequant modules, before convert function QuantStub will just be observer, it observes the input tensor, after convert, QuantStub will be swapped to nnq.Quantize which does actual quantization. Similarly for DeQuantStub. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.quantization.MinMaxObserver", "item_type": "class", "code": "classtorch.quantization.MinMaxObserver(dtype=torch.quint8,qscheme=torch.per_tensor_affine,reduce_range=False)", "description": "Observer module for computing the quantization parameters based on the running min and max values. This observer uses the tensor min/max statistics to compute the quantization parameters. The module records the running minimum and maximum of incoming tensors, and uses this statistic to compute the quantization parameters.  Parameters  dtype \u2013 Quantized data type qscheme \u2013 Quantization scheme to be used reduce_range \u2013 Reduces the range of the quantized data type by 1 bit    Given running min/max as xminx_\\text{min}xmin\u200b   and xmaxx_\\text{max}xmax\u200b  , scale sss   and zero point zzz   are computed as: The running minimum/maximum xmin/maxx_\\text{min/max}xmin/max\u200b   is computed as:  xmin={min\u2061(X)if\u00a0xmin=Nonemin\u2061(xmin,min\u2061(X))otherwisexmax={max\u2061(X)if\u00a0xmax=Nonemax\u2061(xmax,max\u2061(X))otherwise\\begin{array}{ll} x_\\text{min} &amp;= \\begin{cases}     \\min(X) &amp; \\text{if~}x_\\text{min} = \\text{None} \\\\     \\min\\left(x_\\text{min}, \\min(X)\\right) &amp; \\text{otherwise} \\end{cases}\\\\ x_\\text{max} &amp;= \\begin{cases}     \\max(X) &amp; \\text{if~}x_\\text{max} = \\text{None} \\\\     \\max\\left(x_\\text{max}, \\max(X)\\right) &amp; \\text{otherwise} \\end{cases}\\\\ \\end{array}xmin\u200bxmax\u200b\u200b={min(X)min(xmin\u200b,min(X))\u200bif\u00a0xmin\u200b=Noneotherwise\u200b={max(X)max(xmax\u200b,max(X))\u200bif\u00a0xmax\u200b=Noneotherwise\u200b\u200b  where XXX   is the observed tensor. The scale sss   and zero point zzz   are then computed as:  if\u00a0Symmetric:s=2max\u2061(\u2223xmin\u2223,xmax)/(Qmax\u2212Qmin)z={0if\u00a0dtype\u00a0is\u00a0qint8128otherwiseOtherwise:s=(xmax\u2212xmin)/(Qmax\u2212Qmin)z=Qmin\u2212round(xmin/s)\\begin{aligned}     \\text{if Symmetric:}&amp;\\\\     &amp;s = 2 \\max(|x_\\text{min}|, x_\\text{max}) /         \\left( Q_\\text{max} - Q_\\text{min} \\right) \\\\     &amp;z = \\begin{cases}         0 &amp; \\text{if dtype is qint8} \\\\         128 &amp; \\text{otherwise}     \\end{cases}\\\\     \\text{Otherwise:}&amp;\\\\         &amp;s = \\left( x_\\text{max} - x_\\text{min}  \\right ) /             \\left( Q_\\text{max} - Q_\\text{min} \\right ) \\\\         &amp;z = Q_\\text{min} - \\text{round}(x_\\text{min} / s) \\end{aligned}if\u00a0Symmetric:Otherwise:\u200bs=2max(\u2223xmin\u200b\u2223,xmax\u200b)/(Qmax\u200b\u2212Qmin\u200b)z={0128\u200bif\u00a0dtype\u00a0is\u00a0qint8otherwise\u200bs=(xmax\u200b\u2212xmin\u200b)/(Qmax\u200b\u2212Qmin\u200b)z=Qmin\u200b\u2212round(xmin\u200b/s)\u200b  where QminQ_\\text{min}Qmin\u200b   and QmaxQ_\\text{max}Qmax\u200b   are the minimum and maximum of the quantized data type.  Warning Only works with torch.per_tensor_symmetric quantization scheme   Warning dtype can only take torch.qint8 or torch.quint8.   Note If the running minimum equals to the running maximum, the scale and zero_point are set to 1.0 and 0.  ", "parameters": ["dtype : Quantized data type", "qscheme : Quantization scheme to be used", "reduce_range : Reduces the range of the quantized data type by 1 bit"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.optim.SGD", "item_type": "class", "code": "classtorch.optim.SGD(params,lr=&lt;requiredparameter&gt;,momentum=0,dampening=0,weight_decay=0,nesterov=False)", "description": "Implements stochastic gradient descent (optionally with momentum). Nesterov momentum is based on the formula from On the importance of initialization and momentum in deep learning.  Parameters  params (iterable) \u2013 iterable of parameters to optimize or dicts defining parameter groups lr (python:float) \u2013 learning rate momentum (python:float, optional) \u2013 momentum factor (default: 0) weight_decay (python:float, optional) \u2013 weight decay (L2 penalty) (default: 0) dampening (python:float, optional) \u2013 dampening for momentum (default: 0) nesterov (bool, optional) \u2013 enables Nesterov momentum (default: False)    Example &gt;&gt;&gt; optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9) &gt;&gt;&gt; optimizer.zero_grad() &gt;&gt;&gt; loss_fn(model(input), target).backward() &gt;&gt;&gt; optimizer.step()    Note The implementation of SGD with Momentum/Nesterov subtly differs from Sutskever et. al. and implementations in some other frameworks. Considering the specific case of Momentum, the update can be written as  vt+1=\u03bc\u2217vt+gt+1pt+1=pt\u2212lr\u2217vt+1v_{t+1} = \\mu * v_{t} + g_{t+1} \\\\ p_{t+1} = p_{t} - lr * v_{t+1}  vt+1\u200b=\u03bc\u2217vt\u200b+gt+1\u200bpt+1\u200b=pt\u200b\u2212lr\u2217vt+1\u200b  where p, g, v and \u03bc\\mu\u03bc   denote the parameters, gradient, velocity, and momentum respectively. This is in contrast to Sutskever et. al. and other frameworks which employ an update of the form  vt+1=\u03bc\u2217vt+lr\u2217gt+1pt+1=pt\u2212vt+1v_{t+1} = \\mu * v_{t} + lr * g_{t+1} \\\\ p_{t+1} = p_{t} - v_{t+1}  vt+1\u200b=\u03bc\u2217vt\u200b+lr\u2217gt+1\u200bpt+1\u200b=pt\u200b\u2212vt+1\u200b  The Nesterov version is analogously modified.    step(closure=None)  Performs a single optimization step.  Parameters closure (callable, optional) \u2013 A closure that reevaluates the model and returns the loss.     ", "parameters": ["params (iterable) : iterable of parameters to optimize or dicts definingparameter groups", "lr (python:float) : learning rate", "momentum (python:float, optional) : momentum factor (default: 0)", "weight_decay (python:float, optional) : weight decay (L2 penalty) (default: 0)", "dampening (python:float, optional) : dampening for momentum (default: 0)", "nesterov (bool, optional) : enables Nesterov momentum (default: False)"], "returns": null, "example": " optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n optimizer.zero_grad()\n loss_fn(model(input), target).backward()\n optimizer.step()\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.optim.lr_scheduler.LambdaLR", "item_type": "class", "code": "classtorch.optim.lr_scheduler.LambdaLR(optimizer,lr_lambda,last_epoch=-1)", "description": "Sets the learning rate of each parameter group to the initial lr times a given function. When last_epoch=-1, sets initial lr as lr.  Parameters  optimizer (Optimizer) \u2013 Wrapped optimizer. lr_lambda (function or list) \u2013 A function which computes a multiplicative factor given an integer parameter epoch, or a list of such functions, one for each group in optimizer.param_groups. last_epoch (python:int) \u2013 The index of last epoch. Default: -1.    Example &gt;&gt;&gt; # Assuming optimizer has two groups. &gt;&gt;&gt; lambda1 = lambda epoch: epoch // 30 &gt;&gt;&gt; lambda2 = lambda epoch: 0.95 ** epoch &gt;&gt;&gt; scheduler = LambdaLR(optimizer, lr_lambda=[lambda1, lambda2]) &gt;&gt;&gt; for epoch in range(100): &gt;&gt;&gt;     train(...) &gt;&gt;&gt;     validate(...) &gt;&gt;&gt;     scheduler.step()     load_state_dict(state_dict)  Loads the schedulers state.  Parameters state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict().       state_dict()  Returns the state of the scheduler as a dict. It contains an entry for every variable in self.__dict__ which is not the optimizer. The learning rate lambda functions will only be saved if they are callable objects and not if they are functions or lambdas.   ", "parameters": ["optimizer (Optimizer) : Wrapped optimizer.", "lr_lambda (function or list) : A function which computes a multiplicativefactor given an integer parameter epoch, or a list of suchfunctions, one for each group in optimizer.param_groups.", "last_epoch (python:int) : The index of last epoch. Default: -1."], "returns": null, "example": " # Assuming optimizer has two groups.\n lambda1 = lambda epoch: epoch // 30\n lambda2 = lambda epoch: 0.95 ** epoch\n scheduler = LambdaLR(optimizer, lr_lambda=[lambda1, lambda2])\n for epoch in range(100):\n     train(...)\n     validate(...)\n     scheduler.step()\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.optim.lr_scheduler.MultiplicativeLR", "item_type": "class", "code": "classtorch.optim.lr_scheduler.MultiplicativeLR(optimizer,lr_lambda,last_epoch=-1)", "description": "Multiply the learning rate of each parameter group by the factor given in the specified function. When last_epoch=-1, sets initial lr as lr.  Parameters  optimizer (Optimizer) \u2013 Wrapped optimizer. lr_lambda (function or list) \u2013 A function which computes a multiplicative factor given an integer parameter epoch, or a list of such functions, one for each group in optimizer.param_groups. last_epoch (python:int) \u2013 The index of last epoch. Default: -1.    Example &gt;&gt;&gt; # Assuming optimizer has two groups. &gt;&gt;&gt; lmbda = lambda epoch: 0.95 &gt;&gt;&gt; scheduler = LambdaLR(optimizer, lr_lambda=lmbda) &gt;&gt;&gt; for epoch in range(100): &gt;&gt;&gt;     train(...) &gt;&gt;&gt;     validate(...) &gt;&gt;&gt;     scheduler.step()     load_state_dict(state_dict)  Loads the schedulers state.  Parameters state_dict (dict) \u2013 scheduler state. Should be an object returned from a call to state_dict().       state_dict()  Returns the state of the scheduler as a dict. It contains an entry for every variable in self.__dict__ which is not the optimizer. The learning rate lambda functions will only be saved if they are callable objects and not if they are functions or lambdas.   ", "parameters": ["optimizer (Optimizer) : Wrapped optimizer.", "lr_lambda (function or list) : A function which computes a multiplicativefactor given an integer parameter epoch, or a list of suchfunctions, one for each group in optimizer.param_groups.", "last_epoch (python:int) : The index of last epoch. Default: -1."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.optim.lr_scheduler.StepLR", "item_type": "class", "code": "classtorch.optim.lr_scheduler.StepLR(optimizer,step_size,gamma=0.1,last_epoch=-1)", "description": "Decays the learning rate of each parameter group by gamma every step_size epochs. Notice that such decay can happen simultaneously with other changes to the learning rate from outside this scheduler. When last_epoch=-1, sets initial lr as lr.  Parameters  optimizer (Optimizer) \u2013 Wrapped optimizer. step_size (python:int) \u2013 Period of learning rate decay. gamma (python:float) \u2013 Multiplicative factor of learning rate decay. Default: 0.1. last_epoch (python:int) \u2013 The index of last epoch. Default: -1.    Example &gt;&gt;&gt; # Assuming optimizer uses lr = 0.05 for all groups &gt;&gt;&gt; # lr = 0.05     if epoch &lt; 30 &gt;&gt;&gt; # lr = 0.005    if 30 &lt;= epoch &lt; 60 &gt;&gt;&gt; # lr = 0.0005   if 60 &lt;= epoch &lt; 90 &gt;&gt;&gt; # ... &gt;&gt;&gt; scheduler = StepLR(optimizer, step_size=30, gamma=0.1) &gt;&gt;&gt; for epoch in range(100): &gt;&gt;&gt;     train(...) &gt;&gt;&gt;     validate(...) &gt;&gt;&gt;     scheduler.step()   ", "parameters": ["optimizer (Optimizer) : Wrapped optimizer.", "step_size (python:int) : Period of learning rate decay.", "gamma (python:float) : Multiplicative factor of learning rate decay.Default: 0.1.", "last_epoch (python:int) : The index of last epoch. Default: -1."], "returns": null, "example": " # Assuming optimizer uses lr = 0.05 for all groups\n # lr = 0.05     if epoch &lt; 30\n # lr = 0.005    if 30 &lt;= epoch &lt; 60\n # lr = 0.0005   if 60 &lt;= epoch &lt; 90\n # ...\n scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n for epoch in range(100):\n     train(...)\n     validate(...)\n     scheduler.step()\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.addr", "item_type": "method", "code": "addr(beta=1,alpha=1,vec1,vec2)\u2192Tensor", "description": "See torch.addr() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.addr_", "item_type": "method", "code": "addr_(beta=1,alpha=1,vec1,vec2)\u2192Tensor", "description": "In-place version of addr() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.allclose", "item_type": "method", "code": "allclose(other,rtol=1e-05,atol=1e-08,equal_nan=False)\u2192Tensor", "description": "See torch.allclose() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.angle", "item_type": "method", "code": "angle()\u2192Tensor", "description": "See torch.angle() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.apply_", "item_type": "method", "code": "apply_(callable)\u2192Tensor", "description": "Applies the function callable to each element in the tensor, replacing each element with the value returned by callable.  Note This function only works with CPU tensors and should not be used in code sections that require high performance.  ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.argmax", "item_type": "method", "code": "argmax(dim=None,keepdim=False)\u2192LongTensor", "description": "See torch.argmax() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.argmin", "item_type": "method", "code": "argmin(dim=None,keepdim=False)\u2192LongTensor", "description": "See torch.argmin() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.argsort", "item_type": "method", "code": "argsort(dim=-1,descending=False)\u2192LongTensor", "description": "See :func: torch.argsort ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.asin", "item_type": "method", "code": "asin()\u2192Tensor", "description": "See torch.asin() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.asin_", "item_type": "method", "code": "asin_()\u2192Tensor", "description": "In-place version of asin() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.as_strided", "item_type": "method", "code": "as_strided(size,stride,storage_offset=0)\u2192Tensor", "description": "See torch.as_strided() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.atan", "item_type": "method", "code": "atan()\u2192Tensor", "description": "See torch.atan() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.atan2", "item_type": "method", "code": "atan2(other)\u2192Tensor", "description": "See torch.atan2() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.max_pool1d", "item_type": "function", "code": "torch.nn.functional.max_pool1d(*args,**kwargs)", "description": "Applies a 1D max pooling over an input signal composed of several input planes. See MaxPool1d for details. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.max_pool2d", "item_type": "function", "code": "torch.nn.functional.max_pool2d(*args,**kwargs)", "description": "Applies a 2D max pooling over an input signal composed of several input planes. See MaxPool2d for details. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.max_pool3d", "item_type": "function", "code": "torch.nn.functional.max_pool3d(*args,**kwargs)", "description": "Applies a 3D max pooling over an input signal composed of several input planes. See MaxPool3d for details. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.max_unpool1d", "item_type": "function", "code": "torch.nn.functional.max_unpool1d(input,indices,kernel_size,stride=None,padding=0,output_size=None)", "description": "Computes a partial inverse of MaxPool1d. See MaxUnpool1d for details. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.max_unpool2d", "item_type": "function", "code": "torch.nn.functional.max_unpool2d(input,indices,kernel_size,stride=None,padding=0,output_size=None)", "description": "Computes a partial inverse of MaxPool2d. See MaxUnpool2d for details. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.max_unpool3d", "item_type": "function", "code": "torch.nn.functional.max_unpool3d(input,indices,kernel_size,stride=None,padding=0,output_size=None)", "description": "Computes a partial inverse of MaxPool3d. See MaxUnpool3d for details. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.lp_pool1d", "item_type": "function", "code": "torch.nn.functional.lp_pool1d(input,norm_type,kernel_size,stride=None,ceil_mode=False)", "description": "Applies a 1D power-average pooling over an input signal composed of several input planes. If the sum of all inputs to the power of p is zero, the gradient is set to zero as well. See LPPool1d for details. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.lp_pool2d", "item_type": "function", "code": "torch.nn.functional.lp_pool2d(input,norm_type,kernel_size,stride=None,ceil_mode=False)", "description": "Applies a 2D power-average pooling over an input signal composed of several input planes. If the sum of all inputs to the power of p is zero, the gradient is set to zero as well. See LPPool2d for details. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.adaptive_max_pool1d", "item_type": "function", "code": "torch.nn.functional.adaptive_max_pool1d(*args,**kwargs)", "description": "Applies a 1D adaptive max pooling over an input signal composed of several input planes. See AdaptiveMaxPool1d for details and output shape.  Parameters  output_size \u2013 the target output size (single integer) return_indices \u2013 whether to return pooling indices. Default: False    ", "parameters": ["output_size : the target output size (single integer)", "return_indices : whether to return pooling indices. Default: False"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.adaptive_max_pool2d", "item_type": "function", "code": "torch.nn.functional.adaptive_max_pool2d(*args,**kwargs)", "description": "Applies a 2D adaptive max pooling over an input signal composed of several input planes. See AdaptiveMaxPool2d for details and output shape.  Parameters  output_size \u2013 the target output size (single integer or double-integer tuple) return_indices \u2013 whether to return pooling indices. Default: False    ", "parameters": ["output_size : the target output size (single integer ordouble-integer tuple)", "return_indices : whether to return pooling indices. Default: False"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.adaptive_max_pool3d", "item_type": "function", "code": "torch.nn.functional.adaptive_max_pool3d(*args,**kwargs)", "description": "Applies a 3D adaptive max pooling over an input signal composed of several input planes. See AdaptiveMaxPool3d for details and output shape.  Parameters  output_size \u2013 the target output size (single integer or triple-integer tuple) return_indices \u2013 whether to return pooling indices. Default: False    ", "parameters": ["output_size : the target output size (single integer ortriple-integer tuple)", "return_indices : whether to return pooling indices. Default: False"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.Resize", "item_type": "class", "code": "classtorchvision.transforms.Resize(size,interpolation=2)", "description": "Resize the input PIL Image to the given size.  Parameters  size (sequence or python:int) \u2013 Desired output size. If size is a sequence like (h, w), output size will be matched to this. If size is an int, smaller edge of the image will be matched to this number. i.e, if height &gt; width, then image will be rescaled to (size * height / width, size) interpolation (python:int, optional) \u2013 Desired interpolation. Default is PIL.Image.BILINEAR    ", "parameters": ["size (sequence or python:int) : Desired output size. If size is a sequence like(h, w), output size will be matched to this. If size is an int,smaller edge of the image will be matched to this number.i.e, if height &gt; width, then image will be rescaled to(size * height / width, size)", "interpolation (python:int, optional) : Desired interpolation. Default isPIL.Image.BILINEAR"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.Scale", "item_type": "class", "code": "classtorchvision.transforms.Scale(*args,**kwargs)", "description": "Note: This transform is deprecated in favor of Resize. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.TenCrop", "item_type": "class", "code": "classtorchvision.transforms.TenCrop(size,vertical_flip=False)", "description": "Crop the given PIL Image into four corners and the central crop plus the flipped version of these (horizontal flipping is used by default)  Note This transform returns a tuple of images and there may be a mismatch in the number of inputs and targets your Dataset returns. See below for an example of how to deal with this.   Parameters  size (sequence or python:int) \u2013 Desired output size of the crop. If size is an int instead of sequence like (h, w), a square crop (size, size) is made. vertical_flip (bool) \u2013 Use vertical flipping instead of horizontal    Example &gt;&gt;&gt; transform = Compose([ &gt;&gt;&gt;    TenCrop(size), # this is a list of PIL Images &gt;&gt;&gt;    Lambda(lambda crops: torch.stack([ToTensor()(crop) for crop in crops])) # returns a 4D tensor &gt;&gt;&gt; ]) &gt;&gt;&gt; #In your test loop you can do the following: &gt;&gt;&gt; input, target = batch # input is a 5d tensor, target is 2d &gt;&gt;&gt; bs, ncrops, c, h, w = input.size() &gt;&gt;&gt; result = model(input.view(-1, c, h, w)) # fuse batch size and ncrops &gt;&gt;&gt; result_avg = result.view(bs, ncrops, -1).mean(1) # avg over crops   ", "parameters": ["size (sequence or python:int) : Desired output size of the crop. If size is anint instead of sequence like (h, w), a square crop (size, size) ismade.", "vertical_flip (bool) : Use vertical flipping instead of horizontal"], "returns": null, "example": " transform = Compose([\n    TenCrop(size), # this is a list of PIL Images\n    Lambda(lambda crops: torch.stack([ToTensor()(crop) for crop in crops])) # returns a 4D tensor\n ])\n #In your test loop you can do the following:\n input, target = batch # input is a 5d tensor, target is 2d\n bs, ncrops, c, h, w = input.size()\n result = model(input.view(-1, c, h, w)) # fuse batch size and ncrops\n result_avg = result.view(bs, ncrops, -1).mean(1) # avg over crops\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.LinearTransformation", "item_type": "class", "code": "classtorchvision.transforms.LinearTransformation(transformation_matrix,mean_vector)", "description": "Transform a tensor image with a square transformation matrix and a mean_vector computed offline. Given transformation_matrix and mean_vector, will flatten the torch.*Tensor and subtract mean_vector from it which is then followed by computing the dot product with the transformation matrix and then reshaping the tensor to its original shape.  Applications:whitening transformation: Suppose X is a column vector zero-centered data. Then compute the data covariance matrix [D x D] with torch.mm(X.t(), X), perform SVD on this matrix and pass it as transformation_matrix.    Parameters  transformation_matrix (Tensor) \u2013 tensor [D x D], D = C x H x W mean_vector (Tensor) \u2013 tensor [D], D = C x H x W    ", "parameters": ["transformation_matrix (Tensor) : tensor [D x D], D = C x H x W", "mean_vector (Tensor) : tensor [D], D = C x H x W"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.is_storage", "item_type": "function", "code": "torch.is_storage(obj)", "description": "Returns True if obj is a PyTorch storage object.  Parameters obj (Object) \u2013 Object to test   ", "parameters": ["obj (Object) : Object to test"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.is_floating_point", "item_type": "function", "code": "torch.is_floating_point(input)-&gt;(bool)", "description": "Returns True if the data type of input is a floating point data type i.e., one of torch.float64, torch.float32 and torch.float16.  Parameters input (Tensor) \u2013 the PyTorch tensor to test   ", "parameters": ["input (Tensor) : the PyTorch tensor to test"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.set_default_dtype", "item_type": "function", "code": "torch.set_default_dtype(d)", "description": "Sets the default floating point dtype to d. This type will be used as default floating point type for type inference in torch.tensor(). The default floating point dtype is initially torch.float32.  Parameters d (torch.dtype) \u2013 the floating point dtype to make the default   Example: &gt;&gt;&gt; torch.tensor([1.2, 3]).dtype           # initial default for floating point is torch.float32 torch.float32 &gt;&gt;&gt; torch.set_default_dtype(torch.float64) &gt;&gt;&gt; torch.tensor([1.2, 3]).dtype           # a new floating point tensor torch.float64   ", "parameters": ["d (torch.dtype) : the floating point dtype to make the default"], "returns": null, "example": " torch.tensor([1.2, 3]).dtype           # initial default for floating point is torch.float32\ntorch.float32\n torch.set_default_dtype(torch.float64)\n torch.tensor([1.2, 3]).dtype           # a new floating point tensor\ntorch.float64\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.get_default_dtype", "item_type": "function", "code": "torch.get_default_dtype()\u2192torch.dtype", "description": "Get the current default floating point torch.dtype. Example: &gt;&gt;&gt; torch.get_default_dtype()  # initial default for floating point is torch.float32 torch.float32 &gt;&gt;&gt; torch.set_default_dtype(torch.float64) &gt;&gt;&gt; torch.get_default_dtype()  # default is now changed to torch.float64 torch.float64 &gt;&gt;&gt; torch.set_default_tensor_type(torch.FloatTensor)  # setting tensor type also affects this &gt;&gt;&gt; torch.get_default_dtype()  # changed to torch.float32, the dtype for torch.FloatTensor torch.float32   ", "parameters": [], "returns": null, "example": " torch.get_default_dtype()  # initial default for floating point is torch.float32\ntorch.float32\n torch.set_default_dtype(torch.float64)\n torch.get_default_dtype()  # default is now changed to torch.float64\ntorch.float64\n torch.set_default_tensor_type(torch.FloatTensor)  # setting tensor type also affects this\n torch.get_default_dtype()  # changed to torch.float32, the dtype for torch.FloatTensor\ntorch.float32\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.set_default_tensor_type", "item_type": "function", "code": "torch.set_default_tensor_type(t)", "description": "Sets the default torch.Tensor type to floating point tensor type t. This type will also be used as default floating point type for type inference in torch.tensor(). The default floating point tensor type is initially torch.FloatTensor.  Parameters t (python:type or string) \u2013 the floating point tensor type or its name   Example: &gt;&gt;&gt; torch.tensor([1.2, 3]).dtype    # initial default for floating point is torch.float32 torch.float32 &gt;&gt;&gt; torch.set_default_tensor_type(torch.DoubleTensor) &gt;&gt;&gt; torch.tensor([1.2, 3]).dtype    # a new floating point tensor torch.float64   ", "parameters": ["t (python:type or string) : the floating point tensor type or its name"], "returns": null, "example": " torch.tensor([1.2, 3]).dtype    # initial default for floating point is torch.float32\ntorch.float32\n torch.set_default_tensor_type(torch.DoubleTensor)\n torch.tensor([1.2, 3]).dtype    # a new floating point tensor\ntorch.float64\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.numel", "item_type": "function", "code": "torch.numel(input)\u2192int", "description": "Returns the total number of elements in the input tensor.  Parameters input (Tensor) \u2013 the input tensor.   Example: &gt;&gt;&gt; a = torch.randn(1, 2, 3, 4, 5) &gt;&gt;&gt; torch.numel(a) 120 &gt;&gt;&gt; a = torch.zeros(4,4) &gt;&gt;&gt; torch.numel(a) 16   ", "parameters": ["input (Tensor) : the input tensor."], "returns": null, "example": " a = torch.randn(1, 2, 3, 4, 5)\n torch.numel(a)\n120\n a = torch.zeros(4,4)\n torch.numel(a)\n16\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.set_printoptions", "item_type": "function", "code": "torch.set_printoptions(precision=None,threshold=None,edgeitems=None,linewidth=None,profile=None,sci_mode=None)", "description": "Set options for printing. Items shamelessly taken from NumPy  Parameters  precision \u2013 Number of digits of precision for floating point output (default = 4). threshold \u2013 Total number of array elements which trigger summarization rather than full repr (default = 1000). edgeitems \u2013 Number of array items in summary at beginning and end of each dimension (default = 3). linewidth \u2013 The number of characters per line for the purpose of inserting line breaks (default = 80). Thresholded matrices will ignore this parameter. profile \u2013 Sane defaults for pretty printing. Can override with any of the above options. (any one of default, short, full) sci_mode \u2013 Enable (True) or disable (False) scientific notation. If None (default) is specified, the value is defined by _Formatter    ", "parameters": ["precision : Number of digits of precision for floating point output(default = 4).", "threshold : Total number of array elements which trigger summarizationrather than full repr (default = 1000).", "edgeitems : Number of array items in summary at beginning and end ofeach dimension (default = 3).", "linewidth : The number of characters per line for the purpose ofinserting line breaks (default = 80). Thresholded matrices willignore this parameter.", "profile : Sane defaults for pretty printing. Can override with any ofthe above options. (any one of default, short, full)", "sci_mode : Enable (True) or disable (False) scientific notation. IfNone (default) is specified, the value is defined by _Formatter"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.categorical.Categorical.enumerate_support", "item_type": "method", "code": "enumerate_support(expand=True)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.categorical.Categorical.expand", "item_type": "method", "code": "expand(batch_shape,_instance=None)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.categorical.Categorical.log_prob", "item_type": "method", "code": "log_prob(value)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.categorical.Categorical.mean", "item_type": "method", "code": "propertymean", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.categorical.Categorical.param_shape", "item_type": "method", "code": "propertyparam_shape", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.categorical.Categorical.sample", "item_type": "method", "code": "sample(sample_shape=torch.Size([]))", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.categorical.Categorical.support", "item_type": "method", "code": "propertysupport", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.categorical.Categorical.variance", "item_type": "method", "code": "propertyvariance", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.cauchy.Cauchy.cdf", "item_type": "method", "code": "cdf(value)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.cauchy.Cauchy.entropy", "item_type": "method", "code": "entropy()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.cauchy.Cauchy.expand", "item_type": "method", "code": "expand(batch_shape,_instance=None)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.cauchy.Cauchy.icdf", "item_type": "method", "code": "icdf(value)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.cauchy.Cauchy.log_prob", "item_type": "method", "code": "log_prob(value)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.cauchy.Cauchy.mean", "item_type": "method", "code": "propertymean", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "to(device=None,dtype=None,non_blocking=False)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.memory_reserved", "item_type": "function", "code": "torch.cuda.memory_reserved(device=None)", "description": "Returns the current GPU memory managed by the caching allocator in bytes for a given device.  Parameters device (torch.device or python:int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).    Note See Memory management for more details about GPU memory management.  ", "parameters": ["device (torch.device or python:int, optional) : selected device. Returnsstatistic for the current device, given by current_device(),if device is None (default)."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.max_memory_reserved", "item_type": "function", "code": "torch.cuda.max_memory_reserved(device=None)", "description": "Returns the maximum GPU memory managed by the caching allocator in bytes for a given device. By default, this returns the peak cached memory since the beginning of this program. reset_peak_stats() can be used to reset the starting point in tracking this metric. For example, these two functions can measure the peak cached memory amount of each iteration in a training loop.  Parameters device (torch.device or python:int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).    Note See Memory management for more details about GPU memory management.  ", "parameters": ["device (torch.device or python:int, optional) : selected device. Returnsstatistic for the current device, given by current_device(),if device is None (default)."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.memory_cached", "item_type": "function", "code": "torch.cuda.memory_cached(device=None)", "description": "Deprecated; see memory_reserved(). ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.max_memory_cached", "item_type": "function", "code": "torch.cuda.max_memory_cached(device=None)", "description": "Deprecated; see max_memory_reserved(). ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.reset_max_memory_cached", "item_type": "function", "code": "torch.cuda.reset_max_memory_cached(device=None)", "description": "Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device. See max_memory_cached() for details.  Parameters device (torch.device or python:int, optional) \u2013 selected device. Returns statistic for the current device, given by current_device(), if device is None (default).    Warning This function now calls reset_peak_memory_stats(), which resets /all/ peak memory stats.   Note See Memory management for more details about GPU memory management.  ", "parameters": ["device (torch.device or python:int, optional) : selected device. Returnsstatistic for the current device, given by current_device(),if device is None (default)."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.nvtx.mark", "item_type": "function", "code": "torch.cuda.nvtx.mark(msg)", "description": "Describe an instantaneous event that occurred at some point.  Parameters msg (string) \u2013 ASCII message to associate with the event.   ", "parameters": ["msg (string) : ASCII message to associate with the event."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.nvtx.range_push", "item_type": "function", "code": "torch.cuda.nvtx.range_push(msg)", "description": "Pushes a range onto a stack of nested range span.  Returns zero-based depth of the range that is started.  Parameters msg (string) \u2013 ASCII message to associate with range   ", "parameters": ["msg (string) : ASCII message to associate with range"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.nvtx.range_pop", "item_type": "function", "code": "torch.cuda.nvtx.range_pop()", "description": "Pops a range off of a stack of nested range spans.  Returns the zero-based depth of the range that is ended. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.Stream.query", "item_type": "method", "code": "query()", "description": "Checks if all the work submitted has been completed.  Returns A boolean indicating if all kernels in this stream are completed.   ", "parameters": [], "returns": "A boolean indicating if all kernels in this stream are completed.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.Stream.record_event", "item_type": "method", "code": "record_event(event=None)", "description": "Records an event.  Parameters event (Event, optional) \u2013 event to record. If not given, a new one will be allocated.  Returns Recorded event.   ", "parameters": ["event (Event, optional) : event to record. If not given, a new onewill be allocated.", "Recorded event."], "returns": "Recorded event.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.quantization.MovingAverageMinMaxObserver", "item_type": "class", "code": "classtorch.quantization.MovingAverageMinMaxObserver(averaging_constant=0.01,dtype=torch.quint8,qscheme=torch.per_tensor_affine,reduce_range=False)", "description": "Observer module for computing the quantization parameters based on the moving average of the min and max values. This observer computes the quantization parameters based on the moving averages of minimums and maximums of the incoming tensors. The module records the average minimum and maximum of incoming tensors, and uses this statistic to compute the quantization parameters.  Parameters  averaging_constant \u2013 Averaging constant for min/max. dtype \u2013 Quantized data type qscheme \u2013 Quantization scheme to be used reduce_range \u2013 Reduces the range of the quantized data type by 1 bit    The moving average min/max is computed as follows  xmin={min\u2061(X)if\u00a0xmin=None(1\u2212c)xmin+cmin\u2061(X)otherwisexmax={max\u2061(X)if\u00a0xmax=None(1\u2212c)xmax+cmax\u2061(X)otherwise\\begin{array}{ll}         x_\\text{min} = \\begin{cases}             \\min(X) &amp; \\text{if~}x_\\text{min} = \\text{None} \\\\             (1 - c) x_\\text{min} + c \\min(X) &amp; \\text{otherwise}         \\end{cases}\\\\         x_\\text{max} = \\begin{cases}             \\max(X) &amp; \\text{if~}x_\\text{max} = \\text{None} \\\\             (1 - c) x_\\text{max} + c \\max(X) &amp; \\text{otherwise}         \\end{cases}\\\\ \\end{array}xmin\u200b={min(X)(1\u2212c)xmin\u200b+cmin(X)\u200bif\u00a0xmin\u200b=Noneotherwise\u200bxmax\u200b={max(X)(1\u2212c)xmax\u200b+cmax(X)\u200bif\u00a0xmax\u200b=Noneotherwise\u200b\u200b  where xmin/maxx_\\text{min/max}xmin/max\u200b   is the running average min/max, XXX   is is the incoming tensor, and ccc   is the averaging_constant. The scale and zero point are then computed as in MinMaxObserver.  Note Only works with torch.per_tensor_affine quantization shceme.   Note If the running minimum equals to the running maximum, the scale and zero_point are set to 1.0 and 0.  ", "parameters": ["averaging_constant : Averaging constant for min/max.", "dtype : Quantized data type", "qscheme : Quantization scheme to be used", "reduce_range : Reduces the range of the quantized data type by 1 bit"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.quantization.PerChannelMinMaxObserver", "item_type": "class", "code": "classtorch.quantization.PerChannelMinMaxObserver(ch_axis=0,dtype=torch.quint8,qscheme=torch.per_channel_affine,reduce_range=False)", "description": "Observer module for computing the quantization parameters based on the running per channel min and max values. This observer uses the tensor min/max statistics to compute the per channel quantization parameters. The module records the running minimum and maximum of incoming tensors, and uses this statistic to compute the quantization parameters.  Parameters  ch_axis \u2013 Channel axis dtype \u2013 Quantized data type qscheme \u2013 Quantization scheme to be used reduce_range \u2013 Reduces the range of the quantized data type by 1 bit    The quantization parameters are computed the same way as in MinMaxObserver, with the difference that the running min/max values are stored per channel. Scales and zero points are thus computed per channel as well.  Note If the running minimum equals to the running maximum, the scales and zero_points are set to 1.0 and 0.  ", "parameters": ["ch_axis : Channel axis", "dtype : Quantized data type", "qscheme : Quantization scheme to be used", "reduce_range : Reduces the range of the quantized data type by 1 bit"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.quantization.MovingAveragePerChannelMinMaxObserver", "item_type": "class", "code": "classtorch.quantization.MovingAveragePerChannelMinMaxObserver(averaging_constant=0.01,ch_axis=0,dtype=torch.quint8,qscheme=torch.per_channel_affine,reduce_range=False)", "description": "Observer module for computing the quantization parameters based on the running per channel min and max values. This observer uses the tensor min/max statistics to compute the per channel quantization parameters. The module records the running minimum and maximum of incoming tensors, and uses this statistic to compute the quantization parameters.  Parameters  averaging_constant \u2013 Averaging constant for min/max. ch_axis \u2013 Channel axis dtype \u2013 Quantized data type qscheme \u2013 Quantization scheme to be used reduce_range \u2013 Reduces the range of the quantized data type by 1 bit    The quantization parameters are computed the same way as in MovingAverageMinMaxObserver, with the difference that the running min/max values are stored per channel. Scales and zero points are thus computed per channel as well.  Note If the running minimum equals to the running maximum, the scales and zero_points are set to 1.0 and 0.  ", "parameters": ["averaging_constant : Averaging constant for min/max.", "ch_axis : Channel axis", "dtype : Quantized data type", "qscheme : Quantization scheme to be used", "reduce_range : Reduces the range of the quantized data type by 1 bit"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.quantization.HistogramObserver", "item_type": "class", "code": "classtorch.quantization.HistogramObserver(bins=2048,upsample_rate=128,dtype=torch.quint8,qscheme=torch.per_tensor_affine,reduce_range=False)", "description": "The module records the running histogram of tensor values along with min/max values. calculate_qparams will calculate scale and zero_point.  Parameters  bins \u2013 Number of bins to use for the histogram upsample_rate \u2013 Factor by which the histograms are upsampled, this is used to interpolate histograms with varying ranges across observations dtype \u2013 Quantized data type qscheme \u2013 Quantization scheme to be used reduce_range \u2013 Reduces the range of the quantized data type by 1 bit    The scale and zero point are computed as follows:   Create the histogram of the incoming inputs.The histogram is computed continuously, and the ranges per bin change with every new tensor observed.     Search the distribution in the histogram for optimal min/max values.The search for the min/max values ensures the minimization of the quantization error with respect to the floating point model.     Compute the scale and zero point the same way as in theMinMaxObserver     ", "parameters": ["bins : Number of bins to use for the histogram", "upsample_rate : Factor by which the histograms are upsampled, this isused to interpolate histograms with varying ranges across observations", "dtype : Quantized data type", "qscheme : Quantization scheme to be used", "reduce_range : Reduces the range of the quantized data type by 1 bit"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.quantization.FakeQuantize", "item_type": "class", "code": "classtorch.quantization.FakeQuantize(observer=&lt;class'torch.quantization.observer.MovingAverageMinMaxObserver'&gt;,quant_min=0,quant_max=255,**observer_kwargs)", "description": "Simulate the quantize and dequantize operations in training time. The output of this module is given by x_out = (clamp(round(x/scale + zero_point), quant_min, quant_max)-zero_point)*scale  scale defines the scale factor used for quantization. zero_point specifies the quantized value to which 0 in floating point maps to quant_min specifies the minimum allowable quantized value. quant_max specifies the maximum allowable quantized value. fake_quant_enable controls the application of fake quantization on tensors, note that statistics can still be updated. observer_enable controls statistics collection on tensors  dtype specifies the quantized dtype that is being emulated with fake-quantization,allowable values are torch.qint8 and torch.quint8. The values of quant_min and quant_max should be chosen to be consistent with the dtype      Parameters  observer (module) \u2013 Module for observing statistics on input tensors and calculating scale and zero-point. quant_min (python:int) \u2013 The minimum allowable quantized value. quant_max (python:int) \u2013 The maximum allowable quantized value. observer_kwargs (optional) \u2013 Arguments for the observer module   Variables ~FakeQuantize.observer (Module) \u2013 User provided module that collects statistics on the input tensor and provides a method to calculate scale and zero-point.   ", "parameters": ["observer (module) : Module for observing statistics on input tensors and calculating scaleand zero-point.", "quant_min (python:int) : The minimum allowable quantized value.", "quant_max (python:int) : The maximum allowable quantized value.", "observer_kwargs (optional) : Arguments for the observer module"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.quantization.NoopObserver", "item_type": "class", "code": "classtorch.quantization.NoopObserver(dtype=torch.float16)", "description": "Observer that doesn\u2019t do anything and just passes its configuration to the quantized module\u2019s .from_float(). Primarily used for quantization to float16 which doesn\u2019t require determining ranges.  Parameters dtype \u2013 Quantized data type   ", "parameters": ["dtype : Quantized data type"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.quantization.RecordingObserver", "item_type": "class", "code": "classtorch.quantization.RecordingObserver(**kwargs)", "description": "The module is mainly for debug and records the tensor values during runtime.  Parameters  dtype \u2013 Quantized data type qscheme \u2013 Quantization scheme to be used reduce_range \u2013 Reduces the range of the quantized data type by 1 bit    ", "parameters": ["dtype : Quantized data type", "qscheme : Quantization scheme to be used", "reduce_range : Reduces the range of the quantized data type by 1 bit"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.optim.lr_scheduler.MultiStepLR", "item_type": "class", "code": "classtorch.optim.lr_scheduler.MultiStepLR(optimizer,milestones,gamma=0.1,last_epoch=-1)", "description": "Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. Notice that such decay can happen simultaneously with other changes to the learning rate from outside this scheduler. When last_epoch=-1, sets initial lr as lr.  Parameters  optimizer (Optimizer) \u2013 Wrapped optimizer. milestones (list) \u2013 List of epoch indices. Must be increasing. gamma (python:float) \u2013 Multiplicative factor of learning rate decay. Default: 0.1. last_epoch (python:int) \u2013 The index of last epoch. Default: -1.    Example &gt;&gt;&gt; # Assuming optimizer uses lr = 0.05 for all groups &gt;&gt;&gt; # lr = 0.05     if epoch &lt; 30 &gt;&gt;&gt; # lr = 0.005    if 30 &lt;= epoch &lt; 80 &gt;&gt;&gt; # lr = 0.0005   if epoch &gt;= 80 &gt;&gt;&gt; scheduler = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1) &gt;&gt;&gt; for epoch in range(100): &gt;&gt;&gt;     train(...) &gt;&gt;&gt;     validate(...) &gt;&gt;&gt;     scheduler.step()   ", "parameters": ["optimizer (Optimizer) : Wrapped optimizer.", "milestones (list) : List of epoch indices. Must be increasing.", "gamma (python:float) : Multiplicative factor of learning rate decay.Default: 0.1.", "last_epoch (python:int) : The index of last epoch. Default: -1."], "returns": null, "example": " # Assuming optimizer uses lr = 0.05 for all groups\n # lr = 0.05     if epoch &lt; 30\n # lr = 0.005    if 30 &lt;= epoch &lt; 80\n # lr = 0.0005   if epoch = 80\n scheduler = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n for epoch in range(100):\n     train(...)\n     validate(...)\n     scheduler.step()\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.optim.lr_scheduler.ExponentialLR", "item_type": "class", "code": "classtorch.optim.lr_scheduler.ExponentialLR(optimizer,gamma,last_epoch=-1)", "description": "Decays the learning rate of each parameter group by gamma every epoch. When last_epoch=-1, sets initial lr as lr.  Parameters  optimizer (Optimizer) \u2013 Wrapped optimizer. gamma (python:float) \u2013 Multiplicative factor of learning rate decay. last_epoch (python:int) \u2013 The index of last epoch. Default: -1.    ", "parameters": ["optimizer (Optimizer) : Wrapped optimizer.", "gamma (python:float) : Multiplicative factor of learning rate decay.", "last_epoch (python:int) : The index of last epoch. Default: -1."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.optim.lr_scheduler.CosineAnnealingLR", "item_type": "class", "code": "classtorch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max,eta_min=0,last_epoch=-1)", "description": "Set the learning rate of each parameter group using a cosine annealing schedule, where \u03b7max\\eta_{max}\u03b7max\u200b   is set to the initial lr and TcurT_{cur}Tcur\u200b   is the number of epochs since the last restart in SGDR:  \u03b7t=\u03b7min+12(\u03b7max\u2212\u03b7min)(1+cos\u2061(TcurTmax\u03c0))Tcur\u2260(2k+1)Tmax;\u03b7t+1=\u03b7t+(\u03b7max\u2212\u03b7min)1\u2212cos\u2061(1Tmax\u03c0)2,Tcur=(2k+1)Tmax.\\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\\left(1 + \\cos\\left(\\frac{T_{cur}}{T_{max}}\\pi\\right)\\right) T_{cur} \\neq (2k+1)T_{max};\\\\ \\eta_{t+1} = \\eta_{t} + (\\eta_{max} - \\eta_{min})\\frac{1 - \\cos(\\frac{1}{T_{max}}\\pi)}{2}, T_{cur} = (2k+1)T_{max}.\\\\  \u03b7t\u200b=\u03b7min\u200b+21\u200b(\u03b7max\u200b\u2212\u03b7min\u200b)(1+cos(Tmax\u200bTcur\u200b\u200b\u03c0))Tcur\u200b\ue020\u200b=(2k+1)Tmax\u200b;\u03b7t+1\u200b=\u03b7t\u200b+(\u03b7max\u200b\u2212\u03b7min\u200b)21\u2212cos(Tmax\u200b1\u200b\u03c0)\u200b,Tcur\u200b=(2k+1)Tmax\u200b.  When last_epoch=-1, sets initial lr as lr. Notice that because the schedule is defined recursively, the learning rate can be simultaneously modified outside this scheduler by other operators. If the learning rate is set solely by this scheduler, the learning rate at each step becomes:  \u03b7t=\u03b7min+12(\u03b7max\u2212\u03b7min)(1+cos\u2061(TcurTmax\u03c0))\\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\\left(1 + \\cos\\left(\\frac{T_{cur}}{T_{max}}\\pi\\right)\\right)  \u03b7t\u200b=\u03b7min\u200b+21\u200b(\u03b7max\u200b\u2212\u03b7min\u200b)(1+cos(Tmax\u200bTcur\u200b\u200b\u03c0))  It has been proposed in SGDR: Stochastic Gradient Descent with Warm Restarts. Note that this only implements the cosine annealing part of SGDR, and not the restarts.  Parameters  optimizer (Optimizer) \u2013 Wrapped optimizer. T_max (python:int) \u2013 Maximum number of iterations. eta_min (python:float) \u2013 Minimum learning rate. Default: 0. last_epoch (python:int) \u2013 The index of last epoch. Default: -1.    ", "parameters": ["optimizer (Optimizer) : Wrapped optimizer.", "T_max (python:int) : Maximum number of iterations.", "eta_min (python:float) : Minimum learning rate. Default: 0.", "last_epoch (python:int) : The index of last epoch. Default: -1."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.optim.lr_scheduler.ReduceLROnPlateau", "item_type": "class", "code": "classtorch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode='min',factor=0.1,patience=10,verbose=False,threshold=0.0001,threshold_mode='rel',cooldown=0,min_lr=0,eps=1e-08)", "description": "Reduce learning rate when a metric has stopped improving. Models often benefit from reducing the learning rate by a factor of 2-10 once learning stagnates. This scheduler reads a metrics quantity and if no improvement is seen for a \u2018patience\u2019 number of epochs, the learning rate is reduced.  Parameters  optimizer (Optimizer) \u2013 Wrapped optimizer. mode (str) \u2013 One of min, max. In min mode, lr will be reduced when the quantity monitored has stopped decreasing; in max mode it will be reduced when the quantity monitored has stopped increasing. Default: \u2018min\u2019. factor (python:float) \u2013 Factor by which the learning rate will be reduced. new_lr = lr * factor. Default: 0.1. patience (python:int) \u2013 Number of epochs with no improvement after which learning rate will be reduced. For example, if patience = 2, then we will ignore the first 2 epochs with no improvement, and will only decrease the LR after the 3rd epoch if the loss still hasn\u2019t improved then. Default: 10. verbose (bool) \u2013 If True, prints a message to stdout for each update. Default: False. threshold (python:float) \u2013 Threshold for measuring the new optimum, to only focus on significant changes. Default: 1e-4. threshold_mode (str) \u2013 One of rel, abs. In rel mode, dynamic_threshold = best * ( 1 + threshold ) in \u2018max\u2019 mode or best * ( 1 - threshold ) in min mode. In abs mode, dynamic_threshold = best + threshold in max mode or best - threshold in min mode. Default: \u2018rel\u2019. cooldown (python:int) \u2013 Number of epochs to wait before resuming normal operation after lr has been reduced. Default: 0. min_lr (python:float or list) \u2013 A scalar or a list of scalars. A lower bound on the learning rate of all param groups or each group respectively. Default: 0. eps (python:float) \u2013 Minimal decay applied to lr. If the difference between new and old lr is smaller than eps, the update is ignored. Default: 1e-8.    Example &gt;&gt;&gt; optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9) &gt;&gt;&gt; scheduler = ReduceLROnPlateau(optimizer, 'min') &gt;&gt;&gt; for epoch in range(10): &gt;&gt;&gt;     train(...) &gt;&gt;&gt;     val_loss = validate(...) &gt;&gt;&gt;     # Note that step should be called after validate() &gt;&gt;&gt;     scheduler.step(val_loss)   ", "parameters": ["optimizer (Optimizer) : Wrapped optimizer.", "mode (str) : One of min, max. In min mode, lr willbe reduced when the quantity monitored has stoppeddecreasing; in max mode it will be reduced when thequantity monitored has stopped increasing. Default: \u2018min\u2019.", "factor (python:float) : Factor by which the learning rate will bereduced. new_lr = lr * factor. Default: 0.1.", "patience (python:int) : Number of epochs with no improvement afterwhich learning rate will be reduced. For example, ifpatience = 2, then we will ignore the first 2 epochswith no improvement, and will only decrease the LR after the3rd epoch if the loss still hasn\u2019t improved then.Default: 10.", "verbose (bool) : If True, prints a message to stdout foreach update. Default: False.", "threshold (python:float) : Threshold for measuring the new optimum,to only focus on significant changes. Default: 1e-4.", "threshold_mode (str) : One of rel, abs. In rel mode,dynamic_threshold = best * ( 1 + threshold ) in \u2018max\u2019mode or best * ( 1 - threshold ) in min mode.In abs mode, dynamic_threshold = best + threshold inmax mode or best - threshold in min mode. Default: \u2018rel\u2019.", "cooldown (python:int) : Number of epochs to wait before resumingnormal operation after lr has been reduced. Default: 0.", "min_lr (python:float or list) : A scalar or a list of scalars. Alower bound on the learning rate of all param groupsor each group respectively. Default: 0.", "eps (python:float) : Minimal decay applied to lr. If the differencebetween new and old lr is smaller than eps, the update isignored. Default: 1e-8."], "returns": null, "example": " optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n scheduler = ReduceLROnPlateau(optimizer, 'min')\n for epoch in range(10):\n     train(...)\n     val_loss = validate(...)\n     # Note that step should be called after validate()\n     scheduler.step(val_loss)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.optim.lr_scheduler.CyclicLR", "item_type": "class", "code": "classtorch.optim.lr_scheduler.CyclicLR(optimizer,base_lr,max_lr,step_size_up=2000,step_size_down=None,mode='triangular',gamma=1.0,scale_fn=None,scale_mode='cycle',cycle_momentum=True,base_momentum=0.8,max_momentum=0.9,last_epoch=-1)", "description": "Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR). The policy cycles the learning rate between two boundaries with a constant frequency, as detailed in the paper Cyclical Learning Rates for Training Neural Networks. The distance between the two boundaries can be scaled on a per-iteration or per-cycle basis. Cyclical learning rate policy changes the learning rate after every batch. step should be called after a batch has been used for training. This class has three built-in policies, as put forth in the paper:  \u201ctriangular\u201d: A basic triangular cycle without amplitude scaling. \u201ctriangular2\u201d: A basic triangular cycle that scales initial amplitude by half each cycle. \u201cexp_range\u201d: A cycle that scales initial amplitude by gammacycle\u00a0iterations\\text{gamma}^{\\text{cycle iterations}}gammacycle\u00a0iterations   at each cycle iteration.  This implementation was adapted from the github repo: bckenstler/CLR  Parameters  optimizer (Optimizer) \u2013 Wrapped optimizer. base_lr (python:float or list) \u2013 Initial learning rate which is the lower boundary in the cycle for each parameter group. max_lr (python:float or list) \u2013 Upper learning rate boundaries in the cycle for each parameter group. Functionally, it defines the cycle amplitude (max_lr - base_lr). The lr at any cycle is the sum of base_lr and some scaling of the amplitude; therefore max_lr may not actually be reached depending on scaling function. step_size_up (python:int) \u2013 Number of training iterations in the increasing half of a cycle. Default: 2000 step_size_down (python:int) \u2013 Number of training iterations in the decreasing half of a cycle. If step_size_down is None, it is set to step_size_up. Default: None mode (str) \u2013 One of {triangular, triangular2, exp_range}. Values correspond to policies detailed above. If scale_fn is not None, this argument is ignored. Default: \u2018triangular\u2019 gamma (python:float) \u2013 Constant in \u2018exp_range\u2019 scaling function: gamma**(cycle iterations) Default: 1.0 scale_fn (function) \u2013 Custom scaling policy defined by a single argument lambda function, where 0 &lt;= scale_fn(x) &lt;= 1 for all x &gt;= 0. If specified, then \u2018mode\u2019 is ignored. Default: None scale_mode (str) \u2013 {\u2018cycle\u2019, \u2018iterations\u2019}. Defines whether scale_fn is evaluated on cycle number or cycle iterations (training iterations since start of cycle). Default: \u2018cycle\u2019 cycle_momentum (bool) \u2013 If True, momentum is cycled inversely to learning rate between \u2018base_momentum\u2019 and \u2018max_momentum\u2019. Default: True base_momentum (python:float or list) \u2013 Lower momentum boundaries in the cycle for each parameter group. Note that momentum is cycled inversely to learning rate; at the peak of a cycle, momentum is \u2018base_momentum\u2019 and learning rate is \u2018max_lr\u2019. Default: 0.8 max_momentum (python:float or list) \u2013 Upper momentum boundaries in the cycle for each parameter group. Functionally, it defines the cycle amplitude (max_momentum - base_momentum). The momentum at any cycle is the difference of max_momentum and some scaling of the amplitude; therefore base_momentum may not actually be reached depending on scaling function. Note that momentum is cycled inversely to learning rate; at the start of a cycle, momentum is \u2018max_momentum\u2019 and learning rate is \u2018base_lr\u2019 Default: 0.9 last_epoch (python:int) \u2013 The index of the last batch. This parameter is used when resuming a training job. Since step() should be invoked after each batch instead of after each epoch, this number represents the total number of batches computed, not the total number of epochs computed. When last_epoch=-1, the schedule is started from the beginning. Default: -1    Example &gt;&gt;&gt; optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9) &gt;&gt;&gt; scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.01, max_lr=0.1) &gt;&gt;&gt; data_loader = torch.utils.data.DataLoader(...) &gt;&gt;&gt; for epoch in range(10): &gt;&gt;&gt;     for batch in data_loader: &gt;&gt;&gt;         train_batch(...) &gt;&gt;&gt;         scheduler.step()     get_lr()  Calculates the learning rate at batch index. This function treats self.last_epoch as the last batch index. If self.cycle_momentum is True, this function has a side effect of updating the optimizer\u2019s momentum.   ", "parameters": ["optimizer (Optimizer) : Wrapped optimizer.", "base_lr (python:float or list) : Initial learning rate which is thelower boundary in the cycle for each parameter group.", "max_lr (python:float or list) : Upper learning rate boundaries in the cyclefor each parameter group. Functionally,it defines the cycle amplitude (max_lr - base_lr).The lr at any cycle is the sum of base_lrand some scaling of the amplitude; thereforemax_lr may not actually be reached depending onscaling function.", "step_size_up (python:int) : Number of training iterations in theincreasing half of a cycle. Default: 2000", "step_size_down (python:int) : Number of training iterations in thedecreasing half of a cycle. If step_size_down is None,it is set to step_size_up. Default: None", "mode (str) : One of {triangular, triangular2, exp_range}.Values correspond to policies detailed above.If scale_fn is not None, this argument is ignored.Default: \u2018triangular\u2019", "gamma (python:float) : Constant in \u2018exp_range\u2019 scaling function:gamma**(cycle iterations)Default: 1.0", "scale_fn (function) : Custom scaling policy defined by a singleargument lambda function, where0 &lt;= scale_fn(x) &lt;= 1 for all x &gt;= 0.If specified, then \u2018mode\u2019 is ignored.Default: None", "scale_mode (str) : {\u2018cycle\u2019, \u2018iterations\u2019}.Defines whether scale_fn is evaluated oncycle number or cycle iterations (trainingiterations since start of cycle).Default: \u2018cycle\u2019", "cycle_momentum (bool) : If True, momentum is cycled inverselyto learning rate between \u2018base_momentum\u2019 and \u2018max_momentum\u2019.Default: True", "base_momentum (python:float or list) : Lower momentum boundaries in the cyclefor each parameter group. Note that momentum is cycled inverselyto learning rate; at the peak of a cycle, momentum is\u2018base_momentum\u2019 and learning rate is \u2018max_lr\u2019.Default: 0.8", "max_momentum (python:float or list) : Upper momentum boundaries in the cyclefor each parameter group. Functionally,it defines the cycle amplitude (max_momentum - base_momentum).The momentum at any cycle is the difference of max_momentumand some scaling of the amplitude; thereforebase_momentum may not actually be reached depending onscaling function. Note that momentum is cycled inverselyto learning rate; at the start of a cycle, momentum is \u2018max_momentum\u2019and learning rate is \u2018base_lr\u2019Default: 0.9", "last_epoch (python:int) : The index of the last batch. This parameter is used whenresuming a training job. Since step() should be invoked after eachbatch instead of after each epoch, this number represents the totalnumber of batches computed, not the total number of epochs computed.When last_epoch=-1, the schedule is started from the beginning.Default: -1"], "returns": null, "example": " optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.01, max_lr=0.1)\n data_loader = torch.utils.data.DataLoader(...)\n for epoch in range(10):\n     for batch in data_loader:\n         train_batch(...)\n         scheduler.step()\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.atan2_", "item_type": "method", "code": "atan2_(other)\u2192Tensor", "description": "In-place version of atan2() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.atan_", "item_type": "method", "code": "atan_()\u2192Tensor", "description": "In-place version of atan() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "method", "code": "backward(gradient=None,retain_graph=None,create_graph=False)", "description": "Computes the gradient of current tensor w.r.t. graph leaves. The graph is differentiated using the chain rule. If the tensor is non-scalar (i.e. its data has more than one element) and requires gradient, the function additionally requires specifying gradient. It should be a tensor of matching type and location, that contains the gradient of the differentiated function w.r.t. self. This function accumulates gradients in the leaves - you might need to zero them before calling it.  Parameters  gradient (Tensor or None) \u2013 Gradient w.r.t. the tensor. If it is a tensor, it will be automatically converted to a Tensor that does not require grad unless create_graph is True. None values can be specified for scalar Tensors or ones that don\u2019t require grad. If a None value would be acceptable then this argument is optional. retain_graph (bool, optional) \u2013 If False, the graph used to compute the grads will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Defaults to the value of create_graph. create_graph (bool, optional) \u2013 If True, graph of the derivative will be constructed, allowing to compute higher order derivative products. Defaults to False.    ", "parameters": ["gradient (Tensor or None) : Gradient w.r.t. thetensor. If it is a tensor, it will be automatically convertedto a Tensor that does not require grad unless create_graph is True.None values can be specified for scalar Tensors or ones thatdon\u2019t require grad. If a None value would be acceptable thenthis argument is optional.", "retain_graph (bool, optional) : If False, the graph used to computethe grads will be freed. Note that in nearly all cases settingthis option to True is not needed and often can be worked aroundin a much more efficient way. Defaults to the value ofcreate_graph.", "create_graph (bool, optional) : If True, graph of the derivative willbe constructed, allowing to compute higher order derivativeproducts. Defaults to False."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.baddbmm", "item_type": "method", "code": "baddbmm(beta=1,alpha=1,batch1,batch2)\u2192Tensor", "description": "See torch.baddbmm() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.baddbmm_", "item_type": "method", "code": "baddbmm_(beta=1,alpha=1,batch1,batch2)\u2192Tensor", "description": "In-place version of baddbmm() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.bernoulli", "item_type": "method", "code": "bernoulli(*,generator=None)\u2192Tensor", "description": "Returns a result tensor where each result[i]\\texttt{result[i]}result[i]   is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i])  . self must have floating point dtype, and the result will have the same dtype. See torch.bernoulli() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.bernoulli_", "item_type": "method", "code": "bernoulli_()", "description": "  bernoulli_(p=0.5, *, generator=None) \u2192 Tensor Fills each location of self with an independent sample from Bernoulli(p)\\text{Bernoulli}(\\texttt{p})Bernoulli(p)  . self can have integral dtype.     bernoulli_(p_tensor, *, generator=None) \u2192 Tensor p_tensor should be a tensor containing probabilities to be used for drawing the binary random number. The ith\\text{i}^{th}ith   element of self tensor will be set to a value sampled from Bernoulli(p_tensor[i])\\text{Bernoulli}(\\texttt{p\\_tensor[i]})Bernoulli(p_tensor[i])  . self can have integral dtype, but p_tensor must have floating point dtype.   See also bernoulli() and torch.bernoulli() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.bfloat16", "item_type": "method", "code": "bfloat16()\u2192Tensor", "description": "self.bfloat16() is equivalent to self.to(torch.bfloat16). See to(). ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.bincount", "item_type": "method", "code": "bincount(weights=None,minlength=0)\u2192Tensor", "description": "See torch.bincount() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.bitwise_not", "item_type": "method", "code": "bitwise_not()\u2192Tensor", "description": "See torch.bitwise_not() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.bitwise_not_", "item_type": "method", "code": "bitwise_not_()\u2192Tensor", "description": "In-place version of bitwise_not() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.adaptive_avg_pool1d", "item_type": "function", "code": "torch.nn.functional.adaptive_avg_pool1d(input,output_size)\u2192Tensor", "description": "Applies a 1D adaptive average pooling over an input signal composed of several input planes. See AdaptiveAvgPool1d for details and output shape.  Parameters output_size \u2013 the target output size (single integer)   ", "parameters": ["output_size : the target output size (single integer)"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.adaptive_avg_pool2d", "item_type": "function", "code": "torch.nn.functional.adaptive_avg_pool2d(input,output_size)", "description": "Applies a 2D adaptive average pooling over an input signal composed of several input planes. See AdaptiveAvgPool2d for details and output shape.  Parameters output_size \u2013 the target output size (single integer or double-integer tuple)   ", "parameters": ["output_size : the target output size (single integer ordouble-integer tuple)"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.adaptive_avg_pool3d", "item_type": "function", "code": "torch.nn.functional.adaptive_avg_pool3d(input,output_size)", "description": "Applies a 3D adaptive average pooling over an input signal composed of several input planes. See AdaptiveAvgPool3d for details and output shape.  Parameters output_size \u2013 the target output size (single integer or triple-integer tuple)   ", "parameters": ["output_size : the target output size (single integer ortriple-integer tuple)"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.threshold", "item_type": "function", "code": "torch.nn.functional.threshold(input,threshold,value,inplace=False)", "description": "Thresholds each element of the input Tensor. See Threshold for more details. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.threshold_", "item_type": "function", "code": "torch.nn.functional.threshold_(input,threshold,value)\u2192Tensor", "description": "In-place version of threshold(). ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.relu", "item_type": "function", "code": "torch.nn.functional.relu(input,inplace=False)\u2192Tensor", "description": "Applies the rectified linear unit function element-wise. See ReLU for more details. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.relu_", "item_type": "function", "code": "torch.nn.functional.relu_(input)\u2192Tensor", "description": "In-place version of relu(). ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.hardtanh", "item_type": "function", "code": "torch.nn.functional.hardtanh(input,min_val=-1.,max_val=1.,inplace=False)\u2192Tensor", "description": "Applies the HardTanh function element-wise. See Hardtanh for more details. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.hardtanh_", "item_type": "function", "code": "torch.nn.functional.hardtanh_(input,min_val=-1.,max_val=1.)\u2192Tensor", "description": "In-place version of hardtanh(). ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.relu6", "item_type": "function", "code": "torch.nn.functional.relu6(input,inplace=False)\u2192Tensor", "description": "Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6)  . See ReLU6 for more details. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.elu", "item_type": "function", "code": "torch.nn.functional.elu(input,alpha=1.0,inplace=False)", "description": "Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121))  . See ELU for more details. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.Normalize", "item_type": "class", "code": "classtorchvision.transforms.Normalize(mean,std,inplace=False)", "description": "Normalize a tensor image with mean and standard deviation. Given mean: (M1,...,Mn) and std: (S1,..,Sn) for n channels, this transform will normalize each channel of the input torch.*Tensor i.e. input[channel] = (input[channel] - mean[channel]) / std[channel]  Note This transform acts out of place, i.e., it does not mutates the input tensor.   Parameters  mean (sequence) \u2013 Sequence of means for each channel. std (sequence) \u2013 Sequence of standard deviations for each channel. inplace (bool,optional) \u2013 Bool to make this operation in-place.      __call__(tensor)   Parameters tensor (Tensor) \u2013 Tensor image of size (C, H, W) to be normalized.  Returns Normalized Tensor image.  Return type Tensor     ", "parameters": ["mean (sequence) : Sequence of means for each channel.", "std (sequence) : Sequence of standard deviations for each channel.", "inplace (bool,optional) : Bool to make this operation in-place."], "returns": "Normalized Tensor image.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.RandomErasing", "item_type": "class", "code": "classtorchvision.transforms.RandomErasing(p=0.5,scale=(0.02,0.33),ratio=(0.3,3.3),value=0,inplace=False)", "description": " Randomly selects a rectangle region in an image and erases its pixels.\u2018Random Erasing Data Augmentation\u2019 by Zhong et al. See https://arxiv.org/pdf/1708.04896.pdf    Parameters  p \u2013 probability that the random erasing operation will be performed. scale \u2013 range of proportion of erased area against input image. ratio \u2013 range of aspect ratio of erased area. value \u2013 erasing value. Default is 0. If a single int, it is used to erase all pixels. If a tuple of length 3, it is used to erase R, G, B channels respectively. If a str of \u2018random\u2019, erasing each pixel with random values. inplace \u2013 boolean to make this transform inplace. Default set to False.   Returns Erased Image.    # Examples:&gt;&gt;&gt; transform = transforms.Compose([ &gt;&gt;&gt; transforms.RandomHorizontalFlip(), &gt;&gt;&gt; transforms.ToTensor(), &gt;&gt;&gt; transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), &gt;&gt;&gt; transforms.RandomErasing(), &gt;&gt;&gt; ])     ", "parameters": ["p : probability that the random erasing operation will be performed.", "scale : range of proportion of erased area against input image.", "ratio : range of aspect ratio of erased area.", "value : erasing value. Default is 0. If a single int, it is used toerase all pixels. If a tuple of length 3, it is used to eraseR, G, B channels respectively.If a str of \u2018random\u2019, erasing each pixel with random values.", "inplace : boolean to make this transform inplace. Default set to False."], "returns": "Erased Image.", "example": " transform = transforms.Compose([\n transforms.RandomHorizontalFlip(),\n transforms.ToTensor(),\n transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n transforms.RandomErasing(),\n ])\n\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.ToPILImage", "item_type": "class", "code": "classtorchvision.transforms.ToPILImage(mode=None)", "description": "Convert a tensor or an ndarray to PIL Image. Converts a torch.*Tensor of shape C x H x W or a numpy ndarray of shape H x W x C to a PIL Image while preserving the value range.  Parameters mode (PIL.Image mode) \u2013 color space and pixel depth of input data (optional). If mode is None (default) there are some assumptions made about the input data:   If the input has 4 channels, the mode is assumed to be RGBA. If the input has 3 channels, the mode is assumed to be RGB. If the input has 2 channels, the mode is assumed to be LA. If the input has 1 channel, the mode is determined by the data type (i.e int, float, short).        __call__(pic)   Parameters pic (Tensor or numpy.ndarray) \u2013 Image to be converted to PIL Image.  Returns Image converted to PIL Image.  Return type PIL Image     ", "parameters": ["If the input has 4 channels, the mode is assumed to be RGBA.", "If the input has 3 channels, the mode is assumed to be RGB.", "If the input has 2 channels, the mode is assumed to be LA.", "If the input has 1 channel, the mode is determined by the data type (i.e int, float,short)."], "returns": "Image converted to PIL Image.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.ToTensor", "item_type": "class", "code": "classtorchvision.transforms.ToTensor", "description": "Convert a PIL Image or numpy.ndarray to tensor. Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1) or if the numpy.ndarray has dtype = np.uint8 In the other cases, tensors are returned without scaling.   __call__(pic)   Parameters pic (PIL Image or numpy.ndarray) \u2013 Image to be converted to tensor.  Returns Converted image.  Return type Tensor     ", "parameters": ["pic (PIL Image or numpy.ndarray) : Image to be converted to tensor.", "Converted image.", "Tensor"], "returns": "Converted image.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torchvision.transforms.Lambda", "item_type": "class", "code": "classtorchvision.transforms.Lambda(lambd)", "description": "Apply a user-defined lambda as a transform.  Parameters lambd (function) \u2013 Lambda/function to be used for transform.   ", "parameters": ["lambd (function) : Lambda/function to be used for transform."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.set_flush_denormal", "item_type": "function", "code": "torch.set_flush_denormal(mode)\u2192bool", "description": "Disables denormal floating numbers on CPU. Returns True if your system supports flushing denormal numbers and it successfully configures flush denormal mode.  set_flush_denormal() is only supported on x86 architectures supporting SSE3.  Parameters mode (bool) \u2013 Controls whether to enable flush denormal mode or not   Example: &gt;&gt;&gt; torch.set_flush_denormal(True) True &gt;&gt;&gt; torch.tensor([1e-323], dtype=torch.float64) tensor([ 0.], dtype=torch.float64) &gt;&gt;&gt; torch.set_flush_denormal(False) True &gt;&gt;&gt; torch.tensor([1e-323], dtype=torch.float64) tensor(9.88131e-324 *        [ 1.0000], dtype=torch.float64)   ", "parameters": ["mode (bool) : Controls whether to enable flush denormal mode or not"], "returns": null, "example": " torch.set_flush_denormal(True)\nTrue\n torch.tensor([1e-323], dtype=torch.float64)\ntensor([ 0.], dtype=torch.float64)\n torch.set_flush_denormal(False)\nTrue\n torch.tensor([1e-323], dtype=torch.float64)\ntensor(9.88131e-324 *\n       [ 1.0000], dtype=torch.float64)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.tensor", "item_type": "function", "code": "torch.tensor(data,dtype=None,device=None,requires_grad=False,pin_memory=False)\u2192Tensor", "description": "Constructs a tensor with data.  Warning torch.tensor() always copies data. If you have a Tensor data and want to avoid a copy, use torch.Tensor.requires_grad_() or torch.Tensor.detach(). If you have a NumPy ndarray and want to avoid a copy, use torch.as_tensor().   Warning When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed, and constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach() and torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True). The equivalents using clone() and detach() are recommended.   Parameters  data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple, NumPy ndarray, scalar, and other types. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, infers data type from data. device (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False. pin_memory (bool, optional) \u2013 If set, returned tensor would be allocated in the pinned memory. Works only for CPU tensors. Default: False.    Example: &gt;&gt;&gt; torch.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]]) tensor([[ 0.1000,  1.2000],         [ 2.2000,  3.1000],         [ 4.9000,  5.2000]])  &gt;&gt;&gt; torch.tensor([0, 1])  # Type inference on data tensor([ 0,  1])  &gt;&gt;&gt; torch.tensor([[0.11111, 0.222222, 0.3333333]],                  dtype=torch.float64,                  device=torch.device('cuda:0'))  # creates a torch.cuda.DoubleTensor tensor([[ 0.1111,  0.2222,  0.3333]], dtype=torch.float64, device='cuda:0')  &gt;&gt;&gt; torch.tensor(3.14159)  # Create a scalar (zero-dimensional tensor) tensor(3.1416)  &gt;&gt;&gt; torch.tensor([])  # Create an empty tensor (of size (0,)) tensor([])   ", "parameters": ["data (array_like) : Initial data for the tensor. Can be a list, tuple,NumPy ndarray, scalar, and other types.", "dtype (torch.dtype, optional) : the desired data type of returned tensor.Default: if None, infers data type from data.", "device (torch.device, optional) : the desired device of returned tensor.Default: if None, uses the current device for the default tensor type(see torch.set_default_tensor_type()). device will be the CPUfor CPU tensor types and the current CUDA device for CUDA tensor types.", "requires_grad (bool, optional) : If autograd should record operations on thereturned tensor. Default: False.", "pin_memory (bool, optional) : If set, returned tensor would be allocated inthe pinned memory. Works only for CPU tensors. Default: False."], "returns": null, "example": " torch.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]])\ntensor([[ 0.1000,  1.2000],\n        [ 2.2000,  3.1000],\n        [ 4.9000,  5.2000]])\n\n torch.tensor([0, 1])  # Type inference on data\ntensor([ 0,  1])\n\n torch.tensor([[0.11111, 0.222222, 0.3333333]],\n                 dtype=torch.float64,\n                 device=torch.device('cuda:0'))  # creates a torch.cuda.DoubleTensor\ntensor([[ 0.1111,  0.2222,  0.3333]], dtype=torch.float64, device='cuda:0')\n\n torch.tensor(3.14159)  # Create a scalar (zero-dimensional tensor)\ntensor(3.1416)\n\n torch.tensor([])  # Create an empty tensor (of size (0,))\ntensor([])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.sparse_coo_tensor", "item_type": "function", "code": "torch.sparse_coo_tensor(indices,values,size=None,dtype=None,device=None,requires_grad=False)\u2192Tensor", "description": "Constructs a sparse tensors in COO(rdinate) format with non-zero elements at the given indices with the given values. A sparse tensor can be uncoalesced, in that case, there are duplicate coordinates in the indices, and the value at that index is the sum of all duplicate value entries: torch.sparse.  Parameters  indices (array_like) \u2013 Initial data for the tensor. Can be a list, tuple, NumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor internally. The indices are the coordinates of the non-zero values in the matrix, and thus should be two-dimensional where the first dimension is the number of tensor dimensions and the second dimension is the number of non-zero values. values (array_like) \u2013 Initial values for the tensor. Can be a list, tuple, NumPy ndarray, scalar, and other types. size (list, tuple, or torch.Size, optional) \u2013 Size of the sparse tensor. If not provided the size will be inferred as the minimum size big enough to hold all non-zero elements. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, infers data type from values. device (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: &gt;&gt;&gt; i = torch.tensor([[0, 1, 1],                       [2, 0, 2]]) &gt;&gt;&gt; v = torch.tensor([3, 4, 5], dtype=torch.float32) &gt;&gt;&gt; torch.sparse_coo_tensor(i, v, [2, 4]) tensor(indices=tensor([[0, 1, 1],                        [2, 0, 2]]),        values=tensor([3., 4., 5.]),        size=(2, 4), nnz=3, layout=torch.sparse_coo)  &gt;&gt;&gt; torch.sparse_coo_tensor(i, v)  # Shape inference tensor(indices=tensor([[0, 1, 1],                        [2, 0, 2]]),        values=tensor([3., 4., 5.]),        size=(2, 3), nnz=3, layout=torch.sparse_coo)  &gt;&gt;&gt; torch.sparse_coo_tensor(i, v, [2, 4],                             dtype=torch.float64,                             device=torch.device('cuda:0')) tensor(indices=tensor([[0, 1, 1],                        [2, 0, 2]]),        values=tensor([3., 4., 5.]),        device='cuda:0', size=(2, 4), nnz=3, dtype=torch.float64,        layout=torch.sparse_coo)  # Create an empty sparse tensor with the following invariants: #   1. sparse_dim + dense_dim = len(SparseTensor.shape) #   2. SparseTensor._indices().shape = (sparse_dim, nnz) #   3. SparseTensor._values().shape = (nnz, SparseTensor.shape[sparse_dim:]) # # For instance, to create an empty sparse tensor with nnz = 0, dense_dim = 0 and # sparse_dim = 1 (hence indices is a 2D tensor of shape = (1, 0)) &gt;&gt;&gt; S = torch.sparse_coo_tensor(torch.empty([1, 0]), [], [1]) tensor(indices=tensor([], size=(1, 0)),        values=tensor([], size=(0,)),        size=(1,), nnz=0, layout=torch.sparse_coo)  # and to create an empty sparse tensor with nnz = 0, dense_dim = 1 and # sparse_dim = 1 &gt;&gt;&gt; S = torch.sparse_coo_tensor(torch.empty([1, 0]), torch.empty([0, 2]), [1, 2]) tensor(indices=tensor([], size=(1, 0)),        values=tensor([], size=(0, 2)),        size=(1, 2), nnz=0, layout=torch.sparse_coo)   ", "parameters": ["indices (array_like) : Initial data for the tensor. Can be a list, tuple,NumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensorinternally. The indices are the coordinates of the non-zero values in the matrix, and thusshould be two-dimensional where the first dimension is the number of tensor dimensions andthe second dimension is the number of non-zero values.", "values (array_like) : Initial values for the tensor. Can be a list, tuple,NumPy ndarray, scalar, and other types.", "size (list, tuple, or torch.Size, optional) : Size of the sparse tensor. If notprovided the size will be inferred as the minimum size big enough to hold all non-zeroelements.", "dtype (torch.dtype, optional) : the desired data type of returned tensor.Default: if None, infers data type from values.", "device (torch.device, optional) : the desired device of returned tensor.Default: if None, uses the current device for the default tensor type(see torch.set_default_tensor_type()). device will be the CPUfor CPU tensor types and the current CUDA device for CUDA tensor types.", "requires_grad (bool, optional) : If autograd should record operations on thereturned tensor. Default: False."], "returns": null, "example": " i = torch.tensor([[0, 1, 1],\n                      [2, 0, 2]])\n v = torch.tensor([3, 4, 5], dtype=torch.float32)\n torch.sparse_coo_tensor(i, v, [2, 4])\ntensor(indices=tensor([[0, 1, 1],\n                       [2, 0, 2]]),\n       values=tensor([3., 4., 5.]),\n       size=(2, 4), nnz=3, layout=torch.sparse_coo)\n\n torch.sparse_coo_tensor(i, v)  # Shape inference\ntensor(indices=tensor([[0, 1, 1],\n                       [2, 0, 2]]),\n       values=tensor([3., 4., 5.]),\n       size=(2, 3), nnz=3, layout=torch.sparse_coo)\n\n torch.sparse_coo_tensor(i, v, [2, 4],\n                            dtype=torch.float64,\n                            device=torch.device('cuda:0'))\ntensor(indices=tensor([[0, 1, 1],\n                       [2, 0, 2]]),\n       values=tensor([3., 4., 5.]),\n       device='cuda:0', size=(2, 4), nnz=3, dtype=torch.float64,\n       layout=torch.sparse_coo)\n\n# Create an empty sparse tensor with the following invariants:\n#   1. sparse_dim + dense_dim = len(SparseTensor.shape)\n#   2. SparseTensor._indices().shape = (sparse_dim, nnz)\n#   3. SparseTensor._values().shape = (nnz, SparseTensor.shape[sparse_dim:])\n#\n# For instance, to create an empty sparse tensor with nnz = 0, dense_dim = 0 and\n# sparse_dim = 1 (hence indices is a 2D tensor of shape = (1, 0))\n S = torch.sparse_coo_tensor(torch.empty([1, 0]), [], [1])\ntensor(indices=tensor([], size=(1, 0)),\n       values=tensor([], size=(0,)),\n       size=(1,), nnz=0, layout=torch.sparse_coo)\n\n# and to create an empty sparse tensor with nnz = 0, dense_dim = 1 and\n# sparse_dim = 1\n S = torch.sparse_coo_tensor(torch.empty([1, 0]), torch.empty([0, 2]), [1, 2])\ntensor(indices=tensor([], size=(1, 0)),\n       values=tensor([], size=(0, 2)),\n       size=(1, 2), nnz=0, layout=torch.sparse_coo)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.cauchy.Cauchy.rsample", "item_type": "method", "code": "rsample(sample_shape=torch.Size([]))", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.cauchy.Cauchy.variance", "item_type": "method", "code": "propertyvariance", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.chi2.Chi2.df", "item_type": "method", "code": "propertydf", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.chi2.Chi2.expand", "item_type": "method", "code": "expand(batch_shape,_instance=None)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.dirichlet.Dirichlet.entropy", "item_type": "method", "code": "entropy()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.dirichlet.Dirichlet.expand", "item_type": "method", "code": "expand(batch_shape,_instance=None)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.dirichlet.Dirichlet.log_prob", "item_type": "method", "code": "log_prob(value)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.dirichlet.Dirichlet.mean", "item_type": "method", "code": "propertymean", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.dirichlet.Dirichlet.rsample", "item_type": "method", "code": "rsample(sample_shape=())", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.dirichlet.Dirichlet.variance", "item_type": "method", "code": "propertyvariance", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.exponential.Exponential.cdf", "item_type": "method", "code": "cdf(value)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.exponential.Exponential.entropy", "item_type": "method", "code": "entropy()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "to(dtype,non_blocking=False)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "to(tensor,non_blocking=False)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.clip_grad_norm_", "item_type": "function", "code": "torch.nn.utils.clip_grad_norm_(parameters,max_norm,norm_type=2)", "description": "Clips gradient norm of an iterable of parameters. The norm is computed over all gradients together, as if they were concatenated into a single vector. Gradients are modified in-place.  Parameters  parameters (Iterable[Tensor] or Tensor) \u2013 an iterable of Tensors or a single Tensor that will have gradients normalized max_norm (python:float or python:int) \u2013 max norm of the gradients norm_type (python:float or python:int) \u2013 type of the used p-norm. Can be 'inf' for infinity norm.   Returns Total norm of the parameters (viewed as a single vector).   ", "parameters": ["parameters (Iterable[Tensor] or Tensor) : an iterable of Tensors or asingle Tensor that will have gradients normalized", "max_norm (python:float or python:int) : max norm of the gradients", "norm_type (python:float or python:int) : type of the used p-norm. Can be 'inf' forinfinity norm."], "returns": "Total norm of the parameters (viewed as a single vector).", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.clip_grad_value_", "item_type": "function", "code": "torch.nn.utils.clip_grad_value_(parameters,clip_value)", "description": "Clips gradient of an iterable of parameters at specified value. Gradients are modified in-place.  Parameters  parameters (Iterable[Tensor] or Tensor) \u2013 an iterable of Tensors or a single Tensor that will have gradients normalized clip_value (python:float or python:int) \u2013 maximum allowed value of the gradients. The gradients are clipped in the range [-clip_value,clip_value]\\left[\\text{-clip\\_value}, \\text{clip\\_value}\\right][-clip_value,clip_value]      ", "parameters": ["parameters (Iterable[Tensor] or Tensor) : an iterable of Tensors or asingle Tensor that will have gradients normalized", "clip_value (python:float or python:int) : maximum allowed value of the gradients.The gradients are clipped in the range[-clip_value,clip_value]\\left[\\text{-clip\\_value}, \\text{clip\\_value}\\right][-clip_value,clip_value]"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.parameters_to_vector", "item_type": "function", "code": "torch.nn.utils.parameters_to_vector(parameters)", "description": "Convert parameters to one vector  Parameters parameters (Iterable[Tensor]) \u2013 an iterator of Tensors that are the parameters of a model.  Returns The parameters represented by a single vector   ", "parameters": ["parameters (Iterable[Tensor]) : an iterator of Tensors that are theparameters of a model.", "The parameters represented by a single vector"], "returns": "The parameters represented by a single vector", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.vector_to_parameters", "item_type": "function", "code": "torch.nn.utils.vector_to_parameters(vec,parameters)", "description": "Convert one vector to the parameters  Parameters  vec (Tensor) \u2013 a single vector represents the parameters of a model. parameters (Iterable[Tensor]) \u2013 an iterator of Tensors that are the parameters of a model.    ", "parameters": ["vec (Tensor) : a single vector represents the parameters of a model.", "parameters (Iterable[Tensor]) : an iterator of Tensors that are theparameters of a model."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.identity", "item_type": "function", "code": "torch.nn.utils.prune.identity(module,name)", "description": "Applies pruning reparametrization to the tensor corresponding to the parameter called name in module without actually pruning any units. Modifies module in place (and also return the modified module) by: 1) adding a named buffer called name+'_mask' corresponding to the binary mask applied to the parameter name by the pruning method. 2) replacing the parameter name by its pruned version, while the original (unpruned) parameter is stored in a new parameter named name+'_orig'.  Note The mask is a tensor of ones.   Parameters  module (nn.Module) \u2013 module containing the tensor to prune. name (str) \u2013 parameter name within module on which pruning will act.   Returns modified (i.e. pruned) version of the input module  Return type module (nn.Module)   Examples &gt;&gt;&gt; m = prune.identity(nn.Linear(2, 3), 'bias') &gt;&gt;&gt; print(m.bias_mask) tensor([1., 1., 1.])   ", "parameters": ["module (nn.Module) : module containing the tensor to prune.", "name (str) : parameter name within module on which pruningwill act."], "returns": "modified (i.e. pruned) version of the input module", "example": " m = prune.identity(nn.Linear(2, 3), 'bias')\n print(m.bias_mask)\ntensor([1., 1., 1.])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.random_unstructured", "item_type": "function", "code": "torch.nn.utils.prune.random_unstructured(module,name,amount)", "description": "Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) units selected at random. Modifies module in place (and also return the modified module) by: 1) adding a named buffer called name+'_mask' corresponding to the binary mask applied to the parameter name by the pruning method. 2) replacing the parameter name by its pruned version, while the original (unpruned) parameter is stored in a new parameter named name+'_orig'.  Parameters  module (nn.Module) \u2013 module containing the tensor to prune name (str) \u2013 parameter name within module on which pruning will act. amount (python:int or python:float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune.   Returns modified (i.e. pruned) version of the input module  Return type module (nn.Module)   Examples &gt;&gt;&gt; m = prune.random_unstructured(nn.Linear(2, 3), 'weight', amount=1) &gt;&gt;&gt; torch.sum(m.weight_mask == 0) tensor(1)   ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "name (str) : parameter name within module on which pruningwill act.", "amount (python:int or python:float) : quantity of parameters to prune.If float, should be between 0.0 and 1.0 and represent thefraction of parameters to prune. If int, it represents theabsolute number of parameters to prune."], "returns": "modified (i.e. pruned) version of the input module", "example": " m = prune.random_unstructured(nn.Linear(2, 3), 'weight', amount=1)\n torch.sum(m.weight_mask == 0)\ntensor(1)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.Stream.synchronize", "item_type": "method", "code": "synchronize()", "description": "Wait for all the kernels in this stream to complete.  Note This is a wrapper around cudaStreamSynchronize(): see CUDA Stream documentation for more info.  ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.Stream.wait_event", "item_type": "method", "code": "wait_event(event)", "description": "Makes all future work submitted to the stream wait for an event.  Parameters event (Event) \u2013 an event to wait for.    Note This is a wrapper around cudaStreamWaitEvent(): see CUDA Stream documentation for more info. This function returns without waiting for event: only future operations are affected.  ", "parameters": ["event (Event) : an event to wait for."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.Stream.wait_stream", "item_type": "method", "code": "wait_stream(stream)", "description": "Synchronizes with another stream. All future work submitted to this stream will wait until all kernels submitted to a given stream at the time of call complete.  Parameters stream (Stream) \u2013 a stream to synchronize.    Note This function returns without waiting for currently enqueued kernels in stream: only future operations are affected.  ", "parameters": ["stream (Stream) : a stream to synchronize."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.Event.elapsed_time", "item_type": "method", "code": "elapsed_time(end_event)", "description": "Returns the time elapsed in milliseconds after the event was recorded and before the end_event was recorded. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.Event.from_ipc_handle", "item_type": "method", "code": "classmethodfrom_ipc_handle(device,handle)", "description": "Reconstruct an event from an IPC handle on the given device. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.Event.ipc_handle", "item_type": "method", "code": "ipc_handle()", "description": "Returns an IPC handle of this event. If not recorded yet, the event will use the current device. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.Event.query", "item_type": "method", "code": "query()", "description": "Checks if all work currently captured by event has completed.  Returns A boolean indicating if all work currently captured by event has completed.   ", "parameters": [], "returns": "A boolean indicating if all work currently captured by event hascompleted.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.Event.record", "item_type": "method", "code": "record(stream=None)", "description": "Records the event in a given stream. Uses torch.cuda.current_stream() if no stream is specified. The stream\u2019s device must match the event\u2019s device. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.Event.synchronize", "item_type": "method", "code": "synchronize()", "description": "Waits for the event to complete. Waits until the completion of all work currently captured in this event. This prevents the CPU thread from proceeding until the event completes.   Note This is a wrapper around cudaEventSynchronize(): see CUDA Event documentation for more info.   ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.Event.wait", "item_type": "method", "code": "wait(stream=None)", "description": "Makes all future work submitted to the given stream wait for this event. Use torch.cuda.current_stream() if no stream is specified. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.device", "item_type": "class", "code": "classtorch.cuda.device(device)", "description": "Context-manager that changes the selected device.  Parameters device (torch.device or python:int) \u2013 device index to select. It\u2019s a no-op if this argument is a negative integer or None.   ", "parameters": ["device (torch.device or python:int) : device index to select. It\u2019s a no-op ifthis argument is a negative integer or None."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.intrinsic.ConvBn2d", "item_type": "class", "code": "classtorch.nn.intrinsic.ConvBn2d(conv,bn)", "description": "This is a sequential container which calls the Conv 2d and Batch Norm 2d modules. During quantization this will be replaced with the corresponding fused module. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.intrinsic.ConvBnReLU2d", "item_type": "class", "code": "classtorch.nn.intrinsic.ConvBnReLU2d(conv,bn,relu)", "description": "This is a sequential container which calls the Conv 2d, Batch Norm 2d, and ReLU modules. During quantization this will be replaced with the corresponding fused module. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.intrinsic.ConvReLU2d", "item_type": "class", "code": "classtorch.nn.intrinsic.ConvReLU2d(conv,relu)", "description": "This is a sequential container which calls the Conv 2d and ReLU modules. During quantization this will be replaced with the corresponding fused module. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.intrinsic.ConvReLU3d", "item_type": "class", "code": "classtorch.nn.intrinsic.ConvReLU3d(conv,relu)", "description": "This is a sequential container which calls the Conv 3d and ReLU modules. During quantization this will be replaced with the corresponding fused module. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.intrinsic.LinearReLU", "item_type": "class", "code": "classtorch.nn.intrinsic.LinearReLU(linear,relu)", "description": "This is a sequential container which calls the Linear and ReLU modules. During quantization this will be replaced with the corresponding fused module. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.intrinsic.qat.ConvBn2d", "item_type": "class", "code": "classtorch.nn.intrinsic.qat.ConvBn2d(in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,padding_mode='zeros',eps=1e-05,momentum=0.1,freeze_bn=False,qconfig=None)", "description": "A ConvBn2d module is a module fused from Conv2d and BatchNorm2d, attached with FakeQuantize modules for both output activation and weight, used in quantization aware training. We combined the interface of torch.nn.Conv2d and torch.nn.BatchNorm2d. Implementation details: https://arxiv.org/pdf/1806.08342.pdf section 3.2.2 Similar to torch.nn.Conv2d, with FakeQuantize modules initialized to default.  Variables  ~ConvBn2d.freeze_bn \u2013  ~ConvBn2d.activation_post_process \u2013 fake quant module for output activation ~ConvBn2d.weight_fake_quant \u2013 fake quant module for weight      classmethod from_float(mod, qconfig=None)  Create a qat module from a float module or qparams_dict Args: mod a float module, either produced by torch.quantization utilities or directly from user   ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.intrinsic.qat.ConvBnReLU2d", "item_type": "class", "code": "classtorch.nn.intrinsic.qat.ConvBnReLU2d(in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,padding_mode='zeros',eps=1e-05,momentum=0.1,freeze_bn=False,qconfig=None)", "description": "A ConvBnReLU2d module is a module fused from Conv2d, BatchNorm2d and ReLU, attached with FakeQuantize modules for both output activation and weight, used in quantization aware training. We combined the interface of torch.nn.Conv2d and torch.nn.BatchNorm2d and torch.nn.ReLU. Implementation details: https://arxiv.org/pdf/1806.08342.pdf Similar to torch.nn.Conv2d, with FakeQuantize modules initialized to default.  Variables  ~ConvBnReLU2d.observer \u2013 fake quant module for output activation, it\u2019s called observer to align with post training flow ~ConvBnReLU2d.weight_fake_quant \u2013 fake quant module for weight    ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.intrinsic.qat.ConvReLU2d", "item_type": "class", "code": "classtorch.nn.intrinsic.qat.ConvReLU2d(in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',qconfig=None)", "description": "A ConvReLU2d module is a fused module of Conv2d and ReLU, attached with FakeQuantize modules for both output activation and weight for quantization aware training. We combined the interface of Conv2d and BatchNorm2d.  Variables  ~ConvReLU2d.activation_post_process \u2013 fake quant module for output activation ~ConvReLU2d.weight_fake_quant \u2013 fake quant module for weight    ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.optim.lr_scheduler.OneCycleLR", "item_type": "class", "code": "classtorch.optim.lr_scheduler.OneCycleLR(optimizer,max_lr,total_steps=None,epochs=None,steps_per_epoch=None,pct_start=0.3,anneal_strategy='cos',cycle_momentum=True,base_momentum=0.85,max_momentum=0.95,div_factor=25.0,final_div_factor=10000.0,last_epoch=-1)", "description": "Sets the learning rate of each parameter group according to the 1cycle learning rate policy. The 1cycle policy anneals the learning rate from an initial learning rate to some maximum learning rate and then from that maximum learning rate to some minimum learning rate much lower than the initial learning rate. This policy was initially described in the paper Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates. The 1cycle learning rate policy changes the learning rate after every batch. step should be called after a batch has been used for training. This scheduler is not chainable. Note also that the total number of steps in the cycle can be determined in one of two ways (listed in order of precedence):  A value for total_steps is explicitly provided. A number of epochs (epochs) and a number of steps per epoch (steps_per_epoch) are provided. In this case, the number of total steps is inferred by total_steps = epochs * steps_per_epoch  You must either provide a value for total_steps or provide a value for both epochs and steps_per_epoch.  Parameters  optimizer (Optimizer) \u2013 Wrapped optimizer. max_lr (python:float or list) \u2013 Upper learning rate boundaries in the cycle for each parameter group. total_steps (python:int) \u2013 The total number of steps in the cycle. Note that if a value is provided here, then it must be inferred by providing a value for epochs and steps_per_epoch. Default: None epochs (python:int) \u2013 The number of epochs to train for. This is used along with steps_per_epoch in order to infer the total number of steps in the cycle if a value for total_steps is not provided. Default: None steps_per_epoch (python:int) \u2013 The number of steps per epoch to train for. This is used along with epochs in order to infer the total number of steps in the cycle if a value for total_steps is not provided. Default: None pct_start (python:float) \u2013 The percentage of the cycle (in number of steps) spent increasing the learning rate. Default: 0.3 anneal_strategy (str) \u2013 {\u2018cos\u2019, \u2018linear\u2019} Specifies the annealing strategy: \u201ccos\u201d for cosine annealing, \u201clinear\u201d for linear annealing. Default: \u2018cos\u2019 cycle_momentum (bool) \u2013 If True, momentum is cycled inversely to learning rate between \u2018base_momentum\u2019 and \u2018max_momentum\u2019. Default: True base_momentum (python:float or list) \u2013 Lower momentum boundaries in the cycle for each parameter group. Note that momentum is cycled inversely to learning rate; at the peak of a cycle, momentum is \u2018base_momentum\u2019 and learning rate is \u2018max_lr\u2019. Default: 0.85 max_momentum (python:float or list) \u2013 Upper momentum boundaries in the cycle for each parameter group. Functionally, it defines the cycle amplitude (max_momentum - base_momentum). Note that momentum is cycled inversely to learning rate; at the start of a cycle, momentum is \u2018max_momentum\u2019 and learning rate is \u2018base_lr\u2019 Default: 0.95 div_factor (python:float) \u2013 Determines the initial learning rate via initial_lr = max_lr/div_factor Default: 25 final_div_factor (python:float) \u2013 Determines the minimum learning rate via min_lr = initial_lr/final_div_factor Default: 1e4 last_epoch (python:int) \u2013 The index of the last batch. This parameter is used when resuming a training job. Since step() should be invoked after each batch instead of after each epoch, this number represents the total number of batches computed, not the total number of epochs computed. When last_epoch=-1, the schedule is started from the beginning. Default: -1    Example &gt;&gt;&gt; data_loader = torch.utils.data.DataLoader(...) &gt;&gt;&gt; optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9) &gt;&gt;&gt; scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(data_loader), epochs=10) &gt;&gt;&gt; for epoch in range(10): &gt;&gt;&gt;     for batch in data_loader: &gt;&gt;&gt;         train_batch(...) &gt;&gt;&gt;         scheduler.step()   ", "parameters": ["optimizer (Optimizer) : Wrapped optimizer.", "max_lr (python:float or list) : Upper learning rate boundaries in the cyclefor each parameter group.", "total_steps (python:int) : The total number of steps in the cycle. Note thatif a value is provided here, then it must be inferred by providinga value for epochs and steps_per_epoch.Default: None", "epochs (python:int) : The number of epochs to train for. This is used alongwith steps_per_epoch in order to infer the total number of steps in the cycleif a value for total_steps is not provided.Default: None", "steps_per_epoch (python:int) : The number of steps per epoch to train for. This isused along with epochs in order to infer the total number of steps in thecycle if a value for total_steps is not provided.Default: None", "pct_start (python:float) : The percentage of the cycle (in number of steps) spentincreasing the learning rate.Default: 0.3", "anneal_strategy (str) : {\u2018cos\u2019, \u2018linear\u2019}Specifies the annealing strategy: \u201ccos\u201d for cosine annealing, \u201clinear\u201d forlinear annealing.Default: \u2018cos\u2019", "cycle_momentum (bool) : If True, momentum is cycled inverselyto learning rate between \u2018base_momentum\u2019 and \u2018max_momentum\u2019.Default: True", "base_momentum (python:float or list) : Lower momentum boundaries in the cyclefor each parameter group. Note that momentum is cycled inverselyto learning rate; at the peak of a cycle, momentum is\u2018base_momentum\u2019 and learning rate is \u2018max_lr\u2019.Default: 0.85", "max_momentum (python:float or list) : Upper momentum boundaries in the cyclefor each parameter group. Functionally,it defines the cycle amplitude (max_momentum - base_momentum).Note that momentum is cycled inverselyto learning rate; at the start of a cycle, momentum is \u2018max_momentum\u2019and learning rate is \u2018base_lr\u2019Default: 0.95", "div_factor (python:float) : Determines the initial learning rate viainitial_lr = max_lr/div_factorDefault: 25", "final_div_factor (python:float) : Determines the minimum learning rate viamin_lr = initial_lr/final_div_factorDefault: 1e4", "last_epoch (python:int) : The index of the last batch. This parameter is used whenresuming a training job. Since step() should be invoked after eachbatch instead of after each epoch, this number represents the totalnumber of batches computed, not the total number of epochs computed.When last_epoch=-1, the schedule is started from the beginning.Default: -1"], "returns": null, "example": " data_loader = torch.utils.data.DataLoader(...)\n optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(data_loader), epochs=10)\n for epoch in range(10):\n     for batch in data_loader:\n         train_batch(...)\n         scheduler.step()\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.optim.lr_scheduler.CosineAnnealingWarmRestarts", "item_type": "class", "code": "classtorch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0,T_mult=1,eta_min=0,last_epoch=-1)", "description": "Set the learning rate of each parameter group using a cosine annealing schedule, where \u03b7max\\eta_{max}\u03b7max\u200b   is set to the initial lr, TcurT_{cur}Tcur\u200b   is the number of epochs since the last restart and TiT_{i}Ti\u200b   is the number of epochs between two warm restarts in SGDR:  \u03b7t=\u03b7min+12(\u03b7max\u2212\u03b7min)(1+cos\u2061(TcurTi\u03c0))\\eta_t = \\eta_{min} + \\frac{1}{2}(\\eta_{max} - \\eta_{min})\\left(1 + \\cos\\left(\\frac{T_{cur}}{T_{i}}\\pi\\right)\\right)  \u03b7t\u200b=\u03b7min\u200b+21\u200b(\u03b7max\u200b\u2212\u03b7min\u200b)(1+cos(Ti\u200bTcur\u200b\u200b\u03c0))  When Tcur=TiT_{cur}=T_{i}Tcur\u200b=Ti\u200b  , set \u03b7t=\u03b7min\\eta_t = \\eta_{min}\u03b7t\u200b=\u03b7min\u200b  . When Tcur=0T_{cur}=0Tcur\u200b=0   after restart, set \u03b7t=\u03b7max\\eta_t=\\eta_{max}\u03b7t\u200b=\u03b7max\u200b  . It has been proposed in SGDR: Stochastic Gradient Descent with Warm Restarts.  Parameters  optimizer (Optimizer) \u2013 Wrapped optimizer. T_0 (python:int) \u2013 Number of iterations for the first restart. T_mult (python:int, optional) \u2013 A factor increases TiT_{i}Ti\u200b   after a restart. Default: 1. eta_min (python:float, optional) \u2013 Minimum learning rate. Default: 0. last_epoch (python:int, optional) \u2013 The index of last epoch. Default: -1.      step(epoch=None)  Step could be called after every batch update Example &gt;&gt;&gt; scheduler = CosineAnnealingWarmRestarts(optimizer, T_0, T_mult) &gt;&gt;&gt; iters = len(dataloader) &gt;&gt;&gt; for epoch in range(20): &gt;&gt;&gt;     for i, sample in enumerate(dataloader): &gt;&gt;&gt;         inputs, labels = sample['inputs'], sample['labels'] &gt;&gt;&gt;         scheduler.step(epoch + i / iters) &gt;&gt;&gt;         optimizer.zero_grad() &gt;&gt;&gt;         outputs = net(inputs) &gt;&gt;&gt;         loss = criterion(outputs, labels) &gt;&gt;&gt;         loss.backward() &gt;&gt;&gt;         optimizer.step()   This function can be called in an interleaved way. Example &gt;&gt;&gt; scheduler = CosineAnnealingWarmRestarts(optimizer, T_0, T_mult) &gt;&gt;&gt; for epoch in range(20): &gt;&gt;&gt;     scheduler.step() &gt;&gt;&gt; scheduler.step(26) &gt;&gt;&gt; scheduler.step() # scheduler.step(27), instead of scheduler(20)     ", "parameters": ["optimizer (Optimizer) : Wrapped optimizer.", "T_0 (python:int) : Number of iterations for the first restart.", "T_mult (python:int, optional) : A factor increases TiT_{i}Ti\u200b after a restart. Default: 1.", "eta_min (python:float, optional) : Minimum learning rate. Default: 0.", "last_epoch (python:int, optional) : The index of last epoch. Default: -1."], "returns": null, "example": " scheduler = CosineAnnealingWarmRestarts(optimizer, T_0, T_mult)\n iters = len(dataloader)\n for epoch in range(20):\n     for i, sample in enumerate(dataloader):\n         inputs, labels = sample['inputs'], sample['labels']\n         scheduler.step(epoch + i / iters)\n         optimizer.zero_grad()\n         outputs = net(inputs)\n         loss = criterion(outputs, labels)\n         loss.backward()\n         optimizer.step()\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.bitwise_xor", "item_type": "method", "code": "bitwise_xor()\u2192Tensor", "description": "See torch.bitwise_xor() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.bitwise_xor_", "item_type": "method", "code": "bitwise_xor_()\u2192Tensor", "description": "In-place version of bitwise_xor() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.bmm", "item_type": "method", "code": "bmm(batch2)\u2192Tensor", "description": "See torch.bmm() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.bool", "item_type": "method", "code": "bool()\u2192Tensor", "description": "self.bool() is equivalent to self.to(torch.bool). See to(). ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.byte", "item_type": "method", "code": "byte()\u2192Tensor", "description": "self.byte() is equivalent to self.to(torch.uint8). See to(). ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.cauchy_", "item_type": "method", "code": "cauchy_(median=0,sigma=1,*,generator=None)\u2192Tensor", "description": "Fills the tensor with numbers drawn from the Cauchy distribution:  f(x)=1\u03c0\u03c3(x\u2212median)2+\u03c32f(x) = \\dfrac{1}{\\pi} \\dfrac{\\sigma}{(x - \\text{median})^2 + \\sigma^2}f(x)=\u03c01\u200b(x\u2212median)2+\u03c32\u03c3\u200b  ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.ceil", "item_type": "method", "code": "ceil()\u2192Tensor", "description": "See torch.ceil() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.ceil_", "item_type": "method", "code": "ceil_()\u2192Tensor", "description": "In-place version of ceil() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.char", "item_type": "method", "code": "char()\u2192Tensor", "description": "self.char() is equivalent to self.to(torch.int8). See to(). ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.cholesky", "item_type": "method", "code": "cholesky(upper=False)\u2192Tensor", "description": "See torch.cholesky() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.cholesky_inverse", "item_type": "method", "code": "cholesky_inverse(upper=False)\u2192Tensor", "description": "See torch.cholesky_inverse() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.cholesky_solve", "item_type": "method", "code": "cholesky_solve(input2,upper=False)\u2192Tensor", "description": "See torch.cholesky_solve() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.elu_", "item_type": "function", "code": "torch.nn.functional.elu_(input,alpha=1.)\u2192Tensor", "description": "In-place version of elu(). ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.selu", "item_type": "function", "code": "torch.nn.functional.selu(input,inplace=False)\u2192Tensor", "description": "Applies element-wise, SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))SELU(x)=scale\u2217(max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)))  , with \u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\u03b1=1.6732632423543772848170429916717   and scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946  . See SELU for more details. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.celu", "item_type": "function", "code": "torch.nn.functional.celu(input,alpha=1.,inplace=False)\u2192Tensor", "description": "Applies element-wise, CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))CELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x/\u03b1)\u22121))  . See CELU for more details. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.leaky_relu", "item_type": "function", "code": "torch.nn.functional.leaky_relu(input,negative_slope=0.01,inplace=False)\u2192Tensor", "description": "Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   See LeakyReLU for more details. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.leaky_relu_", "item_type": "function", "code": "torch.nn.functional.leaky_relu_(input,negative_slope=0.01)\u2192Tensor", "description": "In-place version of leaky_relu(). ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.prelu", "item_type": "function", "code": "torch.nn.functional.prelu(input,weight)\u2192Tensor", "description": "Applies element-wise the function PReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight\u2217min(0,x)   where weight is a learnable parameter. See PReLU for more details. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.rrelu", "item_type": "function", "code": "torch.nn.functional.rrelu(input,lower=1./8,upper=1./3,training=False,inplace=False)\u2192Tensor", "description": "Randomized leaky ReLU. See RReLU for more details. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.rrelu_", "item_type": "function", "code": "torch.nn.functional.rrelu_(input,lower=1./8,upper=1./3,training=False)\u2192Tensor", "description": "In-place version of rrelu(). ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.glu", "item_type": "function", "code": "torch.nn.functional.glu(input,dim=-1)\u2192Tensor", "description": "The gated linear unit. Computes:  GLU(a,b)=a\u2297\u03c3(b)\\text{GLU}(a, b) = a \\otimes \\sigma(b)  GLU(a,b)=a\u2297\u03c3(b)  where input is split in half along dim to form a and b, \u03c3\\sigma\u03c3   is the sigmoid function and \u2297\\otimes\u2297   is the element-wise product between matrices. See Language Modeling with Gated Convolutional Networks.  Parameters  input (Tensor) \u2013 input tensor dim (python:int) \u2013 dimension on which to split the input. Default: -1    ", "parameters": ["input (Tensor) : input tensor", "dim (python:int) : dimension on which to split the input. Default: -1"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.as_tensor", "item_type": "function", "code": "torch.as_tensor(data,dtype=None,device=None)\u2192Tensor", "description": "Convert the data into a torch.Tensor. If the data is already a Tensor with the same dtype and device, no copy will be performed, otherwise a new Tensor will be returned with computational graph retained if data Tensor has requires_grad=True. Similarly, if the data is an ndarray of the corresponding dtype and the device is the cpu, no copy will be performed.  Parameters  data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple, NumPy ndarray, scalar, and other types. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, infers data type from data. device (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.    Example: &gt;&gt;&gt; a = numpy.array([1, 2, 3]) &gt;&gt;&gt; t = torch.as_tensor(a) &gt;&gt;&gt; t tensor([ 1,  2,  3]) &gt;&gt;&gt; t[0] = -1 &gt;&gt;&gt; a array([-1,  2,  3])  &gt;&gt;&gt; a = numpy.array([1, 2, 3]) &gt;&gt;&gt; t = torch.as_tensor(a, device=torch.device('cuda')) &gt;&gt;&gt; t tensor([ 1,  2,  3]) &gt;&gt;&gt; t[0] = -1 &gt;&gt;&gt; a array([1,  2,  3])   ", "parameters": ["data (array_like) : Initial data for the tensor. Can be a list, tuple,NumPy ndarray, scalar, and other types.", "dtype (torch.dtype, optional) : the desired data type of returned tensor.Default: if None, infers data type from data.", "device (torch.device, optional) : the desired device of returned tensor.Default: if None, uses the current device for the default tensor type(see torch.set_default_tensor_type()). device will be the CPUfor CPU tensor types and the current CUDA device for CUDA tensor types."], "returns": null, "example": " a = numpy.array([1, 2, 3])\n t = torch.as_tensor(a)\n t\ntensor([ 1,  2,  3])\n t[0] = -1\n a\narray([-1,  2,  3])\n\n a = numpy.array([1, 2, 3])\n t = torch.as_tensor(a, device=torch.device('cuda'))\n t\ntensor([ 1,  2,  3])\n t[0] = -1\n a\narray([1,  2,  3])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.as_strided", "item_type": "function", "code": "torch.as_strided(input,size,stride,storage_offset=0)\u2192Tensor", "description": "Create a view of an existing torch.Tensor input with specified size, stride and storage_offset.  Warning More than one element of a created tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensors, please clone them first. Many PyTorch functions, which return a view of a tensor, are internally implemented with this function. Those functions, like torch.Tensor.expand(), are easier to read and are therefore more advisable to use.   Parameters  input (Tensor) \u2013 the input tensor. size (tuple or python:ints) \u2013 the shape of the output tensor stride (tuple or python:ints) \u2013 the stride of the output tensor storage_offset (python:int, optional) \u2013 the offset in the underlying storage of the output tensor    Example: &gt;&gt;&gt; x = torch.randn(3, 3) &gt;&gt;&gt; x tensor([[ 0.9039,  0.6291,  1.0795],         [ 0.1586,  2.1939, -0.4900],         [-0.1909, -0.7503,  1.9355]]) &gt;&gt;&gt; t = torch.as_strided(x, (2, 2), (1, 2)) &gt;&gt;&gt; t tensor([[0.9039, 1.0795],         [0.6291, 0.1586]]) &gt;&gt;&gt; t = torch.as_strided(x, (2, 2), (1, 2), 1) tensor([[0.6291, 0.1586],         [1.0795, 2.1939]])   ", "parameters": ["input (Tensor) : the input tensor.", "size (tuple or python:ints) : the shape of the output tensor", "stride (tuple or python:ints) : the stride of the output tensor", "storage_offset (python:int, optional) : the offset in the underlying storage of the output tensor"], "returns": null, "example": " x = torch.randn(3, 3)\n x\ntensor([[ 0.9039,  0.6291,  1.0795],\n        [ 0.1586,  2.1939, -0.4900],\n        [-0.1909, -0.7503,  1.9355]])\n t = torch.as_strided(x, (2, 2), (1, 2))\n t\ntensor([[0.9039, 1.0795],\n        [0.6291, 0.1586]])\n t = torch.as_strided(x, (2, 2), (1, 2), 1)\ntensor([[0.6291, 0.1586],\n        [1.0795, 2.1939]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.from_numpy", "item_type": "function", "code": "torch.from_numpy(ndarray)\u2192Tensor", "description": "Creates a Tensor from a numpy.ndarray. The returned tensor and ndarray share the same memory. Modifications to the tensor will be reflected in the ndarray and vice versa. The returned tensor is not resizable. It currently accepts ndarray with dtypes of numpy.float64, numpy.float32, numpy.float16, numpy.int64, numpy.int32, numpy.int16, numpy.int8, numpy.uint8, and numpy.bool. Example: &gt;&gt;&gt; a = numpy.array([1, 2, 3]) &gt;&gt;&gt; t = torch.from_numpy(a) &gt;&gt;&gt; t tensor([ 1,  2,  3]) &gt;&gt;&gt; t[0] = -1 &gt;&gt;&gt; a array([-1,  2,  3])   ", "parameters": [], "returns": null, "example": " a = numpy.array([1, 2, 3])\n t = torch.from_numpy(a)\n t\ntensor([ 1,  2,  3])\n t[0] = -1\n a\narray([-1,  2,  3])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.zeros", "item_type": "function", "code": "torch.zeros(*size,out=None,dtype=None,layout=torch.strided,device=None,requires_grad=False)\u2192Tensor", "description": "Returns a tensor filled with the scalar value 0, with the shape defined by the variable argument size.  Parameters  size (python:int...) \u2013 a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple. out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor. Default: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: &gt;&gt;&gt; torch.zeros(2, 3) tensor([[ 0.,  0.,  0.],         [ 0.,  0.,  0.]])  &gt;&gt;&gt; torch.zeros(5) tensor([ 0.,  0.,  0.,  0.,  0.])   ", "parameters": ["size (python:int...) : a sequence of integers defining the shape of the output tensor.Can be a variable number of arguments or a collection like a list or tuple.", "out (Tensor, optional) : the output tensor.", "dtype (torch.dtype, optional) : the desired data type of returned tensor.Default: if None, uses a global default (see torch.set_default_tensor_type()).", "layout (torch.layout, optional) : the desired layout of returned Tensor.Default: torch.strided.", "device (torch.device, optional) : the desired device of returned tensor.Default: if None, uses the current device for the default tensor type(see torch.set_default_tensor_type()). device will be the CPUfor CPU tensor types and the current CUDA device for CUDA tensor types.", "requires_grad (bool, optional) : If autograd should record operations on thereturned tensor. Default: False."], "returns": null, "example": " torch.zeros(2, 3)\ntensor([[ 0.,  0.,  0.],\n        [ 0.,  0.,  0.]])\n\n torch.zeros(5)\ntensor([ 0.,  0.,  0.,  0.,  0.])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.exponential.Exponential.expand", "item_type": "method", "code": "expand(batch_shape,_instance=None)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.exponential.Exponential.icdf", "item_type": "method", "code": "icdf(value)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.exponential.Exponential.log_prob", "item_type": "method", "code": "log_prob(value)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.exponential.Exponential.mean", "item_type": "method", "code": "propertymean", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.exponential.Exponential.rsample", "item_type": "method", "code": "rsample(sample_shape=torch.Size([]))", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.exponential.Exponential.stddev", "item_type": "method", "code": "propertystddev", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.exponential.Exponential.variance", "item_type": "method", "code": "propertyvariance", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.fishersnedecor.FisherSnedecor.expand", "item_type": "method", "code": "expand(batch_shape,_instance=None)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.fishersnedecor.FisherSnedecor.log_prob", "item_type": "method", "code": "log_prob(value)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.fishersnedecor.FisherSnedecor.mean", "item_type": "method", "code": "propertymean", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.fishersnedecor.FisherSnedecor.rsample", "item_type": "method", "code": "rsample(sample_shape=torch.Size([]))", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.fishersnedecor.FisherSnedecor.variance", "item_type": "method", "code": "propertyvariance", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.l1_unstructured", "item_type": "function", "code": "torch.nn.utils.prune.l1_unstructured(module,name,amount)", "description": "Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) units with the lowest L1-norm. Modifies module in place (and also return the modified module) by: 1) adding a named buffer called name+'_mask' corresponding to the binary mask applied to the parameter name by the pruning method. 2) replacing the parameter name by its pruned version, while the original (unpruned) parameter is stored in a new parameter named name+'_orig'.  Parameters  module (nn.Module) \u2013 module containing the tensor to prune name (str) \u2013 parameter name within module on which pruning will act. amount (python:int or python:float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune.   Returns modified (i.e. pruned) version of the input module  Return type module (nn.Module)   Examples &gt;&gt;&gt; m = prune.l1_unstructured(nn.Linear(2, 3), 'weight', amount=0.2) &gt;&gt;&gt; m.state_dict().keys() odict_keys(['bias', 'weight_orig', 'weight_mask'])   ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "name (str) : parameter name within module on which pruningwill act.", "amount (python:int or python:float) : quantity of parameters to prune.If float, should be between 0.0 and 1.0 and represent thefraction of parameters to prune. If int, it represents theabsolute number of parameters to prune."], "returns": "modified (i.e. pruned) version of the input module", "example": " m = prune.l1_unstructured(nn.Linear(2, 3), 'weight', amount=0.2)\n m.state_dict().keys()\nodict_keys(['bias', 'weight_orig', 'weight_mask'])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.random_structured", "item_type": "function", "code": "torch.nn.utils.prune.random_structured(module,name,amount,dim)", "description": "Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) channels along the specified dim selected at random. Modifies module in place (and also return the modified module) by: 1) adding a named buffer called name+'_mask' corresponding to the binary mask applied to the parameter name by the pruning method. 2) replacing the parameter name by its pruned version, while the original (unpruned) parameter is stored in a new parameter named name+'_orig'.  Parameters  module (nn.Module) \u2013 module containing the tensor to prune name (str) \u2013 parameter name within module on which pruning will act. amount (python:int or python:float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune. dim (python:int) \u2013 index of the dim along which we define channels to prune.   Returns modified (i.e. pruned) version of the input module  Return type module (nn.Module)   Examples &gt;&gt;&gt; m = prune.random_structured(         nn.Linear(5, 3), 'weight', amount=3, dim=1     ) &gt;&gt;&gt; columns_pruned = int(sum(torch.sum(m.weight, dim=0) == 0)) &gt;&gt;&gt; print(columns_pruned) 3   ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "name (str) : parameter name within module on which pruningwill act.", "amount (python:int or python:float) : quantity of parameters to prune.If float, should be between 0.0 and 1.0 and represent thefraction of parameters to prune. If int, it represents theabsolute number of parameters to prune.", "dim (python:int) : index of the dim along which we define channels to prune."], "returns": "modified (i.e. pruned) version of the input module", "example": " m = prune.random_structured(\n        nn.Linear(5, 3), 'weight', amount=3, dim=1\n    )\n columns_pruned = int(sum(torch.sum(m.weight, dim=0) == 0))\n print(columns_pruned)\n3\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.ln_structured", "item_type": "function", "code": "torch.nn.utils.prune.ln_structured(module,name,amount,n,dim)", "description": "Prunes tensor corresponding to parameter called name in module by removing the specified amount of (currently unpruned) channels along the specified dim with the lowest L``n``-norm. Modifies module in place (and also return the modified module) by: 1) adding a named buffer called name+'_mask' corresponding to the binary mask applied to the parameter name by the pruning method. 2) replacing the parameter name by its pruned version, while the original (unpruned) parameter is stored in a new parameter named name+'_orig'.  Parameters  module (nn.Module) \u2013 module containing the tensor to prune name (str) \u2013 parameter name within module on which pruning will act. amount (python:int or python:float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune. n (python:int, python:float, inf, -inf, 'fro', 'nuc') \u2013 See documentation of valid entries for argument p in torch.norm(). dim (python:int) \u2013 index of the dim along which we define channels to prune.   Returns modified (i.e. pruned) version of the input module  Return type module (nn.Module)   Examples &gt;&gt;&gt; m = prune.ln_structured(        nn.Conv2d(5, 3, 2), 'weight', amount=0.3, dim=1, n=float('-inf')     )   ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "name (str) : parameter name within module on which pruningwill act.", "amount (python:int or python:float) : quantity of parameters to prune.If float, should be between 0.0 and 1.0 and represent thefraction of parameters to prune. If int, it represents theabsolute number of parameters to prune.", "n (python:int, python:float, inf, -inf, 'fro', 'nuc') : See documentation of validentries for argument p in torch.norm().", "dim (python:int) : index of the dim along which we define channels to prune."], "returns": "modified (i.e. pruned) version of the input module", "example": " m = prune.ln_structured(\n       nn.Conv2d(5, 3, 2), 'weight', amount=0.3, dim=1, n=float('-inf')\n    )\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.global_unstructured", "item_type": "function", "code": "torch.nn.utils.prune.global_unstructured(parameters,pruning_method,**kwargs)", "description": "Globally prunes tensors corresponding to all parameters in parameters by applying the specified pruning_method. Modifies modules in place by: 1) adding a named buffer called name+'_mask' corresponding to the binary mask applied to the parameter name by the pruning method. 2) replacing the parameter name by its pruned version, while the original (unpruned) parameter is stored in a new parameter named name+'_orig'.  Parameters  parameters (Iterable of (module, name) tuples) \u2013 parameters of the model to prune in a global fashion, i.e. by aggregating all weights prior to deciding which ones to prune. module must be of type nn.Module, and name must be a string. pruning_method (function) \u2013 a valid pruning function from this module, or a custom one implemented by the user that satisfies the implementation guidelines and has PRUNING_TYPE='unstructured'. kwargs \u2013 other keyword arguments such as: amount (int or float): quantity of parameters to prune across the specified parameters. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune.   Raises TypeError \u2013 if PRUNING_TYPE != 'unstructured'    Note Since global structured pruning doesn\u2019t make much sense unless the norm is normalized by the size of the parameter, we now limit the scope of global pruning to unstructured methods.  Examples &gt;&gt;&gt; net = nn.Sequential(OrderedDict([         ('first', nn.Linear(10, 4)),         ('second', nn.Linear(4, 1)),     ])) &gt;&gt;&gt; parameters_to_prune = (         (net.first, 'weight'),         (net.second, 'weight'),     ) &gt;&gt;&gt; prune.global_unstructured(         parameters_to_prune,         pruning_method=prune.L1Unstructured,         amount=10,     ) &gt;&gt;&gt; print(sum(torch.nn.utils.parameters_to_vector(net.buffers()) == 0)) tensor(10, dtype=torch.uint8)   ", "parameters": ["parameters (Iterable of (module, name) tuples) : parameters ofthe model to prune in a global fashion, i.e. by aggregating allweights prior to deciding which ones to prune. module must be oftype nn.Module, and name must be a string.", "pruning_method (function) : a valid pruning function from this module,or a custom one implemented by the user that satisfies theimplementation guidelines and has PRUNING_TYPE='unstructured'.", "kwargs : other keyword arguments such as:amount (int or float): quantity of parameters to prune across thespecified parameters.If float, should be between 0.0 and 1.0 and represent thefraction of parameters to prune. If int, it represents theabsolute number of parameters to prune."], "returns": null, "example": " net = nn.Sequential(OrderedDict([\n        ('first', nn.Linear(10, 4)),\n        ('second', nn.Linear(4, 1)),\n    ]))\n parameters_to_prune = (\n        (net.first, 'weight'),\n        (net.second, 'weight'),\n    )\n prune.global_unstructured(\n        parameters_to_prune,\n        pruning_method=prune.L1Unstructured,\n        amount=10,\n    )\n print(sum(torch.nn.utils.parameters_to_vector(net.buffers()) == 0))\ntensor(10, dtype=torch.uint8)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.device_of", "item_type": "class", "code": "classtorch.cuda.device_of(obj)", "description": "Context-manager that changes the current device to that of given object. You can use both tensors and storages as arguments. If a given object is not allocated on a GPU, this is a no-op.  Parameters obj (Tensor or Storage) \u2013 object allocated on the selected device.   ", "parameters": ["obj (Tensor or Storage) : object allocated on the selected device."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.Stream", "item_type": "class", "code": "classtorch.cuda.Stream", "description": "Wrapper around a CUDA stream. A CUDA stream is a linear sequence of execution that belongs to a specific device, independent from other streams.  See CUDA semantics for details.  Parameters  device (torch.device or python:int, optional) \u2013 a device on which to allocate the stream. If device is None (default) or a negative integer, this will use the current device. priority (python:int, optional) \u2013 priority of the stream. Lower numbers represent higher priorities.      query()  Checks if all the work submitted has been completed.  Returns A boolean indicating if all kernels in this stream are completed.       record_event(event=None)  Records an event.  Parameters event (Event, optional) \u2013 event to record. If not given, a new one will be allocated.  Returns Recorded event.       synchronize()  Wait for all the kernels in this stream to complete.  Note This is a wrapper around cudaStreamSynchronize(): see CUDA Stream documentation for more info.      wait_event(event)  Makes all future work submitted to the stream wait for an event.  Parameters event (Event) \u2013 an event to wait for.    Note This is a wrapper around cudaStreamWaitEvent(): see CUDA Stream documentation for more info. This function returns without waiting for event: only future operations are affected.      wait_stream(stream)  Synchronizes with another stream. All future work submitted to this stream will wait until all kernels submitted to a given stream at the time of call complete.  Parameters stream (Stream) \u2013 a stream to synchronize.    Note This function returns without waiting for currently enqueued kernels in stream: only future operations are affected.    ", "parameters": ["device (torch.device or python:int, optional) : a device on which to allocatethe stream. If device is None (default) or a negativeinteger, this will use the current device.", "priority (python:int, optional) : priority of the stream. Lower numbersrepresent higher priorities."], "returns": "A boolean indicating if all kernels in this stream are completed.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.cuda.Event", "item_type": "class", "code": "classtorch.cuda.Event", "description": "Wrapper around a CUDA event. CUDA events are synchronization markers that can be used to monitor the device\u2019s progress, to accurately measure timing, and to synchronize CUDA streams. The underlying CUDA events are lazily initialized when the event is first recorded or exported to another process. After creation, only streams on the same device may record the event. However, streams on any device can wait on the event.  Parameters  enable_timing (bool, optional) \u2013 indicates if the event should measure time (default: False) blocking (bool, optional) \u2013 if True, wait() will be blocking (default: False) interprocess (bool) \u2013 if True, the event can be shared between processes (default: False)      elapsed_time(end_event)  Returns the time elapsed in milliseconds after the event was recorded and before the end_event was recorded.     classmethod from_ipc_handle(device, handle)  Reconstruct an event from an IPC handle on the given device.     ipc_handle()  Returns an IPC handle of this event. If not recorded yet, the event will use the current device.     query()  Checks if all work currently captured by event has completed.  Returns A boolean indicating if all work currently captured by event has completed.       record(stream=None)  Records the event in a given stream. Uses torch.cuda.current_stream() if no stream is specified. The stream\u2019s device must match the event\u2019s device.     synchronize()  Waits for the event to complete. Waits until the completion of all work currently captured in this event. This prevents the CPU thread from proceeding until the event completes.   Note This is a wrapper around cudaEventSynchronize(): see CUDA Event documentation for more info.       wait(stream=None)  Makes all future work submitted to the given stream wait for this event. Use torch.cuda.current_stream() if no stream is specified.   ", "parameters": ["enable_timing (bool, optional) : indicates if the event should measure time(default: False)", "blocking (bool, optional) : if True, wait() will be blocking (default: False)", "interprocess (bool) : if True, the event can be shared between processes(default: False)"], "returns": "A boolean indicating if all work currently captured by event hascompleted.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.intrinsic.qat.LinearReLU", "item_type": "class", "code": "classtorch.nn.intrinsic.qat.LinearReLU(in_features,out_features,bias=True,qconfig=None)", "description": "A LinearReLU module fused from Linear and ReLU modules, attached with FakeQuantize modules for output activation and weight, used in quantization aware training. We adopt the same interface as torch.nn.Linear. Similar to torch.nn.intrinsic.LinearReLU, with FakeQuantize modules initialized to default.  Variables  ~LinearReLU.activation_post_process \u2013 fake quant module for output activation ~LinearReLU.weight \u2013 fake quant module for weight    Examples: &gt;&gt;&gt; m = nn.qat.LinearReLU(20, 30) &gt;&gt;&gt; input = torch.randn(128, 20) &gt;&gt;&gt; output = m(input) &gt;&gt;&gt; print(output.size()) torch.Size([128, 30])   ", "parameters": [], "returns": null, "example": " m = nn.qat.LinearReLU(20, 30)\n input = torch.randn(128, 20)\n output = m(input)\n print(output.size())\ntorch.Size([128, 30])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.intrinsic.quantized.ConvReLU2d", "item_type": "class", "code": "classtorch.nn.intrinsic.quantized.ConvReLU2d(in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')", "description": "A ConvReLU2d module is a fused module of Conv2d and ReLU We adopt the same interface as torch.nn.quantized.Conv2d.  Variables as torch.nn.quantized.Conv2d (Same) \u2013    ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.intrinsic.quantized.ConvReLU3d", "item_type": "class", "code": "classtorch.nn.intrinsic.quantized.ConvReLU3d(in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')", "description": "A ConvReLU3d module is a fused module of Conv3d and ReLU We adopt the same interface as torch.nn.quantized.Conv3d. Attributes: Same as torch.nn.quantized.Conv3d ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.intrinsic.quantized.LinearReLU", "item_type": "class", "code": "classtorch.nn.intrinsic.quantized.LinearReLU(in_features,out_features,bias=True)", "description": "A LinearReLU module fused from Linear and ReLU modules We adopt the same interface as torch.nn.quantized.Linear.  Variables as torch.nn.quantized.Linear (Same) \u2013    Examples: &gt;&gt;&gt; m = nn.intrinsic.LinearReLU(20, 30) &gt;&gt;&gt; input = torch.randn(128, 20) &gt;&gt;&gt; output = m(input) &gt;&gt;&gt; print(output.size()) torch.Size([128, 30])   ", "parameters": [], "returns": null, "example": " m = nn.intrinsic.LinearReLU(20, 30)\n input = torch.randn(128, 20)\n output = m(input)\n print(output.size())\ntorch.Size([128, 30])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.qat.Conv2d", "item_type": "class", "code": "classtorch.nn.qat.Conv2d(in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros',qconfig=None)", "description": "A Conv2d module attached with FakeQuantize modules for both output activation and weight, used for quantization aware training. We adopt the same interface as torch.nn.Conv2d, please see https://pytorch.org/docs/stable/nn.html?highlight=conv2d#torch.nn.Conv2d for documentation. Similar to torch.nn.Conv2d, with FakeQuantize modules initialized to default.  Variables  ~Conv2d.activation_post_process \u2013 fake quant module for output activation ~Conv2d.weight_fake_quant \u2013 fake quant module for weight      classmethod from_float(mod, qconfig=None)  Create a qat module from a float module or qparams_dict Args: mod a float module, either produced by torch.quantization utilities or directly from user   ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.qat.Linear", "item_type": "class", "code": "classtorch.nn.qat.Linear(in_features,out_features,bias=True,qconfig=None)", "description": "A linear module attached with FakeQuantize modules for both output activation and weight, used for quantization aware training. We adopt the same interface as torch.nn.Linear, please see https://pytorch.org/docs/stable/nn.html#torch.nn.Linear for documentation. Similar to torch.nn.Linear, with FakeQuantize modules initialized to default.  Variables  ~Linear.activation_post_process \u2013 fake quant module for output activation ~Linear.weight \u2013 fake quant module for weight      classmethod from_float(mod, qconfig=None)  Create a qat module from a float module or qparams_dict Args: mod a float module, either produced by torch.quantization utilities or directly from user   ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.quantized.ReLU", "item_type": "class", "code": "classtorch.nn.quantized.ReLU(inplace=False)", "description": "Applies quantized rectified linear unit function element-wise: ReLU(x)=max\u2061(x0,x)\\text{ReLU}(x)= \\max(x_0, x)ReLU(x)=max(x0\u200b,x)  , where x0x_0x0\u200b   is the zero point. Please see https://pytorch.org/docs/stable/nn.html#torch.nn.ReLU for more documentation on ReLU.  Parameters inplace \u2013 (Currently not supported) can optionally do the operation in-place.    Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input    Examples: &gt;&gt;&gt; m = nn.quantized.ReLU() &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; input = torch.quantize_per_tensor(input, 1.0, 0, dtype=torch.qint32) &gt;&gt;&gt; output = m(input)   ", "parameters": ["inplace : (Currently not supported) can optionally do the operation in-place."], "returns": null, "example": " m = nn.quantized.ReLU()\n input = torch.randn(2)\n input = torch.quantize_per_tensor(input, 1.0, 0, dtype=torch.qint32)\n output = m(input)\n\n", "shape": " Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input  "},
{"library": "torch", "item_id": "torch.nn.quantized.ReLU6", "item_type": "class", "code": "classtorch.nn.quantized.ReLU6(inplace=False)", "description": "Applies the element-wise function: ReLU6(x)=min\u2061(max\u2061(x0,x),q(6))\\text{ReLU6}(x) = \\min(\\max(x_0, x), q(6))ReLU6(x)=min(max(x0\u200b,x),q(6))  , where x0x_0x0\u200b   is the zero_point, and q(6)q(6)q(6)   is the quantized representation of number 6.  Parameters inplace \u2013 can optionally do the operation in-place. Default: False    Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input     Examples: &gt;&gt;&gt; m = nn.quantized.ReLU6() &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; input = torch.quantize_per_tensor(input, 1.0, 0, dtype=torch.qint32) &gt;&gt;&gt; output = m(input)   ", "parameters": ["inplace : can optionally do the operation in-place. Default: False"], "returns": null, "example": " m = nn.quantized.ReLU6()\n input = torch.randn(2)\n input = torch.quantize_per_tensor(input, 1.0, 0, dtype=torch.qint32)\n output = m(input)\n\n", "shape": " Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input  "},
{"library": "torch", "item_id": "torch.Tensor.chunk", "item_type": "method", "code": "chunk(chunks,dim=0)\u2192ListofTensors", "description": "See torch.chunk() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.clamp", "item_type": "method", "code": "clamp(min,max)\u2192Tensor", "description": "See torch.clamp() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.clamp_", "item_type": "method", "code": "clamp_(min,max)\u2192Tensor", "description": "In-place version of clamp() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.clone", "item_type": "method", "code": "clone()\u2192Tensor", "description": "Returns a copy of the self tensor. The copy has the same size and data type as self.  Note Unlike copy_(), this function is recorded in the computation graph. Gradients propagating to the cloned tensor will propagate to the original tensor.  ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.contiguous", "item_type": "method", "code": "contiguous()\u2192Tensor", "description": "Returns a contiguous tensor containing the same data as self tensor. If self tensor is contiguous, this function returns the self tensor. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.copy_", "item_type": "method", "code": "copy_(src,non_blocking=False)\u2192Tensor", "description": "Copies the elements from src into self tensor and returns self. The src tensor must be broadcastable with the self tensor. It may be of a different data type or reside on a different device.  Parameters  src (Tensor) \u2013 the source tensor to copy from non_blocking (bool) \u2013 if True and this copy is between CPU and GPU, the copy may occur asynchronously with respect to the host. For other cases, this argument has no effect.    ", "parameters": ["src (Tensor) : the source tensor to copy from", "non_blocking (bool) : if True and this copy is between CPU and GPU,the copy may occur asynchronously with respect to the host. For othercases, this argument has no effect."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.conj", "item_type": "method", "code": "conj()\u2192Tensor", "description": "See torch.conj() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.cos", "item_type": "method", "code": "cos()\u2192Tensor", "description": "See torch.cos() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.cos_", "item_type": "method", "code": "cos_()\u2192Tensor", "description": "In-place version of cos() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.cosh", "item_type": "method", "code": "cosh()\u2192Tensor", "description": "See torch.cosh() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.cosh_", "item_type": "method", "code": "cosh_()\u2192Tensor", "description": "In-place version of cosh() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.cpu", "item_type": "method", "code": "cpu()\u2192Tensor", "description": "Returns a copy of this object in CPU memory. If this object is already in CPU memory and on the correct device, then no copy is performed and the original object is returned. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.gelu", "item_type": "function", "code": "torch.nn.functional.gelu(input)\u2192Tensor", "description": "Applies element-wise the function GELU(x)=x\u2217\u03a6(x)\\text{GELU}(x) = x * \\Phi(x)GELU(x)=x\u2217\u03a6(x)   where \u03a6(x)\\Phi(x)\u03a6(x)   is the Cumulative Distribution Function for Gaussian Distribution. See Gaussian Error Linear Units (GELUs). ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.logsigmoid", "item_type": "function", "code": "torch.nn.functional.logsigmoid(input)\u2192Tensor", "description": "Applies element-wise LogSigmoid(xi)=log\u2061(11+exp\u2061(\u2212xi))\\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)LogSigmoid(xi\u200b)=log(1+exp(\u2212xi\u200b)1\u200b)   See LogSigmoid for more details. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.hardshrink", "item_type": "function", "code": "torch.nn.functional.hardshrink(input,lambd=0.5)\u2192Tensor", "description": "Applies the hard shrinkage function element-wise See Hardshrink for more details. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.tanhshrink", "item_type": "function", "code": "torch.nn.functional.tanhshrink(input)\u2192Tensor", "description": "Applies element-wise, Tanhshrink(x)=x\u2212Tanh(x)\\text{Tanhshrink}(x) = x - \\text{Tanh}(x)Tanhshrink(x)=x\u2212Tanh(x)   See Tanhshrink for more details. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.softsign", "item_type": "function", "code": "torch.nn.functional.softsign(input)\u2192Tensor", "description": "Applies element-wise, the function SoftSign(x)=x1+\u2223x\u2223\\text{SoftSign}(x) = \\frac{x}{1 + |x|}SoftSign(x)=1+\u2223x\u2223x\u200b   See Softsign for more details. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.softplus", "item_type": "function", "code": "torch.nn.functional.softplus(input,beta=1,threshold=20)\u2192Tensor", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.softmin", "item_type": "function", "code": "torch.nn.functional.softmin(input,dim=None,_stacklevel=3,dtype=None)", "description": "Applies a softmin function. Note that Softmin(x)=Softmax(\u2212x)\\text{Softmin}(x) = \\text{Softmax}(-x)Softmin(x)=Softmax(\u2212x)  . See softmax definition for mathematical formula. See Softmin for more details.  Parameters  input (Tensor) \u2013 input dim (python:int) \u2013 A dimension along which softmin will be computed (so every slice along dim will sum to 1). dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None.    ", "parameters": ["input (Tensor) : input", "dim (python:int) : A dimension along which softmin will be computed (so every slicealong dim will sum to 1).", "dtype (torch.dtype, optional) : the desired data type of returned tensor.If specified, the input tensor is casted to dtype before the operationis performed. This is useful for preventing data type overflows. Default: None."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.softmax", "item_type": "function", "code": "torch.nn.functional.softmax(input,dim=None,_stacklevel=3,dtype=None)", "description": "Applies a softmax function. Softmax is defined as: Softmax(xi)=exp(xi)\u2211jexp(xj)\\text{Softmax}(x_{i}) = \\frac{exp(x_i)}{\\sum_j exp(x_j)}Softmax(xi\u200b)=\u2211j\u200bexp(xj\u200b)exp(xi\u200b)\u200b   It is applied to all slices along dim, and will re-scale them so that the elements lie in the range [0, 1] and sum to 1. See Softmax for more details.  Parameters  input (Tensor) \u2013 input dim (python:int) \u2013 A dimension along which softmax will be computed. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None.     Note This function doesn\u2019t work directly with NLLLoss, which expects the Log to be computed between the Softmax and itself. Use log_softmax instead (it\u2019s faster and has better numerical properties).  ", "parameters": ["input (Tensor) : input", "dim (python:int) : A dimension along which softmax will be computed.", "dtype (torch.dtype, optional) : the desired data type of returned tensor.If specified, the input tensor is casted to dtype before the operationis performed. This is useful for preventing data type overflows. Default: None."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.zeros_like", "item_type": "function", "code": "torch.zeros_like(input,dtype=None,layout=None,device=None,requires_grad=False)\u2192Tensor", "description": "Returns a tensor filled with the scalar value 0, with the same size as input. torch.zeros_like(input) is equivalent to torch.zeros(input.size(), dtype=input.dtype, layout=input.layout, device=input.device).  Warning As of 0.4, this function does not support an out keyword. As an alternative, the old torch.zeros_like(input, out=output) is equivalent to torch.zeros(input.size(), out=output).   Parameters  input (Tensor) \u2013 the size of input will determine size of the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor. Default: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor. Default: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: &gt;&gt;&gt; input = torch.empty(2, 3) &gt;&gt;&gt; torch.zeros_like(input) tensor([[ 0.,  0.,  0.],         [ 0.,  0.,  0.]])   ", "parameters": ["input (Tensor) : the size of input will determine size of the output tensor.", "dtype (torch.dtype, optional) : the desired data type of returned Tensor.Default: if None, defaults to the dtype of input.", "layout (torch.layout, optional) : the desired layout of returned tensor.Default: if None, defaults to the layout of input.", "device (torch.device, optional) : the desired device of returned tensor.Default: if None, defaults to the device of input.", "requires_grad (bool, optional) : If autograd should record operations on thereturned tensor. Default: False."], "returns": null, "example": " input = torch.empty(2, 3)\n torch.zeros_like(input)\ntensor([[ 0.,  0.,  0.],\n        [ 0.,  0.,  0.]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.ones", "item_type": "function", "code": "torch.ones(*size,out=None,dtype=None,layout=torch.strided,device=None,requires_grad=False)\u2192Tensor", "description": "Returns a tensor filled with the scalar value 1, with the shape defined by the variable argument size.  Parameters  size (python:int...) \u2013 a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple. out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor. Default: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: &gt;&gt;&gt; torch.ones(2, 3) tensor([[ 1.,  1.,  1.],         [ 1.,  1.,  1.]])  &gt;&gt;&gt; torch.ones(5) tensor([ 1.,  1.,  1.,  1.,  1.])   ", "parameters": ["size (python:int...) : a sequence of integers defining the shape of the output tensor.Can be a variable number of arguments or a collection like a list or tuple.", "out (Tensor, optional) : the output tensor.", "dtype (torch.dtype, optional) : the desired data type of returned tensor.Default: if None, uses a global default (see torch.set_default_tensor_type()).", "layout (torch.layout, optional) : the desired layout of returned Tensor.Default: torch.strided.", "device (torch.device, optional) : the desired device of returned tensor.Default: if None, uses the current device for the default tensor type(see torch.set_default_tensor_type()). device will be the CPUfor CPU tensor types and the current CUDA device for CUDA tensor types.", "requires_grad (bool, optional) : If autograd should record operations on thereturned tensor. Default: False."], "returns": null, "example": " torch.ones(2, 3)\ntensor([[ 1.,  1.,  1.],\n        [ 1.,  1.,  1.]])\n\n torch.ones(5)\ntensor([ 1.,  1.,  1.,  1.,  1.])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.ones_like", "item_type": "function", "code": "torch.ones_like(input,dtype=None,layout=None,device=None,requires_grad=False)\u2192Tensor", "description": "Returns a tensor filled with the scalar value 1, with the same size as input. torch.ones_like(input) is equivalent to torch.ones(input.size(), dtype=input.dtype, layout=input.layout, device=input.device).  Warning As of 0.4, this function does not support an out keyword. As an alternative, the old torch.ones_like(input, out=output) is equivalent to torch.ones(input.size(), out=output).   Parameters  input (Tensor) \u2013 the size of input will determine size of the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor. Default: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor. Default: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: &gt;&gt;&gt; input = torch.empty(2, 3) &gt;&gt;&gt; torch.ones_like(input) tensor([[ 1.,  1.,  1.],         [ 1.,  1.,  1.]])   ", "parameters": ["input (Tensor) : the size of input will determine size of the output tensor.", "dtype (torch.dtype, optional) : the desired data type of returned Tensor.Default: if None, defaults to the dtype of input.", "layout (torch.layout, optional) : the desired layout of returned tensor.Default: if None, defaults to the layout of input.", "device (torch.device, optional) : the desired device of returned tensor.Default: if None, defaults to the device of input.", "requires_grad (bool, optional) : If autograd should record operations on thereturned tensor. Default: False."], "returns": null, "example": " input = torch.empty(2, 3)\n torch.ones_like(input)\ntensor([[ 1.,  1.,  1.],\n        [ 1.,  1.,  1.]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.gamma.Gamma.entropy", "item_type": "method", "code": "entropy()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.gamma.Gamma.expand", "item_type": "method", "code": "expand(batch_shape,_instance=None)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.gamma.Gamma.log_prob", "item_type": "method", "code": "log_prob(value)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.gamma.Gamma.mean", "item_type": "method", "code": "propertymean", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.gamma.Gamma.rsample", "item_type": "method", "code": "rsample(sample_shape=torch.Size([]))", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.gamma.Gamma.variance", "item_type": "method", "code": "propertyvariance", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.geometric.Geometric.entropy", "item_type": "method", "code": "entropy()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.geometric.Geometric.expand", "item_type": "method", "code": "expand(batch_shape,_instance=None)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.geometric.Geometric.log_prob", "item_type": "method", "code": "log_prob(value)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.geometric.Geometric.mean", "item_type": "method", "code": "propertymean", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.geometric.Geometric.sample", "item_type": "method", "code": "sample(sample_shape=torch.Size([]))", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.geometric.Geometric.variance", "item_type": "method", "code": "propertyvariance", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.custom_from_mask", "item_type": "function", "code": "torch.nn.utils.prune.custom_from_mask(module,name,mask)", "description": "Prunes tensor corresponding to parameter called name in module by applying the pre-computed mask in mask. Modifies module in place (and also return the modified module) by: 1) adding a named buffer called name+'_mask' corresponding to the binary mask applied to the parameter name by the pruning method. 2) replacing the parameter name by its pruned version, while the original (unpruned) parameter is stored in a new parameter named name+'_orig'.  Parameters  module (nn.Module) \u2013 module containing the tensor to prune name (str) \u2013 parameter name within module on which pruning will act. mask (Tensor) \u2013 binary mask to be applied to the parameter.   Returns modified (i.e. pruned) version of the input module  Return type module (nn.Module)   Examples &gt;&gt;&gt; m = prune.custom_from_mask(         nn.Linear(5, 3), name='bias', mask=torch.Tensor([0, 1, 0])     ) &gt;&gt;&gt; print(m.bias_mask) tensor([0., 1., 0.])   ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "name (str) : parameter name within module on which pruningwill act.", "mask (Tensor) : binary mask to be applied to the parameter."], "returns": "modified (i.e. pruned) version of the input module", "example": " m = prune.custom_from_mask(\n        nn.Linear(5, 3), name='bias', mask=torch.Tensor([0, 1, 0])\n    )\n print(m.bias_mask)\ntensor([0., 1., 0.])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.remove", "item_type": "function", "code": "torch.nn.utils.prune.remove(module,name)", "description": "Removes the pruning reparameterization from a module and the pruning method from the forward hook. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!   Parameters  module (nn.Module) \u2013 module containing the tensor to prune name (str) \u2013 parameter name within module on which pruning will act.    Examples &gt;&gt;&gt; m = random_pruning(nn.Linear(5, 7), name='weight', amount=0.2) &gt;&gt;&gt; m = remove_pruning(m, name='weight')   ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "name (str) : parameter name within module on which pruningwill act."], "returns": null, "example": " m = random_pruning(nn.Linear(5, 7), name='weight', amount=0.2)\n m = remove_pruning(m, name='weight')\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.is_pruned", "item_type": "function", "code": "torch.nn.utils.prune.is_pruned(module)", "description": "Check whether module is pruned by looking for forward_pre_hooks in its modules that inherit from the BasePruningMethod.  Parameters module (nn.Module) \u2013 object that is either pruned or unpruned  Returns binary answer to whether module is pruned.   Examples &gt;&gt;&gt; m = nn.Linear(5, 7) &gt;&gt;&gt; print(prune.is_pruned(m)) False &gt;&gt;&gt; prune.random_pruning(m, name='weight', amount=0.2) &gt;&gt;&gt; print(prune.is_pruned(m)) True   ", "parameters": ["module (nn.Module) : object that is either pruned or unpruned", "binary answer to whether module is pruned."], "returns": "binary answer to whether module is pruned.", "example": " m = nn.Linear(5, 7)\n print(prune.is_pruned(m))\nFalse\n prune.random_pruning(m, name='weight', amount=0.2)\n print(prune.is_pruned(m))\nTrue\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.weight_norm", "item_type": "function", "code": "torch.nn.utils.weight_norm(module,name='weight',dim=0)", "description": "Applies weight normalization to a parameter in the given module.  w=gv\u2225v\u2225\\mathbf{w} = g \\dfrac{\\mathbf{v}}{\\|\\mathbf{v}\\|}  w=g\u2225v\u2225v\u200b  Weight normalization is a reparameterization that decouples the magnitude of a weight tensor from its direction. This replaces the parameter specified by name (e.g. 'weight') with two parameters: one specifying the magnitude (e.g. 'weight_g') and one specifying the direction (e.g. 'weight_v'). Weight normalization is implemented via a hook that recomputes the weight tensor from the magnitude and direction before every forward() call. By default, with dim=0, the norm is computed independently per output channel/plane. To compute a norm over the entire weight tensor, use dim=None. See https://arxiv.org/abs/1602.07868  Parameters  module (Module) \u2013 containing module name (str, optional) \u2013 name of weight parameter dim (python:int, optional) \u2013 dimension over which to compute the norm   Returns The original module with the weight norm hook   Example: &gt;&gt;&gt; m = weight_norm(nn.Linear(20, 40), name='weight') &gt;&gt;&gt; m Linear(in_features=20, out_features=40, bias=True) &gt;&gt;&gt; m.weight_g.size() torch.Size([40, 1]) &gt;&gt;&gt; m.weight_v.size() torch.Size([40, 20])   ", "parameters": ["module (Module) : containing module", "name (str, optional) : name of weight parameter", "dim (python:int, optional) : dimension over which to compute the norm"], "returns": "The original module with the weight norm hook", "example": " m = weight_norm(nn.Linear(20, 40), name='weight')\n m\nLinear(in_features=20, out_features=40, bias=True)\n m.weight_g.size()\ntorch.Size([40, 1])\n m.weight_v.size()\ntorch.Size([40, 20])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.remove_weight_norm", "item_type": "function", "code": "torch.nn.utils.remove_weight_norm(module,name='weight')", "description": "Removes the weight normalization reparameterization from a module.  Parameters  module (Module) \u2013 containing module name (str, optional) \u2013 name of weight parameter    Example &gt;&gt;&gt; m = weight_norm(nn.Linear(20, 40)) &gt;&gt;&gt; remove_weight_norm(m)   ", "parameters": ["module (Module) : containing module", "name (str, optional) : name of weight parameter"], "returns": null, "example": " m = weight_norm(nn.Linear(20, 40))\n remove_weight_norm(m)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.quantized.Conv2d", "item_type": "class", "code": "classtorch.nn.quantized.Conv2d(in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')", "description": "Applies a 2D convolution over a quantized input signal composed of several quantized input planes. For details on input arguments, parameters, and implementation see Conv2d.  Note Only zeros is supported for the padding_mode argument.   Note Only torch.quint8 is supported for the input data type.   Variables  ~Conv2d.weight (Tensor) \u2013 packed tensor derived from the learnable weight parameter. ~Conv2d.scale (Tensor) \u2013 scalar for the output scale ~Conv2d.zero_point (Tensor) \u2013 scalar for the output zero point    See Conv2d for other attributes. Examples: &gt;&gt;&gt; # With square kernels and equal stride &gt;&gt;&gt; m = nn.quantized.Conv2d(16, 33, 3, stride=2) &gt;&gt;&gt; # non-square kernels and unequal stride and with padding &gt;&gt;&gt; m = nn.quantized.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2)) &gt;&gt;&gt; # non-square kernels and unequal stride and with padding and dilation &gt;&gt;&gt; m = nn.quantized.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1)) &gt;&gt;&gt; input = torch.randn(20, 16, 50, 100) &gt;&gt;&gt; # quantize input to qint8 &gt;&gt;&gt; q_input = torch.quantize_per_tensor(input, scale=1.0, zero_point=0, dtype=torch.qint32) &gt;&gt;&gt; output = m(input)     classmethod from_float(mod)  Creates a quantized module from a float module or qparams_dict.  Parameters mod (Module) \u2013 a float module, either produced by torch.quantization utilities or provided by the user     ", "parameters": ["~Conv2d.weight (Tensor) : packed tensor derived from the learnable weightparameter.", "~Conv2d.scale (Tensor) : scalar for the output scale", "~Conv2d.zero_point (Tensor) : scalar for the output zero point"], "returns": null, "example": " # With square kernels and equal stride\n m = nn.quantized.Conv2d(16, 33, 3, stride=2)\n # non-square kernels and unequal stride and with padding\n m = nn.quantized.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n # non-square kernels and unequal stride and with padding and dilation\n m = nn.quantized.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))\n input = torch.randn(20, 16, 50, 100)\n # quantize input to qint8\n q_input = torch.quantize_per_tensor(input, scale=1.0, zero_point=0, dtype=torch.qint32)\n output = m(input)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.quantized.Conv3d", "item_type": "class", "code": "classtorch.nn.quantized.Conv3d(in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')", "description": "Applies a 3D convolution over a quantized input signal composed of several quantized input planes. For details on input arguments, parameters, and implementation see Conv3d.  Note Only zeros is supported for the padding_mode argument.   Note Only torch.quint8 is supported for the input data type.   Variables  ~Conv3d.weight (Tensor) \u2013 packed tensor derived from the learnable weight parameter. ~Conv3d.scale (Tensor) \u2013 scalar for the output scale ~Conv3d.zero_point (Tensor) \u2013 scalar for the output zero point    See Conv3d for other attributes. Examples: &gt;&gt;&gt; # With square kernels and equal stride &gt;&gt;&gt; m = nn.quantized.Conv3d(16, 33, 3, stride=2) &gt;&gt;&gt; # non-square kernels and unequal stride and with padding &gt;&gt;&gt; m = nn.quantized.Conv3d(16, 33, (3, 5, 5), stride=(1, 2, 2), padding=(1, 2, 2)) &gt;&gt;&gt; # non-square kernels and unequal stride and with padding and dilation &gt;&gt;&gt; m = nn.quantized.Conv3d(16, 33, (3, 5, 5), stride=(1, 2, 2), padding=(1, 2, 2), dilation=(1, 2, 2)) &gt;&gt;&gt; input = torch.randn(20, 16, 56, 56, 56) &gt;&gt;&gt; # quantize input to qint8 &gt;&gt;&gt; q_input = torch.quantize_per_tensor(input, scale=1.0, zero_point=0, dtype=torch.qint32) &gt;&gt;&gt; output = m(input)     classmethod from_float(mod)  Creates a quantized module from a float module or qparams_dict.  Parameters mod (Module) \u2013 a float module, either produced by torch.quantization utilities or provided by the user     ", "parameters": ["~Conv3d.weight (Tensor) : packed tensor derived from the learnable weightparameter.", "~Conv3d.scale (Tensor) : scalar for the output scale", "~Conv3d.zero_point (Tensor) : scalar for the output zero point"], "returns": null, "example": " # With square kernels and equal stride\n m = nn.quantized.Conv3d(16, 33, 3, stride=2)\n # non-square kernels and unequal stride and with padding\n m = nn.quantized.Conv3d(16, 33, (3, 5, 5), stride=(1, 2, 2), padding=(1, 2, 2))\n # non-square kernels and unequal stride and with padding and dilation\n m = nn.quantized.Conv3d(16, 33, (3, 5, 5), stride=(1, 2, 2), padding=(1, 2, 2), dilation=(1, 2, 2))\n input = torch.randn(20, 16, 56, 56, 56)\n # quantize input to qint8\n q_input = torch.quantize_per_tensor(input, scale=1.0, zero_point=0, dtype=torch.qint32)\n output = m(input)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.quantized.FloatFunctional", "item_type": "class", "code": "classtorch.nn.quantized.FloatFunctional", "description": "State collector class for float operatitons. The instance of this class can be used instead of the torch. prefix for some operations. See example usage below.  Note This class does not provide a forward hook. Instead, you must use one of the underlying functions (e.g. add).  Examples: &gt;&gt;&gt; f_add = FloatFunctional() &gt;&gt;&gt; a = torch.tensor(3.0) &gt;&gt;&gt; b = torch.tensor(4.0) &gt;&gt;&gt; f_add.add(a, b)  # Equivalent to ``torch.add(3, 4)    Valid operation names: add cat mul add_relu add_scalar mul_scalar    ", "parameters": [], "returns": null, "example": " f_add = FloatFunctional()\n a = torch.tensor(3.0)\n b = torch.tensor(4.0)\n f_add.add(a, b)  # Equivalent to ``torch.add(3, 4)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.cross", "item_type": "method", "code": "cross(other,dim=-1)\u2192Tensor", "description": "See torch.cross() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.cuda", "item_type": "method", "code": "cuda(device=None,non_blocking=False)\u2192Tensor", "description": "Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then no copy is performed and the original object is returned.  Parameters  device (torch.device) \u2013 The destination GPU device. Defaults to the current CUDA device. non_blocking (bool) \u2013 If True and the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, the argument has no effect. Default: False.    ", "parameters": ["device (torch.device) : The destination GPU device.Defaults to the current CUDA device.", "non_blocking (bool) : If True and the source is in pinned memory,the copy will be asynchronous with respect to the host.Otherwise, the argument has no effect. Default: False."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.cumprod", "item_type": "method", "code": "cumprod(dim,dtype=None)\u2192Tensor", "description": "See torch.cumprod() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.cumsum", "item_type": "method", "code": "cumsum(dim,dtype=None)\u2192Tensor", "description": "See torch.cumsum() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.data_ptr", "item_type": "method", "code": "data_ptr()\u2192int", "description": "Returns the address of the first element of self tensor. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.dequantize", "item_type": "method", "code": "dequantize()\u2192Tensor", "description": "Given a quantized Tensor, dequantize it and return the dequantized float Tensor. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.det", "item_type": "method", "code": "det()\u2192Tensor", "description": "See torch.det() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.dense_dim", "item_type": "method", "code": "dense_dim()\u2192int", "description": "If self is a sparse COO tensor (i.e., with torch.sparse_coo layout), this returns the number of dense dimensions. Otherwise, this throws an error. See also Tensor.sparse_dim(). ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "method", "code": "detach()", "description": "Returns a new Tensor, detached from the current graph. The result will never require gradient.  Note Returned Tensor shares the same storage with the original one. In-place modifications on either of them will be seen, and may trigger errors in correctness checks. IMPORTANT NOTE: Previously, in-place size / stride / storage changes (such as resize_ / resize_as_ / set_ / transpose_) to the returned tensor also update the original tensor. Now, these in-place changes will not update the original tensor anymore, and will instead trigger an error. For sparse tensors: In-place indices / values changes (such as zero_ / copy_ / add_) to the returned tensor will not update the original tensor anymore, and will instead trigger an error.  ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "method", "code": "detach_()", "description": "Detaches the Tensor from the graph that created it, making it a leaf. Views cannot be detached in-place. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.diag", "item_type": "method", "code": "diag(diagonal=0)\u2192Tensor", "description": "See torch.diag() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.softshrink", "item_type": "function", "code": "torch.nn.functional.softshrink(input,lambd=0.5)\u2192Tensor", "description": "Applies the soft shrinkage function elementwise See Softshrink for more details. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.gumbel_softmax", "item_type": "function", "code": "torch.nn.functional.gumbel_softmax(logits,tau=1,hard=False,eps=1e-10,dim=-1)", "description": "Samples from the Gumbel-Softmax distribution (Link 1  Link 2) and optionally discretizes.  Parameters  logits \u2013 [\u2026, num_features] unnormalized log probabilities tau \u2013 non-negative scalar temperature hard \u2013 if True, the returned samples will be discretized as one-hot vectors, but will be differentiated as if it is the soft sample in autograd dim (python:int) \u2013 A dimension along which softmax will be computed. Default: -1.   Returns Sampled tensor of same shape as logits from the Gumbel-Softmax distribution. If hard=True, the returned samples will be one-hot, otherwise they will be probability distributions that sum to 1 across dim.    Note This function is here for legacy reasons, may be removed from nn.Functional in the future.   Note The main trick for hard is to do  y_hard - y_soft.detach() + y_soft It achieves two things: - makes the output value exactly one-hot (since we add then subtract y_soft value) - makes the gradient equal to y_soft gradient (since we strip all other gradients)   Examples::&gt;&gt;&gt; logits = torch.randn(20, 32) &gt;&gt;&gt; # Sample soft categorical using reparametrization trick: &gt;&gt;&gt; F.gumbel_softmax(logits, tau=1, hard=False) &gt;&gt;&gt; # Sample hard categorical using \"Straight-through\" trick: &gt;&gt;&gt; F.gumbel_softmax(logits, tau=1, hard=True)     ", "parameters": ["logits : [\u2026, num_features] unnormalized log probabilities", "tau : non-negative scalar temperature", "hard : if True, the returned samples will be discretized as one-hot vectors,but will be differentiated as if it is the soft sample in autograd", "dim (python:int) : A dimension along which softmax will be computed. Default: -1."], "returns": "Sampled tensor of same shape as logits from the Gumbel-Softmax distribution.If hard=True, the returned samples will be one-hot, otherwise they willbe probability distributions that sum to 1 across dim.", "example": " logits = torch.randn(20, 32)\n # Sample soft categorical using reparametrization trick:\n F.gumbel_softmax(logits, tau=1, hard=False)\n # Sample hard categorical using \"Straight-through\" trick:\n F.gumbel_softmax(logits, tau=1, hard=True)\n\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.log_softmax", "item_type": "function", "code": "torch.nn.functional.log_softmax(input,dim=None,_stacklevel=3,dtype=None)", "description": "Applies a softmax followed by a logarithm. While mathematically equivalent to log(softmax(x)), doing these two operations separately is slower, and numerically unstable. This function uses an alternative formulation to compute the output and gradient correctly. See LogSoftmax for more details.  Parameters  input (Tensor) \u2013 input dim (python:int) \u2013 A dimension along which log_softmax will be computed. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None.    ", "parameters": ["input (Tensor) : input", "dim (python:int) : A dimension along which log_softmax will be computed.", "dtype (torch.dtype, optional) : the desired data type of returned tensor.If specified, the input tensor is casted to dtype before the operationis performed. This is useful for preventing data type overflows. Default: None."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.tanh", "item_type": "function", "code": "torch.nn.functional.tanh(input)\u2192Tensor", "description": "Applies element-wise, Tanh(x)=tanh\u2061(x)=exp\u2061(x)\u2212exp\u2061(\u2212x)exp\u2061(x)+exp\u2061(\u2212x)\\text{Tanh}(x) = \\tanh(x) = \\frac{\\exp(x) - \\exp(-x)}{\\exp(x) + \\exp(-x)}Tanh(x)=tanh(x)=exp(x)+exp(\u2212x)exp(x)\u2212exp(\u2212x)\u200b   See Tanh for more details. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.sigmoid", "item_type": "function", "code": "torch.nn.functional.sigmoid(input)\u2192Tensor", "description": "Applies the element-wise function Sigmoid(x)=11+exp\u2061(\u2212x)\\text{Sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}Sigmoid(x)=1+exp(\u2212x)1\u200b   See Sigmoid for more details. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.batch_norm", "item_type": "function", "code": "torch.nn.functional.batch_norm(input,running_mean,running_var,weight=None,bias=None,training=False,momentum=0.1,eps=1e-05)", "description": "Applies Batch Normalization for each channel across a batch of data. See BatchNorm1d, BatchNorm2d, BatchNorm3d for details. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.instance_norm", "item_type": "function", "code": "torch.nn.functional.instance_norm(input,running_mean=None,running_var=None,weight=None,bias=None,use_input_stats=True,momentum=0.1,eps=1e-05)", "description": "Applies Instance Normalization for each channel in each data sample in a batch. See InstanceNorm1d, InstanceNorm2d, InstanceNorm3d for details. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.arange", "item_type": "function", "code": "torch.arange(start=0,end,step=1,out=None,dtype=None,layout=torch.strided,device=None,requires_grad=False)\u2192Tensor", "description": "Returns a 1-D tensor of size \u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309   with values from the interval [start, end) taken with common difference step beginning from start. Note that non-integer step is subject to floating point rounding errors when comparing against end; to avoid inconsistency, we advise adding a small epsilon to end in such cases.  outi+1=outi+step\\text{out}_{{i+1}} = \\text{out}_{i} + \\text{step}  outi+1\u200b=outi\u200b+step   Parameters  start (Number) \u2013 the starting value for the set of points. Default: 0. end (Number) \u2013 the ending value for the set of points step (Number) \u2013 the gap between each pair of adjacent points. Default: 1. out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). If dtype is not given, infer the data type from the other input arguments. If any of start, end, or stop are floating-point, the dtype is inferred to be the default dtype, see get_default_dtype(). Otherwise, the dtype is inferred to be torch.int64. layout (torch.layout, optional) \u2013 the desired layout of returned Tensor. Default: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: &gt;&gt;&gt; torch.arange(5) tensor([ 0,  1,  2,  3,  4]) &gt;&gt;&gt; torch.arange(1, 4) tensor([ 1,  2,  3]) &gt;&gt;&gt; torch.arange(1, 2.5, 0.5) tensor([ 1.0000,  1.5000,  2.0000])   ", "parameters": ["start (Number) : the starting value for the set of points. Default: 0.", "end (Number) : the ending value for the set of points", "step (Number) : the gap between each pair of adjacent points. Default: 1.", "out (Tensor, optional) : the output tensor.", "dtype (torch.dtype, optional) : the desired data type of returned tensor.Default: if None, uses a global default (see torch.set_default_tensor_type()). If dtype is not given, infer the data type from the other inputarguments. If any of start, end, or stop are floating-point, thedtype is inferred to be the default dtype, seeget_default_dtype(). Otherwise, the dtype is inferred tobe torch.int64.", "layout (torch.layout, optional) : the desired layout of returned Tensor.Default: torch.strided.", "device (torch.device, optional) : the desired device of returned tensor.Default: if None, uses the current device for the default tensor type(see torch.set_default_tensor_type()). device will be the CPUfor CPU tensor types and the current CUDA device for CUDA tensor types.", "requires_grad (bool, optional) : If autograd should record operations on thereturned tensor. Default: False."], "returns": null, "example": " torch.arange(5)\ntensor([ 0,  1,  2,  3,  4])\n torch.arange(1, 4)\ntensor([ 1,  2,  3])\n torch.arange(1, 2.5, 0.5)\ntensor([ 1.0000,  1.5000,  2.0000])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.range", "item_type": "function", "code": "torch.range(start=0,end,step=1,out=None,dtype=None,layout=torch.strided,device=None,requires_grad=False)\u2192Tensor", "description": "Returns a 1-D tensor of size \u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1   with values from start to end with step step. Step is the gap between two values in the tensor.  outi+1=outi+step.\\text{out}_{i+1} = \\text{out}_i + \\text{step}.  outi+1\u200b=outi\u200b+step.   Warning This function is deprecated in favor of torch.arange().   Parameters  start (python:float) \u2013 the starting value for the set of points. Default: 0. end (python:float) \u2013 the ending value for the set of points step (python:float) \u2013 the gap between each pair of adjacent points. Default: 1. out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). If dtype is not given, infer the data type from the other input arguments. If any of start, end, or stop are floating-point, the dtype is inferred to be the default dtype, see get_default_dtype(). Otherwise, the dtype is inferred to be torch.int64. layout (torch.layout, optional) \u2013 the desired layout of returned Tensor. Default: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: &gt;&gt;&gt; torch.range(1, 4) tensor([ 1.,  2.,  3.,  4.]) &gt;&gt;&gt; torch.range(1, 4, 0.5) tensor([ 1.0000,  1.5000,  2.0000,  2.5000,  3.0000,  3.5000,  4.0000])   ", "parameters": ["start (python:float) : the starting value for the set of points. Default: 0.", "end (python:float) : the ending value for the set of points", "step (python:float) : the gap between each pair of adjacent points. Default: 1.", "out (Tensor, optional) : the output tensor.", "dtype (torch.dtype, optional) : the desired data type of returned tensor.Default: if None, uses a global default (see torch.set_default_tensor_type()). If dtype is not given, infer the data type from the other inputarguments. If any of start, end, or stop are floating-point, thedtype is inferred to be the default dtype, seeget_default_dtype(). Otherwise, the dtype is inferred tobe torch.int64.", "layout (torch.layout, optional) : the desired layout of returned Tensor.Default: torch.strided.", "device (torch.device, optional) : the desired device of returned tensor.Default: if None, uses the current device for the default tensor type(see torch.set_default_tensor_type()). device will be the CPUfor CPU tensor types and the current CUDA device for CUDA tensor types.", "requires_grad (bool, optional) : If autograd should record operations on thereturned tensor. Default: False."], "returns": null, "example": " torch.range(1, 4)\ntensor([ 1.,  2.,  3.,  4.])\n torch.range(1, 4, 0.5)\ntensor([ 1.0000,  1.5000,  2.0000,  2.5000,  3.0000,  3.5000,  4.0000])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.gumbel.Gumbel.entropy", "item_type": "method", "code": "entropy()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.gumbel.Gumbel.expand", "item_type": "method", "code": "expand(batch_shape,_instance=None)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.gumbel.Gumbel.log_prob", "item_type": "method", "code": "log_prob(value)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.gumbel.Gumbel.mean", "item_type": "method", "code": "propertymean", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.gumbel.Gumbel.stddev", "item_type": "method", "code": "propertystddev", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.gumbel.Gumbel.variance", "item_type": "method", "code": "propertyvariance", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.half_cauchy.HalfCauchy.cdf", "item_type": "method", "code": "cdf(value)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.half_cauchy.HalfCauchy.entropy", "item_type": "method", "code": "entropy()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.half_cauchy.HalfCauchy.expand", "item_type": "method", "code": "expand(batch_shape,_instance=None)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.half_cauchy.HalfCauchy.icdf", "item_type": "method", "code": "icdf(prob)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.half_cauchy.HalfCauchy.log_prob", "item_type": "method", "code": "log_prob(value)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.half_cauchy.HalfCauchy.mean", "item_type": "method", "code": "propertymean", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.half_cauchy.HalfCauchy.scale", "item_type": "method", "code": "propertyscale", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.half_cauchy.HalfCauchy.variance", "item_type": "method", "code": "propertyvariance", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.spectral_norm", "item_type": "function", "code": "torch.nn.utils.spectral_norm(module,name='weight',n_power_iterations=1,eps=1e-12,dim=None)", "description": "Applies spectral normalization to a parameter in the given module.  WSN=W\u03c3(W),\u03c3(W)=max\u2061h:h\u22600\u2225Wh\u22252\u2225h\u22252\\mathbf{W}_{SN} = \\dfrac{\\mathbf{W}}{\\sigma(\\mathbf{W})}, \\sigma(\\mathbf{W}) = \\max_{\\mathbf{h}: \\mathbf{h} \\ne 0} \\dfrac{\\|\\mathbf{W} \\mathbf{h}\\|_2}{\\|\\mathbf{h}\\|_2}  WSN\u200b=\u03c3(W)W\u200b,\u03c3(W)=h:h\ue020\u200b=0max\u200b\u2225h\u22252\u200b\u2225Wh\u22252\u200b\u200b  Spectral normalization stabilizes the training of discriminators (critics) in Generative Adversarial Networks (GANs) by rescaling the weight tensor with spectral norm \u03c3\\sigma\u03c3   of the weight matrix calculated using power iteration method. If the dimension of the weight tensor is greater than 2, it is reshaped to 2D in power iteration method to get spectral norm. This is implemented via a hook that calculates spectral norm and rescales weight before every forward() call. See Spectral Normalization for Generative Adversarial Networks .  Parameters  module (nn.Module) \u2013 containing module name (str, optional) \u2013 name of weight parameter n_power_iterations (python:int, optional) \u2013 number of power iterations to calculate spectral norm eps (python:float, optional) \u2013 epsilon for numerical stability in calculating norms dim (python:int, optional) \u2013 dimension corresponding to number of outputs, the default is 0, except for modules that are instances of ConvTranspose{1,2,3}d, when it is 1   Returns The original module with the spectral norm hook   Example: &gt;&gt;&gt; m = spectral_norm(nn.Linear(20, 40)) &gt;&gt;&gt; m Linear(in_features=20, out_features=40, bias=True) &gt;&gt;&gt; m.weight_u.size() torch.Size([40])   ", "parameters": ["module (nn.Module) : containing module", "name (str, optional) : name of weight parameter", "n_power_iterations (python:int, optional) : number of power iterations tocalculate spectral norm", "eps (python:float, optional) : epsilon for numerical stability incalculating norms", "dim (python:int, optional) : dimension corresponding to number of outputs,the default is 0, except for modules that are instances ofConvTranspose{1,2,3}d, when it is 1"], "returns": "The original module with the spectral norm hook", "example": " m = spectral_norm(nn.Linear(20, 40))\n m\nLinear(in_features=20, out_features=40, bias=True)\n m.weight_u.size()\ntorch.Size([40])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.remove_spectral_norm", "item_type": "function", "code": "torch.nn.utils.remove_spectral_norm(module,name='weight')", "description": "Removes the spectral normalization reparameterization from a module.  Parameters  module (Module) \u2013 containing module name (str, optional) \u2013 name of weight parameter    Example &gt;&gt;&gt; m = spectral_norm(nn.Linear(40, 10)) &gt;&gt;&gt; remove_spectral_norm(m)   ", "parameters": ["module (Module) : containing module", "name (str, optional) : name of weight parameter"], "returns": null, "example": " m = spectral_norm(nn.Linear(40, 10))\n remove_spectral_norm(m)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.rnn.PackedSequence", "item_type": "function", "code": "torch.nn.utils.rnn.PackedSequence(data,batch_sizes=None,sorted_indices=None,unsorted_indices=None)", "description": "Holds the data and list of batch_sizes of a packed sequence. All RNN modules accept packed sequences as inputs.  Note Instances of this class should never be created manually. They are meant to be instantiated by functions like pack_padded_sequence(). Batch sizes represent the number elements at each sequence step in the batch, not the varying sequence lengths passed to pack_padded_sequence().  For instance, given data abc and x the PackedSequence would contain data axbc with batch_sizes=[2,1,1].   Variables  ~PackedSequence.data (Tensor) \u2013 Tensor containing packed sequence ~PackedSequence.batch_sizes (Tensor) \u2013 Tensor of integers holding information about the batch size at each sequence step ~PackedSequence.sorted_indices (Tensor, optional) \u2013 Tensor of integers holding how this PackedSequence is constructed from sequences. ~PackedSequence.unsorted_indices (Tensor, optional) \u2013 Tensor of integers holding how this to recover the original sequences with correct order.     Note data can be on arbitrary device and of arbitrary dtype. sorted_indices and unsorted_indices must be torch.int64 tensors on the same device as data. However, batch_sizes should always be a CPU torch.int64 tensor. This invariant is maintained throughout PackedSequence class, and all functions that construct a :class:PackedSequence in PyTorch (i.e., they only pass in tensors conforming to this constraint).  ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.rnn.pack_padded_sequence", "item_type": "function", "code": "torch.nn.utils.rnn.pack_padded_sequence(input,lengths,batch_first=False,enforce_sorted=True)", "description": "Packs a Tensor containing padded sequences of variable length. input can be of size T x B x * where T is the length of the longest sequence (equal to lengths[0]), B is the batch size, and * is any number of dimensions (including 0). If batch_first is True, B x T x * input is expected. For unsorted sequences, use enforce_sorted = False. If enforce_sorted is True, the sequences should be sorted by length in a decreasing order, i.e. input[:,0] should be the longest sequence, and input[:,B-1] the shortest one. enforce_sorted = True is only necessary for ONNX export.  Note This function accepts any input that has at least two dimensions. You can apply it to pack the labels, and use the output of the RNN with them to compute the loss directly. A Tensor can be retrieved from a PackedSequence object by accessing its .data attribute.   Parameters  input (Tensor) \u2013 padded batch of variable length sequences. lengths (Tensor) \u2013 list of sequences lengths of each batch element. batch_first (bool, optional) \u2013 if True, the input is expected in B x T x * format. enforce_sorted (bool, optional) \u2013 if True, the input is expected to contain sequences sorted by length in a decreasing order. If False, this condition is not checked. Default: True.   Returns a PackedSequence object   ", "parameters": ["input (Tensor) : padded batch of variable length sequences.", "lengths (Tensor) : list of sequences lengths of each batch element.", "batch_first (bool, optional) : if True, the input is expected in B x T x *format.", "enforce_sorted (bool, optional) : if True, the input is expected tocontain sequences sorted by length in a decreasing order. IfFalse, this condition is not checked. Default: True."], "returns": "a PackedSequence object", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.rnn.pad_packed_sequence", "item_type": "function", "code": "torch.nn.utils.rnn.pad_packed_sequence(sequence,batch_first=False,padding_value=0.0,total_length=None)", "description": "Pads a packed batch of variable length sequences. It is an inverse operation to pack_padded_sequence(). The returned Tensor\u2019s data will be of size T x B x *, where T is the length of the longest sequence and B is the batch size. If batch_first is True, the data will be transposed into B x T x * format. Batch elements will be ordered decreasingly by their length.  Note total_length is useful to implement the pack sequence -&gt; recurrent network -&gt; unpack sequence pattern in a Module wrapped in DataParallel. See this FAQ section for details.   Parameters  sequence (PackedSequence) \u2013 batch to pad batch_first (bool, optional) \u2013 if True, the output will be in B x T x * format. padding_value (python:float, optional) \u2013 values for padded elements. total_length (python:int, optional) \u2013 if not None, the output will be padded to have length total_length. This method will throw ValueError if total_length is less than the max sequence length in sequence.   Returns Tuple of Tensor containing the padded sequence, and a Tensor containing the list of lengths of each sequence in the batch.   ", "parameters": ["sequence (PackedSequence) : batch to pad", "batch_first (bool, optional) : if True, the output will be in B x T x *format.", "padding_value (python:float, optional) : values for padded elements.", "total_length (python:int, optional) : if not None, the output will be padded tohave length total_length. This method will throw ValueErrorif total_length is less than the max sequence length insequence."], "returns": "Tuple of Tensor containing the padded sequence, and a Tensorcontaining the list of lengths of each sequence in the batch.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.rnn.pad_sequence", "item_type": "function", "code": "torch.nn.utils.rnn.pad_sequence(sequences,batch_first=False,padding_value=0)", "description": "Pad a list of variable length Tensors with padding_value pad_sequence stacks a list of Tensors along a new dimension, and pads them to equal length. For example, if the input is list of sequences with size L x * and if batch_first is False, and T x B x * otherwise. B is batch size. It is equal to the number of elements in sequences. T is length of the longest sequence. L is length of the sequence. * is any number of trailing dimensions, including none. Example &gt;&gt;&gt; from torch.nn.utils.rnn import pad_sequence &gt;&gt;&gt; a = torch.ones(25, 300) &gt;&gt;&gt; b = torch.ones(22, 300) &gt;&gt;&gt; c = torch.ones(15, 300) &gt;&gt;&gt; pad_sequence([a, b, c]).size() torch.Size([25, 3, 300])    Note This function returns a Tensor of size T x B x * or B x T x * where T is the length of the longest sequence. This function assumes trailing dimensions and type of all the Tensors in sequences are same.   Parameters  sequences (list[Tensor]) \u2013 list of variable length sequences. batch_first (bool, optional) \u2013 output will be in B x T x * if True, or in T x B x * otherwise padding_value (python:float, optional) \u2013 value for padded elements. Default: 0.   Returns Tensor of size T x B x * if batch_first is False. Tensor of size B x T x * otherwise   ", "parameters": ["sequences (list[Tensor]) : list of variable length sequences.", "batch_first (bool, optional) : output will be in B x T x * if True, or inT x B x * otherwise", "padding_value (python:float, optional) : value for padded elements. Default: 0."], "returns": "Tensor of size T x B x * if batch_first is False.Tensor of size B x T x * otherwise", "example": " from torch.nn.utils.rnn import pad_sequence\n a = torch.ones(25, 300)\n b = torch.ones(22, 300)\n c = torch.ones(15, 300)\n pad_sequence([a, b, c]).size()\ntorch.Size([25, 3, 300])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.quantized.QFunctional", "item_type": "class", "code": "classtorch.nn.quantized.QFunctional", "description": "Wrapper class for quantized operatitons. The instance of this class can be used instead of the torch.ops.quantized prefix. See example usage below.  Note This class does not provide a forward hook. Instead, you must use one of the underlying functions (e.g. add).  Examples: &gt;&gt;&gt; q_add = QFunctional('add') &gt;&gt;&gt; a = torch.quantize_per_tensor(torch.tensor(3.0), 1.0, 0, torch.qint32) &gt;&gt;&gt; b = torch.quantize_per_tensor(torch.tensor(4.0), 1.0, 0, torch.qint32) &gt;&gt;&gt; q_add.add(a, b)  # Equivalent to ``torch.ops.quantized.add(3, 4)    Valid operation names: add cat mul add_relu add_scalar mul_scalar    ", "parameters": [], "returns": null, "example": " q_add = QFunctional('add')\n a = torch.quantize_per_tensor(torch.tensor(3.0), 1.0, 0, torch.qint32)\n b = torch.quantize_per_tensor(torch.tensor(4.0), 1.0, 0, torch.qint32)\n q_add.add(a, b)  # Equivalent to ``torch.ops.quantized.add(3, 4)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.quantized.Quantize", "item_type": "class", "code": "classtorch.nn.quantized.Quantize(scale,zero_point,dtype)", "description": "Quantizes an incoming tensor  Parameters  scale \u2013 scale of the output Quantized Tensor zero_point \u2013 zero_point of output Quantized Tensor dtype \u2013 data type of output Quantized Tensor   Variables zero_point, dtype (`scale`,) \u2013     Examples::&gt;&gt;&gt; t = torch.tensor([[1., -1.], [1., -1.]]) &gt;&gt;&gt; scale, zero_point, dtype = 1.0, 2, torch.qint8 &gt;&gt;&gt; qm = Quantize(scale, zero_point, dtype) &gt;&gt;&gt; qt = qm(t) &gt;&gt;&gt; print(qt) tensor([[ 1., -1.],         [ 1., -1.]], size=(2, 2), dtype=torch.qint8, scale=1.0, zero_point=2)     ", "parameters": ["scale : scale of the output Quantized Tensor", "zero_point : zero_point of output Quantized Tensor", "dtype : data type of output Quantized Tensor"], "returns": null, "example": " t = torch.tensor([[1., -1.], [1., -1.]])\n scale, zero_point, dtype = 1.0, 2, torch.qint8\n qm = Quantize(scale, zero_point, dtype)\n qt = qm(t)\n print(qt)\ntensor([[ 1., -1.],\n        [ 1., -1.]], size=(2, 2), dtype=torch.qint8, scale=1.0, zero_point=2)\n\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.quantized.DeQuantize", "item_type": "class", "code": "classtorch.nn.quantized.DeQuantize", "description": "Dequantizes an incoming tensor  Examples::&gt;&gt;&gt; input = torch.tensor([[1., -1.], [1., -1.]]) &gt;&gt;&gt; scale, zero_point, dtype = 1.0, 2, torch.qint8 &gt;&gt;&gt; qm = Quantize(scale, zero_point, dtype) &gt;&gt;&gt; quantized_input = qm(input) &gt;&gt;&gt; dqm = DeQuantize() &gt;&gt;&gt; dequantized = dqm(quantized_input) &gt;&gt;&gt; print(dequantized) tensor([[ 1., -1.],         [ 1., -1.]], dtype=torch.float32)     ", "parameters": [], "returns": null, "example": " input = torch.tensor([[1., -1.], [1., -1.]])\n scale, zero_point, dtype = 1.0, 2, torch.qint8\n qm = Quantize(scale, zero_point, dtype)\n quantized_input = qm(input)\n dqm = DeQuantize()\n dequantized = dqm(quantized_input)\n print(dequantized)\ntensor([[ 1., -1.],\n        [ 1., -1.]], dtype=torch.float32)\n\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.quantized.Linear", "item_type": "class", "code": "classtorch.nn.quantized.Linear(in_features,out_features,bias_=True)", "description": "A quantized linear module with quantized tensor as inputs and outputs. We adopt the same interface as torch.nn.Linear, please see https://pytorch.org/docs/stable/nn.html#torch.nn.Linear for documentation. Similar to Linear, attributes will be randomly initialized at module creation time and will be overwritten later  Variables  ~Linear.weight (Tensor) \u2013 the non-learnable quantized weights of the module of shape (out_features,in_features)(\\text{out\\_features}, \\text{in\\_features})(out_features,in_features)  . ~Linear.bias (Tensor) \u2013 the non-learnable bias of the module of shape (out_features)(\\text{out\\_features})(out_features)  . If bias is True, the values are initialized to zero. ~Linear.scale \u2013 scale parameter of output Quantized Tensor, type: double ~Linear.zero_point \u2013 zero_point parameter for output Quantized Tensor, type: long    Examples: &gt;&gt;&gt; m = nn.quantized.Linear(20, 30) &gt;&gt;&gt; input = torch.randn(128, 20) &gt;&gt;&gt; input = torch.quantize_per_tensor(input, 1.0, 0, torch.quint8) &gt;&gt;&gt; output = m(input) &gt;&gt;&gt; print(output.size()) torch.Size([128, 30])     classmethod from_float(mod)  Create a quantized module from a float module or qparams_dict  Parameters mod (Module) \u2013 a float module, either produced by torch.quantization utilities or provided by the user     ", "parameters": ["~Linear.weight (Tensor) : the non-learnable quantized weights of the module ofshape (out_features,in_features)(\\text{out\\_features}, \\text{in\\_features})(out_features,in_features).", "~Linear.bias (Tensor) : the non-learnable bias of the module of shape (out_features)(\\text{out\\_features})(out_features).If bias is True, the values are initialized to zero.", "~Linear.scale : scale parameter of output Quantized Tensor, type: double", "~Linear.zero_point : zero_point parameter for output Quantized Tensor, type: long"], "returns": null, "example": " m = nn.quantized.Linear(20, 30)\n input = torch.randn(128, 20)\n input = torch.quantize_per_tensor(input, 1.0, 0, torch.quint8)\n output = m(input)\n print(output.size())\ntorch.Size([128, 30])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.quantized.dynamic.Linear", "item_type": "class", "code": "classtorch.nn.quantized.dynamic.Linear(in_features,out_features,bias_=True)", "description": "A dynamic quantized linear module with quantized tensor as inputs and outputs. We adopt the same interface as torch.nn.Linear, please see https://pytorch.org/docs/stable/nn.html#torch.nn.Linear for documentation. Similar to torch.nn.Linear, attributes will be randomly initialized at module creation time and will be overwritten later  Variables  ~Linear.weight (Tensor) \u2013 the non-learnable quantized weights of the module which are of shape (out_features,in_features)(\\text{out\\_features}, \\text{in\\_features})(out_features,in_features)  . ~Linear.bias (Tensor) \u2013 the non-learnable bias of the module of shape (out_features)(\\text{out\\_features})(out_features)  . If bias is True, the values are initialized to zero.    Examples: &gt;&gt;&gt; m = nn.quantized.dynamic.Linear(20, 30) &gt;&gt;&gt; input = torch.randn(128, 20) &gt;&gt;&gt; output = m(input) &gt;&gt;&gt; print(output.size()) torch.Size([128, 30])     classmethod from_float(mod)  Create a dynamic quantized module from a float module or qparams_dict  Parameters mod (Module) \u2013 a float module, either produced by torch.quantization utilities or provided by the user     ", "parameters": ["~Linear.weight (Tensor) : the non-learnable quantized weights of the module which are ofshape (out_features,in_features)(\\text{out\\_features}, \\text{in\\_features})(out_features,in_features).", "~Linear.bias (Tensor) : the non-learnable bias of the module of shape (out_features)(\\text{out\\_features})(out_features).If bias is True, the values are initialized to zero."], "returns": null, "example": " m = nn.quantized.dynamic.Linear(20, 30)\n input = torch.randn(128, 20)\n output = m(input)\n print(output.size())\ntorch.Size([128, 30])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.quantized.dynamic.LSTM", "item_type": "class", "code": "classtorch.nn.quantized.dynamic.LSTM(*args,**kwargs)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.diag_embed", "item_type": "method", "code": "diag_embed(offset=0,dim1=-2,dim2=-1)\u2192Tensor", "description": "See torch.diag_embed() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.diagflat", "item_type": "method", "code": "diagflat(offset=0)\u2192Tensor", "description": "See torch.diagflat() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.diagonal", "item_type": "method", "code": "diagonal(offset=0,dim1=0,dim2=1)\u2192Tensor", "description": "See torch.diagonal() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.fill_diagonal_", "item_type": "method", "code": "fill_diagonal_(fill_value,wrap=False)\u2192Tensor", "description": "Fill the main diagonal of a tensor that has at least 2-dimensions. When dims&gt;2, all dimensions of input must be of equal length. This function modifies the input tensor in-place, and returns the input tensor.  Parameters  fill_value (Scalar) \u2013 the fill value wrap (bool) \u2013 the diagonal \u2018wrapped\u2019 after N columns for tall matrices.    Example: &gt;&gt;&gt; a = torch.zeros(3, 3) &gt;&gt;&gt; a.fill_diagonal_(5) tensor([[5., 0., 0.],         [0., 5., 0.],         [0., 0., 5.]]) &gt;&gt;&gt; b = torch.zeros(7, 3) &gt;&gt;&gt; b.fill_diagonal_(5) tensor([[5., 0., 0.],         [0., 5., 0.],         [0., 0., 5.],         [0., 0., 0.],         [0., 0., 0.],         [0., 0., 0.],         [0., 0., 0.]]) &gt;&gt;&gt; c = torch.zeros(7, 3) &gt;&gt;&gt; c.fill_diagonal_(5, wrap=True) tensor([[5., 0., 0.],         [0., 5., 0.],         [0., 0., 5.],         [0., 0., 0.],         [5., 0., 0.],         [0., 5., 0.],         [0., 0., 5.]])   ", "parameters": ["fill_value (Scalar) : the fill value", "wrap (bool) : the diagonal \u2018wrapped\u2019 after N columns for tall matrices."], "returns": null, "example": " a = torch.zeros(3, 3)\n a.fill_diagonal_(5)\ntensor([[5., 0., 0.],\n        [0., 5., 0.],\n        [0., 0., 5.]])\n b = torch.zeros(7, 3)\n b.fill_diagonal_(5)\ntensor([[5., 0., 0.],\n        [0., 5., 0.],\n        [0., 0., 5.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.],\n        [0., 0., 0.]])\n c = torch.zeros(7, 3)\n c.fill_diagonal_(5, wrap=True)\ntensor([[5., 0., 0.],\n        [0., 5., 0.],\n        [0., 0., 5.],\n        [0., 0., 0.],\n        [5., 0., 0.],\n        [0., 5., 0.],\n        [0., 0., 5.]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.digamma", "item_type": "method", "code": "digamma()\u2192Tensor", "description": "See torch.digamma() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.digamma_", "item_type": "method", "code": "digamma_()\u2192Tensor", "description": "In-place version of digamma() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.dim", "item_type": "method", "code": "dim()\u2192int", "description": "Returns the number of dimensions of self tensor. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.dist", "item_type": "method", "code": "dist(other,p=2)\u2192Tensor", "description": "See torch.dist() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.div", "item_type": "method", "code": "div(value)\u2192Tensor", "description": "See torch.div() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.div_", "item_type": "method", "code": "div_(value)\u2192Tensor", "description": "In-place version of div() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.dot", "item_type": "method", "code": "dot(tensor2)\u2192Tensor", "description": "See torch.dot() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.double", "item_type": "method", "code": "double()\u2192Tensor", "description": "self.double() is equivalent to self.to(torch.float64). See to(). ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.eig", "item_type": "method", "code": "eig(eigenvectors=False)-&gt;(Tensor,Tensor)", "description": "See torch.eig() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.layer_norm", "item_type": "function", "code": "torch.nn.functional.layer_norm(input,normalized_shape,weight=None,bias=None,eps=1e-05)", "description": "Applies Layer Normalization for last certain number of dimensions. See LayerNorm for details. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.local_response_norm", "item_type": "function", "code": "torch.nn.functional.local_response_norm(input,size,alpha=0.0001,beta=0.75,k=1.0)", "description": "Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension. Applies normalization across channels. See LocalResponseNorm for details. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.normalize", "item_type": "function", "code": "torch.nn.functional.normalize(input,p=2,dim=1,eps=1e-12,out=None)", "description": "Performs LpL_pLp\u200b   normalization of inputs over specified dimension. For a tensor input of sizes (n0,...,ndim,...,nk)(n_0, ..., n_{dim}, ..., n_k)(n0\u200b,...,ndim\u200b,...,nk\u200b)  , each ndimn_{dim}ndim\u200b   -element vector vvv   along dimension dim is transformed as  v=vmax\u2061(\u2225v\u2225p,\u03f5).v = \\frac{v}{\\max(\\lVert v \\rVert_p, \\epsilon)}.  v=max(\u2225v\u2225p\u200b,\u03f5)v\u200b.  With the default arguments it uses the Euclidean norm over vectors along dimension 111   for normalization.  Parameters  input \u2013 input tensor of any shape p (python:float) \u2013 the exponent value in the norm formulation. Default: 2 dim (python:int) \u2013 the dimension to reduce. Default: 1 eps (python:float) \u2013 small value to avoid division by zero. Default: 1e-12 out (Tensor, optional) \u2013 the output tensor. If out is used, this operation won\u2019t be differentiable.    ", "parameters": ["input : input tensor of any shape", "p (python:float) : the exponent value in the norm formulation. Default: 2", "dim (python:int) : the dimension to reduce. Default: 1", "eps (python:float) : small value to avoid division by zero. Default: 1e-12", "out (Tensor, optional) : the output tensor. If out is used, thisoperation won\u2019t be differentiable."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.linear", "item_type": "function", "code": "torch.nn.functional.linear(input,weight,bias=None)", "description": "Applies a linear transformation to the incoming data: y=xAT+by = xA^T + by=xAT+b  . Shape:   Input: (N,\u2217,in_features)(N, *, in\\_features)(N,\u2217,in_features)   where * means any number of additional dimensions Weight: (out_features,in_features)(out\\_features, in\\_features)(out_features,in_features)   Bias: (out_features)(out\\_features)(out_features)   Output: (N,\u2217,out_features)(N, *, out\\_features)(N,\u2217,out_features)     ", "parameters": [], "returns": null, "example": "NA", "shape": "  Input: (N,\u2217,in_features)(N, *, in\\_features)(N,\u2217,in_features)   where * means any number of additional dimensions Weight: (out_features,in_features)(out\\_features, in\\_features)(out_features,in_features)   Bias: (out_features)(out\\_features)(out_features)   Output: (N,\u2217,out_features)(N, *, out\\_features)(N,\u2217,out_features)    "},
{"library": "torch", "item_id": "torch.nn.functional.bilinear", "item_type": "function", "code": "torch.nn.functional.bilinear(input1,input2,weight,bias=None)", "description": "Applies a bilinear transformation to the incoming data: y=x1Ax2+by = x_1 A x_2 + by=x1\u200bAx2\u200b+b   Shape:   input1: (N,\u2217,Hin1)(N, *, H_{in1})(N,\u2217,Hin1\u200b)   where Hin1=in1_featuresH_{in1}=\\text{in1\\_features}Hin1\u200b=in1_features   and \u2217*\u2217   means any number of additional dimensions. All but the last dimension of the inputs should be the same. input2: (N,\u2217,Hin2)(N, *, H_{in2})(N,\u2217,Hin2\u200b)   where Hin2=in2_featuresH_{in2}=\\text{in2\\_features}Hin2\u200b=in2_features   weight: (out_features,in1_features,in2_features)(\\text{out\\_features}, \\text{in1\\_features}, \\text{in2\\_features})(out_features,in1_features,in2_features)   bias: (out_features)(\\text{out\\_features})(out_features)   output: (N,\u2217,Hout)(N, *, H_{out})(N,\u2217,Hout\u200b)   where Hout=out_featuresH_{out}=\\text{out\\_features}Hout\u200b=out_features   and all but the last dimension are the same shape as the input.   ", "parameters": [], "returns": null, "example": "NA", "shape": "  input1: (N,\u2217,Hin1)(N, *, H_{in1})(N,\u2217,Hin1\u200b)   where Hin1=in1_featuresH_{in1}=\\text{in1\\_features}Hin1\u200b=in1_features   and \u2217*\u2217   means any number of additional dimensions. All but the last dimension of the inputs should be the same. input2: (N,\u2217,Hin2)(N, *, H_{in2})(N,\u2217,Hin2\u200b)   where Hin2=in2_featuresH_{in2}=\\text{in2\\_features}Hin2\u200b=in2_features   weight: (out_features,in1_features,in2_features)(\\text{out\\_features}, \\text{in1\\_features}, \\text{in2\\_features})(out_features,in1_features,in2_features)   bias: (out_features)(\\text{out\\_features})(out_features)   output: (N,\u2217,Hout)(N, *, H_{out})(N,\u2217,Hout\u200b)   where Hout=out_featuresH_{out}=\\text{out\\_features}Hout\u200b=out_features   and all but the last dimension are the same shape as the input.  "},
{"library": "torch", "item_id": "torch.linspace", "item_type": "function", "code": "torch.linspace(start,end,steps=100,out=None,dtype=None,layout=torch.strided,device=None,requires_grad=False)\u2192Tensor", "description": "Returns a one-dimensional tensor of steps equally spaced points between start and end. The output tensor is 1-D of size steps.  Parameters  start (python:float) \u2013 the starting value for the set of points end (python:float) \u2013 the ending value for the set of points steps (python:int) \u2013 number of points to sample between start and end. Default: 100. out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor. Default: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: &gt;&gt;&gt; torch.linspace(3, 10, steps=5) tensor([  3.0000,   4.7500,   6.5000,   8.2500,  10.0000]) &gt;&gt;&gt; torch.linspace(-10, 10, steps=5) tensor([-10.,  -5.,   0.,   5.,  10.]) &gt;&gt;&gt; torch.linspace(start=-10, end=10, steps=5) tensor([-10.,  -5.,   0.,   5.,  10.]) &gt;&gt;&gt; torch.linspace(start=-10, end=10, steps=1) tensor([-10.])   ", "parameters": ["start (python:float) : the starting value for the set of points", "end (python:float) : the ending value for the set of points", "steps (python:int) : number of points to sample between startand end. Default: 100.", "out (Tensor, optional) : the output tensor.", "dtype (torch.dtype, optional) : the desired data type of returned tensor.Default: if None, uses a global default (see torch.set_default_tensor_type()).", "layout (torch.layout, optional) : the desired layout of returned Tensor.Default: torch.strided.", "device (torch.device, optional) : the desired device of returned tensor.Default: if None, uses the current device for the default tensor type(see torch.set_default_tensor_type()). device will be the CPUfor CPU tensor types and the current CUDA device for CUDA tensor types.", "requires_grad (bool, optional) : If autograd should record operations on thereturned tensor. Default: False."], "returns": null, "example": " torch.linspace(3, 10, steps=5)\ntensor([  3.0000,   4.7500,   6.5000,   8.2500,  10.0000])\n torch.linspace(-10, 10, steps=5)\ntensor([-10.,  -5.,   0.,   5.,  10.])\n torch.linspace(start=-10, end=10, steps=5)\ntensor([-10.,  -5.,   0.,   5.,  10.])\n torch.linspace(start=-10, end=10, steps=1)\ntensor([-10.])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.logspace", "item_type": "function", "code": "torch.logspace(start,end,steps=100,base=10.0,out=None,dtype=None,layout=torch.strided,device=None,requires_grad=False)\u2192Tensor", "description": "Returns a one-dimensional tensor of steps points logarithmically spaced with base base between basestart{\\text{base}}^{\\text{start}}basestart   and baseend{\\text{base}}^{\\text{end}}baseend  . The output tensor is 1-D of size steps.  Parameters  start (python:float) \u2013 the starting value for the set of points end (python:float) \u2013 the ending value for the set of points steps (python:int) \u2013 number of points to sample between start and end. Default: 100. base (python:float) \u2013 base of the logarithm function. Default: 10.0. out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor. Default: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: &gt;&gt;&gt; torch.logspace(start=-10, end=10, steps=5) tensor([ 1.0000e-10,  1.0000e-05,  1.0000e+00,  1.0000e+05,  1.0000e+10]) &gt;&gt;&gt; torch.logspace(start=0.1, end=1.0, steps=5) tensor([  1.2589,   2.1135,   3.5481,   5.9566,  10.0000]) &gt;&gt;&gt; torch.logspace(start=0.1, end=1.0, steps=1) tensor([1.2589]) &gt;&gt;&gt; torch.logspace(start=2, end=2, steps=1, base=2) tensor([4.0])   ", "parameters": ["start (python:float) : the starting value for the set of points", "end (python:float) : the ending value for the set of points", "steps (python:int) : number of points to sample between startand end. Default: 100.", "base (python:float) : base of the logarithm function. Default: 10.0.", "out (Tensor, optional) : the output tensor.", "dtype (torch.dtype, optional) : the desired data type of returned tensor.Default: if None, uses a global default (see torch.set_default_tensor_type()).", "layout (torch.layout, optional) : the desired layout of returned Tensor.Default: torch.strided.", "device (torch.device, optional) : the desired device of returned tensor.Default: if None, uses the current device for the default tensor type(see torch.set_default_tensor_type()). device will be the CPUfor CPU tensor types and the current CUDA device for CUDA tensor types.", "requires_grad (bool, optional) : If autograd should record operations on thereturned tensor. Default: False."], "returns": null, "example": " torch.logspace(start=-10, end=10, steps=5)\ntensor([ 1.0000e-10,  1.0000e-05,  1.0000e+00,  1.0000e+05,  1.0000e+10])\n torch.logspace(start=0.1, end=1.0, steps=5)\ntensor([  1.2589,   2.1135,   3.5481,   5.9566,  10.0000])\n torch.logspace(start=0.1, end=1.0, steps=1)\ntensor([1.2589])\n torch.logspace(start=2, end=2, steps=1, base=2)\ntensor([4.0])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.eye", "item_type": "function", "code": "torch.eye(n,m=None,out=None,dtype=None,layout=torch.strided,device=None,requires_grad=False)\u2192Tensor", "description": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.  Parameters  n (python:int) \u2013 the number of rows m (python:int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor. Default: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.   Returns A 2-D tensor with ones on the diagonal and zeros elsewhere  Return type Tensor   Example: &gt;&gt;&gt; torch.eye(3) tensor([[ 1.,  0.,  0.],         [ 0.,  1.,  0.],         [ 0.,  0.,  1.]])   ", "parameters": ["n (python:int) : the number of rows", "m (python:int, optional) : the number of columns with default being n", "out (Tensor, optional) : the output tensor.", "dtype (torch.dtype, optional) : the desired data type of returned tensor.Default: if None, uses a global default (see torch.set_default_tensor_type()).", "layout (torch.layout, optional) : the desired layout of returned Tensor.Default: torch.strided.", "device (torch.device, optional) : the desired device of returned tensor.Default: if None, uses the current device for the default tensor type(see torch.set_default_tensor_type()). device will be the CPUfor CPU tensor types and the current CUDA device for CUDA tensor types.", "requires_grad (bool, optional) : If autograd should record operations on thereturned tensor. Default: False."], "returns": "A 2-D tensor with ones on the diagonal and zeros elsewhere", "example": " torch.eye(3)\ntensor([[ 1.,  0.,  0.],\n        [ 0.,  1.,  0.],\n        [ 0.,  0.,  1.]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.empty", "item_type": "function", "code": "torch.empty(*size,out=None,dtype=None,layout=torch.strided,device=None,requires_grad=False,pin_memory=False)\u2192Tensor", "description": "Returns a tensor filled with uninitialized data. The shape of the tensor is defined by the variable argument size.  Parameters  size (python:int...) \u2013 a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple. out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor. Default: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False. pin_memory (bool, optional) \u2013 If set, returned tensor would be allocated in the pinned memory. Works only for CPU tensors. Default: False.    Example: &gt;&gt;&gt; torch.empty(2, 3) tensor(1.00000e-08 *        [[ 6.3984,  0.0000,  0.0000],         [ 0.0000,  0.0000,  0.0000]])   ", "parameters": ["size (python:int...) : a sequence of integers defining the shape of the output tensor.Can be a variable number of arguments or a collection like a list or tuple.", "out (Tensor, optional) : the output tensor.", "dtype (torch.dtype, optional) : the desired data type of returned tensor.Default: if None, uses a global default (see torch.set_default_tensor_type()).", "layout (torch.layout, optional) : the desired layout of returned Tensor.Default: torch.strided.", "device (torch.device, optional) : the desired device of returned tensor.Default: if None, uses the current device for the default tensor type(see torch.set_default_tensor_type()). device will be the CPUfor CPU tensor types and the current CUDA device for CUDA tensor types.", "requires_grad (bool, optional) : If autograd should record operations on thereturned tensor. Default: False.", "pin_memory (bool, optional) : If set, returned tensor would be allocated inthe pinned memory. Works only for CPU tensors. Default: False."], "returns": null, "example": " torch.empty(2, 3)\ntensor(1.00000e-08 *\n       [[ 6.3984,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.half_normal.HalfNormal.cdf", "item_type": "method", "code": "cdf(value)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.half_normal.HalfNormal.entropy", "item_type": "method", "code": "entropy()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.half_normal.HalfNormal.expand", "item_type": "method", "code": "expand(batch_shape,_instance=None)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.half_normal.HalfNormal.icdf", "item_type": "method", "code": "icdf(prob)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.half_normal.HalfNormal.log_prob", "item_type": "method", "code": "log_prob(value)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.half_normal.HalfNormal.mean", "item_type": "method", "code": "propertymean", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.half_normal.HalfNormal.scale", "item_type": "method", "code": "propertyscale", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.half_normal.HalfNormal.variance", "item_type": "method", "code": "propertyvariance", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.independent.Independent.entropy", "item_type": "method", "code": "entropy()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.independent.Independent.enumerate_support", "item_type": "method", "code": "enumerate_support(expand=True)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.independent.Independent.expand", "item_type": "method", "code": "expand(batch_shape,_instance=None)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.independent.Independent.has_enumerate_support", "item_type": "method", "code": "propertyhas_enumerate_support", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.independent.Independent.has_rsample", "item_type": "method", "code": "propertyhas_rsample", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.independent.Independent.log_prob", "item_type": "method", "code": "log_prob(value)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.rnn.pack_sequence", "item_type": "function", "code": "torch.nn.utils.rnn.pack_sequence(sequences,enforce_sorted=True)", "description": "Packs a list of variable length Tensors sequences should be a list of Tensors of size L x *, where L is the length of a sequence and * is any number of trailing dimensions, including zero. For unsorted sequences, use enforce_sorted = False. If enforce_sorted is True, the sequences should be sorted in the order of decreasing length. enforce_sorted = True is only necessary for ONNX export. Example &gt;&gt;&gt; from torch.nn.utils.rnn import pack_sequence &gt;&gt;&gt; a = torch.tensor([1,2,3]) &gt;&gt;&gt; b = torch.tensor([4,5]) &gt;&gt;&gt; c = torch.tensor([6]) &gt;&gt;&gt; pack_sequence([a, b, c]) PackedSequence(data=tensor([ 1,  4,  6,  2,  5,  3]), batch_sizes=tensor([ 3,  2,  1]))    Parameters  sequences (list[Tensor]) \u2013 A list of sequences of decreasing length. enforce_sorted (bool, optional) \u2013 if True, checks that the input contains sequences sorted by length in a decreasing order. If False, this condition is not checked. Default: True.   Returns a PackedSequence object   ", "parameters": ["sequences (list[Tensor]) : A list of sequences of decreasing length.", "enforce_sorted (bool, optional) : if True, checks that the inputcontains sequences sorted by length in a decreasing order. IfFalse, this condition is not checked. Default: True."], "returns": "a PackedSequence object", "example": " from torch.nn.utils.rnn import pack_sequence\n a = torch.tensor([1,2,3])\n b = torch.tensor([4,5])\n c = torch.tensor([6])\n pack_sequence([a, b, c])\nPackedSequence(data=tensor([ 1,  4,  6,  2,  5,  3]), batch_sizes=tensor([ 3,  2,  1]))\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.Module.add_module", "item_type": "method", "code": "add_module(name,module)", "description": "Adds a child module to the current module. The module can be accessed as an attribute using the given name.  Parameters  name (string) \u2013 name of the child module. The child module can be accessed from this module using the given name module (Module) \u2013 child module to be added to the module.    ", "parameters": ["name (string) : name of the child module. The child module can beaccessed from this module using the given name", "module (Module) : child module to be added to the module."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.Module.apply", "item_type": "method", "code": "apply(fn)", "description": "Applies fn recursively to every submodule (as returned by .children()) as well as self. Typical use includes initializing the parameters of a model (see also torch.nn.init).  Parameters fn (Module -&gt; None) \u2013 function to be applied to each submodule  Returns self  Return type Module   Example: &gt;&gt;&gt; def init_weights(m): &gt;&gt;&gt;     print(m) &gt;&gt;&gt;     if type(m) == nn.Linear: &gt;&gt;&gt;         m.weight.data.fill_(1.0) &gt;&gt;&gt;         print(m.weight) &gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) &gt;&gt;&gt; net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1.,  1.],         [ 1.,  1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1.,  1.],         [ 1.,  1.]]) Sequential(   (0): Linear(in_features=2, out_features=2, bias=True)   (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential(   (0): Linear(in_features=2, out_features=2, bias=True)   (1): Linear(in_features=2, out_features=2, bias=True) )   ", "parameters": ["fn (Module -&gt; None) : function to be applied to each submodule", "self", "Module"], "returns": "self", "example": " def init_weights(m):\n     print(m)\n     if type(m) == nn.Linear:\n         m.weight.data.fill_(1.0)\n         print(m.weight)\n net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n net.apply(init_weights)\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[ 1.,  1.],\n        [ 1.,  1.]])\nLinear(in_features=2, out_features=2, bias=True)\nParameter containing:\ntensor([[ 1.,  1.],\n        [ 1.,  1.]])\nSequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\nSequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.element_size", "item_type": "method", "code": "element_size()\u2192int", "description": "Returns the size in bytes of an individual element. Example: &gt;&gt;&gt; torch.tensor([]).element_size() 4 &gt;&gt;&gt; torch.tensor([], dtype=torch.uint8).element_size() 1   ", "parameters": [], "returns": null, "example": " torch.tensor([]).element_size()\n4\n torch.tensor([], dtype=torch.uint8).element_size()\n1\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.eq", "item_type": "method", "code": "eq(other)\u2192Tensor", "description": "See torch.eq() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.eq_", "item_type": "method", "code": "eq_(other)\u2192Tensor", "description": "In-place version of eq() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.equal", "item_type": "method", "code": "equal(other)\u2192bool", "description": "See torch.equal() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.erf", "item_type": "method", "code": "erf()\u2192Tensor", "description": "See torch.erf() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.erf_", "item_type": "method", "code": "erf_()\u2192Tensor", "description": "In-place version of erf() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.erfc", "item_type": "method", "code": "erfc()\u2192Tensor", "description": "See torch.erfc() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.erfc_", "item_type": "method", "code": "erfc_()\u2192Tensor", "description": "In-place version of erfc() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.erfinv", "item_type": "method", "code": "erfinv()\u2192Tensor", "description": "See torch.erfinv() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.erfinv_", "item_type": "method", "code": "erfinv_()\u2192Tensor", "description": "In-place version of erfinv() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.exp", "item_type": "method", "code": "exp()\u2192Tensor", "description": "See torch.exp() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.exp_", "item_type": "method", "code": "exp_()\u2192Tensor", "description": "In-place version of exp() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.expm1", "item_type": "method", "code": "expm1()\u2192Tensor", "description": "See torch.expm1() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.dropout", "item_type": "function", "code": "torch.nn.functional.dropout(input,p=0.5,training=True,inplace=False)", "description": "During training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution. See Dropout for details.  Parameters  p \u2013 probability of an element to be zeroed. Default: 0.5 training \u2013 apply dropout if is True. Default: True inplace \u2013 If set to True, will do this operation in-place. Default: False    ", "parameters": ["p : probability of an element to be zeroed. Default: 0.5", "training : apply dropout if is True. Default: True", "inplace : If set to True, will do this operation in-place. Default: False"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.alpha_dropout", "item_type": "function", "code": "torch.nn.functional.alpha_dropout(input,p=0.5,training=False,inplace=False)", "description": "Applies alpha dropout to the input. See AlphaDropout for details. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.dropout2d", "item_type": "function", "code": "torch.nn.functional.dropout2d(input,p=0.5,training=True,inplace=False)", "description": "Randomly zero out entire channels (a channel is a 2D feature map, e.g., the jjj  -th channel of the iii  -th sample in the batched input is a 2D tensor input[i,j]\\text{input}[i, j]input[i,j]  ) of the input tensor). Each channel will be zeroed out independently on every forward call with probability p using samples from a Bernoulli distribution. See Dropout2d for details.  Parameters  p \u2013 probability of a channel to be zeroed. Default: 0.5 training \u2013 apply dropout if is True. Default: True inplace \u2013 If set to True, will do this operation in-place. Default: False    ", "parameters": ["p : probability of a channel to be zeroed. Default: 0.5", "training : apply dropout if is True. Default: True", "inplace : If set to True, will do this operation in-place. Default: False"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.dropout3d", "item_type": "function", "code": "torch.nn.functional.dropout3d(input,p=0.5,training=True,inplace=False)", "description": "Randomly zero out entire channels (a channel is a 3D feature map, e.g., the jjj  -th channel of the iii  -th sample in the batched input is a 3D tensor input[i,j]\\text{input}[i, j]input[i,j]  ) of the input tensor). Each channel will be zeroed out independently on every forward call with probability p using samples from a Bernoulli distribution. See Dropout3d for details.  Parameters  p \u2013 probability of a channel to be zeroed. Default: 0.5 training \u2013 apply dropout if is True. Default: True inplace \u2013 If set to True, will do this operation in-place. Default: False    ", "parameters": ["p : probability of a channel to be zeroed. Default: 0.5", "training : apply dropout if is True. Default: True", "inplace : If set to True, will do this operation in-place. Default: False"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.embedding", "item_type": "function", "code": "torch.nn.functional.embedding(input,weight,padding_idx=None,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,sparse=False)", "description": "A simple lookup table that looks up embeddings in a fixed dictionary and size. This module is often used to retrieve word embeddings using indices. The input to the module is a list of indices, and the embedding matrix, and the output is the corresponding word embeddings. See torch.nn.Embedding for more details.  Parameters  input (LongTensor) \u2013 Tensor containing indices into the embedding matrix weight (Tensor) \u2013 The embedding matrix with number of rows equal to the maximum possible index + 1, and number of columns equal to the embedding size padding_idx (python:int, optional) \u2013 If given, pads the output with the embedding vector at padding_idx (initialized to zeros) whenever it encounters the index. max_norm (python:float, optional) \u2013 If given, each embedding vector with norm larger than max_norm is renormalized to have norm max_norm. Note: this will modify weight in-place. norm_type (python:float, optional) \u2013 The p of the p-norm to compute for the max_norm option. Default 2. scale_grad_by_freq (boolean, optional) \u2013 If given, this will scale gradients by the inverse of frequency of the words in the mini-batch. Default False. sparse (bool, optional) \u2013 If True, gradient w.r.t. weight will be a sparse tensor. See Notes under torch.nn.Embedding for more details regarding sparse gradients.     Shape: Input: LongTensor of arbitrary shape containing the indices to extract  Weight: Embedding matrix of floating point type with shape (V, embedding_dim),where V = maximum index + 1 and embedding_dim = the embedding size    Output: (*, embedding_dim), where * is the input shape    Examples: &gt;&gt;&gt; # a batch of 2 samples of 4 indices each &gt;&gt;&gt; input = torch.tensor([[1,2,4,5],[4,3,2,9]]) &gt;&gt;&gt; # an embedding matrix containing 10 tensors of size 3 &gt;&gt;&gt; embedding_matrix = torch.rand(10, 3) &gt;&gt;&gt; F.embedding(input, embedding_matrix) tensor([[[ 0.8490,  0.9625,  0.6753],          [ 0.9666,  0.7761,  0.6108],          [ 0.6246,  0.9751,  0.3618],          [ 0.4161,  0.2419,  0.7383]],          [[ 0.6246,  0.9751,  0.3618],          [ 0.0237,  0.7794,  0.0528],          [ 0.9666,  0.7761,  0.6108],          [ 0.3385,  0.8612,  0.1867]]])  &gt;&gt;&gt; # example with padding_idx &gt;&gt;&gt; weights = torch.rand(10, 3) &gt;&gt;&gt; weights[0, :].zero_() &gt;&gt;&gt; embedding_matrix = weights &gt;&gt;&gt; input = torch.tensor([[0,2,0,5]]) &gt;&gt;&gt; F.embedding(input, embedding_matrix, padding_idx=0) tensor([[[ 0.0000,  0.0000,  0.0000],          [ 0.5609,  0.5384,  0.8720],          [ 0.0000,  0.0000,  0.0000],          [ 0.6262,  0.2438,  0.7471]]])   ", "parameters": ["input (LongTensor) : Tensor containing indices into the embedding matrix", "weight (Tensor) : The embedding matrix with number of rows equal to the maximum possible index + 1,and number of columns equal to the embedding size", "padding_idx (python:int, optional) : If given, pads the output with the embedding vector at padding_idx(initialized to zeros) whenever it encounters the index.", "max_norm (python:float, optional) : If given, each embedding vector with norm larger than max_normis renormalized to have norm max_norm.Note: this will modify weight in-place.", "norm_type (python:float, optional) : The p of the p-norm to compute for the max_norm option. Default 2.", "scale_grad_by_freq (boolean, optional) : If given, this will scale gradients by the inverse of frequency ofthe words in the mini-batch. Default False.", "sparse (bool, optional) : If True, gradient w.r.t. weight will be a sparse tensor. See Notes undertorch.nn.Embedding for more details regarding sparse gradients."], "returns": null, "example": " # a batch of 2 samples of 4 indices each\n input = torch.tensor([[1,2,4,5],[4,3,2,9]])\n # an embedding matrix containing 10 tensors of size 3\n embedding_matrix = torch.rand(10, 3)\n F.embedding(input, embedding_matrix)\ntensor([[[ 0.8490,  0.9625,  0.6753],\n         [ 0.9666,  0.7761,  0.6108],\n         [ 0.6246,  0.9751,  0.3618],\n         [ 0.4161,  0.2419,  0.7383]],\n\n        [[ 0.6246,  0.9751,  0.3618],\n         [ 0.0237,  0.7794,  0.0528],\n         [ 0.9666,  0.7761,  0.6108],\n         [ 0.3385,  0.8612,  0.1867]]])\n\n # example with padding_idx\n weights = torch.rand(10, 3)\n weights[0, :].zero_()\n embedding_matrix = weights\n input = torch.tensor([[0,2,0,5]])\n F.embedding(input, embedding_matrix, padding_idx=0)\ntensor([[[ 0.0000,  0.0000,  0.0000],\n         [ 0.5609,  0.5384,  0.8720],\n         [ 0.0000,  0.0000,  0.0000],\n         [ 0.6262,  0.2438,  0.7471]]])\n\n", "shape": " Input: LongTensor of arbitrary shape containing the indices to extract  Weight: Embedding matrix of floating point type with shape (V, embedding_dim),where V = maximum index + 1 and embedding_dim = the embedding size    Output: (*, embedding_dim), where * is the input shape  "},
{"library": "torch", "item_id": "torch.empty_like", "item_type": "function", "code": "torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False)\u2192Tensor", "description": "Returns an uninitialized tensor with the same size as input. torch.empty_like(input) is equivalent to torch.empty(input.size(), dtype=input.dtype, layout=input.layout, device=input.device).  Parameters  input (Tensor) \u2013 the size of input will determine size of the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor. Default: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor. Default: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: &gt;&gt;&gt; torch.empty((2,3), dtype=torch.int64) tensor([[ 9.4064e+13,  2.8000e+01,  9.3493e+13],         [ 7.5751e+18,  7.1428e+18,  7.5955e+18]])   ", "parameters": ["input (Tensor) : the size of input will determine size of the output tensor.", "dtype (torch.dtype, optional) : the desired data type of returned Tensor.Default: if None, defaults to the dtype of input.", "layout (torch.layout, optional) : the desired layout of returned tensor.Default: if None, defaults to the layout of input.", "device (torch.device, optional) : the desired device of returned tensor.Default: if None, defaults to the device of input.", "requires_grad (bool, optional) : If autograd should record operations on thereturned tensor. Default: False."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.empty_strided", "item_type": "function", "code": "torch.empty_strided(size,stride,dtype=None,layout=None,device=None,requires_grad=False,pin_memory=False)\u2192Tensor", "description": "Returns a tensor filled with uninitialized data. The shape and strides of the tensor is defined by the variable argument size and stride respectively. torch.empty_strided(size, stride) is equivalent to torch.empty(size).as_strided(size, stride).  Warning More than one element of the created tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensors, please clone them first.   Parameters  size (tuple of python:ints) \u2013 the shape of the output tensor stride (tuple of python:ints) \u2013 the strides of the output tensor dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor. Default: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False. pin_memory (bool, optional) \u2013 If set, returned tensor would be allocated in the pinned memory. Works only for CPU tensors. Default: False.    Example: &gt;&gt;&gt; a = torch.empty_strided((2, 3), (1, 2)) &gt;&gt;&gt; a tensor([[8.9683e-44, 4.4842e-44, 5.1239e+07],         [0.0000e+00, 0.0000e+00, 3.0705e-41]]) &gt;&gt;&gt; a.stride() (1, 2) &gt;&gt;&gt; a.size() torch.Size([2, 3])   ", "parameters": ["size (tuple of python:ints) : the shape of the output tensor", "stride (tuple of python:ints) : the strides of the output tensor", "dtype (torch.dtype, optional) : the desired data type of returned tensor.Default: if None, uses a global default (see torch.set_default_tensor_type()).", "layout (torch.layout, optional) : the desired layout of returned Tensor.Default: torch.strided.", "device (torch.device, optional) : the desired device of returned tensor.Default: if None, uses the current device for the default tensor type(see torch.set_default_tensor_type()). device will be the CPUfor CPU tensor types and the current CUDA device for CUDA tensor types.", "requires_grad (bool, optional) : If autograd should record operations on thereturned tensor. Default: False.", "pin_memory (bool, optional) : If set, returned tensor would be allocated inthe pinned memory. Works only for CPU tensors. Default: False."], "returns": null, "example": " a = torch.empty_strided((2, 3), (1, 2))\n a\ntensor([[8.9683e-44, 4.4842e-44, 5.1239e+07],\n        [0.0000e+00, 0.0000e+00, 3.0705e-41]])\n a.stride()\n(1, 2)\n a.size()\ntorch.Size([2, 3])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.full", "item_type": "function", "code": "torch.full(size,fill_value,out=None,dtype=None,layout=torch.strided,device=None,requires_grad=False)\u2192Tensor", "description": "Returns a tensor of size size filled with fill_value.  Parameters  size (python:int...) \u2013 a list, tuple, or torch.Size of integers defining the shape of the output tensor. fill_value \u2013 the number to fill the output tensor with. out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor. Default: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: &gt;&gt;&gt; torch.full((2, 3), 3.141592) tensor([[ 3.1416,  3.1416,  3.1416],         [ 3.1416,  3.1416,  3.1416]])   ", "parameters": ["size (python:int...) : a list, tuple, or torch.Size of integers defining theshape of the output tensor.", "fill_value : the number to fill the output tensor with.", "out (Tensor, optional) : the output tensor.", "dtype (torch.dtype, optional) : the desired data type of returned tensor.Default: if None, uses a global default (see torch.set_default_tensor_type()).", "layout (torch.layout, optional) : the desired layout of returned Tensor.Default: torch.strided.", "device (torch.device, optional) : the desired device of returned tensor.Default: if None, uses the current device for the default tensor type(see torch.set_default_tensor_type()). device will be the CPUfor CPU tensor types and the current CUDA device for CUDA tensor types.", "requires_grad (bool, optional) : If autograd should record operations on thereturned tensor. Default: False."], "returns": null, "example": " torch.full((2, 3), 3.141592)\ntensor([[ 3.1416,  3.1416,  3.1416],\n        [ 3.1416,  3.1416,  3.1416]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.full_like", "item_type": "function", "code": "torch.full_like(input,fill_value,out=None,dtype=None,layout=torch.strided,device=None,requires_grad=False)\u2192Tensor", "description": "Returns a tensor with the same size as input filled with fill_value. torch.full_like(input, fill_value) is equivalent to torch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device).  Parameters  input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor. Default: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor. Default: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    ", "parameters": ["input (Tensor) : the size of input will determine size of the output tensor.", "fill_value : the number to fill the output tensor with.", "dtype (torch.dtype, optional) : the desired data type of returned Tensor.Default: if None, defaults to the dtype of input.", "layout (torch.layout, optional) : the desired layout of returned tensor.Default: if None, defaults to the layout of input.", "device (torch.device, optional) : the desired device of returned tensor.Default: if None, defaults to the device of input.", "requires_grad (bool, optional) : If autograd should record operations on thereturned tensor. Default: False."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.independent.Independent.mean", "item_type": "method", "code": "propertymean", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.independent.Independent.rsample", "item_type": "method", "code": "rsample(sample_shape=torch.Size([]))", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.independent.Independent.sample", "item_type": "method", "code": "sample(sample_shape=torch.Size([]))", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.independent.Independent.support", "item_type": "method", "code": "propertysupport", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.independent.Independent.variance", "item_type": "method", "code": "propertyvariance", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.laplace.Laplace.cdf", "item_type": "method", "code": "cdf(value)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.laplace.Laplace.entropy", "item_type": "method", "code": "entropy()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.laplace.Laplace.expand", "item_type": "method", "code": "expand(batch_shape,_instance=None)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.laplace.Laplace.icdf", "item_type": "method", "code": "icdf(value)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.laplace.Laplace.log_prob", "item_type": "method", "code": "log_prob(value)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.laplace.Laplace.mean", "item_type": "method", "code": "propertymean", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.Module.buffers", "item_type": "method", "code": "buffers(recurse=True)", "description": "Returns an iterator over module buffers.  Parameters recurse (bool) \u2013 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.  Yields torch.Tensor \u2013 module buffer   Example: &gt;&gt;&gt; for buf in model.buffers(): &gt;&gt;&gt;     print(type(buf.data), buf.size()) &lt;class 'torch.FloatTensor'&gt; (20L,) &lt;class 'torch.FloatTensor'&gt; (20L, 1L, 5L, 5L)   ", "parameters": ["recurse (bool) : if True, then yields buffers of this moduleand all submodules. Otherwise, yields only buffers thatare direct members of this module.", "torch.Tensor : module buffer"], "returns": null, "example": " for buf in model.buffers():\n     print(type(buf.data), buf.size())\n&lt;class 'torch.FloatTensor' (20L,)\n&lt;class 'torch.FloatTensor' (20L, 1L, 5L, 5L)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.Module.children", "item_type": "method", "code": "children()", "description": "Returns an iterator over immediate children modules.  Yields Module \u2013 a child module   ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.Module.cpu", "item_type": "method", "code": "cpu()", "description": "Moves all model parameters and buffers to the CPU.  Returns self  Return type Module   ", "parameters": [], "returns": "self", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.Module.cuda", "item_type": "method", "code": "cuda(device=None)", "description": "Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.  Parameters device (python:int, optional) \u2013 if specified, all parameters will be copied to that device  Returns self  Return type Module   ", "parameters": ["device (python:int, optional) : if specified, all parameters will becopied to that device", "self", "Module"], "returns": "self", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.Module.double", "item_type": "method", "code": "double()", "description": "Casts all floating point parameters and buffers to double datatype.  Returns self  Return type Module   ", "parameters": [], "returns": "self", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.Module.eval", "item_type": "method", "code": "eval()", "description": "Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc. This is equivalent with self.train(False).  Returns self  Return type Module   ", "parameters": [], "returns": "self", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.Module.extra_repr", "item_type": "method", "code": "extra_repr()", "description": "Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.Module.float", "item_type": "method", "code": "float()", "description": "Casts all floating point parameters and buffers to float datatype.  Returns self  Return type Module   ", "parameters": [], "returns": "self", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.Module.forward", "item_type": "method", "code": "forward(*input)", "description": "Defines the computation performed at every call. Should be overridden by all subclasses.  Note Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.  ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.expm1_", "item_type": "method", "code": "expm1_()\u2192Tensor", "description": "In-place version of expm1() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.expand", "item_type": "method", "code": "expand(*sizes)\u2192Tensor", "description": "Returns a new view of the self tensor with singleton dimensions expanded to a larger size. Passing -1 as the size for a dimension means not changing the size of that dimension. Tensor can be also expanded to a larger number of dimensions, and the new ones will be appended at the front. For the new dimensions, the size cannot be set to -1. Expanding a tensor does not allocate new memory, but only creates a new view on the existing tensor where a dimension of size one is expanded to a larger size by setting the stride to 0. Any dimension of size 1 can be expanded to an arbitrary value without allocating new memory.  Parameters *sizes (torch.Size or python:int...) \u2013 the desired expanded size    Warning More than one element of an expanded tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensors, please clone them first.  Example: &gt;&gt;&gt; x = torch.tensor([[1], [2], [3]]) &gt;&gt;&gt; x.size() torch.Size([3, 1]) &gt;&gt;&gt; x.expand(3, 4) tensor([[ 1,  1,  1,  1],         [ 2,  2,  2,  2],         [ 3,  3,  3,  3]]) &gt;&gt;&gt; x.expand(-1, 4)   # -1 means not changing the size of that dimension tensor([[ 1,  1,  1,  1],         [ 2,  2,  2,  2],         [ 3,  3,  3,  3]])   ", "parameters": ["*sizes (torch.Size or python:int...) : the desired expanded size"], "returns": null, "example": " x = torch.tensor([[1], [2], [3]])\n x.size()\ntorch.Size([3, 1])\n x.expand(3, 4)\ntensor([[ 1,  1,  1,  1],\n        [ 2,  2,  2,  2],\n        [ 3,  3,  3,  3]])\n x.expand(-1, 4)   # -1 means not changing the size of that dimension\ntensor([[ 1,  1,  1,  1],\n        [ 2,  2,  2,  2],\n        [ 3,  3,  3,  3]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.expand_as", "item_type": "method", "code": "expand_as(other)\u2192Tensor", "description": "Expand this tensor to the same size as other. self.expand_as(other) is equivalent to self.expand(other.size()). Please see expand() for more information about expand.  Parameters other (torch.Tensor) \u2013 The result tensor has the same size as other.   ", "parameters": ["other (torch.Tensor) : The result tensor has the same sizeas other."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.exponential_", "item_type": "method", "code": "exponential_(lambd=1,*,generator=None)\u2192Tensor", "description": "Fills self tensor with elements drawn from the exponential distribution:  f(x)=\u03bbe\u2212\u03bbxf(x) = \\lambda e^{-\\lambda x}f(x)=\u03bbe\u2212\u03bbx  ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.fft", "item_type": "method", "code": "fft(signal_ndim,normalized=False)\u2192Tensor", "description": "See torch.fft() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.fill_", "item_type": "method", "code": "fill_(value)\u2192Tensor", "description": "Fills self tensor with the specified value. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.flatten", "item_type": "method", "code": "flatten(input,start_dim=0,end_dim=-1)\u2192Tensor", "description": "see torch.flatten() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.flip", "item_type": "method", "code": "flip(dims)\u2192Tensor", "description": "See torch.flip() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.float", "item_type": "method", "code": "float()\u2192Tensor", "description": "self.float() is equivalent to self.to(torch.float32). See to(). ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.embedding_bag", "item_type": "function", "code": "torch.nn.functional.embedding_bag(input,weight,offsets=None,max_norm=None,norm_type=2,scale_grad_by_freq=False,mode='mean',sparse=False,per_sample_weights=None)", "description": "Computes sums, means or maxes of bags of embeddings, without instantiating the intermediate embeddings. See torch.nn.EmbeddingBag for more details.  Note When using the CUDA backend, this operation may induce nondeterministic behaviour in its backward pass that is not easily switched off. Please see the notes on Reproducibility for background.   Parameters  input (LongTensor) \u2013 Tensor containing bags of indices into the embedding matrix weight (Tensor) \u2013 The embedding matrix with number of rows equal to the maximum possible index + 1, and number of columns equal to the embedding size offsets (LongTensor, optional) \u2013 Only used when input is 1D. offsets determines the starting index position of each bag (sequence) in input. max_norm (python:float, optional) \u2013 If given, each embedding vector with norm larger than max_norm is renormalized to have norm max_norm. Note: this will modify weight in-place. norm_type (python:float, optional) \u2013 The p in the p-norm to compute for the max_norm option. Default 2. scale_grad_by_freq (boolean, optional) \u2013 if given, this will scale gradients by the inverse of frequency of the words in the mini-batch. Default False. Note: this option is not supported when mode=\"max\". mode (string, optional) \u2013 \"sum\", \"mean\" or \"max\". Specifies the way to reduce the bag. Default: \"mean\" sparse (bool, optional) \u2013 if True, gradient w.r.t. weight will be a sparse tensor. See Notes under torch.nn.Embedding for more details regarding sparse gradients. Note: this option is not supported when mode=\"max\". per_sample_weights (Tensor, optional) \u2013 a tensor of float / double weights, or None to indicate all weights should be taken to be 1. If specified, per_sample_weights must have exactly the same shape as input and is treated as having the same offsets, if those are not None.    Shape:   input (LongTensor) and offsets (LongTensor, optional)  If input is 2D of shape (B, N), it will be treated as B bags (sequences) each of fixed length N, and this will return B values aggregated in a way depending on the mode. offsets is ignored and required to be None in this case.  If input is 1D of shape (N), it will be treated as a concatenation of multiple bags (sequences). offsets is required to be a 1D tensor containing the starting index positions of each bag in input. Therefore, for offsets of shape (B), input will be viewed as having B bags. Empty bags (i.e., having 0-length) will have returned vectors filled by zeros.    weight (Tensor): the learnable weights of the module of shape (num_embeddings, embedding_dim) per_sample_weights (Tensor, optional). Has the same shape as input. output: aggregated embedding values of shape (B, embedding_dim)   Examples: &gt;&gt;&gt; # an Embedding module containing 10 tensors of size 3 &gt;&gt;&gt; embedding_matrix = torch.rand(10, 3) &gt;&gt;&gt; # a batch of 2 samples of 4 indices each &gt;&gt;&gt; input = torch.tensor([1,2,4,5,4,3,2,9]) &gt;&gt;&gt; offsets = torch.tensor([0,4]) &gt;&gt;&gt; F.embedding_bag(embedding_matrix, input, offsets) tensor([[ 0.3397,  0.3552,  0.5545],         [ 0.5893,  0.4386,  0.5882]])   ", "parameters": ["input (LongTensor) : Tensor containing bags of indices into the embedding matrix", "weight (Tensor) : The embedding matrix with number of rows equal to the maximum possible index + 1,and number of columns equal to the embedding size", "offsets (LongTensor, optional) : Only used when input is 1D. offsets determinesthe starting index position of each bag (sequence) in input.", "max_norm (python:float, optional) : If given, each embedding vector with norm larger than max_normis renormalized to have norm max_norm.Note: this will modify weight in-place.", "norm_type (python:float, optional) : The p in the p-norm to compute for the max_norm option.Default 2.", "scale_grad_by_freq (boolean, optional) : if given, this will scale gradients by the inverse of frequency ofthe words in the mini-batch. Default False.Note: this option is not supported when mode=\"max\".", "mode (string, optional) : \"sum\", \"mean\" or \"max\". Specifies the way to reduce the bag.Default: \"mean\"", "sparse (bool, optional) : if True, gradient w.r.t. weight will be a sparse tensor. See Notes undertorch.nn.Embedding for more details regarding sparse gradients.Note: this option is not supported when mode=\"max\".", "per_sample_weights (Tensor, optional) : a tensor of float / double weights, or Noneto indicate all weights should be taken to be 1. If specified, per_sample_weightsmust have exactly the same shape as input and is treated as having the sameoffsets, if those are not None."], "returns": null, "example": " # an Embedding module containing 10 tensors of size 3\n embedding_matrix = torch.rand(10, 3)\n # a batch of 2 samples of 4 indices each\n input = torch.tensor([1,2,4,5,4,3,2,9])\n offsets = torch.tensor([0,4])\n F.embedding_bag(embedding_matrix, input, offsets)\ntensor([[ 0.3397,  0.3552,  0.5545],\n        [ 0.5893,  0.4386,  0.5882]])\n\n", "shape": "  input (LongTensor) and offsets (LongTensor, optional)  If input is 2D of shape (B, N), it will be treated as B bags (sequences) each of fixed length N, and this will return B values aggregated in a way depending on the mode. offsets is ignored and required to be None in this case.  If input is 1D of shape (N), it will be treated as a concatenation of multiple bags (sequences). offsets is required to be a 1D tensor containing the starting index positions of each bag in input. Therefore, for offsets of shape (B), input will be viewed as having B bags. Empty bags (i.e., having 0-length) will have returned vectors filled by zeros.    weight (Tensor): the learnable weights of the module of shape (num_embeddings, embedding_dim) per_sample_weights (Tensor, optional). Has the same shape as input. output: aggregated embedding values of shape (B, embedding_dim)  "},
{"library": "torch", "item_id": "torch.nn.functional.one_hot", "item_type": "function", "code": "torch.nn.functional.one_hot(tensor,num_classes=-1)\u2192LongTensor", "description": "Takes LongTensor with index values of shape (*) and returns a tensor of shape (*, num_classes) that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1. See also One-hot on Wikipedia .  Parameters  tensor (LongTensor) \u2013 class values of any shape. num_classes (python:int) \u2013 Total number of classes. If set to -1, the number of classes will be inferred as one greater than the largest class value in the input tensor.   Returns LongTensor that has one more dimension with 1 values at the index of last dimension indicated by the input, and 0 everywhere else.   Examples &gt;&gt;&gt; F.one_hot(torch.arange(0, 5) % 3) tensor([[1, 0, 0],         [0, 1, 0],         [0, 0, 1],         [1, 0, 0],         [0, 1, 0]]) &gt;&gt;&gt; F.one_hot(torch.arange(0, 5) % 3, num_classes=5) tensor([[1, 0, 0, 0, 0],         [0, 1, 0, 0, 0],         [0, 0, 1, 0, 0],         [1, 0, 0, 0, 0],         [0, 1, 0, 0, 0]]) &gt;&gt;&gt; F.one_hot(torch.arange(0, 6).view(3,2) % 3) tensor([[[1, 0, 0],          [0, 1, 0]],         [[0, 0, 1],          [1, 0, 0]],         [[0, 1, 0],          [0, 0, 1]]])   ", "parameters": ["tensor (LongTensor) : class values of any shape.", "num_classes (python:int) : Total number of classes. If set to -1, the numberof classes will be inferred as one greater than the largest classvalue in the input tensor."], "returns": "LongTensor that has one more dimension with 1 values at theindex of last dimension indicated by the input, and 0 everywhereelse.", "example": " F.one_hot(torch.arange(0, 5) % 3)\ntensor([[1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1],\n        [1, 0, 0],\n        [0, 1, 0]])\n F.one_hot(torch.arange(0, 5) % 3, num_classes=5)\ntensor([[1, 0, 0, 0, 0],\n        [0, 1, 0, 0, 0],\n        [0, 0, 1, 0, 0],\n        [1, 0, 0, 0, 0],\n        [0, 1, 0, 0, 0]])\n F.one_hot(torch.arange(0, 6).view(3,2) % 3)\ntensor([[[1, 0, 0],\n         [0, 1, 0]],\n        [[0, 0, 1],\n         [1, 0, 0]],\n        [[0, 1, 0],\n         [0, 0, 1]]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.quantize_per_tensor", "item_type": "function", "code": "torch.quantize_per_tensor(input,scale,zero_point,dtype)\u2192Tensor", "description": "Converts a float tensor to quantized tensor with given scale and zero point.  Parameters  input (Tensor) \u2013 float tensor to quantize scale (python:float) \u2013 scale to apply in quantization formula zero_point (python:int) \u2013 offset in integer value that maps to float zero dtype (torch.dtype) \u2013 the desired data type of returned tensor. Has to be one of the quantized dtypes: torch.quint8, torch.qint8, torch.qint32   Returns A newly quantized tensor  Return type Tensor   Example: &gt;&gt;&gt; torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8) tensor([-1.,  0.,  1.,  2.], size=(4,), dtype=torch.quint8,        quantization_scheme=torch.per_tensor_affine, scale=0.1, zero_point=10) &gt;&gt;&gt; torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8).int_repr() tensor([ 0, 10, 20, 30], dtype=torch.uint8)   ", "parameters": ["input (Tensor) : float tensor to quantize", "scale (python:float) : scale to apply in quantization formula", "zero_point (python:int) : offset in integer value that maps to float zero", "dtype (torch.dtype) : the desired data type of returned tensor.Has to be one of the quantized dtypes: torch.quint8, torch.qint8, torch.qint32"], "returns": "A newly quantized tensor", "example": " torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8)\ntensor([-1.,  0.,  1.,  2.], size=(4,), dtype=torch.quint8,\n       quantization_scheme=torch.per_tensor_affine, scale=0.1, zero_point=10)\n torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8).int_repr()\ntensor([ 0, 10, 20, 30], dtype=torch.uint8)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.quantize_per_channel", "item_type": "function", "code": "torch.quantize_per_channel(input,scales,zero_points,axis,dtype)\u2192Tensor", "description": "Converts a float tensor to per-channel quantized tensor with given scales and zero points.  Parameters  input (Tensor) \u2013 float tensor to quantize scales (Tensor) \u2013 float 1D tensor of scales to use, size should match input.size(axis) zero_points (python:int) \u2013 integer 1D tensor of offset to use, size should match input.size(axis) axis (python:int) \u2013 dimension on which apply per-channel quantization dtype (torch.dtype) \u2013 the desired data type of returned tensor. Has to be one of the quantized dtypes: torch.quint8, torch.qint8, torch.qint32   Returns A newly quantized tensor  Return type Tensor   Example: &gt;&gt;&gt; x = torch.tensor([[-1.0, 0.0], [1.0, 2.0]]) &gt;&gt;&gt; torch.quantize_per_channel(x, torch.tensor([0.1, 0.01]), torch.tensor([10, 0]), 0, torch.quint8) tensor([[-1.,  0.],         [ 1.,  2.]], size=(2, 2), dtype=torch.quint8,        quantization_scheme=torch.per_channel_affine,        scale=tensor([0.1000, 0.0100], dtype=torch.float64),        zero_point=tensor([10,  0]), axis=0) &gt;&gt;&gt; torch.quantize_per_channel(x, torch.tensor([0.1, 0.01]), torch.tensor([10, 0]), 0, torch.quint8).int_repr() tensor([[  0,  10],         [100, 200]], dtype=torch.uint8)   ", "parameters": ["input (Tensor) : float tensor to quantize", "scales (Tensor) : float 1D tensor of scales to use, size should match input.size(axis)", "zero_points (python:int) : integer 1D tensor of offset to use, size should match input.size(axis)", "axis (python:int) : dimension on which apply per-channel quantization", "dtype (torch.dtype) : the desired data type of returned tensor.Has to be one of the quantized dtypes: torch.quint8, torch.qint8, torch.qint32"], "returns": "A newly quantized tensor", "example": " x = torch.tensor([[-1.0, 0.0], [1.0, 2.0]])\n torch.quantize_per_channel(x, torch.tensor([0.1, 0.01]), torch.tensor([10, 0]), 0, torch.quint8)\ntensor([[-1.,  0.],\n        [ 1.,  2.]], size=(2, 2), dtype=torch.quint8,\n       quantization_scheme=torch.per_channel_affine,\n       scale=tensor([0.1000, 0.0100], dtype=torch.float64),\n       zero_point=tensor([10,  0]), axis=0)\n torch.quantize_per_channel(x, torch.tensor([0.1, 0.01]), torch.tensor([10, 0]), 0, torch.quint8).int_repr()\ntensor([[  0,  10],\n        [100, 200]], dtype=torch.uint8)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.cat", "item_type": "function", "code": "torch.cat(tensors,dim=0,out=None)\u2192Tensor", "description": "Concatenates the given sequence of seq tensors in the given dimension. All tensors must either have the same shape (except in the concatenating dimension) or be empty. torch.cat() can be seen as an inverse operation for torch.split() and torch.chunk(). torch.cat() can be best understood via examples.  Parameters  tensors (sequence of Tensors) \u2013 any python sequence of tensors of the same type. Non-empty tensors provided must have the same shape, except in the cat dimension. dim (python:int, optional) \u2013 the dimension over which the tensors are concatenated out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; x = torch.randn(2, 3) &gt;&gt;&gt; x tensor([[ 0.6580, -1.0969, -0.4614],         [-0.1034, -0.5790,  0.1497]]) &gt;&gt;&gt; torch.cat((x, x, x), 0) tensor([[ 0.6580, -1.0969, -0.4614],         [-0.1034, -0.5790,  0.1497],         [ 0.6580, -1.0969, -0.4614],         [-0.1034, -0.5790,  0.1497],         [ 0.6580, -1.0969, -0.4614],         [-0.1034, -0.5790,  0.1497]]) &gt;&gt;&gt; torch.cat((x, x, x), 1) tensor([[ 0.6580, -1.0969, -0.4614,  0.6580, -1.0969, -0.4614,  0.6580,          -1.0969, -0.4614],         [-0.1034, -0.5790,  0.1497, -0.1034, -0.5790,  0.1497, -0.1034,          -0.5790,  0.1497]])   ", "parameters": ["tensors (sequence of Tensors) : any python sequence of tensors of the same type.Non-empty tensors provided must have the same shape, except in thecat dimension.", "dim (python:int, optional) : the dimension over which the tensors are concatenated", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " x = torch.randn(2, 3)\n x\ntensor([[ 0.6580, -1.0969, -0.4614],\n        [-0.1034, -0.5790,  0.1497]])\n torch.cat((x, x, x), 0)\ntensor([[ 0.6580, -1.0969, -0.4614],\n        [-0.1034, -0.5790,  0.1497],\n        [ 0.6580, -1.0969, -0.4614],\n        [-0.1034, -0.5790,  0.1497],\n        [ 0.6580, -1.0969, -0.4614],\n        [-0.1034, -0.5790,  0.1497]])\n torch.cat((x, x, x), 1)\ntensor([[ 0.6580, -1.0969, -0.4614,  0.6580, -1.0969, -0.4614,  0.6580,\n         -1.0969, -0.4614],\n        [-0.1034, -0.5790,  0.1497, -0.1034, -0.5790,  0.1497, -0.1034,\n         -0.5790,  0.1497]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.laplace.Laplace.rsample", "item_type": "method", "code": "rsample(sample_shape=torch.Size([]))", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.laplace.Laplace.stddev", "item_type": "method", "code": "propertystddev", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.laplace.Laplace.variance", "item_type": "method", "code": "propertyvariance", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.log_normal.LogNormal.entropy", "item_type": "method", "code": "entropy()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.log_normal.LogNormal.expand", "item_type": "method", "code": "expand(batch_shape,_instance=None)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.log_normal.LogNormal.loc", "item_type": "method", "code": "propertyloc", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.log_normal.LogNormal.mean", "item_type": "method", "code": "propertymean", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.log_normal.LogNormal.scale", "item_type": "method", "code": "propertyscale", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.log_normal.LogNormal.variance", "item_type": "method", "code": "propertyvariance", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.entropy", "item_type": "method", "code": "entropy()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.expand", "item_type": "method", "code": "expand(batch_shape,_instance=None)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.log_prob", "item_type": "method", "code": "log_prob(value)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.Module.half", "item_type": "method", "code": "half()", "description": "Casts all floating point parameters and buffers to half datatype.  Returns self  Return type Module   ", "parameters": [], "returns": "self", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.Module.load_state_dict", "item_type": "method", "code": "load_state_dict(state_dict,strict=True)", "description": "Copies parameters and buffers from state_dict into this module and its descendants. If strict is True, then the keys of state_dict must exactly match the keys returned by this module\u2019s state_dict() function.  Parameters  state_dict (dict) \u2013 a dict containing parameters and persistent buffers. strict (bool, optional) \u2013 whether to strictly enforce that the keys in state_dict match the keys returned by this module\u2019s state_dict() function. Default: True   Returns  missing_keys is a list of str containing the missing keys unexpected_keys is a list of str containing the unexpected keys    Return type NamedTuple with missing_keys and unexpected_keys fields   ", "parameters": ["state_dict (dict) : a dict containing parameters andpersistent buffers.", "strict (bool, optional) : whether to strictly enforce that the keysin state_dict match the keys returned by this module\u2019sstate_dict() function. Default: True", "missing_keys is a list of str containing the missing keys", "unexpected_keys is a list of str containing the unexpected keys"], "returns": "missing_keys is a list of str containing the missing keysunexpected_keys is a list of str containing the unexpected keys", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.Module.modules", "item_type": "method", "code": "modules()", "description": "Returns an iterator over all modules in the network.  Yields Module \u2013 a module in the network    Note Duplicate modules are returned only once. In the following example, l will be returned only once.  Example: &gt;&gt;&gt; l = nn.Linear(2, 2) &gt;&gt;&gt; net = nn.Sequential(l, l) &gt;&gt;&gt; for idx, m in enumerate(net.modules()):         print(idx, '-&gt;', m)  0 -&gt; Sequential(   (0): Linear(in_features=2, out_features=2, bias=True)   (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -&gt; Linear(in_features=2, out_features=2, bias=True)   ", "parameters": [], "returns": null, "example": " l = nn.Linear(2, 2)\n net = nn.Sequential(l, l)\n for idx, m in enumerate(net.modules()):\n        print(idx, '-', m)\n\n0 - Sequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n)\n1 - Linear(in_features=2, out_features=2, bias=True)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.Module.named_buffers", "item_type": "method", "code": "named_buffers(prefix='',recurse=True)", "description": "Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.  Parameters  prefix (str) \u2013 prefix to prepend to all buffer names. recurse (bool) \u2013 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.   Yields (string, torch.Tensor) \u2013 Tuple containing the name and buffer   Example: &gt;&gt;&gt; for name, buf in self.named_buffers(): &gt;&gt;&gt;    if name in ['running_var']: &gt;&gt;&gt;        print(buf.size())   ", "parameters": ["prefix (str) : prefix to prepend to all buffer names.", "recurse (bool) : if True, then yields buffers of this moduleand all submodules. Otherwise, yields only buffers thatare direct members of this module."], "returns": null, "example": " for name, buf in self.named_buffers():\n    if name in ['running_var']:\n        print(buf.size())\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.Module.named_children", "item_type": "method", "code": "named_children()", "description": "Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.  Yields (string, Module) \u2013 Tuple containing a name and child module   Example: &gt;&gt;&gt; for name, module in model.named_children(): &gt;&gt;&gt;     if name in ['conv4', 'conv5']: &gt;&gt;&gt;         print(module)   ", "parameters": [], "returns": null, "example": " for name, module in model.named_children():\n     if name in ['conv4', 'conv5']:\n         print(module)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.Module.named_modules", "item_type": "method", "code": "named_modules(memo=None,prefix='')", "description": "Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.  Yields (string, Module) \u2013 Tuple of name and module    Note Duplicate modules are returned only once. In the following example, l will be returned only once.  Example: &gt;&gt;&gt; l = nn.Linear(2, 2) &gt;&gt;&gt; net = nn.Sequential(l, l) &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):         print(idx, '-&gt;', m)  0 -&gt; ('', Sequential(   (0): Linear(in_features=2, out_features=2, bias=True)   (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -&gt; ('0', Linear(in_features=2, out_features=2, bias=True))   ", "parameters": [], "returns": null, "example": " l = nn.Linear(2, 2)\n net = nn.Sequential(l, l)\n for idx, m in enumerate(net.named_modules()):\n        print(idx, '-', m)\n\n0 - ('', Sequential(\n  (0): Linear(in_features=2, out_features=2, bias=True)\n  (1): Linear(in_features=2, out_features=2, bias=True)\n))\n1 - ('0', Linear(in_features=2, out_features=2, bias=True))\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.Module.named_parameters", "item_type": "method", "code": "named_parameters(prefix='',recurse=True)", "description": "Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.  Parameters  prefix (str) \u2013 prefix to prepend to all parameter names. recurse (bool) \u2013 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.   Yields (string, Parameter) \u2013 Tuple containing the name and parameter   Example: &gt;&gt;&gt; for name, param in self.named_parameters(): &gt;&gt;&gt;    if name in ['bias']: &gt;&gt;&gt;        print(param.size())   ", "parameters": ["prefix (str) : prefix to prepend to all parameter names.", "recurse (bool) : if True, then yields parameters of this moduleand all submodules. Otherwise, yields only parameters thatare direct members of this module."], "returns": null, "example": " for name, param in self.named_parameters():\n    if name in ['bias']:\n        print(param.size())\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.Module.parameters", "item_type": "method", "code": "parameters(recurse=True)", "description": "Returns an iterator over module parameters. This is typically passed to an optimizer.  Parameters recurse (bool) \u2013 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.  Yields Parameter \u2013 module parameter   Example: &gt;&gt;&gt; for param in model.parameters(): &gt;&gt;&gt;     print(type(param.data), param.size()) &lt;class 'torch.FloatTensor'&gt; (20L,) &lt;class 'torch.FloatTensor'&gt; (20L, 1L, 5L, 5L)   ", "parameters": ["recurse (bool) : if True, then yields parameters of this moduleand all submodules. Otherwise, yields only parameters thatare direct members of this module.", "Parameter : module parameter"], "returns": null, "example": " for param in model.parameters():\n     print(type(param.data), param.size())\n&lt;class 'torch.FloatTensor' (20L,)\n&lt;class 'torch.FloatTensor' (20L, 1L, 5L, 5L)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.floor", "item_type": "method", "code": "floor()\u2192Tensor", "description": "See torch.floor() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.floor_", "item_type": "method", "code": "floor_()\u2192Tensor", "description": "In-place version of floor() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.fmod", "item_type": "method", "code": "fmod(divisor)\u2192Tensor", "description": "See torch.fmod() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.fmod_", "item_type": "method", "code": "fmod_(divisor)\u2192Tensor", "description": "In-place version of fmod() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.frac", "item_type": "method", "code": "frac()\u2192Tensor", "description": "See torch.frac() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.frac_", "item_type": "method", "code": "frac_()\u2192Tensor", "description": "In-place version of frac() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.gather", "item_type": "method", "code": "gather(dim,index)\u2192Tensor", "description": "See torch.gather() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.ge", "item_type": "method", "code": "ge(other)\u2192Tensor", "description": "See torch.ge() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.ge_", "item_type": "method", "code": "ge_(other)\u2192Tensor", "description": "In-place version of ge() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.geometric_", "item_type": "method", "code": "geometric_(p,*,generator=None)\u2192Tensor", "description": "Fills self tensor with elements drawn from the geometric distribution:  f(X=k)=pk\u22121(1\u2212p)f(X=k) = p^{k - 1} (1 - p)f(X=k)=pk\u22121(1\u2212p)  ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.geqrf", "item_type": "method", "code": "geqrf()-&gt;(Tensor,Tensor)", "description": "See torch.geqrf() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.ger", "item_type": "method", "code": "ger(vec2)\u2192Tensor", "description": "See torch.ger() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.get_device", "item_type": "method", "code": "get_device()-&gt;Deviceordinal(Integer)", "description": "For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides. For CPU tensors, an error is thrown. Example: &gt;&gt;&gt; x = torch.randn(3, 4, 5, device='cuda:0') &gt;&gt;&gt; x.get_device() 0 &gt;&gt;&gt; x.cpu().get_device()  # RuntimeError: get_device is not implemented for type torch.FloatTensor   ", "parameters": [], "returns": null, "example": " x = torch.randn(3, 4, 5, device='cuda:0')\n x.get_device()\n0\n x.cpu().get_device()  # RuntimeError: get_device is not implemented for type torch.FloatTensor\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.pairwise_distance", "item_type": "function", "code": "torch.nn.functional.pairwise_distance(x1,x2,p=2.0,eps=1e-06,keepdim=False)", "description": "See torch.nn.PairwiseDistance for details ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.cosine_similarity", "item_type": "function", "code": "torch.nn.functional.cosine_similarity(x1,x2,dim=1,eps=1e-8)\u2192Tensor", "description": "Returns cosine similarity between x1 and x2, computed along dim.  similarity=x1\u22c5x2max\u2061(\u2225x1\u22252\u22c5\u2225x2\u22252,\u03f5)\\text{similarity} = \\dfrac{x_1 \\cdot x_2}{\\max(\\Vert x_1 \\Vert _2 \\cdot \\Vert x_2 \\Vert _2, \\epsilon)}  similarity=max(\u2225x1\u200b\u22252\u200b\u22c5\u2225x2\u200b\u22252\u200b,\u03f5)x1\u200b\u22c5x2\u200b\u200b   Parameters  x1 (Tensor) \u2013 First input. x2 (Tensor) \u2013 Second input (of size matching x1). dim (python:int, optional) \u2013 Dimension of vectors. Default: 1 eps (python:float, optional) \u2013 Small value to avoid division by zero. Default: 1e-8     Shape: Input: (\u22171,D,\u22172)(\\ast_1, D, \\ast_2)(\u22171\u200b,D,\u22172\u200b)   where D is at position dim. Output: (\u22171,\u22172)(\\ast_1, \\ast_2)(\u22171\u200b,\u22172\u200b)   where 1 is at position dim.    Example: &gt;&gt;&gt; input1 = torch.randn(100, 128) &gt;&gt;&gt; input2 = torch.randn(100, 128) &gt;&gt;&gt; output = F.cosine_similarity(input1, input2) &gt;&gt;&gt; print(output)   ", "parameters": ["x1 (Tensor) : First input.", "x2 (Tensor) : Second input (of size matching x1).", "dim (python:int, optional) : Dimension of vectors. Default: 1", "eps (python:float, optional) : Small value to avoid division by zero.Default: 1e-8"], "returns": null, "example": " input1 = torch.randn(100, 128)\n input2 = torch.randn(100, 128)\n output = F.cosine_similarity(input1, input2)\n print(output)\n\n", "shape": " Input: (\u22171,D,\u22172)(\\ast_1, D, \\ast_2)(\u22171\u200b,D,\u22172\u200b)   where D is at position dim. Output: (\u22171,\u22172)(\\ast_1, \\ast_2)(\u22171\u200b,\u22172\u200b)   where 1 is at position dim.  "},
{"library": "torch", "item_id": "torch.nn.functional.pdist", "item_type": "function", "code": "torch.nn.functional.pdist(input,p=2)\u2192Tensor", "description": "Computes the p-norm distance between every pair of row vectors in the input. This is identical to the upper triangular portion, excluding the diagonal, of torch.norm(input[:, None] - input, dim=2, p=p). This function will be faster if the rows are contiguous. If input has shape N\u00d7MN \\times MN\u00d7M   then the output will have shape 12N(N\u22121)\\frac{1}{2} N (N - 1)21\u200bN(N\u22121)  . This function is equivalent to scipy.spatial.distance.pdist(input, \u2018minkowski\u2019, p=p) if p\u2208(0,\u221e)p \\in (0, \\infty)p\u2208(0,\u221e)  . When p=0p = 0p=0   it is equivalent to scipy.spatial.distance.pdist(input, \u2018hamming\u2019) * M. When p=\u221ep = \\inftyp=\u221e  , the closest scipy function is scipy.spatial.distance.pdist(xn, lambda x, y: np.abs(x - y).max()).  Parameters  input \u2013 input tensor of shape N\u00d7MN \\times MN\u00d7M  . p \u2013 p value for the p-norm distance to calculate between each vector pair \u2208[0,\u221e]\\in [0, \\infty]\u2208[0,\u221e]  .    ", "parameters": ["input : input tensor of shape N\u00d7MN \\times MN\u00d7M.", "p : p value for the p-norm distance to calculate between each vector pair\u2208[0,\u221e]\\in [0, \\infty]\u2208[0,\u221e]."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.binary_cross_entropy", "item_type": "function", "code": "torch.nn.functional.binary_cross_entropy(input,target,weight=None,size_average=None,reduce=None,reduction='mean')", "description": "Function that measures the Binary Cross Entropy between the target and the output. See BCELoss for details.  Parameters  input \u2013 Tensor of arbitrary shape target \u2013 Tensor of the same shape as input weight (Tensor, optional) \u2013 a manual rescaling weight if provided it\u2019s repeated to match input tensor shape size_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'    Examples: &gt;&gt;&gt; input = torch.randn((3, 2), requires_grad=True) &gt;&gt;&gt; target = torch.rand((3, 2), requires_grad=False) &gt;&gt;&gt; loss = F.binary_cross_entropy(F.sigmoid(input), target) &gt;&gt;&gt; loss.backward()   ", "parameters": ["input : Tensor of arbitrary shape", "target : Tensor of the same shape as input", "weight (Tensor, optional) : a manual rescaling weightif provided it\u2019s repeated to match input tensor shape", "size_average (bool, optional) : Deprecated (see reduction). By default,the losses are averaged over each loss element in the batch. Note that forsome losses, there multiple elements per sample. If the field size_averageis set to False, the losses are instead summed for each minibatch. Ignoredwhen reduce is False. Default: True", "reduce (bool, optional) : Deprecated (see reduction). By default, thelosses are averaged or summed over observations for each minibatch dependingon size_average. When reduce is False, returns a loss perbatch element instead and ignores size_average. Default: True", "reduction (string, optional) : Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"], "returns": null, "example": " input = torch.randn((3, 2), requires_grad=True)\n target = torch.rand((3, 2), requires_grad=False)\n loss = F.binary_cross_entropy(F.sigmoid(input), target)\n loss.backward()\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.binary_cross_entropy_with_logits", "item_type": "function", "code": "torch.nn.functional.binary_cross_entropy_with_logits(input,target,weight=None,size_average=None,reduce=None,reduction='mean',pos_weight=None)", "description": "Function that measures Binary Cross Entropy between target and output logits. See BCEWithLogitsLoss for details.  Parameters  input \u2013 Tensor of arbitrary shape target \u2013 Tensor of the same shape as input weight (Tensor, optional) \u2013 a manual rescaling weight if provided it\u2019s repeated to match input tensor shape size_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean' pos_weight (Tensor, optional) \u2013 a weight of positive examples. Must be a vector with length equal to the number of classes.    Examples: &gt;&gt;&gt; input = torch.randn(3, requires_grad=True) &gt;&gt;&gt; target = torch.empty(3).random_(2) &gt;&gt;&gt; loss = F.binary_cross_entropy_with_logits(input, target) &gt;&gt;&gt; loss.backward()   ", "parameters": ["input : Tensor of arbitrary shape", "target : Tensor of the same shape as input", "weight (Tensor, optional) : a manual rescaling weightif provided it\u2019s repeated to match input tensor shape", "size_average (bool, optional) : Deprecated (see reduction). By default,the losses are averaged over each loss element in the batch. Note that forsome losses, there multiple elements per sample. If the field size_averageis set to False, the losses are instead summed for each minibatch. Ignoredwhen reduce is False. Default: True", "reduce (bool, optional) : Deprecated (see reduction). By default, thelosses are averaged or summed over observations for each minibatch dependingon size_average. When reduce is False, returns a loss perbatch element instead and ignores size_average. Default: True", "reduction (string, optional) : Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'", "pos_weight (Tensor, optional) : a weight of positive examples.Must be a vector with length equal to the number of classes."], "returns": null, "example": " input = torch.randn(3, requires_grad=True)\n target = torch.empty(3).random_(2)\n loss = F.binary_cross_entropy_with_logits(input, target)\n loss.backward()\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.chunk", "item_type": "function", "code": "torch.chunk(input,chunks,dim=0)\u2192ListofTensors", "description": "Splits a tensor into a specific number of chunks. Last chunk will be smaller if the tensor size along the given dimension dim is not divisible by chunks.  Parameters  input (Tensor) \u2013 the tensor to split chunks (python:int) \u2013 number of chunks to return dim (python:int) \u2013 dimension along which to split the tensor    ", "parameters": ["input (Tensor) : the tensor to split", "chunks (python:int) : number of chunks to return", "dim (python:int) : dimension along which to split the tensor"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.gather", "item_type": "function", "code": "torch.gather(input,dim,index,out=None,sparse_grad=False)\u2192Tensor", "description": "Gathers values along an axis specified by dim. For a 3-D tensor the output is specified by: out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0 out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1 out[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2   If input is an n-dimensional tensor with size (x0,x1...,xi\u22121,xi,xi+1,...,xn\u22121)(x_0, x_1..., x_{i-1}, x_i, x_{i+1}, ..., x_{n-1})(x0\u200b,x1\u200b...,xi\u22121\u200b,xi\u200b,xi+1\u200b,...,xn\u22121\u200b)   and dim = i, then index must be an nnn  -dimensional tensor with size (x0,x1,...,xi\u22121,y,xi+1,...,xn\u22121)(x_0, x_1, ..., x_{i-1}, y, x_{i+1}, ..., x_{n-1})(x0\u200b,x1\u200b,...,xi\u22121\u200b,y,xi+1\u200b,...,xn\u22121\u200b)   where y\u22651y \\geq 1y\u22651   and out will have the same size as index.  Parameters  input (Tensor) \u2013 the source tensor dim (python:int) \u2013 the axis along which to index index (LongTensor) \u2013 the indices of elements to gather out (Tensor, optional) \u2013 the destination tensor sparse_grad (bool,optional) \u2013 If True, gradient w.r.t. input will be a sparse tensor.    Example: &gt;&gt;&gt; t = torch.tensor([[1,2],[3,4]]) &gt;&gt;&gt; torch.gather(t, 1, torch.tensor([[0,0],[1,0]])) tensor([[ 1,  1],         [ 4,  3]])   ", "parameters": ["input (Tensor) : the source tensor", "dim (python:int) : the axis along which to index", "index (LongTensor) : the indices of elements to gather", "out (Tensor, optional) : the destination tensor", "sparse_grad (bool,optional) : If True, gradient w.r.t. input will be a sparse tensor."], "returns": null, "example": " t = torch.tensor([[1,2],[3,4]])\n torch.gather(t, 1, torch.tensor([[0,0],[1,0]]))\ntensor([[ 1,  1],\n        [ 4,  3]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.index_select", "item_type": "function", "code": "torch.index_select(input,dim,index,out=None)\u2192Tensor", "description": "Returns a new tensor which indexes the input tensor along dimension dim using the entries in index which is a LongTensor. The returned tensor has the same number of dimensions as the original tensor (input).  The dimth dimension has the same size as the length of index; other dimensions have the same size as in the original tensor.  Note The returned tensor does not use the same storage as the original tensor.  If out has a different shape than expected, we silently change it to the correct shape, reallocating the underlying storage if necessary.   Parameters  input (Tensor) \u2013 the input tensor. dim (python:int) \u2013 the dimension in which we index index (LongTensor) \u2013 the 1-D tensor containing the indices to index out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; x = torch.randn(3, 4) &gt;&gt;&gt; x tensor([[ 0.1427,  0.0231, -0.5414, -1.0009],         [-0.4664,  0.2647, -0.1228, -1.1068],         [-1.1734, -0.6571,  0.7230, -0.6004]]) &gt;&gt;&gt; indices = torch.tensor([0, 2]) &gt;&gt;&gt; torch.index_select(x, 0, indices) tensor([[ 0.1427,  0.0231, -0.5414, -1.0009],         [-1.1734, -0.6571,  0.7230, -0.6004]]) &gt;&gt;&gt; torch.index_select(x, 1, indices) tensor([[ 0.1427, -0.5414],         [-0.4664, -0.1228],         [-1.1734,  0.7230]])   ", "parameters": ["input (Tensor) : the input tensor.", "dim (python:int) : the dimension in which we index", "index (LongTensor) : the 1-D tensor containing the indices to index", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " x = torch.randn(3, 4)\n x\ntensor([[ 0.1427,  0.0231, -0.5414, -1.0009],\n        [-0.4664,  0.2647, -0.1228, -1.1068],\n        [-1.1734, -0.6571,  0.7230, -0.6004]])\n indices = torch.tensor([0, 2])\n torch.index_select(x, 0, indices)\ntensor([[ 0.1427,  0.0231, -0.5414, -1.0009],\n        [-1.1734, -0.6571,  0.7230, -0.6004]])\n torch.index_select(x, 1, indices)\ntensor([[ 0.1427, -0.5414],\n        [-0.4664, -0.1228],\n        [-1.1734,  0.7230]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.masked_select", "item_type": "function", "code": "torch.masked_select(input,mask,out=None)\u2192Tensor", "description": "Returns a new 1-D tensor which indexes the input tensor according to the boolean mask mask which is a BoolTensor. The shapes of the mask tensor and the input tensor don\u2019t need to match, but they must be broadcastable.  Note The returned tensor does not use the same storage as the original tensor   Parameters  input (Tensor) \u2013 the input tensor. mask (ByteTensor) \u2013 the tensor containing the binary mask to index with out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; x = torch.randn(3, 4) &gt;&gt;&gt; x tensor([[ 0.3552, -2.3825, -0.8297,  0.3477],         [-1.2035,  1.2252,  0.5002,  0.6248],         [ 0.1307, -2.0608,  0.1244,  2.0139]]) &gt;&gt;&gt; mask = x.ge(0.5) &gt;&gt;&gt; mask tensor([[False, False, False, False],         [False, True, True, True],         [False, False, False, True]]) &gt;&gt;&gt; torch.masked_select(x, mask) tensor([ 1.2252,  0.5002,  0.6248,  2.0139])   ", "parameters": ["input (Tensor) : the input tensor.", "mask (ByteTensor) : the tensor containing the binary mask to index with", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " x = torch.randn(3, 4)\n x\ntensor([[ 0.3552, -2.3825, -0.8297,  0.3477],\n        [-1.2035,  1.2252,  0.5002,  0.6248],\n        [ 0.1307, -2.0608,  0.1244,  2.0139]])\n mask = x.ge(0.5)\n mask\ntensor([[False, False, False, False],\n        [False, True, True, True],\n        [False, False, False, True]])\n torch.masked_select(x, mask)\ntensor([ 1.2252,  0.5002,  0.6248,  2.0139])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.mean", "item_type": "method", "code": "propertymean", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.rsample", "item_type": "method", "code": "rsample(sample_shape=torch.Size([]))", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.multinomial.Multinomial.expand", "item_type": "method", "code": "expand(batch_shape,_instance=None)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.multinomial.Multinomial.log_prob", "item_type": "method", "code": "log_prob(value)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.multinomial.Multinomial.logits", "item_type": "method", "code": "propertylogits", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.multinomial.Multinomial.mean", "item_type": "method", "code": "propertymean", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.multinomial.Multinomial.param_shape", "item_type": "method", "code": "propertyparam_shape", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.multinomial.Multinomial.probs", "item_type": "method", "code": "propertyprobs", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.multinomial.Multinomial.sample", "item_type": "method", "code": "sample(sample_shape=torch.Size([]))", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.multinomial.Multinomial.support", "item_type": "method", "code": "propertysupport", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.multinomial.Multinomial.variance", "item_type": "method", "code": "propertyvariance", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.multivariate_normal.MultivariateNormal.entropy", "item_type": "method", "code": "entropy()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.multivariate_normal.MultivariateNormal.expand", "item_type": "method", "code": "expand(batch_shape,_instance=None)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.multivariate_normal.MultivariateNormal.log_prob", "item_type": "method", "code": "log_prob(value)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.Module.register_backward_hook", "item_type": "method", "code": "register_backward_hook(hook)", "description": "Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature: hook(module, grad_input, grad_output) -&gt; Tensor or None   The grad_input and grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of grad_input in subsequent computations.  Returns a handle that can be used to remove the added hook by calling handle.remove()  Return type torch.utils.hooks.RemovableHandle    Warning The current implementation will not have the presented behavior for complex Module that perform many operations. In some failure cases, grad_input and grad_output will only contain the gradients for a subset of the inputs and outputs. For such Module, you should use torch.Tensor.register_hook() directly on a specific input or output to get the required gradients.  ", "parameters": [], "returns": "a handle that can be used to remove the added hook by callinghandle.remove()", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.Module.register_buffer", "item_type": "method", "code": "register_buffer(name,tensor)", "description": "Adds a persistent buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm\u2019s running_mean is not a parameter, but is part of the persistent state. Buffers can be accessed as attributes using given names.  Parameters  name (string) \u2013 name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor) \u2013 buffer to be registered.    Example: &gt;&gt;&gt; self.register_buffer('running_mean', torch.zeros(num_features))   ", "parameters": ["name (string) : name of the buffer. The buffer can be accessedfrom this module using the given name", "tensor (Tensor) : buffer to be registered."], "returns": null, "example": " self.register_buffer('running_mean', torch.zeros(num_features))\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.Module.register_forward_hook", "item_type": "method", "code": "register_forward_hook(hook)", "description": "Registers a forward hook on the module. The hook will be called every time after forward() has computed an output. It should have the following signature: hook(module, input, output) -&gt; None or modified output   The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after forward() is called.  Returns a handle that can be used to remove the added hook by calling handle.remove()  Return type torch.utils.hooks.RemovableHandle   ", "parameters": [], "returns": "a handle that can be used to remove the added hook by callinghandle.remove()", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.Module.register_forward_pre_hook", "item_type": "method", "code": "register_forward_pre_hook(hook)", "description": "Registers a forward pre-hook on the module. The hook will be called every time before forward() is invoked. It should have the following signature: hook(module, input) -&gt; None or modified input   The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple).  Returns a handle that can be used to remove the added hook by calling handle.remove()  Return type torch.utils.hooks.RemovableHandle   ", "parameters": [], "returns": "a handle that can be used to remove the added hook by callinghandle.remove()", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.Module.register_parameter", "item_type": "method", "code": "register_parameter(name,param)", "description": "Adds a parameter to the module. The parameter can be accessed as an attribute using given name.  Parameters  name (string) \u2013 name of the parameter. The parameter can be accessed from this module using the given name param (Parameter) \u2013 parameter to be added to the module.    ", "parameters": ["name (string) : name of the parameter. The parameter can be accessedfrom this module using the given name", "param (Parameter) : parameter to be added to the module."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.Module.requires_grad_", "item_type": "method", "code": "requires_grad_(requires_grad=True)", "description": "Change if autograd should record operations on parameters in this module. This method sets the parameters\u2019 requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).  Parameters requires_grad (bool) \u2013 whether autograd should record operations on parameters in this module. Default: True.  Returns self  Return type Module   ", "parameters": ["requires_grad (bool) : whether autograd should record operations onparameters in this module. Default: True.", "self", "Module"], "returns": "self", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.Module.state_dict", "item_type": "method", "code": "state_dict(destination=None,prefix='',keep_vars=False)", "description": "Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names.  Returns a dictionary containing a whole state of the module  Return type dict   Example: &gt;&gt;&gt; module.state_dict().keys() ['bias', 'weight']   ", "parameters": [], "returns": "a dictionary containing a whole state of the module", "example": " module.state_dict().keys()\n['bias', 'weight']\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.Module.to", "item_type": "method", "code": "to(*args,**kwargs)", "description": "Moves and/or casts the parameters and buffers. This can be called as   to(device=None, dtype=None, non_blocking=False)      to(dtype, non_blocking=False)      to(tensor, non_blocking=False)    Its signature is similar to torch.Tensor.to(), but only accepts floating point desired dtype s. In addition, this method will only cast the floating point parameters and buffers to dtype (if given). The integral parameters and buffers will be moved device, if that is given, but with dtypes unchanged. When non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples.  Note This method modifies the module in-place.   Parameters  device (torch.device) \u2013 the desired device of the parameters and buffers in this module dtype (torch.dtype) \u2013 the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor) \u2013 Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module   Returns self  Return type Module   Example: &gt;&gt;&gt; linear = nn.Linear(2, 2) &gt;&gt;&gt; linear.weight Parameter containing: tensor([[ 0.1913, -0.3420],         [-0.5113, -0.2325]]) &gt;&gt;&gt; linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) &gt;&gt;&gt; linear.weight Parameter containing: tensor([[ 0.1913, -0.3420],         [-0.5113, -0.2325]], dtype=torch.float64) &gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\") &gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) &gt;&gt;&gt; linear.weight Parameter containing: tensor([[ 0.1914, -0.3420],         [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') &gt;&gt;&gt; cpu = torch.device(\"cpu\") &gt;&gt;&gt; linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) &gt;&gt;&gt; linear.weight Parameter containing: tensor([[ 0.1914, -0.3420],         [-0.5112, -0.2324]], dtype=torch.float16)   ", "parameters": ["device (torch.device) : the desired device of the parametersand buffers in this module", "dtype (torch.dtype) : the desired floating point type ofthe floating point parameters and buffers in this module", "tensor (torch.Tensor) : Tensor whose dtype and device are the desireddtype and device for all parameters and buffers in this module"], "returns": "self", "example": " linear = nn.Linear(2, 2)\n linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]])\n linear.to(torch.double)\nLinear(in_features=2, out_features=2, bias=True)\n linear.weight\nParameter containing:\ntensor([[ 0.1913, -0.3420],\n        [-0.5113, -0.2325]], dtype=torch.float64)\n gpu1 = torch.device(\"cuda:1\")\n linear.to(gpu1, dtype=torch.half, non_blocking=True)\nLinear(in_features=2, out_features=2, bias=True)\n linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n cpu = torch.device(\"cpu\")\n linear.to(cpu)\nLinear(in_features=2, out_features=2, bias=True)\n linear.weight\nParameter containing:\ntensor([[ 0.1914, -0.3420],\n        [-0.5112, -0.2324]], dtype=torch.float16)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.gt", "item_type": "method", "code": "gt(other)\u2192Tensor", "description": "See torch.gt() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.gt_", "item_type": "method", "code": "gt_(other)\u2192Tensor", "description": "In-place version of gt() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.half", "item_type": "method", "code": "half()\u2192Tensor", "description": "self.half() is equivalent to self.to(torch.float16). See to(). ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.hardshrink", "item_type": "method", "code": "hardshrink(lambd=0.5)\u2192Tensor", "description": "See torch.nn.functional.hardshrink() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.histc", "item_type": "method", "code": "histc(bins=100,min=0,max=0)\u2192Tensor", "description": "See torch.histc() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.ifft", "item_type": "method", "code": "ifft(signal_ndim,normalized=False)\u2192Tensor", "description": "See torch.ifft() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.imag", "item_type": "method", "code": "imag()\u2192Tensor", "description": "See torch.imag() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.index_add_", "item_type": "method", "code": "index_add_(dim,index,tensor)\u2192Tensor", "description": "Accumulate the elements of tensor into the self tensor by adding to the indices in the order given in index. For example, if dim == 0 and index[i] == j, then the ith row of tensor is added to the jth row of self. The dimth dimension of tensor must have the same size as the length of index (which must be a vector), and all other dimensions must match self, or an error will be raised.  Note When using the CUDA backend, this operation may induce nondeterministic behaviour that is not easily switched off. Please see the notes on Reproducibility for background.   Parameters  dim (python:int) \u2013 dimension along which to index index (LongTensor) \u2013 indices of tensor to select from tensor (Tensor) \u2013 the tensor containing values to add    Example: &gt;&gt;&gt; x = torch.ones(5, 3) &gt;&gt;&gt; t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float) &gt;&gt;&gt; index = torch.tensor([0, 4, 2]) &gt;&gt;&gt; x.index_add_(0, index, t) tensor([[  2.,   3.,   4.],         [  1.,   1.,   1.],         [  8.,   9.,  10.],         [  1.,   1.,   1.],         [  5.,   6.,   7.]])   ", "parameters": ["dim (python:int) : dimension along which to index", "index (LongTensor) : indices of tensor to select from", "tensor (Tensor) : the tensor containing values to add"], "returns": null, "example": " x = torch.ones(5, 3)\n t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n index = torch.tensor([0, 4, 2])\n x.index_add_(0, index, t)\ntensor([[  2.,   3.,   4.],\n        [  1.,   1.,   1.],\n        [  8.,   9.,  10.],\n        [  1.,   1.,   1.],\n        [  5.,   6.,   7.]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.index_add", "item_type": "method", "code": "index_add(dim,index,tensor)\u2192Tensor", "description": "Out-of-place version of torch.Tensor.index_add_() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.poisson_nll_loss", "item_type": "function", "code": "torch.nn.functional.poisson_nll_loss(input,target,log_input=True,full=False,size_average=None,eps=1e-08,reduce=None,reduction='mean')", "description": "Poisson negative log likelihood loss. See PoissonNLLLoss for details.  Parameters  input \u2013 expectation of underlying Poisson distribution. target \u2013 random sample target\u223cPoisson(input)target \\sim \\text{Poisson}(input)target\u223cPoisson(input)  . log_input \u2013 if True the loss is computed as exp\u2061(input)\u2212target\u2217input\\exp(\\text{input}) - \\text{target} * \\text{input}exp(input)\u2212target\u2217input  , if False then loss is input\u2212target\u2217log\u2061(input+eps)\\text{input} - \\text{target} * \\log(\\text{input}+\\text{eps})input\u2212target\u2217log(input+eps)  . Default: True full \u2013 whether to compute full loss, i. e. to add the Stirling approximation term. Default: False target\u2217log\u2061(target)\u2212target+0.5\u2217log\u2061(2\u2217\u03c0\u2217target)\\text{target} * \\log(\\text{target}) - \\text{target} + 0.5 * \\log(2 * \\pi * \\text{target})target\u2217log(target)\u2212target+0.5\u2217log(2\u2217\u03c0\u2217target)  . size_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True eps (python:float, optional) \u2013 Small value to avoid evaluation of log\u2061(0)\\log(0)log(0)   when log_input`=``False`. Default: 1e-8 reduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'    ", "parameters": ["input : expectation of underlying Poisson distribution.", "target : random sample target\u223cPoisson(input)target \\sim \\text{Poisson}(input)target\u223cPoisson(input).", "log_input : if True the loss is computed asexp\u2061(input)\u2212target\u2217input\\exp(\\text{input}) - \\text{target} * \\text{input}exp(input)\u2212target\u2217input, if False then loss isinput\u2212target\u2217log\u2061(input+eps)\\text{input} - \\text{target} * \\log(\\text{input}+\\text{eps})input\u2212target\u2217log(input+eps). Default: True", "full : whether to compute full loss, i. e. to add the Stirlingapproximation term. Default: Falsetarget\u2217log\u2061(target)\u2212target+0.5\u2217log\u2061(2\u2217\u03c0\u2217target)\\text{target} * \\log(\\text{target}) - \\text{target} + 0.5 * \\log(2 * \\pi * \\text{target})target\u2217log(target)\u2212target+0.5\u2217log(2\u2217\u03c0\u2217target).", "size_average (bool, optional) : Deprecated (see reduction). By default,the losses are averaged over each loss element in the batch. Note that forsome losses, there multiple elements per sample. If the field size_averageis set to False, the losses are instead summed for each minibatch. Ignoredwhen reduce is False. Default: True", "eps (python:float, optional) : Small value to avoid evaluation of log\u2061(0)\\log(0)log(0) whenlog_input`=``False`. Default: 1e-8", "reduce (bool, optional) : Deprecated (see reduction). By default, thelosses are averaged or summed over observations for each minibatch dependingon size_average. When reduce is False, returns a loss perbatch element instead and ignores size_average. Default: True", "reduction (string, optional) : Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.cosine_embedding_loss", "item_type": "function", "code": "torch.nn.functional.cosine_embedding_loss(input1,input2,target,margin=0,size_average=None,reduce=None,reduction='mean')\u2192Tensor", "description": "See CosineEmbeddingLoss for details. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.cross_entropy", "item_type": "function", "code": "torch.nn.functional.cross_entropy(input,target,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction='mean')", "description": "This criterion combines log_softmax and nll_loss in a single function. See CrossEntropyLoss for details.  Parameters  input (Tensor) \u2013 (N,C)(N, C)(N,C)   where C = number of classes or (N,C,H,W)(N, C, H, W)(N,C,H,W)   in case of 2D Loss, or (N,C,d1,d2,...,dK)(N, C, d_1, d_2, ..., d_K)(N,C,d1\u200b,d2\u200b,...,dK\u200b)   where K\u22651K \\geq 1K\u22651   in the case of K-dimensional loss. target (Tensor) \u2013 (N)(N)(N)   where each value is 0\u2264targets[i]\u2264C\u221210 \\leq \\text{targets}[i] \\leq C-10\u2264targets[i]\u2264C\u22121  , or (N,d1,d2,...,dK)(N, d_1, d_2, ..., d_K)(N,d1\u200b,d2\u200b,...,dK\u200b)   where K\u22651K \\geq 1K\u22651   for K-dimensional loss. weight (Tensor, optional) \u2013 a manual rescaling weight given to each class. If given, has to be a Tensor of size C size_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True ignore_index (python:int, optional) \u2013 Specifies a target value that is ignored and does not contribute to the input gradient. When size_average is True, the loss is averaged over non-ignored targets. Default: -100 reduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'    Examples: &gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True) &gt;&gt;&gt; target = torch.randint(5, (3,), dtype=torch.int64) &gt;&gt;&gt; loss = F.cross_entropy(input, target) &gt;&gt;&gt; loss.backward()   ", "parameters": ["input (Tensor) : (N,C)(N, C)(N,C) where C = number of classes or (N,C,H,W)(N, C, H, W)(N,C,H,W)in case of 2D Loss, or (N,C,d1,d2,...,dK)(N, C, d_1, d_2, ..., d_K)(N,C,d1\u200b,d2\u200b,...,dK\u200b) where K\u22651K \\geq 1K\u22651in the case of K-dimensional loss.", "target (Tensor) : (N)(N)(N) where each value is 0\u2264targets[i]\u2264C\u221210 \\leq \\text{targets}[i] \\leq C-10\u2264targets[i]\u2264C\u22121,or (N,d1,d2,...,dK)(N, d_1, d_2, ..., d_K)(N,d1\u200b,d2\u200b,...,dK\u200b) where K\u22651K \\geq 1K\u22651 forK-dimensional loss.", "weight (Tensor, optional) : a manual rescaling weight given to eachclass. If given, has to be a Tensor of size C", "size_average (bool, optional) : Deprecated (see reduction). By default,the losses are averaged over each loss element in the batch. Note that forsome losses, there multiple elements per sample. If the field size_averageis set to False, the losses are instead summed for each minibatch. Ignoredwhen reduce is False. Default: True", "ignore_index (python:int, optional) : Specifies a target value that is ignoredand does not contribute to the input gradient. When size_average isTrue, the loss is averaged over non-ignored targets. Default: -100", "reduce (bool, optional) : Deprecated (see reduction). By default, thelosses are averaged or summed over observations for each minibatch dependingon size_average. When reduce is False, returns a loss perbatch element instead and ignores size_average. Default: True", "reduction (string, optional) : Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"], "returns": null, "example": " input = torch.randn(3, 5, requires_grad=True)\n target = torch.randint(5, (3,), dtype=torch.int64)\n loss = F.cross_entropy(input, target)\n loss.backward()\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.narrow", "item_type": "function", "code": "torch.narrow(input,dim,start,length)\u2192Tensor", "description": "Returns a new tensor that is a narrowed version of input tensor. The dimension dim is input from start to start + length. The returned tensor and input tensor share the same underlying storage.  Parameters  input (Tensor) \u2013 the tensor to narrow dim (python:int) \u2013 the dimension along which to narrow start (python:int) \u2013 the starting dimension length (python:int) \u2013 the distance to the ending dimension    Example: &gt;&gt;&gt; x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) &gt;&gt;&gt; torch.narrow(x, 0, 0, 2) tensor([[ 1,  2,  3],         [ 4,  5,  6]]) &gt;&gt;&gt; torch.narrow(x, 1, 1, 2) tensor([[ 2,  3],         [ 5,  6],         [ 8,  9]])   ", "parameters": ["input (Tensor) : the tensor to narrow", "dim (python:int) : the dimension along which to narrow", "start (python:int) : the starting dimension", "length (python:int) : the distance to the ending dimension"], "returns": null, "example": " x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n torch.narrow(x, 0, 0, 2)\ntensor([[ 1,  2,  3],\n        [ 4,  5,  6]])\n torch.narrow(x, 1, 1, 2)\ntensor([[ 2,  3],\n        [ 5,  6],\n        [ 8,  9]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nonzero", "item_type": "function", "code": "torch.nonzero(input,*,out=None,as_tuple=False)\u2192LongTensorortupleofLongTensors", "description": " Note torch.nonzero(..., as_tuple=False) (default) returns a 2-D tensor where each row is the index for a nonzero value. torch.nonzero(..., as_tuple=True) returns a tuple of 1-D index tensors, allowing for advanced indexing, so x[x.nonzero(as_tuple=True)] gives all nonzero values of tensor x. Of the returned tuple, each index tensor contains nonzero indices for a certain dimension. See below for more details on the two behaviors.  When as_tuple is ``False`` (default): Returns a tensor containing the indices of all non-zero elements of input.  Each row in the result contains the indices of a non-zero element in input. The result is sorted lexicographically, with the last index changing the fastest (C-style). If input has nnn   dimensions, then the resulting indices tensor out is of size (z\u00d7n)(z \\times n)(z\u00d7n)  , where zzz   is the total number of non-zero elements in the input tensor. When as_tuple is ``True``: Returns a tuple of 1-D tensors, one for each dimension in input, each containing the indices (in that dimension) of all non-zero elements of input . If input has nnn   dimensions, then the resulting tuple contains nnn   tensors of size zzz  , where zzz   is the total number of non-zero elements in the input tensor. As a special case, when input has zero dimensions and a nonzero scalar value, it is treated as a one-dimensional tensor with one element.  Parameters  input (Tensor) \u2013 the input tensor. out (LongTensor, optional) \u2013 the output tensor containing indices   Returns If as_tuple is False, the output tensor containing indices. If as_tuple is True, one 1-D tensor for each dimension, containing the indices of each nonzero element along that dimension.  Return type LongTensor or tuple of LongTensor   Example: &gt;&gt;&gt; torch.nonzero(torch.tensor([1, 1, 1, 0, 1])) tensor([[ 0],         [ 1],         [ 2],         [ 4]]) &gt;&gt;&gt; torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],                                 [0.0, 0.4, 0.0, 0.0],                                 [0.0, 0.0, 1.2, 0.0],                                 [0.0, 0.0, 0.0,-0.4]])) tensor([[ 0,  0],         [ 1,  1],         [ 2,  2],         [ 3,  3]]) &gt;&gt;&gt; torch.nonzero(torch.tensor([1, 1, 1, 0, 1]), as_tuple=True) (tensor([0, 1, 2, 4]),) &gt;&gt;&gt; torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],                                 [0.0, 0.4, 0.0, 0.0],                                 [0.0, 0.0, 1.2, 0.0],                                 [0.0, 0.0, 0.0,-0.4]]), as_tuple=True) (tensor([0, 1, 2, 3]), tensor([0, 1, 2, 3])) &gt;&gt;&gt; torch.nonzero(torch.tensor(5), as_tuple=True) (tensor([0]),)   ", "parameters": ["input (Tensor) : the input tensor.", "out (LongTensor, optional) : the output tensor containing indices"], "returns": "If as_tuple is False, the outputtensor containing indices. If as_tuple is True, one 1-D tensor foreach dimension, containing the indices of each nonzero element along thatdimension.", "example": " torch.nonzero(torch.tensor([1, 1, 1, 0, 1]))\ntensor([[ 0],\n        [ 1],\n        [ 2],\n        [ 4]])\n torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],\n                                [0.0, 0.4, 0.0, 0.0],\n                                [0.0, 0.0, 1.2, 0.0],\n                                [0.0, 0.0, 0.0,-0.4]]))\ntensor([[ 0,  0],\n        [ 1,  1],\n        [ 2,  2],\n        [ 3,  3]])\n torch.nonzero(torch.tensor([1, 1, 1, 0, 1]), as_tuple=True)\n(tensor([0, 1, 2, 4]),)\n torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],\n                                [0.0, 0.4, 0.0, 0.0],\n                                [0.0, 0.0, 1.2, 0.0],\n                                [0.0, 0.0, 0.0,-0.4]]), as_tuple=True)\n(tensor([0, 1, 2, 3]), tensor([0, 1, 2, 3]))\n torch.nonzero(torch.tensor(5), as_tuple=True)\n(tensor([0]),)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.reshape", "item_type": "function", "code": "torch.reshape(input,shape)\u2192Tensor", "description": "Returns a tensor with the same data and number of elements as input, but with the specified shape. When possible, the returned tensor will be a view of input. Otherwise, it will be a copy. Contiguous inputs and inputs with compatible strides can be reshaped without copying, but you should not depend on the copying vs. viewing behavior. See torch.Tensor.view() on when it is possible to return a view. A single dimension may be -1, in which case it\u2019s inferred from the remaining dimensions and the number of elements in input.  Parameters  input (Tensor) \u2013 the tensor to be reshaped shape (tuple of python:ints) \u2013 the new shape    Example: &gt;&gt;&gt; a = torch.arange(4.) &gt;&gt;&gt; torch.reshape(a, (2, 2)) tensor([[ 0.,  1.],         [ 2.,  3.]]) &gt;&gt;&gt; b = torch.tensor([[0, 1], [2, 3]]) &gt;&gt;&gt; torch.reshape(b, (-1,)) tensor([ 0,  1,  2,  3])   ", "parameters": ["input (Tensor) : the tensor to be reshaped", "shape (tuple of python:ints) : the new shape"], "returns": null, "example": " a = torch.arange(4.)\n torch.reshape(a, (2, 2))\ntensor([[ 0.,  1.],\n        [ 2.,  3.]])\n b = torch.tensor([[0, 1], [2, 3]])\n torch.reshape(b, (-1,))\ntensor([ 0,  1,  2,  3])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.split", "item_type": "function", "code": "torch.split(tensor,split_size_or_sections,dim=0)", "description": "Splits the tensor into chunks. If split_size_or_sections is an integer type, then tensor will be split into equally sized chunks (if possible). Last chunk will be smaller if the tensor size along the given dimension dim is not divisible by split_size. If split_size_or_sections is a list, then tensor will be split into len(split_size_or_sections) chunks with sizes in dim according to split_size_or_sections.  Parameters  tensor (Tensor) \u2013 tensor to split. split_size_or_sections (python:int) or (list(python:int)) \u2013 size of a single chunk or list of sizes for each chunk dim (python:int) \u2013 dimension along which to split the tensor.    ", "parameters": ["tensor (Tensor) : tensor to split.", "split_size_or_sections (python:int) or (list(python:int)) : size of a single chunk orlist of sizes for each chunk", "dim (python:int) : dimension along which to split the tensor."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.multivariate_normal.MultivariateNormal.mean", "item_type": "method", "code": "propertymean", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.multivariate_normal.MultivariateNormal.rsample", "item_type": "method", "code": "rsample(sample_shape=torch.Size([]))", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.multivariate_normal.MultivariateNormal.variance", "item_type": "method", "code": "propertyvariance", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.negative_binomial.NegativeBinomial.expand", "item_type": "method", "code": "expand(batch_shape,_instance=None)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.negative_binomial.NegativeBinomial.log_prob", "item_type": "method", "code": "log_prob(value)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.negative_binomial.NegativeBinomial.mean", "item_type": "method", "code": "propertymean", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.negative_binomial.NegativeBinomial.param_shape", "item_type": "method", "code": "propertyparam_shape", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.negative_binomial.NegativeBinomial.sample", "item_type": "method", "code": "sample(sample_shape=torch.Size([]))", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.negative_binomial.NegativeBinomial.variance", "item_type": "method", "code": "propertyvariance", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.normal.Normal.cdf", "item_type": "method", "code": "cdf(value)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.Module.train", "item_type": "method", "code": "train(mode=True)", "description": "Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc.  Parameters mode (bool) \u2013 whether to set training mode (True) or evaluation mode (False). Default: True.  Returns self  Return type Module   ", "parameters": ["mode (bool) : whether to set training mode (True) or evaluationmode (False). Default: True.", "self", "Module"], "returns": "self", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.Module.type", "item_type": "method", "code": "type(dst_type)", "description": "Casts all parameters and buffers to dst_type.  Parameters dst_type (python:type or string) \u2013 the desired type  Returns self  Return type Module   ", "parameters": ["dst_type (python:type or string) : the desired type", "self", "Module"], "returns": "self", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.Module.zero_grad", "item_type": "method", "code": "zero_grad()", "description": "Sets gradients of all model parameters to zero. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.ModuleList.append", "item_type": "method", "code": "append(module)", "description": "Appends a given module to the end of the list.  Parameters module (nn.Module) \u2013 module to append   ", "parameters": ["module (nn.Module) : module to append"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.ModuleList.extend", "item_type": "method", "code": "extend(modules)", "description": "Appends modules from a Python iterable to the end of the list.  Parameters modules (iterable) \u2013 iterable of modules to append   ", "parameters": ["modules (iterable) : iterable of modules to append"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.ModuleList.insert", "item_type": "method", "code": "insert(index,module)", "description": "Insert a given module before a given index in the list.  Parameters  index (python:int) \u2013 index to insert. module (nn.Module) \u2013 module to insert    ", "parameters": ["index (python:int) : index to insert.", "module (nn.Module) : module to insert"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.ModuleDict.clear", "item_type": "method", "code": "clear()", "description": "Remove all items from the ModuleDict. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.ModuleDict.items", "item_type": "method", "code": "items()", "description": "Return an iterable of the ModuleDict key/value pairs. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.ModuleDict.keys", "item_type": "method", "code": "keys()", "description": "Return an iterable of the ModuleDict keys. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.ModuleDict.pop", "item_type": "method", "code": "pop(key)", "description": "Remove key from the ModuleDict and return its module.  Parameters key (string) \u2013 key to pop from the ModuleDict   ", "parameters": ["key (string) : key to pop from the ModuleDict"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.index_copy_", "item_type": "method", "code": "index_copy_(dim,index,tensor)\u2192Tensor", "description": "Copies the elements of tensor into the self tensor by selecting the indices in the order given in index. For example, if dim == 0 and index[i] == j, then the ith row of tensor is copied to the jth row of self. The dimth dimension of tensor must have the same size as the length of index (which must be a vector), and all other dimensions must match self, or an error will be raised.  Parameters  dim (python:int) \u2013 dimension along which to index index (LongTensor) \u2013 indices of tensor to select from tensor (Tensor) \u2013 the tensor containing values to copy    Example: &gt;&gt;&gt; x = torch.zeros(5, 3) &gt;&gt;&gt; t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float) &gt;&gt;&gt; index = torch.tensor([0, 4, 2]) &gt;&gt;&gt; x.index_copy_(0, index, t) tensor([[ 1.,  2.,  3.],         [ 0.,  0.,  0.],         [ 7.,  8.,  9.],         [ 0.,  0.,  0.],         [ 4.,  5.,  6.]])   ", "parameters": ["dim (python:int) : dimension along which to index", "index (LongTensor) : indices of tensor to select from", "tensor (Tensor) : the tensor containing values to copy"], "returns": null, "example": " x = torch.zeros(5, 3)\n t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n index = torch.tensor([0, 4, 2])\n x.index_copy_(0, index, t)\ntensor([[ 1.,  2.,  3.],\n        [ 0.,  0.,  0.],\n        [ 7.,  8.,  9.],\n        [ 0.,  0.,  0.],\n        [ 4.,  5.,  6.]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.index_copy", "item_type": "method", "code": "index_copy(dim,index,tensor)\u2192Tensor", "description": "Out-of-place version of torch.Tensor.index_copy_() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.index_fill_", "item_type": "method", "code": "index_fill_(dim,index,val)\u2192Tensor", "description": "Fills the elements of the self tensor with value val by selecting the indices in the order given in index.  Parameters  dim (python:int) \u2013 dimension along which to index index (LongTensor) \u2013 indices of self tensor to fill in val (python:float) \u2013 the value to fill with     Example::&gt;&gt;&gt; x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float) &gt;&gt;&gt; index = torch.tensor([0, 2]) &gt;&gt;&gt; x.index_fill_(1, index, -1) tensor([[-1.,  2., -1.],         [-1.,  5., -1.],         [-1.,  8., -1.]])     ", "parameters": ["dim (python:int) : dimension along which to index", "index (LongTensor) : indices of self tensor to fill in", "val (python:float) : the value to fill with"], "returns": null, "example": " x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n index = torch.tensor([0, 2])\n x.index_fill_(1, index, -1)\ntensor([[-1.,  2., -1.],\n        [-1.,  5., -1.],\n        [-1.,  8., -1.]])\n\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.index_fill", "item_type": "method", "code": "index_fill(dim,index,value)\u2192Tensor", "description": "Out-of-place version of torch.Tensor.index_fill_() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.index_put_", "item_type": "method", "code": "index_put_(indices,value,accumulate=False)\u2192Tensor", "description": "Puts values from the tensor value into the tensor self using the indices specified in indices (which is a tuple of Tensors). The expression tensor.index_put_(indices, value) is equivalent to tensor[indices] = value. Returns self. If accumulate is True, the elements in tensor are added to self. If accumulate is False, the behavior is undefined if indices contain duplicate elements.  Parameters  indices (tuple of LongTensor) \u2013 tensors used to index into self. value (Tensor) \u2013 tensor of same dtype as self. accumulate (bool) \u2013 whether to accumulate into self    ", "parameters": ["indices (tuple of LongTensor) : tensors used to index into self.", "value (Tensor) : tensor of same dtype as self.", "accumulate (bool) : whether to accumulate into self"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.ctc_loss", "item_type": "function", "code": "torch.nn.functional.ctc_loss(log_probs,targets,input_lengths,target_lengths,blank=0,reduction='mean',zero_infinity=False)", "description": "The Connectionist Temporal Classification loss. See CTCLoss for details.  Note In some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. Please see the notes on Reproducibility for background.   Note When using the CUDA backend, this operation may induce nondeterministic behaviour in its backward pass that is not easily switched off. Please see the notes on Reproducibility for background.   Parameters  log_probs \u2013 (T,N,C)(T, N, C)(T,N,C)   where C = number of characters in alphabet including blank, T = input length, and N = batch size. The logarithmized probabilities of the outputs (e.g. obtained with torch.nn.functional.log_softmax()). targets \u2013 (N,S)(N, S)(N,S)   or (sum(target_lengths)). Targets cannot be blank. In the second form, the targets are assumed to be concatenated. input_lengths \u2013 (N)(N)(N)  . Lengths of the inputs (must each be \u2264T\\leq T\u2264T  ) target_lengths \u2013 (N)(N)(N)  . Lengths of the targets blank (python:int, optional) \u2013 Blank label. Default 000  . reduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the output losses will be divided by the target lengths and then the mean over the batch is taken, 'sum': the output will be summed. Default: 'mean' zero_infinity (bool, optional) \u2013 Whether to zero infinite losses and the associated gradients. Default: False Infinite losses mainly occur when the inputs are too short to be aligned to the targets.    Example: &gt;&gt;&gt; log_probs = torch.randn(50, 16, 20).log_softmax(2).detach().requires_grad_() &gt;&gt;&gt; targets = torch.randint(1, 20, (16, 30), dtype=torch.long) &gt;&gt;&gt; input_lengths = torch.full((16,), 50, dtype=torch.long) &gt;&gt;&gt; target_lengths = torch.randint(10,30,(16,), dtype=torch.long) &gt;&gt;&gt; loss = F.ctc_loss(log_probs, targets, input_lengths, target_lengths) &gt;&gt;&gt; loss.backward()   ", "parameters": ["log_probs : (T,N,C)(T, N, C)(T,N,C) where C = number of characters in alphabet including blank,T = input length, and N = batch size.The logarithmized probabilities of the outputs(e.g. obtained with torch.nn.functional.log_softmax()).", "targets : (N,S)(N, S)(N,S) or (sum(target_lengths)).Targets cannot be blank. In the second form, the targets are assumed to be concatenated.", "input_lengths : (N)(N)(N).Lengths of the inputs (must each be \u2264T\\leq T\u2264T)", "target_lengths : (N)(N)(N).Lengths of the targets", "blank (python:int, optional) : Blank label. Default 000.", "reduction (string, optional) : Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the output losses will be divided by the target lengths andthen the mean over the batch is taken, 'sum': the output will besummed. Default: 'mean'", "zero_infinity (bool, optional) : Whether to zero infinite losses and the associated gradients.Default: FalseInfinite losses mainly occur when the inputs are too shortto be aligned to the targets."], "returns": null, "example": " log_probs = torch.randn(50, 16, 20).log_softmax(2).detach().requires_grad_()\n targets = torch.randint(1, 20, (16, 30), dtype=torch.long)\n input_lengths = torch.full((16,), 50, dtype=torch.long)\n target_lengths = torch.randint(10,30,(16,), dtype=torch.long)\n loss = F.ctc_loss(log_probs, targets, input_lengths, target_lengths)\n loss.backward()\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.hinge_embedding_loss", "item_type": "function", "code": "torch.nn.functional.hinge_embedding_loss(input,target,margin=1.0,size_average=None,reduce=None,reduction='mean')\u2192Tensor", "description": "See HingeEmbeddingLoss for details. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.kl_div", "item_type": "function", "code": "torch.nn.functional.kl_div(input,target,size_average=None,reduce=None,reduction='mean')", "description": "The Kullback-Leibler divergence Loss. See KLDivLoss for details.  Parameters  input \u2013 Tensor of arbitrary shape target \u2013 Tensor of the same shape as input size_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'batchmean' | 'sum' | 'mean'. 'none': no reduction will be applied 'batchmean': the sum of the output will be divided by the batchsize 'sum': the output will be summed 'mean': the output will be divided by the number of elements in the output Default: 'mean'     Note size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction.   Note :attr:reduction = 'mean' doesn\u2019t return the true kl divergence value, please use :attr:reduction = 'batchmean' which aligns with KL math definition. In the next major release, 'mean' will be changed to be the same as \u2018batchmean\u2019.  ", "parameters": ["input : Tensor of arbitrary shape", "target : Tensor of the same shape as input", "size_average (bool, optional) : Deprecated (see reduction). By default,the losses are averaged over each loss element in the batch. Note that forsome losses, there multiple elements per sample. If the field size_averageis set to False, the losses are instead summed for each minibatch. Ignoredwhen reduce is False. Default: True", "reduce (bool, optional) : Deprecated (see reduction). By default, thelosses are averaged or summed over observations for each minibatch dependingon size_average. When reduce is False, returns a loss perbatch element instead and ignores size_average. Default: True", "reduction (string, optional) : Specifies the reduction to apply to the output:'none' | 'batchmean' | 'sum' | 'mean'.'none': no reduction will be applied'batchmean': the sum of the output will be divided by the batchsize'sum': the output will be summed'mean': the output will be divided by the number of elements in the outputDefault: 'mean'"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.squeeze", "item_type": "function", "code": "torch.squeeze(input,dim=None,out=None)\u2192Tensor", "description": "Returns a tensor with all the dimensions of input of size 1 removed. For example, if input is of shape: (A\u00d71\u00d7B\u00d7C\u00d71\u00d7D)(A \\times 1 \\times B \\times C \\times 1 \\times D)(A\u00d71\u00d7B\u00d7C\u00d71\u00d7D)   then the out tensor will be of shape: (A\u00d7B\u00d7C\u00d7D)(A \\times B \\times C \\times D)(A\u00d7B\u00d7C\u00d7D)  . When dim is given, a squeeze operation is done only in the given dimension. If input is of shape: (A\u00d71\u00d7B)(A \\times 1 \\times B)(A\u00d71\u00d7B)  , squeeze(input, 0) leaves the tensor unchanged, but squeeze(input, 1) will squeeze the tensor to the shape (A\u00d7B)(A \\times B)(A\u00d7B)  .  Note The returned tensor shares the storage with the input tensor, so changing the contents of one will change the contents of the other.   Parameters  input (Tensor) \u2013 the input tensor. dim (python:int, optional) \u2013 if given, the input will be squeezed only in this dimension out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; x = torch.zeros(2, 1, 2, 1, 2) &gt;&gt;&gt; x.size() torch.Size([2, 1, 2, 1, 2]) &gt;&gt;&gt; y = torch.squeeze(x) &gt;&gt;&gt; y.size() torch.Size([2, 2, 2]) &gt;&gt;&gt; y = torch.squeeze(x, 0) &gt;&gt;&gt; y.size() torch.Size([2, 1, 2, 1, 2]) &gt;&gt;&gt; y = torch.squeeze(x, 1) &gt;&gt;&gt; y.size() torch.Size([2, 2, 1, 2])   ", "parameters": ["input (Tensor) : the input tensor.", "dim (python:int, optional) : if given, the input will be squeezed only inthis dimension", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " x = torch.zeros(2, 1, 2, 1, 2)\n x.size()\ntorch.Size([2, 1, 2, 1, 2])\n y = torch.squeeze(x)\n y.size()\ntorch.Size([2, 2, 2])\n y = torch.squeeze(x, 0)\n y.size()\ntorch.Size([2, 1, 2, 1, 2])\n y = torch.squeeze(x, 1)\n y.size()\ntorch.Size([2, 2, 1, 2])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.stack", "item_type": "function", "code": "torch.stack(tensors,dim=0,out=None)\u2192Tensor", "description": "Concatenates sequence of tensors along a new dimension. All tensors need to be of the same size.  Parameters  tensors (sequence of Tensors) \u2013 sequence of tensors to concatenate dim (python:int) \u2013 dimension to insert. Has to be between 0 and the number of dimensions of concatenated tensors (inclusive) out (Tensor, optional) \u2013 the output tensor.    ", "parameters": ["tensors (sequence of Tensors) : sequence of tensors to concatenate", "dim (python:int) : dimension to insert. Has to be between 0 and the numberof dimensions of concatenated tensors (inclusive)", "out (Tensor, optional) : the output tensor."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.t", "item_type": "function", "code": "torch.t(input)\u2192Tensor", "description": "Expects input to be &lt;= 2-D tensor and transposes dimensions 0 and 1. 0-D and 1-D tensors are returned as it is and 2-D tensor can be seen as a short-hand function for transpose(input, 0, 1).  Parameters input (Tensor) \u2013 the input tensor.   Example: &gt;&gt;&gt; x = torch.randn(()) &gt;&gt;&gt; x tensor(0.1995) &gt;&gt;&gt; torch.t(x) tensor(0.1995) &gt;&gt;&gt; x = torch.randn(3) &gt;&gt;&gt; x tensor([ 2.4320, -0.4608,  0.7702]) &gt;&gt;&gt; torch.t(x) tensor([.2.4320,.-0.4608,..0.7702]) &gt;&gt;&gt; x = torch.randn(2, 3) &gt;&gt;&gt; x tensor([[ 0.4875,  0.9158, -0.5872],         [ 0.3938, -0.6929,  0.6932]]) &gt;&gt;&gt; torch.t(x) tensor([[ 0.4875,  0.3938],         [ 0.9158, -0.6929],         [-0.5872,  0.6932]])   ", "parameters": ["input (Tensor) : the input tensor."], "returns": null, "example": " x = torch.randn(())\n x\ntensor(0.1995)\n torch.t(x)\ntensor(0.1995)\n x = torch.randn(3)\n x\ntensor([ 2.4320, -0.4608,  0.7702])\n torch.t(x)\ntensor([.2.4320,.-0.4608,..0.7702])\n x = torch.randn(2, 3)\n x\ntensor([[ 0.4875,  0.9158, -0.5872],\n        [ 0.3938, -0.6929,  0.6932]])\n torch.t(x)\ntensor([[ 0.4875,  0.3938],\n        [ 0.9158, -0.6929],\n        [-0.5872,  0.6932]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.take", "item_type": "function", "code": "torch.take(input,index)\u2192Tensor", "description": "Returns a new tensor with the elements of input at the given indices. The input tensor is treated as if it were viewed as a 1-D tensor. The result takes the same shape as the indices.  Parameters  input (Tensor) \u2013 the input tensor. indices (LongTensor) \u2013 the indices into tensor    Example: &gt;&gt;&gt; src = torch.tensor([[4, 3, 5],                         [6, 7, 8]]) &gt;&gt;&gt; torch.take(src, torch.tensor([0, 2, 5])) tensor([ 4,  5,  8])   ", "parameters": ["input (Tensor) : the input tensor.", "indices (LongTensor) : the indices into tensor"], "returns": null, "example": " src = torch.tensor([[4, 3, 5],\n                        [6, 7, 8]])\n torch.take(src, torch.tensor([0, 2, 5]))\ntensor([ 4,  5,  8])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.transpose", "item_type": "function", "code": "torch.transpose(input,dim0,dim1)\u2192Tensor", "description": "Returns a tensor that is a transposed version of input. The given dimensions dim0 and dim1 are swapped. The resulting out tensor shares it\u2019s underlying storage with the input tensor, so changing the content of one would change the content of the other.  Parameters  input (Tensor) \u2013 the input tensor. dim0 (python:int) \u2013 the first dimension to be transposed dim1 (python:int) \u2013 the second dimension to be transposed    Example: &gt;&gt;&gt; x = torch.randn(2, 3) &gt;&gt;&gt; x tensor([[ 1.0028, -0.9893,  0.5809],         [-0.1669,  0.7299,  0.4942]]) &gt;&gt;&gt; torch.transpose(x, 0, 1) tensor([[ 1.0028, -0.1669],         [-0.9893,  0.7299],         [ 0.5809,  0.4942]])   ", "parameters": ["input (Tensor) : the input tensor.", "dim0 (python:int) : the first dimension to be transposed", "dim1 (python:int) : the second dimension to be transposed"], "returns": null, "example": " x = torch.randn(2, 3)\n x\ntensor([[ 1.0028, -0.9893,  0.5809],\n        [-0.1669,  0.7299,  0.4942]])\n torch.transpose(x, 0, 1)\ntensor([[ 1.0028, -0.1669],\n        [-0.9893,  0.7299],\n        [ 0.5809,  0.4942]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.unbind", "item_type": "function", "code": "torch.unbind(input,dim=0)\u2192seq", "description": "Removes a tensor dimension. Returns a tuple of all slices along a given dimension, already without it.  Parameters  input (Tensor) \u2013 the tensor to unbind dim (python:int) \u2013 dimension to remove    Example: &gt;&gt;&gt; torch.unbind(torch.tensor([[1, 2, 3], &gt;&gt;&gt;                            [4, 5, 6], &gt;&gt;&gt;                            [7, 8, 9]])) (tensor([1, 2, 3]), tensor([4, 5, 6]), tensor([7, 8, 9]))   ", "parameters": ["input (Tensor) : the tensor to unbind", "dim (python:int) : dimension to remove"], "returns": null, "example": " torch.unbind(torch.tensor([[1, 2, 3],\n                            [4, 5, 6],\n                            [7, 8, 9]]))\n(tensor([1, 2, 3]), tensor([4, 5, 6]), tensor([7, 8, 9]))\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.normal.Normal.entropy", "item_type": "method", "code": "entropy()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.normal.Normal.expand", "item_type": "method", "code": "expand(batch_shape,_instance=None)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.normal.Normal.icdf", "item_type": "method", "code": "icdf(value)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.normal.Normal.log_prob", "item_type": "method", "code": "log_prob(value)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.normal.Normal.mean", "item_type": "method", "code": "propertymean", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.normal.Normal.rsample", "item_type": "method", "code": "rsample(sample_shape=torch.Size([]))", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.normal.Normal.sample", "item_type": "method", "code": "sample(sample_shape=torch.Size([]))", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.normal.Normal.stddev", "item_type": "method", "code": "propertystddev", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.normal.Normal.variance", "item_type": "method", "code": "propertyvariance", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.one_hot_categorical.OneHotCategorical.entropy", "item_type": "method", "code": "entropy()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.one_hot_categorical.OneHotCategorical.enumerate_support", "item_type": "method", "code": "enumerate_support(expand=True)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.one_hot_categorical.OneHotCategorical.expand", "item_type": "method", "code": "expand(batch_shape,_instance=None)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.ModuleDict.update", "item_type": "method", "code": "update(modules)", "description": "Update the ModuleDict with the key-value pairs from a mapping or an iterable, overwriting existing keys.  Note If modules is an OrderedDict, a ModuleDict, or an iterable of key-value pairs, the order of new elements in it is preserved.   Parameters modules (iterable) \u2013 a mapping (dictionary) from string to Module, or an iterable of key-value pairs of type (string, Module)   ", "parameters": ["modules (iterable) : a mapping (dictionary) from string to Module,or an iterable of key-value pairs of type (string, Module)"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.ModuleDict.values", "item_type": "method", "code": "values()", "description": "Return an iterable of the ModuleDict values. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.ParameterList.append", "item_type": "method", "code": "append(parameter)", "description": "Appends a given parameter at the end of the list.  Parameters parameter (nn.Parameter) \u2013 parameter to append   ", "parameters": ["parameter (nn.Parameter) : parameter to append"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.ParameterList.extend", "item_type": "method", "code": "extend(parameters)", "description": "Appends parameters from a Python iterable to the end of the list.  Parameters parameters (iterable) \u2013 iterable of parameters to append   ", "parameters": ["parameters (iterable) : iterable of parameters to append"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.ParameterDict.clear", "item_type": "method", "code": "clear()", "description": "Remove all items from the ParameterDict. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.ParameterDict.items", "item_type": "method", "code": "items()", "description": "Return an iterable of the ParameterDict key/value pairs. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.ParameterDict.keys", "item_type": "method", "code": "keys()", "description": "Return an iterable of the ParameterDict keys. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.ParameterDict.pop", "item_type": "method", "code": "pop(key)", "description": "Remove key from the ParameterDict and return its parameter.  Parameters key (string) \u2013 key to pop from the ParameterDict   ", "parameters": ["key (string) : key to pop from the ParameterDict"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.ParameterDict.update", "item_type": "method", "code": "update(parameters)", "description": "Update the ParameterDict with the key-value pairs from a mapping or an iterable, overwriting existing keys.  Note If parameters is an OrderedDict, a ParameterDict, or an iterable of key-value pairs, the order of new elements in it is preserved.   Parameters parameters (iterable) \u2013 a mapping (dictionary) from string to Parameter, or an iterable of key-value pairs of type (string, Parameter)   ", "parameters": ["parameters (iterable) : a mapping (dictionary) from string toParameter, or an iterable ofkey-value pairs of type (string, Parameter)"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.index_put", "item_type": "method", "code": "index_put(indices,value,accumulate=False)\u2192Tensor", "description": "Out-place version of index_put_() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.index_select", "item_type": "method", "code": "index_select(dim,index)\u2192Tensor", "description": "See torch.index_select() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.indices", "item_type": "method", "code": "indices()\u2192Tensor", "description": "If self is a sparse COO tensor (i.e., with torch.sparse_coo layout), this returns a view of the contained indices tensor. Otherwise, this throws an error. See also Tensor.values().  Note This method can only be called on a coalesced sparse tensor. See Tensor.coalesce() for details.  ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.int", "item_type": "method", "code": "int()\u2192Tensor", "description": "self.int() is equivalent to self.to(torch.int32). See to(). ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.int_repr", "item_type": "method", "code": "int_repr()\u2192Tensor", "description": "Given a quantized Tensor, self.int_repr() returns a CPU Tensor with uint8_t as data type that stores the underlying uint8_t values of the given Tensor. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.inverse", "item_type": "method", "code": "inverse()\u2192Tensor", "description": "See torch.inverse() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.irfft", "item_type": "method", "code": "irfft(signal_ndim,normalized=False,onesided=True,signal_sizes=None)\u2192Tensor", "description": "See torch.irfft() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.is_contiguous", "item_type": "method", "code": "is_contiguous()\u2192bool", "description": "Returns True if self tensor is contiguous in memory in C order. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.is_floating_point", "item_type": "method", "code": "is_floating_point()\u2192bool", "description": "Returns True if the data type of self is a floating point data type. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.is_pinned", "item_type": "method", "code": "is_pinned()", "description": "Returns true if this tensor resides in pinned memory. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.is_set_to", "item_type": "method", "code": "is_set_to(tensor)\u2192bool", "description": "Returns True if this object refers to the same THTensor object from the Torch C API as the given tensor. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.is_shared", "item_type": "method", "code": "is_shared()", "description": "Checks if tensor is in shared memory. This is always True for CUDA tensors. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.l1_loss", "item_type": "function", "code": "torch.nn.functional.l1_loss(input,target,size_average=None,reduce=None,reduction='mean')\u2192Tensor", "description": "Function that takes the mean element-wise absolute value difference. See L1Loss for details. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.mse_loss", "item_type": "function", "code": "torch.nn.functional.mse_loss(input,target,size_average=None,reduce=None,reduction='mean')\u2192Tensor", "description": "Measures the element-wise mean squared error. See MSELoss for details. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.margin_ranking_loss", "item_type": "function", "code": "torch.nn.functional.margin_ranking_loss(input1,input2,target,margin=0,size_average=None,reduce=None,reduction='mean')\u2192Tensor", "description": "See MarginRankingLoss for details. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.multilabel_margin_loss", "item_type": "function", "code": "torch.nn.functional.multilabel_margin_loss(input,target,size_average=None,reduce=None,reduction='mean')\u2192Tensor", "description": "See MultiLabelMarginLoss for details. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.multilabel_soft_margin_loss", "item_type": "function", "code": "torch.nn.functional.multilabel_soft_margin_loss(input,target,weight=None,size_average=None)\u2192Tensor", "description": "See MultiLabelSoftMarginLoss for details. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.multi_margin_loss", "item_type": "function", "code": "torch.nn.functional.multi_margin_loss(input,target,p=1,margin=1.0,weight=None,size_average=None,reduce=None,reduction='mean')", "description": " multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,reduce=None, reduction=\u2019mean\u2019) -&gt; Tensor   See MultiMarginLoss for details. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.nll_loss", "item_type": "function", "code": "torch.nn.functional.nll_loss(input,target,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction='mean')", "description": "The negative log likelihood loss. See NLLLoss for details.  Parameters  input \u2013 (N,C)(N, C)(N,C)   where C = number of classes or (N,C,H,W)(N, C, H, W)(N,C,H,W)   in case of 2D Loss, or (N,C,d1,d2,...,dK)(N, C, d_1, d_2, ..., d_K)(N,C,d1\u200b,d2\u200b,...,dK\u200b)   where K\u22651K \\geq 1K\u22651   in the case of K-dimensional loss. target \u2013 (N)(N)(N)   where each value is 0\u2264targets[i]\u2264C\u221210 \\leq \\text{targets}[i] \\leq C-10\u2264targets[i]\u2264C\u22121  , or (N,d1,d2,...,dK)(N, d_1, d_2, ..., d_K)(N,d1\u200b,d2\u200b,...,dK\u200b)   where K\u22651K \\geq 1K\u22651   for K-dimensional loss. weight (Tensor, optional) \u2013 a manual rescaling weight given to each class. If given, has to be a Tensor of size C size_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True ignore_index (python:int, optional) \u2013 Specifies a target value that is ignored and does not contribute to the input gradient. When size_average is True, the loss is averaged over non-ignored targets. Default: -100 reduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'    Example: &gt;&gt;&gt; # input is of size N x C = 3 x 5 &gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True) &gt;&gt;&gt; # each element in target has to have 0 &lt;= value &lt; C &gt;&gt;&gt; target = torch.tensor([1, 0, 4]) &gt;&gt;&gt; output = F.nll_loss(F.log_softmax(input), target) &gt;&gt;&gt; output.backward()   ", "parameters": ["input : (N,C)(N, C)(N,C) where C = number of classes or (N,C,H,W)(N, C, H, W)(N,C,H,W)in case of 2D Loss, or (N,C,d1,d2,...,dK)(N, C, d_1, d_2, ..., d_K)(N,C,d1\u200b,d2\u200b,...,dK\u200b) where K\u22651K \\geq 1K\u22651in the case of K-dimensional loss.", "target : (N)(N)(N) where each value is 0\u2264targets[i]\u2264C\u221210 \\leq \\text{targets}[i] \\leq C-10\u2264targets[i]\u2264C\u22121,or (N,d1,d2,...,dK)(N, d_1, d_2, ..., d_K)(N,d1\u200b,d2\u200b,...,dK\u200b) where K\u22651K \\geq 1K\u22651 forK-dimensional loss.", "weight (Tensor, optional) : a manual rescaling weight given to eachclass. If given, has to be a Tensor of size C", "size_average (bool, optional) : Deprecated (see reduction). By default,the losses are averaged over each loss element in the batch. Note that forsome losses, there multiple elements per sample. If the field size_averageis set to False, the losses are instead summed for each minibatch. Ignoredwhen reduce is False. Default: True", "ignore_index (python:int, optional) : Specifies a target value that is ignoredand does not contribute to the input gradient. When size_average isTrue, the loss is averaged over non-ignored targets. Default: -100", "reduce (bool, optional) : Deprecated (see reduction). By default, thelosses are averaged or summed over observations for each minibatch dependingon size_average. When reduce is False, returns a loss perbatch element instead and ignores size_average. Default: True", "reduction (string, optional) : Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"], "returns": null, "example": " # input is of size N x C = 3 x 5\n input = torch.randn(3, 5, requires_grad=True)\n # each element in target has to have 0 &lt;= value &lt; C\n target = torch.tensor([1, 0, 4])\n output = F.nll_loss(F.log_softmax(input), target)\n output.backward()\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.unsqueeze", "item_type": "function", "code": "torch.unsqueeze(input,dim,out=None)\u2192Tensor", "description": "Returns a new tensor with a dimension of size one inserted at the specified position. The returned tensor shares the same underlying data with this tensor. A dim value within the range [-input.dim() - 1, input.dim() + 1) can be used. Negative dim will correspond to unsqueeze() applied at dim = dim + input.dim() + 1.  Parameters  input (Tensor) \u2013 the input tensor. dim (python:int) \u2013 the index at which to insert the singleton dimension out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; x = torch.tensor([1, 2, 3, 4]) &gt;&gt;&gt; torch.unsqueeze(x, 0) tensor([[ 1,  2,  3,  4]]) &gt;&gt;&gt; torch.unsqueeze(x, 1) tensor([[ 1],         [ 2],         [ 3],         [ 4]])   ", "parameters": ["input (Tensor) : the input tensor.", "dim (python:int) : the index at which to insert the singleton dimension", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " x = torch.tensor([1, 2, 3, 4])\n torch.unsqueeze(x, 0)\ntensor([[ 1,  2,  3,  4]])\n torch.unsqueeze(x, 1)\ntensor([[ 1],\n        [ 2],\n        [ 3],\n        [ 4]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.where", "item_type": "function", "code": "torch.where()", "description": "  torch.where(condition, x, y) \u2192 Tensor   Return a tensor of elements selected from either x or y, depending on condition. The operation is defined as:  outi={xiif\u00a0conditioniyiotherwise\\text{out}_i = \\begin{cases}     \\text{x}_i &amp; \\text{if } \\text{condition}_i \\\\     \\text{y}_i &amp; \\text{otherwise} \\\\ \\end{cases}  outi\u200b={xi\u200byi\u200b\u200bif\u00a0conditioni\u200botherwise\u200b   Note The tensors condition, x, y must be broadcastable.   Parameters  condition (BoolTensor) \u2013 When True (nonzero), yield x, otherwise yield y x (Tensor) \u2013 values selected at indices where condition is True y (Tensor) \u2013 values selected at indices where condition is False   Returns A tensor of shape equal to the broadcasted shape of condition, x, y  Return type Tensor   Example: &gt;&gt;&gt; x = torch.randn(3, 2) &gt;&gt;&gt; y = torch.ones(3, 2) &gt;&gt;&gt; x tensor([[-0.4620,  0.3139],         [ 0.3898, -0.7197],         [ 0.0478, -0.1657]]) &gt;&gt;&gt; torch.where(x &gt; 0, x, y) tensor([[ 1.0000,  0.3139],         [ 0.3898,  1.0000],         [ 0.0478,  1.0000]])     torch.where(condition) \u2192 tuple of LongTensor   torch.where(condition) is identical to torch.nonzero(condition, as_tuple=True).  Note See also torch.nonzero().  ", "parameters": ["condition (BoolTensor) : When True (nonzero), yield x, otherwise yield y", "x (Tensor) : values selected at indices where condition is True", "y (Tensor) : values selected at indices where condition is False"], "returns": "A tensor of shape equal to the broadcasted shape of condition, x, y", "example": " x = torch.randn(3, 2)\n y = torch.ones(3, 2)\n x\ntensor([[-0.4620,  0.3139],\n        [ 0.3898, -0.7197],\n        [ 0.0478, -0.1657]])\n torch.where(x  0, x, y)\ntensor([[ 1.0000,  0.3139],\n        [ 0.3898,  1.0000],\n        [ 0.0478,  1.0000]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.where(condition,x,y)\u2192Tensor", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.where(condition)\u2192tupleofLongTensor", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.seed", "item_type": "function", "code": "torch.seed()", "description": "Sets the seed for generating random numbers to a non-deterministic random number. Returns a 64 bit number used to seed the RNG. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.manual_seed", "item_type": "function", "code": "torch.manual_seed(seed)", "description": "Sets the seed for generating random numbers. Returns a torch.Generator object.  Parameters seed (python:int) \u2013 The desired seed.   ", "parameters": ["seed (python:int) : The desired seed."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.initial_seed", "item_type": "function", "code": "torch.initial_seed()", "description": "Returns the initial seed for generating random numbers as a Python long. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.one_hot_categorical.OneHotCategorical.log_prob", "item_type": "method", "code": "log_prob(value)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.one_hot_categorical.OneHotCategorical.logits", "item_type": "method", "code": "propertylogits", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.one_hot_categorical.OneHotCategorical.mean", "item_type": "method", "code": "propertymean", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.one_hot_categorical.OneHotCategorical.param_shape", "item_type": "method", "code": "propertyparam_shape", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.one_hot_categorical.OneHotCategorical.probs", "item_type": "method", "code": "propertyprobs", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.one_hot_categorical.OneHotCategorical.sample", "item_type": "method", "code": "sample(sample_shape=torch.Size([]))", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.one_hot_categorical.OneHotCategorical.variance", "item_type": "method", "code": "propertyvariance", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.pareto.Pareto.entropy", "item_type": "method", "code": "entropy()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.pareto.Pareto.expand", "item_type": "method", "code": "expand(batch_shape,_instance=None)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.pareto.Pareto.mean", "item_type": "method", "code": "propertymean", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.pareto.Pareto.support", "item_type": "method", "code": "propertysupport", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.pareto.Pareto.variance", "item_type": "method", "code": "propertyvariance", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.poisson.Poisson.expand", "item_type": "method", "code": "expand(batch_shape,_instance=None)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.poisson.Poisson.log_prob", "item_type": "method", "code": "log_prob(value)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.poisson.Poisson.mean", "item_type": "method", "code": "propertymean", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.ParameterDict.values", "item_type": "method", "code": "values()", "description": "Return an iterable of the ParameterDict values. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.MultiheadAttention.forward", "item_type": "method", "code": "forward(query,key,value,key_padding_mask=None,need_weights=True,attn_mask=None)", "description": " Parameters  key, value (query,) \u2013 map a query and a set of key-value pairs to an output. See \u201cAttention Is All You Need\u201d for more details. key_padding_mask \u2013 if provided, specified padding elements in the key will be ignored by the attention. This is an binary mask. When the value is True, the corresponding value on the attention layer will be filled with -inf. need_weights \u2013 output attn_output_weights. attn_mask \u2013 mask that prevents attention to certain positions. This is an additive mask (i.e. the values will be added to the attention layer).     Shape: Inputs: query: (L,N,E)(L, N, E)(L,N,E)   where L is the target sequence length, N is the batch size, E is the embedding dimension. key: (S,N,E)(S, N, E)(S,N,E)  , where S is the source sequence length, N is the batch size, E is the embedding dimension. value: (S,N,E)(S, N, E)(S,N,E)   where S is the source sequence length, N is the batch size, E is the embedding dimension. key_padding_mask: (N,S)(N, S)(N,S)  , ByteTensor, where N is the batch size, S is the source sequence length. attn_mask: (L,S)(L, S)(L,S)   where L is the target sequence length, S is the source sequence length. Outputs: attn_output: (L,N,E)(L, N, E)(L,N,E)   where L is the target sequence length, N is the batch size, E is the embedding dimension. attn_output_weights: (N,L,S)(N, L, S)(N,L,S)   where N is the batch size, L is the target sequence length, S is the source sequence length.    ", "parameters": ["key, value (query,) : map a query and a set of key-value pairs to an output.See \u201cAttention Is All You Need\u201d for more details.", "key_padding_mask : if provided, specified padding elements in the key willbe ignored by the attention. This is an binary mask. When the value is True,the corresponding value on the attention layer will be filled with -inf.", "need_weights : output attn_output_weights.", "attn_mask : mask that prevents attention to certain positions. This is an additive mask(i.e. the values will be added to the attention layer)."], "returns": null, "example": "NA", "shape": " Inputs: query: (L,N,E)(L, N, E)(L,N,E)   where L is the target sequence length, N is the batch size, E is the embedding dimension. key: (S,N,E)(S, N, E)(S,N,E)  , where S is the source sequence length, N is the batch size, E is the embedding dimension. value: (S,N,E)(S, N, E)(S,N,E)   where S is the source sequence length, N is the batch size, E is the embedding dimension. key_padding_mask: (N,S)(N, S)(N,S)  , ByteTensor, where N is the batch size, S is the source sequence length. attn_mask: (L,S)(L, S)(L,S)   where L is the target sequence length, S is the source sequence length. Outputs: attn_output: (L,N,E)(L, N, E)(L,N,E)   where L is the target sequence length, N is the batch size, E is the embedding dimension. attn_output_weights: (N,L,S)(N, L, S)(N,L,S)   where N is the batch size, L is the target sequence length, S is the source sequence length.  "},
{"library": "torch", "item_id": "torch.nn.AdaptiveLogSoftmaxWithLoss.log_prob", "item_type": "method", "code": "log_prob(input)", "description": "Computes log probabilities for all n_classesn\\_classesn_classes    Parameters input (Tensor) \u2013 a minibatch of examples  Returns log-probabilities of for each class ccc   in range 0&lt;=c&lt;=n_classes0 &lt;= c &lt;= n\\_classes0&lt;=c&lt;=n_classes  , where n_classesn\\_classesn_classes   is a parameter passed to AdaptiveLogSoftmaxWithLoss constructor.    Shape: Input: (N,in_features)(N, in\\_features)(N,in_features)   Output: (N,n_classes)(N, n\\_classes)(N,n_classes)      ", "parameters": ["input (Tensor) : a minibatch of examples", "log-probabilities of for each class cccin range 0&lt;=c&lt;=n_classes0 &lt;= c &lt;= n\\_classes0&lt;=c&lt;=n_classes, where n_classesn\\_classesn_classes is aparameter passed to AdaptiveLogSoftmaxWithLoss constructor."], "returns": "log-probabilities of for each class cccin range 0&lt;=c&lt;=n_classes0 &lt;= c &lt;= n\\_classes0&lt;=c&lt;=n_classes, where n_classesn\\_classesn_classes is aparameter passed to AdaptiveLogSoftmaxWithLoss constructor.", "example": "NA", "shape": " Input: (N,in_features)(N, in\\_features)(N,in_features)   Output: (N,n_classes)(N, n\\_classes)(N,n_classes)    "},
{"library": "torch", "item_id": "torch.nn.AdaptiveLogSoftmaxWithLoss.predict", "item_type": "method", "code": "predict(input)", "description": "This is equivalent to self.log_pob(input).argmax(dim=1), but is more efficient in some cases.  Parameters input (Tensor) \u2013 a minibatch of examples  Returns a class with the highest probability for each example  Return type output (Tensor)    Shape: Input: (N,in_features)(N, in\\_features)(N,in_features)   Output: (N)(N)(N)      ", "parameters": ["input (Tensor) : a minibatch of examples", "a class with the highest probability for each example", "output (Tensor)"], "returns": "a class with the highest probability for each example", "example": "NA", "shape": " Input: (N,in_features)(N, in\\_features)(N,in_features)   Output: (N)(N)(N)    "},
{"library": "torch", "item_id": "torch.nn.SyncBatchNorm.convert_sync_batchnorm", "item_type": "method", "code": "classmethodconvert_sync_batchnorm(module,process_group=None)", "description": "Helper function to convert torch.nn.BatchNormND layer in the model to torch.nn.SyncBatchNorm layer.  Parameters  module (nn.Module) \u2013 containing module process_group (optional) \u2013 process group to scope synchronization,    default is the whole world  Returns The original module with the converted torch.nn.SyncBatchNorm layer   Example: &gt;&gt;&gt; # Network with nn.BatchNorm layer &gt;&gt;&gt; module = torch.nn.Sequential( &gt;&gt;&gt;            torch.nn.Linear(20, 100), &gt;&gt;&gt;            torch.nn.BatchNorm1d(100) &gt;&gt;&gt;          ).cuda() &gt;&gt;&gt; # creating process group (optional) &gt;&gt;&gt; # process_ids is a list of int identifying rank ids. &gt;&gt;&gt; process_group = torch.distributed.new_group(process_ids) &gt;&gt;&gt; sync_bn_module = convert_sync_batchnorm(module, process_group)   ", "parameters": ["module (nn.Module) : containing module", "process_group (optional) : process group to scope synchronization,"], "returns": "The original module with the converted torch.nn.SyncBatchNorm layer", "example": " # Network with nn.BatchNorm layer\n module = torch.nn.Sequential(\n            torch.nn.Linear(20, 100),\n            torch.nn.BatchNorm1d(100)\n          ).cuda()\n # creating process group (optional)\n # process_ids is a list of int identifying rank ids.\n process_group = torch.distributed.new_group(process_ids)\n sync_bn_module = convert_sync_batchnorm(module, process_group)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.RNNBase.flatten_parameters", "item_type": "method", "code": "flatten_parameters()", "description": "Resets parameter data pointer so that they can use faster code paths. Right now, this works only if the module is on the GPU and cuDNN is enabled. Otherwise, it\u2019s a no-op. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.is_signed", "item_type": "method", "code": "is_signed()\u2192bool", "description": "Returns True if the data type of self is a signed data type. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.item", "item_type": "method", "code": "item()\u2192number", "description": "Returns the value of this tensor as a standard Python number. This only works for tensors with one element. For other cases, see tolist(). This operation is not differentiable. Example: &gt;&gt;&gt; x = torch.tensor([1.0]) &gt;&gt;&gt; x.item() 1.0   ", "parameters": [], "returns": null, "example": " x = torch.tensor([1.0])\n x.item()\n1.0\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.kthvalue", "item_type": "method", "code": "kthvalue(k,dim=None,keepdim=False)-&gt;(Tensor,LongTensor)", "description": "See torch.kthvalue() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.le", "item_type": "method", "code": "le(other)\u2192Tensor", "description": "See torch.le() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.le_", "item_type": "method", "code": "le_(other)\u2192Tensor", "description": "In-place version of le() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.lerp", "item_type": "method", "code": "lerp(end,weight)\u2192Tensor", "description": "See torch.lerp() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.lerp_", "item_type": "method", "code": "lerp_(end,weight)\u2192Tensor", "description": "In-place version of lerp() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.lgamma", "item_type": "method", "code": "lgamma()\u2192Tensor", "description": "See torch.lgamma() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.lgamma_", "item_type": "method", "code": "lgamma_()\u2192Tensor", "description": "In-place version of lgamma() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.log", "item_type": "method", "code": "log()\u2192Tensor", "description": "See torch.log() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.log_", "item_type": "method", "code": "log_()\u2192Tensor", "description": "In-place version of log() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.logdet", "item_type": "method", "code": "logdet()\u2192Tensor", "description": "See torch.logdet() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.log10", "item_type": "method", "code": "log10()\u2192Tensor", "description": "See torch.log10() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.smooth_l1_loss", "item_type": "function", "code": "torch.nn.functional.smooth_l1_loss(input,target,size_average=None,reduce=None,reduction='mean')", "description": "Function that uses a squared term if the absolute element-wise error falls below 1 and an L1 term otherwise. See SmoothL1Loss for details. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.soft_margin_loss", "item_type": "function", "code": "torch.nn.functional.soft_margin_loss(input,target,size_average=None,reduce=None,reduction='mean')\u2192Tensor", "description": "See SoftMarginLoss for details. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.triplet_margin_loss", "item_type": "function", "code": "torch.nn.functional.triplet_margin_loss(anchor,positive,negative,margin=1.0,p=2,eps=1e-06,swap=False,size_average=None,reduce=None,reduction='mean')", "description": "See TripletMarginLoss for details ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.pixel_shuffle", "item_type": "function", "code": "torch.nn.functional.pixel_shuffle()", "description": "Rearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W)   to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r)  . See PixelShuffle for details.  Parameters  input (Tensor) \u2013 the input tensor upscale_factor (python:int) \u2013 factor to increase spatial resolution by    Examples: &gt;&gt;&gt; input = torch.randn(1, 9, 4, 4) &gt;&gt;&gt; output = torch.nn.functional.pixel_shuffle(input, 3) &gt;&gt;&gt; print(output.size()) torch.Size([1, 1, 12, 12])   ", "parameters": ["input (Tensor) : the input tensor", "upscale_factor (python:int) : factor to increase spatial resolution by"], "returns": null, "example": " input = torch.randn(1, 9, 4, 4)\n output = torch.nn.functional.pixel_shuffle(input, 3)\n print(output.size())\ntorch.Size([1, 1, 12, 12])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.pad", "item_type": "function", "code": "torch.nn.functional.pad(input,pad,mode='constant',value=0)", "description": "Pads tensor.  Padding size:The padding size by which to pad some dimensions of input are described starting from the last dimension and moving forward. \u230alen(pad)2\u230b\\left\\lfloor\\frac{\\text{len(pad)}}{2}\\right\\rfloor\u230a2len(pad)\u200b\u230b   dimensions of input will be padded. For example, to pad only the last dimension of the input tensor, then pad has the form (padding_left,padding_right)(\\text{padding\\_left}, \\text{padding\\_right})(padding_left,padding_right)  ; to pad the last 2 dimensions of the input tensor, then use (padding_left,padding_right,(\\text{padding\\_left}, \\text{padding\\_right},(padding_left,padding_right,   padding_top,padding_bottom)\\text{padding\\_top}, \\text{padding\\_bottom})padding_top,padding_bottom)  ; to pad the last 3 dimensions, use (padding_left,padding_right,(\\text{padding\\_left}, \\text{padding\\_right},(padding_left,padding_right,   padding_top,padding_bottom\\text{padding\\_top}, \\text{padding\\_bottom}padding_top,padding_bottom   padding_front,padding_back)\\text{padding\\_front}, \\text{padding\\_back})padding_front,padding_back)  .  Padding mode:See torch.nn.ConstantPad2d, torch.nn.ReflectionPad2d, and torch.nn.ReplicationPad2d for concrete examples on how each of the padding modes works. Constant padding is implemented for arbitrary dimensions. Replicate padding is implemented for padding the last 3 dimensions of 5D input tensor, or the last 2 dimensions of 4D input tensor, or the last dimension of 3D input tensor. Reflect padding is only implemented for padding the last 2 dimensions of 4D input tensor, or the last dimension of 3D input tensor.    Note When using the CUDA backend, this operation may induce nondeterministic behaviour in its backward pass that is not easily switched off. Please see the notes on Reproducibility for background.   Parameters  input (Tensor) \u2013 N-dimensional tensor pad (tuple) \u2013 m-elements tuple, where m2\u2264\\frac{m}{2} \\leq2m\u200b\u2264   input dimensions and mmm   is even. mode \u2013 'constant', 'reflect', 'replicate' or 'circular'. Default: 'constant' value \u2013 fill value for 'constant' padding. Default: 0    Examples: &gt;&gt;&gt; t4d = torch.empty(3, 3, 4, 2) &gt;&gt;&gt; p1d = (1, 1) # pad last dim by 1 on each side &gt;&gt;&gt; out = F.pad(t4d, p1d, \"constant\", 0)  # effectively zero padding &gt;&gt;&gt; print(out.data.size()) torch.Size([3, 3, 4, 4]) &gt;&gt;&gt; p2d = (1, 1, 2, 2) # pad last dim by (1, 1) and 2nd to last by (2, 2) &gt;&gt;&gt; out = F.pad(t4d, p2d, \"constant\", 0) &gt;&gt;&gt; print(out.data.size()) torch.Size([3, 3, 8, 4]) &gt;&gt;&gt; t4d = torch.empty(3, 3, 4, 2) &gt;&gt;&gt; p3d = (0, 1, 2, 1, 3, 3) # pad by (0, 1), (2, 1), and (3, 3) &gt;&gt;&gt; out = F.pad(t4d, p3d, \"constant\", 0) &gt;&gt;&gt; print(out.data.size()) torch.Size([3, 9, 7, 3])   ", "parameters": ["input (Tensor) : N-dimensional tensor", "pad (tuple) : m-elements tuple, wherem2\u2264\\frac{m}{2} \\leq2m\u200b\u2264 input dimensions and mmm is even.", "mode : 'constant', 'reflect', 'replicate' or 'circular'.Default: 'constant'", "value : fill value for 'constant' padding. Default: 0"], "returns": null, "example": " t4d = torch.empty(3, 3, 4, 2)\n p1d = (1, 1) # pad last dim by 1 on each side\n out = F.pad(t4d, p1d, \"constant\", 0)  # effectively zero padding\n print(out.data.size())\ntorch.Size([3, 3, 4, 4])\n p2d = (1, 1, 2, 2) # pad last dim by (1, 1) and 2nd to last by (2, 2)\n out = F.pad(t4d, p2d, \"constant\", 0)\n print(out.data.size())\ntorch.Size([3, 3, 8, 4])\n t4d = torch.empty(3, 3, 4, 2)\n p3d = (0, 1, 2, 1, 3, 3) # pad by (0, 1), (2, 1), and (3, 3)\n out = F.pad(t4d, p3d, \"constant\", 0)\n print(out.data.size())\ntorch.Size([3, 9, 7, 3])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.get_rng_state", "item_type": "function", "code": "torch.get_rng_state()", "description": "Returns the random number generator state as a torch.ByteTensor. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.set_rng_state", "item_type": "function", "code": "torch.set_rng_state(new_state)", "description": "Sets the random number generator state.  Parameters new_state (torch.ByteTensor) \u2013 The desired state   ", "parameters": ["new_state (torch.ByteTensor) : The desired state"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.bernoulli", "item_type": "function", "code": "torch.bernoulli(input,*,generator=None,out=None)\u2192Tensor", "description": "Draws binary random numbers (0 or 1) from a Bernoulli distribution. The input tensor should be a tensor containing probabilities to be used for drawing the binary random number. Hence, all values in input have to be in the range: 0\u2264inputi\u226410 \\leq \\text{input}_i \\leq 10\u2264inputi\u200b\u22641  . The ith\\text{i}^{th}ith   element of the output tensor will draw a value 111   according to the ith\\text{i}^{th}ith   probability value given in input.  outi\u223cBernoulli(p=inputi)\\text{out}_{i} \\sim \\mathrm{Bernoulli}(p = \\text{input}_{i})  outi\u200b\u223cBernoulli(p=inputi\u200b)  The returned out tensor only has values 0 or 1 and is of the same shape as input. out can have integral dtype, but input must have floating point dtype.  Parameters  input (Tensor) \u2013 the input tensor of probability values for the Bernoulli distribution generator (torch.Generator, optional) \u2013 a pseudorandom number generator for sampling out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; a = torch.empty(3, 3).uniform_(0, 1)  # generate a uniform random matrix with range [0, 1] &gt;&gt;&gt; a tensor([[ 0.1737,  0.0950,  0.3609],         [ 0.7148,  0.0289,  0.2676],         [ 0.9456,  0.8937,  0.7202]]) &gt;&gt;&gt; torch.bernoulli(a) tensor([[ 1.,  0.,  0.],         [ 0.,  0.,  0.],         [ 1.,  1.,  1.]])  &gt;&gt;&gt; a = torch.ones(3, 3) # probability of drawing \"1\" is 1 &gt;&gt;&gt; torch.bernoulli(a) tensor([[ 1.,  1.,  1.],         [ 1.,  1.,  1.],         [ 1.,  1.,  1.]]) &gt;&gt;&gt; a = torch.zeros(3, 3) # probability of drawing \"1\" is 0 &gt;&gt;&gt; torch.bernoulli(a) tensor([[ 0.,  0.,  0.],         [ 0.,  0.,  0.],         [ 0.,  0.,  0.]])   ", "parameters": ["input (Tensor) : the input tensor of probability values for the Bernoulli distribution", "generator (torch.Generator, optional) : a pseudorandom number generator for sampling", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " a = torch.empty(3, 3).uniform_(0, 1)  # generate a uniform random matrix with range [0, 1]\n a\ntensor([[ 0.1737,  0.0950,  0.3609],\n        [ 0.7148,  0.0289,  0.2676],\n        [ 0.9456,  0.8937,  0.7202]])\n torch.bernoulli(a)\ntensor([[ 1.,  0.,  0.],\n        [ 0.,  0.,  0.],\n        [ 1.,  1.,  1.]])\n\n a = torch.ones(3, 3) # probability of drawing \"1\" is 1\n torch.bernoulli(a)\ntensor([[ 1.,  1.,  1.],\n        [ 1.,  1.,  1.],\n        [ 1.,  1.,  1.]])\n a = torch.zeros(3, 3) # probability of drawing \"1\" is 0\n torch.bernoulli(a)\ntensor([[ 0.,  0.,  0.],\n        [ 0.,  0.,  0.],\n        [ 0.,  0.,  0.]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.multinomial", "item_type": "function", "code": "torch.multinomial(input,num_samples,replacement=False,*,generator=None,out=None)\u2192LongTensor", "description": "Returns a tensor where each row contains num_samples indices sampled from the multinomial probability distribution located in the corresponding row of tensor input.  Note The rows of input do not need to sum to one (in which case we use the values as weights), but must be non-negative, finite and have a non-zero sum.  Indices are ordered from left to right according to when each was sampled (first samples are placed in first column). If input is a vector, out is a vector of size num_samples. If input is a matrix with m rows, out is an matrix of shape (m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples)  . If replacement is True, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a sample index is drawn for a row, it cannot be drawn again for that row.  Note When drawn without replacement, num_samples must be lower than number of non-zero elements in input (or the min number of non-zero elements in each row of input if it is a matrix).   Parameters  input (Tensor) \u2013 the input tensor containing probabilities num_samples (python:int) \u2013 number of samples to draw replacement (bool, optional) \u2013 whether to draw with replacement or not generator (torch.Generator, optional) \u2013 a pseudorandom number generator for sampling out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; weights = torch.tensor([0, 10, 3, 0], dtype=torch.float) # create a tensor of weights &gt;&gt;&gt; torch.multinomial(weights, 2) tensor([1, 2]) &gt;&gt;&gt; torch.multinomial(weights, 4) # ERROR! RuntimeError: invalid argument 2: invalid multinomial distribution (with replacement=False, not enough non-negative category to sample) at ../aten/src/TH/generic/THTensorRandom.cpp:320 &gt;&gt;&gt; torch.multinomial(weights, 4, replacement=True) tensor([ 2,  1,  1,  1])   ", "parameters": ["input (Tensor) : the input tensor containing probabilities", "num_samples (python:int) : number of samples to draw", "replacement (bool, optional) : whether to draw with replacement or not", "generator (torch.Generator, optional) : a pseudorandom number generator for sampling", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " weights = torch.tensor([0, 10, 3, 0], dtype=torch.float) # create a tensor of weights\n torch.multinomial(weights, 2)\ntensor([1, 2])\n torch.multinomial(weights, 4) # ERROR!\nRuntimeError: invalid argument 2: invalid multinomial distribution (with replacement=False,\nnot enough non-negative category to sample) at ../aten/src/TH/generic/THTensorRandom.cpp:320\n torch.multinomial(weights, 4, replacement=True)\ntensor([ 2,  1,  1,  1])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.normal", "item_type": "function", "code": "torch.normal()", "description": "  torch.normal(mean, std, *, generator=None, out=None) \u2192 Tensor   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given. The mean is a tensor with the mean of each output element\u2019s normal distribution The std is a tensor with the standard deviation of each output element\u2019s normal distribution The shapes of mean and std don\u2019t need to match, but the total number of elements in each tensor need to be the same.  Note When the shapes do not match, the shape of mean is used as the shape for the returned output tensor   Parameters  mean (Tensor) \u2013 the tensor of per-element means std (Tensor) \u2013 the tensor of per-element standard deviations generator (torch.Generator, optional) \u2013 a pseudorandom number generator for sampling out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; torch.normal(mean=torch.arange(1., 11.), std=torch.arange(1, 0, -0.1)) tensor([  1.0425,   3.5672,   2.7969,   4.2925,   4.7229,   6.2134,           8.0505,   8.1408,   9.0563,  10.0566])     torch.normal(mean=0.0, std, out=None) \u2192 Tensor   Similar to the function above, but the means are shared among all drawn elements.  Parameters  mean (python:float, optional) \u2013 the mean for all distributions std (Tensor) \u2013 the tensor of per-element standard deviations out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; torch.normal(mean=0.5, std=torch.arange(1., 6.)) tensor([-1.2793, -1.0732, -2.0687,  5.1177, -1.2303])     torch.normal(mean, std=1.0, out=None) \u2192 Tensor   Similar to the function above, but the standard-deviations are shared among all drawn elements.  Parameters  mean (Tensor) \u2013 the tensor of per-element means std (python:float, optional) \u2013 the standard deviation for all distributions out (Tensor, optional) \u2013 the output tensor    Example: &gt;&gt;&gt; torch.normal(mean=torch.arange(1., 6.)) tensor([ 1.1552,  2.6148,  2.6535,  5.8318,  4.2361])     torch.normal(mean, std, size, *, out=None) \u2192 Tensor   Similar to the function above, but the means and standard deviations are shared among all drawn elements. The resulting tensor has size given by size.  Parameters  mean (python:float) \u2013 the mean for all distributions std (python:float) \u2013 the standard deviation for all distributions size (python:int...) \u2013 a sequence of integers defining the shape of the output tensor. out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; torch.normal(2, 3, size=(1, 4)) tensor([[-1.3987, -1.9544,  3.6048,  0.7909]])   ", "parameters": ["mean (Tensor) : the tensor of per-element means", "std (Tensor) : the tensor of per-element standard deviations", "generator (torch.Generator, optional) : a pseudorandom number generator for sampling", "out (Tensor, optional) : the output tensor.", "mean (python:float, optional) : the mean for all distributions", "std (Tensor) : the tensor of per-element standard deviations", "out (Tensor, optional) : the output tensor.", "mean (Tensor) : the tensor of per-element means", "std (python:float, optional) : the standard deviation for all distributions", "out (Tensor, optional) : the output tensor", "mean (python:float) : the mean for all distributions", "std (python:float) : the standard deviation for all distributions", "size (python:int...) : a sequence of integers defining the shape of the output tensor.", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " torch.normal(mean=torch.arange(1., 11.), std=torch.arange(1, 0, -0.1))\ntensor([  1.0425,   3.5672,   2.7969,   4.2925,   4.7229,   6.2134,\n          8.0505,   8.1408,   9.0563,  10.0566])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.poisson.Poisson.sample", "item_type": "method", "code": "sample(sample_shape=torch.Size([]))", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.poisson.Poisson.variance", "item_type": "method", "code": "propertyvariance", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.expand", "item_type": "method", "code": "expand(batch_shape,_instance=None)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.logits", "item_type": "method", "code": "propertylogits", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.probs", "item_type": "method", "code": "propertyprobs", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.temperature", "item_type": "method", "code": "propertytemperature", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.expand", "item_type": "method", "code": "expand(batch_shape,_instance=None)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.log_prob", "item_type": "method", "code": "log_prob(value)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.param_shape", "item_type": "method", "code": "propertyparam_shape", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.rsample", "item_type": "method", "code": "rsample(sample_shape=torch.Size([]))", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.expand", "item_type": "method", "code": "expand(batch_shape,_instance=None)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.logits", "item_type": "method", "code": "propertylogits", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.probs", "item_type": "method", "code": "propertyprobs", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.temperature", "item_type": "method", "code": "propertytemperature", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.studentT.StudentT.entropy", "item_type": "method", "code": "entropy()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.Transformer.forward", "item_type": "method", "code": "forward(src,tgt,src_mask=None,tgt_mask=None,memory_mask=None,src_key_padding_mask=None,tgt_key_padding_mask=None,memory_key_padding_mask=None)", "description": "Take in and process masked source/target sequences.  Parameters  src \u2013 the sequence to the encoder (required). tgt \u2013 the sequence to the decoder (required). src_mask \u2013 the additive mask for the src sequence (optional). tgt_mask \u2013 the additive mask for the tgt sequence (optional). memory_mask \u2013 the additive mask for the encoder output (optional). src_key_padding_mask \u2013 the ByteTensor mask for src keys per batch (optional). tgt_key_padding_mask \u2013 the ByteTensor mask for tgt keys per batch (optional). memory_key_padding_mask \u2013 the ByteTensor mask for memory keys per batch (optional).     Shape: src: (S,N,E)(S, N, E)(S,N,E)  . tgt: (T,N,E)(T, N, E)(T,N,E)  . src_mask: (S,S)(S, S)(S,S)  . tgt_mask: (T,T)(T, T)(T,T)  . memory_mask: (T,S)(T, S)(T,S)  . src_key_padding_mask: (N,S)(N, S)(N,S)  . tgt_key_padding_mask: (N,T)(N, T)(N,T)  . memory_key_padding_mask: (N,S)(N, S)(N,S)  .  Note: [src/tgt/memory]_mask should be filled with float(\u2018-inf\u2019) for the masked positions and float(0.0) else. These masks ensure that predictions for position i depend only on the unmasked positions j and are applied identically for each sequence in a batch. [src/tgt/memory]_key_padding_mask should be a ByteTensor where True values are positions that should be masked with float(\u2018-inf\u2019) and False values will be unchanged. This mask ensures that no information will be taken from position i if it is masked, and has a separate mask for each sequence in a batch.  output: (T,N,E)(T, N, E)(T,N,E)  .  Note: Due to the multi-head attention architecture in the transformer model, the output sequence length of a transformer is same as the input sequence (i.e. target) length of the decode. where S is the source sequence length, T is the target sequence length, N is the batch size, E is the feature number   Examples &gt;&gt;&gt; output = transformer_model(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)   ", "parameters": ["src : the sequence to the encoder (required).", "tgt : the sequence to the decoder (required).", "src_mask : the additive mask for the src sequence (optional).", "tgt_mask : the additive mask for the tgt sequence (optional).", "memory_mask : the additive mask for the encoder output (optional).", "src_key_padding_mask : the ByteTensor mask for src keys per batch (optional).", "tgt_key_padding_mask : the ByteTensor mask for tgt keys per batch (optional).", "memory_key_padding_mask : the ByteTensor mask for memory keys per batch (optional)."], "returns": null, "example": "NA", "shape": " src: (S,N,E)(S, N, E)(S,N,E)  . tgt: (T,N,E)(T, N, E)(T,N,E)  . src_mask: (S,S)(S, S)(S,S)  . tgt_mask: (T,T)(T, T)(T,T)  . memory_mask: (T,S)(T, S)(T,S)  . src_key_padding_mask: (N,S)(N, S)(N,S)  . tgt_key_padding_mask: (N,T)(N, T)(N,T)  . memory_key_padding_mask: (N,S)(N, S)(N,S)  .  Note: [src/tgt/memory]_mask should be filled with float(\u2018-inf\u2019) for the masked positions and float(0.0) else. These masks ensure that predictions for position i depend only on the unmasked positions j and are applied identically for each sequence in a batch. [src/tgt/memory]_key_padding_mask should be a ByteTensor where True values are positions that should be masked with float(\u2018-inf\u2019) and False values will be unchanged. This mask ensures that no information will be taken from position i if it is masked, and has a separate mask for each sequence in a batch.  output: (T,N,E)(T, N, E)(T,N,E)  .  Note: Due to the multi-head attention architecture in the transformer model, the output sequence length of a transformer is same as the input sequence (i.e. target) length of the decode. where S is the source sequence length, T is the target sequence length, N is the batch size, E is the feature number "},
{"library": "torch", "item_id": "torch.nn.Transformer.generate_square_subsequent_mask", "item_type": "method", "code": "generate_square_subsequent_mask(sz)", "description": "Generate a square mask for the sequence. The masked positions are filled with float(\u2018-inf\u2019). Unmasked positions are filled with float(0.0). ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.TransformerEncoder.forward", "item_type": "method", "code": "forward(src,mask=None,src_key_padding_mask=None)", "description": "Pass the input through the encoder layers in turn.  Parameters  src \u2013 the sequnce to the encoder (required). mask \u2013 the mask for the src sequence (optional). src_key_padding_mask \u2013 the mask for the src keys per batch (optional).     Shape:see the docs in Transformer class.   ", "parameters": ["src : the sequnce to the encoder (required).", "mask : the mask for the src sequence (optional).", "src_key_padding_mask : the mask for the src keys per batch (optional)."], "returns": null, "example": "NA", "shape": "see the docs in Transformer class. "},
{"library": "torch", "item_id": "torch.Tensor.log10_", "item_type": "method", "code": "log10_()\u2192Tensor", "description": "In-place version of log10() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.log1p", "item_type": "method", "code": "log1p()\u2192Tensor", "description": "See torch.log1p() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.log1p_", "item_type": "method", "code": "log1p_()\u2192Tensor", "description": "In-place version of log1p() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.log2", "item_type": "method", "code": "log2()\u2192Tensor", "description": "See torch.log2() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.log2_", "item_type": "method", "code": "log2_()\u2192Tensor", "description": "In-place version of log2() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.log_normal_", "item_type": "method", "code": "log_normal_(mean=1,std=2,*,generator=None)", "description": "Fills self tensor with numbers samples from the log-normal distribution parameterized by the given mean \u03bc\\mu\u03bc   and standard deviation \u03c3\\sigma\u03c3  . Note that mean and std are the mean and standard deviation of the underlying normal distribution, and not of the returned distribution:  f(x)=1x\u03c32\u03c0\u00a0e\u2212(ln\u2061x\u2212\u03bc)22\u03c32f(x) = \\dfrac{1}{x \\sigma \\sqrt{2\\pi}}\\ e^{-\\frac{(\\ln x - \\mu)^2}{2\\sigma^2}}f(x)=x\u03c32\u03c0\u200b1\u200b\u00a0e\u22122\u03c32(lnx\u2212\u03bc)2\u200b  ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.logsumexp", "item_type": "method", "code": "logsumexp(dim,keepdim=False)\u2192Tensor", "description": "See torch.logsumexp() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.logical_not", "item_type": "method", "code": "logical_not()\u2192Tensor", "description": "See torch.logical_not() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.logical_not_", "item_type": "method", "code": "logical_not_()\u2192Tensor", "description": "In-place version of logical_not() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.logical_xor", "item_type": "method", "code": "logical_xor()\u2192Tensor", "description": "See torch.logical_xor() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.logical_xor_", "item_type": "method", "code": "logical_xor_()\u2192Tensor", "description": "In-place version of logical_xor() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.interpolate", "item_type": "function", "code": "torch.nn.functional.interpolate(input,size=None,scale_factor=None,mode='nearest',align_corners=None)", "description": "Down/up samples the input to either the given size or the given scale_factor The algorithm used for interpolation is determined by mode. Currently temporal, spatial and volumetric sampling are supported, i.e. expected inputs are 3-D, 4-D or 5-D in shape. The input dimensions are interpreted in the form: mini-batch x channels x [optional depth] x [optional height] x width. The modes available for resizing are: nearest, linear (3D-only), bilinear, bicubic (4D-only), trilinear (5D-only), area  Parameters  input (Tensor) \u2013 the input tensor size (python:int or Tuple[python:int] or Tuple[python:int, python:int] or Tuple[python:int, python:int, python:int]) \u2013 output spatial size. scale_factor (python:float or Tuple[python:float]) \u2013 multiplier for spatial size. Has to match input size if it is a tuple. mode (str) \u2013 algorithm used for upsampling: 'nearest' | 'linear' | 'bilinear' | 'bicubic' | 'trilinear' | 'area'. Default: 'nearest' align_corners (bool, optional) \u2013 Geometrically, we consider the pixels of the input and output as squares rather than points. If set to True, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to False, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when scale_factor is kept the same. This only has an effect when mode is 'linear', 'bilinear', 'bicubic' or 'trilinear'. Default: False     Note With mode='bicubic', it\u2019s possible to cause overshoot, in other words it can produce negative values or values greater than 255 for images. Explicitly call result.clamp(min=0, max=255) if you want to reduce the overshoot when displaying the image.   Warning With align_corners = True, the linearly interpolating modes (linear, bilinear, and trilinear) don\u2019t proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is align_corners = False. See Upsample for concrete examples on how this affects the outputs.   Note When using the CUDA backend, this operation may induce nondeterministic behaviour in its backward pass that is not easily switched off. Please see the notes on Reproducibility for background.  ", "parameters": ["input (Tensor) : the input tensor", "size (python:int or Tuple[python:int] or Tuple[python:int, python:int] or Tuple[python:int, python:int, python:int]) : output spatial size.", "scale_factor (python:float or Tuple[python:float]) : multiplier for spatial size. Has to match input size if it is a tuple.", "mode (str) : algorithm used for upsampling:'nearest' | 'linear' | 'bilinear' | 'bicubic' |'trilinear' | 'area'. Default: 'nearest'", "align_corners (bool, optional) : Geometrically, we consider the pixels of theinput and output as squares rather than points.If set to True, the input and output tensors are aligned by thecenter points of their corner pixels, preserving the values at the corner pixels.If set to False, the input and output tensors are aligned by the cornerpoints of their corner pixels, and the interpolation uses edge value paddingfor out-of-boundary values, making this operation independent of input sizewhen scale_factor is kept the same. This only has an effect when modeis 'linear', 'bilinear', 'bicubic' or 'trilinear'.Default: False"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.upsample", "item_type": "function", "code": "torch.nn.functional.upsample(input,size=None,scale_factor=None,mode='nearest',align_corners=None)", "description": "Upsamples the input to either the given size or the given scale_factor  Warning This function is deprecated in favor of torch.nn.functional.interpolate(). This is equivalent with nn.functional.interpolate(...).   Note When using the CUDA backend, this operation may induce nondeterministic behaviour in its backward pass that is not easily switched off. Please see the notes on Reproducibility for background.  The algorithm used for upsampling is determined by mode. Currently temporal, spatial and volumetric upsampling are supported, i.e. expected inputs are 3-D, 4-D or 5-D in shape. The input dimensions are interpreted in the form: mini-batch x channels x [optional depth] x [optional height] x width. The modes available for upsampling are: nearest, linear (3D-only), bilinear, bicubic (4D-only), trilinear (5D-only)  Parameters  input (Tensor) \u2013 the input tensor size (python:int or Tuple[python:int] or Tuple[python:int, python:int] or Tuple[python:int, python:int, python:int]) \u2013 output spatial size. scale_factor (python:float or Tuple[python:float]) \u2013 multiplier for spatial size. Has to be an integer. mode (string) \u2013 algorithm used for upsampling: 'nearest' | 'linear' | 'bilinear' | 'bicubic' | 'trilinear'. Default: 'nearest' align_corners (bool, optional) \u2013 Geometrically, we consider the pixels of the input and output as squares rather than points. If set to True, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to False, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when scale_factor is kept the same. This only has an effect when mode is 'linear', 'bilinear', 'bicubic' or 'trilinear'. Default: False     Note With mode='bicubic', it\u2019s possible to cause overshoot, in other words it can produce negative values or values greater than 255 for images. Explicitly call result.clamp(min=0, max=255) if you want to reduce the overshoot when displaying the image.   Warning With align_corners = True, the linearly interpolating modes (linear, bilinear, and trilinear) don\u2019t proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is align_corners = False. See Upsample for concrete examples on how this affects the outputs.  ", "parameters": ["input (Tensor) : the input tensor", "size (python:int or Tuple[python:int] or Tuple[python:int, python:int] or Tuple[python:int, python:int, python:int]) : output spatial size.", "scale_factor (python:float or Tuple[python:float]) : multiplier for spatial size. Has to be an integer.", "mode (string) : algorithm used for upsampling:'nearest' | 'linear' | 'bilinear' | 'bicubic' |'trilinear'. Default: 'nearest'", "align_corners (bool, optional) : Geometrically, we consider the pixels of theinput and output as squares rather than points.If set to True, the input and output tensors are aligned by thecenter points of their corner pixels, preserving the values at the corner pixels.If set to False, the input and output tensors are aligned by the cornerpoints of their corner pixels, and the interpolation uses edge value paddingfor out-of-boundary values, making this operation independent of input sizewhen scale_factor is kept the same. This only has an effect when modeis 'linear', 'bilinear', 'bicubic' or 'trilinear'.Default: False"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.normal(mean,std,*,generator=None,out=None)\u2192Tensor", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.normal(mean=0.0,std,out=None)\u2192Tensor", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.normal(mean,std=1.0,out=None)\u2192Tensor", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.normal(mean,std,size,*,out=None)\u2192Tensor", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.rand", "item_type": "function", "code": "torch.rand(*size,out=None,dtype=None,layout=torch.strided,device=None,requires_grad=False)\u2192Tensor", "description": "Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)   The shape of the tensor is defined by the variable argument size.  Parameters  size (python:int...) \u2013 a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple. out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor. Default: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: &gt;&gt;&gt; torch.rand(4) tensor([ 0.5204,  0.2503,  0.3525,  0.5673]) &gt;&gt;&gt; torch.rand(2, 3) tensor([[ 0.8237,  0.5781,  0.6879],         [ 0.3816,  0.7249,  0.0998]])   ", "parameters": ["size (python:int...) : a sequence of integers defining the shape of the output tensor.Can be a variable number of arguments or a collection like a list or tuple.", "out (Tensor, optional) : the output tensor.", "dtype (torch.dtype, optional) : the desired data type of returned tensor.Default: if None, uses a global default (see torch.set_default_tensor_type()).", "layout (torch.layout, optional) : the desired layout of returned Tensor.Default: torch.strided.", "device (torch.device, optional) : the desired device of returned tensor.Default: if None, uses the current device for the default tensor type(see torch.set_default_tensor_type()). device will be the CPUfor CPU tensor types and the current CUDA device for CUDA tensor types.", "requires_grad (bool, optional) : If autograd should record operations on thereturned tensor. Default: False."], "returns": null, "example": " torch.rand(4)\ntensor([ 0.5204,  0.2503,  0.3525,  0.5673])\n torch.rand(2, 3)\ntensor([[ 0.8237,  0.5781,  0.6879],\n        [ 0.3816,  0.7249,  0.0998]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.rand_like", "item_type": "function", "code": "torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False)\u2192Tensor", "description": "Returns a tensor with the same size as input that is filled with random numbers from a uniform distribution on the interval [0,1)[0, 1)[0,1)  . torch.rand_like(input) is equivalent to torch.rand(input.size(), dtype=input.dtype, layout=input.layout, device=input.device).  Parameters  input (Tensor) \u2013 the size of input will determine size of the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor. Default: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor. Default: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    ", "parameters": ["input (Tensor) : the size of input will determine size of the output tensor.", "dtype (torch.dtype, optional) : the desired data type of returned Tensor.Default: if None, defaults to the dtype of input.", "layout (torch.layout, optional) : the desired layout of returned tensor.Default: if None, defaults to the layout of input.", "device (torch.device, optional) : the desired device of returned tensor.Default: if None, defaults to the device of input.", "requires_grad (bool, optional) : If autograd should record operations on thereturned tensor. Default: False."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.randint", "item_type": "function", "code": "torch.randint(low=0,high,size,*,generator=None,out=None,dtype=None,layout=torch.strided,device=None,requires_grad=False)\u2192Tensor", "description": "Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive). The shape of the tensor is defined by the variable argument size.  Parameters  low (python:int, optional) \u2013 Lowest integer to be drawn from the distribution. Default: 0. high (python:int) \u2013 One above the highest integer to be drawn from the distribution. size (tuple) \u2013 a tuple defining the shape of the output tensor. generator (torch.Generator, optional) \u2013 a pseudorandom number generator for sampling out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor. Default: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: &gt;&gt;&gt; torch.randint(3, 5, (3,)) tensor([4, 3, 4])   &gt;&gt;&gt; torch.randint(10, (2, 2)) tensor([[0, 2],         [5, 5]])   &gt;&gt;&gt; torch.randint(3, 10, (2, 2)) tensor([[4, 5],         [6, 7]])   ", "parameters": ["low (python:int, optional) : Lowest integer to be drawn from the distribution. Default: 0.", "high (python:int) : One above the highest integer to be drawn from the distribution.", "size (tuple) : a tuple defining the shape of the output tensor.", "generator (torch.Generator, optional) : a pseudorandom number generator for sampling", "out (Tensor, optional) : the output tensor.", "dtype (torch.dtype, optional) : the desired data type of returned tensor.Default: if None, uses a global default (see torch.set_default_tensor_type()).", "layout (torch.layout, optional) : the desired layout of returned Tensor.Default: torch.strided.", "device (torch.device, optional) : the desired device of returned tensor.Default: if None, uses the current device for the default tensor type(see torch.set_default_tensor_type()). device will be the CPUfor CPU tensor types and the current CUDA device for CUDA tensor types.", "requires_grad (bool, optional) : If autograd should record operations on thereturned tensor. Default: False."], "returns": null, "example": " torch.randint(3, 5, (3,))\ntensor([4, 3, 4])\n\n\n torch.randint(10, (2, 2))\ntensor([[0, 2],\n        [5, 5]])\n\n\n torch.randint(3, 10, (2, 2))\ntensor([[4, 5],\n        [6, 7]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.studentT.StudentT.expand", "item_type": "method", "code": "expand(batch_shape,_instance=None)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.studentT.StudentT.log_prob", "item_type": "method", "code": "log_prob(value)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.studentT.StudentT.mean", "item_type": "method", "code": "propertymean", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.studentT.StudentT.rsample", "item_type": "method", "code": "rsample(sample_shape=torch.Size([]))", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.studentT.StudentT.variance", "item_type": "method", "code": "propertyvariance", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.transformed_distribution.TransformedDistribution.cdf", "item_type": "method", "code": "cdf(value)", "description": "Computes the cumulative distribution function by inverting the transform(s) and computing the score of the base distribution. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.transformed_distribution.TransformedDistribution.expand", "item_type": "method", "code": "expand(batch_shape,_instance=None)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.transformed_distribution.TransformedDistribution.has_rsample", "item_type": "method", "code": "propertyhas_rsample", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.transformed_distribution.TransformedDistribution.icdf", "item_type": "method", "code": "icdf(value)", "description": "Computes the inverse cumulative distribution function using transform(s) and computing the score of the base distribution. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.transformed_distribution.TransformedDistribution.log_prob", "item_type": "method", "code": "log_prob(value)", "description": "Scores the sample by inverting the transform(s) and computing the score using the score of the base distribution and the log abs det jacobian. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.transformed_distribution.TransformedDistribution.rsample", "item_type": "method", "code": "rsample(sample_shape=torch.Size([]))", "description": "Generates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched. Samples first from base distribution and applies transform() for every transform in the list. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.transformed_distribution.TransformedDistribution.sample", "item_type": "method", "code": "sample(sample_shape=torch.Size([]))", "description": "Generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched. Samples first from base distribution and applies transform() for every transform in the list. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.TransformerDecoder.forward", "item_type": "method", "code": "forward(tgt,memory,tgt_mask=None,memory_mask=None,tgt_key_padding_mask=None,memory_key_padding_mask=None)", "description": "Pass the inputs (and mask) through the decoder layer in turn.  Parameters  tgt \u2013 the sequence to the decoder (required). memory \u2013 the sequnce from the last layer of the encoder (required). tgt_mask \u2013 the mask for the tgt sequence (optional). memory_mask \u2013 the mask for the memory sequence (optional). tgt_key_padding_mask \u2013 the mask for the tgt keys per batch (optional). memory_key_padding_mask \u2013 the mask for the memory keys per batch (optional).     Shape:see the docs in Transformer class.   ", "parameters": ["tgt : the sequence to the decoder (required).", "memory : the sequnce from the last layer of the encoder (required).", "tgt_mask : the mask for the tgt sequence (optional).", "memory_mask : the mask for the memory sequence (optional).", "tgt_key_padding_mask : the mask for the tgt keys per batch (optional).", "memory_key_padding_mask : the mask for the memory keys per batch (optional)."], "returns": null, "example": "NA", "shape": "see the docs in Transformer class. "},
{"library": "torch", "item_id": "torch.nn.TransformerEncoderLayer.forward", "item_type": "method", "code": "forward(src,src_mask=None,src_key_padding_mask=None)", "description": "Pass the input through the encoder layer.  Parameters  src \u2013 the sequnce to the encoder layer (required). src_mask \u2013 the mask for the src sequence (optional). src_key_padding_mask \u2013 the mask for the src keys per batch (optional).     Shape:see the docs in Transformer class.   ", "parameters": ["src : the sequnce to the encoder layer (required).", "src_mask : the mask for the src sequence (optional).", "src_key_padding_mask : the mask for the src keys per batch (optional)."], "returns": null, "example": "NA", "shape": "see the docs in Transformer class. "},
{"library": "torch", "item_id": "torch.nn.TransformerDecoderLayer.forward", "item_type": "method", "code": "forward(tgt,memory,tgt_mask=None,memory_mask=None,tgt_key_padding_mask=None,memory_key_padding_mask=None)", "description": "Pass the inputs (and mask) through the decoder layer.  Parameters  tgt \u2013 the sequence to the decoder layer (required). memory \u2013 the sequnce from the last layer of the encoder (required). tgt_mask \u2013 the mask for the tgt sequence (optional). memory_mask \u2013 the mask for the memory sequence (optional). tgt_key_padding_mask \u2013 the mask for the tgt keys per batch (optional). memory_key_padding_mask \u2013 the mask for the memory keys per batch (optional).     Shape:see the docs in Transformer class.   ", "parameters": ["tgt : the sequence to the decoder layer (required).", "memory : the sequnce from the last layer of the encoder (required).", "tgt_mask : the mask for the tgt sequence (optional).", "memory_mask : the mask for the memory sequence (optional).", "tgt_key_padding_mask : the mask for the tgt keys per batch (optional).", "memory_key_padding_mask : the mask for the memory keys per batch (optional)."], "returns": null, "example": "NA", "shape": "see the docs in Transformer class. "},
{"library": "torch", "item_id": "torch.nn.Embedding.from_pretrained", "item_type": "method", "code": "classmethodfrom_pretrained(embeddings,freeze=True,padding_idx=None,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,sparse=False)", "description": "Creates Embedding instance from given 2-dimensional FloatTensor.  Parameters  embeddings (Tensor) \u2013 FloatTensor containing weights for the Embedding. First dimension is being passed to Embedding as num_embeddings, second as embedding_dim. freeze (boolean, optional) \u2013 If True, the tensor does not get updated in the learning process. Equivalent to embedding.weight.requires_grad = False. Default: True padding_idx (python:int, optional) \u2013 See module initialization documentation. max_norm (python:float, optional) \u2013 See module initialization documentation. norm_type (python:float, optional) \u2013 See module initialization documentation. Default 2. scale_grad_by_freq (boolean, optional) \u2013 See module initialization documentation. Default False. sparse (bool, optional) \u2013 See module initialization documentation.    Examples: &gt;&gt;&gt; # FloatTensor containing pretrained weights &gt;&gt;&gt; weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]]) &gt;&gt;&gt; embedding = nn.Embedding.from_pretrained(weight) &gt;&gt;&gt; # Get embeddings for index 1 &gt;&gt;&gt; input = torch.LongTensor([1]) &gt;&gt;&gt; embedding(input) tensor([[ 4.0000,  5.1000,  6.3000]])   ", "parameters": ["embeddings (Tensor) : FloatTensor containing weights for the Embedding.First dimension is being passed to Embedding as num_embeddings, second as embedding_dim.", "freeze (boolean, optional) : If True, the tensor does not get updated in the learning process.Equivalent to embedding.weight.requires_grad = False. Default: True", "padding_idx (python:int, optional) : See module initialization documentation.", "max_norm (python:float, optional) : See module initialization documentation.", "norm_type (python:float, optional) : See module initialization documentation. Default 2.", "scale_grad_by_freq (boolean, optional) : See module initialization documentation. Default False.", "sparse (bool, optional) : See module initialization documentation."], "returns": null, "example": " # FloatTensor containing pretrained weights\n weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])\n embedding = nn.Embedding.from_pretrained(weight)\n # Get embeddings for index 1\n input = torch.LongTensor([1])\n embedding(input)\ntensor([[ 4.0000,  5.1000,  6.3000]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.EmbeddingBag.from_pretrained", "item_type": "method", "code": "classmethodfrom_pretrained(embeddings,freeze=True,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,mode='mean',sparse=False)", "description": "Creates EmbeddingBag instance from given 2-dimensional FloatTensor.  Parameters  embeddings (Tensor) \u2013 FloatTensor containing weights for the EmbeddingBag. First dimension is being passed to EmbeddingBag as \u2018num_embeddings\u2019, second as \u2018embedding_dim\u2019. freeze (boolean, optional) \u2013 If True, the tensor does not get updated in the learning process. Equivalent to embeddingbag.weight.requires_grad = False. Default: True max_norm (python:float, optional) \u2013 See module initialization documentation. Default: None norm_type (python:float, optional) \u2013 See module initialization documentation. Default 2. scale_grad_by_freq (boolean, optional) \u2013 See module initialization documentation. Default False. mode (string, optional) \u2013 See module initialization documentation. Default: \"mean\" sparse (bool, optional) \u2013 See module initialization documentation. Default: False.    Examples: &gt;&gt;&gt; # FloatTensor containing pretrained weights &gt;&gt;&gt; weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]]) &gt;&gt;&gt; embeddingbag = nn.EmbeddingBag.from_pretrained(weight) &gt;&gt;&gt; # Get embeddings for index 1 &gt;&gt;&gt; input = torch.LongTensor([[1, 0]]) &gt;&gt;&gt; embeddingbag(input) tensor([[ 2.5000,  3.7000,  4.6500]])   ", "parameters": ["embeddings (Tensor) : FloatTensor containing weights for the EmbeddingBag.First dimension is being passed to EmbeddingBag as \u2018num_embeddings\u2019, second as \u2018embedding_dim\u2019.", "freeze (boolean, optional) : If True, the tensor does not get updated in the learning process.Equivalent to embeddingbag.weight.requires_grad = False. Default: True", "max_norm (python:float, optional) : See module initialization documentation. Default: None", "norm_type (python:float, optional) : See module initialization documentation. Default 2.", "scale_grad_by_freq (boolean, optional) : See module initialization documentation. Default False.", "mode (string, optional) : See module initialization documentation. Default: \"mean\"", "sparse (bool, optional) : See module initialization documentation. Default: False."], "returns": null, "example": " # FloatTensor containing pretrained weights\n weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])\n embeddingbag = nn.EmbeddingBag.from_pretrained(weight)\n # Get embeddings for index 1\n input = torch.LongTensor([[1, 0]])\n embeddingbag(input)\ntensor([[ 2.5000,  3.7000,  4.6500]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.long", "item_type": "method", "code": "long()\u2192Tensor", "description": "self.long() is equivalent to self.to(torch.int64). See to(). ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.lstsq", "item_type": "method", "code": "lstsq(A)-&gt;(Tensor,Tensor)", "description": "See torch.lstsq() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.lt", "item_type": "method", "code": "lt(other)\u2192Tensor", "description": "See torch.lt() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.lt_", "item_type": "method", "code": "lt_(other)\u2192Tensor", "description": "In-place version of lt() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.lu", "item_type": "method", "code": "lu(pivot=True,get_infos=False)", "description": "See torch.lu() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.lu_solve", "item_type": "method", "code": "lu_solve(LU_data,LU_pivots)\u2192Tensor", "description": "See torch.lu_solve() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.map_", "item_type": "method", "code": "map_(tensor,callable)", "description": "Applies callable for each element in self tensor and the given tensor and stores the results in self tensor. self tensor and the given tensor must be broadcastable. The callable should have the signature: def callable(a, b) -&gt; number   ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.masked_scatter_", "item_type": "method", "code": "masked_scatter_(mask,source)", "description": "Copies elements from source into self tensor at positions where the mask is True. The shape of mask must be broadcastable with the shape of the underlying tensor. The source should have at least as many elements as the number of ones in mask  Parameters  mask (BoolTensor) \u2013 the boolean mask source (Tensor) \u2013 the tensor to copy from     Note The mask operates on the self tensor, not on the given source tensor.  ", "parameters": ["mask (BoolTensor) : the boolean mask", "source (Tensor) : the tensor to copy from"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.masked_scatter", "item_type": "method", "code": "masked_scatter(mask,tensor)\u2192Tensor", "description": "Out-of-place version of torch.Tensor.masked_scatter_() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.masked_fill_", "item_type": "method", "code": "masked_fill_(mask,value)", "description": "Fills elements of self tensor with value where mask is True. The shape of mask must be broadcastable with the shape of the underlying tensor.  Parameters  mask (BoolTensor) \u2013 the boolean mask value (python:float) \u2013 the value to fill in with    ", "parameters": ["mask (BoolTensor) : the boolean mask", "value (python:float) : the value to fill in with"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.masked_fill", "item_type": "method", "code": "masked_fill(mask,value)\u2192Tensor", "description": "Out-of-place version of torch.Tensor.masked_fill_() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.upsample_nearest", "item_type": "function", "code": "torch.nn.functional.upsample_nearest(input,size=None,scale_factor=None)", "description": "Upsamples the input, using nearest neighbours\u2019 pixel values.  Warning This function is deprecated in favor of torch.nn.functional.interpolate(). This is equivalent with nn.functional.interpolate(..., mode='nearest').  Currently spatial and volumetric upsampling are supported (i.e. expected inputs are 4 or 5 dimensional).  Parameters  input (Tensor) \u2013 input size (python:int or Tuple[python:int, python:int] or Tuple[python:int, python:int, python:int]) \u2013 output spatia size. scale_factor (python:int) \u2013 multiplier for spatial size. Has to be an integer.     Note When using the CUDA backend, this operation may induce nondeterministic behaviour in its backward pass that is not easily switched off. Please see the notes on Reproducibility for background.  ", "parameters": ["input (Tensor) : input", "size (python:int or Tuple[python:int, python:int] or Tuple[python:int, python:int, python:int]) : output spatiasize.", "scale_factor (python:int) : multiplier for spatial size. Has to be an integer."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.upsample_bilinear", "item_type": "function", "code": "torch.nn.functional.upsample_bilinear(input,size=None,scale_factor=None)", "description": "Upsamples the input, using bilinear upsampling.  Warning This function is deprecated in favor of torch.nn.functional.interpolate(). This is equivalent with nn.functional.interpolate(..., mode='bilinear', align_corners=True).  Expected inputs are spatial (4 dimensional). Use upsample_trilinear fo volumetric (5 dimensional) inputs.  Parameters  input (Tensor) \u2013 input size (python:int or Tuple[python:int, python:int]) \u2013 output spatial size. scale_factor (python:int or Tuple[python:int, python:int]) \u2013 multiplier for spatial size     Note When using the CUDA backend, this operation may induce nondeterministic behaviour in its backward pass that is not easily switched off. Please see the notes on Reproducibility for background.  ", "parameters": ["input (Tensor) : input", "size (python:int or Tuple[python:int, python:int]) : output spatial size.", "scale_factor (python:int or Tuple[python:int, python:int]) : multiplier for spatial size"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.grid_sample", "item_type": "function", "code": "torch.nn.functional.grid_sample(input,grid,mode='bilinear',padding_mode='zeros',align_corners=None)", "description": "Given an input and a flow-field grid, computes the output using input values and pixel locations from grid. Currently, only spatial (4-D) and volumetric (5-D) input are supported. In the spatial (4-D) case, for input with shape (N,C,Hin,Win)(N, C, H_\\text{in}, W_\\text{in})(N,C,Hin\u200b,Win\u200b)   and grid with shape (N,Hout,Wout,2)(N, H_\\text{out}, W_\\text{out}, 2)(N,Hout\u200b,Wout\u200b,2)  , the output will have shape (N,C,Hout,Wout)(N, C, H_\\text{out}, W_\\text{out})(N,C,Hout\u200b,Wout\u200b)  . For each output location output[n, :, h, w], the size-2 vector grid[n, h, w] specifies input pixel locations x and y, which are used to interpolate the output value output[n, :, h, w]. In the case of 5D inputs, grid[n, d, h, w] specifies the x, y, z pixel locations for interpolating output[n, :, d, h, w]. mode argument specifies nearest or bilinear interpolation method to sample the input pixels. grid specifies the sampling pixel locations normalized by the input spatial dimensions. Therefore, it should have most values in the range of [-1, 1]. For example, values x = -1, y = -1 is the left-top pixel of input, and values  x = 1, y = 1 is the right-bottom pixel of input. If grid has values outside the range of [-1, 1], the corresponding outputs are handled as defined by padding_mode. Options are   padding_mode=\"zeros\": use 0 for out-of-bound grid locations, padding_mode=\"border\": use border values for out-of-bound grid locations, padding_mode=\"reflection\": use values at locations reflected by the border for out-of-bound grid locations. For location far away from the border, it will keep being reflected until becoming in bound, e.g., (normalized) pixel location x = -3.5 reflects by border -1 and becomes x' = 1.5, then reflects by border 1 and becomes x'' = -0.5.    Note This function is often used in conjunction with affine_grid() to build Spatial Transformer Networks .   Note When using the CUDA backend, this operation may induce nondeterministic behaviour in its backward pass that is not easily switched off. Please see the notes on Reproducibility for background.   Parameters  input (Tensor) \u2013 input of shape (N,C,Hin,Win)(N, C, H_\\text{in}, W_\\text{in})(N,C,Hin\u200b,Win\u200b)   (4-D case) or (N,C,Din,Hin,Win)(N, C, D_\\text{in}, H_\\text{in}, W_\\text{in})(N,C,Din\u200b,Hin\u200b,Win\u200b)   (5-D case) grid (Tensor) \u2013 flow-field of shape (N,Hout,Wout,2)(N, H_\\text{out}, W_\\text{out}, 2)(N,Hout\u200b,Wout\u200b,2)   (4-D case) or (N,Dout,Hout,Wout,3)(N, D_\\text{out}, H_\\text{out}, W_\\text{out}, 3)(N,Dout\u200b,Hout\u200b,Wout\u200b,3)   (5-D case) mode (str) \u2013 interpolation mode to calculate output values 'bilinear' | 'nearest'. Default: 'bilinear' padding_mode (str) \u2013 padding mode for outside grid values 'zeros' | 'border' | 'reflection'. Default: 'zeros' align_corners (bool, optional) \u2013 Geometrically, we consider the pixels of the input  as squares rather than points. If set to True, the extrema (-1 and 1) are considered as referring to the center points of the input\u2019s corner pixels. If set to False, they are instead considered as referring to the corner points of the input\u2019s corner pixels, making the sampling more resolution agnostic. This option parallels the align_corners option in interpolate(), and so whichever option is used here should also be used there to resize the input image before grid sampling. Default: False   Returns output Tensor  Return type output (Tensor)    Warning When align_corners = True, the grid positions depend on the pixel size relative to the input image size, and so the locations sampled by grid_sample() will differ for the same input given at different resolutions (that is, after being upsampled or downsampled). The default behavior up to version 1.2.0 was align_corners = True. Since then, the default behavior has been changed to align_corners = False, in order to bring it in line with the default for interpolate().  ", "parameters": ["input (Tensor) : input of shape (N,C,Hin,Win)(N, C, H_\\text{in}, W_\\text{in})(N,C,Hin\u200b,Win\u200b) (4-D case)or (N,C,Din,Hin,Win)(N, C, D_\\text{in}, H_\\text{in}, W_\\text{in})(N,C,Din\u200b,Hin\u200b,Win\u200b) (5-D case)", "grid (Tensor) : flow-field of shape (N,Hout,Wout,2)(N, H_\\text{out}, W_\\text{out}, 2)(N,Hout\u200b,Wout\u200b,2) (4-D case)or (N,Dout,Hout,Wout,3)(N, D_\\text{out}, H_\\text{out}, W_\\text{out}, 3)(N,Dout\u200b,Hout\u200b,Wout\u200b,3) (5-D case)", "mode (str) : interpolation mode to calculate output values'bilinear' | 'nearest'. Default: 'bilinear'", "padding_mode (str) : padding mode for outside grid values'zeros' | 'border' | 'reflection'. Default: 'zeros'", "align_corners (bool, optional) : Geometrically, we consider the pixels of theinput  as squares rather than points.If set to True, the extrema (-1 and 1) are considered as referringto the center points of the input\u2019s corner pixels. If set to False, theyare instead considered as referring to the corner points of the input\u2019s cornerpixels, making the sampling more resolution agnostic.This option parallels the align_corners option ininterpolate(), and so whichever option is used hereshould also be used there to resize the input image before grid sampling.Default: False"], "returns": "output Tensor", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.randint_like", "item_type": "function", "code": "torch.randint_like(input,low=0,high,dtype=None,layout=torch.strided,device=None,requires_grad=False)\u2192Tensor", "description": "Returns a tensor with the same shape as Tensor input filled with random integers generated uniformly between low (inclusive) and high (exclusive).  Parameters  input (Tensor) \u2013 the size of input will determine size of the output tensor. low (python:int, optional) \u2013 Lowest integer to be drawn from the distribution. Default: 0. high (python:int) \u2013 One above the highest integer to be drawn from the distribution. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor. Default: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor. Default: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    ", "parameters": ["input (Tensor) : the size of input will determine size of the output tensor.", "low (python:int, optional) : Lowest integer to be drawn from the distribution. Default: 0.", "high (python:int) : One above the highest integer to be drawn from the distribution.", "dtype (torch.dtype, optional) : the desired data type of returned Tensor.Default: if None, defaults to the dtype of input.", "layout (torch.layout, optional) : the desired layout of returned tensor.Default: if None, defaults to the layout of input.", "device (torch.device, optional) : the desired device of returned tensor.Default: if None, defaults to the device of input.", "requires_grad (bool, optional) : If autograd should record operations on thereturned tensor. Default: False."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.randn", "item_type": "function", "code": "torch.randn(*size,out=None,dtype=None,layout=torch.strided,device=None,requires_grad=False)\u2192Tensor", "description": "Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 (also called the standard normal distribution).  outi\u223cN(0,1)\\text{out}_{i} \\sim \\mathcal{N}(0, 1)  outi\u200b\u223cN(0,1)  The shape of the tensor is defined by the variable argument size.  Parameters  size (python:int...) \u2013 a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple. out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor. Default: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: &gt;&gt;&gt; torch.randn(4) tensor([-2.1436,  0.9966,  2.3426, -0.6366]) &gt;&gt;&gt; torch.randn(2, 3) tensor([[ 1.5954,  2.8929, -1.0923],         [ 1.1719, -0.4709, -0.1996]])   ", "parameters": ["size (python:int...) : a sequence of integers defining the shape of the output tensor.Can be a variable number of arguments or a collection like a list or tuple.", "out (Tensor, optional) : the output tensor.", "dtype (torch.dtype, optional) : the desired data type of returned tensor.Default: if None, uses a global default (see torch.set_default_tensor_type()).", "layout (torch.layout, optional) : the desired layout of returned Tensor.Default: torch.strided.", "device (torch.device, optional) : the desired device of returned tensor.Default: if None, uses the current device for the default tensor type(see torch.set_default_tensor_type()). device will be the CPUfor CPU tensor types and the current CUDA device for CUDA tensor types.", "requires_grad (bool, optional) : If autograd should record operations on thereturned tensor. Default: False."], "returns": null, "example": " torch.randn(4)\ntensor([-2.1436,  0.9966,  2.3426, -0.6366])\n torch.randn(2, 3)\ntensor([[ 1.5954,  2.8929, -1.0923],\n        [ 1.1719, -0.4709, -0.1996]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.randn_like", "item_type": "function", "code": "torch.randn_like(input,dtype=None,layout=None,device=None,requires_grad=False)\u2192Tensor", "description": "Returns a tensor with the same size as input that is filled with random numbers from a normal distribution with mean 0 and variance 1. torch.randn_like(input) is equivalent to torch.randn(input.size(), dtype=input.dtype, layout=input.layout, device=input.device).  Parameters  input (Tensor) \u2013 the size of input will determine size of the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor. Default: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor. Default: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    ", "parameters": ["input (Tensor) : the size of input will determine size of the output tensor.", "dtype (torch.dtype, optional) : the desired data type of returned Tensor.Default: if None, defaults to the dtype of input.", "layout (torch.layout, optional) : the desired layout of returned tensor.Default: if None, defaults to the layout of input.", "device (torch.device, optional) : the desired device of returned tensor.Default: if None, defaults to the device of input.", "requires_grad (bool, optional) : If autograd should record operations on thereturned tensor. Default: False."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.randperm", "item_type": "function", "code": "torch.randperm(n,out=None,dtype=torch.int64,layout=torch.strided,device=None,requires_grad=False)\u2192LongTensor", "description": "Returns a random permutation of integers from 0 to n - 1.  Parameters  n (python:int) \u2013 the upper bound (exclusive) out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: torch.int64. layout (torch.layout, optional) \u2013 the desired layout of returned Tensor. Default: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: &gt;&gt;&gt; torch.randperm(4) tensor([2, 1, 0, 3])   ", "parameters": ["n (python:int) : the upper bound (exclusive)", "out (Tensor, optional) : the output tensor.", "dtype (torch.dtype, optional) : the desired data type of returned tensor.Default: torch.int64.", "layout (torch.layout, optional) : the desired layout of returned Tensor.Default: torch.strided.", "device (torch.device, optional) : the desired device of returned tensor.Default: if None, uses the current device for the default tensor type(see torch.set_default_tensor_type()). device will be the CPUfor CPU tensor types and the current CUDA device for CUDA tensor types.", "requires_grad (bool, optional) : If autograd should record operations on thereturned tensor. Default: False."], "returns": null, "example": " torch.randperm(4)\ntensor([2, 1, 0, 3])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.transformed_distribution.TransformedDistribution.support", "item_type": "method", "code": "propertysupport", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.uniform.Uniform.cdf", "item_type": "method", "code": "cdf(value)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.uniform.Uniform.entropy", "item_type": "method", "code": "entropy()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.uniform.Uniform.expand", "item_type": "method", "code": "expand(batch_shape,_instance=None)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.uniform.Uniform.icdf", "item_type": "method", "code": "icdf(value)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.uniform.Uniform.log_prob", "item_type": "method", "code": "log_prob(value)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.uniform.Uniform.mean", "item_type": "method", "code": "propertymean", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.uniform.Uniform.rsample", "item_type": "method", "code": "rsample(sample_shape=torch.Size([]))", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.uniform.Uniform.stddev", "item_type": "method", "code": "propertystddev", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.uniform.Uniform.support", "item_type": "method", "code": "propertysupport", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.uniform.Uniform.variance", "item_type": "method", "code": "propertyvariance", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.weibull.Weibull.entropy", "item_type": "method", "code": "entropy()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.weibull.Weibull.expand", "item_type": "method", "code": "expand(batch_shape,_instance=None)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.parallel.DistributedDataParallel.no_sync", "item_type": "method", "code": "no_sync()", "description": "A context manager to disable gradient synchronizations across DDP processes. Within this context, gradients will be accumulated on module variables, which will later be synchronized in the first forward-backward pass exiting the context. Example: &gt;&gt;&gt; ddp = torch.nn.DistributedDataParallel(model, pg) &gt;&gt;&gt; with ddp.no_sync(): ...   for input in inputs: ...     ddp(input).backward()  # no synchronization, accumulate grads ... ddp(another_input).backward()  # synchronize grads   ", "parameters": [], "returns": null, "example": " ddp = torch.nn.DistributedDataParallel(model, pg)\n with ddp.no_sync():\n...   for input in inputs:\n...     ddp(input).backward()  # no synchronization, accumulate grads\n... ddp(another_input).backward()  # synchronize grads\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.BasePruningMethod.apply", "item_type": "method", "code": "classmethodapply(module,name,*args,**kwargs)", "description": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters  module (nn.Module) \u2013 module containing the tensor to prune name (str) \u2013 parameter name within module on which pruning will act. args \u2013 arguments passed on to a subclass of BasePruningMethod kwargs \u2013 keyword arguments passed on to a subclass of a BasePruningMethod    ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "name (str) : parameter name within module on which pruningwill act.", "args : arguments passed on to a subclass ofBasePruningMethod", "kwargs : keyword arguments passed on to a subclass of aBasePruningMethod"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.BasePruningMethod.apply_mask", "item_type": "method", "code": "apply_mask(module)", "description": "Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters module (nn.Module) \u2013 module containing the tensor to prune  Returns pruned version of the input tensor  Return type pruned_tensor (torch.Tensor)   ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)"], "returns": "pruned version of the input tensor", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.BasePruningMethod.compute_mask", "item_type": "method", "code": "abstractcompute_mask(t,default_mask)", "description": "Computes and returns a mask for the input tensor t. Starting from a base default_mask (which should be a mask of ones if the tensor has not been pruned yet), generate a random mask to apply on top of the default_mask according to the specific pruning method recipe.  Parameters  t (torch.Tensor) \u2013 tensor representing the parameter to prune default_mask (torch.Tensor) \u2013 Base mask from previous pruning iterations, that need to be respected after the new mask is applied. Same dims as t.   Returns mask to apply to t, of same dims as t  Return type mask (torch.Tensor)   ", "parameters": ["t (torch.Tensor) : tensor representing the parameter to prune", "default_mask (torch.Tensor) : Base mask from previous pruningiterations, that need to be respected after the new mask isapplied. Same dims as t."], "returns": "mask to apply to t, of same dims as t", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.BasePruningMethod.prune", "item_type": "method", "code": "prune(t,default_mask=None)", "description": "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters  t (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). default_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns pruned version of tensor t.   ", "parameters": ["t (torch.Tensor) : tensor to prune (of same dimensions asdefault_mask).", "default_mask (torch.Tensor, optional) : mask from previous pruningiteration, if any. To be considered when determining whatportion of the tensor that pruning should act on. If None,default to a mask of ones."], "returns": "pruned version of tensor t.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.BasePruningMethod.remove", "item_type": "method", "code": "remove(module)", "description": "Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!  ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.PruningContainer.add_pruning_method", "item_type": "method", "code": "add_pruning_method(method)", "description": "Adds a child pruning method to the container.  Parameters method (subclass of BasePruningMethod) \u2013 child pruning method to be added to the container.   ", "parameters": ["method (subclass of BasePruningMethod) : child pruning methodto be added to the container."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.PruningContainer.apply", "item_type": "method", "code": "classmethodapply(module,name,*args,**kwargs)", "description": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters  module (nn.Module) \u2013 module containing the tensor to prune name (str) \u2013 parameter name within module on which pruning will act. args \u2013 arguments passed on to a subclass of BasePruningMethod kwargs \u2013 keyword arguments passed on to a subclass of a BasePruningMethod    ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "name (str) : parameter name within module on which pruningwill act.", "args : arguments passed on to a subclass ofBasePruningMethod", "kwargs : keyword arguments passed on to a subclass of aBasePruningMethod"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.masked_select", "item_type": "method", "code": "masked_select(mask)\u2192Tensor", "description": "See torch.masked_select() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.matmul", "item_type": "method", "code": "matmul(tensor2)\u2192Tensor", "description": "See torch.matmul() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.matrix_power", "item_type": "method", "code": "matrix_power(n)\u2192Tensor", "description": "See torch.matrix_power() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.max", "item_type": "method", "code": "max(dim=None,keepdim=False)-&gt;Tensoror(Tensor,Tensor)", "description": "See torch.max() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.mean", "item_type": "method", "code": "mean(dim=None,keepdim=False)-&gt;Tensoror(Tensor,Tensor)", "description": "See torch.mean() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.median", "item_type": "method", "code": "median(dim=None,keepdim=False)-&gt;(Tensor,LongTensor)", "description": "See torch.median() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.min", "item_type": "method", "code": "min(dim=None,keepdim=False)-&gt;Tensoror(Tensor,Tensor)", "description": "See torch.min() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.mm", "item_type": "method", "code": "mm(mat2)\u2192Tensor", "description": "See torch.mm() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.mode", "item_type": "method", "code": "mode(dim=None,keepdim=False)-&gt;(Tensor,LongTensor)", "description": "See torch.mode() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.mul", "item_type": "method", "code": "mul(value)\u2192Tensor", "description": "See torch.mul() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.mul_", "item_type": "method", "code": "mul_(value)", "description": "In-place version of mul() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.multinomial", "item_type": "method", "code": "multinomial(num_samples,replacement=False,*,generator=None)\u2192Tensor", "description": "See torch.multinomial() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.functional.affine_grid", "item_type": "function", "code": "torch.nn.functional.affine_grid(theta,size,align_corners=None)", "description": "Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta.  Note This function is often used in conjunction with grid_sample() to build Spatial Transformer Networks .   Parameters  theta (Tensor) \u2013 input batch of affine matrices with shape (N\u00d72\u00d73N \\times 2 \\times 3N\u00d72\u00d73  ) for 2D or (N\u00d73\u00d74N \\times 3 \\times 4N\u00d73\u00d74  ) for 3D size (torch.Size) \u2013 the target output image size. (N\u00d7C\u00d7H\u00d7WN \\times C \\times H \\times WN\u00d7C\u00d7H\u00d7W   for 2D or N\u00d7C\u00d7D\u00d7H\u00d7WN \\times C \\times D \\times H \\times WN\u00d7C\u00d7D\u00d7H\u00d7W   for 3D) Example: torch.Size((32, 3, 24, 24)) align_corners (bool, optional) \u2013 if True, consider -1 and 1 to refer to the centers of the corner pixels rather than the image corners. Refer to grid_sample() for a more complete description. A grid generated by affine_grid() should be passed to grid_sample() with the same setting for this option. Default: False   Returns output Tensor of size (N\u00d7H\u00d7W\u00d72N \\times H \\times W \\times 2N\u00d7H\u00d7W\u00d72  )  Return type output (Tensor)    Warning When align_corners = True, the grid positions depend on the pixel size relative to the input image size, and so the locations sampled by grid_sample() will differ for the same input given at different resolutions (that is, after being upsampled or downsampled). The default behavior up to version 1.2.0 was align_corners = True. Since then, the default behavior has been changed to align_corners = False, in order to bring it in line with the default for interpolate().   Warning When align_corners = True, 2D affine transforms on 1D data and 3D affine transforms on 2D data (that is, when one of the spatial dimensions has unit size) are ill-defined, and not an intended use case. This is not a problem when align_corners = False. Up to version 1.2.0, all grid points along a unit dimension were considered arbitrarily to be at -1. From version 1.3.0, under align_corners = True all grid points along a unit dimension are condsidered to be at `0 (the center of the input image).  ", "parameters": ["theta (Tensor) : input batch of affine matrices with shape(N\u00d72\u00d73N \\times 2 \\times 3N\u00d72\u00d73) for 2D or(N\u00d73\u00d74N \\times 3 \\times 4N\u00d73\u00d74) for 3D", "size (torch.Size) : the target output image size.(N\u00d7C\u00d7H\u00d7WN \\times C \\times H \\times WN\u00d7C\u00d7H\u00d7W for 2D orN\u00d7C\u00d7D\u00d7H\u00d7WN \\times C \\times D \\times H \\times WN\u00d7C\u00d7D\u00d7H\u00d7W for 3D)Example: torch.Size((32, 3, 24, 24))", "align_corners (bool, optional) : if True, consider -1 and 1to refer to the centers of the corner pixels rather than the image corners.Refer to grid_sample() for a more complete description.A grid generated by affine_grid() should be passed to grid_sample()with the same setting for this option.Default: False"], "returns": "output Tensor of size (N\u00d7H\u00d7W\u00d72N \\times H \\times W \\times 2N\u00d7H\u00d7W\u00d72)", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.parallel.data_parallel", "item_type": "function", "code": "torch.nn.parallel.data_parallel(module,inputs,device_ids=None,output_device=None,dim=0,module_kwargs=None)", "description": "Evaluates module(input) in parallel across the GPUs given in device_ids. This is the functional version of the DataParallel module.  Parameters  module (Module) \u2013 the module to evaluate in parallel inputs (Tensor) \u2013 inputs to the module device_ids (list of python:int or torch.device) \u2013 GPU ids on which to replicate module output_device (list of python:int or torch.device) \u2013 GPU location of the output  Use -1 to indicate the CPU. (default: device_ids[0])   Returns a Tensor containing the result of module(input) located on output_device   ", "parameters": ["module (Module) : the module to evaluate in parallel", "inputs (Tensor) : inputs to the module", "device_ids (list of python:int or torch.device) : GPU ids on which to replicate module", "output_device (list of python:int or torch.device) : GPU location of the output  Use -1 to indicate the CPU.(default: device_ids[0])"], "returns": "a Tensor containing the result of module(input) located onoutput_device", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.save", "item_type": "function", "code": "torch.save(obj,f,pickle_module=&lt;module'pickle'from'/opt/conda/lib/python3.6/pickle.py'&gt;,pickle_protocol=2,_use_new_zipfile_serialization=False)", "description": "Saves an object to a disk file. See also: Recommended approach for saving a model  Parameters  obj \u2013 saved object f \u2013 a file-like object (has to implement write and flush) or a string containing a file name pickle_module \u2013 module used for pickling metadata and objects pickle_protocol \u2013 can be specified to override the default protocol     Warning If you are using Python 2, torch.save() does NOT support StringIO.StringIO as a valid file-like object. This is because the write method should return the number of bytes written; StringIO.write() does not do this. Please use something like io.BytesIO instead.  Example &gt;&gt;&gt; # Save to file &gt;&gt;&gt; x = torch.tensor([0, 1, 2, 3, 4]) &gt;&gt;&gt; torch.save(x, 'tensor.pt') &gt;&gt;&gt; # Save to io.BytesIO buffer &gt;&gt;&gt; buffer = io.BytesIO() &gt;&gt;&gt; torch.save(x, buffer)   ", "parameters": ["obj : saved object", "f : a file-like object (has to implement write and flush) or a stringcontaining a file name", "pickle_module : module used for pickling metadata and objects", "pickle_protocol : can be specified to override the default protocol"], "returns": null, "example": " # Save to file\n x = torch.tensor([0, 1, 2, 3, 4])\n torch.save(x, 'tensor.pt')\n # Save to io.BytesIO buffer\n buffer = io.BytesIO()\n torch.save(x, buffer)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.load", "item_type": "function", "code": "torch.load(f,map_location=None,pickle_module=&lt;module'pickle'from'/opt/conda/lib/python3.6/pickle.py'&gt;,**pickle_load_args)", "description": "Loads an object saved with torch.save() from a file. torch.load() uses Python\u2019s unpickling facilities but treats storages, which underlie tensors, specially. They are first deserialized on the CPU and are then moved to the device they were saved from. If this fails (e.g. because the run time system doesn\u2019t have certain devices), an exception is raised. However, storages can be dynamically remapped to an alternative set of devices using the map_location argument. If map_location is a callable, it will be called once for each serialized storage with two arguments: storage and location. The storage argument will be the initial deserialization of the storage, residing on the CPU. Each serialized storage has a location tag associated with it which identifies the device it was saved from, and this tag is the second argument passed to map_location. The builtin location tags are 'cpu' for CPU tensors and 'cuda:device_id' (e.g. 'cuda:2') for CUDA tensors. map_location should return either None or a storage. If map_location returns a storage, it will be used as the final deserialized object, already moved to the right device. Otherwise, torch.load() will fall back to the default behavior, as if map_location wasn\u2019t specified. If map_location is a torch.device object or a string contraining a device tag, it indicates the location where all tensors should be loaded. Otherwise, if map_location is a dict, it will be used to remap location tags appearing in the file (keys), to ones that specify where to put the storages (values). User extensions can register their own location tags and tagging and deserialization methods using torch.serialization.register_package().  Parameters  f \u2013 a file-like object (has to implement read(), :meth`readline`, :meth`tell`, and :meth`seek`), or a string containing a file name map_location \u2013 a function, torch.device, string or a dict specifying how to remap storage locations pickle_module \u2013 module used for unpickling metadata and objects (has to match the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to pickle_module.load() and pickle_module.Unpickler(), e.g., errors=....     Note When you call torch.load() on a file which contains GPU tensors, those tensors will be loaded to GPU by default. You can call torch.load(.., map_location='cpu') and then load_state_dict() to avoid GPU RAM surge when loading a model checkpoint.   Note By default, we decode byte strings as utf-8.  This is to avoid a common error case UnicodeDecodeError: 'ascii' codec can't decode byte 0x... when loading files saved by Python 2 in Python 3.  If this default is incorrect, you may use an extra encoding keyword argument to specify how these objects should be loaded, e.g., encoding='latin1' decodes them to strings using latin1 encoding, and encoding='bytes' keeps them as byte arrays which can be decoded later with byte_array.decode(...).  Example &gt;&gt;&gt; torch.load('tensors.pt') # Load all tensors onto the CPU &gt;&gt;&gt; torch.load('tensors.pt', map_location=torch.device('cpu')) # Load all tensors onto the CPU, using a function &gt;&gt;&gt; torch.load('tensors.pt', map_location=lambda storage, loc: storage) # Load all tensors onto GPU 1 &gt;&gt;&gt; torch.load('tensors.pt', map_location=lambda storage, loc: storage.cuda(1)) # Map tensors from GPU 1 to GPU 0 &gt;&gt;&gt; torch.load('tensors.pt', map_location={'cuda:1':'cuda:0'}) # Load tensor from io.BytesIO object &gt;&gt;&gt; with open('tensor.pt', 'rb') as f:         buffer = io.BytesIO(f.read()) &gt;&gt;&gt; torch.load(buffer) # Load a module with 'ascii' encoding for unpickling &gt;&gt;&gt; torch.load('module.pt', encoding='ascii')   ", "parameters": ["f : a file-like object (has to implement read(), :meth`readline`, :meth`tell`, and :meth`seek`),or a string containing a file name", "map_location : a function, torch.device, string or a dict specifying how to remap storagelocations", "pickle_module : module used for unpickling metadata and objects (has tomatch the pickle_module used to serialize file)", "pickle_load_args : (Python 3 only) optional keyword arguments passed over topickle_module.load() and pickle_module.Unpickler(), e.g.,errors=...."], "returns": null, "example": " torch.load('tensors.pt')\n# Load all tensors onto the CPU\n torch.load('tensors.pt', map_location=torch.device('cpu'))\n# Load all tensors onto the CPU, using a function\n torch.load('tensors.pt', map_location=lambda storage, loc: storage)\n# Load all tensors onto GPU 1\n torch.load('tensors.pt', map_location=lambda storage, loc: storage.cuda(1))\n# Map tensors from GPU 1 to GPU 0\n torch.load('tensors.pt', map_location={'cuda:1':'cuda:0'})\n# Load tensor from io.BytesIO object\n with open('tensor.pt', 'rb') as f:\n        buffer = io.BytesIO(f.read())\n torch.load(buffer)\n# Load a module with 'ascii' encoding for unpickling\n torch.load('module.pt', encoding='ascii')\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.weibull.Weibull.mean", "item_type": "method", "code": "propertymean", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.weibull.Weibull.variance", "item_type": "method", "code": "propertyvariance", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.transforms.Transform.inv", "item_type": "method", "code": "propertyinv", "description": "Returns the inverse Transform of this transform. This should satisfy t.inv.inv is t. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.transforms.Transform.sign", "item_type": "method", "code": "propertysign", "description": "Returns the sign of the determinant of the Jacobian, if applicable. In general this only makes sense for bijective transforms. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.transforms.Transform.log_abs_det_jacobian", "item_type": "method", "code": "log_abs_det_jacobian(x,y)", "description": "Computes the log det jacobian log |dy/dx| given input and output. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.constraints.Constraint.check", "item_type": "method", "code": "check(value)", "description": "Returns a byte tensor of sample_shape + batch_shape indicating whether each event in value satisfies this constraint. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.constraint_registry.ConstraintRegistry.register", "item_type": "method", "code": "register(constraint,factory=None)", "description": "Registers a Constraint subclass in this registry. Usage: @my_registry.register(MyConstraintClass) def construct_transform(constraint):     assert isinstance(constraint, MyConstraint)     return MyTransform(constraint.arg_constraints)    Parameters  constraint (subclass of Constraint) \u2013 A subclass of Constraint, or a singleton object of the desired class. factory (callable) \u2013 A callable that inputs a constraint object and returns a  Transform object.    ", "parameters": ["constraint (subclass of Constraint) : A subclass of Constraint, ora singleton object of the desired class.", "factory (callable) : A callable that inputs a constraint object and returnsa  Transform object."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.distribution.Distribution", "item_type": "class", "code": "classtorch.distributions.distribution.Distribution(batch_shape=torch.Size([]),event_shape=torch.Size([]),validate_args=None)", "description": "Bases: object Distribution is the abstract base class for probability distributions.   property arg_constraints Returns a dictionary from argument names to Constraint objects that should be satisfied by each argument of this distribution. Args that are not tensors need not appear in this dict.     property batch_shape Returns the shape over which parameters are batched.     cdf(value)  Returns the cumulative density/mass function evaluated at value.  Parameters value (Tensor) \u2013        entropy()  Returns entropy of distribution, batched over batch_shape.  Returns Tensor of shape batch_shape.       enumerate_support(expand=True)  Returns tensor containing all values supported by a discrete distribution. The result will enumerate over dimension 0, so the shape of the result will be (cardinality,) + batch_shape + event_shape (where event_shape = () for univariate distributions). Note that this enumerates over all batched tensors in lock-step [[0, 0], [1, 1], \u2026]. With expand=False, enumeration happens along dim 0, but with the remaining batch dimensions being singleton dimensions, [[0], [1], ... To iterate over the full Cartesian product use itertools.product(m.enumerate_support()).  Parameters expand (bool) \u2013 whether to expand the support over the batch dims to match the distribution\u2019s batch_shape.  Returns Tensor iterating over dimension 0.       property event_shape Returns the shape of a single sample (without batching).     expand(batch_shape, _instance=None)  Returns a new distribution instance (or populates an existing instance provided by a derived class) with batch dimensions expanded to batch_shape. This method calls expand on the distribution\u2019s parameters. As such, this does not allocate new memory for the expanded distribution instance. Additionally, this does not repeat any args checking or parameter broadcasting in __init__.py, when an instance is first created.  Parameters  batch_shape (torch.Size) \u2013 the desired expanded size. _instance \u2013 new instance provided by subclasses that need to override .expand.   Returns New distribution instance with batch dimensions expanded to batch_size.       icdf(value)  Returns the inverse cumulative density/mass function evaluated at value.  Parameters value (Tensor) \u2013        log_prob(value)  Returns the log of the probability density/mass function evaluated at value.  Parameters value (Tensor) \u2013        property mean Returns the mean of the distribution.     perplexity()  Returns perplexity of distribution, batched over batch_shape.  Returns Tensor of shape batch_shape.       rsample(sample_shape=torch.Size([]))  Generates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched.     sample(sample_shape=torch.Size([]))  Generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched.     sample_n(n)  Generates n samples or n batches of samples if the distribution parameters are batched.     property stddev Returns the standard deviation of the distribution.     property support Returns a Constraint object representing this distribution\u2019s support.     property variance Returns the variance of the distribution.   ", "parameters": ["batch_shape (torch.Size) : the desired expanded size.", "_instance : new instance provided by subclasses thatneed to override .expand."], "returns": "Tensor of shape batch_shape.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.PruningContainer.apply_mask", "item_type": "method", "code": "apply_mask(module)", "description": "Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters module (nn.Module) \u2013 module containing the tensor to prune  Returns pruned version of the input tensor  Return type pruned_tensor (torch.Tensor)   ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)"], "returns": "pruned version of the input tensor", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.PruningContainer.compute_mask", "item_type": "method", "code": "compute_mask(t,default_mask)", "description": "Applies the latest method by computing the new partial masks and returning its combination with the default_mask. The new partial mask should be computed on the entries or channels that were not zeroed out by the default_mask. Which portions of the tensor t the new mask will be calculated from depends on the PRUNING_TYPE (handled by the type handler):   for \u2018unstructured\u2019, the mask will be computed from the raveled  list of nonmasked entries;  for \u2018structured\u2019, the mask will be computed from the nonmasked  channels in the tensor;  for \u2018global\u2019, the mask will be computed across all entries.    Parameters  t (torch.Tensor) \u2013 tensor representing the parameter to prune (of same dimensions as default_mask). default_mask (torch.Tensor) \u2013 mask from previous pruning iteration.   Returns new mask that combines the effects of the default_mask and the new mask from the current pruning method (of same dimensions as default_mask and t).  Return type mask (torch.Tensor)   ", "parameters": ["t (torch.Tensor) : tensor representing the parameter to prune(of same dimensions as default_mask).", "default_mask (torch.Tensor) : mask from previous pruning iteration."], "returns": "new mask that combines the effectsof the default_mask and the new mask from the currentpruning method (of same dimensions as default_mask andt).", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.PruningContainer.prune", "item_type": "method", "code": "prune(t,default_mask=None)", "description": "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters  t (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). default_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns pruned version of tensor t.   ", "parameters": ["t (torch.Tensor) : tensor to prune (of same dimensions asdefault_mask).", "default_mask (torch.Tensor, optional) : mask from previous pruningiteration, if any. To be considered when determining whatportion of the tensor that pruning should act on. If None,default to a mask of ones."], "returns": "pruned version of tensor t.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.PruningContainer.remove", "item_type": "method", "code": "remove(module)", "description": "Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!  ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.Identity.apply", "item_type": "method", "code": "classmethodapply(module,name)", "description": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters  module (nn.Module) \u2013 module containing the tensor to prune name (str) \u2013 parameter name within module on which pruning will act.    ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "name (str) : parameter name within module on which pruningwill act."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.Identity.apply_mask", "item_type": "method", "code": "apply_mask(module)", "description": "Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters module (nn.Module) \u2013 module containing the tensor to prune  Returns pruned version of the input tensor  Return type pruned_tensor (torch.Tensor)   ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)"], "returns": "pruned version of the input tensor", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.Identity.prune", "item_type": "method", "code": "prune(t,default_mask=None)", "description": "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters  t (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). default_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns pruned version of tensor t.   ", "parameters": ["t (torch.Tensor) : tensor to prune (of same dimensions asdefault_mask).", "default_mask (torch.Tensor, optional) : mask from previous pruningiteration, if any. To be considered when determining whatportion of the tensor that pruning should act on. If None,default to a mask of ones."], "returns": "pruned version of tensor t.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.mv", "item_type": "method", "code": "mv(vec)\u2192Tensor", "description": "See torch.mv() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.mvlgamma", "item_type": "method", "code": "mvlgamma(p)\u2192Tensor", "description": "See torch.mvlgamma() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.mvlgamma_", "item_type": "method", "code": "mvlgamma_(p)\u2192Tensor", "description": "In-place version of mvlgamma() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.narrow", "item_type": "method", "code": "narrow(dimension,start,length)\u2192Tensor", "description": "See torch.narrow() Example: &gt;&gt;&gt; x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) &gt;&gt;&gt; x.narrow(0, 0, 2) tensor([[ 1,  2,  3],         [ 4,  5,  6]]) &gt;&gt;&gt; x.narrow(1, 1, 2) tensor([[ 2,  3],         [ 5,  6],         [ 8,  9]])   ", "parameters": [], "returns": null, "example": " x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n x.narrow(0, 0, 2)\ntensor([[ 1,  2,  3],\n        [ 4,  5,  6]])\n x.narrow(1, 1, 2)\ntensor([[ 2,  3],\n        [ 5,  6],\n        [ 8,  9]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.narrow_copy", "item_type": "method", "code": "narrow_copy(dimension,start,length)\u2192Tensor", "description": "Same as Tensor.narrow() except returning a copy rather than shared storage.  This is primarily for sparse tensors, which do not have a shared-storage narrow method.  Calling `narrow_copy with `dimemsion &gt; self.sparse_dim()` will return a copy with the relevant dense dimension narrowed, and `self.shape` updated accordingly. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.ndimension", "item_type": "method", "code": "ndimension()\u2192int", "description": "Alias for dim() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.ne", "item_type": "method", "code": "ne(other)\u2192Tensor", "description": "See torch.ne() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.ne_", "item_type": "method", "code": "ne_(other)\u2192Tensor", "description": "In-place version of ne() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.neg", "item_type": "method", "code": "neg()\u2192Tensor", "description": "See torch.neg() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.neg_", "item_type": "method", "code": "neg_()\u2192Tensor", "description": "In-place version of neg() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.nelement", "item_type": "method", "code": "nelement()\u2192int", "description": "Alias for numel() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.nonzero", "item_type": "method", "code": "nonzero()\u2192LongTensor", "description": "See torch.nonzero() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.get_num_threads", "item_type": "function", "code": "torch.get_num_threads()\u2192int", "description": "Returns the number of threads used for parallelizing CPU operations ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.set_num_threads", "item_type": "function", "code": "torch.set_num_threads(int)", "description": "Sets the number of threads used for intraop parallelism on CPU. WARNING: To ensure that the correct number of threads is used, set_num_threads must be called before running eager, JIT or autograd code. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.get_num_interop_threads", "item_type": "function", "code": "torch.get_num_interop_threads()\u2192int", "description": "Returns the number of threads used for inter-op parallelism on CPU (e.g. in JIT interpreter) ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.set_num_interop_threads", "item_type": "function", "code": "torch.set_num_interop_threads(int)", "description": "Sets the number of threads used for interop parallelism (e.g. in JIT interpreter) on CPU. WARNING: Can only be called once and before any inter-op parallel work is started (e.g. JIT execution). ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.abs", "item_type": "function", "code": "torch.abs(input,out=None)\u2192Tensor", "description": "Computes the element-wise absolute value of the given input tensor.  outi=\u2223inputi\u2223\\text{out}_{i} = |\\text{input}_{i}|  outi\u200b=\u2223inputi\u200b\u2223   Parameters  input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; torch.abs(torch.tensor([-1, -2, 3])) tensor([ 1,  2,  3])   ", "parameters": ["input (Tensor) : the input tensor.", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " torch.abs(torch.tensor([-1, -2, 3]))\ntensor([ 1,  2,  3])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.acos", "item_type": "function", "code": "torch.acos(input,out=None)\u2192Tensor", "description": "Returns a new tensor with the arccosine  of the elements of input.  outi=cos\u2061\u22121(inputi)\\text{out}_{i} = \\cos^{-1}(\\text{input}_{i})  outi\u200b=cos\u22121(inputi\u200b)   Parameters  input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a tensor([ 0.3348, -0.5889,  0.2005, -0.1584]) &gt;&gt;&gt; torch.acos(a) tensor([ 1.2294,  2.2004,  1.3690,  1.7298])   ", "parameters": ["input (Tensor) : the input tensor.", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " a = torch.randn(4)\n a\ntensor([ 0.3348, -0.5889,  0.2005, -0.1584])\n torch.acos(a)\ntensor([ 1.2294,  2.2004,  1.3690,  1.7298])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.add", "item_type": "function", "code": "torch.add()", "description": "  torch.add(input, other, out=None)   Adds the scalar other to each element of the input input and returns a new resulting tensor.  out=input+other\\text{out} = \\text{input} + \\text{other}  out=input+other  If input is of type FloatTensor or DoubleTensor, other must be a real number, otherwise it should be an integer.  Parameters  input (Tensor) \u2013 the input tensor. value (Number) \u2013 the number to be added to each element of input   Keyword Arguments out (Tensor, optional) \u2013 the output tensor.   Example: &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a tensor([ 0.0202,  1.0985,  1.3506, -0.6056]) &gt;&gt;&gt; torch.add(a, 20) tensor([ 20.0202,  21.0985,  21.3506,  19.3944])     torch.add(input, alpha=1, other, out=None)   Each element of the tensor other is multiplied by the scalar alpha and added to each element of the tensor input. The resulting tensor is returned. The shapes of input and other must be broadcastable.  out=input+alpha\u00d7other\\text{out} = \\text{input} + \\text{alpha} \\times \\text{other}  out=input+alpha\u00d7other  If other is of type FloatTensor or DoubleTensor, alpha must be a real number, otherwise it should be an integer.  Parameters  input (Tensor) \u2013 the first input tensor alpha (Number) \u2013 the scalar multiplier for other other (Tensor) \u2013 the second input tensor   Keyword Arguments out (Tensor, optional) \u2013 the output tensor.   Example: &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a tensor([-0.9732, -0.3497,  0.6245,  0.4022]) &gt;&gt;&gt; b = torch.randn(4, 1) &gt;&gt;&gt; b tensor([[ 0.3743],         [-1.7724],         [-0.5811],         [-0.8017]]) &gt;&gt;&gt; torch.add(a, 10, b) tensor([[  2.7695,   3.3930,   4.3672,   4.1450],         [-18.6971, -18.0736, -17.0994, -17.3216],         [ -6.7845,  -6.1610,  -5.1868,  -5.4090],         [ -8.9902,  -8.3667,  -7.3925,  -7.6147]])   ", "parameters": ["input (Tensor) : the input tensor.", "value (Number) : the number to be added to each element of input", "input (Tensor) : the first input tensor", "alpha (Number) : the scalar multiplier for other", "other (Tensor) : the second input tensor"], "returns": null, "example": " a = torch.randn(4)\n a\ntensor([ 0.0202,  1.0985,  1.3506, -0.6056])\n torch.add(a, 20)\ntensor([ 20.0202,  21.0985,  21.3506,  19.3944])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.add(input,other,out=None)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.add(input,alpha=1,other,out=None)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.exp_family.ExponentialFamily", "item_type": "class", "code": "classtorch.distributions.exp_family.ExponentialFamily(batch_shape=torch.Size([]),event_shape=torch.Size([]),validate_args=None)", "description": "Bases: torch.distributions.distribution.Distribution ExponentialFamily is the abstract base class for probability distributions belonging to an exponential family, whose probability mass/density function has the form is defined below  pF(x;\u03b8)=exp\u2061(\u27e8t(x),\u03b8\u27e9\u2212F(\u03b8)+k(x))p_{F}(x; \\theta) = \\exp(\\langle t(x), \\theta\\rangle - F(\\theta) + k(x))pF\u200b(x;\u03b8)=exp(\u27e8t(x),\u03b8\u27e9\u2212F(\u03b8)+k(x))  where \u03b8\\theta\u03b8   denotes the natural parameters, t(x)t(x)t(x)   denotes the sufficient statistic, F(\u03b8)F(\\theta)F(\u03b8)   is the log normalizer function for a given family and k(x)k(x)k(x)   is the carrier measure.  Note This class is an intermediary between the Distribution class and distributions which belong to an exponential family mainly to check the correctness of the .entropy() and analytic KL divergence methods. We use this class to compute the entropy and KL divergence using the AD framework and Bregman divergences (courtesy of: Frank Nielsen and Richard Nock, Entropies and Cross-entropies of Exponential Families).    entropy()  Method to compute the entropy using Bregman divergence of the log normalizer.   ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.bernoulli.Bernoulli", "item_type": "class", "code": "classtorch.distributions.bernoulli.Bernoulli(probs=None,logits=None,validate_args=None)", "description": "Bases: torch.distributions.exp_family.ExponentialFamily Creates a Bernoulli distribution parameterized by probs or logits (but not both). Samples are binary (0 or 1). They take the value 1 with probability p and 0 with probability 1 - p. Example: &gt;&gt;&gt; m = Bernoulli(torch.tensor([0.3])) &gt;&gt;&gt; m.sample()  # 30% chance 1; 70% chance 0 tensor([ 0.])    Parameters  probs (Number, Tensor) \u2013 the probability of sampling 1 logits (Number, Tensor) \u2013 the log-odds of sampling 1      arg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)}     entropy()      enumerate_support(expand=True)      expand(batch_shape, _instance=None)      has_enumerate_support = True     log_prob(value)      logits      property mean     property param_shape     probs      sample(sample_shape=torch.Size([]))      support = Boolean()     property variance   ", "parameters": ["probs (Number, Tensor) : the probability of sampling 1", "logits (Number, Tensor) : the log-odds of sampling 1"], "returns": null, "example": " m = Bernoulli(torch.tensor([0.3]))\n m.sample()  # 30% chance 1; 70% chance 0\ntensor([ 0.])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.bernoulli.Bernoulli.arg_constraints", "item_type": "attribute", "code": "arg_constraints={'logits':Real(),'probs':Interval(lower_bound=0.0,upper_bound=1.0)}", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.bernoulli.Bernoulli.has_enumerate_support", "item_type": "attribute", "code": "has_enumerate_support=True", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.bernoulli.Bernoulli.logits", "item_type": "attribute", "code": "logits", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.bernoulli.Bernoulli.probs", "item_type": "attribute", "code": "probs", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.bernoulli.Bernoulli.support", "item_type": "attribute", "code": "support=Boolean()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.beta.Beta", "item_type": "class", "code": "classtorch.distributions.beta.Beta(concentration1,concentration0,validate_args=None)", "description": "Bases: torch.distributions.exp_family.ExponentialFamily Beta distribution parameterized by concentration1 and concentration0. Example: &gt;&gt;&gt; m = Beta(torch.tensor([0.5]), torch.tensor([0.5])) &gt;&gt;&gt; m.sample()  # Beta distributed with concentration concentration1 and concentration0 tensor([ 0.1046])    Parameters  concentration1 (python:float or Tensor) \u2013 1st concentration parameter of the distribution (often referred to as alpha) concentration0 (python:float or Tensor) \u2013 2nd concentration parameter of the distribution (often referred to as beta)      arg_constraints = {'concentration0': GreaterThan(lower_bound=0.0), 'concentration1': GreaterThan(lower_bound=0.0)}     property concentration0     property concentration1     entropy()      expand(batch_shape, _instance=None)      has_rsample = True     log_prob(value)      property mean     rsample(sample_shape=())      support = Interval(lower_bound=0.0, upper_bound=1.0)     property variance   ", "parameters": ["concentration1 (python:float or Tensor) : 1st concentration parameter of the distribution(often referred to as alpha)", "concentration0 (python:float or Tensor) : 2nd concentration parameter of the distribution(often referred to as beta)"], "returns": null, "example": " m = Beta(torch.tensor([0.5]), torch.tensor([0.5]))\n m.sample()  # Beta distributed with concentration concentration1 and concentration0\ntensor([ 0.1046])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.beta.Beta.arg_constraints", "item_type": "attribute", "code": "arg_constraints={'concentration0':GreaterThan(lower_bound=0.0),'concentration1':GreaterThan(lower_bound=0.0)}", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.Identity.remove", "item_type": "method", "code": "remove(module)", "description": "Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!  ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.RandomUnstructured.apply", "item_type": "method", "code": "classmethodapply(module,name,amount)", "description": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters  module (nn.Module) \u2013 module containing the tensor to prune name (str) \u2013 parameter name within module on which pruning will act. amount (python:int or python:float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune.    ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "name (str) : parameter name within module on which pruningwill act.", "amount (python:int or python:float) : quantity of parameters to prune.If float, should be between 0.0 and 1.0 and represent thefraction of parameters to prune. If int, it represents theabsolute number of parameters to prune."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.RandomUnstructured.apply_mask", "item_type": "method", "code": "apply_mask(module)", "description": "Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters module (nn.Module) \u2013 module containing the tensor to prune  Returns pruned version of the input tensor  Return type pruned_tensor (torch.Tensor)   ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)"], "returns": "pruned version of the input tensor", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.RandomUnstructured.prune", "item_type": "method", "code": "prune(t,default_mask=None)", "description": "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters  t (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). default_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns pruned version of tensor t.   ", "parameters": ["t (torch.Tensor) : tensor to prune (of same dimensions asdefault_mask).", "default_mask (torch.Tensor, optional) : mask from previous pruningiteration, if any. To be considered when determining whatportion of the tensor that pruning should act on. If None,default to a mask of ones."], "returns": "pruned version of tensor t.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.RandomUnstructured.remove", "item_type": "method", "code": "remove(module)", "description": "Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!  ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.L1Unstructured.apply", "item_type": "method", "code": "classmethodapply(module,name,amount)", "description": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters  module (nn.Module) \u2013 module containing the tensor to prune name (str) \u2013 parameter name within module on which pruning will act. amount (python:int or python:float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune.    ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "name (str) : parameter name within module on which pruningwill act.", "amount (python:int or python:float) : quantity of parameters to prune.If float, should be between 0.0 and 1.0 and represent thefraction of parameters to prune. If int, it represents theabsolute number of parameters to prune."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.L1Unstructured.apply_mask", "item_type": "method", "code": "apply_mask(module)", "description": "Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters module (nn.Module) \u2013 module containing the tensor to prune  Returns pruned version of the input tensor  Return type pruned_tensor (torch.Tensor)   ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)"], "returns": "pruned version of the input tensor", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.L1Unstructured.prune", "item_type": "method", "code": "prune(t,default_mask=None)", "description": "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters  t (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). default_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns pruned version of tensor t.   ", "parameters": ["t (torch.Tensor) : tensor to prune (of same dimensions asdefault_mask).", "default_mask (torch.Tensor, optional) : mask from previous pruningiteration, if any. To be considered when determining whatportion of the tensor that pruning should act on. If None,default to a mask of ones."], "returns": "pruned version of tensor t.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.L1Unstructured.remove", "item_type": "method", "code": "remove(module)", "description": "Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!  ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.norm", "item_type": "method", "code": "norm(p='fro',dim=None,keepdim=False,dtype=None)", "description": "See torch.norm() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.normal_", "item_type": "method", "code": "normal_(mean=0,std=1,*,generator=None)\u2192Tensor", "description": "Fills self tensor with elements samples from the normal distribution parameterized by mean and std. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.numel", "item_type": "method", "code": "numel()\u2192int", "description": "See torch.numel() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.numpy", "item_type": "method", "code": "numpy()\u2192numpy.ndarray", "description": "Returns self tensor as a NumPy ndarray. This tensor and the returned ndarray share the same underlying storage. Changes to self tensor will be reflected in the ndarray and vice versa. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.orgqr", "item_type": "method", "code": "orgqr(input2)\u2192Tensor", "description": "See torch.orgqr() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.ormqr", "item_type": "method", "code": "ormqr(input2,input3,left=True,transpose=False)\u2192Tensor", "description": "See torch.ormqr() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.permute", "item_type": "method", "code": "permute(*dims)\u2192Tensor", "description": "Permute the dimensions of this tensor.  Parameters *dims (python:int...) \u2013 The desired ordering of dimensions   Example &gt;&gt;&gt; x = torch.randn(2, 3, 5) &gt;&gt;&gt; x.size() torch.Size([2, 3, 5]) &gt;&gt;&gt; x.permute(2, 0, 1).size() torch.Size([5, 2, 3])   ", "parameters": ["*dims (python:int...) : The desired ordering of dimensions"], "returns": null, "example": " x = torch.randn(2, 3, 5)\n x.size()\ntorch.Size([2, 3, 5])\n x.permute(2, 0, 1).size()\ntorch.Size([5, 2, 3])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.pin_memory", "item_type": "method", "code": "pin_memory()\u2192Tensor", "description": "Copies the tensor to pinned memory, if it\u2019s not already pinned. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.pinverse", "item_type": "method", "code": "pinverse()\u2192Tensor", "description": "See torch.pinverse() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.polygamma", "item_type": "method", "code": "polygamma(n)\u2192Tensor", "description": "See torch.polygamma() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.addcdiv", "item_type": "function", "code": "torch.addcdiv(input,value=1,tensor1,tensor2,out=None)\u2192Tensor", "description": "Performs the element-wise division of tensor1 by tensor2, multiply the result by the scalar value and add it to input.  outi=inputi+value\u00d7tensor1itensor2i\\text{out}_i = \\text{input}_i + \\text{value} \\times \\frac{\\text{tensor1}_i}{\\text{tensor2}_i}  outi\u200b=inputi\u200b+value\u00d7tensor2i\u200btensor1i\u200b\u200b  The shapes of input, tensor1, and tensor2 must be broadcastable. For inputs of type FloatTensor or DoubleTensor, value must be a real number, otherwise an integer.  Parameters  input (Tensor) \u2013 the tensor to be added value (Number, optional) \u2013 multiplier for tensor1/tensor2\\text{tensor1} / \\text{tensor2}tensor1/tensor2   tensor1 (Tensor) \u2013 the numerator tensor tensor2 (Tensor) \u2013 the denominator tensor out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; t = torch.randn(1, 3) &gt;&gt;&gt; t1 = torch.randn(3, 1) &gt;&gt;&gt; t2 = torch.randn(1, 3) &gt;&gt;&gt; torch.addcdiv(t, 0.1, t1, t2) tensor([[-0.2312, -3.6496,  0.1312],         [-1.0428,  3.4292, -0.1030],         [-0.5369, -0.9829,  0.0430]])   ", "parameters": ["input (Tensor) : the tensor to be added", "value (Number, optional) : multiplier for tensor1/tensor2\\text{tensor1} / \\text{tensor2}tensor1/tensor2", "tensor1 (Tensor) : the numerator tensor", "tensor2 (Tensor) : the denominator tensor", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " t = torch.randn(1, 3)\n t1 = torch.randn(3, 1)\n t2 = torch.randn(1, 3)\n torch.addcdiv(t, 0.1, t1, t2)\ntensor([[-0.2312, -3.6496,  0.1312],\n        [-1.0428,  3.4292, -0.1030],\n        [-0.5369, -0.9829,  0.0430]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.addcmul", "item_type": "function", "code": "torch.addcmul(input,value=1,tensor1,tensor2,out=None)\u2192Tensor", "description": "Performs the element-wise multiplication of tensor1 by tensor2, multiply the result by the scalar value and add it to input.  outi=inputi+value\u00d7tensor1i\u00d7tensor2i\\text{out}_i = \\text{input}_i + \\text{value} \\times \\text{tensor1}_i \\times \\text{tensor2}_i  outi\u200b=inputi\u200b+value\u00d7tensor1i\u200b\u00d7tensor2i\u200b  The shapes of tensor, tensor1, and tensor2 must be broadcastable. For inputs of type FloatTensor or DoubleTensor, value must be a real number, otherwise an integer.  Parameters  input (Tensor) \u2013 the tensor to be added value (Number, optional) \u2013 multiplier for tensor1.\u2217tensor2tensor1 .* tensor2tensor1.\u2217tensor2   tensor1 (Tensor) \u2013 the tensor to be multiplied tensor2 (Tensor) \u2013 the tensor to be multiplied out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; t = torch.randn(1, 3) &gt;&gt;&gt; t1 = torch.randn(3, 1) &gt;&gt;&gt; t2 = torch.randn(1, 3) &gt;&gt;&gt; torch.addcmul(t, 0.1, t1, t2) tensor([[-0.8635, -0.6391,  1.6174],         [-0.7617, -0.5879,  1.7388],         [-0.8353, -0.6249,  1.6511]])   ", "parameters": ["input (Tensor) : the tensor to be added", "value (Number, optional) : multiplier for tensor1.\u2217tensor2tensor1 .* tensor2tensor1.\u2217tensor2", "tensor1 (Tensor) : the tensor to be multiplied", "tensor2 (Tensor) : the tensor to be multiplied", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " t = torch.randn(1, 3)\n t1 = torch.randn(3, 1)\n t2 = torch.randn(1, 3)\n torch.addcmul(t, 0.1, t1, t2)\ntensor([[-0.8635, -0.6391,  1.6174],\n        [-0.7617, -0.5879,  1.7388],\n        [-0.8353, -0.6249,  1.6511]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.angle", "item_type": "function", "code": "torch.angle(input,out=None)\u2192Tensor", "description": "Computes the element-wise angle (in radians) of the given input tensor.  outi=angle(inputi)\\text{out}_{i} = angle(\\text{input}_{i})  outi\u200b=angle(inputi\u200b)   Parameters  input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; torch.angle(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))*180/3.14159 tensor([ 135.,  135,  -45])   ", "parameters": ["input (Tensor) : the input tensor.", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " torch.angle(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))*180/3.14159\ntensor([ 135.,  135,  -45])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.asin", "item_type": "function", "code": "torch.asin(input,out=None)\u2192Tensor", "description": "Returns a new tensor with the arcsine  of the elements of input.  outi=sin\u2061\u22121(inputi)\\text{out}_{i} = \\sin^{-1}(\\text{input}_{i})  outi\u200b=sin\u22121(inputi\u200b)   Parameters  input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a tensor([-0.5962,  1.4985, -0.4396,  1.4525]) &gt;&gt;&gt; torch.asin(a) tensor([-0.6387,     nan, -0.4552,     nan])   ", "parameters": ["input (Tensor) : the input tensor.", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " a = torch.randn(4)\n a\ntensor([-0.5962,  1.4985, -0.4396,  1.4525])\n torch.asin(a)\ntensor([-0.6387,     nan, -0.4552,     nan])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.atan", "item_type": "function", "code": "torch.atan(input,out=None)\u2192Tensor", "description": "Returns a new tensor with the arctangent  of the elements of input.  outi=tan\u2061\u22121(inputi)\\text{out}_{i} = \\tan^{-1}(\\text{input}_{i})  outi\u200b=tan\u22121(inputi\u200b)   Parameters  input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a tensor([ 0.2341,  0.2539, -0.6256, -0.6448]) &gt;&gt;&gt; torch.atan(a) tensor([ 0.2299,  0.2487, -0.5591, -0.5727])   ", "parameters": ["input (Tensor) : the input tensor.", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " a = torch.randn(4)\n a\ntensor([ 0.2341,  0.2539, -0.6256, -0.6448])\n torch.atan(a)\ntensor([ 0.2299,  0.2487, -0.5591, -0.5727])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.beta.Beta.has_rsample", "item_type": "attribute", "code": "has_rsample=True", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.beta.Beta.support", "item_type": "attribute", "code": "support=Interval(lower_bound=0.0,upper_bound=1.0)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.binomial.Binomial", "item_type": "class", "code": "classtorch.distributions.binomial.Binomial(total_count=1,probs=None,logits=None,validate_args=None)", "description": "Bases: torch.distributions.distribution.Distribution Creates a Binomial distribution parameterized by total_count and either probs or logits (but not both). total_count must be broadcastable with probs/logits. Example: &gt;&gt;&gt; m = Binomial(100, torch.tensor([0 , .2, .8, 1])) &gt;&gt;&gt; x = m.sample() tensor([   0.,   22.,   71.,  100.])  &gt;&gt;&gt; m = Binomial(torch.tensor([[5.], [10.]]), torch.tensor([0.5, 0.8])) &gt;&gt;&gt; x = m.sample() tensor([[ 4.,  5.],         [ 7.,  6.]])    Parameters  total_count (python:int or Tensor) \u2013 number of Bernoulli trials probs (Tensor) \u2013 Event probabilities logits (Tensor) \u2013 Event log-odds      arg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0), 'total_count': IntegerGreaterThan(lower_bound=0)}     enumerate_support(expand=True)      expand(batch_shape, _instance=None)      has_enumerate_support = True     log_prob(value)      logits      property mean     property param_shape     probs      sample(sample_shape=torch.Size([]))      property support     property variance   ", "parameters": ["total_count (python:int or Tensor) : number of Bernoulli trials", "probs (Tensor) : Event probabilities", "logits (Tensor) : Event log-odds"], "returns": null, "example": " m = Binomial(100, torch.tensor([0 , .2, .8, 1]))\n x = m.sample()\ntensor([   0.,   22.,   71.,  100.])\n\n m = Binomial(torch.tensor([[5.], [10.]]), torch.tensor([0.5, 0.8]))\n x = m.sample()\ntensor([[ 4.,  5.],\n        [ 7.,  6.]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.binomial.Binomial.arg_constraints", "item_type": "attribute", "code": "arg_constraints={'logits':Real(),'probs':Interval(lower_bound=0.0,upper_bound=1.0),'total_count':IntegerGreaterThan(lower_bound=0)}", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.binomial.Binomial.has_enumerate_support", "item_type": "attribute", "code": "has_enumerate_support=True", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.binomial.Binomial.logits", "item_type": "attribute", "code": "logits", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.binomial.Binomial.probs", "item_type": "attribute", "code": "probs", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.RandomStructured.apply", "item_type": "method", "code": "classmethodapply(module,name,amount,dim=-1)", "description": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters  module (nn.Module) \u2013 module containing the tensor to prune name (str) \u2013 parameter name within module on which pruning will act. amount (python:int or python:float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune. dim (python:int, optional) \u2013 index of the dim along which we define channels to prune. Default: -1.    ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "name (str) : parameter name within module on which pruningwill act.", "amount (python:int or python:float) : quantity of parameters to prune.If float, should be between 0.0 and 1.0 and represent thefraction of parameters to prune. If int, it represents theabsolute number of parameters to prune.", "dim (python:int, optional) : index of the dim along which we definechannels to prune. Default: -1."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.RandomStructured.apply_mask", "item_type": "method", "code": "apply_mask(module)", "description": "Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters module (nn.Module) \u2013 module containing the tensor to prune  Returns pruned version of the input tensor  Return type pruned_tensor (torch.Tensor)   ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)"], "returns": "pruned version of the input tensor", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.RandomStructured.compute_mask", "item_type": "method", "code": "compute_mask(t,default_mask)", "description": "Computes and returns a mask for the input tensor t. Starting from a base default_mask (which should be a mask of ones if the tensor has not been pruned yet), generate a random mask to apply on top of the default_mask by randomly zeroing out channels along the specified dim of the tensor.  Parameters  t (torch.Tensor) \u2013 tensor representing the parameter to prune default_mask (torch.Tensor) \u2013 Base mask from previous pruning iterations, that need to be respected after the new mask is applied. Same dims as t.   Returns mask to apply to t, of same dims as t  Return type mask (torch.Tensor)  Raises IndexError \u2013 if self.dim &gt;= len(t.shape)   ", "parameters": ["t (torch.Tensor) : tensor representing the parameter to prune", "default_mask (torch.Tensor) : Base mask from previous pruningiterations, that need to be respected after the new mask isapplied. Same dims as t."], "returns": "mask to apply to t, of same dims as t", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.RandomStructured.prune", "item_type": "method", "code": "prune(t,default_mask=None)", "description": "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters  t (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). default_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns pruned version of tensor t.   ", "parameters": ["t (torch.Tensor) : tensor to prune (of same dimensions asdefault_mask).", "default_mask (torch.Tensor, optional) : mask from previous pruningiteration, if any. To be considered when determining whatportion of the tensor that pruning should act on. If None,default to a mask of ones."], "returns": "pruned version of tensor t.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.RandomStructured.remove", "item_type": "method", "code": "remove(module)", "description": "Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!  ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.LnStructured.apply", "item_type": "method", "code": "classmethodapply(module,name,amount,n,dim)", "description": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters  module (nn.Module) \u2013 module containing the tensor to prune name (str) \u2013 parameter name within module on which pruning will act. amount (python:int or python:float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune. n (python:int, python:float, inf, -inf, 'fro', 'nuc') \u2013 See documentation of valid entries for argument p in torch.norm(). dim (python:int) \u2013 index of the dim along which we define channels to prune.    ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "name (str) : parameter name within module on which pruningwill act.", "amount (python:int or python:float) : quantity of parameters to prune.If float, should be between 0.0 and 1.0 and represent thefraction of parameters to prune. If int, it represents theabsolute number of parameters to prune.", "n (python:int, python:float, inf, -inf, 'fro', 'nuc') : See documentation of validentries for argument p in torch.norm().", "dim (python:int) : index of the dim along which we define channels toprune."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.LnStructured.apply_mask", "item_type": "method", "code": "apply_mask(module)", "description": "Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters module (nn.Module) \u2013 module containing the tensor to prune  Returns pruned version of the input tensor  Return type pruned_tensor (torch.Tensor)   ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)"], "returns": "pruned version of the input tensor", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.polygamma_", "item_type": "method", "code": "polygamma_(n)\u2192Tensor", "description": "In-place version of polygamma() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.pow", "item_type": "method", "code": "pow(exponent)\u2192Tensor", "description": "See torch.pow() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.pow_", "item_type": "method", "code": "pow_(exponent)\u2192Tensor", "description": "In-place version of pow() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.prod", "item_type": "method", "code": "prod(dim=None,keepdim=False,dtype=None)\u2192Tensor", "description": "See torch.prod() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.put_", "item_type": "method", "code": "put_(indices,tensor,accumulate=False)\u2192Tensor", "description": "Copies the elements from tensor into the positions specified by indices. For the purpose of indexing, the self tensor is treated as if it were a 1-D tensor. If accumulate is True, the elements in tensor are added to self. If accumulate is False, the behavior is undefined if indices contain duplicate elements.  Parameters  indices (LongTensor) \u2013 the indices into self tensor (Tensor) \u2013 the tensor containing values to copy from accumulate (bool) \u2013 whether to accumulate into self    Example: &gt;&gt;&gt; src = torch.tensor([[4, 3, 5],                         [6, 7, 8]]) &gt;&gt;&gt; src.put_(torch.tensor([1, 3]), torch.tensor([9, 10])) tensor([[  4,   9,   5],         [ 10,   7,   8]])   ", "parameters": ["indices (LongTensor) : the indices into self", "tensor (Tensor) : the tensor containing values to copy from", "accumulate (bool) : whether to accumulate into self"], "returns": null, "example": " src = torch.tensor([[4, 3, 5],\n                        [6, 7, 8]])\n src.put_(torch.tensor([1, 3]), torch.tensor([9, 10]))\ntensor([[  4,   9,   5],\n        [ 10,   7,   8]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.qr", "item_type": "method", "code": "qr(some=True)-&gt;(Tensor,Tensor)", "description": "See torch.qr() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.qscheme", "item_type": "method", "code": "qscheme()\u2192torch.qscheme", "description": "Returns the quantization scheme of a given QTensor. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.q_scale", "item_type": "method", "code": "q_scale()\u2192float", "description": "Given a Tensor quantized by linear(affine) quantization, returns the scale of the underlying quantizer(). ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.q_zero_point", "item_type": "method", "code": "q_zero_point()\u2192int", "description": "Given a Tensor quantized by linear(affine) quantization, returns the zero_point of the underlying quantizer(). ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.q_per_channel_scales", "item_type": "method", "code": "q_per_channel_scales()\u2192Tensor", "description": "Given a Tensor quantized by linear (affine) per-channel quantization, returns a Tensor of scales of the underlying quantizer. It has the number of elements that matches the corresponding dimensions (from q_per_channel_axis) of the tensor. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.q_per_channel_zero_points", "item_type": "method", "code": "q_per_channel_zero_points()\u2192Tensor", "description": "Given a Tensor quantized by linear (affine) per-channel quantization, returns a tensor of zero_points of the underlying quantizer. It has the number of elements that matches the corresponding dimensions (from q_per_channel_axis) of the tensor. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.q_per_channel_axis", "item_type": "method", "code": "q_per_channel_axis()\u2192int", "description": "Given a Tensor quantized by linear (affine) per-channel quantization, returns the index of dimension on which per-channel quantization is applied. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.atan2", "item_type": "function", "code": "torch.atan2(input,other,out=None)\u2192Tensor", "description": "Element-wise arctangent of inputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200b   with consideration of the quadrant. Returns a new tensor with the signed angles in radians between vector (otheri,inputi)(\\text{other}_{i}, \\text{input}_{i})(otheri\u200b,inputi\u200b)   and vector (1,0)(1, 0)(1,0)  . (Note that otheri\\text{other}_{i}otheri\u200b  , the second parameter, is the x-coordinate, while inputi\\text{input}_{i}inputi\u200b  , the first parameter, is the y-coordinate.) The shapes of input and other must be broadcastable.  Parameters  input (Tensor) \u2013 the first input tensor other (Tensor) \u2013 the second input tensor out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a tensor([ 0.9041,  0.0196, -0.3108, -2.4423]) &gt;&gt;&gt; torch.atan2(a, torch.randn(4)) tensor([ 0.9833,  0.0811, -1.9743, -1.4151])   ", "parameters": ["input (Tensor) : the first input tensor", "other (Tensor) : the second input tensor", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " a = torch.randn(4)\n a\ntensor([ 0.9041,  0.0196, -0.3108, -2.4423])\n torch.atan2(a, torch.randn(4))\ntensor([ 0.9833,  0.0811, -1.9743, -1.4151])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.bitwise_not", "item_type": "function", "code": "torch.bitwise_not(input,out=None)\u2192Tensor", "description": "Computes the bitwise NOT of the given input tensor. The input tensor must be of integral or Boolean types. For bool tensors, it computes the logical NOT.  Parameters  input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor.    Example &gt;&gt;&gt; torch.bitwise_not(torch.tensor([-1, -2, 3], dtype=torch.int8)) tensor([ 0,  1, -4], dtype=torch.int8)   ", "parameters": ["input (Tensor) : the input tensor.", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " torch.bitwise_not(torch.tensor([-1, -2, 3], dtype=torch.int8))\ntensor([ 0,  1, -4], dtype=torch.int8)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.bitwise_xor", "item_type": "function", "code": "torch.bitwise_xor(input,other,out=None)\u2192Tensor", "description": "Computes the bitwise XOR of input and other. The input tensor must be of integral or Boolean types. For bool tensors, it computes the logical XOR.  Parameters  input \u2013 the first input tensor other \u2013 the second input tensor out (Tensor, optional) \u2013 the output tensor.    Example &gt;&gt;&gt; torch.bitwise_xor(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8)) tensor([-2, -2,  0], dtype=torch.int8) &gt;&gt;&gt; torch.bitwise_xor(torch.tensor([True, True, False]), torch.tensor([False, True, False])) tensor([ True, False, False])   ", "parameters": ["input : the first input tensor", "other : the second input tensor", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " torch.bitwise_xor(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))\ntensor([-2, -2,  0], dtype=torch.int8)\n torch.bitwise_xor(torch.tensor([True, True, False]), torch.tensor([False, True, False]))\ntensor([ True, False, False])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.ceil", "item_type": "function", "code": "torch.ceil(input,out=None)\u2192Tensor", "description": "Returns a new tensor with the ceil of the elements of input, the smallest integer greater than or equal to each element.  outi=\u2308inputi\u2309=\u230ainputi\u230b+1\\text{out}_{i} = \\left\\lceil \\text{input}_{i} \\right\\rceil = \\left\\lfloor \\text{input}_{i} \\right\\rfloor + 1  outi\u200b=\u2308inputi\u200b\u2309=\u230ainputi\u200b\u230b+1   Parameters  input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a tensor([-0.6341, -1.4208, -1.0900,  0.5826]) &gt;&gt;&gt; torch.ceil(a) tensor([-0., -1., -1.,  1.])   ", "parameters": ["input (Tensor) : the input tensor.", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " a = torch.randn(4)\n a\ntensor([-0.6341, -1.4208, -1.0900,  0.5826])\n torch.ceil(a)\ntensor([-0., -1., -1.,  1.])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.categorical.Categorical", "item_type": "class", "code": "classtorch.distributions.categorical.Categorical(probs=None,logits=None,validate_args=None)", "description": "Bases: torch.distributions.distribution.Distribution Creates a categorical distribution parameterized by either probs or logits (but not both).  Note It is equivalent to the distribution that torch.multinomial() samples from.  Samples are integers from {0,\u2026,K\u22121}\\{0, \\ldots, K-1\\}{0,\u2026,K\u22121}   where K is probs.size(-1). If probs is 1D with length-K, each element is the relative probability of sampling the class at that index. If probs is 2D, it is treated as a batch of relative probability vectors.  Note probs must be non-negative, finite and have a non-zero sum, and it will be normalized to sum to 1.  See also: torch.multinomial() Example: &gt;&gt;&gt; m = Categorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ])) &gt;&gt;&gt; m.sample()  # equal probability of 0, 1, 2, 3 tensor(3)    Parameters  probs (Tensor) \u2013 event probabilities logits (Tensor) \u2013 event log-odds      arg_constraints = {'logits': Real(), 'probs': Simplex()}     entropy()      enumerate_support(expand=True)      expand(batch_shape, _instance=None)      has_enumerate_support = True     log_prob(value)      logits      property mean     property param_shape     probs      sample(sample_shape=torch.Size([]))      property support     property variance   ", "parameters": ["probs (Tensor) : event probabilities", "logits (Tensor) : event log-odds"], "returns": null, "example": " m = Categorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))\n m.sample()  # equal probability of 0, 1, 2, 3\ntensor(3)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.categorical.Categorical.arg_constraints", "item_type": "attribute", "code": "arg_constraints={'logits':Real(),'probs':Simplex()}", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.categorical.Categorical.has_enumerate_support", "item_type": "attribute", "code": "has_enumerate_support=True", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.categorical.Categorical.logits", "item_type": "attribute", "code": "logits", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.categorical.Categorical.probs", "item_type": "attribute", "code": "probs", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.cauchy.Cauchy", "item_type": "class", "code": "classtorch.distributions.cauchy.Cauchy(loc,scale,validate_args=None)", "description": "Bases: torch.distributions.distribution.Distribution Samples from a Cauchy (Lorentz) distribution. The distribution of the ratio of independent normally distributed random variables with means 0 follows a Cauchy distribution. Example: &gt;&gt;&gt; m = Cauchy(torch.tensor([0.0]), torch.tensor([1.0])) &gt;&gt;&gt; m.sample()  # sample from a Cauchy distribution with loc=0 and scale=1 tensor([ 2.3214])    Parameters  loc (python:float or Tensor) \u2013 mode or median of the distribution. scale (python:float or Tensor) \u2013 half width at half maximum.      arg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)}     cdf(value)      entropy()      expand(batch_shape, _instance=None)      has_rsample = True     icdf(value)      log_prob(value)      property mean     rsample(sample_shape=torch.Size([]))      support = Real()     property variance   ", "parameters": ["loc (python:float or Tensor) : mode or median of the distribution.", "scale (python:float or Tensor) : half width at half maximum."], "returns": null, "example": " m = Cauchy(torch.tensor([0.0]), torch.tensor([1.0]))\n m.sample()  # sample from a Cauchy distribution with loc=0 and scale=1\ntensor([ 2.3214])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.cauchy.Cauchy.arg_constraints", "item_type": "attribute", "code": "arg_constraints={'loc':Real(),'scale':GreaterThan(lower_bound=0.0)}", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.cauchy.Cauchy.has_rsample", "item_type": "attribute", "code": "has_rsample=True", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.cauchy.Cauchy.support", "item_type": "attribute", "code": "support=Real()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.LnStructured.compute_mask", "item_type": "method", "code": "compute_mask(t,default_mask)", "description": "Computes and returns a mask for the input tensor t. Starting from a base default_mask (which should be a mask of ones if the tensor has not been pruned yet), generate a mask to apply on top of the default_mask by zeroing out the channels along the specified dim with the lowest Ln-norm.  Parameters  t (torch.Tensor) \u2013 tensor representing the parameter to prune default_mask (torch.Tensor) \u2013 Base mask from previous pruning iterations, that need to be respected after the new mask is applied.  Same dims as t.   Returns mask to apply to t, of same dims as t  Return type mask (torch.Tensor)  Raises IndexError \u2013 if self.dim &gt;= len(t.shape)   ", "parameters": ["t (torch.Tensor) : tensor representing the parameter to prune", "default_mask (torch.Tensor) : Base mask from previous pruningiterations, that need to be respected after the new mask isapplied.  Same dims as t."], "returns": "mask to apply to t, of same dims as t", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.LnStructured.prune", "item_type": "method", "code": "prune(t,default_mask=None)", "description": "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters  t (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). default_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns pruned version of tensor t.   ", "parameters": ["t (torch.Tensor) : tensor to prune (of same dimensions asdefault_mask).", "default_mask (torch.Tensor, optional) : mask from previous pruningiteration, if any. To be considered when determining whatportion of the tensor that pruning should act on. If None,default to a mask of ones."], "returns": "pruned version of tensor t.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.LnStructured.remove", "item_type": "method", "code": "remove(module)", "description": "Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!  ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.CustomFromMask.apply", "item_type": "method", "code": "classmethodapply(module,name,mask)", "description": "Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters  module (nn.Module) \u2013 module containing the tensor to prune name (str) \u2013 parameter name within module on which pruning will act.    ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "name (str) : parameter name within module on which pruningwill act."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.CustomFromMask.apply_mask", "item_type": "method", "code": "apply_mask(module)", "description": "Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters module (nn.Module) \u2013 module containing the tensor to prune  Returns pruned version of the input tensor  Return type pruned_tensor (torch.Tensor)   ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "pruned version of the input tensor", "pruned_tensor (torch.Tensor)"], "returns": "pruned version of the input tensor", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.CustomFromMask.prune", "item_type": "method", "code": "prune(t,default_mask=None)", "description": "Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters  t (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). default_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns pruned version of tensor t.   ", "parameters": ["t (torch.Tensor) : tensor to prune (of same dimensions asdefault_mask).", "default_mask (torch.Tensor, optional) : mask from previous pruningiteration, if any. To be considered when determining whatportion of the tensor that pruning should act on. If None,default to a mask of ones."], "returns": "pruned version of tensor t.", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.CustomFromMask.remove", "item_type": "method", "code": "remove(module)", "description": "Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!  ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.random_", "item_type": "method", "code": "random_(from=0,to=None,*,generator=None)\u2192Tensor", "description": "Fills self tensor with numbers sampled from the discrete uniform distribution over [from, to - 1]. If not specified, the values are usually only bounded by self tensor\u2019s data type. However, for floating point types, if unspecified, range will be [0, 2^mantissa] to ensure that every value is representable. For example, torch.tensor(1, dtype=torch.double).random_() will be uniform in [0, 2^53]. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.reciprocal", "item_type": "method", "code": "reciprocal()\u2192Tensor", "description": "See torch.reciprocal() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.reciprocal_", "item_type": "method", "code": "reciprocal_()\u2192Tensor", "description": "In-place version of reciprocal() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.record_stream", "item_type": "method", "code": "record_stream(stream)", "description": "Ensures that the tensor memory is not reused for another tensor until all current work queued on stream are complete.  Note The caching allocator is aware of only the stream where a tensor was allocated. Due to the awareness, it already correctly manages the life cycle of tensors on only one stream. But if a tensor is used on a stream different from the stream of origin, the allocator might reuse the memory unexpectedly. Calling this method lets the allocator know which streams have used the tensor.  ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "method", "code": "register_hook(hook)", "description": "Registers a backward hook. The hook will be called every time a gradient with respect to the Tensor is computed. The hook should have the following signature: hook(grad) -&gt; Tensor or None   The hook should not modify its argument, but it can optionally return a new gradient which will be used in place of grad. This function returns a handle with a method handle.remove() that removes the hook from the module. Example: &gt;&gt;&gt; v = torch.tensor([0., 0., 0.], requires_grad=True) &gt;&gt;&gt; h = v.register_hook(lambda grad: grad * 2)  # double the gradient &gt;&gt;&gt; v.backward(torch.tensor([1., 2., 3.])) &gt;&gt;&gt; v.grad   2  4  6 [torch.FloatTensor of size (3,)]  &gt;&gt;&gt; h.remove()  # removes the hook   ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.remainder", "item_type": "method", "code": "remainder(divisor)\u2192Tensor", "description": "See torch.remainder() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.remainder_", "item_type": "method", "code": "remainder_(divisor)\u2192Tensor", "description": "In-place version of remainder() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.real", "item_type": "method", "code": "real()\u2192Tensor", "description": "See torch.real() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.renorm", "item_type": "method", "code": "renorm(p,dim,maxnorm)\u2192Tensor", "description": "See torch.renorm() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.clamp", "item_type": "function", "code": "torch.clamp(input,min,max,out=None)\u2192Tensor", "description": "Clamp all elements in input into the range [ min, max ] and return a resulting tensor:  yi={minif\u00a0xi&lt;minxiif\u00a0min\u2264xi\u2264maxmaxif\u00a0xi&gt;maxy_i = \\begin{cases}     \\text{min} &amp; \\text{if } x_i &lt; \\text{min} \\\\     x_i &amp; \\text{if } \\text{min} \\leq x_i \\leq \\text{max} \\\\     \\text{max} &amp; \\text{if } x_i &gt; \\text{max} \\end{cases}  yi\u200b=\u23a9\u23aa\u23aa\u23a8\u23aa\u23aa\u23a7\u200bminxi\u200bmax\u200bif\u00a0xi\u200b&lt;minif\u00a0min\u2264xi\u200b\u2264maxif\u00a0xi\u200b&gt;max\u200b  If input is of type FloatTensor or DoubleTensor, args min and max must be real numbers, otherwise they should be integers.  Parameters  input (Tensor) \u2013 the input tensor. min (Number) \u2013 lower-bound of the range to be clamped to max (Number) \u2013 upper-bound of the range to be clamped to out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a tensor([-1.7120,  0.1734, -0.0478, -0.0922]) &gt;&gt;&gt; torch.clamp(a, min=-0.5, max=0.5) tensor([-0.5000,  0.1734, -0.0478, -0.0922])     torch.clamp(input, *, min, out=None) \u2192 Tensor   Clamps all elements in input to be larger or equal min. If input is of type FloatTensor or DoubleTensor, value should be a real number, otherwise it should be an integer.  Parameters  input (Tensor) \u2013 the input tensor. value (Number) \u2013 minimal value of each element in the output out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a tensor([-0.0299, -2.3184,  2.1593, -0.8883]) &gt;&gt;&gt; torch.clamp(a, min=0.5) tensor([ 0.5000,  0.5000,  2.1593,  0.5000])     torch.clamp(input, *, max, out=None) \u2192 Tensor   Clamps all elements in input to be smaller or equal max. If input is of type FloatTensor or DoubleTensor, value should be a real number, otherwise it should be an integer.  Parameters  input (Tensor) \u2013 the input tensor. value (Number) \u2013 maximal value of each element in the output out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a tensor([ 0.7753, -0.4702, -0.4599,  1.1899]) &gt;&gt;&gt; torch.clamp(a, max=0.5) tensor([ 0.5000, -0.4702, -0.4599,  0.5000])   ", "parameters": ["input (Tensor) : the input tensor.", "min (Number) : lower-bound of the range to be clamped to", "max (Number) : upper-bound of the range to be clamped to", "out (Tensor, optional) : the output tensor.", "input (Tensor) : the input tensor.", "value (Number) : minimal value of each element in the output", "out (Tensor, optional) : the output tensor.", "input (Tensor) : the input tensor.", "value (Number) : maximal value of each element in the output", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " a = torch.randn(4)\n a\ntensor([-1.7120,  0.1734, -0.0478, -0.0922])\n torch.clamp(a, min=-0.5, max=0.5)\ntensor([-0.5000,  0.1734, -0.0478, -0.0922])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.clamp(input,*,min,out=None)\u2192Tensor", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.clamp(input,*,max,out=None)\u2192Tensor", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.chi2.Chi2", "item_type": "class", "code": "classtorch.distributions.chi2.Chi2(df,validate_args=None)", "description": "Bases: torch.distributions.gamma.Gamma Creates a Chi2 distribution parameterized by shape parameter df. This is exactly equivalent to Gamma(alpha=0.5*df, beta=0.5) Example: &gt;&gt;&gt; m = Chi2(torch.tensor([1.0])) &gt;&gt;&gt; m.sample()  # Chi2 distributed with shape df=1 tensor([ 0.1046])    Parameters df (python:float or Tensor) \u2013 shape parameter of the distribution     arg_constraints = {'df': GreaterThan(lower_bound=0.0)}     property df     expand(batch_shape, _instance=None)    ", "parameters": ["df (python:float or Tensor) : shape parameter of the distribution"], "returns": null, "example": " m = Chi2(torch.tensor([1.0]))\n m.sample()  # Chi2 distributed with shape df=1\ntensor([ 0.1046])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.chi2.Chi2.arg_constraints", "item_type": "attribute", "code": "arg_constraints={'df':GreaterThan(lower_bound=0.0)}", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.dirichlet.Dirichlet", "item_type": "class", "code": "classtorch.distributions.dirichlet.Dirichlet(concentration,validate_args=None)", "description": "Bases: torch.distributions.exp_family.ExponentialFamily Creates a Dirichlet distribution parameterized by concentration concentration. Example: &gt;&gt;&gt; m = Dirichlet(torch.tensor([0.5, 0.5])) &gt;&gt;&gt; m.sample()  # Dirichlet distributed with concentrarion concentration tensor([ 0.1046,  0.8954])    Parameters concentration (Tensor) \u2013 concentration parameter of the distribution (often referred to as alpha)     arg_constraints = {'concentration': GreaterThan(lower_bound=0.0)}     entropy()      expand(batch_shape, _instance=None)      has_rsample = True     log_prob(value)      property mean     rsample(sample_shape=())      support = Simplex()     property variance   ", "parameters": ["concentration (Tensor) : concentration parameter of the distribution(often referred to as alpha)"], "returns": null, "example": " m = Dirichlet(torch.tensor([0.5, 0.5]))\n m.sample()  # Dirichlet distributed with concentrarion concentration\ntensor([ 0.1046,  0.8954])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.dirichlet.Dirichlet.arg_constraints", "item_type": "attribute", "code": "arg_constraints={'concentration':GreaterThan(lower_bound=0.0)}", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.dirichlet.Dirichlet.has_rsample", "item_type": "attribute", "code": "has_rsample=True", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.dirichlet.Dirichlet.support", "item_type": "attribute", "code": "support=Simplex()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.exponential.Exponential", "item_type": "class", "code": "classtorch.distributions.exponential.Exponential(rate,validate_args=None)", "description": "Bases: torch.distributions.exp_family.ExponentialFamily Creates a Exponential distribution parameterized by rate. Example: &gt;&gt;&gt; m = Exponential(torch.tensor([1.0])) &gt;&gt;&gt; m.sample()  # Exponential distributed with rate=1 tensor([ 0.1046])    Parameters rate (python:float or Tensor) \u2013 rate = 1 / scale of the distribution     arg_constraints = {'rate': GreaterThan(lower_bound=0.0)}     cdf(value)      entropy()      expand(batch_shape, _instance=None)      has_rsample = True     icdf(value)      log_prob(value)      property mean     rsample(sample_shape=torch.Size([]))      property stddev     support = GreaterThan(lower_bound=0.0)     property variance   ", "parameters": ["rate (python:float or Tensor) : rate = 1 / scale of the distribution"], "returns": null, "example": " m = Exponential(torch.tensor([1.0]))\n m.sample()  # Exponential distributed with rate=1\ntensor([ 0.1046])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.exponential.Exponential.arg_constraints", "item_type": "attribute", "code": "arg_constraints={'rate':GreaterThan(lower_bound=0.0)}", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.Parameter", "item_type": "class", "code": "classtorch.nn.Parameter", "description": "A kind of Tensor that is to be considered a module parameter. Parameters are Tensor subclasses, that have a very special property when used with Module s - when they\u2019re assigned as Module attributes they are automatically added to the list of its parameters, and will appear e.g. in parameters() iterator. Assigning a Tensor doesn\u2019t have such effect. This is because one might want to cache some temporary state, like last hidden state of the RNN, in the model. If there was no such class as Parameter, these temporaries would get registered too.  Parameters  data (Tensor) \u2013 parameter tensor. requires_grad (bool, optional) \u2013 if the parameter requires gradient. See Excluding subgraphs from backward for more details. Default: True    ", "parameters": ["data (Tensor) : parameter tensor.", "requires_grad (bool, optional) : if the parameter requires gradient. SeeExcluding subgraphs from backward for more details. Default: True"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.Module", "item_type": "class", "code": "classtorch.nn.Module", "description": "Base class for all neural network modules. Your models should also subclass this class. Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes: import torch.nn as nn import torch.nn.functional as F  class Model(nn.Module):     def __init__(self):         super(Model, self).__init__()         self.conv1 = nn.Conv2d(1, 20, 5)         self.conv2 = nn.Conv2d(20, 20, 5)      def forward(self, x):         x = F.relu(self.conv1(x))         return F.relu(self.conv2(x))   Submodules assigned in this way will be registered, and will have their parameters converted too when you call to(), etc.   add_module(name, module)  Adds a child module to the current module. The module can be accessed as an attribute using the given name.  Parameters  name (string) \u2013 name of the child module. The child module can be accessed from this module using the given name module (Module) \u2013 child module to be added to the module.        apply(fn)  Applies fn recursively to every submodule (as returned by .children()) as well as self. Typical use includes initializing the parameters of a model (see also torch.nn.init).  Parameters fn (Module -&gt; None) \u2013 function to be applied to each submodule  Returns self  Return type Module   Example: &gt;&gt;&gt; def init_weights(m): &gt;&gt;&gt;     print(m) &gt;&gt;&gt;     if type(m) == nn.Linear: &gt;&gt;&gt;         m.weight.data.fill_(1.0) &gt;&gt;&gt;         print(m.weight) &gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) &gt;&gt;&gt; net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1.,  1.],         [ 1.,  1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1.,  1.],         [ 1.,  1.]]) Sequential(   (0): Linear(in_features=2, out_features=2, bias=True)   (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential(   (0): Linear(in_features=2, out_features=2, bias=True)   (1): Linear(in_features=2, out_features=2, bias=True) )       buffers(recurse=True)  Returns an iterator over module buffers.  Parameters recurse (bool) \u2013 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.  Yields torch.Tensor \u2013 module buffer   Example: &gt;&gt;&gt; for buf in model.buffers(): &gt;&gt;&gt;     print(type(buf.data), buf.size()) &lt;class 'torch.FloatTensor'&gt; (20L,) &lt;class 'torch.FloatTensor'&gt; (20L, 1L, 5L, 5L)       children()  Returns an iterator over immediate children modules.  Yields Module \u2013 a child module       cpu()  Moves all model parameters and buffers to the CPU.  Returns self  Return type Module       cuda(device=None)  Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.  Parameters device (python:int, optional) \u2013 if specified, all parameters will be copied to that device  Returns self  Return type Module       double()  Casts all floating point parameters and buffers to double datatype.  Returns self  Return type Module       dump_patches = False This allows better BC support for load_state_dict(). In state_dict(), the version number will be saved as in the attribute _metadata of the returned state dict, and thus pickled. _metadata is a dictionary with keys that follow the naming convention of state dict. See _load_from_state_dict on how to use this information in loading. If new parameters/buffers are added/removed from a module, this number shall be bumped, and the module\u2019s _load_from_state_dict method can compare the version number and do appropriate changes if the state dict is from before the change.     eval()  Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc. This is equivalent with self.train(False).  Returns self  Return type Module       extra_repr()  Set the extra representation of the module To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable.     float()  Casts all floating point parameters and buffers to float datatype.  Returns self  Return type Module       forward(*input)  Defines the computation performed at every call. Should be overridden by all subclasses.  Note Although the recipe for forward pass needs to be defined within this function, one should call the Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.      half()  Casts all floating point parameters and buffers to half datatype.  Returns self  Return type Module       load_state_dict(state_dict, strict=True)  Copies parameters and buffers from state_dict into this module and its descendants. If strict is True, then the keys of state_dict must exactly match the keys returned by this module\u2019s state_dict() function.  Parameters  state_dict (dict) \u2013 a dict containing parameters and persistent buffers. strict (bool, optional) \u2013 whether to strictly enforce that the keys in state_dict match the keys returned by this module\u2019s state_dict() function. Default: True   Returns  missing_keys is a list of str containing the missing keys unexpected_keys is a list of str containing the unexpected keys    Return type NamedTuple with missing_keys and unexpected_keys fields       modules()  Returns an iterator over all modules in the network.  Yields Module \u2013 a module in the network    Note Duplicate modules are returned only once. In the following example, l will be returned only once.  Example: &gt;&gt;&gt; l = nn.Linear(2, 2) &gt;&gt;&gt; net = nn.Sequential(l, l) &gt;&gt;&gt; for idx, m in enumerate(net.modules()):         print(idx, '-&gt;', m)  0 -&gt; Sequential(   (0): Linear(in_features=2, out_features=2, bias=True)   (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -&gt; Linear(in_features=2, out_features=2, bias=True)       named_buffers(prefix='', recurse=True)  Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.  Parameters  prefix (str) \u2013 prefix to prepend to all buffer names. recurse (bool) \u2013 if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.   Yields (string, torch.Tensor) \u2013 Tuple containing the name and buffer   Example: &gt;&gt;&gt; for name, buf in self.named_buffers(): &gt;&gt;&gt;    if name in ['running_var']: &gt;&gt;&gt;        print(buf.size())       named_children()  Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.  Yields (string, Module) \u2013 Tuple containing a name and child module   Example: &gt;&gt;&gt; for name, module in model.named_children(): &gt;&gt;&gt;     if name in ['conv4', 'conv5']: &gt;&gt;&gt;         print(module)       named_modules(memo=None, prefix='')  Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.  Yields (string, Module) \u2013 Tuple of name and module    Note Duplicate modules are returned only once. In the following example, l will be returned only once.  Example: &gt;&gt;&gt; l = nn.Linear(2, 2) &gt;&gt;&gt; net = nn.Sequential(l, l) &gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):         print(idx, '-&gt;', m)  0 -&gt; ('', Sequential(   (0): Linear(in_features=2, out_features=2, bias=True)   (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -&gt; ('0', Linear(in_features=2, out_features=2, bias=True))       named_parameters(prefix='', recurse=True)  Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.  Parameters  prefix (str) \u2013 prefix to prepend to all parameter names. recurse (bool) \u2013 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.   Yields (string, Parameter) \u2013 Tuple containing the name and parameter   Example: &gt;&gt;&gt; for name, param in self.named_parameters(): &gt;&gt;&gt;    if name in ['bias']: &gt;&gt;&gt;        print(param.size())       parameters(recurse=True)  Returns an iterator over module parameters. This is typically passed to an optimizer.  Parameters recurse (bool) \u2013 if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.  Yields Parameter \u2013 module parameter   Example: &gt;&gt;&gt; for param in model.parameters(): &gt;&gt;&gt;     print(type(param.data), param.size()) &lt;class 'torch.FloatTensor'&gt; (20L,) &lt;class 'torch.FloatTensor'&gt; (20L, 1L, 5L, 5L)       register_backward_hook(hook)  Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature: hook(module, grad_input, grad_output) -&gt; Tensor or None   The grad_input and grad_output may be tuples if the module has multiple inputs or outputs. The hook should not modify its arguments, but it can optionally return a new gradient with respect to input that will be used in place of grad_input in subsequent computations.  Returns a handle that can be used to remove the added hook by calling handle.remove()  Return type torch.utils.hooks.RemovableHandle    Warning The current implementation will not have the presented behavior for complex Module that perform many operations. In some failure cases, grad_input and grad_output will only contain the gradients for a subset of the inputs and outputs. For such Module, you should use torch.Tensor.register_hook() directly on a specific input or output to get the required gradients.      register_buffer(name, tensor)  Adds a persistent buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm\u2019s running_mean is not a parameter, but is part of the persistent state. Buffers can be accessed as attributes using given names.  Parameters  name (string) \u2013 name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor) \u2013 buffer to be registered.    Example: &gt;&gt;&gt; self.register_buffer('running_mean', torch.zeros(num_features))       register_forward_hook(hook)  Registers a forward hook on the module. The hook will be called every time after forward() has computed an output. It should have the following signature: hook(module, input, output) -&gt; None or modified output   The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after forward() is called.  Returns a handle that can be used to remove the added hook by calling handle.remove()  Return type torch.utils.hooks.RemovableHandle       register_forward_pre_hook(hook)  Registers a forward pre-hook on the module. The hook will be called every time before forward() is invoked. It should have the following signature: hook(module, input) -&gt; None or modified input   The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple).  Returns a handle that can be used to remove the added hook by calling handle.remove()  Return type torch.utils.hooks.RemovableHandle       register_parameter(name, param)  Adds a parameter to the module. The parameter can be accessed as an attribute using given name.  Parameters  name (string) \u2013 name of the parameter. The parameter can be accessed from this module using the given name param (Parameter) \u2013 parameter to be added to the module.        requires_grad_(requires_grad=True)  Change if autograd should record operations on parameters in this module. This method sets the parameters\u2019 requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).  Parameters requires_grad (bool) \u2013 whether autograd should record operations on parameters in this module. Default: True.  Returns self  Return type Module       state_dict(destination=None, prefix='', keep_vars=False)  Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names.  Returns a dictionary containing a whole state of the module  Return type dict   Example: &gt;&gt;&gt; module.state_dict().keys() ['bias', 'weight']       to(*args, **kwargs)  Moves and/or casts the parameters and buffers. This can be called as   to(device=None, dtype=None, non_blocking=False)      to(dtype, non_blocking=False)      to(tensor, non_blocking=False)    Its signature is similar to torch.Tensor.to(), but only accepts floating point desired dtype s. In addition, this method will only cast the floating point parameters and buffers to dtype (if given). The integral parameters and buffers will be moved device, if that is given, but with dtypes unchanged. When non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples.  Note This method modifies the module in-place.   Parameters  device (torch.device) \u2013 the desired device of the parameters and buffers in this module dtype (torch.dtype) \u2013 the desired floating point type of the floating point parameters and buffers in this module tensor (torch.Tensor) \u2013 Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module   Returns self  Return type Module   Example: &gt;&gt;&gt; linear = nn.Linear(2, 2) &gt;&gt;&gt; linear.weight Parameter containing: tensor([[ 0.1913, -0.3420],         [-0.5113, -0.2325]]) &gt;&gt;&gt; linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) &gt;&gt;&gt; linear.weight Parameter containing: tensor([[ 0.1913, -0.3420],         [-0.5113, -0.2325]], dtype=torch.float64) &gt;&gt;&gt; gpu1 = torch.device(\"cuda:1\") &gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) &gt;&gt;&gt; linear.weight Parameter containing: tensor([[ 0.1914, -0.3420],         [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') &gt;&gt;&gt; cpu = torch.device(\"cpu\") &gt;&gt;&gt; linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) &gt;&gt;&gt; linear.weight Parameter containing: tensor([[ 0.1914, -0.3420],         [-0.5112, -0.2324]], dtype=torch.float16)       train(mode=True)  Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc.  Parameters mode (bool) \u2013 whether to set training mode (True) or evaluation mode (False). Default: True.  Returns self  Return type Module       type(dst_type)  Casts all parameters and buffers to dst_type.  Parameters dst_type (python:type or string) \u2013 the desired type  Returns self  Return type Module       zero_grad()  Sets gradients of all model parameters to zero.   ", "parameters": ["name (string) : name of the child module. The child module can beaccessed from this module using the given name", "module (Module) : child module to be added to the module.", "state_dict (dict) : a dict containing parameters andpersistent buffers.", "strict (bool, optional) : whether to strictly enforce that the keysin state_dict match the keys returned by this module\u2019sstate_dict() function. Default: True", "missing_keys is a list of str containing the missing keys", "unexpected_keys is a list of str containing the unexpected keys", "prefix (str) : prefix to prepend to all buffer names.", "recurse (bool) : if True, then yields buffers of this moduleand all submodules. Otherwise, yields only buffers thatare direct members of this module.", "prefix (str) : prefix to prepend to all parameter names.", "recurse (bool) : if True, then yields parameters of this moduleand all submodules. Otherwise, yields only parameters thatare direct members of this module.", "name (string) : name of the buffer. The buffer can be accessedfrom this module using the given name", "tensor (Tensor) : buffer to be registered.", "name (string) : name of the parameter. The parameter can be accessedfrom this module using the given name", "param (Parameter) : parameter to be added to the module.", "device (torch.device) : the desired device of the parametersand buffers in this module", "dtype (torch.dtype) : the desired floating point type ofthe floating point parameters and buffers in this module", "tensor (torch.Tensor) : Tensor whose dtype and device are the desireddtype and device for all parameters and buffers in this module"], "returns": "self", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.renorm_", "item_type": "method", "code": "renorm_(p,dim,maxnorm)\u2192Tensor", "description": "In-place version of renorm() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.repeat", "item_type": "method", "code": "repeat(*sizes)\u2192Tensor", "description": "Repeats this tensor along the specified dimensions. Unlike expand(), this function copies the tensor\u2019s data.  Warning torch.repeat() behaves differently from numpy.repeat, but is more similar to numpy.tile. For the operator similar to numpy.repeat, see torch.repeat_interleave().   Parameters sizes (torch.Size or python:int...) \u2013 The number of times to repeat this tensor along each dimension   Example: &gt;&gt;&gt; x = torch.tensor([1, 2, 3]) &gt;&gt;&gt; x.repeat(4, 2) tensor([[ 1,  2,  3,  1,  2,  3],         [ 1,  2,  3,  1,  2,  3],         [ 1,  2,  3,  1,  2,  3],         [ 1,  2,  3,  1,  2,  3]]) &gt;&gt;&gt; x.repeat(4, 2, 1).size() torch.Size([4, 2, 3])   ", "parameters": ["sizes (torch.Size or python:int...) : The number of times to repeat this tensor along eachdimension"], "returns": null, "example": " x = torch.tensor([1, 2, 3])\n x.repeat(4, 2)\ntensor([[ 1,  2,  3,  1,  2,  3],\n        [ 1,  2,  3,  1,  2,  3],\n        [ 1,  2,  3,  1,  2,  3],\n        [ 1,  2,  3,  1,  2,  3]])\n x.repeat(4, 2, 1).size()\ntorch.Size([4, 2, 3])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.repeat_interleave", "item_type": "method", "code": "repeat_interleave(repeats,dim=None)\u2192Tensor", "description": "See torch.repeat_interleave(). ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.requires_grad_", "item_type": "method", "code": "requires_grad_(requires_grad=True)\u2192Tensor", "description": "Change if autograd should record operations on this tensor: sets this tensor\u2019s requires_grad attribute in-place. Returns this tensor. requires_grad_()\u2019s main use case is to tell autograd to begin recording operations on a Tensor tensor. If tensor has requires_grad=False (because it was obtained through a DataLoader, or required preprocessing or initialization), tensor.requires_grad_() makes it so that autograd will begin to record operations on tensor.  Parameters requires_grad (bool) \u2013 If autograd should record operations on this tensor. Default: True.   Example: &gt;&gt;&gt; # Let's say we want to preprocess some saved weights and use &gt;&gt;&gt; # the result as new weights. &gt;&gt;&gt; saved_weights = [0.1, 0.2, 0.3, 0.25] &gt;&gt;&gt; loaded_weights = torch.tensor(saved_weights) &gt;&gt;&gt; weights = preprocess(loaded_weights)  # some function &gt;&gt;&gt; weights tensor([-0.5503,  0.4926, -2.1158, -0.8303])  &gt;&gt;&gt; # Now, start to record operations done to weights &gt;&gt;&gt; weights.requires_grad_() &gt;&gt;&gt; out = weights.pow(2).sum() &gt;&gt;&gt; out.backward() &gt;&gt;&gt; weights.grad tensor([-1.1007,  0.9853, -4.2316, -1.6606])   ", "parameters": ["requires_grad (bool) : If autograd should record operations on this tensor.Default: True."], "returns": null, "example": " # Let's say we want to preprocess some saved weights and use\n # the result as new weights.\n saved_weights = [0.1, 0.2, 0.3, 0.25]\n loaded_weights = torch.tensor(saved_weights)\n weights = preprocess(loaded_weights)  # some function\n weights\ntensor([-0.5503,  0.4926, -2.1158, -0.8303])\n\n # Now, start to record operations done to weights\n weights.requires_grad_()\n out = weights.pow(2).sum()\n out.backward()\n weights.grad\ntensor([-1.1007,  0.9853, -4.2316, -1.6606])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.reshape", "item_type": "method", "code": "reshape(*shape)\u2192Tensor", "description": "Returns a tensor with the same data and number of elements as self but with the specified shape. This method returns a view if shape is compatible with the current shape. See torch.Tensor.view() on when it is possible to return a view. See torch.reshape()  Parameters shape (tuple of python:ints or python:int...) \u2013 the desired shape   ", "parameters": ["shape (tuple of python:ints or python:int...) : the desired shape"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.reshape_as", "item_type": "method", "code": "reshape_as(other)\u2192Tensor", "description": "Returns this tensor as the same shape as other. self.reshape_as(other) is equivalent to self.reshape(other.sizes()). This method returns a view if other.sizes() is compatible with the current shape. See torch.Tensor.view() on when it is possible to return a view. Please see reshape() for more information about reshape.  Parameters other (torch.Tensor) \u2013 The result tensor has the same shape as other.   ", "parameters": ["other (torch.Tensor) : The result tensor has the same shapeas other."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.resize_", "item_type": "method", "code": "resize_(*sizes)\u2192Tensor", "description": "Resizes self tensor to the specified size. If the number of elements is larger than the current storage size, then the underlying storage is resized to fit the new number of elements. If the number of elements is smaller, the underlying storage is not changed. Existing elements are preserved but any new memory is uninitialized.  Warning This is a low-level method. The storage is reinterpreted as C-contiguous, ignoring the current strides (unless the target size equals the current size, in which case the tensor is left unchanged). For most purposes, you will instead want to use view(), which checks for contiguity, or reshape(), which copies data if needed. To change the size in-place with custom strides, see set_().   Parameters sizes (torch.Size or python:int...) \u2013 the desired size   Example: &gt;&gt;&gt; x = torch.tensor([[1, 2], [3, 4], [5, 6]]) &gt;&gt;&gt; x.resize_(2, 2) tensor([[ 1,  2],         [ 3,  4]])   ", "parameters": ["sizes (torch.Size or python:int...) : the desired size"], "returns": null, "example": " x = torch.tensor([[1, 2], [3, 4], [5, 6]])\n x.resize_(2, 2)\ntensor([[ 1,  2],\n        [ 3,  4]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.conj", "item_type": "function", "code": "torch.conj(input,out=None)\u2192Tensor", "description": "Computes the element-wise conjugate of the given input tensor.  outi=conj(inputi)\\text{out}_{i} = conj(\\text{input}_{i})  outi\u200b=conj(inputi\u200b)   Parameters  input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; torch.conj(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j])) tensor([-1 - 1j, -2 - 2j, 3 + 3j])   ", "parameters": ["input (Tensor) : the input tensor.", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " torch.conj(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))\ntensor([-1 - 1j, -2 - 2j, 3 + 3j])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.cos", "item_type": "function", "code": "torch.cos(input,out=None)\u2192Tensor", "description": "Returns a new tensor with the cosine  of the elements of input.  outi=cos\u2061(inputi)\\text{out}_{i} = \\cos(\\text{input}_{i})  outi\u200b=cos(inputi\u200b)   Parameters  input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a tensor([ 1.4309,  1.2706, -0.8562,  0.9796]) &gt;&gt;&gt; torch.cos(a) tensor([ 0.1395,  0.2957,  0.6553,  0.5574])   ", "parameters": ["input (Tensor) : the input tensor.", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " a = torch.randn(4)\n a\ntensor([ 1.4309,  1.2706, -0.8562,  0.9796])\n torch.cos(a)\ntensor([ 0.1395,  0.2957,  0.6553,  0.5574])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.cosh", "item_type": "function", "code": "torch.cosh(input,out=None)\u2192Tensor", "description": "Returns a new tensor with the hyperbolic cosine  of the elements of input.  outi=cosh\u2061(inputi)\\text{out}_{i} = \\cosh(\\text{input}_{i})  outi\u200b=cosh(inputi\u200b)   Parameters  input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a tensor([ 0.1632,  1.1835, -0.6979, -0.7325]) &gt;&gt;&gt; torch.cosh(a) tensor([ 1.0133,  1.7860,  1.2536,  1.2805])   ", "parameters": ["input (Tensor) : the input tensor.", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " a = torch.randn(4)\n a\ntensor([ 0.1632,  1.1835, -0.6979, -0.7325])\n torch.cosh(a)\ntensor([ 1.0133,  1.7860,  1.2536,  1.2805])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.div", "item_type": "function", "code": "torch.div()", "description": "  torch.div(input, other, out=None) \u2192 Tensor   Divides each element of the input input with the scalar other and returns a new resulting tensor.  outi=inputiother\\text{out}_i = \\frac{\\text{input}_i}{\\text{other}}  outi\u200b=otherinputi\u200b\u200b  If the torch.dtype of input and other differ, the torch.dtype of the result tensor is determined following rules described in the type promotion documentation. If out is specified, the result must be castable to the torch.dtype of the specified output tensor. Integral division by zero leads to undefined behavior.  Parameters  input (Tensor) \u2013 the input tensor. other (Number) \u2013 the number to be divided to each element of input   Keyword Arguments out (Tensor, optional) \u2013 the output tensor.   Example: &gt;&gt;&gt; a = torch.randn(5) &gt;&gt;&gt; a tensor([ 0.3810,  1.2774, -0.2972, -0.3719,  0.4637]) &gt;&gt;&gt; torch.div(a, 0.5) tensor([ 0.7620,  2.5548, -0.5944, -0.7439,  0.9275])     torch.div(input, other, out=None) \u2192 Tensor   Each element of the tensor input is divided by each element of the tensor other. The resulting tensor is returned.  outi=inputiotheri\\text{out}_i = \\frac{\\text{input}_i}{\\text{other}_i}  outi\u200b=otheri\u200binputi\u200b\u200b  The shapes of input and other must be broadcastable. If the torch.dtype of input and other differ, the torch.dtype of the result tensor is determined following rules described in the type promotion documentation. If out is specified, the result must be castable to the torch.dtype of the specified output tensor. Integral division by zero leads to undefined behavior.  Parameters  input (Tensor) \u2013 the numerator tensor other (Tensor) \u2013 the denominator tensor   Keyword Arguments out (Tensor, optional) \u2013 the output tensor.   Example: &gt;&gt;&gt; a = torch.randn(4, 4) &gt;&gt;&gt; a tensor([[-0.3711, -1.9353, -0.4605, -0.2917],         [ 0.1815, -1.0111,  0.9805, -1.5923],         [ 0.1062,  1.4581,  0.7759, -1.2344],         [-0.1830, -0.0313,  1.1908, -1.4757]]) &gt;&gt;&gt; b = torch.randn(4) &gt;&gt;&gt; b tensor([ 0.8032,  0.2930, -0.8113, -0.2308]) &gt;&gt;&gt; torch.div(a, b) tensor([[-0.4620, -6.6051,  0.5676,  1.2637],         [ 0.2260, -3.4507, -1.2086,  6.8988],         [ 0.1322,  4.9764, -0.9564,  5.3480],         [-0.2278, -0.1068, -1.4678,  6.3936]])   ", "parameters": ["input (Tensor) : the input tensor.", "other (Number) : the number to be divided to each element of input", "input (Tensor) : the numerator tensor", "other (Tensor) : the denominator tensor"], "returns": null, "example": " a = torch.randn(5)\n a\ntensor([ 0.3810,  1.2774, -0.2972, -0.3719,  0.4637])\n torch.div(a, 0.5)\ntensor([ 0.7620,  2.5548, -0.5944, -0.7439,  0.9275])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.div(input,other,out=None)\u2192Tensor", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.div(input,other,out=None)\u2192Tensor", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.digamma", "item_type": "function", "code": "torch.digamma(input,out=None)\u2192Tensor", "description": "Computes the logarithmic derivative of the gamma function on input.  \u03c8(x)=ddxln\u2061(\u0393(x))=\u0393\u2032(x)\u0393(x)\\psi(x) = \\frac{d}{dx} \\ln\\left(\\Gamma\\left(x\\right)\\right) = \\frac{\\Gamma'(x)}{\\Gamma(x)}  \u03c8(x)=dxd\u200bln(\u0393(x))=\u0393(x)\u0393\u2032(x)\u200b   Parameters input (Tensor) \u2013 the tensor to compute the digamma function on   Example: &gt;&gt;&gt; a = torch.tensor([1, 0.5]) &gt;&gt;&gt; torch.digamma(a) tensor([-0.5772, -1.9635])   ", "parameters": ["input (Tensor) : the tensor to compute the digamma function on"], "returns": null, "example": " a = torch.tensor([1, 0.5])\n torch.digamma(a)\ntensor([-0.5772, -1.9635])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.exponential.Exponential.has_rsample", "item_type": "attribute", "code": "has_rsample=True", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.exponential.Exponential.support", "item_type": "attribute", "code": "support=GreaterThan(lower_bound=0.0)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.fishersnedecor.FisherSnedecor", "item_type": "class", "code": "classtorch.distributions.fishersnedecor.FisherSnedecor(df1,df2,validate_args=None)", "description": "Bases: torch.distributions.distribution.Distribution Creates a Fisher-Snedecor distribution parameterized by df1 and df2. Example: &gt;&gt;&gt; m = FisherSnedecor(torch.tensor([1.0]), torch.tensor([2.0])) &gt;&gt;&gt; m.sample()  # Fisher-Snedecor-distributed with df1=1 and df2=2 tensor([ 0.2453])    Parameters  df1 (python:float or Tensor) \u2013 degrees of freedom parameter 1 df2 (python:float or Tensor) \u2013 degrees of freedom parameter 2      arg_constraints = {'df1': GreaterThan(lower_bound=0.0), 'df2': GreaterThan(lower_bound=0.0)}     expand(batch_shape, _instance=None)      has_rsample = True     log_prob(value)      property mean     rsample(sample_shape=torch.Size([]))      support = GreaterThan(lower_bound=0.0)     property variance   ", "parameters": ["df1 (python:float or Tensor) : degrees of freedom parameter 1", "df2 (python:float or Tensor) : degrees of freedom parameter 2"], "returns": null, "example": " m = FisherSnedecor(torch.tensor([1.0]), torch.tensor([2.0]))\n m.sample()  # Fisher-Snedecor-distributed with df1=1 and df2=2\ntensor([ 0.2453])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.fishersnedecor.FisherSnedecor.arg_constraints", "item_type": "attribute", "code": "arg_constraints={'df1':GreaterThan(lower_bound=0.0),'df2':GreaterThan(lower_bound=0.0)}", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.fishersnedecor.FisherSnedecor.has_rsample", "item_type": "attribute", "code": "has_rsample=True", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.fishersnedecor.FisherSnedecor.support", "item_type": "attribute", "code": "support=GreaterThan(lower_bound=0.0)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.gamma.Gamma", "item_type": "class", "code": "classtorch.distributions.gamma.Gamma(concentration,rate,validate_args=None)", "description": "Bases: torch.distributions.exp_family.ExponentialFamily Creates a Gamma distribution parameterized by shape concentration and rate. Example: &gt;&gt;&gt; m = Gamma(torch.tensor([1.0]), torch.tensor([1.0])) &gt;&gt;&gt; m.sample()  # Gamma distributed with concentration=1 and rate=1 tensor([ 0.1046])    Parameters  concentration (python:float or Tensor) \u2013 shape parameter of the distribution (often referred to as alpha) rate (python:float or Tensor) \u2013 rate = 1 / scale of the distribution (often referred to as beta)      arg_constraints = {'concentration': GreaterThan(lower_bound=0.0), 'rate': GreaterThan(lower_bound=0.0)}     entropy()      expand(batch_shape, _instance=None)      has_rsample = True     log_prob(value)      property mean     rsample(sample_shape=torch.Size([]))      support = GreaterThan(lower_bound=0.0)     property variance   ", "parameters": ["concentration (python:float or Tensor) : shape parameter of the distribution(often referred to as alpha)", "rate (python:float or Tensor) : rate = 1 / scale of the distribution(often referred to as beta)"], "returns": null, "example": " m = Gamma(torch.tensor([1.0]), torch.tensor([1.0]))\n m.sample()  # Gamma distributed with concentration=1 and rate=1\ntensor([ 0.1046])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.Module.dump_patches", "item_type": "attribute", "code": "dump_patches=False", "description": "This allows better BC support for load_state_dict(). In state_dict(), the version number will be saved as in the attribute _metadata of the returned state dict, and thus pickled. _metadata is a dictionary with keys that follow the naming convention of state dict. See _load_from_state_dict on how to use this information in loading. If new parameters/buffers are added/removed from a module, this number shall be bumped, and the module\u2019s _load_from_state_dict method can compare the version number and do appropriate changes if the state dict is from before the change. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.Sequential", "item_type": "class", "code": "classtorch.nn.Sequential(*args)", "description": "A sequential container. Modules will be added to it in the order they are passed in the constructor. Alternatively, an ordered dict of modules can also be passed in. To make it easier to understand, here is a small example: # Example of using Sequential model = nn.Sequential(           nn.Conv2d(1,20,5),           nn.ReLU(),           nn.Conv2d(20,64,5),           nn.ReLU()         )  # Example of using Sequential with OrderedDict model = nn.Sequential(OrderedDict([           ('conv1', nn.Conv2d(1,20,5)),           ('relu1', nn.ReLU()),           ('conv2', nn.Conv2d(20,64,5)),           ('relu2', nn.ReLU())         ]))   ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.ModuleList", "item_type": "class", "code": "classtorch.nn.ModuleList(modules=None)", "description": "Holds submodules in a list. ModuleList can be indexed like a regular Python list, but modules it contains are properly registered, and will be visible by all Module methods.  Parameters modules (iterable, optional) \u2013 an iterable of modules to add   Example: class MyModule(nn.Module):     def __init__(self):         super(MyModule, self).__init__()         self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])      def forward(self, x):         # ModuleList can act as an iterable, or be indexed using ints         for i, l in enumerate(self.linears):             x = self.linears[i // 2](x) + l(x)         return x     append(module)  Appends a given module to the end of the list.  Parameters module (nn.Module) \u2013 module to append       extend(modules)  Appends modules from a Python iterable to the end of the list.  Parameters modules (iterable) \u2013 iterable of modules to append       insert(index, module)  Insert a given module before a given index in the list.  Parameters  index (python:int) \u2013 index to insert. module (nn.Module) \u2013 module to insert      ", "parameters": ["index (python:int) : index to insert.", "module (nn.Module) : module to insert"], "returns": null, "example": "class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])\n\n    def forward(self, x):\n        # ModuleList can act as an iterable, or be indexed using ints\n        for i, l in enumerate(self.linears):\n            x = self.linears[i // 2](x) + l(x)\n        return x\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.ModuleDict", "item_type": "class", "code": "classtorch.nn.ModuleDict(modules=None)", "description": "Holds submodules in a dictionary. ModuleDict can be indexed like a regular Python dictionary, but modules it contains are properly registered, and will be visible by all Module methods. ModuleDict is an ordered dictionary that respects  the order of insertion, and in update(), the order of the merged OrderedDict or another ModuleDict (the argument to update()).  Note that update() with other unordered mapping types (e.g., Python\u2019s plain dict) does not preserve the order of the merged mapping.  Parameters modules (iterable, optional) \u2013 a mapping (dictionary) of (string: module) or an iterable of key-value pairs of type (string, module)   Example: class MyModule(nn.Module):     def __init__(self):         super(MyModule, self).__init__()         self.choices = nn.ModuleDict({                 'conv': nn.Conv2d(10, 10, 3),                 'pool': nn.MaxPool2d(3)         })         self.activations = nn.ModuleDict([                 ['lrelu', nn.LeakyReLU()],                 ['prelu', nn.PReLU()]         ])      def forward(self, x, choice, act):         x = self.choices[choice](x)         x = self.activations[act](x)         return x     clear()  Remove all items from the ModuleDict.     items()  Return an iterable of the ModuleDict key/value pairs.     keys()  Return an iterable of the ModuleDict keys.     pop(key)  Remove key from the ModuleDict and return its module.  Parameters key (string) \u2013 key to pop from the ModuleDict       update(modules)  Update the ModuleDict with the key-value pairs from a mapping or an iterable, overwriting existing keys.  Note If modules is an OrderedDict, a ModuleDict, or an iterable of key-value pairs, the order of new elements in it is preserved.   Parameters modules (iterable) \u2013 a mapping (dictionary) from string to Module, or an iterable of key-value pairs of type (string, Module)       values()  Return an iterable of the ModuleDict values.   ", "parameters": ["modules (iterable, optional) : a mapping (dictionary) of (string: module)or an iterable of key-value pairs of type (string, module)", "key (string) : key to pop from the ModuleDict", "modules (iterable) : a mapping (dictionary) from string to Module,or an iterable of key-value pairs of type (string, Module)"], "returns": null, "example": "class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.choices = nn.ModuleDict({\n                'conv': nn.Conv2d(10, 10, 3),\n                'pool': nn.MaxPool2d(3)\n        })\n        self.activations = nn.ModuleDict([\n                ['lrelu', nn.LeakyReLU()],\n                ['prelu', nn.PReLU()]\n        ])\n\n    def forward(self, x, choice, act):\n        x = self.choices[choice](x)\n        x = self.activations[act](x)\n        return x\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.ParameterList", "item_type": "class", "code": "classtorch.nn.ParameterList(parameters=None)", "description": "Holds parameters in a list. ParameterList can be indexed like a regular Python list, but parameters it contains are properly registered, and will be visible by all Module methods.  Parameters parameters (iterable, optional) \u2013 an iterable of Parameter to add   Example: class MyModule(nn.Module):     def __init__(self):         super(MyModule, self).__init__()         self.params = nn.ParameterList([nn.Parameter(torch.randn(10, 10)) for i in range(10)])      def forward(self, x):         # ParameterList can act as an iterable, or be indexed using ints         for i, p in enumerate(self.params):             x = self.params[i // 2].mm(x) + p.mm(x)         return x     append(parameter)  Appends a given parameter at the end of the list.  Parameters parameter (nn.Parameter) \u2013 parameter to append       extend(parameters)  Appends parameters from a Python iterable to the end of the list.  Parameters parameters (iterable) \u2013 iterable of parameters to append     ", "parameters": ["parameters (iterable, optional) : an iterable of Parameter to add", "parameter (nn.Parameter) : parameter to append", "parameters (iterable) : iterable of parameters to append"], "returns": null, "example": "class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.params = nn.ParameterList([nn.Parameter(torch.randn(10, 10)) for i in range(10)])\n\n    def forward(self, x):\n        # ParameterList can act as an iterable, or be indexed using ints\n        for i, p in enumerate(self.params):\n            x = self.params[i // 2].mm(x) + p.mm(x)\n        return x\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.ParameterDict", "item_type": "class", "code": "classtorch.nn.ParameterDict(parameters=None)", "description": "Holds parameters in a dictionary. ParameterDict can be indexed like a regular Python dictionary, but parameters it contains are properly registered, and will be visible by all Module methods. ParameterDict is an ordered dictionary that respects  the order of insertion, and in update(), the order of the merged OrderedDict or another ParameterDict (the argument to update()).  Note that update() with other unordered mapping types (e.g., Python\u2019s plain dict) does not preserve the order of the merged mapping.  Parameters parameters (iterable, optional) \u2013 a mapping (dictionary) of (string : Parameter) or an iterable of key-value pairs of type (string, Parameter)   Example: class MyModule(nn.Module):     def __init__(self):         super(MyModule, self).__init__()         self.params = nn.ParameterDict({                 'left': nn.Parameter(torch.randn(5, 10)),                 'right': nn.Parameter(torch.randn(5, 10))         })      def forward(self, x, choice):         x = self.params[choice].mm(x)         return x     clear()  Remove all items from the ParameterDict.     items()  Return an iterable of the ParameterDict key/value pairs.     keys()  Return an iterable of the ParameterDict keys.     pop(key)  Remove key from the ParameterDict and return its parameter.  Parameters key (string) \u2013 key to pop from the ParameterDict       update(parameters)  Update the ParameterDict with the key-value pairs from a mapping or an iterable, overwriting existing keys.  Note If parameters is an OrderedDict, a ParameterDict, or an iterable of key-value pairs, the order of new elements in it is preserved.   Parameters parameters (iterable) \u2013 a mapping (dictionary) from string to Parameter, or an iterable of key-value pairs of type (string, Parameter)       values()  Return an iterable of the ParameterDict values.   ", "parameters": ["parameters (iterable, optional) : a mapping (dictionary) of(string : Parameter) or an iterable of key-value pairsof type (string, Parameter)", "key (string) : key to pop from the ParameterDict", "parameters (iterable) : a mapping (dictionary) from string toParameter, or an iterable ofkey-value pairs of type (string, Parameter)"], "returns": null, "example": "class MyModule(nn.Module):\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.params = nn.ParameterDict({\n                'left': nn.Parameter(torch.randn(5, 10)),\n                'right': nn.Parameter(torch.randn(5, 10))\n        })\n\n    def forward(self, x, choice):\n        x = self.params[choice].mm(x)\n        return x\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.resize_as_", "item_type": "method", "code": "resize_as_(tensor)\u2192Tensor", "description": "Resizes the self tensor to be the same size as the specified tensor. This is equivalent to self.resize_(tensor.size()). ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "method", "code": "retain_grad()", "description": "Enables .grad attribute for non-leaf Tensors. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.rfft", "item_type": "method", "code": "rfft(signal_ndim,normalized=False,onesided=True)\u2192Tensor", "description": "See torch.rfft() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.roll", "item_type": "method", "code": "roll(shifts,dims)\u2192Tensor", "description": "See torch.roll() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.rot90", "item_type": "method", "code": "rot90(k,dims)\u2192Tensor", "description": "See torch.rot90() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.round", "item_type": "method", "code": "round()\u2192Tensor", "description": "See torch.round() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.round_", "item_type": "method", "code": "round_()\u2192Tensor", "description": "In-place version of round() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.rsqrt", "item_type": "method", "code": "rsqrt()\u2192Tensor", "description": "See torch.rsqrt() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.rsqrt_", "item_type": "method", "code": "rsqrt_()\u2192Tensor", "description": "In-place version of rsqrt() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.scatter", "item_type": "method", "code": "scatter(dim,index,source)\u2192Tensor", "description": "Out-of-place version of torch.Tensor.scatter_() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.scatter_", "item_type": "method", "code": "scatter_(dim,index,src)\u2192Tensor", "description": "Writes all values from the tensor src into self at the indices specified in the index tensor. For each value in src, its output index is specified by its index in src for dimension != dim and by the corresponding value in index for dimension = dim. For a 3-D tensor, self is updated as: self[index[i][j][k]][j][k] = src[i][j][k]  # if dim == 0 self[i][index[i][j][k]][k] = src[i][j][k]  # if dim == 1 self[i][j][index[i][j][k]] = src[i][j][k]  # if dim == 2   This is the reverse operation of the manner described in gather(). self, index and src (if it is a Tensor) should have same number of dimensions. It is also required that index.size(d) &lt;= src.size(d) for all dimensions d, and that index.size(d) &lt;= self.size(d) for all dimensions d != dim. Moreover, as for gather(), the values of index must be between 0 and self.size(dim) - 1 inclusive, and all values in a row along the specified dimension dim must be unique.  Parameters  dim (python:int) \u2013 the axis along which to index index (LongTensor) \u2013 the indices of elements to scatter, can be either empty or the same size of src. When empty, the operation returns identity src (Tensor) \u2013 the source element(s) to scatter, incase value is not specified value (python:float) \u2013 the source element(s) to scatter, incase src is not specified    Example: &gt;&gt;&gt; x = torch.rand(2, 5) &gt;&gt;&gt; x tensor([[ 0.3992,  0.2908,  0.9044,  0.4850,  0.6004],         [ 0.5735,  0.9006,  0.6797,  0.4152,  0.1732]]) &gt;&gt;&gt; torch.zeros(3, 5).scatter_(0, torch.tensor([[0, 1, 2, 0, 0], [2, 0, 0, 1, 2]]), x) tensor([[ 0.3992,  0.9006,  0.6797,  0.4850,  0.6004],         [ 0.0000,  0.2908,  0.0000,  0.4152,  0.0000],         [ 0.5735,  0.0000,  0.9044,  0.0000,  0.1732]])  &gt;&gt;&gt; z = torch.zeros(2, 4).scatter_(1, torch.tensor([[2], [3]]), 1.23) &gt;&gt;&gt; z tensor([[ 0.0000,  0.0000,  1.2300,  0.0000],         [ 0.0000,  0.0000,  0.0000,  1.2300]])   ", "parameters": ["dim (python:int) : the axis along which to index", "index (LongTensor) : the indices of elements to scatter,can be either empty or the same size of src.When empty, the operation returns identity", "src (Tensor) : the source element(s) to scatter,incase value is not specified", "value (python:float) : the source element(s) to scatter,incase src is not specified"], "returns": null, "example": " x = torch.rand(2, 5)\n x\ntensor([[ 0.3992,  0.2908,  0.9044,  0.4850,  0.6004],\n        [ 0.5735,  0.9006,  0.6797,  0.4152,  0.1732]])\n torch.zeros(3, 5).scatter_(0, torch.tensor([[0, 1, 2, 0, 0], [2, 0, 0, 1, 2]]), x)\ntensor([[ 0.3992,  0.9006,  0.6797,  0.4850,  0.6004],\n        [ 0.0000,  0.2908,  0.0000,  0.4152,  0.0000],\n        [ 0.5735,  0.0000,  0.9044,  0.0000,  0.1732]])\n\n z = torch.zeros(2, 4).scatter_(1, torch.tensor([[2], [3]]), 1.23)\n z\ntensor([[ 0.0000,  0.0000,  1.2300,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  1.2300]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.erf", "item_type": "function", "code": "torch.erf(input,out=None)\u2192Tensor", "description": "Computes the error function of each element. The error function is defined as follows:  erf(x)=2\u03c0\u222b0xe\u2212t2dt\\mathrm{erf}(x) = \\frac{2}{\\sqrt{\\pi}} \\int_{0}^{x} e^{-t^2} dt  erf(x)=\u03c0\u200b2\u200b\u222b0x\u200be\u2212t2dt   Parameters  input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; torch.erf(torch.tensor([0, -1., 10.])) tensor([ 0.0000, -0.8427,  1.0000])   ", "parameters": ["input (Tensor) : the input tensor.", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " torch.erf(torch.tensor([0, -1., 10.]))\ntensor([ 0.0000, -0.8427,  1.0000])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.erfc", "item_type": "function", "code": "torch.erfc(input,out=None)\u2192Tensor", "description": "Computes the complementary error function of each element of input. The complementary error function is defined as follows:  erfc(x)=1\u22122\u03c0\u222b0xe\u2212t2dt\\mathrm{erfc}(x) = 1 - \\frac{2}{\\sqrt{\\pi}} \\int_{0}^{x} e^{-t^2} dt  erfc(x)=1\u2212\u03c0\u200b2\u200b\u222b0x\u200be\u2212t2dt   Parameters  input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; torch.erfc(torch.tensor([0, -1., 10.])) tensor([ 1.0000, 1.8427,  0.0000])   ", "parameters": ["input (Tensor) : the input tensor.", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " torch.erfc(torch.tensor([0, -1., 10.]))\ntensor([ 1.0000, 1.8427,  0.0000])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.erfinv", "item_type": "function", "code": "torch.erfinv(input,out=None)\u2192Tensor", "description": "Computes the inverse error function of each element of input. The inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1)   as:  erfinv(erf(x))=x\\mathrm{erfinv}(\\mathrm{erf}(x)) = x  erfinv(erf(x))=x   Parameters  input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; torch.erfinv(torch.tensor([0, 0.5, -1.])) tensor([ 0.0000,  0.4769,    -inf])   ", "parameters": ["input (Tensor) : the input tensor.", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " torch.erfinv(torch.tensor([0, 0.5, -1.]))\ntensor([ 0.0000,  0.4769,    -inf])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.exp", "item_type": "function", "code": "torch.exp(input,out=None)\u2192Tensor", "description": "Returns a new tensor with the exponential of the elements of the input tensor input.  yi=exiy_{i} = e^{x_{i}}  yi\u200b=exi\u200b   Parameters  input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; torch.exp(torch.tensor([0, math.log(2.)])) tensor([ 1.,  2.])   ", "parameters": ["input (Tensor) : the input tensor.", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " torch.exp(torch.tensor([0, math.log(2.)]))\ntensor([ 1.,  2.])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.expm1", "item_type": "function", "code": "torch.expm1(input,out=None)\u2192Tensor", "description": "Returns a new tensor with the exponential of the elements minus 1 of input.  yi=exi\u22121y_{i} = e^{x_{i}} - 1  yi\u200b=exi\u200b\u22121   Parameters  input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; torch.expm1(torch.tensor([0, math.log(2.)])) tensor([ 0.,  1.])   ", "parameters": ["input (Tensor) : the input tensor.", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " torch.expm1(torch.tensor([0, math.log(2.)]))\ntensor([ 0.,  1.])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.floor", "item_type": "function", "code": "torch.floor(input,out=None)\u2192Tensor", "description": "Returns a new tensor with the floor of the elements of input, the largest integer less than or equal to each element.  outi=\u230ainputi\u230b\\text{out}_{i} = \\left\\lfloor \\text{input}_{i} \\right\\rfloor  outi\u200b=\u230ainputi\u200b\u230b   Parameters  input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a tensor([-0.8166,  1.5308, -0.2530, -0.2091]) &gt;&gt;&gt; torch.floor(a) tensor([-1.,  1., -1., -1.])   ", "parameters": ["input (Tensor) : the input tensor.", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " a = torch.randn(4)\n a\ntensor([-0.8166,  1.5308, -0.2530, -0.2091])\n torch.floor(a)\ntensor([-1.,  1., -1., -1.])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.gamma.Gamma.arg_constraints", "item_type": "attribute", "code": "arg_constraints={'concentration':GreaterThan(lower_bound=0.0),'rate':GreaterThan(lower_bound=0.0)}", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.gamma.Gamma.has_rsample", "item_type": "attribute", "code": "has_rsample=True", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.gamma.Gamma.support", "item_type": "attribute", "code": "support=GreaterThan(lower_bound=0.0)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.geometric.Geometric", "item_type": "class", "code": "classtorch.distributions.geometric.Geometric(probs=None,logits=None,validate_args=None)", "description": "Bases: torch.distributions.distribution.Distribution Creates a Geometric distribution parameterized by probs, where probs is the probability of success of Bernoulli trials. It represents the probability that in k+1k + 1k+1   Bernoulli trials, the first kkk   trials failed, before seeing a success. Samples are non-negative integers [0, inf\u2061\\infinf  ). Example: &gt;&gt;&gt; m = Geometric(torch.tensor([0.3])) &gt;&gt;&gt; m.sample()  # underlying Bernoulli has 30% chance 1; 70% chance 0 tensor([ 2.])    Parameters  probs (Number, Tensor) \u2013 the probability of sampling 1. Must be in range (0, 1] logits (Number, Tensor) \u2013 the log-odds of sampling 1.      arg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)}     entropy()      expand(batch_shape, _instance=None)      log_prob(value)      logits      property mean     probs      sample(sample_shape=torch.Size([]))      support = IntegerGreaterThan(lower_bound=0)     property variance   ", "parameters": ["probs (Number, Tensor) : the probability of sampling 1. Must be in range (0, 1]", "logits (Number, Tensor) : the log-odds of sampling 1."], "returns": null, "example": " m = Geometric(torch.tensor([0.3]))\n m.sample()  # underlying Bernoulli has 30% chance 1; 70% chance 0\ntensor([ 2.])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.geometric.Geometric.arg_constraints", "item_type": "attribute", "code": "arg_constraints={'logits':Real(),'probs':Interval(lower_bound=0.0,upper_bound=1.0)}", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.geometric.Geometric.logits", "item_type": "attribute", "code": "logits", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.geometric.Geometric.probs", "item_type": "attribute", "code": "probs", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.geometric.Geometric.support", "item_type": "attribute", "code": "support=IntegerGreaterThan(lower_bound=0)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.gumbel.Gumbel", "item_type": "class", "code": "classtorch.distributions.gumbel.Gumbel(loc,scale,validate_args=None)", "description": "Bases: torch.distributions.transformed_distribution.TransformedDistribution Samples from a Gumbel Distribution. Examples: &gt;&gt;&gt; m = Gumbel(torch.tensor([1.0]), torch.tensor([2.0])) &gt;&gt;&gt; m.sample()  # sample from Gumbel distribution with loc=1, scale=2 tensor([ 1.0124])    Parameters  loc (python:float or Tensor) \u2013 Location parameter of the distribution scale (python:float or Tensor) \u2013 Scale parameter of the distribution      arg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)}     entropy()      expand(batch_shape, _instance=None)      log_prob(value)      property mean     property stddev     support = Real()     property variance   ", "parameters": ["loc (python:float or Tensor) : Location parameter of the distribution", "scale (python:float or Tensor) : Scale parameter of the distribution"], "returns": null, "example": " m = Gumbel(torch.tensor([1.0]), torch.tensor([2.0]))\n m.sample()  # sample from Gumbel distribution with loc=1, scale=2\ntensor([ 1.0124])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.Conv1d", "item_type": "class", "code": "classtorch.nn.Conv1d(in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')", "description": "Applies a 1D convolution over an input signal composed of several input planes. In the simplest case, the output value of the layer with input size (N,Cin,L)(N, C_{\\text{in}}, L)(N,Cin\u200b,L)   and output (N,Cout,Lout)(N, C_{\\text{out}}, L_{\\text{out}})(N,Cout\u200b,Lout\u200b)   can be precisely described as:  out(Ni,Coutj)=bias(Coutj)+\u2211k=0Cin\u22121weight(Coutj,k)\u22c6input(Ni,k)\\text{out}(N_i, C_{\\text{out}_j}) = \\text{bias}(C_{\\text{out}_j}) + \\sum_{k = 0}^{C_{in} - 1} \\text{weight}(C_{\\text{out}_j}, k) \\star \\text{input}(N_i, k)  out(Ni\u200b,Coutj\u200b\u200b)=bias(Coutj\u200b\u200b)+k=0\u2211Cin\u200b\u22121\u200bweight(Coutj\u200b\u200b,k)\u22c6input(Ni\u200b,k)  where \u22c6\\star\u22c6   is the valid cross-correlation operator, NNN   is a batch size, CCC   denotes a number of channels, LLL   is a length of signal sequence.  stride controls the stride for the cross-correlation, a single number or a one-element tuple. padding controls the amount of implicit zero-paddings on both sides for padding number of points. dilation controls the spacing between the kernel points; also known as the \u00e0 trous algorithm. It is harder to describe, but this link has a nice visualization of what dilation does. groups controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups. For example,   At groups=1, all inputs are convolved to all outputs. At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels, and producing half the output channels, and both subsequently concatenated. At groups= in_channels, each input channel is convolved with its own set of filters, of size \u230aout_channelsin_channels\u230b\\left\\lfloor\\frac{out\\_channels}{in\\_channels}\\right\\rfloor\u230ain_channelsout_channels\u200b\u230b  .      Note Depending of the size of your kernel, several (of the last) columns of the input might be lost, because it is a valid cross-correlation, and not a full cross-correlation. It is up to the user to add proper padding.   Note When groups == in_channels and out_channels == K * in_channels, where K is a positive integer, this operation is also termed in literature as depthwise convolution. In other words, for an input of size (N,Cin,Lin)(N, C_{in}, L_{in})(N,Cin\u200b,Lin\u200b)  , a depthwise convolution with a depthwise multiplier K, can be constructed by arguments (Cin=Cin,Cout=Cin\u00d7K,...,groups=Cin)(C_\\text{in}=C_{in}, C_\\text{out}=C_{in} \\times K, ..., \\text{groups}=C_{in})(Cin\u200b=Cin\u200b,Cout\u200b=Cin\u200b\u00d7K,...,groups=Cin\u200b)  .   Note In some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. Please see the notes on Reproducibility for background.   Parameters  in_channels (python:int) \u2013 Number of channels in the input image out_channels (python:int) \u2013 Number of channels produced by the convolution kernel_size (python:int or tuple) \u2013 Size of the convolving kernel stride (python:int or tuple, optional) \u2013 Stride of the convolution. Default: 1 padding (python:int or tuple, optional) \u2013 Zero-padding added to both sides of the input. Default: 0 padding_mode (string, optional) \u2013 zeros dilation (python:int or tuple, optional) \u2013 Spacing between kernel elements. Default: 1 groups (python:int, optional) \u2013 Number of blocked connections from input channels to output channels. Default: 1 bias (bool, optional) \u2013 If True, adds a learnable bias to the output. Default: True     Shape: Input: (N,Cin,Lin)(N, C_{in}, L_{in})(N,Cin\u200b,Lin\u200b)   Output: (N,Cout,Lout)(N, C_{out}, L_{out})(N,Cout\u200b,Lout\u200b)   where  Lout=\u230aLin+2\u00d7padding\u2212dilation\u00d7(kernel_size\u22121)\u22121stride+1\u230bL_{out} = \\left\\lfloor\\frac{L_{in} + 2 \\times \\text{padding} - \\text{dilation}           \\times (\\text{kernel\\_size} - 1) - 1}{\\text{stride}} + 1\\right\\rfloor  Lout\u200b=\u230astrideLin\u200b+2\u00d7padding\u2212dilation\u00d7(kernel_size\u22121)\u22121\u200b+1\u230b       Variables  ~Conv1d.weight (Tensor) \u2013 the learnable weights of the module of shape (out_channels,in_channelsgroups,kernel_size)(\\text{out\\_channels}, \\frac{\\text{in\\_channels}}{\\text{groups}}, \\text{kernel\\_size})(out_channels,groupsin_channels\u200b,kernel_size)  . The values of these weights are sampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b)   where k=1Cin\u2217kernel_sizek = \\frac{1}{C_\\text{in} * \\text{kernel\\_size}}k=Cin\u200b\u2217kernel_size1\u200b   ~Conv1d.bias (Tensor) \u2013 the learnable bias of the module of shape (out_channels). If bias is True, then the values of these weights are sampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b)   where k=1Cin\u2217kernel_sizek = \\frac{1}{C_\\text{in} * \\text{kernel\\_size}}k=Cin\u200b\u2217kernel_size1\u200b      Examples: &gt;&gt;&gt; m = nn.Conv1d(16, 33, 3, stride=2) &gt;&gt;&gt; input = torch.randn(20, 16, 50) &gt;&gt;&gt; output = m(input)   ", "parameters": ["in_channels (python:int) : Number of channels in the input image", "out_channels (python:int) : Number of channels produced by the convolution", "kernel_size (python:int or tuple) : Size of the convolving kernel", "stride (python:int or tuple, optional) : Stride of the convolution. Default: 1", "padding (python:int or tuple, optional) : Zero-padding added to both sides ofthe input. Default: 0", "padding_mode (string, optional) : zeros", "dilation (python:int or tuple, optional) : Spacing between kernelelements. Default: 1", "groups (python:int, optional) : Number of blocked connections from inputchannels to output channels. Default: 1", "bias (bool, optional) : If True, adds a learnable bias to the output. Default: True", "~Conv1d.weight (Tensor) : the learnable weights of the module of shape(out_channels,in_channelsgroups,kernel_size)(\\text{out\\_channels}, \\frac{\\text{in\\_channels}}{\\text{groups}}, \\text{kernel\\_size})(out_channels,groupsin_channels\u200b,kernel_size).The values of these weights are sampled fromU(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b) wherek=1Cin\u2217kernel_sizek = \\frac{1}{C_\\text{in} * \\text{kernel\\_size}}k=Cin\u200b\u2217kernel_size1\u200b", "~Conv1d.bias (Tensor) : the learnable bias of the module of shape(out_channels). If bias is True, then the values of these weights aresampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b) wherek=1Cin\u2217kernel_sizek = \\frac{1}{C_\\text{in} * \\text{kernel\\_size}}k=Cin\u200b\u2217kernel_size1\u200b"], "returns": null, "example": " m = nn.Conv1d(16, 33, 3, stride=2)\n input = torch.randn(20, 16, 50)\n output = m(input)\n\n", "shape": " Input: (N,Cin,Lin)(N, C_{in}, L_{in})(N,Cin\u200b,Lin\u200b)   Output: (N,Cout,Lout)(N, C_{out}, L_{out})(N,Cout\u200b,Lout\u200b)   where  Lout=\u230aLin+2\u00d7padding\u2212dilation\u00d7(kernel_size\u22121)\u22121stride+1\u230bL_{out} = \\left\\lfloor\\frac{L_{in} + 2 \\times \\text{padding} - \\text{dilation}           \\times (\\text{kernel\\_size} - 1) - 1}{\\text{stride}} + 1\\right\\rfloor  Lout\u200b=\u230astrideLin\u200b+2\u00d7padding\u2212dilation\u00d7(kernel_size\u22121)\u22121\u200b+1\u230b    "},
{"library": "torch", "item_id": "torch.Tensor.scatter_add_", "item_type": "method", "code": "scatter_add_(dim,index,other)\u2192Tensor", "description": "Adds all values from the tensor other into self at the indices specified in the index tensor in a similar fashion as scatter_(). For each value in other, it is added to an index in self which is specified by its index in other for dimension != dim and by the corresponding value in index for dimension = dim. For a 3-D tensor, self is updated as: self[index[i][j][k]][j][k] += other[i][j][k]  # if dim == 0 self[i][index[i][j][k]][k] += other[i][j][k]  # if dim == 1 self[i][j][index[i][j][k]] += other[i][j][k]  # if dim == 2   self, index and other should have same number of dimensions. It is also required that index.size(d) &lt;= other.size(d) for all dimensions d, and that index.size(d) &lt;= self.size(d) for all dimensions d != dim.  Note When using the CUDA backend, this operation may induce nondeterministic behaviour that is not easily switched off. Please see the notes on Reproducibility for background.   Parameters  dim (python:int) \u2013 the axis along which to index index (LongTensor) \u2013 the indices of elements to scatter and add, can be either empty or the same size of src. When empty, the operation returns identity. other (Tensor) \u2013 the source elements to scatter and add    Example: &gt;&gt;&gt; x = torch.rand(2, 5) &gt;&gt;&gt; x tensor([[0.7404, 0.0427, 0.6480, 0.3806, 0.8328],         [0.7953, 0.2009, 0.9154, 0.6782, 0.9620]]) &gt;&gt;&gt; torch.ones(3, 5).scatter_add_(0, torch.tensor([[0, 1, 2, 0, 0], [2, 0, 0, 1, 2]]), x) tensor([[1.7404, 1.2009, 1.9154, 1.3806, 1.8328],         [1.0000, 1.0427, 1.0000, 1.6782, 1.0000],         [1.7953, 1.0000, 1.6480, 1.0000, 1.9620]])   ", "parameters": ["dim (python:int) : the axis along which to index", "index (LongTensor) : the indices of elements to scatter and add,can be either empty or the same size of src.When empty, the operation returns identity.", "other (Tensor) : the source elements to scatter and add"], "returns": null, "example": " x = torch.rand(2, 5)\n x\ntensor([[0.7404, 0.0427, 0.6480, 0.3806, 0.8328],\n        [0.7953, 0.2009, 0.9154, 0.6782, 0.9620]])\n torch.ones(3, 5).scatter_add_(0, torch.tensor([[0, 1, 2, 0, 0], [2, 0, 0, 1, 2]]), x)\ntensor([[1.7404, 1.2009, 1.9154, 1.3806, 1.8328],\n        [1.0000, 1.0427, 1.0000, 1.6782, 1.0000],\n        [1.7953, 1.0000, 1.6480, 1.0000, 1.9620]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.scatter_add", "item_type": "method", "code": "scatter_add(dim,index,source)\u2192Tensor", "description": "Out-of-place version of torch.Tensor.scatter_add_() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.select", "item_type": "method", "code": "select(dim,index)\u2192Tensor", "description": "Slices the self tensor along the selected dimension at the given index. This function returns a tensor with the given dimension removed.  Parameters  dim (python:int) \u2013 the dimension to slice index (python:int) \u2013 the index to select with     Note select() is equivalent to slicing. For example, tensor.select(0, index) is equivalent to tensor[index] and tensor.select(2, index) is equivalent to tensor[:,:,index].  ", "parameters": ["dim (python:int) : the dimension to slice", "index (python:int) : the index to select with"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.set_", "item_type": "method", "code": "set_(source=None,storage_offset=0,size=None,stride=None)\u2192Tensor", "description": "Sets the underlying storage, size, and strides. If source is a tensor, self tensor will share the same storage and have the same size and strides as source. Changes to elements in one tensor will be reflected in the other. If source is a Storage, the method sets the underlying storage, offset, size, and stride.  Parameters  source (Tensor or Storage) \u2013 the tensor or storage to use storage_offset (python:int, optional) \u2013 the offset in the storage size (torch.Size, optional) \u2013 the desired size. Defaults to the size of the source. stride (tuple, optional) \u2013 the desired stride. Defaults to C-contiguous strides.    ", "parameters": ["source (Tensor or Storage) : the tensor or storage to use", "storage_offset (python:int, optional) : the offset in the storage", "size (torch.Size, optional) : the desired size. Defaults to the size of the source.", "stride (tuple, optional) : the desired stride. Defaults to C-contiguous strides."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.share_memory_", "item_type": "method", "code": "share_memory_()", "description": "Moves the underlying storage to shared memory. This is a no-op if the underlying storage is already in shared memory and for CUDA tensors. Tensors in shared memory cannot be resized. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.short", "item_type": "method", "code": "short()\u2192Tensor", "description": "self.short() is equivalent to self.to(torch.int16). See to(). ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.fmod", "item_type": "function", "code": "torch.fmod(input,other,out=None)\u2192Tensor", "description": "Computes the element-wise remainder of division. The dividend and divisor may contain both for integer and floating point numbers. The remainder has the same sign as the dividend input. When other is a tensor, the shapes of input and other must be broadcastable.  Parameters  input (Tensor) \u2013 the dividend other (Tensor or python:float) \u2013 the divisor, which may be either a number or a tensor of the same shape as the dividend out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; torch.fmod(torch.tensor([-3., -2, -1, 1, 2, 3]), 2) tensor([-1., -0., -1.,  1.,  0.,  1.]) &gt;&gt;&gt; torch.fmod(torch.tensor([1., 2, 3, 4, 5]), 1.5) tensor([ 1.0000,  0.5000,  0.0000,  1.0000,  0.5000])   ", "parameters": ["input (Tensor) : the dividend", "other (Tensor or python:float) : the divisor, which may be either a number or a tensor of the same shape as the dividend", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " torch.fmod(torch.tensor([-3., -2, -1, 1, 2, 3]), 2)\ntensor([-1., -0., -1.,  1.,  0.,  1.])\n torch.fmod(torch.tensor([1., 2, 3, 4, 5]), 1.5)\ntensor([ 1.0000,  0.5000,  0.0000,  1.0000,  0.5000])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.frac", "item_type": "function", "code": "torch.frac(input,out=None)\u2192Tensor", "description": "Computes the fractional portion of each element in input.  outi=inputi\u2212\u230a\u2223inputi\u2223\u230b\u2217sgn\u2061(inputi)\\text{out}_{i} = \\text{input}_{i} - \\left\\lfloor |\\text{input}_{i}| \\right\\rfloor * \\operatorname{sgn}(\\text{input}_{i})  outi\u200b=inputi\u200b\u2212\u230a\u2223inputi\u200b\u2223\u230b\u2217sgn(inputi\u200b)  Example: &gt;&gt;&gt; torch.frac(torch.tensor([1, 2.5, -3.2])) tensor([ 0.0000,  0.5000, -0.2000])   ", "parameters": [], "returns": null, "example": " torch.frac(torch.tensor([1, 2.5, -3.2]))\ntensor([ 0.0000,  0.5000, -0.2000])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.imag", "item_type": "function", "code": "torch.imag(input,out=None)\u2192Tensor", "description": "Computes the element-wise imag value of the given input tensor.  outi=imag(inputi)\\text{out}_{i} = imag(\\text{input}_{i})  outi\u200b=imag(inputi\u200b)   Parameters  input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; torch.imag(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j])) tensor([ 1,  2,  -3])   ", "parameters": ["input (Tensor) : the input tensor.", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " torch.imag(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))\ntensor([ 1,  2,  -3])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.lerp", "item_type": "function", "code": "torch.lerp(input,end,weight,out=None)", "description": "Does a linear interpolation of two tensors start (given by input) and end based on a scalar or tensor weight and returns the resulting out tensor.  outi=starti+weighti\u00d7(endi\u2212starti)\\text{out}_i = \\text{start}_i + \\text{weight}_i \\times (\\text{end}_i - \\text{start}_i)  outi\u200b=starti\u200b+weighti\u200b\u00d7(endi\u200b\u2212starti\u200b)  The shapes of start and end must be broadcastable. If weight is a tensor, then the shapes of weight, start, and end must be broadcastable.  Parameters  input (Tensor) \u2013 the tensor with the starting points end (Tensor) \u2013 the tensor with the ending points weight (python:float or tensor) \u2013 the weight for the interpolation formula out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; start = torch.arange(1., 5.) &gt;&gt;&gt; end = torch.empty(4).fill_(10) &gt;&gt;&gt; start tensor([ 1.,  2.,  3.,  4.]) &gt;&gt;&gt; end tensor([ 10.,  10.,  10.,  10.]) &gt;&gt;&gt; torch.lerp(start, end, 0.5) tensor([ 5.5000,  6.0000,  6.5000,  7.0000]) &gt;&gt;&gt; torch.lerp(start, end, torch.full_like(start, 0.5)) tensor([ 5.5000,  6.0000,  6.5000,  7.0000])   ", "parameters": ["input (Tensor) : the tensor with the starting points", "end (Tensor) : the tensor with the ending points", "weight (python:float or tensor) : the weight for the interpolation formula", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " start = torch.arange(1., 5.)\n end = torch.empty(4).fill_(10)\n start\ntensor([ 1.,  2.,  3.,  4.])\n end\ntensor([ 10.,  10.,  10.,  10.])\n torch.lerp(start, end, 0.5)\ntensor([ 5.5000,  6.0000,  6.5000,  7.0000])\n torch.lerp(start, end, torch.full_like(start, 0.5))\ntensor([ 5.5000,  6.0000,  6.5000,  7.0000])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.lgamma", "item_type": "function", "code": "torch.lgamma(input,out=None)\u2192Tensor", "description": "Computes the logarithm of the gamma function on input.  outi=log\u2061\u0393(inputi)\\text{out}_{i} = \\log \\Gamma(\\text{input}_{i})  outi\u200b=log\u0393(inputi\u200b)   Parameters  input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; a = torch.arange(0.5, 2, 0.5) &gt;&gt;&gt; torch.lgamma(a) tensor([ 0.5724,  0.0000, -0.1208])   ", "parameters": ["input (Tensor) : the input tensor.", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " a = torch.arange(0.5, 2, 0.5)\n torch.lgamma(a)\ntensor([ 0.5724,  0.0000, -0.1208])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.log", "item_type": "function", "code": "torch.log(input,out=None)\u2192Tensor", "description": "Returns a new tensor with the natural logarithm of the elements of input.  yi=log\u2061e(xi)y_{i} = \\log_{e} (x_{i})  yi\u200b=loge\u200b(xi\u200b)   Parameters  input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; a = torch.randn(5) &gt;&gt;&gt; a tensor([-0.7168, -0.5471, -0.8933, -1.4428, -0.1190]) &gt;&gt;&gt; torch.log(a) tensor([ nan,  nan,  nan,  nan,  nan])   ", "parameters": ["input (Tensor) : the input tensor.", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " a = torch.randn(5)\n a\ntensor([-0.7168, -0.5471, -0.8933, -1.4428, -0.1190])\n torch.log(a)\ntensor([ nan,  nan,  nan,  nan,  nan])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.gumbel.Gumbel.arg_constraints", "item_type": "attribute", "code": "arg_constraints={'loc':Real(),'scale':GreaterThan(lower_bound=0.0)}", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.gumbel.Gumbel.support", "item_type": "attribute", "code": "support=Real()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.half_cauchy.HalfCauchy", "item_type": "class", "code": "classtorch.distributions.half_cauchy.HalfCauchy(scale,validate_args=None)", "description": "Bases: torch.distributions.transformed_distribution.TransformedDistribution Creates a half-normal distribution parameterized by scale where: X ~ Cauchy(0, scale) Y = |X| ~ HalfCauchy(scale)   Example: &gt;&gt;&gt; m = HalfCauchy(torch.tensor([1.0])) &gt;&gt;&gt; m.sample()  # half-cauchy distributed with scale=1 tensor([ 2.3214])    Parameters scale (python:float or Tensor) \u2013 scale of the full Cauchy distribution     arg_constraints = {'scale': GreaterThan(lower_bound=0.0)}     cdf(value)      entropy()      expand(batch_shape, _instance=None)      has_rsample = True     icdf(prob)      log_prob(value)      property mean     property scale     support = GreaterThan(lower_bound=0.0)     property variance   ", "parameters": ["scale (python:float or Tensor) : scale of the full Cauchy distribution"], "returns": null, "example": " m = HalfCauchy(torch.tensor([1.0]))\n m.sample()  # half-cauchy distributed with scale=1\ntensor([ 2.3214])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.half_cauchy.HalfCauchy.arg_constraints", "item_type": "attribute", "code": "arg_constraints={'scale':GreaterThan(lower_bound=0.0)}", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.Conv2d", "item_type": "class", "code": "classtorch.nn.Conv2d(in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')", "description": "Applies a 2D convolution over an input signal composed of several input planes. In the simplest case, the output value of the layer with input size (N,Cin,H,W)(N, C_{\\text{in}}, H, W)(N,Cin\u200b,H,W)   and output (N,Cout,Hout,Wout)(N, C_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})(N,Cout\u200b,Hout\u200b,Wout\u200b)   can be precisely described as:  out(Ni,Coutj)=bias(Coutj)+\u2211k=0Cin\u22121weight(Coutj,k)\u22c6input(Ni,k)\\text{out}(N_i, C_{\\text{out}_j}) = \\text{bias}(C_{\\text{out}_j}) + \\sum_{k = 0}^{C_{\\text{in}} - 1} \\text{weight}(C_{\\text{out}_j}, k) \\star \\text{input}(N_i, k)  out(Ni\u200b,Coutj\u200b\u200b)=bias(Coutj\u200b\u200b)+k=0\u2211Cin\u200b\u22121\u200bweight(Coutj\u200b\u200b,k)\u22c6input(Ni\u200b,k)  where \u22c6\\star\u22c6   is the valid 2D cross-correlation operator, NNN   is a batch size, CCC   denotes a number of channels, HHH   is a height of input planes in pixels, and WWW   is width in pixels.  stride controls the stride for the cross-correlation, a single number or a tuple. padding controls the amount of implicit zero-paddings on both sides for padding number of points for each dimension. dilation controls the spacing between the kernel points; also known as the \u00e0 trous algorithm. It is harder to describe, but this link has a nice visualization of what dilation does. groups controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups. For example,   At groups=1, all inputs are convolved to all outputs. At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels, and producing half the output channels, and both subsequently concatenated. At groups= in_channels, each input channel is convolved with its own set of filters, of size: \u230aout_channelsin_channels\u230b\\left\\lfloor\\frac{out\\_channels}{in\\_channels}\\right\\rfloor\u230ain_channelsout_channels\u200b\u230b  .     The parameters kernel_size, stride, padding, dilation can either be:   a single int \u2013 in which case the same value is used for the height and width dimension a tuple of two ints \u2013 in which case, the first int is used for the height dimension, and the second int for the width dimension    Note Depending of the size of your kernel, several (of the last) columns of the input might be lost, because it is a valid cross-correlation, and not a full cross-correlation. It is up to the user to add proper padding.   Note When groups == in_channels and out_channels == K * in_channels, where K is a positive integer, this operation is also termed in literature as depthwise convolution. In other words, for an input of size (N,Cin,Hin,Win)(N, C_{in}, H_{in}, W_{in})(N,Cin\u200b,Hin\u200b,Win\u200b)  , a depthwise convolution with a depthwise multiplier K, can be constructed by arguments (in_channels=Cin,out_channels=Cin\u00d7K,...,groups=Cin)(in\\_channels=C_{in}, out\\_channels=C_{in} \\times K, ..., groups=C_{in})(in_channels=Cin\u200b,out_channels=Cin\u200b\u00d7K,...,groups=Cin\u200b)  .   Note In some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. Please see the notes on Reproducibility for background.   Parameters  in_channels (python:int) \u2013 Number of channels in the input image out_channels (python:int) \u2013 Number of channels produced by the convolution kernel_size (python:int or tuple) \u2013 Size of the convolving kernel stride (python:int or tuple, optional) \u2013 Stride of the convolution. Default: 1 padding (python:int or tuple, optional) \u2013 Zero-padding added to both sides of the input. Default: 0 padding_mode (string, optional) \u2013 zeros dilation (python:int or tuple, optional) \u2013 Spacing between kernel elements. Default: 1 groups (python:int, optional) \u2013 Number of blocked connections from input channels to output channels. Default: 1 bias (bool, optional) \u2013 If True, adds a learnable bias to the output. Default: True     Shape: Input: (N,Cin,Hin,Win)(N, C_{in}, H_{in}, W_{in})(N,Cin\u200b,Hin\u200b,Win\u200b)   Output: (N,Cout,Hout,Wout)(N, C_{out}, H_{out}, W_{out})(N,Cout\u200b,Hout\u200b,Wout\u200b)   where  Hout=\u230aHin+2\u00d7padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121stride[0]+1\u230bH_{out} = \\left\\lfloor\\frac{H_{in}  + 2 \\times \\text{padding}[0] - \\text{dilation}[0]           \\times (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor  Hout\u200b=\u230astride[0]Hin\u200b+2\u00d7padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121\u200b+1\u230b   Wout=\u230aWin+2\u00d7padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121stride[1]+1\u230bW_{out} = \\left\\lfloor\\frac{W_{in}  + 2 \\times \\text{padding}[1] - \\text{dilation}[1]           \\times (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor  Wout\u200b=\u230astride[1]Win\u200b+2\u00d7padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121\u200b+1\u230b       Variables  ~Conv2d.weight (Tensor) \u2013 the learnable weights of the module of shape (out_channels,in_channelsgroups,(\\text{out\\_channels}, \\frac{\\text{in\\_channels}}{\\text{groups}},(out_channels,groupsin_channels\u200b,   kernel_size[0],kernel_size[1])\\text{kernel\\_size[0]}, \\text{kernel\\_size[1]})kernel_size[0],kernel_size[1])  . The values of these weights are sampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b)   where k=1Cin\u2217\u220fi=01kernel_size[i]k = \\frac{1}{C_\\text{in} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}k=Cin\u200b\u2217\u220fi=01\u200bkernel_size[i]1\u200b   ~Conv2d.bias (Tensor) \u2013 the learnable bias of the module of shape (out_channels). If bias is True, then the values of these weights are sampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b)   where k=1Cin\u2217\u220fi=01kernel_size[i]k = \\frac{1}{C_\\text{in} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}k=Cin\u200b\u2217\u220fi=01\u200bkernel_size[i]1\u200b      Examples: &gt;&gt;&gt; # With square kernels and equal stride &gt;&gt;&gt; m = nn.Conv2d(16, 33, 3, stride=2) &gt;&gt;&gt; # non-square kernels and unequal stride and with padding &gt;&gt;&gt; m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2)) &gt;&gt;&gt; # non-square kernels and unequal stride and with padding and dilation &gt;&gt;&gt; m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1)) &gt;&gt;&gt; input = torch.randn(20, 16, 50, 100) &gt;&gt;&gt; output = m(input)   ", "parameters": ["in_channels (python:int) : Number of channels in the input image", "out_channels (python:int) : Number of channels produced by the convolution", "kernel_size (python:int or tuple) : Size of the convolving kernel", "stride (python:int or tuple, optional) : Stride of the convolution. Default: 1", "padding (python:int or tuple, optional) : Zero-padding added to both sides of the input. Default: 0", "padding_mode (string, optional) : zeros", "dilation (python:int or tuple, optional) : Spacing between kernel elements. Default: 1", "groups (python:int, optional) : Number of blocked connections from input channels to output channels. Default: 1", "bias (bool, optional) : If True, adds a learnable bias to the output. Default: True", "~Conv2d.weight (Tensor) : the learnable weights of the module of shape(out_channels,in_channelsgroups,(\\text{out\\_channels}, \\frac{\\text{in\\_channels}}{\\text{groups}},(out_channels,groupsin_channels\u200b,kernel_size[0],kernel_size[1])\\text{kernel\\_size[0]}, \\text{kernel\\_size[1]})kernel_size[0],kernel_size[1]).The values of these weights are sampled fromU(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b) wherek=1Cin\u2217\u220fi=01kernel_size[i]k = \\frac{1}{C_\\text{in} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}k=Cin\u200b\u2217\u220fi=01\u200bkernel_size[i]1\u200b", "~Conv2d.bias (Tensor) : the learnable bias of the module of shape (out_channels). If bias is True,then the values of these weights aresampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b) wherek=1Cin\u2217\u220fi=01kernel_size[i]k = \\frac{1}{C_\\text{in} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}k=Cin\u200b\u2217\u220fi=01\u200bkernel_size[i]1\u200b"], "returns": null, "example": " # With square kernels and equal stride\n m = nn.Conv2d(16, 33, 3, stride=2)\n # non-square kernels and unequal stride and with padding\n m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n # non-square kernels and unequal stride and with padding and dilation\n m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))\n input = torch.randn(20, 16, 50, 100)\n output = m(input)\n\n", "shape": " Input: (N,Cin,Hin,Win)(N, C_{in}, H_{in}, W_{in})(N,Cin\u200b,Hin\u200b,Win\u200b)   Output: (N,Cout,Hout,Wout)(N, C_{out}, H_{out}, W_{out})(N,Cout\u200b,Hout\u200b,Wout\u200b)   where  Hout=\u230aHin+2\u00d7padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121stride[0]+1\u230bH_{out} = \\left\\lfloor\\frac{H_{in}  + 2 \\times \\text{padding}[0] - \\text{dilation}[0]           \\times (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor  Hout\u200b=\u230astride[0]Hin\u200b+2\u00d7padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121\u200b+1\u230b   Wout=\u230aWin+2\u00d7padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121stride[1]+1\u230bW_{out} = \\left\\lfloor\\frac{W_{in}  + 2 \\times \\text{padding}[1] - \\text{dilation}[1]           \\times (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor  Wout\u200b=\u230astride[1]Win\u200b+2\u00d7padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121\u200b+1\u230b    "},
{"library": "torch", "item_id": "torch.Tensor.sigmoid", "item_type": "method", "code": "sigmoid()\u2192Tensor", "description": "See torch.sigmoid() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.sigmoid_", "item_type": "method", "code": "sigmoid_()\u2192Tensor", "description": "In-place version of sigmoid() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.sign", "item_type": "method", "code": "sign()\u2192Tensor", "description": "See torch.sign() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.sign_", "item_type": "method", "code": "sign_()\u2192Tensor", "description": "In-place version of sign() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.sin", "item_type": "method", "code": "sin()\u2192Tensor", "description": "See torch.sin() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.sin_", "item_type": "method", "code": "sin_()\u2192Tensor", "description": "In-place version of sin() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.sinh", "item_type": "method", "code": "sinh()\u2192Tensor", "description": "See torch.sinh() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.sinh_", "item_type": "method", "code": "sinh_()\u2192Tensor", "description": "In-place version of sinh() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.size", "item_type": "method", "code": "size()\u2192torch.Size", "description": "Returns the size of the self tensor. The returned value is a subclass of tuple. Example: &gt;&gt;&gt; torch.empty(3, 4, 5).size() torch.Size([3, 4, 5])   ", "parameters": [], "returns": null, "example": " torch.empty(3, 4, 5).size()\ntorch.Size([3, 4, 5])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.slogdet", "item_type": "method", "code": "slogdet()-&gt;(Tensor,Tensor)", "description": "See torch.slogdet() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.solve", "item_type": "method", "code": "solve(A)\u2192Tensor,Tensor", "description": "See torch.solve() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.sort", "item_type": "method", "code": "sort(dim=-1,descending=False)-&gt;(Tensor,LongTensor)", "description": "See torch.sort() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.split", "item_type": "method", "code": "split(split_size,dim=0)", "description": "See torch.split() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.log10", "item_type": "function", "code": "torch.log10(input,out=None)\u2192Tensor", "description": "Returns a new tensor with the logarithm to the base 10 of the elements of input.  yi=log\u206110(xi)y_{i} = \\log_{10} (x_{i})  yi\u200b=log10\u200b(xi\u200b)   Parameters  input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; a = torch.rand(5) &gt;&gt;&gt; a tensor([ 0.5224,  0.9354,  0.7257,  0.1301,  0.2251])   &gt;&gt;&gt; torch.log10(a) tensor([-0.2820, -0.0290, -0.1392, -0.8857, -0.6476])   ", "parameters": ["input (Tensor) : the input tensor.", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " a = torch.rand(5)\n a\ntensor([ 0.5224,  0.9354,  0.7257,  0.1301,  0.2251])\n\n\n torch.log10(a)\ntensor([-0.2820, -0.0290, -0.1392, -0.8857, -0.6476])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.log1p", "item_type": "function", "code": "torch.log1p(input,out=None)\u2192Tensor", "description": "Returns a new tensor with the natural logarithm of (1 + input).  yi=log\u2061e(xi+1)y_i = \\log_{e} (x_i + 1)  yi\u200b=loge\u200b(xi\u200b+1)   Note This function is more accurate than torch.log() for small values of input   Parameters  input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; a = torch.randn(5) &gt;&gt;&gt; a tensor([-1.0090, -0.9923,  1.0249, -0.5372,  0.2492]) &gt;&gt;&gt; torch.log1p(a) tensor([    nan, -4.8653,  0.7055, -0.7705,  0.2225])   ", "parameters": ["input (Tensor) : the input tensor.", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " a = torch.randn(5)\n a\ntensor([-1.0090, -0.9923,  1.0249, -0.5372,  0.2492])\n torch.log1p(a)\ntensor([    nan, -4.8653,  0.7055, -0.7705,  0.2225])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.log2", "item_type": "function", "code": "torch.log2(input,out=None)\u2192Tensor", "description": "Returns a new tensor with the logarithm to the base 2 of the elements of input.  yi=log\u20612(xi)y_{i} = \\log_{2} (x_{i})  yi\u200b=log2\u200b(xi\u200b)   Parameters  input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; a = torch.rand(5) &gt;&gt;&gt; a tensor([ 0.8419,  0.8003,  0.9971,  0.5287,  0.0490])   &gt;&gt;&gt; torch.log2(a) tensor([-0.2483, -0.3213, -0.0042, -0.9196, -4.3504])   ", "parameters": ["input (Tensor) : the input tensor.", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " a = torch.rand(5)\n a\ntensor([ 0.8419,  0.8003,  0.9971,  0.5287,  0.0490])\n\n\n torch.log2(a)\ntensor([-0.2483, -0.3213, -0.0042, -0.9196, -4.3504])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.logical_not", "item_type": "function", "code": "torch.logical_not(input,out=None)\u2192Tensor", "description": "Computes the element-wise logical NOT of the given input tensor. If not specified, the output tensor will have the bool dtype. If the input tensor is not a bool tensor, zeros are treated as False and non-zeros are treated as True.  Parameters  input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; torch.logical_not(torch.tensor([True, False])) tensor([ False,  True]) &gt;&gt;&gt; torch.logical_not(torch.tensor([0, 1, -10], dtype=torch.int8)) tensor([ True, False, False]) &gt;&gt;&gt; torch.logical_not(torch.tensor([0., 1.5, -10.], dtype=torch.double)) tensor([ True, False, False]) &gt;&gt;&gt; torch.logical_not(torch.tensor([0., 1., -10.], dtype=torch.double), out=torch.empty(3, dtype=torch.int16)) tensor([1, 0, 0], dtype=torch.int16)   ", "parameters": ["input (Tensor) : the input tensor.", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " torch.logical_not(torch.tensor([True, False]))\ntensor([ False,  True])\n torch.logical_not(torch.tensor([0, 1, -10], dtype=torch.int8))\ntensor([ True, False, False])\n torch.logical_not(torch.tensor([0., 1.5, -10.], dtype=torch.double))\ntensor([ True, False, False])\n torch.logical_not(torch.tensor([0., 1., -10.], dtype=torch.double), out=torch.empty(3, dtype=torch.int16))\ntensor([1, 0, 0], dtype=torch.int16)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.logical_xor", "item_type": "function", "code": "torch.logical_xor(input,other,out=None)\u2192Tensor", "description": "Computes the element-wise logical XOR of the given input tensors. Zeros are treated as False and nonzeros are treated as True.  Parameters  input (Tensor) \u2013 the input tensor. other (Tensor) \u2013 the tensor to compute XOR with out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; torch.logical_xor(torch.tensor([True, False, True]), torch.tensor([True, False, False])) tensor([ False, False,  True]) &gt;&gt;&gt; a = torch.tensor([0, 1, 10, 0], dtype=torch.int8) &gt;&gt;&gt; b = torch.tensor([4, 0, 1, 0], dtype=torch.int8) &gt;&gt;&gt; torch.logical_xor(a, b) tensor([ True,  True, False, False]) &gt;&gt;&gt; torch.logical_xor(a.double(), b.double()) tensor([ True,  True, False, False]) &gt;&gt;&gt; torch.logical_xor(a.double(), b) tensor([ True,  True, False, False]) &gt;&gt;&gt; torch.logical_xor(a, b, out=torch.empty(4, dtype=torch.bool)) tensor([ True,  True, False, False])   ", "parameters": ["input (Tensor) : the input tensor.", "other (Tensor) : the tensor to compute XOR with", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " torch.logical_xor(torch.tensor([True, False, True]), torch.tensor([True, False, False]))\ntensor([ False, False,  True])\n a = torch.tensor([0, 1, 10, 0], dtype=torch.int8)\n b = torch.tensor([4, 0, 1, 0], dtype=torch.int8)\n torch.logical_xor(a, b)\ntensor([ True,  True, False, False])\n torch.logical_xor(a.double(), b.double())\ntensor([ True,  True, False, False])\n torch.logical_xor(a.double(), b)\ntensor([ True,  True, False, False])\n torch.logical_xor(a, b, out=torch.empty(4, dtype=torch.bool))\ntensor([ True,  True, False, False])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.mul", "item_type": "function", "code": "torch.mul()", "description": "  torch.mul(input, other, out=None)   Multiplies each element of the input input with the scalar other and returns a new resulting tensor.  outi=other\u00d7inputi\\text{out}_i = \\text{other} \\times \\text{input}_i  outi\u200b=other\u00d7inputi\u200b  If input is of type FloatTensor or DoubleTensor, other should be a real number, otherwise it should be an integer  Parameters  {input} \u2013  value (Number) \u2013 the number to be multiplied to each element of input {out} \u2013     Example: &gt;&gt;&gt; a = torch.randn(3) &gt;&gt;&gt; a tensor([ 0.2015, -0.4255,  2.6087]) &gt;&gt;&gt; torch.mul(a, 100) tensor([  20.1494,  -42.5491,  260.8663])     torch.mul(input, other, out=None)   Each element of the tensor input is multiplied by the corresponding element of the Tensor other. The resulting tensor is returned. The shapes of input and other must be broadcastable.  outi=inputi\u00d7otheri\\text{out}_i = \\text{input}_i \\times \\text{other}_i  outi\u200b=inputi\u200b\u00d7otheri\u200b   Parameters  input (Tensor) \u2013 the first multiplicand tensor other (Tensor) \u2013 the second multiplicand tensor out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; a = torch.randn(4, 1) &gt;&gt;&gt; a tensor([[ 1.1207],         [-0.3137],         [ 0.0700],         [ 0.8378]]) &gt;&gt;&gt; b = torch.randn(1, 4) &gt;&gt;&gt; b tensor([[ 0.5146,  0.1216, -0.5244,  2.2382]]) &gt;&gt;&gt; torch.mul(a, b) tensor([[ 0.5767,  0.1363, -0.5877,  2.5083],         [-0.1614, -0.0382,  0.1645, -0.7021],         [ 0.0360,  0.0085, -0.0367,  0.1567],         [ 0.4312,  0.1019, -0.4394,  1.8753]])   ", "parameters": ["{input} : ", "value (Number) : the number to be multiplied to each element of input", "{out} : ", "input (Tensor) : the first multiplicand tensor", "other (Tensor) : the second multiplicand tensor", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " a = torch.randn(3)\n a\ntensor([ 0.2015, -0.4255,  2.6087])\n torch.mul(a, 100)\ntensor([  20.1494,  -42.5491,  260.8663])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.half_cauchy.HalfCauchy.has_rsample", "item_type": "attribute", "code": "has_rsample=True", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.half_cauchy.HalfCauchy.support", "item_type": "attribute", "code": "support=GreaterThan(lower_bound=0.0)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.half_normal.HalfNormal", "item_type": "class", "code": "classtorch.distributions.half_normal.HalfNormal(scale,validate_args=None)", "description": "Bases: torch.distributions.transformed_distribution.TransformedDistribution Creates a half-normal distribution parameterized by scale where: X ~ Normal(0, scale) Y = |X| ~ HalfNormal(scale)   Example: &gt;&gt;&gt; m = HalfNormal(torch.tensor([1.0])) &gt;&gt;&gt; m.sample()  # half-normal distributed with scale=1 tensor([ 0.1046])    Parameters scale (python:float or Tensor) \u2013 scale of the full Normal distribution     arg_constraints = {'scale': GreaterThan(lower_bound=0.0)}     cdf(value)      entropy()      expand(batch_shape, _instance=None)      has_rsample = True     icdf(prob)      log_prob(value)      property mean     property scale     support = GreaterThan(lower_bound=0.0)     property variance   ", "parameters": ["scale (python:float or Tensor) : scale of the full Normal distribution"], "returns": null, "example": " m = HalfNormal(torch.tensor([1.0]))\n m.sample()  # half-normal distributed with scale=1\ntensor([ 0.1046])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.half_normal.HalfNormal.arg_constraints", "item_type": "attribute", "code": "arg_constraints={'scale':GreaterThan(lower_bound=0.0)}", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.half_normal.HalfNormal.has_rsample", "item_type": "attribute", "code": "has_rsample=True", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.half_normal.HalfNormal.support", "item_type": "attribute", "code": "support=GreaterThan(lower_bound=0.0)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.independent.Independent", "item_type": "class", "code": "classtorch.distributions.independent.Independent(base_distribution,reinterpreted_batch_ndims,validate_args=None)", "description": "Bases: torch.distributions.distribution.Distribution Reinterprets some of the batch dims of a distribution as event dims. This is mainly useful for changing the shape of the result of log_prob(). For example to create a diagonal Normal distribution with the same shape as a Multivariate Normal distribution (so they are interchangeable), you can: &gt;&gt;&gt; loc = torch.zeros(3) &gt;&gt;&gt; scale = torch.ones(3) &gt;&gt;&gt; mvn = MultivariateNormal(loc, scale_tril=torch.diag(scale)) &gt;&gt;&gt; [mvn.batch_shape, mvn.event_shape] [torch.Size(()), torch.Size((3,))] &gt;&gt;&gt; normal = Normal(loc, scale) &gt;&gt;&gt; [normal.batch_shape, normal.event_shape] [torch.Size((3,)), torch.Size(())] &gt;&gt;&gt; diagn = Independent(normal, 1) &gt;&gt;&gt; [diagn.batch_shape, diagn.event_shape] [torch.Size(()), torch.Size((3,))]    Parameters  base_distribution (torch.distributions.distribution.Distribution) \u2013 a base distribution reinterpreted_batch_ndims (python:int) \u2013 the number of batch dims to reinterpret as event dims      arg_constraints = {}     entropy()      enumerate_support(expand=True)      expand(batch_shape, _instance=None)      property has_enumerate_support     property has_rsample     log_prob(value)      property mean     rsample(sample_shape=torch.Size([]))      sample(sample_shape=torch.Size([]))      property support     property variance   ", "parameters": ["base_distribution (torch.distributions.distribution.Distribution) : abase distribution", "reinterpreted_batch_ndims (python:int) : the number of batch dims toreinterpret as event dims"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.independent.Independent.arg_constraints", "item_type": "attribute", "code": "arg_constraints={}", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.laplace.Laplace", "item_type": "class", "code": "classtorch.distributions.laplace.Laplace(loc,scale,validate_args=None)", "description": "Bases: torch.distributions.distribution.Distribution Creates a Laplace distribution parameterized by loc and :attr:\u2019scale\u2019. Example: &gt;&gt;&gt; m = Laplace(torch.tensor([0.0]), torch.tensor([1.0])) &gt;&gt;&gt; m.sample()  # Laplace distributed with loc=0, scale=1 tensor([ 0.1046])    Parameters  loc (python:float or Tensor) \u2013 mean of the distribution scale (python:float or Tensor) \u2013 scale of the distribution      arg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)}     cdf(value)      entropy()      expand(batch_shape, _instance=None)      has_rsample = True     icdf(value)      log_prob(value)      property mean     rsample(sample_shape=torch.Size([]))      property stddev     support = Real()     property variance   ", "parameters": ["loc (python:float or Tensor) : mean of the distribution", "scale (python:float or Tensor) : scale of the distribution"], "returns": null, "example": " m = Laplace(torch.tensor([0.0]), torch.tensor([1.0]))\n m.sample()  # Laplace distributed with loc=0, scale=1\ntensor([ 0.1046])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.Conv3d", "item_type": "class", "code": "classtorch.nn.Conv3d(in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode='zeros')", "description": "Applies a 3D convolution over an input signal composed of several input planes. In the simplest case, the output value of the layer with input size (N,Cin,D,H,W)(N, C_{in}, D, H, W)(N,Cin\u200b,D,H,W)   and output (N,Cout,Dout,Hout,Wout)(N, C_{out}, D_{out}, H_{out}, W_{out})(N,Cout\u200b,Dout\u200b,Hout\u200b,Wout\u200b)   can be precisely described as:  out(Ni,Coutj)=bias(Coutj)+\u2211k=0Cin\u22121weight(Coutj,k)\u22c6input(Ni,k)out(N_i, C_{out_j}) = bias(C_{out_j}) +                         \\sum_{k = 0}^{C_{in} - 1} weight(C_{out_j}, k) \\star input(N_i, k)  out(Ni\u200b,Coutj\u200b\u200b)=bias(Coutj\u200b\u200b)+k=0\u2211Cin\u200b\u22121\u200bweight(Coutj\u200b\u200b,k)\u22c6input(Ni\u200b,k)  where \u22c6\\star\u22c6   is the valid 3D cross-correlation operator  stride controls the stride for the cross-correlation. padding controls the amount of implicit zero-paddings on both sides for padding number of points for each dimension. dilation controls the spacing between the kernel points; also known as the \u00e0 trous algorithm. It is harder to describe, but this link has a nice visualization of what dilation does. groups controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups. For example,   At groups=1, all inputs are convolved to all outputs. At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels, and producing half the output channels, and both subsequently concatenated. At groups= in_channels, each input channel is convolved with its own set of filters, of size \u230aout_channelsin_channels\u230b\\left\\lfloor\\frac{out\\_channels}{in\\_channels}\\right\\rfloor\u230ain_channelsout_channels\u200b\u230b  .     The parameters kernel_size, stride, padding, dilation can either be:   a single int \u2013 in which case the same value is used for the depth, height and width dimension a tuple of three ints \u2013 in which case, the first int is used for the depth dimension, the second int for the height dimension and the third int for the width dimension    Note Depending of the size of your kernel, several (of the last) columns of the input might be lost, because it is a valid cross-correlation, and not a full cross-correlation. It is up to the user to add proper padding.   Note When groups == in_channels and out_channels == K * in_channels, where K is a positive integer, this operation is also termed in literature as depthwise convolution. In other words, for an input of size (N,Cin,Din,Hin,Win)(N, C_{in}, D_{in}, H_{in}, W_{in})(N,Cin\u200b,Din\u200b,Hin\u200b,Win\u200b)  , a depthwise convolution with a depthwise multiplier K, can be constructed by arguments (in_channels=Cin,out_channels=Cin\u00d7K,...,groups=Cin)(in\\_channels=C_{in}, out\\_channels=C_{in} \\times K, ..., groups=C_{in})(in_channels=Cin\u200b,out_channels=Cin\u200b\u00d7K,...,groups=Cin\u200b)  .   Note In some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. Please see the notes on Reproducibility for background.   Parameters  in_channels (python:int) \u2013 Number of channels in the input image out_channels (python:int) \u2013 Number of channels produced by the convolution kernel_size (python:int or tuple) \u2013 Size of the convolving kernel stride (python:int or tuple, optional) \u2013 Stride of the convolution. Default: 1 padding (python:int or tuple, optional) \u2013 Zero-padding added to all three sides of the input. Default: 0 padding_mode (string, optional) \u2013 zeros dilation (python:int or tuple, optional) \u2013 Spacing between kernel elements. Default: 1 groups (python:int, optional) \u2013 Number of blocked connections from input channels to output channels. Default: 1 bias (bool, optional) \u2013 If True, adds a learnable bias to the output. Default: True     Shape: Input: (N,Cin,Din,Hin,Win)(N, C_{in}, D_{in}, H_{in}, W_{in})(N,Cin\u200b,Din\u200b,Hin\u200b,Win\u200b)   Output: (N,Cout,Dout,Hout,Wout)(N, C_{out}, D_{out}, H_{out}, W_{out})(N,Cout\u200b,Dout\u200b,Hout\u200b,Wout\u200b)   where  Dout=\u230aDin+2\u00d7padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121stride[0]+1\u230bD_{out} = \\left\\lfloor\\frac{D_{in} + 2 \\times \\text{padding}[0] - \\text{dilation}[0]       \\times (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor  Dout\u200b=\u230astride[0]Din\u200b+2\u00d7padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121\u200b+1\u230b   Hout=\u230aHin+2\u00d7padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121stride[1]+1\u230bH_{out} = \\left\\lfloor\\frac{H_{in} + 2 \\times \\text{padding}[1] - \\text{dilation}[1]       \\times (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor  Hout\u200b=\u230astride[1]Hin\u200b+2\u00d7padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121\u200b+1\u230b   Wout=\u230aWin+2\u00d7padding[2]\u2212dilation[2]\u00d7(kernel_size[2]\u22121)\u22121stride[2]+1\u230bW_{out} = \\left\\lfloor\\frac{W_{in} + 2 \\times \\text{padding}[2] - \\text{dilation}[2]       \\times (\\text{kernel\\_size}[2] - 1) - 1}{\\text{stride}[2]} + 1\\right\\rfloor  Wout\u200b=\u230astride[2]Win\u200b+2\u00d7padding[2]\u2212dilation[2]\u00d7(kernel_size[2]\u22121)\u22121\u200b+1\u230b       Variables  ~Conv3d.weight (Tensor) \u2013 the learnable weights of the module of shape (out_channels,in_channelsgroups,(\\text{out\\_channels}, \\frac{\\text{in\\_channels}}{\\text{groups}},(out_channels,groupsin_channels\u200b,   kernel_size[0],kernel_size[1],kernel_size[2])\\text{kernel\\_size[0]}, \\text{kernel\\_size[1]}, \\text{kernel\\_size[2]})kernel_size[0],kernel_size[1],kernel_size[2])  . The values of these weights are sampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b)   where k=1Cin\u2217\u220fi=02kernel_size[i]k = \\frac{1}{C_\\text{in} * \\prod_{i=0}^{2}\\text{kernel\\_size}[i]}k=Cin\u200b\u2217\u220fi=02\u200bkernel_size[i]1\u200b   ~Conv3d.bias (Tensor) \u2013 the learnable bias of the module of shape (out_channels). If bias is True, then the values of these weights are sampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b)   where k=1Cin\u2217\u220fi=02kernel_size[i]k = \\frac{1}{C_\\text{in} * \\prod_{i=0}^{2}\\text{kernel\\_size}[i]}k=Cin\u200b\u2217\u220fi=02\u200bkernel_size[i]1\u200b      Examples: &gt;&gt;&gt; # With square kernels and equal stride &gt;&gt;&gt; m = nn.Conv3d(16, 33, 3, stride=2) &gt;&gt;&gt; # non-square kernels and unequal stride and with padding &gt;&gt;&gt; m = nn.Conv3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(4, 2, 0)) &gt;&gt;&gt; input = torch.randn(20, 16, 10, 50, 100) &gt;&gt;&gt; output = m(input)   ", "parameters": ["in_channels (python:int) : Number of channels in the input image", "out_channels (python:int) : Number of channels produced by the convolution", "kernel_size (python:int or tuple) : Size of the convolving kernel", "stride (python:int or tuple, optional) : Stride of the convolution. Default: 1", "padding (python:int or tuple, optional) : Zero-padding added to all three sides of the input. Default: 0", "padding_mode (string, optional) : zeros", "dilation (python:int or tuple, optional) : Spacing between kernel elements. Default: 1", "groups (python:int, optional) : Number of blocked connections from input channels to output channels. Default: 1", "bias (bool, optional) : If True, adds a learnable bias to the output. Default: True", "~Conv3d.weight (Tensor) : the learnable weights of the module of shape(out_channels,in_channelsgroups,(\\text{out\\_channels}, \\frac{\\text{in\\_channels}}{\\text{groups}},(out_channels,groupsin_channels\u200b,kernel_size[0],kernel_size[1],kernel_size[2])\\text{kernel\\_size[0]}, \\text{kernel\\_size[1]}, \\text{kernel\\_size[2]})kernel_size[0],kernel_size[1],kernel_size[2]).The values of these weights are sampled fromU(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b) wherek=1Cin\u2217\u220fi=02kernel_size[i]k = \\frac{1}{C_\\text{in} * \\prod_{i=0}^{2}\\text{kernel\\_size}[i]}k=Cin\u200b\u2217\u220fi=02\u200bkernel_size[i]1\u200b", "~Conv3d.bias (Tensor) : the learnable bias of the module of shape (out_channels). If bias is True,then the values of these weights aresampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b) wherek=1Cin\u2217\u220fi=02kernel_size[i]k = \\frac{1}{C_\\text{in} * \\prod_{i=0}^{2}\\text{kernel\\_size}[i]}k=Cin\u200b\u2217\u220fi=02\u200bkernel_size[i]1\u200b"], "returns": null, "example": " # With square kernels and equal stride\n m = nn.Conv3d(16, 33, 3, stride=2)\n # non-square kernels and unequal stride and with padding\n m = nn.Conv3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(4, 2, 0))\n input = torch.randn(20, 16, 10, 50, 100)\n output = m(input)\n\n", "shape": " Input: (N,Cin,Din,Hin,Win)(N, C_{in}, D_{in}, H_{in}, W_{in})(N,Cin\u200b,Din\u200b,Hin\u200b,Win\u200b)   Output: (N,Cout,Dout,Hout,Wout)(N, C_{out}, D_{out}, H_{out}, W_{out})(N,Cout\u200b,Dout\u200b,Hout\u200b,Wout\u200b)   where  Dout=\u230aDin+2\u00d7padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121stride[0]+1\u230bD_{out} = \\left\\lfloor\\frac{D_{in} + 2 \\times \\text{padding}[0] - \\text{dilation}[0]       \\times (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor  Dout\u200b=\u230astride[0]Din\u200b+2\u00d7padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121\u200b+1\u230b   Hout=\u230aHin+2\u00d7padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121stride[1]+1\u230bH_{out} = \\left\\lfloor\\frac{H_{in} + 2 \\times \\text{padding}[1] - \\text{dilation}[1]       \\times (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor  Hout\u200b=\u230astride[1]Hin\u200b+2\u00d7padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121\u200b+1\u230b   Wout=\u230aWin+2\u00d7padding[2]\u2212dilation[2]\u00d7(kernel_size[2]\u22121)\u22121stride[2]+1\u230bW_{out} = \\left\\lfloor\\frac{W_{in} + 2 \\times \\text{padding}[2] - \\text{dilation}[2]       \\times (\\text{kernel\\_size}[2] - 1) - 1}{\\text{stride}[2]} + 1\\right\\rfloor  Wout\u200b=\u230astride[2]Win\u200b+2\u00d7padding[2]\u2212dilation[2]\u00d7(kernel_size[2]\u22121)\u22121\u200b+1\u230b    "},
{"library": "torch", "item_id": "torch.Tensor.sparse_mask", "item_type": "method", "code": "sparse_mask(input,mask)\u2192Tensor", "description": "Returns a new SparseTensor with values from Tensor input filtered by indices of mask and values are ignored. input and mask must have the same shape.  Parameters  input (Tensor) \u2013 an input Tensor mask (SparseTensor) \u2013 a SparseTensor which we filter input based on its indices    Example: &gt;&gt;&gt; nnz = 5 &gt;&gt;&gt; dims = [5, 5, 2, 2] &gt;&gt;&gt; I = torch.cat([torch.randint(0, dims[0], size=(nnz,)),                    torch.randint(0, dims[1], size=(nnz,))], 0).reshape(2, nnz) &gt;&gt;&gt; V = torch.randn(nnz, dims[2], dims[3]) &gt;&gt;&gt; size = torch.Size(dims) &gt;&gt;&gt; S = torch.sparse_coo_tensor(I, V, size).coalesce() &gt;&gt;&gt; D = torch.randn(dims) &gt;&gt;&gt; D.sparse_mask(S) tensor(indices=tensor([[0, 0, 0, 2],                        [0, 1, 4, 3]]),        values=tensor([[[ 1.6550,  0.2397],                        [-0.1611, -0.0779]],                        [[ 0.2326, -1.0558],                        [ 1.4711,  1.9678]],                        [[-0.5138, -0.0411],                        [ 1.9417,  0.5158]],                        [[ 0.0793,  0.0036],                        [-0.2569, -0.1055]]]),        size=(5, 5, 2, 2), nnz=4, layout=torch.sparse_coo)   ", "parameters": ["input (Tensor) : an input Tensor", "mask (SparseTensor) : a SparseTensor which we filter input based on its indices"], "returns": null, "example": " nnz = 5\n dims = [5, 5, 2, 2]\n I = torch.cat([torch.randint(0, dims[0], size=(nnz,)),\n                   torch.randint(0, dims[1], size=(nnz,))], 0).reshape(2, nnz)\n V = torch.randn(nnz, dims[2], dims[3])\n size = torch.Size(dims)\n S = torch.sparse_coo_tensor(I, V, size).coalesce()\n D = torch.randn(dims)\n D.sparse_mask(S)\ntensor(indices=tensor([[0, 0, 0, 2],\n                       [0, 1, 4, 3]]),\n       values=tensor([[[ 1.6550,  0.2397],\n                       [-0.1611, -0.0779]],\n\n                      [[ 0.2326, -1.0558],\n                       [ 1.4711,  1.9678]],\n\n                      [[-0.5138, -0.0411],\n                       [ 1.9417,  0.5158]],\n\n                      [[ 0.0793,  0.0036],\n                       [-0.2569, -0.1055]]]),\n       size=(5, 5, 2, 2), nnz=4, layout=torch.sparse_coo)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.sparse_dim", "item_type": "method", "code": "sparse_dim()\u2192int", "description": "If self is a sparse COO tensor (i.e., with torch.sparse_coo layout), this returns the number of sparse dimensions. Otherwise, this throws an error. See also Tensor.dense_dim(). ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.sqrt", "item_type": "method", "code": "sqrt()\u2192Tensor", "description": "See torch.sqrt() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.sqrt_", "item_type": "method", "code": "sqrt_()\u2192Tensor", "description": "In-place version of sqrt() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.squeeze", "item_type": "method", "code": "squeeze(dim=None)\u2192Tensor", "description": "See torch.squeeze() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.squeeze_", "item_type": "method", "code": "squeeze_(dim=None)\u2192Tensor", "description": "In-place version of squeeze() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.std", "item_type": "method", "code": "std(dim=None,unbiased=True,keepdim=False)\u2192Tensor", "description": "See torch.std() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.mul(input,other,out=None)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.mul(input,other,out=None)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.mvlgamma", "item_type": "function", "code": "torch.mvlgamma(input,p)\u2192Tensor", "description": "Computes the multivariate log-gamma function ([reference]) with dimension ppp   element-wise, given by  log\u2061(\u0393p(a))=C+\u2211i=1plog\u2061(\u0393(a\u2212i\u221212))\\log(\\Gamma_{p}(a)) = C + \\displaystyle \\sum_{i=1}^{p} \\log\\left(\\Gamma\\left(a - \\frac{i - 1}{2}\\right)\\right)  log(\u0393p\u200b(a))=C+i=1\u2211p\u200blog(\u0393(a\u22122i\u22121\u200b))  where C=log\u2061(\u03c0)\u00d7p(p\u22121)4C = \\log(\\pi) \\times \\frac{p (p - 1)}{4}C=log(\u03c0)\u00d74p(p\u22121)\u200b   and \u0393(\u22c5)\\Gamma(\\cdot)\u0393(\u22c5)   is the Gamma function. If any of the elements are less than or equal to p\u221212\\frac{p - 1}{2}2p\u22121\u200b  , then an error is thrown.  Parameters  input (Tensor) \u2013 the tensor to compute the multivariate log-gamma function p (python:int) \u2013 the number of dimensions    Example: &gt;&gt;&gt; a = torch.empty(2, 3).uniform_(1, 2) &gt;&gt;&gt; a tensor([[1.6835, 1.8474, 1.1929],         [1.0475, 1.7162, 1.4180]]) &gt;&gt;&gt; torch.mvlgamma(a, 2) tensor([[0.3928, 0.4007, 0.7586],         [1.0311, 0.3901, 0.5049]])   ", "parameters": ["input (Tensor) : the tensor to compute the multivariate log-gamma function", "p (python:int) : the number of dimensions"], "returns": null, "example": " a = torch.empty(2, 3).uniform_(1, 2)\n a\ntensor([[1.6835, 1.8474, 1.1929],\n        [1.0475, 1.7162, 1.4180]])\n torch.mvlgamma(a, 2)\ntensor([[0.3928, 0.4007, 0.7586],\n        [1.0311, 0.3901, 0.5049]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.neg", "item_type": "function", "code": "torch.neg(input,out=None)\u2192Tensor", "description": "Returns a new tensor with the negative of the elements of input.  out=\u22121\u00d7input\\text{out} = -1 \\times \\text{input}  out=\u22121\u00d7input   Parameters  input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; a = torch.randn(5) &gt;&gt;&gt; a tensor([ 0.0090, -0.2262, -0.0682, -0.2866,  0.3940]) &gt;&gt;&gt; torch.neg(a) tensor([-0.0090,  0.2262,  0.0682,  0.2866, -0.3940])   ", "parameters": ["input (Tensor) : the input tensor.", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " a = torch.randn(5)\n a\ntensor([ 0.0090, -0.2262, -0.0682, -0.2866,  0.3940])\n torch.neg(a)\ntensor([-0.0090,  0.2262,  0.0682,  0.2866, -0.3940])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.polygamma", "item_type": "function", "code": "torch.polygamma(n,input,out=None)\u2192Tensor", "description": "Computes the nthn^{th}nth   derivative of the digamma function on input. n\u22650n \\geq 0n\u22650   is called the order of the polygamma function.  \u03c8(n)(x)=d(n)dx(n)\u03c8(x)\\psi^{(n)}(x) = \\frac{d^{(n)}}{dx^{(n)}} \\psi(x)  \u03c8(n)(x)=dx(n)d(n)\u200b\u03c8(x)   Note This function is not implemented for n\u22652n \\geq 2n\u22652  .   Parameters  n (python:int) \u2013 the order of the polygamma function input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor.     Example::&gt;&gt;&gt; a = torch.tensor([1, 0.5]) &gt;&gt;&gt; torch.polygamma(1, a) tensor([1.64493, 4.9348])     ", "parameters": ["n (python:int) : the order of the polygamma function", "input (Tensor) : the input tensor.", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " a = torch.tensor([1, 0.5])\n torch.polygamma(1, a)\ntensor([1.64493, 4.9348])\n\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.pow", "item_type": "function", "code": "torch.pow()", "description": "  torch.pow(input, exponent, out=None) \u2192 Tensor   Takes the power of each element in input with exponent and returns a tensor with the result. exponent can be either a single float number or a Tensor with the same number of elements as input. When exponent is a scalar value, the operation applied is:  outi=xiexponent\\text{out}_i = x_i ^ \\text{exponent}  outi\u200b=xiexponent\u200b  When exponent is a tensor, the operation applied is:  outi=xiexponenti\\text{out}_i = x_i ^ {\\text{exponent}_i}  outi\u200b=xiexponenti\u200b\u200b  When exponent is a tensor, the shapes of input and exponent must be broadcastable.  Parameters  input (Tensor) \u2013 the input tensor. exponent (python:float or tensor) \u2013 the exponent value out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a tensor([ 0.4331,  1.2475,  0.6834, -0.2791]) &gt;&gt;&gt; torch.pow(a, 2) tensor([ 0.1875,  1.5561,  0.4670,  0.0779]) &gt;&gt;&gt; exp = torch.arange(1., 5.)  &gt;&gt;&gt; a = torch.arange(1., 5.) &gt;&gt;&gt; a tensor([ 1.,  2.,  3.,  4.]) &gt;&gt;&gt; exp tensor([ 1.,  2.,  3.,  4.]) &gt;&gt;&gt; torch.pow(a, exp) tensor([   1.,    4.,   27.,  256.])     torch.pow(self, exponent, out=None) \u2192 Tensor   self is a scalar float value, and exponent is a tensor. The returned tensor out is of the same shape as exponent The operation applied is:  outi=selfexponenti\\text{out}_i = \\text{self} ^ {\\text{exponent}_i}  outi\u200b=selfexponenti\u200b   Parameters  self (python:float) \u2013 the scalar base value for the power operation exponent (Tensor) \u2013 the exponent tensor out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; exp = torch.arange(1., 5.) &gt;&gt;&gt; base = 2 &gt;&gt;&gt; torch.pow(base, exp) tensor([  2.,   4.,   8.,  16.])   ", "parameters": ["input (Tensor) : the input tensor.", "exponent (python:float or tensor) : the exponent value", "out (Tensor, optional) : the output tensor.", "self (python:float) : the scalar base value for the power operation", "exponent (Tensor) : the exponent tensor", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " a = torch.randn(4)\n a\ntensor([ 0.4331,  1.2475,  0.6834, -0.2791])\n torch.pow(a, 2)\ntensor([ 0.1875,  1.5561,  0.4670,  0.0779])\n exp = torch.arange(1., 5.)\n\n a = torch.arange(1., 5.)\n a\ntensor([ 1.,  2.,  3.,  4.])\n exp\ntensor([ 1.,  2.,  3.,  4.])\n torch.pow(a, exp)\ntensor([   1.,    4.,   27.,  256.])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.laplace.Laplace.arg_constraints", "item_type": "attribute", "code": "arg_constraints={'loc':Real(),'scale':GreaterThan(lower_bound=0.0)}", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.laplace.Laplace.has_rsample", "item_type": "attribute", "code": "has_rsample=True", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.laplace.Laplace.support", "item_type": "attribute", "code": "support=Real()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.log_normal.LogNormal", "item_type": "class", "code": "classtorch.distributions.log_normal.LogNormal(loc,scale,validate_args=None)", "description": "Bases: torch.distributions.transformed_distribution.TransformedDistribution Creates a log-normal distribution parameterized by loc and scale where: X ~ Normal(loc, scale) Y = exp(X) ~ LogNormal(loc, scale)   Example: &gt;&gt;&gt; m = LogNormal(torch.tensor([0.0]), torch.tensor([1.0])) &gt;&gt;&gt; m.sample()  # log-normal distributed with mean=0 and stddev=1 tensor([ 0.1046])    Parameters  loc (python:float or Tensor) \u2013 mean of log of distribution scale (python:float or Tensor) \u2013 standard deviation of log of the distribution      arg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)}     entropy()      expand(batch_shape, _instance=None)      has_rsample = True     property loc     property mean     property scale     support = GreaterThan(lower_bound=0.0)     property variance   ", "parameters": ["loc (python:float or Tensor) : mean of log of distribution", "scale (python:float or Tensor) : standard deviation of log of the distribution"], "returns": null, "example": " m = LogNormal(torch.tensor([0.0]), torch.tensor([1.0]))\n m.sample()  # log-normal distributed with mean=0 and stddev=1\ntensor([ 0.1046])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.log_normal.LogNormal.arg_constraints", "item_type": "attribute", "code": "arg_constraints={'loc':Real(),'scale':GreaterThan(lower_bound=0.0)}", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.log_normal.LogNormal.has_rsample", "item_type": "attribute", "code": "has_rsample=True", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.log_normal.LogNormal.support", "item_type": "attribute", "code": "support=GreaterThan(lower_bound=0.0)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal", "item_type": "class", "code": "classtorch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal(loc,cov_factor,cov_diag,validate_args=None)", "description": "Bases: torch.distributions.distribution.Distribution Creates a multivariate normal distribution with covariance matrix having a low-rank form parameterized by cov_factor and cov_diag: covariance_matrix = cov_factor @ cov_factor.T + cov_diag   Example &gt;&gt;&gt; m = LowRankMultivariateNormal(torch.zeros(2), torch.tensor([1, 0]), torch.tensor([1, 1])) &gt;&gt;&gt; m.sample()  # normally distributed with mean=`[0,0]`, cov_factor=`[1,0]`, cov_diag=`[1,1]` tensor([-0.2102, -0.5429])    Parameters  loc (Tensor) \u2013 mean of the distribution with shape batch_shape + event_shape cov_factor (Tensor) \u2013 factor part of low-rank form of covariance matrix with shape batch_shape + event_shape + (rank,) cov_diag (Tensor) \u2013 diagonal part of low-rank form of covariance matrix with shape batch_shape + event_shape     Note The computation for determinant and inverse of covariance matrix is avoided when cov_factor.shape[1] &lt;&lt; cov_factor.shape[0] thanks to Woodbury matrix identity and matrix determinant lemma. Thanks to these formulas, we just need to compute the determinant and inverse of the small size \u201ccapacitance\u201d matrix: capacitance = I + cov_factor.T @ inv(cov_diag) @ cov_factor      arg_constraints = {'cov_diag': GreaterThan(lower_bound=0.0), 'cov_factor': Real(), 'loc': Real()}     covariance_matrix      entropy()      expand(batch_shape, _instance=None)      has_rsample = True     log_prob(value)      property mean     precision_matrix      rsample(sample_shape=torch.Size([]))      scale_tril      support = Real()     variance    ", "parameters": ["loc (Tensor) : mean of the distribution with shape batch_shape + event_shape", "cov_factor (Tensor) : factor part of low-rank form of covariance matrix with shapebatch_shape + event_shape + (rank,)", "cov_diag (Tensor) : diagonal part of low-rank form of covariance matrix with shapebatch_shape + event_shape"], "returns": null, "example": " m = LowRankMultivariateNormal(torch.zeros(2), torch.tensor([1, 0]), torch.tensor([1, 1]))\n m.sample()  # normally distributed with mean=`[0,0]`, cov_factor=`[1,0]`, cov_diag=`[1,1]`\ntensor([-0.2102, -0.5429])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.arg_constraints", "item_type": "attribute", "code": "arg_constraints={'cov_diag':GreaterThan(lower_bound=0.0),'cov_factor':Real(),'loc':Real()}", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.covariance_matrix", "item_type": "attribute", "code": "covariance_matrix", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.has_rsample", "item_type": "attribute", "code": "has_rsample=True", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.ConvTranspose1d", "item_type": "class", "code": "classtorch.nn.ConvTranspose1d(in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode='zeros')", "description": "Applies a 1D transposed convolution operator over an input image composed of several input planes. This module can be seen as the gradient of Conv1d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation).  stride controls the stride for the cross-correlation. padding controls the amount of implicit zero-paddings on both sides for dilation * (kernel_size - 1) - padding number of points. See note below for details. output_padding controls the additional size added to one side of the output shape. See note below for details. dilation controls the spacing between the kernel points; also known as the \u00e0 trous algorithm. It is harder to describe, but this link has a nice visualization of what dilation does. groups controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups. For example,   At groups=1, all inputs are convolved to all outputs. At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels, and producing half the output channels, and both subsequently concatenated. At groups= in_channels, each input channel is convolved with its own set of filters (of size \u230aout_channelsin_channels\u230b\\left\\lfloor\\frac{out\\_channels}{in\\_channels}\\right\\rfloor\u230ain_channelsout_channels\u200b\u230b  ).      Note Depending of the size of your kernel, several (of the last) columns of the input might be lost, because it is a valid cross-correlation, and not a full cross-correlation. It is up to the user to add proper padding.   Note The padding argument effectively adds dilation * (kernel_size - 1) - padding amount of zero padding to both sizes of the input. This is set so that when a Conv1d and a ConvTranspose1d are initialized with same parameters, they are inverses of each other in regard to the input and output shapes. However, when stride &gt; 1, Conv1d maps multiple input shapes to the same output shape. output_padding is provided to resolve this ambiguity by effectively increasing the calculated output shape on one side. Note that output_padding is only used to find output shape, but does not actually add zero-padding to output.   Note In some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. Please see the notes on Reproducibility for background.   Parameters  in_channels (python:int) \u2013 Number of channels in the input image out_channels (python:int) \u2013 Number of channels produced by the convolution kernel_size (python:int or tuple) \u2013 Size of the convolving kernel stride (python:int or tuple, optional) \u2013 Stride of the convolution. Default: 1 padding (python:int or tuple, optional) \u2013 dilation * (kernel_size - 1) - padding zero-padding will be added to both sides of the input. Default: 0 output_padding (python:int or tuple, optional) \u2013 Additional size added to one side of the output shape. Default: 0 groups (python:int, optional) \u2013 Number of blocked connections from input channels to output channels. Default: 1 bias (bool, optional) \u2013 If True, adds a learnable bias to the output. Default: True dilation (python:int or tuple, optional) \u2013 Spacing between kernel elements. Default: 1     Shape: Input: (N,Cin,Lin)(N, C_{in}, L_{in})(N,Cin\u200b,Lin\u200b)   Output: (N,Cout,Lout)(N, C_{out}, L_{out})(N,Cout\u200b,Lout\u200b)   where  Lout=(Lin\u22121)\u00d7stride\u22122\u00d7padding+dilation\u00d7(kernel_size\u22121)+output_padding+1L_{out} = (L_{in} - 1) \\times \\text{stride} - 2 \\times \\text{padding} + \\text{dilation}           \\times (\\text{kernel\\_size} - 1) + \\text{output\\_padding} + 1  Lout\u200b=(Lin\u200b\u22121)\u00d7stride\u22122\u00d7padding+dilation\u00d7(kernel_size\u22121)+output_padding+1       Variables  ~ConvTranspose1d.weight (Tensor) \u2013 the learnable weights of the module of shape (in_channels,out_channelsgroups,(\\text{in\\_channels}, \\frac{\\text{out\\_channels}}{\\text{groups}},(in_channels,groupsout_channels\u200b,   kernel_size)\\text{kernel\\_size})kernel_size)  . The values of these weights are sampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b)   where k=1Cin\u2217kernel_sizek = \\frac{1}{C_\\text{in} * \\text{kernel\\_size}}k=Cin\u200b\u2217kernel_size1\u200b   ~ConvTranspose1d.bias (Tensor) \u2013 the learnable bias of the module of shape (out_channels). If bias is True, then the values of these weights are sampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b)   where k=1Cin\u2217kernel_sizek = \\frac{1}{C_\\text{in} * \\text{kernel\\_size}}k=Cin\u200b\u2217kernel_size1\u200b      ", "parameters": ["in_channels (python:int) : Number of channels in the input image", "out_channels (python:int) : Number of channels produced by the convolution", "kernel_size (python:int or tuple) : Size of the convolving kernel", "stride (python:int or tuple, optional) : Stride of the convolution. Default: 1", "padding (python:int or tuple, optional) : dilation * (kernel_size - 1) - padding zero-paddingwill be added to both sides of the input. Default: 0", "output_padding (python:int or tuple, optional) : Additional size added to one sideof the output shape. Default: 0", "groups (python:int, optional) : Number of blocked connections from input channels to output channels. Default: 1", "bias (bool, optional) : If True, adds a learnable bias to the output. Default: True", "dilation (python:int or tuple, optional) : Spacing between kernel elements. Default: 1", "~ConvTranspose1d.weight (Tensor) : the learnable weights of the module of shape(in_channels,out_channelsgroups,(\\text{in\\_channels}, \\frac{\\text{out\\_channels}}{\\text{groups}},(in_channels,groupsout_channels\u200b,kernel_size)\\text{kernel\\_size})kernel_size).The values of these weights are sampled fromU(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b) wherek=1Cin\u2217kernel_sizek = \\frac{1}{C_\\text{in} * \\text{kernel\\_size}}k=Cin\u200b\u2217kernel_size1\u200b", "~ConvTranspose1d.bias (Tensor) : the learnable bias of the module of shape (out_channels).If bias is True, then the values of these weights aresampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b) wherek=1Cin\u2217kernel_sizek = \\frac{1}{C_\\text{in} * \\text{kernel\\_size}}k=Cin\u200b\u2217kernel_size1\u200b"], "returns": null, "example": "NA", "shape": " Input: (N,Cin,Lin)(N, C_{in}, L_{in})(N,Cin\u200b,Lin\u200b)   Output: (N,Cout,Lout)(N, C_{out}, L_{out})(N,Cout\u200b,Lout\u200b)   where  Lout=(Lin\u22121)\u00d7stride\u22122\u00d7padding+dilation\u00d7(kernel_size\u22121)+output_padding+1L_{out} = (L_{in} - 1) \\times \\text{stride} - 2 \\times \\text{padding} + \\text{dilation}           \\times (\\text{kernel\\_size} - 1) + \\text{output\\_padding} + 1  Lout\u200b=(Lin\u200b\u22121)\u00d7stride\u22122\u00d7padding+dilation\u00d7(kernel_size\u22121)+output_padding+1    "},
{"library": "torch", "item_id": "torch.nn.ConvTranspose2d", "item_type": "class", "code": "classtorch.nn.ConvTranspose2d(in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode='zeros')", "description": "Applies a 2D transposed convolution operator over an input image composed of several input planes. This module can be seen as the gradient of Conv2d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation).  stride controls the stride for the cross-correlation. padding controls the amount of implicit zero-paddings on both sides for dilation * (kernel_size - 1) - padding number of points. See note below for details. output_padding controls the additional size added to one side of the output shape. See note below for details. dilation controls the spacing between the kernel points; also known as the \u00e0 trous algorithm. It is harder to describe, but this link has a nice visualization of what dilation does. groups controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups. For example,   At groups=1, all inputs are convolved to all outputs. At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels, and producing half the output channels, and both subsequently concatenated. At groups= in_channels, each input channel is convolved with its own set of filters (of size \u230aout_channelsin_channels\u230b\\left\\lfloor\\frac{out\\_channels}{in\\_channels}\\right\\rfloor\u230ain_channelsout_channels\u200b\u230b  ).     The parameters kernel_size, stride, padding, output_padding can either be:   a single int \u2013 in which case the same value is used for the height and width dimensions a tuple of two ints \u2013 in which case, the first int is used for the height dimension, and the second int for the width dimension    Note Depending of the size of your kernel, several (of the last) columns of the input might be lost, because it is a valid cross-correlation, and not a full cross-correlation. It is up to the user to add proper padding.   Note The padding argument effectively adds dilation * (kernel_size - 1) - padding amount of zero padding to both sizes of the input. This is set so that when a Conv2d and a ConvTranspose2d are initialized with same parameters, they are inverses of each other in regard to the input and output shapes. However, when stride &gt; 1, Conv2d maps multiple input shapes to the same output shape. output_padding is provided to resolve this ambiguity by effectively increasing the calculated output shape on one side. Note that output_padding is only used to find output shape, but does not actually add zero-padding to output.   Note In some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. Please see the notes on Reproducibility for background.   Parameters  in_channels (python:int) \u2013 Number of channels in the input image out_channels (python:int) \u2013 Number of channels produced by the convolution kernel_size (python:int or tuple) \u2013 Size of the convolving kernel stride (python:int or tuple, optional) \u2013 Stride of the convolution. Default: 1 padding (python:int or tuple, optional) \u2013 dilation * (kernel_size - 1) - padding zero-padding will be added to both sides of each dimension in the input. Default: 0 output_padding (python:int or tuple, optional) \u2013 Additional size added to one side of each dimension in the output shape. Default: 0 groups (python:int, optional) \u2013 Number of blocked connections from input channels to output channels. Default: 1 bias (bool, optional) \u2013 If True, adds a learnable bias to the output. Default: True dilation (python:int or tuple, optional) \u2013 Spacing between kernel elements. Default: 1     Shape: Input: (N,Cin,Hin,Win)(N, C_{in}, H_{in}, W_{in})(N,Cin\u200b,Hin\u200b,Win\u200b)   Output: (N,Cout,Hout,Wout)(N, C_{out}, H_{out}, W_{out})(N,Cout\u200b,Hout\u200b,Wout\u200b)   where   Hout=(Hin\u22121)\u00d7stride[0]\u22122\u00d7padding[0]+dilation[0]\u00d7(kernel_size[0]\u22121)+output_padding[0]+1H_{out} = (H_{in} - 1) \\times \\text{stride}[0] - 2 \\times \\text{padding}[0] + \\text{dilation}[0]           \\times (\\text{kernel\\_size}[0] - 1) + \\text{output\\_padding}[0] + 1  Hout\u200b=(Hin\u200b\u22121)\u00d7stride[0]\u22122\u00d7padding[0]+dilation[0]\u00d7(kernel_size[0]\u22121)+output_padding[0]+1   Wout=(Win\u22121)\u00d7stride[1]\u22122\u00d7padding[1]+dilation[1]\u00d7(kernel_size[1]\u22121)+output_padding[1]+1W_{out} = (W_{in} - 1) \\times \\text{stride}[1] - 2 \\times \\text{padding}[1] + \\text{dilation}[1]           \\times (\\text{kernel\\_size}[1] - 1) + \\text{output\\_padding}[1] + 1  Wout\u200b=(Win\u200b\u22121)\u00d7stride[1]\u22122\u00d7padding[1]+dilation[1]\u00d7(kernel_size[1]\u22121)+output_padding[1]+1     Variables  ~ConvTranspose2d.weight (Tensor) \u2013 the learnable weights of the module of shape (in_channels,out_channelsgroups,(\\text{in\\_channels}, \\frac{\\text{out\\_channels}}{\\text{groups}},(in_channels,groupsout_channels\u200b,   kernel_size[0],kernel_size[1])\\text{kernel\\_size[0]}, \\text{kernel\\_size[1]})kernel_size[0],kernel_size[1])  . The values of these weights are sampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b)   where k=1Cin\u2217\u220fi=01kernel_size[i]k = \\frac{1}{C_\\text{in} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}k=Cin\u200b\u2217\u220fi=01\u200bkernel_size[i]1\u200b   ~ConvTranspose2d.bias (Tensor) \u2013 the learnable bias of the module of shape (out_channels) If bias is True, then the values of these weights are sampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b)   where k=1Cin\u2217\u220fi=01kernel_size[i]k = \\frac{1}{C_\\text{in} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}k=Cin\u200b\u2217\u220fi=01\u200bkernel_size[i]1\u200b      Examples: &gt;&gt;&gt; # With square kernels and equal stride &gt;&gt;&gt; m = nn.ConvTranspose2d(16, 33, 3, stride=2) &gt;&gt;&gt; # non-square kernels and unequal stride and with padding &gt;&gt;&gt; m = nn.ConvTranspose2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2)) &gt;&gt;&gt; input = torch.randn(20, 16, 50, 100) &gt;&gt;&gt; output = m(input) &gt;&gt;&gt; # exact output size can be also specified as an argument &gt;&gt;&gt; input = torch.randn(1, 16, 12, 12) &gt;&gt;&gt; downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1) &gt;&gt;&gt; upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1) &gt;&gt;&gt; h = downsample(input) &gt;&gt;&gt; h.size() torch.Size([1, 16, 6, 6]) &gt;&gt;&gt; output = upsample(h, output_size=input.size()) &gt;&gt;&gt; output.size() torch.Size([1, 16, 12, 12])   ", "parameters": ["in_channels (python:int) : Number of channels in the input image", "out_channels (python:int) : Number of channels produced by the convolution", "kernel_size (python:int or tuple) : Size of the convolving kernel", "stride (python:int or tuple, optional) : Stride of the convolution. Default: 1", "padding (python:int or tuple, optional) : dilation * (kernel_size - 1) - padding zero-paddingwill be added to both sides of each dimension in the input. Default: 0", "output_padding (python:int or tuple, optional) : Additional size added to one sideof each dimension in the output shape. Default: 0", "groups (python:int, optional) : Number of blocked connections from input channels to output channels. Default: 1", "bias (bool, optional) : If True, adds a learnable bias to the output. Default: True", "dilation (python:int or tuple, optional) : Spacing between kernel elements. Default: 1", "~ConvTranspose2d.weight (Tensor) : the learnable weights of the module of shape(in_channels,out_channelsgroups,(\\text{in\\_channels}, \\frac{\\text{out\\_channels}}{\\text{groups}},(in_channels,groupsout_channels\u200b,kernel_size[0],kernel_size[1])\\text{kernel\\_size[0]}, \\text{kernel\\_size[1]})kernel_size[0],kernel_size[1]).The values of these weights are sampled fromU(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b) wherek=1Cin\u2217\u220fi=01kernel_size[i]k = \\frac{1}{C_\\text{in} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}k=Cin\u200b\u2217\u220fi=01\u200bkernel_size[i]1\u200b", "~ConvTranspose2d.bias (Tensor) : the learnable bias of the module of shape (out_channels)If bias is True, then the values of these weights aresampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b) wherek=1Cin\u2217\u220fi=01kernel_size[i]k = \\frac{1}{C_\\text{in} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}k=Cin\u200b\u2217\u220fi=01\u200bkernel_size[i]1\u200b"], "returns": null, "example": " # With square kernels and equal stride\n m = nn.ConvTranspose2d(16, 33, 3, stride=2)\n # non-square kernels and unequal stride and with padding\n m = nn.ConvTranspose2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n input = torch.randn(20, 16, 50, 100)\n output = m(input)\n # exact output size can be also specified as an argument\n input = torch.randn(1, 16, 12, 12)\n downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1)\n upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1)\n h = downsample(input)\n h.size()\ntorch.Size([1, 16, 6, 6])\n output = upsample(h, output_size=input.size())\n output.size()\ntorch.Size([1, 16, 12, 12])\n\n", "shape": " Input: (N,Cin,Hin,Win)(N, C_{in}, H_{in}, W_{in})(N,Cin\u200b,Hin\u200b,Win\u200b)   Output: (N,Cout,Hout,Wout)(N, C_{out}, H_{out}, W_{out})(N,Cout\u200b,Hout\u200b,Wout\u200b)   where   Hout=(Hin\u22121)\u00d7stride[0]\u22122\u00d7padding[0]+dilation[0]\u00d7(kernel_size[0]\u22121)+output_padding[0]+1H_{out} = (H_{in} - 1) \\times \\text{stride}[0] - 2 \\times \\text{padding}[0] + \\text{dilation}[0]           \\times (\\text{kernel\\_size}[0] - 1) + \\text{output\\_padding}[0] + 1  Hout\u200b=(Hin\u200b\u22121)\u00d7stride[0]\u22122\u00d7padding[0]+dilation[0]\u00d7(kernel_size[0]\u22121)+output_padding[0]+1   Wout=(Win\u22121)\u00d7stride[1]\u22122\u00d7padding[1]+dilation[1]\u00d7(kernel_size[1]\u22121)+output_padding[1]+1W_{out} = (W_{in} - 1) \\times \\text{stride}[1] - 2 \\times \\text{padding}[1] + \\text{dilation}[1]           \\times (\\text{kernel\\_size}[1] - 1) + \\text{output\\_padding}[1] + 1  Wout\u200b=(Win\u200b\u22121)\u00d7stride[1]\u22122\u00d7padding[1]+dilation[1]\u00d7(kernel_size[1]\u22121)+output_padding[1]+1  "},
{"library": "torch", "item_id": "torch.Tensor.stft", "item_type": "method", "code": "stft(n_fft,hop_length=None,win_length=None,window=None,center=True,pad_mode='reflect',normalized=False,onesided=True)", "description": "See torch.stft()  Warning This function changed signature at version 0.4.1. Calling with the previous signature may cause error or return incorrect result.  ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.storage", "item_type": "method", "code": "storage()\u2192torch.Storage", "description": "Returns the underlying storage. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.storage_offset", "item_type": "method", "code": "storage_offset()\u2192int", "description": "Returns self tensor\u2019s offset in the underlying storage in terms of number of storage elements (not bytes). Example: &gt;&gt;&gt; x = torch.tensor([1, 2, 3, 4, 5]) &gt;&gt;&gt; x.storage_offset() 0 &gt;&gt;&gt; x[3:].storage_offset() 3   ", "parameters": [], "returns": null, "example": " x = torch.tensor([1, 2, 3, 4, 5])\n x.storage_offset()\n0\n x[3:].storage_offset()\n3\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.storage_type", "item_type": "method", "code": "storage_type()\u2192type", "description": "Returns the type of the underlying storage. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.stride", "item_type": "method", "code": "stride(dim)\u2192tupleorint", "description": "Returns the stride of self tensor. Stride is the jump necessary to go from one element to the next one in the specified dimension dim. A tuple of all strides is returned when no argument is passed in. Otherwise, an integer value is returned as the stride in the particular dimension dim.  Parameters dim (python:int, optional) \u2013 the desired dimension in which stride is required   Example: &gt;&gt;&gt; x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]) &gt;&gt;&gt; x.stride() (5, 1) &gt;&gt;&gt;x.stride(0) 5 &gt;&gt;&gt; x.stride(-1) 1   ", "parameters": ["dim (python:int, optional) : the desired dimension in which stride is required"], "returns": null, "example": " x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n x.stride()\n(5, 1)\nx.stride(0)\n5\n x.stride(-1)\n1\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.sub", "item_type": "method", "code": "sub(value,other)\u2192Tensor", "description": "Subtracts a scalar or tensor from self tensor. If both value and other are specified, each element of other is scaled by value before being used. When other is a tensor, the shape of other must be broadcastable with the shape of the underlying tensor. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.sub_", "item_type": "method", "code": "sub_(x)\u2192Tensor", "description": "In-place version of sub() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.sum", "item_type": "method", "code": "sum(dim=None,keepdim=False,dtype=None)\u2192Tensor", "description": "See torch.sum() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.sum_to_size", "item_type": "method", "code": "sum_to_size(*size)\u2192Tensor", "description": "Sum this tensor to size. size must be broadcastable to this tensor size.  Parameters size (python:int...) \u2013 a sequence of integers defining the shape of the output tensor.   ", "parameters": ["size (python:int...) : a sequence of integers defining the shape of the output tensor."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.svd", "item_type": "method", "code": "svd(some=True,compute_uv=True)-&gt;(Tensor,Tensor,Tensor)", "description": "See torch.svd() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.pow(input,exponent,out=None)\u2192Tensor", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.pow(self,exponent,out=None)\u2192Tensor", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.real", "item_type": "function", "code": "torch.real(input,out=None)\u2192Tensor", "description": "Computes the element-wise real value of the given input tensor.  outi=real(inputi)\\text{out}_{i} = real(\\text{input}_{i})  outi\u200b=real(inputi\u200b)   Parameters  input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; torch.real(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j])) tensor([ -1,  -2,  3])   ", "parameters": ["input (Tensor) : the input tensor.", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " torch.real(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))\ntensor([ -1,  -2,  3])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.reciprocal", "item_type": "function", "code": "torch.reciprocal(input,out=None)\u2192Tensor", "description": "Returns a new tensor with the reciprocal of the elements of input  outi=1inputi\\text{out}_{i} = \\frac{1}{\\text{input}_{i}}  outi\u200b=inputi\u200b1\u200b   Parameters  input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a tensor([-0.4595, -2.1219, -1.4314,  0.7298]) &gt;&gt;&gt; torch.reciprocal(a) tensor([-2.1763, -0.4713, -0.6986,  1.3702])   ", "parameters": ["input (Tensor) : the input tensor.", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " a = torch.randn(4)\n a\ntensor([-0.4595, -2.1219, -1.4314,  0.7298])\n torch.reciprocal(a)\ntensor([-2.1763, -0.4713, -0.6986,  1.3702])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.remainder", "item_type": "function", "code": "torch.remainder(input,other,out=None)\u2192Tensor", "description": "Computes the element-wise remainder of division. The divisor and dividend may contain both for integer and floating point numbers. The remainder has the same sign as the divisor. When other is a tensor, the shapes of input and other must be broadcastable.  Parameters  input (Tensor) \u2013 the dividend other (Tensor or python:float) \u2013 the divisor that may be either a number or a Tensor of the same shape as the dividend out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; torch.remainder(torch.tensor([-3., -2, -1, 1, 2, 3]), 2) tensor([ 1.,  0.,  1.,  1.,  0.,  1.]) &gt;&gt;&gt; torch.remainder(torch.tensor([1., 2, 3, 4, 5]), 1.5) tensor([ 1.0000,  0.5000,  0.0000,  1.0000,  0.5000])    See also torch.fmod(), which computes the element-wise remainder of division equivalently to the C library function fmod().  ", "parameters": ["input (Tensor) : the dividend", "other (Tensor or python:float) : the divisor that may be either a number or aTensor of the same shape as the dividend", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " torch.remainder(torch.tensor([-3., -2, -1, 1, 2, 3]), 2)\ntensor([ 1.,  0.,  1.,  1.,  0.,  1.])\n torch.remainder(torch.tensor([1., 2, 3, 4, 5]), 1.5)\ntensor([ 1.0000,  0.5000,  0.0000,  1.0000,  0.5000])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.round", "item_type": "function", "code": "torch.round(input,out=None)\u2192Tensor", "description": "Returns a new tensor with each of the elements of input rounded to the closest integer.  Parameters  input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a tensor([ 0.9920,  0.6077,  0.9734, -1.0362]) &gt;&gt;&gt; torch.round(a) tensor([ 1.,  1.,  1., -1.])   ", "parameters": ["input (Tensor) : the input tensor.", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " a = torch.randn(4)\n a\ntensor([ 0.9920,  0.6077,  0.9734, -1.0362])\n torch.round(a)\ntensor([ 1.,  1.,  1., -1.])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.rsqrt", "item_type": "function", "code": "torch.rsqrt(input,out=None)\u2192Tensor", "description": "Returns a new tensor with the reciprocal of the square-root of each of the elements of input.  outi=1inputi\\text{out}_{i} = \\frac{1}{\\sqrt{\\text{input}_{i}}}  outi\u200b=inputi\u200b\u200b1\u200b   Parameters  input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a tensor([-0.0370,  0.2970,  1.5420, -0.9105]) &gt;&gt;&gt; torch.rsqrt(a) tensor([    nan,  1.8351,  0.8053,     nan])   ", "parameters": ["input (Tensor) : the input tensor.", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " a = torch.randn(4)\n a\ntensor([-0.0370,  0.2970,  1.5420, -0.9105])\n torch.rsqrt(a)\ntensor([    nan,  1.8351,  0.8053,     nan])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.precision_matrix", "item_type": "attribute", "code": "precision_matrix", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.scale_tril", "item_type": "attribute", "code": "scale_tril", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.support", "item_type": "attribute", "code": "support=Real()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.variance", "item_type": "attribute", "code": "variance", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.multinomial.Multinomial", "item_type": "class", "code": "classtorch.distributions.multinomial.Multinomial(total_count=1,probs=None,logits=None,validate_args=None)", "description": "Bases: torch.distributions.distribution.Distribution Creates a Multinomial distribution parameterized by total_count and either probs or logits (but not both). The innermost dimension of probs indexes over categories. All other dimensions index over batches. Note that total_count need not be specified if only log_prob() is called (see example below)  Note probs must be non-negative, finite and have a non-zero sum, and it will be normalized to sum to 1.   sample() requires a single shared total_count for all parameters and samples. log_prob() allows different total_count for each parameter and sample.  Example: &gt;&gt;&gt; m = Multinomial(100, torch.tensor([ 1., 1., 1., 1.])) &gt;&gt;&gt; x = m.sample()  # equal probability of 0, 1, 2, 3 tensor([ 21.,  24.,  30.,  25.])  &gt;&gt;&gt; Multinomial(probs=torch.tensor([1., 1., 1., 1.])).log_prob(x) tensor([-4.1338])    Parameters  total_count (python:int) \u2013 number of trials probs (Tensor) \u2013 event probabilities logits (Tensor) \u2013 event log probabilities      arg_constraints = {'logits': Real(), 'probs': Simplex()}     expand(batch_shape, _instance=None)      log_prob(value)      property logits     property mean     property param_shape     property probs     sample(sample_shape=torch.Size([]))      property support     property variance   ", "parameters": ["total_count (python:int) : number of trials", "probs (Tensor) : event probabilities", "logits (Tensor) : event log probabilities"], "returns": null, "example": " m = Multinomial(100, torch.tensor([ 1., 1., 1., 1.]))\n x = m.sample()  # equal probability of 0, 1, 2, 3\ntensor([ 21.,  24.,  30.,  25.])\n\n Multinomial(probs=torch.tensor([1., 1., 1., 1.])).log_prob(x)\ntensor([-4.1338])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.multinomial.Multinomial.arg_constraints", "item_type": "attribute", "code": "arg_constraints={'logits':Real(),'probs':Simplex()}", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.multivariate_normal.MultivariateNormal", "item_type": "class", "code": "classtorch.distributions.multivariate_normal.MultivariateNormal(loc,covariance_matrix=None,precision_matrix=None,scale_tril=None,validate_args=None)", "description": "Bases: torch.distributions.distribution.Distribution Creates a multivariate normal (also called Gaussian) distribution parameterized by a mean vector and a covariance matrix. The multivariate normal distribution can be parameterized either in terms of a positive definite covariance matrix \u03a3\\mathbf{\\Sigma}\u03a3   or a positive definite precision matrix \u03a3\u22121\\mathbf{\\Sigma}^{-1}\u03a3\u22121   or a lower-triangular matrix L\\mathbf{L}L   with positive-valued diagonal entries, such that \u03a3=LL\u22a4\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}^\\top\u03a3=LL\u22a4  . This triangular matrix can be obtained via e.g. Cholesky decomposition of the covariance. Example &gt;&gt;&gt; m = MultivariateNormal(torch.zeros(2), torch.eye(2)) &gt;&gt;&gt; m.sample()  # normally distributed with mean=`[0,0]` and covariance_matrix=`I` tensor([-0.2102, -0.5429])    Parameters  loc (Tensor) \u2013 mean of the distribution covariance_matrix (Tensor) \u2013 positive-definite covariance matrix precision_matrix (Tensor) \u2013 positive-definite precision matrix scale_tril (Tensor) \u2013 lower-triangular factor of covariance, with positive-valued diagonal     Note Only one of covariance_matrix or precision_matrix or scale_tril can be specified. Using scale_tril will be more efficient: all computations internally are based on scale_tril. If covariance_matrix or precision_matrix is passed instead, it is only used to compute the corresponding lower triangular matrices using a Cholesky decomposition.    arg_constraints = {'covariance_matrix': PositiveDefinite(), 'loc': RealVector(), 'precision_matrix': PositiveDefinite(), 'scale_tril': LowerCholesky()}     covariance_matrix      entropy()      expand(batch_shape, _instance=None)      has_rsample = True     log_prob(value)      property mean     precision_matrix      rsample(sample_shape=torch.Size([]))      scale_tril      support = Real()     property variance   ", "parameters": ["loc (Tensor) : mean of the distribution", "covariance_matrix (Tensor) : positive-definite covariance matrix", "precision_matrix (Tensor) : positive-definite precision matrix", "scale_tril (Tensor) : lower-triangular factor of covariance, with positive-valued diagonal"], "returns": null, "example": " m = MultivariateNormal(torch.zeros(2), torch.eye(2))\n m.sample()  # normally distributed with mean=`[0,0]` and covariance_matrix=`I`\ntensor([-0.2102, -0.5429])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.ConvTranspose3d", "item_type": "class", "code": "classtorch.nn.ConvTranspose3d(in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode='zeros')", "description": "Applies a 3D transposed convolution operator over an input image composed of several input planes. The transposed convolution operator multiplies each input value element-wise by a learnable kernel, and sums over the outputs from all input feature planes. This module can be seen as the gradient of Conv3d with respect to its input. It is also known as a fractionally-strided convolution or a deconvolution (although it is not an actual deconvolution operation).  stride controls the stride for the cross-correlation. padding controls the amount of implicit zero-paddings on both sides for dilation * (kernel_size - 1) - padding number of points. See note below for details. output_padding controls the additional size added to one side of the output shape. See note below for details. dilation controls the spacing between the kernel points; also known as the \u00e0 trous algorithm. It is harder to describe, but this link has a nice visualization of what dilation does. groups controls the connections between inputs and outputs. in_channels and out_channels must both be divisible by groups. For example,   At groups=1, all inputs are convolved to all outputs. At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels, and producing half the output channels, and both subsequently concatenated. At groups= in_channels, each input channel is convolved with its own set of filters (of size \u230aout_channelsin_channels\u230b\\left\\lfloor\\frac{out\\_channels}{in\\_channels}\\right\\rfloor\u230ain_channelsout_channels\u200b\u230b  ).     The parameters kernel_size, stride, padding, output_padding can either be:   a single int \u2013 in which case the same value is used for the depth, height and width dimensions a tuple of three ints \u2013 in which case, the first int is used for the depth dimension, the second int for the height dimension and the third int for the width dimension    Note Depending of the size of your kernel, several (of the last) columns of the input might be lost, because it is a valid cross-correlation, and not a full cross-correlation. It is up to the user to add proper padding.   Note The padding argument effectively adds dilation * (kernel_size - 1) - padding amount of zero padding to both sizes of the input. This is set so that when a Conv3d and a ConvTranspose3d are initialized with same parameters, they are inverses of each other in regard to the input and output shapes. However, when stride &gt; 1, Conv3d maps multiple input shapes to the same output shape. output_padding is provided to resolve this ambiguity by effectively increasing the calculated output shape on one side. Note that output_padding is only used to find output shape, but does not actually add zero-padding to output.   Note In some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. Please see the notes on Reproducibility for background.   Parameters  in_channels (python:int) \u2013 Number of channels in the input image out_channels (python:int) \u2013 Number of channels produced by the convolution kernel_size (python:int or tuple) \u2013 Size of the convolving kernel stride (python:int or tuple, optional) \u2013 Stride of the convolution. Default: 1 padding (python:int or tuple, optional) \u2013 dilation * (kernel_size - 1) - padding zero-padding will be added to both sides of each dimension in the input. Default: 0 output_padding (python:int or tuple, optional) \u2013 Additional size added to one side of each dimension in the output shape. Default: 0 groups (python:int, optional) \u2013 Number of blocked connections from input channels to output channels. Default: 1 bias (bool, optional) \u2013 If True, adds a learnable bias to the output. Default: True dilation (python:int or tuple, optional) \u2013 Spacing between kernel elements. Default: 1     Shape: Input: (N,Cin,Din,Hin,Win)(N, C_{in}, D_{in}, H_{in}, W_{in})(N,Cin\u200b,Din\u200b,Hin\u200b,Win\u200b)   Output: (N,Cout,Dout,Hout,Wout)(N, C_{out}, D_{out}, H_{out}, W_{out})(N,Cout\u200b,Dout\u200b,Hout\u200b,Wout\u200b)   where   Dout=(Din\u22121)\u00d7stride[0]\u22122\u00d7padding[0]+dilation[0]\u00d7(kernel_size[0]\u22121)+output_padding[0]+1D_{out} = (D_{in} - 1) \\times \\text{stride}[0] - 2 \\times \\text{padding}[0] + \\text{dilation}[0]           \\times (\\text{kernel\\_size}[0] - 1) + \\text{output\\_padding}[0] + 1  Dout\u200b=(Din\u200b\u22121)\u00d7stride[0]\u22122\u00d7padding[0]+dilation[0]\u00d7(kernel_size[0]\u22121)+output_padding[0]+1   Hout=(Hin\u22121)\u00d7stride[1]\u22122\u00d7padding[1]+dilation[1]\u00d7(kernel_size[1]\u22121)+output_padding[1]+1H_{out} = (H_{in} - 1) \\times \\text{stride}[1] - 2 \\times \\text{padding}[1] + \\text{dilation}[1]           \\times (\\text{kernel\\_size}[1] - 1) + \\text{output\\_padding}[1] + 1  Hout\u200b=(Hin\u200b\u22121)\u00d7stride[1]\u22122\u00d7padding[1]+dilation[1]\u00d7(kernel_size[1]\u22121)+output_padding[1]+1   Wout=(Win\u22121)\u00d7stride[2]\u22122\u00d7padding[2]+dilation[2]\u00d7(kernel_size[2]\u22121)+output_padding[2]+1W_{out} = (W_{in} - 1) \\times \\text{stride}[2] - 2 \\times \\text{padding}[2] + \\text{dilation}[2]           \\times (\\text{kernel\\_size}[2] - 1) + \\text{output\\_padding}[2] + 1  Wout\u200b=(Win\u200b\u22121)\u00d7stride[2]\u22122\u00d7padding[2]+dilation[2]\u00d7(kernel_size[2]\u22121)+output_padding[2]+1     Variables  ~ConvTranspose3d.weight (Tensor) \u2013 the learnable weights of the module of shape (in_channels,out_channelsgroups,(\\text{in\\_channels}, \\frac{\\text{out\\_channels}}{\\text{groups}},(in_channels,groupsout_channels\u200b,   kernel_size[0],kernel_size[1],kernel_size[2])\\text{kernel\\_size[0]}, \\text{kernel\\_size[1]}, \\text{kernel\\_size[2]})kernel_size[0],kernel_size[1],kernel_size[2])  . The values of these weights are sampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b)   where k=1Cin\u2217\u220fi=02kernel_size[i]k = \\frac{1}{C_\\text{in} * \\prod_{i=0}^{2}\\text{kernel\\_size}[i]}k=Cin\u200b\u2217\u220fi=02\u200bkernel_size[i]1\u200b   ~ConvTranspose3d.bias (Tensor) \u2013 the learnable bias of the module of shape (out_channels) If bias is True, then the values of these weights are sampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b)   where k=1Cin\u2217\u220fi=02kernel_size[i]k = \\frac{1}{C_\\text{in} * \\prod_{i=0}^{2}\\text{kernel\\_size}[i]}k=Cin\u200b\u2217\u220fi=02\u200bkernel_size[i]1\u200b      Examples: &gt;&gt;&gt; # With square kernels and equal stride &gt;&gt;&gt; m = nn.ConvTranspose3d(16, 33, 3, stride=2) &gt;&gt;&gt; # non-square kernels and unequal stride and with padding &gt;&gt;&gt; m = nn.ConvTranspose3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(0, 4, 2)) &gt;&gt;&gt; input = torch.randn(20, 16, 10, 50, 100) &gt;&gt;&gt; output = m(input)   ", "parameters": ["in_channels (python:int) : Number of channels in the input image", "out_channels (python:int) : Number of channels produced by the convolution", "kernel_size (python:int or tuple) : Size of the convolving kernel", "stride (python:int or tuple, optional) : Stride of the convolution. Default: 1", "padding (python:int or tuple, optional) : dilation * (kernel_size - 1) - padding zero-paddingwill be added to both sides of each dimension in the input. Default: 0", "output_padding (python:int or tuple, optional) : Additional size added to one sideof each dimension in the output shape. Default: 0", "groups (python:int, optional) : Number of blocked connections from input channels to output channels. Default: 1", "bias (bool, optional) : If True, adds a learnable bias to the output. Default: True", "dilation (python:int or tuple, optional) : Spacing between kernel elements. Default: 1", "~ConvTranspose3d.weight (Tensor) : the learnable weights of the module of shape(in_channels,out_channelsgroups,(\\text{in\\_channels}, \\frac{\\text{out\\_channels}}{\\text{groups}},(in_channels,groupsout_channels\u200b,kernel_size[0],kernel_size[1],kernel_size[2])\\text{kernel\\_size[0]}, \\text{kernel\\_size[1]}, \\text{kernel\\_size[2]})kernel_size[0],kernel_size[1],kernel_size[2]).The values of these weights are sampled fromU(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b) wherek=1Cin\u2217\u220fi=02kernel_size[i]k = \\frac{1}{C_\\text{in} * \\prod_{i=0}^{2}\\text{kernel\\_size}[i]}k=Cin\u200b\u2217\u220fi=02\u200bkernel_size[i]1\u200b", "~ConvTranspose3d.bias (Tensor) : the learnable bias of the module of shape (out_channels)If bias is True, then the values of these weights aresampled from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b) wherek=1Cin\u2217\u220fi=02kernel_size[i]k = \\frac{1}{C_\\text{in} * \\prod_{i=0}^{2}\\text{kernel\\_size}[i]}k=Cin\u200b\u2217\u220fi=02\u200bkernel_size[i]1\u200b"], "returns": null, "example": " # With square kernels and equal stride\n m = nn.ConvTranspose3d(16, 33, 3, stride=2)\n # non-square kernels and unequal stride and with padding\n m = nn.ConvTranspose3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(0, 4, 2))\n input = torch.randn(20, 16, 10, 50, 100)\n output = m(input)\n\n", "shape": " Input: (N,Cin,Din,Hin,Win)(N, C_{in}, D_{in}, H_{in}, W_{in})(N,Cin\u200b,Din\u200b,Hin\u200b,Win\u200b)   Output: (N,Cout,Dout,Hout,Wout)(N, C_{out}, D_{out}, H_{out}, W_{out})(N,Cout\u200b,Dout\u200b,Hout\u200b,Wout\u200b)   where   Dout=(Din\u22121)\u00d7stride[0]\u22122\u00d7padding[0]+dilation[0]\u00d7(kernel_size[0]\u22121)+output_padding[0]+1D_{out} = (D_{in} - 1) \\times \\text{stride}[0] - 2 \\times \\text{padding}[0] + \\text{dilation}[0]           \\times (\\text{kernel\\_size}[0] - 1) + \\text{output\\_padding}[0] + 1  Dout\u200b=(Din\u200b\u22121)\u00d7stride[0]\u22122\u00d7padding[0]+dilation[0]\u00d7(kernel_size[0]\u22121)+output_padding[0]+1   Hout=(Hin\u22121)\u00d7stride[1]\u22122\u00d7padding[1]+dilation[1]\u00d7(kernel_size[1]\u22121)+output_padding[1]+1H_{out} = (H_{in} - 1) \\times \\text{stride}[1] - 2 \\times \\text{padding}[1] + \\text{dilation}[1]           \\times (\\text{kernel\\_size}[1] - 1) + \\text{output\\_padding}[1] + 1  Hout\u200b=(Hin\u200b\u22121)\u00d7stride[1]\u22122\u00d7padding[1]+dilation[1]\u00d7(kernel_size[1]\u22121)+output_padding[1]+1   Wout=(Win\u22121)\u00d7stride[2]\u22122\u00d7padding[2]+dilation[2]\u00d7(kernel_size[2]\u22121)+output_padding[2]+1W_{out} = (W_{in} - 1) \\times \\text{stride}[2] - 2 \\times \\text{padding}[2] + \\text{dilation}[2]           \\times (\\text{kernel\\_size}[2] - 1) + \\text{output\\_padding}[2] + 1  Wout\u200b=(Win\u200b\u22121)\u00d7stride[2]\u22122\u00d7padding[2]+dilation[2]\u00d7(kernel_size[2]\u22121)+output_padding[2]+1  "},
{"library": "torch", "item_id": "torch.Tensor.symeig", "item_type": "method", "code": "symeig(eigenvectors=False,upper=True)-&gt;(Tensor,Tensor)", "description": "See torch.symeig() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.t", "item_type": "method", "code": "t()\u2192Tensor", "description": "See torch.t() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.t_", "item_type": "method", "code": "t_()\u2192Tensor", "description": "In-place version of t() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.to", "item_type": "method", "code": "to(*args,**kwargs)\u2192Tensor", "description": "Performs Tensor dtype and/or device conversion. A torch.dtype and torch.device are inferred from the arguments of self.to(*args, **kwargs).  Note If the self Tensor already has the correct torch.dtype and torch.device, then self is returned. Otherwise, the returned tensor is a copy of self with the desired torch.dtype and torch.device.  Here are the ways to call to:   to(dtype, non_blocking=False, copy=False) \u2192 Tensor Returns a Tensor with the specified dtype     to(device=None, dtype=None, non_blocking=False, copy=False) \u2192 Tensor Returns a Tensor with the specified device and (optional) dtype. If dtype is None it is inferred to be self.dtype. When non_blocking, tries to convert asynchronously with respect to the host if possible, e.g., converting a CPU Tensor with pinned memory to a CUDA Tensor. When copy is set, a new Tensor is created even when the Tensor already matches the desired conversion.     to(other, non_blocking=False, copy=False) \u2192 Tensor Returns a Tensor with same torch.dtype and torch.device as the Tensor other. When non_blocking, tries to convert asynchronously with respect to the host if possible, e.g., converting a CPU Tensor with pinned memory to a CUDA Tensor. When copy is set, a new Tensor is created even when the Tensor already matches the desired conversion.   Example: &gt;&gt;&gt; tensor = torch.randn(2, 2)  # Initially dtype=float32, device=cpu &gt;&gt;&gt; tensor.to(torch.float64) tensor([[-0.5044,  0.0005],         [ 0.3310, -0.0584]], dtype=torch.float64)  &gt;&gt;&gt; cuda0 = torch.device('cuda:0') &gt;&gt;&gt; tensor.to(cuda0) tensor([[-0.5044,  0.0005],         [ 0.3310, -0.0584]], device='cuda:0')  &gt;&gt;&gt; tensor.to(cuda0, dtype=torch.float64) tensor([[-0.5044,  0.0005],         [ 0.3310, -0.0584]], dtype=torch.float64, device='cuda:0')  &gt;&gt;&gt; other = torch.randn((), dtype=torch.float64, device=cuda0) &gt;&gt;&gt; tensor.to(other, non_blocking=True) tensor([[-0.5044,  0.0005],         [ 0.3310, -0.0584]], dtype=torch.float64, device='cuda:0')   ", "parameters": [], "returns": null, "example": " tensor = torch.randn(2, 2)  # Initially dtype=float32, device=cpu\n tensor.to(torch.float64)\ntensor([[-0.5044,  0.0005],\n        [ 0.3310, -0.0584]], dtype=torch.float64)\n\n cuda0 = torch.device('cuda:0')\n tensor.to(cuda0)\ntensor([[-0.5044,  0.0005],\n        [ 0.3310, -0.0584]], device='cuda:0')\n\n tensor.to(cuda0, dtype=torch.float64)\ntensor([[-0.5044,  0.0005],\n        [ 0.3310, -0.0584]], dtype=torch.float64, device='cuda:0')\n\n other = torch.randn((), dtype=torch.float64, device=cuda0)\n tensor.to(other, non_blocking=True)\ntensor([[-0.5044,  0.0005],\n        [ 0.3310, -0.0584]], dtype=torch.float64, device='cuda:0')\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.to_mkldnn", "item_type": "method", "code": "to_mkldnn()\u2192Tensor", "description": "Returns a copy of the tensor in torch.mkldnn layout. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.take", "item_type": "method", "code": "take(indices)\u2192Tensor", "description": "See torch.take() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.tan", "item_type": "method", "code": "tan()\u2192Tensor", "description": "See torch.tan() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.tan_", "item_type": "method", "code": "tan_()\u2192Tensor", "description": "In-place version of tan() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.tanh", "item_type": "method", "code": "tanh()\u2192Tensor", "description": "See torch.tanh() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.tanh_", "item_type": "method", "code": "tanh_()\u2192Tensor", "description": "In-place version of tanh() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.sigmoid", "item_type": "function", "code": "torch.sigmoid(input,out=None)\u2192Tensor", "description": "Returns a new tensor with the sigmoid of the elements of input.  outi=11+e\u2212inputi\\text{out}_{i} = \\frac{1}{1 + e^{-\\text{input}_{i}}}  outi\u200b=1+e\u2212inputi\u200b1\u200b   Parameters  input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a tensor([ 0.9213,  1.0887, -0.8858, -1.7683]) &gt;&gt;&gt; torch.sigmoid(a) tensor([ 0.7153,  0.7481,  0.2920,  0.1458])   ", "parameters": ["input (Tensor) : the input tensor.", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " a = torch.randn(4)\n a\ntensor([ 0.9213,  1.0887, -0.8858, -1.7683])\n torch.sigmoid(a)\ntensor([ 0.7153,  0.7481,  0.2920,  0.1458])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.sign", "item_type": "function", "code": "torch.sign(input,out=None)\u2192Tensor", "description": "Returns a new tensor with the signs of the elements of input.  outi=sgn\u2061(inputi)\\text{out}_{i} = \\operatorname{sgn}(\\text{input}_{i})  outi\u200b=sgn(inputi\u200b)   Parameters  input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; a = torch.tensor([0.7, -1.2, 0., 2.3]) &gt;&gt;&gt; a tensor([ 0.7000, -1.2000,  0.0000,  2.3000]) &gt;&gt;&gt; torch.sign(a) tensor([ 1., -1.,  0.,  1.])   ", "parameters": ["input (Tensor) : the input tensor.", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " a = torch.tensor([0.7, -1.2, 0., 2.3])\n a\ntensor([ 0.7000, -1.2000,  0.0000,  2.3000])\n torch.sign(a)\ntensor([ 1., -1.,  0.,  1.])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.sin", "item_type": "function", "code": "torch.sin(input,out=None)\u2192Tensor", "description": "Returns a new tensor with the sine of the elements of input.  outi=sin\u2061(inputi)\\text{out}_{i} = \\sin(\\text{input}_{i})  outi\u200b=sin(inputi\u200b)   Parameters  input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a tensor([-0.5461,  0.1347, -2.7266, -0.2746]) &gt;&gt;&gt; torch.sin(a) tensor([-0.5194,  0.1343, -0.4032, -0.2711])   ", "parameters": ["input (Tensor) : the input tensor.", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " a = torch.randn(4)\n a\ntensor([-0.5461,  0.1347, -2.7266, -0.2746])\n torch.sin(a)\ntensor([-0.5194,  0.1343, -0.4032, -0.2711])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.sinh", "item_type": "function", "code": "torch.sinh(input,out=None)\u2192Tensor", "description": "Returns a new tensor with the hyperbolic sine of the elements of input.  outi=sinh\u2061(inputi)\\text{out}_{i} = \\sinh(\\text{input}_{i})  outi\u200b=sinh(inputi\u200b)   Parameters  input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a tensor([ 0.5380, -0.8632, -0.1265,  0.9399]) &gt;&gt;&gt; torch.sinh(a) tensor([ 0.5644, -0.9744, -0.1268,  1.0845])   ", "parameters": ["input (Tensor) : the input tensor.", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " a = torch.randn(4)\n a\ntensor([ 0.5380, -0.8632, -0.1265,  0.9399])\n torch.sinh(a)\ntensor([ 0.5644, -0.9744, -0.1268,  1.0845])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.sqrt", "item_type": "function", "code": "torch.sqrt(input,out=None)\u2192Tensor", "description": "Returns a new tensor with the square-root of the elements of input.  outi=inputi\\text{out}_{i} = \\sqrt{\\text{input}_{i}}  outi\u200b=inputi\u200b\u200b   Parameters  input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a tensor([-2.0755,  1.0226,  0.0831,  0.4806]) &gt;&gt;&gt; torch.sqrt(a) tensor([    nan,  1.0112,  0.2883,  0.6933])   ", "parameters": ["input (Tensor) : the input tensor.", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " a = torch.randn(4)\n a\ntensor([-2.0755,  1.0226,  0.0831,  0.4806])\n torch.sqrt(a)\ntensor([    nan,  1.0112,  0.2883,  0.6933])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.tan", "item_type": "function", "code": "torch.tan(input,out=None)\u2192Tensor", "description": "Returns a new tensor with the tangent of the elements of input.  outi=tan\u2061(inputi)\\text{out}_{i} = \\tan(\\text{input}_{i})  outi\u200b=tan(inputi\u200b)   Parameters  input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a tensor([-1.2027, -1.7687,  0.4412, -1.3856]) &gt;&gt;&gt; torch.tan(a) tensor([-2.5930,  4.9859,  0.4722, -5.3366])   ", "parameters": ["input (Tensor) : the input tensor.", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " a = torch.randn(4)\n a\ntensor([-1.2027, -1.7687,  0.4412, -1.3856])\n torch.tan(a)\ntensor([-2.5930,  4.9859,  0.4722, -5.3366])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.multivariate_normal.MultivariateNormal.arg_constraints", "item_type": "attribute", "code": "arg_constraints={'covariance_matrix':PositiveDefinite(),'loc':RealVector(),'precision_matrix':PositiveDefinite(),'scale_tril':LowerCholesky()}", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix", "item_type": "attribute", "code": "covariance_matrix", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.multivariate_normal.MultivariateNormal.has_rsample", "item_type": "attribute", "code": "has_rsample=True", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix", "item_type": "attribute", "code": "precision_matrix", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.multivariate_normal.MultivariateNormal.scale_tril", "item_type": "attribute", "code": "scale_tril", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.multivariate_normal.MultivariateNormal.support", "item_type": "attribute", "code": "support=Real()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.negative_binomial.NegativeBinomial", "item_type": "class", "code": "classtorch.distributions.negative_binomial.NegativeBinomial(total_count,probs=None,logits=None,validate_args=None)", "description": "Bases: torch.distributions.distribution.Distribution Creates a Negative Binomial distribution, i.e. distribution of the number of successful independent and identical Bernoulli trials before total_count failures are achieved. The probability of success of each Bernoulli trial is probs.  Parameters  total_count (python:float or Tensor) \u2013 non-negative number of negative Bernoulli trials to stop, although the distribution is still valid for real valued count probs (Tensor) \u2013 Event probabilities of success in the half open interval [0, 1) logits (Tensor) \u2013 Event log-odds for probabilities of success      arg_constraints = {'logits': Real(), 'probs': HalfOpenInterval(lower_bound=0.0, upper_bound=1.0), 'total_count': GreaterThanEq(lower_bound=0)}     expand(batch_shape, _instance=None)      log_prob(value)      logits      property mean     property param_shape     probs      sample(sample_shape=torch.Size([]))      support = IntegerGreaterThan(lower_bound=0)     property variance   ", "parameters": ["total_count (python:float or Tensor) : non-negative number of negative Bernoullitrials to stop, although the distribution is still valid for realvalued count", "probs (Tensor) : Event probabilities of success in the half open interval [0, 1)", "logits (Tensor) : Event log-odds for probabilities of success"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.negative_binomial.NegativeBinomial.arg_constraints", "item_type": "attribute", "code": "arg_constraints={'logits':Real(),'probs':HalfOpenInterval(lower_bound=0.0,upper_bound=1.0),'total_count':GreaterThanEq(lower_bound=0)}", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.negative_binomial.NegativeBinomial.logits", "item_type": "attribute", "code": "logits", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.negative_binomial.NegativeBinomial.probs", "item_type": "attribute", "code": "probs", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.negative_binomial.NegativeBinomial.support", "item_type": "attribute", "code": "support=IntegerGreaterThan(lower_bound=0)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.Unfold", "item_type": "class", "code": "classtorch.nn.Unfold(kernel_size,dilation=1,padding=0,stride=1)", "description": "Extracts sliding local blocks from a batched input tensor. Consider a batched input tensor of shape (N,C,\u2217)(N, C, *)(N,C,\u2217)  , where NNN   is the batch dimension, CCC   is the channel dimension, and \u2217*\u2217   represent arbitrary spatial dimensions. This operation flattens each sliding kernel_size-sized block within the spatial dimensions of input into a column (i.e., last dimension) of a 3-D output tensor of shape (N,C\u00d7\u220f(kernel_size),L)(N, C \\times \\prod(\\text{kernel\\_size}), L)(N,C\u00d7\u220f(kernel_size),L)  , where C\u00d7\u220f(kernel_size)C \\times \\prod(\\text{kernel\\_size})C\u00d7\u220f(kernel_size)   is the total number of values within each block (a block has \u220f(kernel_size)\\prod(\\text{kernel\\_size})\u220f(kernel_size)   spatial locations each containing a CCC  -channeled vector), and LLL   is the total number of such blocks:  L=\u220fd\u230aspatial_size[d]+2\u00d7padding[d]\u2212dilation[d]\u00d7(kernel_size[d]\u22121)\u22121stride[d]+1\u230b,L = \\prod_d \\left\\lfloor\\frac{\\text{spatial\\_size}[d] + 2 \\times \\text{padding}[d] %     - \\text{dilation}[d] \\times (\\text{kernel\\_size}[d] - 1) - 1}{\\text{stride}[d]} + 1\\right\\rfloor,  L=d\u220f\u200b\u230astride[d]spatial_size[d]+2\u00d7padding[d]\u2212dilation[d]\u00d7(kernel_size[d]\u22121)\u22121\u200b+1\u230b,  where spatial_size\\text{spatial\\_size}spatial_size   is formed by the spatial dimensions of input (\u2217*\u2217   above), and ddd   is over all spatial dimensions. Therefore, indexing output at the last dimension (column dimension) gives all values within a certain block. The padding, stride and dilation arguments specify how the sliding blocks are retrieved.  stride controls the stride for the sliding blocks. padding controls the amount of implicit zero-paddings on both sides for padding number of points for each dimension before reshaping. dilation controls the spacing between the kernel points; also known as the \u00e0 trous algorithm. It is harder to describe, but this link has a nice visualization of what dilation does.   Parameters  kernel_size (python:int or tuple) \u2013 the size of the sliding blocks stride (python:int or tuple, optional) \u2013 the stride of the sliding blocks in the input spatial dimensions. Default: 1 padding (python:int or tuple, optional) \u2013 implicit zero padding to be added on both sides of input. Default: 0 dilation (python:int or tuple, optional) \u2013 a parameter that controls the stride of elements within the neighborhood. Default: 1     If kernel_size, dilation, padding or stride is an int or a tuple of length 1, their values will be replicated across all spatial dimensions. For the case of two input spatial dimensions this operation is sometimes called im2col.   Note Fold calculates each combined value in the resulting large tensor by summing all values from all containing blocks. Unfold extracts the values in the local blocks by copying from the large tensor. So, if the blocks overlap, they are not inverses of each other. In general, folding and unfolding operations are related as follows. Consider Fold and Unfold instances created with the same parameters: &gt;&gt;&gt; fold_params = dict(kernel_size=..., dilation=..., padding=..., stride=...) &gt;&gt;&gt; fold = nn.Fold(output_size=..., **fold_params) &gt;&gt;&gt; unfold = nn.Unfold(**fold_params)   Then for any (supported) input tensor the following equality holds: fold(unfold(input)) == divisor * input   where divisor is a tensor that depends only on the shape and dtype of the input: &gt;&gt;&gt; input_ones = torch.ones(input.shape, dtype=input.dtype) &gt;&gt;&gt; divisor = fold(unfold(input_ones))   When the divisor tensor contains no zero elements, then fold and unfold operations are inverses of each other (upto constant divisor).   Warning Currently, only 4-D input tensors (batched image-like tensors) are supported.   Shape: Input: (N,C,\u2217)(N, C, *)(N,C,\u2217)   Output: (N,C\u00d7\u220f(kernel_size),L)(N, C \\times \\prod(\\text{kernel\\_size}), L)(N,C\u00d7\u220f(kernel_size),L)   as described above    Examples: &gt;&gt;&gt; unfold = nn.Unfold(kernel_size=(2, 3)) &gt;&gt;&gt; input = torch.randn(2, 5, 3, 4) &gt;&gt;&gt; output = unfold(input) &gt;&gt;&gt; # each patch contains 30 values (2x3=6 vectors, each of 5 channels) &gt;&gt;&gt; # 4 blocks (2x3 kernels) in total in the 3x4 input &gt;&gt;&gt; output.size() torch.Size([2, 30, 4])  &gt;&gt;&gt; # Convolution is equivalent with Unfold + Matrix Multiplication + Fold (or view to output shape) &gt;&gt;&gt; inp = torch.randn(1, 3, 10, 12) &gt;&gt;&gt; w = torch.randn(2, 3, 4, 5) &gt;&gt;&gt; inp_unf = torch.nn.functional.unfold(inp, (4, 5)) &gt;&gt;&gt; out_unf = inp_unf.transpose(1, 2).matmul(w.view(w.size(0), -1).t()).transpose(1, 2) &gt;&gt;&gt; out = torch.nn.functional.fold(out_unf, (7, 8), (1, 1)) &gt;&gt;&gt; # or equivalently (and avoiding a copy), &gt;&gt;&gt; # out = out_unf.view(1, 2, 7, 8) &gt;&gt;&gt; (torch.nn.functional.conv2d(inp, w) - out).abs().max() tensor(1.9073e-06)   ", "parameters": ["kernel_size (python:int or tuple) : the size of the sliding blocks", "stride (python:int or tuple, optional) : the stride of the sliding blocks in the inputspatial dimensions. Default: 1", "padding (python:int or tuple, optional) : implicit zero padding to be added onboth sides of input. Default: 0", "dilation (python:int or tuple, optional) : a parameter that controls thestride of elements within theneighborhood. Default: 1"], "returns": null, "example": " unfold = nn.Unfold(kernel_size=(2, 3))\n input = torch.randn(2, 5, 3, 4)\n output = unfold(input)\n # each patch contains 30 values (2x3=6 vectors, each of 5 channels)\n # 4 blocks (2x3 kernels) in total in the 3x4 input\n output.size()\ntorch.Size([2, 30, 4])\n\n # Convolution is equivalent with Unfold + Matrix Multiplication + Fold (or view to output shape)\n inp = torch.randn(1, 3, 10, 12)\n w = torch.randn(2, 3, 4, 5)\n inp_unf = torch.nn.functional.unfold(inp, (4, 5))\n out_unf = inp_unf.transpose(1, 2).matmul(w.view(w.size(0), -1).t()).transpose(1, 2)\n out = torch.nn.functional.fold(out_unf, (7, 8), (1, 1))\n # or equivalently (and avoiding a copy),\n # out = out_unf.view(1, 2, 7, 8)\n (torch.nn.functional.conv2d(inp, w) - out).abs().max()\ntensor(1.9073e-06)\n\n", "shape": " Input: (N,C,\u2217)(N, C, *)(N,C,\u2217)   Output: (N,C\u00d7\u220f(kernel_size),L)(N, C \\times \\prod(\\text{kernel\\_size}), L)(N,C\u00d7\u220f(kernel_size),L)   as described above  "},
{"library": "torch", "item_id": "torch.nn.Fold", "item_type": "class", "code": "classtorch.nn.Fold(output_size,kernel_size,dilation=1,padding=0,stride=1)", "description": "Combines an array of sliding local blocks into a large containing tensor. Consider a batched input tensor containing sliding local blocks, e.g., patches of images, of shape (N,C\u00d7\u220f(kernel_size),L)(N, C \\times  \\prod(\\text{kernel\\_size}), L)(N,C\u00d7\u220f(kernel_size),L)  , where NNN   is batch dimension, C\u00d7\u220f(kernel_size)C \\times \\prod(\\text{kernel\\_size})C\u00d7\u220f(kernel_size)   is the number of values within a block (a block has \u220f(kernel_size)\\prod(\\text{kernel\\_size})\u220f(kernel_size)   spatial locations each containing a CCC  -channeled vector), and LLL   is the total number of blocks. (This is exactly the same specification as the output shape of Unfold.) This operation combines these local blocks into the large output tensor of shape (N,C,output_size[0],output_size[1],\u2026\u2009)(N, C, \\text{output\\_size}[0], \\text{output\\_size}[1], \\dots)(N,C,output_size[0],output_size[1],\u2026)   by summing the overlapping values. Similar to Unfold, the arguments must satisfy  L=\u220fd\u230aoutput_size[d]+2\u00d7padding[d]\u2212dilation[d]\u00d7(kernel_size[d]\u22121)\u22121stride[d]+1\u230b,L = \\prod_d \\left\\lfloor\\frac{\\text{output\\_size}[d] + 2 \\times \\text{padding}[d] %     - \\text{dilation}[d] \\times (\\text{kernel\\_size}[d] - 1) - 1}{\\text{stride}[d]} + 1\\right\\rfloor,  L=d\u220f\u200b\u230astride[d]output_size[d]+2\u00d7padding[d]\u2212dilation[d]\u00d7(kernel_size[d]\u22121)\u22121\u200b+1\u230b,  where ddd   is over all spatial dimensions.  output_size describes the spatial shape of the large containing tensor of the sliding local blocks. It is useful to resolve the ambiguity when multiple input shapes map to same number of sliding blocks, e.g., with stride &gt; 0.  The padding, stride and dilation arguments specify how the sliding blocks are retrieved.  stride controls the stride for the sliding blocks. padding controls the amount of implicit zero-paddings on both sides for padding number of points for each dimension before reshaping. dilation controls the spacing between the kernel points; also known as the \u00e0 trous algorithm. It is harder to describe, but this link has a nice visualization of what dilation does.   Parameters  output_size (python:int or tuple) \u2013 the shape of the spatial dimensions of the output (i.e., output.sizes()[2:]) kernel_size (python:int or tuple) \u2013 the size of the sliding blocks stride (python:int or tuple) \u2013 the stride of the sliding blocks in the input spatial dimensions. Default: 1 padding (python:int or tuple, optional) \u2013 implicit zero padding to be added on both sides of input. Default: 0 dilation (python:int or tuple, optional) \u2013 a parameter that controls the stride of elements within the neighborhood. Default: 1     If output_size, kernel_size, dilation, padding or stride is an int or a tuple of length 1 then their values will be replicated across all spatial dimensions. For the case of two output spatial dimensions this operation is sometimes called col2im.   Note Fold calculates each combined value in the resulting large tensor by summing all values from all containing blocks. Unfold extracts the values in the local blocks by copying from the large tensor. So, if the blocks overlap, they are not inverses of each other. In general, folding and unfolding operations are related as follows. Consider Fold and Unfold instances created with the same parameters: &gt;&gt;&gt; fold_params = dict(kernel_size=..., dilation=..., padding=..., stride=...) &gt;&gt;&gt; fold = nn.Fold(output_size=..., **fold_params) &gt;&gt;&gt; unfold = nn.Unfold(**fold_params)   Then for any (supported) input tensor the following equality holds: fold(unfold(input)) == divisor * input   where divisor is a tensor that depends only on the shape and dtype of the input: &gt;&gt;&gt; input_ones = torch.ones(input.shape, dtype=input.dtype) &gt;&gt;&gt; divisor = fold(unfold(input_ones))   When the divisor tensor contains no zero elements, then fold and unfold operations are inverses of each other (upto constant divisor).   Warning Currently, only 4-D output tensors (batched image-like tensors) are supported.   Shape: Input: (N,C\u00d7\u220f(kernel_size),L)(N, C \\times \\prod(\\text{kernel\\_size}), L)(N,C\u00d7\u220f(kernel_size),L)   Output: (N,C,output_size[0],output_size[1],\u2026\u2009)(N, C, \\text{output\\_size}[0], \\text{output\\_size}[1], \\dots)(N,C,output_size[0],output_size[1],\u2026)   as described above    Examples: &gt;&gt;&gt; fold = nn.Fold(output_size=(4, 5), kernel_size=(2, 2)) &gt;&gt;&gt; input = torch.randn(1, 3 * 2 * 2, 12) &gt;&gt;&gt; output = fold(input) &gt;&gt;&gt; output.size() torch.Size([1, 3, 4, 5])   ", "parameters": ["output_size (python:int or tuple) : the shape of the spatial dimensions of theoutput (i.e., output.sizes()[2:])", "kernel_size (python:int or tuple) : the size of the sliding blocks", "stride (python:int or tuple) : the stride of the sliding blocks in the inputspatial dimensions. Default: 1", "padding (python:int or tuple, optional) : implicit zero padding to be added onboth sides of input. Default: 0", "dilation (python:int or tuple, optional) : a parameter that controls thestride of elements within theneighborhood. Default: 1"], "returns": null, "example": " fold = nn.Fold(output_size=(4, 5), kernel_size=(2, 2))\n input = torch.randn(1, 3 * 2 * 2, 12)\n output = fold(input)\n output.size()\ntorch.Size([1, 3, 4, 5])\n\n", "shape": " Input: (N,C\u00d7\u220f(kernel_size),L)(N, C \\times \\prod(\\text{kernel\\_size}), L)(N,C\u00d7\u220f(kernel_size),L)   Output: (N,C,output_size[0],output_size[1],\u2026\u2009)(N, C, \\text{output\\_size}[0], \\text{output\\_size}[1], \\dots)(N,C,output_size[0],output_size[1],\u2026)   as described above  "},
{"library": "torch", "item_id": "torch.Tensor.tolist", "item_type": "method", "code": "tolist()", "description": "\u201d tolist() -&gt; list or number Returns the tensor as a (nested) list. For scalars, a standard Python number is returned, just like with item(). Tensors are automatically moved to the CPU first if necessary. This operation is not differentiable. Examples: &gt;&gt;&gt; a = torch.randn(2, 2) &gt;&gt;&gt; a.tolist() [[0.012766935862600803, 0.5415473580360413],  [-0.08909505605697632, 0.7729271650314331]] &gt;&gt;&gt; a[0,0].tolist() 0.012766935862600803   ", "parameters": [], "returns": null, "example": " a = torch.randn(2, 2)\n a.tolist()\n[[0.012766935862600803, 0.5415473580360413],\n [-0.08909505605697632, 0.7729271650314331]]\n a[0,0].tolist()\n0.012766935862600803\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.topk", "item_type": "method", "code": "topk(k,dim=None,largest=True,sorted=True)-&gt;(Tensor,LongTensor)", "description": "See torch.topk() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.to_sparse", "item_type": "method", "code": "to_sparse(sparseDims)\u2192Tensor", "description": "Returns a sparse copy of the tensor.  PyTorch supports sparse tensors in coordinate format.  Parameters sparseDims (python:int, optional) \u2013 the number of sparse dimensions to include in the new sparse tensor   Example: &gt;&gt;&gt; d = torch.tensor([[0, 0, 0], [9, 0, 10], [0, 0, 0]]) &gt;&gt;&gt; d tensor([[ 0,  0,  0],         [ 9,  0, 10],         [ 0,  0,  0]]) &gt;&gt;&gt; d.to_sparse() tensor(indices=tensor([[1, 1],                        [0, 2]]),        values=tensor([ 9, 10]),        size=(3, 3), nnz=2, layout=torch.sparse_coo) &gt;&gt;&gt; d.to_sparse(1) tensor(indices=tensor([[1]]),        values=tensor([[ 9,  0, 10]]),        size=(3, 3), nnz=1, layout=torch.sparse_coo)   ", "parameters": ["sparseDims (python:int, optional) : the number of sparse dimensions to include in the new sparse tensor"], "returns": null, "example": " d = torch.tensor([[0, 0, 0], [9, 0, 10], [0, 0, 0]])\n d\ntensor([[ 0,  0,  0],\n        [ 9,  0, 10],\n        [ 0,  0,  0]])\n d.to_sparse()\ntensor(indices=tensor([[1, 1],\n                       [0, 2]]),\n       values=tensor([ 9, 10]),\n       size=(3, 3), nnz=2, layout=torch.sparse_coo)\n d.to_sparse(1)\ntensor(indices=tensor([[1]]),\n       values=tensor([[ 9,  0, 10]]),\n       size=(3, 3), nnz=1, layout=torch.sparse_coo)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.trace", "item_type": "method", "code": "trace()\u2192Tensor", "description": "See torch.trace() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.transpose", "item_type": "method", "code": "transpose(dim0,dim1)\u2192Tensor", "description": "See torch.transpose() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.transpose_", "item_type": "method", "code": "transpose_(dim0,dim1)\u2192Tensor", "description": "In-place version of transpose() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.triangular_solve", "item_type": "method", "code": "triangular_solve(A,upper=True,transpose=False,unitriangular=False)-&gt;(Tensor,Tensor)", "description": "See torch.triangular_solve() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.tril", "item_type": "method", "code": "tril(k=0)\u2192Tensor", "description": "See torch.tril() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.tril_", "item_type": "method", "code": "tril_(k=0)\u2192Tensor", "description": "In-place version of tril() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.triu", "item_type": "method", "code": "triu(k=0)\u2192Tensor", "description": "See torch.triu() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.triu_", "item_type": "method", "code": "triu_(k=0)\u2192Tensor", "description": "In-place version of triu() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.tanh", "item_type": "function", "code": "torch.tanh(input,out=None)\u2192Tensor", "description": "Returns a new tensor with the hyperbolic tangent of the elements of input.  outi=tanh\u2061(inputi)\\text{out}_{i} = \\tanh(\\text{input}_{i})  outi\u200b=tanh(inputi\u200b)   Parameters  input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a tensor([ 0.8986, -0.7279,  1.1745,  0.2611]) &gt;&gt;&gt; torch.tanh(a) tensor([ 0.7156, -0.6218,  0.8257,  0.2553])   ", "parameters": ["input (Tensor) : the input tensor.", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " a = torch.randn(4)\n a\ntensor([ 0.8986, -0.7279,  1.1745,  0.2611])\n torch.tanh(a)\ntensor([ 0.7156, -0.6218,  0.8257,  0.2553])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.trunc", "item_type": "function", "code": "torch.trunc(input,out=None)\u2192Tensor", "description": "Returns a new tensor with the truncated integer values of the elements of input.  Parameters  input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a tensor([ 3.4742,  0.5466, -0.8008, -0.9079]) &gt;&gt;&gt; torch.trunc(a) tensor([ 3.,  0., -0., -0.])   ", "parameters": ["input (Tensor) : the input tensor.", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " a = torch.randn(4)\n a\ntensor([ 3.4742,  0.5466, -0.8008, -0.9079])\n torch.trunc(a)\ntensor([ 3.,  0., -0., -0.])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.argmax", "item_type": "function", "code": "torch.argmax()", "description": "  torch.argmax(input) \u2192 LongTensor   Returns the indices of the maximum value of all elements in the input tensor. This is the second value returned by torch.max(). See its documentation for the exact semantics of this method.  Parameters input (Tensor) \u2013 the input tensor.   Example: &gt;&gt;&gt; a = torch.randn(4, 4) &gt;&gt;&gt; a tensor([[ 1.3398,  0.2663, -0.2686,  0.2450],         [-0.7401, -0.8805, -0.3402, -1.1936],         [ 0.4907, -1.3948, -1.0691, -0.3132],         [-1.6092,  0.5419, -0.2993,  0.3195]]) &gt;&gt;&gt; torch.argmax(a) tensor(0)     torch.argmax(input, dim, keepdim=False) \u2192 LongTensor   Returns the indices of the maximum values of a tensor across a dimension. This is the second value returned by torch.max(). See its documentation for the exact semantics of this method.  Parameters  input (Tensor) \u2013 the input tensor. dim (python:int) \u2013 the dimension to reduce. If None, the argmax of the flattened input is returned. keepdim (bool) \u2013 whether the output tensor has dim retained or not. Ignored if dim=None.    Example: &gt;&gt;&gt; a = torch.randn(4, 4) &gt;&gt;&gt; a tensor([[ 1.3398,  0.2663, -0.2686,  0.2450],         [-0.7401, -0.8805, -0.3402, -1.1936],         [ 0.4907, -1.3948, -1.0691, -0.3132],         [-1.6092,  0.5419, -0.2993,  0.3195]]) &gt;&gt;&gt; torch.argmax(a, dim=1) tensor([ 0,  2,  0,  1])   ", "parameters": ["input (Tensor) : the input tensor.", "dim (python:int) : the dimension to reduce. If None, the argmax of the flattened input is returned.", "keepdim (bool) : whether the output tensor has dim retained or not. Ignored if dim=None."], "returns": null, "example": " a = torch.randn(4, 4)\n a\ntensor([[ 1.3398,  0.2663, -0.2686,  0.2450],\n        [-0.7401, -0.8805, -0.3402, -1.1936],\n        [ 0.4907, -1.3948, -1.0691, -0.3132],\n        [-1.6092,  0.5419, -0.2993,  0.3195]])\n torch.argmax(a)\ntensor(0)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.argmax(input)\u2192LongTensor", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.argmax(input,dim,keepdim=False)\u2192LongTensor", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.argmin", "item_type": "function", "code": "torch.argmin()", "description": "  torch.argmin(input) \u2192 LongTensor   Returns the indices of the minimum value of all elements in the input tensor. This is the second value returned by torch.min(). See its documentation for the exact semantics of this method.  Parameters input (Tensor) \u2013 the input tensor.   Example: &gt;&gt;&gt; a = torch.randn(4, 4) &gt;&gt;&gt; a tensor([[ 0.1139,  0.2254, -0.1381,  0.3687],         [ 1.0100, -1.1975, -0.0102, -0.4732],         [-0.9240,  0.1207, -0.7506, -1.0213],         [ 1.7809, -1.2960,  0.9384,  0.1438]]) &gt;&gt;&gt; torch.argmin(a) tensor(13)     torch.argmin(input, dim, keepdim=False, out=None) \u2192 LongTensor   Returns the indices of the minimum values of a tensor across a dimension. This is the second value returned by torch.min(). See its documentation for the exact semantics of this method.  Parameters  input (Tensor) \u2013 the input tensor. dim (python:int) \u2013 the dimension to reduce. If None, the argmin of the flattened input is returned. keepdim (bool) \u2013 whether the output tensor has dim retained or not. Ignored if dim=None.    Example: &gt;&gt;&gt; a = torch.randn(4, 4) &gt;&gt;&gt; a tensor([[ 0.1139,  0.2254, -0.1381,  0.3687],         [ 1.0100, -1.1975, -0.0102, -0.4732],         [-0.9240,  0.1207, -0.7506, -1.0213],         [ 1.7809, -1.2960,  0.9384,  0.1438]]) &gt;&gt;&gt; torch.argmin(a, dim=1) tensor([ 2,  1,  3,  1])   ", "parameters": ["input (Tensor) : the input tensor.", "dim (python:int) : the dimension to reduce. If None, the argmin of the flattened input is returned.", "keepdim (bool) : whether the output tensor has dim retained or not. Ignored if dim=None."], "returns": null, "example": " a = torch.randn(4, 4)\n a\ntensor([[ 0.1139,  0.2254, -0.1381,  0.3687],\n        [ 1.0100, -1.1975, -0.0102, -0.4732],\n        [-0.9240,  0.1207, -0.7506, -1.0213],\n        [ 1.7809, -1.2960,  0.9384,  0.1438]])\n torch.argmin(a)\ntensor(13)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.argmin(input)\u2192LongTensor", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.argmin(input,dim,keepdim=False,out=None)\u2192LongTensor", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.normal.Normal", "item_type": "class", "code": "classtorch.distributions.normal.Normal(loc,scale,validate_args=None)", "description": "Bases: torch.distributions.exp_family.ExponentialFamily Creates a normal (also called Gaussian) distribution parameterized by loc and scale. Example: &gt;&gt;&gt; m = Normal(torch.tensor([0.0]), torch.tensor([1.0])) &gt;&gt;&gt; m.sample()  # normally distributed with loc=0 and scale=1 tensor([ 0.1046])    Parameters  loc (python:float or Tensor) \u2013 mean of the distribution (often referred to as mu) scale (python:float or Tensor) \u2013 standard deviation of the distribution (often referred to as sigma)      arg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)}     cdf(value)      entropy()      expand(batch_shape, _instance=None)      has_rsample = True     icdf(value)      log_prob(value)      property mean     rsample(sample_shape=torch.Size([]))      sample(sample_shape=torch.Size([]))      property stddev     support = Real()     property variance   ", "parameters": ["loc (python:float or Tensor) : mean of the distribution (often referred to as mu)", "scale (python:float or Tensor) : standard deviation of the distribution(often referred to as sigma)"], "returns": null, "example": " m = Normal(torch.tensor([0.0]), torch.tensor([1.0]))\n m.sample()  # normally distributed with loc=0 and scale=1\ntensor([ 0.1046])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.normal.Normal.arg_constraints", "item_type": "attribute", "code": "arg_constraints={'loc':Real(),'scale':GreaterThan(lower_bound=0.0)}", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.normal.Normal.has_rsample", "item_type": "attribute", "code": "has_rsample=True", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.normal.Normal.support", "item_type": "attribute", "code": "support=Real()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.one_hot_categorical.OneHotCategorical", "item_type": "class", "code": "classtorch.distributions.one_hot_categorical.OneHotCategorical(probs=None,logits=None,validate_args=None)", "description": "Bases: torch.distributions.distribution.Distribution Creates a one-hot categorical distribution parameterized by probs or logits. Samples are one-hot coded vectors of size probs.size(-1).  Note probs must be non-negative, finite and have a non-zero sum, and it will be normalized to sum to 1.  See also: torch.distributions.Categorical() for specifications of probs and logits. Example: &gt;&gt;&gt; m = OneHotCategorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ])) &gt;&gt;&gt; m.sample()  # equal probability of 0, 1, 2, 3 tensor([ 0.,  0.,  0.,  1.])    Parameters  probs (Tensor) \u2013 event probabilities logits (Tensor) \u2013 event log probabilities      arg_constraints = {'logits': Real(), 'probs': Simplex()}     entropy()      enumerate_support(expand=True)      expand(batch_shape, _instance=None)      has_enumerate_support = True     log_prob(value)      property logits     property mean     property param_shape     property probs     sample(sample_shape=torch.Size([]))      support = Simplex()     property variance   ", "parameters": ["probs (Tensor) : event probabilities", "logits (Tensor) : event log probabilities"], "returns": null, "example": " m = OneHotCategorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))\n m.sample()  # equal probability of 0, 1, 2, 3\ntensor([ 0.,  0.,  0.,  1.])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.one_hot_categorical.OneHotCategorical.arg_constraints", "item_type": "attribute", "code": "arg_constraints={'logits':Real(),'probs':Simplex()}", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.one_hot_categorical.OneHotCategorical.has_enumerate_support", "item_type": "attribute", "code": "has_enumerate_support=True", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.one_hot_categorical.OneHotCategorical.support", "item_type": "attribute", "code": "support=Simplex()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.pareto.Pareto", "item_type": "class", "code": "classtorch.distributions.pareto.Pareto(scale,alpha,validate_args=None)", "description": "Bases: torch.distributions.transformed_distribution.TransformedDistribution Samples from a Pareto Type 1 distribution. Example: &gt;&gt;&gt; m = Pareto(torch.tensor([1.0]), torch.tensor([1.0])) &gt;&gt;&gt; m.sample()  # sample from a Pareto distribution with scale=1 and alpha=1 tensor([ 1.5623])    Parameters  scale (python:float or Tensor) \u2013 Scale parameter of the distribution alpha (python:float or Tensor) \u2013 Shape parameter of the distribution      arg_constraints = {'alpha': GreaterThan(lower_bound=0.0), 'scale': GreaterThan(lower_bound=0.0)}     entropy()      expand(batch_shape, _instance=None)      property mean     property support     property variance   ", "parameters": ["scale (python:float or Tensor) : Scale parameter of the distribution", "alpha (python:float or Tensor) : Shape parameter of the distribution"], "returns": null, "example": " m = Pareto(torch.tensor([1.0]), torch.tensor([1.0]))\n m.sample()  # sample from a Pareto distribution with scale=1 and alpha=1\ntensor([ 1.5623])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.MaxPool1d", "item_type": "class", "code": "classtorch.nn.MaxPool1d(kernel_size,stride=None,padding=0,dilation=1,return_indices=False,ceil_mode=False)", "description": "Applies a 1D max pooling over an input signal composed of several input planes. In the simplest case, the output value of the layer with input size (N,C,L)(N, C, L)(N,C,L)   and output (N,C,Lout)(N, C, L_{out})(N,C,Lout\u200b)   can be precisely described as:  out(Ni,Cj,k)=max\u2061m=0,\u2026,kernel_size\u22121input(Ni,Cj,stride\u00d7k+m)out(N_i, C_j, k) = \\max_{m=0, \\ldots, \\text{kernel\\_size} - 1}         input(N_i, C_j, stride \\times k + m)  out(Ni\u200b,Cj\u200b,k)=m=0,\u2026,kernel_size\u22121max\u200binput(Ni\u200b,Cj\u200b,stride\u00d7k+m)  If padding is non-zero, then the input is implicitly zero-padded on both sides for padding number of points. dilation controls the spacing between the kernel points. It is harder to describe, but this link has a nice visualization of what dilation does.  Parameters  kernel_size \u2013 the size of the window to take a max over stride \u2013 the stride of the window. Default value is kernel_size padding \u2013 implicit zero padding to be added on both sides dilation \u2013 a parameter that controls the stride of elements in the window return_indices \u2013 if True, will return the max indices along with the outputs. Useful for torch.nn.MaxUnpool1d later ceil_mode \u2013 when True, will use ceil instead of floor to compute the output shape     Shape: Input: (N,C,Lin)(N, C, L_{in})(N,C,Lin\u200b)   Output: (N,C,Lout)(N, C, L_{out})(N,C,Lout\u200b)  , where  Lout=\u230aLin+2\u00d7padding\u2212dilation\u00d7(kernel_size\u22121)\u22121stride+1\u230bL_{out} = \\left\\lfloor \\frac{L_{in} + 2 \\times \\text{padding} - \\text{dilation}       \\times (\\text{kernel\\_size} - 1) - 1}{\\text{stride}} + 1\\right\\rfloor  Lout\u200b=\u230astrideLin\u200b+2\u00d7padding\u2212dilation\u00d7(kernel_size\u22121)\u22121\u200b+1\u230b      Examples: &gt;&gt;&gt; # pool of size=3, stride=2 &gt;&gt;&gt; m = nn.MaxPool1d(3, stride=2) &gt;&gt;&gt; input = torch.randn(20, 16, 50) &gt;&gt;&gt; output = m(input)   ", "parameters": ["kernel_size : the size of the window to take a max over", "stride : the stride of the window. Default value is kernel_size", "padding : implicit zero padding to be added on both sides", "dilation : a parameter that controls the stride of elements in the window", "return_indices : if True, will return the max indices along with the outputs.Useful for torch.nn.MaxUnpool1d later", "ceil_mode : when True, will use ceil instead of floor to compute the output shape"], "returns": null, "example": " # pool of size=3, stride=2\n m = nn.MaxPool1d(3, stride=2)\n input = torch.randn(20, 16, 50)\n output = m(input)\n\n", "shape": " Input: (N,C,Lin)(N, C, L_{in})(N,C,Lin\u200b)   Output: (N,C,Lout)(N, C, L_{out})(N,C,Lout\u200b)  , where  Lout=\u230aLin+2\u00d7padding\u2212dilation\u00d7(kernel_size\u22121)\u22121stride+1\u230bL_{out} = \\left\\lfloor \\frac{L_{in} + 2 \\times \\text{padding} - \\text{dilation}       \\times (\\text{kernel\\_size} - 1) - 1}{\\text{stride}} + 1\\right\\rfloor  Lout\u200b=\u230astrideLin\u200b+2\u00d7padding\u2212dilation\u00d7(kernel_size\u22121)\u22121\u200b+1\u230b    "},
{"library": "torch", "item_id": "torch.nn.MaxPool2d", "item_type": "class", "code": "classtorch.nn.MaxPool2d(kernel_size,stride=None,padding=0,dilation=1,return_indices=False,ceil_mode=False)", "description": "Applies a 2D max pooling over an input signal composed of several input planes. In the simplest case, the output value of the layer with input size (N,C,H,W)(N, C, H, W)(N,C,H,W)  , output (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)   and kernel_size (kH,kW)(kH, kW)(kH,kW)   can be precisely described as:  out(Ni,Cj,h,w)=max\u2061m=0,\u2026,kH\u22121max\u2061n=0,\u2026,kW\u22121input(Ni,Cj,stride[0]\u00d7h+m,stride[1]\u00d7w+n)\\begin{aligned}     out(N_i, C_j, h, w) ={} &amp; \\max_{m=0, \\ldots, kH-1} \\max_{n=0, \\ldots, kW-1} \\\\                             &amp; \\text{input}(N_i, C_j, \\text{stride[0]} \\times h + m,                                            \\text{stride[1]} \\times w + n) \\end{aligned}  out(Ni\u200b,Cj\u200b,h,w)=\u200bm=0,\u2026,kH\u22121max\u200bn=0,\u2026,kW\u22121max\u200binput(Ni\u200b,Cj\u200b,stride[0]\u00d7h+m,stride[1]\u00d7w+n)\u200b  If padding is non-zero, then the input is implicitly zero-padded on both sides for padding number of points. dilation controls the spacing between the kernel points. It is harder to describe, but this link has a nice visualization of what dilation does. The parameters kernel_size, stride, padding, dilation can either be:   a single int \u2013 in which case the same value is used for the height and width dimension a tuple of two ints \u2013 in which case, the first int is used for the height dimension, and the second int for the width dimension    Parameters  kernel_size \u2013 the size of the window to take a max over stride \u2013 the stride of the window. Default value is kernel_size padding \u2013 implicit zero padding to be added on both sides dilation \u2013 a parameter that controls the stride of elements in the window return_indices \u2013 if True, will return the max indices along with the outputs. Useful for torch.nn.MaxUnpool2d later ceil_mode \u2013 when True, will use ceil instead of floor to compute the output shape     Shape: Input: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)   Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)  , where  Hout=\u230aHin+2\u2217padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121stride[0]+1\u230bH_{out} = \\left\\lfloor\\frac{H_{in} + 2 * \\text{padding[0]} - \\text{dilation[0]}       \\times (\\text{kernel\\_size[0]} - 1) - 1}{\\text{stride[0]}} + 1\\right\\rfloor  Hout\u200b=\u230astride[0]Hin\u200b+2\u2217padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121\u200b+1\u230b   Wout=\u230aWin+2\u2217padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121stride[1]+1\u230bW_{out} = \\left\\lfloor\\frac{W_{in} + 2 * \\text{padding[1]} - \\text{dilation[1]}       \\times (\\text{kernel\\_size[1]} - 1) - 1}{\\text{stride[1]}} + 1\\right\\rfloor  Wout\u200b=\u230astride[1]Win\u200b+2\u2217padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121\u200b+1\u230b      Examples: &gt;&gt;&gt; # pool of square window of size=3, stride=2 &gt;&gt;&gt; m = nn.MaxPool2d(3, stride=2) &gt;&gt;&gt; # pool of non-square window &gt;&gt;&gt; m = nn.MaxPool2d((3, 2), stride=(2, 1)) &gt;&gt;&gt; input = torch.randn(20, 16, 50, 32) &gt;&gt;&gt; output = m(input)   ", "parameters": ["kernel_size : the size of the window to take a max over", "stride : the stride of the window. Default value is kernel_size", "padding : implicit zero padding to be added on both sides", "dilation : a parameter that controls the stride of elements in the window", "return_indices : if True, will return the max indices along with the outputs.Useful for torch.nn.MaxUnpool2d later", "ceil_mode : when True, will use ceil instead of floor to compute the output shape"], "returns": null, "example": " # pool of square window of size=3, stride=2\n m = nn.MaxPool2d(3, stride=2)\n # pool of non-square window\n m = nn.MaxPool2d((3, 2), stride=(2, 1))\n input = torch.randn(20, 16, 50, 32)\n output = m(input)\n\n", "shape": " Input: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)   Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)  , where  Hout=\u230aHin+2\u2217padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121stride[0]+1\u230bH_{out} = \\left\\lfloor\\frac{H_{in} + 2 * \\text{padding[0]} - \\text{dilation[0]}       \\times (\\text{kernel\\_size[0]} - 1) - 1}{\\text{stride[0]}} + 1\\right\\rfloor  Hout\u200b=\u230astride[0]Hin\u200b+2\u2217padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121\u200b+1\u230b   Wout=\u230aWin+2\u2217padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121stride[1]+1\u230bW_{out} = \\left\\lfloor\\frac{W_{in} + 2 * \\text{padding[1]} - \\text{dilation[1]}       \\times (\\text{kernel\\_size[1]} - 1) - 1}{\\text{stride[1]}} + 1\\right\\rfloor  Wout\u200b=\u230astride[1]Win\u200b+2\u2217padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121\u200b+1\u230b    "},
{"library": "torch", "item_id": "torch.Tensor.trunc", "item_type": "method", "code": "trunc()\u2192Tensor", "description": "See torch.trunc() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.trunc_", "item_type": "method", "code": "trunc_()\u2192Tensor", "description": "In-place version of trunc() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.type", "item_type": "method", "code": "type(dtype=None,non_blocking=False,**kwargs)\u2192strorTensor", "description": "Returns the type if dtype is not provided, else casts this object to the specified type. If this is already of the correct type, no copy is performed and the original object is returned.  Parameters  dtype (python:type or string) \u2013 The desired type non_blocking (bool) \u2013 If True, and the source is in pinned memory and destination is on the GPU or vice versa, the copy is performed asynchronously with respect to the host. Otherwise, the argument has no effect. **kwargs \u2013 For compatibility, may contain the key async in place of the non_blocking argument. The async arg is deprecated.    ", "parameters": ["dtype (python:type or string) : The desired type", "non_blocking (bool) : If True, and the source is in pinned memoryand destination is on the GPU or vice versa, the copy is performedasynchronously with respect to the host. Otherwise, the argumenthas no effect.", "**kwargs : For compatibility, may contain the key async in place ofthe non_blocking argument. The async arg is deprecated."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.type_as", "item_type": "method", "code": "type_as(tensor)\u2192Tensor", "description": "Returns this tensor cast to the type of the given tensor. This is a no-op if the tensor is already of the correct type. This is equivalent to self.type(tensor.type())  Parameters tensor (Tensor) \u2013 the tensor which has the desired type   ", "parameters": ["tensor (Tensor) : the tensor which has the desired type"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.unbind", "item_type": "method", "code": "unbind(dim=0)\u2192seq", "description": "See torch.unbind() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.unfold", "item_type": "method", "code": "unfold(dimension,size,step)\u2192Tensor", "description": "Returns a tensor which contains all slices of size size from self tensor in the dimension dimension. Step between two slices is given by step. If sizedim is the size of dimension dimension for self, the size of dimension dimension in the returned tensor will be (sizedim - size) / step + 1. An additional dimension of size size is appended in the returned tensor.  Parameters  dimension (python:int) \u2013 dimension in which unfolding happens size (python:int) \u2013 the size of each slice that is unfolded step (python:int) \u2013 the step between each slice    Example: &gt;&gt;&gt; x = torch.arange(1., 8) &gt;&gt;&gt; x tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.]) &gt;&gt;&gt; x.unfold(0, 2, 1) tensor([[ 1.,  2.],         [ 2.,  3.],         [ 3.,  4.],         [ 4.,  5.],         [ 5.,  6.],         [ 6.,  7.]]) &gt;&gt;&gt; x.unfold(0, 2, 2) tensor([[ 1.,  2.],         [ 3.,  4.],         [ 5.,  6.]])   ", "parameters": ["dimension (python:int) : dimension in which unfolding happens", "size (python:int) : the size of each slice that is unfolded", "step (python:int) : the step between each slice"], "returns": null, "example": " x = torch.arange(1., 8)\n x\ntensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.])\n x.unfold(0, 2, 1)\ntensor([[ 1.,  2.],\n        [ 2.,  3.],\n        [ 3.,  4.],\n        [ 4.,  5.],\n        [ 5.,  6.],\n        [ 6.,  7.]])\n x.unfold(0, 2, 2)\ntensor([[ 1.,  2.],\n        [ 3.,  4.],\n        [ 5.,  6.]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.uniform_", "item_type": "method", "code": "uniform_(from=0,to=1)\u2192Tensor", "description": "Fills self tensor with numbers sampled from the continuous uniform distribution:  P(x)=1to\u2212fromP(x) = \\dfrac{1}{\\text{to} - \\text{from}}  P(x)=to\u2212from1\u200b  ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.unique", "item_type": "method", "code": "unique(sorted=True,return_inverse=False,return_counts=False,dim=None)", "description": "Returns the unique elements of the input tensor. See torch.unique() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.unique_consecutive", "item_type": "method", "code": "unique_consecutive(return_inverse=False,return_counts=False,dim=None)", "description": "Eliminates all but the first element from every consecutive group of equivalent elements. See torch.unique_consecutive() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.unsqueeze", "item_type": "method", "code": "unsqueeze(dim)\u2192Tensor", "description": "See torch.unsqueeze() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.dist", "item_type": "function", "code": "torch.dist(input,other,p=2)\u2192Tensor", "description": "Returns the p-norm of (input - other) The shapes of input and other must be broadcastable.  Parameters  input (Tensor) \u2013 the input tensor. other (Tensor) \u2013 the Right-hand-side input tensor p (python:float, optional) \u2013 the norm to be computed    Example: &gt;&gt;&gt; x = torch.randn(4) &gt;&gt;&gt; x tensor([-1.5393, -0.8675,  0.5916,  1.6321]) &gt;&gt;&gt; y = torch.randn(4) &gt;&gt;&gt; y tensor([ 0.0967, -1.0511,  0.6295,  0.8360]) &gt;&gt;&gt; torch.dist(x, y, 3.5) tensor(1.6727) &gt;&gt;&gt; torch.dist(x, y, 3) tensor(1.6973) &gt;&gt;&gt; torch.dist(x, y, 0) tensor(inf) &gt;&gt;&gt; torch.dist(x, y, 1) tensor(2.6537)   ", "parameters": ["input (Tensor) : the input tensor.", "other (Tensor) : the Right-hand-side input tensor", "p (python:float, optional) : the norm to be computed"], "returns": null, "example": " x = torch.randn(4)\n x\ntensor([-1.5393, -0.8675,  0.5916,  1.6321])\n y = torch.randn(4)\n y\ntensor([ 0.0967, -1.0511,  0.6295,  0.8360])\n torch.dist(x, y, 3.5)\ntensor(1.6727)\n torch.dist(x, y, 3)\ntensor(1.6973)\n torch.dist(x, y, 0)\ntensor(inf)\n torch.dist(x, y, 1)\ntensor(2.6537)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.logsumexp", "item_type": "function", "code": "torch.logsumexp(input,dim,keepdim=False,out=None)", "description": "Returns the log of summed exponentials of each row of the input tensor in the given dimension dim. The computation is numerically stabilized. For summation index jjj   given by dim and other indices iii  , the result is   logsumexp(x)i=log\u2061\u2211jexp\u2061(xij)\\text{logsumexp}(x)_{i} = \\log \\sum_j \\exp(x_{ij})  logsumexp(x)i\u200b=logj\u2211\u200bexp(xij\u200b)   If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s).  Parameters  input (Tensor) \u2013 the input tensor. dim (python:int or tuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not. out (Tensor, optional) \u2013 the output tensor.     Example::&gt;&gt;&gt; a = torch.randn(3, 3) &gt;&gt;&gt; torch.logsumexp(a, 1) tensor([ 0.8442,  1.4322,  0.8711])     ", "parameters": ["input (Tensor) : the input tensor.", "dim (python:int or tuple of python:ints) : the dimension or dimensions to reduce.", "keepdim (bool) : whether the output tensor has dim retained or not.", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " a = torch.randn(3, 3)\n torch.logsumexp(a, 1)\ntensor([ 0.8442,  1.4322,  0.8711])\n\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.mean", "item_type": "function", "code": "torch.mean()", "description": "  torch.mean(input) \u2192 Tensor   Returns the mean value of all elements in the input tensor.  Parameters input (Tensor) \u2013 the input tensor.   Example: &gt;&gt;&gt; a = torch.randn(1, 3) &gt;&gt;&gt; a tensor([[ 0.2294, -0.5481,  1.3288]]) &gt;&gt;&gt; torch.mean(a) tensor(0.3367)     torch.mean(input, dim, keepdim=False, out=None) \u2192 Tensor   Returns the mean value of each row of the input tensor in the given dimension dim. If dim is a list of dimensions, reduce over all of them. If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s).  Parameters  input (Tensor) \u2013 the input tensor. dim (python:int or tuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not. out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; a = torch.randn(4, 4) &gt;&gt;&gt; a tensor([[-0.3841,  0.6320,  0.4254, -0.7384],         [-0.9644,  1.0131, -0.6549, -1.4279],         [-0.2951, -1.3350, -0.7694,  0.5600],         [ 1.0842, -0.9580,  0.3623,  0.2343]]) &gt;&gt;&gt; torch.mean(a, 1) tensor([-0.0163, -0.5085, -0.4599,  0.1807]) &gt;&gt;&gt; torch.mean(a, 1, True) tensor([[-0.0163],         [-0.5085],         [-0.4599],         [ 0.1807]])   ", "parameters": ["input (Tensor) : the input tensor.", "dim (python:int or tuple of python:ints) : the dimension or dimensions to reduce.", "keepdim (bool) : whether the output tensor has dim retained or not.", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " a = torch.randn(1, 3)\n a\ntensor([[ 0.2294, -0.5481,  1.3288]])\n torch.mean(a)\ntensor(0.3367)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.mean(input)\u2192Tensor", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.mean(input,dim,keepdim=False,out=None)\u2192Tensor", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.median", "item_type": "function", "code": "torch.median()", "description": "  torch.median(input) \u2192 Tensor   Returns the median value of all elements in the input tensor.  Parameters input (Tensor) \u2013 the input tensor.   Example: &gt;&gt;&gt; a = torch.randn(1, 3) &gt;&gt;&gt; a tensor([[ 1.5219, -1.5212,  0.2202]]) &gt;&gt;&gt; torch.median(a) tensor(0.2202)     torch.median(input, dim=-1, keepdim=False, values=None, indices=None) -&gt; (Tensor, LongTensor)   Returns a namedtuple (values, indices) where values is the median value of each row of the input tensor in the given dimension dim. And indices is the index location of each median value found. By default, dim is the last dimension of the input tensor. If keepdim is True, the output tensors are of the same size as input except in the dimension dim where they are of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the outputs tensor having 1 fewer dimension than input.  Parameters  input (Tensor) \u2013 the input tensor. dim (python:int) \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not. values (Tensor, optional) \u2013 the output tensor indices (Tensor, optional) \u2013 the output index tensor    Example: &gt;&gt;&gt; a = torch.randn(4, 5) &gt;&gt;&gt; a tensor([[ 0.2505, -0.3982, -0.9948,  0.3518, -1.3131],         [ 0.3180, -0.6993,  1.0436,  0.0438,  0.2270],         [-0.2751,  0.7303,  0.2192,  0.3321,  0.2488],         [ 1.0778, -1.9510,  0.7048,  0.4742, -0.7125]]) &gt;&gt;&gt; torch.median(a, 1) torch.return_types.median(values=tensor([-0.3982,  0.2270,  0.2488,  0.4742]), indices=tensor([1, 4, 4, 3]))   ", "parameters": ["input (Tensor) : the input tensor.", "dim (python:int) : the dimension to reduce.", "keepdim (bool) : whether the output tensor has dim retained or not.", "values (Tensor, optional) : the output tensor", "indices (Tensor, optional) : the output index tensor"], "returns": null, "example": " a = torch.randn(1, 3)\n a\ntensor([[ 1.5219, -1.5212,  0.2202]])\n torch.median(a)\ntensor(0.2202)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.median(input)\u2192Tensor", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.median(input,dim=-1,keepdim=False,values=None,indices=None)-&gt;(Tensor,LongTensor)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.pareto.Pareto.arg_constraints", "item_type": "attribute", "code": "arg_constraints={'alpha':GreaterThan(lower_bound=0.0),'scale':GreaterThan(lower_bound=0.0)}", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.poisson.Poisson", "item_type": "class", "code": "classtorch.distributions.poisson.Poisson(rate,validate_args=None)", "description": "Bases: torch.distributions.exp_family.ExponentialFamily Creates a Poisson distribution parameterized by rate, the rate parameter. Samples are nonnegative integers, with a pmf given by  rateke\u2212ratek!\\mathrm{rate}^k \\frac{e^{-\\mathrm{rate}}}{k!}  ratekk!e\u2212rate\u200b  Example: &gt;&gt;&gt; m = Poisson(torch.tensor([4])) &gt;&gt;&gt; m.sample() tensor([ 3.])    Parameters rate (Number, Tensor) \u2013 the rate parameter     arg_constraints = {'rate': GreaterThan(lower_bound=0.0)}     expand(batch_shape, _instance=None)      log_prob(value)      property mean     sample(sample_shape=torch.Size([]))      support = IntegerGreaterThan(lower_bound=0)     property variance   ", "parameters": ["rate (Number, Tensor) : the rate parameter"], "returns": null, "example": " m = Poisson(torch.tensor([4]))\n m.sample()\ntensor([ 3.])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.poisson.Poisson.arg_constraints", "item_type": "attribute", "code": "arg_constraints={'rate':GreaterThan(lower_bound=0.0)}", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.poisson.Poisson.support", "item_type": "attribute", "code": "support=IntegerGreaterThan(lower_bound=0)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli", "item_type": "class", "code": "classtorch.distributions.relaxed_bernoulli.RelaxedBernoulli(temperature,probs=None,logits=None,validate_args=None)", "description": "Bases: torch.distributions.transformed_distribution.TransformedDistribution Creates a RelaxedBernoulli distribution, parametrized by temperature, and either probs or logits (but not both). This is a relaxed version of the Bernoulli distribution, so the values are in (0, 1), and has reparametrizable samples. Example: &gt;&gt;&gt; m = RelaxedBernoulli(torch.tensor([2.2]),                          torch.tensor([0.1, 0.2, 0.3, 0.99])) &gt;&gt;&gt; m.sample() tensor([ 0.2951,  0.3442,  0.8918,  0.9021])    Parameters  temperature (Tensor) \u2013 relaxation temperature probs (Number, Tensor) \u2013 the probability of sampling 1 logits (Number, Tensor) \u2013 the log-odds of sampling 1      arg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)}     expand(batch_shape, _instance=None)      has_rsample = True     property logits     property probs     support = Interval(lower_bound=0.0, upper_bound=1.0)     property temperature   ", "parameters": ["temperature (Tensor) : relaxation temperature", "probs (Number, Tensor) : the probability of sampling 1", "logits (Number, Tensor) : the log-odds of sampling 1"], "returns": null, "example": " m = RelaxedBernoulli(torch.tensor([2.2]),\n                         torch.tensor([0.1, 0.2, 0.3, 0.99]))\n m.sample()\ntensor([ 0.2951,  0.3442,  0.8918,  0.9021])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.arg_constraints", "item_type": "attribute", "code": "arg_constraints={'logits':Real(),'probs':Interval(lower_bound=0.0,upper_bound=1.0)}", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.has_rsample", "item_type": "attribute", "code": "has_rsample=True", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.relaxed_bernoulli.RelaxedBernoulli.support", "item_type": "attribute", "code": "support=Interval(lower_bound=0.0,upper_bound=1.0)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli", "item_type": "class", "code": "classtorch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli(temperature,probs=None,logits=None,validate_args=None)", "description": "Bases: torch.distributions.distribution.Distribution Creates a LogitRelaxedBernoulli distribution parameterized by probs or logits (but not both), which is the logit of a RelaxedBernoulli distribution. Samples are logits of values in (0, 1). See [1] for more details.  Parameters  temperature (Tensor) \u2013 relaxation temperature probs (Number, Tensor) \u2013 the probability of sampling 1 logits (Number, Tensor) \u2013 the log-odds of sampling 1    [1] The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables (Maddison et al, 2017) [2] Categorical Reparametrization with Gumbel-Softmax (Jang et al, 2017)   arg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)}     expand(batch_shape, _instance=None)      log_prob(value)      logits      property param_shape     probs      rsample(sample_shape=torch.Size([]))      support = Real()   ", "parameters": ["temperature (Tensor) : relaxation temperature", "probs (Number, Tensor) : the probability of sampling 1", "logits (Number, Tensor) : the log-odds of sampling 1"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.MaxPool3d", "item_type": "class", "code": "classtorch.nn.MaxPool3d(kernel_size,stride=None,padding=0,dilation=1,return_indices=False,ceil_mode=False)", "description": "Applies a 3D max pooling over an input signal composed of several input planes. In the simplest case, the output value of the layer with input size (N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W)  , output (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\u200b,Hout\u200b,Wout\u200b)   and kernel_size (kD,kH,kW)(kD, kH, kW)(kD,kH,kW)   can be precisely described as:  out(Ni,Cj,d,h,w)=max\u2061k=0,\u2026,kD\u22121max\u2061m=0,\u2026,kH\u22121max\u2061n=0,\u2026,kW\u22121input(Ni,Cj,stride[0]\u00d7d+k,stride[1]\u00d7h+m,stride[2]\u00d7w+n)\\begin{aligned}     \\text{out}(N_i, C_j, d, h, w) ={} &amp; \\max_{k=0, \\ldots, kD-1} \\max_{m=0, \\ldots, kH-1} \\max_{n=0, \\ldots, kW-1} \\\\                                       &amp; \\text{input}(N_i, C_j, \\text{stride[0]} \\times d + k,                                                      \\text{stride[1]} \\times h + m, \\text{stride[2]} \\times w + n) \\end{aligned}  out(Ni\u200b,Cj\u200b,d,h,w)=\u200bk=0,\u2026,kD\u22121max\u200bm=0,\u2026,kH\u22121max\u200bn=0,\u2026,kW\u22121max\u200binput(Ni\u200b,Cj\u200b,stride[0]\u00d7d+k,stride[1]\u00d7h+m,stride[2]\u00d7w+n)\u200b  If padding is non-zero, then the input is implicitly zero-padded on both sides for padding number of points. dilation controls the spacing between the kernel points. It is harder to describe, but this link has a nice visualization of what dilation does. The parameters kernel_size, stride, padding, dilation can either be:   a single int \u2013 in which case the same value is used for the depth, height and width dimension a tuple of three ints \u2013 in which case, the first int is used for the depth dimension, the second int for the height dimension and the third int for the width dimension    Parameters  kernel_size \u2013 the size of the window to take a max over stride \u2013 the stride of the window. Default value is kernel_size padding \u2013 implicit zero padding to be added on all three sides dilation \u2013 a parameter that controls the stride of elements in the window return_indices \u2013 if True, will return the max indices along with the outputs. Useful for torch.nn.MaxUnpool3d later ceil_mode \u2013 when True, will use ceil instead of floor to compute the output shape     Shape: Input: (N,C,Din,Hin,Win)(N, C, D_{in}, H_{in}, W_{in})(N,C,Din\u200b,Hin\u200b,Win\u200b)   Output: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\u200b,Hout\u200b,Wout\u200b)  , where  Dout=\u230aDin+2\u00d7padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121stride[0]+1\u230bD_{out} = \\left\\lfloor\\frac{D_{in} + 2 \\times \\text{padding}[0] - \\text{dilation}[0] \\times   (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor  Dout\u200b=\u230astride[0]Din\u200b+2\u00d7padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121\u200b+1\u230b   Hout=\u230aHin+2\u00d7padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121stride[1]+1\u230bH_{out} = \\left\\lfloor\\frac{H_{in} + 2 \\times \\text{padding}[1] - \\text{dilation}[1] \\times   (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor  Hout\u200b=\u230astride[1]Hin\u200b+2\u00d7padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121\u200b+1\u230b   Wout=\u230aWin+2\u00d7padding[2]\u2212dilation[2]\u00d7(kernel_size[2]\u22121)\u22121stride[2]+1\u230bW_{out} = \\left\\lfloor\\frac{W_{in} + 2 \\times \\text{padding}[2] - \\text{dilation}[2] \\times   (\\text{kernel\\_size}[2] - 1) - 1}{\\text{stride}[2]} + 1\\right\\rfloor  Wout\u200b=\u230astride[2]Win\u200b+2\u00d7padding[2]\u2212dilation[2]\u00d7(kernel_size[2]\u22121)\u22121\u200b+1\u230b      Examples: &gt;&gt;&gt; # pool of square window of size=3, stride=2 &gt;&gt;&gt; m = nn.MaxPool3d(3, stride=2) &gt;&gt;&gt; # pool of non-square window &gt;&gt;&gt; m = nn.MaxPool3d((3, 2, 2), stride=(2, 1, 2)) &gt;&gt;&gt; input = torch.randn(20, 16, 50,44, 31) &gt;&gt;&gt; output = m(input)   ", "parameters": ["kernel_size : the size of the window to take a max over", "stride : the stride of the window. Default value is kernel_size", "padding : implicit zero padding to be added on all three sides", "dilation : a parameter that controls the stride of elements in the window", "return_indices : if True, will return the max indices along with the outputs.Useful for torch.nn.MaxUnpool3d later", "ceil_mode : when True, will use ceil instead of floor to compute the output shape"], "returns": null, "example": " # pool of square window of size=3, stride=2\n m = nn.MaxPool3d(3, stride=2)\n # pool of non-square window\n m = nn.MaxPool3d((3, 2, 2), stride=(2, 1, 2))\n input = torch.randn(20, 16, 50,44, 31)\n output = m(input)\n\n", "shape": " Input: (N,C,Din,Hin,Win)(N, C, D_{in}, H_{in}, W_{in})(N,C,Din\u200b,Hin\u200b,Win\u200b)   Output: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\u200b,Hout\u200b,Wout\u200b)  , where  Dout=\u230aDin+2\u00d7padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121stride[0]+1\u230bD_{out} = \\left\\lfloor\\frac{D_{in} + 2 \\times \\text{padding}[0] - \\text{dilation}[0] \\times   (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor  Dout\u200b=\u230astride[0]Din\u200b+2\u00d7padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121\u200b+1\u230b   Hout=\u230aHin+2\u00d7padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121stride[1]+1\u230bH_{out} = \\left\\lfloor\\frac{H_{in} + 2 \\times \\text{padding}[1] - \\text{dilation}[1] \\times   (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor  Hout\u200b=\u230astride[1]Hin\u200b+2\u00d7padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121\u200b+1\u230b   Wout=\u230aWin+2\u00d7padding[2]\u2212dilation[2]\u00d7(kernel_size[2]\u22121)\u22121stride[2]+1\u230bW_{out} = \\left\\lfloor\\frac{W_{in} + 2 \\times \\text{padding}[2] - \\text{dilation}[2] \\times   (\\text{kernel\\_size}[2] - 1) - 1}{\\text{stride}[2]} + 1\\right\\rfloor  Wout\u200b=\u230astride[2]Win\u200b+2\u00d7padding[2]\u2212dilation[2]\u00d7(kernel_size[2]\u22121)\u22121\u200b+1\u230b    "},
{"library": "torch", "item_id": "torch.Tensor.unsqueeze_", "item_type": "method", "code": "unsqueeze_(dim)\u2192Tensor", "description": "In-place version of unsqueeze() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.values", "item_type": "method", "code": "values()\u2192Tensor", "description": "If self is a sparse COO tensor (i.e., with torch.sparse_coo layout), this returns a view of the contained values tensor. Otherwise, this throws an error. See also Tensor.indices().  Note This method can only be called on a coalesced sparse tensor. See Tensor.coalesce() for details.  ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.var", "item_type": "method", "code": "var(dim=None,unbiased=True,keepdim=False)\u2192Tensor", "description": "See torch.var() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.view", "item_type": "method", "code": "view(*shape)\u2192Tensor", "description": "Returns a new tensor with the same data as the self tensor but of a different shape. The returned tensor shares the same data and must have the same number of elements, but may have a different size. For a tensor to be viewed, the new view size must be compatible with its original size and stride, i.e., each new view dimension must either be a subspace of an original dimension, or only span across original dimensions d,d+1,\u2026,d+kd, d+1, \\dots, d+kd,d+1,\u2026,d+k   that satisfy the following contiguity-like condition that \u2200i=0,\u2026,k\u22121\\forall i = 0, \\dots, k-1\u2200i=0,\u2026,k\u22121  ,  stride[i]=stride[i+1]\u00d7size[i+1]\\text{stride}[i] = \\text{stride}[i+1] \\times \\text{size}[i+1]stride[i]=stride[i+1]\u00d7size[i+1]  Otherwise, contiguous() needs to be called before the tensor can be viewed. See also: reshape(), which returns a view if the shapes are compatible, and copies (equivalent to calling contiguous()) otherwise.  Parameters shape (torch.Size or python:int...) \u2013 the desired size   Example: &gt;&gt;&gt; x = torch.randn(4, 4) &gt;&gt;&gt; x.size() torch.Size([4, 4]) &gt;&gt;&gt; y = x.view(16) &gt;&gt;&gt; y.size() torch.Size([16]) &gt;&gt;&gt; z = x.view(-1, 8)  # the size -1 is inferred from other dimensions &gt;&gt;&gt; z.size() torch.Size([2, 8])  &gt;&gt;&gt; a = torch.randn(1, 2, 3, 4) &gt;&gt;&gt; a.size() torch.Size([1, 2, 3, 4]) &gt;&gt;&gt; b = a.transpose(1, 2)  # Swaps 2nd and 3rd dimension &gt;&gt;&gt; b.size() torch.Size([1, 3, 2, 4]) &gt;&gt;&gt; c = a.view(1, 3, 2, 4)  # Does not change tensor layout in memory &gt;&gt;&gt; c.size() torch.Size([1, 3, 2, 4]) &gt;&gt;&gt; torch.equal(b, c) False   ", "parameters": ["shape (torch.Size or python:int...) : the desired size"], "returns": null, "example": " x = torch.randn(4, 4)\n x.size()\ntorch.Size([4, 4])\n y = x.view(16)\n y.size()\ntorch.Size([16])\n z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n z.size()\ntorch.Size([2, 8])\n\n a = torch.randn(1, 2, 3, 4)\n a.size()\ntorch.Size([1, 2, 3, 4])\n b = a.transpose(1, 2)  # Swaps 2nd and 3rd dimension\n b.size()\ntorch.Size([1, 3, 2, 4])\n c = a.view(1, 3, 2, 4)  # Does not change tensor layout in memory\n c.size()\ntorch.Size([1, 3, 2, 4])\n torch.equal(b, c)\nFalse\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.view_as", "item_type": "method", "code": "view_as(other)\u2192Tensor", "description": "View this tensor as the same size as other. self.view_as(other) is equivalent to self.view(other.size()). Please see view() for more information about view.  Parameters other (torch.Tensor) \u2013 The result tensor has the same size as other.   ", "parameters": ["other (torch.Tensor) : The result tensor has the same sizeas other."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.mode", "item_type": "function", "code": "torch.mode(input,dim=-1,keepdim=False,values=None,indices=None)-&gt;(Tensor,LongTensor)", "description": "Returns a namedtuple (values, indices) where values is the mode value of each row of the input tensor in the given dimension dim, i.e. a value which appears most often in that row, and indices is the index location of each mode value found. By default, dim is the last dimension of the input tensor. If keepdim is True, the output tensors are of the same size as input except in the dimension dim where they are of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensors having 1 fewer dimension than input.  Note This function is not defined for torch.cuda.Tensor yet.   Parameters  input (Tensor) \u2013 the input tensor. dim (python:int) \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not. values (Tensor, optional) \u2013 the output tensor indices (Tensor, optional) \u2013 the output index tensor    Example: &gt;&gt;&gt; a = torch.randint(10, (5,)) &gt;&gt;&gt; a tensor([6, 5, 1, 0, 2]) &gt;&gt;&gt; b = a + (torch.randn(50, 1) * 5).long() &gt;&gt;&gt; torch.mode(b, 0) torch.return_types.mode(values=tensor([6, 5, 1, 0, 2]), indices=tensor([2, 2, 2, 2, 2]))   ", "parameters": ["input (Tensor) : the input tensor.", "dim (python:int) : the dimension to reduce.", "keepdim (bool) : whether the output tensor has dim retained or not.", "values (Tensor, optional) : the output tensor", "indices (Tensor, optional) : the output index tensor"], "returns": null, "example": " a = torch.randint(10, (5,))\n a\ntensor([6, 5, 1, 0, 2])\n b = a + (torch.randn(50, 1) * 5).long()\n torch.mode(b, 0)\ntorch.return_types.mode(values=tensor([6, 5, 1, 0, 2]), indices=tensor([2, 2, 2, 2, 2]))\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.norm", "item_type": "function", "code": "torch.norm(input,p='fro',dim=None,keepdim=False,out=None,dtype=None)", "description": "Returns the matrix norm or vector norm of a given tensor.  Parameters  input (Tensor) \u2013 the input tensor p (python:int, python:float, inf, -inf, 'fro', 'nuc', optional) \u2013 the order of norm. Default: 'fro' The following norms can be calculated:        ord matrix norm vector norm    None Frobenius norm 2-norm  \u2019fro\u2019 Frobenius norm \u2013  \u2018nuc\u2019 nuclear norm \u2013  Other as vec norm when dim is None sum(abs(x)**ord)**(1./ord)     dim (python:int, 2-tuple of python:ints, 2-list of python:ints, optional) \u2013 If it is an int, vector norm will be calculated, if it is 2-tuple of ints, matrix norm will be calculated. If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension. If the input tensor has more than two dimensions, the vector norm will be applied to last dimension. keepdim (bool, optional) \u2013 whether the output tensors have dim retained or not. Ignored if dim = None and out = None. Default: False out (Tensor, optional) \u2013 the output tensor. Ignored if dim = None and out = None. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to :attr:\u2019dtype\u2019 while performing the operation. Default: None.    Example: &gt;&gt;&gt; import torch &gt;&gt;&gt; a = torch.arange(9, dtype= torch.float) - 4 &gt;&gt;&gt; b = a.reshape((3, 3)) &gt;&gt;&gt; torch.norm(a) tensor(7.7460) &gt;&gt;&gt; torch.norm(b) tensor(7.7460) &gt;&gt;&gt; torch.norm(a, float('inf')) tensor(4.) &gt;&gt;&gt; torch.norm(b, float('inf')) tensor(4.) &gt;&gt;&gt; c = torch.tensor([[ 1, 2, 3],[-1, 1, 4]] , dtype= torch.float) &gt;&gt;&gt; torch.norm(c, dim=0) tensor([1.4142, 2.2361, 5.0000]) &gt;&gt;&gt; torch.norm(c, dim=1) tensor([3.7417, 4.2426]) &gt;&gt;&gt; torch.norm(c, p=1, dim=1) tensor([6., 6.]) &gt;&gt;&gt; d = torch.arange(8, dtype= torch.float).reshape(2,2,2) &gt;&gt;&gt; torch.norm(d, dim=(1,2)) tensor([ 3.7417, 11.2250]) &gt;&gt;&gt; torch.norm(d[0, :, :]), torch.norm(d[1, :, :]) (tensor(3.7417), tensor(11.2250))   ", "parameters": ["input (Tensor) : the input tensor", "p (python:int, python:float, inf, -inf, 'fro', 'nuc', optional) : the order of norm. Default: 'fro'The following norms can be calculated:ordmatrix normvector normNoneFrobenius norm2-norm\u2019fro\u2019Frobenius norm:\u2018nuc\u2019nuclear norm:Otheras vec norm when dim is Nonesum(abs(x)**ord)**(1./ord)", "dim (python:int, 2-tuple of python:ints, 2-list of python:ints, optional) : If it is an int,vector norm will be calculated, if it is 2-tuple of ints, matrix normwill be calculated. If the value is None, matrix norm will be calculatedwhen the input tensor only has two dimensions, vector norm will becalculated when the input tensor only has one dimension. If the inputtensor has more than two dimensions, the vector norm will be applied tolast dimension.", "keepdim (bool, optional) : whether the output tensors have dimretained or not. Ignored if dim = None andout = None. Default: False", "out (Tensor, optional) : the output tensor. Ignored ifdim = None and out = None.", "dtype (torch.dtype, optional) : the desired data type ofreturned tensor. If specified, the input tensor is casted to:attr:\u2019dtype\u2019 while performing the operation. Default: None."], "returns": null, "example": " import torch\n a = torch.arange(9, dtype= torch.float) - 4\n b = a.reshape((3, 3))\n torch.norm(a)\ntensor(7.7460)\n torch.norm(b)\ntensor(7.7460)\n torch.norm(a, float('inf'))\ntensor(4.)\n torch.norm(b, float('inf'))\ntensor(4.)\n c = torch.tensor([[ 1, 2, 3],[-1, 1, 4]] , dtype= torch.float)\n torch.norm(c, dim=0)\ntensor([1.4142, 2.2361, 5.0000])\n torch.norm(c, dim=1)\ntensor([3.7417, 4.2426])\n torch.norm(c, p=1, dim=1)\ntensor([6., 6.])\n d = torch.arange(8, dtype= torch.float).reshape(2,2,2)\n torch.norm(d, dim=(1,2))\ntensor([ 3.7417, 11.2250])\n torch.norm(d[0, :, :]), torch.norm(d[1, :, :])\n(tensor(3.7417), tensor(11.2250))\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.prod", "item_type": "function", "code": "torch.prod()", "description": "  torch.prod(input, dtype=None) \u2192 Tensor   Returns the product of all elements in the input tensor.  Parameters  input (Tensor) \u2013 the input tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None.    Example: &gt;&gt;&gt; a = torch.randn(1, 3) &gt;&gt;&gt; a tensor([[-0.8020,  0.5428, -1.5854]]) &gt;&gt;&gt; torch.prod(a) tensor(0.6902)     torch.prod(input, dim, keepdim=False, dtype=None) \u2192 Tensor   Returns the product of each row of the input tensor in the given dimension dim. If keepdim is True, the output tensor is of the same size as input except in the dimension dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 fewer dimension than input.  Parameters  input (Tensor) \u2013 the input tensor. dim (python:int) \u2013 the dimension to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None.    Example: &gt;&gt;&gt; a = torch.randn(4, 2) &gt;&gt;&gt; a tensor([[ 0.5261, -0.3837],         [ 1.1857, -0.2498],         [-1.1646,  0.0705],         [ 1.1131, -1.0629]]) &gt;&gt;&gt; torch.prod(a, 1) tensor([-0.2018, -0.2962, -0.0821, -1.1831])   ", "parameters": ["input (Tensor) : the input tensor.", "dtype (torch.dtype, optional) : the desired data type of returned tensor.If specified, the input tensor is casted to dtype before the operationis performed. This is useful for preventing data type overflows. Default: None.", "input (Tensor) : the input tensor.", "dim (python:int) : the dimension to reduce.", "keepdim (bool) : whether the output tensor has dim retained or not.", "dtype (torch.dtype, optional) : the desired data type of returned tensor.If specified, the input tensor is casted to dtype before the operationis performed. This is useful for preventing data type overflows. Default: None."], "returns": null, "example": " a = torch.randn(1, 3)\n a\ntensor([[-0.8020,  0.5428, -1.5854]])\n torch.prod(a)\ntensor(0.6902)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.prod(input,dtype=None)\u2192Tensor", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.arg_constraints", "item_type": "attribute", "code": "arg_constraints={'logits':Real(),'probs':Interval(lower_bound=0.0,upper_bound=1.0)}", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.logits", "item_type": "attribute", "code": "logits", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.probs", "item_type": "attribute", "code": "probs", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.support", "item_type": "attribute", "code": "support=Real()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical", "item_type": "class", "code": "classtorch.distributions.relaxed_categorical.RelaxedOneHotCategorical(temperature,probs=None,logits=None,validate_args=None)", "description": "Bases: torch.distributions.transformed_distribution.TransformedDistribution Creates a RelaxedOneHotCategorical distribution parametrized by temperature, and either probs or logits. This is a relaxed version of the OneHotCategorical distribution, so its samples are on simplex, and are reparametrizable. Example: &gt;&gt;&gt; m = RelaxedOneHotCategorical(torch.tensor([2.2]),                                  torch.tensor([0.1, 0.2, 0.3, 0.4])) &gt;&gt;&gt; m.sample() tensor([ 0.1294,  0.2324,  0.3859,  0.2523])    Parameters  temperature (Tensor) \u2013 relaxation temperature probs (Tensor) \u2013 event probabilities logits (Tensor) \u2013 the log probability of each event.      arg_constraints = {'logits': Real(), 'probs': Simplex()}     expand(batch_shape, _instance=None)      has_rsample = True     property logits     property probs     support = Simplex()     property temperature   ", "parameters": ["temperature (Tensor) : relaxation temperature", "probs (Tensor) : event probabilities", "logits (Tensor) : the log probability of each event."], "returns": null, "example": " m = RelaxedOneHotCategorical(torch.tensor([2.2]),\n                                 torch.tensor([0.1, 0.2, 0.3, 0.4]))\n m.sample()\ntensor([ 0.1294,  0.2324,  0.3859,  0.2523])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.arg_constraints", "item_type": "attribute", "code": "arg_constraints={'logits':Real(),'probs':Simplex()}", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.MaxUnpool1d", "item_type": "class", "code": "classtorch.nn.MaxUnpool1d(kernel_size,stride=None,padding=0)", "description": "Computes a partial inverse of MaxPool1d. MaxPool1d is not fully invertible, since the non-maximal values are lost. MaxUnpool1d takes in as input the output of MaxPool1d including the indices of the maximal values and computes a partial inverse in which all non-maximal values are set to zero.  Note MaxPool1d can map several input sizes to the same output sizes. Hence, the inversion process can get ambiguous. To accommodate this, you can provide the needed output size as an additional argument output_size in the forward call. See the Inputs and Example below.   Parameters  kernel_size (python:int or tuple) \u2013 Size of the max pooling window. stride (python:int or tuple) \u2013 Stride of the max pooling window. It is set to kernel_size by default. padding (python:int or tuple) \u2013 Padding that was added to the input     Inputs: input: the input Tensor to invert indices: the indices given out by MaxPool1d output_size (optional): the targeted output size   Shape: Input: (N,C,Hin)(N, C, H_{in})(N,C,Hin\u200b)   Output: (N,C,Hout)(N, C, H_{out})(N,C,Hout\u200b)  , where  Hout=(Hin\u22121)\u00d7stride[0]\u22122\u00d7padding[0]+kernel_size[0]H_{out} = (H_{in} - 1) \\times \\text{stride}[0] - 2 \\times \\text{padding}[0] + \\text{kernel\\_size}[0]  Hout\u200b=(Hin\u200b\u22121)\u00d7stride[0]\u22122\u00d7padding[0]+kernel_size[0]  or as given by output_size in the call operator     Example: &gt;&gt;&gt; pool = nn.MaxPool1d(2, stride=2, return_indices=True) &gt;&gt;&gt; unpool = nn.MaxUnpool1d(2, stride=2) &gt;&gt;&gt; input = torch.tensor([[[1., 2, 3, 4, 5, 6, 7, 8]]]) &gt;&gt;&gt; output, indices = pool(input) &gt;&gt;&gt; unpool(output, indices) tensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.]]])  &gt;&gt;&gt; # Example showcasing the use of output_size &gt;&gt;&gt; input = torch.tensor([[[1., 2, 3, 4, 5, 6, 7, 8, 9]]]) &gt;&gt;&gt; output, indices = pool(input) &gt;&gt;&gt; unpool(output, indices, output_size=input.size()) tensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.,  0.]]])  &gt;&gt;&gt; unpool(output, indices) tensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.]]])   ", "parameters": ["kernel_size (python:int or tuple) : Size of the max pooling window.", "stride (python:int or tuple) : Stride of the max pooling window.It is set to kernel_size by default.", "padding (python:int or tuple) : Padding that was added to the input"], "returns": null, "example": " pool = nn.MaxPool1d(2, stride=2, return_indices=True)\n unpool = nn.MaxUnpool1d(2, stride=2)\n input = torch.tensor([[[1., 2, 3, 4, 5, 6, 7, 8]]])\n output, indices = pool(input)\n unpool(output, indices)\ntensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.]]])\n\n # Example showcasing the use of output_size\n input = torch.tensor([[[1., 2, 3, 4, 5, 6, 7, 8, 9]]])\n output, indices = pool(input)\n unpool(output, indices, output_size=input.size())\ntensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.,  0.]]])\n\n unpool(output, indices)\ntensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.]]])\n\n", "shape": " Input: (N,C,Hin)(N, C, H_{in})(N,C,Hin\u200b)   Output: (N,C,Hout)(N, C, H_{out})(N,C,Hout\u200b)  , where  Hout=(Hin\u22121)\u00d7stride[0]\u22122\u00d7padding[0]+kernel_size[0]H_{out} = (H_{in} - 1) \\times \\text{stride}[0] - 2 \\times \\text{padding}[0] + \\text{kernel\\_size}[0]  Hout\u200b=(Hin\u200b\u22121)\u00d7stride[0]\u22122\u00d7padding[0]+kernel_size[0]  or as given by output_size in the call operator   "},
{"library": "torch", "item_id": "torch.nn.MaxUnpool2d", "item_type": "class", "code": "classtorch.nn.MaxUnpool2d(kernel_size,stride=None,padding=0)", "description": "Computes a partial inverse of MaxPool2d. MaxPool2d is not fully invertible, since the non-maximal values are lost. MaxUnpool2d takes in as input the output of MaxPool2d including the indices of the maximal values and computes a partial inverse in which all non-maximal values are set to zero.  Note MaxPool2d can map several input sizes to the same output sizes. Hence, the inversion process can get ambiguous. To accommodate this, you can provide the needed output size as an additional argument output_size in the forward call. See the Inputs and Example below.   Parameters  kernel_size (python:int or tuple) \u2013 Size of the max pooling window. stride (python:int or tuple) \u2013 Stride of the max pooling window. It is set to kernel_size by default. padding (python:int or tuple) \u2013 Padding that was added to the input     Inputs: input: the input Tensor to invert indices: the indices given out by MaxPool2d output_size (optional): the targeted output size   Shape: Input: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)   Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)  , where  Hout=(Hin\u22121)\u00d7stride[0]\u22122\u00d7padding[0]+kernel_size[0]H_{out} = (H_{in} - 1) \\times \\text{stride[0]} - 2 \\times \\text{padding[0]} + \\text{kernel\\_size[0]}  Hout\u200b=(Hin\u200b\u22121)\u00d7stride[0]\u22122\u00d7padding[0]+kernel_size[0]   Wout=(Win\u22121)\u00d7stride[1]\u22122\u00d7padding[1]+kernel_size[1]W_{out} = (W_{in} - 1) \\times \\text{stride[1]} - 2 \\times \\text{padding[1]} + \\text{kernel\\_size[1]}  Wout\u200b=(Win\u200b\u22121)\u00d7stride[1]\u22122\u00d7padding[1]+kernel_size[1]  or as given by output_size in the call operator     Example: &gt;&gt;&gt; pool = nn.MaxPool2d(2, stride=2, return_indices=True) &gt;&gt;&gt; unpool = nn.MaxUnpool2d(2, stride=2) &gt;&gt;&gt; input = torch.tensor([[[[ 1.,  2,  3,  4],                             [ 5,  6,  7,  8],                             [ 9, 10, 11, 12],                             [13, 14, 15, 16]]]]) &gt;&gt;&gt; output, indices = pool(input) &gt;&gt;&gt; unpool(output, indices) tensor([[[[  0.,   0.,   0.,   0.],           [  0.,   6.,   0.,   8.],           [  0.,   0.,   0.,   0.],           [  0.,  14.,   0.,  16.]]]])  &gt;&gt;&gt; # specify a different output size than input size &gt;&gt;&gt; unpool(output, indices, output_size=torch.Size([1, 1, 5, 5])) tensor([[[[  0.,   0.,   0.,   0.,   0.],           [  6.,   0.,   8.,   0.,   0.],           [  0.,   0.,   0.,  14.,   0.],           [ 16.,   0.,   0.,   0.,   0.],           [  0.,   0.,   0.,   0.,   0.]]]])   ", "parameters": ["kernel_size (python:int or tuple) : Size of the max pooling window.", "stride (python:int or tuple) : Stride of the max pooling window.It is set to kernel_size by default.", "padding (python:int or tuple) : Padding that was added to the input"], "returns": null, "example": " pool = nn.MaxPool2d(2, stride=2, return_indices=True)\n unpool = nn.MaxUnpool2d(2, stride=2)\n input = torch.tensor([[[[ 1.,  2,  3,  4],\n                            [ 5,  6,  7,  8],\n                            [ 9, 10, 11, 12],\n                            [13, 14, 15, 16]]]])\n output, indices = pool(input)\n unpool(output, indices)\ntensor([[[[  0.,   0.,   0.,   0.],\n          [  0.,   6.,   0.,   8.],\n          [  0.,   0.,   0.,   0.],\n          [  0.,  14.,   0.,  16.]]]])\n\n # specify a different output size than input size\n unpool(output, indices, output_size=torch.Size([1, 1, 5, 5]))\ntensor([[[[  0.,   0.,   0.,   0.,   0.],\n          [  6.,   0.,   8.,   0.,   0.],\n          [  0.,   0.,   0.,  14.,   0.],\n          [ 16.,   0.,   0.,   0.,   0.],\n          [  0.,   0.,   0.,   0.,   0.]]]])\n\n", "shape": " Input: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)   Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)  , where  Hout=(Hin\u22121)\u00d7stride[0]\u22122\u00d7padding[0]+kernel_size[0]H_{out} = (H_{in} - 1) \\times \\text{stride[0]} - 2 \\times \\text{padding[0]} + \\text{kernel\\_size[0]}  Hout\u200b=(Hin\u200b\u22121)\u00d7stride[0]\u22122\u00d7padding[0]+kernel_size[0]   Wout=(Win\u22121)\u00d7stride[1]\u22122\u00d7padding[1]+kernel_size[1]W_{out} = (W_{in} - 1) \\times \\text{stride[1]} - 2 \\times \\text{padding[1]} + \\text{kernel\\_size[1]}  Wout\u200b=(Win\u200b\u22121)\u00d7stride[1]\u22122\u00d7padding[1]+kernel_size[1]  or as given by output_size in the call operator   "},
{"library": "torch", "item_id": "torch.Tensor.where", "item_type": "method", "code": "where(condition,y)\u2192Tensor", "description": "self.where(condition, y) is equivalent to torch.where(condition, self, y). See torch.where() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.zero_", "item_type": "method", "code": "zero_()\u2192Tensor", "description": "Fills self tensor with zeros. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.BoolTensor.all", "item_type": "method", "code": "all()", "description": "  all() \u2192 bool   Returns True if all elements in the tensor are True, False otherwise. Example: &gt;&gt;&gt; a = torch.rand(1, 2).bool() &gt;&gt;&gt; a tensor([[False, True]], dtype=torch.bool) &gt;&gt;&gt; a.all() tensor(False, dtype=torch.bool)     all(dim, keepdim=False, out=None) \u2192 Tensor   Returns True if all elements in each row of the tensor in the given dimension dim are True, False otherwise. If keepdim is True, the output tensor is of the same size as input except in the dimension dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 fewer dimension than input.  Parameters  dim (python:int) \u2013 the dimension to reduce keepdim (bool) \u2013 whether the output tensor has dim retained or not out (Tensor, optional) \u2013 the output tensor    Example: &gt;&gt;&gt; a = torch.rand(4, 2).bool() &gt;&gt;&gt; a tensor([[True, True],         [True, False],         [True, True],         [True, True]], dtype=torch.bool) &gt;&gt;&gt; a.all(dim=1) tensor([ True, False,  True,  True], dtype=torch.bool) &gt;&gt;&gt; a.all(dim=0) tensor([ True, False], dtype=torch.bool)   ", "parameters": ["dim (python:int) : the dimension to reduce", "keepdim (bool) : whether the output tensor has dim retained or not", "out (Tensor, optional) : the output tensor"], "returns": null, "example": " a = torch.rand(1, 2).bool()\n a\ntensor([[False, True]], dtype=torch.bool)\n a.all()\ntensor(False, dtype=torch.bool)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.BoolTensor.any", "item_type": "method", "code": "any()", "description": "  any() \u2192 bool   Returns True if any elements in the tensor are True, False otherwise. Example: &gt;&gt;&gt; a = torch.rand(1, 2).bool() &gt;&gt;&gt; a tensor([[False, True]], dtype=torch.bool) &gt;&gt;&gt; a.any() tensor(True, dtype=torch.bool)     any(dim, keepdim=False, out=None) \u2192 Tensor   Returns True if any elements in each row of the tensor in the given dimension dim are True, False otherwise. If keepdim is True, the output tensor is of the same size as input except in the dimension dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 fewer dimension than input.  Parameters  dim (python:int) \u2013 the dimension to reduce keepdim (bool) \u2013 whether the output tensor has dim retained or not out (Tensor, optional) \u2013 the output tensor    Example: &gt;&gt;&gt; a = torch.randn(4, 2) &lt; 0 &gt;&gt;&gt; a tensor([[ True,  True],         [False,  True],         [ True,  True],         [False, False]]) &gt;&gt;&gt; a.any(1) tensor([ True,  True,  True, False]) &gt;&gt;&gt; a.any(0) tensor([True, True])   ", "parameters": ["dim (python:int) : the dimension to reduce", "keepdim (bool) : whether the output tensor has dim retained or not", "out (Tensor, optional) : the output tensor"], "returns": null, "example": " a = torch.rand(1, 2).bool()\n a\ntensor([[False, True]], dtype=torch.bool)\n a.any()\ntensor(True, dtype=torch.bool)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor", "item_type": "class", "code": "classtorch.Tensor", "description": "There are a few main ways to create a tensor, depending on your use case.  To create a tensor with pre-existing data, use torch.tensor(). To create a tensor with specific size, use torch.* tensor creation ops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor, use torch.*_like tensor creation ops (see Creation Ops). To create a tensor with similar type but different size as another tensor, use tensor.new_* creation ops.    new_tensor(data, dtype=None, device=None, requires_grad=False) \u2192 Tensor Returns a new Tensor with data as the tensor data. By default, the returned Tensor has the same torch.dtype and torch.device as this tensor.  Warning new_tensor() always copies data. If you have a Tensor data and want to avoid a copy, use torch.Tensor.requires_grad_() or torch.Tensor.detach(). If you have a numpy array and want to avoid a copy, use torch.from_numpy().   Warning When data is a tensor x, new_tensor() reads out \u2018the data\u2019 from whatever it is passed, and constructs a leaf variable. Therefore tensor.new_tensor(x) is equivalent to x.clone().detach() and tensor.new_tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True). The equivalents using clone() and detach() are recommended.   Parameters  data (array_like) \u2013 The returned Tensor copies data. dtype (torch.dtype, optional) \u2013 the desired type of returned tensor. Default: if None, same torch.dtype as this tensor. device (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, same torch.device as this tensor. requires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: &gt;&gt;&gt; tensor = torch.ones((2,), dtype=torch.int8) &gt;&gt;&gt; data = [[0, 1], [2, 3]] &gt;&gt;&gt; tensor.new_tensor(data) tensor([[ 0,  1],         [ 2,  3]], dtype=torch.int8)       new_full(size, fill_value, dtype=None, device=None, requires_grad=False) \u2192 Tensor Returns a Tensor of size size filled with fill_value. By default, the returned Tensor has the same torch.dtype and torch.device as this tensor.  Parameters  fill_value (scalar) \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired type of returned tensor. Default: if None, same torch.dtype as this tensor. device (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, same torch.device as this tensor. requires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: &gt;&gt;&gt; tensor = torch.ones((2,), dtype=torch.float64) &gt;&gt;&gt; tensor.new_full((3, 4), 3.141592) tensor([[ 3.1416,  3.1416,  3.1416,  3.1416],         [ 3.1416,  3.1416,  3.1416,  3.1416],         [ 3.1416,  3.1416,  3.1416,  3.1416]], dtype=torch.float64)       new_empty(size, dtype=None, device=None, requires_grad=False) \u2192 Tensor Returns a Tensor of size size filled with uninitialized data. By default, the returned Tensor has the same torch.dtype and torch.device as this tensor.  Parameters  dtype (torch.dtype, optional) \u2013 the desired type of returned tensor. Default: if None, same torch.dtype as this tensor. device (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, same torch.device as this tensor. requires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: &gt;&gt;&gt; tensor = torch.ones(()) &gt;&gt;&gt; tensor.new_empty((2, 3)) tensor([[ 5.8182e-18,  4.5765e-41, -1.0545e+30],         [ 3.0949e-41,  4.4842e-44,  0.0000e+00]])       new_ones(size, dtype=None, device=None, requires_grad=False) \u2192 Tensor Returns a Tensor of size size filled with 1. By default, the returned Tensor has the same torch.dtype and torch.device as this tensor.  Parameters  size (python:int...) \u2013 a list, tuple, or torch.Size of integers defining the shape of the output tensor. dtype (torch.dtype, optional) \u2013 the desired type of returned tensor. Default: if None, same torch.dtype as this tensor. device (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, same torch.device as this tensor. requires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: &gt;&gt;&gt; tensor = torch.tensor((), dtype=torch.int32) &gt;&gt;&gt; tensor.new_ones((2, 3)) tensor([[ 1,  1,  1],         [ 1,  1,  1]], dtype=torch.int32)       new_zeros(size, dtype=None, device=None, requires_grad=False) \u2192 Tensor Returns a Tensor of size size filled with 0. By default, the returned Tensor has the same torch.dtype and torch.device as this tensor.  Parameters  size (python:int...) \u2013 a list, tuple, or torch.Size of integers defining the shape of the output tensor. dtype (torch.dtype, optional) \u2013 the desired type of returned tensor. Default: if None, same torch.dtype as this tensor. device (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, same torch.device as this tensor. requires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.    Example: &gt;&gt;&gt; tensor = torch.tensor((), dtype=torch.float64) &gt;&gt;&gt; tensor.new_zeros((2, 3)) tensor([[ 0.,  0.,  0.],         [ 0.,  0.,  0.]], dtype=torch.float64)       is_cuda Is True if the Tensor is stored on the GPU, False otherwise.     device Is the torch.device where this Tensor is.     grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. The attribute will then contain the gradients computed and future calls to backward() will accumulate (add) gradients into it.     ndim Alias for dim()     T Is this Tensor with its dimensions reversed. If n is the number of dimensions in x, x.T is equivalent to x.permute(n-1, n-2, ..., 0).     abs() \u2192 Tensor See torch.abs()     abs_() \u2192 Tensor In-place version of abs()     acos() \u2192 Tensor See torch.acos()     acos_() \u2192 Tensor In-place version of acos()     add(value) \u2192 Tensor add(value=1, other) -&gt; Tensor See torch.add()     add_(value) \u2192 Tensor add_(value=1, other) -&gt; Tensor In-place version of add()     addbmm(beta=1, alpha=1, batch1, batch2) \u2192 Tensor See torch.addbmm()     addbmm_(beta=1, alpha=1, batch1, batch2) \u2192 Tensor In-place version of addbmm()     addcdiv(value=1, tensor1, tensor2) \u2192 Tensor See torch.addcdiv()     addcdiv_(value=1, tensor1, tensor2) \u2192 Tensor In-place version of addcdiv()     addcmul(value=1, tensor1, tensor2) \u2192 Tensor See torch.addcmul()     addcmul_(value=1, tensor1, tensor2) \u2192 Tensor In-place version of addcmul()     addmm(beta=1, alpha=1, mat1, mat2) \u2192 Tensor See torch.addmm()     addmm_(beta=1, alpha=1, mat1, mat2) \u2192 Tensor In-place version of addmm()     addmv(beta=1, alpha=1, mat, vec) \u2192 Tensor See torch.addmv()     addmv_(beta=1, alpha=1, mat, vec) \u2192 Tensor In-place version of addmv()     addr(beta=1, alpha=1, vec1, vec2) \u2192 Tensor See torch.addr()     addr_(beta=1, alpha=1, vec1, vec2) \u2192 Tensor In-place version of addr()     allclose(other, rtol=1e-05, atol=1e-08, equal_nan=False) \u2192 Tensor See torch.allclose()     angle() \u2192 Tensor See torch.angle()     apply_(callable) \u2192 Tensor Applies the function callable to each element in the tensor, replacing each element with the value returned by callable.  Note This function only works with CPU tensors and should not be used in code sections that require high performance.      argmax(dim=None, keepdim=False) \u2192 LongTensor See torch.argmax()     argmin(dim=None, keepdim=False) \u2192 LongTensor See torch.argmin()     argsort(dim=-1, descending=False) \u2192 LongTensor See :func: torch.argsort     asin() \u2192 Tensor See torch.asin()     asin_() \u2192 Tensor In-place version of asin()     as_strided(size, stride, storage_offset=0) \u2192 Tensor See torch.as_strided()     atan() \u2192 Tensor See torch.atan()     atan2(other) \u2192 Tensor See torch.atan2()     atan2_(other) \u2192 Tensor In-place version of atan2()     atan_() \u2192 Tensor In-place version of atan()     backward(gradient=None, retain_graph=None, create_graph=False)  Computes the gradient of current tensor w.r.t. graph leaves. The graph is differentiated using the chain rule. If the tensor is non-scalar (i.e. its data has more than one element) and requires gradient, the function additionally requires specifying gradient. It should be a tensor of matching type and location, that contains the gradient of the differentiated function w.r.t. self. This function accumulates gradients in the leaves - you might need to zero them before calling it.  Parameters  gradient (Tensor or None) \u2013 Gradient w.r.t. the tensor. If it is a tensor, it will be automatically converted to a Tensor that does not require grad unless create_graph is True. None values can be specified for scalar Tensors or ones that don\u2019t require grad. If a None value would be acceptable then this argument is optional. retain_graph (bool, optional) \u2013 If False, the graph used to compute the grads will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Defaults to the value of create_graph. create_graph (bool, optional) \u2013 If True, graph of the derivative will be constructed, allowing to compute higher order derivative products. Defaults to False.        baddbmm(beta=1, alpha=1, batch1, batch2) \u2192 Tensor See torch.baddbmm()     baddbmm_(beta=1, alpha=1, batch1, batch2) \u2192 Tensor In-place version of baddbmm()     bernoulli(*, generator=None) \u2192 Tensor Returns a result tensor where each result[i]\\texttt{result[i]}result[i]   is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i])  . self must have floating point dtype, and the result will have the same dtype. See torch.bernoulli()     bernoulli_()   bernoulli_(p=0.5, *, generator=None) \u2192 Tensor Fills each location of self with an independent sample from Bernoulli(p)\\text{Bernoulli}(\\texttt{p})Bernoulli(p)  . self can have integral dtype.     bernoulli_(p_tensor, *, generator=None) \u2192 Tensor p_tensor should be a tensor containing probabilities to be used for drawing the binary random number. The ith\\text{i}^{th}ith   element of self tensor will be set to a value sampled from Bernoulli(p_tensor[i])\\text{Bernoulli}(\\texttt{p\\_tensor[i]})Bernoulli(p_tensor[i])  . self can have integral dtype, but p_tensor must have floating point dtype.   See also bernoulli() and torch.bernoulli()     bfloat16() \u2192 Tensor self.bfloat16() is equivalent to self.to(torch.bfloat16). See to().     bincount(weights=None, minlength=0) \u2192 Tensor See torch.bincount()     bitwise_not() \u2192 Tensor See torch.bitwise_not()     bitwise_not_() \u2192 Tensor In-place version of bitwise_not()     bitwise_xor() \u2192 Tensor See torch.bitwise_xor()     bitwise_xor_() \u2192 Tensor In-place version of bitwise_xor()     bmm(batch2) \u2192 Tensor See torch.bmm()     bool() \u2192 Tensor self.bool() is equivalent to self.to(torch.bool). See to().     byte() \u2192 Tensor self.byte() is equivalent to self.to(torch.uint8). See to().     cauchy_(median=0, sigma=1, *, generator=None) \u2192 Tensor Fills the tensor with numbers drawn from the Cauchy distribution:  f(x)=1\u03c0\u03c3(x\u2212median)2+\u03c32f(x) = \\dfrac{1}{\\pi} \\dfrac{\\sigma}{(x - \\text{median})^2 + \\sigma^2}f(x)=\u03c01\u200b(x\u2212median)2+\u03c32\u03c3\u200b      ceil() \u2192 Tensor See torch.ceil()     ceil_() \u2192 Tensor In-place version of ceil()     char() \u2192 Tensor self.char() is equivalent to self.to(torch.int8). See to().     cholesky(upper=False) \u2192 Tensor See torch.cholesky()     cholesky_inverse(upper=False) \u2192 Tensor See torch.cholesky_inverse()     cholesky_solve(input2, upper=False) \u2192 Tensor See torch.cholesky_solve()     chunk(chunks, dim=0) \u2192 List of Tensors See torch.chunk()     clamp(min, max) \u2192 Tensor See torch.clamp()     clamp_(min, max) \u2192 Tensor In-place version of clamp()     clone() \u2192 Tensor Returns a copy of the self tensor. The copy has the same size and data type as self.  Note Unlike copy_(), this function is recorded in the computation graph. Gradients propagating to the cloned tensor will propagate to the original tensor.      contiguous() \u2192 Tensor Returns a contiguous tensor containing the same data as self tensor. If self tensor is contiguous, this function returns the self tensor.     copy_(src, non_blocking=False) \u2192 Tensor Copies the elements from src into self tensor and returns self. The src tensor must be broadcastable with the self tensor. It may be of a different data type or reside on a different device.  Parameters  src (Tensor) \u2013 the source tensor to copy from non_blocking (bool) \u2013 if True and this copy is between CPU and GPU, the copy may occur asynchronously with respect to the host. For other cases, this argument has no effect.        conj() \u2192 Tensor See torch.conj()     cos() \u2192 Tensor See torch.cos()     cos_() \u2192 Tensor In-place version of cos()     cosh() \u2192 Tensor See torch.cosh()     cosh_() \u2192 Tensor In-place version of cosh()     cpu() \u2192 Tensor Returns a copy of this object in CPU memory. If this object is already in CPU memory and on the correct device, then no copy is performed and the original object is returned.     cross(other, dim=-1) \u2192 Tensor See torch.cross()     cuda(device=None, non_blocking=False) \u2192 Tensor Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then no copy is performed and the original object is returned.  Parameters  device (torch.device) \u2013 The destination GPU device. Defaults to the current CUDA device. non_blocking (bool) \u2013 If True and the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, the argument has no effect. Default: False.        cumprod(dim, dtype=None) \u2192 Tensor See torch.cumprod()     cumsum(dim, dtype=None) \u2192 Tensor See torch.cumsum()     data_ptr() \u2192 int Returns the address of the first element of self tensor.     dequantize() \u2192 Tensor Given a quantized Tensor, dequantize it and return the dequantized float Tensor.     det() \u2192 Tensor See torch.det()     dense_dim() \u2192 int If self is a sparse COO tensor (i.e., with torch.sparse_coo layout), this returns the number of dense dimensions. Otherwise, this throws an error. See also Tensor.sparse_dim().     detach() Returns a new Tensor, detached from the current graph. The result will never require gradient.  Note Returned Tensor shares the same storage with the original one. In-place modifications on either of them will be seen, and may trigger errors in correctness checks. IMPORTANT NOTE: Previously, in-place size / stride / storage changes (such as resize_ / resize_as_ / set_ / transpose_) to the returned tensor also update the original tensor. Now, these in-place changes will not update the original tensor anymore, and will instead trigger an error. For sparse tensors: In-place indices / values changes (such as zero_ / copy_ / add_) to the returned tensor will not update the original tensor anymore, and will instead trigger an error.      detach_() Detaches the Tensor from the graph that created it, making it a leaf. Views cannot be detached in-place.     diag(diagonal=0) \u2192 Tensor See torch.diag()     diag_embed(offset=0, dim1=-2, dim2=-1) \u2192 Tensor See torch.diag_embed()     diagflat(offset=0) \u2192 Tensor See torch.diagflat()     diagonal(offset=0, dim1=0, dim2=1) \u2192 Tensor See torch.diagonal()     fill_diagonal_(fill_value, wrap=False) \u2192 Tensor Fill the main diagonal of a tensor that has at least 2-dimensions. When dims&gt;2, all dimensions of input must be of equal length. This function modifies the input tensor in-place, and returns the input tensor.  Parameters  fill_value (Scalar) \u2013 the fill value wrap (bool) \u2013 the diagonal \u2018wrapped\u2019 after N columns for tall matrices.    Example: &gt;&gt;&gt; a = torch.zeros(3, 3) &gt;&gt;&gt; a.fill_diagonal_(5) tensor([[5., 0., 0.],         [0., 5., 0.],         [0., 0., 5.]]) &gt;&gt;&gt; b = torch.zeros(7, 3) &gt;&gt;&gt; b.fill_diagonal_(5) tensor([[5., 0., 0.],         [0., 5., 0.],         [0., 0., 5.],         [0., 0., 0.],         [0., 0., 0.],         [0., 0., 0.],         [0., 0., 0.]]) &gt;&gt;&gt; c = torch.zeros(7, 3) &gt;&gt;&gt; c.fill_diagonal_(5, wrap=True) tensor([[5., 0., 0.],         [0., 5., 0.],         [0., 0., 5.],         [0., 0., 0.],         [5., 0., 0.],         [0., 5., 0.],         [0., 0., 5.]])       digamma() \u2192 Tensor See torch.digamma()     digamma_() \u2192 Tensor In-place version of digamma()     dim() \u2192 int Returns the number of dimensions of self tensor.     dist(other, p=2) \u2192 Tensor See torch.dist()     div(value) \u2192 Tensor See torch.div()     div_(value) \u2192 Tensor In-place version of div()     dot(tensor2) \u2192 Tensor See torch.dot()     double() \u2192 Tensor self.double() is equivalent to self.to(torch.float64). See to().     eig(eigenvectors=False) -&gt; (Tensor, Tensor) See torch.eig()     element_size() \u2192 int Returns the size in bytes of an individual element. Example: &gt;&gt;&gt; torch.tensor([]).element_size() 4 &gt;&gt;&gt; torch.tensor([], dtype=torch.uint8).element_size() 1       eq(other) \u2192 Tensor See torch.eq()     eq_(other) \u2192 Tensor In-place version of eq()     equal(other) \u2192 bool See torch.equal()     erf() \u2192 Tensor See torch.erf()     erf_() \u2192 Tensor In-place version of erf()     erfc() \u2192 Tensor See torch.erfc()     erfc_() \u2192 Tensor In-place version of erfc()     erfinv() \u2192 Tensor See torch.erfinv()     erfinv_() \u2192 Tensor In-place version of erfinv()     exp() \u2192 Tensor See torch.exp()     exp_() \u2192 Tensor In-place version of exp()     expm1() \u2192 Tensor See torch.expm1()     expm1_() \u2192 Tensor In-place version of expm1()     expand(*sizes) \u2192 Tensor Returns a new view of the self tensor with singleton dimensions expanded to a larger size. Passing -1 as the size for a dimension means not changing the size of that dimension. Tensor can be also expanded to a larger number of dimensions, and the new ones will be appended at the front. For the new dimensions, the size cannot be set to -1. Expanding a tensor does not allocate new memory, but only creates a new view on the existing tensor where a dimension of size one is expanded to a larger size by setting the stride to 0. Any dimension of size 1 can be expanded to an arbitrary value without allocating new memory.  Parameters *sizes (torch.Size or python:int...) \u2013 the desired expanded size    Warning More than one element of an expanded tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensors, please clone them first.  Example: &gt;&gt;&gt; x = torch.tensor([[1], [2], [3]]) &gt;&gt;&gt; x.size() torch.Size([3, 1]) &gt;&gt;&gt; x.expand(3, 4) tensor([[ 1,  1,  1,  1],         [ 2,  2,  2,  2],         [ 3,  3,  3,  3]]) &gt;&gt;&gt; x.expand(-1, 4)   # -1 means not changing the size of that dimension tensor([[ 1,  1,  1,  1],         [ 2,  2,  2,  2],         [ 3,  3,  3,  3]])       expand_as(other) \u2192 Tensor Expand this tensor to the same size as other. self.expand_as(other) is equivalent to self.expand(other.size()). Please see expand() for more information about expand.  Parameters other (torch.Tensor) \u2013 The result tensor has the same size as other.       exponential_(lambd=1, *, generator=None) \u2192 Tensor Fills self tensor with elements drawn from the exponential distribution:  f(x)=\u03bbe\u2212\u03bbxf(x) = \\lambda e^{-\\lambda x}f(x)=\u03bbe\u2212\u03bbx      fft(signal_ndim, normalized=False) \u2192 Tensor See torch.fft()     fill_(value) \u2192 Tensor Fills self tensor with the specified value.     flatten(input, start_dim=0, end_dim=-1) \u2192 Tensor see torch.flatten()     flip(dims) \u2192 Tensor See torch.flip()     float() \u2192 Tensor self.float() is equivalent to self.to(torch.float32). See to().     floor() \u2192 Tensor See torch.floor()     floor_() \u2192 Tensor In-place version of floor()     fmod(divisor) \u2192 Tensor See torch.fmod()     fmod_(divisor) \u2192 Tensor In-place version of fmod()     frac() \u2192 Tensor See torch.frac()     frac_() \u2192 Tensor In-place version of frac()     gather(dim, index) \u2192 Tensor See torch.gather()     ge(other) \u2192 Tensor See torch.ge()     ge_(other) \u2192 Tensor In-place version of ge()     geometric_(p, *, generator=None) \u2192 Tensor Fills self tensor with elements drawn from the geometric distribution:  f(X=k)=pk\u22121(1\u2212p)f(X=k) = p^{k - 1} (1 - p)f(X=k)=pk\u22121(1\u2212p)      geqrf() -&gt; (Tensor, Tensor) See torch.geqrf()     ger(vec2) \u2192 Tensor See torch.ger()     get_device() -&gt; Device ordinal (Integer) For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides. For CPU tensors, an error is thrown. Example: &gt;&gt;&gt; x = torch.randn(3, 4, 5, device='cuda:0') &gt;&gt;&gt; x.get_device() 0 &gt;&gt;&gt; x.cpu().get_device()  # RuntimeError: get_device is not implemented for type torch.FloatTensor       gt(other) \u2192 Tensor See torch.gt()     gt_(other) \u2192 Tensor In-place version of gt()     half() \u2192 Tensor self.half() is equivalent to self.to(torch.float16). See to().     hardshrink(lambd=0.5) \u2192 Tensor See torch.nn.functional.hardshrink()     histc(bins=100, min=0, max=0) \u2192 Tensor See torch.histc()     ifft(signal_ndim, normalized=False) \u2192 Tensor See torch.ifft()     imag() \u2192 Tensor See torch.imag()     index_add_(dim, index, tensor) \u2192 Tensor Accumulate the elements of tensor into the self tensor by adding to the indices in the order given in index. For example, if dim == 0 and index[i] == j, then the ith row of tensor is added to the jth row of self. The dimth dimension of tensor must have the same size as the length of index (which must be a vector), and all other dimensions must match self, or an error will be raised.  Note When using the CUDA backend, this operation may induce nondeterministic behaviour that is not easily switched off. Please see the notes on Reproducibility for background.   Parameters  dim (python:int) \u2013 dimension along which to index index (LongTensor) \u2013 indices of tensor to select from tensor (Tensor) \u2013 the tensor containing values to add    Example: &gt;&gt;&gt; x = torch.ones(5, 3) &gt;&gt;&gt; t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float) &gt;&gt;&gt; index = torch.tensor([0, 4, 2]) &gt;&gt;&gt; x.index_add_(0, index, t) tensor([[  2.,   3.,   4.],         [  1.,   1.,   1.],         [  8.,   9.,  10.],         [  1.,   1.,   1.],         [  5.,   6.,   7.]])       index_add(dim, index, tensor) \u2192 Tensor Out-of-place version of torch.Tensor.index_add_()     index_copy_(dim, index, tensor) \u2192 Tensor Copies the elements of tensor into the self tensor by selecting the indices in the order given in index. For example, if dim == 0 and index[i] == j, then the ith row of tensor is copied to the jth row of self. The dimth dimension of tensor must have the same size as the length of index (which must be a vector), and all other dimensions must match self, or an error will be raised.  Parameters  dim (python:int) \u2013 dimension along which to index index (LongTensor) \u2013 indices of tensor to select from tensor (Tensor) \u2013 the tensor containing values to copy    Example: &gt;&gt;&gt; x = torch.zeros(5, 3) &gt;&gt;&gt; t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float) &gt;&gt;&gt; index = torch.tensor([0, 4, 2]) &gt;&gt;&gt; x.index_copy_(0, index, t) tensor([[ 1.,  2.,  3.],         [ 0.,  0.,  0.],         [ 7.,  8.,  9.],         [ 0.,  0.,  0.],         [ 4.,  5.,  6.]])       index_copy(dim, index, tensor) \u2192 Tensor Out-of-place version of torch.Tensor.index_copy_()     index_fill_(dim, index, val) \u2192 Tensor Fills the elements of the self tensor with value val by selecting the indices in the order given in index.  Parameters  dim (python:int) \u2013 dimension along which to index index (LongTensor) \u2013 indices of self tensor to fill in val (python:float) \u2013 the value to fill with     Example::&gt;&gt;&gt; x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float) &gt;&gt;&gt; index = torch.tensor([0, 2]) &gt;&gt;&gt; x.index_fill_(1, index, -1) tensor([[-1.,  2., -1.],         [-1.,  5., -1.],         [-1.,  8., -1.]])         index_fill(dim, index, value) \u2192 Tensor Out-of-place version of torch.Tensor.index_fill_()     index_put_(indices, value, accumulate=False) \u2192 Tensor Puts values from the tensor value into the tensor self using the indices specified in indices (which is a tuple of Tensors). The expression tensor.index_put_(indices, value) is equivalent to tensor[indices] = value. Returns self. If accumulate is True, the elements in tensor are added to self. If accumulate is False, the behavior is undefined if indices contain duplicate elements.  Parameters  indices (tuple of LongTensor) \u2013 tensors used to index into self. value (Tensor) \u2013 tensor of same dtype as self. accumulate (bool) \u2013 whether to accumulate into self        index_put(indices, value, accumulate=False) \u2192 Tensor Out-place version of index_put_()     index_select(dim, index) \u2192 Tensor See torch.index_select()     indices() \u2192 Tensor If self is a sparse COO tensor (i.e., with torch.sparse_coo layout), this returns a view of the contained indices tensor. Otherwise, this throws an error. See also Tensor.values().  Note This method can only be called on a coalesced sparse tensor. See Tensor.coalesce() for details.      int() \u2192 Tensor self.int() is equivalent to self.to(torch.int32). See to().     int_repr() \u2192 Tensor Given a quantized Tensor, self.int_repr() returns a CPU Tensor with uint8_t as data type that stores the underlying uint8_t values of the given Tensor.     inverse() \u2192 Tensor See torch.inverse()     irfft(signal_ndim, normalized=False, onesided=True, signal_sizes=None) \u2192 Tensor See torch.irfft()     is_contiguous() \u2192 bool Returns True if self tensor is contiguous in memory in C order.     is_floating_point() \u2192 bool Returns True if the data type of self is a floating point data type.     is_leaf All Tensors that have requires_grad which is False will be leaf Tensors by convention. For Tensors that have requires_grad which is True, they will be leaf Tensors if they were created by the user. This means that they are not the result of an operation and so grad_fn is None. Only leaf Tensors will have their grad populated during a call to backward(). To get grad populated for non-leaf Tensors, you can use retain_grad(). Example: &gt;&gt;&gt; a = torch.rand(10, requires_grad=True) &gt;&gt;&gt; a.is_leaf True &gt;&gt;&gt; b = torch.rand(10, requires_grad=True).cuda() &gt;&gt;&gt; b.is_leaf False # b was created by the operation that cast a cpu Tensor into a cuda Tensor &gt;&gt;&gt; c = torch.rand(10, requires_grad=True) + 2 &gt;&gt;&gt; c.is_leaf False # c was created by the addition operation &gt;&gt;&gt; d = torch.rand(10).cuda() &gt;&gt;&gt; d.is_leaf True # d does not require gradients and so has no operation creating it (that is tracked by the autograd engine) &gt;&gt;&gt; e = torch.rand(10).cuda().requires_grad_() &gt;&gt;&gt; e.is_leaf True # e requires gradients and has no operations creating it &gt;&gt;&gt; f = torch.rand(10, requires_grad=True, device=\"cuda\") &gt;&gt;&gt; f.is_leaf True # f requires grad, has no operation creating it       is_pinned() Returns true if this tensor resides in pinned memory.     is_set_to(tensor) \u2192 bool Returns True if this object refers to the same THTensor object from the Torch C API as the given tensor.     is_shared()  Checks if tensor is in shared memory. This is always True for CUDA tensors.     is_signed() \u2192 bool Returns True if the data type of self is a signed data type.     is_sparse     item() \u2192 number Returns the value of this tensor as a standard Python number. This only works for tensors with one element. For other cases, see tolist(). This operation is not differentiable. Example: &gt;&gt;&gt; x = torch.tensor([1.0]) &gt;&gt;&gt; x.item() 1.0       kthvalue(k, dim=None, keepdim=False) -&gt; (Tensor, LongTensor) See torch.kthvalue()     le(other) \u2192 Tensor See torch.le()     le_(other) \u2192 Tensor In-place version of le()     lerp(end, weight) \u2192 Tensor See torch.lerp()     lerp_(end, weight) \u2192 Tensor In-place version of lerp()     lgamma() \u2192 Tensor See torch.lgamma()     lgamma_() \u2192 Tensor In-place version of lgamma()     log() \u2192 Tensor See torch.log()     log_() \u2192 Tensor In-place version of log()     logdet() \u2192 Tensor See torch.logdet()     log10() \u2192 Tensor See torch.log10()     log10_() \u2192 Tensor In-place version of log10()     log1p() \u2192 Tensor See torch.log1p()     log1p_() \u2192 Tensor In-place version of log1p()     log2() \u2192 Tensor See torch.log2()     log2_() \u2192 Tensor In-place version of log2()     log_normal_(mean=1, std=2, *, generator=None) Fills self tensor with numbers samples from the log-normal distribution parameterized by the given mean \u03bc\\mu\u03bc   and standard deviation \u03c3\\sigma\u03c3  . Note that mean and std are the mean and standard deviation of the underlying normal distribution, and not of the returned distribution:  f(x)=1x\u03c32\u03c0\u00a0e\u2212(ln\u2061x\u2212\u03bc)22\u03c32f(x) = \\dfrac{1}{x \\sigma \\sqrt{2\\pi}}\\ e^{-\\frac{(\\ln x - \\mu)^2}{2\\sigma^2}}f(x)=x\u03c32\u03c0\u200b1\u200b\u00a0e\u22122\u03c32(lnx\u2212\u03bc)2\u200b      logsumexp(dim, keepdim=False) \u2192 Tensor See torch.logsumexp()     logical_not() \u2192 Tensor See torch.logical_not()     logical_not_() \u2192 Tensor In-place version of logical_not()     logical_xor() \u2192 Tensor See torch.logical_xor()     logical_xor_() \u2192 Tensor In-place version of logical_xor()     long() \u2192 Tensor self.long() is equivalent to self.to(torch.int64). See to().     lstsq(A) -&gt; (Tensor, Tensor) See torch.lstsq()     lt(other) \u2192 Tensor See torch.lt()     lt_(other) \u2192 Tensor In-place version of lt()     lu(pivot=True, get_infos=False)  See torch.lu()     lu_solve(LU_data, LU_pivots) \u2192 Tensor See torch.lu_solve()     map_(tensor, callable) Applies callable for each element in self tensor and the given tensor and stores the results in self tensor. self tensor and the given tensor must be broadcastable. The callable should have the signature: def callable(a, b) -&gt; number       masked_scatter_(mask, source) Copies elements from source into self tensor at positions where the mask is True. The shape of mask must be broadcastable with the shape of the underlying tensor. The source should have at least as many elements as the number of ones in mask  Parameters  mask (BoolTensor) \u2013 the boolean mask source (Tensor) \u2013 the tensor to copy from     Note The mask operates on the self tensor, not on the given source tensor.      masked_scatter(mask, tensor) \u2192 Tensor Out-of-place version of torch.Tensor.masked_scatter_()     masked_fill_(mask, value) Fills elements of self tensor with value where mask is True. The shape of mask must be broadcastable with the shape of the underlying tensor.  Parameters  mask (BoolTensor) \u2013 the boolean mask value (python:float) \u2013 the value to fill in with        masked_fill(mask, value) \u2192 Tensor Out-of-place version of torch.Tensor.masked_fill_()     masked_select(mask) \u2192 Tensor See torch.masked_select()     matmul(tensor2) \u2192 Tensor See torch.matmul()     matrix_power(n) \u2192 Tensor See torch.matrix_power()     max(dim=None, keepdim=False) -&gt; Tensor or (Tensor, Tensor) See torch.max()     mean(dim=None, keepdim=False) -&gt; Tensor or (Tensor, Tensor) See torch.mean()     median(dim=None, keepdim=False) -&gt; (Tensor, LongTensor) See torch.median()     min(dim=None, keepdim=False) -&gt; Tensor or (Tensor, Tensor) See torch.min()     mm(mat2) \u2192 Tensor See torch.mm()     mode(dim=None, keepdim=False) -&gt; (Tensor, LongTensor) See torch.mode()     mul(value) \u2192 Tensor See torch.mul()     mul_(value) In-place version of mul()     multinomial(num_samples, replacement=False, *, generator=None) \u2192 Tensor See torch.multinomial()     mv(vec) \u2192 Tensor See torch.mv()     mvlgamma(p) \u2192 Tensor See torch.mvlgamma()     mvlgamma_(p) \u2192 Tensor In-place version of mvlgamma()     narrow(dimension, start, length) \u2192 Tensor See torch.narrow() Example: &gt;&gt;&gt; x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) &gt;&gt;&gt; x.narrow(0, 0, 2) tensor([[ 1,  2,  3],         [ 4,  5,  6]]) &gt;&gt;&gt; x.narrow(1, 1, 2) tensor([[ 2,  3],         [ 5,  6],         [ 8,  9]])       narrow_copy(dimension, start, length) \u2192 Tensor Same as Tensor.narrow() except returning a copy rather than shared storage.  This is primarily for sparse tensors, which do not have a shared-storage narrow method.  Calling `narrow_copy with `dimemsion &gt; self.sparse_dim()` will return a copy with the relevant dense dimension narrowed, and `self.shape` updated accordingly.     ndimension() \u2192 int Alias for dim()     ne(other) \u2192 Tensor See torch.ne()     ne_(other) \u2192 Tensor In-place version of ne()     neg() \u2192 Tensor See torch.neg()     neg_() \u2192 Tensor In-place version of neg()     nelement() \u2192 int Alias for numel()     nonzero() \u2192 LongTensor See torch.nonzero()     norm(p='fro', dim=None, keepdim=False, dtype=None)  See torch.norm()     normal_(mean=0, std=1, *, generator=None) \u2192 Tensor Fills self tensor with elements samples from the normal distribution parameterized by mean and std.     numel() \u2192 int See torch.numel()     numpy() \u2192 numpy.ndarray Returns self tensor as a NumPy ndarray. This tensor and the returned ndarray share the same underlying storage. Changes to self tensor will be reflected in the ndarray and vice versa.     orgqr(input2) \u2192 Tensor See torch.orgqr()     ormqr(input2, input3, left=True, transpose=False) \u2192 Tensor See torch.ormqr()     permute(*dims) \u2192 Tensor Permute the dimensions of this tensor.  Parameters *dims (python:int...) \u2013 The desired ordering of dimensions   Example &gt;&gt;&gt; x = torch.randn(2, 3, 5) &gt;&gt;&gt; x.size() torch.Size([2, 3, 5]) &gt;&gt;&gt; x.permute(2, 0, 1).size() torch.Size([5, 2, 3])       pin_memory() \u2192 Tensor Copies the tensor to pinned memory, if it\u2019s not already pinned.     pinverse() \u2192 Tensor See torch.pinverse()     polygamma(n) \u2192 Tensor See torch.polygamma()     polygamma_(n) \u2192 Tensor In-place version of polygamma()     pow(exponent) \u2192 Tensor See torch.pow()     pow_(exponent) \u2192 Tensor In-place version of pow()     prod(dim=None, keepdim=False, dtype=None) \u2192 Tensor See torch.prod()     put_(indices, tensor, accumulate=False) \u2192 Tensor Copies the elements from tensor into the positions specified by indices. For the purpose of indexing, the self tensor is treated as if it were a 1-D tensor. If accumulate is True, the elements in tensor are added to self. If accumulate is False, the behavior is undefined if indices contain duplicate elements.  Parameters  indices (LongTensor) \u2013 the indices into self tensor (Tensor) \u2013 the tensor containing values to copy from accumulate (bool) \u2013 whether to accumulate into self    Example: &gt;&gt;&gt; src = torch.tensor([[4, 3, 5],                         [6, 7, 8]]) &gt;&gt;&gt; src.put_(torch.tensor([1, 3]), torch.tensor([9, 10])) tensor([[  4,   9,   5],         [ 10,   7,   8]])       qr(some=True) -&gt; (Tensor, Tensor) See torch.qr()     qscheme() \u2192 torch.qscheme Returns the quantization scheme of a given QTensor.     q_scale() \u2192 float Given a Tensor quantized by linear(affine) quantization, returns the scale of the underlying quantizer().     q_zero_point() \u2192 int Given a Tensor quantized by linear(affine) quantization, returns the zero_point of the underlying quantizer().     q_per_channel_scales() \u2192 Tensor Given a Tensor quantized by linear (affine) per-channel quantization, returns a Tensor of scales of the underlying quantizer. It has the number of elements that matches the corresponding dimensions (from q_per_channel_axis) of the tensor.     q_per_channel_zero_points() \u2192 Tensor Given a Tensor quantized by linear (affine) per-channel quantization, returns a tensor of zero_points of the underlying quantizer. It has the number of elements that matches the corresponding dimensions (from q_per_channel_axis) of the tensor.     q_per_channel_axis() \u2192 int Given a Tensor quantized by linear (affine) per-channel quantization, returns the index of dimension on which per-channel quantization is applied.     random_(from=0, to=None, *, generator=None) \u2192 Tensor Fills self tensor with numbers sampled from the discrete uniform distribution over [from, to - 1]. If not specified, the values are usually only bounded by self tensor\u2019s data type. However, for floating point types, if unspecified, range will be [0, 2^mantissa] to ensure that every value is representable. For example, torch.tensor(1, dtype=torch.double).random_() will be uniform in [0, 2^53].     reciprocal() \u2192 Tensor See torch.reciprocal()     reciprocal_() \u2192 Tensor In-place version of reciprocal()     record_stream(stream) Ensures that the tensor memory is not reused for another tensor until all current work queued on stream are complete.  Note The caching allocator is aware of only the stream where a tensor was allocated. Due to the awareness, it already correctly manages the life cycle of tensors on only one stream. But if a tensor is used on a stream different from the stream of origin, the allocator might reuse the memory unexpectedly. Calling this method lets the allocator know which streams have used the tensor.      register_hook(hook)  Registers a backward hook. The hook will be called every time a gradient with respect to the Tensor is computed. The hook should have the following signature: hook(grad) -&gt; Tensor or None   The hook should not modify its argument, but it can optionally return a new gradient which will be used in place of grad. This function returns a handle with a method handle.remove() that removes the hook from the module. Example: &gt;&gt;&gt; v = torch.tensor([0., 0., 0.], requires_grad=True) &gt;&gt;&gt; h = v.register_hook(lambda grad: grad * 2)  # double the gradient &gt;&gt;&gt; v.backward(torch.tensor([1., 2., 3.])) &gt;&gt;&gt; v.grad   2  4  6 [torch.FloatTensor of size (3,)]  &gt;&gt;&gt; h.remove()  # removes the hook       remainder(divisor) \u2192 Tensor See torch.remainder()     remainder_(divisor) \u2192 Tensor In-place version of remainder()     real() \u2192 Tensor See torch.real()     renorm(p, dim, maxnorm) \u2192 Tensor See torch.renorm()     renorm_(p, dim, maxnorm) \u2192 Tensor In-place version of renorm()     repeat(*sizes) \u2192 Tensor Repeats this tensor along the specified dimensions. Unlike expand(), this function copies the tensor\u2019s data.  Warning torch.repeat() behaves differently from numpy.repeat, but is more similar to numpy.tile. For the operator similar to numpy.repeat, see torch.repeat_interleave().   Parameters sizes (torch.Size or python:int...) \u2013 The number of times to repeat this tensor along each dimension   Example: &gt;&gt;&gt; x = torch.tensor([1, 2, 3]) &gt;&gt;&gt; x.repeat(4, 2) tensor([[ 1,  2,  3,  1,  2,  3],         [ 1,  2,  3,  1,  2,  3],         [ 1,  2,  3,  1,  2,  3],         [ 1,  2,  3,  1,  2,  3]]) &gt;&gt;&gt; x.repeat(4, 2, 1).size() torch.Size([4, 2, 3])       repeat_interleave(repeats, dim=None) \u2192 Tensor See torch.repeat_interleave().     requires_grad Is True if gradients need to be computed for this Tensor, False otherwise.  Note The fact that gradients need to be computed for a Tensor do not mean that the grad attribute will be populated, see is_leaf for more details.      requires_grad_(requires_grad=True) \u2192 Tensor Change if autograd should record operations on this tensor: sets this tensor\u2019s requires_grad attribute in-place. Returns this tensor. requires_grad_()\u2019s main use case is to tell autograd to begin recording operations on a Tensor tensor. If tensor has requires_grad=False (because it was obtained through a DataLoader, or required preprocessing or initialization), tensor.requires_grad_() makes it so that autograd will begin to record operations on tensor.  Parameters requires_grad (bool) \u2013 If autograd should record operations on this tensor. Default: True.   Example: &gt;&gt;&gt; # Let's say we want to preprocess some saved weights and use &gt;&gt;&gt; # the result as new weights. &gt;&gt;&gt; saved_weights = [0.1, 0.2, 0.3, 0.25] &gt;&gt;&gt; loaded_weights = torch.tensor(saved_weights) &gt;&gt;&gt; weights = preprocess(loaded_weights)  # some function &gt;&gt;&gt; weights tensor([-0.5503,  0.4926, -2.1158, -0.8303])  &gt;&gt;&gt; # Now, start to record operations done to weights &gt;&gt;&gt; weights.requires_grad_() &gt;&gt;&gt; out = weights.pow(2).sum() &gt;&gt;&gt; out.backward() &gt;&gt;&gt; weights.grad tensor([-1.1007,  0.9853, -4.2316, -1.6606])       reshape(*shape) \u2192 Tensor Returns a tensor with the same data and number of elements as self but with the specified shape. This method returns a view if shape is compatible with the current shape. See torch.Tensor.view() on when it is possible to return a view. See torch.reshape()  Parameters shape (tuple of python:ints or python:int...) \u2013 the desired shape       reshape_as(other) \u2192 Tensor Returns this tensor as the same shape as other. self.reshape_as(other) is equivalent to self.reshape(other.sizes()). This method returns a view if other.sizes() is compatible with the current shape. See torch.Tensor.view() on when it is possible to return a view. Please see reshape() for more information about reshape.  Parameters other (torch.Tensor) \u2013 The result tensor has the same shape as other.       resize_(*sizes) \u2192 Tensor Resizes self tensor to the specified size. If the number of elements is larger than the current storage size, then the underlying storage is resized to fit the new number of elements. If the number of elements is smaller, the underlying storage is not changed. Existing elements are preserved but any new memory is uninitialized.  Warning This is a low-level method. The storage is reinterpreted as C-contiguous, ignoring the current strides (unless the target size equals the current size, in which case the tensor is left unchanged). For most purposes, you will instead want to use view(), which checks for contiguity, or reshape(), which copies data if needed. To change the size in-place with custom strides, see set_().   Parameters sizes (torch.Size or python:int...) \u2013 the desired size   Example: &gt;&gt;&gt; x = torch.tensor([[1, 2], [3, 4], [5, 6]]) &gt;&gt;&gt; x.resize_(2, 2) tensor([[ 1,  2],         [ 3,  4]])       resize_as_(tensor) \u2192 Tensor Resizes the self tensor to be the same size as the specified tensor. This is equivalent to self.resize_(tensor.size()).     retain_grad()  Enables .grad attribute for non-leaf Tensors.     rfft(signal_ndim, normalized=False, onesided=True) \u2192 Tensor See torch.rfft()     roll(shifts, dims) \u2192 Tensor See torch.roll()     rot90(k, dims) \u2192 Tensor See torch.rot90()     round() \u2192 Tensor See torch.round()     round_() \u2192 Tensor In-place version of round()     rsqrt() \u2192 Tensor See torch.rsqrt()     rsqrt_() \u2192 Tensor In-place version of rsqrt()     scatter(dim, index, source) \u2192 Tensor Out-of-place version of torch.Tensor.scatter_()     scatter_(dim, index, src) \u2192 Tensor Writes all values from the tensor src into self at the indices specified in the index tensor. For each value in src, its output index is specified by its index in src for dimension != dim and by the corresponding value in index for dimension = dim. For a 3-D tensor, self is updated as: self[index[i][j][k]][j][k] = src[i][j][k]  # if dim == 0 self[i][index[i][j][k]][k] = src[i][j][k]  # if dim == 1 self[i][j][index[i][j][k]] = src[i][j][k]  # if dim == 2   This is the reverse operation of the manner described in gather(). self, index and src (if it is a Tensor) should have same number of dimensions. It is also required that index.size(d) &lt;= src.size(d) for all dimensions d, and that index.size(d) &lt;= self.size(d) for all dimensions d != dim. Moreover, as for gather(), the values of index must be between 0 and self.size(dim) - 1 inclusive, and all values in a row along the specified dimension dim must be unique.  Parameters  dim (python:int) \u2013 the axis along which to index index (LongTensor) \u2013 the indices of elements to scatter, can be either empty or the same size of src. When empty, the operation returns identity src (Tensor) \u2013 the source element(s) to scatter, incase value is not specified value (python:float) \u2013 the source element(s) to scatter, incase src is not specified    Example: &gt;&gt;&gt; x = torch.rand(2, 5) &gt;&gt;&gt; x tensor([[ 0.3992,  0.2908,  0.9044,  0.4850,  0.6004],         [ 0.5735,  0.9006,  0.6797,  0.4152,  0.1732]]) &gt;&gt;&gt; torch.zeros(3, 5).scatter_(0, torch.tensor([[0, 1, 2, 0, 0], [2, 0, 0, 1, 2]]), x) tensor([[ 0.3992,  0.9006,  0.6797,  0.4850,  0.6004],         [ 0.0000,  0.2908,  0.0000,  0.4152,  0.0000],         [ 0.5735,  0.0000,  0.9044,  0.0000,  0.1732]])  &gt;&gt;&gt; z = torch.zeros(2, 4).scatter_(1, torch.tensor([[2], [3]]), 1.23) &gt;&gt;&gt; z tensor([[ 0.0000,  0.0000,  1.2300,  0.0000],         [ 0.0000,  0.0000,  0.0000,  1.2300]])       scatter_add_(dim, index, other) \u2192 Tensor Adds all values from the tensor other into self at the indices specified in the index tensor in a similar fashion as scatter_(). For each value in other, it is added to an index in self which is specified by its index in other for dimension != dim and by the corresponding value in index for dimension = dim. For a 3-D tensor, self is updated as: self[index[i][j][k]][j][k] += other[i][j][k]  # if dim == 0 self[i][index[i][j][k]][k] += other[i][j][k]  # if dim == 1 self[i][j][index[i][j][k]] += other[i][j][k]  # if dim == 2   self, index and other should have same number of dimensions. It is also required that index.size(d) &lt;= other.size(d) for all dimensions d, and that index.size(d) &lt;= self.size(d) for all dimensions d != dim.  Note When using the CUDA backend, this operation may induce nondeterministic behaviour that is not easily switched off. Please see the notes on Reproducibility for background.   Parameters  dim (python:int) \u2013 the axis along which to index index (LongTensor) \u2013 the indices of elements to scatter and add, can be either empty or the same size of src. When empty, the operation returns identity. other (Tensor) \u2013 the source elements to scatter and add    Example: &gt;&gt;&gt; x = torch.rand(2, 5) &gt;&gt;&gt; x tensor([[0.7404, 0.0427, 0.6480, 0.3806, 0.8328],         [0.7953, 0.2009, 0.9154, 0.6782, 0.9620]]) &gt;&gt;&gt; torch.ones(3, 5).scatter_add_(0, torch.tensor([[0, 1, 2, 0, 0], [2, 0, 0, 1, 2]]), x) tensor([[1.7404, 1.2009, 1.9154, 1.3806, 1.8328],         [1.0000, 1.0427, 1.0000, 1.6782, 1.0000],         [1.7953, 1.0000, 1.6480, 1.0000, 1.9620]])       scatter_add(dim, index, source) \u2192 Tensor Out-of-place version of torch.Tensor.scatter_add_()     select(dim, index) \u2192 Tensor Slices the self tensor along the selected dimension at the given index. This function returns a tensor with the given dimension removed.  Parameters  dim (python:int) \u2013 the dimension to slice index (python:int) \u2013 the index to select with     Note select() is equivalent to slicing. For example, tensor.select(0, index) is equivalent to tensor[index] and tensor.select(2, index) is equivalent to tensor[:,:,index].      set_(source=None, storage_offset=0, size=None, stride=None) \u2192 Tensor Sets the underlying storage, size, and strides. If source is a tensor, self tensor will share the same storage and have the same size and strides as source. Changes to elements in one tensor will be reflected in the other. If source is a Storage, the method sets the underlying storage, offset, size, and stride.  Parameters  source (Tensor or Storage) \u2013 the tensor or storage to use storage_offset (python:int, optional) \u2013 the offset in the storage size (torch.Size, optional) \u2013 the desired size. Defaults to the size of the source. stride (tuple, optional) \u2013 the desired stride. Defaults to C-contiguous strides.        share_memory_()  Moves the underlying storage to shared memory. This is a no-op if the underlying storage is already in shared memory and for CUDA tensors. Tensors in shared memory cannot be resized.     short() \u2192 Tensor self.short() is equivalent to self.to(torch.int16). See to().     sigmoid() \u2192 Tensor See torch.sigmoid()     sigmoid_() \u2192 Tensor In-place version of sigmoid()     sign() \u2192 Tensor See torch.sign()     sign_() \u2192 Tensor In-place version of sign()     sin() \u2192 Tensor See torch.sin()     sin_() \u2192 Tensor In-place version of sin()     sinh() \u2192 Tensor See torch.sinh()     sinh_() \u2192 Tensor In-place version of sinh()     size() \u2192 torch.Size Returns the size of the self tensor. The returned value is a subclass of tuple. Example: &gt;&gt;&gt; torch.empty(3, 4, 5).size() torch.Size([3, 4, 5])       slogdet() -&gt; (Tensor, Tensor) See torch.slogdet()     solve(A) \u2192 Tensor, Tensor See torch.solve()     sort(dim=-1, descending=False) -&gt; (Tensor, LongTensor) See torch.sort()     split(split_size, dim=0)  See torch.split()     sparse_mask(input, mask) \u2192 Tensor Returns a new SparseTensor with values from Tensor input filtered by indices of mask and values are ignored. input and mask must have the same shape.  Parameters  input (Tensor) \u2013 an input Tensor mask (SparseTensor) \u2013 a SparseTensor which we filter input based on its indices    Example: &gt;&gt;&gt; nnz = 5 &gt;&gt;&gt; dims = [5, 5, 2, 2] &gt;&gt;&gt; I = torch.cat([torch.randint(0, dims[0], size=(nnz,)),                    torch.randint(0, dims[1], size=(nnz,))], 0).reshape(2, nnz) &gt;&gt;&gt; V = torch.randn(nnz, dims[2], dims[3]) &gt;&gt;&gt; size = torch.Size(dims) &gt;&gt;&gt; S = torch.sparse_coo_tensor(I, V, size).coalesce() &gt;&gt;&gt; D = torch.randn(dims) &gt;&gt;&gt; D.sparse_mask(S) tensor(indices=tensor([[0, 0, 0, 2],                        [0, 1, 4, 3]]),        values=tensor([[[ 1.6550,  0.2397],                        [-0.1611, -0.0779]],                        [[ 0.2326, -1.0558],                        [ 1.4711,  1.9678]],                        [[-0.5138, -0.0411],                        [ 1.9417,  0.5158]],                        [[ 0.0793,  0.0036],                        [-0.2569, -0.1055]]]),        size=(5, 5, 2, 2), nnz=4, layout=torch.sparse_coo)       sparse_dim() \u2192 int If self is a sparse COO tensor (i.e., with torch.sparse_coo layout), this returns the number of sparse dimensions. Otherwise, this throws an error. See also Tensor.dense_dim().     sqrt() \u2192 Tensor See torch.sqrt()     sqrt_() \u2192 Tensor In-place version of sqrt()     squeeze(dim=None) \u2192 Tensor See torch.squeeze()     squeeze_(dim=None) \u2192 Tensor In-place version of squeeze()     std(dim=None, unbiased=True, keepdim=False) \u2192 Tensor See torch.std()     stft(n_fft, hop_length=None, win_length=None, window=None, center=True, pad_mode='reflect', normalized=False, onesided=True)  See torch.stft()  Warning This function changed signature at version 0.4.1. Calling with the previous signature may cause error or return incorrect result.      storage() \u2192 torch.Storage Returns the underlying storage.     storage_offset() \u2192 int Returns self tensor\u2019s offset in the underlying storage in terms of number of storage elements (not bytes). Example: &gt;&gt;&gt; x = torch.tensor([1, 2, 3, 4, 5]) &gt;&gt;&gt; x.storage_offset() 0 &gt;&gt;&gt; x[3:].storage_offset() 3       storage_type() \u2192 type Returns the type of the underlying storage.     stride(dim) \u2192 tuple or int Returns the stride of self tensor. Stride is the jump necessary to go from one element to the next one in the specified dimension dim. A tuple of all strides is returned when no argument is passed in. Otherwise, an integer value is returned as the stride in the particular dimension dim.  Parameters dim (python:int, optional) \u2013 the desired dimension in which stride is required   Example: &gt;&gt;&gt; x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]) &gt;&gt;&gt; x.stride() (5, 1) &gt;&gt;&gt;x.stride(0) 5 &gt;&gt;&gt; x.stride(-1) 1       sub(value, other) \u2192 Tensor Subtracts a scalar or tensor from self tensor. If both value and other are specified, each element of other is scaled by value before being used. When other is a tensor, the shape of other must be broadcastable with the shape of the underlying tensor.     sub_(x) \u2192 Tensor In-place version of sub()     sum(dim=None, keepdim=False, dtype=None) \u2192 Tensor See torch.sum()     sum_to_size(*size) \u2192 Tensor Sum this tensor to size. size must be broadcastable to this tensor size.  Parameters size (python:int...) \u2013 a sequence of integers defining the shape of the output tensor.       svd(some=True, compute_uv=True) -&gt; (Tensor, Tensor, Tensor) See torch.svd()     symeig(eigenvectors=False, upper=True) -&gt; (Tensor, Tensor) See torch.symeig()     t() \u2192 Tensor See torch.t()     t_() \u2192 Tensor In-place version of t()     to(*args, **kwargs) \u2192 Tensor Performs Tensor dtype and/or device conversion. A torch.dtype and torch.device are inferred from the arguments of self.to(*args, **kwargs).  Note If the self Tensor already has the correct torch.dtype and torch.device, then self is returned. Otherwise, the returned tensor is a copy of self with the desired torch.dtype and torch.device.  Here are the ways to call to:   to(dtype, non_blocking=False, copy=False) \u2192 Tensor Returns a Tensor with the specified dtype     to(device=None, dtype=None, non_blocking=False, copy=False) \u2192 Tensor Returns a Tensor with the specified device and (optional) dtype. If dtype is None it is inferred to be self.dtype. When non_blocking, tries to convert asynchronously with respect to the host if possible, e.g., converting a CPU Tensor with pinned memory to a CUDA Tensor. When copy is set, a new Tensor is created even when the Tensor already matches the desired conversion.     to(other, non_blocking=False, copy=False) \u2192 Tensor Returns a Tensor with same torch.dtype and torch.device as the Tensor other. When non_blocking, tries to convert asynchronously with respect to the host if possible, e.g., converting a CPU Tensor with pinned memory to a CUDA Tensor. When copy is set, a new Tensor is created even when the Tensor already matches the desired conversion.   Example: &gt;&gt;&gt; tensor = torch.randn(2, 2)  # Initially dtype=float32, device=cpu &gt;&gt;&gt; tensor.to(torch.float64) tensor([[-0.5044,  0.0005],         [ 0.3310, -0.0584]], dtype=torch.float64)  &gt;&gt;&gt; cuda0 = torch.device('cuda:0') &gt;&gt;&gt; tensor.to(cuda0) tensor([[-0.5044,  0.0005],         [ 0.3310, -0.0584]], device='cuda:0')  &gt;&gt;&gt; tensor.to(cuda0, dtype=torch.float64) tensor([[-0.5044,  0.0005],         [ 0.3310, -0.0584]], dtype=torch.float64, device='cuda:0')  &gt;&gt;&gt; other = torch.randn((), dtype=torch.float64, device=cuda0) &gt;&gt;&gt; tensor.to(other, non_blocking=True) tensor([[-0.5044,  0.0005],         [ 0.3310, -0.0584]], dtype=torch.float64, device='cuda:0')       to_mkldnn() \u2192 Tensor Returns a copy of the tensor in torch.mkldnn layout.     take(indices) \u2192 Tensor See torch.take()     tan() \u2192 Tensor See torch.tan()     tan_() \u2192 Tensor In-place version of tan()     tanh() \u2192 Tensor See torch.tanh()     tanh_() \u2192 Tensor In-place version of tanh()     tolist() \u201d tolist() -&gt; list or number Returns the tensor as a (nested) list. For scalars, a standard Python number is returned, just like with item(). Tensors are automatically moved to the CPU first if necessary. This operation is not differentiable. Examples: &gt;&gt;&gt; a = torch.randn(2, 2) &gt;&gt;&gt; a.tolist() [[0.012766935862600803, 0.5415473580360413],  [-0.08909505605697632, 0.7729271650314331]] &gt;&gt;&gt; a[0,0].tolist() 0.012766935862600803       topk(k, dim=None, largest=True, sorted=True) -&gt; (Tensor, LongTensor) See torch.topk()     to_sparse(sparseDims) \u2192 Tensor Returns a sparse copy of the tensor.  PyTorch supports sparse tensors in coordinate format.  Parameters sparseDims (python:int, optional) \u2013 the number of sparse dimensions to include in the new sparse tensor   Example: &gt;&gt;&gt; d = torch.tensor([[0, 0, 0], [9, 0, 10], [0, 0, 0]]) &gt;&gt;&gt; d tensor([[ 0,  0,  0],         [ 9,  0, 10],         [ 0,  0,  0]]) &gt;&gt;&gt; d.to_sparse() tensor(indices=tensor([[1, 1],                        [0, 2]]),        values=tensor([ 9, 10]),        size=(3, 3), nnz=2, layout=torch.sparse_coo) &gt;&gt;&gt; d.to_sparse(1) tensor(indices=tensor([[1]]),        values=tensor([[ 9,  0, 10]]),        size=(3, 3), nnz=1, layout=torch.sparse_coo)       trace() \u2192 Tensor See torch.trace()     transpose(dim0, dim1) \u2192 Tensor See torch.transpose()     transpose_(dim0, dim1) \u2192 Tensor In-place version of transpose()     triangular_solve(A, upper=True, transpose=False, unitriangular=False) -&gt; (Tensor, Tensor) See torch.triangular_solve()     tril(k=0) \u2192 Tensor See torch.tril()     tril_(k=0) \u2192 Tensor In-place version of tril()     triu(k=0) \u2192 Tensor See torch.triu()     triu_(k=0) \u2192 Tensor In-place version of triu()     trunc() \u2192 Tensor See torch.trunc()     trunc_() \u2192 Tensor In-place version of trunc()     type(dtype=None, non_blocking=False, **kwargs) \u2192 str or Tensor Returns the type if dtype is not provided, else casts this object to the specified type. If this is already of the correct type, no copy is performed and the original object is returned.  Parameters  dtype (python:type or string) \u2013 The desired type non_blocking (bool) \u2013 If True, and the source is in pinned memory and destination is on the GPU or vice versa, the copy is performed asynchronously with respect to the host. Otherwise, the argument has no effect. **kwargs \u2013 For compatibility, may contain the key async in place of the non_blocking argument. The async arg is deprecated.        type_as(tensor) \u2192 Tensor Returns this tensor cast to the type of the given tensor. This is a no-op if the tensor is already of the correct type. This is equivalent to self.type(tensor.type())  Parameters tensor (Tensor) \u2013 the tensor which has the desired type       unbind(dim=0) \u2192 seq See torch.unbind()     unfold(dimension, size, step) \u2192 Tensor Returns a tensor which contains all slices of size size from self tensor in the dimension dimension. Step between two slices is given by step. If sizedim is the size of dimension dimension for self, the size of dimension dimension in the returned tensor will be (sizedim - size) / step + 1. An additional dimension of size size is appended in the returned tensor.  Parameters  dimension (python:int) \u2013 dimension in which unfolding happens size (python:int) \u2013 the size of each slice that is unfolded step (python:int) \u2013 the step between each slice    Example: &gt;&gt;&gt; x = torch.arange(1., 8) &gt;&gt;&gt; x tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.]) &gt;&gt;&gt; x.unfold(0, 2, 1) tensor([[ 1.,  2.],         [ 2.,  3.],         [ 3.,  4.],         [ 4.,  5.],         [ 5.,  6.],         [ 6.,  7.]]) &gt;&gt;&gt; x.unfold(0, 2, 2) tensor([[ 1.,  2.],         [ 3.,  4.],         [ 5.,  6.]])       uniform_(from=0, to=1) \u2192 Tensor Fills self tensor with numbers sampled from the continuous uniform distribution:  P(x)=1to\u2212fromP(x) = \\dfrac{1}{\\text{to} - \\text{from}}  P(x)=to\u2212from1\u200b      unique(sorted=True, return_inverse=False, return_counts=False, dim=None)  Returns the unique elements of the input tensor. See torch.unique()     unique_consecutive(return_inverse=False, return_counts=False, dim=None)  Eliminates all but the first element from every consecutive group of equivalent elements. See torch.unique_consecutive()     unsqueeze(dim) \u2192 Tensor See torch.unsqueeze()     unsqueeze_(dim) \u2192 Tensor In-place version of unsqueeze()     values() \u2192 Tensor If self is a sparse COO tensor (i.e., with torch.sparse_coo layout), this returns a view of the contained values tensor. Otherwise, this throws an error. See also Tensor.indices().  Note This method can only be called on a coalesced sparse tensor. See Tensor.coalesce() for details.      var(dim=None, unbiased=True, keepdim=False) \u2192 Tensor See torch.var()     view(*shape) \u2192 Tensor Returns a new tensor with the same data as the self tensor but of a different shape. The returned tensor shares the same data and must have the same number of elements, but may have a different size. For a tensor to be viewed, the new view size must be compatible with its original size and stride, i.e., each new view dimension must either be a subspace of an original dimension, or only span across original dimensions d,d+1,\u2026,d+kd, d+1, \\dots, d+kd,d+1,\u2026,d+k   that satisfy the following contiguity-like condition that \u2200i=0,\u2026,k\u22121\\forall i = 0, \\dots, k-1\u2200i=0,\u2026,k\u22121  ,  stride[i]=stride[i+1]\u00d7size[i+1]\\text{stride}[i] = \\text{stride}[i+1] \\times \\text{size}[i+1]stride[i]=stride[i+1]\u00d7size[i+1]  Otherwise, contiguous() needs to be called before the tensor can be viewed. See also: reshape(), which returns a view if the shapes are compatible, and copies (equivalent to calling contiguous()) otherwise.  Parameters shape (torch.Size or python:int...) \u2013 the desired size   Example: &gt;&gt;&gt; x = torch.randn(4, 4) &gt;&gt;&gt; x.size() torch.Size([4, 4]) &gt;&gt;&gt; y = x.view(16) &gt;&gt;&gt; y.size() torch.Size([16]) &gt;&gt;&gt; z = x.view(-1, 8)  # the size -1 is inferred from other dimensions &gt;&gt;&gt; z.size() torch.Size([2, 8])  &gt;&gt;&gt; a = torch.randn(1, 2, 3, 4) &gt;&gt;&gt; a.size() torch.Size([1, 2, 3, 4]) &gt;&gt;&gt; b = a.transpose(1, 2)  # Swaps 2nd and 3rd dimension &gt;&gt;&gt; b.size() torch.Size([1, 3, 2, 4]) &gt;&gt;&gt; c = a.view(1, 3, 2, 4)  # Does not change tensor layout in memory &gt;&gt;&gt; c.size() torch.Size([1, 3, 2, 4]) &gt;&gt;&gt; torch.equal(b, c) False       view_as(other) \u2192 Tensor View this tensor as the same size as other. self.view_as(other) is equivalent to self.view(other.size()). Please see view() for more information about view.  Parameters other (torch.Tensor) \u2013 The result tensor has the same size as other.       where(condition, y) \u2192 Tensor self.where(condition, y) is equivalent to torch.where(condition, self, y). See torch.where()     zero_() \u2192 Tensor Fills self tensor with zeros.   ", "parameters": ["data (array_like) : The returned Tensor copies data.", "dtype (torch.dtype, optional) : the desired type of returned tensor.Default: if None, same torch.dtype as this tensor.", "device (torch.device, optional) : the desired device of returned tensor.Default: if None, same torch.device as this tensor.", "requires_grad (bool, optional) : If autograd should record operations on thereturned tensor. Default: False.", "fill_value (scalar) : the number to fill the output tensor with.", "dtype (torch.dtype, optional) : the desired type of returned tensor.Default: if None, same torch.dtype as this tensor.", "device (torch.device, optional) : the desired device of returned tensor.Default: if None, same torch.device as this tensor.", "requires_grad (bool, optional) : If autograd should record operations on thereturned tensor. Default: False.", "dtype (torch.dtype, optional) : the desired type of returned tensor.Default: if None, same torch.dtype as this tensor.", "device (torch.device, optional) : the desired device of returned tensor.Default: if None, same torch.device as this tensor.", "requires_grad (bool, optional) : If autograd should record operations on thereturned tensor. Default: False.", "size (python:int...) : a list, tuple, or torch.Size of integers defining theshape of the output tensor.", "dtype (torch.dtype, optional) : the desired type of returned tensor.Default: if None, same torch.dtype as this tensor.", "device (torch.device, optional) : the desired device of returned tensor.Default: if None, same torch.device as this tensor.", "requires_grad (bool, optional) : If autograd should record operations on thereturned tensor. Default: False.", "size (python:int...) : a list, tuple, or torch.Size of integers defining theshape of the output tensor.", "dtype (torch.dtype, optional) : the desired type of returned tensor.Default: if None, same torch.dtype as this tensor.", "device (torch.device, optional) : the desired device of returned tensor.Default: if None, same torch.device as this tensor.", "requires_grad (bool, optional) : If autograd should record operations on thereturned tensor. Default: False.", "gradient (Tensor or None) : Gradient w.r.t. thetensor. If it is a tensor, it will be automatically convertedto a Tensor that does not require grad unless create_graph is True.None values can be specified for scalar Tensors or ones thatdon\u2019t require grad. If a None value would be acceptable thenthis argument is optional.", "retain_graph (bool, optional) : If False, the graph used to computethe grads will be freed. Note that in nearly all cases settingthis option to True is not needed and often can be worked aroundin a much more efficient way. Defaults to the value ofcreate_graph.", "create_graph (bool, optional) : If True, graph of the derivative willbe constructed, allowing to compute higher order derivativeproducts. Defaults to False.", "src (Tensor) : the source tensor to copy from", "non_blocking (bool) : if True and this copy is between CPU and GPU,the copy may occur asynchronously with respect to the host. For othercases, this argument has no effect.", "device (torch.device) : The destination GPU device.Defaults to the current CUDA device.", "non_blocking (bool) : If True and the source is in pinned memory,the copy will be asynchronous with respect to the host.Otherwise, the argument has no effect. Default: False.", "fill_value (Scalar) : the fill value", "wrap (bool) : the diagonal \u2018wrapped\u2019 after N columns for tall matrices.", "dim (python:int) : dimension along which to index", "index (LongTensor) : indices of tensor to select from", "tensor (Tensor) : the tensor containing values to add", "dim (python:int) : dimension along which to index", "index (LongTensor) : indices of tensor to select from", "tensor (Tensor) : the tensor containing values to copy", "dim (python:int) : dimension along which to index", "index (LongTensor) : indices of self tensor to fill in", "val (python:float) : the value to fill with", "indices (tuple of LongTensor) : tensors used to index into self.", "value (Tensor) : tensor of same dtype as self.", "accumulate (bool) : whether to accumulate into self", "mask (BoolTensor) : the boolean mask", "source (Tensor) : the tensor to copy from", "mask (BoolTensor) : the boolean mask", "value (python:float) : the value to fill in with", "indices (LongTensor) : the indices into self", "tensor (Tensor) : the tensor containing values to copy from", "accumulate (bool) : whether to accumulate into self", "dim (python:int) : the axis along which to index", "index (LongTensor) : the indices of elements to scatter,can be either empty or the same size of src.When empty, the operation returns identity", "src (Tensor) : the source element(s) to scatter,incase value is not specified", "value (python:float) : the source element(s) to scatter,incase src is not specified", "dim (python:int) : the axis along which to index", "index (LongTensor) : the indices of elements to scatter and add,can be either empty or the same size of src.When empty, the operation returns identity.", "other (Tensor) : the source elements to scatter and add", "dim (python:int) : the dimension to slice", "index (python:int) : the index to select with", "source (Tensor or Storage) : the tensor or storage to use", "storage_offset (python:int, optional) : the offset in the storage", "size (torch.Size, optional) : the desired size. Defaults to the size of the source.", "stride (tuple, optional) : the desired stride. Defaults to C-contiguous strides.", "input (Tensor) : an input Tensor", "mask (SparseTensor) : a SparseTensor which we filter input based on its indices", "dtype (python:type or string) : The desired type", "non_blocking (bool) : If True, and the source is in pinned memoryand destination is on the GPU or vice versa, the copy is performedasynchronously with respect to the host. Otherwise, the argumenthas no effect.", "**kwargs : For compatibility, may contain the key async in place ofthe non_blocking argument. The async arg is deprecated.", "dimension (python:int) : dimension in which unfolding happens", "size (python:int) : the size of each slice that is unfolded", "step (python:int) : the step between each slice"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.prod(input,dim,keepdim=False,dtype=None)\u2192Tensor", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.std", "item_type": "function", "code": "torch.std()", "description": "  torch.std(input, unbiased=True) \u2192 Tensor   Returns the standard-deviation of all elements in the input tensor. If unbiased is False, then the standard-deviation will be calculated via the biased estimator. Otherwise, Bessel\u2019s correction will be used.  Parameters  input (Tensor) \u2013 the input tensor. unbiased (bool) \u2013 whether to use the unbiased estimation or not    Example: &gt;&gt;&gt; a = torch.randn(1, 3) &gt;&gt;&gt; a tensor([[-0.8166, -1.3802, -0.3560]]) &gt;&gt;&gt; torch.std(a) tensor(0.5130)     torch.std(input, dim, keepdim=False, unbiased=True, out=None) \u2192 Tensor   Returns the standard-deviation of each row of the input tensor in the dimension dim. If dim is a list of dimensions, reduce over all of them. If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s). If unbiased is False, then the standard-deviation will be calculated via the biased estimator. Otherwise, Bessel\u2019s correction will be used.  Parameters  input (Tensor) \u2013 the input tensor. dim (python:int or tuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not. unbiased (bool) \u2013 whether to use the unbiased estimation or not out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; a = torch.randn(4, 4) &gt;&gt;&gt; a tensor([[ 0.2035,  1.2959,  1.8101, -0.4644],         [ 1.5027, -0.3270,  0.5905,  0.6538],         [-1.5745,  1.3330, -0.5596, -0.6548],         [ 0.1264, -0.5080,  1.6420,  0.1992]]) &gt;&gt;&gt; torch.std(a, dim=1) tensor([ 1.0311,  0.7477,  1.2204,  0.9087])   ", "parameters": ["input (Tensor) : the input tensor.", "unbiased (bool) : whether to use the unbiased estimation or not", "input (Tensor) : the input tensor.", "dim (python:int or tuple of python:ints) : the dimension or dimensions to reduce.", "keepdim (bool) : whether the output tensor has dim retained or not.", "unbiased (bool) : whether to use the unbiased estimation or not", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " a = torch.randn(1, 3)\n a\ntensor([[-0.8166, -1.3802, -0.3560]])\n torch.std(a)\ntensor(0.5130)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.std(input,unbiased=True)\u2192Tensor", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.std(input,dim,keepdim=False,unbiased=True,out=None)\u2192Tensor", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.std_mean", "item_type": "function", "code": "torch.std_mean()", "description": "  torch.std_mean(input, unbiased=True) -&gt; (Tensor, Tensor)   Returns the standard-deviation and mean of all elements in the input tensor. If unbiased is False, then the standard-deviation will be calculated via the biased estimator. Otherwise, Bessel\u2019s correction will be used.  Parameters  input (Tensor) \u2013 the input tensor. unbiased (bool) \u2013 whether to use the unbiased estimation or not    Example: &gt;&gt;&gt; a = torch.randn(1, 3) &gt;&gt;&gt; a tensor([[0.3364, 0.3591, 0.9462]]) &gt;&gt;&gt; torch.std_mean(a) (tensor(0.3457), tensor(0.5472))     torch.std(input, dim, keepdim=False, unbiased=True) -&gt; (Tensor, Tensor)   Returns the standard-deviation and mean of each row of the input tensor in the dimension dim. If dim is a list of dimensions, reduce over all of them. If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s). If unbiased is False, then the standard-deviation will be calculated via the biased estimator. Otherwise, Bessel\u2019s correction will be used.  Parameters  input (Tensor) \u2013 the input tensor. dim (python:int or tuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not. unbiased (bool) \u2013 whether to use the unbiased estimation or not    Example: &gt;&gt;&gt; a = torch.randn(4, 4) &gt;&gt;&gt; a tensor([[ 0.5648, -0.5984, -1.2676, -1.4471],         [ 0.9267,  1.0612,  1.1050, -0.6014],         [ 0.0154,  1.9301,  0.0125, -1.0904],         [-1.9711, -0.7748, -1.3840,  0.5067]]) &gt;&gt;&gt; torch.std_mean(a, 1) (tensor([0.9110, 0.8197, 1.2552, 1.0608]), tensor([-0.6871,  0.6229,  0.2169, -0.9058]))   ", "parameters": ["input (Tensor) : the input tensor.", "unbiased (bool) : whether to use the unbiased estimation or not", "input (Tensor) : the input tensor.", "dim (python:int or tuple of python:ints) : the dimension or dimensions to reduce.", "keepdim (bool) : whether the output tensor has dim retained or not.", "unbiased (bool) : whether to use the unbiased estimation or not"], "returns": null, "example": " a = torch.randn(1, 3)\n a\ntensor([[0.3364, 0.3591, 0.9462]])\n torch.std_mean(a)\n(tensor(0.3457), tensor(0.5472))\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.has_rsample", "item_type": "attribute", "code": "has_rsample=True", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.support", "item_type": "attribute", "code": "support=Simplex()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.studentT.StudentT", "item_type": "class", "code": "classtorch.distributions.studentT.StudentT(df,loc=0.0,scale=1.0,validate_args=None)", "description": "Bases: torch.distributions.distribution.Distribution Creates a Student\u2019s t-distribution parameterized by degree of freedom df, mean loc and scale scale. Example: &gt;&gt;&gt; m = StudentT(torch.tensor([2.0])) &gt;&gt;&gt; m.sample()  # Student's t-distributed with degrees of freedom=2 tensor([ 0.1046])    Parameters  df (python:float or Tensor) \u2013 degrees of freedom loc (python:float or Tensor) \u2013 mean of the distribution scale (python:float or Tensor) \u2013 scale of the distribution      arg_constraints = {'df': GreaterThan(lower_bound=0.0), 'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)}     entropy()      expand(batch_shape, _instance=None)      has_rsample = True     log_prob(value)      property mean     rsample(sample_shape=torch.Size([]))      support = Real()     property variance   ", "parameters": ["df (python:float or Tensor) : degrees of freedom", "loc (python:float or Tensor) : mean of the distribution", "scale (python:float or Tensor) : scale of the distribution"], "returns": null, "example": " m = StudentT(torch.tensor([2.0]))\n m.sample()  # Student's t-distributed with degrees of freedom=2\ntensor([ 0.1046])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.studentT.StudentT.arg_constraints", "item_type": "attribute", "code": "arg_constraints={'df':GreaterThan(lower_bound=0.0),'loc':Real(),'scale':GreaterThan(lower_bound=0.0)}", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.studentT.StudentT.has_rsample", "item_type": "attribute", "code": "has_rsample=True", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.studentT.StudentT.support", "item_type": "attribute", "code": "support=Real()", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.transformed_distribution.TransformedDistribution", "item_type": "class", "code": "classtorch.distributions.transformed_distribution.TransformedDistribution(base_distribution,transforms,validate_args=None)", "description": "Bases: torch.distributions.distribution.Distribution Extension of the Distribution class, which applies a sequence of Transforms to a base distribution.  Let f be the composition of transforms applied: X ~ BaseDistribution Y = f(X) ~ TransformedDistribution(BaseDistribution, f) log p(Y) = log p(X) + log |det (dX/dY)|   Note that the .event_shape of a TransformedDistribution is the maximum shape of its base distribution and its transforms, since transforms can introduce correlations among events. An example for the usage of TransformedDistribution would be: # Building a Logistic Distribution # X ~ Uniform(0, 1) # f = a + b * logit(X) # Y ~ f(X) ~ Logistic(a, b) base_distribution = Uniform(0, 1) transforms = [SigmoidTransform().inv, AffineTransform(loc=a, scale=b)] logistic = TransformedDistribution(base_distribution, transforms)   For more examples, please look at the implementations of Gumbel, HalfCauchy, HalfNormal, LogNormal, Pareto, Weibull, RelaxedBernoulli and RelaxedOneHotCategorical   arg_constraints = {}     cdf(value)  Computes the cumulative distribution function by inverting the transform(s) and computing the score of the base distribution.     expand(batch_shape, _instance=None)      property has_rsample     icdf(value)  Computes the inverse cumulative distribution function using transform(s) and computing the score of the base distribution.     log_prob(value)  Scores the sample by inverting the transform(s) and computing the score using the score of the base distribution and the log abs det jacobian.     rsample(sample_shape=torch.Size([]))  Generates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched. Samples first from base distribution and applies transform() for every transform in the list.     sample(sample_shape=torch.Size([]))  Generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched. Samples first from base distribution and applies transform() for every transform in the list.     property support   ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.MaxUnpool3d", "item_type": "class", "code": "classtorch.nn.MaxUnpool3d(kernel_size,stride=None,padding=0)", "description": "Computes a partial inverse of MaxPool3d. MaxPool3d is not fully invertible, since the non-maximal values are lost. MaxUnpool3d takes in as input the output of MaxPool3d including the indices of the maximal values and computes a partial inverse in which all non-maximal values are set to zero.  Note MaxPool3d can map several input sizes to the same output sizes. Hence, the inversion process can get ambiguous. To accommodate this, you can provide the needed output size as an additional argument output_size in the forward call. See the Inputs section below.   Parameters  kernel_size (python:int or tuple) \u2013 Size of the max pooling window. stride (python:int or tuple) \u2013 Stride of the max pooling window. It is set to kernel_size by default. padding (python:int or tuple) \u2013 Padding that was added to the input     Inputs: input: the input Tensor to invert indices: the indices given out by MaxPool3d output_size (optional): the targeted output size   Shape: Input: (N,C,Din,Hin,Win)(N, C, D_{in}, H_{in}, W_{in})(N,C,Din\u200b,Hin\u200b,Win\u200b)   Output: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\u200b,Hout\u200b,Wout\u200b)  , where  Dout=(Din\u22121)\u00d7stride[0]\u22122\u00d7padding[0]+kernel_size[0]D_{out} = (D_{in} - 1) \\times \\text{stride[0]} - 2 \\times \\text{padding[0]} + \\text{kernel\\_size[0]}  Dout\u200b=(Din\u200b\u22121)\u00d7stride[0]\u22122\u00d7padding[0]+kernel_size[0]   Hout=(Hin\u22121)\u00d7stride[1]\u22122\u00d7padding[1]+kernel_size[1]H_{out} = (H_{in} - 1) \\times \\text{stride[1]} - 2 \\times \\text{padding[1]} + \\text{kernel\\_size[1]}  Hout\u200b=(Hin\u200b\u22121)\u00d7stride[1]\u22122\u00d7padding[1]+kernel_size[1]   Wout=(Win\u22121)\u00d7stride[2]\u22122\u00d7padding[2]+kernel_size[2]W_{out} = (W_{in} - 1) \\times \\text{stride[2]} - 2 \\times \\text{padding[2]} + \\text{kernel\\_size[2]}  Wout\u200b=(Win\u200b\u22121)\u00d7stride[2]\u22122\u00d7padding[2]+kernel_size[2]  or as given by output_size in the call operator     Example: &gt;&gt;&gt; # pool of square window of size=3, stride=2 &gt;&gt;&gt; pool = nn.MaxPool3d(3, stride=2, return_indices=True) &gt;&gt;&gt; unpool = nn.MaxUnpool3d(3, stride=2) &gt;&gt;&gt; output, indices = pool(torch.randn(20, 16, 51, 33, 15)) &gt;&gt;&gt; unpooled_output = unpool(output, indices) &gt;&gt;&gt; unpooled_output.size() torch.Size([20, 16, 51, 33, 15])   ", "parameters": ["kernel_size (python:int or tuple) : Size of the max pooling window.", "stride (python:int or tuple) : Stride of the max pooling window.It is set to kernel_size by default.", "padding (python:int or tuple) : Padding that was added to the input"], "returns": null, "example": " # pool of square window of size=3, stride=2\n pool = nn.MaxPool3d(3, stride=2, return_indices=True)\n unpool = nn.MaxUnpool3d(3, stride=2)\n output, indices = pool(torch.randn(20, 16, 51, 33, 15))\n unpooled_output = unpool(output, indices)\n unpooled_output.size()\ntorch.Size([20, 16, 51, 33, 15])\n\n", "shape": " Input: (N,C,Din,Hin,Win)(N, C, D_{in}, H_{in}, W_{in})(N,C,Din\u200b,Hin\u200b,Win\u200b)   Output: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\u200b,Hout\u200b,Wout\u200b)  , where  Dout=(Din\u22121)\u00d7stride[0]\u22122\u00d7padding[0]+kernel_size[0]D_{out} = (D_{in} - 1) \\times \\text{stride[0]} - 2 \\times \\text{padding[0]} + \\text{kernel\\_size[0]}  Dout\u200b=(Din\u200b\u22121)\u00d7stride[0]\u22122\u00d7padding[0]+kernel_size[0]   Hout=(Hin\u22121)\u00d7stride[1]\u22122\u00d7padding[1]+kernel_size[1]H_{out} = (H_{in} - 1) \\times \\text{stride[1]} - 2 \\times \\text{padding[1]} + \\text{kernel\\_size[1]}  Hout\u200b=(Hin\u200b\u22121)\u00d7stride[1]\u22122\u00d7padding[1]+kernel_size[1]   Wout=(Win\u22121)\u00d7stride[2]\u22122\u00d7padding[2]+kernel_size[2]W_{out} = (W_{in} - 1) \\times \\text{stride[2]} - 2 \\times \\text{padding[2]} + \\text{kernel\\_size[2]}  Wout\u200b=(Win\u200b\u22121)\u00d7stride[2]\u22122\u00d7padding[2]+kernel_size[2]  or as given by output_size in the call operator   "},
{"library": "torch", "item_id": "torch.nn.AvgPool1d", "item_type": "class", "code": "classtorch.nn.AvgPool1d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)", "description": "Applies a 1D average pooling over an input signal composed of several input planes. In the simplest case, the output value of the layer with input size (N,C,L)(N, C, L)(N,C,L)  , output (N,C,Lout)(N, C, L_{out})(N,C,Lout\u200b)   and kernel_size kkk   can be precisely described as:  out(Ni,Cj,l)=1k\u2211m=0k\u22121input(Ni,Cj,stride\u00d7l+m)\\text{out}(N_i, C_j, l) = \\frac{1}{k} \\sum_{m=0}^{k-1}                        \\text{input}(N_i, C_j, \\text{stride} \\times l + m)out(Ni\u200b,Cj\u200b,l)=k1\u200bm=0\u2211k\u22121\u200binput(Ni\u200b,Cj\u200b,stride\u00d7l+m)  If padding is non-zero, then the input is implicitly zero-padded on both sides for padding number of points. The parameters kernel_size, stride, padding can each be an int or a one-element tuple.  Parameters  kernel_size \u2013 the size of the window stride \u2013 the stride of the window. Default value is kernel_size padding \u2013 implicit zero padding to be added on both sides ceil_mode \u2013 when True, will use ceil instead of floor to compute the output shape count_include_pad \u2013 when True, will include the zero-padding in the averaging calculation     Shape: Input: (N,C,Lin)(N, C, L_{in})(N,C,Lin\u200b)   Output: (N,C,Lout)(N, C, L_{out})(N,C,Lout\u200b)  , where  Lout=\u230aLin+2\u00d7padding\u2212kernel_sizestride+1\u230bL_{out} = \\left\\lfloor \\frac{L_{in} + 2 \\times \\text{padding} - \\text{kernel\\_size}}{\\text{stride}} + 1\\right\\rfloor  Lout\u200b=\u230astrideLin\u200b+2\u00d7padding\u2212kernel_size\u200b+1\u230b      Examples: &gt;&gt;&gt; # pool with window of size=3, stride=2 &gt;&gt;&gt; m = nn.AvgPool1d(3, stride=2) &gt;&gt;&gt; m(torch.tensor([[[1.,2,3,4,5,6,7]]])) tensor([[[ 2.,  4.,  6.]]])   ", "parameters": ["kernel_size : the size of the window", "stride : the stride of the window. Default value is kernel_size", "padding : implicit zero padding to be added on both sides", "ceil_mode : when True, will use ceil instead of floor to compute the output shape", "count_include_pad : when True, will include the zero-padding in the averaging calculation"], "returns": null, "example": " # pool with window of size=3, stride=2\n m = nn.AvgPool1d(3, stride=2)\n m(torch.tensor([[[1.,2,3,4,5,6,7]]]))\ntensor([[[ 2.,  4.,  6.]]])\n\n", "shape": " Input: (N,C,Lin)(N, C, L_{in})(N,C,Lin\u200b)   Output: (N,C,Lout)(N, C, L_{out})(N,C,Lout\u200b)  , where  Lout=\u230aLin+2\u00d7padding\u2212kernel_sizestride+1\u230bL_{out} = \\left\\lfloor \\frac{L_{in} + 2 \\times \\text{padding} - \\text{kernel\\_size}}{\\text{stride}} + 1\\right\\rfloor  Lout\u200b=\u230astrideLin\u200b+2\u00d7padding\u2212kernel_size\u200b+1\u230b    "},
{"library": "torch", "item_id": "torch.Tensor.is_cuda", "item_type": "attribute", "code": "is_cuda", "description": "Is True if the Tensor is stored on the GPU, False otherwise. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.device", "item_type": "attribute", "code": "device", "description": "Is the torch.device where this Tensor is. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "attribute", "code": "grad", "description": "This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. The attribute will then contain the gradients computed and future calls to backward() will accumulate (add) gradients into it. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.ndim", "item_type": "attribute", "code": "ndim", "description": "Alias for dim() ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.T", "item_type": "attribute", "code": "T", "description": "Is this Tensor with its dimensions reversed. If n is the number of dimensions in x, x.T is equivalent to x.permute(n-1, n-2, ..., 0). ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "attribute", "code": "is_leaf", "description": "All Tensors that have requires_grad which is False will be leaf Tensors by convention. For Tensors that have requires_grad which is True, they will be leaf Tensors if they were created by the user. This means that they are not the result of an operation and so grad_fn is None. Only leaf Tensors will have their grad populated during a call to backward(). To get grad populated for non-leaf Tensors, you can use retain_grad(). Example: &gt;&gt;&gt; a = torch.rand(10, requires_grad=True) &gt;&gt;&gt; a.is_leaf True &gt;&gt;&gt; b = torch.rand(10, requires_grad=True).cuda() &gt;&gt;&gt; b.is_leaf False # b was created by the operation that cast a cpu Tensor into a cuda Tensor &gt;&gt;&gt; c = torch.rand(10, requires_grad=True) + 2 &gt;&gt;&gt; c.is_leaf False # c was created by the addition operation &gt;&gt;&gt; d = torch.rand(10).cuda() &gt;&gt;&gt; d.is_leaf True # d does not require gradients and so has no operation creating it (that is tracked by the autograd engine) &gt;&gt;&gt; e = torch.rand(10).cuda().requires_grad_() &gt;&gt;&gt; e.is_leaf True # e requires gradients and has no operations creating it &gt;&gt;&gt; f = torch.rand(10, requires_grad=True, device=\"cuda\") &gt;&gt;&gt; f.is_leaf True # f requires grad, has no operation creating it   ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.Tensor.is_sparse", "item_type": "attribute", "code": "is_sparse", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "attribute", "code": "requires_grad", "description": "Is True if gradients need to be computed for this Tensor, False otherwise.  Note The fact that gradients need to be computed for a Tensor do not mean that the grad attribute will be populated, see is_leaf for more details.  ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.BoolTensor", "item_type": "class", "code": "classtorch.BoolTensor", "description": "The following methods are unique to torch.BoolTensor.   all()   all() \u2192 bool   Returns True if all elements in the tensor are True, False otherwise. Example: &gt;&gt;&gt; a = torch.rand(1, 2).bool() &gt;&gt;&gt; a tensor([[False, True]], dtype=torch.bool) &gt;&gt;&gt; a.all() tensor(False, dtype=torch.bool)     all(dim, keepdim=False, out=None) \u2192 Tensor   Returns True if all elements in each row of the tensor in the given dimension dim are True, False otherwise. If keepdim is True, the output tensor is of the same size as input except in the dimension dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 fewer dimension than input.  Parameters  dim (python:int) \u2013 the dimension to reduce keepdim (bool) \u2013 whether the output tensor has dim retained or not out (Tensor, optional) \u2013 the output tensor    Example: &gt;&gt;&gt; a = torch.rand(4, 2).bool() &gt;&gt;&gt; a tensor([[True, True],         [True, False],         [True, True],         [True, True]], dtype=torch.bool) &gt;&gt;&gt; a.all(dim=1) tensor([ True, False,  True,  True], dtype=torch.bool) &gt;&gt;&gt; a.all(dim=0) tensor([ True, False], dtype=torch.bool)       any()   any() \u2192 bool   Returns True if any elements in the tensor are True, False otherwise. Example: &gt;&gt;&gt; a = torch.rand(1, 2).bool() &gt;&gt;&gt; a tensor([[False, True]], dtype=torch.bool) &gt;&gt;&gt; a.any() tensor(True, dtype=torch.bool)     any(dim, keepdim=False, out=None) \u2192 Tensor   Returns True if any elements in each row of the tensor in the given dimension dim are True, False otherwise. If keepdim is True, the output tensor is of the same size as input except in the dimension dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 fewer dimension than input.  Parameters  dim (python:int) \u2013 the dimension to reduce keepdim (bool) \u2013 whether the output tensor has dim retained or not out (Tensor, optional) \u2013 the output tensor    Example: &gt;&gt;&gt; a = torch.randn(4, 2) &lt; 0 &gt;&gt;&gt; a tensor([[ True,  True],         [False,  True],         [ True,  True],         [False, False]]) &gt;&gt;&gt; a.any(1) tensor([ True,  True,  True, False]) &gt;&gt;&gt; a.any(0) tensor([True, True])     ", "parameters": ["dim (python:int) : the dimension to reduce", "keepdim (bool) : whether the output tensor has dim retained or not", "out (Tensor, optional) : the output tensor", "dim (python:int) : the dimension to reduce", "keepdim (bool) : whether the output tensor has dim retained or not", "out (Tensor, optional) : the output tensor"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.std_mean(input,unbiased=True)-&gt;(Tensor,Tensor)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.std(input,dim,keepdim=False,unbiased=True)-&gt;(Tensor,Tensor)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.sum", "item_type": "function", "code": "torch.sum()", "description": "  torch.sum(input, dtype=None) \u2192 Tensor   Returns the sum of all elements in the input tensor.  Parameters  input (Tensor) \u2013 the input tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None.    Example: &gt;&gt;&gt; a = torch.randn(1, 3) &gt;&gt;&gt; a tensor([[ 0.1133, -0.9567,  0.2958]]) &gt;&gt;&gt; torch.sum(a) tensor(-0.5475)     torch.sum(input, dim, keepdim=False, dtype=None) \u2192 Tensor   Returns the sum of each row of the input tensor in the given dimension dim. If dim is a list of dimensions, reduce over all of them. If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s).  Parameters  input (Tensor) \u2013 the input tensor. dim (python:int or tuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None.    Example: &gt;&gt;&gt; a = torch.randn(4, 4) &gt;&gt;&gt; a tensor([[ 0.0569, -0.2475,  0.0737, -0.3429],         [-0.2993,  0.9138,  0.9337, -1.6864],         [ 0.1132,  0.7892, -0.1003,  0.5688],         [ 0.3637, -0.9906, -0.4752, -1.5197]]) &gt;&gt;&gt; torch.sum(a, 1) tensor([-0.4598, -0.1381,  1.3708, -2.6217]) &gt;&gt;&gt; b = torch.arange(4 * 5 * 6).view(4, 5, 6) &gt;&gt;&gt; torch.sum(b, (2, 1)) tensor([  435.,  1335.,  2235.,  3135.])   ", "parameters": ["input (Tensor) : the input tensor.", "dtype (torch.dtype, optional) : the desired data type of returned tensor.If specified, the input tensor is casted to dtype before the operationis performed. This is useful for preventing data type overflows. Default: None.", "input (Tensor) : the input tensor.", "dim (python:int or tuple of python:ints) : the dimension or dimensions to reduce.", "keepdim (bool) : whether the output tensor has dim retained or not.", "dtype (torch.dtype, optional) : the desired data type of returned tensor.If specified, the input tensor is casted to dtype before the operationis performed. This is useful for preventing data type overflows. Default: None."], "returns": null, "example": " a = torch.randn(1, 3)\n a\ntensor([[ 0.1133, -0.9567,  0.2958]])\n torch.sum(a)\ntensor(-0.5475)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.sum(input,dtype=None)\u2192Tensor", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.sum(input,dim,keepdim=False,dtype=None)\u2192Tensor", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.unique", "item_type": "function", "code": "torch.unique(input,sorted=True,return_inverse=False,return_counts=False,dim=None)", "description": "Returns the unique elements of the input tensor.  Note This function is different from torch.unique_consecutive() in the sense that this function also eliminates non-consecutive duplicate values.   Note Currently in the CUDA implementation and the CPU implementation when dim is specified, torch.unique always sort the tensor at the beginning regardless of the sort argument. Sorting could be slow, so if your input tensor is already sorted, it is recommended to use torch.unique_consecutive() which avoids the sorting.   Parameters  input (Tensor) \u2013 the input tensor sorted (bool) \u2013 Whether to sort the unique elements in ascending order before returning as output. return_inverse (bool) \u2013 Whether to also return the indices for where elements in the original input ended up in the returned unique list. return_counts (bool) \u2013 Whether to also return the counts for each unique element. dim (python:int) \u2013 the dimension to apply unique. If None, the unique of the flattened input is returned. default: None   Returns A tensor or a tuple of tensors containing   output (Tensor): the output list of unique scalar elements. inverse_indices (Tensor): (optional) if return_inverse is True, there will be an additional returned tensor (same shape as input) representing the indices for where elements in the original input map to in the output; otherwise, this function will only return a single tensor. counts (Tensor): (optional) if return_counts is True, there will be an additional returned tensor (same shape as output or output.size(dim), if dim was specified) representing the number of occurrences for each unique value or tensor.     Return type (Tensor, Tensor (optional), Tensor (optional))   Example: &gt;&gt;&gt; output = torch.unique(torch.tensor([1, 3, 2, 3], dtype=torch.long)) &gt;&gt;&gt; output tensor([ 2,  3,  1])  &gt;&gt;&gt; output, inverse_indices = torch.unique(         torch.tensor([1, 3, 2, 3], dtype=torch.long), sorted=True, return_inverse=True) &gt;&gt;&gt; output tensor([ 1,  2,  3]) &gt;&gt;&gt; inverse_indices tensor([ 0,  2,  1,  2])  &gt;&gt;&gt; output, inverse_indices = torch.unique(         torch.tensor([[1, 3], [2, 3]], dtype=torch.long), sorted=True, return_inverse=True) &gt;&gt;&gt; output tensor([ 1,  2,  3]) &gt;&gt;&gt; inverse_indices tensor([[ 0,  2],         [ 1,  2]])   ", "parameters": ["input (Tensor) : the input tensor", "sorted (bool) : Whether to sort the unique elements in ascending orderbefore returning as output.", "return_inverse (bool) : Whether to also return the indices for whereelements in the original input ended up in the returned unique list.", "return_counts (bool) : Whether to also return the counts for each uniqueelement.", "dim (python:int) : the dimension to apply unique. If None, the unique of theflattened input is returned. default: None", "output (Tensor): the output list of unique scalar elements.", "inverse_indices (Tensor): (optional) ifreturn_inverse is True, there will be an additionalreturned tensor (same shape as input) representing the indicesfor where elements in the original input map to in the output;otherwise, this function will only return a single tensor.", "counts (Tensor): (optional) ifreturn_counts is True, there will be an additionalreturned tensor (same shape as output or output.size(dim),if dim was specified) representing the number of occurrencesfor each unique value or tensor."], "returns": "A tensor or a tuple of tensors containingoutput (Tensor): the output list of unique scalar elements.inverse_indices (Tensor): (optional) ifreturn_inverse is True, there will be an additionalreturned tensor (same shape as input) representing the indicesfor where elements in the original input map to in the output;otherwise, this function will only return a single tensor.counts (Tensor): (optional) ifreturn_counts is True, there will be an additionalreturned tensor (same shape as output or output.size(dim),if dim was specified) representing the number of occurrencesfor each unique value or tensor.", "example": " output = torch.unique(torch.tensor([1, 3, 2, 3], dtype=torch.long))\n output\ntensor([ 2,  3,  1])\n\n output, inverse_indices = torch.unique(\n        torch.tensor([1, 3, 2, 3], dtype=torch.long), sorted=True, return_inverse=True)\n output\ntensor([ 1,  2,  3])\n inverse_indices\ntensor([ 0,  2,  1,  2])\n\n output, inverse_indices = torch.unique(\n        torch.tensor([[1, 3], [2, 3]], dtype=torch.long), sorted=True, return_inverse=True)\n output\ntensor([ 1,  2,  3])\n inverse_indices\ntensor([[ 0,  2],\n        [ 1,  2]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.transformed_distribution.TransformedDistribution.arg_constraints", "item_type": "attribute", "code": "arg_constraints={}", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.uniform.Uniform", "item_type": "class", "code": "classtorch.distributions.uniform.Uniform(low,high,validate_args=None)", "description": "Bases: torch.distributions.distribution.Distribution Generates uniformly distributed random samples from the half-open interval [low, high). Example: &gt;&gt;&gt; m = Uniform(torch.tensor([0.0]), torch.tensor([5.0])) &gt;&gt;&gt; m.sample()  # uniformly distributed in the range [0.0, 5.0) tensor([ 2.3418])    Parameters  low (python:float or Tensor) \u2013 lower range (inclusive). high (python:float or Tensor) \u2013 upper range (exclusive).      arg_constraints = {'high': Dependent(), 'low': Dependent()}     cdf(value)      entropy()      expand(batch_shape, _instance=None)      has_rsample = True     icdf(value)      log_prob(value)      property mean     rsample(sample_shape=torch.Size([]))      property stddev     property support     property variance   ", "parameters": ["low (python:float or Tensor) : lower range (inclusive).", "high (python:float or Tensor) : upper range (exclusive)."], "returns": null, "example": " m = Uniform(torch.tensor([0.0]), torch.tensor([5.0]))\n m.sample()  # uniformly distributed in the range [0.0, 5.0)\ntensor([ 2.3418])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.uniform.Uniform.arg_constraints", "item_type": "attribute", "code": "arg_constraints={'high':Dependent(),'low':Dependent()}", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.uniform.Uniform.has_rsample", "item_type": "attribute", "code": "has_rsample=True", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.weibull.Weibull", "item_type": "class", "code": "classtorch.distributions.weibull.Weibull(scale,concentration,validate_args=None)", "description": "Bases: torch.distributions.transformed_distribution.TransformedDistribution Samples from a two-parameter Weibull distribution. Example &gt;&gt;&gt; m = Weibull(torch.tensor([1.0]), torch.tensor([1.0])) &gt;&gt;&gt; m.sample()  # sample from a Weibull distribution with scale=1, concentration=1 tensor([ 0.4784])    Parameters  scale (python:float or Tensor) \u2013 Scale parameter of distribution (lambda). concentration (python:float or Tensor) \u2013 Concentration parameter of distribution (k/shape).      arg_constraints = {'concentration': GreaterThan(lower_bound=0.0), 'scale': GreaterThan(lower_bound=0.0)}     entropy()      expand(batch_shape, _instance=None)      property mean     support = GreaterThan(lower_bound=0.0)     property variance   ", "parameters": ["scale (python:float or Tensor) : Scale parameter of distribution (lambda).", "concentration (python:float or Tensor) : Concentration parameter of distribution (k/shape)."], "returns": null, "example": " m = Weibull(torch.tensor([1.0]), torch.tensor([1.0]))\n m.sample()  # sample from a Weibull distribution with scale=1, concentration=1\ntensor([ 0.4784])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.weibull.Weibull.arg_constraints", "item_type": "attribute", "code": "arg_constraints={'concentration':GreaterThan(lower_bound=0.0),'scale':GreaterThan(lower_bound=0.0)}", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.weibull.Weibull.support", "item_type": "attribute", "code": "support=GreaterThan(lower_bound=0.0)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.transforms.Transform", "item_type": "class", "code": "classtorch.distributions.transforms.Transform(cache_size=0)", "description": "Abstract class for invertable transformations with computable log det jacobians. They are primarily used in torch.distributions.TransformedDistribution. Caching is useful for tranforms whose inverses are either expensive or numerically unstable. Note that care must be taken with memoized values since the autograd graph may be reversed. For example while the following works with or without caching: y = t(x) t.log_abs_det_jacobian(x, y).backward()  # x will receive gradients.   However the following will error when caching due to dependency reversal: y = t(x) z = t.inv(y) grad(z.sum(), [y])  # error because z is x   Derived classes should implement one or both of _call() or _inverse(). Derived classes that set bijective=True should also implement log_abs_det_jacobian().  Parameters cache_size (python:int) \u2013 Size of cache. If zero, no caching is done. If one, the latest single value is cached. Only 0 and 1 are supported.  Variables  ~Transform.domain (Constraint) \u2013 The constraint representing valid inputs to this transform. ~Transform.codomain (Constraint) \u2013 The constraint representing valid outputs to this transform which are inputs to the inverse transform. ~Transform.bijective (bool) \u2013 Whether this transform is bijective. A transform t is bijective iff t.inv(t(x)) == x and t(t.inv(y)) == y for every x in the domain and y in the codomain. Transforms that are not bijective should at least maintain the weaker pseudoinverse properties t(t.inv(t(x)) == t(x) and t.inv(t(t.inv(y))) == t.inv(y). ~Transform.sign (python:int or Tensor) \u2013 For bijective univariate transforms, this should be +1 or -1 depending on whether transform is monotone increasing or decreasing. ~Transform.event_dim (python:int) \u2013 Number of dimensions that are correlated together in the transform event_shape. This should be 0 for pointwise transforms, 1 for transforms that act jointly on vectors, 2 for transforms that act jointly on matrices, etc.      property inv Returns the inverse Transform of this transform. This should satisfy t.inv.inv is t.     property sign Returns the sign of the determinant of the Jacobian, if applicable. In general this only makes sense for bijective transforms.     log_abs_det_jacobian(x, y)  Computes the log det jacobian log |dy/dx| given input and output.   ", "parameters": ["~Transform.domain (Constraint) : The constraint representing valid inputs to this transform.", "~Transform.codomain (Constraint) : The constraint representing valid outputs to this transformwhich are inputs to the inverse transform.", "~Transform.bijective (bool) : Whether this transform is bijective. A transformt is bijective iff t.inv(t(x)) == x andt(t.inv(y)) == y for every x in the domain and y inthe codomain. Transforms that are not bijective should at leastmaintain the weaker pseudoinverse propertiest(t.inv(t(x)) == t(x) and t.inv(t(t.inv(y))) == t.inv(y).", "~Transform.sign (python:int or Tensor) : For bijective univariate transforms, thisshould be +1 or -1 depending on whether transform is monotoneincreasing or decreasing.", "~Transform.event_dim (python:int) : Number of dimensions that are correlated together inthe transform event_shape. This should be 0 for pointwisetransforms, 1 for transforms that act jointly on vectors, 2 fortransforms that act jointly on matrices, etc."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.AvgPool2d", "item_type": "class", "code": "classtorch.nn.AvgPool2d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)", "description": "Applies a 2D average pooling over an input signal composed of several input planes. In the simplest case, the output value of the layer with input size (N,C,H,W)(N, C, H, W)(N,C,H,W)  , output (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)   and kernel_size (kH,kW)(kH, kW)(kH,kW)   can be precisely described as:  out(Ni,Cj,h,w)=1kH\u2217kW\u2211m=0kH\u22121\u2211n=0kW\u22121input(Ni,Cj,stride[0]\u00d7h+m,stride[1]\u00d7w+n)out(N_i, C_j, h, w)  = \\frac{1}{kH * kW} \\sum_{m=0}^{kH-1} \\sum_{n=0}^{kW-1}                        input(N_i, C_j, stride[0] \\times h + m, stride[1] \\times w + n)out(Ni\u200b,Cj\u200b,h,w)=kH\u2217kW1\u200bm=0\u2211kH\u22121\u200bn=0\u2211kW\u22121\u200binput(Ni\u200b,Cj\u200b,stride[0]\u00d7h+m,stride[1]\u00d7w+n)  If padding is non-zero, then the input is implicitly zero-padded on both sides for padding number of points. The parameters kernel_size, stride, padding can either be:   a single int \u2013 in which case the same value is used for the height and width dimension a tuple of two ints \u2013 in which case, the first int is used for the height dimension, and the second int for the width dimension    Parameters  kernel_size \u2013 the size of the window stride \u2013 the stride of the window. Default value is kernel_size padding \u2013 implicit zero padding to be added on both sides ceil_mode \u2013 when True, will use ceil instead of floor to compute the output shape count_include_pad \u2013 when True, will include the zero-padding in the averaging calculation divisor_override \u2013 if specified, it will be used as divisor, otherwise attr:kernel_size will be used     Shape: Input: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)   Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)  , where  Hout=\u230aHin+2\u00d7padding[0]\u2212kernel_size[0]stride[0]+1\u230bH_{out} = \\left\\lfloor\\frac{H_{in}  + 2 \\times \\text{padding}[0] -   \\text{kernel\\_size}[0]}{\\text{stride}[0]} + 1\\right\\rfloor  Hout\u200b=\u230astride[0]Hin\u200b+2\u00d7padding[0]\u2212kernel_size[0]\u200b+1\u230b   Wout=\u230aWin+2\u00d7padding[1]\u2212kernel_size[1]stride[1]+1\u230bW_{out} = \\left\\lfloor\\frac{W_{in}  + 2 \\times \\text{padding}[1] -   \\text{kernel\\_size}[1]}{\\text{stride}[1]} + 1\\right\\rfloor  Wout\u200b=\u230astride[1]Win\u200b+2\u00d7padding[1]\u2212kernel_size[1]\u200b+1\u230b      Examples: &gt;&gt;&gt; # pool of square window of size=3, stride=2 &gt;&gt;&gt; m = nn.AvgPool2d(3, stride=2) &gt;&gt;&gt; # pool of non-square window &gt;&gt;&gt; m = nn.AvgPool2d((3, 2), stride=(2, 1)) &gt;&gt;&gt; input = torch.randn(20, 16, 50, 32) &gt;&gt;&gt; output = m(input)   ", "parameters": ["kernel_size : the size of the window", "stride : the stride of the window. Default value is kernel_size", "padding : implicit zero padding to be added on both sides", "ceil_mode : when True, will use ceil instead of floor to compute the output shape", "count_include_pad : when True, will include the zero-padding in the averaging calculation", "divisor_override : if specified, it will be used as divisor, otherwise attr:kernel_size will be used"], "returns": null, "example": " # pool of square window of size=3, stride=2\n m = nn.AvgPool2d(3, stride=2)\n # pool of non-square window\n m = nn.AvgPool2d((3, 2), stride=(2, 1))\n input = torch.randn(20, 16, 50, 32)\n output = m(input)\n\n", "shape": " Input: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)   Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)  , where  Hout=\u230aHin+2\u00d7padding[0]\u2212kernel_size[0]stride[0]+1\u230bH_{out} = \\left\\lfloor\\frac{H_{in}  + 2 \\times \\text{padding}[0] -   \\text{kernel\\_size}[0]}{\\text{stride}[0]} + 1\\right\\rfloor  Hout\u200b=\u230astride[0]Hin\u200b+2\u00d7padding[0]\u2212kernel_size[0]\u200b+1\u230b   Wout=\u230aWin+2\u00d7padding[1]\u2212kernel_size[1]stride[1]+1\u230bW_{out} = \\left\\lfloor\\frac{W_{in}  + 2 \\times \\text{padding}[1] -   \\text{kernel\\_size}[1]}{\\text{stride}[1]} + 1\\right\\rfloor  Wout\u200b=\u230astride[1]Win\u200b+2\u00d7padding[1]\u2212kernel_size[1]\u200b+1\u230b    "},
{"library": "torch", "item_id": "torch.nn.AvgPool3d", "item_type": "class", "code": "classtorch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)", "description": "Applies a 3D average pooling over an input signal composed of several input planes. In the simplest case, the output value of the layer with input size (N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W)  , output (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\u200b,Hout\u200b,Wout\u200b)   and kernel_size (kD,kH,kW)(kD, kH, kW)(kD,kH,kW)   can be precisely described as:  out(Ni,Cj,d,h,w)=\u2211k=0kD\u22121\u2211m=0kH\u22121\u2211n=0kW\u22121input(Ni,Cj,stride[0]\u00d7d+k,stride[1]\u00d7h+m,stride[2]\u00d7w+n)kD\u00d7kH\u00d7kW\\begin{aligned}     \\text{out}(N_i, C_j, d, h, w) ={} &amp; \\sum_{k=0}^{kD-1} \\sum_{m=0}^{kH-1} \\sum_{n=0}^{kW-1} \\\\                                       &amp; \\frac{\\text{input}(N_i, C_j, \\text{stride}[0] \\times d + k,                                               \\text{stride}[1] \\times h + m, \\text{stride}[2] \\times w + n)}                                              {kD \\times kH \\times kW} \\end{aligned}  out(Ni\u200b,Cj\u200b,d,h,w)=\u200bk=0\u2211kD\u22121\u200bm=0\u2211kH\u22121\u200bn=0\u2211kW\u22121\u200bkD\u00d7kH\u00d7kWinput(Ni\u200b,Cj\u200b,stride[0]\u00d7d+k,stride[1]\u00d7h+m,stride[2]\u00d7w+n)\u200b\u200b  If padding is non-zero, then the input is implicitly zero-padded on all three sides for padding number of points. The parameters kernel_size, stride can either be:   a single int \u2013 in which case the same value is used for the depth, height and width dimension a tuple of three ints \u2013 in which case, the first int is used for the depth dimension, the second int for the height dimension and the third int for the width dimension    Parameters  kernel_size \u2013 the size of the window stride \u2013 the stride of the window. Default value is kernel_size padding \u2013 implicit zero padding to be added on all three sides ceil_mode \u2013 when True, will use ceil instead of floor to compute the output shape count_include_pad \u2013 when True, will include the zero-padding in the averaging calculation divisor_override \u2013 if specified, it will be used as divisor, otherwise attr:kernel_size will be used     Shape: Input: (N,C,Din,Hin,Win)(N, C, D_{in}, H_{in}, W_{in})(N,C,Din\u200b,Hin\u200b,Win\u200b)   Output: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\u200b,Hout\u200b,Wout\u200b)  , where  Dout=\u230aDin+2\u00d7padding[0]\u2212kernel_size[0]stride[0]+1\u230bD_{out} = \\left\\lfloor\\frac{D_{in} + 2 \\times \\text{padding}[0] -       \\text{kernel\\_size}[0]}{\\text{stride}[0]} + 1\\right\\rfloor  Dout\u200b=\u230astride[0]Din\u200b+2\u00d7padding[0]\u2212kernel_size[0]\u200b+1\u230b   Hout=\u230aHin+2\u00d7padding[1]\u2212kernel_size[1]stride[1]+1\u230bH_{out} = \\left\\lfloor\\frac{H_{in} + 2 \\times \\text{padding}[1] -       \\text{kernel\\_size}[1]}{\\text{stride}[1]} + 1\\right\\rfloor  Hout\u200b=\u230astride[1]Hin\u200b+2\u00d7padding[1]\u2212kernel_size[1]\u200b+1\u230b   Wout=\u230aWin+2\u00d7padding[2]\u2212kernel_size[2]stride[2]+1\u230bW_{out} = \\left\\lfloor\\frac{W_{in} + 2 \\times \\text{padding}[2] -       \\text{kernel\\_size}[2]}{\\text{stride}[2]} + 1\\right\\rfloor  Wout\u200b=\u230astride[2]Win\u200b+2\u00d7padding[2]\u2212kernel_size[2]\u200b+1\u230b      Examples: &gt;&gt;&gt; # pool of square window of size=3, stride=2 &gt;&gt;&gt; m = nn.AvgPool3d(3, stride=2) &gt;&gt;&gt; # pool of non-square window &gt;&gt;&gt; m = nn.AvgPool3d((3, 2, 2), stride=(2, 1, 2)) &gt;&gt;&gt; input = torch.randn(20, 16, 50,44, 31) &gt;&gt;&gt; output = m(input)   ", "parameters": ["kernel_size : the size of the window", "stride : the stride of the window. Default value is kernel_size", "padding : implicit zero padding to be added on all three sides", "ceil_mode : when True, will use ceil instead of floor to compute the output shape", "count_include_pad : when True, will include the zero-padding in the averaging calculation", "divisor_override : if specified, it will be used as divisor, otherwise attr:kernel_size will be used"], "returns": null, "example": " # pool of square window of size=3, stride=2\n m = nn.AvgPool3d(3, stride=2)\n # pool of non-square window\n m = nn.AvgPool3d((3, 2, 2), stride=(2, 1, 2))\n input = torch.randn(20, 16, 50,44, 31)\n output = m(input)\n\n", "shape": " Input: (N,C,Din,Hin,Win)(N, C, D_{in}, H_{in}, W_{in})(N,C,Din\u200b,Hin\u200b,Win\u200b)   Output: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\u200b,Hout\u200b,Wout\u200b)  , where  Dout=\u230aDin+2\u00d7padding[0]\u2212kernel_size[0]stride[0]+1\u230bD_{out} = \\left\\lfloor\\frac{D_{in} + 2 \\times \\text{padding}[0] -       \\text{kernel\\_size}[0]}{\\text{stride}[0]} + 1\\right\\rfloor  Dout\u200b=\u230astride[0]Din\u200b+2\u00d7padding[0]\u2212kernel_size[0]\u200b+1\u230b   Hout=\u230aHin+2\u00d7padding[1]\u2212kernel_size[1]stride[1]+1\u230bH_{out} = \\left\\lfloor\\frac{H_{in} + 2 \\times \\text{padding}[1] -       \\text{kernel\\_size}[1]}{\\text{stride}[1]} + 1\\right\\rfloor  Hout\u200b=\u230astride[1]Hin\u200b+2\u00d7padding[1]\u2212kernel_size[1]\u200b+1\u230b   Wout=\u230aWin+2\u00d7padding[2]\u2212kernel_size[2]stride[2]+1\u230bW_{out} = \\left\\lfloor\\frac{W_{in} + 2 \\times \\text{padding}[2] -       \\text{kernel\\_size}[2]}{\\text{stride}[2]} + 1\\right\\rfloor  Wout\u200b=\u230astride[2]Win\u200b+2\u00d7padding[2]\u2212kernel_size[2]\u200b+1\u230b    "},
{"library": "torch", "item_id": "torch.unique_consecutive", "item_type": "function", "code": "torch.unique_consecutive(input,return_inverse=False,return_counts=False,dim=None)", "description": "Eliminates all but the first element from every consecutive group of equivalent elements.  Note This function is different from torch.unique() in the sense that this function only eliminates consecutive duplicate values. This semantics is similar to std::unique in C++.   Parameters  input (Tensor) \u2013 the input tensor return_inverse (bool) \u2013 Whether to also return the indices for where elements in the original input ended up in the returned unique list. return_counts (bool) \u2013 Whether to also return the counts for each unique element. dim (python:int) \u2013 the dimension to apply unique. If None, the unique of the flattened input is returned. default: None   Returns A tensor or a tuple of tensors containing   output (Tensor): the output list of unique scalar elements. inverse_indices (Tensor): (optional) if return_inverse is True, there will be an additional returned tensor (same shape as input) representing the indices for where elements in the original input map to in the output; otherwise, this function will only return a single tensor. counts (Tensor): (optional) if return_counts is True, there will be an additional returned tensor (same shape as output or output.size(dim), if dim was specified) representing the number of occurrences for each unique value or tensor.     Return type (Tensor, Tensor (optional), Tensor (optional))   Example: &gt;&gt;&gt; x = torch.tensor([1, 1, 2, 2, 3, 1, 1, 2]) &gt;&gt;&gt; output = torch.unique_consecutive(x) &gt;&gt;&gt; output tensor([1, 2, 3, 1, 2])  &gt;&gt;&gt; output, inverse_indices = torch.unique_consecutive(x, return_inverse=True) &gt;&gt;&gt; output tensor([1, 2, 3, 1, 2]) &gt;&gt;&gt; inverse_indices tensor([0, 0, 1, 1, 2, 3, 3, 4])  &gt;&gt;&gt; output, counts = torch.unique_consecutive(x, return_counts=True) &gt;&gt;&gt; output tensor([1, 2, 3, 1, 2]) &gt;&gt;&gt; counts tensor([2, 2, 1, 2, 1])   ", "parameters": ["input (Tensor) : the input tensor", "return_inverse (bool) : Whether to also return the indices for whereelements in the original input ended up in the returned unique list.", "return_counts (bool) : Whether to also return the counts for each uniqueelement.", "dim (python:int) : the dimension to apply unique. If None, the unique of theflattened input is returned. default: None", "output (Tensor): the output list of unique scalar elements.", "inverse_indices (Tensor): (optional) ifreturn_inverse is True, there will be an additionalreturned tensor (same shape as input) representing the indicesfor where elements in the original input map to in the output;otherwise, this function will only return a single tensor.", "counts (Tensor): (optional) ifreturn_counts is True, there will be an additionalreturned tensor (same shape as output or output.size(dim),if dim was specified) representing the number of occurrencesfor each unique value or tensor."], "returns": "A tensor or a tuple of tensors containingoutput (Tensor): the output list of unique scalar elements.inverse_indices (Tensor): (optional) ifreturn_inverse is True, there will be an additionalreturned tensor (same shape as input) representing the indicesfor where elements in the original input map to in the output;otherwise, this function will only return a single tensor.counts (Tensor): (optional) ifreturn_counts is True, there will be an additionalreturned tensor (same shape as output or output.size(dim),if dim was specified) representing the number of occurrencesfor each unique value or tensor.", "example": " x = torch.tensor([1, 1, 2, 2, 3, 1, 1, 2])\n output = torch.unique_consecutive(x)\n output\ntensor([1, 2, 3, 1, 2])\n\n output, inverse_indices = torch.unique_consecutive(x, return_inverse=True)\n output\ntensor([1, 2, 3, 1, 2])\n inverse_indices\ntensor([0, 0, 1, 1, 2, 3, 3, 4])\n\n output, counts = torch.unique_consecutive(x, return_counts=True)\n output\ntensor([1, 2, 3, 1, 2])\n counts\ntensor([2, 2, 1, 2, 1])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.var", "item_type": "function", "code": "torch.var()", "description": "  torch.var(input, unbiased=True) \u2192 Tensor   Returns the variance of all elements in the input tensor. If unbiased is False, then the variance will be calculated via the biased estimator. Otherwise, Bessel\u2019s correction will be used.  Parameters  input (Tensor) \u2013 the input tensor. unbiased (bool) \u2013 whether to use the unbiased estimation or not    Example: &gt;&gt;&gt; a = torch.randn(1, 3) &gt;&gt;&gt; a tensor([[-0.3425, -1.2636, -0.4864]]) &gt;&gt;&gt; torch.var(a) tensor(0.2455)     torch.var(input, dim, keepdim=False, unbiased=True, out=None) \u2192 Tensor   Returns the variance of each row of the input tensor in the given dimension dim. If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s). If unbiased is False, then the variance will be calculated via the biased estimator. Otherwise, Bessel\u2019s correction will be used.  Parameters  input (Tensor) \u2013 the input tensor. dim (python:int or tuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not. unbiased (bool) \u2013 whether to use the unbiased estimation or not out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; a = torch.randn(4, 4) &gt;&gt;&gt; a tensor([[-0.3567,  1.7385, -1.3042,  0.7423],         [ 1.3436, -0.1015, -0.9834, -0.8438],         [ 0.6056,  0.1089, -0.3112, -1.4085],         [-0.7700,  0.6074, -0.1469,  0.7777]]) &gt;&gt;&gt; torch.var(a, 1) tensor([ 1.7444,  1.1363,  0.7356,  0.5112])   ", "parameters": ["input (Tensor) : the input tensor.", "unbiased (bool) : whether to use the unbiased estimation or not", "input (Tensor) : the input tensor.", "dim (python:int or tuple of python:ints) : the dimension or dimensions to reduce.", "keepdim (bool) : whether the output tensor has dim retained or not.", "unbiased (bool) : whether to use the unbiased estimation or not", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " a = torch.randn(1, 3)\n a\ntensor([[-0.3425, -1.2636, -0.4864]])\n torch.var(a)\ntensor(0.2455)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.var(input,unbiased=True)\u2192Tensor", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.var(input,dim,keepdim=False,unbiased=True,out=None)\u2192Tensor", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.transforms.ComposeTransform", "item_type": "class", "code": "classtorch.distributions.transforms.ComposeTransform(parts)", "description": "Composes multiple transforms in a chain. The transforms being composed are responsible for caching.  Parameters parts (list of Transform) \u2013 A list of transforms to compose.   ", "parameters": ["parts (list of Transform) : A list of transforms to compose."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.transforms.ExpTransform", "item_type": "class", "code": "classtorch.distributions.transforms.ExpTransform(cache_size=0)", "description": "Transform via the mapping y=exp\u2061(x)y = \\exp(x)y=exp(x)  . ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.transforms.PowerTransform", "item_type": "class", "code": "classtorch.distributions.transforms.PowerTransform(exponent,cache_size=0)", "description": "Transform via the mapping y=xexponenty = x^{\\text{exponent}}y=xexponent  . ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.transforms.SigmoidTransform", "item_type": "class", "code": "classtorch.distributions.transforms.SigmoidTransform(cache_size=0)", "description": "Transform via the mapping y=11+exp\u2061(\u2212x)y = \\frac{1}{1 + \\exp(-x)}y=1+exp(\u2212x)1\u200b   and x=logit(y)x = \\text{logit}(y)x=logit(y)  . ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.transforms.AbsTransform", "item_type": "class", "code": "classtorch.distributions.transforms.AbsTransform(cache_size=0)", "description": "Transform via the mapping y=\u2223x\u2223y = |x|y=\u2223x\u2223  . ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.transforms.AffineTransform", "item_type": "class", "code": "classtorch.distributions.transforms.AffineTransform(loc,scale,event_dim=0,cache_size=0)", "description": "Transform via the pointwise affine mapping y=loc+scale\u00d7xy = \\text{loc} + \\text{scale} \\times xy=loc+scale\u00d7x  .  Parameters  loc (Tensor or python:float) \u2013 Location parameter. scale (Tensor or python:float) \u2013 Scale parameter. event_dim (python:int) \u2013 Optional size of event_shape. This should be zero for univariate random variables, 1 for distributions over vectors, 2 for distributions over matrices, etc.    ", "parameters": ["loc (Tensor or python:float) : Location parameter.", "scale (Tensor or python:float) : Scale parameter.", "event_dim (python:int) : Optional size of event_shape. This should be zerofor univariate random variables, 1 for distributions over vectors,2 for distributions over matrices, etc."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.transforms.SoftmaxTransform", "item_type": "class", "code": "classtorch.distributions.transforms.SoftmaxTransform(cache_size=0)", "description": "Transform from unconstrained space to the simplex via y=exp\u2061(x)y = \\exp(x)y=exp(x)   then normalizing. This is not bijective and cannot be used for HMC. However this acts mostly coordinate-wise (except for the final normalization), and thus is appropriate for coordinate-wise optimization algorithms. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.FractionalMaxPool2d", "item_type": "class", "code": "classtorch.nn.FractionalMaxPool2d(kernel_size,output_size=None,output_ratio=None,return_indices=False,_random_samples=None)", "description": "Applies a 2D fractional max pooling over an input signal composed of several input planes. Fractional MaxPooling is described in detail in the paper Fractional MaxPooling by Ben Graham The max-pooling operation is applied in kH\u00d7kWkH \\times kWkH\u00d7kW   regions by a stochastic step size determined by the target output size. The number of output features is equal to the number of input planes.  Parameters  kernel_size \u2013 the size of the window to take a max over. Can be a single number k (for a square kernel of k x k) or a tuple (kh, kw) output_size \u2013 the target output size of the image of the form oH x oW. Can be a tuple (oH, oW) or a single number oH for a square image oH x oH output_ratio \u2013 If one wants to have an output size as a ratio of the input size, this option can be given. This has to be a number or tuple in the range (0, 1) return_indices \u2013 if True, will return the indices along with the outputs. Useful to pass to nn.MaxUnpool2d(). Default: False    Examples &gt;&gt;&gt; # pool of square window of size=3, and target output size 13x12 &gt;&gt;&gt; m = nn.FractionalMaxPool2d(3, output_size=(13, 12)) &gt;&gt;&gt; # pool of square window and target output size being half of input image size &gt;&gt;&gt; m = nn.FractionalMaxPool2d(3, output_ratio=(0.5, 0.5)) &gt;&gt;&gt; input = torch.randn(20, 16, 50, 32) &gt;&gt;&gt; output = m(input)   ", "parameters": ["kernel_size : the size of the window to take a max over.Can be a single number k (for a square kernel of k x k) or a tuple (kh, kw)", "output_size : the target output size of the image of the form oH x oW.Can be a tuple (oH, oW) or a single number oH for a square image oH x oH", "output_ratio : If one wants to have an output size as a ratio of the input size, this option can be given.This has to be a number or tuple in the range (0, 1)", "return_indices : if True, will return the indices along with the outputs.Useful to pass to nn.MaxUnpool2d(). Default: False"], "returns": null, "example": " # pool of square window of size=3, and target output size 13x12\n m = nn.FractionalMaxPool2d(3, output_size=(13, 12))\n # pool of square window and target output size being half of input image size\n m = nn.FractionalMaxPool2d(3, output_ratio=(0.5, 0.5))\n input = torch.randn(20, 16, 50, 32)\n output = m(input)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.LPPool1d", "item_type": "class", "code": "classtorch.nn.LPPool1d(norm_type,kernel_size,stride=None,ceil_mode=False)", "description": "Applies a 1D power-average pooling over an input signal composed of several input planes. On each window, the function computed is:  f(X)=\u2211x\u2208Xxppf(X) = \\sqrt[p]{\\sum_{x \\in X} x^{p}}  f(X)=p\u200bx\u2208X\u2211\u200bxp\u200b   At p = \u221e\\infty\u221e  , one gets Max Pooling At p = 1, one gets Sum Pooling (which is proportional to Average Pooling)   Note If the sum to the power of p is zero, the gradient of this function is not defined. This implementation will set the gradient to zero in this case.   Parameters  kernel_size \u2013 a single int, the size of the window stride \u2013 a single int, the stride of the window. Default value is kernel_size ceil_mode \u2013 when True, will use ceil instead of floor to compute the output shape     Shape: Input: (N,C,Lin)(N, C, L_{in})(N,C,Lin\u200b)   Output: (N,C,Lout)(N, C, L_{out})(N,C,Lout\u200b)  , where  Lout=\u230aLin+2\u00d7padding\u2212kernel_sizestride+1\u230bL_{out} = \\left\\lfloor\\frac{L_{in} + 2 \\times \\text{padding} - \\text{kernel\\_size}}{\\text{stride}} + 1\\right\\rfloor  Lout\u200b=\u230astrideLin\u200b+2\u00d7padding\u2212kernel_size\u200b+1\u230b     Examples::&gt;&gt;&gt; # power-2 pool of window of length 3, with stride 2. &gt;&gt;&gt; m = nn.LPPool1d(2, 3, stride=2) &gt;&gt;&gt; input = torch.randn(20, 16, 50) &gt;&gt;&gt; output = m(input)     ", "parameters": ["kernel_size : a single int, the size of the window", "stride : a single int, the stride of the window. Default value is kernel_size", "ceil_mode : when True, will use ceil instead of floor to compute the output shape"], "returns": null, "example": " # power-2 pool of window of length 3, with stride 2.\n m = nn.LPPool1d(2, 3, stride=2)\n input = torch.randn(20, 16, 50)\n output = m(input)\n\n\n", "shape": " Input: (N,C,Lin)(N, C, L_{in})(N,C,Lin\u200b)   Output: (N,C,Lout)(N, C, L_{out})(N,C,Lout\u200b)  , where  Lout=\u230aLin+2\u00d7padding\u2212kernel_sizestride+1\u230bL_{out} = \\left\\lfloor\\frac{L_{in} + 2 \\times \\text{padding} - \\text{kernel\\_size}}{\\text{stride}} + 1\\right\\rfloor  Lout\u200b=\u230astrideLin\u200b+2\u00d7padding\u2212kernel_size\u200b+1\u230b    "},
{"library": "torch", "item_id": "torch.nn.LPPool2d", "item_type": "class", "code": "classtorch.nn.LPPool2d(norm_type,kernel_size,stride=None,ceil_mode=False)", "description": "Applies a 2D power-average pooling over an input signal composed of several input planes. On each window, the function computed is:  f(X)=\u2211x\u2208Xxppf(X) = \\sqrt[p]{\\sum_{x \\in X} x^{p}}  f(X)=p\u200bx\u2208X\u2211\u200bxp\u200b   At p = \u221e\\infty\u221e  , one gets Max Pooling At p = 1, one gets Sum Pooling (which is proportional to average pooling)  The parameters kernel_size, stride can either be:   a single int \u2013 in which case the same value is used for the height and width dimension a tuple of two ints \u2013 in which case, the first int is used for the height dimension, and the second int for the width dimension    Note If the sum to the power of p is zero, the gradient of this function is not defined. This implementation will set the gradient to zero in this case.   Parameters  kernel_size \u2013 the size of the window stride \u2013 the stride of the window. Default value is kernel_size ceil_mode \u2013 when True, will use ceil instead of floor to compute the output shape     Shape: Input: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)   Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)  , where  Hout=\u230aHin+2\u00d7padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121stride[0]+1\u230bH_{out} = \\left\\lfloor\\frac{H_{in}  + 2 \\times \\text{padding}[0] - \\text{dilation}[0] \\times       (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor  Hout\u200b=\u230astride[0]Hin\u200b+2\u00d7padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121\u200b+1\u230b   Wout=\u230aWin+2\u00d7padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121stride[1]+1\u230bW_{out} = \\left\\lfloor\\frac{W_{in}  + 2 \\times \\text{padding}[1] - \\text{dilation}[1] \\times       (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor  Wout\u200b=\u230astride[1]Win\u200b+2\u00d7padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121\u200b+1\u230b      Examples: &gt;&gt;&gt; # power-2 pool of square window of size=3, stride=2 &gt;&gt;&gt; m = nn.LPPool2d(2, 3, stride=2) &gt;&gt;&gt; # pool of non-square window of power 1.2 &gt;&gt;&gt; m = nn.LPPool2d(1.2, (3, 2), stride=(2, 1)) &gt;&gt;&gt; input = torch.randn(20, 16, 50, 32) &gt;&gt;&gt; output = m(input)   ", "parameters": ["kernel_size : the size of the window", "stride : the stride of the window. Default value is kernel_size", "ceil_mode : when True, will use ceil instead of floor to compute the output shape"], "returns": null, "example": " # power-2 pool of square window of size=3, stride=2\n m = nn.LPPool2d(2, 3, stride=2)\n # pool of non-square window of power 1.2\n m = nn.LPPool2d(1.2, (3, 2), stride=(2, 1))\n input = torch.randn(20, 16, 50, 32)\n output = m(input)\n\n", "shape": " Input: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)   Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)  , where  Hout=\u230aHin+2\u00d7padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121stride[0]+1\u230bH_{out} = \\left\\lfloor\\frac{H_{in}  + 2 \\times \\text{padding}[0] - \\text{dilation}[0] \\times       (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor  Hout\u200b=\u230astride[0]Hin\u200b+2\u00d7padding[0]\u2212dilation[0]\u00d7(kernel_size[0]\u22121)\u22121\u200b+1\u230b   Wout=\u230aWin+2\u00d7padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121stride[1]+1\u230bW_{out} = \\left\\lfloor\\frac{W_{in}  + 2 \\times \\text{padding}[1] - \\text{dilation}[1] \\times       (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor  Wout\u200b=\u230astride[1]Win\u200b+2\u00d7padding[1]\u2212dilation[1]\u00d7(kernel_size[1]\u22121)\u22121\u200b+1\u230b    "},
{"library": "torch", "item_id": "torch.var_mean", "item_type": "function", "code": "torch.var_mean()", "description": "  torch.var_mean(input, unbiased=True) -&gt; (Tensor, Tensor)   Returns the variance and mean of all elements in the input tensor. If unbiased is False, then the variance will be calculated via the biased estimator. Otherwise, Bessel\u2019s correction will be used.  Parameters  input (Tensor) \u2013 the input tensor. unbiased (bool) \u2013 whether to use the unbiased estimation or not    Example: &gt;&gt;&gt; a = torch.randn(1, 3) &gt;&gt;&gt; a tensor([[0.0146, 0.4258, 0.2211]]) &gt;&gt;&gt; torch.var_mean(a) (tensor(0.0423), tensor(0.2205))     torch.var_mean(input, dim, keepdim=False, unbiased=True) -&gt; (Tensor, Tensor)   Returns the variance and mean of each row of the input tensor in the given dimension dim. If keepdim is True, the output tensor is of the same size as input except in the dimension(s) dim where it is of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensor having 1 (or len(dim)) fewer dimension(s). If unbiased is False, then the variance will be calculated via the biased estimator. Otherwise, Bessel\u2019s correction will be used.  Parameters  input (Tensor) \u2013 the input tensor. dim (python:int or tuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim (bool) \u2013 whether the output tensor has dim retained or not. unbiased (bool) \u2013 whether to use the unbiased estimation or not    Example: &gt;&gt;&gt; a = torch.randn(4, 4) &gt;&gt;&gt; a tensor([[-1.5650,  2.0415, -0.1024, -0.5790],         [ 0.2325, -2.6145, -1.6428, -0.3537],         [-0.2159, -1.1069,  1.2882, -1.3265],         [-0.6706, -1.5893,  0.6827,  1.6727]]) &gt;&gt;&gt; torch.var_mean(a, 1) (tensor([2.3174, 1.6403, 1.4092, 2.0791]), tensor([-0.0512, -1.0946, -0.3403,  0.0239]))   ", "parameters": ["input (Tensor) : the input tensor.", "unbiased (bool) : whether to use the unbiased estimation or not", "input (Tensor) : the input tensor.", "dim (python:int or tuple of python:ints) : the dimension or dimensions to reduce.", "keepdim (bool) : whether the output tensor has dim retained or not.", "unbiased (bool) : whether to use the unbiased estimation or not"], "returns": null, "example": " a = torch.randn(1, 3)\n a\ntensor([[0.0146, 0.4258, 0.2211]])\n torch.var_mean(a)\n(tensor(0.0423), tensor(0.2205))\n\n", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.var_mean(input,unbiased=True)-&gt;(Tensor,Tensor)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.var_mean(input,dim,keepdim=False,unbiased=True)-&gt;(Tensor,Tensor)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.allclose", "item_type": "function", "code": "torch.allclose(input,other,rtol=1e-05,atol=1e-08,equal_nan=False)\u2192bool", "description": "This function checks if all input and other satisfy the condition:  \u2223input\u2212other\u2223\u2264atol+rtol\u00d7\u2223other\u2223\\lvert \\text{input} - \\text{other} \\rvert \\leq \\texttt{atol} + \\texttt{rtol} \\times \\lvert \\text{other} \\rvert  \u2223input\u2212other\u2223\u2264atol+rtol\u00d7\u2223other\u2223  elementwise, for all elements of input and other. The behaviour of this function is analogous to numpy.allclose  Parameters  input (Tensor) \u2013 first tensor to compare other (Tensor) \u2013 second tensor to compare atol (python:float, optional) \u2013 absolute tolerance. Default: 1e-08 rtol (python:float, optional) \u2013 relative tolerance. Default: 1e-05 equal_nan (bool, optional) \u2013 if True, then two NaN s will be compared as equal. Default: False    Example: &gt;&gt;&gt; torch.allclose(torch.tensor([10000., 1e-07]), torch.tensor([10000.1, 1e-08])) False &gt;&gt;&gt; torch.allclose(torch.tensor([10000., 1e-08]), torch.tensor([10000.1, 1e-09])) True &gt;&gt;&gt; torch.allclose(torch.tensor([1.0, float('nan')]), torch.tensor([1.0, float('nan')])) False &gt;&gt;&gt; torch.allclose(torch.tensor([1.0, float('nan')]), torch.tensor([1.0, float('nan')]), equal_nan=True) True   ", "parameters": ["input (Tensor) : first tensor to compare", "other (Tensor) : second tensor to compare", "atol (python:float, optional) : absolute tolerance. Default: 1e-08", "rtol (python:float, optional) : relative tolerance. Default: 1e-05", "equal_nan (bool, optional) : if True, then two NaN s will be compared as equal. Default: False"], "returns": null, "example": " torch.allclose(torch.tensor([10000., 1e-07]), torch.tensor([10000.1, 1e-08]))\nFalse\n torch.allclose(torch.tensor([10000., 1e-08]), torch.tensor([10000.1, 1e-09]))\nTrue\n torch.allclose(torch.tensor([1.0, float('nan')]), torch.tensor([1.0, float('nan')]))\nFalse\n torch.allclose(torch.tensor([1.0, float('nan')]), torch.tensor([1.0, float('nan')]), equal_nan=True)\nTrue\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.argsort", "item_type": "function", "code": "torch.argsort(input,dim=-1,descending=False,out=None)\u2192LongTensor", "description": "Returns the indices that sort a tensor along a given dimension in ascending order by value. This is the second value returned by torch.sort().  See its documentation for the exact semantics of this method.  Parameters  input (Tensor) \u2013 the input tensor. dim (python:int, optional) \u2013 the dimension to sort along descending (bool, optional) \u2013 controls the sorting order (ascending or descending)    Example: &gt;&gt;&gt; a = torch.randn(4, 4) &gt;&gt;&gt; a tensor([[ 0.0785,  1.5267, -0.8521,  0.4065],         [ 0.1598,  0.0788, -0.0745, -1.2700],         [ 1.2208,  1.0722, -0.7064,  1.2564],         [ 0.0669, -0.2318, -0.8229, -0.9280]])   &gt;&gt;&gt; torch.argsort(a, dim=1) tensor([[2, 0, 3, 1],         [3, 2, 1, 0],         [2, 1, 0, 3],         [3, 2, 1, 0]])   ", "parameters": ["input (Tensor) : the input tensor.", "dim (python:int, optional) : the dimension to sort along", "descending (bool, optional) : controls the sorting order (ascending or descending)"], "returns": null, "example": " a = torch.randn(4, 4)\n a\ntensor([[ 0.0785,  1.5267, -0.8521,  0.4065],\n        [ 0.1598,  0.0788, -0.0745, -1.2700],\n        [ 1.2208,  1.0722, -0.7064,  1.2564],\n        [ 0.0669, -0.2318, -0.8229, -0.9280]])\n\n\n torch.argsort(a, dim=1)\ntensor([[2, 0, 3, 1],\n        [3, 2, 1, 0],\n        [2, 1, 0, 3],\n        [3, 2, 1, 0]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.eq", "item_type": "function", "code": "torch.eq(input,other,out=None)\u2192Tensor", "description": "Computes element-wise equality The second argument can be a number or a tensor whose shape is broadcastable with the first argument.  Parameters  input (Tensor) \u2013 the tensor to compare other (Tensor or python:float) \u2013 the tensor or value to compare out (Tensor, optional) \u2013 the output tensor. Must be a ByteTensor   Returns A torch.BoolTensor containing a True at each location where comparison is true  Return type Tensor   Example: &gt;&gt;&gt; torch.eq(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]])) tensor([[ 1,  0],         [ 0,  1]], dtype=torch.uint8)   ", "parameters": ["input (Tensor) : the tensor to compare", "other (Tensor or python:float) : the tensor or value to compare", "out (Tensor, optional) : the output tensor. Must be a ByteTensor"], "returns": "A torch.BoolTensor containing a True at each location where comparison is true", "example": " torch.eq(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\ntensor([[ 1,  0],\n        [ 0,  1]], dtype=torch.uint8)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.equal", "item_type": "function", "code": "torch.equal(input,other)\u2192bool", "description": "True if two tensors have the same size and elements, False otherwise. Example: &gt;&gt;&gt; torch.equal(torch.tensor([1, 2]), torch.tensor([1, 2])) True   ", "parameters": [], "returns": null, "example": " torch.equal(torch.tensor([1, 2]), torch.tensor([1, 2]))\nTrue\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.transforms.StickBreakingTransform", "item_type": "class", "code": "classtorch.distributions.transforms.StickBreakingTransform(cache_size=0)", "description": "Transform from unconstrained space to the simplex of one additional dimension via a stick-breaking process. This transform arises as an iterated sigmoid transform in a stick-breaking construction of the Dirichlet distribution: the first logit is transformed via sigmoid to the first probability and the probability of everything else, and then the process recurses. This is bijective and appropriate for use in HMC; however it mixes coordinates together and is less appropriate for optimization. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.transforms.LowerCholeskyTransform", "item_type": "class", "code": "classtorch.distributions.transforms.LowerCholeskyTransform(cache_size=0)", "description": "Transform from unconstrained matrices to lower-triangular matrices with nonnegative diagonal entries. This is useful for parameterizing positive definite matrices in terms of their Cholesky factorization. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.transforms.CatTransform", "item_type": "class", "code": "classtorch.distributions.transforms.CatTransform(tseq,dim=0,lengths=None)", "description": "Transform functor that applies a sequence of transforms tseq component-wise to each submatrix at dim, of length lengths[dim], in a way compatible with torch.cat().  Example::x0 = torch.cat([torch.range(1, 10), torch.range(1, 10)], dim=0) x = torch.cat([x0, x0], dim=0) t0 = CatTransform([ExpTransform(), identity_transform], dim=0, lengths=[10, 10]) t = CatTransform([t0, t0], dim=0, lengths=[20, 20]) y = t(x)   ", "parameters": [], "returns": null, "example": "x0 = torch.cat([torch.range(1, 10), torch.range(1, 10)], dim=0)\nx = torch.cat([x0, x0], dim=0)\nt0 = CatTransform([ExpTransform(), identity_transform], dim=0, lengths=[10, 10])\nt = CatTransform([t0, t0], dim=0, lengths=[20, 20])\ny = t(x)\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.transforms.StackTransform", "item_type": "class", "code": "classtorch.distributions.transforms.StackTransform(tseq,dim=0)", "description": "Transform functor that applies a sequence of transforms tseq component-wise to each submatrix at dim in a way compatible with torch.stack().  Example::x = torch.stack([torch.range(1, 10), torch.range(1, 10)], dim=1) t = StackTransform([ExpTransform(), identity_transform], dim=1) y = t(x)   ", "parameters": [], "returns": null, "example": "x = torch.stack([torch.range(1, 10), torch.range(1, 10)], dim=1)\nt = StackTransform([ExpTransform(), identity_transform], dim=1)\ny = t(x)\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.constraints.Constraint", "item_type": "class", "code": "classtorch.distributions.constraints.Constraint", "description": "Abstract base class for constraints. A constraint object represents a region over which a variable is valid, e.g. within which a variable can be optimized.   check(value)  Returns a byte tensor of sample_shape + batch_shape indicating whether each event in value satisfies this constraint.   ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.distributions.constraint_registry.ConstraintRegistry", "item_type": "class", "code": "classtorch.distributions.constraint_registry.ConstraintRegistry", "description": "Registry to link constraints to transforms.   register(constraint, factory=None)  Registers a Constraint subclass in this registry. Usage: @my_registry.register(MyConstraintClass) def construct_transform(constraint):     assert isinstance(constraint, MyConstraint)     return MyTransform(constraint.arg_constraints)    Parameters  constraint (subclass of Constraint) \u2013 A subclass of Constraint, or a singleton object of the desired class. factory (callable) \u2013 A callable that inputs a constraint object and returns a  Transform object.      ", "parameters": ["constraint (subclass of Constraint) : A subclass of Constraint, ora singleton object of the desired class.", "factory (callable) : A callable that inputs a constraint object and returnsa  Transform object."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.AdaptiveMaxPool1d", "item_type": "class", "code": "classtorch.nn.AdaptiveMaxPool1d(output_size,return_indices=False)", "description": "Applies a 1D adaptive max pooling over an input signal composed of several input planes. The output size is H, for any input size. The number of output features is equal to the number of input planes.  Parameters  output_size \u2013 the target output size H return_indices \u2013 if True, will return the indices along with the outputs. Useful to pass to nn.MaxUnpool1d. Default: False    Examples &gt;&gt;&gt; # target output size of 5 &gt;&gt;&gt; m = nn.AdaptiveMaxPool1d(5) &gt;&gt;&gt; input = torch.randn(1, 64, 8) &gt;&gt;&gt; output = m(input)   ", "parameters": ["output_size : the target output size H", "return_indices : if True, will return the indices along with the outputs.Useful to pass to nn.MaxUnpool1d. Default: False"], "returns": null, "example": " # target output size of 5\n m = nn.AdaptiveMaxPool1d(5)\n input = torch.randn(1, 64, 8)\n output = m(input)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.AdaptiveMaxPool2d", "item_type": "class", "code": "classtorch.nn.AdaptiveMaxPool2d(output_size,return_indices=False)", "description": "Applies a 2D adaptive max pooling over an input signal composed of several input planes. The output is of size H x W, for any input size. The number of output features is equal to the number of input planes.  Parameters  output_size \u2013 the target output size of the image of the form H x W. Can be a tuple (H, W) or a single H for a square image H x H. H and W can be either a int, or None which means the size will be the same as that of the input. return_indices \u2013 if True, will return the indices along with the outputs. Useful to pass to nn.MaxUnpool2d. Default: False    Examples &gt;&gt;&gt; # target output size of 5x7 &gt;&gt;&gt; m = nn.AdaptiveMaxPool2d((5,7)) &gt;&gt;&gt; input = torch.randn(1, 64, 8, 9) &gt;&gt;&gt; output = m(input) &gt;&gt;&gt; # target output size of 7x7 (square) &gt;&gt;&gt; m = nn.AdaptiveMaxPool2d(7) &gt;&gt;&gt; input = torch.randn(1, 64, 10, 9) &gt;&gt;&gt; output = m(input) &gt;&gt;&gt; # target output size of 10x7 &gt;&gt;&gt; m = nn.AdaptiveMaxPool2d((None, 7)) &gt;&gt;&gt; input = torch.randn(1, 64, 10, 9) &gt;&gt;&gt; output = m(input)   ", "parameters": ["output_size : the target output size of the image of the form H x W.Can be a tuple (H, W) or a single H for a square image H x H.H and W can be either a int, or None which means the size willbe the same as that of the input.", "return_indices : if True, will return the indices along with the outputs.Useful to pass to nn.MaxUnpool2d. Default: False"], "returns": null, "example": " # target output size of 5x7\n m = nn.AdaptiveMaxPool2d((5,7))\n input = torch.randn(1, 64, 8, 9)\n output = m(input)\n # target output size of 7x7 (square)\n m = nn.AdaptiveMaxPool2d(7)\n input = torch.randn(1, 64, 10, 9)\n output = m(input)\n # target output size of 10x7\n m = nn.AdaptiveMaxPool2d((None, 7))\n input = torch.randn(1, 64, 10, 9)\n output = m(input)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.AdaptiveMaxPool3d", "item_type": "class", "code": "classtorch.nn.AdaptiveMaxPool3d(output_size,return_indices=False)", "description": "Applies a 3D adaptive max pooling over an input signal composed of several input planes. The output is of size D x H x W, for any input size. The number of output features is equal to the number of input planes.  Parameters  output_size \u2013 the target output size of the image of the form D x H x W. Can be a tuple (D, H, W) or a single D for a cube D x D x D. D, H and W can be either a int, or None which means the size will be the same as that of the input. return_indices \u2013 if True, will return the indices along with the outputs. Useful to pass to nn.MaxUnpool3d. Default: False    Examples &gt;&gt;&gt; # target output size of 5x7x9 &gt;&gt;&gt; m = nn.AdaptiveMaxPool3d((5,7,9)) &gt;&gt;&gt; input = torch.randn(1, 64, 8, 9, 10) &gt;&gt;&gt; output = m(input) &gt;&gt;&gt; # target output size of 7x7x7 (cube) &gt;&gt;&gt; m = nn.AdaptiveMaxPool3d(7) &gt;&gt;&gt; input = torch.randn(1, 64, 10, 9, 8) &gt;&gt;&gt; output = m(input) &gt;&gt;&gt; # target output size of 7x9x8 &gt;&gt;&gt; m = nn.AdaptiveMaxPool3d((7, None, None)) &gt;&gt;&gt; input = torch.randn(1, 64, 10, 9, 8) &gt;&gt;&gt; output = m(input)   ", "parameters": ["output_size : the target output size of the image of the form D x H x W.Can be a tuple (D, H, W) or a single D for a cube D x D x D.D, H and W can be either a int, or None which means the size willbe the same as that of the input.", "return_indices : if True, will return the indices along with the outputs.Useful to pass to nn.MaxUnpool3d. Default: False"], "returns": null, "example": " # target output size of 5x7x9\n m = nn.AdaptiveMaxPool3d((5,7,9))\n input = torch.randn(1, 64, 8, 9, 10)\n output = m(input)\n # target output size of 7x7x7 (cube)\n m = nn.AdaptiveMaxPool3d(7)\n input = torch.randn(1, 64, 10, 9, 8)\n output = m(input)\n # target output size of 7x9x8\n m = nn.AdaptiveMaxPool3d((7, None, None))\n input = torch.randn(1, 64, 10, 9, 8)\n output = m(input)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.ge", "item_type": "function", "code": "torch.ge(input,other,out=None)\u2192Tensor", "description": "Computes input\u2265other\\text{input} \\geq \\text{other}input\u2265other   element-wise. The second argument can be a number or a tensor whose shape is broadcastable with the first argument.  Parameters  input (Tensor) \u2013 the tensor to compare other (Tensor or python:float) \u2013 the tensor or value to compare out (Tensor, optional) \u2013 the output tensor that must be a BoolTensor   Returns A torch.BoolTensor containing a True at each location where comparison is true  Return type Tensor   Example: &gt;&gt;&gt; torch.ge(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]])) tensor([[True, True], [False, True]])   ", "parameters": ["input (Tensor) : the tensor to compare", "other (Tensor or python:float) : the tensor or value to compare", "out (Tensor, optional) : the output tensor that must be a BoolTensor"], "returns": "A torch.BoolTensor containing a True at each location where comparison is true", "example": " torch.ge(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\ntensor([[True, True], [False, True]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.gt", "item_type": "function", "code": "torch.gt(input,other,out=None)\u2192Tensor", "description": "Computes input&gt;other\\text{input} &gt; \\text{other}input&gt;other   element-wise. The second argument can be a number or a tensor whose shape is broadcastable with the first argument.  Parameters  input (Tensor) \u2013 the tensor to compare other (Tensor or python:float) \u2013 the tensor or value to compare out (Tensor, optional) \u2013 the output tensor that must be a BoolTensor   Returns A torch.BoolTensor containing a True at each location where comparison is true  Return type Tensor   Example: &gt;&gt;&gt; torch.gt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]])) tensor([[False, True], [False, False]])   ", "parameters": ["input (Tensor) : the tensor to compare", "other (Tensor or python:float) : the tensor or value to compare", "out (Tensor, optional) : the output tensor that must be a BoolTensor"], "returns": "A torch.BoolTensor containing a True at each location where comparison is true", "example": " torch.gt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\ntensor([[False, True], [False, False]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.isfinite", "item_type": "function", "code": "torch.isfinite()", "description": "Returns a new tensor with boolean elements representing if each element is Finite or not.   Arguments:tensor (Tensor): A tensor to check  Returns:Tensor: A torch.Tensor with dtype torch.bool containing a True at each location of finite elements and False otherwise   Example: &gt;&gt;&gt; torch.isfinite(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')])) tensor([True,  False,  True,  False,  False])    ", "parameters": [], "returns": "Tensor: A torch.Tensor with dtype torch.bool containing a True at each location of finite elements and False otherwise", "example": " torch.isfinite(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')]))\ntensor([True,  False,  True,  False,  False])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.isinf", "item_type": "function", "code": "torch.isinf(tensor)", "description": "Returns a new tensor with boolean elements representing if each element is +/-INF or not.  Parameters tensor (Tensor) \u2013 A tensor to check  Returns A torch.Tensor with dtype torch.bool containing a True at each location of +/-INF elements and False otherwise  Return type Tensor   Example: &gt;&gt;&gt; torch.isinf(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')])) tensor([False,  True,  False,  True,  False])   ", "parameters": ["tensor (Tensor) : A tensor to check", "A torch.Tensor with dtype torch.bool containing a True at each location of +/-INF elements and False otherwise", "Tensor"], "returns": "A torch.Tensor with dtype torch.bool containing a True at each location of +/-INF elements and False otherwise", "example": " torch.isinf(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')]))\ntensor([False,  True,  False,  True,  False])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.AdaptiveAvgPool1d", "item_type": "class", "code": "classtorch.nn.AdaptiveAvgPool1d(output_size)", "description": "Applies a 1D adaptive average pooling over an input signal composed of several input planes. The output size is H, for any input size. The number of output features is equal to the number of input planes.  Parameters output_size \u2013 the target output size H   Examples &gt;&gt;&gt; # target output size of 5 &gt;&gt;&gt; m = nn.AdaptiveAvgPool1d(5) &gt;&gt;&gt; input = torch.randn(1, 64, 8) &gt;&gt;&gt; output = m(input)   ", "parameters": ["output_size : the target output size H"], "returns": null, "example": " # target output size of 5\n m = nn.AdaptiveAvgPool1d(5)\n input = torch.randn(1, 64, 8)\n output = m(input)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.AdaptiveAvgPool2d", "item_type": "class", "code": "classtorch.nn.AdaptiveAvgPool2d(output_size)", "description": "Applies a 2D adaptive average pooling over an input signal composed of several input planes. The output is of size H x W, for any input size. The number of output features is equal to the number of input planes.  Parameters output_size \u2013 the target output size of the image of the form H x W. Can be a tuple (H, W) or a single H for a square image H x H. H and W can be either a int, or None which means the size will be the same as that of the input.   Examples &gt;&gt;&gt; # target output size of 5x7 &gt;&gt;&gt; m = nn.AdaptiveAvgPool2d((5,7)) &gt;&gt;&gt; input = torch.randn(1, 64, 8, 9) &gt;&gt;&gt; output = m(input) &gt;&gt;&gt; # target output size of 7x7 (square) &gt;&gt;&gt; m = nn.AdaptiveAvgPool2d(7) &gt;&gt;&gt; input = torch.randn(1, 64, 10, 9) &gt;&gt;&gt; output = m(input) &gt;&gt;&gt; # target output size of 10x7 &gt;&gt;&gt; m = nn.AdaptiveMaxPool2d((None, 7)) &gt;&gt;&gt; input = torch.randn(1, 64, 10, 9) &gt;&gt;&gt; output = m(input)   ", "parameters": ["output_size : the target output size of the image of the form H x W.Can be a tuple (H, W) or a single H for a square image H x H.H and W can be either a int, or None which means the size willbe the same as that of the input."], "returns": null, "example": " # target output size of 5x7\n m = nn.AdaptiveAvgPool2d((5,7))\n input = torch.randn(1, 64, 8, 9)\n output = m(input)\n # target output size of 7x7 (square)\n m = nn.AdaptiveAvgPool2d(7)\n input = torch.randn(1, 64, 10, 9)\n output = m(input)\n # target output size of 10x7\n m = nn.AdaptiveMaxPool2d((None, 7))\n input = torch.randn(1, 64, 10, 9)\n output = m(input)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.AdaptiveAvgPool3d", "item_type": "class", "code": "classtorch.nn.AdaptiveAvgPool3d(output_size)", "description": "Applies a 3D adaptive average pooling over an input signal composed of several input planes. The output is of size D x H x W, for any input size. The number of output features is equal to the number of input planes.  Parameters output_size \u2013 the target output size of the form D x H x W. Can be a tuple (D, H, W) or a single number D for a cube D x D x D. D, H and W can be either a int, or None which means the size will be the same as that of the input.   Examples &gt;&gt;&gt; # target output size of 5x7x9 &gt;&gt;&gt; m = nn.AdaptiveAvgPool3d((5,7,9)) &gt;&gt;&gt; input = torch.randn(1, 64, 8, 9, 10) &gt;&gt;&gt; output = m(input) &gt;&gt;&gt; # target output size of 7x7x7 (cube) &gt;&gt;&gt; m = nn.AdaptiveAvgPool3d(7) &gt;&gt;&gt; input = torch.randn(1, 64, 10, 9, 8) &gt;&gt;&gt; output = m(input) &gt;&gt;&gt; # target output size of 7x9x8 &gt;&gt;&gt; m = nn.AdaptiveMaxPool3d((7, None, None)) &gt;&gt;&gt; input = torch.randn(1, 64, 10, 9, 8) &gt;&gt;&gt; output = m(input)   ", "parameters": ["output_size : the target output size of the form D x H x W.Can be a tuple (D, H, W) or a single number D for a cube D x D x D.D, H and W can be either a int, or None which means the size willbe the same as that of the input."], "returns": null, "example": " # target output size of 5x7x9\n m = nn.AdaptiveAvgPool3d((5,7,9))\n input = torch.randn(1, 64, 8, 9, 10)\n output = m(input)\n # target output size of 7x7x7 (cube)\n m = nn.AdaptiveAvgPool3d(7)\n input = torch.randn(1, 64, 10, 9, 8)\n output = m(input)\n # target output size of 7x9x8\n m = nn.AdaptiveMaxPool3d((7, None, None))\n input = torch.randn(1, 64, 10, 9, 8)\n output = m(input)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.ReflectionPad1d", "item_type": "class", "code": "classtorch.nn.ReflectionPad1d(padding)", "description": "Pads the input tensor using the reflection of the input boundary. For N-dimensional padding, use torch.nn.functional.pad().  Parameters padding (python:int, tuple) \u2013 the size of the padding. If is int, uses the same padding in all boundaries. If a 2-tuple, uses (padding_left\\text{padding\\_left}padding_left  , padding_right\\text{padding\\_right}padding_right  )    Shape: Input: (N,C,Win)(N, C, W_{in})(N,C,Win\u200b)   Output: (N,C,Wout)(N, C, W_{out})(N,C,Wout\u200b)   where Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout\u200b=Win\u200b+padding_left+padding_right       Examples: &gt;&gt;&gt; m = nn.ReflectionPad1d(2) &gt;&gt;&gt; input = torch.arange(8, dtype=torch.float).reshape(1, 2, 4) &gt;&gt;&gt; input tensor([[[0., 1., 2., 3.],          [4., 5., 6., 7.]]]) &gt;&gt;&gt; m(input) tensor([[[2., 1., 0., 1., 2., 3., 2., 1.],          [6., 5., 4., 5., 6., 7., 6., 5.]]]) &gt;&gt;&gt; # using different paddings for different sides &gt;&gt;&gt; m = nn.ReflectionPad1d((3, 1)) &gt;&gt;&gt; m(input) tensor([[[3., 2., 1., 0., 1., 2., 3., 2.],          [7., 6., 5., 4., 5., 6., 7., 6.]]])   ", "parameters": ["padding (python:int, tuple) : the size of the padding. If is int, uses the samepadding in all boundaries. If a 2-tuple, uses(padding_left\\text{padding\\_left}padding_left, padding_right\\text{padding\\_right}padding_right)"], "returns": null, "example": " m = nn.ReflectionPad1d(2)\n input = torch.arange(8, dtype=torch.float).reshape(1, 2, 4)\n input\ntensor([[[0., 1., 2., 3.],\n         [4., 5., 6., 7.]]])\n m(input)\ntensor([[[2., 1., 0., 1., 2., 3., 2., 1.],\n         [6., 5., 4., 5., 6., 7., 6., 5.]]])\n # using different paddings for different sides\n m = nn.ReflectionPad1d((3, 1))\n m(input)\ntensor([[[3., 2., 1., 0., 1., 2., 3., 2.],\n         [7., 6., 5., 4., 5., 6., 7., 6.]]])\n\n", "shape": " Input: (N,C,Win)(N, C, W_{in})(N,C,Win\u200b)   Output: (N,C,Wout)(N, C, W_{out})(N,C,Wout\u200b)   where Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout\u200b=Win\u200b+padding_left+padding_right     "},
{"library": "torch", "item_id": "torch.isnan", "item_type": "function", "code": "torch.isnan()", "description": "Returns a new tensor with boolean elements representing if each element is NaN or not.  Parameters input (Tensor) \u2013 A tensor to check  Returns A torch.BoolTensor containing a True at each location of NaN elements.  Return type Tensor   Example: &gt;&gt;&gt; torch.isnan(torch.tensor([1, float('nan'), 2])) tensor([False, True, False])   ", "parameters": ["input (Tensor) : A tensor to check", "A torch.BoolTensor containing a True at each location of NaN elements.", "Tensor"], "returns": "A torch.BoolTensor containing a True at each location of NaN elements.", "example": " torch.isnan(torch.tensor([1, float('nan'), 2]))\ntensor([False, True, False])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.kthvalue", "item_type": "function", "code": "torch.kthvalue(input,k,dim=None,keepdim=False,out=None)-&gt;(Tensor,LongTensor)", "description": "Returns a namedtuple (values, indices) where values is the k th smallest element of each row of the input tensor in the given dimension dim. And indices is the index location of each element found. If dim is not given, the last dimension of the input is chosen. If keepdim is True, both the values and indices tensors are the same size as input, except in the dimension dim where they are of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in both the values and indices tensors having 1 fewer dimension than the input tensor.  Parameters  input (Tensor) \u2013 the input tensor. k (python:int) \u2013 k for the k-th smallest element dim (python:int, optional) \u2013 the dimension to find the kth value along keepdim (bool) \u2013 whether the output tensor has dim retained or not. out (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) can be optionally given to be used as output buffers    Example: &gt;&gt;&gt; x = torch.arange(1., 6.) &gt;&gt;&gt; x tensor([ 1.,  2.,  3.,  4.,  5.]) &gt;&gt;&gt; torch.kthvalue(x, 4) torch.return_types.kthvalue(values=tensor(4.), indices=tensor(3))  &gt;&gt;&gt; x=torch.arange(1.,7.).resize_(2,3) &gt;&gt;&gt; x tensor([[ 1.,  2.,  3.],         [ 4.,  5.,  6.]]) &gt;&gt;&gt; torch.kthvalue(x, 2, 0, True) torch.return_types.kthvalue(values=tensor([[4., 5., 6.]]), indices=tensor([[1, 1, 1]]))   ", "parameters": ["input (Tensor) : the input tensor.", "k (python:int) : k for the k-th smallest element", "dim (python:int, optional) : the dimension to find the kth value along", "keepdim (bool) : whether the output tensor has dim retained or not.", "out (tuple, optional) : the output tuple of (Tensor, LongTensor)can be optionally given to be used as output buffers"], "returns": null, "example": " x = torch.arange(1., 6.)\n x\ntensor([ 1.,  2.,  3.,  4.,  5.])\n torch.kthvalue(x, 4)\ntorch.return_types.kthvalue(values=tensor(4.), indices=tensor(3))\n\n x=torch.arange(1.,7.).resize_(2,3)\n x\ntensor([[ 1.,  2.,  3.],\n        [ 4.,  5.,  6.]])\n torch.kthvalue(x, 2, 0, True)\ntorch.return_types.kthvalue(values=tensor([[4., 5., 6.]]), indices=tensor([[1, 1, 1]]))\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.le", "item_type": "function", "code": "torch.le(input,other,out=None)\u2192Tensor", "description": "Computes input\u2264other\\text{input} \\leq \\text{other}input\u2264other   element-wise. The second argument can be a number or a tensor whose shape is broadcastable with the first argument.  Parameters  input (Tensor) \u2013 the tensor to compare other (Tensor or python:float) \u2013 the tensor or value to compare out (Tensor, optional) \u2013 the output tensor that must be a BoolTensor   Returns A torch.BoolTensor containing a True at each location where comparison is true  Return type Tensor   Example: &gt;&gt;&gt; torch.le(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]])) tensor([[True, False], [True, True]])   ", "parameters": ["input (Tensor) : the tensor to compare", "other (Tensor or python:float) : the tensor or value to compare", "out (Tensor, optional) : the output tensor that must be a BoolTensor"], "returns": "A torch.BoolTensor containing a True at each location where comparison is true", "example": " torch.le(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\ntensor([[True, False], [True, True]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.lt", "item_type": "function", "code": "torch.lt(input,other,out=None)\u2192Tensor", "description": "Computes input&lt;other\\text{input} &lt; \\text{other}input&lt;other   element-wise. The second argument can be a number or a tensor whose shape is broadcastable with the first argument.  Parameters  input (Tensor) \u2013 the tensor to compare other (Tensor or python:float) \u2013 the tensor or value to compare out (Tensor, optional) \u2013 the output tensor that must be a BoolTensor   Returns A torch.BoolTensor containing a True at each location where comparison is true  Return type Tensor   Example: &gt;&gt;&gt; torch.lt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]])) tensor([[False, False], [True, False]])   ", "parameters": ["input (Tensor) : the tensor to compare", "other (Tensor or python:float) : the tensor or value to compare", "out (Tensor, optional) : the output tensor that must be a BoolTensor"], "returns": "A torch.BoolTensor containing a True at each location where comparison is true", "example": " torch.lt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\ntensor([[False, False], [True, False]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.ReflectionPad2d", "item_type": "class", "code": "classtorch.nn.ReflectionPad2d(padding)", "description": "Pads the input tensor using the reflection of the input boundary. For N-dimensional padding, use torch.nn.functional.pad().  Parameters padding (python:int, tuple) \u2013 the size of the padding. If is int, uses the same padding in all boundaries. If a 4-tuple, uses (padding_left\\text{padding\\_left}padding_left  , padding_right\\text{padding\\_right}padding_right  , padding_top\\text{padding\\_top}padding_top  , padding_bottom\\text{padding\\_bottom}padding_bottom  )    Shape: Input: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)   Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)   where Hout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}Hout\u200b=Hin\u200b+padding_top+padding_bottom   Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout\u200b=Win\u200b+padding_left+padding_right       Examples: &gt;&gt;&gt; m = nn.ReflectionPad2d(2) &gt;&gt;&gt; input = torch.arange(9, dtype=torch.float).reshape(1, 1, 3, 3) &gt;&gt;&gt; input tensor([[[[0., 1., 2.],           [3., 4., 5.],           [6., 7., 8.]]]]) &gt;&gt;&gt; m(input) tensor([[[[8., 7., 6., 7., 8., 7., 6.],           [5., 4., 3., 4., 5., 4., 3.],           [2., 1., 0., 1., 2., 1., 0.],           [5., 4., 3., 4., 5., 4., 3.],           [8., 7., 6., 7., 8., 7., 6.],           [5., 4., 3., 4., 5., 4., 3.],           [2., 1., 0., 1., 2., 1., 0.]]]]) &gt;&gt;&gt; # using different paddings for different sides &gt;&gt;&gt; m = nn.ReflectionPad2d((1, 1, 2, 0)) &gt;&gt;&gt; m(input) tensor([[[[7., 6., 7., 8., 7.],           [4., 3., 4., 5., 4.],           [1., 0., 1., 2., 1.],           [4., 3., 4., 5., 4.],           [7., 6., 7., 8., 7.]]]])   ", "parameters": ["padding (python:int, tuple) : the size of the padding. If is int, uses the samepadding in all boundaries. If a 4-tuple, uses (padding_left\\text{padding\\_left}padding_left,padding_right\\text{padding\\_right}padding_right, padding_top\\text{padding\\_top}padding_top, padding_bottom\\text{padding\\_bottom}padding_bottom)"], "returns": null, "example": " m = nn.ReflectionPad2d(2)\n input = torch.arange(9, dtype=torch.float).reshape(1, 1, 3, 3)\n input\ntensor([[[[0., 1., 2.],\n          [3., 4., 5.],\n          [6., 7., 8.]]]])\n m(input)\ntensor([[[[8., 7., 6., 7., 8., 7., 6.],\n          [5., 4., 3., 4., 5., 4., 3.],\n          [2., 1., 0., 1., 2., 1., 0.],\n          [5., 4., 3., 4., 5., 4., 3.],\n          [8., 7., 6., 7., 8., 7., 6.],\n          [5., 4., 3., 4., 5., 4., 3.],\n          [2., 1., 0., 1., 2., 1., 0.]]]])\n # using different paddings for different sides\n m = nn.ReflectionPad2d((1, 1, 2, 0))\n m(input)\ntensor([[[[7., 6., 7., 8., 7.],\n          [4., 3., 4., 5., 4.],\n          [1., 0., 1., 2., 1.],\n          [4., 3., 4., 5., 4.],\n          [7., 6., 7., 8., 7.]]]])\n\n", "shape": " Input: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)   Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)   where Hout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}Hout\u200b=Hin\u200b+padding_top+padding_bottom   Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout\u200b=Win\u200b+padding_left+padding_right     "},
{"library": "torch", "item_id": "torch.nn.ReplicationPad1d", "item_type": "class", "code": "classtorch.nn.ReplicationPad1d(padding)", "description": "Pads the input tensor using replication of the input boundary. For N-dimensional padding, use torch.nn.functional.pad().  Parameters padding (python:int, tuple) \u2013 the size of the padding. If is int, uses the same padding in all boundaries. If a 2-tuple, uses (padding_left\\text{padding\\_left}padding_left  , padding_right\\text{padding\\_right}padding_right  )    Shape: Input: (N,C,Win)(N, C, W_{in})(N,C,Win\u200b)   Output: (N,C,Wout)(N, C, W_{out})(N,C,Wout\u200b)   where Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout\u200b=Win\u200b+padding_left+padding_right       Examples: &gt;&gt;&gt; m = nn.ReplicationPad1d(2) &gt;&gt;&gt; input = torch.arange(8, dtype=torch.float).reshape(1, 2, 4) &gt;&gt;&gt; input tensor([[[0., 1., 2., 3.],          [4., 5., 6., 7.]]]) &gt;&gt;&gt; m(input) tensor([[[0., 0., 0., 1., 2., 3., 3., 3.],          [4., 4., 4., 5., 6., 7., 7., 7.]]]) &gt;&gt;&gt; # using different paddings for different sides &gt;&gt;&gt; m = nn.ReplicationPad1d((3, 1)) &gt;&gt;&gt; m(input) tensor([[[0., 0., 0., 0., 1., 2., 3., 3.],          [4., 4., 4., 4., 5., 6., 7., 7.]]])   ", "parameters": ["padding (python:int, tuple) : the size of the padding. If is int, uses the samepadding in all boundaries. If a 2-tuple, uses(padding_left\\text{padding\\_left}padding_left, padding_right\\text{padding\\_right}padding_right)"], "returns": null, "example": " m = nn.ReplicationPad1d(2)\n input = torch.arange(8, dtype=torch.float).reshape(1, 2, 4)\n input\ntensor([[[0., 1., 2., 3.],\n         [4., 5., 6., 7.]]])\n m(input)\ntensor([[[0., 0., 0., 1., 2., 3., 3., 3.],\n         [4., 4., 4., 5., 6., 7., 7., 7.]]])\n # using different paddings for different sides\n m = nn.ReplicationPad1d((3, 1))\n m(input)\ntensor([[[0., 0., 0., 0., 1., 2., 3., 3.],\n         [4., 4., 4., 4., 5., 6., 7., 7.]]])\n\n", "shape": " Input: (N,C,Win)(N, C, W_{in})(N,C,Win\u200b)   Output: (N,C,Wout)(N, C, W_{out})(N,C,Wout\u200b)   where Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout\u200b=Win\u200b+padding_left+padding_right     "},
{"library": "torch", "item_id": "torch.nn.ReplicationPad2d", "item_type": "class", "code": "classtorch.nn.ReplicationPad2d(padding)", "description": "Pads the input tensor using replication of the input boundary. For N-dimensional padding, use torch.nn.functional.pad().  Parameters padding (python:int, tuple) \u2013 the size of the padding. If is int, uses the same padding in all boundaries. If a 4-tuple, uses (padding_left\\text{padding\\_left}padding_left  , padding_right\\text{padding\\_right}padding_right  , padding_top\\text{padding\\_top}padding_top  , padding_bottom\\text{padding\\_bottom}padding_bottom  )    Shape: Input: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)   Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)   where Hout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}Hout\u200b=Hin\u200b+padding_top+padding_bottom   Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout\u200b=Win\u200b+padding_left+padding_right       Examples: &gt;&gt;&gt; m = nn.ReplicationPad2d(2) &gt;&gt;&gt; input = torch.arange(9, dtype=torch.float).reshape(1, 1, 3, 3) &gt;&gt;&gt; input tensor([[[[0., 1., 2.],           [3., 4., 5.],           [6., 7., 8.]]]]) &gt;&gt;&gt; m(input) tensor([[[[0., 0., 0., 1., 2., 2., 2.],           [0., 0., 0., 1., 2., 2., 2.],           [0., 0., 0., 1., 2., 2., 2.],           [3., 3., 3., 4., 5., 5., 5.],           [6., 6., 6., 7., 8., 8., 8.],           [6., 6., 6., 7., 8., 8., 8.],           [6., 6., 6., 7., 8., 8., 8.]]]]) &gt;&gt;&gt; # using different paddings for different sides &gt;&gt;&gt; m = nn.ReplicationPad2d((1, 1, 2, 0)) &gt;&gt;&gt; m(input) tensor([[[[0., 0., 1., 2., 2.],           [0., 0., 1., 2., 2.],           [0., 0., 1., 2., 2.],           [3., 3., 4., 5., 5.],           [6., 6., 7., 8., 8.]]]])   ", "parameters": ["padding (python:int, tuple) : the size of the padding. If is int, uses the samepadding in all boundaries. If a 4-tuple, uses (padding_left\\text{padding\\_left}padding_left,padding_right\\text{padding\\_right}padding_right, padding_top\\text{padding\\_top}padding_top, padding_bottom\\text{padding\\_bottom}padding_bottom)"], "returns": null, "example": " m = nn.ReplicationPad2d(2)\n input = torch.arange(9, dtype=torch.float).reshape(1, 1, 3, 3)\n input\ntensor([[[[0., 1., 2.],\n          [3., 4., 5.],\n          [6., 7., 8.]]]])\n m(input)\ntensor([[[[0., 0., 0., 1., 2., 2., 2.],\n          [0., 0., 0., 1., 2., 2., 2.],\n          [0., 0., 0., 1., 2., 2., 2.],\n          [3., 3., 3., 4., 5., 5., 5.],\n          [6., 6., 6., 7., 8., 8., 8.],\n          [6., 6., 6., 7., 8., 8., 8.],\n          [6., 6., 6., 7., 8., 8., 8.]]]])\n # using different paddings for different sides\n m = nn.ReplicationPad2d((1, 1, 2, 0))\n m(input)\ntensor([[[[0., 0., 1., 2., 2.],\n          [0., 0., 1., 2., 2.],\n          [0., 0., 1., 2., 2.],\n          [3., 3., 4., 5., 5.],\n          [6., 6., 7., 8., 8.]]]])\n\n", "shape": " Input: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)   Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)   where Hout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}Hout\u200b=Hin\u200b+padding_top+padding_bottom   Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout\u200b=Win\u200b+padding_left+padding_right     "},
{"library": "torch", "item_id": "torch.max", "item_type": "function", "code": "torch.max()", "description": "  torch.max(input) \u2192 Tensor   Returns the maximum value of all elements in the input tensor.  Parameters {input} \u2013    Example: &gt;&gt;&gt; a = torch.randn(1, 3) &gt;&gt;&gt; a tensor([[ 0.6763,  0.7445, -2.2369]]) &gt;&gt;&gt; torch.max(a) tensor(0.7445)     torch.max(input, dim, keepdim=False, out=None) -&gt; (Tensor, LongTensor)   Returns a namedtuple (values, indices) where values is the maximum value of each row of the input tensor in the given dimension dim. And indices is the index location of each maximum value found (argmax). If keepdim is True, the output tensors are of the same size as input except in the dimension dim where they are of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensors having 1 fewer dimension than input.  Parameters  {input} \u2013  {dim} \u2013  Default ({keepdim}) \u2013 False. out (tuple, optional) \u2013 the result tuple of two output tensors (max, max_indices)    Example: &gt;&gt;&gt; a = torch.randn(4, 4) &gt;&gt;&gt; a tensor([[-1.2360, -0.2942, -0.1222,  0.8475],         [ 1.1949, -1.1127, -2.2379, -0.6702],         [ 1.5717, -0.9207,  0.1297, -1.8768],         [-0.6172,  1.0036, -0.6060, -0.2432]]) &gt;&gt;&gt; torch.max(a, 1) torch.return_types.max(values=tensor([0.8475, 1.1949, 1.5717, 1.0036]), indices=tensor([3, 0, 0, 1]))     torch.max(input, other, out=None) \u2192 Tensor   Each element of the tensor input is compared with the corresponding element of the tensor other and an element-wise maximum is taken. The shapes of input and other don\u2019t need to match, but they must be broadcastable.  outi=max\u2061(tensori,otheri)\\text{out}_i = \\max(\\text{tensor}_i, \\text{other}_i)  outi\u200b=max(tensori\u200b,otheri\u200b)   Note When the shapes do not match, the shape of the returned output tensor follows the broadcasting rules.   Parameters  input (Tensor) \u2013 the input tensor. other (Tensor) \u2013 the second input tensor out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a tensor([ 0.2942, -0.7416,  0.2653, -0.1584]) &gt;&gt;&gt; b = torch.randn(4) &gt;&gt;&gt; b tensor([ 0.8722, -1.7421, -0.4141, -0.5055]) &gt;&gt;&gt; torch.max(a, b) tensor([ 0.8722, -0.7416,  0.2653, -0.1584])   ", "parameters": ["{input} : ", "{dim} : ", "Default ({keepdim}) : False.", "out (tuple, optional) : the result tuple of two output tensors (max, max_indices)", "input (Tensor) : the input tensor.", "other (Tensor) : the second input tensor", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " a = torch.randn(1, 3)\n a\ntensor([[ 0.6763,  0.7445, -2.2369]])\n torch.max(a)\ntensor(0.7445)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.max(input)\u2192Tensor", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.max(input,dim,keepdim=False,out=None)-&gt;(Tensor,LongTensor)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.max(input,other,out=None)\u2192Tensor", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.min", "item_type": "function", "code": "torch.min()", "description": "  torch.min(input) \u2192 Tensor   Returns the minimum value of all elements in the input tensor.  Parameters {input} \u2013    Example: &gt;&gt;&gt; a = torch.randn(1, 3) &gt;&gt;&gt; a tensor([[ 0.6750,  1.0857,  1.7197]]) &gt;&gt;&gt; torch.min(a) tensor(0.6750)     torch.min(input, dim, keepdim=False, out=None) -&gt; (Tensor, LongTensor)   Returns a namedtuple (values, indices) where values is the minimum value of each row of the input tensor in the given dimension dim. And indices is the index location of each minimum value found (argmin). If keepdim is True, the output tensors are of the same size as input except in the dimension dim where they are of size 1. Otherwise, dim is squeezed (see torch.squeeze()), resulting in the output tensors having 1 fewer dimension than input.  Parameters  {input} \u2013  {dim} \u2013  {keepdim} \u2013  out (tuple, optional) \u2013 the tuple of two output tensors (min, min_indices)    Example: &gt;&gt;&gt; a = torch.randn(4, 4) &gt;&gt;&gt; a tensor([[-0.6248,  1.1334, -1.1899, -0.2803],         [-1.4644, -0.2635, -0.3651,  0.6134],         [ 0.2457,  0.0384,  1.0128,  0.7015],         [-0.1153,  2.9849,  2.1458,  0.5788]]) &gt;&gt;&gt; torch.min(a, 1) torch.return_types.min(values=tensor([-1.1899, -1.4644,  0.0384, -0.1153]), indices=tensor([2, 0, 1, 0]))     torch.min(input, other, out=None) \u2192 Tensor   Each element of the tensor input is compared with the corresponding element of the tensor other and an element-wise minimum is taken. The resulting tensor is returned. The shapes of input and other don\u2019t need to match, but they must be broadcastable.  outi=min\u2061(tensori,otheri)\\text{out}_i = \\min(\\text{tensor}_i, \\text{other}_i)  outi\u200b=min(tensori\u200b,otheri\u200b)   Note When the shapes do not match, the shape of the returned output tensor follows the broadcasting rules.   Parameters  input (Tensor) \u2013 the input tensor. other (Tensor) \u2013 the second input tensor out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; a = torch.randn(4) &gt;&gt;&gt; a tensor([ 0.8137, -1.1740, -0.6460,  0.6308]) &gt;&gt;&gt; b = torch.randn(4) &gt;&gt;&gt; b tensor([-0.1369,  0.1555,  0.4019, -0.1929]) &gt;&gt;&gt; torch.min(a, b) tensor([-0.1369, -1.1740, -0.6460, -0.1929])   ", "parameters": ["{input} : ", "{dim} : ", "{keepdim} : ", "out (tuple, optional) : the tuple of two output tensors (min, min_indices)", "input (Tensor) : the input tensor.", "other (Tensor) : the second input tensor", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " a = torch.randn(1, 3)\n a\ntensor([[ 0.6750,  1.0857,  1.7197]])\n torch.min(a)\ntensor(0.6750)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.ReplicationPad3d", "item_type": "class", "code": "classtorch.nn.ReplicationPad3d(padding)", "description": "Pads the input tensor using replication of the input boundary. For N-dimensional padding, use torch.nn.functional.pad().  Parameters padding (python:int, tuple) \u2013 the size of the padding. If is int, uses the same padding in all boundaries. If a 6-tuple, uses (padding_left\\text{padding\\_left}padding_left  , padding_right\\text{padding\\_right}padding_right  , padding_top\\text{padding\\_top}padding_top  , padding_bottom\\text{padding\\_bottom}padding_bottom  , padding_front\\text{padding\\_front}padding_front  , padding_back\\text{padding\\_back}padding_back  )    Shape: Input: (N,C,Din,Hin,Win)(N, C, D_{in}, H_{in}, W_{in})(N,C,Din\u200b,Hin\u200b,Win\u200b)   Output: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\u200b,Hout\u200b,Wout\u200b)   where Dout=Din+padding_front+padding_backD_{out} = D_{in} + \\text{padding\\_front} + \\text{padding\\_back}Dout\u200b=Din\u200b+padding_front+padding_back   Hout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}Hout\u200b=Hin\u200b+padding_top+padding_bottom   Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout\u200b=Win\u200b+padding_left+padding_right       Examples: &gt;&gt;&gt; m = nn.ReplicationPad3d(3) &gt;&gt;&gt; input = torch.randn(16, 3, 8, 320, 480) &gt;&gt;&gt; output = m(input) &gt;&gt;&gt; # using different paddings for different sides &gt;&gt;&gt; m = nn.ReplicationPad3d((3, 3, 6, 6, 1, 1)) &gt;&gt;&gt; output = m(input)   ", "parameters": ["padding (python:int, tuple) : the size of the padding. If is int, uses the samepadding in all boundaries. If a 6-tuple, uses(padding_left\\text{padding\\_left}padding_left, padding_right\\text{padding\\_right}padding_right,padding_top\\text{padding\\_top}padding_top, padding_bottom\\text{padding\\_bottom}padding_bottom,padding_front\\text{padding\\_front}padding_front, padding_back\\text{padding\\_back}padding_back)"], "returns": null, "example": " m = nn.ReplicationPad3d(3)\n input = torch.randn(16, 3, 8, 320, 480)\n output = m(input)\n # using different paddings for different sides\n m = nn.ReplicationPad3d((3, 3, 6, 6, 1, 1))\n output = m(input)\n\n", "shape": " Input: (N,C,Din,Hin,Win)(N, C, D_{in}, H_{in}, W_{in})(N,C,Din\u200b,Hin\u200b,Win\u200b)   Output: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\u200b,Hout\u200b,Wout\u200b)   where Dout=Din+padding_front+padding_backD_{out} = D_{in} + \\text{padding\\_front} + \\text{padding\\_back}Dout\u200b=Din\u200b+padding_front+padding_back   Hout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}Hout\u200b=Hin\u200b+padding_top+padding_bottom   Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout\u200b=Win\u200b+padding_left+padding_right     "},
{"library": "torch", "item_id": "torch.nn.ZeroPad2d", "item_type": "class", "code": "classtorch.nn.ZeroPad2d(padding)", "description": "Pads the input tensor boundaries with zero. For N-dimensional padding, use torch.nn.functional.pad().  Parameters padding (python:int, tuple) \u2013 the size of the padding. If is int, uses the same padding in all boundaries. If a 4-tuple, uses (padding_left\\text{padding\\_left}padding_left  , padding_right\\text{padding\\_right}padding_right  , padding_top\\text{padding\\_top}padding_top  , padding_bottom\\text{padding\\_bottom}padding_bottom  )    Shape: Input: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)   Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)   where Hout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}Hout\u200b=Hin\u200b+padding_top+padding_bottom   Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout\u200b=Win\u200b+padding_left+padding_right       Examples: &gt;&gt;&gt; m = nn.ZeroPad2d(2) &gt;&gt;&gt; input = torch.randn(1, 1, 3, 3) &gt;&gt;&gt; input tensor([[[[-0.1678, -0.4418,  1.9466],           [ 0.9604, -0.4219, -0.5241],           [-0.9162, -0.5436, -0.6446]]]]) &gt;&gt;&gt; m(input) tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],           [ 0.0000,  0.0000, -0.1678, -0.4418,  1.9466,  0.0000,  0.0000],           [ 0.0000,  0.0000,  0.9604, -0.4219, -0.5241,  0.0000,  0.0000],           [ 0.0000,  0.0000, -0.9162, -0.5436, -0.6446,  0.0000,  0.0000],           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]]) &gt;&gt;&gt; # using different paddings for different sides &gt;&gt;&gt; m = nn.ZeroPad2d((1, 1, 2, 0)) &gt;&gt;&gt; m(input) tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],           [ 0.0000, -0.1678, -0.4418,  1.9466,  0.0000],           [ 0.0000,  0.9604, -0.4219, -0.5241,  0.0000],           [ 0.0000, -0.9162, -0.5436, -0.6446,  0.0000]]]])   ", "parameters": ["padding (python:int, tuple) : the size of the padding. If is int, uses the samepadding in all boundaries. If a 4-tuple, uses (padding_left\\text{padding\\_left}padding_left,padding_right\\text{padding\\_right}padding_right, padding_top\\text{padding\\_top}padding_top, padding_bottom\\text{padding\\_bottom}padding_bottom)"], "returns": null, "example": " m = nn.ZeroPad2d(2)\n input = torch.randn(1, 1, 3, 3)\n input\ntensor([[[[-0.1678, -0.4418,  1.9466],\n          [ 0.9604, -0.4219, -0.5241],\n          [-0.9162, -0.5436, -0.6446]]]])\n m(input)\ntensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000, -0.1678, -0.4418,  1.9466,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.9604, -0.4219, -0.5241,  0.0000,  0.0000],\n          [ 0.0000,  0.0000, -0.9162, -0.5436, -0.6446,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]])\n # using different paddings for different sides\n m = nn.ZeroPad2d((1, 1, 2, 0))\n m(input)\ntensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n          [ 0.0000, -0.1678, -0.4418,  1.9466,  0.0000],\n          [ 0.0000,  0.9604, -0.4219, -0.5241,  0.0000],\n          [ 0.0000, -0.9162, -0.5436, -0.6446,  0.0000]]]])\n\n", "shape": " Input: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)   Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)   where Hout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}Hout\u200b=Hin\u200b+padding_top+padding_bottom   Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout\u200b=Win\u200b+padding_left+padding_right     "},
{"library": "torch", "item_id": "torch.nn.ConstantPad1d", "item_type": "class", "code": "classtorch.nn.ConstantPad1d(padding,value)", "description": "Pads the input tensor boundaries with a constant value. For N-dimensional padding, use torch.nn.functional.pad().  Parameters padding (python:int, tuple) \u2013 the size of the padding. If is int, uses the same padding in both boundaries. If a 2-tuple, uses (padding_left\\text{padding\\_left}padding_left  , padding_right\\text{padding\\_right}padding_right  )    Shape: Input: (N,C,Win)(N, C, W_{in})(N,C,Win\u200b)   Output: (N,C,Wout)(N, C, W_{out})(N,C,Wout\u200b)   where Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout\u200b=Win\u200b+padding_left+padding_right       Examples: &gt;&gt;&gt; m = nn.ConstantPad1d(2, 3.5) &gt;&gt;&gt; input = torch.randn(1, 2, 4) &gt;&gt;&gt; input tensor([[[-1.0491, -0.7152, -0.0749,  0.8530],          [-1.3287,  1.8966,  0.1466, -0.2771]]]) &gt;&gt;&gt; m(input) tensor([[[ 3.5000,  3.5000, -1.0491, -0.7152, -0.0749,  0.8530,  3.5000,            3.5000],          [ 3.5000,  3.5000, -1.3287,  1.8966,  0.1466, -0.2771,  3.5000,            3.5000]]]) &gt;&gt;&gt; m = nn.ConstantPad1d(2, 3.5) &gt;&gt;&gt; input = torch.randn(1, 2, 3) &gt;&gt;&gt; input tensor([[[ 1.6616,  1.4523, -1.1255],          [-3.6372,  0.1182, -1.8652]]]) &gt;&gt;&gt; m(input) tensor([[[ 3.5000,  3.5000,  1.6616,  1.4523, -1.1255,  3.5000,  3.5000],          [ 3.5000,  3.5000, -3.6372,  0.1182, -1.8652,  3.5000,  3.5000]]]) &gt;&gt;&gt; # using different paddings for different sides &gt;&gt;&gt; m = nn.ConstantPad1d((3, 1), 3.5) &gt;&gt;&gt; m(input) tensor([[[ 3.5000,  3.5000,  3.5000,  1.6616,  1.4523, -1.1255,  3.5000],          [ 3.5000,  3.5000,  3.5000, -3.6372,  0.1182, -1.8652,  3.5000]]])   ", "parameters": ["padding (python:int, tuple) : the size of the padding. If is int, uses the samepadding in both boundaries. If a 2-tuple, uses(padding_left\\text{padding\\_left}padding_left, padding_right\\text{padding\\_right}padding_right)"], "returns": null, "example": " m = nn.ConstantPad1d(2, 3.5)\n input = torch.randn(1, 2, 4)\n input\ntensor([[[-1.0491, -0.7152, -0.0749,  0.8530],\n         [-1.3287,  1.8966,  0.1466, -0.2771]]])\n m(input)\ntensor([[[ 3.5000,  3.5000, -1.0491, -0.7152, -0.0749,  0.8530,  3.5000,\n           3.5000],\n         [ 3.5000,  3.5000, -1.3287,  1.8966,  0.1466, -0.2771,  3.5000,\n           3.5000]]])\n m = nn.ConstantPad1d(2, 3.5)\n input = torch.randn(1, 2, 3)\n input\ntensor([[[ 1.6616,  1.4523, -1.1255],\n         [-3.6372,  0.1182, -1.8652]]])\n m(input)\ntensor([[[ 3.5000,  3.5000,  1.6616,  1.4523, -1.1255,  3.5000,  3.5000],\n         [ 3.5000,  3.5000, -3.6372,  0.1182, -1.8652,  3.5000,  3.5000]]])\n # using different paddings for different sides\n m = nn.ConstantPad1d((3, 1), 3.5)\n m(input)\ntensor([[[ 3.5000,  3.5000,  3.5000,  1.6616,  1.4523, -1.1255,  3.5000],\n         [ 3.5000,  3.5000,  3.5000, -3.6372,  0.1182, -1.8652,  3.5000]]])\n\n", "shape": " Input: (N,C,Win)(N, C, W_{in})(N,C,Win\u200b)   Output: (N,C,Wout)(N, C, W_{out})(N,C,Wout\u200b)   where Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout\u200b=Win\u200b+padding_left+padding_right     "},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.min(input)\u2192Tensor", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.min(input,dim,keepdim=False,out=None)-&gt;(Tensor,LongTensor)", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.min(input,other,out=None)\u2192Tensor", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.ne", "item_type": "function", "code": "torch.ne(input,other,out=None)\u2192Tensor", "description": "Computes input\u2260otherinput \\neq otherinput\ue020\u200b=other   element-wise. The second argument can be a number or a tensor whose shape is broadcastable with the first argument.  Parameters  input (Tensor) \u2013 the tensor to compare other (Tensor or python:float) \u2013 the tensor or value to compare out (Tensor, optional) \u2013 the output tensor that must be a BoolTensor   Returns A torch.BoolTensor containing a True at each location where comparison is true.  Return type Tensor   Example: &gt;&gt;&gt; torch.ne(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]])) tensor([[False, True], [True, False]])   ", "parameters": ["input (Tensor) : the tensor to compare", "other (Tensor or python:float) : the tensor or value to compare", "out (Tensor, optional) : the output tensor that must be a BoolTensor"], "returns": "A torch.BoolTensor containing a True at each location where comparison is true.", "example": " torch.ne(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\ntensor([[False, True], [True, False]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.sort", "item_type": "function", "code": "torch.sort(input,dim=-1,descending=False,out=None)-&gt;(Tensor,LongTensor)", "description": "Sorts the elements of the input tensor along a given dimension in ascending order by value. If dim is not given, the last dimension of the input is chosen. If descending is True then the elements are sorted in descending order by value. A namedtuple of (values, indices) is returned, where the values are the sorted values and indices are the indices of the elements in the original input tensor.  Parameters  input (Tensor) \u2013 the input tensor. dim (python:int, optional) \u2013 the dimension to sort along descending (bool, optional) \u2013 controls the sorting order (ascending or descending) out (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers    Example: &gt;&gt;&gt; x = torch.randn(3, 4) &gt;&gt;&gt; sorted, indices = torch.sort(x) &gt;&gt;&gt; sorted tensor([[-0.2162,  0.0608,  0.6719,  2.3332],         [-0.5793,  0.0061,  0.6058,  0.9497],         [-0.5071,  0.3343,  0.9553,  1.0960]]) &gt;&gt;&gt; indices tensor([[ 1,  0,  2,  3],         [ 3,  1,  0,  2],         [ 0,  3,  1,  2]])  &gt;&gt;&gt; sorted, indices = torch.sort(x, 0) &gt;&gt;&gt; sorted tensor([[-0.5071, -0.2162,  0.6719, -0.5793],         [ 0.0608,  0.0061,  0.9497,  0.3343],         [ 0.6058,  0.9553,  1.0960,  2.3332]]) &gt;&gt;&gt; indices tensor([[ 2,  0,  0,  1],         [ 0,  1,  1,  2],         [ 1,  2,  2,  0]])   ", "parameters": ["input (Tensor) : the input tensor.", "dim (python:int, optional) : the dimension to sort along", "descending (bool, optional) : controls the sorting order (ascending or descending)", "out (tuple, optional) : the output tuple of (Tensor, LongTensor) that canbe optionally given to be used as output buffers"], "returns": null, "example": " x = torch.randn(3, 4)\n sorted, indices = torch.sort(x)\n sorted\ntensor([[-0.2162,  0.0608,  0.6719,  2.3332],\n        [-0.5793,  0.0061,  0.6058,  0.9497],\n        [-0.5071,  0.3343,  0.9553,  1.0960]])\n indices\ntensor([[ 1,  0,  2,  3],\n        [ 3,  1,  0,  2],\n        [ 0,  3,  1,  2]])\n\n sorted, indices = torch.sort(x, 0)\n sorted\ntensor([[-0.5071, -0.2162,  0.6719, -0.5793],\n        [ 0.0608,  0.0061,  0.9497,  0.3343],\n        [ 0.6058,  0.9553,  1.0960,  2.3332]])\n indices\ntensor([[ 2,  0,  0,  1],\n        [ 0,  1,  1,  2],\n        [ 1,  2,  2,  0]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.topk", "item_type": "function", "code": "torch.topk(input,k,dim=None,largest=True,sorted=True,out=None)-&gt;(Tensor,LongTensor)", "description": "Returns the k largest elements of the given input tensor along a given dimension. If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices of the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned k elements are themselves sorted  Parameters  input (Tensor) \u2013 the input tensor. k (python:int) \u2013 the k in \u201ctop-k\u201d dim (python:int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or smallest elements sorted (bool, optional) \u2013 controls whether to return the elements in sorted order out (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) that can be optionally given to be used as output buffers    Example: &gt;&gt;&gt; x = torch.arange(1., 6.) &gt;&gt;&gt; x tensor([ 1.,  2.,  3.,  4.,  5.]) &gt;&gt;&gt; torch.topk(x, 3) torch.return_types.topk(values=tensor([5., 4., 3.]), indices=tensor([4, 3, 2]))   ", "parameters": ["input (Tensor) : the input tensor.", "k (python:int) : the k in \u201ctop-k\u201d", "dim (python:int, optional) : the dimension to sort along", "largest (bool, optional) : controls whether to return largest orsmallest elements", "sorted (bool, optional) : controls whether to return the elementsin sorted order", "out (tuple, optional) : the output tuple of (Tensor, LongTensor) that can beoptionally given to be used as output buffers"], "returns": null, "example": " x = torch.arange(1., 6.)\n x\ntensor([ 1.,  2.,  3.,  4.,  5.])\n torch.topk(x, 3)\ntorch.return_types.topk(values=tensor([5., 4., 3.]), indices=tensor([4, 3, 2]))\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.fft", "item_type": "function", "code": "torch.fft(input,signal_ndim,normalized=False)\u2192Tensor", "description": "Complex-to-complex Discrete Fourier Transform This method computes the complex-to-complex discrete Fourier transform. Ignoring the batch dimensions, it computes the following expression:  X[\u03c91,\u2026,\u03c9d]=\u2211n1=0N1\u22121\u22ef\u2211nd=0Nd\u22121x[n1,\u2026,nd]e\u2212j\u00a02\u03c0\u2211i=0d\u03c9iniNi,X[\\omega_1, \\dots, \\omega_d] =     \\sum_{n_1=0}^{N_1-1} \\dots \\sum_{n_d=0}^{N_d-1} x[n_1, \\dots, n_d]      e^{-j\\ 2 \\pi \\sum_{i=0}^d \\frac{\\omega_i n_i}{N_i}},  X[\u03c91\u200b,\u2026,\u03c9d\u200b]=n1\u200b=0\u2211N1\u200b\u22121\u200b\u22efnd\u200b=0\u2211Nd\u200b\u22121\u200bx[n1\u200b,\u2026,nd\u200b]e\u2212j\u00a02\u03c0\u2211i=0d\u200bNi\u200b\u03c9i\u200bni\u200b\u200b,  where ddd   = signal_ndim is number of dimensions for the signal, and NiN_iNi\u200b   is the size of signal dimension iii  . This method supports 1D, 2D and 3D complex-to-complex transforms, indicated by signal_ndim. input must be a tensor with last dimension of size 2, representing the real and imaginary components of complex numbers, and should have at least signal_ndim + 1 dimensions with optionally arbitrary number of leading batch dimensions. If normalized is set to True, this normalizes the result by dividing it with \u220fi=1KNi\\sqrt{\\prod_{i=1}^K N_i}\u220fi=1K\u200bNi\u200b\u200b   so that the operator is unitary. Returns the real and the imaginary parts together as one tensor of the same shape of input. The inverse of this function is ifft().  Note For CUDA tensors, an LRU cache is used for cuFFT plans to speed up repeatedly running FFT methods on tensors of same geometry with same configuration. See cuFFT plan cache for more details on how to monitor and control the cache.   Warning For CPU tensors, this method is currently only available with MKL. Use torch.backends.mkl.is_available() to check if MKL is installed.   Parameters  input (Tensor) \u2013 the input tensor of at least signal_ndim + 1 dimensions signal_ndim (python:int) \u2013 the number of dimensions in each signal. signal_ndim can only be 1, 2 or 3 normalized (bool, optional) \u2013 controls whether to return normalized results. Default: False   Returns A tensor containing the complex-to-complex Fourier transform result  Return type Tensor   Example: &gt;&gt;&gt; # unbatched 2D FFT &gt;&gt;&gt; x = torch.randn(4, 3, 2) &gt;&gt;&gt; torch.fft(x, 2) tensor([[[-0.0876,  1.7835],          [-2.0399, -2.9754],          [ 4.4773, -5.0119]],          [[-1.5716,  2.7631],          [-3.8846,  5.2652],          [ 0.2046, -0.7088]],          [[ 1.9938, -0.5901],          [ 6.5637,  6.4556],          [ 2.9865,  4.9318]],          [[ 7.0193,  1.1742],          [-1.3717, -2.1084],          [ 2.0289,  2.9357]]]) &gt;&gt;&gt; # batched 1D FFT &gt;&gt;&gt; torch.fft(x, 1) tensor([[[ 1.8385,  1.2827],          [-0.1831,  1.6593],          [ 2.4243,  0.5367]],          [[-0.9176, -1.5543],          [-3.9943, -2.9860],          [ 1.2838, -2.9420]],          [[-0.8854, -0.6860],          [ 2.4450,  0.0808],          [ 1.3076, -0.5768]],          [[-0.1231,  2.7411],          [-0.3075, -1.7295],          [-0.5384, -2.0299]]]) &gt;&gt;&gt; # arbitrary number of batch dimensions, 2D FFT &gt;&gt;&gt; x = torch.randn(3, 3, 5, 5, 2) &gt;&gt;&gt; y = torch.fft(x, 2) &gt;&gt;&gt; y.shape torch.Size([3, 3, 5, 5, 2])   ", "parameters": ["input (Tensor) : the input tensor of at least signal_ndim + 1dimensions", "signal_ndim (python:int) : the number of dimensions in each signal.signal_ndim can only be 1, 2 or 3", "normalized (bool, optional) : controls whether to return normalized results.Default: False"], "returns": "A tensor containing the complex-to-complex Fourier transform result", "example": " # unbatched 2D FFT\n x = torch.randn(4, 3, 2)\n torch.fft(x, 2)\ntensor([[[-0.0876,  1.7835],\n         [-2.0399, -2.9754],\n         [ 4.4773, -5.0119]],\n\n        [[-1.5716,  2.7631],\n         [-3.8846,  5.2652],\n         [ 0.2046, -0.7088]],\n\n        [[ 1.9938, -0.5901],\n         [ 6.5637,  6.4556],\n         [ 2.9865,  4.9318]],\n\n        [[ 7.0193,  1.1742],\n         [-1.3717, -2.1084],\n         [ 2.0289,  2.9357]]])\n # batched 1D FFT\n torch.fft(x, 1)\ntensor([[[ 1.8385,  1.2827],\n         [-0.1831,  1.6593],\n         [ 2.4243,  0.5367]],\n\n        [[-0.9176, -1.5543],\n         [-3.9943, -2.9860],\n         [ 1.2838, -2.9420]],\n\n        [[-0.8854, -0.6860],\n         [ 2.4450,  0.0808],\n         [ 1.3076, -0.5768]],\n\n        [[-0.1231,  2.7411],\n         [-0.3075, -1.7295],\n         [-0.5384, -2.0299]]])\n # arbitrary number of batch dimensions, 2D FFT\n x = torch.randn(3, 3, 5, 5, 2)\n y = torch.fft(x, 2)\n y.shape\ntorch.Size([3, 3, 5, 5, 2])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.ConstantPad2d", "item_type": "class", "code": "classtorch.nn.ConstantPad2d(padding,value)", "description": "Pads the input tensor boundaries with a constant value. For N-dimensional padding, use torch.nn.functional.pad().  Parameters padding (python:int, tuple) \u2013 the size of the padding. If is int, uses the same padding in all boundaries. If a 4-tuple, uses (padding_left\\text{padding\\_left}padding_left  , padding_right\\text{padding\\_right}padding_right  , padding_top\\text{padding\\_top}padding_top  , padding_bottom\\text{padding\\_bottom}padding_bottom  )    Shape: Input: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)   Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)   where Hout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}Hout\u200b=Hin\u200b+padding_top+padding_bottom   Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout\u200b=Win\u200b+padding_left+padding_right       Examples: &gt;&gt;&gt; m = nn.ConstantPad2d(2, 3.5) &gt;&gt;&gt; input = torch.randn(1, 2, 2) &gt;&gt;&gt; input tensor([[[ 1.6585,  0.4320],          [-0.8701, -0.4649]]]) &gt;&gt;&gt; m(input) tensor([[[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],          [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],          [ 3.5000,  3.5000,  1.6585,  0.4320,  3.5000,  3.5000],          [ 3.5000,  3.5000, -0.8701, -0.4649,  3.5000,  3.5000],          [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],          [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000]]]) &gt;&gt;&gt; # using different paddings for different sides &gt;&gt;&gt; m = nn.ConstantPad2d((3, 0, 2, 1), 3.5) &gt;&gt;&gt; m(input) tensor([[[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000],          [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000],          [ 3.5000,  3.5000,  3.5000,  1.6585,  0.4320],          [ 3.5000,  3.5000,  3.5000, -0.8701, -0.4649],          [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000]]])   ", "parameters": ["padding (python:int, tuple) : the size of the padding. If is int, uses the samepadding in all boundaries. If a 4-tuple, uses (padding_left\\text{padding\\_left}padding_left,padding_right\\text{padding\\_right}padding_right, padding_top\\text{padding\\_top}padding_top, padding_bottom\\text{padding\\_bottom}padding_bottom)"], "returns": null, "example": " m = nn.ConstantPad2d(2, 3.5)\n input = torch.randn(1, 2, 2)\n input\ntensor([[[ 1.6585,  0.4320],\n         [-0.8701, -0.4649]]])\n m(input)\ntensor([[[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],\n         [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],\n         [ 3.5000,  3.5000,  1.6585,  0.4320,  3.5000,  3.5000],\n         [ 3.5000,  3.5000, -0.8701, -0.4649,  3.5000,  3.5000],\n         [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],\n         [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000]]])\n # using different paddings for different sides\n m = nn.ConstantPad2d((3, 0, 2, 1), 3.5)\n m(input)\ntensor([[[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000],\n         [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000],\n         [ 3.5000,  3.5000,  3.5000,  1.6585,  0.4320],\n         [ 3.5000,  3.5000,  3.5000, -0.8701, -0.4649],\n         [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000]]])\n\n", "shape": " Input: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)   Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)   where Hout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}Hout\u200b=Hin\u200b+padding_top+padding_bottom   Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout\u200b=Win\u200b+padding_left+padding_right     "},
{"library": "torch", "item_id": "torch.nn.ConstantPad3d", "item_type": "class", "code": "classtorch.nn.ConstantPad3d(padding,value)", "description": "Pads the input tensor boundaries with a constant value. For N-dimensional padding, use torch.nn.functional.pad().  Parameters padding (python:int, tuple) \u2013 the size of the padding. If is int, uses the same padding in all boundaries. If a 6-tuple, uses (padding_left\\text{padding\\_left}padding_left  , padding_right\\text{padding\\_right}padding_right  , padding_top\\text{padding\\_top}padding_top  , padding_bottom\\text{padding\\_bottom}padding_bottom  , padding_front\\text{padding\\_front}padding_front  , padding_back\\text{padding\\_back}padding_back  )    Shape: Input: (N,C,Din,Hin,Win)(N, C, D_{in}, H_{in}, W_{in})(N,C,Din\u200b,Hin\u200b,Win\u200b)   Output: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\u200b,Hout\u200b,Wout\u200b)   where Dout=Din+padding_front+padding_backD_{out} = D_{in} + \\text{padding\\_front} + \\text{padding\\_back}Dout\u200b=Din\u200b+padding_front+padding_back   Hout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}Hout\u200b=Hin\u200b+padding_top+padding_bottom   Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout\u200b=Win\u200b+padding_left+padding_right       Examples: &gt;&gt;&gt; m = nn.ConstantPad3d(3, 3.5) &gt;&gt;&gt; input = torch.randn(16, 3, 10, 20, 30) &gt;&gt;&gt; output = m(input) &gt;&gt;&gt; # using different paddings for different sides &gt;&gt;&gt; m = nn.ConstantPad3d((3, 3, 6, 6, 0, 1), 3.5) &gt;&gt;&gt; output = m(input)   ", "parameters": ["padding (python:int, tuple) : the size of the padding. If is int, uses the samepadding in all boundaries. If a 6-tuple, uses(padding_left\\text{padding\\_left}padding_left, padding_right\\text{padding\\_right}padding_right,padding_top\\text{padding\\_top}padding_top, padding_bottom\\text{padding\\_bottom}padding_bottom,padding_front\\text{padding\\_front}padding_front, padding_back\\text{padding\\_back}padding_back)"], "returns": null, "example": " m = nn.ConstantPad3d(3, 3.5)\n input = torch.randn(16, 3, 10, 20, 30)\n output = m(input)\n # using different paddings for different sides\n m = nn.ConstantPad3d((3, 3, 6, 6, 0, 1), 3.5)\n output = m(input)\n\n", "shape": " Input: (N,C,Din,Hin,Win)(N, C, D_{in}, H_{in}, W_{in})(N,C,Din\u200b,Hin\u200b,Win\u200b)   Output: (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\u200b,Hout\u200b,Wout\u200b)   where Dout=Din+padding_front+padding_backD_{out} = D_{in} + \\text{padding\\_front} + \\text{padding\\_back}Dout\u200b=Din\u200b+padding_front+padding_back   Hout=Hin+padding_top+padding_bottomH_{out} = H_{in} + \\text{padding\\_top} + \\text{padding\\_bottom}Hout\u200b=Hin\u200b+padding_top+padding_bottom   Wout=Win+padding_left+padding_rightW_{out} = W_{in} + \\text{padding\\_left} + \\text{padding\\_right}Wout\u200b=Win\u200b+padding_left+padding_right     "},
{"library": "torch", "item_id": "torch.ifft", "item_type": "function", "code": "torch.ifft(input,signal_ndim,normalized=False)\u2192Tensor", "description": "Complex-to-complex Inverse Discrete Fourier Transform This method computes the complex-to-complex inverse discrete Fourier transform. Ignoring the batch dimensions, it computes the following expression:  X[\u03c91,\u2026,\u03c9d]=1\u220fi=1dNi\u2211n1=0N1\u22121\u22ef\u2211nd=0Nd\u22121x[n1,\u2026,nd]e\u00a0j\u00a02\u03c0\u2211i=0d\u03c9iniNi,X[\\omega_1, \\dots, \\omega_d] =     \\frac{1}{\\prod_{i=1}^d N_i} \\sum_{n_1=0}^{N_1-1} \\dots \\sum_{n_d=0}^{N_d-1} x[n_1, \\dots, n_d]      e^{\\ j\\ 2 \\pi \\sum_{i=0}^d \\frac{\\omega_i n_i}{N_i}},  X[\u03c91\u200b,\u2026,\u03c9d\u200b]=\u220fi=1d\u200bNi\u200b1\u200bn1\u200b=0\u2211N1\u200b\u22121\u200b\u22efnd\u200b=0\u2211Nd\u200b\u22121\u200bx[n1\u200b,\u2026,nd\u200b]e\u00a0j\u00a02\u03c0\u2211i=0d\u200bNi\u200b\u03c9i\u200bni\u200b\u200b,  where ddd   = signal_ndim is number of dimensions for the signal, and NiN_iNi\u200b   is the size of signal dimension iii  . The argument specifications are almost identical with fft(). However, if normalized is set to True, this instead returns the results multiplied by \u220fi=1dNi\\sqrt{\\prod_{i=1}^d N_i}\u220fi=1d\u200bNi\u200b\u200b  , to become a unitary operator. Therefore, to invert a fft(), the normalized argument should be set identically for fft(). Returns the real and the imaginary parts together as one tensor of the same shape of input. The inverse of this function is fft().  Note For CUDA tensors, an LRU cache is used for cuFFT plans to speed up repeatedly running FFT methods on tensors of same geometry with same configuration. See cuFFT plan cache for more details on how to monitor and control the cache.   Warning For CPU tensors, this method is currently only available with MKL. Use torch.backends.mkl.is_available() to check if MKL is installed.   Parameters  input (Tensor) \u2013 the input tensor of at least signal_ndim + 1 dimensions signal_ndim (python:int) \u2013 the number of dimensions in each signal. signal_ndim can only be 1, 2 or 3 normalized (bool, optional) \u2013 controls whether to return normalized results. Default: False   Returns A tensor containing the complex-to-complex inverse Fourier transform result  Return type Tensor   Example: &gt;&gt;&gt; x = torch.randn(3, 3, 2) &gt;&gt;&gt; x tensor([[[ 1.2766,  1.3680],          [-0.8337,  2.0251],          [ 0.9465, -1.4390]],          [[-0.1890,  1.6010],          [ 1.1034, -1.9230],          [-0.9482,  1.0775]],          [[-0.7708, -0.8176],          [-0.1843, -0.2287],          [-1.9034, -0.2196]]]) &gt;&gt;&gt; y = torch.fft(x, 2) &gt;&gt;&gt; torch.ifft(y, 2)  # recover x tensor([[[ 1.2766,  1.3680],          [-0.8337,  2.0251],          [ 0.9465, -1.4390]],          [[-0.1890,  1.6010],          [ 1.1034, -1.9230],          [-0.9482,  1.0775]],          [[-0.7708, -0.8176],          [-0.1843, -0.2287],          [-1.9034, -0.2196]]])   ", "parameters": ["input (Tensor) : the input tensor of at least signal_ndim + 1dimensions", "signal_ndim (python:int) : the number of dimensions in each signal.signal_ndim can only be 1, 2 or 3", "normalized (bool, optional) : controls whether to return normalized results.Default: False"], "returns": "A tensor containing the complex-to-complex inverse Fourier transform result", "example": " x = torch.randn(3, 3, 2)\n x\ntensor([[[ 1.2766,  1.3680],\n         [-0.8337,  2.0251],\n         [ 0.9465, -1.4390]],\n\n        [[-0.1890,  1.6010],\n         [ 1.1034, -1.9230],\n         [-0.9482,  1.0775]],\n\n        [[-0.7708, -0.8176],\n         [-0.1843, -0.2287],\n         [-1.9034, -0.2196]]])\n y = torch.fft(x, 2)\n torch.ifft(y, 2)  # recover x\ntensor([[[ 1.2766,  1.3680],\n         [-0.8337,  2.0251],\n         [ 0.9465, -1.4390]],\n\n        [[-0.1890,  1.6010],\n         [ 1.1034, -1.9230],\n         [-0.9482,  1.0775]],\n\n        [[-0.7708, -0.8176],\n         [-0.1843, -0.2287],\n         [-1.9034, -0.2196]]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.rfft", "item_type": "function", "code": "torch.rfft(input,signal_ndim,normalized=False,onesided=True)\u2192Tensor", "description": "Real-to-complex Discrete Fourier Transform This method computes the real-to-complex discrete Fourier transform. It is mathematically equivalent with fft() with differences only in formats of the input and output. This method supports 1D, 2D and 3D real-to-complex transforms, indicated by signal_ndim. input must be a tensor with at least signal_ndim dimensions with optionally arbitrary number of leading batch dimensions. If normalized is set to True, this normalizes the result by dividing it with \u220fi=1KNi\\sqrt{\\prod_{i=1}^K N_i}\u220fi=1K\u200bNi\u200b\u200b   so that the operator is unitary, where NiN_iNi\u200b   is the size of signal dimension iii  . The real-to-complex Fourier transform results follow conjugate symmetry:  X[\u03c91,\u2026,\u03c9d]=X\u2217[N1\u2212\u03c91,\u2026,Nd\u2212\u03c9d],X[\\omega_1, \\dots, \\omega_d] = X^*[N_1 - \\omega_1, \\dots, N_d - \\omega_d],  X[\u03c91\u200b,\u2026,\u03c9d\u200b]=X\u2217[N1\u200b\u2212\u03c91\u200b,\u2026,Nd\u200b\u2212\u03c9d\u200b],  where the index arithmetic is computed modulus the size of the corresponding dimension, \u00a0\u2217\\ ^*\u00a0\u2217   is the conjugate operator, and ddd   = signal_ndim. onesided flag controls whether to avoid redundancy in the output results. If set to True (default), the output will not be full complex result of shape (\u2217,2)(*, 2)(\u2217,2)  , where \u2217*\u2217   is the shape of input, but instead the last dimension will be halfed as of size \u230aNd2\u230b+1\\lfloor \\frac{N_d}{2} \\rfloor + 1\u230a2Nd\u200b\u200b\u230b+1  . The inverse of this function is irfft().  Note For CUDA tensors, an LRU cache is used for cuFFT plans to speed up repeatedly running FFT methods on tensors of same geometry with same configuration. See cuFFT plan cache for more details on how to monitor and control the cache.   Warning For CPU tensors, this method is currently only available with MKL. Use torch.backends.mkl.is_available() to check if MKL is installed.   Parameters  input (Tensor) \u2013 the input tensor of at least signal_ndim dimensions signal_ndim (python:int) \u2013 the number of dimensions in each signal. signal_ndim can only be 1, 2 or 3 normalized (bool, optional) \u2013 controls whether to return normalized results. Default: False onesided (bool, optional) \u2013 controls whether to return half of results to avoid redundancy. Default: True   Returns A tensor containing the real-to-complex Fourier transform result  Return type Tensor   Example: &gt;&gt;&gt; x = torch.randn(5, 5) &gt;&gt;&gt; torch.rfft(x, 2).shape torch.Size([5, 3, 2]) &gt;&gt;&gt; torch.rfft(x, 2, onesided=False).shape torch.Size([5, 5, 2])   ", "parameters": ["input (Tensor) : the input tensor of at least signal_ndim dimensions", "signal_ndim (python:int) : the number of dimensions in each signal.signal_ndim can only be 1, 2 or 3", "normalized (bool, optional) : controls whether to return normalized results.Default: False", "onesided (bool, optional) : controls whether to return half of results toavoid redundancy. Default: True"], "returns": "A tensor containing the real-to-complex Fourier transform result", "example": " x = torch.randn(5, 5)\n torch.rfft(x, 2).shape\ntorch.Size([5, 3, 2])\n torch.rfft(x, 2, onesided=False).shape\ntorch.Size([5, 5, 2])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.ELU", "item_type": "class", "code": "classtorch.nn.ELU(alpha=1.0,inplace=False)", "description": "Applies the element-wise function:  ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))  ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121))   Parameters  alpha \u2013 the \u03b1\\alpha\u03b1   value for the ELU formulation. Default: 1.0 inplace \u2013 can optionally do the operation in-place. Default: False     Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input     Examples: &gt;&gt;&gt; m = nn.ELU() &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; output = m(input)   ", "parameters": ["alpha : the \u03b1\\alpha\u03b1 value for the ELU formulation. Default: 1.0", "inplace : can optionally do the operation in-place. Default: False"], "returns": null, "example": " m = nn.ELU()\n input = torch.randn(2)\n output = m(input)\n\n", "shape": " Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input  "},
{"library": "torch", "item_id": "torch.nn.Hardshrink", "item_type": "class", "code": "classtorch.nn.Hardshrink(lambd=0.5)", "description": "Applies the hard shrinkage function element-wise:  HardShrink(x)={x,\u00a0if\u00a0x&gt;\u03bbx,\u00a0if\u00a0x&lt;\u2212\u03bb0,\u00a0otherwise\u00a0\\text{HardShrink}(x) = \\begin{cases} x, &amp; \\text{ if } x &gt; \\lambda \\\\ x, &amp; \\text{ if } x &lt; -\\lambda \\\\ 0, &amp; \\text{ otherwise } \\end{cases}  HardShrink(x)=\u23a9\u23aa\u23aa\u23a8\u23aa\u23aa\u23a7\u200bx,x,0,\u200b\u00a0if\u00a0x&gt;\u03bb\u00a0if\u00a0x&lt;\u2212\u03bb\u00a0otherwise\u00a0\u200b   Parameters lambd \u2013 the \u03bb\\lambda\u03bb   value for the Hardshrink formulation. Default: 0.5    Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input     Examples: &gt;&gt;&gt; m = nn.Hardshrink() &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; output = m(input)   ", "parameters": ["lambd : the \u03bb\\lambda\u03bb value for the Hardshrink formulation. Default: 0.5"], "returns": null, "example": " m = nn.Hardshrink()\n input = torch.randn(2)\n output = m(input)\n\n", "shape": " Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input  "},
{"library": "torch", "item_id": "torch.nn.Hardtanh", "item_type": "class", "code": "classtorch.nn.Hardtanh(min_val=-1.0,max_val=1.0,inplace=False,min_value=None,max_value=None)", "description": "Applies the HardTanh function element-wise HardTanh is defined as:  HardTanh(x)={1\u00a0if\u00a0x&gt;1\u22121\u00a0if\u00a0x&lt;\u22121x\u00a0otherwise\u00a0\\text{HardTanh}(x) = \\begin{cases}     1 &amp; \\text{ if } x &gt; 1 \\\\     -1 &amp; \\text{ if } x &lt; -1 \\\\     x &amp; \\text{ otherwise } \\\\ \\end{cases}  HardTanh(x)=\u23a9\u23aa\u23aa\u23a8\u23aa\u23aa\u23a7\u200b1\u22121x\u200b\u00a0if\u00a0x&gt;1\u00a0if\u00a0x&lt;\u22121\u00a0otherwise\u00a0\u200b  The range of the linear region [\u22121,1][-1, 1][\u22121,1]   can be adjusted using min_val and max_val.  Parameters  min_val \u2013 minimum value of the linear region range. Default: -1 max_val \u2013 maximum value of the linear region range. Default: 1 inplace \u2013 can optionally do the operation in-place. Default: False    Keyword arguments min_value and max_value have been deprecated in favor of min_val and max_val.  Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input     Examples: &gt;&gt;&gt; m = nn.Hardtanh(-2, 2) &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; output = m(input)   ", "parameters": ["min_val : minimum value of the linear region range. Default: -1", "max_val : maximum value of the linear region range. Default: 1", "inplace : can optionally do the operation in-place. Default: False"], "returns": null, "example": " m = nn.Hardtanh(-2, 2)\n input = torch.randn(2)\n output = m(input)\n\n", "shape": " Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input  "},
{"library": "torch", "item_id": "torch.nn.LeakyReLU", "item_type": "class", "code": "classtorch.nn.LeakyReLU(negative_slope=0.01,inplace=False)", "description": "Applies the element-wise function:  LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)  LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)  or  LeakyRELU(x)={x,\u00a0if\u00a0x\u22650negative_slope\u00d7x,\u00a0otherwise\u00a0\\text{LeakyRELU}(x) = \\begin{cases} x, &amp; \\text{ if } x \\geq 0 \\\\ \\text{negative\\_slope} \\times x, &amp; \\text{ otherwise } \\end{cases}  LeakyRELU(x)={x,negative_slope\u00d7x,\u200b\u00a0if\u00a0x\u22650\u00a0otherwise\u00a0\u200b   Parameters  negative_slope \u2013 Controls the angle of the negative slope. Default: 1e-2 inplace \u2013 can optionally do the operation in-place. Default: False     Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input     Examples: &gt;&gt;&gt; m = nn.LeakyReLU(0.1) &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; output = m(input)   ", "parameters": ["negative_slope : Controls the angle of the negative slope. Default: 1e-2", "inplace : can optionally do the operation in-place. Default: False"], "returns": null, "example": " m = nn.LeakyReLU(0.1)\n input = torch.randn(2)\n output = m(input)\n\n", "shape": " Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input  "},
{"library": "torch", "item_id": "torch.nn.LogSigmoid", "item_type": "class", "code": "classtorch.nn.LogSigmoid", "description": "Applies the element-wise function:  LogSigmoid(x)=log\u2061(11+exp\u2061(\u2212x))\\text{LogSigmoid}(x) = \\log\\left(\\frac{ 1 }{ 1 + \\exp(-x)}\\right)  LogSigmoid(x)=log(1+exp(\u2212x)1\u200b)   Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input     Examples: &gt;&gt;&gt; m = nn.LogSigmoid() &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; output = m(input)   ", "parameters": [], "returns": null, "example": " m = nn.LogSigmoid()\n input = torch.randn(2)\n output = m(input)\n\n", "shape": " Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input  "},
{"library": "torch", "item_id": "torch.irfft", "item_type": "function", "code": "torch.irfft(input,signal_ndim,normalized=False,onesided=True,signal_sizes=None)\u2192Tensor", "description": "Complex-to-real Inverse Discrete Fourier Transform This method computes the complex-to-real inverse discrete Fourier transform. It is mathematically equivalent with ifft() with differences only in formats of the input and output. The argument specifications are almost identical with ifft(). Similar to ifft(), if normalized is set to True, this normalizes the result by multiplying it with \u220fi=1KNi\\sqrt{\\prod_{i=1}^K N_i}\u220fi=1K\u200bNi\u200b\u200b   so that the operator is unitary, where NiN_iNi\u200b   is the size of signal dimension iii  .  Note Due to the conjugate symmetry, input do not need to contain the full complex frequency values. Roughly half of the values will be sufficient, as is the case when input is given by rfft() with rfft(signal, onesided=True). In such case, set the onesided argument of this method to True. Moreover, the original signal shape information can sometimes be lost, optionally set signal_sizes to be the size of the original signal (without the batch dimensions if in batched mode) to recover it with correct shape. Therefore, to invert an rfft(), the normalized and onesided arguments should be set identically for irfft(), and preferrably a signal_sizes is given to avoid size mismatch. See the example below for a case of size mismatch. See rfft() for details on conjugate symmetry.  The inverse of this function is rfft().  Warning Generally speaking, input to this function should contain values following conjugate symmetry. Note that even if onesided is True, often symmetry on some part is still needed. When this requirement is not satisfied, the behavior of irfft() is undefined. Since torch.autograd.gradcheck() estimates numerical Jacobian with point perturbations, irfft() will almost certainly fail the check.   Note For CUDA tensors, an LRU cache is used for cuFFT plans to speed up repeatedly running FFT methods on tensors of same geometry with same configuration. See cuFFT plan cache for more details on how to monitor and control the cache.   Warning For CPU tensors, this method is currently only available with MKL. Use torch.backends.mkl.is_available() to check if MKL is installed.   Parameters  input (Tensor) \u2013 the input tensor of at least signal_ndim + 1 dimensions signal_ndim (python:int) \u2013 the number of dimensions in each signal. signal_ndim can only be 1, 2 or 3 normalized (bool, optional) \u2013 controls whether to return normalized results. Default: False onesided (bool, optional) \u2013 controls whether input was halfed to avoid redundancy, e.g., by rfft(). Default: True signal_sizes (list or torch.Size, optional) \u2013 the size of the original signal (without batch dimension). Default: None   Returns A tensor containing the complex-to-real inverse Fourier transform result  Return type Tensor   Example: &gt;&gt;&gt; x = torch.randn(4, 4) &gt;&gt;&gt; torch.rfft(x, 2, onesided=True).shape torch.Size([4, 3, 2]) &gt;&gt;&gt; &gt;&gt;&gt; # notice that with onesided=True, output size does not determine the original signal size &gt;&gt;&gt; x = torch.randn(4, 5)  &gt;&gt;&gt; torch.rfft(x, 2, onesided=True).shape torch.Size([4, 3, 2]) &gt;&gt;&gt; &gt;&gt;&gt; # now we use the original shape to recover x &gt;&gt;&gt; x tensor([[-0.8992,  0.6117, -1.6091, -0.4155, -0.8346],         [-2.1596, -0.0853,  0.7232,  0.1941, -0.0789],         [-2.0329,  1.1031,  0.6869, -0.5042,  0.9895],         [-0.1884,  0.2858, -1.5831,  0.9917, -0.8356]]) &gt;&gt;&gt; y = torch.rfft(x, 2, onesided=True) &gt;&gt;&gt; torch.irfft(y, 2, onesided=True, signal_sizes=x.shape)  # recover x tensor([[-0.8992,  0.6117, -1.6091, -0.4155, -0.8346],         [-2.1596, -0.0853,  0.7232,  0.1941, -0.0789],         [-2.0329,  1.1031,  0.6869, -0.5042,  0.9895],         [-0.1884,  0.2858, -1.5831,  0.9917, -0.8356]])   ", "parameters": ["input (Tensor) : the input tensor of at least signal_ndim + 1dimensions", "signal_ndim (python:int) : the number of dimensions in each signal.signal_ndim can only be 1, 2 or 3", "normalized (bool, optional) : controls whether to return normalized results.Default: False", "onesided (bool, optional) : controls whether input was halfed to avoidredundancy, e.g., by rfft(). Default: True", "signal_sizes (list or torch.Size, optional) : the size of the originalsignal (without batch dimension). Default: None"], "returns": "A tensor containing the complex-to-real inverse Fourier transform result", "example": " x = torch.randn(4, 4)\n torch.rfft(x, 2, onesided=True).shape\ntorch.Size([4, 3, 2])\n\n # notice that with onesided=True, output size does not determine the original signal size\n x = torch.randn(4, 5)\n\n torch.rfft(x, 2, onesided=True).shape\ntorch.Size([4, 3, 2])\n\n # now we use the original shape to recover x\n x\ntensor([[-0.8992,  0.6117, -1.6091, -0.4155, -0.8346],\n        [-2.1596, -0.0853,  0.7232,  0.1941, -0.0789],\n        [-2.0329,  1.1031,  0.6869, -0.5042,  0.9895],\n        [-0.1884,  0.2858, -1.5831,  0.9917, -0.8356]])\n y = torch.rfft(x, 2, onesided=True)\n torch.irfft(y, 2, onesided=True, signal_sizes=x.shape)  # recover x\ntensor([[-0.8992,  0.6117, -1.6091, -0.4155, -0.8346],\n        [-2.1596, -0.0853,  0.7232,  0.1941, -0.0789],\n        [-2.0329,  1.1031,  0.6869, -0.5042,  0.9895],\n        [-0.1884,  0.2858, -1.5831,  0.9917, -0.8356]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.stft", "item_type": "function", "code": "torch.stft(input,n_fft,hop_length=None,win_length=None,window=None,center=True,pad_mode='reflect',normalized=False,onesided=True)", "description": "Short-time Fourier transform (STFT). Ignoring the optional batch dimension, this method computes the following expression:  X[m,\u03c9]=\u2211k=0win_length-1window[k]\u00a0input[m\u00d7hop_length+k]\u00a0exp\u2061(\u2212j2\u03c0\u22c5\u03c9kwin_length),X[m, \\omega] = \\sum_{k = 0}^{\\text{win\\_length-1}}%                     \\text{window}[k]\\ \\text{input}[m \\times \\text{hop\\_length} + k]\\ %                     \\exp\\left(- j \\frac{2 \\pi \\cdot \\omega k}{\\text{win\\_length}}\\right),  X[m,\u03c9]=k=0\u2211win_length-1\u200bwindow[k]\u00a0input[m\u00d7hop_length+k]\u00a0exp(\u2212jwin_length2\u03c0\u22c5\u03c9k\u200b),  where mmm   is the index of the sliding window, and \u03c9\\omega\u03c9   is the frequency that 0\u2264\u03c9&lt;n_fft0 \\leq \\omega &lt; \\text{n\\_fft}0\u2264\u03c9&lt;n_fft  . When onesided is the default value True,  input must be either a 1-D time sequence or a 2-D batch of time sequences. If hop_length is None (default), it is treated as equal to floor(n_fft / 4). If win_length is None (default), it is treated as equal to n_fft. window can be a 1-D tensor of size win_length, e.g., from torch.hann_window(). If window is None (default), it is treated as if having 111   everywhere in the window. If win_length&lt;n_fft\\text{win\\_length} &lt; \\text{n\\_fft}win_length&lt;n_fft  , window will be padded on both sides to length n_fft before being applied. If center is True (default), input will be padded on both sides so that the ttt  -th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length  . Otherwise, the ttt  -th frame begins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length  . pad_mode determines the padding method used on input when center is True. See torch.nn.functional.pad() for all available options. Default is \"reflect\". If onesided is True (default), only values for \u03c9\\omega\u03c9   in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor \\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]   are returned because the real-to-complex Fourier transform satisfies the conjugate symmetry, i.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217  . If normalized is True (default is False), the function returns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5  .  Returns the real and the imaginary parts together as one tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N \\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72)  , where \u2217*\u2217   is the optional batch size of input, NNN   is the number of frequencies where STFT is applied, TTT   is the total number of frames used, and each pair in the last dimension represents a complex number as the real part and the imaginary part.  Warning This function changed signature at version 0.4.1. Calling with the previous signature may cause error or return incorrect result.   Parameters  input (Tensor) \u2013 the input tensor n_fft (python:int) \u2013 size of Fourier transform hop_length (python:int, optional) \u2013 the distance between neighboring sliding window frames. Default: None (treated as equal to floor(n_fft / 4)) win_length (python:int, optional) \u2013 the size of window frame and STFT filter. Default: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function. Default: None (treated as window of all 111   s) center (bool, optional) \u2013 whether to pad input on both sides so that the ttt  -th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length  . Default: True pad_mode (string, optional) \u2013 controls the padding method used when center is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results Default: False onesided (bool, optional) \u2013 controls whether to return half of results to avoid redundancy Default: True   Returns A tensor containing the STFT result with shape described above  Return type Tensor   ", "parameters": ["input (Tensor) : the input tensor", "n_fft (python:int) : size of Fourier transform", "hop_length (python:int, optional) : the distance between neighboring sliding windowframes. Default: None (treated as equal to floor(n_fft / 4))", "win_length (python:int, optional) : the size of window frame and STFT filter.Default: None  (treated as equal to n_fft)", "window (Tensor, optional) : the optional window function.Default: None (treated as window of all 111 s)", "center (bool, optional) : whether to pad input on both sides sothat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.Default: True", "pad_mode (string, optional) : controls the padding method used whencenter is True. Default: \"reflect\"", "normalized (bool, optional) : controls whether to return the normalized STFT resultsDefault: False", "onesided (bool, optional) : controls whether to return half of results toavoid redundancy Default: True"], "returns": "A tensor containing the STFT result with shape described above", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.MultiheadAttention", "item_type": "class", "code": "classtorch.nn.MultiheadAttention(embed_dim,num_heads,dropout=0.0,bias=True,add_bias_kv=False,add_zero_attn=False,kdim=None,vdim=None)", "description": "Allows the model to jointly attend to information from different representation subspaces. See reference: Attention Is All You Need  MultiHead(Q,K,V)=Concat(head1,\u2026,headh)WOwhereheadi=Attention(QWiQ,KWiK,VWiV)\\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1,\\dots,head_h)W^O \\text{where} head_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)  MultiHead(Q,K,V)=Concat(head1\u200b,\u2026,headh\u200b)WOwhereheadi\u200b=Attention(QWiQ\u200b,KWiK\u200b,VWiV\u200b)   Parameters  embed_dim \u2013 total dimension of the model. num_heads \u2013 parallel attention heads. dropout \u2013 a Dropout layer on attn_output_weights. Default: 0.0. bias \u2013 add bias as module parameter. Default: True. add_bias_kv \u2013 add bias to the key and value sequences at dim=0. add_zero_attn \u2013 add a new batch of zeros to the key and value sequences at dim=1. kdim \u2013 total number of features in key. Default: None. vdim \u2013 total number of features in key. Default: None. Note \u2013 if kdim and vdim are None, they will be set to embed_dim such that key, and value have the same number of features. (query,) \u2013     Examples: &gt;&gt;&gt; multihead_attn = nn.MultiheadAttention(embed_dim, num_heads) &gt;&gt;&gt; attn_output, attn_output_weights = multihead_attn(query, key, value)     forward(query, key, value, key_padding_mask=None, need_weights=True, attn_mask=None)   Parameters  key, value (query,) \u2013 map a query and a set of key-value pairs to an output. See \u201cAttention Is All You Need\u201d for more details. key_padding_mask \u2013 if provided, specified padding elements in the key will be ignored by the attention. This is an binary mask. When the value is True, the corresponding value on the attention layer will be filled with -inf. need_weights \u2013 output attn_output_weights. attn_mask \u2013 mask that prevents attention to certain positions. This is an additive mask (i.e. the values will be added to the attention layer).     Shape: Inputs: query: (L,N,E)(L, N, E)(L,N,E)   where L is the target sequence length, N is the batch size, E is the embedding dimension. key: (S,N,E)(S, N, E)(S,N,E)  , where S is the source sequence length, N is the batch size, E is the embedding dimension. value: (S,N,E)(S, N, E)(S,N,E)   where S is the source sequence length, N is the batch size, E is the embedding dimension. key_padding_mask: (N,S)(N, S)(N,S)  , ByteTensor, where N is the batch size, S is the source sequence length. attn_mask: (L,S)(L, S)(L,S)   where L is the target sequence length, S is the source sequence length. Outputs: attn_output: (L,N,E)(L, N, E)(L,N,E)   where L is the target sequence length, N is the batch size, E is the embedding dimension. attn_output_weights: (N,L,S)(N, L, S)(N,L,S)   where N is the batch size, L is the target sequence length, S is the source sequence length.      ", "parameters": ["embed_dim : total dimension of the model.", "num_heads : parallel attention heads.", "dropout : a Dropout layer on attn_output_weights. Default: 0.0.", "bias : add bias as module parameter. Default: True.", "add_bias_kv : add bias to the key and value sequences at dim=0.", "add_zero_attn : add a new batch of zeros to the key andvalue sequences at dim=1.", "kdim : total number of features in key. Default: None.", "vdim : total number of features in key. Default: None.", "Note : if kdim and vdim are None, they will be set to embed_dim such that", "key, and value have the same number of features. (query,) : ", "key, value (query,) : map a query and a set of key-value pairs to an output.See \u201cAttention Is All You Need\u201d for more details.", "key_padding_mask : if provided, specified padding elements in the key willbe ignored by the attention. This is an binary mask. When the value is True,the corresponding value on the attention layer will be filled with -inf.", "need_weights : output attn_output_weights.", "attn_mask : mask that prevents attention to certain positions. This is an additive mask(i.e. the values will be added to the attention layer)."], "returns": null, "example": " multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n attn_output, attn_output_weights = multihead_attn(query, key, value)\n\n", "shape": " Inputs: query: (L,N,E)(L, N, E)(L,N,E)   where L is the target sequence length, N is the batch size, E is the embedding dimension. key: (S,N,E)(S, N, E)(S,N,E)  , where S is the source sequence length, N is the batch size, E is the embedding dimension. value: (S,N,E)(S, N, E)(S,N,E)   where S is the source sequence length, N is the batch size, E is the embedding dimension. key_padding_mask: (N,S)(N, S)(N,S)  , ByteTensor, where N is the batch size, S is the source sequence length. attn_mask: (L,S)(L, S)(L,S)   where L is the target sequence length, S is the source sequence length. Outputs: attn_output: (L,N,E)(L, N, E)(L,N,E)   where L is the target sequence length, N is the batch size, E is the embedding dimension. attn_output_weights: (N,L,S)(N, L, S)(N,L,S)   where N is the batch size, L is the target sequence length, S is the source sequence length.  "},
{"library": "torch", "item_id": "torch.nn.PReLU", "item_type": "class", "code": "classtorch.nn.PReLU(num_parameters=1,init=0.25)", "description": "Applies the element-wise function:  PReLU(x)=max\u2061(0,x)+a\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + a * \\min(0,x)  PReLU(x)=max(0,x)+a\u2217min(0,x)  or  PReLU(x)={x,\u00a0if\u00a0x\u22650ax,\u00a0otherwise\u00a0\\text{PReLU}(x) = \\begin{cases} x, &amp; \\text{ if } x \\geq 0 \\\\ ax, &amp; \\text{ otherwise } \\end{cases}  PReLU(x)={x,ax,\u200b\u00a0if\u00a0x\u22650\u00a0otherwise\u00a0\u200b  Here aaa   is a learnable parameter. When called without arguments, nn.PReLU() uses a single parameter aaa   across all input channels. If called with nn.PReLU(nChannels), a separate aaa   is used for each input channel.  Note weight decay should not be used when learning aaa   for good performance.   Note Channel dim is the 2nd dim of input. When input has dims &lt; 2, then there is no channel dim and the number of channels = 1.   Parameters  num_parameters (python:int) \u2013 number of aaa   to learn. Although it takes an int as input, there is only two values are legitimate: 1, or the number of channels at input. Default: 1 init (python:float) \u2013 the initial value of aaa  . Default: 0.25     Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input     Variables ~PReLU.weight (Tensor) \u2013 the learnable weights of shape (num_parameters).    Examples: &gt;&gt;&gt; m = nn.PReLU() &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; output = m(input)   ", "parameters": ["num_parameters (python:int) : number of aaa to learn.Although it takes an int as input, there is only two values are legitimate:1, or the number of channels at input. Default: 1", "init (python:float) : the initial value of aaa. Default: 0.25"], "returns": null, "example": " m = nn.PReLU()\n input = torch.randn(2)\n output = m(input)\n\n", "shape": " Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input  "},
{"library": "torch", "item_id": "torch.nn.ReLU", "item_type": "class", "code": "classtorch.nn.ReLU(inplace=False)", "description": "Applies the rectified linear unit function element-wise: ReLU(x)=max\u2061(0,x)\\text{ReLU}(x)= \\max(0, x)ReLU(x)=max(0,x)    Parameters inplace \u2013 can optionally do the operation in-place. Default: False    Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input     Examples:   &gt;&gt;&gt; m = nn.ReLU()   &gt;&gt;&gt; input = torch.randn(2)   &gt;&gt;&gt; output = m(input)   An implementation of CReLU - https://arxiv.org/abs/1603.05201    &gt;&gt;&gt; m = nn.ReLU()   &gt;&gt;&gt; input = torch.randn(2).unsqueeze(0)   &gt;&gt;&gt; output = torch.cat((m(input),m(-input)))   ", "parameters": ["inplace : can optionally do the operation in-place. Default: False"], "returns": null, "example": "   m = nn.ReLU()\n   input = torch.randn(2)\n   output = m(input)\n\n\nAn implementation of CReLU - https://arxiv.org/abs/1603.05201\n\n   m = nn.ReLU()\n   input = torch.randn(2).unsqueeze(0)\n   output = torch.cat((m(input),m(-input)))\n\n", "shape": " Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input  "},
{"library": "torch", "item_id": "torch.bartlett_window", "item_type": "function", "code": "torch.bartlett_window(window_length,periodic=True,dtype=None,layout=torch.strided,device=None,requires_grad=False)\u2192Tensor", "description": "Bartlett window function.  w[n]=1\u2212\u22232nN\u22121\u22121\u2223={2nN\u22121if\u00a00\u2264n\u2264N\u2212122\u22122nN\u22121if\u00a0N\u221212&lt;n&lt;N,w[n] = 1 - \\left| \\frac{2n}{N-1} - 1 \\right| = \\begin{cases}     \\frac{2n}{N - 1} &amp; \\text{if } 0 \\leq n \\leq \\frac{N - 1}{2} \\\\     2 - \\frac{2n}{N - 1} &amp; \\text{if } \\frac{N - 1}{2} &lt; n &lt; N \\\\ \\end{cases},  w[n]=1\u2212\u2223\u2223\u2223\u2223\u2223\u200bN\u221212n\u200b\u22121\u2223\u2223\u2223\u2223\u2223\u200b={N\u221212n\u200b2\u2212N\u221212n\u200b\u200bif\u00a00\u2264n\u22642N\u22121\u200bif\u00a02N\u22121\u200b&lt;n&lt;N\u200b,  where NNN   is the full window size. The input window_length is a positive integer controlling the returned window size. periodic flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft(). Therefore, if periodic is true, the NNN   in above formula is in fact window_length+1\\text{window\\_length} + 1window_length+1  . Also, we always have torch.bartlett_window(L, periodic=True) equal to torch.bartlett_window(L + 1, periodic=False)[:-1]).  Note If window_length =1=1=1  , the returned window contains a single value 1.   Parameters  window_length (python:int) \u2013 the size of returned window periodic (bool, optional) \u2013 If True, returns a window to be used as periodic function. If False, return a symmetric window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). Only floating point types are supported. layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only torch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.   Returns A 1-D tensor of size (window_length,)(\\text{window\\_length},)(window_length,)   containing the window  Return type Tensor   ", "parameters": ["window_length (python:int) : the size of returned window", "periodic (bool, optional) : If True, returns a window to be used as periodicfunction. If False, return a symmetric window.", "dtype (torch.dtype, optional) : the desired data type of returned tensor.Default: if None, uses a global default (see torch.set_default_tensor_type()). Only floating point types are supported.", "layout (torch.layout, optional) : the desired layout of returned window tensor. Onlytorch.strided (dense layout) is supported.", "device (torch.device, optional) : the desired device of returned tensor.Default: if None, uses the current device for the default tensor type(see torch.set_default_tensor_type()). device will be the CPUfor CPU tensor types and the current CUDA device for CUDA tensor types.", "requires_grad (bool, optional) : If autograd should record operations on thereturned tensor. Default: False."], "returns": "A 1-D tensor of size (window_length,)(\\text{window\\_length},)(window_length,) containing the window", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.blackman_window", "item_type": "function", "code": "torch.blackman_window(window_length,periodic=True,dtype=None,layout=torch.strided,device=None,requires_grad=False)\u2192Tensor", "description": "Blackman window function.  w[n]=0.42\u22120.5cos\u2061(2\u03c0nN\u22121)+0.08cos\u2061(4\u03c0nN\u22121)w[n] = 0.42 - 0.5 \\cos \\left( \\frac{2 \\pi n}{N - 1} \\right) + 0.08 \\cos \\left( \\frac{4 \\pi n}{N - 1} \\right)  w[n]=0.42\u22120.5cos(N\u221212\u03c0n\u200b)+0.08cos(N\u221214\u03c0n\u200b)  where NNN   is the full window size. The input window_length is a positive integer controlling the returned window size. periodic flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft(). Therefore, if periodic is true, the NNN   in above formula is in fact window_length+1\\text{window\\_length} + 1window_length+1  . Also, we always have torch.blackman_window(L, periodic=True) equal to torch.blackman_window(L + 1, periodic=False)[:-1]).  Note If window_length =1=1=1  , the returned window contains a single value 1.   Parameters  window_length (python:int) \u2013 the size of returned window periodic (bool, optional) \u2013 If True, returns a window to be used as periodic function. If False, return a symmetric window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). Only floating point types are supported. layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only torch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.   Returns A 1-D tensor of size (window_length,)(\\text{window\\_length},)(window_length,)   containing the window  Return type Tensor   ", "parameters": ["window_length (python:int) : the size of returned window", "periodic (bool, optional) : If True, returns a window to be used as periodicfunction. If False, return a symmetric window.", "dtype (torch.dtype, optional) : the desired data type of returned tensor.Default: if None, uses a global default (see torch.set_default_tensor_type()). Only floating point types are supported.", "layout (torch.layout, optional) : the desired layout of returned window tensor. Onlytorch.strided (dense layout) is supported.", "device (torch.device, optional) : the desired device of returned tensor.Default: if None, uses the current device for the default tensor type(see torch.set_default_tensor_type()). device will be the CPUfor CPU tensor types and the current CUDA device for CUDA tensor types.", "requires_grad (bool, optional) : If autograd should record operations on thereturned tensor. Default: False."], "returns": "A 1-D tensor of size (window_length,)(\\text{window\\_length},)(window_length,) containing the window", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.ReLU6", "item_type": "class", "code": "classtorch.nn.ReLU6(inplace=False)", "description": "Applies the element-wise function:  ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)  ReLU6(x)=min(max(0,x),6)   Parameters inplace \u2013 can optionally do the operation in-place. Default: False    Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input     Examples: &gt;&gt;&gt; m = nn.ReLU6() &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; output = m(input)   ", "parameters": ["inplace : can optionally do the operation in-place. Default: False"], "returns": null, "example": " m = nn.ReLU6()\n input = torch.randn(2)\n output = m(input)\n\n", "shape": " Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input  "},
{"library": "torch", "item_id": "torch.nn.RReLU", "item_type": "class", "code": "classtorch.nn.RReLU(lower=0.125,upper=0.3333333333333333,inplace=False)", "description": "Applies the randomized leaky rectified liner unit function, element-wise, as described in the paper: Empirical Evaluation of Rectified Activations in Convolutional Network. The function is defined as:  RReLU(x)={xif\u00a0x\u22650ax\u00a0otherwise\u00a0\\text{RReLU}(x) = \\begin{cases}     x &amp; \\text{if } x \\geq 0 \\\\     ax &amp; \\text{ otherwise } \\end{cases}  RReLU(x)={xax\u200bif\u00a0x\u22650\u00a0otherwise\u00a0\u200b  where aaa   is randomly sampled from uniform distribution U(lower,upper)\\mathcal{U}(\\text{lower}, \\text{upper})U(lower,upper)  .  See: https://arxiv.org/pdf/1505.00853.pdf   Parameters  lower \u2013 lower bound of the uniform distribution. Default: 18\\frac{1}{8}81\u200b   upper \u2013 upper bound of the uniform distribution. Default: 13\\frac{1}{3}31\u200b   inplace \u2013 can optionally do the operation in-place. Default: False     Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input    Examples: &gt;&gt;&gt; m = nn.RReLU(0.1, 0.3) &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; output = m(input)   ", "parameters": ["lower : lower bound of the uniform distribution. Default: 18\\frac{1}{8}81\u200b", "upper : upper bound of the uniform distribution. Default: 13\\frac{1}{3}31\u200b", "inplace : can optionally do the operation in-place. Default: False"], "returns": null, "example": " m = nn.RReLU(0.1, 0.3)\n input = torch.randn(2)\n output = m(input)\n\n", "shape": " Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input  "},
{"library": "torch", "item_id": "torch.nn.SELU", "item_type": "class", "code": "classtorch.nn.SELU(inplace=False)", "description": "Applied element-wise, as:  SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = \\text{scale} * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))  SELU(x)=scale\u2217(max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)))  with \u03b1=1.6732632423543772848170429916717\\alpha = 1.6732632423543772848170429916717\u03b1=1.6732632423543772848170429916717   and scale=1.0507009873554804934193349852946\\text{scale} = 1.0507009873554804934193349852946scale=1.0507009873554804934193349852946  . More details can be found in the paper Self-Normalizing Neural Networks .  Parameters inplace (bool, optional) \u2013 can optionally do the operation in-place. Default: False    Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input     Examples: &gt;&gt;&gt; m = nn.SELU() &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; output = m(input)   ", "parameters": ["inplace (bool, optional) : can optionally do the operation in-place. Default: False"], "returns": null, "example": " m = nn.SELU()\n input = torch.randn(2)\n output = m(input)\n\n", "shape": " Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input  "},
{"library": "torch", "item_id": "torch.nn.CELU", "item_type": "class", "code": "classtorch.nn.CELU(alpha=1.0,inplace=False)", "description": "Applies the element-wise function:  CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))  CELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x/\u03b1)\u22121))  More details can be found in the paper Continuously Differentiable Exponential Linear Units .  Parameters  alpha \u2013 the \u03b1\\alpha\u03b1   value for the CELU formulation. Default: 1.0 inplace \u2013 can optionally do the operation in-place. Default: False     Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input     Examples: &gt;&gt;&gt; m = nn.CELU() &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; output = m(input)   ", "parameters": ["alpha : the \u03b1\\alpha\u03b1 value for the CELU formulation. Default: 1.0", "inplace : can optionally do the operation in-place. Default: False"], "returns": null, "example": " m = nn.CELU()\n input = torch.randn(2)\n output = m(input)\n\n", "shape": " Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input  "},
{"library": "torch", "item_id": "torch.hamming_window", "item_type": "function", "code": "torch.hamming_window(window_length,periodic=True,alpha=0.54,beta=0.46,dtype=None,layout=torch.strided,device=None,requires_grad=False)\u2192Tensor", "description": "Hamming window function.  w[n]=\u03b1\u2212\u03b2\u00a0cos\u2061(2\u03c0nN\u22121),w[n] = \\alpha - \\beta\\ \\cos \\left( \\frac{2 \\pi n}{N - 1} \\right),  w[n]=\u03b1\u2212\u03b2\u00a0cos(N\u221212\u03c0n\u200b),  where NNN   is the full window size. The input window_length is a positive integer controlling the returned window size. periodic flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft(). Therefore, if periodic is true, the NNN   in above formula is in fact window_length+1\\text{window\\_length} + 1window_length+1  . Also, we always have torch.hamming_window(L, periodic=True) equal to torch.hamming_window(L + 1, periodic=False)[:-1]).  Note If window_length =1=1=1  , the returned window contains a single value 1.   Note This is a generalized version of torch.hann_window().   Parameters  window_length (python:int) \u2013 the size of returned window periodic (bool, optional) \u2013 If True, returns a window to be used as periodic function. If False, return a symmetric window. alpha (python:float, optional) \u2013 The coefficient \u03b1\\alpha\u03b1   in the equation above beta (python:float, optional) \u2013 The coefficient \u03b2\\beta\u03b2   in the equation above dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). Only floating point types are supported. layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only torch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.   Returns A 1-D tensor of size (window_length,)(\\text{window\\_length},)(window_length,)   containing the window  Return type Tensor   ", "parameters": ["window_length (python:int) : the size of returned window", "periodic (bool, optional) : If True, returns a window to be used as periodicfunction. If False, return a symmetric window.", "alpha (python:float, optional) : The coefficient \u03b1\\alpha\u03b1 in the equation above", "beta (python:float, optional) : The coefficient \u03b2\\beta\u03b2 in the equation above", "dtype (torch.dtype, optional) : the desired data type of returned tensor.Default: if None, uses a global default (see torch.set_default_tensor_type()). Only floating point types are supported.", "layout (torch.layout, optional) : the desired layout of returned window tensor. Onlytorch.strided (dense layout) is supported.", "device (torch.device, optional) : the desired device of returned tensor.Default: if None, uses the current device for the default tensor type(see torch.set_default_tensor_type()). device will be the CPUfor CPU tensor types and the current CUDA device for CUDA tensor types.", "requires_grad (bool, optional) : If autograd should record operations on thereturned tensor. Default: False."], "returns": "A 1-D tensor of size (window_length,)(\\text{window\\_length},)(window_length,) containing the window", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.hann_window", "item_type": "function", "code": "torch.hann_window(window_length,periodic=True,dtype=None,layout=torch.strided,device=None,requires_grad=False)\u2192Tensor", "description": "Hann window function.  w[n]=12\u00a0[1\u2212cos\u2061(2\u03c0nN\u22121)]=sin\u20612(\u03c0nN\u22121),w[n] = \\frac{1}{2}\\ \\left[1 - \\cos \\left( \\frac{2 \\pi n}{N - 1} \\right)\\right] =         \\sin^2 \\left( \\frac{\\pi n}{N - 1} \\right),  w[n]=21\u200b\u00a0[1\u2212cos(N\u221212\u03c0n\u200b)]=sin2(N\u22121\u03c0n\u200b),  where NNN   is the full window size. The input window_length is a positive integer controlling the returned window size. periodic flag determines whether the returned window trims off the last duplicate value from the symmetric window and is ready to be used as a periodic window with functions like torch.stft(). Therefore, if periodic is true, the NNN   in above formula is in fact window_length+1\\text{window\\_length} + 1window_length+1  . Also, we always have torch.hann_window(L, periodic=True) equal to torch.hann_window(L + 1, periodic=False)[:-1]).  Note If window_length =1=1=1  , the returned window contains a single value 1.   Parameters  window_length (python:int) \u2013 the size of returned window periodic (bool, optional) \u2013 If True, returns a window to be used as periodic function. If False, return a symmetric window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). Only floating point types are supported. layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only torch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the returned tensor. Default: False.   Returns A 1-D tensor of size (window_length,)(\\text{window\\_length},)(window_length,)   containing the window  Return type Tensor   ", "parameters": ["window_length (python:int) : the size of returned window", "periodic (bool, optional) : If True, returns a window to be used as periodicfunction. If False, return a symmetric window.", "dtype (torch.dtype, optional) : the desired data type of returned tensor.Default: if None, uses a global default (see torch.set_default_tensor_type()). Only floating point types are supported.", "layout (torch.layout, optional) : the desired layout of returned window tensor. Onlytorch.strided (dense layout) is supported.", "device (torch.device, optional) : the desired device of returned tensor.Default: if None, uses the current device for the default tensor type(see torch.set_default_tensor_type()). device will be the CPUfor CPU tensor types and the current CUDA device for CUDA tensor types.", "requires_grad (bool, optional) : If autograd should record operations on thereturned tensor. Default: False."], "returns": "A 1-D tensor of size (window_length,)(\\text{window\\_length},)(window_length,) containing the window", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.GELU", "item_type": "class", "code": "classtorch.nn.GELU", "description": "Applies the Gaussian Error Linear Units function:  GELU(x)=x\u2217\u03a6(x)\\text{GELU}(x) = x * \\Phi(x)  GELU(x)=x\u2217\u03a6(x)  where \u03a6(x)\\Phi(x)\u03a6(x)   is the Cumulative Distribution Function for Gaussian Distribution.  Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input     Examples: &gt;&gt;&gt; m = nn.GELU() &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; output = m(input)   ", "parameters": [], "returns": null, "example": " m = nn.GELU()\n input = torch.randn(2)\n output = m(input)\n\n", "shape": " Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input  "},
{"library": "torch", "item_id": "torch.nn.Sigmoid", "item_type": "class", "code": "classtorch.nn.Sigmoid", "description": "Applies the element-wise function:  Sigmoid(x)=11+exp\u2061(\u2212x)\\text{Sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}  Sigmoid(x)=1+exp(\u2212x)1\u200b   Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input     Examples: &gt;&gt;&gt; m = nn.Sigmoid() &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; output = m(input)   ", "parameters": [], "returns": null, "example": " m = nn.Sigmoid()\n input = torch.randn(2)\n output = m(input)\n\n", "shape": " Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input  "},
{"library": "torch", "item_id": "torch.nn.Softplus", "item_type": "class", "code": "classtorch.nn.Softplus(beta=1,threshold=20)", "description": "Applies the element-wise function:  Softplus(x)=1\u03b2\u2217log\u2061(1+exp\u2061(\u03b2\u2217x))\\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x))  Softplus(x)=\u03b21\u200b\u2217log(1+exp(\u03b2\u2217x))  SoftPlus is a smooth approximation to the ReLU function and can be used to constrain the output of a machine to always be positive. For numerical stability the implementation reverts to the linear function for inputs above a certain value.  Parameters  beta \u2013 the \u03b2\\beta\u03b2   value for the Softplus formulation. Default: 1 threshold \u2013 values above this revert to a linear function. Default: 20     Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input     Examples: &gt;&gt;&gt; m = nn.Softplus() &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; output = m(input)   ", "parameters": ["beta : the \u03b2\\beta\u03b2 value for the Softplus formulation. Default: 1", "threshold : values above this revert to a linear function. Default: 20"], "returns": null, "example": " m = nn.Softplus()\n input = torch.randn(2)\n output = m(input)\n\n", "shape": " Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input  "},
{"library": "torch", "item_id": "torch.nn.Softshrink", "item_type": "class", "code": "classtorch.nn.Softshrink(lambd=0.5)", "description": "Applies the soft shrinkage function elementwise:  SoftShrinkage(x)={x\u2212\u03bb,\u00a0if\u00a0x&gt;\u03bbx+\u03bb,\u00a0if\u00a0x&lt;\u2212\u03bb0,\u00a0otherwise\u00a0\\text{SoftShrinkage}(x) = \\begin{cases} x - \\lambda, &amp; \\text{ if } x &gt; \\lambda \\\\ x + \\lambda, &amp; \\text{ if } x &lt; -\\lambda \\\\ 0, &amp; \\text{ otherwise } \\end{cases}  SoftShrinkage(x)=\u23a9\u23aa\u23aa\u23a8\u23aa\u23aa\u23a7\u200bx\u2212\u03bb,x+\u03bb,0,\u200b\u00a0if\u00a0x&gt;\u03bb\u00a0if\u00a0x&lt;\u2212\u03bb\u00a0otherwise\u00a0\u200b   Parameters lambd \u2013 the \u03bb\\lambda\u03bb   value for the Softshrink formulation. Default: 0.5    Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input     Examples: &gt;&gt;&gt; m = nn.Softshrink() &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; output = m(input)   ", "parameters": ["lambd : the \u03bb\\lambda\u03bb value for the Softshrink formulation. Default: 0.5"], "returns": null, "example": " m = nn.Softshrink()\n input = torch.randn(2)\n output = m(input)\n\n", "shape": " Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input  "},
{"library": "torch", "item_id": "torch.nn.Softsign", "item_type": "class", "code": "classtorch.nn.Softsign", "description": "Applies the element-wise function:  SoftSign(x)=x1+\u2223x\u2223\\text{SoftSign}(x) = \\frac{x}{ 1 + |x|}  SoftSign(x)=1+\u2223x\u2223x\u200b   Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input     Examples: &gt;&gt;&gt; m = nn.Softsign() &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; output = m(input)   ", "parameters": [], "returns": null, "example": " m = nn.Softsign()\n input = torch.randn(2)\n output = m(input)\n\n", "shape": " Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input  "},
{"library": "torch", "item_id": "torch.bincount", "item_type": "function", "code": "torch.bincount(input,weights=None,minlength=0)\u2192Tensor", "description": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in input unless input is empty, in which case the result is a tensor of size 0. If minlength is specified, the number of bins is at least minlength and if input is empty, then the result is tensor of size minlength filled with zeros. If n is the value at position i, out[n] += weights[i] if weights is specified else out[n] += 1.  Note When using the CUDA backend, this operation may induce nondeterministic behaviour that is not easily switched off. Please see the notes on Reproducibility for background.   Parameters  input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor. Should be of same size as input tensor. minlength (python:int) \u2013 optional, minimum number of bins. Should be non-negative.   Returns a tensor of shape Size([max(input) + 1]) if input is non-empty, else Size(0)  Return type output (Tensor)   Example: &gt;&gt;&gt; input = torch.randint(0, 8, (5,), dtype=torch.int64) &gt;&gt;&gt; weights = torch.linspace(0, 1, steps=5) &gt;&gt;&gt; input, weights (tensor([4, 3, 6, 3, 4]),  tensor([ 0.0000,  0.2500,  0.5000,  0.7500,  1.0000])  &gt;&gt;&gt; torch.bincount(input) tensor([0, 0, 0, 2, 2, 0, 1])  &gt;&gt;&gt; input.bincount(weights) tensor([0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.5000])   ", "parameters": ["input (Tensor) : 1-d int tensor", "weights (Tensor) : optional, weight for each value in the input tensor.Should be of same size as input tensor.", "minlength (python:int) : optional, minimum number of bins. Should be non-negative."], "returns": "a tensor of shape Size([max(input) + 1]) ifinput is non-empty, else Size(0)", "example": " input = torch.randint(0, 8, (5,), dtype=torch.int64)\n weights = torch.linspace(0, 1, steps=5)\n input, weights\n(tensor([4, 3, 6, 3, 4]),\n tensor([ 0.0000,  0.2500,  0.5000,  0.7500,  1.0000])\n\n torch.bincount(input)\ntensor([0, 0, 0, 2, 2, 0, 1])\n\n input.bincount(weights)\ntensor([0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.5000])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.broadcast_tensors", "item_type": "function", "code": "torch.broadcast_tensors(*tensors)\u2192ListofTensors", "description": "Broadcasts the given tensors according to Broadcasting semantics.  Parameters *tensors \u2013 any number of tensors of the same type    Warning More than one element of a broadcasted tensor may refer to a single memory location. As a result, in-place operations (especially ones that are vectorized) may result in incorrect behavior. If you need to write to the tensors, please clone them first.  Example: &gt;&gt;&gt; x = torch.arange(3).view(1, 3) &gt;&gt;&gt; y = torch.arange(2).view(2, 1) &gt;&gt;&gt; a, b = torch.broadcast_tensors(x, y) &gt;&gt;&gt; a.size() torch.Size([2, 3]) &gt;&gt;&gt; a tensor([[0, 1, 2],         [0, 1, 2]])   ", "parameters": ["*tensors : any number of tensors of the same type"], "returns": null, "example": " x = torch.arange(3).view(1, 3)\n y = torch.arange(2).view(2, 1)\n a, b = torch.broadcast_tensors(x, y)\n a.size()\ntorch.Size([2, 3])\n a\ntensor([[0, 1, 2],\n        [0, 1, 2]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.cartesian_prod", "item_type": "function", "code": "torch.cartesian_prod(*tensors)", "description": "Do cartesian product of the given sequence of tensors. The behavior is similar to python\u2019s itertools.product.  Parameters *tensors \u2013 any number of 1 dimensional tensors.  Returns  A tensor equivalent to converting all the input tensors into lists,do itertools.product on these lists, and finally convert the resulting list into tensor.     Return type Tensor   Example: &gt;&gt;&gt; a = [1, 2, 3] &gt;&gt;&gt; b = [4, 5] &gt;&gt;&gt; list(itertools.product(a, b)) [(1, 4), (1, 5), (2, 4), (2, 5), (3, 4), (3, 5)] &gt;&gt;&gt; tensor_a = torch.tensor(a) &gt;&gt;&gt; tensor_b = torch.tensor(b) &gt;&gt;&gt; torch.cartesian_prod(tensor_a, tensor_b) tensor([[1, 4],         [1, 5],         [2, 4],         [2, 5],         [3, 4],         [3, 5]])   ", "parameters": ["*tensors : any number of 1 dimensional tensors.", "A tensor equivalent to converting all the input tensors into lists,do itertools.product on these lists, and finally convert the resulting listinto tensor.", "do itertools.product on these lists, and finally convert the resulting listinto tensor.", "Tensor"], "returns": "A tensor equivalent to converting all the input tensors into lists,do itertools.product on these lists, and finally convert the resulting listinto tensor.", "example": " a = [1, 2, 3]\n b = [4, 5]\n list(itertools.product(a, b))\n[(1, 4), (1, 5), (2, 4), (2, 5), (3, 4), (3, 5)]\n tensor_a = torch.tensor(a)\n tensor_b = torch.tensor(b)\n torch.cartesian_prod(tensor_a, tensor_b)\ntensor([[1, 4],\n        [1, 5],\n        [2, 4],\n        [2, 5],\n        [3, 4],\n        [3, 5]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.cdist", "item_type": "function", "code": "torch.cdist(x1,x2,p=2,compute_mode='use_mm_for_euclid_dist_if_necessary')", "description": "Computes batched the p-norm distance between each pair of the two collections of row vectors.  Parameters  x1 (Tensor) \u2013 input tensor of shape B\u00d7P\u00d7MB \\times P \\times MB\u00d7P\u00d7M  . x2 (Tensor) \u2013 input tensor of shape B\u00d7R\u00d7MB \\times R \\times MB\u00d7R\u00d7M  . p \u2013 p value for the p-norm distance to calculate between each vector pair \u2208[0,\u221e]\\in [0, \\infty]\u2208[0,\u221e]  . compute_mode \u2013 \u2018use_mm_for_euclid_dist_if_necessary\u2019 - will use matrix multiplication approach to calculate euclidean distance (p = 2) if P &gt; 25 or R &gt; 25 \u2018use_mm_for_euclid_dist\u2019 - will always use matrix multiplication approach to calculate euclidean distance (p = 2) \u2018donot_use_mm_for_euclid_dist\u2019 - will never use matrix multiplication approach to calculate euclidean distance (p = 2) Default: use_mm_for_euclid_dist_if_necessary.    If x1 has shape B\u00d7P\u00d7MB \\times P \\times MB\u00d7P\u00d7M   and x2 has shape B\u00d7R\u00d7MB \\times R \\times MB\u00d7R\u00d7M   then the output will have shape B\u00d7P\u00d7RB \\times P \\times RB\u00d7P\u00d7R  . This function is equivalent to scipy.spatial.distance.cdist(input,\u2019minkowski\u2019, p=p) if p\u2208(0,\u221e)p \\in (0, \\infty)p\u2208(0,\u221e)  . When p=0p = 0p=0   it is equivalent to scipy.spatial.distance.cdist(input, \u2018hamming\u2019) * M. When p=\u221ep = \\inftyp=\u221e  , the closest scipy function is scipy.spatial.distance.cdist(xn, lambda x, y: np.abs(x - y).max()). Example &gt;&gt;&gt; a = torch.tensor([[0.9041,  0.0196], [-0.3108, -2.4423], [-0.4821,  1.059]]) &gt;&gt;&gt; a tensor([[ 0.9041,  0.0196],         [-0.3108, -2.4423],         [-0.4821,  1.0590]]) &gt;&gt;&gt; b = torch.tensor([[-2.1763, -0.4713], [-0.6986,  1.3702]]) &gt;&gt;&gt; b tensor([[-2.1763, -0.4713],         [-0.6986,  1.3702]]) &gt;&gt;&gt; torch.cdist(a, b, p=2) tensor([[3.1193, 2.0959],         [2.7138, 3.8322],         [2.2830, 0.3791]])   ", "parameters": ["x1 (Tensor) : input tensor of shape B\u00d7P\u00d7MB \\times P \\times MB\u00d7P\u00d7M.", "x2 (Tensor) : input tensor of shape B\u00d7R\u00d7MB \\times R \\times MB\u00d7R\u00d7M.", "p : p value for the p-norm distance to calculate between each vector pair\u2208[0,\u221e]\\in [0, \\infty]\u2208[0,\u221e].", "compute_mode : \u2018use_mm_for_euclid_dist_if_necessary\u2019 - will use matrix multiplication approach to calculateeuclidean distance (p = 2) if P &gt; 25 or R &gt; 25\u2018use_mm_for_euclid_dist\u2019 - will always use matrix multiplication approach to calculateeuclidean distance (p = 2)\u2018donot_use_mm_for_euclid_dist\u2019 - will never use matrix multiplication approach to calculateeuclidean distance (p = 2)Default: use_mm_for_euclid_dist_if_necessary."], "returns": null, "example": " a = torch.tensor([[0.9041,  0.0196], [-0.3108, -2.4423], [-0.4821,  1.059]])\n a\ntensor([[ 0.9041,  0.0196],\n        [-0.3108, -2.4423],\n        [-0.4821,  1.0590]])\n b = torch.tensor([[-2.1763, -0.4713], [-0.6986,  1.3702]])\n b\ntensor([[-2.1763, -0.4713],\n        [-0.6986,  1.3702]])\n torch.cdist(a, b, p=2)\ntensor([[3.1193, 2.0959],\n        [2.7138, 3.8322],\n        [2.2830, 0.3791]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.Tanh", "item_type": "class", "code": "classtorch.nn.Tanh", "description": "Applies the element-wise function:  Tanh(x)=tanh\u2061(x)=ex\u2212e\u2212xex+e\u2212x\\text{Tanh}(x) = \\tanh(x) = \\frac{e^x - e^{-x}} {e^x + e^{-x}}  Tanh(x)=tanh(x)=ex+e\u2212xex\u2212e\u2212x\u200b   Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input     Examples: &gt;&gt;&gt; m = nn.Tanh() &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; output = m(input)   ", "parameters": [], "returns": null, "example": " m = nn.Tanh()\n input = torch.randn(2)\n output = m(input)\n\n", "shape": " Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input  "},
{"library": "torch", "item_id": "torch.nn.Tanhshrink", "item_type": "class", "code": "classtorch.nn.Tanhshrink", "description": "Applies the element-wise function:  Tanhshrink(x)=x\u2212Tanh(x)\\text{Tanhshrink}(x) = x - \\text{Tanh}(x)  Tanhshrink(x)=x\u2212Tanh(x)   Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input     Examples: &gt;&gt;&gt; m = nn.Tanhshrink() &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; output = m(input)   ", "parameters": [], "returns": null, "example": " m = nn.Tanhshrink()\n input = torch.randn(2)\n output = m(input)\n\n", "shape": " Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input  "},
{"library": "torch", "item_id": "torch.nn.Threshold", "item_type": "class", "code": "classtorch.nn.Threshold(threshold,value,inplace=False)", "description": "Thresholds each element of the input Tensor. Threshold is defined as:  y={x,\u00a0if\u00a0x&gt;thresholdvalue,\u00a0otherwise\u00a0y = \\begin{cases} x, &amp;\\text{ if } x &gt; \\text{threshold} \\\\ \\text{value}, &amp;\\text{ otherwise } \\end{cases}  y={x,value,\u200b\u00a0if\u00a0x&gt;threshold\u00a0otherwise\u00a0\u200b   Parameters  threshold \u2013 The value to threshold at value \u2013 The value to replace with inplace \u2013 can optionally do the operation in-place. Default: False     Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input    Examples: &gt;&gt;&gt; m = nn.Threshold(0.1, 20) &gt;&gt;&gt; input = torch.randn(2) &gt;&gt;&gt; output = m(input)   ", "parameters": ["threshold : The value to threshold at", "value : The value to replace with", "inplace : can optionally do the operation in-place. Default: False"], "returns": null, "example": " m = nn.Threshold(0.1, 20)\n input = torch.randn(2)\n output = m(input)\n\n", "shape": " Input: (N,\u2217)(N, *)(N,\u2217)   where * means, any number of additional dimensions Output: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input  "},
{"library": "torch", "item_id": "torch.nn.Softmin", "item_type": "class", "code": "classtorch.nn.Softmin(dim=None)", "description": "Applies the Softmin function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0, 1] and sum to 1. Softmin is defined as:  Softmin(xi)=exp\u2061(\u2212xi)\u2211jexp\u2061(\u2212xj)\\text{Softmin}(x_{i}) = \\frac{\\exp(-x_i)}{\\sum_j \\exp(-x_j)}  Softmin(xi\u200b)=\u2211j\u200bexp(\u2212xj\u200b)exp(\u2212xi\u200b)\u200b   Shape: Input: (\u2217)(*)(\u2217)   where * means, any number of additional dimensions Output: (\u2217)(*)(\u2217)  , same shape as the input     Parameters dim (python:int) \u2013 A dimension along which Softmin will be computed (so every slice along dim will sum to 1).  Returns a Tensor of the same dimension and shape as the input, with values in the range [0, 1]   Examples: &gt;&gt;&gt; m = nn.Softmin() &gt;&gt;&gt; input = torch.randn(2, 3) &gt;&gt;&gt; output = m(input)   ", "parameters": ["dim (python:int) : A dimension along which Softmin will be computed (so every slicealong dim will sum to 1).", "a Tensor of the same dimension and shape as the input, withvalues in the range [0, 1]"], "returns": "a Tensor of the same dimension and shape as the input, withvalues in the range [0, 1]", "example": " m = nn.Softmin()\n input = torch.randn(2, 3)\n output = m(input)\n\n", "shape": " Input: (\u2217)(*)(\u2217)   where * means, any number of additional dimensions Output: (\u2217)(*)(\u2217)  , same shape as the input  "},
{"library": "torch", "item_id": "torch.nn.Softmax", "item_type": "class", "code": "classtorch.nn.Softmax(dim=None)", "description": "Applies the Softmax function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0,1] and sum to 1. Softmax is defined as:  Softmax(xi)=exp\u2061(xi)\u2211jexp\u2061(xj)\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}  Softmax(xi\u200b)=\u2211j\u200bexp(xj\u200b)exp(xi\u200b)\u200b   Shape: Input: (\u2217)(*)(\u2217)   where * means, any number of additional dimensions Output: (\u2217)(*)(\u2217)  , same shape as the input     Returns a Tensor of the same dimension and shape as the input with values in the range [0, 1]  Parameters dim (python:int) \u2013 A dimension along which Softmax will be computed (so every slice along dim will sum to 1).    Note This module doesn\u2019t work directly with NLLLoss, which expects the Log to be computed between the Softmax and itself. Use LogSoftmax instead (it\u2019s faster and has better numerical properties).  Examples: &gt;&gt;&gt; m = nn.Softmax(dim=1) &gt;&gt;&gt; input = torch.randn(2, 3) &gt;&gt;&gt; output = m(input)   ", "parameters": ["a Tensor of the same dimension and shape as the input withvalues in the range [0, 1]", "dim (python:int) : A dimension along which Softmax will be computed (so every slicealong dim will sum to 1)."], "returns": "a Tensor of the same dimension and shape as the input withvalues in the range [0, 1]", "example": " m = nn.Softmax(dim=1)\n input = torch.randn(2, 3)\n output = m(input)\n\n", "shape": " Input: (\u2217)(*)(\u2217)   where * means, any number of additional dimensions Output: (\u2217)(*)(\u2217)  , same shape as the input  "},
{"library": "torch", "item_id": "torch.combinations", "item_type": "function", "code": "torch.combinations(input,r=2,with_replacement=False)\u2192seq", "description": "Compute combinations of length rrr   of the given tensor. The behavior is similar to python\u2019s itertools.combinations when with_replacement is set to False, and itertools.combinations_with_replacement when with_replacement is set to True.  Parameters  input (Tensor) \u2013 1D vector. r (python:int, optional) \u2013 number of elements to combine with_replacement (boolean, optional) \u2013 whether to allow duplication in combination   Returns A tensor equivalent to converting all the input tensors into lists, do itertools.combinations or itertools.combinations_with_replacement on these lists, and finally convert the resulting list into tensor.  Return type Tensor   Example: &gt;&gt;&gt; a = [1, 2, 3] &gt;&gt;&gt; list(itertools.combinations(a, r=2)) [(1, 2), (1, 3), (2, 3)] &gt;&gt;&gt; list(itertools.combinations(a, r=3)) [(1, 2, 3)] &gt;&gt;&gt; list(itertools.combinations_with_replacement(a, r=2)) [(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)] &gt;&gt;&gt; tensor_a = torch.tensor(a) &gt;&gt;&gt; torch.combinations(tensor_a) tensor([[1, 2],         [1, 3],         [2, 3]]) &gt;&gt;&gt; torch.combinations(tensor_a, r=3) tensor([[1, 2, 3]]) &gt;&gt;&gt; torch.combinations(tensor_a, with_replacement=True) tensor([[1, 1],         [1, 2],         [1, 3],         [2, 2],         [2, 3],         [3, 3]])   ", "parameters": ["input (Tensor) : 1D vector.", "r (python:int, optional) : number of elements to combine", "with_replacement (boolean, optional) : whether to allow duplication in combination"], "returns": "A tensor equivalent to converting all the input tensors into lists, doitertools.combinations or itertools.combinations_with_replacement on theselists, and finally convert the resulting list into tensor.", "example": " a = [1, 2, 3]\n list(itertools.combinations(a, r=2))\n[(1, 2), (1, 3), (2, 3)]\n list(itertools.combinations(a, r=3))\n[(1, 2, 3)]\n list(itertools.combinations_with_replacement(a, r=2))\n[(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)]\n tensor_a = torch.tensor(a)\n torch.combinations(tensor_a)\ntensor([[1, 2],\n        [1, 3],\n        [2, 3]])\n torch.combinations(tensor_a, r=3)\ntensor([[1, 2, 3]])\n torch.combinations(tensor_a, with_replacement=True)\ntensor([[1, 1],\n        [1, 2],\n        [1, 3],\n        [2, 2],\n        [2, 3],\n        [3, 3]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.cross", "item_type": "function", "code": "torch.cross(input,other,dim=-1,out=None)\u2192Tensor", "description": "Returns the cross product of vectors in dimension dim of input and other. input and other must have the same size, and the size of their dim dimension should be 3. If dim is not given, it defaults to the first dimension found with the size 3.  Parameters  input (Tensor) \u2013 the input tensor. other (Tensor) \u2013 the second input tensor dim (python:int, optional) \u2013 the dimension to take the cross-product in. out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; a = torch.randn(4, 3) &gt;&gt;&gt; a tensor([[-0.3956,  1.1455,  1.6895],         [-0.5849,  1.3672,  0.3599],         [-1.1626,  0.7180, -0.0521],         [-0.1339,  0.9902, -2.0225]]) &gt;&gt;&gt; b = torch.randn(4, 3) &gt;&gt;&gt; b tensor([[-0.0257, -1.4725, -1.2251],         [-1.1479, -0.7005, -1.9757],         [-1.3904,  0.3726, -1.1836],         [-0.9688, -0.7153,  0.2159]]) &gt;&gt;&gt; torch.cross(a, b, dim=1) tensor([[ 1.0844, -0.5281,  0.6120],         [-2.4490, -1.5687,  1.9792],         [-0.8304, -1.3037,  0.5650],         [-1.2329,  1.9883,  1.0551]]) &gt;&gt;&gt; torch.cross(a, b) tensor([[ 1.0844, -0.5281,  0.6120],         [-2.4490, -1.5687,  1.9792],         [-0.8304, -1.3037,  0.5650],         [-1.2329,  1.9883,  1.0551]])   ", "parameters": ["input (Tensor) : the input tensor.", "other (Tensor) : the second input tensor", "dim (python:int, optional) : the dimension to take the cross-product in.", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " a = torch.randn(4, 3)\n a\ntensor([[-0.3956,  1.1455,  1.6895],\n        [-0.5849,  1.3672,  0.3599],\n        [-1.1626,  0.7180, -0.0521],\n        [-0.1339,  0.9902, -2.0225]])\n b = torch.randn(4, 3)\n b\ntensor([[-0.0257, -1.4725, -1.2251],\n        [-1.1479, -0.7005, -1.9757],\n        [-1.3904,  0.3726, -1.1836],\n        [-0.9688, -0.7153,  0.2159]])\n torch.cross(a, b, dim=1)\ntensor([[ 1.0844, -0.5281,  0.6120],\n        [-2.4490, -1.5687,  1.9792],\n        [-0.8304, -1.3037,  0.5650],\n        [-1.2329,  1.9883,  1.0551]])\n torch.cross(a, b)\ntensor([[ 1.0844, -0.5281,  0.6120],\n        [-2.4490, -1.5687,  1.9792],\n        [-0.8304, -1.3037,  0.5650],\n        [-1.2329,  1.9883,  1.0551]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.cumprod", "item_type": "function", "code": "torch.cumprod(input,dim,out=None,dtype=None)\u2192Tensor", "description": "Returns the cumulative product of elements of input in the dimension dim. For example, if input is a vector of size N, the result will also be a vector of size N, with elements.  yi=x1\u00d7x2\u00d7x3\u00d7\u22ef\u00d7xiy_i = x_1 \\times x_2\\times x_3\\times \\dots \\times x_i  yi\u200b=x1\u200b\u00d7x2\u200b\u00d7x3\u200b\u00d7\u22ef\u00d7xi\u200b   Parameters  input (Tensor) \u2013 the input tensor. dim (python:int) \u2013 the dimension to do the operation over dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None. out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; a = torch.randn(10) &gt;&gt;&gt; a tensor([ 0.6001,  0.2069, -0.1919,  0.9792,  0.6727,  1.0062,  0.4126,         -0.2129, -0.4206,  0.1968]) &gt;&gt;&gt; torch.cumprod(a, dim=0) tensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0158, -0.0065,          0.0014, -0.0006, -0.0001])  &gt;&gt;&gt; a[5] = 0.0 &gt;&gt;&gt; torch.cumprod(a, dim=0) tensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0000, -0.0000,          0.0000, -0.0000, -0.0000])   ", "parameters": ["input (Tensor) : the input tensor.", "dim (python:int) : the dimension to do the operation over", "dtype (torch.dtype, optional) : the desired data type of returned tensor.If specified, the input tensor is casted to dtype before the operationis performed. This is useful for preventing data type overflows. Default: None.", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " a = torch.randn(10)\n a\ntensor([ 0.6001,  0.2069, -0.1919,  0.9792,  0.6727,  1.0062,  0.4126,\n        -0.2129, -0.4206,  0.1968])\n torch.cumprod(a, dim=0)\ntensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0158, -0.0065,\n         0.0014, -0.0006, -0.0001])\n\n a[5] = 0.0\n torch.cumprod(a, dim=0)\ntensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0000, -0.0000,\n         0.0000, -0.0000, -0.0000])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.Softmax2d", "item_type": "class", "code": "classtorch.nn.Softmax2d", "description": "Applies SoftMax over features to each spatial location. When given an image of Channels x Height x Width, it will apply Softmax to each location (Channels,hi,wj)(Channels, h_i, w_j)(Channels,hi\u200b,wj\u200b)    Shape: Input: (N,C,H,W)(N, C, H, W)(N,C,H,W)   Output: (N,C,H,W)(N, C, H, W)(N,C,H,W)   (same shape as input)     Returns a Tensor of the same dimension and shape as the input with values in the range [0, 1]   Examples: &gt;&gt;&gt; m = nn.Softmax2d() &gt;&gt;&gt; # you softmax over the 2nd dimension &gt;&gt;&gt; input = torch.randn(2, 3, 12, 13) &gt;&gt;&gt; output = m(input)   ", "parameters": [], "returns": "a Tensor of the same dimension and shape as the input withvalues in the range [0, 1]", "example": " m = nn.Softmax2d()\n # you softmax over the 2nd dimension\n input = torch.randn(2, 3, 12, 13)\n output = m(input)\n\n", "shape": " Input: (N,C,H,W)(N, C, H, W)(N,C,H,W)   Output: (N,C,H,W)(N, C, H, W)(N,C,H,W)   (same shape as input)  "},
{"library": "torch", "item_id": "torch.nn.LogSoftmax", "item_type": "class", "code": "classtorch.nn.LogSoftmax(dim=None)", "description": "Applies the log\u2061(Softmax(x))\\log(\\text{Softmax}(x))log(Softmax(x))   function to an n-dimensional input Tensor. The LogSoftmax formulation can be simplified as:  LogSoftmax(xi)=log\u2061(exp\u2061(xi)\u2211jexp\u2061(xj))\\text{LogSoftmax}(x_{i}) = \\log\\left(\\frac{\\exp(x_i) }{ \\sum_j \\exp(x_j)} \\right)  LogSoftmax(xi\u200b)=log(\u2211j\u200bexp(xj\u200b)exp(xi\u200b)\u200b)   Shape: Input: (\u2217)(*)(\u2217)   where * means, any number of additional dimensions Output: (\u2217)(*)(\u2217)  , same shape as the input     Parameters dim (python:int) \u2013 A dimension along which LogSoftmax will be computed.  Returns a Tensor of the same dimension and shape as the input with values in the range [-inf, 0)   Examples: &gt;&gt;&gt; m = nn.LogSoftmax() &gt;&gt;&gt; input = torch.randn(2, 3) &gt;&gt;&gt; output = m(input)   ", "parameters": ["dim (python:int) : A dimension along which LogSoftmax will be computed.", "a Tensor of the same dimension and shape as the input withvalues in the range [-inf, 0)"], "returns": "a Tensor of the same dimension and shape as the input withvalues in the range [-inf, 0)", "example": " m = nn.LogSoftmax()\n input = torch.randn(2, 3)\n output = m(input)\n\n", "shape": " Input: (\u2217)(*)(\u2217)   where * means, any number of additional dimensions Output: (\u2217)(*)(\u2217)  , same shape as the input  "},
{"library": "torch", "item_id": "torch.nn.AdaptiveLogSoftmaxWithLoss", "item_type": "class", "code": "classtorch.nn.AdaptiveLogSoftmaxWithLoss(in_features,n_classes,cutoffs,div_value=4.0,head_bias=False)", "description": "Efficient softmax approximation as described in Efficient softmax approximation for GPUs by Edouard Grave, Armand Joulin, Moustapha Ciss\u00e9, David Grangier, and Herv\u00e9 J\u00e9gou. Adaptive softmax is an approximate strategy for training models with large output spaces. It is most effective when the label distribution is highly imbalanced, for example in natural language modelling, where the word frequency distribution approximately follows the Zipf\u2019s law. Adaptive softmax partitions the labels into several clusters, according to their frequency. These clusters may contain different number of targets each. Additionally, clusters containing less frequent labels assign lower dimensional embeddings to those labels, which speeds up the computation. For each minibatch, only clusters for which at least one target is present are evaluated. The idea is that the clusters which are accessed frequently (like the first one, containing most frequent labels), should also be cheap to compute \u2013 that is, contain a small number of assigned labels. We highly recommend taking a look at the original paper for more details.  cutoffs should be an ordered Sequence of integers sorted in the increasing order. It controls number of clusters and the partitioning of targets into clusters. For example setting cutoffs = [10, 100, 1000] means that first 10 targets will be assigned to the \u2018head\u2019 of the adaptive softmax, targets 11, 12, \u2026, 100 will be assigned to the first cluster, and targets 101, 102, \u2026, 1000 will be assigned to the second cluster, while targets 1001, 1002, \u2026, n_classes - 1 will be assigned to the last, third cluster. div_value is used to compute the size of each additional cluster, which is given as \u230ain_featuresdiv_valueidx\u230b\\left\\lfloor\\frac{in\\_features}{div\\_value^{idx}}\\right\\rfloor\u230adiv_valueidxin_features\u200b\u230b  , where idxidxidx   is the cluster index (with clusters for less frequent words having larger indices, and indices starting from 111  ). head_bias if set to True, adds a bias term to the \u2018head\u2019 of the adaptive softmax. See paper for details. Set to False in the official implementation.   Warning Labels passed as inputs to this module should be sorted accoridng to their frequency. This means that the most frequent label should be represented by the index 0, and the least frequent label should be represented by the index n_classes - 1.   Note This module returns a NamedTuple with output and loss fields. See further documentation for details.   Note To compute log-probabilities for all classes, the log_prob method can be used.   Parameters  in_features (python:int) \u2013 Number of features in the input tensor n_classes (python:int) \u2013 Number of classes in the dataset cutoffs (Sequence) \u2013 Cutoffs used to assign targets to their buckets div_value (python:float, optional) \u2013 value used as an exponent to compute sizes of the clusters. Default: 4.0 head_bias (bool, optional) \u2013 If True, adds a bias term to the \u2018head\u2019 of the adaptive softmax. Default: False   Returns  output is a Tensor of size N containing computed target log probabilities for each example loss is a Scalar representing the computed negative log likelihood loss    Return type NamedTuple with output and loss fields    Shape: input: (N,in_features)(N, in\\_features)(N,in_features)   target: (N)(N)(N)   where each value satisfies 0&lt;=target[i]&lt;=n_classes0 &lt;= target[i] &lt;= n\\_classes0&lt;=target[i]&lt;=n_classes   output1: (N)(N)(N)   output2: Scalar      log_prob(input)  Computes log probabilities for all n_classesn\\_classesn_classes    Parameters input (Tensor) \u2013 a minibatch of examples  Returns log-probabilities of for each class ccc   in range 0&lt;=c&lt;=n_classes0 &lt;= c &lt;= n\\_classes0&lt;=c&lt;=n_classes  , where n_classesn\\_classesn_classes   is a parameter passed to AdaptiveLogSoftmaxWithLoss constructor.    Shape: Input: (N,in_features)(N, in\\_features)(N,in_features)   Output: (N,n_classes)(N, n\\_classes)(N,n_classes)          predict(input)  This is equivalent to self.log_pob(input).argmax(dim=1), but is more efficient in some cases.  Parameters input (Tensor) \u2013 a minibatch of examples  Returns a class with the highest probability for each example  Return type output (Tensor)    Shape: Input: (N,in_features)(N, in\\_features)(N,in_features)   Output: (N)(N)(N)        ", "parameters": ["in_features (python:int) : Number of features in the input tensor", "n_classes (python:int) : Number of classes in the dataset", "cutoffs (Sequence) : Cutoffs used to assign targets to their buckets", "div_value (python:float, optional) : value used as an exponent to compute sizesof the clusters. Default: 4.0", "head_bias (bool, optional) : If True, adds a bias term to the \u2018head\u2019 of theadaptive softmax. Default: False", "output is a Tensor of size N containing computed targetlog probabilities for each example", "loss is a Scalar representing the computed negativelog likelihood loss"], "returns": "output is a Tensor of size N containing computed targetlog probabilities for each exampleloss is a Scalar representing the computed negativelog likelihood loss", "example": "NA", "shape": " input: (N,in_features)(N, in\\_features)(N,in_features)   target: (N)(N)(N)   where each value satisfies 0&lt;=target[i]&lt;=n_classes0 &lt;= target[i] &lt;= n\\_classes0&lt;=target[i]&lt;=n_classes   output1: (N)(N)(N)   output2: Scalar  "},
{"library": "torch", "item_id": "torch.cumsum", "item_type": "function", "code": "torch.cumsum(input,dim,out=None,dtype=None)\u2192Tensor", "description": "Returns the cumulative sum of elements of input in the dimension dim. For example, if input is a vector of size N, the result will also be a vector of size N, with elements.  yi=x1+x2+x3+\u22ef+xiy_i = x_1 + x_2 + x_3 + \\dots + x_i  yi\u200b=x1\u200b+x2\u200b+x3\u200b+\u22ef+xi\u200b   Parameters  input (Tensor) \u2013 the input tensor. dim (python:int) \u2013 the dimension to do the operation over dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. If specified, the input tensor is casted to dtype before the operation is performed. This is useful for preventing data type overflows. Default: None. out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; a = torch.randn(10) &gt;&gt;&gt; a tensor([-0.8286, -0.4890,  0.5155,  0.8443,  0.1865, -0.1752, -2.0595,          0.1850, -1.1571, -0.4243]) &gt;&gt;&gt; torch.cumsum(a, dim=0) tensor([-0.8286, -1.3175, -0.8020,  0.0423,  0.2289,  0.0537, -2.0058,         -1.8209, -2.9780, -3.4022])   ", "parameters": ["input (Tensor) : the input tensor.", "dim (python:int) : the dimension to do the operation over", "dtype (torch.dtype, optional) : the desired data type of returned tensor.If specified, the input tensor is casted to dtype before the operationis performed. This is useful for preventing data type overflows. Default: None.", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " a = torch.randn(10)\n a\ntensor([-0.8286, -0.4890,  0.5155,  0.8443,  0.1865, -0.1752, -2.0595,\n         0.1850, -1.1571, -0.4243])\n torch.cumsum(a, dim=0)\ntensor([-0.8286, -1.3175, -0.8020,  0.0423,  0.2289,  0.0537, -2.0058,\n        -1.8209, -2.9780, -3.4022])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.diag", "item_type": "function", "code": "torch.diag(input,diagonal=0,out=None)\u2192Tensor", "description": " If input is a vector (1-D tensor), then returns a 2-D square tensor with the elements of input as the diagonal. If input is a matrix (2-D tensor), then returns a 1-D tensor with the diagonal elements of input.  The argument diagonal controls which diagonal to consider:  If diagonal = 0, it is the main diagonal. If diagonal &gt; 0, it is above the main diagonal. If diagonal &lt; 0, it is below the main diagonal.   Parameters  input (Tensor) \u2013 the input tensor. diagonal (python:int, optional) \u2013 the diagonal to consider out (Tensor, optional) \u2013 the output tensor.     See also torch.diagonal() always returns the diagonal of its input. torch.diagflat() always constructs a tensor with diagonal elements specified by the input.  Examples: Get the square matrix where the input vector is the diagonal: &gt;&gt;&gt; a = torch.randn(3) &gt;&gt;&gt; a tensor([ 0.5950,-0.0872, 2.3298]) &gt;&gt;&gt; torch.diag(a) tensor([[ 0.5950, 0.0000, 0.0000],         [ 0.0000,-0.0872, 0.0000],         [ 0.0000, 0.0000, 2.3298]]) &gt;&gt;&gt; torch.diag(a, 1) tensor([[ 0.0000, 0.5950, 0.0000, 0.0000],         [ 0.0000, 0.0000,-0.0872, 0.0000],         [ 0.0000, 0.0000, 0.0000, 2.3298],         [ 0.0000, 0.0000, 0.0000, 0.0000]])   Get the k-th diagonal of a given matrix: &gt;&gt;&gt; a = torch.randn(3, 3) &gt;&gt;&gt; a tensor([[-0.4264, 0.0255,-0.1064],         [ 0.8795,-0.2429, 0.1374],         [ 0.1029,-0.6482,-1.6300]]) &gt;&gt;&gt; torch.diag(a, 0) tensor([-0.4264,-0.2429,-1.6300]) &gt;&gt;&gt; torch.diag(a, 1) tensor([ 0.0255, 0.1374])   ", "parameters": ["input (Tensor) : the input tensor.", "diagonal (python:int, optional) : the diagonal to consider", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " a = torch.randn(3)\n a\ntensor([ 0.5950,-0.0872, 2.3298])\n torch.diag(a)\ntensor([[ 0.5950, 0.0000, 0.0000],\n        [ 0.0000,-0.0872, 0.0000],\n        [ 0.0000, 0.0000, 2.3298]])\n torch.diag(a, 1)\ntensor([[ 0.0000, 0.5950, 0.0000, 0.0000],\n        [ 0.0000, 0.0000,-0.0872, 0.0000],\n        [ 0.0000, 0.0000, 0.0000, 2.3298],\n        [ 0.0000, 0.0000, 0.0000, 0.0000]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.diag_embed", "item_type": "function", "code": "torch.diag_embed(input,offset=0,dim1=-2,dim2=-1)\u2192Tensor", "description": "Creates a tensor whose diagonals of certain 2D planes (specified by dim1 and dim2) are filled by input. To facilitate creating batched diagonal matrices, the 2D planes formed by the last two dimensions of the returned tensor are chosen by default. The argument offset controls which diagonal to consider:  If offset = 0, it is the main diagonal. If offset &gt; 0, it is above the main diagonal. If offset &lt; 0, it is below the main diagonal.  The size of the new matrix will be calculated to make the specified diagonal of the size of the last input dimension. Note that for offset other than 000  , the order of dim1 and dim2 matters. Exchanging them is equivalent to changing the sign of offset. Applying torch.diagonal() to the output of this function with the same arguments yields a matrix identical to input. However, torch.diagonal() has different default dimensions, so those need to be explicitly specified.  Parameters  input (Tensor) \u2013 the input tensor. Must be at least 1-dimensional. offset (python:int, optional) \u2013 which diagonal to consider. Default: 0 (main diagonal). dim1 (python:int, optional) \u2013 first dimension with respect to which to take diagonal. Default: -2. dim2 (python:int, optional) \u2013 second dimension with respect to which to take diagonal. Default: -1.    Example: &gt;&gt;&gt; a = torch.randn(2, 3) &gt;&gt;&gt; torch.diag_embed(a) tensor([[[ 1.5410,  0.0000,  0.0000],          [ 0.0000, -0.2934,  0.0000],          [ 0.0000,  0.0000, -2.1788]],          [[ 0.5684,  0.0000,  0.0000],          [ 0.0000, -1.0845,  0.0000],          [ 0.0000,  0.0000, -1.3986]]])  &gt;&gt;&gt; torch.diag_embed(a, offset=1, dim1=0, dim2=2) tensor([[[ 0.0000,  1.5410,  0.0000,  0.0000],          [ 0.0000,  0.5684,  0.0000,  0.0000]],          [[ 0.0000,  0.0000, -0.2934,  0.0000],          [ 0.0000,  0.0000, -1.0845,  0.0000]],          [[ 0.0000,  0.0000,  0.0000, -2.1788],          [ 0.0000,  0.0000,  0.0000, -1.3986]],          [[ 0.0000,  0.0000,  0.0000,  0.0000],          [ 0.0000,  0.0000,  0.0000,  0.0000]]])   ", "parameters": ["input (Tensor) : the input tensor. Must be at least 1-dimensional.", "offset (python:int, optional) : which diagonal to consider. Default: 0(main diagonal).", "dim1 (python:int, optional) : first dimension with respect to which totake diagonal. Default: -2.", "dim2 (python:int, optional) : second dimension with respect to which totake diagonal. Default: -1."], "returns": null, "example": " a = torch.randn(2, 3)\n torch.diag_embed(a)\ntensor([[[ 1.5410,  0.0000,  0.0000],\n         [ 0.0000, -0.2934,  0.0000],\n         [ 0.0000,  0.0000, -2.1788]],\n\n        [[ 0.5684,  0.0000,  0.0000],\n         [ 0.0000, -1.0845,  0.0000],\n         [ 0.0000,  0.0000, -1.3986]]])\n\n torch.diag_embed(a, offset=1, dim1=0, dim2=2)\ntensor([[[ 0.0000,  1.5410,  0.0000,  0.0000],\n         [ 0.0000,  0.5684,  0.0000,  0.0000]],\n\n        [[ 0.0000,  0.0000, -0.2934,  0.0000],\n         [ 0.0000,  0.0000, -1.0845,  0.0000]],\n\n        [[ 0.0000,  0.0000,  0.0000, -2.1788],\n         [ 0.0000,  0.0000,  0.0000, -1.3986]],\n\n        [[ 0.0000,  0.0000,  0.0000,  0.0000],\n         [ 0.0000,  0.0000,  0.0000,  0.0000]]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.BatchNorm1d", "item_type": "class", "code": "classtorch.nn.BatchNorm1d(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)", "description": "Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .  y=x\u2212E[x]Var[x]+\u03f5\u2217\u03b3+\u03b2y = \\frac{x - \\mathrm{E}[x]}{\\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\betay=Var[x]+\u03f5\u200bx\u2212E[x]\u200b\u2217\u03b3+\u03b2  The mean and standard-deviation are calculated per-dimension over the mini-batches and \u03b3\\gamma\u03b3   and \u03b2\\beta\u03b2   are learnable parameter vectors of size C (where C is the input size). By default, the elements of \u03b3\\gamma\u03b3   are set to 1 and the elements of \u03b2\\beta\u03b2   are set to 0. Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1. If track_running_stats is set to False, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well.  Note This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is x^new=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_tx^new\u200b=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\u200b  , where x^\\hat{x}x^   is the estimated statistic and xtx_txt\u200b   is the new observed value.  Because the Batch Normalization is done over the C dimension, computing statistics on (N, L) slices, it\u2019s common terminology to call this Temporal Batch Normalization.  Parameters  num_features \u2013 CCC   from an expected input of size (N,C,L)(N, C, L)(N,C,L)   or LLL   from input of size (N,L)(N, L)(N,L)   eps \u2013 a value added to the denominator for numerical stability. Default: 1e-5 momentum \u2013 the value used for the running_mean and running_var computation. Can be set to None for cumulative moving average (i.e. simple average). Default: 0.1 affine \u2013 a boolean value that when set to True, this module has learnable affine parameters. Default: True track_running_stats \u2013 a boolean value that when set to True, this module tracks the running mean and variance, and when set to False, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: True     Shape: Input: (N,C)(N, C)(N,C)   or (N,C,L)(N, C, L)(N,C,L)   Output: (N,C)(N, C)(N,C)   or (N,C,L)(N, C, L)(N,C,L)   (same shape as input)    Examples: &gt;&gt;&gt; # With Learnable Parameters &gt;&gt;&gt; m = nn.BatchNorm1d(100) &gt;&gt;&gt; # Without Learnable Parameters &gt;&gt;&gt; m = nn.BatchNorm1d(100, affine=False) &gt;&gt;&gt; input = torch.randn(20, 100) &gt;&gt;&gt; output = m(input)   ", "parameters": ["num_features : CCC from an expected input of size(N,C,L)(N, C, L)(N,C,L) or LLL from input of size (N,L)(N, L)(N,L)", "eps : a value added to the denominator for numerical stability.Default: 1e-5", "momentum : the value used for the running_mean and running_varcomputation. Can be set to None for cumulative moving average(i.e. simple average). Default: 0.1", "affine : a boolean value that when set to True, this module haslearnable affine parameters. Default: True", "track_running_stats : a boolean value that when set to True, thismodule tracks the running mean and variance, and when set to False,this module does not track such statistics and always uses batchstatistics in both training and eval modes. Default: True"], "returns": null, "example": " # With Learnable Parameters\n m = nn.BatchNorm1d(100)\n # Without Learnable Parameters\n m = nn.BatchNorm1d(100, affine=False)\n input = torch.randn(20, 100)\n output = m(input)\n\n", "shape": " Input: (N,C)(N, C)(N,C)   or (N,C,L)(N, C, L)(N,C,L)   Output: (N,C)(N, C)(N,C)   or (N,C,L)(N, C, L)(N,C,L)   (same shape as input)  "},
{"library": "torch", "item_id": "torch.nn.BatchNorm2d", "item_type": "class", "code": "classtorch.nn.BatchNorm2d(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)", "description": "Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .  y=x\u2212E[x]Var[x]+\u03f5\u2217\u03b3+\u03b2y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\betay=Var[x]+\u03f5\u200bx\u2212E[x]\u200b\u2217\u03b3+\u03b2  The mean and standard-deviation are calculated per-dimension over the mini-batches and \u03b3\\gamma\u03b3   and \u03b2\\beta\u03b2   are learnable parameter vectors of size C (where C is the input size). By default, the elements of \u03b3\\gamma\u03b3   are set to 1 and the elements of \u03b2\\beta\u03b2   are set to 0. Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1. If track_running_stats is set to False, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well.  Note This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is x^new=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_tx^new\u200b=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\u200b  , where x^\\hat{x}x^   is the estimated statistic and xtx_txt\u200b   is the new observed value.  Because the Batch Normalization is done over the C dimension, computing statistics on (N, H, W) slices, it\u2019s common terminology to call this Spatial Batch Normalization.  Parameters  num_features \u2013 CCC   from an expected input of size (N,C,H,W)(N, C, H, W)(N,C,H,W)   eps \u2013 a value added to the denominator for numerical stability. Default: 1e-5 momentum \u2013 the value used for the running_mean and running_var computation. Can be set to None for cumulative moving average (i.e. simple average). Default: 0.1 affine \u2013 a boolean value that when set to True, this module has learnable affine parameters. Default: True track_running_stats \u2013 a boolean value that when set to True, this module tracks the running mean and variance, and when set to False, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: True     Shape: Input: (N,C,H,W)(N, C, H, W)(N,C,H,W)   Output: (N,C,H,W)(N, C, H, W)(N,C,H,W)   (same shape as input)    Examples: &gt;&gt;&gt; # With Learnable Parameters &gt;&gt;&gt; m = nn.BatchNorm2d(100) &gt;&gt;&gt; # Without Learnable Parameters &gt;&gt;&gt; m = nn.BatchNorm2d(100, affine=False) &gt;&gt;&gt; input = torch.randn(20, 100, 35, 45) &gt;&gt;&gt; output = m(input)   ", "parameters": ["num_features : CCC from an expected input of size(N,C,H,W)(N, C, H, W)(N,C,H,W)", "eps : a value added to the denominator for numerical stability.Default: 1e-5", "momentum : the value used for the running_mean and running_varcomputation. Can be set to None for cumulative moving average(i.e. simple average). Default: 0.1", "affine : a boolean value that when set to True, this module haslearnable affine parameters. Default: True", "track_running_stats : a boolean value that when set to True, thismodule tracks the running mean and variance, and when set to False,this module does not track such statistics and always uses batchstatistics in both training and eval modes. Default: True"], "returns": null, "example": " # With Learnable Parameters\n m = nn.BatchNorm2d(100)\n # Without Learnable Parameters\n m = nn.BatchNorm2d(100, affine=False)\n input = torch.randn(20, 100, 35, 45)\n output = m(input)\n\n", "shape": " Input: (N,C,H,W)(N, C, H, W)(N,C,H,W)   Output: (N,C,H,W)(N, C, H, W)(N,C,H,W)   (same shape as input)  "},
{"library": "torch", "item_id": "torch.diagflat", "item_type": "function", "code": "torch.diagflat(input,offset=0)\u2192Tensor", "description": " If input is a vector (1-D tensor), then returns a 2-D square tensor with the elements of input as the diagonal. If input is a tensor with more than one dimension, then returns a 2-D tensor with diagonal elements equal to a flattened input.  The argument offset controls which diagonal to consider:  If offset = 0, it is the main diagonal. If offset &gt; 0, it is above the main diagonal. If offset &lt; 0, it is below the main diagonal.   Parameters  input (Tensor) \u2013 the input tensor. offset (python:int, optional) \u2013 the diagonal to consider. Default: 0 (main diagonal).    Examples: &gt;&gt;&gt; a = torch.randn(3) &gt;&gt;&gt; a tensor([-0.2956, -0.9068,  0.1695]) &gt;&gt;&gt; torch.diagflat(a) tensor([[-0.2956,  0.0000,  0.0000],         [ 0.0000, -0.9068,  0.0000],         [ 0.0000,  0.0000,  0.1695]]) &gt;&gt;&gt; torch.diagflat(a, 1) tensor([[ 0.0000, -0.2956,  0.0000,  0.0000],         [ 0.0000,  0.0000, -0.9068,  0.0000],         [ 0.0000,  0.0000,  0.0000,  0.1695],         [ 0.0000,  0.0000,  0.0000,  0.0000]])  &gt;&gt;&gt; a = torch.randn(2, 2) &gt;&gt;&gt; a tensor([[ 0.2094, -0.3018],         [-0.1516,  1.9342]]) &gt;&gt;&gt; torch.diagflat(a) tensor([[ 0.2094,  0.0000,  0.0000,  0.0000],         [ 0.0000, -0.3018,  0.0000,  0.0000],         [ 0.0000,  0.0000, -0.1516,  0.0000],         [ 0.0000,  0.0000,  0.0000,  1.9342]])   ", "parameters": ["input (Tensor) : the input tensor.", "offset (python:int, optional) : the diagonal to consider. Default: 0 (maindiagonal)."], "returns": null, "example": " a = torch.randn(3)\n a\ntensor([-0.2956, -0.9068,  0.1695])\n torch.diagflat(a)\ntensor([[-0.2956,  0.0000,  0.0000],\n        [ 0.0000, -0.9068,  0.0000],\n        [ 0.0000,  0.0000,  0.1695]])\n torch.diagflat(a, 1)\ntensor([[ 0.0000, -0.2956,  0.0000,  0.0000],\n        [ 0.0000,  0.0000, -0.9068,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.1695],\n        [ 0.0000,  0.0000,  0.0000,  0.0000]])\n\n a = torch.randn(2, 2)\n a\ntensor([[ 0.2094, -0.3018],\n        [-0.1516,  1.9342]])\n torch.diagflat(a)\ntensor([[ 0.2094,  0.0000,  0.0000,  0.0000],\n        [ 0.0000, -0.3018,  0.0000,  0.0000],\n        [ 0.0000,  0.0000, -0.1516,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  1.9342]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.diagonal", "item_type": "function", "code": "torch.diagonal(input,offset=0,dim1=0,dim2=1)\u2192Tensor", "description": "Returns a partial view of input with the its diagonal elements with respect to dim1 and dim2 appended as a dimension at the end of the shape. The argument offset controls which diagonal to consider:  If offset = 0, it is the main diagonal. If offset &gt; 0, it is above the main diagonal. If offset &lt; 0, it is below the main diagonal.  Applying torch.diag_embed() to the output of this function with the same arguments yields a diagonal matrix with the diagonal entries of the input. However, torch.diag_embed() has different default dimensions, so those need to be explicitly specified.  Parameters  input (Tensor) \u2013 the input tensor. Must be at least 2-dimensional. offset (python:int, optional) \u2013 which diagonal to consider. Default: 0 (main diagonal). dim1 (python:int, optional) \u2013 first dimension with respect to which to take diagonal. Default: 0. dim2 (python:int, optional) \u2013 second dimension with respect to which to take diagonal. Default: 1.     Note To take a batch diagonal, pass in dim1=-2, dim2=-1.  Examples: &gt;&gt;&gt; a = torch.randn(3, 3) &gt;&gt;&gt; a tensor([[-1.0854,  1.1431, -0.1752],         [ 0.8536, -0.0905,  0.0360],         [ 0.6927, -0.3735, -0.4945]])   &gt;&gt;&gt; torch.diagonal(a, 0) tensor([-1.0854, -0.0905, -0.4945])   &gt;&gt;&gt; torch.diagonal(a, 1) tensor([ 1.1431,  0.0360])   &gt;&gt;&gt; x = torch.randn(2, 5, 4, 2) &gt;&gt;&gt; torch.diagonal(x, offset=-1, dim1=1, dim2=2) tensor([[[-1.2631,  0.3755, -1.5977, -1.8172],          [-1.1065,  1.0401, -0.2235, -0.7938]],          [[-1.7325, -0.3081,  0.6166,  0.2335],          [ 1.0500,  0.7336, -0.3836, -1.1015]]])   ", "parameters": ["input (Tensor) : the input tensor. Must be at least 2-dimensional.", "offset (python:int, optional) : which diagonal to consider. Default: 0(main diagonal).", "dim1 (python:int, optional) : first dimension with respect to which totake diagonal. Default: 0.", "dim2 (python:int, optional) : second dimension with respect to which totake diagonal. Default: 1."], "returns": null, "example": " a = torch.randn(3, 3)\n a\ntensor([[-1.0854,  1.1431, -0.1752],\n        [ 0.8536, -0.0905,  0.0360],\n        [ 0.6927, -0.3735, -0.4945]])\n\n\n torch.diagonal(a, 0)\ntensor([-1.0854, -0.0905, -0.4945])\n\n\n torch.diagonal(a, 1)\ntensor([ 1.1431,  0.0360])\n\n\n x = torch.randn(2, 5, 4, 2)\n torch.diagonal(x, offset=-1, dim1=1, dim2=2)\ntensor([[[-1.2631,  0.3755, -1.5977, -1.8172],\n         [-1.1065,  1.0401, -0.2235, -0.7938]],\n\n        [[-1.7325, -0.3081,  0.6166,  0.2335],\n         [ 1.0500,  0.7336, -0.3836, -1.1015]]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.einsum", "item_type": "function", "code": "torch.einsum(equation,*operands)\u2192Tensor", "description": "This function provides a way of computing multilinear expressions (i.e. sums of products) using the Einstein summation convention.  Parameters  equation (string) \u2013 The equation is given in terms of lower case letters (indices) to be associated with each dimension of the operands and result. The left hand side lists the operands dimensions, separated by commas. There should be one index letter per tensor dimension. The right hand side follows after -&gt; and gives the indices for the output. If the -&gt; and right hand side are omitted, it implicitly defined as the alphabetically sorted list of all indices appearing exactly once in the left hand side. The indices not apprearing in the output are summed over after multiplying the operands entries. If an index appears several times for the same operand, a diagonal is taken. Ellipses \u2026 represent a fixed number of dimensions. If the right hand side is inferred, the ellipsis dimensions are at the beginning of the output. operands (Tensor) \u2013 The operands to compute the Einstein sum of.    Examples: &gt;&gt;&gt; x = torch.randn(5) &gt;&gt;&gt; y = torch.randn(4) &gt;&gt;&gt; torch.einsum('i,j-&gt;ij', x, y)  # outer product tensor([[-0.0570, -0.0286, -0.0231,  0.0197],         [ 1.2616,  0.6335,  0.5113, -0.4351],         [ 1.4452,  0.7257,  0.5857, -0.4984],         [-0.4647, -0.2333, -0.1883,  0.1603],         [-1.1130, -0.5588, -0.4510,  0.3838]])   &gt;&gt;&gt; A = torch.randn(3,5,4) &gt;&gt;&gt; l = torch.randn(2,5) &gt;&gt;&gt; r = torch.randn(2,4) &gt;&gt;&gt; torch.einsum('bn,anm,bm-&gt;ba', l, A, r) # compare torch.nn.functional.bilinear tensor([[-0.3430, -5.2405,  0.4494],         [ 0.3311,  5.5201, -3.0356]])   &gt;&gt;&gt; As = torch.randn(3,2,5) &gt;&gt;&gt; Bs = torch.randn(3,5,4) &gt;&gt;&gt; torch.einsum('bij,bjk-&gt;bik', As, Bs) # batch matrix multiplication tensor([[[-1.0564, -1.5904,  3.2023,  3.1271],          [-1.6706, -0.8097, -0.8025, -2.1183]],          [[ 4.2239,  0.3107, -0.5756, -0.2354],          [-1.4558, -0.3460,  1.5087, -0.8530]],          [[ 2.8153,  1.8787, -4.3839, -1.2112],          [ 0.3728, -2.1131,  0.0921,  0.8305]]])  &gt;&gt;&gt; A = torch.randn(3, 3) &gt;&gt;&gt; torch.einsum('ii-&gt;i', A) # diagonal tensor([-0.7825,  0.8291, -0.1936])  &gt;&gt;&gt; A = torch.randn(4, 3, 3) &gt;&gt;&gt; torch.einsum('...ii-&gt;...i', A) # batch diagonal tensor([[-1.0864,  0.7292,  0.0569],         [-0.9725, -1.0270,  0.6493],         [ 0.5832, -1.1716, -1.5084],         [ 0.4041, -1.1690,  0.8570]])  &gt;&gt;&gt; A = torch.randn(2, 3, 4, 5) &gt;&gt;&gt; torch.einsum('...ij-&gt;...ji', A).shape # batch permute torch.Size([2, 3, 5, 4])   ", "parameters": ["equation (string) : The equation is given in terms of lower case letters (indices) to be associatedwith each dimension of the operands and result. The left hand side lists the operandsdimensions, separated by commas. There should be one index letter per tensor dimension.The right hand side follows after -&gt; and gives the indices for the output.If the -&gt; and right hand side are omitted, it implicitly defined as the alphabeticallysorted list of all indices appearing exactly once in the left hand side.The indices not apprearing in the output are summed over after multiplying the operandsentries.If an index appears several times for the same operand, a diagonal is taken.Ellipses \u2026 represent a fixed number of dimensions. If the right hand side is inferred,the ellipsis dimensions are at the beginning of the output.", "operands (Tensor) : The operands to compute the Einstein sum of."], "returns": null, "example": " x = torch.randn(5)\n y = torch.randn(4)\n torch.einsum('i,j-ij', x, y)  # outer product\ntensor([[-0.0570, -0.0286, -0.0231,  0.0197],\n        [ 1.2616,  0.6335,  0.5113, -0.4351],\n        [ 1.4452,  0.7257,  0.5857, -0.4984],\n        [-0.4647, -0.2333, -0.1883,  0.1603],\n        [-1.1130, -0.5588, -0.4510,  0.3838]])\n\n\n A = torch.randn(3,5,4)\n l = torch.randn(2,5)\n r = torch.randn(2,4)\n torch.einsum('bn,anm,bm-ba', l, A, r) # compare torch.nn.functional.bilinear\ntensor([[-0.3430, -5.2405,  0.4494],\n        [ 0.3311,  5.5201, -3.0356]])\n\n\n As = torch.randn(3,2,5)\n Bs = torch.randn(3,5,4)\n torch.einsum('bij,bjk-bik', As, Bs) # batch matrix multiplication\ntensor([[[-1.0564, -1.5904,  3.2023,  3.1271],\n         [-1.6706, -0.8097, -0.8025, -2.1183]],\n\n        [[ 4.2239,  0.3107, -0.5756, -0.2354],\n         [-1.4558, -0.3460,  1.5087, -0.8530]],\n\n        [[ 2.8153,  1.8787, -4.3839, -1.2112],\n         [ 0.3728, -2.1131,  0.0921,  0.8305]]])\n\n A = torch.randn(3, 3)\n torch.einsum('ii-i', A) # diagonal\ntensor([-0.7825,  0.8291, -0.1936])\n\n A = torch.randn(4, 3, 3)\n torch.einsum('...ii-...i', A) # batch diagonal\ntensor([[-1.0864,  0.7292,  0.0569],\n        [-0.9725, -1.0270,  0.6493],\n        [ 0.5832, -1.1716, -1.5084],\n        [ 0.4041, -1.1690,  0.8570]])\n\n A = torch.randn(2, 3, 4, 5)\n torch.einsum('...ij-...ji', A).shape # batch permute\ntorch.Size([2, 3, 5, 4])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.BatchNorm3d", "item_type": "class", "code": "classtorch.nn.BatchNorm3d(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)", "description": "Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .  y=x\u2212E[x]Var[x]+\u03f5\u2217\u03b3+\u03b2y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\betay=Var[x]+\u03f5\u200bx\u2212E[x]\u200b\u2217\u03b3+\u03b2  The mean and standard-deviation are calculated per-dimension over the mini-batches and \u03b3\\gamma\u03b3   and \u03b2\\beta\u03b2   are learnable parameter vectors of size C (where C is the input size). By default, the elements of \u03b3\\gamma\u03b3   are set to 1 and the elements of \u03b2\\beta\u03b2   are set to 0. Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1. If track_running_stats is set to False, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well.  Note This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is x^new=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_tx^new\u200b=(1\u2212momentum)\u00d7x^+momentum\u00d7xt\u200b  , where x^\\hat{x}x^   is the estimated statistic and xtx_txt\u200b   is the new observed value.  Because the Batch Normalization is done over the C dimension, computing statistics on (N, D, H, W) slices, it\u2019s common terminology to call this Volumetric Batch Normalization or Spatio-temporal Batch Normalization.  Parameters  num_features \u2013 CCC   from an expected input of size (N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W)   eps \u2013 a value added to the denominator for numerical stability. Default: 1e-5 momentum \u2013 the value used for the running_mean and running_var computation. Can be set to None for cumulative moving average (i.e. simple average). Default: 0.1 affine \u2013 a boolean value that when set to True, this module has learnable affine parameters. Default: True track_running_stats \u2013 a boolean value that when set to True, this module tracks the running mean and variance, and when set to False, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: True     Shape: Input: (N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W)   Output: (N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W)   (same shape as input)    Examples: &gt;&gt;&gt; # With Learnable Parameters &gt;&gt;&gt; m = nn.BatchNorm3d(100) &gt;&gt;&gt; # Without Learnable Parameters &gt;&gt;&gt; m = nn.BatchNorm3d(100, affine=False) &gt;&gt;&gt; input = torch.randn(20, 100, 35, 45, 10) &gt;&gt;&gt; output = m(input)   ", "parameters": ["num_features : CCC from an expected input of size(N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W)", "eps : a value added to the denominator for numerical stability.Default: 1e-5", "momentum : the value used for the running_mean and running_varcomputation. Can be set to None for cumulative moving average(i.e. simple average). Default: 0.1", "affine : a boolean value that when set to True, this module haslearnable affine parameters. Default: True", "track_running_stats : a boolean value that when set to True, thismodule tracks the running mean and variance, and when set to False,this module does not track such statistics and always uses batchstatistics in both training and eval modes. Default: True"], "returns": null, "example": " # With Learnable Parameters\n m = nn.BatchNorm3d(100)\n # Without Learnable Parameters\n m = nn.BatchNorm3d(100, affine=False)\n input = torch.randn(20, 100, 35, 45, 10)\n output = m(input)\n\n", "shape": " Input: (N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W)   Output: (N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W)   (same shape as input)  "},
{"library": "torch", "item_id": "torch.nn.GroupNorm", "item_type": "class", "code": "classtorch.nn.GroupNorm(num_groups,num_channels,eps=1e-05,affine=True)", "description": "Applies Group Normalization over a mini-batch of inputs as described in the paper Group Normalization .  y=x\u2212E[x]Var[x]+\u03f5\u2217\u03b3+\u03b2y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta  y=Var[x]+\u03f5\u200bx\u2212E[x]\u200b\u2217\u03b3+\u03b2  The input channels are separated into num_groups groups, each containing num_channels / num_groups channels. The mean and standard-deviation are calculated separately over the each group. \u03b3\\gamma\u03b3   and \u03b2\\beta\u03b2   are learnable per-channel affine transform parameter vectors of size num_channels if affine is True. This layer uses statistics computed from input data in both training and evaluation modes.  Parameters  num_groups (python:int) \u2013 number of groups to separate the channels into num_channels (python:int) \u2013 number of channels expected in input eps \u2013 a value added to the denominator for numerical stability. Default: 1e-5 affine \u2013 a boolean value that when set to True, this module has learnable per-channel affine parameters initialized to ones (for weights) and zeros (for biases). Default: True.     Shape: Input: (N,C,\u2217)(N, C, *)(N,C,\u2217)   where C=num_channelsC=\\text{num\\_channels}C=num_channels   Output: (N,C,\u2217)(N, C, *)(N,C,\u2217)   (same shape as input)    Examples: &gt;&gt;&gt; input = torch.randn(20, 6, 10, 10) &gt;&gt;&gt; # Separate 6 channels into 3 groups &gt;&gt;&gt; m = nn.GroupNorm(3, 6) &gt;&gt;&gt; # Separate 6 channels into 6 groups (equivalent with InstanceNorm) &gt;&gt;&gt; m = nn.GroupNorm(6, 6) &gt;&gt;&gt; # Put all 6 channels into a single group (equivalent with LayerNorm) &gt;&gt;&gt; m = nn.GroupNorm(1, 6) &gt;&gt;&gt; # Activating the module &gt;&gt;&gt; output = m(input)   ", "parameters": ["num_groups (python:int) : number of groups to separate the channels into", "num_channels (python:int) : number of channels expected in input", "eps : a value added to the denominator for numerical stability. Default: 1e-5", "affine : a boolean value that when set to True, this modulehas learnable per-channel affine parameters initialized to ones (for weights)and zeros (for biases). Default: True."], "returns": null, "example": " input = torch.randn(20, 6, 10, 10)\n # Separate 6 channels into 3 groups\n m = nn.GroupNorm(3, 6)\n # Separate 6 channels into 6 groups (equivalent with InstanceNorm)\n m = nn.GroupNorm(6, 6)\n # Put all 6 channels into a single group (equivalent with LayerNorm)\n m = nn.GroupNorm(1, 6)\n # Activating the module\n output = m(input)\n\n", "shape": " Input: (N,C,\u2217)(N, C, *)(N,C,\u2217)   where C=num_channelsC=\\text{num\\_channels}C=num_channels   Output: (N,C,\u2217)(N, C, *)(N,C,\u2217)   (same shape as input)  "},
{"library": "torch", "item_id": "torch.nn.SyncBatchNorm", "item_type": "class", "code": "classtorch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)", "description": "Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift .  y=x\u2212E[x]Var[x]+\u03f5\u2217\u03b3+\u03b2y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\betay=Var[x]+\u03f5\u200bx\u2212E[x]\u200b\u2217\u03b3+\u03b2  The mean and standard-deviation are calculated per-dimension over all mini-batches of the same process groups. \u03b3\\gamma\u03b3   and \u03b2\\beta\u03b2   are learnable parameter vectors of size C (where C is the input size). By default, the elements of \u03b3\\gamma\u03b3   are sampled from U(0,1)\\mathcal{U}(0, 1)U(0,1)   and the elements of \u03b2\\beta\u03b2   are set to 0. Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1. If track_running_stats is set to False, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well.  Note This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is x^new=(1\u2212momentum)\u00d7x^+momemtum\u00d7xt\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momemtum} \\times x_tx^new\u200b=(1\u2212momentum)\u00d7x^+momemtum\u00d7xt\u200b  , where x^\\hat{x}x^   is the estimated statistic and xtx_txt\u200b   is the new observed value.  Because the Batch Normalization is done over the C dimension, computing statistics on (N, +) slices, it\u2019s common terminology to call this Volumetric Batch Normalization or Spatio-temporal Batch Normalization. Currently SyncBatchNorm only supports DistributedDataParallel with single GPU per process. Use torch.nn.SyncBatchNorm.convert_sync_batchnorm() to convert BatchNorm layer to SyncBatchNorm before wrapping Network with DDP.  Parameters  num_features \u2013 CCC   from an expected input of size (N,C,+)(N, C, +)(N,C,+)   eps \u2013 a value added to the denominator for numerical stability. Default: 1e-5 momentum \u2013 the value used for the running_mean and running_var computation. Can be set to None for cumulative moving average (i.e. simple average). Default: 0.1 affine \u2013 a boolean value that when set to True, this module has learnable affine parameters. Default: True track_running_stats \u2013 a boolean value that when set to True, this module tracks the running mean and variance, and when set to False, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: True process_group \u2013 synchronization of stats happen within each process group individually. Default behavior is synchronization across the whole world     Shape: Input: (N,C,+)(N, C, +)(N,C,+)   Output: (N,C,+)(N, C, +)(N,C,+)   (same shape as input)    Examples: &gt;&gt;&gt; # With Learnable Parameters &gt;&gt;&gt; m = nn.SyncBatchNorm(100) &gt;&gt;&gt; # creating process group (optional) &gt;&gt;&gt; # process_ids is a list of int identifying rank ids. &gt;&gt;&gt; process_group = torch.distributed.new_group(process_ids) &gt;&gt;&gt; # Without Learnable Parameters &gt;&gt;&gt; m = nn.BatchNorm3d(100, affine=False, process_group=process_group) &gt;&gt;&gt; input = torch.randn(20, 100, 35, 45, 10) &gt;&gt;&gt; output = m(input)  &gt;&gt;&gt; # network is nn.BatchNorm layer &gt;&gt;&gt; sync_bn_network = nn.SyncBatchNorm.convert_sync_batchnorm(network, process_group) &gt;&gt;&gt; # only single gpu per process is currently supported &gt;&gt;&gt; ddp_sync_bn_network = torch.nn.parallel.DistributedDataParallel( &gt;&gt;&gt;                         sync_bn_network, &gt;&gt;&gt;                         device_ids=[args.local_rank], &gt;&gt;&gt;                         output_device=args.local_rank)     classmethod convert_sync_batchnorm(module, process_group=None)  Helper function to convert torch.nn.BatchNormND layer in the model to torch.nn.SyncBatchNorm layer.  Parameters  module (nn.Module) \u2013 containing module process_group (optional) \u2013 process group to scope synchronization,    default is the whole world  Returns The original module with the converted torch.nn.SyncBatchNorm layer   Example: &gt;&gt;&gt; # Network with nn.BatchNorm layer &gt;&gt;&gt; module = torch.nn.Sequential( &gt;&gt;&gt;            torch.nn.Linear(20, 100), &gt;&gt;&gt;            torch.nn.BatchNorm1d(100) &gt;&gt;&gt;          ).cuda() &gt;&gt;&gt; # creating process group (optional) &gt;&gt;&gt; # process_ids is a list of int identifying rank ids. &gt;&gt;&gt; process_group = torch.distributed.new_group(process_ids) &gt;&gt;&gt; sync_bn_module = convert_sync_batchnorm(module, process_group)     ", "parameters": ["num_features : CCC from an expected input of size(N,C,+)(N, C, +)(N,C,+)", "eps : a value added to the denominator for numerical stability.Default: 1e-5", "momentum : the value used for the running_mean and running_varcomputation. Can be set to None for cumulative moving average(i.e. simple average). Default: 0.1", "affine : a boolean value that when set to True, this module haslearnable affine parameters. Default: True", "track_running_stats : a boolean value that when set to True, thismodule tracks the running mean and variance, and when set to False,this module does not track such statistics and always uses batchstatistics in both training and eval modes. Default: True", "process_group : synchronization of stats happen within each process groupindividually. Default behavior is synchronization across the wholeworld", "module (nn.Module) : containing module", "process_group (optional) : process group to scope synchronization,"], "returns": "The original module with the converted torch.nn.SyncBatchNorm layer", "example": " # With Learnable Parameters\n m = nn.SyncBatchNorm(100)\n # creating process group (optional)\n # process_ids is a list of int identifying rank ids.\n process_group = torch.distributed.new_group(process_ids)\n # Without Learnable Parameters\n m = nn.BatchNorm3d(100, affine=False, process_group=process_group)\n input = torch.randn(20, 100, 35, 45, 10)\n output = m(input)\n\n # network is nn.BatchNorm layer\n sync_bn_network = nn.SyncBatchNorm.convert_sync_batchnorm(network, process_group)\n # only single gpu per process is currently supported\n ddp_sync_bn_network = torch.nn.parallel.DistributedDataParallel(\n                         sync_bn_network,\n                         device_ids=[args.local_rank],\n                         output_device=args.local_rank)\n\n", "shape": " Input: (N,C,+)(N, C, +)(N,C,+)   Output: (N,C,+)(N, C, +)(N,C,+)   (same shape as input)  "},
{"library": "torch", "item_id": "torch.flatten", "item_type": "function", "code": "torch.flatten(input,start_dim=0,end_dim=-1)\u2192Tensor", "description": "Flattens a contiguous range of dims in a tensor.  Parameters  input (Tensor) \u2013 the input tensor. start_dim (python:int) \u2013 the first dim to flatten end_dim (python:int) \u2013 the last dim to flatten    Example: &gt;&gt;&gt; t = torch.tensor([[[1, 2],                        [3, 4]],                       [[5, 6],                        [7, 8]]]) &gt;&gt;&gt; torch.flatten(t) tensor([1, 2, 3, 4, 5, 6, 7, 8]) &gt;&gt;&gt; torch.flatten(t, start_dim=1) tensor([[1, 2, 3, 4],         [5, 6, 7, 8]])   ", "parameters": ["input (Tensor) : the input tensor.", "start_dim (python:int) : the first dim to flatten", "end_dim (python:int) : the last dim to flatten"], "returns": null, "example": " t = torch.tensor([[[1, 2],\n                       [3, 4]],\n                      [[5, 6],\n                       [7, 8]]])\n torch.flatten(t)\ntensor([1, 2, 3, 4, 5, 6, 7, 8])\n torch.flatten(t, start_dim=1)\ntensor([[1, 2, 3, 4],\n        [5, 6, 7, 8]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.flip", "item_type": "function", "code": "torch.flip(input,dims)\u2192Tensor", "description": "Reverse the order of a n-D tensor along given axis in dims.  Parameters  input (Tensor) \u2013 the input tensor. dims (a list or tuple) \u2013 axis to flip on    Example: &gt;&gt;&gt; x = torch.arange(8).view(2, 2, 2) &gt;&gt;&gt; x tensor([[[ 0,  1],          [ 2,  3]],          [[ 4,  5],          [ 6,  7]]]) &gt;&gt;&gt; torch.flip(x, [0, 1]) tensor([[[ 6,  7],          [ 4,  5]],          [[ 2,  3],          [ 0,  1]]])   ", "parameters": ["input (Tensor) : the input tensor.", "dims (a list or tuple) : axis to flip on"], "returns": null, "example": " x = torch.arange(8).view(2, 2, 2)\n x\ntensor([[[ 0,  1],\n         [ 2,  3]],\n\n        [[ 4,  5],\n         [ 6,  7]]])\n torch.flip(x, [0, 1])\ntensor([[[ 6,  7],\n         [ 4,  5]],\n\n        [[ 2,  3],\n         [ 0,  1]]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.rot90", "item_type": "function", "code": "torch.rot90(input,k,dims)\u2192Tensor", "description": "Rotate a n-D tensor by 90 degrees in the plane specified by dims axis. Rotation direction is from the first towards the second axis if k &gt; 0, and from the second towards the first for k &lt; 0.  Parameters  input (Tensor) \u2013 the input tensor. k (python:int) \u2013 number of times to rotate dims (a list or tuple) \u2013 axis to rotate    Example: &gt;&gt;&gt; x = torch.arange(4).view(2, 2) &gt;&gt;&gt; x tensor([[0, 1],         [2, 3]]) &gt;&gt;&gt; torch.rot90(x, 1, [0, 1]) tensor([[1, 3],         [0, 2]])  &gt;&gt;&gt; x = torch.arange(8).view(2, 2, 2) &gt;&gt;&gt; x tensor([[[0, 1],          [2, 3]],          [[4, 5],          [6, 7]]]) &gt;&gt;&gt; torch.rot90(x, 1, [1, 2]) tensor([[[1, 3],          [0, 2]],          [[5, 7],          [4, 6]]])   ", "parameters": ["input (Tensor) : the input tensor.", "k (python:int) : number of times to rotate", "dims (a list or tuple) : axis to rotate"], "returns": null, "example": " x = torch.arange(4).view(2, 2)\n x\ntensor([[0, 1],\n        [2, 3]])\n torch.rot90(x, 1, [0, 1])\ntensor([[1, 3],\n        [0, 2]])\n\n x = torch.arange(8).view(2, 2, 2)\n x\ntensor([[[0, 1],\n         [2, 3]],\n\n        [[4, 5],\n         [6, 7]]])\n torch.rot90(x, 1, [1, 2])\ntensor([[[1, 3],\n         [0, 2]],\n\n        [[5, 7],\n         [4, 6]]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.histc", "item_type": "function", "code": "torch.histc(input,bins=100,min=0,max=0,out=None)\u2192Tensor", "description": "Computes the histogram of a tensor. The elements are sorted into equal width bins between min and max. If min and max are both zero, the minimum and maximum values of the data are used.  Parameters  input (Tensor) \u2013 the input tensor. bins (python:int) \u2013 number of histogram bins min (python:int) \u2013 lower end of the range (inclusive) max (python:int) \u2013 upper end of the range (inclusive) out (Tensor, optional) \u2013 the output tensor.   Returns Histogram represented as a tensor  Return type Tensor   Example: &gt;&gt;&gt; torch.histc(torch.tensor([1., 2, 1]), bins=4, min=0, max=3) tensor([ 0.,  2.,  1.,  0.])   ", "parameters": ["input (Tensor) : the input tensor.", "bins (python:int) : number of histogram bins", "min (python:int) : lower end of the range (inclusive)", "max (python:int) : upper end of the range (inclusive)", "out (Tensor, optional) : the output tensor."], "returns": "Histogram represented as a tensor", "example": " torch.histc(torch.tensor([1., 2, 1]), bins=4, min=0, max=3)\ntensor([ 0.,  2.,  1.,  0.])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.meshgrid", "item_type": "function", "code": "torch.meshgrid(*tensors,**kwargs)", "description": "Take NNN   tensors, each of which can be either scalar or 1-dimensional vector, and create NNN   N-dimensional grids, where the iii   th grid is defined by expanding the iii   th input over dimensions defined by other inputs.   Args:tensors (list of Tensor): list of scalars or 1 dimensional tensors. Scalars will be treated as tensors of size (1,)(1,)(1,)   automatically  Returns:seq (sequence of Tensors): If the input has kkk   tensors of size (N1,),(N2,),\u2026,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,)(N1\u200b,),(N2\u200b,),\u2026,(Nk\u200b,)  , then the output would also have kkk   tensors, where all tensors are of size (N1,N2,\u2026,Nk)(N_1, N_2, \\ldots , N_k)(N1\u200b,N2\u200b,\u2026,Nk\u200b)  .   Example: &gt;&gt;&gt; x = torch.tensor([1, 2, 3]) &gt;&gt;&gt; y = torch.tensor([4, 5, 6]) &gt;&gt;&gt; grid_x, grid_y = torch.meshgrid(x, y) &gt;&gt;&gt; grid_x tensor([[1, 1, 1],         [2, 2, 2],         [3, 3, 3]]) &gt;&gt;&gt; grid_y tensor([[4, 5, 6],         [4, 5, 6],         [4, 5, 6]])    ", "parameters": [], "returns": "seq (sequence of Tensors): If the input has kkk tensors of size(N1,),(N2,),\u2026,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,)(N1\u200b,),(N2\u200b,),\u2026,(Nk\u200b,), then the output would also have kkk tensors,where all tensors are of size (N1,N2,\u2026,Nk)(N_1, N_2, \\ldots , N_k)(N1\u200b,N2\u200b,\u2026,Nk\u200b).", "example": " x = torch.tensor([1, 2, 3])\n y = torch.tensor([4, 5, 6])\n grid_x, grid_y = torch.meshgrid(x, y)\n grid_x\ntensor([[1, 1, 1],\n        [2, 2, 2],\n        [3, 3, 3]])\n grid_y\ntensor([[4, 5, 6],\n        [4, 5, 6],\n        [4, 5, 6]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.renorm", "item_type": "function", "code": "torch.renorm(input,p,dim,maxnorm,out=None)\u2192Tensor", "description": "Returns a tensor where each sub-tensor of input along dimension dim is normalized such that the p-norm of the sub-tensor is lower than the value maxnorm  Note If the norm of a row is lower than maxnorm, the row is unchanged   Parameters  input (Tensor) \u2013 the input tensor. p (python:float) \u2013 the power for the norm computation dim (python:int) \u2013 the dimension to slice over to get the sub-tensors maxnorm (python:float) \u2013 the maximum norm to keep each sub-tensor under out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; x = torch.ones(3, 3) &gt;&gt;&gt; x[1].fill_(2) tensor([ 2.,  2.,  2.]) &gt;&gt;&gt; x[2].fill_(3) tensor([ 3.,  3.,  3.]) &gt;&gt;&gt; x tensor([[ 1.,  1.,  1.],         [ 2.,  2.,  2.],         [ 3.,  3.,  3.]]) &gt;&gt;&gt; torch.renorm(x, 1, 0, 5) tensor([[ 1.0000,  1.0000,  1.0000],         [ 1.6667,  1.6667,  1.6667],         [ 1.6667,  1.6667,  1.6667]])   ", "parameters": ["input (Tensor) : the input tensor.", "p (python:float) : the power for the norm computation", "dim (python:int) : the dimension to slice over to get the sub-tensors", "maxnorm (python:float) : the maximum norm to keep each sub-tensor under", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " x = torch.ones(3, 3)\n x[1].fill_(2)\ntensor([ 2.,  2.,  2.])\n x[2].fill_(3)\ntensor([ 3.,  3.,  3.])\n x\ntensor([[ 1.,  1.,  1.],\n        [ 2.,  2.,  2.],\n        [ 3.,  3.,  3.]])\n torch.renorm(x, 1, 0, 5)\ntensor([[ 1.0000,  1.0000,  1.0000],\n        [ 1.6667,  1.6667,  1.6667],\n        [ 1.6667,  1.6667,  1.6667]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.InstanceNorm1d", "item_type": "class", "code": "classtorch.nn.InstanceNorm1d(num_features,eps=1e-05,momentum=0.1,affine=False,track_running_stats=False)", "description": "Applies Instance Normalization over a 3D input (a mini-batch of 1D inputs with optional additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization .  y=x\u2212E[x]Var[x]+\u03f5\u2217\u03b3+\u03b2y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\betay=Var[x]+\u03f5\u200bx\u2212E[x]\u200b\u2217\u03b3+\u03b2  The mean and standard-deviation are calculated per-dimension separately for each object in a mini-batch. \u03b3\\gamma\u03b3   and \u03b2\\beta\u03b2   are learnable parameter vectors of size C (where C is the input size) if affine is True. By default, this layer uses instance statistics computed from input data in both training and evaluation modes. If track_running_stats is set to True, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1.  Note This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is x^new=(1\u2212momentum)\u00d7x^+momemtum\u00d7xt\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momemtum} \\times x_tx^new\u200b=(1\u2212momentum)\u00d7x^+momemtum\u00d7xt\u200b  , where x^\\hat{x}x^   is the estimated statistic and xtx_txt\u200b   is the new observed value.   Note InstanceNorm1d and LayerNorm are very similar, but have some subtle differences. InstanceNorm1d is applied on each channel of channeled data like multidimensional time series, but LayerNorm is usually applied on entire sample and often in NLP tasks. Additionaly, LayerNorm applies elementwise affine transform, while InstanceNorm1d usually don\u2019t apply affine transform.   Parameters  num_features \u2013 CCC   from an expected input of size (N,C,L)(N, C, L)(N,C,L)   or LLL   from input of size (N,L)(N, L)(N,L)   eps \u2013 a value added to the denominator for numerical stability. Default: 1e-5 momentum \u2013 the value used for the running_mean and running_var computation. Default: 0.1 affine \u2013 a boolean value that when set to True, this module has learnable affine parameters, initialized the same way as done for batch normalization. Default: False. track_running_stats \u2013 a boolean value that when set to True, this module tracks the running mean and variance, and when set to False, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: False     Shape: Input: (N,C,L)(N, C, L)(N,C,L)   Output: (N,C,L)(N, C, L)(N,C,L)   (same shape as input)    Examples: &gt;&gt;&gt; # Without Learnable Parameters &gt;&gt;&gt; m = nn.InstanceNorm1d(100) &gt;&gt;&gt; # With Learnable Parameters &gt;&gt;&gt; m = nn.InstanceNorm1d(100, affine=True) &gt;&gt;&gt; input = torch.randn(20, 100, 40) &gt;&gt;&gt; output = m(input)   ", "parameters": ["num_features : CCC from an expected input of size(N,C,L)(N, C, L)(N,C,L) or LLL from input of size (N,L)(N, L)(N,L)", "eps : a value added to the denominator for numerical stability. Default: 1e-5", "momentum : the value used for the running_mean and running_var computation. Default: 0.1", "affine : a boolean value that when set to True, this module haslearnable affine parameters, initialized the same way as done for batch normalization.Default: False.", "track_running_stats : a boolean value that when set to True, thismodule tracks the running mean and variance, and when set to False,this module does not track such statistics and always uses batchstatistics in both training and eval modes. Default: False"], "returns": null, "example": " # Without Learnable Parameters\n m = nn.InstanceNorm1d(100)\n # With Learnable Parameters\n m = nn.InstanceNorm1d(100, affine=True)\n input = torch.randn(20, 100, 40)\n output = m(input)\n\n", "shape": " Input: (N,C,L)(N, C, L)(N,C,L)   Output: (N,C,L)(N, C, L)(N,C,L)   (same shape as input)  "},
{"library": "torch", "item_id": "torch.nn.InstanceNorm2d", "item_type": "class", "code": "classtorch.nn.InstanceNorm2d(num_features,eps=1e-05,momentum=0.1,affine=False,track_running_stats=False)", "description": "Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization .  y=x\u2212E[x]Var[x]+\u03f5\u2217\u03b3+\u03b2y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\betay=Var[x]+\u03f5\u200bx\u2212E[x]\u200b\u2217\u03b3+\u03b2  The mean and standard-deviation are calculated per-dimension separately for each object in a mini-batch. \u03b3\\gamma\u03b3   and \u03b2\\beta\u03b2   are learnable parameter vectors of size C (where C is the input size) if affine is True. By default, this layer uses instance statistics computed from input data in both training and evaluation modes. If track_running_stats is set to True, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1.  Note This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is x^new=(1\u2212momentum)\u00d7x^+momemtum\u00d7xt\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momemtum} \\times x_tx^new\u200b=(1\u2212momentum)\u00d7x^+momemtum\u00d7xt\u200b  , where x^\\hat{x}x^   is the estimated statistic and xtx_txt\u200b   is the new observed value.   Note InstanceNorm2d and LayerNorm are very similar, but have some subtle differences. InstanceNorm2d is applied on each channel of channeled data like RGB images, but LayerNorm is usually applied on entire sample and often in NLP tasks. Additionaly, LayerNorm applies elementwise affine transform, while InstanceNorm2d usually don\u2019t apply affine transform.   Parameters  num_features \u2013 CCC   from an expected input of size (N,C,H,W)(N, C, H, W)(N,C,H,W)   eps \u2013 a value added to the denominator for numerical stability. Default: 1e-5 momentum \u2013 the value used for the running_mean and running_var computation. Default: 0.1 affine \u2013 a boolean value that when set to True, this module has learnable affine parameters, initialized the same way as done for batch normalization. Default: False. track_running_stats \u2013 a boolean value that when set to True, this module tracks the running mean and variance, and when set to False, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: False     Shape: Input: (N,C,H,W)(N, C, H, W)(N,C,H,W)   Output: (N,C,H,W)(N, C, H, W)(N,C,H,W)   (same shape as input)    Examples: &gt;&gt;&gt; # Without Learnable Parameters &gt;&gt;&gt; m = nn.InstanceNorm2d(100) &gt;&gt;&gt; # With Learnable Parameters &gt;&gt;&gt; m = nn.InstanceNorm2d(100, affine=True) &gt;&gt;&gt; input = torch.randn(20, 100, 35, 45) &gt;&gt;&gt; output = m(input)   ", "parameters": ["num_features : CCC from an expected input of size(N,C,H,W)(N, C, H, W)(N,C,H,W)", "eps : a value added to the denominator for numerical stability. Default: 1e-5", "momentum : the value used for the running_mean and running_var computation. Default: 0.1", "affine : a boolean value that when set to True, this module haslearnable affine parameters, initialized the same way as done for batch normalization.Default: False.", "track_running_stats : a boolean value that when set to True, thismodule tracks the running mean and variance, and when set to False,this module does not track such statistics and always uses batchstatistics in both training and eval modes. Default: False"], "returns": null, "example": " # Without Learnable Parameters\n m = nn.InstanceNorm2d(100)\n # With Learnable Parameters\n m = nn.InstanceNorm2d(100, affine=True)\n input = torch.randn(20, 100, 35, 45)\n output = m(input)\n\n", "shape": " Input: (N,C,H,W)(N, C, H, W)(N,C,H,W)   Output: (N,C,H,W)(N, C, H, W)(N,C,H,W)   (same shape as input)  "},
{"library": "torch", "item_id": "torch.repeat_interleave", "item_type": "function", "code": "torch.repeat_interleave()", "description": "  torch.repeat_interleave(input, repeats, dim=None) \u2192 Tensor   Repeat elements of a tensor.  Warning This is different from torch.repeat() but similar to numpy.repeat.   Parameters  input (Tensor) \u2013 the input tensor. repeats (Tensor or python:int) \u2013 The number of repetitions for each element. repeats is broadcasted to fit the shape of the given axis. dim (python:int, optional) \u2013 The dimension along which to repeat values. By default, use the flattened input array, and return a flat output array.   Returns  Repeated tensor which has the same shape as input, except along thegiven axis.     Return type Tensor   Example: &gt;&gt;&gt; x = torch.tensor([1, 2, 3]) &gt;&gt;&gt; x.repeat_interleave(2) tensor([1, 1, 2, 2, 3, 3]) &gt;&gt;&gt; y = torch.tensor([[1, 2], [3, 4]]) &gt;&gt;&gt; torch.repeat_interleave(y, 2) tensor([1, 1, 2, 2, 3, 3, 4, 4]) &gt;&gt;&gt; torch.repeat_interleave(y, 3, dim=1) tensor([[1, 1, 1, 2, 2, 2],         [3, 3, 3, 4, 4, 4]]) &gt;&gt;&gt; torch.repeat_interleave(y, torch.tensor([1, 2]), dim=0) tensor([[1, 2],         [3, 4],         [3, 4]])     torch.repeat_interleave(repeats) \u2192 Tensor   If the repeats is tensor([n1, n2, n3, \u2026]), then the output will be tensor([0, 0, \u2026, 1, 1, \u2026, 2, 2, \u2026, \u2026]) where 0 appears n1 times, 1 appears n2 times, 2 appears n3 times, etc. ", "parameters": ["input (Tensor) : the input tensor.", "repeats (Tensor or python:int) : The number of repetitions for each element.repeats is broadcasted to fit the shape of the given axis.", "dim (python:int, optional) : The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray."], "returns": "Repeated tensor which has the same shape as input, except along thegiven axis.", "example": " x = torch.tensor([1, 2, 3])\n x.repeat_interleave(2)\ntensor([1, 1, 2, 2, 3, 3])\n y = torch.tensor([[1, 2], [3, 4]])\n torch.repeat_interleave(y, 2)\ntensor([1, 1, 2, 2, 3, 3, 4, 4])\n torch.repeat_interleave(y, 3, dim=1)\ntensor([[1, 1, 1, 2, 2, 2],\n        [3, 3, 3, 4, 4, 4]])\n torch.repeat_interleave(y, torch.tensor([1, 2]), dim=0)\ntensor([[1, 2],\n        [3, 4],\n        [3, 4]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.repeat_interleave(input,repeats,dim=None)\u2192Tensor", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.repeat_interleave(repeats)\u2192Tensor", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.roll", "item_type": "function", "code": "torch.roll(input,shifts,dims=None)\u2192Tensor", "description": "Roll the tensor along the given dimension(s). Elements that are shifted beyond the last position are re-introduced at the first position. If a dimension is not specified, the tensor will be flattened before rolling and then restored to the original shape.  Parameters  input (Tensor) \u2013 the input tensor. shifts (python:int or tuple of python:ints) \u2013 The number of places by which the elements of the tensor are shifted. If shifts is a tuple, dims must be a tuple of the same size, and each dimension will be rolled by the corresponding value dims (python:int or tuple of python:ints) \u2013 Axis along which to roll    Example: &gt;&gt;&gt; x = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8]).view(4, 2) &gt;&gt;&gt; x tensor([[1, 2],         [3, 4],         [5, 6],         [7, 8]]) &gt;&gt;&gt; torch.roll(x, 1, 0) tensor([[7, 8],         [1, 2],         [3, 4],         [5, 6]]) &gt;&gt;&gt; torch.roll(x, -1, 0) tensor([[3, 4],         [5, 6],         [7, 8],         [1, 2]]) &gt;&gt;&gt; torch.roll(x, shifts=(2, 1), dims=(0, 1)) tensor([[6, 5],         [8, 7],         [2, 1],         [4, 3]])   ", "parameters": ["input (Tensor) : the input tensor.", "shifts (python:int or tuple of python:ints) : The number of places by which the elementsof the tensor are shifted. If shifts is a tuple, dims must be a tuple ofthe same size, and each dimension will be rolled by the correspondingvalue", "dims (python:int or tuple of python:ints) : Axis along which to roll"], "returns": null, "example": " x = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8]).view(4, 2)\n x\ntensor([[1, 2],\n        [3, 4],\n        [5, 6],\n        [7, 8]])\n torch.roll(x, 1, 0)\ntensor([[7, 8],\n        [1, 2],\n        [3, 4],\n        [5, 6]])\n torch.roll(x, -1, 0)\ntensor([[3, 4],\n        [5, 6],\n        [7, 8],\n        [1, 2]])\n torch.roll(x, shifts=(2, 1), dims=(0, 1))\ntensor([[6, 5],\n        [8, 7],\n        [2, 1],\n        [4, 3]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.tensordot", "item_type": "function", "code": "torch.tensordot(a,b,dims=2)", "description": "Returns a contraction of a and b over multiple dimensions. tensordot implements a generalized matrix product.  Parameters  a (Tensor) \u2013 Left tensor to contract b (Tensor) \u2013 Right tensor to contract dims (python:int or tuple of two lists of python:integers) \u2013 number of dimensions to contract or explicit lists of dimensions for a and b respectively    When called with an integer argument dims = ddd  , and the number of dimensions of a and b is mmm   and nnn  , respectively, it computes  ri0,...,im\u2212d,id,...,in=\u2211k0,...,kd\u22121ai0,...,im\u2212d,k0,...,kd\u22121\u00d7bk0,...,kd\u22121,id,...,in.r_{i_0,...,i_{m-d}, i_d,...,i_n}   = \\sum_{k_0,...,k_{d-1}} a_{i_0,...,i_{m-d},k_0,...,k_{d-1}} \\times b_{k_0,...,k_{d-1}, i_d,...,i_n}.  ri0\u200b,...,im\u2212d\u200b,id\u200b,...,in\u200b\u200b=k0\u200b,...,kd\u22121\u200b\u2211\u200bai0\u200b,...,im\u2212d\u200b,k0\u200b,...,kd\u22121\u200b\u200b\u00d7bk0\u200b,...,kd\u22121\u200b,id\u200b,...,in\u200b\u200b.  When called with dims of the list form, the given dimensions will be contracted in place of the last ddd   of a and the first ddd   of bbb  . The sizes in these dimensions must match, but tensordot will deal with broadcasted dimensions. Examples: &gt;&gt;&gt; a = torch.arange(60.).reshape(3, 4, 5) &gt;&gt;&gt; b = torch.arange(24.).reshape(4, 3, 2) &gt;&gt;&gt; torch.tensordot(a, b, dims=([1, 0], [0, 1])) tensor([[4400., 4730.],         [4532., 4874.],         [4664., 5018.],         [4796., 5162.],         [4928., 5306.]])  &gt;&gt;&gt; a = torch.randn(3, 4, 5, device='cuda') &gt;&gt;&gt; b = torch.randn(4, 5, 6, device='cuda') &gt;&gt;&gt; c = torch.tensordot(a, b, dims=2).cpu() tensor([[ 8.3504, -2.5436,  6.2922,  2.7556, -1.0732,  3.2741],         [ 3.3161,  0.0704,  5.0187, -0.4079, -4.3126,  4.8744],         [ 0.8223,  3.9445,  3.2168, -0.2400,  3.4117,  1.7780]])   ", "parameters": ["a (Tensor) : Left tensor to contract", "b (Tensor) : Right tensor to contract", "dims (python:int or tuple of two lists of python:integers) : number of dimensions tocontract or explicit lists of dimensions for a andb respectively"], "returns": null, "example": " a = torch.arange(60.).reshape(3, 4, 5)\n b = torch.arange(24.).reshape(4, 3, 2)\n torch.tensordot(a, b, dims=([1, 0], [0, 1]))\ntensor([[4400., 4730.],\n        [4532., 4874.],\n        [4664., 5018.],\n        [4796., 5162.],\n        [4928., 5306.]])\n\n a = torch.randn(3, 4, 5, device='cuda')\n b = torch.randn(4, 5, 6, device='cuda')\n c = torch.tensordot(a, b, dims=2).cpu()\ntensor([[ 8.3504, -2.5436,  6.2922,  2.7556, -1.0732,  3.2741],\n        [ 3.3161,  0.0704,  5.0187, -0.4079, -4.3126,  4.8744],\n        [ 0.8223,  3.9445,  3.2168, -0.2400,  3.4117,  1.7780]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.InstanceNorm3d", "item_type": "class", "code": "classtorch.nn.InstanceNorm3d(num_features,eps=1e-05,momentum=0.1,affine=False,track_running_stats=False)", "description": "Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs with additional channel dimension) as described in the paper Instance Normalization: The Missing Ingredient for Fast Stylization .  y=x\u2212E[x]Var[x]+\u03f5\u2217\u03b3+\u03b2y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\betay=Var[x]+\u03f5\u200bx\u2212E[x]\u200b\u2217\u03b3+\u03b2  The mean and standard-deviation are calculated per-dimension separately for each object in a mini-batch. \u03b3\\gamma\u03b3   and \u03b2\\beta\u03b2   are learnable parameter vectors of size C (where C is the input size) if affine is True. By default, this layer uses instance statistics computed from input data in both training and evaluation modes. If track_running_stats is set to True, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1.  Note This momentum argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is x^new=(1\u2212momentum)\u00d7x^+momemtum\u00d7xt\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momemtum} \\times x_tx^new\u200b=(1\u2212momentum)\u00d7x^+momemtum\u00d7xt\u200b  , where x^\\hat{x}x^   is the estimated statistic and xtx_txt\u200b   is the new observed value.   Note InstanceNorm3d and LayerNorm are very similar, but have some subtle differences. InstanceNorm3d is applied on each channel of channeled data like 3D models with RGB color, but LayerNorm is usually applied on entire sample and often in NLP tasks. Additionaly, LayerNorm applies elementwise affine transform, while InstanceNorm3d usually don\u2019t apply affine transform.   Parameters  num_features \u2013 CCC   from an expected input of size (N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W)   eps \u2013 a value added to the denominator for numerical stability. Default: 1e-5 momentum \u2013 the value used for the running_mean and running_var computation. Default: 0.1 affine \u2013 a boolean value that when set to True, this module has learnable affine parameters, initialized the same way as done for batch normalization. Default: False. track_running_stats \u2013 a boolean value that when set to True, this module tracks the running mean and variance, and when set to False, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: False     Shape: Input: (N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W)   Output: (N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W)   (same shape as input)    Examples: &gt;&gt;&gt; # Without Learnable Parameters &gt;&gt;&gt; m = nn.InstanceNorm3d(100) &gt;&gt;&gt; # With Learnable Parameters &gt;&gt;&gt; m = nn.InstanceNorm3d(100, affine=True) &gt;&gt;&gt; input = torch.randn(20, 100, 35, 45, 10) &gt;&gt;&gt; output = m(input)   ", "parameters": ["num_features : CCC from an expected input of size(N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W)", "eps : a value added to the denominator for numerical stability. Default: 1e-5", "momentum : the value used for the running_mean and running_var computation. Default: 0.1", "affine : a boolean value that when set to True, this module haslearnable affine parameters, initialized the same way as done for batch normalization.Default: False.", "track_running_stats : a boolean value that when set to True, thismodule tracks the running mean and variance, and when set to False,this module does not track such statistics and always uses batchstatistics in both training and eval modes. Default: False"], "returns": null, "example": " # Without Learnable Parameters\n m = nn.InstanceNorm3d(100)\n # With Learnable Parameters\n m = nn.InstanceNorm3d(100, affine=True)\n input = torch.randn(20, 100, 35, 45, 10)\n output = m(input)\n\n", "shape": " Input: (N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W)   Output: (N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W)   (same shape as input)  "},
{"library": "torch", "item_id": "torch.nn.LayerNorm", "item_type": "class", "code": "classtorch.nn.LayerNorm(normalized_shape,eps=1e-05,elementwise_affine=True)", "description": "Applies Layer Normalization over a mini-batch of inputs as described in the paper Layer Normalization .  y=x\u2212E[x]Var[x]+\u03f5\u2217\u03b3+\u03b2y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta  y=Var[x]+\u03f5\u200bx\u2212E[x]\u200b\u2217\u03b3+\u03b2  The mean and standard-deviation are calculated separately over the last certain number dimensions which have to be of the shape specified by normalized_shape. \u03b3\\gamma\u03b3   and \u03b2\\beta\u03b2   are learnable affine transform parameters of normalized_shape if elementwise_affine is True.  Note Unlike Batch Normalization and Instance Normalization, which applies scalar scale and bias for each entire channel/plane with the affine option, Layer Normalization applies per-element scale and bias with elementwise_affine.  This layer uses statistics computed from input data in both training and evaluation modes.  Parameters  normalized_shape (python:int or list or torch.Size) \u2013 input shape from an expected input of size  [\u2217\u00d7normalized_shape[0]\u00d7normalized_shape[1]\u00d7\u2026\u00d7normalized_shape[\u22121]][* \\times \\text{normalized\\_shape}[0] \\times \\text{normalized\\_shape}[1]     \\times \\ldots \\times \\text{normalized\\_shape}[-1]]  [\u2217\u00d7normalized_shape[0]\u00d7normalized_shape[1]\u00d7\u2026\u00d7normalized_shape[\u22121]]  If a single integer is used, it is treated as a singleton list, and this module will normalize over the last dimension which is expected to be of that specific size.  eps \u2013 a value added to the denominator for numerical stability. Default: 1e-5 elementwise_affine \u2013 a boolean value that when set to True, this module has learnable per-element affine parameters initialized to ones (for weights) and zeros (for biases). Default: True.     Shape: Input: (N,\u2217)(N, *)(N,\u2217)   Output: (N,\u2217)(N, *)(N,\u2217)   (same shape as input)    Examples: &gt;&gt;&gt; input = torch.randn(20, 5, 10, 10) &gt;&gt;&gt; # With Learnable Parameters &gt;&gt;&gt; m = nn.LayerNorm(input.size()[1:]) &gt;&gt;&gt; # Without Learnable Parameters &gt;&gt;&gt; m = nn.LayerNorm(input.size()[1:], elementwise_affine=False) &gt;&gt;&gt; # Normalize over last two dimensions &gt;&gt;&gt; m = nn.LayerNorm([10, 10]) &gt;&gt;&gt; # Normalize over last dimension of size 10 &gt;&gt;&gt; m = nn.LayerNorm(10) &gt;&gt;&gt; # Activating the module &gt;&gt;&gt; output = m(input)   ", "parameters": ["normalized_shape (python:int or list or torch.Size) : input shape from an expected inputof size[\u2217\u00d7normalized_shape[0]\u00d7normalized_shape[1]\u00d7\u2026\u00d7normalized_shape[\u22121]][* \\times \\text{normalized\\_shape}[0] \\times \\text{normalized\\_shape}[1]    \\times \\ldots \\times \\text{normalized\\_shape}[-1]][\u2217\u00d7normalized_shape[0]\u00d7normalized_shape[1]\u00d7\u2026\u00d7normalized_shape[\u22121]]If a single integer is used, it is treated as a singleton list, and this module willnormalize over the last dimension which is expected to be of that specific size.", "eps : a value added to the denominator for numerical stability. Default: 1e-5", "elementwise_affine : a boolean value that when set to True, this modulehas learnable per-element affine parameters initialized to ones (for weights)and zeros (for biases). Default: True."], "returns": null, "example": " input = torch.randn(20, 5, 10, 10)\n # With Learnable Parameters\n m = nn.LayerNorm(input.size()[1:])\n # Without Learnable Parameters\n m = nn.LayerNorm(input.size()[1:], elementwise_affine=False)\n # Normalize over last two dimensions\n m = nn.LayerNorm([10, 10])\n # Normalize over last dimension of size 10\n m = nn.LayerNorm(10)\n # Activating the module\n output = m(input)\n\n", "shape": " Input: (N,\u2217)(N, *)(N,\u2217)   Output: (N,\u2217)(N, *)(N,\u2217)   (same shape as input)  "},
{"library": "torch", "item_id": "torch.trace", "item_type": "function", "code": "torch.trace(input)\u2192Tensor", "description": "Returns the sum of the elements of the diagonal of the input 2-D matrix. Example: &gt;&gt;&gt; x = torch.arange(1., 10.).view(3, 3) &gt;&gt;&gt; x tensor([[ 1.,  2.,  3.],         [ 4.,  5.,  6.],         [ 7.,  8.,  9.]]) &gt;&gt;&gt; torch.trace(x) tensor(15.)   ", "parameters": [], "returns": null, "example": " x = torch.arange(1., 10.).view(3, 3)\n x\ntensor([[ 1.,  2.,  3.],\n        [ 4.,  5.,  6.],\n        [ 7.,  8.,  9.]])\n torch.trace(x)\ntensor(15.)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.tril", "item_type": "function", "code": "torch.tril(input,diagonal=0,out=None)\u2192Tensor", "description": "Returns the lower triangular part of the matrix (2-D tensor) or batch of matrices input, the other elements of the result tensor out are set to 0. The lower triangular part of the matrix is defined as the elements on and below the diagonal. The argument diagonal controls which diagonal to consider. If diagonal = 0, all elements on and below the main diagonal are retained. A positive value includes just as many diagonals above the main diagonal, and similarly a negative value excludes just as many diagonals below the main diagonal. The main diagonal are the set of indices {(i,i)}\\lbrace (i, i) \\rbrace{(i,i)}   for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]   where d1,d2d_{1}, d_{2}d1\u200b,d2\u200b   are the dimensions of the matrix.  Parameters  input (Tensor) \u2013 the input tensor. diagonal (python:int, optional) \u2013 the diagonal to consider out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; a = torch.randn(3, 3) &gt;&gt;&gt; a tensor([[-1.0813, -0.8619,  0.7105],         [ 0.0935,  0.1380,  2.2112],         [-0.3409, -0.9828,  0.0289]]) &gt;&gt;&gt; torch.tril(a) tensor([[-1.0813,  0.0000,  0.0000],         [ 0.0935,  0.1380,  0.0000],         [-0.3409, -0.9828,  0.0289]])  &gt;&gt;&gt; b = torch.randn(4, 6) &gt;&gt;&gt; b tensor([[ 1.2219,  0.5653, -0.2521, -0.2345,  1.2544,  0.3461],         [ 0.4785, -0.4477,  0.6049,  0.6368,  0.8775,  0.7145],         [ 1.1502,  3.2716, -1.1243, -0.5413,  0.3615,  0.6864],         [-0.0614, -0.7344, -1.3164, -0.7648, -1.4024,  0.0978]]) &gt;&gt;&gt; torch.tril(b, diagonal=1) tensor([[ 1.2219,  0.5653,  0.0000,  0.0000,  0.0000,  0.0000],         [ 0.4785, -0.4477,  0.6049,  0.0000,  0.0000,  0.0000],         [ 1.1502,  3.2716, -1.1243, -0.5413,  0.0000,  0.0000],         [-0.0614, -0.7344, -1.3164, -0.7648, -1.4024,  0.0000]]) &gt;&gt;&gt; torch.tril(b, diagonal=-1) tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],         [ 0.4785,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],         [ 1.1502,  3.2716,  0.0000,  0.0000,  0.0000,  0.0000],         [-0.0614, -0.7344, -1.3164,  0.0000,  0.0000,  0.0000]])   ", "parameters": ["input (Tensor) : the input tensor.", "diagonal (python:int, optional) : the diagonal to consider", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " a = torch.randn(3, 3)\n a\ntensor([[-1.0813, -0.8619,  0.7105],\n        [ 0.0935,  0.1380,  2.2112],\n        [-0.3409, -0.9828,  0.0289]])\n torch.tril(a)\ntensor([[-1.0813,  0.0000,  0.0000],\n        [ 0.0935,  0.1380,  0.0000],\n        [-0.3409, -0.9828,  0.0289]])\n\n b = torch.randn(4, 6)\n b\ntensor([[ 1.2219,  0.5653, -0.2521, -0.2345,  1.2544,  0.3461],\n        [ 0.4785, -0.4477,  0.6049,  0.6368,  0.8775,  0.7145],\n        [ 1.1502,  3.2716, -1.1243, -0.5413,  0.3615,  0.6864],\n        [-0.0614, -0.7344, -1.3164, -0.7648, -1.4024,  0.0978]])\n torch.tril(b, diagonal=1)\ntensor([[ 1.2219,  0.5653,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.4785, -0.4477,  0.6049,  0.0000,  0.0000,  0.0000],\n        [ 1.1502,  3.2716, -1.1243, -0.5413,  0.0000,  0.0000],\n        [-0.0614, -0.7344, -1.3164, -0.7648, -1.4024,  0.0000]])\n torch.tril(b, diagonal=-1)\ntensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.4785,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 1.1502,  3.2716,  0.0000,  0.0000,  0.0000,  0.0000],\n        [-0.0614, -0.7344, -1.3164,  0.0000,  0.0000,  0.0000]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.tril_indices", "item_type": "function", "code": "torch.tril_indices(row,col,offset=0,dtype=torch.long,device='cpu',layout=torch.strided)\u2192Tensor", "description": "Returns the indices of the lower triangular part of a row-by- col matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates. Indices are ordered based on rows and then columns. The lower triangular part of the matrix is defined as the elements on and below the diagonal. The argument offset controls which diagonal to consider. If offset = 0, all elements on and below the main diagonal are retained. A positive value includes just as many diagonals above the main diagonal, and similarly a negative value excludes just as many diagonals below the main diagonal. The main diagonal are the set of indices {(i,i)}\\lbrace (i, i) \\rbrace{(i,i)}   for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]   where d1,d2d_{1}, d_{2}d1\u200b,d2\u200b   are the dimensions of the matrix. NOTE: when running on \u2018cuda\u2019, row * col must be less than 2592^{59}259   to prevent overflow during calculation.  Parameters  row (int) \u2013 number of rows in the 2-D matrix. col (int) \u2013 number of columns in the 2-D matrix. offset (int) \u2013 diagonal offset from the main diagonal. Default: if not provided, 0. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, torch.long. device (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. layout (torch.layout, optional) \u2013 currently only support torch.strided.     Example::&gt;&gt;&gt; a = torch.tril_indices(3, 3) &gt;&gt;&gt; a tensor([[0, 1, 1, 2, 2, 2],         [0, 0, 1, 0, 1, 2]])   &gt;&gt;&gt; a = torch.tril_indices(4, 3, -1) &gt;&gt;&gt; a tensor([[1, 2, 2, 3, 3, 3],         [0, 0, 1, 0, 1, 2]])   &gt;&gt;&gt; a = torch.tril_indices(4, 3, 1) &gt;&gt;&gt; a tensor([[0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3],         [0, 1, 0, 1, 2, 0, 1, 2, 0, 1, 2]])     ", "parameters": ["row (int) : number of rows in the 2-D matrix.", "col (int) : number of columns in the 2-D matrix.", "offset (int) : diagonal offset from the main diagonal.Default: if not provided, 0.", "dtype (torch.dtype, optional) : the desired data type of returned tensor.Default: if None, torch.long.", "device (torch.device, optional) : the desired device of returned tensor.Default: if None, uses the current device for the default tensor type(see torch.set_default_tensor_type()). device will be the CPUfor CPU tensor types and the current CUDA device for CUDA tensor types.", "layout (torch.layout, optional) : currently only support torch.strided."], "returns": null, "example": " a = torch.tril_indices(3, 3)\n a\ntensor([[0, 1, 1, 2, 2, 2],\n        [0, 0, 1, 0, 1, 2]])\n\n\n a = torch.tril_indices(4, 3, -1)\n a\ntensor([[1, 2, 2, 3, 3, 3],\n        [0, 0, 1, 0, 1, 2]])\n\n\n a = torch.tril_indices(4, 3, 1)\n a\ntensor([[0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3],\n        [0, 1, 0, 1, 2, 0, 1, 2, 0, 1, 2]])\n\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.LocalResponseNorm", "item_type": "class", "code": "classtorch.nn.LocalResponseNorm(size,alpha=0.0001,beta=0.75,k=1.0)", "description": "Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension. Applies normalization across channels.  bc=ac(k+\u03b1n\u2211c\u2032=max\u2061(0,c\u2212n/2)min\u2061(N\u22121,c+n/2)ac\u20322)\u2212\u03b2b_{c} = a_{c}\\left(k + \\frac{\\alpha}{n} \\sum_{c'=\\max(0, c-n/2)}^{\\min(N-1,c+n/2)}a_{c'}^2\\right)^{-\\beta}  bc\u200b=ac\u200b\u239d\u239c\u239b\u200bk+n\u03b1\u200bc\u2032=max(0,c\u2212n/2)\u2211min(N\u22121,c+n/2)\u200bac\u20322\u200b\u23a0\u239f\u239e\u200b\u2212\u03b2   Parameters  size \u2013 amount of neighbouring channels used for normalization alpha \u2013 multiplicative factor. Default: 0.0001 beta \u2013 exponent. Default: 0.75 k \u2013 additive factor. Default: 1     Shape: Input: (N,C,\u2217)(N, C, *)(N,C,\u2217)   Output: (N,C,\u2217)(N, C, *)(N,C,\u2217)   (same shape as input)    Examples: &gt;&gt;&gt; lrn = nn.LocalResponseNorm(2) &gt;&gt;&gt; signal_2d = torch.randn(32, 5, 24, 24) &gt;&gt;&gt; signal_4d = torch.randn(16, 5, 7, 7, 7, 7) &gt;&gt;&gt; output_2d = lrn(signal_2d) &gt;&gt;&gt; output_4d = lrn(signal_4d)   ", "parameters": ["size : amount of neighbouring channels used for normalization", "alpha : multiplicative factor. Default: 0.0001", "beta : exponent. Default: 0.75", "k : additive factor. Default: 1"], "returns": null, "example": " lrn = nn.LocalResponseNorm(2)\n signal_2d = torch.randn(32, 5, 24, 24)\n signal_4d = torch.randn(16, 5, 7, 7, 7, 7)\n output_2d = lrn(signal_2d)\n output_4d = lrn(signal_4d)\n\n", "shape": " Input: (N,C,\u2217)(N, C, *)(N,C,\u2217)   Output: (N,C,\u2217)(N, C, *)(N,C,\u2217)   (same shape as input)  "},
{"library": "torch", "item_id": "torch.nn.RNNBase", "item_type": "class", "code": "classtorch.nn.RNNBase(mode,input_size,hidden_size,num_layers=1,bias=True,batch_first=False,dropout=0.0,bidirectional=False)", "description": "  flatten_parameters()  Resets parameter data pointer so that they can use faster code paths. Right now, this works only if the module is on the GPU and cuDNN is enabled. Otherwise, it\u2019s a no-op.   ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.RNN", "item_type": "class", "code": "classtorch.nn.RNN(*args,**kwargs)", "description": "Applies a multi-layer Elman RNN with tanhtanhtanh   or ReLUReLUReLU   non-linearity to an input sequence. For each element in the input sequence, each layer computes the following function:  ht=tanh(Wihxt+bih+Whhh(t\u22121)+bhh)h_t = \\text{tanh}(W_{ih} x_t + b_{ih} + W_{hh} h_{(t-1)} + b_{hh})  ht\u200b=tanh(Wih\u200bxt\u200b+bih\u200b+Whh\u200bh(t\u22121)\u200b+bhh\u200b)  where hth_tht\u200b   is the hidden state at time t, xtx_txt\u200b   is the input at time t, and h(t\u22121)h_{(t-1)}h(t\u22121)\u200b   is the hidden state of the previous layer at time t-1 or the initial hidden state at time 0. If nonlinearity is 'relu', then ReLU is used instead of tanh.  Parameters  input_size \u2013 The number of expected features in the input x hidden_size \u2013 The number of features in the hidden state h num_layers \u2013 Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two RNNs together to form a stacked RNN, with the second RNN taking in outputs of the first RNN and computing the final results. Default: 1 nonlinearity \u2013 The non-linearity to use. Can be either 'tanh' or 'relu'. Default: 'tanh' bias \u2013 If False, then the layer does not use bias weights b_ih and b_hh. Default: True batch_first \u2013 If True, then the input and output tensors are provided as (batch, seq, feature). Default: False dropout \u2013 If non-zero, introduces a Dropout layer on the outputs of each RNN layer except the last layer, with dropout probability equal to dropout. Default: 0 bidirectional \u2013 If True, becomes a bidirectional RNN. Default: False     Inputs: input, h_0 input of shape (seq_len, batch, input_size): tensor containing the features of the input sequence. The input can also be a packed variable length sequence. See torch.nn.utils.rnn.pack_padded_sequence() or torch.nn.utils.rnn.pack_sequence() for details. h_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided. If the RNN is bidirectional, num_directions should be 2, else it should be 1.   Outputs: output, h_n output of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features (h_t) from the last layer of the RNN, for each t.  If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence. For the unpacked case, the directions can be separated using output.view(seq_len, batch, num_directions, hidden_size), with forward and backward being direction 0 and 1 respectively. Similarly, the directions can be separated in the packed case.  h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len. Like output, the layers can be separated using h_n.view(num_layers, num_directions, batch, hidden_size).    Shape: Input1: (L,N,Hin)(L, N, H_{in})(L,N,Hin\u200b)   tensor containing input features where Hin=input_sizeH_{in}=\\text{input\\_size}Hin\u200b=input_size   and L represents a sequence length. Input2: (S,N,Hout)(S, N, H_{out})(S,N,Hout\u200b)   tensor containing the initial hidden state for each element in the batch. Hout=hidden_sizeH_{out}=\\text{hidden\\_size}Hout\u200b=hidden_size   Defaults to zero if not provided. where S=num_layers\u2217num_directionsS=\\text{num\\_layers} * \\text{num\\_directions}S=num_layers\u2217num_directions   If the RNN is bidirectional, num_directions should be 2, else it should be 1. Output1: (L,N,Hall)(L, N, H_{all})(L,N,Hall\u200b)   where Hall=num_directions\u2217hidden_sizeH_{all}=\\text{num\\_directions} * \\text{hidden\\_size}Hall\u200b=num_directions\u2217hidden_size   Output2: (S,N,Hout)(S, N, H_{out})(S,N,Hout\u200b)   tensor containing the next hidden state for each element in the batch     Variables  ~RNN.weight_ih_l[k] \u2013 the learnable input-hidden weights of the k-th layer, of shape (hidden_size, input_size) for k = 0. Otherwise, the shape is (hidden_size, num_directions * hidden_size) ~RNN.weight_hh_l[k] \u2013 the learnable hidden-hidden weights of the k-th layer, of shape (hidden_size, hidden_size) ~RNN.bias_ih_l[k] \u2013 the learnable input-hidden bias of the k-th layer, of shape (hidden_size) ~RNN.bias_hh_l[k] \u2013 the learnable hidden-hidden bias of the k-th layer, of shape (hidden_size)     Note All the weights and biases are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b)   where k=1hidden_sizek = \\frac{1}{\\text{hidden\\_size}}k=hidden_size1\u200b     Note If the following conditions are satisfied: 1) cudnn is enabled, 2) input data is on the GPU 3) input data has dtype torch.float16 4) V100 GPU is used, 5) input data is not in PackedSequence format persistent algorithm can be selected to improve performance.  Examples: &gt;&gt;&gt; rnn = nn.RNN(10, 20, 2) &gt;&gt;&gt; input = torch.randn(5, 3, 10) &gt;&gt;&gt; h0 = torch.randn(2, 3, 20) &gt;&gt;&gt; output, hn = rnn(input, h0)   ", "parameters": ["input_size : The number of expected features in the input x", "hidden_size : The number of features in the hidden state h", "num_layers : Number of recurrent layers. E.g., setting num_layers=2would mean stacking two RNNs together to form a stacked RNN,with the second RNN taking in outputs of the first RNN andcomputing the final results. Default: 1", "nonlinearity : The non-linearity to use. Can be either 'tanh' or 'relu'. Default: 'tanh'", "bias : If False, then the layer does not use bias weights b_ih and b_hh.Default: True", "batch_first : If True, then the input and output tensors are providedas (batch, seq, feature). Default: False", "dropout : If non-zero, introduces a Dropout layer on the outputs of eachRNN layer except the last layer, with dropout probability equal todropout. Default: 0", "bidirectional : If True, becomes a bidirectional RNN. Default: False", "~RNN.weight_ih_l[k] : the learnable input-hidden weights of the k-th layer,of shape (hidden_size, input_size) for k = 0. Otherwise, the shape is(hidden_size, num_directions * hidden_size)", "~RNN.weight_hh_l[k] : the learnable hidden-hidden weights of the k-th layer,of shape (hidden_size, hidden_size)", "~RNN.bias_ih_l[k] : the learnable input-hidden bias of the k-th layer,of shape (hidden_size)", "~RNN.bias_hh_l[k] : the learnable hidden-hidden bias of the k-th layer,of shape (hidden_size)"], "returns": null, "example": " rnn = nn.RNN(10, 20, 2)\n input = torch.randn(5, 3, 10)\n h0 = torch.randn(2, 3, 20)\n output, hn = rnn(input, h0)\n\n", "shape": " Input1: (L,N,Hin)(L, N, H_{in})(L,N,Hin\u200b)   tensor containing input features where Hin=input_sizeH_{in}=\\text{input\\_size}Hin\u200b=input_size   and L represents a sequence length. Input2: (S,N,Hout)(S, N, H_{out})(S,N,Hout\u200b)   tensor containing the initial hidden state for each element in the batch. Hout=hidden_sizeH_{out}=\\text{hidden\\_size}Hout\u200b=hidden_size   Defaults to zero if not provided. where S=num_layers\u2217num_directionsS=\\text{num\\_layers} * \\text{num\\_directions}S=num_layers\u2217num_directions   If the RNN is bidirectional, num_directions should be 2, else it should be 1. Output1: (L,N,Hall)(L, N, H_{all})(L,N,Hall\u200b)   where Hall=num_directions\u2217hidden_sizeH_{all}=\\text{num\\_directions} * \\text{hidden\\_size}Hall\u200b=num_directions\u2217hidden_size   Output2: (S,N,Hout)(S, N, H_{out})(S,N,Hout\u200b)   tensor containing the next hidden state for each element in the batch  "},
{"library": "torch", "item_id": "torch.triu", "item_type": "function", "code": "torch.triu(input,diagonal=0,out=None)\u2192Tensor", "description": "Returns the upper triangular part of a matrix (2-D tensor) or batch of matrices input, the other elements of the result tensor out are set to 0. The upper triangular part of the matrix is defined as the elements on and above the diagonal. The argument diagonal controls which diagonal to consider. If diagonal = 0, all elements on and above the main diagonal are retained. A positive value excludes just as many diagonals above the main diagonal, and similarly a negative value includes just as many diagonals below the main diagonal. The main diagonal are the set of indices {(i,i)}\\lbrace (i, i) \\rbrace{(i,i)}   for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]   where d1,d2d_{1}, d_{2}d1\u200b,d2\u200b   are the dimensions of the matrix.  Parameters  input (Tensor) \u2013 the input tensor. diagonal (python:int, optional) \u2013 the diagonal to consider out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; a = torch.randn(3, 3) &gt;&gt;&gt; a tensor([[ 0.2309,  0.5207,  2.0049],         [ 0.2072, -1.0680,  0.6602],         [ 0.3480, -0.5211, -0.4573]]) &gt;&gt;&gt; torch.triu(a) tensor([[ 0.2309,  0.5207,  2.0049],         [ 0.0000, -1.0680,  0.6602],         [ 0.0000,  0.0000, -0.4573]]) &gt;&gt;&gt; torch.triu(a, diagonal=1) tensor([[ 0.0000,  0.5207,  2.0049],         [ 0.0000,  0.0000,  0.6602],         [ 0.0000,  0.0000,  0.0000]]) &gt;&gt;&gt; torch.triu(a, diagonal=-1) tensor([[ 0.2309,  0.5207,  2.0049],         [ 0.2072, -1.0680,  0.6602],         [ 0.0000, -0.5211, -0.4573]])  &gt;&gt;&gt; b = torch.randn(4, 6) &gt;&gt;&gt; b tensor([[ 0.5876, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],         [-0.2447,  0.9556, -1.2919,  1.3378, -0.1768, -1.0857],         [ 0.4333,  0.3146,  0.6576, -1.0432,  0.9348, -0.4410],         [-0.9888,  1.0679, -1.3337, -1.6556,  0.4798,  0.2830]]) &gt;&gt;&gt; torch.triu(b, diagonal=1) tensor([[ 0.0000, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],         [ 0.0000,  0.0000, -1.2919,  1.3378, -0.1768, -1.0857],         [ 0.0000,  0.0000,  0.0000, -1.0432,  0.9348, -0.4410],         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.4798,  0.2830]]) &gt;&gt;&gt; torch.triu(b, diagonal=-1) tensor([[ 0.5876, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],         [-0.2447,  0.9556, -1.2919,  1.3378, -0.1768, -1.0857],         [ 0.0000,  0.3146,  0.6576, -1.0432,  0.9348, -0.4410],         [ 0.0000,  0.0000, -1.3337, -1.6556,  0.4798,  0.2830]])   ", "parameters": ["input (Tensor) : the input tensor.", "diagonal (python:int, optional) : the diagonal to consider", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " a = torch.randn(3, 3)\n a\ntensor([[ 0.2309,  0.5207,  2.0049],\n        [ 0.2072, -1.0680,  0.6602],\n        [ 0.3480, -0.5211, -0.4573]])\n torch.triu(a)\ntensor([[ 0.2309,  0.5207,  2.0049],\n        [ 0.0000, -1.0680,  0.6602],\n        [ 0.0000,  0.0000, -0.4573]])\n torch.triu(a, diagonal=1)\ntensor([[ 0.0000,  0.5207,  2.0049],\n        [ 0.0000,  0.0000,  0.6602],\n        [ 0.0000,  0.0000,  0.0000]])\n torch.triu(a, diagonal=-1)\ntensor([[ 0.2309,  0.5207,  2.0049],\n        [ 0.2072, -1.0680,  0.6602],\n        [ 0.0000, -0.5211, -0.4573]])\n\n b = torch.randn(4, 6)\n b\ntensor([[ 0.5876, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],\n        [-0.2447,  0.9556, -1.2919,  1.3378, -0.1768, -1.0857],\n        [ 0.4333,  0.3146,  0.6576, -1.0432,  0.9348, -0.4410],\n        [-0.9888,  1.0679, -1.3337, -1.6556,  0.4798,  0.2830]])\n torch.triu(b, diagonal=1)\ntensor([[ 0.0000, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],\n        [ 0.0000,  0.0000, -1.2919,  1.3378, -0.1768, -1.0857],\n        [ 0.0000,  0.0000,  0.0000, -1.0432,  0.9348, -0.4410],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.4798,  0.2830]])\n torch.triu(b, diagonal=-1)\ntensor([[ 0.5876, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],\n        [-0.2447,  0.9556, -1.2919,  1.3378, -0.1768, -1.0857],\n        [ 0.0000,  0.3146,  0.6576, -1.0432,  0.9348, -0.4410],\n        [ 0.0000,  0.0000, -1.3337, -1.6556,  0.4798,  0.2830]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.triu_indices", "item_type": "function", "code": "torch.triu_indices(row,col,offset=0,dtype=torch.long,device='cpu',layout=torch.strided)\u2192Tensor", "description": "Returns the indices of the upper triangular part of a row by col matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates. Indices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and above the diagonal. The argument offset controls which diagonal to consider. If offset = 0, all elements on and above the main diagonal are retained. A positive value excludes just as many diagonals above the main diagonal, and similarly a negative value includes just as many diagonals below the main diagonal. The main diagonal are the set of indices {(i,i)}\\lbrace (i, i) \\rbrace{(i,i)}   for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]   where d1,d2d_{1}, d_{2}d1\u200b,d2\u200b   are the dimensions of the matrix. NOTE: when running on \u2018cuda\u2019, row * col must be less than 2592^{59}259   to prevent overflow during calculation.  Parameters  row (int) \u2013 number of rows in the 2-D matrix. col (int) \u2013 number of columns in the 2-D matrix. offset (int) \u2013 diagonal offset from the main diagonal. Default: if not provided, 0. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor. Default: if None, torch.long. device (torch.device, optional) \u2013 the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types. layout (torch.layout, optional) \u2013 currently only support torch.strided.     Example::&gt;&gt;&gt; a = torch.triu_indices(3, 3) &gt;&gt;&gt; a tensor([[0, 0, 0, 1, 1, 2],         [0, 1, 2, 1, 2, 2]])   &gt;&gt;&gt; a = torch.triu_indices(4, 3, -1) &gt;&gt;&gt; a tensor([[0, 0, 0, 1, 1, 1, 2, 2, 3],         [0, 1, 2, 0, 1, 2, 1, 2, 2]])   &gt;&gt;&gt; a = torch.triu_indices(4, 3, 1) &gt;&gt;&gt; a tensor([[0, 0, 1],         [1, 2, 2]])     ", "parameters": ["row (int) : number of rows in the 2-D matrix.", "col (int) : number of columns in the 2-D matrix.", "offset (int) : diagonal offset from the main diagonal.Default: if not provided, 0.", "dtype (torch.dtype, optional) : the desired data type of returned tensor.Default: if None, torch.long.", "device (torch.device, optional) : the desired device of returned tensor.Default: if None, uses the current device for the default tensor type(see torch.set_default_tensor_type()). device will be the CPUfor CPU tensor types and the current CUDA device for CUDA tensor types.", "layout (torch.layout, optional) : currently only support torch.strided."], "returns": null, "example": " a = torch.triu_indices(3, 3)\n a\ntensor([[0, 0, 0, 1, 1, 2],\n        [0, 1, 2, 1, 2, 2]])\n\n\n a = torch.triu_indices(4, 3, -1)\n a\ntensor([[0, 0, 0, 1, 1, 1, 2, 2, 3],\n        [0, 1, 2, 0, 1, 2, 1, 2, 2]])\n\n\n a = torch.triu_indices(4, 3, 1)\n a\ntensor([[0, 0, 1],\n        [1, 2, 2]])\n\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.LSTM", "item_type": "class", "code": "classtorch.nn.LSTM(*args,**kwargs)", "description": "Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence. For each element in the input sequence, each layer computes the following function:  it=\u03c3(Wiixt+bii+Whih(t\u22121)+bhi)ft=\u03c3(Wifxt+bif+Whfh(t\u22121)+bhf)gt=tanh\u2061(Wigxt+big+Whgh(t\u22121)+bhg)ot=\u03c3(Wioxt+bio+Whoh(t\u22121)+bho)ct=ft\u2217c(t\u22121)+it\u2217gtht=ot\u2217tanh\u2061(ct)\\begin{array}{ll} \\\\     i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{(t-1)} + b_{hi}) \\\\     f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{(t-1)} + b_{hf}) \\\\     g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{(t-1)} + b_{hg}) \\\\     o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{(t-1)} + b_{ho}) \\\\     c_t = f_t * c_{(t-1)} + i_t * g_t \\\\     h_t = o_t * \\tanh(c_t) \\\\ \\end{array}  it\u200b=\u03c3(Wii\u200bxt\u200b+bii\u200b+Whi\u200bh(t\u22121)\u200b+bhi\u200b)ft\u200b=\u03c3(Wif\u200bxt\u200b+bif\u200b+Whf\u200bh(t\u22121)\u200b+bhf\u200b)gt\u200b=tanh(Wig\u200bxt\u200b+big\u200b+Whg\u200bh(t\u22121)\u200b+bhg\u200b)ot\u200b=\u03c3(Wio\u200bxt\u200b+bio\u200b+Who\u200bh(t\u22121)\u200b+bho\u200b)ct\u200b=ft\u200b\u2217c(t\u22121)\u200b+it\u200b\u2217gt\u200bht\u200b=ot\u200b\u2217tanh(ct\u200b)\u200b  where hth_tht\u200b   is the hidden state at time t, ctc_tct\u200b   is the cell state at time t, xtx_txt\u200b   is the input at time t, h(t\u22121)h_{(t-1)}h(t\u22121)\u200b   is the hidden state of the layer at time t-1 or the initial hidden state at time 0, and iti_tit\u200b  , ftf_tft\u200b  , gtg_tgt\u200b  , oto_tot\u200b   are the input, forget, cell, and output gates, respectively. \u03c3\\sigma\u03c3   is the sigmoid function, and \u2217*\u2217   is the Hadamard product. In a multilayer LSTM, the input xt(l)x^{(l)}_txt(l)\u200b   of the lll   -th layer (l&gt;=2l &gt;= 2l&gt;=2  ) is the hidden state ht(l\u22121)h^{(l-1)}_tht(l\u22121)\u200b   of the previous layer multiplied by dropout \u03b4t(l\u22121)\\delta^{(l-1)}_t\u03b4t(l\u22121)\u200b   where each \u03b4t(l\u22121)\\delta^{(l-1)}_t\u03b4t(l\u22121)\u200b   is a Bernoulli random variable which is 000   with probability dropout.  Parameters  input_size \u2013 The number of expected features in the input x hidden_size \u2013 The number of features in the hidden state h num_layers \u2013 Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results. Default: 1 bias \u2013 If False, then the layer does not use bias weights b_ih and b_hh. Default: True batch_first \u2013 If True, then the input and output tensors are provided as (batch, seq, feature). Default: False dropout \u2013 If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to dropout. Default: 0 bidirectional \u2013 If True, becomes a bidirectional LSTM. Default: False     Inputs: input, (h_0, c_0) input of shape (seq_len, batch, input_size): tensor containing the features of the input sequence. The input can also be a packed variable length sequence. See torch.nn.utils.rnn.pack_padded_sequence() or torch.nn.utils.rnn.pack_sequence() for details. h_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. If the LSTM is bidirectional, num_directions should be 2, else it should be 1. c_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial cell state for each element in the batch. If (h_0, c_0) is not provided, both h_0 and c_0 default to zero.    Outputs: output, (h_n, c_n) output of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features (h_t) from the last layer of the LSTM, for each t. If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence. For the unpacked case, the directions can be separated using output.view(seq_len, batch, num_directions, hidden_size), with forward and backward being direction 0 and 1 respectively. Similarly, the directions can be separated in the packed case.  h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len. Like output, the layers can be separated using h_n.view(num_layers, num_directions, batch, hidden_size) and similarly for c_n.  c_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the cell state for t = seq_len.     Variables  ~LSTM.weight_ih_l[k] \u2013 the learnable input-hidden weights of the kth\\text{k}^{th}kth   layer (W_ii|W_if|W_ig|W_io), of shape (4*hidden_size, input_size) for k = 0. Otherwise, the shape is (4*hidden_size, num_directions * hidden_size) ~LSTM.weight_hh_l[k] \u2013 the learnable hidden-hidden weights of the kth\\text{k}^{th}kth   layer (W_hi|W_hf|W_hg|W_ho), of shape (4*hidden_size, hidden_size) ~LSTM.bias_ih_l[k] \u2013 the learnable input-hidden bias of the kth\\text{k}^{th}kth   layer (b_ii|b_if|b_ig|b_io), of shape (4*hidden_size) ~LSTM.bias_hh_l[k] \u2013 the learnable hidden-hidden bias of the kth\\text{k}^{th}kth   layer (b_hi|b_hf|b_hg|b_ho), of shape (4*hidden_size)     Note All the weights and biases are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b)   where k=1hidden_sizek = \\frac{1}{\\text{hidden\\_size}}k=hidden_size1\u200b     Note If the following conditions are satisfied: 1) cudnn is enabled, 2) input data is on the GPU 3) input data has dtype torch.float16 4) V100 GPU is used, 5) input data is not in PackedSequence format persistent algorithm can be selected to improve performance.  Examples: &gt;&gt;&gt; rnn = nn.LSTM(10, 20, 2) &gt;&gt;&gt; input = torch.randn(5, 3, 10) &gt;&gt;&gt; h0 = torch.randn(2, 3, 20) &gt;&gt;&gt; c0 = torch.randn(2, 3, 20) &gt;&gt;&gt; output, (hn, cn) = rnn(input, (h0, c0))   ", "parameters": ["input_size : The number of expected features in the input x", "hidden_size : The number of features in the hidden state h", "num_layers : Number of recurrent layers. E.g., setting num_layers=2would mean stacking two LSTMs together to form a stacked LSTM,with the second LSTM taking in outputs of the first LSTM andcomputing the final results. Default: 1", "bias : If False, then the layer does not use bias weights b_ih and b_hh.Default: True", "batch_first : If True, then the input and output tensors are providedas (batch, seq, feature). Default: False", "dropout : If non-zero, introduces a Dropout layer on the outputs of eachLSTM layer except the last layer, with dropout probability equal todropout. Default: 0", "bidirectional : If True, becomes a bidirectional LSTM. Default: False", "~LSTM.weight_ih_l[k] : the learnable input-hidden weights of the kth\\text{k}^{th}kth layer(W_ii|W_if|W_ig|W_io), of shape (4*hidden_size, input_size) for k = 0.Otherwise, the shape is (4*hidden_size, num_directions * hidden_size)", "~LSTM.weight_hh_l[k] : the learnable hidden-hidden weights of the kth\\text{k}^{th}kth layer(W_hi|W_hf|W_hg|W_ho), of shape (4*hidden_size, hidden_size)", "~LSTM.bias_ih_l[k] : the learnable input-hidden bias of the kth\\text{k}^{th}kth layer(b_ii|b_if|b_ig|b_io), of shape (4*hidden_size)", "~LSTM.bias_hh_l[k] : the learnable hidden-hidden bias of the kth\\text{k}^{th}kth layer(b_hi|b_hf|b_hg|b_ho), of shape (4*hidden_size)"], "returns": null, "example": " rnn = nn.LSTM(10, 20, 2)\n input = torch.randn(5, 3, 10)\n h0 = torch.randn(2, 3, 20)\n c0 = torch.randn(2, 3, 20)\n output, (hn, cn) = rnn(input, (h0, c0))\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.addbmm", "item_type": "function", "code": "torch.addbmm(beta=1,input,alpha=1,batch1,batch2,out=None)\u2192Tensor", "description": "Performs a batch matrix-matrix product of matrices stored in batch1 and batch2, with a reduced add step (all matrix multiplications get accumulated along the first dimension). input is added to the final result. batch1 and batch2 must be 3-D tensors each containing the same number of matrices. If batch1 is a (b\u00d7n\u00d7m)(b \\times n \\times m)(b\u00d7n\u00d7m)   tensor, batch2 is a (b\u00d7m\u00d7p)(b \\times m \\times p)(b\u00d7m\u00d7p)   tensor, input must be broadcastable with a (n\u00d7p)(n \\times p)(n\u00d7p)   tensor and out will be a (n\u00d7p)(n \\times p)(n\u00d7p)   tensor.  out=\u03b2\u00a0input+\u03b1\u00a0(\u2211i=0b\u22121batch1i@batch2i)out = \\beta\\ \\text{input} + \\alpha\\ (\\sum_{i=0}^{b-1} \\text{batch1}_i \\mathbin{@} \\text{batch2}_i)  out=\u03b2\u00a0input+\u03b1\u00a0(i=0\u2211b\u22121\u200bbatch1i\u200b@batch2i\u200b)  For inputs of type FloatTensor or DoubleTensor, arguments beta and alpha must be real numbers, otherwise they should be integers.  Parameters  beta (Number, optional) \u2013 multiplier for input (\u03b2\\beta\u03b2  ) input (Tensor) \u2013 matrix to be added alpha (Number, optional) \u2013 multiplier for batch1 @ batch2 (\u03b1\\alpha\u03b1  ) batch1 (Tensor) \u2013 the first batch of matrices to be multiplied batch2 (Tensor) \u2013 the second batch of matrices to be multiplied out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; M = torch.randn(3, 5) &gt;&gt;&gt; batch1 = torch.randn(10, 3, 4) &gt;&gt;&gt; batch2 = torch.randn(10, 4, 5) &gt;&gt;&gt; torch.addbmm(M, batch1, batch2) tensor([[  6.6311,   0.0503,   6.9768, -12.0362,  -2.1653],         [ -4.8185,  -1.4255,  -6.6760,   8.9453,   2.5743],         [ -3.8202,   4.3691,   1.0943,  -1.1109,   5.4730]])   ", "parameters": ["beta (Number, optional) : multiplier for input (\u03b2\\beta\u03b2)", "input (Tensor) : matrix to be added", "alpha (Number, optional) : multiplier for batch1 @ batch2 (\u03b1\\alpha\u03b1)", "batch1 (Tensor) : the first batch of matrices to be multiplied", "batch2 (Tensor) : the second batch of matrices to be multiplied", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " M = torch.randn(3, 5)\n batch1 = torch.randn(10, 3, 4)\n batch2 = torch.randn(10, 4, 5)\n torch.addbmm(M, batch1, batch2)\ntensor([[  6.6311,   0.0503,   6.9768, -12.0362,  -2.1653],\n        [ -4.8185,  -1.4255,  -6.6760,   8.9453,   2.5743],\n        [ -3.8202,   4.3691,   1.0943,  -1.1109,   5.4730]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.addmm", "item_type": "function", "code": "torch.addmm(beta=1,input,alpha=1,mat1,mat2,out=None)\u2192Tensor", "description": "Performs a matrix multiplication of the matrices mat1 and mat2. The matrix input is added to the final result. If mat1 is a (n\u00d7m)(n \\times m)(n\u00d7m)   tensor, mat2 is a (m\u00d7p)(m \\times p)(m\u00d7p)   tensor, then input must be broadcastable with a (n\u00d7p)(n \\times p)(n\u00d7p)   tensor and out will be a (n\u00d7p)(n \\times p)(n\u00d7p)   tensor. alpha and beta are scaling factors on matrix-vector product between mat1 and mat2 and the added matrix input respectively.  out=\u03b2\u00a0input+\u03b1\u00a0(mat1i@mat2i)\\text{out} = \\beta\\ \\text{input} + \\alpha\\ (\\text{mat1}_i \\mathbin{@} \\text{mat2}_i)  out=\u03b2\u00a0input+\u03b1\u00a0(mat1i\u200b@mat2i\u200b)  For inputs of type FloatTensor or DoubleTensor, arguments beta and alpha must be real numbers, otherwise they should be integers.  Parameters  beta (Number, optional) \u2013 multiplier for input (\u03b2\\beta\u03b2  ) input (Tensor) \u2013 matrix to be added alpha (Number, optional) \u2013 multiplier for mat1@mat2mat1 @ mat2mat1@mat2   (\u03b1\\alpha\u03b1  ) mat1 (Tensor) \u2013 the first matrix to be multiplied mat2 (Tensor) \u2013 the second matrix to be multiplied out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; M = torch.randn(2, 3) &gt;&gt;&gt; mat1 = torch.randn(2, 3) &gt;&gt;&gt; mat2 = torch.randn(3, 3) &gt;&gt;&gt; torch.addmm(M, mat1, mat2) tensor([[-4.8716,  1.4671, -1.3746],         [ 0.7573, -3.9555, -2.8681]])   ", "parameters": ["beta (Number, optional) : multiplier for input (\u03b2\\beta\u03b2)", "input (Tensor) : matrix to be added", "alpha (Number, optional) : multiplier for mat1@mat2mat1 @ mat2mat1@mat2 (\u03b1\\alpha\u03b1)", "mat1 (Tensor) : the first matrix to be multiplied", "mat2 (Tensor) : the second matrix to be multiplied", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " M = torch.randn(2, 3)\n mat1 = torch.randn(2, 3)\n mat2 = torch.randn(3, 3)\n torch.addmm(M, mat1, mat2)\ntensor([[-4.8716,  1.4671, -1.3746],\n        [ 0.7573, -3.9555, -2.8681]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.addmv", "item_type": "function", "code": "torch.addmv(beta=1,input,alpha=1,mat,vec,out=None)\u2192Tensor", "description": "Performs a matrix-vector product of the matrix mat and the vector vec. The vector input is added to the final result. If mat is a (n\u00d7m)(n \\times m)(n\u00d7m)   tensor, vec is a 1-D tensor of size m, then input must be broadcastable with a 1-D tensor of size n and out will be 1-D tensor of size n. alpha and beta are scaling factors on matrix-vector product between mat and vec and the added tensor input respectively.  out=\u03b2\u00a0input+\u03b1\u00a0(mat@vec)\\text{out} = \\beta\\ \\text{input} + \\alpha\\ (\\text{mat} \\mathbin{@} \\text{vec})  out=\u03b2\u00a0input+\u03b1\u00a0(mat@vec)  For inputs of type FloatTensor or DoubleTensor, arguments beta and alpha must be real numbers, otherwise they should be integers  Parameters  beta (Number, optional) \u2013 multiplier for input (\u03b2\\beta\u03b2  ) input (Tensor) \u2013 vector to be added alpha (Number, optional) \u2013 multiplier for mat@vecmat @ vecmat@vec   (\u03b1\\alpha\u03b1  ) mat (Tensor) \u2013 matrix to be multiplied vec (Tensor) \u2013 vector to be multiplied out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; M = torch.randn(2) &gt;&gt;&gt; mat = torch.randn(2, 3) &gt;&gt;&gt; vec = torch.randn(3) &gt;&gt;&gt; torch.addmv(M, mat, vec) tensor([-0.3768, -5.5565])   ", "parameters": ["beta (Number, optional) : multiplier for input (\u03b2\\beta\u03b2)", "input (Tensor) : vector to be added", "alpha (Number, optional) : multiplier for mat@vecmat @ vecmat@vec (\u03b1\\alpha\u03b1)", "mat (Tensor) : matrix to be multiplied", "vec (Tensor) : vector to be multiplied", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " M = torch.randn(2)\n mat = torch.randn(2, 3)\n vec = torch.randn(3)\n torch.addmv(M, mat, vec)\ntensor([-0.3768, -5.5565])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.addr", "item_type": "function", "code": "torch.addr(beta=1,input,alpha=1,vec1,vec2,out=None)\u2192Tensor", "description": "Performs the outer-product of vectors vec1 and vec2 and adds it to the matrix input. Optional values beta and alpha are scaling factors on the outer product between vec1 and vec2 and the added matrix input respectively.  out=\u03b2\u00a0input+\u03b1\u00a0(vec1\u2297vec2)\\text{out} = \\beta\\ \\text{input} + \\alpha\\ (\\text{vec1} \\otimes \\text{vec2})  out=\u03b2\u00a0input+\u03b1\u00a0(vec1\u2297vec2)  If vec1 is a vector of size n and vec2 is a vector of size m, then input must be broadcastable with a matrix of size (n\u00d7m)(n \\times m)(n\u00d7m)   and out will be a matrix of size (n\u00d7m)(n \\times m)(n\u00d7m)  . For inputs of type FloatTensor or DoubleTensor, arguments beta and alpha must be real numbers, otherwise they should be integers  Parameters  beta (Number, optional) \u2013 multiplier for input (\u03b2\\beta\u03b2  ) input (Tensor) \u2013 matrix to be added alpha (Number, optional) \u2013 multiplier for vec1\u2297vec2\\text{vec1} \\otimes \\text{vec2}vec1\u2297vec2   (\u03b1\\alpha\u03b1  ) vec1 (Tensor) \u2013 the first vector of the outer product vec2 (Tensor) \u2013 the second vector of the outer product out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; vec1 = torch.arange(1., 4.) &gt;&gt;&gt; vec2 = torch.arange(1., 3.) &gt;&gt;&gt; M = torch.zeros(3, 2) &gt;&gt;&gt; torch.addr(M, vec1, vec2) tensor([[ 1.,  2.],         [ 2.,  4.],         [ 3.,  6.]])   ", "parameters": ["beta (Number, optional) : multiplier for input (\u03b2\\beta\u03b2)", "input (Tensor) : matrix to be added", "alpha (Number, optional) : multiplier for vec1\u2297vec2\\text{vec1} \\otimes \\text{vec2}vec1\u2297vec2 (\u03b1\\alpha\u03b1)", "vec1 (Tensor) : the first vector of the outer product", "vec2 (Tensor) : the second vector of the outer product", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " vec1 = torch.arange(1., 4.)\n vec2 = torch.arange(1., 3.)\n M = torch.zeros(3, 2)\n torch.addr(M, vec1, vec2)\ntensor([[ 1.,  2.],\n        [ 2.,  4.],\n        [ 3.,  6.]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.GRU", "item_type": "class", "code": "classtorch.nn.GRU(*args,**kwargs)", "description": "Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence. For each element in the input sequence, each layer computes the following function:  rt=\u03c3(Wirxt+bir+Whrh(t\u22121)+bhr)zt=\u03c3(Wizxt+biz+Whzh(t\u22121)+bhz)nt=tanh\u2061(Winxt+bin+rt\u2217(Whnh(t\u22121)+bhn))ht=(1\u2212zt)\u2217nt+zt\u2217h(t\u22121)\\begin{array}{ll}     r_t = \\sigma(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\\\     z_t = \\sigma(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\\\     n_t = \\tanh(W_{in} x_t + b_{in} + r_t * (W_{hn} h_{(t-1)}+ b_{hn})) \\\\     h_t = (1 - z_t) * n_t + z_t * h_{(t-1)} \\end{array}  rt\u200b=\u03c3(Wir\u200bxt\u200b+bir\u200b+Whr\u200bh(t\u22121)\u200b+bhr\u200b)zt\u200b=\u03c3(Wiz\u200bxt\u200b+biz\u200b+Whz\u200bh(t\u22121)\u200b+bhz\u200b)nt\u200b=tanh(Win\u200bxt\u200b+bin\u200b+rt\u200b\u2217(Whn\u200bh(t\u22121)\u200b+bhn\u200b))ht\u200b=(1\u2212zt\u200b)\u2217nt\u200b+zt\u200b\u2217h(t\u22121)\u200b\u200b  where hth_tht\u200b   is the hidden state at time t, xtx_txt\u200b   is the input at time t, h(t\u22121)h_{(t-1)}h(t\u22121)\u200b   is the hidden state of the layer at time t-1 or the initial hidden state at time 0, and rtr_trt\u200b  , ztz_tzt\u200b  , ntn_tnt\u200b   are the reset, update, and new gates, respectively. \u03c3\\sigma\u03c3   is the sigmoid function, and \u2217*\u2217   is the Hadamard product. In a multilayer GRU, the input xt(l)x^{(l)}_txt(l)\u200b   of the lll   -th layer (l&gt;=2l &gt;= 2l&gt;=2  ) is the hidden state ht(l\u22121)h^{(l-1)}_tht(l\u22121)\u200b   of the previous layer multiplied by dropout \u03b4t(l\u22121)\\delta^{(l-1)}_t\u03b4t(l\u22121)\u200b   where each \u03b4t(l\u22121)\\delta^{(l-1)}_t\u03b4t(l\u22121)\u200b   is a Bernoulli random variable which is 000   with probability dropout.  Parameters  input_size \u2013 The number of expected features in the input x hidden_size \u2013 The number of features in the hidden state h num_layers \u2013 Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two GRUs together to form a stacked GRU, with the second GRU taking in outputs of the first GRU and computing the final results. Default: 1 bias \u2013 If False, then the layer does not use bias weights b_ih and b_hh. Default: True batch_first \u2013 If True, then the input and output tensors are provided as (batch, seq, feature). Default: False dropout \u2013 If non-zero, introduces a Dropout layer on the outputs of each GRU layer except the last layer, with dropout probability equal to dropout. Default: 0 bidirectional \u2013 If True, becomes a bidirectional GRU. Default: False     Inputs: input, h_0 input of shape (seq_len, batch, input_size): tensor containing the features of the input sequence. The input can also be a packed variable length sequence. See torch.nn.utils.rnn.pack_padded_sequence() for details. h_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided. If the RNN is bidirectional, num_directions should be 2, else it should be 1.   Outputs: output, h_n output of shape (seq_len, batch, num_directions * hidden_size): tensor containing the output features h_t from the last layer of the GRU, for each t. If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence. For the unpacked case, the directions can be separated using output.view(seq_len, batch, num_directions, hidden_size), with forward and backward being direction 0 and 1 respectively. Similarly, the directions can be separated in the packed case.  h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len Like output, the layers can be separated using h_n.view(num_layers, num_directions, batch, hidden_size).    Shape: Input1: (L,N,Hin)(L, N, H_{in})(L,N,Hin\u200b)   tensor containing input features where Hin=input_sizeH_{in}=\\text{input\\_size}Hin\u200b=input_size   and L represents a sequence length. Input2: (S,N,Hout)(S, N, H_{out})(S,N,Hout\u200b)   tensor containing the initial hidden state for each element in the batch. Hout=hidden_sizeH_{out}=\\text{hidden\\_size}Hout\u200b=hidden_size   Defaults to zero if not provided. where S=num_layers\u2217num_directionsS=\\text{num\\_layers} * \\text{num\\_directions}S=num_layers\u2217num_directions   If the RNN is bidirectional, num_directions should be 2, else it should be 1. Output1: (L,N,Hall)(L, N, H_{all})(L,N,Hall\u200b)   where Hall=num_directions\u2217hidden_sizeH_{all}=\\text{num\\_directions} * \\text{hidden\\_size}Hall\u200b=num_directions\u2217hidden_size   Output2: (S,N,Hout)(S, N, H_{out})(S,N,Hout\u200b)   tensor containing the next hidden state for each element in the batch     Variables  ~GRU.weight_ih_l[k] \u2013 the learnable input-hidden weights of the kth\\text{k}^{th}kth   layer (W_ir|W_iz|W_in), of shape (3*hidden_size, input_size) for k = 0. Otherwise, the shape is (3*hidden_size, num_directions * hidden_size) ~GRU.weight_hh_l[k] \u2013 the learnable hidden-hidden weights of the kth\\text{k}^{th}kth   layer (W_hr|W_hz|W_hn), of shape (3*hidden_size, hidden_size) ~GRU.bias_ih_l[k] \u2013 the learnable input-hidden bias of the kth\\text{k}^{th}kth   layer (b_ir|b_iz|b_in), of shape (3*hidden_size) ~GRU.bias_hh_l[k] \u2013 the learnable hidden-hidden bias of the kth\\text{k}^{th}kth   layer (b_hr|b_hz|b_hn), of shape (3*hidden_size)     Note All the weights and biases are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b)   where k=1hidden_sizek = \\frac{1}{\\text{hidden\\_size}}k=hidden_size1\u200b     Note If the following conditions are satisfied: 1) cudnn is enabled, 2) input data is on the GPU 3) input data has dtype torch.float16 4) V100 GPU is used, 5) input data is not in PackedSequence format persistent algorithm can be selected to improve performance.  Examples: &gt;&gt;&gt; rnn = nn.GRU(10, 20, 2) &gt;&gt;&gt; input = torch.randn(5, 3, 10) &gt;&gt;&gt; h0 = torch.randn(2, 3, 20) &gt;&gt;&gt; output, hn = rnn(input, h0)   ", "parameters": ["input_size : The number of expected features in the input x", "hidden_size : The number of features in the hidden state h", "num_layers : Number of recurrent layers. E.g., setting num_layers=2would mean stacking two GRUs together to form a stacked GRU,with the second GRU taking in outputs of the first GRU andcomputing the final results. Default: 1", "bias : If False, then the layer does not use bias weights b_ih and b_hh.Default: True", "batch_first : If True, then the input and output tensors are providedas (batch, seq, feature). Default: False", "dropout : If non-zero, introduces a Dropout layer on the outputs of eachGRU layer except the last layer, with dropout probability equal todropout. Default: 0", "bidirectional : If True, becomes a bidirectional GRU. Default: False", "~GRU.weight_ih_l[k] : the learnable input-hidden weights of the kth\\text{k}^{th}kth layer(W_ir|W_iz|W_in), of shape (3*hidden_size, input_size) for k = 0.Otherwise, the shape is (3*hidden_size, num_directions * hidden_size)", "~GRU.weight_hh_l[k] : the learnable hidden-hidden weights of the kth\\text{k}^{th}kth layer(W_hr|W_hz|W_hn), of shape (3*hidden_size, hidden_size)", "~GRU.bias_ih_l[k] : the learnable input-hidden bias of the kth\\text{k}^{th}kth layer(b_ir|b_iz|b_in), of shape (3*hidden_size)", "~GRU.bias_hh_l[k] : the learnable hidden-hidden bias of the kth\\text{k}^{th}kth layer(b_hr|b_hz|b_hn), of shape (3*hidden_size)"], "returns": null, "example": " rnn = nn.GRU(10, 20, 2)\n input = torch.randn(5, 3, 10)\n h0 = torch.randn(2, 3, 20)\n output, hn = rnn(input, h0)\n\n", "shape": " Input1: (L,N,Hin)(L, N, H_{in})(L,N,Hin\u200b)   tensor containing input features where Hin=input_sizeH_{in}=\\text{input\\_size}Hin\u200b=input_size   and L represents a sequence length. Input2: (S,N,Hout)(S, N, H_{out})(S,N,Hout\u200b)   tensor containing the initial hidden state for each element in the batch. Hout=hidden_sizeH_{out}=\\text{hidden\\_size}Hout\u200b=hidden_size   Defaults to zero if not provided. where S=num_layers\u2217num_directionsS=\\text{num\\_layers} * \\text{num\\_directions}S=num_layers\u2217num_directions   If the RNN is bidirectional, num_directions should be 2, else it should be 1. Output1: (L,N,Hall)(L, N, H_{all})(L,N,Hall\u200b)   where Hall=num_directions\u2217hidden_sizeH_{all}=\\text{num\\_directions} * \\text{hidden\\_size}Hall\u200b=num_directions\u2217hidden_size   Output2: (S,N,Hout)(S, N, H_{out})(S,N,Hout\u200b)   tensor containing the next hidden state for each element in the batch  "},
{"library": "torch", "item_id": "torch.baddbmm", "item_type": "function", "code": "torch.baddbmm(beta=1,input,alpha=1,batch1,batch2,out=None)\u2192Tensor", "description": "Performs a batch matrix-matrix product of matrices in batch1 and batch2. input is added to the final result. batch1 and batch2 must be 3-D tensors each containing the same number of matrices. If batch1 is a (b\u00d7n\u00d7m)(b \\times n \\times m)(b\u00d7n\u00d7m)   tensor, batch2 is a (b\u00d7m\u00d7p)(b \\times m \\times p)(b\u00d7m\u00d7p)   tensor, then input must be broadcastable with a (b\u00d7n\u00d7p)(b \\times n \\times p)(b\u00d7n\u00d7p)   tensor and out will be a (b\u00d7n\u00d7p)(b \\times n \\times p)(b\u00d7n\u00d7p)   tensor. Both alpha and beta mean the same as the scaling factors used in torch.addbmm().  outi=\u03b2\u00a0inputi+\u03b1\u00a0(batch1i@batch2i)\\text{out}_i = \\beta\\ \\text{input}_i + \\alpha\\ (\\text{batch1}_i \\mathbin{@} \\text{batch2}_i)  outi\u200b=\u03b2\u00a0inputi\u200b+\u03b1\u00a0(batch1i\u200b@batch2i\u200b)  For inputs of type FloatTensor or DoubleTensor, arguments beta and alpha must be real numbers, otherwise they should be integers.  Parameters  beta (Number, optional) \u2013 multiplier for input (\u03b2\\beta\u03b2  ) input (Tensor) \u2013 the tensor to be added alpha (Number, optional) \u2013 multiplier for batch1@batch2\\text{batch1} \\mathbin{@} \\text{batch2}batch1@batch2   (\u03b1\\alpha\u03b1  ) batch1 (Tensor) \u2013 the first batch of matrices to be multiplied batch2 (Tensor) \u2013 the second batch of matrices to be multiplied out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; M = torch.randn(10, 3, 5) &gt;&gt;&gt; batch1 = torch.randn(10, 3, 4) &gt;&gt;&gt; batch2 = torch.randn(10, 4, 5) &gt;&gt;&gt; torch.baddbmm(M, batch1, batch2).size() torch.Size([10, 3, 5])   ", "parameters": ["beta (Number, optional) : multiplier for input (\u03b2\\beta\u03b2)", "input (Tensor) : the tensor to be added", "alpha (Number, optional) : multiplier for batch1@batch2\\text{batch1} \\mathbin{@} \\text{batch2}batch1@batch2 (\u03b1\\alpha\u03b1)", "batch1 (Tensor) : the first batch of matrices to be multiplied", "batch2 (Tensor) : the second batch of matrices to be multiplied", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " M = torch.randn(10, 3, 5)\n batch1 = torch.randn(10, 3, 4)\n batch2 = torch.randn(10, 4, 5)\n torch.baddbmm(M, batch1, batch2).size()\ntorch.Size([10, 3, 5])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.bmm", "item_type": "function", "code": "torch.bmm(input,mat2,out=None)\u2192Tensor", "description": "Performs a batch matrix-matrix product of matrices stored in input and mat2. input and mat2 must be 3-D tensors each containing the same number of matrices. If input is a (b\u00d7n\u00d7m)(b \\times n \\times m)(b\u00d7n\u00d7m)   tensor, mat2 is a (b\u00d7m\u00d7p)(b \\times m \\times p)(b\u00d7m\u00d7p)   tensor, out will be a (b\u00d7n\u00d7p)(b \\times n \\times p)(b\u00d7n\u00d7p)   tensor.  outi=inputi@mat2i\\text{out}_i = \\text{input}_i \\mathbin{@} \\text{mat2}_i  outi\u200b=inputi\u200b@mat2i\u200b   Note This function does not broadcast. For broadcasting matrix products, see torch.matmul().   Parameters  input (Tensor) \u2013 the first batch of matrices to be multiplied mat2 (Tensor) \u2013 the second batch of matrices to be multiplied out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; input = torch.randn(10, 3, 4) &gt;&gt;&gt; mat2 = torch.randn(10, 4, 5) &gt;&gt;&gt; res = torch.bmm(input, mat2) &gt;&gt;&gt; res.size() torch.Size([10, 3, 5])   ", "parameters": ["input (Tensor) : the first batch of matrices to be multiplied", "mat2 (Tensor) : the second batch of matrices to be multiplied", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " input = torch.randn(10, 3, 4)\n mat2 = torch.randn(10, 4, 5)\n res = torch.bmm(input, mat2)\n res.size()\ntorch.Size([10, 3, 5])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.chain_matmul", "item_type": "function", "code": "torch.chain_matmul(*matrices)", "description": "Returns the matrix product of the NNN   2-D tensors. This product is efficiently computed using the matrix chain order algorithm which selects the order in which incurs the lowest cost in terms of arithmetic operations ([CLRS]). Note that since this is a function to compute the product, NNN   needs to be greater than or equal to 2; if equal to 2 then a trivial matrix-matrix product is returned. If NNN   is 1, then this is a no-op - the original matrix is returned as is.  Parameters matrices (Tensors...) \u2013 a sequence of 2 or more 2-D tensors whose product is to be determined.  Returns if the ithi^{th}ith   tensor was of dimensions pi\u00d7pi+1p_{i} \\times p_{i + 1}pi\u200b\u00d7pi+1\u200b  , then the product would be of dimensions p1\u00d7pN+1p_{1} \\times p_{N + 1}p1\u200b\u00d7pN+1\u200b  .  Return type Tensor   Example: &gt;&gt;&gt; a = torch.randn(3, 4) &gt;&gt;&gt; b = torch.randn(4, 5) &gt;&gt;&gt; c = torch.randn(5, 6) &gt;&gt;&gt; d = torch.randn(6, 7) &gt;&gt;&gt; torch.chain_matmul(a, b, c, d) tensor([[ -2.3375,  -3.9790,  -4.1119,  -6.6577,   9.5609, -11.5095,  -3.2614],         [ 21.4038,   3.3378,  -8.4982,  -5.2457, -10.2561,  -2.4684,   2.7163],         [ -0.9647,  -5.8917,  -2.3213,  -5.2284,  12.8615, -12.2816,  -2.5095]])   ", "parameters": ["matrices (Tensors...) : a sequence of 2 or more 2-D tensors whose product is to be determined.", "if the ithi^{th}ith tensor was of dimensions pi\u00d7pi+1p_{i} \\times p_{i + 1}pi\u200b\u00d7pi+1\u200b, then the productwould be of dimensions p1\u00d7pN+1p_{1} \\times p_{N + 1}p1\u200b\u00d7pN+1\u200b.", "Tensor"], "returns": "if the ithi^{th}ith tensor was of dimensions pi\u00d7pi+1p_{i} \\times p_{i + 1}pi\u200b\u00d7pi+1\u200b, then the productwould be of dimensions p1\u00d7pN+1p_{1} \\times p_{N + 1}p1\u200b\u00d7pN+1\u200b.", "example": " a = torch.randn(3, 4)\n b = torch.randn(4, 5)\n c = torch.randn(5, 6)\n d = torch.randn(6, 7)\n torch.chain_matmul(a, b, c, d)\ntensor([[ -2.3375,  -3.9790,  -4.1119,  -6.6577,   9.5609, -11.5095,  -3.2614],\n        [ 21.4038,   3.3378,  -8.4982,  -5.2457, -10.2561,  -2.4684,   2.7163],\n        [ -0.9647,  -5.8917,  -2.3213,  -5.2284,  12.8615, -12.2816,  -2.5095]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.RNNCell", "item_type": "class", "code": "classtorch.nn.RNNCell(input_size,hidden_size,bias=True,nonlinearity='tanh')", "description": "An Elman RNN cell with tanh or ReLU non-linearity.  h\u2032=tanh\u2061(Wihx+bih+Whhh+bhh)h' = \\tanh(W_{ih} x + b_{ih}  +  W_{hh} h + b_{hh})h\u2032=tanh(Wih\u200bx+bih\u200b+Whh\u200bh+bhh\u200b)  If nonlinearity is \u2018relu\u2019, then ReLU is used in place of tanh.  Parameters  input_size \u2013 The number of expected features in the input x hidden_size \u2013 The number of features in the hidden state h bias \u2013 If False, then the layer does not use bias weights b_ih and b_hh. Default: True nonlinearity \u2013 The non-linearity to use. Can be either 'tanh' or 'relu'. Default: 'tanh'     Inputs: input, hidden input of shape (batch, input_size): tensor containing input features hidden of shape (batch, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided.   Outputs: h\u2019 h\u2019 of shape (batch, hidden_size): tensor containing the next hidden state for each element in the batch   Shape: Input1: (N,Hin)(N, H_{in})(N,Hin\u200b)   tensor containing input features where HinH_{in}Hin\u200b   = input_size Input2: (N,Hout)(N, H_{out})(N,Hout\u200b)   tensor containing the initial hidden state for each element in the batch where HoutH_{out}Hout\u200b   = hidden_size Defaults to zero if not provided. Output: (N,Hout)(N, H_{out})(N,Hout\u200b)   tensor containing the next hidden state for each element in the batch     Variables  ~RNNCell.weight_ih \u2013 the learnable input-hidden weights, of shape (hidden_size, input_size) ~RNNCell.weight_hh \u2013 the learnable hidden-hidden weights, of shape (hidden_size, hidden_size) ~RNNCell.bias_ih \u2013 the learnable input-hidden bias, of shape (hidden_size) ~RNNCell.bias_hh \u2013 the learnable hidden-hidden bias, of shape (hidden_size)     Note All the weights and biases are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b)   where k=1hidden_sizek = \\frac{1}{\\text{hidden\\_size}}k=hidden_size1\u200b    Examples: &gt;&gt;&gt; rnn = nn.RNNCell(10, 20) &gt;&gt;&gt; input = torch.randn(6, 3, 10) &gt;&gt;&gt; hx = torch.randn(3, 20) &gt;&gt;&gt; output = [] &gt;&gt;&gt; for i in range(6):         hx = rnn(input[i], hx)         output.append(hx)   ", "parameters": ["input_size : The number of expected features in the input x", "hidden_size : The number of features in the hidden state h", "bias : If False, then the layer does not use bias weights b_ih and b_hh.Default: True", "nonlinearity : The non-linearity to use. Can be either 'tanh' or 'relu'. Default: 'tanh'", "~RNNCell.weight_ih : the learnable input-hidden weights, of shape(hidden_size, input_size)", "~RNNCell.weight_hh : the learnable hidden-hidden weights, of shape(hidden_size, hidden_size)", "~RNNCell.bias_ih : the learnable input-hidden bias, of shape (hidden_size)", "~RNNCell.bias_hh : the learnable hidden-hidden bias, of shape (hidden_size)"], "returns": null, "example": " rnn = nn.RNNCell(10, 20)\n input = torch.randn(6, 3, 10)\n hx = torch.randn(3, 20)\n output = []\n for i in range(6):\n        hx = rnn(input[i], hx)\n        output.append(hx)\n\n", "shape": " Input1: (N,Hin)(N, H_{in})(N,Hin\u200b)   tensor containing input features where HinH_{in}Hin\u200b   = input_size Input2: (N,Hout)(N, H_{out})(N,Hout\u200b)   tensor containing the initial hidden state for each element in the batch where HoutH_{out}Hout\u200b   = hidden_size Defaults to zero if not provided. Output: (N,Hout)(N, H_{out})(N,Hout\u200b)   tensor containing the next hidden state for each element in the batch  "},
{"library": "torch", "item_id": "torch.nn.LSTMCell", "item_type": "class", "code": "classtorch.nn.LSTMCell(input_size,hidden_size,bias=True)", "description": "A long short-term memory (LSTM) cell.  i=\u03c3(Wiix+bii+Whih+bhi)f=\u03c3(Wifx+bif+Whfh+bhf)g=tanh\u2061(Wigx+big+Whgh+bhg)o=\u03c3(Wiox+bio+Whoh+bho)c\u2032=f\u2217c+i\u2217gh\u2032=o\u2217tanh\u2061(c\u2032)\\begin{array}{ll} i = \\sigma(W_{ii} x + b_{ii} + W_{hi} h + b_{hi}) \\\\ f = \\sigma(W_{if} x + b_{if} + W_{hf} h + b_{hf}) \\\\ g = \\tanh(W_{ig} x + b_{ig} + W_{hg} h + b_{hg}) \\\\ o = \\sigma(W_{io} x + b_{io} + W_{ho} h + b_{ho}) \\\\ c' = f * c + i * g \\\\ h' = o * \\tanh(c') \\\\ \\end{array}i=\u03c3(Wii\u200bx+bii\u200b+Whi\u200bh+bhi\u200b)f=\u03c3(Wif\u200bx+bif\u200b+Whf\u200bh+bhf\u200b)g=tanh(Wig\u200bx+big\u200b+Whg\u200bh+bhg\u200b)o=\u03c3(Wio\u200bx+bio\u200b+Who\u200bh+bho\u200b)c\u2032=f\u2217c+i\u2217gh\u2032=o\u2217tanh(c\u2032)\u200b  where \u03c3\\sigma\u03c3   is the sigmoid function, and \u2217*\u2217   is the Hadamard product.  Parameters  input_size \u2013 The number of expected features in the input x hidden_size \u2013 The number of features in the hidden state h bias \u2013 If False, then the layer does not use bias weights b_ih and b_hh. Default: True     Inputs: input, (h_0, c_0) input of shape (batch, input_size): tensor containing input features h_0 of shape (batch, hidden_size): tensor containing the initial hidden state for each element in the batch. c_0 of shape (batch, hidden_size): tensor containing the initial cell state for each element in the batch. If (h_0, c_0) is not provided, both h_0 and c_0 default to zero.    Outputs: (h_1, c_1) h_1 of shape (batch, hidden_size): tensor containing the next hidden state for each element in the batch c_1 of shape (batch, hidden_size): tensor containing the next cell state for each element in the batch     Variables  ~LSTMCell.weight_ih \u2013 the learnable input-hidden weights, of shape (4*hidden_size, input_size) ~LSTMCell.weight_hh \u2013 the learnable hidden-hidden weights, of shape (4*hidden_size, hidden_size) ~LSTMCell.bias_ih \u2013 the learnable input-hidden bias, of shape (4*hidden_size) ~LSTMCell.bias_hh \u2013 the learnable hidden-hidden bias, of shape (4*hidden_size)     Note All the weights and biases are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b)   where k=1hidden_sizek = \\frac{1}{\\text{hidden\\_size}}k=hidden_size1\u200b    Examples: &gt;&gt;&gt; rnn = nn.LSTMCell(10, 20) &gt;&gt;&gt; input = torch.randn(6, 3, 10) &gt;&gt;&gt; hx = torch.randn(3, 20) &gt;&gt;&gt; cx = torch.randn(3, 20) &gt;&gt;&gt; output = [] &gt;&gt;&gt; for i in range(6):         hx, cx = rnn(input[i], (hx, cx))         output.append(hx)   ", "parameters": ["input_size : The number of expected features in the input x", "hidden_size : The number of features in the hidden state h", "bias : If False, then the layer does not use bias weights b_ih andb_hh. Default: True", "~LSTMCell.weight_ih : the learnable input-hidden weights, of shape(4*hidden_size, input_size)", "~LSTMCell.weight_hh : the learnable hidden-hidden weights, of shape(4*hidden_size, hidden_size)", "~LSTMCell.bias_ih : the learnable input-hidden bias, of shape (4*hidden_size)", "~LSTMCell.bias_hh : the learnable hidden-hidden bias, of shape (4*hidden_size)"], "returns": null, "example": " rnn = nn.LSTMCell(10, 20)\n input = torch.randn(6, 3, 10)\n hx = torch.randn(3, 20)\n cx = torch.randn(3, 20)\n output = []\n for i in range(6):\n        hx, cx = rnn(input[i], (hx, cx))\n        output.append(hx)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.cholesky", "item_type": "function", "code": "torch.cholesky(input,upper=False,out=None)\u2192Tensor", "description": "Computes the Cholesky decomposition of a symmetric positive-definite matrix AAA   or for batches of symmetric positive-definite matrices. If upper is True, the returned matrix U is upper-triangular, and the decomposition has the form:  A=UTUA = U^TUA=UTU  If upper is False, the returned matrix L is lower-triangular, and the decomposition has the form:  A=LLTA = LL^TA=LLT  If upper is True, and AAA   is a batch of symmetric positive-definite matrices, then the returned tensor will be composed of upper-triangular Cholesky factors of each of the individual matrices. Similarly, when upper is False, the returned tensor will be composed of lower-triangular Cholesky factors of each of the individual matrices.  Parameters  input (Tensor) \u2013 the input tensor AAA   of size (\u2217,n,n)(*, n, n)(\u2217,n,n)   where * is zero or more batch dimensions consisting of symmetric positive-definite matrices. upper (bool, optional) \u2013 flag that indicates whether to return a upper or lower triangular matrix. Default: False out (Tensor, optional) \u2013 the output matrix    Example: &gt;&gt;&gt; a = torch.randn(3, 3) &gt;&gt;&gt; a = torch.mm(a, a.t()) # make symmetric positive-definite &gt;&gt;&gt; l = torch.cholesky(a) &gt;&gt;&gt; a tensor([[ 2.4112, -0.7486,  1.4551],         [-0.7486,  1.3544,  0.1294],         [ 1.4551,  0.1294,  1.6724]]) &gt;&gt;&gt; l tensor([[ 1.5528,  0.0000,  0.0000],         [-0.4821,  1.0592,  0.0000],         [ 0.9371,  0.5487,  0.7023]]) &gt;&gt;&gt; torch.mm(l, l.t()) tensor([[ 2.4112, -0.7486,  1.4551],         [-0.7486,  1.3544,  0.1294],         [ 1.4551,  0.1294,  1.6724]]) &gt;&gt;&gt; a = torch.randn(3, 2, 2) &gt;&gt;&gt; a = torch.matmul(a, a.transpose(-1, -2)) + 1e-03 # make symmetric positive-definite &gt;&gt;&gt; l = torch.cholesky(a) &gt;&gt;&gt; z = torch.matmul(l, l.transpose(-1, -2)) &gt;&gt;&gt; torch.max(torch.abs(z - a)) # Max non-zero tensor(2.3842e-07)   ", "parameters": ["input (Tensor) : the input tensor AAA of size (\u2217,n,n)(*, n, n)(\u2217,n,n) where * is zero or morebatch dimensions consisting of symmetric positive-definite matrices.", "upper (bool, optional) : flag that indicates whether to return aupper or lower triangular matrix. Default: False", "out (Tensor, optional) : the output matrix"], "returns": null, "example": " a = torch.randn(3, 3)\n a = torch.mm(a, a.t()) # make symmetric positive-definite\n l = torch.cholesky(a)\n a\ntensor([[ 2.4112, -0.7486,  1.4551],\n        [-0.7486,  1.3544,  0.1294],\n        [ 1.4551,  0.1294,  1.6724]])\n l\ntensor([[ 1.5528,  0.0000,  0.0000],\n        [-0.4821,  1.0592,  0.0000],\n        [ 0.9371,  0.5487,  0.7023]])\n torch.mm(l, l.t())\ntensor([[ 2.4112, -0.7486,  1.4551],\n        [-0.7486,  1.3544,  0.1294],\n        [ 1.4551,  0.1294,  1.6724]])\n a = torch.randn(3, 2, 2)\n a = torch.matmul(a, a.transpose(-1, -2)) + 1e-03 # make symmetric positive-definite\n l = torch.cholesky(a)\n z = torch.matmul(l, l.transpose(-1, -2))\n torch.max(torch.abs(z - a)) # Max non-zero\ntensor(2.3842e-07)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.cholesky_inverse", "item_type": "function", "code": "torch.cholesky_inverse(input,upper=False,out=None)\u2192Tensor", "description": "Computes the inverse of a symmetric positive-definite matrix AAA   using its Cholesky factor uuu  : returns matrix inv. The inverse is computed using LAPACK routines dpotri and spotri (and the corresponding MAGMA routines). If upper is False, uuu   is lower triangular such that the returned tensor is  inv=(uuT)\u22121inv = (uu^{{T}})^{{-1}}  inv=(uuT)\u22121  If upper is True or not provided, uuu   is upper triangular such that the returned tensor is  inv=(uTu)\u22121inv = (u^T u)^{{-1}}  inv=(uTu)\u22121   Parameters  input (Tensor) \u2013 the input 2-D tensor uuu  , a upper or lower triangular Cholesky factor upper (bool, optional) \u2013 whether to return a lower (default) or upper triangular matrix out (Tensor, optional) \u2013 the output tensor for inv    Example: &gt;&gt;&gt; a = torch.randn(3, 3) &gt;&gt;&gt; a = torch.mm(a, a.t()) + 1e-05 * torch.eye(3) # make symmetric positive definite &gt;&gt;&gt; u = torch.cholesky(a) &gt;&gt;&gt; a tensor([[  0.9935,  -0.6353,   1.5806],         [ -0.6353,   0.8769,  -1.7183],         [  1.5806,  -1.7183,  10.6618]]) &gt;&gt;&gt; torch.cholesky_inverse(u) tensor([[ 1.9314,  1.2251, -0.0889],         [ 1.2251,  2.4439,  0.2122],         [-0.0889,  0.2122,  0.1412]]) &gt;&gt;&gt; a.inverse() tensor([[ 1.9314,  1.2251, -0.0889],         [ 1.2251,  2.4439,  0.2122],         [-0.0889,  0.2122,  0.1412]])   ", "parameters": ["input (Tensor) : the input 2-D tensor uuu, a upper or lower triangularCholesky factor", "upper (bool, optional) : whether to return a lower (default) or upper triangular matrix", "out (Tensor, optional) : the output tensor for inv"], "returns": null, "example": " a = torch.randn(3, 3)\n a = torch.mm(a, a.t()) + 1e-05 * torch.eye(3) # make symmetric positive definite\n u = torch.cholesky(a)\n a\ntensor([[  0.9935,  -0.6353,   1.5806],\n        [ -0.6353,   0.8769,  -1.7183],\n        [  1.5806,  -1.7183,  10.6618]])\n torch.cholesky_inverse(u)\ntensor([[ 1.9314,  1.2251, -0.0889],\n        [ 1.2251,  2.4439,  0.2122],\n        [-0.0889,  0.2122,  0.1412]])\n a.inverse()\ntensor([[ 1.9314,  1.2251, -0.0889],\n        [ 1.2251,  2.4439,  0.2122],\n        [-0.0889,  0.2122,  0.1412]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.cholesky_solve", "item_type": "function", "code": "torch.cholesky_solve(input,input2,upper=False,out=None)\u2192Tensor", "description": "Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix uuu  . If upper is False, uuu   is and lower triangular and c is returned such that:  c=(uuT)\u22121bc = (u u^T)^{{-1}} b  c=(uuT)\u22121b  If upper is True or not provided, uuu   is upper triangular and c is returned such that:  c=(uTu)\u22121bc = (u^T u)^{{-1}} b  c=(uTu)\u22121b  torch.cholesky_solve(b, u) can take in 2D inputs b, u or inputs that are batches of 2D matrices. If the inputs are batches, then returns batched outputs c  Parameters  input (Tensor) \u2013 input matrix bbb   of size (\u2217,m,k)(*, m, k)(\u2217,m,k)  , where \u2217*\u2217   is zero or more batch dimensions input2 (Tensor) \u2013 input matrix uuu   of size (\u2217,m,m)(*, m, m)(\u2217,m,m)  , where \u2217*\u2217   is zero of more batch dimensions composed of upper or lower triangular Cholesky factor upper (bool, optional) \u2013 whether to consider the Cholesky factor as a lower or upper triangular matrix. Default: False. out (Tensor, optional) \u2013 the output tensor for c    Example: &gt;&gt;&gt; a = torch.randn(3, 3) &gt;&gt;&gt; a = torch.mm(a, a.t()) # make symmetric positive definite &gt;&gt;&gt; u = torch.cholesky(a) &gt;&gt;&gt; a tensor([[ 0.7747, -1.9549,  1.3086],         [-1.9549,  6.7546, -5.4114],         [ 1.3086, -5.4114,  4.8733]]) &gt;&gt;&gt; b = torch.randn(3, 2) &gt;&gt;&gt; b tensor([[-0.6355,  0.9891],         [ 0.1974,  1.4706],         [-0.4115, -0.6225]]) &gt;&gt;&gt; torch.cholesky_solve(b, u) tensor([[ -8.1625,  19.6097],         [ -5.8398,  14.2387],         [ -4.3771,  10.4173]]) &gt;&gt;&gt; torch.mm(a.inverse(), b) tensor([[ -8.1626,  19.6097],         [ -5.8398,  14.2387],         [ -4.3771,  10.4173]])   ", "parameters": ["input (Tensor) : input matrix bbb of size (\u2217,m,k)(*, m, k)(\u2217,m,k),where \u2217*\u2217 is zero or more batch dimensions", "input2 (Tensor) : input matrix uuu of size (\u2217,m,m)(*, m, m)(\u2217,m,m),where \u2217*\u2217 is zero of more batch dimensions composed ofupper or lower triangular Cholesky factor", "upper (bool, optional) : whether to consider the Cholesky factor as alower or upper triangular matrix. Default: False.", "out (Tensor, optional) : the output tensor for c"], "returns": null, "example": " a = torch.randn(3, 3)\n a = torch.mm(a, a.t()) # make symmetric positive definite\n u = torch.cholesky(a)\n a\ntensor([[ 0.7747, -1.9549,  1.3086],\n        [-1.9549,  6.7546, -5.4114],\n        [ 1.3086, -5.4114,  4.8733]])\n b = torch.randn(3, 2)\n b\ntensor([[-0.6355,  0.9891],\n        [ 0.1974,  1.4706],\n        [-0.4115, -0.6225]])\n torch.cholesky_solve(b, u)\ntensor([[ -8.1625,  19.6097],\n        [ -5.8398,  14.2387],\n        [ -4.3771,  10.4173]])\n torch.mm(a.inverse(), b)\ntensor([[ -8.1626,  19.6097],\n        [ -5.8398,  14.2387],\n        [ -4.3771,  10.4173]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.GRUCell", "item_type": "class", "code": "classtorch.nn.GRUCell(input_size,hidden_size,bias=True)", "description": "A gated recurrent unit (GRU) cell  r=\u03c3(Wirx+bir+Whrh+bhr)z=\u03c3(Wizx+biz+Whzh+bhz)n=tanh\u2061(Winx+bin+r\u2217(Whnh+bhn))h\u2032=(1\u2212z)\u2217n+z\u2217h\\begin{array}{ll} r = \\sigma(W_{ir} x + b_{ir} + W_{hr} h + b_{hr}) \\\\ z = \\sigma(W_{iz} x + b_{iz} + W_{hz} h + b_{hz}) \\\\ n = \\tanh(W_{in} x + b_{in} + r * (W_{hn} h + b_{hn})) \\\\ h' = (1 - z) * n + z * h \\end{array}r=\u03c3(Wir\u200bx+bir\u200b+Whr\u200bh+bhr\u200b)z=\u03c3(Wiz\u200bx+biz\u200b+Whz\u200bh+bhz\u200b)n=tanh(Win\u200bx+bin\u200b+r\u2217(Whn\u200bh+bhn\u200b))h\u2032=(1\u2212z)\u2217n+z\u2217h\u200b  where \u03c3\\sigma\u03c3   is the sigmoid function, and \u2217*\u2217   is the Hadamard product.  Parameters  input_size \u2013 The number of expected features in the input x hidden_size \u2013 The number of features in the hidden state h bias \u2013 If False, then the layer does not use bias weights b_ih and b_hh. Default: True     Inputs: input, hidden input of shape (batch, input_size): tensor containing input features hidden of shape (batch, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided.   Outputs: h\u2019 h\u2019 of shape (batch, hidden_size): tensor containing the next hidden state for each element in the batch   Shape: Input1: (N,Hin)(N, H_{in})(N,Hin\u200b)   tensor containing input features where HinH_{in}Hin\u200b   = input_size Input2: (N,Hout)(N, H_{out})(N,Hout\u200b)   tensor containing the initial hidden state for each element in the batch where HoutH_{out}Hout\u200b   = hidden_size Defaults to zero if not provided. Output: (N,Hout)(N, H_{out})(N,Hout\u200b)   tensor containing the next hidden state for each element in the batch     Variables  ~GRUCell.weight_ih \u2013 the learnable input-hidden weights, of shape (3*hidden_size, input_size) ~GRUCell.weight_hh \u2013 the learnable hidden-hidden weights, of shape (3*hidden_size, hidden_size) ~GRUCell.bias_ih \u2013 the learnable input-hidden bias, of shape (3*hidden_size) ~GRUCell.bias_hh \u2013 the learnable hidden-hidden bias, of shape (3*hidden_size)     Note All the weights and biases are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b)   where k=1hidden_sizek = \\frac{1}{\\text{hidden\\_size}}k=hidden_size1\u200b    Examples: &gt;&gt;&gt; rnn = nn.GRUCell(10, 20) &gt;&gt;&gt; input = torch.randn(6, 3, 10) &gt;&gt;&gt; hx = torch.randn(3, 20) &gt;&gt;&gt; output = [] &gt;&gt;&gt; for i in range(6):         hx = rnn(input[i], hx)         output.append(hx)   ", "parameters": ["input_size : The number of expected features in the input x", "hidden_size : The number of features in the hidden state h", "bias : If False, then the layer does not use bias weights b_ih andb_hh. Default: True", "~GRUCell.weight_ih : the learnable input-hidden weights, of shape(3*hidden_size, input_size)", "~GRUCell.weight_hh : the learnable hidden-hidden weights, of shape(3*hidden_size, hidden_size)", "~GRUCell.bias_ih : the learnable input-hidden bias, of shape (3*hidden_size)", "~GRUCell.bias_hh : the learnable hidden-hidden bias, of shape (3*hidden_size)"], "returns": null, "example": " rnn = nn.GRUCell(10, 20)\n input = torch.randn(6, 3, 10)\n hx = torch.randn(3, 20)\n output = []\n for i in range(6):\n        hx = rnn(input[i], hx)\n        output.append(hx)\n\n", "shape": " Input1: (N,Hin)(N, H_{in})(N,Hin\u200b)   tensor containing input features where HinH_{in}Hin\u200b   = input_size Input2: (N,Hout)(N, H_{out})(N,Hout\u200b)   tensor containing the initial hidden state for each element in the batch where HoutH_{out}Hout\u200b   = hidden_size Defaults to zero if not provided. Output: (N,Hout)(N, H_{out})(N,Hout\u200b)   tensor containing the next hidden state for each element in the batch  "},
{"library": "torch", "item_id": "torch.nn.Transformer", "item_type": "class", "code": "classtorch.nn.Transformer(d_model=512,nhead=8,num_encoder_layers=6,num_decoder_layers=6,dim_feedforward=2048,dropout=0.1,activation='relu',custom_encoder=None,custom_decoder=None)", "description": "A transformer model. User is able to modify the attributes as needed. The architecture is based on the paper \u201cAttention Is All You Need\u201d. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users can build the BERT(https://arxiv.org/abs/1810.04805) model with corresponding parameters.  Parameters  d_model \u2013 the number of expected features in the encoder/decoder inputs (default=512). nhead \u2013 the number of heads in the multiheadattention models (default=8). num_encoder_layers \u2013 the number of sub-encoder-layers in the encoder (default=6). num_decoder_layers \u2013 the number of sub-decoder-layers in the decoder (default=6). dim_feedforward \u2013 the dimension of the feedforward network model (default=2048). dropout \u2013 the dropout value (default=0.1). activation \u2013 the activation function of encoder/decoder intermediate layer, relu or gelu (default=relu). custom_encoder \u2013 custom encoder (default=None). custom_decoder \u2013 custom decoder (default=None).     Examples::&gt;&gt;&gt; transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12) &gt;&gt;&gt; src = torch.rand((10, 32, 512)) &gt;&gt;&gt; tgt = torch.rand((20, 32, 512)) &gt;&gt;&gt; out = transformer_model(src, tgt)     Note: A full example to apply nn.Transformer module for the word language model is available in https://github.com/pytorch/examples/tree/master/word_language_model   forward(src, tgt, src_mask=None, tgt_mask=None, memory_mask=None, src_key_padding_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None)  Take in and process masked source/target sequences.  Parameters  src \u2013 the sequence to the encoder (required). tgt \u2013 the sequence to the decoder (required). src_mask \u2013 the additive mask for the src sequence (optional). tgt_mask \u2013 the additive mask for the tgt sequence (optional). memory_mask \u2013 the additive mask for the encoder output (optional). src_key_padding_mask \u2013 the ByteTensor mask for src keys per batch (optional). tgt_key_padding_mask \u2013 the ByteTensor mask for tgt keys per batch (optional). memory_key_padding_mask \u2013 the ByteTensor mask for memory keys per batch (optional).     Shape: src: (S,N,E)(S, N, E)(S,N,E)  . tgt: (T,N,E)(T, N, E)(T,N,E)  . src_mask: (S,S)(S, S)(S,S)  . tgt_mask: (T,T)(T, T)(T,T)  . memory_mask: (T,S)(T, S)(T,S)  . src_key_padding_mask: (N,S)(N, S)(N,S)  . tgt_key_padding_mask: (N,T)(N, T)(N,T)  . memory_key_padding_mask: (N,S)(N, S)(N,S)  .  Note: [src/tgt/memory]_mask should be filled with float(\u2018-inf\u2019) for the masked positions and float(0.0) else. These masks ensure that predictions for position i depend only on the unmasked positions j and are applied identically for each sequence in a batch. [src/tgt/memory]_key_padding_mask should be a ByteTensor where True values are positions that should be masked with float(\u2018-inf\u2019) and False values will be unchanged. This mask ensures that no information will be taken from position i if it is masked, and has a separate mask for each sequence in a batch.  output: (T,N,E)(T, N, E)(T,N,E)  .  Note: Due to the multi-head attention architecture in the transformer model, the output sequence length of a transformer is same as the input sequence (i.e. target) length of the decode. where S is the source sequence length, T is the target sequence length, N is the batch size, E is the feature number   Examples &gt;&gt;&gt; output = transformer_model(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)       generate_square_subsequent_mask(sz)  Generate a square mask for the sequence. The masked positions are filled with float(\u2018-inf\u2019). Unmasked positions are filled with float(0.0).   ", "parameters": ["d_model : the number of expected features in the encoder/decoder inputs (default=512).", "nhead : the number of heads in the multiheadattention models (default=8).", "num_encoder_layers : the number of sub-encoder-layers in the encoder (default=6).", "num_decoder_layers : the number of sub-decoder-layers in the decoder (default=6).", "dim_feedforward : the dimension of the feedforward network model (default=2048).", "dropout : the dropout value (default=0.1).", "activation : the activation function of encoder/decoder intermediate layer, relu or gelu (default=relu).", "custom_encoder : custom encoder (default=None).", "custom_decoder : custom decoder (default=None).", "src : the sequence to the encoder (required).", "tgt : the sequence to the decoder (required).", "src_mask : the additive mask for the src sequence (optional).", "tgt_mask : the additive mask for the tgt sequence (optional).", "memory_mask : the additive mask for the encoder output (optional).", "src_key_padding_mask : the ByteTensor mask for src keys per batch (optional).", "tgt_key_padding_mask : the ByteTensor mask for tgt keys per batch (optional).", "memory_key_padding_mask : the ByteTensor mask for memory keys per batch (optional)."], "returns": null, "example": " transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)\n src = torch.rand((10, 32, 512))\n tgt = torch.rand((20, 32, 512))\n out = transformer_model(src, tgt)\n\n\n", "shape": " src: (S,N,E)(S, N, E)(S,N,E)  . tgt: (T,N,E)(T, N, E)(T,N,E)  . src_mask: (S,S)(S, S)(S,S)  . tgt_mask: (T,T)(T, T)(T,T)  . memory_mask: (T,S)(T, S)(T,S)  . src_key_padding_mask: (N,S)(N, S)(N,S)  . tgt_key_padding_mask: (N,T)(N, T)(N,T)  . memory_key_padding_mask: (N,S)(N, S)(N,S)  .  Note: [src/tgt/memory]_mask should be filled with float(\u2018-inf\u2019) for the masked positions and float(0.0) else. These masks ensure that predictions for position i depend only on the unmasked positions j and are applied identically for each sequence in a batch. [src/tgt/memory]_key_padding_mask should be a ByteTensor where True values are positions that should be masked with float(\u2018-inf\u2019) and False values will be unchanged. This mask ensures that no information will be taken from position i if it is masked, and has a separate mask for each sequence in a batch.  output: (T,N,E)(T, N, E)(T,N,E)  .  Note: Due to the multi-head attention architecture in the transformer model, the output sequence length of a transformer is same as the input sequence (i.e. target) length of the decode. where S is the source sequence length, T is the target sequence length, N is the batch size, E is the feature number "},
{"library": "torch", "item_id": "torch.dot", "item_type": "function", "code": "torch.dot(input,tensor)\u2192Tensor", "description": "Computes the dot product (inner product) of two tensors.  Note This function does not broadcast.  Example: &gt;&gt;&gt; torch.dot(torch.tensor([2, 3]), torch.tensor([2, 1])) tensor(7)   ", "parameters": [], "returns": null, "example": " torch.dot(torch.tensor([2, 3]), torch.tensor([2, 1]))\ntensor(7)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.eig", "item_type": "function", "code": "torch.eig(input,eigenvectors=False,out=None)-&gt;(Tensor,Tensor)", "description": "Computes the eigenvalues and eigenvectors of a real square matrix.  Note Since eigenvalues and eigenvectors might be complex, backward pass is supported only for torch.symeig()   Parameters  input (Tensor) \u2013 the square matrix of shape (n\u00d7n)(n \\times n)(n\u00d7n)   for which the eigenvalues and eigenvectors will be computed eigenvectors (bool) \u2013 True to compute both eigenvalues and eigenvectors; otherwise, only eigenvalues will be computed out (tuple, optional) \u2013 the output tensors   Returns A namedtuple (eigenvalues, eigenvectors) containing   eigenvalues (Tensor): Shape (n\u00d72)(n \\times 2)(n\u00d72)  . Each row is an eigenvalue of input, where the first element is the real part and the second element is the imaginary part. The eigenvalues are not necessarily ordered. eigenvectors (Tensor): If eigenvectors=False, it\u2019s an empty tensor. Otherwise, this tensor of shape (n\u00d7n)(n \\times n)(n\u00d7n)   can be used to compute normalized (unit length) eigenvectors of corresponding eigenvalues as follows. If the corresponding eigenvalues[j] is a real number, column eigenvectors[:, j] is the eigenvector corresponding to eigenvalues[j]. If the corresponding eigenvalues[j] and eigenvalues[j + 1] form a complex conjugate pair, then the true eigenvectors can be computed as true\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j] = eigenvectors[:, j] + i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1]  , true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j + 1] = eigenvectors[:, j] - i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]  .     Return type (Tensor, Tensor)   ", "parameters": ["input (Tensor) : the square matrix of shape (n\u00d7n)(n \\times n)(n\u00d7n) for which the eigenvalues and eigenvectorswill be computed", "eigenvectors (bool) : True to compute both eigenvalues and eigenvectors;otherwise, only eigenvalues will be computed", "out (tuple, optional) : the output tensors", "eigenvalues (Tensor): Shape (n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue of input,where the first element is the real part and the second element is the imaginary part.The eigenvalues are not necessarily ordered.", "eigenvectors (Tensor): If eigenvectors=False, it\u2019s an empty tensor.Otherwise, this tensor of shape (n\u00d7n)(n \\times n)(n\u00d7n) can be used to compute normalized (unit length)eigenvectors of corresponding eigenvalues as follows.If the corresponding eigenvalues[j] is a real number, column eigenvectors[:, j] is the eigenvectorcorresponding to eigenvalues[j].If the corresponding eigenvalues[j] and eigenvalues[j + 1] form a complex conjugate pair, then thetrue eigenvectors can be computed astrue\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j] = eigenvectors[:, j] + i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1],true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j + 1] = eigenvectors[:, j] - i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]."], "returns": "A namedtuple (eigenvalues, eigenvectors) containingeigenvalues (Tensor): Shape (n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue of input,where the first element is the real part and the second element is the imaginary part.The eigenvalues are not necessarily ordered.eigenvectors (Tensor): If eigenvectors=False, it\u2019s an empty tensor.Otherwise, this tensor of shape (n\u00d7n)(n \\times n)(n\u00d7n) can be used to compute normalized (unit length)eigenvectors of corresponding eigenvalues as follows.If the corresponding eigenvalues[j] is a real number, column eigenvectors[:, j] is the eigenvectorcorresponding to eigenvalues[j].If the corresponding eigenvalues[j] and eigenvalues[j + 1] form a complex conjugate pair, then thetrue eigenvectors can be computed astrue\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j] = eigenvectors[:, j] + i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1],true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j + 1] = eigenvectors[:, j] - i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1].", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.geqrf", "item_type": "function", "code": "torch.geqrf(input,out=None)-&gt;(Tensor,Tensor)", "description": "This is a low-level function for calling LAPACK directly. This function returns a namedtuple (a, tau) as defined in LAPACK documentation for geqrf . You\u2019ll generally want to use torch.qr() instead. Computes a QR decomposition of input, but without constructing QQQ   and RRR   as explicit separate matrices. Rather, this directly calls the underlying LAPACK function ?geqrf which produces a sequence of \u2018elementary reflectors\u2019. See LAPACK documentation for geqrf for further details.  Parameters  input (Tensor) \u2013 the input matrix out (tuple, optional) \u2013 the output tuple of (Tensor, Tensor)    ", "parameters": ["input (Tensor) : the input matrix", "out (tuple, optional) : the output tuple of (Tensor, Tensor)"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.TransformerEncoder", "item_type": "class", "code": "classtorch.nn.TransformerEncoder(encoder_layer,num_layers,norm=None)", "description": "TransformerEncoder is a stack of N encoder layers  Parameters  encoder_layer \u2013 an instance of the TransformerEncoderLayer() class (required). num_layers \u2013 the number of sub-encoder-layers in the encoder (required). norm \u2013 the layer normalization component (optional).     Examples::&gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8) &gt;&gt;&gt; transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6) &gt;&gt;&gt; src = torch.rand(10, 32, 512) &gt;&gt;&gt; out = transformer_encoder(src)       forward(src, mask=None, src_key_padding_mask=None)  Pass the input through the encoder layers in turn.  Parameters  src \u2013 the sequnce to the encoder (required). mask \u2013 the mask for the src sequence (optional). src_key_padding_mask \u2013 the mask for the src keys per batch (optional).     Shape:see the docs in Transformer class.     ", "parameters": ["encoder_layer : an instance of the TransformerEncoderLayer() class (required).", "num_layers : the number of sub-encoder-layers in the encoder (required).", "norm : the layer normalization component (optional).", "src : the sequnce to the encoder (required).", "mask : the mask for the src sequence (optional).", "src_key_padding_mask : the mask for the src keys per batch (optional)."], "returns": null, "example": " encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n src = torch.rand(10, 32, 512)\n out = transformer_encoder(src)\n\n\n", "shape": "see the docs in Transformer class. "},
{"library": "torch", "item_id": "torch.nn.TransformerDecoder", "item_type": "class", "code": "classtorch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)", "description": "TransformerDecoder is a stack of N decoder layers  Parameters  decoder_layer \u2013 an instance of the TransformerDecoderLayer() class (required). num_layers \u2013 the number of sub-decoder-layers in the decoder (required). norm \u2013 the layer normalization component (optional).     Examples::&gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8) &gt;&gt;&gt; transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=6) &gt;&gt;&gt; memory = torch.rand(10, 32, 512) &gt;&gt;&gt; tgt = torch.rand(20, 32, 512) &gt;&gt;&gt; out = transformer_decoder(tgt, memory)       forward(tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None)  Pass the inputs (and mask) through the decoder layer in turn.  Parameters  tgt \u2013 the sequence to the decoder (required). memory \u2013 the sequnce from the last layer of the encoder (required). tgt_mask \u2013 the mask for the tgt sequence (optional). memory_mask \u2013 the mask for the memory sequence (optional). tgt_key_padding_mask \u2013 the mask for the tgt keys per batch (optional). memory_key_padding_mask \u2013 the mask for the memory keys per batch (optional).     Shape:see the docs in Transformer class.     ", "parameters": ["decoder_layer : an instance of the TransformerDecoderLayer() class (required).", "num_layers : the number of sub-decoder-layers in the decoder (required).", "norm : the layer normalization component (optional).", "tgt : the sequence to the decoder (required).", "memory : the sequnce from the last layer of the encoder (required).", "tgt_mask : the mask for the tgt sequence (optional).", "memory_mask : the mask for the memory sequence (optional).", "tgt_key_padding_mask : the mask for the tgt keys per batch (optional).", "memory_key_padding_mask : the mask for the memory keys per batch (optional)."], "returns": null, "example": " decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n memory = torch.rand(10, 32, 512)\n tgt = torch.rand(20, 32, 512)\n out = transformer_decoder(tgt, memory)\n\n\n", "shape": "see the docs in Transformer class. "},
{"library": "torch", "item_id": "torch.nn.TransformerEncoderLayer", "item_type": "class", "code": "classtorch.nn.TransformerEncoderLayer(d_model,nhead,dim_feedforward=2048,dropout=0.1,activation='relu')", "description": "TransformerEncoderLayer is made up of self-attn and feedforward network. This standard encoder layer is based on the paper \u201cAttention Is All You Need\u201d. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users may modify or implement in a different way during application.  Parameters  d_model \u2013 the number of expected features in the input (required). nhead \u2013 the number of heads in the multiheadattention models (required). dim_feedforward \u2013 the dimension of the feedforward network model (default=2048). dropout \u2013 the dropout value (default=0.1). activation \u2013 the activation function of intermediate layer, relu or gelu (default=relu).     Examples::&gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8) &gt;&gt;&gt; src = torch.rand(10, 32, 512) &gt;&gt;&gt; out = encoder_layer(src)       forward(src, src_mask=None, src_key_padding_mask=None)  Pass the input through the encoder layer.  Parameters  src \u2013 the sequnce to the encoder layer (required). src_mask \u2013 the mask for the src sequence (optional). src_key_padding_mask \u2013 the mask for the src keys per batch (optional).     Shape:see the docs in Transformer class.     ", "parameters": ["d_model : the number of expected features in the input (required).", "nhead : the number of heads in the multiheadattention models (required).", "dim_feedforward : the dimension of the feedforward network model (default=2048).", "dropout : the dropout value (default=0.1).", "activation : the activation function of intermediate layer, relu or gelu (default=relu).", "src : the sequnce to the encoder layer (required).", "src_mask : the mask for the src sequence (optional).", "src_key_padding_mask : the mask for the src keys per batch (optional)."], "returns": null, "example": " encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n src = torch.rand(10, 32, 512)\n out = encoder_layer(src)\n\n\n", "shape": "see the docs in Transformer class. "},
{"library": "torch", "item_id": "torch.nn.TransformerDecoderLayer", "item_type": "class", "code": "classtorch.nn.TransformerDecoderLayer(d_model,nhead,dim_feedforward=2048,dropout=0.1,activation='relu')", "description": "TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network. This standard decoder layer is based on the paper \u201cAttention Is All You Need\u201d. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users may modify or implement in a different way during application.  Parameters  d_model \u2013 the number of expected features in the input (required). nhead \u2013 the number of heads in the multiheadattention models (required). dim_feedforward \u2013 the dimension of the feedforward network model (default=2048). dropout \u2013 the dropout value (default=0.1). activation \u2013 the activation function of intermediate layer, relu or gelu (default=relu).     Examples::&gt;&gt;&gt; decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8) &gt;&gt;&gt; memory = torch.rand(10, 32, 512) &gt;&gt;&gt; tgt = torch.rand(20, 32, 512) &gt;&gt;&gt; out = decoder_layer(tgt, memory)       forward(tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None)  Pass the inputs (and mask) through the decoder layer.  Parameters  tgt \u2013 the sequence to the decoder layer (required). memory \u2013 the sequnce from the last layer of the encoder (required). tgt_mask \u2013 the mask for the tgt sequence (optional). memory_mask \u2013 the mask for the memory sequence (optional). tgt_key_padding_mask \u2013 the mask for the tgt keys per batch (optional). memory_key_padding_mask \u2013 the mask for the memory keys per batch (optional).     Shape:see the docs in Transformer class.     ", "parameters": ["d_model : the number of expected features in the input (required).", "nhead : the number of heads in the multiheadattention models (required).", "dim_feedforward : the dimension of the feedforward network model (default=2048).", "dropout : the dropout value (default=0.1).", "activation : the activation function of intermediate layer, relu or gelu (default=relu).", "tgt : the sequence to the decoder layer (required).", "memory : the sequnce from the last layer of the encoder (required).", "tgt_mask : the mask for the tgt sequence (optional).", "memory_mask : the mask for the memory sequence (optional).", "tgt_key_padding_mask : the mask for the tgt keys per batch (optional).", "memory_key_padding_mask : the mask for the memory keys per batch (optional)."], "returns": null, "example": " decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n memory = torch.rand(10, 32, 512)\n tgt = torch.rand(20, 32, 512)\n out = decoder_layer(tgt, memory)\n\n\n", "shape": "see the docs in Transformer class. "},
{"library": "torch", "item_id": "torch.nn.Identity", "item_type": "class", "code": "classtorch.nn.Identity(*args,**kwargs)", "description": "A placeholder identity operator that is argument-insensitive.  Parameters  args \u2013 any argument (unused) kwargs \u2013 any keyword argument (unused)    Examples: &gt;&gt;&gt; m = nn.Identity(54, unused_argument1=0.1, unused_argument2=False) &gt;&gt;&gt; input = torch.randn(128, 20) &gt;&gt;&gt; output = m(input) &gt;&gt;&gt; print(output.size()) torch.Size([128, 20])   ", "parameters": ["args : any argument (unused)", "kwargs : any keyword argument (unused)"], "returns": null, "example": " m = nn.Identity(54, unused_argument1=0.1, unused_argument2=False)\n input = torch.randn(128, 20)\n output = m(input)\n print(output.size())\ntorch.Size([128, 20])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.ger", "item_type": "function", "code": "torch.ger(input,vec2,out=None)\u2192Tensor", "description": "Outer product of input and vec2. If input is a vector of size nnn   and vec2 is a vector of size mmm  , then out must be a matrix of size (n\u00d7m)(n \\times m)(n\u00d7m)  .  Note This function does not broadcast.   Parameters  input (Tensor) \u2013 1-D input vector vec2 (Tensor) \u2013 1-D input vector out (Tensor, optional) \u2013 optional output matrix    Example: &gt;&gt;&gt; v1 = torch.arange(1., 5.) &gt;&gt;&gt; v2 = torch.arange(1., 4.) &gt;&gt;&gt; torch.ger(v1, v2) tensor([[  1.,   2.,   3.],         [  2.,   4.,   6.],         [  3.,   6.,   9.],         [  4.,   8.,  12.]])   ", "parameters": ["input (Tensor) : 1-D input vector", "vec2 (Tensor) : 1-D input vector", "out (Tensor, optional) : optional output matrix"], "returns": null, "example": " v1 = torch.arange(1., 5.)\n v2 = torch.arange(1., 4.)\n torch.ger(v1, v2)\ntensor([[  1.,   2.,   3.],\n        [  2.,   4.,   6.],\n        [  3.,   6.,   9.],\n        [  4.,   8.,  12.]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.inverse", "item_type": "function", "code": "torch.inverse(input,out=None)\u2192Tensor", "description": "Takes the inverse of the square matrix input. input can be batches of 2D square tensors, in which case this function would return a tensor composed of individual inverses.  Note Irrespective of the original strides, the returned tensors will be transposed, i.e. with strides like input.contiguous().transpose(-2, -1).stride()   Parameters  input (Tensor) \u2013 the input tensor of size (\u2217,n,n)(*, n, n)(\u2217,n,n)   where * is zero or more batch dimensions out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; x = torch.rand(4, 4) &gt;&gt;&gt; y = torch.inverse(x) &gt;&gt;&gt; z = torch.mm(x, y) &gt;&gt;&gt; z tensor([[ 1.0000, -0.0000, -0.0000,  0.0000],         [ 0.0000,  1.0000,  0.0000,  0.0000],         [ 0.0000,  0.0000,  1.0000,  0.0000],         [ 0.0000, -0.0000, -0.0000,  1.0000]]) &gt;&gt;&gt; torch.max(torch.abs(z - torch.eye(4))) # Max non-zero tensor(1.1921e-07) &gt;&gt;&gt; # Batched inverse example &gt;&gt;&gt; x = torch.randn(2, 3, 4, 4) &gt;&gt;&gt; y = torch.inverse(x) &gt;&gt;&gt; z = torch.matmul(x, y) &gt;&gt;&gt; torch.max(torch.abs(z - torch.eye(4).expand_as(x))) # Max non-zero tensor(1.9073e-06)   ", "parameters": ["input (Tensor) : the input tensor of size (\u2217,n,n)(*, n, n)(\u2217,n,n) where * is zero or morebatch dimensions", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " x = torch.rand(4, 4)\n y = torch.inverse(x)\n z = torch.mm(x, y)\n z\ntensor([[ 1.0000, -0.0000, -0.0000,  0.0000],\n        [ 0.0000,  1.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  1.0000,  0.0000],\n        [ 0.0000, -0.0000, -0.0000,  1.0000]])\n torch.max(torch.abs(z - torch.eye(4))) # Max non-zero\ntensor(1.1921e-07)\n # Batched inverse example\n x = torch.randn(2, 3, 4, 4)\n y = torch.inverse(x)\n z = torch.matmul(x, y)\n torch.max(torch.abs(z - torch.eye(4).expand_as(x))) # Max non-zero\ntensor(1.9073e-06)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.det", "item_type": "function", "code": "torch.det(input)\u2192Tensor", "description": "Calculates determinant of a square matrix or batches of square matrices.  Note Backward through det() internally uses SVD results when input is not invertible. In this case, double backward through det() will be unstable in when input doesn\u2019t have distinct singular values. See svd() for details.   Parameters input (Tensor) \u2013 the input tensor of size (*, n, n) where * is zero or more batch dimensions.   Example: &gt;&gt;&gt; A = torch.randn(3, 3) &gt;&gt;&gt; torch.det(A) tensor(3.7641)  &gt;&gt;&gt; A = torch.randn(3, 2, 2) &gt;&gt;&gt; A tensor([[[ 0.9254, -0.6213],          [-0.5787,  1.6843]],          [[ 0.3242, -0.9665],          [ 0.4539, -0.0887]],          [[ 1.1336, -0.4025],          [-0.7089,  0.9032]]]) &gt;&gt;&gt; A.det() tensor([1.1990, 0.4099, 0.7386])   ", "parameters": ["input (Tensor) : the input tensor of size (*, n, n) where * is zero or morebatch dimensions."], "returns": null, "example": " A = torch.randn(3, 3)\n torch.det(A)\ntensor(3.7641)\n\n A = torch.randn(3, 2, 2)\n A\ntensor([[[ 0.9254, -0.6213],\n         [-0.5787,  1.6843]],\n\n        [[ 0.3242, -0.9665],\n         [ 0.4539, -0.0887]],\n\n        [[ 1.1336, -0.4025],\n         [-0.7089,  0.9032]]])\n A.det()\ntensor([1.1990, 0.4099, 0.7386])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.logdet", "item_type": "function", "code": "torch.logdet(input)\u2192Tensor", "description": "Calculates log determinant of a square matrix or batches of square matrices.  Note Result is -inf if input has zero log determinant, and is nan if input has negative determinant.   Note Backward through logdet() internally uses SVD results when input is not invertible. In this case, double backward through logdet() will be unstable in when input doesn\u2019t have distinct singular values. See svd() for details.   Parameters input (Tensor) \u2013 the input tensor of size (*, n, n) where * is zero or more batch dimensions.   Example: &gt;&gt;&gt; A = torch.randn(3, 3) &gt;&gt;&gt; torch.det(A) tensor(0.2611) &gt;&gt;&gt; torch.logdet(A) tensor(-1.3430) &gt;&gt;&gt; A tensor([[[ 0.9254, -0.6213],          [-0.5787,  1.6843]],          [[ 0.3242, -0.9665],          [ 0.4539, -0.0887]],          [[ 1.1336, -0.4025],          [-0.7089,  0.9032]]]) &gt;&gt;&gt; A.det() tensor([1.1990, 0.4099, 0.7386]) &gt;&gt;&gt; A.det().log() tensor([ 0.1815, -0.8917, -0.3031])   ", "parameters": ["input (Tensor) : the input tensor of size (*, n, n) where * is zero or morebatch dimensions."], "returns": null, "example": " A = torch.randn(3, 3)\n torch.det(A)\ntensor(0.2611)\n torch.logdet(A)\ntensor(-1.3430)\n A\ntensor([[[ 0.9254, -0.6213],\n         [-0.5787,  1.6843]],\n\n        [[ 0.3242, -0.9665],\n         [ 0.4539, -0.0887]],\n\n        [[ 1.1336, -0.4025],\n         [-0.7089,  0.9032]]])\n A.det()\ntensor([1.1990, 0.4099, 0.7386])\n A.det().log()\ntensor([ 0.1815, -0.8917, -0.3031])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.slogdet", "item_type": "function", "code": "torch.slogdet(input)-&gt;(Tensor,Tensor)", "description": "Calculates the sign and log absolute value of the determinant(s) of a square matrix or batches of square matrices.  Note If input has zero determinant, this returns (0, -inf).   Note Backward through slogdet() internally uses SVD results when input is not invertible. In this case, double backward through slogdet() will be unstable in when input doesn\u2019t have distinct singular values. See svd() for details.   Parameters input (Tensor) \u2013 the input tensor of size (*, n, n) where * is zero or more batch dimensions.  Returns A namedtuple (sign, logabsdet) containing the sign of the determinant, and the log value of the absolute determinant.   Example: &gt;&gt;&gt; A = torch.randn(3, 3) &gt;&gt;&gt; A tensor([[ 0.0032, -0.2239, -1.1219],         [-0.6690,  0.1161,  0.4053],         [-1.6218, -0.9273, -0.0082]]) &gt;&gt;&gt; torch.det(A) tensor(-0.7576) &gt;&gt;&gt; torch.logdet(A) tensor(nan) &gt;&gt;&gt; torch.slogdet(A) torch.return_types.slogdet(sign=tensor(-1.), logabsdet=tensor(-0.2776))   ", "parameters": ["input (Tensor) : the input tensor of size (*, n, n) where * is zero or morebatch dimensions.", "A namedtuple (sign, logabsdet) containing the sign of the determinant, and the logvalue of the absolute determinant."], "returns": "A namedtuple (sign, logabsdet) containing the sign of the determinant, and the logvalue of the absolute determinant.", "example": " A = torch.randn(3, 3)\n A\ntensor([[ 0.0032, -0.2239, -1.1219],\n        [-0.6690,  0.1161,  0.4053],\n        [-1.6218, -0.9273, -0.0082]])\n torch.det(A)\ntensor(-0.7576)\n torch.logdet(A)\ntensor(nan)\n torch.slogdet(A)\ntorch.return_types.slogdet(sign=tensor(-1.), logabsdet=tensor(-0.2776))\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.Linear", "item_type": "class", "code": "classtorch.nn.Linear(in_features,out_features,bias=True)", "description": "Applies a linear transformation to the incoming data: y=xAT+by = xA^T + by=xAT+b    Parameters  in_features \u2013 size of each input sample out_features \u2013 size of each output sample bias \u2013 If set to False, the layer will not learn an additive bias. Default: True     Shape: Input: (N,\u2217,Hin)(N, *, H_{in})(N,\u2217,Hin\u200b)   where \u2217*\u2217   means any number of additional dimensions and Hin=in_featuresH_{in} = \\text{in\\_features}Hin\u200b=in_features   Output: (N,\u2217,Hout)(N, *, H_{out})(N,\u2217,Hout\u200b)   where all but the last dimension are the same shape as the input and Hout=out_featuresH_{out} = \\text{out\\_features}Hout\u200b=out_features  .     Variables  ~Linear.weight \u2013 the learnable weights of the module of shape (out_features,in_features)(\\text{out\\_features}, \\text{in\\_features})(out_features,in_features)  . The values are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b)  , where k=1in_featuresk = \\frac{1}{\\text{in\\_features}}k=in_features1\u200b   ~Linear.bias \u2013 the learnable bias of the module of shape (out_features)(\\text{out\\_features})(out_features)  . If bias is True, the values are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b)   where k=1in_featuresk = \\frac{1}{\\text{in\\_features}}k=in_features1\u200b      Examples: &gt;&gt;&gt; m = nn.Linear(20, 30) &gt;&gt;&gt; input = torch.randn(128, 20) &gt;&gt;&gt; output = m(input) &gt;&gt;&gt; print(output.size()) torch.Size([128, 30])   ", "parameters": ["in_features : size of each input sample", "out_features : size of each output sample", "bias : If set to False, the layer will not learn an additive bias.Default: True", "~Linear.weight : the learnable weights of the module of shape(out_features,in_features)(\\text{out\\_features}, \\text{in\\_features})(out_features,in_features). The values areinitialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b), wherek=1in_featuresk = \\frac{1}{\\text{in\\_features}}k=in_features1\u200b", "~Linear.bias : the learnable bias of the module of shape (out_features)(\\text{out\\_features})(out_features).If bias is True, the values are initialized fromU(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b) wherek=1in_featuresk = \\frac{1}{\\text{in\\_features}}k=in_features1\u200b"], "returns": null, "example": " m = nn.Linear(20, 30)\n input = torch.randn(128, 20)\n output = m(input)\n print(output.size())\ntorch.Size([128, 30])\n\n", "shape": " Input: (N,\u2217,Hin)(N, *, H_{in})(N,\u2217,Hin\u200b)   where \u2217*\u2217   means any number of additional dimensions and Hin=in_featuresH_{in} = \\text{in\\_features}Hin\u200b=in_features   Output: (N,\u2217,Hout)(N, *, H_{out})(N,\u2217,Hout\u200b)   where all but the last dimension are the same shape as the input and Hout=out_featuresH_{out} = \\text{out\\_features}Hout\u200b=out_features  .  "},
{"library": "torch", "item_id": "torch.nn.Bilinear", "item_type": "class", "code": "classtorch.nn.Bilinear(in1_features,in2_features,out_features,bias=True)", "description": "Applies a bilinear transformation to the incoming data: y=x1Ax2+by = x_1 A x_2 + by=x1\u200bAx2\u200b+b    Parameters  in1_features \u2013 size of each first input sample in2_features \u2013 size of each second input sample out_features \u2013 size of each output sample bias \u2013 If set to False, the layer will not learn an additive bias. Default: True     Shape: Input1: (N,\u2217,Hin1)(N, *, H_{in1})(N,\u2217,Hin1\u200b)   where Hin1=in1_featuresH_{in1}=\\text{in1\\_features}Hin1\u200b=in1_features   and \u2217*\u2217   means any number of additional dimensions. All but the last dimension of the inputs should be the same. Input2: (N,\u2217,Hin2)(N, *, H_{in2})(N,\u2217,Hin2\u200b)   where Hin2=in2_featuresH_{in2}=\\text{in2\\_features}Hin2\u200b=in2_features  . Output: (N,\u2217,Hout)(N, *, H_{out})(N,\u2217,Hout\u200b)   where Hout=out_featuresH_{out}=\\text{out\\_features}Hout\u200b=out_features   and all but the last dimension are the same shape as the input.     Variables  ~Bilinear.weight \u2013 the learnable weights of the module of shape (out_features,in1_features,in2_features)(\\text{out\\_features}, \\text{in1\\_features}, \\text{in2\\_features})(out_features,in1_features,in2_features)  . The values are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b)  , where k=1in1_featuresk = \\frac{1}{\\text{in1\\_features}}k=in1_features1\u200b   ~Bilinear.bias \u2013 the learnable bias of the module of shape (out_features)(\\text{out\\_features})(out_features)  . If bias is True, the values are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b)  , where k=1in1_featuresk = \\frac{1}{\\text{in1\\_features}}k=in1_features1\u200b      Examples: &gt;&gt;&gt; m = nn.Bilinear(20, 30, 40) &gt;&gt;&gt; input1 = torch.randn(128, 20) &gt;&gt;&gt; input2 = torch.randn(128, 30) &gt;&gt;&gt; output = m(input1, input2) &gt;&gt;&gt; print(output.size()) torch.Size([128, 40])   ", "parameters": ["in1_features : size of each first input sample", "in2_features : size of each second input sample", "out_features : size of each output sample", "bias : If set to False, the layer will not learn an additive bias.Default: True", "~Bilinear.weight : the learnable weights of the module of shape(out_features,in1_features,in2_features)(\\text{out\\_features}, \\text{in1\\_features}, \\text{in2\\_features})(out_features,in1_features,in2_features).The values are initialized from U(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b), wherek=1in1_featuresk = \\frac{1}{\\text{in1\\_features}}k=in1_features1\u200b", "~Bilinear.bias : the learnable bias of the module of shape (out_features)(\\text{out\\_features})(out_features).If bias is True, the values are initialized fromU(\u2212k,k)\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})U(\u2212k\u200b,k\u200b), wherek=1in1_featuresk = \\frac{1}{\\text{in1\\_features}}k=in1_features1\u200b"], "returns": null, "example": " m = nn.Bilinear(20, 30, 40)\n input1 = torch.randn(128, 20)\n input2 = torch.randn(128, 30)\n output = m(input1, input2)\n print(output.size())\ntorch.Size([128, 40])\n\n", "shape": " Input1: (N,\u2217,Hin1)(N, *, H_{in1})(N,\u2217,Hin1\u200b)   where Hin1=in1_featuresH_{in1}=\\text{in1\\_features}Hin1\u200b=in1_features   and \u2217*\u2217   means any number of additional dimensions. All but the last dimension of the inputs should be the same. Input2: (N,\u2217,Hin2)(N, *, H_{in2})(N,\u2217,Hin2\u200b)   where Hin2=in2_featuresH_{in2}=\\text{in2\\_features}Hin2\u200b=in2_features  . Output: (N,\u2217,Hout)(N, *, H_{out})(N,\u2217,Hout\u200b)   where Hout=out_featuresH_{out}=\\text{out\\_features}Hout\u200b=out_features   and all but the last dimension are the same shape as the input.  "},
{"library": "torch", "item_id": "torch.lstsq", "item_type": "function", "code": "torch.lstsq(input,A,out=None)\u2192Tensor", "description": "Computes the solution to the least squares and least norm problems for a full rank matrix AAA   of size (m\u00d7n)(m \\times n)(m\u00d7n)   and a matrix BBB   of size (m\u00d7k)(m \\times k)(m\u00d7k)  . If m\u2265nm \\geq nm\u2265n  , lstsq() solves the least-squares problem:  min\u2061X\u2225AX\u2212B\u22252.\\begin{array}{ll} \\min_X &amp; \\|AX-B\\|_2. \\end{array}minX\u200b\u200b\u2225AX\u2212B\u22252\u200b.\u200b  If m&lt;nm &lt; nm&lt;n  , lstsq() solves the least-norm problem:  min\u2061X\u2225X\u22252subject\u00a0toAX=B.\\begin{array}{ll} \\min_X &amp; \\|X\\|_2 &amp; \\text{subject to} &amp; AX = B. \\end{array}minX\u200b\u200b\u2225X\u22252\u200b\u200bsubject\u00a0to\u200bAX=B.\u200b  Returned tensor XXX   has shape (max\u2061(m,n)\u00d7k)(\\max(m, n) \\times k)(max(m,n)\u00d7k)  . The first nnn   rows of XXX   contains the solution. If m\u2265nm \\geq nm\u2265n  , the residual sum of squares for the solution in each column is given by the sum of squares of elements in the remaining m\u2212nm - nm\u2212n   rows of that column.  Note The case when m&lt;nm &lt; nm&lt;n   is not supported on the GPU.   Parameters  input (Tensor) \u2013 the matrix BBB   A (Tensor) \u2013 the mmm   by nnn   matrix AAA   out (tuple, optional) \u2013 the optional destination tensor   Returns A namedtuple (solution, QR) containing:   solution (Tensor): the least squares solution QR (Tensor): the details of the QR factorization     Return type (Tensor, Tensor)    Note The returned matrices will always be transposed, irrespective of the strides of the input matrices. That is, they will have stride (1, m) instead of (m, 1).  Example: &gt;&gt;&gt; A = torch.tensor([[1., 1, 1],                       [2, 3, 4],                       [3, 5, 2],                       [4, 2, 5],                       [5, 4, 3]]) &gt;&gt;&gt; B = torch.tensor([[-10., -3],                       [ 12, 14],                       [ 14, 12],                       [ 16, 16],                       [ 18, 16]]) &gt;&gt;&gt; X, _ = torch.lstsq(B, A) &gt;&gt;&gt; X tensor([[  2.0000,   1.0000],         [  1.0000,   1.0000],         [  1.0000,   2.0000],         [ 10.9635,   4.8501],         [  8.9332,   5.2418]])   ", "parameters": ["input (Tensor) : the matrix BBB", "A (Tensor) : the mmm by nnn matrix AAA", "out (tuple, optional) : the optional destination tensor", "solution (Tensor): the least squares solution", "QR (Tensor): the details of the QR factorization"], "returns": "A namedtuple (solution, QR) containing:solution (Tensor): the least squares solutionQR (Tensor): the details of the QR factorization", "example": " A = torch.tensor([[1., 1, 1],\n                      [2, 3, 4],\n                      [3, 5, 2],\n                      [4, 2, 5],\n                      [5, 4, 3]])\n B = torch.tensor([[-10., -3],\n                      [ 12, 14],\n                      [ 14, 12],\n                      [ 16, 16],\n                      [ 18, 16]])\n X, _ = torch.lstsq(B, A)\n X\ntensor([[  2.0000,   1.0000],\n        [  1.0000,   1.0000],\n        [  1.0000,   2.0000],\n        [ 10.9635,   4.8501],\n        [  8.9332,   5.2418]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.lu", "item_type": "function", "code": "torch.lu(A,pivot=True,get_infos=False,out=None)", "description": "Computes the LU factorization of a matrix or batches of matrices A. Returns a tuple containing the LU factorization and pivots of A.  Pivoting is done if pivot is set to True.  Note The pivots returned by the function are 1-indexed. If pivot is False, then the returned pivots is a tensor filled with zeros of the appropriate size.   Note LU factorization with pivot = False is not available for CPU, and attempting to do so will throw an error. However, LU factorization with pivot = False is available for CUDA.   Note This function does not check if the factorization was successful or not if get_infos is True since the status of the factorization is present in the third element of the return tuple.   Note In the case of batches of square matrices with size less or equal to 32 on a CUDA device, the LU factorization is repeated for singular matrices due to the bug in the MAGMA library (see magma issue 13).   Parameters  A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n)   pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor. Default: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True, then the elements in the tuple are Tensor, IntTensor, and IntTensor. If get_infos is False, then the elements in the tuple are Tensor, IntTensor. Default: None   Returns A tuple of tensors containing   factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n)   pivots (IntTensor): the pivots of size (\u2217,m)(*, m)(\u2217,m)   infos (IntTensor, optional): if get_infos is True, this is a tensor of size (\u2217)(*)(\u2217)   where non-zero values indicate whether factorization for the matrix or each minibatch has succeeded or failed     Return type (Tensor, IntTensor, IntTensor (optional))   Example: &gt;&gt;&gt; A = torch.randn(2, 3, 3) &gt;&gt;&gt; A_LU, pivots = torch.lu(A) &gt;&gt;&gt; A_LU tensor([[[ 1.3506,  2.5558, -0.0816],          [ 0.1684,  1.1551,  0.1940],          [ 0.1193,  0.6189, -0.5497]],          [[ 0.4526,  1.2526, -0.3285],          [-0.7988,  0.7175, -0.9701],          [ 0.2634, -0.9255, -0.3459]]]) &gt;&gt;&gt; pivots tensor([[ 3,  3,  3],         [ 3,  3,  3]], dtype=torch.int32) &gt;&gt;&gt; A_LU, pivots, info = torch.lu(A, get_infos=True) &gt;&gt;&gt; if info.nonzero().size(0) == 0: ...   print('LU factorization succeeded for all samples!') LU factorization succeeded for all samples!   ", "parameters": ["A (Tensor) : the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n)", "pivot (bool, optional) : controls whether pivoting is done. Default: True", "get_infos (bool, optional) : if set to True, returns an info IntTensor.Default: False", "out (tuple, optional) : optional output tuple. If get_infos is True,then the elements in the tuple are Tensor, IntTensor,and IntTensor. If get_infos is False, then theelements in the tuple are Tensor, IntTensor. Default: None", "factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n)", "pivots (IntTensor): the pivots of size (\u2217,m)(*, m)(\u2217,m)", "infos (IntTensor, optional): if get_infos is True, this is a tensor ofsize (\u2217)(*)(\u2217) where non-zero values indicate whether factorization for the matrix oreach minibatch has succeeded or failed"], "returns": "A tuple of tensors containingfactorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n)pivots (IntTensor): the pivots of size (\u2217,m)(*, m)(\u2217,m)infos (IntTensor, optional): if get_infos is True, this is a tensor ofsize (\u2217)(*)(\u2217) where non-zero values indicate whether factorization for the matrix oreach minibatch has succeeded or failed", "example": " A = torch.randn(2, 3, 3)\n A_LU, pivots = torch.lu(A)\n A_LU\ntensor([[[ 1.3506,  2.5558, -0.0816],\n         [ 0.1684,  1.1551,  0.1940],\n         [ 0.1193,  0.6189, -0.5497]],\n\n        [[ 0.4526,  1.2526, -0.3285],\n         [-0.7988,  0.7175, -0.9701],\n         [ 0.2634, -0.9255, -0.3459]]])\n pivots\ntensor([[ 3,  3,  3],\n        [ 3,  3,  3]], dtype=torch.int32)\n A_LU, pivots, info = torch.lu(A, get_infos=True)\n if info.nonzero().size(0) == 0:\n...   print('LU factorization succeeded for all samples!')\nLU factorization succeeded for all samples!\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.Dropout", "item_type": "class", "code": "classtorch.nn.Dropout(p=0.5,inplace=False)", "description": "During training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution. Each channel will be zeroed out independently on every forward call. This has proven to be an effective technique for regularization and preventing the co-adaptation of neurons as described in the paper Improving neural networks by preventing co-adaptation of feature detectors . Furthermore, the outputs are scaled by a factor of 11\u2212p\\frac{1}{1-p}1\u2212p1\u200b   during training. This means that during evaluation the module simply computes an identity function.  Parameters  p \u2013 probability of an element to be zeroed. Default: 0.5 inplace \u2013 If set to True, will do this operation in-place. Default: False     Shape: Input: (\u2217)(*)(\u2217)  . Input can be of any shape Output: (\u2217)(*)(\u2217)  . Output is of the same shape as input    Examples: &gt;&gt;&gt; m = nn.Dropout(p=0.2) &gt;&gt;&gt; input = torch.randn(20, 16) &gt;&gt;&gt; output = m(input)   ", "parameters": ["p : probability of an element to be zeroed. Default: 0.5", "inplace : If set to True, will do this operation in-place. Default: False"], "returns": null, "example": " m = nn.Dropout(p=0.2)\n input = torch.randn(20, 16)\n output = m(input)\n\n", "shape": " Input: (\u2217)(*)(\u2217)  . Input can be of any shape Output: (\u2217)(*)(\u2217)  . Output is of the same shape as input  "},
{"library": "torch", "item_id": "torch.nn.Dropout2d", "item_type": "class", "code": "classtorch.nn.Dropout2d(p=0.5,inplace=False)", "description": "Randomly zero out entire channels (a channel is a 2D feature map, e.g., the jjj  -th channel of the iii  -th sample in the batched input is a 2D tensor input[i,j]\\text{input}[i, j]input[i,j]  ). Each channel will be zeroed out independently on every forward call with probability p using samples from a Bernoulli distribution. Usually the input comes from nn.Conv2d modules. As described in the paper Efficient Object Localization Using Convolutional Networks , if adjacent pixels within feature maps are strongly correlated (as is normally the case in early convolution layers) then i.i.d. dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease. In this case, nn.Dropout2d() will help promote independence between feature maps and should be used instead.  Parameters  p (python:float, optional) \u2013 probability of an element to be zero-ed. inplace (bool, optional) \u2013 If set to True, will do this operation in-place     Shape: Input: (N,C,H,W)(N, C, H, W)(N,C,H,W)   Output: (N,C,H,W)(N, C, H, W)(N,C,H,W)   (same shape as input)    Examples: &gt;&gt;&gt; m = nn.Dropout2d(p=0.2) &gt;&gt;&gt; input = torch.randn(20, 16, 32, 32) &gt;&gt;&gt; output = m(input)   ", "parameters": ["p (python:float, optional) : probability of an element to be zero-ed.", "inplace (bool, optional) : If set to True, will do this operationin-place"], "returns": null, "example": " m = nn.Dropout2d(p=0.2)\n input = torch.randn(20, 16, 32, 32)\n output = m(input)\n\n", "shape": " Input: (N,C,H,W)(N, C, H, W)(N,C,H,W)   Output: (N,C,H,W)(N, C, H, W)(N,C,H,W)   (same shape as input)  "},
{"library": "torch", "item_id": "torch.nn.Dropout3d", "item_type": "class", "code": "classtorch.nn.Dropout3d(p=0.5,inplace=False)", "description": "Randomly zero out entire channels (a channel is a 3D feature map, e.g., the jjj  -th channel of the iii  -th sample in the batched input is a 3D tensor input[i,j]\\text{input}[i, j]input[i,j]  ). Each channel will be zeroed out independently on every forward call with probability p using samples from a Bernoulli distribution. Usually the input comes from nn.Conv3d modules. As described in the paper Efficient Object Localization Using Convolutional Networks , if adjacent pixels within feature maps are strongly correlated (as is normally the case in early convolution layers) then i.i.d. dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease. In this case, nn.Dropout3d() will help promote independence between feature maps and should be used instead.  Parameters  p (python:float, optional) \u2013 probability of an element to be zeroed. inplace (bool, optional) \u2013 If set to True, will do this operation in-place     Shape: Input: (N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W)   Output: (N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W)   (same shape as input)    Examples: &gt;&gt;&gt; m = nn.Dropout3d(p=0.2) &gt;&gt;&gt; input = torch.randn(20, 16, 4, 32, 32) &gt;&gt;&gt; output = m(input)   ", "parameters": ["p (python:float, optional) : probability of an element to be zeroed.", "inplace (bool, optional) : If set to True, will do this operationin-place"], "returns": null, "example": " m = nn.Dropout3d(p=0.2)\n input = torch.randn(20, 16, 4, 32, 32)\n output = m(input)\n\n", "shape": " Input: (N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W)   Output: (N,C,D,H,W)(N, C, D, H, W)(N,C,D,H,W)   (same shape as input)  "},
{"library": "torch", "item_id": "torch.lu_solve", "item_type": "function", "code": "torch.lu_solve(input,LU_data,LU_pivots,out=None)\u2192Tensor", "description": "Returns the LU solve of the linear system Ax=bAx = bAx=b   using the partially pivoted LU factorization of A from torch.lu().  Parameters  b (Tensor) \u2013 the RHS tensor of size (\u2217,m,k)(*, m, k)(\u2217,m,k)  , where \u2217*\u2217   is zero or more batch dimensions. LU_data (Tensor) \u2013 the pivoted LU factorization of A from torch.lu() of size (\u2217,m,m)(*, m, m)(\u2217,m,m)  , where \u2217*\u2217   is zero or more batch dimensions. LU_pivots (IntTensor) \u2013 the pivots of the LU factorization from torch.lu() of size (\u2217,m)(*, m)(\u2217,m)  , where \u2217*\u2217   is zero or more batch dimensions. The batch dimensions of LU_pivots must be equal to the batch dimensions of LU_data. out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; A = torch.randn(2, 3, 3) &gt;&gt;&gt; b = torch.randn(2, 3, 1) &gt;&gt;&gt; A_LU = torch.lu(A) &gt;&gt;&gt; x = torch.lu_solve(b, *A_LU) &gt;&gt;&gt; torch.norm(torch.bmm(A, x) - b) tensor(1.00000e-07 *        2.8312)   ", "parameters": ["b (Tensor) : the RHS tensor of size (\u2217,m,k)(*, m, k)(\u2217,m,k), where \u2217*\u2217is zero or more batch dimensions.", "LU_data (Tensor) : the pivoted LU factorization of A from torch.lu() of size (\u2217,m,m)(*, m, m)(\u2217,m,m),where \u2217*\u2217 is zero or more batch dimensions.", "LU_pivots (IntTensor) : the pivots of the LU factorization from torch.lu() of size (\u2217,m)(*, m)(\u2217,m),where \u2217*\u2217 is zero or more batch dimensions.The batch dimensions of LU_pivots must be equal to the batch dimensions ofLU_data.", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " A = torch.randn(2, 3, 3)\n b = torch.randn(2, 3, 1)\n A_LU = torch.lu(A)\n x = torch.lu_solve(b, *A_LU)\n torch.norm(torch.bmm(A, x) - b)\ntensor(1.00000e-07 *\n       2.8312)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.lu_unpack", "item_type": "function", "code": "torch.lu_unpack(LU_data,LU_pivots,unpack_data=True,unpack_pivots=True)", "description": "Unpacks the data and pivots from a LU factorization of a tensor. Returns a tuple of tensors as (the pivots, the L tensor, the U tensor).  Parameters  LU_data (Tensor) \u2013 the packed LU factorization data LU_pivots (Tensor) \u2013 the packed LU factorization pivots unpack_data (bool) \u2013 flag indicating if the data should be unpacked unpack_pivots (bool) \u2013 flag indicating if the pivots should be unpacked    Examples: &gt;&gt;&gt; A = torch.randn(2, 3, 3) &gt;&gt;&gt; A_LU, pivots = A.lu() &gt;&gt;&gt; P, A_L, A_U = torch.lu_unpack(A_LU, pivots) &gt;&gt;&gt; &gt;&gt;&gt; # can recover A from factorization &gt;&gt;&gt; A_ = torch.bmm(P, torch.bmm(A_L, A_U))  &gt;&gt;&gt; # LU factorization of a rectangular matrix: &gt;&gt;&gt; A = torch.randn(2, 3, 2) &gt;&gt;&gt; A_LU, pivots = A.lu() &gt;&gt;&gt; P, A_L, A_U = torch.lu_unpack(A_LU, pivots) &gt;&gt;&gt; P tensor([[[1., 0., 0.],          [0., 1., 0.],          [0., 0., 1.]],          [[0., 0., 1.],          [0., 1., 0.],          [1., 0., 0.]]]) &gt;&gt;&gt; A_L tensor([[[ 1.0000,  0.0000],          [ 0.4763,  1.0000],          [ 0.3683,  0.1135]],          [[ 1.0000,  0.0000],          [ 0.2957,  1.0000],          [-0.9668, -0.3335]]]) &gt;&gt;&gt; A_U tensor([[[ 2.1962,  1.0881],          [ 0.0000, -0.8681]],          [[-1.0947,  0.3736],          [ 0.0000,  0.5718]]]) &gt;&gt;&gt; A_ = torch.bmm(P, torch.bmm(A_L, A_U)) &gt;&gt;&gt; torch.norm(A_ - A) tensor(2.9802e-08)   ", "parameters": ["LU_data (Tensor) : the packed LU factorization data", "LU_pivots (Tensor) : the packed LU factorization pivots", "unpack_data (bool) : flag indicating if the data should be unpacked", "unpack_pivots (bool) : flag indicating if the pivots should be unpacked"], "returns": null, "example": " A = torch.randn(2, 3, 3)\n A_LU, pivots = A.lu()\n P, A_L, A_U = torch.lu_unpack(A_LU, pivots)\n\n # can recover A from factorization\n A_ = torch.bmm(P, torch.bmm(A_L, A_U))\n\n # LU factorization of a rectangular matrix:\n A = torch.randn(2, 3, 2)\n A_LU, pivots = A.lu()\n P, A_L, A_U = torch.lu_unpack(A_LU, pivots)\n P\ntensor([[[1., 0., 0.],\n         [0., 1., 0.],\n         [0., 0., 1.]],\n\n        [[0., 0., 1.],\n         [0., 1., 0.],\n         [1., 0., 0.]]])\n A_L\ntensor([[[ 1.0000,  0.0000],\n         [ 0.4763,  1.0000],\n         [ 0.3683,  0.1135]],\n\n        [[ 1.0000,  0.0000],\n         [ 0.2957,  1.0000],\n         [-0.9668, -0.3335]]])\n A_U\ntensor([[[ 2.1962,  1.0881],\n         [ 0.0000, -0.8681]],\n\n        [[-1.0947,  0.3736],\n         [ 0.0000,  0.5718]]])\n A_ = torch.bmm(P, torch.bmm(A_L, A_U))\n torch.norm(A_ - A)\ntensor(2.9802e-08)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.matmul", "item_type": "function", "code": "torch.matmul(input,other,out=None)\u2192Tensor", "description": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows:  If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional, a 1 is prepended to its dimension for the purpose of the matrix multiply. After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional, the matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is N-dimensional (where N &gt; 2), then a batched matrix multiply is returned.  If the first argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the batched matrix multiply and removed after.  If the second argument is 1-dimensional, a 1 is appended to its dimension for the purpose of the batched matrix multiple and removed after. The non-matrix (i.e. batch) dimensions are broadcasted (and thus must be broadcastable).  For example, if input is a (j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m)   tensor and other is a (k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)   tensor, out will be an (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p)   tensor.   Note The 1-dimensional dot product version of this function does not support an out parameter.   Parameters  input (Tensor) \u2013 the first tensor to be multiplied other (Tensor) \u2013 the second tensor to be multiplied out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; # vector x vector &gt;&gt;&gt; tensor1 = torch.randn(3) &gt;&gt;&gt; tensor2 = torch.randn(3) &gt;&gt;&gt; torch.matmul(tensor1, tensor2).size() torch.Size([]) &gt;&gt;&gt; # matrix x vector &gt;&gt;&gt; tensor1 = torch.randn(3, 4) &gt;&gt;&gt; tensor2 = torch.randn(4) &gt;&gt;&gt; torch.matmul(tensor1, tensor2).size() torch.Size([3]) &gt;&gt;&gt; # batched matrix x broadcasted vector &gt;&gt;&gt; tensor1 = torch.randn(10, 3, 4) &gt;&gt;&gt; tensor2 = torch.randn(4) &gt;&gt;&gt; torch.matmul(tensor1, tensor2).size() torch.Size([10, 3]) &gt;&gt;&gt; # batched matrix x batched matrix &gt;&gt;&gt; tensor1 = torch.randn(10, 3, 4) &gt;&gt;&gt; tensor2 = torch.randn(10, 4, 5) &gt;&gt;&gt; torch.matmul(tensor1, tensor2).size() torch.Size([10, 3, 5]) &gt;&gt;&gt; # batched matrix x broadcasted matrix &gt;&gt;&gt; tensor1 = torch.randn(10, 3, 4) &gt;&gt;&gt; tensor2 = torch.randn(4, 5) &gt;&gt;&gt; torch.matmul(tensor1, tensor2).size() torch.Size([10, 3, 5])   ", "parameters": ["input (Tensor) : the first tensor to be multiplied", "other (Tensor) : the second tensor to be multiplied", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " # vector x vector\n tensor1 = torch.randn(3)\n tensor2 = torch.randn(3)\n torch.matmul(tensor1, tensor2).size()\ntorch.Size([])\n # matrix x vector\n tensor1 = torch.randn(3, 4)\n tensor2 = torch.randn(4)\n torch.matmul(tensor1, tensor2).size()\ntorch.Size([3])\n # batched matrix x broadcasted vector\n tensor1 = torch.randn(10, 3, 4)\n tensor2 = torch.randn(4)\n torch.matmul(tensor1, tensor2).size()\ntorch.Size([10, 3])\n # batched matrix x batched matrix\n tensor1 = torch.randn(10, 3, 4)\n tensor2 = torch.randn(10, 4, 5)\n torch.matmul(tensor1, tensor2).size()\ntorch.Size([10, 3, 5])\n # batched matrix x broadcasted matrix\n tensor1 = torch.randn(10, 3, 4)\n tensor2 = torch.randn(4, 5)\n torch.matmul(tensor1, tensor2).size()\ntorch.Size([10, 3, 5])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.AlphaDropout", "item_type": "class", "code": "classtorch.nn.AlphaDropout(p=0.5,inplace=False)", "description": "Applies Alpha Dropout over the input. Alpha Dropout is a type of Dropout that maintains the self-normalizing property. For an input with zero mean and unit standard deviation, the output of Alpha Dropout maintains the original mean and standard deviation of the input. Alpha Dropout goes hand-in-hand with SELU activation function, which ensures that the outputs have zero mean and unit standard deviation. During training, it randomly masks some of the elements of the input tensor with probability p using samples from a bernoulli distribution. The elements to masked are randomized on every forward call, and scaled and shifted to maintain zero mean and unit standard deviation. During evaluation the module simply computes an identity function. More details can be found in the paper Self-Normalizing Neural Networks .  Parameters  p (python:float) \u2013 probability of an element to be dropped. Default: 0.5 inplace (bool, optional) \u2013 If set to True, will do this operation in-place     Shape: Input: (\u2217)(*)(\u2217)  . Input can be of any shape Output: (\u2217)(*)(\u2217)  . Output is of the same shape as input    Examples: &gt;&gt;&gt; m = nn.AlphaDropout(p=0.2) &gt;&gt;&gt; input = torch.randn(20, 16) &gt;&gt;&gt; output = m(input)   ", "parameters": ["p (python:float) : probability of an element to be dropped. Default: 0.5", "inplace (bool, optional) : If set to True, will do this operationin-place"], "returns": null, "example": " m = nn.AlphaDropout(p=0.2)\n input = torch.randn(20, 16)\n output = m(input)\n\n", "shape": " Input: (\u2217)(*)(\u2217)  . Input can be of any shape Output: (\u2217)(*)(\u2217)  . Output is of the same shape as input  "},
{"library": "torch", "item_id": "torch.nn.Embedding", "item_type": "class", "code": "classtorch.nn.Embedding(num_embeddings,embedding_dim,padding_idx=None,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,sparse=False,_weight=None)", "description": "A simple lookup table that stores embeddings of a fixed dictionary and size. This module is often used to store word embeddings and retrieve them using indices. The input to the module is a list of indices, and the output is the corresponding word embeddings.  Parameters  num_embeddings (python:int) \u2013 size of the dictionary of embeddings embedding_dim (python:int) \u2013 the size of each embedding vector padding_idx (python:int, optional) \u2013 If given, pads the output with the embedding vector at padding_idx (initialized to zeros) whenever it encounters the index. max_norm (python:float, optional) \u2013 If given, each embedding vector with norm larger than max_norm is renormalized to have norm max_norm. norm_type (python:float, optional) \u2013 The p of the p-norm to compute for the max_norm option. Default 2. scale_grad_by_freq (boolean, optional) \u2013 If given, this will scale gradients by the inverse of frequency of the words in the mini-batch. Default False. sparse (bool, optional) \u2013 If True, gradient w.r.t. weight matrix will be a sparse tensor. See Notes for more details regarding sparse gradients.   Variables ~Embedding.weight (Tensor) \u2013 the learnable weights of the module of shape (num_embeddings, embedding_dim) initialized from N(0,1)\\mathcal{N}(0, 1)N(0,1)      Shape: Input: (\u2217)(*)(\u2217)  , LongTensor of arbitrary shape containing the indices to extract Output: (\u2217,H)(*, H)(\u2217,H)  , where * is the input shape and H=embedding_dimH=\\text{embedding\\_dim}H=embedding_dim       Note Keep in mind that only a limited number of optimizers support sparse gradients: currently it\u2019s optim.SGD (CUDA and CPU), optim.SparseAdam (CUDA and CPU) and optim.Adagrad (CPU)   Note With padding_idx set, the embedding vector at padding_idx is initialized to all zeros. However, note that this vector can be modified afterwards, e.g., using a customized initialization method, and thus changing the vector used to pad the output. The gradient for this vector from Embedding is always zero.  Examples: &gt;&gt;&gt; # an Embedding module containing 10 tensors of size 3 &gt;&gt;&gt; embedding = nn.Embedding(10, 3) &gt;&gt;&gt; # a batch of 2 samples of 4 indices each &gt;&gt;&gt; input = torch.LongTensor([[1,2,4,5],[4,3,2,9]]) &gt;&gt;&gt; embedding(input) tensor([[[-0.0251, -1.6902,  0.7172],          [-0.6431,  0.0748,  0.6969],          [ 1.4970,  1.3448, -0.9685],          [-0.3677, -2.7265, -0.1685]],          [[ 1.4970,  1.3448, -0.9685],          [ 0.4362, -0.4004,  0.9400],          [-0.6431,  0.0748,  0.6969],          [ 0.9124, -2.3616,  1.1151]]])   &gt;&gt;&gt; # example with padding_idx &gt;&gt;&gt; embedding = nn.Embedding(10, 3, padding_idx=0) &gt;&gt;&gt; input = torch.LongTensor([[0,2,0,5]]) &gt;&gt;&gt; embedding(input) tensor([[[ 0.0000,  0.0000,  0.0000],          [ 0.1535, -2.0309,  0.9315],          [ 0.0000,  0.0000,  0.0000],          [-0.1655,  0.9897,  0.0635]]])     classmethod from_pretrained(embeddings, freeze=True, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False)  Creates Embedding instance from given 2-dimensional FloatTensor.  Parameters  embeddings (Tensor) \u2013 FloatTensor containing weights for the Embedding. First dimension is being passed to Embedding as num_embeddings, second as embedding_dim. freeze (boolean, optional) \u2013 If True, the tensor does not get updated in the learning process. Equivalent to embedding.weight.requires_grad = False. Default: True padding_idx (python:int, optional) \u2013 See module initialization documentation. max_norm (python:float, optional) \u2013 See module initialization documentation. norm_type (python:float, optional) \u2013 See module initialization documentation. Default 2. scale_grad_by_freq (boolean, optional) \u2013 See module initialization documentation. Default False. sparse (bool, optional) \u2013 See module initialization documentation.    Examples: &gt;&gt;&gt; # FloatTensor containing pretrained weights &gt;&gt;&gt; weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]]) &gt;&gt;&gt; embedding = nn.Embedding.from_pretrained(weight) &gt;&gt;&gt; # Get embeddings for index 1 &gt;&gt;&gt; input = torch.LongTensor([1]) &gt;&gt;&gt; embedding(input) tensor([[ 4.0000,  5.1000,  6.3000]])     ", "parameters": ["num_embeddings (python:int) : size of the dictionary of embeddings", "embedding_dim (python:int) : the size of each embedding vector", "padding_idx (python:int, optional) : If given, pads the output with the embedding vector at padding_idx(initialized to zeros) whenever it encounters the index.", "max_norm (python:float, optional) : If given, each embedding vector with norm larger than max_normis renormalized to have norm max_norm.", "norm_type (python:float, optional) : The p of the p-norm to compute for the max_norm option. Default 2.", "scale_grad_by_freq (boolean, optional) : If given, this will scale gradients by the inverse of frequency ofthe words in the mini-batch. Default False.", "sparse (bool, optional) : If True, gradient w.r.t. weight matrix will be a sparse tensor.See Notes for more details regarding sparse gradients.", "embeddings (Tensor) : FloatTensor containing weights for the Embedding.First dimension is being passed to Embedding as num_embeddings, second as embedding_dim.", "freeze (boolean, optional) : If True, the tensor does not get updated in the learning process.Equivalent to embedding.weight.requires_grad = False. Default: True", "padding_idx (python:int, optional) : See module initialization documentation.", "max_norm (python:float, optional) : See module initialization documentation.", "norm_type (python:float, optional) : See module initialization documentation. Default 2.", "scale_grad_by_freq (boolean, optional) : See module initialization documentation. Default False.", "sparse (bool, optional) : See module initialization documentation."], "returns": null, "example": " # an Embedding module containing 10 tensors of size 3\n embedding = nn.Embedding(10, 3)\n # a batch of 2 samples of 4 indices each\n input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])\n embedding(input)\ntensor([[[-0.0251, -1.6902,  0.7172],\n         [-0.6431,  0.0748,  0.6969],\n         [ 1.4970,  1.3448, -0.9685],\n         [-0.3677, -2.7265, -0.1685]],\n\n        [[ 1.4970,  1.3448, -0.9685],\n         [ 0.4362, -0.4004,  0.9400],\n         [-0.6431,  0.0748,  0.6969],\n         [ 0.9124, -2.3616,  1.1151]]])\n\n\n # example with padding_idx\n embedding = nn.Embedding(10, 3, padding_idx=0)\n input = torch.LongTensor([[0,2,0,5]])\n embedding(input)\ntensor([[[ 0.0000,  0.0000,  0.0000],\n         [ 0.1535, -2.0309,  0.9315],\n         [ 0.0000,  0.0000,  0.0000],\n         [-0.1655,  0.9897,  0.0635]]])\n\n", "shape": " Input: (\u2217)(*)(\u2217)  , LongTensor of arbitrary shape containing the indices to extract Output: (\u2217,H)(*, H)(\u2217,H)  , where * is the input shape and H=embedding_dimH=\\text{embedding\\_dim}H=embedding_dim    "},
{"library": "torch", "item_id": "torch.nn.EmbeddingBag", "item_type": "class", "code": "classtorch.nn.EmbeddingBag(num_embeddings,embedding_dim,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,mode='mean',sparse=False,_weight=None)", "description": "Computes sums or means of \u2018bags\u2019 of embeddings, without instantiating the intermediate embeddings. For bags of constant length and no per_sample_weights, this class   with mode=\"sum\" is equivalent to Embedding followed by torch.sum(dim=0), with mode=\"mean\" is equivalent to Embedding followed by torch.mean(dim=0), with mode=\"max\" is equivalent to Embedding followed by torch.max(dim=0).   However, EmbeddingBag is much more time and memory efficient than using a chain of these operations. EmbeddingBag also supports per-sample weights as an argument to the forward pass. This scales the output of the Embedding before performing a weighted reduction as specified by mode. If per_sample_weights` is passed, the only supported mode is \"sum\", which computes a weighted sum according to per_sample_weights.  Parameters  num_embeddings (python:int) \u2013 size of the dictionary of embeddings embedding_dim (python:int) \u2013 the size of each embedding vector max_norm (python:float, optional) \u2013 If given, each embedding vector with norm larger than max_norm is renormalized to have norm max_norm. norm_type (python:float, optional) \u2013 The p of the p-norm to compute for the max_norm option. Default 2. scale_grad_by_freq (boolean, optional) \u2013 if given, this will scale gradients by the inverse of frequency of the words in the mini-batch. Default False. Note: this option is not supported when mode=\"max\". mode (string, optional) \u2013 \"sum\", \"mean\" or \"max\". Specifies the way to reduce the bag. \"sum\" computes the weighted sum, taking per_sample_weights into consideration. \"mean\" computes the average of the values in the bag, \"max\" computes the max value over each bag. Default: \"mean\" sparse (bool, optional) \u2013 if True, gradient w.r.t. weight matrix will be a sparse tensor. See Notes for more details regarding sparse gradients. Note: this option is not supported when mode=\"max\".   Variables ~EmbeddingBag.weight (Tensor) \u2013 the learnable weights of the module of shape (num_embeddings, embedding_dim) initialized from N(0,1)\\mathcal{N}(0, 1)N(0,1)  .    Inputs: input (LongTensor), offsets (LongTensor, optional), andper_index_weights (Tensor, optional)  If input is 2D of shape (B, N), it will be treated as B bags (sequences) each of fixed length N, and this will return B values aggregated in a way depending on the mode. offsets is ignored and required to be None in this case.  If input is 1D of shape (N), it will be treated as a concatenation of multiple bags (sequences). offsets is required to be a 1D tensor containing the starting index positions of each bag in input. Therefore, for offsets of shape (B), input will be viewed as having B bags. Empty bags (i.e., having 0-length) will have returned vectors filled by zeros.    per_sample_weights (Tensor, optional): a tensor of float / double weights, or Noneto indicate all weights should be taken to be 1. If specified, per_sample_weights must have exactly the same shape as input and is treated as having the same offsets, if those are not None. Only supported for mode='sum'.     Output shape: (B, embedding_dim) Examples: &gt;&gt;&gt; # an Embedding module containing 10 tensors of size 3 &gt;&gt;&gt; embedding_sum = nn.EmbeddingBag(10, 3, mode='sum') &gt;&gt;&gt; # a batch of 2 samples of 4 indices each &gt;&gt;&gt; input = torch.LongTensor([1,2,4,5,4,3,2,9]) &gt;&gt;&gt; offsets = torch.LongTensor([0,4]) &gt;&gt;&gt; embedding_sum(input, offsets) tensor([[-0.8861, -5.4350, -0.0523],         [ 1.1306, -2.5798, -1.0044]])     classmethod from_pretrained(embeddings, freeze=True, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, mode='mean', sparse=False)  Creates EmbeddingBag instance from given 2-dimensional FloatTensor.  Parameters  embeddings (Tensor) \u2013 FloatTensor containing weights for the EmbeddingBag. First dimension is being passed to EmbeddingBag as \u2018num_embeddings\u2019, second as \u2018embedding_dim\u2019. freeze (boolean, optional) \u2013 If True, the tensor does not get updated in the learning process. Equivalent to embeddingbag.weight.requires_grad = False. Default: True max_norm (python:float, optional) \u2013 See module initialization documentation. Default: None norm_type (python:float, optional) \u2013 See module initialization documentation. Default 2. scale_grad_by_freq (boolean, optional) \u2013 See module initialization documentation. Default False. mode (string, optional) \u2013 See module initialization documentation. Default: \"mean\" sparse (bool, optional) \u2013 See module initialization documentation. Default: False.    Examples: &gt;&gt;&gt; # FloatTensor containing pretrained weights &gt;&gt;&gt; weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]]) &gt;&gt;&gt; embeddingbag = nn.EmbeddingBag.from_pretrained(weight) &gt;&gt;&gt; # Get embeddings for index 1 &gt;&gt;&gt; input = torch.LongTensor([[1, 0]]) &gt;&gt;&gt; embeddingbag(input) tensor([[ 2.5000,  3.7000,  4.6500]])     ", "parameters": ["num_embeddings (python:int) : size of the dictionary of embeddings", "embedding_dim (python:int) : the size of each embedding vector", "max_norm (python:float, optional) : If given, each embedding vector with norm larger than max_normis renormalized to have norm max_norm.", "norm_type (python:float, optional) : The p of the p-norm to compute for the max_norm option. Default 2.", "scale_grad_by_freq (boolean, optional) : if given, this will scale gradients by the inverse of frequency ofthe words in the mini-batch. Default False.Note: this option is not supported when mode=\"max\".", "mode (string, optional) : \"sum\", \"mean\" or \"max\". Specifies the way to reduce the bag.\"sum\" computes the weighted sum, taking per_sample_weightsinto consideration. \"mean\" computes the average of the valuesin the bag, \"max\" computes the max value over each bag.Default: \"mean\"", "sparse (bool, optional) : if True, gradient w.r.t. weight matrix will be a sparse tensor. SeeNotes for more details regarding sparse gradients. Note: this option is notsupported when mode=\"max\".", "embeddings (Tensor) : FloatTensor containing weights for the EmbeddingBag.First dimension is being passed to EmbeddingBag as \u2018num_embeddings\u2019, second as \u2018embedding_dim\u2019.", "freeze (boolean, optional) : If True, the tensor does not get updated in the learning process.Equivalent to embeddingbag.weight.requires_grad = False. Default: True", "max_norm (python:float, optional) : See module initialization documentation. Default: None", "norm_type (python:float, optional) : See module initialization documentation. Default 2.", "scale_grad_by_freq (boolean, optional) : See module initialization documentation. Default False.", "mode (string, optional) : See module initialization documentation. Default: \"mean\"", "sparse (bool, optional) : See module initialization documentation. Default: False."], "returns": null, "example": " # an Embedding module containing 10 tensors of size 3\n embedding_sum = nn.EmbeddingBag(10, 3, mode='sum')\n # a batch of 2 samples of 4 indices each\n input = torch.LongTensor([1,2,4,5,4,3,2,9])\n offsets = torch.LongTensor([0,4])\n embedding_sum(input, offsets)\ntensor([[-0.8861, -5.4350, -0.0523],\n        [ 1.1306, -2.5798, -1.0044]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.matrix_power", "item_type": "function", "code": "torch.matrix_power(input,n)\u2192Tensor", "description": "Returns the matrix raised to the power n for square matrices. For batch of matrices, each individual matrix is raised to the power n. If n is negative, then the inverse of the matrix (if invertible) is raised to the power n.  For a batch of matrices, the batched inverse (if invertible) is raised to the power n. If n is 0, then an identity matrix is returned.  Parameters  input (Tensor) \u2013 the input tensor. n (python:int) \u2013 the power to raise the matrix to    Example: &gt;&gt;&gt; a = torch.randn(2, 2, 2) &gt;&gt;&gt; a tensor([[[-1.9975, -1.9610],          [ 0.9592, -2.3364]],          [[-1.2534, -1.3429],          [ 0.4153, -1.4664]]]) &gt;&gt;&gt; torch.matrix_power(a, 3) tensor([[[  3.9392, -23.9916],          [ 11.7357,  -0.2070]],          [[  0.2468,  -6.7168],          [  2.0774,  -0.8187]]])   ", "parameters": ["input (Tensor) : the input tensor.", "n (python:int) : the power to raise the matrix to"], "returns": null, "example": " a = torch.randn(2, 2, 2)\n a\ntensor([[[-1.9975, -1.9610],\n         [ 0.9592, -2.3364]],\n\n        [[-1.2534, -1.3429],\n         [ 0.4153, -1.4664]]])\n torch.matrix_power(a, 3)\ntensor([[[  3.9392, -23.9916],\n         [ 11.7357,  -0.2070]],\n\n        [[  0.2468,  -6.7168],\n         [  2.0774,  -0.8187]]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.matrix_rank", "item_type": "function", "code": "torch.matrix_rank(input,tol=None,symmetric=False)\u2192Tensor", "description": "Returns the numerical rank of a 2-D tensor. The method to compute the matrix rank is done using SVD by default. If symmetric is True, then input is assumed to be symmetric, and the computation of the rank is done by obtaining the eigenvalues. tol is the threshold below which the singular values (or the eigenvalues when symmetric is True) are considered to be 0. If tol is not specified, tol is set to S.max() * max(S.size()) * eps where S is the singular values (or the eigenvalues when symmetric is True), and eps is the epsilon value for the datatype of input.  Parameters  input (Tensor) \u2013 the input 2-D tensor tol (python:float, optional) \u2013 the tolerance value. Default: None symmetric (bool, optional) \u2013 indicates whether input is symmetric. Default: False    Example: &gt;&gt;&gt; a = torch.eye(10) &gt;&gt;&gt; torch.matrix_rank(a) tensor(10) &gt;&gt;&gt; b = torch.eye(10) &gt;&gt;&gt; b[0, 0] = 0 &gt;&gt;&gt; torch.matrix_rank(b) tensor(9)   ", "parameters": ["input (Tensor) : the input 2-D tensor", "tol (python:float, optional) : the tolerance value. Default: None", "symmetric (bool, optional) : indicates whether input is symmetric.Default: False"], "returns": null, "example": " a = torch.eye(10)\n torch.matrix_rank(a)\ntensor(10)\n b = torch.eye(10)\n b[0, 0] = 0\n torch.matrix_rank(b)\ntensor(9)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.mm", "item_type": "function", "code": "torch.mm(input,mat2,out=None)\u2192Tensor", "description": "Performs a matrix multiplication of the matrices input and mat2. If input is a (n\u00d7m)(n \\times m)(n\u00d7m)   tensor, mat2 is a (m\u00d7p)(m \\times p)(m\u00d7p)   tensor, out will be a (n\u00d7p)(n \\times p)(n\u00d7p)   tensor.  Note This function does not broadcast. For broadcasting matrix products, see torch.matmul().   Parameters  input (Tensor) \u2013 the first matrix to be multiplied mat2 (Tensor) \u2013 the second matrix to be multiplied out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; mat1 = torch.randn(2, 3) &gt;&gt;&gt; mat2 = torch.randn(3, 3) &gt;&gt;&gt; torch.mm(mat1, mat2) tensor([[ 0.4851,  0.5037, -0.3633],         [-0.0760, -3.6705,  2.4784]])   ", "parameters": ["input (Tensor) : the first matrix to be multiplied", "mat2 (Tensor) : the second matrix to be multiplied", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " mat1 = torch.randn(2, 3)\n mat2 = torch.randn(3, 3)\n torch.mm(mat1, mat2)\ntensor([[ 0.4851,  0.5037, -0.3633],\n        [-0.0760, -3.6705,  2.4784]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.mv", "item_type": "function", "code": "torch.mv(input,vec,out=None)\u2192Tensor", "description": "Performs a matrix-vector product of the matrix input and the vector vec. If input is a (n\u00d7m)(n \\times m)(n\u00d7m)   tensor, vec is a 1-D tensor of size mmm  , out will be 1-D of size nnn  .  Note This function does not broadcast.   Parameters  input (Tensor) \u2013 matrix to be multiplied vec (Tensor) \u2013 vector to be multiplied out (Tensor, optional) \u2013 the output tensor.    Example: &gt;&gt;&gt; mat = torch.randn(2, 3) &gt;&gt;&gt; vec = torch.randn(3) &gt;&gt;&gt; torch.mv(mat, vec) tensor([ 1.0404, -0.6361])   ", "parameters": ["input (Tensor) : matrix to be multiplied", "vec (Tensor) : vector to be multiplied", "out (Tensor, optional) : the output tensor."], "returns": null, "example": " mat = torch.randn(2, 3)\n vec = torch.randn(3)\n torch.mv(mat, vec)\ntensor([ 1.0404, -0.6361])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.orgqr", "item_type": "function", "code": "torch.orgqr(input,input2)\u2192Tensor", "description": "Computes the orthogonal matrix Q of a QR factorization, from the (input, input2) tuple returned by torch.geqrf(). This directly calls the underlying LAPACK function ?orgqr. See LAPACK documentation for orgqr for further details.  Parameters  input (Tensor) \u2013 the a from torch.geqrf(). input2 (Tensor) \u2013 the tau from torch.geqrf().    ", "parameters": ["input (Tensor) : the a from torch.geqrf().", "input2 (Tensor) : the tau from torch.geqrf()."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.CosineSimilarity", "item_type": "class", "code": "classtorch.nn.CosineSimilarity(dim=1,eps=1e-08)", "description": "Returns cosine similarity between x1x_1x1\u200b   and x2x_2x2\u200b  , computed along dim.  similarity=x1\u22c5x2max\u2061(\u2225x1\u22252\u22c5\u2225x2\u22252,\u03f5).\\text{similarity} = \\dfrac{x_1 \\cdot x_2}{\\max(\\Vert x_1 \\Vert _2 \\cdot \\Vert x_2 \\Vert _2, \\epsilon)}.  similarity=max(\u2225x1\u200b\u22252\u200b\u22c5\u2225x2\u200b\u22252\u200b,\u03f5)x1\u200b\u22c5x2\u200b\u200b.   Parameters  dim (python:int, optional) \u2013 Dimension where cosine similarity is computed. Default: 1 eps (python:float, optional) \u2013 Small value to avoid division by zero. Default: 1e-8     Shape: Input1: (\u22171,D,\u22172)(\\ast_1, D, \\ast_2)(\u22171\u200b,D,\u22172\u200b)   where D is at position dim Input2: (\u22171,D,\u22172)(\\ast_1, D, \\ast_2)(\u22171\u200b,D,\u22172\u200b)  , same shape as the Input1 Output: (\u22171,\u22172)(\\ast_1, \\ast_2)(\u22171\u200b,\u22172\u200b)     Examples::&gt;&gt;&gt; input1 = torch.randn(100, 128) &gt;&gt;&gt; input2 = torch.randn(100, 128) &gt;&gt;&gt; cos = nn.CosineSimilarity(dim=1, eps=1e-6) &gt;&gt;&gt; output = cos(input1, input2)     ", "parameters": ["dim (python:int, optional) : Dimension where cosine similarity is computed. Default: 1", "eps (python:float, optional) : Small value to avoid division by zero.Default: 1e-8"], "returns": null, "example": " input1 = torch.randn(100, 128)\n input2 = torch.randn(100, 128)\n cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n output = cos(input1, input2)\n\n\n", "shape": " Input1: (\u22171,D,\u22172)(\\ast_1, D, \\ast_2)(\u22171\u200b,D,\u22172\u200b)   where D is at position dim Input2: (\u22171,D,\u22172)(\\ast_1, D, \\ast_2)(\u22171\u200b,D,\u22172\u200b)  , same shape as the Input1 Output: (\u22171,\u22172)(\\ast_1, \\ast_2)(\u22171\u200b,\u22172\u200b)    "},
{"library": "torch", "item_id": "torch.nn.PairwiseDistance", "item_type": "class", "code": "classtorch.nn.PairwiseDistance(p=2.0,eps=1e-06,keepdim=False)", "description": "Computes the batchwise pairwise distance between vectors v1v_1v1\u200b  , v2v_2v2\u200b   using the p-norm:  \u2225x\u2225p=(\u2211i=1n\u2223xi\u2223p)1/p.\\Vert x \\Vert _p = \\left( \\sum_{i=1}^n  \\vert x_i \\vert ^ p \\right) ^ {1/p}.  \u2225x\u2225p\u200b=(i=1\u2211n\u200b\u2223xi\u200b\u2223p)1/p.   Parameters  p (real) \u2013 the norm degree. Default: 2 eps (python:float, optional) \u2013 Small value to avoid division by zero. Default: 1e-6 keepdim (bool, optional) \u2013 Determines whether or not to keep the vector dimension. Default: False     Shape: Input1: (N,D)(N, D)(N,D)   where D = vector dimension Input2: (N,D)(N, D)(N,D)  , same shape as the Input1 Output: (N)(N)(N)  . If keepdim is True, then (N,1)(N, 1)(N,1)  .   Examples::&gt;&gt;&gt; pdist = nn.PairwiseDistance(p=2) &gt;&gt;&gt; input1 = torch.randn(100, 128) &gt;&gt;&gt; input2 = torch.randn(100, 128) &gt;&gt;&gt; output = pdist(input1, input2)     ", "parameters": ["p (real) : the norm degree. Default: 2", "eps (python:float, optional) : Small value to avoid division by zero.Default: 1e-6", "keepdim (bool, optional) : Determines whether or not to keep the vector dimension.Default: False"], "returns": null, "example": " pdist = nn.PairwiseDistance(p=2)\n input1 = torch.randn(100, 128)\n input2 = torch.randn(100, 128)\n output = pdist(input1, input2)\n\n\n", "shape": " Input1: (N,D)(N, D)(N,D)   where D = vector dimension Input2: (N,D)(N, D)(N,D)  , same shape as the Input1 Output: (N)(N)(N)  . If keepdim is True, then (N,1)(N, 1)(N,1)  .  "},
{"library": "torch", "item_id": "torch.nn.L1Loss", "item_type": "class", "code": "classtorch.nn.L1Loss(size_average=None,reduce=None,reduction='mean')", "description": "Creates a criterion that measures the mean absolute error (MAE) between each element in the input xxx   and target yyy  . The unreduced (i.e. with reduction set to 'none') loss can be described as:  \u2113(x,y)=L={l1,\u2026,lN}\u22a4,ln=\u2223xn\u2212yn\u2223,\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad l_n = \\left| x_n - y_n \\right|,  \u2113(x,y)=L={l1\u200b,\u2026,lN\u200b}\u22a4,ln\u200b=\u2223xn\u200b\u2212yn\u200b\u2223,  where NNN   is the batch size. If reduction is not 'none' (default 'mean'), then:  \u2113(x,y)={mean\u2061(L),if\u00a0reduction=\u2019mean\u2019;sum\u2061(L),if\u00a0reduction=\u2019sum\u2019.\\ell(x, y) = \\begin{cases}     \\operatorname{mean}(L), &amp; \\text{if reduction} = \\text{'mean';}\\\\     \\operatorname{sum}(L),  &amp; \\text{if reduction} = \\text{'sum'.} \\end{cases}  \u2113(x,y)={mean(L),sum(L),\u200bif\u00a0reduction=\u2019mean\u2019;if\u00a0reduction=\u2019sum\u2019.\u200b  xxx   and yyy   are tensors of arbitrary shapes with a total of nnn   elements each. The sum operation still operates over all the elements, and divides by nnn  . The division by nnn   can be avoided if one sets reduction = 'sum'.  Parameters  size_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'     Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where \u2217*\u2217   means, any number of additional dimensions Target: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input Output: scalar. If reduction is 'none', then (N,\u2217)(N, *)(N,\u2217)  , same shape as the input    Examples: &gt;&gt;&gt; loss = nn.L1Loss() &gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True) &gt;&gt;&gt; target = torch.randn(3, 5) &gt;&gt;&gt; output = loss(input, target) &gt;&gt;&gt; output.backward()   ", "parameters": ["size_average (bool, optional) : Deprecated (see reduction). By default,the losses are averaged over each loss element in the batch. Note that forsome losses, there are multiple elements per sample. If the field size_averageis set to False, the losses are instead summed for each minibatch. Ignoredwhen reduce is False. Default: True", "reduce (bool, optional) : Deprecated (see reduction). By default, thelosses are averaged or summed over observations for each minibatch dependingon size_average. When reduce is False, returns a loss perbatch element instead and ignores size_average. Default: True", "reduction (string, optional) : Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"], "returns": null, "example": " loss = nn.L1Loss()\n input = torch.randn(3, 5, requires_grad=True)\n target = torch.randn(3, 5)\n output = loss(input, target)\n output.backward()\n\n", "shape": " Input: (N,\u2217)(N, *)(N,\u2217)   where \u2217*\u2217   means, any number of additional dimensions Target: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input Output: scalar. If reduction is 'none', then (N,\u2217)(N, *)(N,\u2217)  , same shape as the input  "},
{"library": "torch", "item_id": "torch.ormqr", "item_type": "function", "code": "torch.ormqr(input,input2,input3,left=True,transpose=False)\u2192Tensor", "description": "Multiplies mat (given by input3) by the orthogonal Q matrix of the QR factorization formed by torch.geqrf() that is represented by (a, tau) (given by (input, input2)). This directly calls the underlying LAPACK function ?ormqr. See LAPACK documentation for ormqr for further details.  Parameters  input (Tensor) \u2013 the a from torch.geqrf(). input2 (Tensor) \u2013 the tau from torch.geqrf(). input3 (Tensor) \u2013 the matrix to be multiplied.    ", "parameters": ["input (Tensor) : the a from torch.geqrf().", "input2 (Tensor) : the tau from torch.geqrf().", "input3 (Tensor) : the matrix to be multiplied."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.pinverse", "item_type": "function", "code": "torch.pinverse(input,rcond=1e-15)\u2192Tensor", "description": "Calculates the pseudo-inverse (also known as the Moore-Penrose inverse) of a 2D tensor. Please look at Moore-Penrose inverse for more details  Note This method is implemented using the Singular Value Decomposition.   Note The pseudo-inverse is not necessarily a continuous function in the elements of the matrix [1]. Therefore, derivatives are not always existent, and exist for a constant rank only [2]. However, this method is backprop-able due to the implementation by using SVD results, and could be unstable. Double-backward will also be unstable due to the usage of SVD internally. See svd() for more details.   Parameters  input (Tensor) \u2013 The input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n)   where \u2217*\u2217   is zero or more batch dimensions rcond (python:float) \u2013 A floating point value to determine the cutoff for small singular values. Default: 1e-15   Returns The pseudo-inverse of input of dimensions (\u2217,n,m)(*, n, m)(\u2217,n,m)     Example: &gt;&gt;&gt; input = torch.randn(3, 5) &gt;&gt;&gt; input tensor([[ 0.5495,  0.0979, -1.4092, -0.1128,  0.4132],         [-1.1143, -0.3662,  0.3042,  1.6374, -0.9294],         [-0.3269, -0.5745, -0.0382, -0.5922, -0.6759]]) &gt;&gt;&gt; torch.pinverse(input) tensor([[ 0.0600, -0.1933, -0.2090],         [-0.0903, -0.0817, -0.4752],         [-0.7124, -0.1631, -0.2272],         [ 0.1356,  0.3933, -0.5023],         [-0.0308, -0.1725, -0.5216]]) &gt;&gt;&gt; # Batched pinverse example &gt;&gt;&gt; a = torch.randn(2,6,3) &gt;&gt;&gt; b = torch.pinverse(a) &gt;&gt;&gt; torch.matmul(b, a) tensor([[[ 1.0000e+00,  1.6391e-07, -1.1548e-07],         [ 8.3121e-08,  1.0000e+00, -2.7567e-07],         [ 3.5390e-08,  1.4901e-08,  1.0000e+00]],          [[ 1.0000e+00, -8.9407e-08,  2.9802e-08],         [-2.2352e-07,  1.0000e+00,  1.1921e-07],         [ 0.0000e+00,  8.9407e-08,  1.0000e+00]]])   ", "parameters": ["input (Tensor) : The input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) where \u2217*\u2217 is zero or more batch dimensions", "rcond (python:float) : A floating point value to determine the cutoff for small singular values.Default: 1e-15"], "returns": "The pseudo-inverse of input of dimensions (\u2217,n,m)(*, n, m)(\u2217,n,m)", "example": " input = torch.randn(3, 5)\n input\ntensor([[ 0.5495,  0.0979, -1.4092, -0.1128,  0.4132],\n        [-1.1143, -0.3662,  0.3042,  1.6374, -0.9294],\n        [-0.3269, -0.5745, -0.0382, -0.5922, -0.6759]])\n torch.pinverse(input)\ntensor([[ 0.0600, -0.1933, -0.2090],\n        [-0.0903, -0.0817, -0.4752],\n        [-0.7124, -0.1631, -0.2272],\n        [ 0.1356,  0.3933, -0.5023],\n        [-0.0308, -0.1725, -0.5216]])\n # Batched pinverse example\n a = torch.randn(2,6,3)\n b = torch.pinverse(a)\n torch.matmul(b, a)\ntensor([[[ 1.0000e+00,  1.6391e-07, -1.1548e-07],\n        [ 8.3121e-08,  1.0000e+00, -2.7567e-07],\n        [ 3.5390e-08,  1.4901e-08,  1.0000e+00]],\n\n        [[ 1.0000e+00, -8.9407e-08,  2.9802e-08],\n        [-2.2352e-07,  1.0000e+00,  1.1921e-07],\n        [ 0.0000e+00,  8.9407e-08,  1.0000e+00]]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.MSELoss", "item_type": "class", "code": "classtorch.nn.MSELoss(size_average=None,reduce=None,reduction='mean')", "description": "Creates a criterion that measures the mean squared error (squared L2 norm) between each element in the input xxx   and target yyy  . The unreduced (i.e. with reduction set to 'none') loss can be described as:  \u2113(x,y)=L={l1,\u2026,lN}\u22a4,ln=(xn\u2212yn)2,\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad l_n = \\left( x_n - y_n \\right)^2,  \u2113(x,y)=L={l1\u200b,\u2026,lN\u200b}\u22a4,ln\u200b=(xn\u200b\u2212yn\u200b)2,  where NNN   is the batch size. If reduction is not 'none' (default 'mean'), then:  \u2113(x,y)={mean\u2061(L),if\u00a0reduction=\u2019mean\u2019;sum\u2061(L),if\u00a0reduction=\u2019sum\u2019.\\ell(x, y) = \\begin{cases}     \\operatorname{mean}(L), &amp;  \\text{if reduction} = \\text{'mean';}\\\\     \\operatorname{sum}(L),  &amp;  \\text{if reduction} = \\text{'sum'.} \\end{cases}  \u2113(x,y)={mean(L),sum(L),\u200bif\u00a0reduction=\u2019mean\u2019;if\u00a0reduction=\u2019sum\u2019.\u200b  xxx   and yyy   are tensors of arbitrary shapes with a total of nnn   elements each. The sum operation still operates over all the elements, and divides by nnn  . The division by nnn   can be avoided if one sets reduction = 'sum'.  Parameters  size_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'     Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where \u2217*\u2217   means, any number of additional dimensions Target: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input    Examples: &gt;&gt;&gt; loss = nn.MSELoss() &gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True) &gt;&gt;&gt; target = torch.randn(3, 5) &gt;&gt;&gt; output = loss(input, target) &gt;&gt;&gt; output.backward()   ", "parameters": ["size_average (bool, optional) : Deprecated (see reduction). By default,the losses are averaged over each loss element in the batch. Note that forsome losses, there are multiple elements per sample. If the field size_averageis set to False, the losses are instead summed for each minibatch. Ignoredwhen reduce is False. Default: True", "reduce (bool, optional) : Deprecated (see reduction). By default, thelosses are averaged or summed over observations for each minibatch dependingon size_average. When reduce is False, returns a loss perbatch element instead and ignores size_average. Default: True", "reduction (string, optional) : Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"], "returns": null, "example": " loss = nn.MSELoss()\n input = torch.randn(3, 5, requires_grad=True)\n target = torch.randn(3, 5)\n output = loss(input, target)\n output.backward()\n\n", "shape": " Input: (N,\u2217)(N, *)(N,\u2217)   where \u2217*\u2217   means, any number of additional dimensions Target: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input  "},
{"library": "torch", "item_id": "torch.nn.CrossEntropyLoss", "item_type": "class", "code": "classtorch.nn.CrossEntropyLoss(weight=None,size_average=None,ignore_index=-100,reduce=None,reduction='mean')", "description": "This criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class. It is useful when training a classification problem with C classes. If provided, the optional argument weight should be a 1D Tensor assigning weight to each of the classes. This is particularly useful when you have an unbalanced training set. The input is expected to contain raw, unnormalized scores for each class. input has to be a Tensor of size either (minibatch,C)(minibatch, C)(minibatch,C)   or (minibatch,C,d1,d2,...,dK)(minibatch, C, d_1, d_2, ..., d_K)(minibatch,C,d1\u200b,d2\u200b,...,dK\u200b)   with K\u22651K \\geq 1K\u22651   for the K-dimensional case (described later). This criterion expects a class index in the range [0,C\u22121][0, C-1][0,C\u22121]   as the target for each value of a 1D tensor of size minibatch; if ignore_index is specified, this criterion also accepts this class index (this index may not necessarily be in the class range). The loss can be described as:  loss(x,class)=\u2212log\u2061(exp\u2061(x[class])\u2211jexp\u2061(x[j]))=\u2212x[class]+log\u2061(\u2211jexp\u2061(x[j]))\\text{loss}(x, class) = -\\log\\left(\\frac{\\exp(x[class])}{\\sum_j \\exp(x[j])}\\right)                = -x[class] + \\log\\left(\\sum_j \\exp(x[j])\\right)  loss(x,class)=\u2212log(\u2211j\u200bexp(x[j])exp(x[class])\u200b)=\u2212x[class]+log(j\u2211\u200bexp(x[j]))  or in the case of the weight argument being specified:  loss(x,class)=weight[class](\u2212x[class]+log\u2061(\u2211jexp\u2061(x[j])))\\text{loss}(x, class) = weight[class] \\left(-x[class] + \\log\\left(\\sum_j \\exp(x[j])\\right)\\right)  loss(x,class)=weight[class](\u2212x[class]+log(j\u2211\u200bexp(x[j])))  The losses are averaged across observations for each minibatch. Can also be used for higher dimension inputs, such as 2D images, by providing an input of size (minibatch,C,d1,d2,...,dK)(minibatch, C, d_1, d_2, ..., d_K)(minibatch,C,d1\u200b,d2\u200b,...,dK\u200b)   with K\u22651K \\geq 1K\u22651  , where KKK   is the number of dimensions, and a target of appropriate shape (see below).  Parameters  weight (Tensor, optional) \u2013 a manual rescaling weight given to each class. If given, has to be a Tensor of size C size_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True ignore_index (python:int, optional) \u2013 Specifies a target value that is ignored and does not contribute to the input gradient. When size_average is True, the loss is averaged over non-ignored targets. reduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'     Shape: Input: (N,C)(N, C)(N,C)   where C = number of classes, or (N,C,d1,d2,...,dK)(N, C, d_1, d_2, ..., d_K)(N,C,d1\u200b,d2\u200b,...,dK\u200b)   with K\u22651K \\geq 1K\u22651   in the case of K-dimensional loss. Target: (N)(N)(N)   where each value is 0\u2264targets[i]\u2264C\u221210 \\leq \\text{targets}[i] \\leq C-10\u2264targets[i]\u2264C\u22121  , or (N,d1,d2,...,dK)(N, d_1, d_2, ..., d_K)(N,d1\u200b,d2\u200b,...,dK\u200b)   with K\u22651K \\geq 1K\u22651   in the case of K-dimensional loss. Output: scalar. If reduction is 'none', then the same size as the target: (N)(N)(N)  , or (N,d1,d2,...,dK)(N, d_1, d_2, ..., d_K)(N,d1\u200b,d2\u200b,...,dK\u200b)   with K\u22651K \\geq 1K\u22651   in the case of K-dimensional loss.    Examples: &gt;&gt;&gt; loss = nn.CrossEntropyLoss() &gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True) &gt;&gt;&gt; target = torch.empty(3, dtype=torch.long).random_(5) &gt;&gt;&gt; output = loss(input, target) &gt;&gt;&gt; output.backward()   ", "parameters": ["weight (Tensor, optional) : a manual rescaling weight given to each class.If given, has to be a Tensor of size C", "size_average (bool, optional) : Deprecated (see reduction). By default,the losses are averaged over each loss element in the batch. Note that forsome losses, there are multiple elements per sample. If the field size_averageis set to False, the losses are instead summed for each minibatch. Ignoredwhen reduce is False. Default: True", "ignore_index (python:int, optional) : Specifies a target value that is ignoredand does not contribute to the input gradient. When size_average isTrue, the loss is averaged over non-ignored targets.", "reduce (bool, optional) : Deprecated (see reduction). By default, thelosses are averaged or summed over observations for each minibatch dependingon size_average. When reduce is False, returns a loss perbatch element instead and ignores size_average. Default: True", "reduction (string, optional) : Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"], "returns": null, "example": " loss = nn.CrossEntropyLoss()\n input = torch.randn(3, 5, requires_grad=True)\n target = torch.empty(3, dtype=torch.long).random_(5)\n output = loss(input, target)\n output.backward()\n\n", "shape": " Input: (N,C)(N, C)(N,C)   where C = number of classes, or (N,C,d1,d2,...,dK)(N, C, d_1, d_2, ..., d_K)(N,C,d1\u200b,d2\u200b,...,dK\u200b)   with K\u22651K \\geq 1K\u22651   in the case of K-dimensional loss. Target: (N)(N)(N)   where each value is 0\u2264targets[i]\u2264C\u221210 \\leq \\text{targets}[i] \\leq C-10\u2264targets[i]\u2264C\u22121  , or (N,d1,d2,...,dK)(N, d_1, d_2, ..., d_K)(N,d1\u200b,d2\u200b,...,dK\u200b)   with K\u22651K \\geq 1K\u22651   in the case of K-dimensional loss. Output: scalar. If reduction is 'none', then the same size as the target: (N)(N)(N)  , or (N,d1,d2,...,dK)(N, d_1, d_2, ..., d_K)(N,d1\u200b,d2\u200b,...,dK\u200b)   with K\u22651K \\geq 1K\u22651   in the case of K-dimensional loss.  "},
{"library": "torch", "item_id": "torch.qr", "item_type": "function", "code": "torch.qr(input,some=True,out=None)-&gt;(Tensor,Tensor)", "description": "Computes the QR decomposition of a matrix or a batch of matrices input, and returns a namedtuple (Q, R) of tensors such that input=QR\\text{input} = Q Rinput=QR   with QQQ   being an orthogonal matrix or batch of orthogonal matrices and RRR   being an upper triangular matrix or batch of upper triangular matrices. If some is True, then this function returns the thin (reduced) QR factorization. Otherwise, if some is False, this function returns the complete QR factorization.  Note precision may be lost if the magnitudes of the elements of input are large   Note While it should always give you a valid decomposition, it may not give you the same one across platforms - it will depend on your LAPACK implementation.   Parameters  input (Tensor) \u2013 the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n)   where * is zero or more batch dimensions consisting of matrices of dimension m\u00d7nm \\times nm\u00d7n  . some (bool, optional) \u2013 Set to True for reduced QR decomposition and False for complete QR decomposition. out (tuple, optional) \u2013 tuple of Q and R tensors satisfying input = torch.matmul(Q, R). The dimensions of Q and R are (\u2217,m,k)(*, m, k)(\u2217,m,k)   and (\u2217,k,n)(*, k, n)(\u2217,k,n)   respectively, where k=min\u2061(m,n)k = \\min(m, n)k=min(m,n)   if some: is True and k=mk = mk=m   otherwise.    Example: &gt;&gt;&gt; a = torch.tensor([[12., -51, 4], [6, 167, -68], [-4, 24, -41]]) &gt;&gt;&gt; q, r = torch.qr(a) &gt;&gt;&gt; q tensor([[-0.8571,  0.3943,  0.3314],         [-0.4286, -0.9029, -0.0343],         [ 0.2857, -0.1714,  0.9429]]) &gt;&gt;&gt; r tensor([[ -14.0000,  -21.0000,   14.0000],         [   0.0000, -175.0000,   70.0000],         [   0.0000,    0.0000,  -35.0000]]) &gt;&gt;&gt; torch.mm(q, r).round() tensor([[  12.,  -51.,    4.],         [   6.,  167.,  -68.],         [  -4.,   24.,  -41.]]) &gt;&gt;&gt; torch.mm(q.t(), q).round() tensor([[ 1.,  0.,  0.],         [ 0.,  1., -0.],         [ 0., -0.,  1.]]) &gt;&gt;&gt; a = torch.randn(3, 4, 5) &gt;&gt;&gt; q, r = torch.qr(a, some=False) &gt;&gt;&gt; torch.allclose(torch.matmul(q, r), a) True &gt;&gt;&gt; torch.allclose(torch.matmul(q.transpose(-2, -1), q), torch.eye(5)) True   ", "parameters": ["input (Tensor) : the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) where * is zero or morebatch dimensions consisting of matrices of dimension m\u00d7nm \\times nm\u00d7n.", "some (bool, optional) : Set to True for reduced QR decomposition and False forcomplete QR decomposition.", "out (tuple, optional) : tuple of Q and R tensorssatisfying input = torch.matmul(Q, R).The dimensions of Q and R are (\u2217,m,k)(*, m, k)(\u2217,m,k) and (\u2217,k,n)(*, k, n)(\u2217,k,n)respectively, where k=min\u2061(m,n)k = \\min(m, n)k=min(m,n) if some: is True andk=mk = mk=m otherwise."], "returns": null, "example": " a = torch.tensor([[12., -51, 4], [6, 167, -68], [-4, 24, -41]])\n q, r = torch.qr(a)\n q\ntensor([[-0.8571,  0.3943,  0.3314],\n        [-0.4286, -0.9029, -0.0343],\n        [ 0.2857, -0.1714,  0.9429]])\n r\ntensor([[ -14.0000,  -21.0000,   14.0000],\n        [   0.0000, -175.0000,   70.0000],\n        [   0.0000,    0.0000,  -35.0000]])\n torch.mm(q, r).round()\ntensor([[  12.,  -51.,    4.],\n        [   6.,  167.,  -68.],\n        [  -4.,   24.,  -41.]])\n torch.mm(q.t(), q).round()\ntensor([[ 1.,  0.,  0.],\n        [ 0.,  1., -0.],\n        [ 0., -0.,  1.]])\n a = torch.randn(3, 4, 5)\n q, r = torch.qr(a, some=False)\n torch.allclose(torch.matmul(q, r), a)\nTrue\n torch.allclose(torch.matmul(q.transpose(-2, -1), q), torch.eye(5))\nTrue\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.solve", "item_type": "function", "code": "torch.solve(input,A,out=None)-&gt;(Tensor,Tensor)", "description": "This function returns the solution to the system of linear equations represented by AX=BAX = BAX=B   and the LU factorization of A, in order as a namedtuple solution, LU. LU contains L and U factors for LU factorization of A. torch.solve(B, A) can take in 2D inputs B, A or inputs that are batches of 2D matrices. If the inputs are batches, then returns batched outputs solution, LU.  Note Irrespective of the original strides, the returned matrices solution and LU will be transposed, i.e. with strides like B.contiguous().transpose(-1, -2).stride() and A.contiguous().transpose(-1, -2).stride() respectively.   Parameters  input (Tensor) \u2013 input matrix BBB   of size (\u2217,m,k)(*, m, k)(\u2217,m,k)   , where \u2217*\u2217   is zero or more batch dimensions. A (Tensor) \u2013 input square matrix of size (\u2217,m,m)(*, m, m)(\u2217,m,m)  , where \u2217*\u2217   is zero or more batch dimensions. out ((Tensor, Tensor), optional) \u2013 optional output tuple.    Example: &gt;&gt;&gt; A = torch.tensor([[6.80, -2.11,  5.66,  5.97,  8.23],                       [-6.05, -3.30,  5.36, -4.44,  1.08],                       [-0.45,  2.58, -2.70,  0.27,  9.04],                       [8.32,  2.71,  4.35,  -7.17,  2.14],                       [-9.67, -5.14, -7.26,  6.08, -6.87]]).t() &gt;&gt;&gt; B = torch.tensor([[4.02,  6.19, -8.22, -7.57, -3.03],                       [-1.56,  4.00, -8.67,  1.75,  2.86],                       [9.81, -4.09, -4.57, -8.61,  8.99]]).t() &gt;&gt;&gt; X, LU = torch.solve(B, A) &gt;&gt;&gt; torch.dist(B, torch.mm(A, X)) tensor(1.00000e-06 *        7.0977)  &gt;&gt;&gt; # Batched solver example &gt;&gt;&gt; A = torch.randn(2, 3, 1, 4, 4) &gt;&gt;&gt; B = torch.randn(2, 3, 1, 4, 6) &gt;&gt;&gt; X, LU = torch.solve(B, A) &gt;&gt;&gt; torch.dist(B, A.matmul(X)) tensor(1.00000e-06 *    3.6386)   ", "parameters": ["input (Tensor) : input matrix BBB of size (\u2217,m,k)(*, m, k)(\u2217,m,k) , where \u2217*\u2217is zero or more batch dimensions.", "A (Tensor) : input square matrix of size (\u2217,m,m)(*, m, m)(\u2217,m,m), where\u2217*\u2217 is zero or more batch dimensions.", "out ((Tensor, Tensor), optional) : optional output tuple."], "returns": null, "example": " A = torch.tensor([[6.80, -2.11,  5.66,  5.97,  8.23],\n                      [-6.05, -3.30,  5.36, -4.44,  1.08],\n                      [-0.45,  2.58, -2.70,  0.27,  9.04],\n                      [8.32,  2.71,  4.35,  -7.17,  2.14],\n                      [-9.67, -5.14, -7.26,  6.08, -6.87]]).t()\n B = torch.tensor([[4.02,  6.19, -8.22, -7.57, -3.03],\n                      [-1.56,  4.00, -8.67,  1.75,  2.86],\n                      [9.81, -4.09, -4.57, -8.61,  8.99]]).t()\n X, LU = torch.solve(B, A)\n torch.dist(B, torch.mm(A, X))\ntensor(1.00000e-06 *\n       7.0977)\n\n # Batched solver example\n A = torch.randn(2, 3, 1, 4, 4)\n B = torch.randn(2, 3, 1, 4, 6)\n X, LU = torch.solve(B, A)\n torch.dist(B, A.matmul(X))\ntensor(1.00000e-06 *\n   3.6386)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.CTCLoss", "item_type": "class", "code": "classtorch.nn.CTCLoss(blank=0,reduction='mean',zero_infinity=False)", "description": "The Connectionist Temporal Classification loss. Calculates loss between a continuous (unsegmented) time series and a target sequence. CTCLoss sums over the probability of possible alignments of input to target, producing a loss value which is differentiable with respect to each input node. The alignment of input to target is assumed to be \u201cmany-to-one\u201d, which limits the length of the target sequence such that it must be \u2264\\leq\u2264   the input length.  Parameters  blank (python:int, optional) \u2013 blank label. Default 000  . reduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the output losses will be divided by the target lengths and then the mean over the batch is taken. Default: 'mean' zero_infinity (bool, optional) \u2013 Whether to zero infinite losses and the associated gradients. Default: False Infinite losses mainly occur when the inputs are too short to be aligned to the targets.     Shape: Log_probs: Tensor of size (T,N,C)(T, N, C)(T,N,C)  , where T=input\u00a0lengthT = \\text{input length}T=input\u00a0length  , N=batch\u00a0sizeN = \\text{batch size}N=batch\u00a0size  , and C=number\u00a0of\u00a0classes\u00a0(including\u00a0blank)C = \\text{number of classes (including blank)}C=number\u00a0of\u00a0classes\u00a0(including\u00a0blank)  . The logarithmized probabilities of the outputs (e.g. obtained with torch.nn.functional.log_softmax()). Targets: Tensor of size (N,S)(N, S)(N,S)   or (sum\u2061(target_lengths))(\\operatorname{sum}(\\text{target\\_lengths}))(sum(target_lengths))  , where N=batch\u00a0sizeN = \\text{batch size}N=batch\u00a0size   and S=max\u00a0target\u00a0length,\u00a0if\u00a0shape\u00a0is\u00a0(N,S)S = \\text{max target length, if shape is } (N, S)S=max\u00a0target\u00a0length,\u00a0if\u00a0shape\u00a0is\u00a0(N,S)  . It represent the target sequences. Each element in the target sequence is a class index. And the target index cannot be blank (default=0). In the (N,S)(N, S)(N,S)   form, targets are padded to the length of the longest sequence, and stacked. In the (sum\u2061(target_lengths))(\\operatorname{sum}(\\text{target\\_lengths}))(sum(target_lengths))   form, the targets are assumed to be un-padded and concatenated within 1 dimension. Input_lengths: Tuple or tensor of size (N)(N)(N)  , where N=batch\u00a0sizeN = \\text{batch size}N=batch\u00a0size  . It represent the lengths of the inputs (must each be \u2264T\\leq T\u2264T  ). And the lengths are specified for each sequence to achieve masking under the assumption that sequences are padded to equal lengths. Target_lengths: Tuple or tensor of size (N)(N)(N)  , where N=batch\u00a0sizeN = \\text{batch size}N=batch\u00a0size  . It represent lengths of the targets. Lengths are specified for each sequence to achieve masking under the assumption that sequences are padded to equal lengths. If target shape is (N,S)(N,S)(N,S)  , target_lengths are effectively the stop index sns_nsn\u200b   for each target sequence, such that target_n = targets[n,0:s_n] for each target in a batch. Lengths must each be \u2264S\\leq S\u2264S   If the targets are given as a 1d tensor that is the concatenation of individual targets, the target_lengths must add up to the total length of the tensor. Output: scalar. If reduction is 'none', then (N)(N)(N)  , where N=batch\u00a0sizeN = \\text{batch size}N=batch\u00a0size  .    Example: &gt;&gt;&gt; T = 50      # Input sequence length &gt;&gt;&gt; C = 20      # Number of classes (including blank) &gt;&gt;&gt; N = 16      # Batch size &gt;&gt;&gt; S = 30      # Target sequence length of longest target in batch &gt;&gt;&gt; S_min = 10  # Minimum target length, for demonstration purposes &gt;&gt;&gt; &gt;&gt;&gt; # Initialize random batch of input vectors, for *size = (T,N,C) &gt;&gt;&gt; input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_() &gt;&gt;&gt; &gt;&gt;&gt; # Initialize random batch of targets (0 = blank, 1:C = classes) &gt;&gt;&gt; target = torch.randint(low=1, high=C, size=(N, S), dtype=torch.long) &gt;&gt;&gt; &gt;&gt;&gt; input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long) &gt;&gt;&gt; target_lengths = torch.randint(low=S_min, high=S, size=(N,), dtype=torch.long) &gt;&gt;&gt; ctc_loss = nn.CTCLoss() &gt;&gt;&gt; loss = ctc_loss(input, target, input_lengths, target_lengths) &gt;&gt;&gt; loss.backward()    Reference:A. Graves et al.: Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks: https://www.cs.toronto.edu/~graves/icml_2006.pdf    Note In order to use CuDNN, the following must be satisfied: targets must be in concatenated format, all input_lengths must be T.  blank=0blank=0blank=0  , target_lengths \u2264256\\leq 256\u2264256  , the integer arguments must be of dtype torch.int32. The regular implementation uses the (more common in PyTorch) torch.long dtype.   Note In some circumstances when using the CUDA backend with CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting torch.backends.cudnn.deterministic = True. Please see the notes on Reproducibility for background.  ", "parameters": ["blank (python:int, optional) : blank label. Default 000.", "reduction (string, optional) : Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the output losses will be divided by the target lengths andthen the mean over the batch is taken. Default: 'mean'", "zero_infinity (bool, optional) : Whether to zero infinite losses and the associated gradients.Default: FalseInfinite losses mainly occur when the inputs are too shortto be aligned to the targets."], "returns": null, "example": " T = 50      # Input sequence length\n C = 20      # Number of classes (including blank)\n N = 16      # Batch size\n S = 30      # Target sequence length of longest target in batch\n S_min = 10  # Minimum target length, for demonstration purposes\n\n # Initialize random batch of input vectors, for *size = (T,N,C)\n input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()\n\n # Initialize random batch of targets (0 = blank, 1:C = classes)\n target = torch.randint(low=1, high=C, size=(N, S), dtype=torch.long)\n\n input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)\n target_lengths = torch.randint(low=S_min, high=S, size=(N,), dtype=torch.long)\n ctc_loss = nn.CTCLoss()\n loss = ctc_loss(input, target, input_lengths, target_lengths)\n loss.backward()\n\n", "shape": " Log_probs: Tensor of size (T,N,C)(T, N, C)(T,N,C)  , where T=input\u00a0lengthT = \\text{input length}T=input\u00a0length  , N=batch\u00a0sizeN = \\text{batch size}N=batch\u00a0size  , and C=number\u00a0of\u00a0classes\u00a0(including\u00a0blank)C = \\text{number of classes (including blank)}C=number\u00a0of\u00a0classes\u00a0(including\u00a0blank)  . The logarithmized probabilities of the outputs (e.g. obtained with torch.nn.functional.log_softmax()). Targets: Tensor of size (N,S)(N, S)(N,S)   or (sum\u2061(target_lengths))(\\operatorname{sum}(\\text{target\\_lengths}))(sum(target_lengths))  , where N=batch\u00a0sizeN = \\text{batch size}N=batch\u00a0size   and S=max\u00a0target\u00a0length,\u00a0if\u00a0shape\u00a0is\u00a0(N,S)S = \\text{max target length, if shape is } (N, S)S=max\u00a0target\u00a0length,\u00a0if\u00a0shape\u00a0is\u00a0(N,S)  . It represent the target sequences. Each element in the target sequence is a class index. And the target index cannot be blank (default=0). In the (N,S)(N, S)(N,S)   form, targets are padded to the length of the longest sequence, and stacked. In the (sum\u2061(target_lengths))(\\operatorname{sum}(\\text{target\\_lengths}))(sum(target_lengths))   form, the targets are assumed to be un-padded and concatenated within 1 dimension. Input_lengths: Tuple or tensor of size (N)(N)(N)  , where N=batch\u00a0sizeN = \\text{batch size}N=batch\u00a0size  . It represent the lengths of the inputs (must each be \u2264T\\leq T\u2264T  ). And the lengths are specified for each sequence to achieve masking under the assumption that sequences are padded to equal lengths. Target_lengths: Tuple or tensor of size (N)(N)(N)  , where N=batch\u00a0sizeN = \\text{batch size}N=batch\u00a0size  . It represent lengths of the targets. Lengths are specified for each sequence to achieve masking under the assumption that sequences are padded to equal lengths. If target shape is (N,S)(N,S)(N,S)  , target_lengths are effectively the stop index sns_nsn\u200b   for each target sequence, such that target_n = targets[n,0:s_n] for each target in a batch. Lengths must each be \u2264S\\leq S\u2264S   If the targets are given as a 1d tensor that is the concatenation of individual targets, the target_lengths must add up to the total length of the tensor. Output: scalar. If reduction is 'none', then (N)(N)(N)  , where N=batch\u00a0sizeN = \\text{batch size}N=batch\u00a0size  .  "},
{"library": "torch", "item_id": "torch.nn.NLLLoss", "item_type": "class", "code": "classtorch.nn.NLLLoss(weight=None,size_average=None,ignore_index=-100,reduce=None,reduction='mean')", "description": "The negative log likelihood loss. It is useful to train a classification problem with C classes. If provided, the optional argument weight should be a 1D Tensor assigning weight to each of the classes. This is particularly useful when you have an unbalanced training set. The input given through a forward call is expected to contain log-probabilities of each class. input has to be a Tensor of size either (minibatch,C)(minibatch, C)(minibatch,C)   or (minibatch,C,d1,d2,...,dK)(minibatch, C, d_1, d_2, ..., d_K)(minibatch,C,d1\u200b,d2\u200b,...,dK\u200b)   with K\u22651K \\geq 1K\u22651   for the K-dimensional case (described later). Obtaining log-probabilities in a neural network is easily achieved by adding a  LogSoftmax  layer in the last layer of your network. You may use CrossEntropyLoss instead, if you prefer not to add an extra layer. The target that this loss expects should be a class index in the range [0,C\u22121][0, C-1][0,C\u22121]   where C = number of classes; if ignore_index is specified, this loss also accepts this class index (this index may not necessarily be in the class range). The unreduced (i.e. with reduction set to 'none') loss can be described as:  \u2113(x,y)=L={l1,\u2026,lN}\u22a4,ln=\u2212wynxn,yn,wc=weight[c]\u22c51{c=\u0338ignore_index},\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad l_n = - w_{y_n} x_{n,y_n}, \\quad w_{c} = \\text{weight}[c] \\cdot \\mathbb{1}\\{c \\not= \\text{ignore\\_index}\\},  \u2113(x,y)=L={l1\u200b,\u2026,lN\u200b}\u22a4,ln\u200b=\u2212wyn\u200b\u200bxn,yn\u200b\u200b,wc\u200b=weight[c]\u22c51{c\ue020\u200b=ignore_index},  where NNN   is the batch size. If reduction is not 'none' (default 'mean'), then  \u2113(x,y)={\u2211n=1N1\u2211n=1Nwynln,if\u00a0reduction=\u2019mean\u2019;\u2211n=1Nln,if\u00a0reduction=\u2019sum\u2019.\\ell(x, y) = \\begin{cases}     \\sum_{n=1}^N \\frac{1}{\\sum_{n=1}^N w_{y_n}} l_n, &amp;     \\text{if reduction} = \\text{'mean';}\\\\     \\sum_{n=1}^N l_n,  &amp;     \\text{if reduction} = \\text{'sum'.} \\end{cases}  \u2113(x,y)={\u2211n=1N\u200b\u2211n=1N\u200bwyn\u200b\u200b1\u200bln\u200b,\u2211n=1N\u200bln\u200b,\u200bif\u00a0reduction=\u2019mean\u2019;if\u00a0reduction=\u2019sum\u2019.\u200b  Can also be used for higher dimension inputs, such as 2D images, by providing an input of size (minibatch,C,d1,d2,...,dK)(minibatch, C, d_1, d_2, ..., d_K)(minibatch,C,d1\u200b,d2\u200b,...,dK\u200b)   with K\u22651K \\geq 1K\u22651  , where KKK   is the number of dimensions, and a target of appropriate shape (see below). In the case of images, it computes NLL loss per-pixel.  Parameters  weight (Tensor, optional) \u2013 a manual rescaling weight given to each class. If given, it has to be a Tensor of size C. Otherwise, it is treated as if having all ones. size_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True ignore_index (python:int, optional) \u2013 Specifies a target value that is ignored and does not contribute to the input gradient. When size_average is True, the loss is averaged over non-ignored targets. reduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'     Shape: Input: (N,C)(N, C)(N,C)   where C = number of classes, or (N,C,d1,d2,...,dK)(N, C, d_1, d_2, ..., d_K)(N,C,d1\u200b,d2\u200b,...,dK\u200b)   with K\u22651K \\geq 1K\u22651   in the case of K-dimensional loss. Target: (N)(N)(N)   where each value is 0\u2264targets[i]\u2264C\u221210 \\leq \\text{targets}[i] \\leq C-10\u2264targets[i]\u2264C\u22121  , or (N,d1,d2,...,dK)(N, d_1, d_2, ..., d_K)(N,d1\u200b,d2\u200b,...,dK\u200b)   with K\u22651K \\geq 1K\u22651   in the case of K-dimensional loss. Output: scalar. If reduction is 'none', then the same size as the target: (N)(N)(N)  , or (N,d1,d2,...,dK)(N, d_1, d_2, ..., d_K)(N,d1\u200b,d2\u200b,...,dK\u200b)   with K\u22651K \\geq 1K\u22651   in the case of K-dimensional loss.    Examples: &gt;&gt;&gt; m = nn.LogSoftmax(dim=1) &gt;&gt;&gt; loss = nn.NLLLoss() &gt;&gt;&gt; # input is of size N x C = 3 x 5 &gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True) &gt;&gt;&gt; # each element in target has to have 0 &lt;= value &lt; C &gt;&gt;&gt; target = torch.tensor([1, 0, 4]) &gt;&gt;&gt; output = loss(m(input), target) &gt;&gt;&gt; output.backward() &gt;&gt;&gt; &gt;&gt;&gt; &gt;&gt;&gt; # 2D loss example (used, for example, with image inputs) &gt;&gt;&gt; N, C = 5, 4 &gt;&gt;&gt; loss = nn.NLLLoss() &gt;&gt;&gt; # input is of size N x C x height x width &gt;&gt;&gt; data = torch.randn(N, 16, 10, 10) &gt;&gt;&gt; conv = nn.Conv2d(16, C, (3, 3)) &gt;&gt;&gt; m = nn.LogSoftmax(dim=1) &gt;&gt;&gt; # each element in target has to have 0 &lt;= value &lt; C &gt;&gt;&gt; target = torch.empty(N, 8, 8, dtype=torch.long).random_(0, C) &gt;&gt;&gt; output = loss(m(conv(data)), target) &gt;&gt;&gt; output.backward()   ", "parameters": ["weight (Tensor, optional) : a manual rescaling weight given to eachclass. If given, it has to be a Tensor of size C. Otherwise, it istreated as if having all ones.", "size_average (bool, optional) : Deprecated (see reduction). By default,the losses are averaged over each loss element in the batch. Note that forsome losses, there are multiple elements per sample. If the field size_averageis set to False, the losses are instead summed for each minibatch. Ignoredwhen reduce is False. Default: True", "ignore_index (python:int, optional) : Specifies a target value that is ignoredand does not contribute to the input gradient. Whensize_average is True, the loss is averaged overnon-ignored targets.", "reduce (bool, optional) : Deprecated (see reduction). By default, thelosses are averaged or summed over observations for each minibatch dependingon size_average. When reduce is False, returns a loss perbatch element instead and ignores size_average. Default: True", "reduction (string, optional) : Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"], "returns": null, "example": " m = nn.LogSoftmax(dim=1)\n loss = nn.NLLLoss()\n # input is of size N x C = 3 x 5\n input = torch.randn(3, 5, requires_grad=True)\n # each element in target has to have 0 &lt;= value &lt; C\n target = torch.tensor([1, 0, 4])\n output = loss(m(input), target)\n output.backward()\n\n\n # 2D loss example (used, for example, with image inputs)\n N, C = 5, 4\n loss = nn.NLLLoss()\n # input is of size N x C x height x width\n data = torch.randn(N, 16, 10, 10)\n conv = nn.Conv2d(16, C, (3, 3))\n m = nn.LogSoftmax(dim=1)\n # each element in target has to have 0 &lt;= value &lt; C\n target = torch.empty(N, 8, 8, dtype=torch.long).random_(0, C)\n output = loss(m(conv(data)), target)\n output.backward()\n\n", "shape": " Input: (N,C)(N, C)(N,C)   where C = number of classes, or (N,C,d1,d2,...,dK)(N, C, d_1, d_2, ..., d_K)(N,C,d1\u200b,d2\u200b,...,dK\u200b)   with K\u22651K \\geq 1K\u22651   in the case of K-dimensional loss. Target: (N)(N)(N)   where each value is 0\u2264targets[i]\u2264C\u221210 \\leq \\text{targets}[i] \\leq C-10\u2264targets[i]\u2264C\u22121  , or (N,d1,d2,...,dK)(N, d_1, d_2, ..., d_K)(N,d1\u200b,d2\u200b,...,dK\u200b)   with K\u22651K \\geq 1K\u22651   in the case of K-dimensional loss. Output: scalar. If reduction is 'none', then the same size as the target: (N)(N)(N)  , or (N,d1,d2,...,dK)(N, d_1, d_2, ..., d_K)(N,d1\u200b,d2\u200b,...,dK\u200b)   with K\u22651K \\geq 1K\u22651   in the case of K-dimensional loss.  "},
{"library": "torch", "item_id": "torch.svd", "item_type": "function", "code": "torch.svd(input,some=True,compute_uv=True,out=None)-&gt;(Tensor,Tensor,Tensor)", "description": "This function returns a namedtuple (U, S, V) which is the singular value decomposition of a input real matrix or batches of real matrices input such that input=U\u00d7diag(S)\u00d7VTinput = U \\times diag(S) \\times V^Tinput=U\u00d7diag(S)\u00d7VT  . If some is True (default), the method returns the reduced singular value decomposition i.e., if the last two dimensions of input are m and n, then the returned U and V matrices will contain only min(n,m)min(n, m)min(n,m)   orthonormal columns. If compute_uv is False, the returned U and V matrices will be zero matrices of shape (m\u00d7m)(m \\times m)(m\u00d7m)   and (n\u00d7n)(n \\times n)(n\u00d7n)   respectively. some will be ignored here.  Note The singular values are returned in descending order. If input is a batch of matrices, then the singular values of each matrix in the batch is returned in descending order.   Note The implementation of SVD on CPU uses the LAPACK routine ?gesdd (a divide-and-conquer algorithm) instead of ?gesvd for speed. Analogously, the SVD on GPU uses the MAGMA routine gesdd as well.   Note Irrespective of the original strides, the returned matrix U will be transposed, i.e. with strides U.contiguous().transpose(-2, -1).stride()   Note Extra care needs to be taken when backward through U and V outputs. Such operation is really only stable when input is full rank with all distinct singular values. Otherwise, NaN can appear as the gradients are not properly defined. Also, notice that double backward will usually do an additional backward through U and V even if the original backward is only on S.   Note When some = False, the gradients on U[..., :, min(m, n):] and V[..., :, min(m, n):] will be ignored in backward as those vectors can be arbitrary bases of the subspaces.   Note When compute_uv = False, backward cannot be performed since U and V from the forward pass is required for the backward operation.   Parameters  input (Tensor) \u2013 the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n)   where * is zero or more batch dimensions consisting of m\u00d7nm \\times nm\u00d7n   matrices. some (bool, optional) \u2013 controls the shape of returned U and V compute_uv (bool, optional) \u2013 option whether to compute U and V or not out (tuple, optional) \u2013 the output tuple of tensors    Example: &gt;&gt;&gt; a = torch.randn(5, 3) &gt;&gt;&gt; a tensor([[ 0.2364, -0.7752,  0.6372],         [ 1.7201,  0.7394, -0.0504],         [-0.3371, -1.0584,  0.5296],         [ 0.3550, -0.4022,  1.5569],         [ 0.2445, -0.0158,  1.1414]]) &gt;&gt;&gt; u, s, v = torch.svd(a) &gt;&gt;&gt; u tensor([[ 0.4027,  0.0287,  0.5434],         [-0.1946,  0.8833,  0.3679],         [ 0.4296, -0.2890,  0.5261],         [ 0.6604,  0.2717, -0.2618],         [ 0.4234,  0.2481, -0.4733]]) &gt;&gt;&gt; s tensor([2.3289, 2.0315, 0.7806]) &gt;&gt;&gt; v tensor([[-0.0199,  0.8766,  0.4809],         [-0.5080,  0.4054, -0.7600],         [ 0.8611,  0.2594, -0.4373]]) &gt;&gt;&gt; torch.dist(a, torch.mm(torch.mm(u, torch.diag(s)), v.t())) tensor(8.6531e-07) &gt;&gt;&gt; a_big = torch.randn(7, 5, 3) &gt;&gt;&gt; u, s, v = torch.svd(a_big) &gt;&gt;&gt; torch.dist(a_big, torch.matmul(torch.matmul(u, torch.diag_embed(s)), v.transpose(-2, -1))) tensor(2.6503e-06)   ", "parameters": ["input (Tensor) : the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) where * is zero or morebatch dimensions consisting of m\u00d7nm \\times nm\u00d7n matrices.", "some (bool, optional) : controls the shape of returned U and V", "compute_uv (bool, optional) : option whether to compute U and V or not", "out (tuple, optional) : the output tuple of tensors"], "returns": null, "example": " a = torch.randn(5, 3)\n a\ntensor([[ 0.2364, -0.7752,  0.6372],\n        [ 1.7201,  0.7394, -0.0504],\n        [-0.3371, -1.0584,  0.5296],\n        [ 0.3550, -0.4022,  1.5569],\n        [ 0.2445, -0.0158,  1.1414]])\n u, s, v = torch.svd(a)\n u\ntensor([[ 0.4027,  0.0287,  0.5434],\n        [-0.1946,  0.8833,  0.3679],\n        [ 0.4296, -0.2890,  0.5261],\n        [ 0.6604,  0.2717, -0.2618],\n        [ 0.4234,  0.2481, -0.4733]])\n s\ntensor([2.3289, 2.0315, 0.7806])\n v\ntensor([[-0.0199,  0.8766,  0.4809],\n        [-0.5080,  0.4054, -0.7600],\n        [ 0.8611,  0.2594, -0.4373]])\n torch.dist(a, torch.mm(torch.mm(u, torch.diag(s)), v.t()))\ntensor(8.6531e-07)\n a_big = torch.randn(7, 5, 3)\n u, s, v = torch.svd(a_big)\n torch.dist(a_big, torch.matmul(torch.matmul(u, torch.diag_embed(s)), v.transpose(-2, -1)))\ntensor(2.6503e-06)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.symeig", "item_type": "function", "code": "torch.symeig(input,eigenvectors=False,upper=True,out=None)-&gt;(Tensor,Tensor)", "description": "This function returns eigenvalues and eigenvectors of a real symmetric matrix input or a batch of real symmetric matrices, represented by a namedtuple (eigenvalues, eigenvectors). This function calculates all eigenvalues (and vectors) of input such that input=Vdiag(e)VT\\text{input} = V \\text{diag}(e) V^Tinput=Vdiag(e)VT  . The boolean argument eigenvectors defines computation of both eigenvectors and eigenvalues or eigenvalues only. If it is False, only eigenvalues are computed. If it is True, both eigenvalues and eigenvectors are computed. Since the input matrix input is supposed to be symmetric, only the upper triangular portion is used by default. If upper is False, then lower triangular portion is used.  Note The eigenvalues are returned in ascending order. If input is a batch of matrices, then the eigenvalues of each matrix in the batch is returned in ascending order.   Note Irrespective of the original strides, the returned matrix V will be transposed, i.e. with strides V.contiguous().transpose(-1, -2).stride().   Note Extra care needs to be taken when backward through outputs. Such operation is really only stable when all eigenvalues are distinct. Otherwise, NaN can appear as the gradients are not properly defined.   Parameters  input (Tensor) \u2013 the input tensor of size (\u2217,n,n)(*, n, n)(\u2217,n,n)   where * is zero or more batch dimensions consisting of symmetric matrices. eigenvectors (boolean, optional) \u2013 controls whether eigenvectors have to be computed upper (boolean, optional) \u2013 controls whether to consider upper-triangular or lower-triangular region out (tuple, optional) \u2013 the output tuple of (Tensor, Tensor)   Returns A namedtuple (eigenvalues, eigenvectors) containing   eigenvalues (Tensor): Shape (\u2217,m)(*, m)(\u2217,m)  . The eigenvalues in ascending order. eigenvectors (Tensor): Shape (\u2217,m,m)(*, m, m)(\u2217,m,m)  . If eigenvectors=False, it\u2019s an empty tensor. Otherwise, this tensor contains the orthonormal eigenvectors of the input.     Return type (Tensor, Tensor)   Examples: &gt;&gt;&gt; a = torch.randn(5, 5) &gt;&gt;&gt; a = a + a.t()  # To make a symmetric &gt;&gt;&gt; a tensor([[-5.7827,  4.4559, -0.2344, -1.7123, -1.8330],         [ 4.4559,  1.4250, -2.8636, -3.2100, -0.1798],         [-0.2344, -2.8636,  1.7112, -5.5785,  7.1988],         [-1.7123, -3.2100, -5.5785, -2.6227,  3.1036],         [-1.8330, -0.1798,  7.1988,  3.1036, -5.1453]]) &gt;&gt;&gt; e, v = torch.symeig(a, eigenvectors=True) &gt;&gt;&gt; e tensor([-13.7012,  -7.7497,  -2.3163,   5.2477,   8.1050]) &gt;&gt;&gt; v tensor([[ 0.1643,  0.9034, -0.0291,  0.3508,  0.1817],         [-0.2417, -0.3071, -0.5081,  0.6534,  0.4026],         [-0.5176,  0.1223, -0.0220,  0.3295, -0.7798],         [-0.4850,  0.2695, -0.5773, -0.5840,  0.1337],         [ 0.6415, -0.0447, -0.6381, -0.0193, -0.4230]]) &gt;&gt;&gt; a_big = torch.randn(5, 2, 2) &gt;&gt;&gt; a_big = a_big + a_big.transpose(-2, -1)  # To make a_big symmetric &gt;&gt;&gt; e, v = a_big.symeig(eigenvectors=True) &gt;&gt;&gt; torch.allclose(torch.matmul(v, torch.matmul(e.diag_embed(), v.transpose(-2, -1))), a_big) True   ", "parameters": ["input (Tensor) : the input tensor of size (\u2217,n,n)(*, n, n)(\u2217,n,n) where * is zero or morebatch dimensions consisting of symmetric matrices.", "eigenvectors (boolean, optional) : controls whether eigenvectors have to be computed", "upper (boolean, optional) : controls whether to consider upper-triangular or lower-triangular region", "out (tuple, optional) : the output tuple of (Tensor, Tensor)", "eigenvalues (Tensor): Shape (\u2217,m)(*, m)(\u2217,m). The eigenvalues in ascending order.", "eigenvectors (Tensor): Shape (\u2217,m,m)(*, m, m)(\u2217,m,m).If eigenvectors=False, it\u2019s an empty tensor.Otherwise, this tensor contains the orthonormal eigenvectors of the input."], "returns": "A namedtuple (eigenvalues, eigenvectors) containingeigenvalues (Tensor): Shape (\u2217,m)(*, m)(\u2217,m). The eigenvalues in ascending order.eigenvectors (Tensor): Shape (\u2217,m,m)(*, m, m)(\u2217,m,m).If eigenvectors=False, it\u2019s an empty tensor.Otherwise, this tensor contains the orthonormal eigenvectors of the input.", "example": " a = torch.randn(5, 5)\n a = a + a.t()  # To make a symmetric\n a\ntensor([[-5.7827,  4.4559, -0.2344, -1.7123, -1.8330],\n        [ 4.4559,  1.4250, -2.8636, -3.2100, -0.1798],\n        [-0.2344, -2.8636,  1.7112, -5.5785,  7.1988],\n        [-1.7123, -3.2100, -5.5785, -2.6227,  3.1036],\n        [-1.8330, -0.1798,  7.1988,  3.1036, -5.1453]])\n e, v = torch.symeig(a, eigenvectors=True)\n e\ntensor([-13.7012,  -7.7497,  -2.3163,   5.2477,   8.1050])\n v\ntensor([[ 0.1643,  0.9034, -0.0291,  0.3508,  0.1817],\n        [-0.2417, -0.3071, -0.5081,  0.6534,  0.4026],\n        [-0.5176,  0.1223, -0.0220,  0.3295, -0.7798],\n        [-0.4850,  0.2695, -0.5773, -0.5840,  0.1337],\n        [ 0.6415, -0.0447, -0.6381, -0.0193, -0.4230]])\n a_big = torch.randn(5, 2, 2)\n a_big = a_big + a_big.transpose(-2, -1)  # To make a_big symmetric\n e, v = a_big.symeig(eigenvectors=True)\n torch.allclose(torch.matmul(v, torch.matmul(e.diag_embed(), v.transpose(-2, -1))), a_big)\nTrue\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.PoissonNLLLoss", "item_type": "class", "code": "classtorch.nn.PoissonNLLLoss(log_input=True,full=False,size_average=None,eps=1e-08,reduce=None,reduction='mean')", "description": "Negative log likelihood loss with Poisson distribution of target. The loss can be described as:  target\u223cPoisson(input)loss(input,target)=input\u2212target\u2217log\u2061(input)+log\u2061(target!)\\text{target} \\sim \\mathrm{Poisson}(\\text{input})  \\text{loss}(\\text{input}, \\text{target}) = \\text{input} - \\text{target} * \\log(\\text{input})                             + \\log(\\text{target!})target\u223cPoisson(input)loss(input,target)=input\u2212target\u2217log(input)+log(target!)  The last term can be omitted or approximated with Stirling formula. The approximation is used for target values more than 1. For targets less or equal to 1 zeros are added to the loss.  Parameters  log_input (bool, optional) \u2013 if True the loss is computed as exp\u2061(input)\u2212target\u2217input\\exp(\\text{input}) - \\text{target}*\\text{input}exp(input)\u2212target\u2217input  , if False the loss is input\u2212target\u2217log\u2061(input+eps)\\text{input} - \\text{target}*\\log(\\text{input}+\\text{eps})input\u2212target\u2217log(input+eps)  . full (bool, optional) \u2013 whether to compute full loss, i. e. to add the Stirling approximation term  target\u2217log\u2061(target)\u2212target+0.5\u2217log\u2061(2\u03c0target).\\text{target}*\\log(\\text{target}) - \\text{target} + 0.5 * \\log(2\\pi\\text{target}).  target\u2217log(target)\u2212target+0.5\u2217log(2\u03c0target).   size_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True eps (python:float, optional) \u2013 Small value to avoid evaluation of log\u2061(0)\\log(0)log(0)   when log_input = False. Default: 1e-8 reduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'    Examples: &gt;&gt;&gt; loss = nn.PoissonNLLLoss() &gt;&gt;&gt; log_input = torch.randn(5, 2, requires_grad=True) &gt;&gt;&gt; target = torch.randn(5, 2) &gt;&gt;&gt; output = loss(log_input, target) &gt;&gt;&gt; output.backward()    Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where \u2217*\u2217   means, any number of additional dimensions Target: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input Output: scalar by default. If reduction is 'none', then (N,\u2217)(N, *)(N,\u2217)  , the same shape as the input    ", "parameters": ["log_input (bool, optional) : if True the loss is computed asexp\u2061(input)\u2212target\u2217input\\exp(\\text{input}) - \\text{target}*\\text{input}exp(input)\u2212target\u2217input, if False the loss isinput\u2212target\u2217log\u2061(input+eps)\\text{input} - \\text{target}*\\log(\\text{input}+\\text{eps})input\u2212target\u2217log(input+eps).", "full (bool, optional) : whether to compute full loss, i. e. to add theStirling approximation termtarget\u2217log\u2061(target)\u2212target+0.5\u2217log\u2061(2\u03c0target).\\text{target}*\\log(\\text{target}) - \\text{target} + 0.5 * \\log(2\\pi\\text{target}).target\u2217log(target)\u2212target+0.5\u2217log(2\u03c0target).", "size_average (bool, optional) : Deprecated (see reduction). By default,the losses are averaged over each loss element in the batch. Note that forsome losses, there are multiple elements per sample. If the field size_averageis set to False, the losses are instead summed for each minibatch. Ignoredwhen reduce is False. Default: True", "eps (python:float, optional) : Small value to avoid evaluation of log\u2061(0)\\log(0)log(0) whenlog_input = False. Default: 1e-8", "reduce (bool, optional) : Deprecated (see reduction). By default, thelosses are averaged or summed over observations for each minibatch dependingon size_average. When reduce is False, returns a loss perbatch element instead and ignores size_average. Default: True", "reduction (string, optional) : Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"], "returns": null, "example": " loss = nn.PoissonNLLLoss()\n log_input = torch.randn(5, 2, requires_grad=True)\n target = torch.randn(5, 2)\n output = loss(log_input, target)\n output.backward()\n\n", "shape": " Input: (N,\u2217)(N, *)(N,\u2217)   where \u2217*\u2217   means, any number of additional dimensions Target: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input Output: scalar by default. If reduction is 'none', then (N,\u2217)(N, *)(N,\u2217)  , the same shape as the input  "},
{"library": "torch", "item_id": "torch.nn.KLDivLoss", "item_type": "class", "code": "classtorch.nn.KLDivLoss(size_average=None,reduce=None,reduction='mean')", "description": "The Kullback-Leibler divergence Loss KL divergence is a useful distance measure for continuous distributions and is often useful when performing direct regression over the space of (discretely sampled) continuous output distributions. As with NLLLoss, the input given is expected to contain log-probabilities and is not restricted to a 2D Tensor. The targets are given as probabilities (i.e. without taking the logarithm). This criterion expects a target Tensor of the same size as the input Tensor. The unreduced (i.e. with reduction set to 'none') loss can be described as:  l(x,y)=L={l1,\u2026,lN},ln=yn\u22c5(log\u2061yn\u2212xn)l(x,y) = L = \\{ l_1,\\dots,l_N \\}, \\quad l_n = y_n \\cdot \\left( \\log y_n - x_n \\right)  l(x,y)=L={l1\u200b,\u2026,lN\u200b},ln\u200b=yn\u200b\u22c5(logyn\u200b\u2212xn\u200b)  where the index NNN   spans all dimensions of input and LLL   has the same shape as input. If reduction is not 'none' (default 'mean'), then:  \u2113(x,y)={mean\u2061(L),if\u00a0reduction=\u2019mean\u2019;sum\u2061(L),if\u00a0reduction=\u2019sum\u2019.\\ell(x, y) = \\begin{cases}     \\operatorname{mean}(L), &amp; \\text{if reduction} = \\text{'mean';} \\\\     \\operatorname{sum}(L),  &amp; \\text{if reduction} = \\text{'sum'.} \\end{cases}  \u2113(x,y)={mean(L),sum(L),\u200bif\u00a0reduction=\u2019mean\u2019;if\u00a0reduction=\u2019sum\u2019.\u200b  In default reduction mode 'mean', the losses are averaged for each minibatch over observations as well as over dimensions. 'batchmean' mode gives the correct KL divergence where losses are averaged over batch dimension only. 'mean' mode\u2019s behavior will be changed to the same as 'batchmean' in the next major release.  Parameters  size_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'batchmean' | 'sum' | 'mean'. 'none': no reduction will be applied. 'batchmean': the sum of the output will be divided by batchsize. 'sum': the output will be summed. 'mean': the output will be divided by the number of elements in the output. Default: 'mean'     Note size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction.   Note reduction = 'mean' doesn\u2019t return the true kl divergence value, please use reduction = 'batchmean' which aligns with KL math definition. In the next major release, 'mean' will be changed to be the same as 'batchmean'.   Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where \u2217*\u2217   means, any number of additional dimensions Target: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input Output: scalar by default. If :attr:reduction is 'none', then (N,\u2217)(N, *)(N,\u2217)  , the same shape as the input    ", "parameters": ["size_average (bool, optional) : Deprecated (see reduction). By default,the losses are averaged over each loss element in the batch. Note that forsome losses, there are multiple elements per sample. If the field size_averageis set to False, the losses are instead summed for each minibatch. Ignoredwhen reduce is False. Default: True", "reduce (bool, optional) : Deprecated (see reduction). By default, thelosses are averaged or summed over observations for each minibatch dependingon size_average. When reduce is False, returns a loss perbatch element instead and ignores size_average. Default: True", "reduction (string, optional) : Specifies the reduction to apply to the output:'none' | 'batchmean' | 'sum' | 'mean'.'none': no reduction will be applied.'batchmean': the sum of the output will be divided by batchsize.'sum': the output will be summed.'mean': the output will be divided by the number of elements in the output.Default: 'mean'"], "returns": null, "example": "NA", "shape": " Input: (N,\u2217)(N, *)(N,\u2217)   where \u2217*\u2217   means, any number of additional dimensions Target: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input Output: scalar by default. If :attr:reduction is 'none', then (N,\u2217)(N, *)(N,\u2217)  , the same shape as the input  "},
{"library": "torch", "item_id": "torch.trapz", "item_type": "function", "code": "torch.trapz()", "description": "  torch.trapz(y, x, *, dim=-1) \u2192 Tensor   Estimate \u222by\u2009dx\\int y\\,dx\u222bydx   along dim, using the trapezoid rule.  Parameters  y (Tensor) \u2013 The values of the function to integrate x (Tensor) \u2013 The points at which the function y is sampled. If x is not in ascending order, intervals on which it is decreasing contribute negatively to the estimated integral (i.e., the convention \u222babf=\u2212\u222bbaf\\int_a^b f = -\\int_b^a f\u222bab\u200bf=\u2212\u222bba\u200bf   is followed). dim (python:int) \u2013 The dimension along which to integrate. By default, use the last dimension.   Returns A Tensor with the same shape as the input, except with dim removed. Each element of the returned tensor represents the estimated integral \u222by\u2009dx\\int y\\,dx\u222bydx   along dim.   Example: &gt;&gt;&gt; y = torch.randn((2, 3)) &gt;&gt;&gt; y tensor([[-2.1156,  0.6857, -0.2700],         [-1.2145,  0.5540,  2.0431]]) &gt;&gt;&gt; x = torch.tensor([[1, 3, 4], [1, 2, 3]]) &gt;&gt;&gt; torch.trapz(y, x) tensor([-1.2220,  0.9683])     torch.trapz(y, *, dx=1, dim=-1) \u2192 Tensor   As above, but the sample points are spaced uniformly at a distance of dx.  Parameters  y (Tensor) \u2013 The values of the function to integrate dx (python:float) \u2013 The distance between points at which y is sampled. dim (python:int) \u2013 The dimension along which to integrate. By default, use the last dimension.   Returns A Tensor with the same shape as the input, except with dim removed. Each element of the returned tensor represents the estimated integral \u222by\u2009dx\\int y\\,dx\u222bydx   along dim.   ", "parameters": ["y (Tensor) : The values of the function to integrate", "x (Tensor) : The points at which the function y is sampled.If x is not in ascending order, intervals on which it is decreasingcontribute negatively to the estimated integral (i.e., the convention\u222babf=\u2212\u222bbaf\\int_a^b f = -\\int_b^a f\u222bab\u200bf=\u2212\u222bba\u200bf is followed).", "dim (python:int) : The dimension along which to integrate.By default, use the last dimension.", "y (Tensor) : The values of the function to integrate", "dx (python:float) : The distance between points at which y is sampled.", "dim (python:int) : The dimension along which to integrate.By default, use the last dimension."], "returns": "A Tensor with the same shape as the input, except with dim removed.Each element of the returned tensor represents the estimated integral\u222by\u2009dx\\int y\\,dx\u222bydx along dim.", "example": " y = torch.randn((2, 3))\n y\ntensor([[-2.1156,  0.6857, -0.2700],\n        [-1.2145,  0.5540,  2.0431]])\n x = torch.tensor([[1, 3, 4], [1, 2, 3]])\n torch.trapz(y, x)\ntensor([-1.2220,  0.9683])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.trapz(y,x,*,dim=-1)\u2192Tensor", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "NA", "item_type": "function", "code": "torch.trapz(y,*,dx=1,dim=-1)\u2192Tensor", "description": "", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.triangular_solve", "item_type": "function", "code": "torch.triangular_solve(input,A,upper=True,transpose=False,unitriangular=False)-&gt;(Tensor,Tensor)", "description": "Solves a system of equations with a triangular coefficient matrix AAA   and multiple right-hand sides bbb  . In particular, solves AX=bAX = bAX=b   and assumes AAA   is upper-triangular with the default keyword arguments. torch.triangular_solve(b, A) can take in 2D inputs b, A or inputs that are batches of 2D matrices. If the inputs are batches, then returns batched outputs X  Parameters  input (Tensor) \u2013 multiple right-hand sides of size (\u2217,m,k)(*, m, k)(\u2217,m,k)   where \u2217*\u2217   is zero of more batch dimensions (bbb  ) A (Tensor) \u2013 the input triangular coefficient matrix of size (\u2217,m,m)(*, m, m)(\u2217,m,m)   where \u2217*\u2217   is zero or more batch dimensions upper (bool, optional) \u2013 whether to solve the upper-triangular system of equations (default) or the lower-triangular system of equations. Default: True. transpose (bool, optional) \u2013 whether AAA   should be transposed before being sent into the solver. Default: False. unitriangular (bool, optional) \u2013 whether AAA   is unit triangular. If True, the diagonal elements of AAA   are assumed to be 1 and not referenced from AAA  . Default: False.   Returns A namedtuple (solution, cloned_coefficient) where cloned_coefficient is a clone of AAA   and solution is the solution XXX   to AX=bAX = bAX=b   (or whatever variant of the system of equations, depending on the keyword arguments.)   Examples: &gt;&gt;&gt; A = torch.randn(2, 2).triu() &gt;&gt;&gt; A tensor([[ 1.1527, -1.0753],         [ 0.0000,  0.7986]]) &gt;&gt;&gt; b = torch.randn(2, 3) &gt;&gt;&gt; b tensor([[-0.0210,  2.3513, -1.5492],         [ 1.5429,  0.7403, -1.0243]]) &gt;&gt;&gt; torch.triangular_solve(b, A) torch.return_types.triangular_solve( solution=tensor([[ 1.7841,  2.9046, -2.5405],         [ 1.9320,  0.9270, -1.2826]]), cloned_coefficient=tensor([[ 1.1527, -1.0753],         [ 0.0000,  0.7986]]))   ", "parameters": ["input (Tensor) : multiple right-hand sides of size (\u2217,m,k)(*, m, k)(\u2217,m,k) where\u2217*\u2217 is zero of more batch dimensions (bbb)", "A (Tensor) : the input triangular coefficient matrix of size (\u2217,m,m)(*, m, m)(\u2217,m,m)where \u2217*\u2217 is zero or more batch dimensions", "upper (bool, optional) : whether to solve the upper-triangular systemof equations (default) or the lower-triangular system of equations. Default: True.", "transpose (bool, optional) : whether AAA should be transposed beforebeing sent into the solver. Default: False.", "unitriangular (bool, optional) : whether AAA is unit triangular.If True, the diagonal elements of AAA are assumed to be1 and not referenced from AAA. Default: False."], "returns": "A namedtuple (solution, cloned_coefficient) where cloned_coefficientis a clone of AAA and solution is the solution XXX to AX=bAX = bAX=b(or whatever variant of the system of equations, depending on the keyword arguments.)", "example": " A = torch.randn(2, 2).triu()\n A\ntensor([[ 1.1527, -1.0753],\n        [ 0.0000,  0.7986]])\n b = torch.randn(2, 3)\n b\ntensor([[-0.0210,  2.3513, -1.5492],\n        [ 1.5429,  0.7403, -1.0243]])\n torch.triangular_solve(b, A)\ntorch.return_types.triangular_solve(\nsolution=tensor([[ 1.7841,  2.9046, -2.5405],\n        [ 1.9320,  0.9270, -1.2826]]),\ncloned_coefficient=tensor([[ 1.1527, -1.0753],\n        [ 0.0000,  0.7986]]))\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.BCELoss", "item_type": "class", "code": "classtorch.nn.BCELoss(weight=None,size_average=None,reduce=None,reduction='mean')", "description": "Creates a criterion that measures the Binary Cross Entropy between the target and the output: The unreduced (i.e. with reduction set to 'none') loss can be described as:  \u2113(x,y)=L={l1,\u2026,lN}\u22a4,ln=\u2212wn[yn\u22c5log\u2061xn+(1\u2212yn)\u22c5log\u2061(1\u2212xn)],\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad l_n = - w_n \\left[ y_n \\cdot \\log x_n + (1 - y_n) \\cdot \\log (1 - x_n) \\right],  \u2113(x,y)=L={l1\u200b,\u2026,lN\u200b}\u22a4,ln\u200b=\u2212wn\u200b[yn\u200b\u22c5logxn\u200b+(1\u2212yn\u200b)\u22c5log(1\u2212xn\u200b)],  where NNN   is the batch size. If reduction is not 'none' (default 'mean'), then  \u2113(x,y)={mean\u2061(L),if\u00a0reduction=\u2019mean\u2019;sum\u2061(L),if\u00a0reduction=\u2019sum\u2019.\\ell(x, y) = \\begin{cases}     \\operatorname{mean}(L), &amp; \\text{if reduction} = \\text{'mean';}\\\\     \\operatorname{sum}(L),  &amp; \\text{if reduction} = \\text{'sum'.} \\end{cases}  \u2113(x,y)={mean(L),sum(L),\u200bif\u00a0reduction=\u2019mean\u2019;if\u00a0reduction=\u2019sum\u2019.\u200b  This is used for measuring the error of a reconstruction in for example an auto-encoder. Note that the targets yyy   should be numbers between 0 and 1.  Parameters  weight (Tensor, optional) \u2013 a manual rescaling weight given to the loss of each batch element. If given, has to be a Tensor of size nbatch. size_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'     Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where \u2217*\u2217   means, any number of additional dimensions Target: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input Output: scalar. If reduction is 'none', then (N,\u2217)(N, *)(N,\u2217)  , same shape as input.    Examples: &gt;&gt;&gt; m = nn.Sigmoid() &gt;&gt;&gt; loss = nn.BCELoss() &gt;&gt;&gt; input = torch.randn(3, requires_grad=True) &gt;&gt;&gt; target = torch.empty(3).random_(2) &gt;&gt;&gt; output = loss(m(input), target) &gt;&gt;&gt; output.backward()   ", "parameters": ["weight (Tensor, optional) : a manual rescaling weight given to the lossof each batch element. If given, has to be a Tensor of size nbatch.", "size_average (bool, optional) : Deprecated (see reduction). By default,the losses are averaged over each loss element in the batch. Note that forsome losses, there are multiple elements per sample. If the field size_averageis set to False, the losses are instead summed for each minibatch. Ignoredwhen reduce is False. Default: True", "reduce (bool, optional) : Deprecated (see reduction). By default, thelosses are averaged or summed over observations for each minibatch dependingon size_average. When reduce is False, returns a loss perbatch element instead and ignores size_average. Default: True", "reduction (string, optional) : Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"], "returns": null, "example": " m = nn.Sigmoid()\n loss = nn.BCELoss()\n input = torch.randn(3, requires_grad=True)\n target = torch.empty(3).random_(2)\n output = loss(m(input), target)\n output.backward()\n\n", "shape": " Input: (N,\u2217)(N, *)(N,\u2217)   where \u2217*\u2217   means, any number of additional dimensions Target: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input Output: scalar. If reduction is 'none', then (N,\u2217)(N, *)(N,\u2217)  , same shape as input.  "},
{"library": "torch", "item_id": "torch.nn.BCEWithLogitsLoss", "item_type": "class", "code": "classtorch.nn.BCEWithLogitsLoss(weight=None,size_average=None,reduce=None,reduction='mean',pos_weight=None)", "description": "This loss combines a Sigmoid layer and the BCELoss in one single class. This version is more numerically stable than using a plain Sigmoid followed by a BCELoss as, by combining the operations into one layer, we take advantage of the log-sum-exp trick for numerical stability. The unreduced (i.e. with reduction set to 'none') loss can be described as:  \u2113(x,y)=L={l1,\u2026,lN}\u22a4,ln=\u2212wn[yn\u22c5log\u2061\u03c3(xn)+(1\u2212yn)\u22c5log\u2061(1\u2212\u03c3(xn))],\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad l_n = - w_n \\left[ y_n \\cdot \\log \\sigma(x_n) + (1 - y_n) \\cdot \\log (1 - \\sigma(x_n)) \\right],  \u2113(x,y)=L={l1\u200b,\u2026,lN\u200b}\u22a4,ln\u200b=\u2212wn\u200b[yn\u200b\u22c5log\u03c3(xn\u200b)+(1\u2212yn\u200b)\u22c5log(1\u2212\u03c3(xn\u200b))],  where NNN   is the batch size. If reduction is not 'none' (default 'mean'), then  \u2113(x,y)={mean\u2061(L),if\u00a0reduction=\u2019mean\u2019;sum\u2061(L),if\u00a0reduction=\u2019sum\u2019.\\ell(x, y) = \\begin{cases}     \\operatorname{mean}(L), &amp; \\text{if reduction} = \\text{'mean';}\\\\     \\operatorname{sum}(L),  &amp; \\text{if reduction} = \\text{'sum'.} \\end{cases}  \u2113(x,y)={mean(L),sum(L),\u200bif\u00a0reduction=\u2019mean\u2019;if\u00a0reduction=\u2019sum\u2019.\u200b  This is used for measuring the error of a reconstruction in for example an auto-encoder. Note that the targets t[i] should be numbers between 0 and 1. It\u2019s possible to trade off recall and precision by adding weights to positive examples. In the case of multi-label classification the loss can be described as:  \u2113c(x,y)=Lc={l1,c,\u2026,lN,c}\u22a4,ln,c=\u2212wn,c[pcyn,c\u22c5log\u2061\u03c3(xn,c)+(1\u2212yn,c)\u22c5log\u2061(1\u2212\u03c3(xn,c))],\\ell_c(x, y) = L_c = \\{l_{1,c},\\dots,l_{N,c}\\}^\\top, \\quad l_{n,c} = - w_{n,c} \\left[ p_c y_{n,c} \\cdot \\log \\sigma(x_{n,c}) + (1 - y_{n,c}) \\cdot \\log (1 - \\sigma(x_{n,c})) \\right],  \u2113c\u200b(x,y)=Lc\u200b={l1,c\u200b,\u2026,lN,c\u200b}\u22a4,ln,c\u200b=\u2212wn,c\u200b[pc\u200byn,c\u200b\u22c5log\u03c3(xn,c\u200b)+(1\u2212yn,c\u200b)\u22c5log(1\u2212\u03c3(xn,c\u200b))],  where ccc   is the class number (c&gt;1c &gt; 1c&gt;1   for multi-label binary classification, c=1c = 1c=1   for single-label binary classification), nnn   is the number of the sample in the batch and pcp_cpc\u200b   is the weight of the positive answer for the class ccc  . pc&gt;1p_c &gt; 1pc\u200b&gt;1   increases the recall, pc&lt;1p_c &lt; 1pc\u200b&lt;1   increases the precision. For example, if a dataset contains 100 positive and 300 negative examples of a single class, then pos_weight for the class should be equal to 300100=3\\frac{300}{100}=3100300\u200b=3  . The loss would act as if the dataset contains 3\u00d7100=3003\\times 100=3003\u00d7100=300   positive examples. Examples: &gt;&gt;&gt; target = torch.ones([10, 64], dtype=torch.float32)  # 64 classes, batch size = 10 &gt;&gt;&gt; output = torch.full([10, 64], 0.999)  # A prediction (logit) &gt;&gt;&gt; pos_weight = torch.ones([64])  # All weights are equal to 1 &gt;&gt;&gt; criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight) &gt;&gt;&gt; criterion(output, target)  # -log(sigmoid(0.999)) tensor(0.3135)    Parameters  weight (Tensor, optional) \u2013 a manual rescaling weight given to the loss of each batch element. If given, has to be a Tensor of size nbatch. size_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean' pos_weight (Tensor, optional) \u2013 a weight of positive examples. Must be a vector with length equal to the number of classes.     Shape:  Input: (N,\u2217)(N, *)(N,\u2217)   where \u2217*\u2217   means, any number of additional dimensions Target: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input Output: scalar. If reduction is 'none', then (N,\u2217)(N, *)(N,\u2217)  , same shape as input.   Examples: &gt;&gt;&gt; loss = nn.BCEWithLogitsLoss() &gt;&gt;&gt; input = torch.randn(3, requires_grad=True) &gt;&gt;&gt; target = torch.empty(3).random_(2) &gt;&gt;&gt; output = loss(input, target) &gt;&gt;&gt; output.backward()     ", "parameters": ["weight (Tensor, optional) : a manual rescaling weight given to the lossof each batch element. If given, has to be a Tensor of size nbatch.", "size_average (bool, optional) : Deprecated (see reduction). By default,the losses are averaged over each loss element in the batch. Note that forsome losses, there are multiple elements per sample. If the field size_averageis set to False, the losses are instead summed for each minibatch. Ignoredwhen reduce is False. Default: True", "reduce (bool, optional) : Deprecated (see reduction). By default, thelosses are averaged or summed over observations for each minibatch dependingon size_average. When reduce is False, returns a loss perbatch element instead and ignores size_average. Default: True", "reduction (string, optional) : Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'", "pos_weight (Tensor, optional) : a weight of positive examples.Must be a vector with length equal to the number of classes."], "returns": null, "example": " target = torch.ones([10, 64], dtype=torch.float32)  # 64 classes, batch size = 10\n output = torch.full([10, 64], 0.999)  # A prediction (logit)\n pos_weight = torch.ones([64])  # All weights are equal to 1\n criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n criterion(output, target)  # -log(sigmoid(0.999))\ntensor(0.3135)\n\n", "shape": "  Input: (N,\u2217)(N, *)(N,\u2217)   where \u2217*\u2217   means, any number of additional dimensions Target: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input Output: scalar. If reduction is 'none', then (N,\u2217)(N, *)(N,\u2217)  , same shape as input.   Examples: &gt;&gt;&gt; loss = nn.BCEWithLogitsLoss() &gt;&gt;&gt; input = torch.randn(3, requires_grad=True) &gt;&gt;&gt; target = torch.empty(3).random_(2) &gt;&gt;&gt; output = loss(input, target) &gt;&gt;&gt; output.backward()   "},
{"library": "torch", "item_id": "torch.compiled_with_cxx11_abi", "item_type": "function", "code": "torch.compiled_with_cxx11_abi()", "description": "Returns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1 ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.result_type", "item_type": "function", "code": "torch.result_type(tensor1,tensor2)\u2192dtype", "description": "Returns the torch.dtype that would result from performing an arithmetic operation on the provided input tensors. See type promotion documentation for more information on the type promotion logic.  Parameters  tensor1 (Tensor or Number) \u2013 an input tensor or number tensor2 (Tensor or Number) \u2013 an input tensor or number    Example: &gt;&gt;&gt; torch.result_type(torch.tensor([1, 2], dtype=torch.int), 1.0) torch.float32 &gt;&gt;&gt; torch.result_type(torch.tensor([1, 2], dtype=torch.uint8), torch.tensor(1)) torch.uint8   ", "parameters": ["tensor1 (Tensor or Number) : an input tensor or number", "tensor2 (Tensor or Number) : an input tensor or number"], "returns": null, "example": " torch.result_type(torch.tensor([1, 2], dtype=torch.int), 1.0)\ntorch.float32\n torch.result_type(torch.tensor([1, 2], dtype=torch.uint8), torch.tensor(1))\ntorch.uint8\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.can_cast", "item_type": "function", "code": "torch.can_cast(from,to)\u2192bool", "description": "Determines if a type conversion is allowed under PyTorch casting rules described in the type promotion documentation.  Parameters  from (dpython:type) \u2013 The original torch.dtype. to (dpython:type) \u2013 The target torch.dtype.    Example: &gt;&gt;&gt; torch.can_cast(torch.double, torch.float) True &gt;&gt;&gt; torch.can_cast(torch.float, torch.int) False   ", "parameters": ["from (dpython:type) : The original torch.dtype.", "to (dpython:type) : The target torch.dtype."], "returns": null, "example": " torch.can_cast(torch.double, torch.float)\nTrue\n torch.can_cast(torch.float, torch.int)\nFalse\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.promote_types", "item_type": "function", "code": "torch.promote_types(type1,type2)\u2192dtype", "description": "Returns the torch.dtype with the smallest size and scalar kind that is not smaller nor of lower kind than either type1 or type2. See type promotion documentation for more information on the type promotion logic.  Parameters  type1 (torch.dtype) \u2013  type2 (torch.dtype) \u2013     Example: &gt;&gt;&gt; torch.promote_types(torch.int32, torch.float32)) torch.float32 &gt;&gt;&gt; torch.promote_types(torch.uint8, torch.long) torch.long   ", "parameters": ["type1 (torch.dtype) : ", "type2 (torch.dtype) : "], "returns": null, "example": " torch.promote_types(torch.int32, torch.float32))\ntorch.float32\n torch.promote_types(torch.uint8, torch.long)\ntorch.long\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch._C.Generator.get_state", "item_type": "method", "code": "get_state()\u2192Tensor", "description": "Returns the Generator state as a torch.ByteTensor.  Returns A torch.ByteTensor which contains all the necessary bits to restore a Generator to a specific point in time.  Return type Tensor   Example: &gt;&gt;&gt; g_cpu = torch.Generator() &gt;&gt;&gt; g_cpu.get_state()   ", "parameters": [], "returns": "A torch.ByteTensor which contains all the necessary bitsto restore a Generator to a specific point in time.", "example": " g_cpu = torch.Generator()\n g_cpu.get_state()\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.MarginRankingLoss", "item_type": "class", "code": "classtorch.nn.MarginRankingLoss(margin=0.0,size_average=None,reduce=None,reduction='mean')", "description": "Creates a criterion that measures the loss given inputs x1x1x1  , x2x2x2  , two 1D mini-batch Tensors, and a label 1D mini-batch tensor yyy   (containing 1 or -1). If y=1y = 1y=1   then it assumed the first input should be ranked higher (have a larger value) than the second input, and vice-versa for y=\u22121y = -1y=\u22121  . The loss function for each sample in the mini-batch is:  loss(x,y)=max\u2061(0,\u2212y\u2217(x1\u2212x2)+margin)\\text{loss}(x, y) = \\max(0, -y * (x1 - x2) + \\text{margin})  loss(x,y)=max(0,\u2212y\u2217(x1\u2212x2)+margin)   Parameters  margin (python:float, optional) \u2013 Has a default value of 000  . size_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'     Shape: Input: (N,D)(N, D)(N,D)   where N is the batch size and D is the size of a sample. Target: (N)(N)(N)   Output: scalar. If reduction is 'none', then (N)(N)(N)  .    ", "parameters": ["margin (python:float, optional) : Has a default value of 000.", "size_average (bool, optional) : Deprecated (see reduction). By default,the losses are averaged over each loss element in the batch. Note that forsome losses, there are multiple elements per sample. If the field size_averageis set to False, the losses are instead summed for each minibatch. Ignoredwhen reduce is False. Default: True", "reduce (bool, optional) : Deprecated (see reduction). By default, thelosses are averaged or summed over observations for each minibatch dependingon size_average. When reduce is False, returns a loss perbatch element instead and ignores size_average. Default: True", "reduction (string, optional) : Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"], "returns": null, "example": "NA", "shape": " Input: (N,D)(N, D)(N,D)   where N is the batch size and D is the size of a sample. Target: (N)(N)(N)   Output: scalar. If reduction is 'none', then (N)(N)(N)  .  "},
{"library": "torch", "item_id": "torch.nn.HingeEmbeddingLoss", "item_type": "class", "code": "classtorch.nn.HingeEmbeddingLoss(margin=1.0,size_average=None,reduce=None,reduction='mean')", "description": "Measures the loss given an input tensor xxx   and a labels tensor yyy   (containing 1 or -1). This is usually used for measuring whether two inputs are similar or dissimilar, e.g. using the L1 pairwise distance as xxx  , and is typically used for learning nonlinear embeddings or semi-supervised learning. The loss function for nnn  -th sample in the mini-batch is  ln={xn,if\u2005\u200ayn=1,max\u2061{0,\u0394\u2212xn},if\u2005\u200ayn=\u22121,l_n = \\begin{cases}     x_n, &amp; \\text{if}\\; y_n = 1,\\\\     \\max \\{0, \\Delta - x_n\\}, &amp; \\text{if}\\; y_n = -1, \\end{cases}  ln\u200b={xn\u200b,max{0,\u0394\u2212xn\u200b},\u200bifyn\u200b=1,ifyn\u200b=\u22121,\u200b  and the total loss functions is  \u2113(x,y)={mean\u2061(L),if\u00a0reduction=\u2019mean\u2019;sum\u2061(L),if\u00a0reduction=\u2019sum\u2019.\\ell(x, y) = \\begin{cases}     \\operatorname{mean}(L), &amp; \\text{if reduction} = \\text{'mean';}\\\\     \\operatorname{sum}(L),  &amp; \\text{if reduction} = \\text{'sum'.} \\end{cases}  \u2113(x,y)={mean(L),sum(L),\u200bif\u00a0reduction=\u2019mean\u2019;if\u00a0reduction=\u2019sum\u2019.\u200b  where L={l1,\u2026,lN}\u22a4L = \\{l_1,\\dots,l_N\\}^\\topL={l1\u200b,\u2026,lN\u200b}\u22a4  .  Parameters  margin (python:float, optional) \u2013 Has a default value of 1. size_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'     Shape: Input: (\u2217)(*)(\u2217)   where \u2217*\u2217   means, any number of dimensions. The sum operation operates over all the elements. Target: (\u2217)(*)(\u2217)  , same shape as the input Output: scalar. If reduction is 'none', then same shape as the input    ", "parameters": ["margin (python:float, optional) : Has a default value of 1.", "size_average (bool, optional) : Deprecated (see reduction). By default,the losses are averaged over each loss element in the batch. Note that forsome losses, there are multiple elements per sample. If the field size_averageis set to False, the losses are instead summed for each minibatch. Ignoredwhen reduce is False. Default: True", "reduce (bool, optional) : Deprecated (see reduction). By default, thelosses are averaged or summed over observations for each minibatch dependingon size_average. When reduce is False, returns a loss perbatch element instead and ignores size_average. Default: True", "reduction (string, optional) : Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"], "returns": null, "example": "NA", "shape": " Input: (\u2217)(*)(\u2217)   where \u2217*\u2217   means, any number of dimensions. The sum operation operates over all the elements. Target: (\u2217)(*)(\u2217)  , same shape as the input Output: scalar. If reduction is 'none', then same shape as the input  "},
{"library": "torch", "item_id": "torch._C.Generator.initial_seed", "item_type": "method", "code": "initial_seed()\u2192int", "description": "Returns the initial seed for generating random numbers. Example: &gt;&gt;&gt; g_cpu = torch.Generator() &gt;&gt;&gt; g_cpu.initial_seed() 2147483647   ", "parameters": [], "returns": null, "example": " g_cpu = torch.Generator()\n g_cpu.initial_seed()\n2147483647\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch._C.Generator.manual_seed", "item_type": "method", "code": "manual_seed(seed)\u2192Generator", "description": "Sets the seed for generating random numbers. Returns a torch.Generator object. It is recommended to set a large seed, i.e. a number that has a good balance of 0 and 1 bits. Avoid having many 0 bits in the seed.  Parameters seed (python:int) \u2013 The desired seed.  Returns An torch.Generator object.  Return type Generator   Example: &gt;&gt;&gt; g_cpu = torch.Generator() &gt;&gt;&gt; g_cpu.manual_seed(2147483647)   ", "parameters": ["seed (python:int) : The desired seed.", "An torch.Generator object.", "Generator"], "returns": "An torch.Generator object.", "example": " g_cpu = torch.Generator()\n g_cpu.manual_seed(2147483647)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch._C.Generator.seed", "item_type": "method", "code": "seed()\u2192int", "description": "Gets a non-deterministic random number from std::random_device or the current time and uses it to seed a Generator. Example: &gt;&gt;&gt; g_cpu = torch.Generator() &gt;&gt;&gt; g_cpu.seed() 1516516984916   ", "parameters": [], "returns": null, "example": " g_cpu = torch.Generator()\n g_cpu.seed()\n1516516984916\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch._C.Generator.set_state", "item_type": "method", "code": "set_state(new_state)\u2192void", "description": "Sets the Generator state.  Parameters new_state (torch.ByteTensor) \u2013 The desired state.   Example: &gt;&gt;&gt; g_cpu = torch.Generator() &gt;&gt;&gt; g_cpu_other = torch.Generator() &gt;&gt;&gt; g_cpu.set_state(g_cpu_other.get_state())   ", "parameters": ["new_state (torch.ByteTensor) : The desired state."], "returns": null, "example": " g_cpu = torch.Generator()\n g_cpu_other = torch.Generator()\n g_cpu.set_state(g_cpu_other.get_state())\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.quasirandom.SobolEngine.draw", "item_type": "method", "code": "draw(n=1,out=None,dtype=torch.float32)", "description": "Function to draw a sequence of n points from a Sobol sequence. Note that the samples are dependent on the previous samples. The size of the result is (n,dimension)(n, dimension)(n,dimension)  .  Parameters  n (Int, optional) \u2013 The length of sequence of points to draw. Default: 1 out (Tensor, optional) \u2013 The output tensor dtype (torch.dtype, optional) \u2013 the desired data type of the returned tensor. Default: torch.float32    ", "parameters": ["n (Int, optional) : The length of sequence of points to draw.Default: 1", "out (Tensor, optional) : The output tensor", "dtype (torch.dtype, optional) : the desired data type of thereturned tensor.Default: torch.float32"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.quasirandom.SobolEngine.fast_forward", "item_type": "method", "code": "fast_forward(n)", "description": "Function to fast-forward the state of the SobolEngine by n steps. This is equivalent to drawing n samples without using the samples.  Parameters n (Int) \u2013 The number of steps to fast-forward by.   ", "parameters": ["n (Int) : The number of steps to fast-forward by."], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.quasirandom.SobolEngine.reset", "item_type": "method", "code": "reset()", "description": "Function to reset the SobolEngine to base state. ", "parameters": [], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch._C.Generator", "item_type": "class", "code": "classtorch._C.Generator(device='cpu')\u2192Generator", "description": "Creates and returns a generator object which manages the state of the algorithm that produces pseudo random numbers. Used as a keyword argument in many In-place random sampling functions.  Parameters device (torch.device, optional) \u2013 the desired device for the generator.  Returns An torch.Generator object.  Return type Generator   Example: &gt;&gt;&gt; g_cpu = torch.Generator() &gt;&gt;&gt; g_cuda = torch.Generator(device='cuda')     device Generator.device -&gt; device Gets the current device of the generator. Example: &gt;&gt;&gt; g_cpu = torch.Generator() &gt;&gt;&gt; g_cpu.device device(type='cpu')       get_state() \u2192 Tensor Returns the Generator state as a torch.ByteTensor.  Returns A torch.ByteTensor which contains all the necessary bits to restore a Generator to a specific point in time.  Return type Tensor   Example: &gt;&gt;&gt; g_cpu = torch.Generator() &gt;&gt;&gt; g_cpu.get_state()       initial_seed() \u2192 int Returns the initial seed for generating random numbers. Example: &gt;&gt;&gt; g_cpu = torch.Generator() &gt;&gt;&gt; g_cpu.initial_seed() 2147483647       manual_seed(seed) \u2192 Generator Sets the seed for generating random numbers. Returns a torch.Generator object. It is recommended to set a large seed, i.e. a number that has a good balance of 0 and 1 bits. Avoid having many 0 bits in the seed.  Parameters seed (python:int) \u2013 The desired seed.  Returns An torch.Generator object.  Return type Generator   Example: &gt;&gt;&gt; g_cpu = torch.Generator() &gt;&gt;&gt; g_cpu.manual_seed(2147483647)       seed() \u2192 int Gets a non-deterministic random number from std::random_device or the current time and uses it to seed a Generator. Example: &gt;&gt;&gt; g_cpu = torch.Generator() &gt;&gt;&gt; g_cpu.seed() 1516516984916       set_state(new_state) \u2192 void Sets the Generator state.  Parameters new_state (torch.ByteTensor) \u2013 The desired state.   Example: &gt;&gt;&gt; g_cpu = torch.Generator() &gt;&gt;&gt; g_cpu_other = torch.Generator() &gt;&gt;&gt; g_cpu.set_state(g_cpu_other.get_state())     ", "parameters": ["device (torch.device, optional) : the desired device for the generator.", "An torch.Generator object.", "Generator", "A torch.ByteTensor which contains all the necessary bitsto restore a Generator to a specific point in time.", "Tensor", "seed (python:int) : The desired seed.", "An torch.Generator object.", "Generator", "new_state (torch.ByteTensor) : The desired state."], "returns": "An torch.Generator object.", "example": " g_cpu = torch.Generator()\n g_cuda = torch.Generator(device='cuda')\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.MultiLabelMarginLoss", "item_type": "class", "code": "classtorch.nn.MultiLabelMarginLoss(size_average=None,reduce=None,reduction='mean')", "description": "Creates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between input xxx   (a 2D mini-batch Tensor) and output yyy   (which is a 2D Tensor of target class indices). For each sample in the mini-batch:  loss(x,y)=\u2211ijmax\u2061(0,1\u2212(x[y[j]]\u2212x[i]))x.size(0)\\text{loss}(x, y) = \\sum_{ij}\\frac{\\max(0, 1 - (x[y[j]] - x[i]))}{\\text{x.size}(0)}  loss(x,y)=ij\u2211\u200bx.size(0)max(0,1\u2212(x[y[j]]\u2212x[i]))\u200b  where x\u2208{0,\u2005\u200a\u22ef\u2009,\u2005\u200ax.size(0)\u22121}x \\in \\left\\{0, \\; \\cdots , \\; \\text{x.size}(0) - 1\\right\\}x\u2208{0,\u22ef,x.size(0)\u22121}  , y\u2208{0,\u2005\u200a\u22ef\u2009,\u2005\u200ay.size(0)\u22121}y \\in \\left\\{0, \\; \\cdots , \\; \\text{y.size}(0) - 1\\right\\}y\u2208{0,\u22ef,y.size(0)\u22121}  , 0\u2264y[j]\u2264x.size(0)\u221210 \\leq y[j] \\leq \\text{x.size}(0)-10\u2264y[j]\u2264x.size(0)\u22121  , and i\u2260y[j]i \\neq y[j]i\ue020\u200b=y[j]   for all iii   and jjj  . yyy   and xxx   must have the same size. The criterion only considers a contiguous block of non-negative targets that starts at the front. This allows for different samples to have variable amounts of target classes.  Parameters  size_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'     Shape: Input: (C)(C)(C)   or (N,C)(N, C)(N,C)   where N is the batch size and C is the number of classes. Target: (C)(C)(C)   or (N,C)(N, C)(N,C)  , label targets padded by -1 ensuring same shape as the input. Output: scalar. If reduction is 'none', then (N)(N)(N)  .    Examples: &gt;&gt;&gt; loss = nn.MultiLabelMarginLoss() &gt;&gt;&gt; x = torch.FloatTensor([[0.1, 0.2, 0.4, 0.8]]) &gt;&gt;&gt; # for target y, only consider labels 3 and 0, not after label -1 &gt;&gt;&gt; y = torch.LongTensor([[3, 0, -1, 1]]) &gt;&gt;&gt; loss(x, y) &gt;&gt;&gt; # 0.25 * ((1-(0.1-0.2)) + (1-(0.1-0.4)) + (1-(0.8-0.2)) + (1-(0.8-0.4))) tensor(0.8500)   ", "parameters": ["size_average (bool, optional) : Deprecated (see reduction). By default,the losses are averaged over each loss element in the batch. Note that forsome losses, there are multiple elements per sample. If the field size_averageis set to False, the losses are instead summed for each minibatch. Ignoredwhen reduce is False. Default: True", "reduce (bool, optional) : Deprecated (see reduction). By default, thelosses are averaged or summed over observations for each minibatch dependingon size_average. When reduce is False, returns a loss perbatch element instead and ignores size_average. Default: True", "reduction (string, optional) : Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"], "returns": null, "example": " loss = nn.MultiLabelMarginLoss()\n x = torch.FloatTensor([[0.1, 0.2, 0.4, 0.8]])\n # for target y, only consider labels 3 and 0, not after label -1\n y = torch.LongTensor([[3, 0, -1, 1]])\n loss(x, y)\n # 0.25 * ((1-(0.1-0.2)) + (1-(0.1-0.4)) + (1-(0.8-0.2)) + (1-(0.8-0.4)))\ntensor(0.8500)\n\n", "shape": " Input: (C)(C)(C)   or (N,C)(N, C)(N,C)   where N is the batch size and C is the number of classes. Target: (C)(C)(C)   or (N,C)(N, C)(N,C)  , label targets padded by -1 ensuring same shape as the input. Output: scalar. If reduction is 'none', then (N)(N)(N)  .  "},
{"library": "torch", "item_id": "torch.nn.SmoothL1Loss", "item_type": "class", "code": "classtorch.nn.SmoothL1Loss(size_average=None,reduce=None,reduction='mean')", "description": "Creates a criterion that uses a squared term if the absolute element-wise error falls below 1 and an L1 term otherwise. It is less sensitive to outliers than the MSELoss and in some cases prevents exploding gradients (e.g. see Fast R-CNN paper by Ross Girshick). Also known as the Huber loss:  loss(x,y)=1n\u2211izi\\text{loss}(x, y) = \\frac{1}{n} \\sum_{i} z_{i}  loss(x,y)=n1\u200bi\u2211\u200bzi\u200b  where ziz_{i}zi\u200b   is given by:  zi={0.5(xi\u2212yi)2,if\u00a0\u2223xi\u2212yi\u2223&lt;1\u2223xi\u2212yi\u2223\u22120.5,otherwise\u00a0z_{i} = \\begin{cases} 0.5 (x_i - y_i)^2, &amp; \\text{if } |x_i - y_i| &lt; 1 \\\\ |x_i - y_i| - 0.5, &amp; \\text{otherwise } \\end{cases}  zi\u200b={0.5(xi\u200b\u2212yi\u200b)2,\u2223xi\u200b\u2212yi\u200b\u2223\u22120.5,\u200bif\u00a0\u2223xi\u200b\u2212yi\u200b\u2223&lt;1otherwise\u00a0\u200b  xxx   and yyy   arbitrary shapes with a total of nnn   elements each the sum operation still operates over all the elements, and divides by nnn  . The division by nnn   can be avoided if sets reduction = 'sum'.  Parameters  size_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'     Shape: Input: (N,\u2217)(N, *)(N,\u2217)   where \u2217*\u2217   means, any number of additional dimensions Target: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input Output: scalar. If reduction is 'none', then (N,\u2217)(N, *)(N,\u2217)  , same shape as the input    ", "parameters": ["size_average (bool, optional) : Deprecated (see reduction). By default,the losses are averaged over each loss element in the batch. Note that forsome losses, there are multiple elements per sample. If the field size_averageis set to False, the losses are instead summed for each minibatch. Ignoredwhen reduce is False. Default: True", "reduce (bool, optional) : Deprecated (see reduction). By default, thelosses are averaged or summed over observations for each minibatch dependingon size_average. When reduce is False, returns a loss perbatch element instead and ignores size_average. Default: True", "reduction (string, optional) : Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"], "returns": null, "example": "NA", "shape": " Input: (N,\u2217)(N, *)(N,\u2217)   where \u2217*\u2217   means, any number of additional dimensions Target: (N,\u2217)(N, *)(N,\u2217)  , same shape as the input Output: scalar. If reduction is 'none', then (N,\u2217)(N, *)(N,\u2217)  , same shape as the input  "},
{"library": "torch", "item_id": "torch._C.Generator.device", "item_type": "attribute", "code": "device", "description": "Generator.device -&gt; device Gets the current device of the generator. Example: &gt;&gt;&gt; g_cpu = torch.Generator() &gt;&gt;&gt; g_cpu.device device(type='cpu')   ", "parameters": [], "returns": null, "example": " g_cpu = torch.Generator()\n g_cpu.device\ndevice(type='cpu')\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.quasirandom.SobolEngine", "item_type": "class", "code": "classtorch.quasirandom.SobolEngine(dimension,scramble=False,seed=None)", "description": "The torch.quasirandom.SobolEngine is an engine for generating (scrambled) Sobol sequences. Sobol sequences are an example of low discrepancy quasi-random sequences. This implementation of an engine for Sobol sequences is capable of sampling sequences up to a maximum dimension of 1111. It uses direction numbers to generate these sequences, and these numbers have been adapted from here. References  Art B. Owen. Scrambling Sobol and Niederreiter-Xing points. Journal of Complexity, 14(4):466-489, December 1998. I. M. Sobol. The distribution of points in a cube and the accurate evaluation of integrals. Zh. Vychisl. Mat. i Mat. Phys., 7:784-802, 1967.   Parameters  dimension (Int) \u2013 The dimensionality of the sequence to be drawn scramble (bool, optional) \u2013 Setting this to True will produce scrambled Sobol sequences. Scrambling is capable of producing better Sobol sequences. Default: False. seed (Int, optional) \u2013 This is the seed for the scrambling. The seed of the random number generator is set to this, if specified. Otherwise, it uses a random seed. Default: None    Examples: &gt;&gt;&gt; soboleng = torch.quasirandom.SobolEngine(dimension=5) &gt;&gt;&gt; soboleng.draw(3) tensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000],         [0.7500, 0.2500, 0.7500, 0.2500, 0.7500],         [0.2500, 0.7500, 0.2500, 0.7500, 0.2500]])     draw(n=1, out=None, dtype=torch.float32)  Function to draw a sequence of n points from a Sobol sequence. Note that the samples are dependent on the previous samples. The size of the result is (n,dimension)(n, dimension)(n,dimension)  .  Parameters  n (Int, optional) \u2013 The length of sequence of points to draw. Default: 1 out (Tensor, optional) \u2013 The output tensor dtype (torch.dtype, optional) \u2013 the desired data type of the returned tensor. Default: torch.float32        fast_forward(n)  Function to fast-forward the state of the SobolEngine by n steps. This is equivalent to drawing n samples without using the samples.  Parameters n (Int) \u2013 The number of steps to fast-forward by.       reset()  Function to reset the SobolEngine to base state.   ", "parameters": ["dimension (Int) : The dimensionality of the sequence to be drawn", "scramble (bool, optional) : Setting this to True will producescrambled Sobol sequences. Scrambling iscapable of producing better Sobolsequences. Default: False.", "seed (Int, optional) : This is the seed for the scrambling. The seedof the random number generator is set to this,if specified. Otherwise, it uses a random seed.Default: None", "n (Int, optional) : The length of sequence of points to draw.Default: 1", "out (Tensor, optional) : The output tensor", "dtype (torch.dtype, optional) : the desired data type of thereturned tensor.Default: torch.float32"], "returns": null, "example": " soboleng = torch.quasirandom.SobolEngine(dimension=5)\n soboleng.draw(3)\ntensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n        [0.7500, 0.2500, 0.7500, 0.2500, 0.7500],\n        [0.2500, 0.7500, 0.2500, 0.7500, 0.2500]])\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.SoftMarginLoss", "item_type": "class", "code": "classtorch.nn.SoftMarginLoss(size_average=None,reduce=None,reduction='mean')", "description": "Creates a criterion that optimizes a two-class classification logistic loss between input tensor xxx   and target tensor yyy   (containing 1 or -1).  loss(x,y)=\u2211ilog\u2061(1+exp\u2061(\u2212y[i]\u2217x[i]))x.nelement()\\text{loss}(x, y) = \\sum_i \\frac{\\log(1 + \\exp(-y[i]*x[i]))}{\\text{x.nelement}()}  loss(x,y)=i\u2211\u200bx.nelement()log(1+exp(\u2212y[i]\u2217x[i]))\u200b   Parameters  size_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'     Shape: Input: (\u2217)(*)(\u2217)   where \u2217*\u2217   means, any number of additional dimensions Target: (\u2217)(*)(\u2217)  , same shape as the input Output: scalar. If reduction is 'none', then same shape as the input    ", "parameters": ["size_average (bool, optional) : Deprecated (see reduction). By default,the losses are averaged over each loss element in the batch. Note that forsome losses, there are multiple elements per sample. If the field size_averageis set to False, the losses are instead summed for each minibatch. Ignoredwhen reduce is False. Default: True", "reduce (bool, optional) : Deprecated (see reduction). By default, thelosses are averaged or summed over observations for each minibatch dependingon size_average. When reduce is False, returns a loss perbatch element instead and ignores size_average. Default: True", "reduction (string, optional) : Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"], "returns": null, "example": "NA", "shape": " Input: (\u2217)(*)(\u2217)   where \u2217*\u2217   means, any number of additional dimensions Target: (\u2217)(*)(\u2217)  , same shape as the input Output: scalar. If reduction is 'none', then same shape as the input  "},
{"library": "torch", "item_id": "torch.nn.MultiLabelSoftMarginLoss", "item_type": "class", "code": "classtorch.nn.MultiLabelSoftMarginLoss(weight=None,size_average=None,reduce=None,reduction='mean')", "description": "Creates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between input xxx   and target yyy   of size (N,C)(N, C)(N,C)  . For each sample in the minibatch:  loss(x,y)=\u22121C\u2217\u2211iy[i]\u2217log\u2061((1+exp\u2061(\u2212x[i]))\u22121)+(1\u2212y[i])\u2217log\u2061(exp\u2061(\u2212x[i])(1+exp\u2061(\u2212x[i])))loss(x, y) = - \\frac{1}{C} * \\sum_i y[i] * \\log((1 + \\exp(-x[i]))^{-1})                  + (1-y[i]) * \\log\\left(\\frac{\\exp(-x[i])}{(1 + \\exp(-x[i]))}\\right)  loss(x,y)=\u2212C1\u200b\u2217i\u2211\u200by[i]\u2217log((1+exp(\u2212x[i]))\u22121)+(1\u2212y[i])\u2217log((1+exp(\u2212x[i]))exp(\u2212x[i])\u200b)  where i\u2208{0,\u2005\u200a\u22ef\u2009,\u2005\u200ax.nElement()\u22121}i \\in \\left\\{0, \\; \\cdots , \\; \\text{x.nElement}() - 1\\right\\}i\u2208{0,\u22ef,x.nElement()\u22121}  , y[i]\u2208{0,\u2005\u200a1}y[i] \\in \\left\\{0, \\; 1\\right\\}y[i]\u2208{0,1}  .  Parameters  weight (Tensor, optional) \u2013 a manual rescaling weight given to each class. If given, it has to be a Tensor of size C. Otherwise, it is treated as if having all ones. size_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'     Shape: Input: (N,C)(N, C)(N,C)   where N is the batch size and C is the number of classes. Target: (N,C)(N, C)(N,C)  , label targets padded by -1 ensuring same shape as the input. Output: scalar. If reduction is 'none', then (N)(N)(N)  .    ", "parameters": ["weight (Tensor, optional) : a manual rescaling weight given to eachclass. If given, it has to be a Tensor of size C. Otherwise, it istreated as if having all ones.", "size_average (bool, optional) : Deprecated (see reduction). By default,the losses are averaged over each loss element in the batch. Note that forsome losses, there are multiple elements per sample. If the field size_averageis set to False, the losses are instead summed for each minibatch. Ignoredwhen reduce is False. Default: True", "reduce (bool, optional) : Deprecated (see reduction). By default, thelosses are averaged or summed over observations for each minibatch dependingon size_average. When reduce is False, returns a loss perbatch element instead and ignores size_average. Default: True", "reduction (string, optional) : Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"], "returns": null, "example": "NA", "shape": " Input: (N,C)(N, C)(N,C)   where N is the batch size and C is the number of classes. Target: (N,C)(N, C)(N,C)  , label targets padded by -1 ensuring same shape as the input. Output: scalar. If reduction is 'none', then (N)(N)(N)  .  "},
{"library": "torch", "item_id": "torch.nn.CosineEmbeddingLoss", "item_type": "class", "code": "classtorch.nn.CosineEmbeddingLoss(margin=0.0,size_average=None,reduce=None,reduction='mean')", "description": "Creates a criterion that measures the loss given input tensors x1x_1x1\u200b  , x2x_2x2\u200b   and a Tensor label yyy   with values 1 or -1. This is used for measuring whether two inputs are similar or dissimilar, using the cosine distance, and is typically used for learning nonlinear embeddings or semi-supervised learning. The loss function for each sample is:  loss(x,y)={1\u2212cos\u2061(x1,x2),if\u00a0y=1max\u2061(0,cos\u2061(x1,x2)\u2212margin),if\u00a0y=\u22121\\text{loss}(x, y) = \\begin{cases} 1 - \\cos(x_1, x_2), &amp; \\text{if } y = 1 \\\\ \\max(0, \\cos(x_1, x_2) - \\text{margin}), &amp; \\text{if } y = -1 \\end{cases}  loss(x,y)={1\u2212cos(x1\u200b,x2\u200b),max(0,cos(x1\u200b,x2\u200b)\u2212margin),\u200bif\u00a0y=1if\u00a0y=\u22121\u200b   Parameters  margin (python:float, optional) \u2013 Should be a number from \u22121-1\u22121   to 111  , 000   to 0.50.50.5   is suggested. If margin is missing, the default value is 000  . size_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'    ", "parameters": ["margin (python:float, optional) : Should be a number from \u22121-1\u22121 to 111,000 to 0.50.50.5 is suggested. If margin is missing, thedefault value is 000.", "size_average (bool, optional) : Deprecated (see reduction). By default,the losses are averaged over each loss element in the batch. Note that forsome losses, there are multiple elements per sample. If the field size_averageis set to False, the losses are instead summed for each minibatch. Ignoredwhen reduce is False. Default: True", "reduce (bool, optional) : Deprecated (see reduction). By default, thelosses are averaged or summed over observations for each minibatch dependingon size_average. When reduce is False, returns a loss perbatch element instead and ignores size_average. Default: True", "reduction (string, optional) : Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.MultiMarginLoss", "item_type": "class", "code": "classtorch.nn.MultiMarginLoss(p=1,margin=1.0,weight=None,size_average=None,reduce=None,reduction='mean')", "description": "Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between input xxx   (a 2D mini-batch Tensor) and output yyy   (which is a 1D tensor of target class indices, 0\u2264y\u2264x.size(1)\u221210 \\leq y \\leq \\text{x.size}(1)-10\u2264y\u2264x.size(1)\u22121  ): For each mini-batch sample, the loss in terms of the 1D input xxx   and scalar output yyy   is:  loss(x,y)=\u2211imax\u2061(0,margin\u2212x[y]+x[i]))px.size(0)\\text{loss}(x, y) = \\frac{\\sum_i \\max(0, \\text{margin} - x[y] + x[i]))^p}{\\text{x.size}(0)}  loss(x,y)=x.size(0)\u2211i\u200bmax(0,margin\u2212x[y]+x[i]))p\u200b  where x\u2208{0,\u2005\u200a\u22ef\u2009,\u2005\u200ax.size(0)\u22121}x \\in \\left\\{0, \\; \\cdots , \\; \\text{x.size}(0) - 1\\right\\}x\u2208{0,\u22ef,x.size(0)\u22121}   and i\u2260yi \\neq yi\ue020\u200b=y  . Optionally, you can give non-equal weighting on the classes by passing a 1D weight tensor into the constructor. The loss function then becomes:  loss(x,y)=\u2211imax\u2061(0,w[y]\u2217(margin\u2212x[y]+x[i]))p)x.size(0)\\text{loss}(x, y) = \\frac{\\sum_i \\max(0, w[y] * (\\text{margin} - x[y] + x[i]))^p)}{\\text{x.size}(0)}  loss(x,y)=x.size(0)\u2211i\u200bmax(0,w[y]\u2217(margin\u2212x[y]+x[i]))p)\u200b   Parameters  p (python:int, optional) \u2013 Has a default value of 111  . 111   and 222   are the only supported values. margin (python:float, optional) \u2013 Has a default value of 111  . weight (Tensor, optional) \u2013 a manual rescaling weight given to each class. If given, it has to be a Tensor of size C. Otherwise, it is treated as if having all ones. size_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'    ", "parameters": ["p (python:int, optional) : Has a default value of 111. 111 and 222are the only supported values.", "margin (python:float, optional) : Has a default value of 111.", "weight (Tensor, optional) : a manual rescaling weight given to eachclass. If given, it has to be a Tensor of size C. Otherwise, it istreated as if having all ones.", "size_average (bool, optional) : Deprecated (see reduction). By default,the losses are averaged over each loss element in the batch. Note that forsome losses, there are multiple elements per sample. If the field size_averageis set to False, the losses are instead summed for each minibatch. Ignoredwhen reduce is False. Default: True", "reduce (bool, optional) : Deprecated (see reduction). By default, thelosses are averaged or summed over observations for each minibatch dependingon size_average. When reduce is False, returns a loss perbatch element instead and ignores size_average. Default: True", "reduction (string, optional) : Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"], "returns": null, "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.TripletMarginLoss", "item_type": "class", "code": "classtorch.nn.TripletMarginLoss(margin=1.0,p=2.0,eps=1e-06,swap=False,size_average=None,reduce=None,reduction='mean')", "description": "Creates a criterion that measures the triplet loss given an input tensors x1x1x1  , x2x2x2  , x3x3x3   and a margin with a value greater than 000  . This is used for measuring a relative similarity between samples. A triplet is composed by a, p and n (i.e., anchor, positive examples and negative examples respectively). The shapes of all input tensors should be (N,D)(N, D)(N,D)  . The distance swap is described in detail in the paper Learning shallow convolutional feature descriptors with triplet losses by V. Balntas, E. Riba et al. The loss function for each sample in the mini-batch is:  L(a,p,n)=max\u2061{d(ai,pi)\u2212d(ai,ni)+margin,0}L(a, p, n) = \\max \\{d(a_i, p_i) - d(a_i, n_i) + {\\rm margin}, 0\\}  L(a,p,n)=max{d(ai\u200b,pi\u200b)\u2212d(ai\u200b,ni\u200b)+margin,0}  where  d(xi,yi)=\u2225xi\u2212yi\u2225pd(x_i, y_i) = \\left\\lVert {\\bf x}_i - {\\bf y}_i \\right\\rVert_p  d(xi\u200b,yi\u200b)=\u2225xi\u200b\u2212yi\u200b\u2225p\u200b   Parameters  margin (python:float, optional) \u2013 Default: 111  . p (python:int, optional) \u2013 The norm degree for pairwise distance. Default: 222  . swap (bool, optional) \u2013 The distance swap is described in detail in the paper Learning shallow convolutional feature descriptors with triplet losses by V. Balntas, E. Riba et al. Default: False. size_average (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field size_average is set to False, the losses are instead summed for each minibatch. Ignored when reduce is False. Default: True reduce (bool, optional) \u2013 Deprecated (see reduction). By default, the losses are averaged or summed over observations for each minibatch depending on size_average. When reduce is False, returns a loss per batch element instead and ignores size_average. Default: True reduction (string, optional) \u2013 Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'. 'none': no reduction will be applied, 'mean': the sum of the output will be divided by the number of elements in the output, 'sum': the output will be summed. Note: size_average and reduce are in the process of being deprecated, and in the meantime, specifying either of those two args will override reduction. Default: 'mean'     Shape: Input: (N,D)(N, D)(N,D)   where DDD   is the vector dimension. Output: scalar. If reduction is 'none', then (N)(N)(N)  .    &gt;&gt;&gt; triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2) &gt;&gt;&gt; anchor = torch.randn(100, 128, requires_grad=True) &gt;&gt;&gt; positive = torch.randn(100, 128, requires_grad=True) &gt;&gt;&gt; negative = torch.randn(100, 128, requires_grad=True) &gt;&gt;&gt; output = triplet_loss(anchor, positive, negative) &gt;&gt;&gt; output.backward()   ", "parameters": ["margin (python:float, optional) : Default: 111.", "p (python:int, optional) : The norm degree for pairwise distance. Default: 222.", "swap (bool, optional) : The distance swap is described in detail in the paperLearning shallow convolutional feature descriptors with triplet losses byV. Balntas, E. Riba et al. Default: False.", "size_average (bool, optional) : Deprecated (see reduction). By default,the losses are averaged over each loss element in the batch. Note that forsome losses, there are multiple elements per sample. If the field size_averageis set to False, the losses are instead summed for each minibatch. Ignoredwhen reduce is False. Default: True", "reduce (bool, optional) : Deprecated (see reduction). By default, thelosses are averaged or summed over observations for each minibatch dependingon size_average. When reduce is False, returns a loss perbatch element instead and ignores size_average. Default: True", "reduction (string, optional) : Specifies the reduction to apply to the output:'none' | 'mean' | 'sum'. 'none': no reduction will be applied,'mean': the sum of the output will be divided by the number ofelements in the output, 'sum': the output will be summed. Note: size_averageand reduce are in the process of being deprecated, and in the meantime,specifying either of those two args will override reduction. Default: 'mean'"], "returns": null, "example": "NA", "shape": " Input: (N,D)(N, D)(N,D)   where DDD   is the vector dimension. Output: scalar. If reduction is 'none', then (N)(N)(N)  .  "},
{"library": "torch", "item_id": "torch.nn.PixelShuffle", "item_type": "class", "code": "classtorch.nn.PixelShuffle(upscale_factor)", "description": "Rearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W)   to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r)  . This is useful for implementing efficient sub-pixel convolution with a stride of 1/r1/r1/r  . Look at the paper: Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network by Shi et. al (2016) for more details.  Parameters upscale_factor (python:int) \u2013 factor to increase spatial resolution by    Shape: Input: (N,L,Hin,Win)(N, L, H_{in}, W_{in})(N,L,Hin\u200b,Win\u200b)   where L=C\u00d7upscale_factor2L=C \\times \\text{upscale\\_factor}^2L=C\u00d7upscale_factor2   Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)   where Hout=Hin\u00d7upscale_factorH_{out} = H_{in} \\times \\text{upscale\\_factor}Hout\u200b=Hin\u200b\u00d7upscale_factor   and Wout=Win\u00d7upscale_factorW_{out} = W_{in} \\times \\text{upscale\\_factor}Wout\u200b=Win\u200b\u00d7upscale_factor      Examples: &gt;&gt;&gt; pixel_shuffle = nn.PixelShuffle(3) &gt;&gt;&gt; input = torch.randn(1, 9, 4, 4) &gt;&gt;&gt; output = pixel_shuffle(input) &gt;&gt;&gt; print(output.size()) torch.Size([1, 1, 12, 12])   ", "parameters": ["upscale_factor (python:int) : factor to increase spatial resolution by"], "returns": null, "example": " pixel_shuffle = nn.PixelShuffle(3)\n input = torch.randn(1, 9, 4, 4)\n output = pixel_shuffle(input)\n print(output.size())\ntorch.Size([1, 1, 12, 12])\n\n", "shape": " Input: (N,L,Hin,Win)(N, L, H_{in}, W_{in})(N,L,Hin\u200b,Win\u200b)   where L=C\u00d7upscale_factor2L=C \\times \\text{upscale\\_factor}^2L=C\u00d7upscale_factor2   Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)   where Hout=Hin\u00d7upscale_factorH_{out} = H_{in} \\times \\text{upscale\\_factor}Hout\u200b=Hin\u200b\u00d7upscale_factor   and Wout=Win\u00d7upscale_factorW_{out} = W_{in} \\times \\text{upscale\\_factor}Wout\u200b=Win\u200b\u00d7upscale_factor    "},
{"library": "torch", "item_id": "torch.nn.Upsample", "item_type": "class", "code": "classtorch.nn.Upsample(size=None,scale_factor=None,mode='nearest',align_corners=None)", "description": "Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data. The input data is assumed to be of the form minibatch x channels x [optional depth] x [optional height] x width. Hence, for spatial inputs, we expect a 4D Tensor and for volumetric inputs, we expect a 5D Tensor. The algorithms available for upsampling are nearest neighbor and linear, bilinear, bicubic and trilinear for 3D, 4D and 5D input Tensor, respectively. One can either give a scale_factor or the target output size to calculate the output size. (You cannot give both, as it is ambiguous)  Parameters  size (python:int or Tuple[python:int] or Tuple[python:int, python:int] or Tuple[python:int, python:int, python:int], optional) \u2013 output spatial sizes scale_factor (python:float or Tuple[python:float] or Tuple[python:float, python:float] or Tuple[python:float, python:float, python:float], optional) \u2013 multiplier for spatial size. Has to match input size if it is a tuple. mode (str, optional) \u2013 the upsampling algorithm: one of 'nearest', 'linear', 'bilinear', 'bicubic' and 'trilinear'. Default: 'nearest' align_corners (bool, optional) \u2013 if True, the corner pixels of the input and output tensors are aligned, and thus preserving the values at those pixels. This only has effect when mode is 'linear', 'bilinear', or 'trilinear'. Default: False     Shape: Input: (N,C,Win)(N, C, W_{in})(N,C,Win\u200b)  , (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)   or (N,C,Din,Hin,Win)(N, C, D_{in}, H_{in}, W_{in})(N,C,Din\u200b,Hin\u200b,Win\u200b)   Output: (N,C,Wout)(N, C, W_{out})(N,C,Wout\u200b)  , (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)   or (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\u200b,Hout\u200b,Wout\u200b)  , where     Dout=\u230aDin\u00d7scale_factor\u230bD_{out} = \\left\\lfloor D_{in} \\times \\text{scale\\_factor} \\right\\rfloor  Dout\u200b=\u230aDin\u200b\u00d7scale_factor\u230b   Hout=\u230aHin\u00d7scale_factor\u230bH_{out} = \\left\\lfloor H_{in} \\times \\text{scale\\_factor} \\right\\rfloor  Hout\u200b=\u230aHin\u200b\u00d7scale_factor\u230b   Wout=\u230aWin\u00d7scale_factor\u230bW_{out} = \\left\\lfloor W_{in} \\times \\text{scale\\_factor} \\right\\rfloor  Wout\u200b=\u230aWin\u200b\u00d7scale_factor\u230b   Warning With align_corners = True, the linearly interpolating modes (linear, bilinear, bicubic, and trilinear) don\u2019t proportionally align the output and input pixels, and thus the output values can depend on the input size. This was the default behavior for these modes up to version 0.3.1. Since then, the default behavior is align_corners = False. See below for concrete examples on how this affects the outputs.   Note If you want downsampling/general resizing, you should use interpolate().  Examples: &gt;&gt;&gt; input = torch.arange(1, 5, dtype=torch.float32).view(1, 1, 2, 2) &gt;&gt;&gt; input tensor([[[[ 1.,  2.],           [ 3.,  4.]]]])  &gt;&gt;&gt; m = nn.Upsample(scale_factor=2, mode='nearest') &gt;&gt;&gt; m(input) tensor([[[[ 1.,  1.,  2.,  2.],           [ 1.,  1.,  2.,  2.],           [ 3.,  3.,  4.,  4.],           [ 3.,  3.,  4.,  4.]]]])  &gt;&gt;&gt; m = nn.Upsample(scale_factor=2, mode='bilinear')  # align_corners=False &gt;&gt;&gt; m(input) tensor([[[[ 1.0000,  1.2500,  1.7500,  2.0000],           [ 1.5000,  1.7500,  2.2500,  2.5000],           [ 2.5000,  2.7500,  3.2500,  3.5000],           [ 3.0000,  3.2500,  3.7500,  4.0000]]]])  &gt;&gt;&gt; m = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True) &gt;&gt;&gt; m(input) tensor([[[[ 1.0000,  1.3333,  1.6667,  2.0000],           [ 1.6667,  2.0000,  2.3333,  2.6667],           [ 2.3333,  2.6667,  3.0000,  3.3333],           [ 3.0000,  3.3333,  3.6667,  4.0000]]]])  &gt;&gt;&gt; # Try scaling the same data in a larger tensor &gt;&gt;&gt; &gt;&gt;&gt; input_3x3 = torch.zeros(3, 3).view(1, 1, 3, 3) &gt;&gt;&gt; input_3x3[:, :, :2, :2].copy_(input) tensor([[[[ 1.,  2.],           [ 3.,  4.]]]]) &gt;&gt;&gt; input_3x3 tensor([[[[ 1.,  2.,  0.],           [ 3.,  4.,  0.],           [ 0.,  0.,  0.]]]])  &gt;&gt;&gt; m = nn.Upsample(scale_factor=2, mode='bilinear')  # align_corners=False &gt;&gt;&gt; # Notice that values in top left corner are the same with the small input (except at boundary) &gt;&gt;&gt; m(input_3x3) tensor([[[[ 1.0000,  1.2500,  1.7500,  1.5000,  0.5000,  0.0000],           [ 1.5000,  1.7500,  2.2500,  1.8750,  0.6250,  0.0000],           [ 2.5000,  2.7500,  3.2500,  2.6250,  0.8750,  0.0000],           [ 2.2500,  2.4375,  2.8125,  2.2500,  0.7500,  0.0000],           [ 0.7500,  0.8125,  0.9375,  0.7500,  0.2500,  0.0000],           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]])  &gt;&gt;&gt; m = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True) &gt;&gt;&gt; # Notice that values in top left corner are now changed &gt;&gt;&gt; m(input_3x3) tensor([[[[ 1.0000,  1.4000,  1.8000,  1.6000,  0.8000,  0.0000],           [ 1.8000,  2.2000,  2.6000,  2.2400,  1.1200,  0.0000],           [ 2.6000,  3.0000,  3.4000,  2.8800,  1.4400,  0.0000],           [ 2.4000,  2.7200,  3.0400,  2.5600,  1.2800,  0.0000],           [ 1.2000,  1.3600,  1.5200,  1.2800,  0.6400,  0.0000],           [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]])   ", "parameters": ["size (python:int or Tuple[python:int] or Tuple[python:int, python:int] or Tuple[python:int, python:int, python:int], optional) : output spatial sizes", "scale_factor (python:float or Tuple[python:float] or Tuple[python:float, python:float] or Tuple[python:float, python:float, python:float], optional) : multiplier for spatial size. Has to match input size if it is a tuple.", "mode (str, optional) : the upsampling algorithm: one of 'nearest','linear', 'bilinear', 'bicubic' and 'trilinear'.Default: 'nearest'", "align_corners (bool, optional) : if True, the corner pixels of the inputand output tensors are aligned, and thus preserving the values atthose pixels. This only has effect when mode is'linear', 'bilinear', or 'trilinear'. Default: False"], "returns": null, "example": " input = torch.arange(1, 5, dtype=torch.float32).view(1, 1, 2, 2)\n input\ntensor([[[[ 1.,  2.],\n          [ 3.,  4.]]]])\n\n m = nn.Upsample(scale_factor=2, mode='nearest')\n m(input)\ntensor([[[[ 1.,  1.,  2.,  2.],\n          [ 1.,  1.,  2.,  2.],\n          [ 3.,  3.,  4.,  4.],\n          [ 3.,  3.,  4.,  4.]]]])\n\n m = nn.Upsample(scale_factor=2, mode='bilinear')  # align_corners=False\n m(input)\ntensor([[[[ 1.0000,  1.2500,  1.7500,  2.0000],\n          [ 1.5000,  1.7500,  2.2500,  2.5000],\n          [ 2.5000,  2.7500,  3.2500,  3.5000],\n          [ 3.0000,  3.2500,  3.7500,  4.0000]]]])\n\n m = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n m(input)\ntensor([[[[ 1.0000,  1.3333,  1.6667,  2.0000],\n          [ 1.6667,  2.0000,  2.3333,  2.6667],\n          [ 2.3333,  2.6667,  3.0000,  3.3333],\n          [ 3.0000,  3.3333,  3.6667,  4.0000]]]])\n\n # Try scaling the same data in a larger tensor\n\n input_3x3 = torch.zeros(3, 3).view(1, 1, 3, 3)\n input_3x3[:, :, :2, :2].copy_(input)\ntensor([[[[ 1.,  2.],\n          [ 3.,  4.]]]])\n input_3x3\ntensor([[[[ 1.,  2.,  0.],\n          [ 3.,  4.,  0.],\n          [ 0.,  0.,  0.]]]])\n\n m = nn.Upsample(scale_factor=2, mode='bilinear')  # align_corners=False\n # Notice that values in top left corner are the same with the small input (except at boundary)\n m(input_3x3)\ntensor([[[[ 1.0000,  1.2500,  1.7500,  1.5000,  0.5000,  0.0000],\n          [ 1.5000,  1.7500,  2.2500,  1.8750,  0.6250,  0.0000],\n          [ 2.5000,  2.7500,  3.2500,  2.6250,  0.8750,  0.0000],\n          [ 2.2500,  2.4375,  2.8125,  2.2500,  0.7500,  0.0000],\n          [ 0.7500,  0.8125,  0.9375,  0.7500,  0.2500,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]])\n\n m = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n # Notice that values in top left corner are now changed\n m(input_3x3)\ntensor([[[[ 1.0000,  1.4000,  1.8000,  1.6000,  0.8000,  0.0000],\n          [ 1.8000,  2.2000,  2.6000,  2.2400,  1.1200,  0.0000],\n          [ 2.6000,  3.0000,  3.4000,  2.8800,  1.4400,  0.0000],\n          [ 2.4000,  2.7200,  3.0400,  2.5600,  1.2800,  0.0000],\n          [ 1.2000,  1.3600,  1.5200,  1.2800,  0.6400,  0.0000],\n          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]])\n\n", "shape": " Input: (N,C,Win)(N, C, W_{in})(N,C,Win\u200b)  , (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)   or (N,C,Din,Hin,Win)(N, C, D_{in}, H_{in}, W_{in})(N,C,Din\u200b,Hin\u200b,Win\u200b)   Output: (N,C,Wout)(N, C, W_{out})(N,C,Wout\u200b)  , (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)   or (N,C,Dout,Hout,Wout)(N, C, D_{out}, H_{out}, W_{out})(N,C,Dout\u200b,Hout\u200b,Wout\u200b)  , where  "},
{"library": "torch", "item_id": "torch.nn.UpsamplingNearest2d", "item_type": "class", "code": "classtorch.nn.UpsamplingNearest2d(size=None,scale_factor=None)", "description": "Applies a 2D nearest neighbor upsampling to an input signal composed of several input channels. To specify the scale, it takes either the size or the scale_factor as it\u2019s constructor argument. When size is given, it is the output size of the image (h, w).  Parameters  size (python:int or Tuple[python:int, python:int], optional) \u2013 output spatial sizes scale_factor (python:float or Tuple[python:float, python:float], optional) \u2013 multiplier for spatial size.     Warning This class is deprecated in favor of interpolate().   Shape: Input: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)   Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)   where     Hout=\u230aHin\u00d7scale_factor\u230bH_{out} = \\left\\lfloor H_{in} \\times \\text{scale\\_factor} \\right\\rfloor  Hout\u200b=\u230aHin\u200b\u00d7scale_factor\u230b   Wout=\u230aWin\u00d7scale_factor\u230bW_{out} = \\left\\lfloor W_{in} \\times \\text{scale\\_factor} \\right\\rfloor  Wout\u200b=\u230aWin\u200b\u00d7scale_factor\u230b  Examples: &gt;&gt;&gt; input = torch.arange(1, 5, dtype=torch.float32).view(1, 1, 2, 2) &gt;&gt;&gt; input tensor([[[[ 1.,  2.],           [ 3.,  4.]]]])  &gt;&gt;&gt; m = nn.UpsamplingNearest2d(scale_factor=2) &gt;&gt;&gt; m(input) tensor([[[[ 1.,  1.,  2.,  2.],           [ 1.,  1.,  2.,  2.],           [ 3.,  3.,  4.,  4.],           [ 3.,  3.,  4.,  4.]]]])   ", "parameters": ["size (python:int or Tuple[python:int, python:int], optional) : output spatial sizes", "scale_factor (python:float or Tuple[python:float, python:float], optional) : multiplier forspatial size."], "returns": null, "example": " input = torch.arange(1, 5, dtype=torch.float32).view(1, 1, 2, 2)\n input\ntensor([[[[ 1.,  2.],\n          [ 3.,  4.]]]])\n\n m = nn.UpsamplingNearest2d(scale_factor=2)\n m(input)\ntensor([[[[ 1.,  1.,  2.,  2.],\n          [ 1.,  1.,  2.,  2.],\n          [ 3.,  3.,  4.,  4.],\n          [ 3.,  3.,  4.,  4.]]]])\n\n", "shape": " Input: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)   Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)   where  "},
{"library": "torch", "item_id": "torch.nn.UpsamplingBilinear2d", "item_type": "class", "code": "classtorch.nn.UpsamplingBilinear2d(size=None,scale_factor=None)", "description": "Applies a 2D bilinear upsampling to an input signal composed of several input channels. To specify the scale, it takes either the size or the scale_factor as it\u2019s constructor argument. When size is given, it is the output size of the image (h, w).  Parameters  size (python:int or Tuple[python:int, python:int], optional) \u2013 output spatial sizes scale_factor (python:float or Tuple[python:float, python:float], optional) \u2013 multiplier for spatial size.     Warning This class is deprecated in favor of interpolate(). It is equivalent to nn.functional.interpolate(..., mode='bilinear', align_corners=True).   Shape: Input: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)   Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)   where     Hout=\u230aHin\u00d7scale_factor\u230bH_{out} = \\left\\lfloor H_{in} \\times \\text{scale\\_factor} \\right\\rfloor  Hout\u200b=\u230aHin\u200b\u00d7scale_factor\u230b   Wout=\u230aWin\u00d7scale_factor\u230bW_{out} = \\left\\lfloor W_{in} \\times \\text{scale\\_factor} \\right\\rfloor  Wout\u200b=\u230aWin\u200b\u00d7scale_factor\u230b  Examples: &gt;&gt;&gt; input = torch.arange(1, 5, dtype=torch.float32).view(1, 1, 2, 2) &gt;&gt;&gt; input tensor([[[[ 1.,  2.],           [ 3.,  4.]]]])  &gt;&gt;&gt; m = nn.UpsamplingBilinear2d(scale_factor=2) &gt;&gt;&gt; m(input) tensor([[[[ 1.0000,  1.3333,  1.6667,  2.0000],           [ 1.6667,  2.0000,  2.3333,  2.6667],           [ 2.3333,  2.6667,  3.0000,  3.3333],           [ 3.0000,  3.3333,  3.6667,  4.0000]]]])   ", "parameters": ["size (python:int or Tuple[python:int, python:int], optional) : output spatial sizes", "scale_factor (python:float or Tuple[python:float, python:float], optional) : multiplier forspatial size."], "returns": null, "example": " input = torch.arange(1, 5, dtype=torch.float32).view(1, 1, 2, 2)\n input\ntensor([[[[ 1.,  2.],\n          [ 3.,  4.]]]])\n\n m = nn.UpsamplingBilinear2d(scale_factor=2)\n m(input)\ntensor([[[[ 1.0000,  1.3333,  1.6667,  2.0000],\n          [ 1.6667,  2.0000,  2.3333,  2.6667],\n          [ 2.3333,  2.6667,  3.0000,  3.3333],\n          [ 3.0000,  3.3333,  3.6667,  4.0000]]]])\n\n", "shape": " Input: (N,C,Hin,Win)(N, C, H_{in}, W_{in})(N,C,Hin\u200b,Win\u200b)   Output: (N,C,Hout,Wout)(N, C, H_{out}, W_{out})(N,C,Hout\u200b,Wout\u200b)   where  "},
{"library": "torch", "item_id": "torch.nn.DataParallel", "item_type": "class", "code": "classtorch.nn.DataParallel(module,device_ids=None,output_device=None,dim=0)", "description": "Implements data parallelism at the module level. This container parallelizes the application of the given module by splitting the input across the specified devices by chunking in the batch dimension (other objects will be copied once per device). In the forward pass, the module is replicated on each device, and each replica handles a portion of the input. During the backwards pass, gradients from each replica are summed into the original module. The batch size should be larger than the number of GPUs used. See also: Use nn.DataParallel instead of multiprocessing Arbitrary positional and keyword inputs are allowed to be passed into DataParallel but some types are specially handled. tensors will be scattered on dim specified (default 0). tuple, list and dict types will be shallow copied. The other types will be shared among different threads and can be corrupted if written to in the model\u2019s forward pass. The parallelized module must have its parameters and buffers on device_ids[0] before running this DataParallel module.  Warning In each forward, module is replicated on each device, so any updates to the running module in forward will be lost. For example, if module has a counter attribute that is incremented in each forward, it will always stay at the initial value because the update is done on the replicas which are destroyed after forward. However, DataParallel guarantees that the replica on device[0] will have its parameters and buffers sharing storage with the base parallelized module. So in-place updates to the parameters or buffers on device[0] will be recorded. E.g., BatchNorm2d and spectral_norm() rely on this behavior to update the buffers.   Warning Forward and backward hooks defined on module and its submodules will be invoked len(device_ids) times, each with inputs located on a particular device. Particularly, the hooks are only guaranteed to be executed in correct order with respect to operations on corresponding devices. For example, it is not guaranteed that hooks set via register_forward_pre_hook() be executed before all len(device_ids) forward() calls, but that each such hook be executed before the corresponding forward() call of that device.   Warning When module returns a scalar (i.e., 0-dimensional tensor) in forward(), this wrapper will return a vector of length equal to number of devices used in data parallelism, containing the result from each device.   Note There is a subtlety in using the pack sequence -&gt; recurrent network -&gt; unpack sequence pattern in a Module wrapped in DataParallel. See My recurrent network doesn\u2019t work with data parallelism section in FAQ for details.   Parameters  module (Module) \u2013 module to be parallelized device_ids (list of python:int or torch.device) \u2013 CUDA devices (default: all devices) output_device (python:int or torch.device) \u2013 device location of output (default: device_ids[0])   Variables ~DataParallel.module (Module) \u2013 the module to be parallelized   Example: &gt;&gt;&gt; net = torch.nn.DataParallel(model, device_ids=[0, 1, 2]) &gt;&gt;&gt; output = net(input_var)  # input_var can be on any device, including CPU   ", "parameters": ["module (Module) : module to be parallelized", "device_ids (list of python:int or torch.device) : CUDA devices (default: all devices)", "output_device (python:int or torch.device) : device location of output (default: device_ids[0])"], "returns": null, "example": " net = torch.nn.DataParallel(model, device_ids=[0, 1, 2])\n output = net(input_var)  # input_var can be on any device, including CPU\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.parallel.DistributedDataParallel", "item_type": "class", "code": "classtorch.nn.parallel.DistributedDataParallel(module,device_ids=None,output_device=None,dim=0,broadcast_buffers=True,process_group=None,bucket_cap_mb=25,find_unused_parameters=False,check_reduction=False)", "description": "Implements distributed data parallelism that is based on torch.distributed package at the module level. This container parallelizes the application of the given module by splitting the input across the specified devices by chunking in the batch dimension. The module is replicated on each machine and each device, and each such replica handles a portion of the input. During the backwards pass, gradients from each node are averaged. The batch size should be larger than the number of GPUs used locally. See also: Basics and Use nn.DataParallel instead of multiprocessing. The same constraints on input as in torch.nn.DataParallel apply. Creation of this class requires that torch.distributed to be already initialized, by calling torch.distributed.init_process_group(). DistributedDataParallel can be used in the following two ways:  Single-Process Multi-GPU  In this case, a single process will be spawned on each host/node and each process will operate on all the GPUs of the node where it\u2019s running. To use DistributedDataParallel in this way, you can simply construct the model as the following: &gt;&gt;&gt; torch.distributed.init_process_group(backend=\"nccl\") &gt;&gt;&gt; model = DistributedDataParallel(model) # device_ids will include all GPU devices by default    Multi-Process Single-GPU  This is the highly recommended way to use DistributedDataParallel, with multiple processes, each of which operates on a single GPU. This is currently the fastest approach to do data parallel training using PyTorch and applies to both single-node(multi-GPU) and multi-node data parallel training. It is proven to be significantly faster than torch.nn.DataParallel for single-node multi-GPU data parallel training. Here is how to use it: on each host with N GPUs, you should spawn up N processes, while ensuring that each process individually works on a single GPU from 0 to N-1. Therefore, it is your job to ensure that your training script operates on a single given GPU by calling: &gt;&gt;&gt; torch.cuda.set_device(i)   where i is from 0 to N-1. In each process, you should refer the following to construct this module: &gt;&gt;&gt; torch.distributed.init_process_group(backend='nccl', world_size=4, init_method='...') &gt;&gt;&gt; model = DistributedDataParallel(model, device_ids=[i], output_device=i)   In order to spawn up multiple processes per node, you can use either torch.distributed.launch or torch.multiprocessing.spawn  Note nccl backend is currently the fastest and highly recommended backend to be used with Multi-Process Single-GPU distributed training and this applies to both single-node and multi-node distributed training   Note This module also supports mixed-precision distributed training. This means that your model can have different types of parameters such as mixed types of fp16 and fp32, the gradient reduction on these mixed types of parameters will just work fine. Also note that nccl backend is currently the fastest and highly recommended backend for fp16/fp32 mixed-precision training.   Note If you use torch.save on one process to checkpoint the module, and torch.load on some other processes to recover it, make sure that map_location is configured properly for every process. Without map_location, torch.load would recover the module to devices where the module was saved from.   Warning This module works only with the gloo and nccl backends.   Warning Constructor, forward method, and differentiation of the output (or a function of the output of this module) is a distributed synchronization point. Take that into account in case different processes might be executing different code.   Warning This module assumes all parameters are registered in the model by the time it is created. No parameters should be added nor removed later. Same applies to buffers.   Warning This module assumes all parameters are registered in the model of each distributed processes are in the same order. The module itself will conduct gradient all-reduction following the reverse order of the registered parameters of the model. In other words, it is users\u2019 responsibility to ensure that each distributed process has the exact same model and thus the exact same parameter registration order.   Warning This module assumes all buffers and gradients are dense.   Warning This module doesn\u2019t work with torch.autograd.grad() (i.e. it will only work if gradients are to be accumulated in .grad attributes of parameters).   Warning If you plan on using this module with a nccl backend or a gloo backend (that uses Infiniband), together with a DataLoader that uses multiple workers, please change the multiprocessing start method to forkserver (Python 3 only) or spawn. Unfortunately Gloo (that uses Infiniband) and NCCL2 are not fork safe, and you will likely experience deadlocks if you don\u2019t change this setting.   Warning Forward and backward hooks defined on module and its submodules won\u2019t be invoked anymore, unless the hooks are initialized in the forward() method.   Warning You should never try to change your model\u2019s parameters after wrapping up your model with DistributedDataParallel. In other words, when wrapping up your model with DistributedDataParallel, the constructor of DistributedDataParallel will register the additional gradient reduction functions on all the parameters of the model itself at the time of construction. If you change the model\u2019s parameters after the DistributedDataParallel construction, this is not supported and unexpected behaviors can happen, since some parameters\u2019 gradient reduction functions might not get called.   Note Parameters are never broadcast between processes. The module performs an all-reduce step on gradients and assumes that they will be modified by the optimizer in all processes in the same way. Buffers (e.g. BatchNorm stats) are broadcast from the module in process of rank 0, to all other replicas in the system in every iteration.   Parameters  module (Module) \u2013 module to be parallelized device_ids (list of python:int or torch.device) \u2013 CUDA devices. This should only be provided when the input module resides on a single CUDA device. For single-device modules, the i``th :attr:`module` replica is placed on ``device_ids[i]. For multi-device modules and CPU modules, device_ids must be None or an empty list, and input data for the forward pass must be placed on the correct device. (default: all devices for single-device modules) output_device (python:int or torch.device) \u2013 device location of output for single-device CUDA modules. For multi-device modules and CPU modules, it must be None, and the module itself dictates the output location. (default: device_ids[0] for single-device modules) broadcast_buffers (bool) \u2013 flag that enables syncing (broadcasting) buffers of the module at beginning of the forward function. (default: True) process_group \u2013 the process group to be used for distributed data all-reduction. If None, the default process group, which is created by `torch.distributed.init_process_group`, will be used. (default: None) bucket_cap_mb \u2013 DistributedDataParallel will bucket parameters into multiple buckets so that gradient reduction of each bucket can potentially overlap with backward computation. bucket_cap_mb controls the bucket size in MegaBytes (MB) (default: 25) find_unused_parameters (bool) \u2013 Traverse the autograd graph of all tensors contained in the return value of the wrapped module\u2019s forward function. Parameters that don\u2019t receive gradients as part of this graph are preemptively marked as being ready to be reduced. Note that all forward outputs that are derived from module parameters must participate in calculating loss and later the gradient computation. If they don\u2019t, this wrapper will hang waiting for autograd to produce gradients for those parameters. Any outputs derived from module parameters that are otherwise unused can be detached from the autograd graph using torch.Tensor.detach. (default: False) check_reduction \u2013 when setting to True, it enables DistributedDataParallel to automatically check if the previous iteration\u2019s backward reductions were successfully issued at the beginning of every iteration\u2019s forward function. You normally don\u2019t need this option enabled unless you are observing weird behaviors such as different ranks are getting different gradients, which should not happen if DistributedDataParallel is correctly used. (default: False)   Variables ~DistributedDataParallel.module (Module) \u2013 the module to be parallelized   Example: &gt;&gt;&gt; torch.distributed.init_process_group(backend='nccl', world_size=4, init_method='...') &gt;&gt;&gt; net = torch.nn.DistributedDataParallel(model, pg)     no_sync()  A context manager to disable gradient synchronizations across DDP processes. Within this context, gradients will be accumulated on module variables, which will later be synchronized in the first forward-backward pass exiting the context. Example: &gt;&gt;&gt; ddp = torch.nn.DistributedDataParallel(model, pg) &gt;&gt;&gt; with ddp.no_sync(): ...   for input in inputs: ...     ddp(input).backward()  # no synchronization, accumulate grads ... ddp(another_input).backward()  # synchronize grads     ", "parameters": ["module (Module) : module to be parallelized", "device_ids (list of python:int or torch.device) : CUDA devices. This shouldonly be provided when the input module resides on a singleCUDA device. For single-device modules, the i``th:attr:`module` replica is placed on ``device_ids[i]. Formulti-device modules and CPU modules, device_ids must be Noneor an empty list, and input data for the forward pass must beplaced on the correct device. (default: all devices forsingle-device modules)", "output_device (python:int or torch.device) : device location of output forsingle-device CUDA modules. For multi-device modules andCPU modules, it must be None, and the module itselfdictates the output location. (default: device_ids[0] forsingle-device modules)", "broadcast_buffers (bool) : flag that enables syncing (broadcasting) buffers ofthe module at beginning of the forward function.(default: True)", "process_group : the process group to be used for distributed dataall-reduction. If None, the default process group, whichis created by `torch.distributed.init_process_group`,will be used. (default: None)", "bucket_cap_mb : DistributedDataParallel will bucket parameters intomultiple buckets so that gradient reduction of eachbucket can potentially overlap with backward computation.bucket_cap_mb controls the bucket size in MegaBytes (MB)(default: 25)", "find_unused_parameters (bool) : Traverse the autograd graph of all tensorscontained in the return value of the wrappedmodule\u2019s forward function.Parameters that don\u2019t receive gradients aspart of this graph are preemptively markedas being ready to be reduced. Note that allforward outputs that are derived frommodule parameters must participate incalculating loss and later the gradientcomputation. If they don\u2019t, this wrapper willhang waiting for autograd to produce gradientsfor those parameters. Any outputs derived frommodule parameters that are otherwise unused canbe detached from the autograd graph usingtorch.Tensor.detach. (default: False)", "check_reduction : when setting to True, it enables DistributedDataParallelto automatically check if the previous iteration\u2019sbackward reductions were successfully issued at thebeginning of every iteration\u2019s forward function.You normally don\u2019t need this option enabled unless youare observing weird behaviors such as different ranksare getting different gradients, which should nothappen if DistributedDataParallel is correctly used.(default: False)"], "returns": null, "example": " torch.distributed.init_process_group(backend='nccl', world_size=4, init_method='...')\n net = torch.nn.DistributedDataParallel(model, pg)\n\n", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.BasePruningMethod", "item_type": "class", "code": "classtorch.nn.utils.prune.BasePruningMethod", "description": "Abstract base class for creation of new pruning techniques. Provides a skeleton for customization requiring the overriding of methods such as compute_mask() and apply().   classmethod apply(module, name, *args, **kwargs)  Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters  module (nn.Module) \u2013 module containing the tensor to prune name (str) \u2013 parameter name within module on which pruning will act. args \u2013 arguments passed on to a subclass of BasePruningMethod kwargs \u2013 keyword arguments passed on to a subclass of a BasePruningMethod        apply_mask(module)  Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters module (nn.Module) \u2013 module containing the tensor to prune  Returns pruned version of the input tensor  Return type pruned_tensor (torch.Tensor)       abstract compute_mask(t, default_mask)  Computes and returns a mask for the input tensor t. Starting from a base default_mask (which should be a mask of ones if the tensor has not been pruned yet), generate a random mask to apply on top of the default_mask according to the specific pruning method recipe.  Parameters  t (torch.Tensor) \u2013 tensor representing the parameter to prune default_mask (torch.Tensor) \u2013 Base mask from previous pruning iterations, that need to be respected after the new mask is applied. Same dims as t.   Returns mask to apply to t, of same dims as t  Return type mask (torch.Tensor)       prune(t, default_mask=None)  Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters  t (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). default_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns pruned version of tensor t.       remove(module)  Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!    ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "name (str) : parameter name within module on which pruningwill act.", "args : arguments passed on to a subclass ofBasePruningMethod", "kwargs : keyword arguments passed on to a subclass of aBasePruningMethod", "t (torch.Tensor) : tensor representing the parameter to prune", "default_mask (torch.Tensor) : Base mask from previous pruningiterations, that need to be respected after the new mask isapplied. Same dims as t.", "t (torch.Tensor) : tensor to prune (of same dimensions asdefault_mask).", "default_mask (torch.Tensor, optional) : mask from previous pruningiteration, if any. To be considered when determining whatportion of the tensor that pruning should act on. If None,default to a mask of ones."], "returns": "pruned version of the input tensor", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.PruningContainer", "item_type": "class", "code": "classtorch.nn.utils.prune.PruningContainer(*args)", "description": "Container holding a sequence of pruning methods for iterative pruning. Keeps track of the order in which pruning methods are applied and handles combining successive pruning calls. Accepts as argument an instance of a BasePruningMethod or an iterable of them.   add_pruning_method(method)  Adds a child pruning method to the container.  Parameters method (subclass of BasePruningMethod) \u2013 child pruning method to be added to the container.       classmethod apply(module, name, *args, **kwargs) Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters  module (nn.Module) \u2013 module containing the tensor to prune name (str) \u2013 parameter name within module on which pruning will act. args \u2013 arguments passed on to a subclass of BasePruningMethod kwargs \u2013 keyword arguments passed on to a subclass of a BasePruningMethod        apply_mask(module) Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters module (nn.Module) \u2013 module containing the tensor to prune  Returns pruned version of the input tensor  Return type pruned_tensor (torch.Tensor)       compute_mask(t, default_mask)  Applies the latest method by computing the new partial masks and returning its combination with the default_mask. The new partial mask should be computed on the entries or channels that were not zeroed out by the default_mask. Which portions of the tensor t the new mask will be calculated from depends on the PRUNING_TYPE (handled by the type handler):   for \u2018unstructured\u2019, the mask will be computed from the raveled  list of nonmasked entries;  for \u2018structured\u2019, the mask will be computed from the nonmasked  channels in the tensor;  for \u2018global\u2019, the mask will be computed across all entries.    Parameters  t (torch.Tensor) \u2013 tensor representing the parameter to prune (of same dimensions as default_mask). default_mask (torch.Tensor) \u2013 mask from previous pruning iteration.   Returns new mask that combines the effects of the default_mask and the new mask from the current pruning method (of same dimensions as default_mask and t).  Return type mask (torch.Tensor)       prune(t, default_mask=None) Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters  t (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). default_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns pruned version of tensor t.       remove(module) Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!    ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "name (str) : parameter name within module on which pruningwill act.", "args : arguments passed on to a subclass ofBasePruningMethod", "kwargs : keyword arguments passed on to a subclass of aBasePruningMethod", "t (torch.Tensor) : tensor representing the parameter to prune(of same dimensions as default_mask).", "default_mask (torch.Tensor) : mask from previous pruning iteration.", "t (torch.Tensor) : tensor to prune (of same dimensions asdefault_mask).", "default_mask (torch.Tensor, optional) : mask from previous pruningiteration, if any. To be considered when determining whatportion of the tensor that pruning should act on. If None,default to a mask of ones."], "returns": "pruned version of the input tensor", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.Identity", "item_type": "class", "code": "classtorch.nn.utils.prune.Identity", "description": "Utility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones.   classmethod apply(module, name)  Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters  module (nn.Module) \u2013 module containing the tensor to prune name (str) \u2013 parameter name within module on which pruning will act.        apply_mask(module) Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters module (nn.Module) \u2013 module containing the tensor to prune  Returns pruned version of the input tensor  Return type pruned_tensor (torch.Tensor)       prune(t, default_mask=None) Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters  t (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). default_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns pruned version of tensor t.       remove(module) Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!    ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "name (str) : parameter name within module on which pruningwill act.", "t (torch.Tensor) : tensor to prune (of same dimensions asdefault_mask).", "default_mask (torch.Tensor, optional) : mask from previous pruningiteration, if any. To be considered when determining whatportion of the tensor that pruning should act on. If None,default to a mask of ones."], "returns": "pruned version of the input tensor", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.RandomUnstructured", "item_type": "class", "code": "classtorch.nn.utils.prune.RandomUnstructured(amount)", "description": "Prune (currently unpruned) units in a tensor at random.  Parameters  name (str) \u2013 parameter name within module on which pruning will act. amount (python:int or python:float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune.      classmethod apply(module, name, amount)  Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters  module (nn.Module) \u2013 module containing the tensor to prune name (str) \u2013 parameter name within module on which pruning will act. amount (python:int or python:float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune.        apply_mask(module) Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters module (nn.Module) \u2013 module containing the tensor to prune  Returns pruned version of the input tensor  Return type pruned_tensor (torch.Tensor)       prune(t, default_mask=None) Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters  t (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). default_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns pruned version of tensor t.       remove(module) Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!    ", "parameters": ["name (str) : parameter name within module on which pruningwill act.", "amount (python:int or python:float) : quantity of parameters to prune.If float, should be between 0.0 and 1.0 and represent thefraction of parameters to prune. If int, it represents theabsolute number of parameters to prune.", "module (nn.Module) : module containing the tensor to prune", "name (str) : parameter name within module on which pruningwill act.", "amount (python:int or python:float) : quantity of parameters to prune.If float, should be between 0.0 and 1.0 and represent thefraction of parameters to prune. If int, it represents theabsolute number of parameters to prune.", "t (torch.Tensor) : tensor to prune (of same dimensions asdefault_mask).", "default_mask (torch.Tensor, optional) : mask from previous pruningiteration, if any. To be considered when determining whatportion of the tensor that pruning should act on. If None,default to a mask of ones."], "returns": "pruned version of the input tensor", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.L1Unstructured", "item_type": "class", "code": "classtorch.nn.utils.prune.L1Unstructured(amount)", "description": "Prune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm.  Parameters amount (python:int or python:float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune.     classmethod apply(module, name, amount)  Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters  module (nn.Module) \u2013 module containing the tensor to prune name (str) \u2013 parameter name within module on which pruning will act. amount (python:int or python:float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune.        apply_mask(module) Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters module (nn.Module) \u2013 module containing the tensor to prune  Returns pruned version of the input tensor  Return type pruned_tensor (torch.Tensor)       prune(t, default_mask=None) Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters  t (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). default_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns pruned version of tensor t.       remove(module) Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!    ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "name (str) : parameter name within module on which pruningwill act.", "amount (python:int or python:float) : quantity of parameters to prune.If float, should be between 0.0 and 1.0 and represent thefraction of parameters to prune. If int, it represents theabsolute number of parameters to prune.", "t (torch.Tensor) : tensor to prune (of same dimensions asdefault_mask).", "default_mask (torch.Tensor, optional) : mask from previous pruningiteration, if any. To be considered when determining whatportion of the tensor that pruning should act on. If None,default to a mask of ones."], "returns": "pruned version of the input tensor", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.RandomStructured", "item_type": "class", "code": "classtorch.nn.utils.prune.RandomStructured(amount,dim=-1)", "description": "Prune entire (currently unpruned) channels in a tensor at random.  Parameters  amount (python:int or python:float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune. dim (python:int, optional) \u2013 index of the dim along which we define channels to prune. Default: -1.      classmethod apply(module, name, amount, dim=-1)  Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters  module (nn.Module) \u2013 module containing the tensor to prune name (str) \u2013 parameter name within module on which pruning will act. amount (python:int or python:float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune. dim (python:int, optional) \u2013 index of the dim along which we define channels to prune. Default: -1.        apply_mask(module) Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters module (nn.Module) \u2013 module containing the tensor to prune  Returns pruned version of the input tensor  Return type pruned_tensor (torch.Tensor)       compute_mask(t, default_mask)  Computes and returns a mask for the input tensor t. Starting from a base default_mask (which should be a mask of ones if the tensor has not been pruned yet), generate a random mask to apply on top of the default_mask by randomly zeroing out channels along the specified dim of the tensor.  Parameters  t (torch.Tensor) \u2013 tensor representing the parameter to prune default_mask (torch.Tensor) \u2013 Base mask from previous pruning iterations, that need to be respected after the new mask is applied. Same dims as t.   Returns mask to apply to t, of same dims as t  Return type mask (torch.Tensor)  Raises IndexError \u2013 if self.dim &gt;= len(t.shape)       prune(t, default_mask=None) Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters  t (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). default_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns pruned version of tensor t.       remove(module) Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!    ", "parameters": ["amount (python:int or python:float) : quantity of parameters to prune.If float, should be between 0.0 and 1.0 and represent thefraction of parameters to prune. If int, it represents theabsolute number of parameters to prune.", "dim (python:int, optional) : index of the dim along which we definechannels to prune. Default: -1.", "module (nn.Module) : module containing the tensor to prune", "name (str) : parameter name within module on which pruningwill act.", "amount (python:int or python:float) : quantity of parameters to prune.If float, should be between 0.0 and 1.0 and represent thefraction of parameters to prune. If int, it represents theabsolute number of parameters to prune.", "dim (python:int, optional) : index of the dim along which we definechannels to prune. Default: -1.", "t (torch.Tensor) : tensor representing the parameter to prune", "default_mask (torch.Tensor) : Base mask from previous pruningiterations, that need to be respected after the new mask isapplied. Same dims as t.", "t (torch.Tensor) : tensor to prune (of same dimensions asdefault_mask).", "default_mask (torch.Tensor, optional) : mask from previous pruningiteration, if any. To be considered when determining whatportion of the tensor that pruning should act on. If None,default to a mask of ones."], "returns": "pruned version of the input tensor", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.LnStructured", "item_type": "class", "code": "classtorch.nn.utils.prune.LnStructured(amount,n,dim=-1)", "description": "Prune entire (currently unpruned) channels in a tensor based on their Ln-norm.  Parameters  amount (python:int or python:float) \u2013 quantity of channels to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune. n (python:int, python:float, inf, -inf, 'fro', 'nuc') \u2013 See documentation of valid entries for argument p in torch.norm(). dim (python:int, optional) \u2013 index of the dim along which we define channels to prune. Default: -1.      classmethod apply(module, name, amount, n, dim)  Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters  module (nn.Module) \u2013 module containing the tensor to prune name (str) \u2013 parameter name within module on which pruning will act. amount (python:int or python:float) \u2013 quantity of parameters to prune. If float, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If int, it represents the absolute number of parameters to prune. n (python:int, python:float, inf, -inf, 'fro', 'nuc') \u2013 See documentation of valid entries for argument p in torch.norm(). dim (python:int) \u2013 index of the dim along which we define channels to prune.        apply_mask(module) Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters module (nn.Module) \u2013 module containing the tensor to prune  Returns pruned version of the input tensor  Return type pruned_tensor (torch.Tensor)       compute_mask(t, default_mask)  Computes and returns a mask for the input tensor t. Starting from a base default_mask (which should be a mask of ones if the tensor has not been pruned yet), generate a mask to apply on top of the default_mask by zeroing out the channels along the specified dim with the lowest Ln-norm.  Parameters  t (torch.Tensor) \u2013 tensor representing the parameter to prune default_mask (torch.Tensor) \u2013 Base mask from previous pruning iterations, that need to be respected after the new mask is applied.  Same dims as t.   Returns mask to apply to t, of same dims as t  Return type mask (torch.Tensor)  Raises IndexError \u2013 if self.dim &gt;= len(t.shape)       prune(t, default_mask=None) Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters  t (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). default_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns pruned version of tensor t.       remove(module) Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!    ", "parameters": ["amount (python:int or python:float) : quantity of channels to prune.If float, should be between 0.0 and 1.0 and represent thefraction of parameters to prune. If int, it represents theabsolute number of parameters to prune.", "n (python:int, python:float, inf, -inf, 'fro', 'nuc') : See documentation of validentries for argument p in torch.norm().", "dim (python:int, optional) : index of the dim along which we definechannels to prune. Default: -1.", "module (nn.Module) : module containing the tensor to prune", "name (str) : parameter name within module on which pruningwill act.", "amount (python:int or python:float) : quantity of parameters to prune.If float, should be between 0.0 and 1.0 and represent thefraction of parameters to prune. If int, it represents theabsolute number of parameters to prune.", "n (python:int, python:float, inf, -inf, 'fro', 'nuc') : See documentation of validentries for argument p in torch.norm().", "dim (python:int) : index of the dim along which we define channels toprune.", "t (torch.Tensor) : tensor representing the parameter to prune", "default_mask (torch.Tensor) : Base mask from previous pruningiterations, that need to be respected after the new mask isapplied.  Same dims as t.", "t (torch.Tensor) : tensor to prune (of same dimensions asdefault_mask).", "default_mask (torch.Tensor, optional) : mask from previous pruningiteration, if any. To be considered when determining whatportion of the tensor that pruning should act on. If None,default to a mask of ones."], "returns": "pruned version of the input tensor", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.utils.prune.CustomFromMask", "item_type": "class", "code": "classtorch.nn.utils.prune.CustomFromMask(mask)", "description": "  classmethod apply(module, name, mask)  Adds the forward pre-hook that enables pruning on the fly and the reparametrization of a tensor in terms of the original tensor and the pruning mask.  Parameters  module (nn.Module) \u2013 module containing the tensor to prune name (str) \u2013 parameter name within module on which pruning will act.        apply_mask(module) Simply handles the multiplication between the parameter being pruned and the generated mask. Fetches the mask and the original tensor from the module and returns the pruned version of the tensor.  Parameters module (nn.Module) \u2013 module containing the tensor to prune  Returns pruned version of the input tensor  Return type pruned_tensor (torch.Tensor)       prune(t, default_mask=None) Computes and returns a pruned version of input tensor t according to the pruning rule specified in compute_mask().  Parameters  t (torch.Tensor) \u2013 tensor to prune (of same dimensions as default_mask). default_mask (torch.Tensor, optional) \u2013 mask from previous pruning iteration, if any. To be considered when determining what portion of the tensor that pruning should act on. If None, default to a mask of ones.   Returns pruned version of tensor t.       remove(module) Removes the pruning reparameterization from a module. The pruned parameter named name remains permanently pruned, and the parameter named name+'_orig' is removed from the parameter list. Similarly, the buffer named name+'_mask' is removed from the buffers.  Note Pruning itself is NOT undone or reversed!    ", "parameters": ["module (nn.Module) : module containing the tensor to prune", "name (str) : parameter name within module on which pruningwill act.", "t (torch.Tensor) : tensor to prune (of same dimensions asdefault_mask).", "default_mask (torch.Tensor, optional) : mask from previous pruningiteration, if any. To be considered when determining whatportion of the tensor that pruning should act on. If None,default to a mask of ones."], "returns": "pruned version of the input tensor", "example": "NA", "shape": "NA"},
{"library": "torch", "item_id": "torch.nn.Flatten", "item_type": "class", "code": "classtorch.nn.Flatten(start_dim=1,end_dim=-1)", "description": "Flattens a contiguous range of dims into a tensor. For use with Sequential. :param start_dim: first dim to flatten (default = 1). :param end_dim: last dim to flatten (default = -1).  Shape: Input: (N,\u2217dims)(N, *dims)(N,\u2217dims)   Output: (N,\u220f\u2217dims)(N, \\prod *dims)(N,\u220f\u2217dims)   (for the default case).   Examples::&gt;&gt;&gt; m = nn.Sequential( &gt;&gt;&gt;     nn.Conv2d(1, 32, 5, 1, 1), &gt;&gt;&gt;     nn.Flatten() &gt;&gt;&gt; )     ", "parameters": [], "returns": null, "example": " m = nn.Sequential(\n     nn.Conv2d(1, 32, 5, 1, 1),\n     nn.Flatten()\n )\n\n\n", "shape": " Input: (N,\u2217dims)(N, *dims)(N,\u2217dims)   Output: (N,\u220f\u2217dims)(N, \\prod *dims)(N,\u220f\u2217dims)   (for the default case).  "}
]