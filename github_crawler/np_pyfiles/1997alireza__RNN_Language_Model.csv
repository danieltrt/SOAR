file_path,api_count,code
RNN_tensorflow.py,11,"b'from __future__ import print_function, division\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nnum_epochs = 100\ntotal_series_length = 50000\ntruncated_backprop_length = 15  # todo?\nstate_size = 4\nnum_classes = 2  # todo?\necho_step = 3\nbatch_size = 5\nnum_batches = total_series_length//batch_size//truncated_backprop_length\n\ngen_d_x = np.array(np.random.choice(2, total_series_length, p=[0.5, 0.5]))\ngen_d_y = np.roll(gen_d_x, echo_step)\ngen_d_y[0:echo_step] = 0\ngen_d_x = gen_d_x.reshape((batch_size, -1))\ngen_d_y = gen_d_y.reshape((batch_size, -1))\n\n\ndef generate_data():\n    # x = np.array(np.random.choice(2, total_series_length, p=[0.5, 0.5]))\n    # y = np.roll(x, echo_step)\n    # y[0:echo_step] = 0\n    #\n    # x = x.reshape((batch_size, -1))\n    # y = y.reshape((batch_size, -1))\n\n    return gen_d_x, gen_d_y\n\n\ndef plot(loss_list, predictions_series, batch_x, batch_y):\n    plt.subplot(2, 3, 1)\n    plt.cla()\n    plt.plot(loss_list)\n\n    for batch_series_idx in range(5):\n        one_hot_output_series = np.array(predictions_series)[:, batch_series_idx, :]\n        single_output_series = np.array([(1 if out[0] < 0.5 else 0) for out in one_hot_output_series])\n\n        plt.subplot(2, 3, batch_series_idx + 2)\n        plt.cla()\n        plt.axis([0, truncated_backprop_length, 0, 2])\n        left_offset = range(truncated_backprop_length)\n        plt.bar(left_offset, batch_x[batch_series_idx, :], width=1, color=""blue"")\n        plt.bar(left_offset, batch_y[batch_series_idx, :] * 0.5, width=1, color=""red"")\n        plt.bar(left_offset, single_output_series * 0.3, width=1, color=""green"")\n\n    plt.draw()\n    plt.pause(0.0001)\n\n\nif __name__ == \'__main__\':\n    x_placeholder = tf.placeholder(tf.float32, [batch_size, truncated_backprop_length])\n    y_placeholder = tf.placeholder(tf.int32, [batch_size, truncated_backprop_length])\n\n    init_state = tf.placeholder(tf.float32, [batch_size, state_size])\n\n    w = tf.Variable(np.random.rand(state_size + 1, state_size), dtype=tf.float32)\n    b = tf.Variable(np.zeros((1, state_size)), dtype=tf.float32)\n\n    w2 = tf.Variable(np.random.rand(state_size, num_classes), dtype=tf.float32)\n    b2 = tf.Variable(np.zeros((1, num_classes)), dtype=tf.float32)\n\n    # Unpack columns\n    inputs_series = tf.unstack(x_placeholder, axis=1)\n    labels_series = tf.unstack(y_placeholder, axis=1)\n\n    # Forward pass\n    current_state = init_state\n    states_series = []\n    for current_input in inputs_series:\n        current_input = tf.reshape(current_input, [batch_size, 1])\n        input_and_state_concatenated = tf.concat([current_input, current_state], axis=1)  # Increasing number of columns\n\n        next_state = tf.tanh(tf.matmul(input_and_state_concatenated, w) + b)  # Broadcasted addition\n        states_series.append(next_state)\n        current_state = next_state\n\n        logits_series = [tf.matmul(current_state, w2) + b2 for state in states_series]\n        predictions_series = [tf.nn.softmax(logits) for logits in logits_series]\n        losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels) for logits, labels\n                  in zip(logits_series, labels_series)]\n        #  The classes are either zero or one, which is the reason for using the \xe2\x80\x9cSparse-softmax\xe2\x80\x9d\n        total_loss = tf.reduce_mean(losses)\n        train_step = tf.train.AdagradOptimizer(0.3).minimize(total_loss)\n\n    with tf.Session() as sess:\n        sess.run(tf.initialize_all_variables())\n        plt.ion()\n        plt.figure()\n        plt.show()\n        loss_list = []\n\n        for epoch_idx in range(num_epochs):\n            x, y = generate_data()\n            _current_state = np.zeros((batch_size, state_size))\n\n            print(""New data, epoch"", epoch_idx)\n\n            for batch_idx in range(num_batches):\n                start_idx = batch_idx * truncated_backprop_length\n                end_idx = start_idx + truncated_backprop_length\n\n                batchX = x[:, start_idx:end_idx]\n                batchY = y[:, start_idx:end_idx]\n\n                _total_loss, _train_step, _current_state, _predictions_series = sess.run(\n                    [total_loss, train_step, current_state, predictions_series],\n                    feed_dict={\n                        x_placeholder: batchX,\n                        y_placeholder: batchY,\n                        init_state: _current_state\n                    })\n\n                loss_list.append(_total_loss)\n\n                if batch_idx % 100 == 0:\n                    print(""Step"", batch_idx, ""Loss"", _total_loss)\n                    plot(loss_list, _predictions_series, batchX, batchY)\n\n    plt.ioff()\n    plt.show()\n'"
LSTM_tensorflow/char_level.py,21,"b'import time\nfrom LSTM_tensorflow.tools import *\nimport tensorflow as tf\nimport os\n\n\nclass LSTM_NN:\n\n    def __init__(self, session, check_point_dir, hidden_size=256, num_layers=2, lr=0.003,\n                 scope_name=""RNN""):\n        self.scope = scope_name\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.session = session\n        self.lr = tf.constant(lr)\n        self.check_point_dir = check_point_dir\n\n        # Defining the computational graph\n\n        # type 0 -> is not tuple (use in non-tuple mode)\n        # self.lstm_last_state = np.zeros(\n        #     (self.num_layers * 2 * self.hidden_size,)\n        #     # num_layer * 2 (one for h and one for c) * hidden_size\n        # )\n\n        # type 1 -> wrong\n        # self.lstm_last_state = [\n        #     np.zeros(\n        #         (self.num_layers * self.hidden_size,)),\n        #     np.zeros(\n        #         (self.num_layers * self.hidden_size,))\n        #\n        # ]\n\n        # type 2\n        self.lstm_last_state = tuple(\n            [\n                tf.nn.rnn_cell.LSTMStateTuple(\n                    np.zeros((1, self.hidden_size)),\n                    np.zeros((1, self.hidden_size))\n                ) for _ in range(self.num_layers)\n            ]\n        )\n\n        with tf.variable_scope(self.scope):\n            self.x_batch = tf.placeholder(\n                tf.float32,\n                shape=(None, None, SIZE_OF_VOCAB),\n                # None, None -> number of sentences in this batch, CHAR_NUM_OF_SENTENCE\n                name=""input""\n            )\n            # type 0 -> is not tuple (use in non-tuple mode)\n            # self.lstm_init_value = tf.placeholder(\n            #     tf.float32,\n            #     shape=(None, self.num_layers * 2 * self.hidden_size),\n            #     # None -> number of sentences in this batch\n            #     name=""lstm_init_value""\n            # )\n\n            # type 1 -> wrong\n            # self.c_layer = tf.placeholder(\n            #     tf.float32,\n            #     shape=[None, self.num_layers * self.hidden_size],\n            #     name=\'c_layer\'\n            # )\n            #\n            # self.h_layer = tf.placeholder(\n            #     tf.float32,\n            #     shape=[None, self.num_layers * self.hidden_size],\n            #     name=\'h_layer\'\n            # )\n            #\n            # self.lstm_init_value = tf.nn.rnn_cell.LSTMStateTuple(self.c_layer, self.h_layer)\n\n            # type 2\n            self.lstm_init_value = tuple(\n                [\n                    tf.nn.rnn_cell.LSTMStateTuple(\n                        tf.placeholder(tf.float32, shape=[None, self.hidden_size], name=\'c_layer\' + str(l_id)),\n                        tf.placeholder(tf.float32, shape=[None, self.hidden_size], name=\'h_layer\' + str(l_id)),\n                    ) for l_id in range(self.num_layers)\n                ]\n            )\n\n            # LSTM\n            self.lstm_cells = [\n                tf.contrib.rnn.BasicLSTMCell(\n                    self.hidden_size,\n                    forget_bias=1.0,\n                    state_is_tuple=True\n                ) for _ in range(self.num_layers)\n            ]\n            self.lstm = tf.contrib.rnn.MultiRNNCell(\n                self.lstm_cells,\n                state_is_tuple=True\n            )\n\n            outputs, self.lstm_new_state = tf.nn.dynamic_rnn(\n                self.lstm,\n                self.x_batch,\n                initial_state=self.lstm_init_value,\n                dtype=tf.float32\n            )\n            # fc layer at the end\n            self.W = tf.Variable(\n                tf.random_normal(\n                    (self.hidden_size, SIZE_OF_VOCAB),\n                    stddev=0.01\n                )\n            )\n            self.B = tf.Variable(\n                tf.random_normal(\n                    (SIZE_OF_VOCAB,), stddev=0.01\n                    # size : SIZE_OF_VOCAB (1d)\n                )\n            )\n            outputs_reshaped = tf.reshape(outputs, [-1, self.hidden_size])\n            network_output = tf.matmul(\n                outputs_reshaped,\n                self.W\n            ) + self.B\n\n            batch_time_shape = tf.shape(outputs)\n            self.final_outputs = tf.reshape(\n                tf.nn.softmax(network_output),\n                (batch_time_shape[0], batch_time_shape[1], SIZE_OF_VOCAB)\n            )\n\n            self.y_batch = tf.placeholder(\n                tf.float32,\n                (None, None, SIZE_OF_VOCAB)\n            )\n            y_batch_long = tf.reshape(self.y_batch, [-1, SIZE_OF_VOCAB])\n            self.cost = tf.reduce_mean(\n                tf.nn.softmax_cross_entropy_with_logits(\n                    logits=network_output,\n                    labels=y_batch_long\n                )\n            )\n            # minimizing the error\n            self.train_op = tf.train.RMSPropOptimizer(\n                self.lr,\n                0.9\n            ).minimize(self.cost)\n\n    def train_on_batch(self, x_batch, y_batch):\n        """"""\n\n        :param x_batch: size=(batch_size, char_num_of_sentence, SIZE_OF_VOCAB)\n        :param y_batch: size=(batch_size, char_num_of_sentence, SIZE_OF_VOCAB)\n        :return:\n        """"""\n\n        # type 0 -> is not tuple (use in non-tuple mode)\n        # init_value = np.zeros(\n        #     (x_batch.shape[0], self.num_layers * 2 * self.hidden_size)\n        # )\n\n        # type 1 -> wrong\n        # init_c_layer = np.zeros((\n        #     x_batch.shape[0], self.num_layers * self.hidden_size\n        # ))\n        # init_h_layer = np.zeros((\n        #     x_batch.shape[0], self.num_layers * self.hidden_size\n        # ))\n        #\n        # init_value = tf.nn.rnn_cell.LSTMStateTuple(self.c_layer, self.h_layer)\n\n        # type 2\n        init_value = tuple(\n            [\n                tf.nn.rnn_cell.LSTMStateTuple(\n                    np.zeros((x_batch.shape[0], self.hidden_size)),\n                    np.zeros((x_batch.shape[0], self.hidden_size))\n                ) for _ in range(self.num_layers)\n            ]\n        )\n\n        cost, _ = self.session.run(\n            [self.cost, self.train_op],\n            feed_dict={\n                self.x_batch: x_batch,\n                self.y_batch: y_batch,\n                self.lstm_init_value: init_value\n            }\n        )\n        return cost\n\n    def run_step(self, x, init_zero_state=False):\n        if init_zero_state:\n            # type 0 -> is not tuple (use in non-tuple mode)\n            # init_value = [np.zeros((self.num_layers * 2 * self.hidden_size,))]\n\n            # type 1 -> wrong\n            # init_value = [\n            #     np.zeros(\n            #         (self.num_layers * self.hidden_size,)),\n            #     np.zeros(\n            #         (self.num_layers * self.hidden_size,))\n            #\n            # ]\n\n            # type 2\n            init_value = tuple(\n                [\n                    tf.nn.rnn_cell.LSTMStateTuple(\n                        np.zeros((1, self.hidden_size)),\n                        np.zeros((1, self.hidden_size))\n                    ) for _ in range(self.num_layers)\n                ]\n            )\n            # x_batch.shape[0] is 1\n\n        else:\n            init_value = self.lstm_last_state\n        out, next_lstm_state = self.session.run(\n            [self.final_outputs, self.lstm_new_state],\n            feed_dict={\n                self.x_batch: [x],  # -> x_batch.shape[0] is 1\n                self.lstm_init_value: init_value\n            }\n        )\n        self.lstm_last_state = next_lstm_state#[0]\n\n        return out[0][0]  # it shows the next character that is predicted\n\n\ndef load_model(check_point_dir):\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    sess = tf.InteractiveSession(config=config)\n\n    net = LSTM_NN(\n        check_point_dir=check_point_dir,\n        session=sess,\n        scope_name=""char_rnn_network""\n    )\n    check_point = check_point_dir + \'/model.ckpt\'\n    sess.run(tf.global_variables_initializer())\n    saver = tf.train.Saver(tf.global_variables())\n    if os.path.exists(check_point):\n        saver.restore(sess, check_point)\n    return net\n\n\ndef train(model, data, mini_batch_size=64, num_train_batches=20000, check_point_period=100):\n    print(""\'\'\'TRAINING started\'\'\'"")\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    sess = model.session\n    sess.run(tf.global_variables_initializer())\n    saver = tf.train.Saver(tf.global_variables())\n    check_point = model.check_point_dir + \'/model.ckpt\'\n\n    last_time = time.time()\n\n    np_start_token = [0.0] * SIZE_OF_VOCAB\n    np_start_token[START] = 1.0\n    np_end_token = [0.0] * SIZE_OF_VOCAB\n    np_end_token[END] = 1.0\n\n    batch_size = len(data)\n    whole_train_iter = 0\n    mini_batch_iter = 0\n    train_iter = 0\n\n    while whole_train_iter < num_train_batches:\n        x_batch = np.zeros((mini_batch_size, CHAR_NUM_OF_SENTENCE, SIZE_OF_VOCAB))\n        y_batch = np.zeros((mini_batch_size, CHAR_NUM_OF_SENTENCE, SIZE_OF_VOCAB))\n\n        for mini_batch_id in range(0, mini_batch_size):\n            batch_id = (mini_batch_id + mini_batch_iter) % batch_size\n            x_batch[mini_batch_id] = np.append([np_start_token], data[batch_id], axis=0)\n            y_batch[mini_batch_id] = np.append(data[batch_id], [np_end_token], axis=0)\n\n        batch_cost = model.train_on_batch(x_batch, y_batch)\n        train_iter += 1\n\n        print(""     ---whole train iteration: {}    mini batch iteration: {}"".format(whole_train_iter, mini_batch_iter))\n\n        if train_iter % check_point_period == 0:\n            new_time = time.time()\n            diff = new_time - last_time\n            last_time = new_time\n            print(""train iteration: {}  loss: {}  speed: {} batches / s"".format(\n                whole_train_iter, batch_cost, check_point_period / diff\n            ))\n            saver.save(sess, check_point)\n            print(\'saved in this path: \', check_point)\n\n        mini_batch_iter += mini_batch_size\n        if mini_batch_iter >= batch_size:\n            mini_batch_iter -= batch_size\n            whole_train_iter += 1\n\n\ndef predict(prefix, model, generate_len=100):\n    prefix = prefix.lower()\n    if len(prefix) == 0:\n        first_char = map_id_to_char(np.random.choice(range(SIZE_OF_VOCAB)))\n    else:\n        first_char = prefix[0]\n    out = model.run_step([get_char_vector(first_char)], True)\n    for i in range(1, len(prefix)):\n        out = model.run_step([get_char_vector(prefix[i])])\n\n    print(""Sentence:"")\n    gen_str = prefix\n    for i in range(generate_len):\n        element_id = np.random.choice(range(SIZE_OF_VOCAB), p=out)\n        element = map_id_to_char(element_id)\n        gen_str += element\n        out = model.run_step([get_one_hot_vector(element_id)])\n\n    print(gen_str)\n\n\nif __name__ == \'__main__\':\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    sess = tf.InteractiveSession(config=config)\n    saver_directory = os.path.abspath(os.path.join(os.getcwd(), \'../.saved\'))\n\n    CHAR_NUM_OF_SENTENCE = 100\n\n    data = load_data(\'../datasets/shakespeare -all.txt\', CHAR_NUM_OF_SENTENCE)\n\n    test = True\n    if not test:\n        model = LSTM_NN(sess, saver_directory)\n        train(model, data)\n    else:\n        model = load_model(saver_directory)\n        predict(\'I am \', model, 500)\n'"
LSTM_tensorflow/tools.py,2,"b'import numpy as np\n\n# <start> = 0, <end> = 1, <unk> = 2\n# main characters: 3-127 = 3-127\n# --persian characters: (char code) 1560-1751 <-> (id) 128-319\nSTART = 0\nEND = 1\nUNK = 2\nSIZE_OF_VOCAB = 128\n\n\ndef map_char_to_id(c):\n    """"""\n\n    :return: a number between 0 to 127\n    """"""\n    code = ord(c)\n    if 2 < code < 128:\n        return code\n    # if 1560 <= code <= 1751:\n    #     return code - 1432\n    return UNK\n\n\ndef map_id_to_char(code):\n    """"""\n\n    :param code: a number between 0 to SIZE_OF_VOCAB-1\n    """"""\n    if code == START: return \'<START>\'\n    if code == END: return \'<END>\'\n    if code == UNK: return \'<UNK>\'\n    if code < 128: return chr(code)\n    # if 128 <= code <= 319: return chr(code + 1432)\n    return \'<?>\'\n\n\ndef convert_to_one_hot(sentence, char_num):\n    """"""\n\n    :param char_num: char_num_of_sentence-1\n    :return: it\'s the one-hot array of the sentence without start and end token\n    """"""\n    s_vector = np.zeros((char_num, SIZE_OF_VOCAB))\n    for char_iter, char_c in enumerate(sentence):\n        v = get_char_vector(char_c)\n        s_vector[char_iter] = v\n\n    return s_vector\n\n\ndef get_char_vector(char):\n    v = [0.0] * SIZE_OF_VOCAB\n    v[map_char_to_id(char)] = 1.0\n    return v\n\n\ndef get_one_hot_vector(one_id, size=SIZE_OF_VOCAB):\n    v = [0.0] * size\n    v[one_id] = 1.0\n    return v\n\n\ndef load_data(input, char_num_of_sentence):\n    """"""\n\n    :param input: data file address\n    :param char_num_of_sentence: fix char size of any sentences\n    :return: a numpy array with the size (len(lines), char_num_of_sentence-1, SIZE_OF_VOCAB)\n    """"""\n    # Load the data\n    with open(input, \'r\') as f:\n        lines = f.readlines()\n    data = np.zeros((len(lines), char_num_of_sentence-1, SIZE_OF_VOCAB))\n    for line_id, line in enumerate(lines):\n        line = line.lower()[0:char_num_of_sentence-1]\n        line += \' \' * (char_num_of_sentence-1 - len(line))\n\n        # Convert to 1-hot coding\n        data[line_id] = convert_to_one_hot(line, char_num_of_sentence-1)\n    return data\n'"
RNN_numpy/RNN.py,40,"b'import numpy as np\n# from tools import read_data\nfrom RNN_numpy.tools import read_data\n\nHIDDEN_LAYER_SIZE = 100\nCHUNK_SIZE = 25  # number of steps on input to unroll the RNN\nLEARNING_RATE = 1e-1\nTHRESHOLD = 5\nLOG_ITER = 1000\n\nw_xh, w_hh, w_hy, b_h, b_y = [None]*5\n\n\ndef initialize_rnn():\n    """"""\n    initialize the weights and the biases of the network\n    """"""\n    global w_xh, w_hh, w_hy, b_h, b_y\n    w_xh = np.random.randn(HIDDEN_LAYER_SIZE, vocab_size) * 0.01\n    w_hh = np.random.randn(HIDDEN_LAYER_SIZE, HIDDEN_LAYER_SIZE) * 0.01\n    w_hy = np.random.randn(vocab_size, HIDDEN_LAYER_SIZE) * 0.01\n    b_h = np.zeros((HIDDEN_LAYER_SIZE, 1))\n    b_y = np.zeros((vocab_size, 1))\n\n\ndef predict(h_state, prefix_ids, predict_n):\n    """"""\n\n    :param h_state: initial value for hidden layer\n    :param prefix_ids: an array containing inputs\n    :param predict_n: predict outputs on predict_n iterations\n    :return: predicted output string\n    """"""\n\n    # predicted_on_prefix = []\n    h = h_state\n    x = np.zeros((vocab_size, 1))\n\n    for idx in prefix_ids:\n        x = np.zeros((vocab_size, 1))\n        x[idx] = 1\n        h = np.tanh(np.dot(w_xh, x) + np.dot(w_hh, h) + b_h)\n        # y = np.dot(w_hy, h) + b_y\n        # p = np.exp(y) / np.sum(np.exp(y))\n        # o_idx = np.random.choice(range(vocab_size), p=p.ravel())\n        # predicted_on_prefix.append(o_idx)\n\n    # predicted_on_prefix_str = [char_at[o_idx] for o_idx in predicted_on_prefix]\n    # print(\'predicted_on_prefix_str: \', predicted_on_prefix_str)\n\n    predicted = []\n    for t in range(predict_n):\n        h = np.tanh(np.dot(w_xh, x) + np.dot(w_hh, h) + b_h)\n        y = np.dot(w_hy, h) + b_y\n        p = np.exp(y) / np.sum(np.exp(y))\n        o_idx = np.random.choice(range(vocab_size), p=p.ravel())\n        x = np.zeros((vocab_size, 1))\n        x[o_idx] = 1\n        predicted.append(o_idx)\n\n    predicted_str = [char_at[o_idx] for o_idx in predicted]\n    # print(\'predicted_str: \', predicted_str)\n\n    return predicted_str\n\n\ndef gradients_and_loss(inputs, targets, h_state):\n    """"""\n\n    :param inputs: input list for the RNN\n    :param targets: target list for the RNN\n    :param h_state: initial value for the hidden state on this chunk\n    :return: the loss, gradients and last hidden state\n    """"""\n    x, y, p = [[None] * len(inputs)] * 3\n    # p : probabilities for next chars using softmax\n    h = {-1: np.copy(h_state)}\n    loss = 0\n\n    # we didn\'t make any changes on the neural network parameters,\n    # we just pass through the hidden layers to make the outputs according to the inputs\n\n    # forward pass\n    for t in range(len(inputs)):\n        x[t] = np.zeros((vocab_size, 1))\n        x[t][inputs[t]] = 1\n        h[t] = np.tanh(np.dot(w_xh, x[t]) + np.dot(w_hh, h[t-1]) + b_h)\n        y[t] = np.dot(w_hy, h[t]) + b_y\n        p[t] = np.exp(y[t]) / np.sum(np.exp(y[t]))\n        loss += -np.log(p[t][targets[t], 0])  # cross-entropy loss\n\n    # backward pass (compute gradients)\n    d_w_xh, d_w_hh, d_w_hy, d_b_h, d_b_y = [np.zeros_like(param) for param in [w_xh, w_hh, w_hy, b_h, b_y]]\n    d_h_next = np.zeros_like(h[0])\n    for t in reversed(range(len(inputs))):\n        d_y = np.copy(p[t])\n        d_y[targets[t]] -= 1\n        # back-propagation into y. see http://cs231n.github.io/neural-networks-case-study/#grad for more details\n        # Todo: understand these mathematics!\n        d_w_hy += np.dot(d_y, h[t].T)\n        d_b_y += d_y\n        d_h = np.dot(w_hy.T, d_y) + d_h_next  # back-propagation into h\n        d_h_raw = (1 - h[t] ** 2) * d_h  # back-propagation through tanh non-linearly\n        d_b_h += d_h_raw\n        d_w_xh += np.dot(d_h_raw, x[t].T)\n        d_w_hh += np.dot(d_h_raw, h[t-1].T)\n        d_h_next = np.dot(w_hh.T, d_h_raw)\n\n    for d_param in [d_w_xh, d_w_hh, d_w_hy, d_b_h, d_b_y]:\n        np.clip(d_param, -THRESHOLD, THRESHOLD, out=d_param)  # to mitigate exploding gradients\n\n    return loss, d_w_xh, d_w_hh, d_w_hy, d_b_h, d_b_y, h[len(inputs)-1]\n\n\ndef execute_rnn():\n    iteration, whole_input_iteration, pointer = 0, 0, 0\n\n    sg_w_xh = np.zeros_like(w_xh)\n    sg_w_hh = np.zeros_like(w_hh)\n    sg_w_hy = np.zeros_like(w_hy)\n    sg_b_h  = np.zeros_like(b_h)\n    sg_b_y  = np.zeros_like(b_y)\n    # sum of squares of gradients, used to decreasing the step lengths by AdaGrad algorithm\n\n    smooth_loss = np.log(vocab_size) * CHUNK_SIZE  # initial value\n\n    h_state = np.zeros((HIDDEN_LAYER_SIZE, 1))\n\n    while True:\n        if pointer + 1 >= data_size:\n            h_state = np.zeros((HIDDEN_LAYER_SIZE, 1))\n            pointer = 0\n            whole_input_iteration += 1\n        elif pointer + CHUNK_SIZE >= data_size:\n            pointer = data_size - CHUNK_SIZE - 1\n\n        inputs  = [id_of[c] for c in data[pointer: pointer + CHUNK_SIZE]]          # inputs  : p ... p + chunk size - 1\n        targets = [id_of[c] for c in data[pointer + 1: pointer + CHUNK_SIZE + 1]]  # targets : p + 1 ... p + chunk size\n\n        if iteration % LOG_ITER == 0:\n            print(prefix, \'\'.join(predict(h_state, prefix_ids, predict_char_nums)))\n\n        loss, d_w_xh, d_w_hh, d_w_hy, d_b_h, d_b_y, h_state = gradients_and_loss(inputs, targets, h_state)\n        smooth_loss = smooth_loss * .999 + loss * .001\n\n        if iteration % LOG_ITER == 0:\n            print(\'---------------------------------------------\\n\',\n                  \'whole iter %d, iter %d, loss: %f\' % (whole_input_iteration, iteration, smooth_loss),\n                  \'\\n---------------------------------------------\\n\\n\')\n\n        for param, d_param, sg_param in zip([w_xh, w_hh, w_hy, b_h, b_y],\n                                            [d_w_xh, d_w_hh, d_w_hy, d_b_h, d_b_y],\n                                            [sg_w_xh, sg_w_hh, sg_w_hy, sg_b_h, sg_b_y]):\n            sg_param += d_param ** 2\n            param += -LEARNING_RATE * d_param / np.sqrt(sg_param + 1e-8)\n            # AdaGrad algorithm\n\n        pointer += CHUNK_SIZE\n        iteration += 1\n\n\nif __name__ == \'__main__\':\n    data, char_vocab = read_data(\'datasets/sample.txt\')\n    data_size, vocab_size = len(data), len(char_vocab)\n    id_of = {c: i for i, c in enumerate(char_vocab)}\n    char_at = {i: c for i, c in enumerate(char_vocab)}\n    # TODO: add unknown to vocab\n\n    prefix = \'In Warwickshire\'\n    prefix_ids = [id_of[c] for c in prefix]\n    predict_char_nums = 100\n\n    initialize_rnn()\n    execute_rnn()\n'"
RNN_numpy/tools.py,1,"b'import numpy as np\n\n\ndef read_data(path=\'datasets/data.txt\'):\n    data = open(path, \'r\').read()\n    chars = list(set(data))\n    return data, chars\n\n\ndef convert_to_one_hot(data_, vocab):\n    data = np.zeros((len(data_), len(vocab)))\n    cnt = 0\n    for s in data_:\n        v = [0.0] * len(vocab)\n        v[vocab.index(s)] = 1.0\n        data[cnt, :] = v\n        cnt += 1\n    return data\n\n\ndef load_data(input):\n    # Load the data\n    data_ = """"\n    with open(input, \'r\') as f:\n        data_ += f.read()\n    data_ = data_.lower()\n    # Convert to 1-hot coding\n    vocab = sorted(list(set(data_)))\n    data = convert_to_one_hot(data_, vocab)\n    return data, vocab\n'"
