file_path,api_count,code
dataloader.py,1,"b'import sys\nimport os\nimport csv\nimport numpy as np\n\ndef load_data(dpath,mode=\'train\'):\n\t\'\'\'\n\tLoad the CSV data\n\tIf mode == \'train\' , Loads from mnist_train.csv\n\tIf mode == \'test\', Loads from mnist_test.csv\n\t\'\'\'\n\tif mode == \'train\':\n\t\tfile_name = os.path.join(dpath,\'mnist_train.csv\')\n\telse:\n\t\tfile_name = os.path.join(dpath,\'mnist_test.csv\')\n\tdata=[]\n\t\n\ttry:\n\t\tfp = open(file_name,\'rb\')\n\t\treader = csv.reader(fp)\n\texcept:\n\t\traise Exception(""Invalid Data Path"")\n\n\tprint ""Loading Dataset===>""\n\t\n\tfor row in reader:\n\t\tdata.append(row)\n\tprint ""Done!""\n\tdata = np.asarray(data)\n\treturn data\n\nif __name__ == \'__main__\':\n\tdpath = sys.argv[1]\n\tmode = sys.argv[2]\n\tdata = load_data(dpath,mode)\n'"
nn.py,44,"b'import sys\nimport os\nimport numpy as np\nfrom dataloader import *\n\nclass NN(object):\n\n\tdef __init__(self,hidden_dims=(1024,2048),n_hidden=2,mode=\'train\',dpath=None,model_path=None):\n\t\t\n\t\t\n\t\tif dpath is not None:\n\t\t\ttry:\n\t\t\t\tself.mnist_data = load_data(dpath,mode=mode)\n\t\t\t\tself.n_samples,self.ip_dim = self.mnist_data[:,1:].shape\n\t\t\texcept:\n\t\t\t\traise Exception(\'Not able to parse MNIST Data\')\n\n\n\t\tself.ncls = 10\n\t\tself.mode = mode\n\t\tself.batch_size = 128\n\t\tself.counter = 0\n\t\tself.nepochs = 10\n\t\tself.maxiters = 1000000\n\t\tself.lr = 1e-3\n\t\tself.eps = 1e-8\n\t\tself.save_model_path = model_path\n\t\tself.n_hidden = n_hidden\n\n\t\tself.weights={}\n\t\tself.biases={}\n\t\tself.batch_norm_params={}\n\t\tself.bn_cache={}\n\t\tself.save_iter = 1\n\n\t\tif mode == \'train\':\n\t\t\tprint ""Initializing Weights......""\n\t\t\tself.init_weights(n_hidden,hidden_dims)\n\t\telse:\n\t\t\tprint ""Loading Trained Weights......""\n\t\t\tself.load_weight(model_path)\n\n\tdef load_weight(self,path):\n\t\t\'\'\'\n\t\tLoads the pretrained model\n\t\tpath = Path of the model\n\t\t\'\'\'\n\t\tif path is None:\n\t\t\traise Exception(\'Model Load Path Invalid\')\n\t\ttry:\t\n\t\t\tself.weights = np.load(os.path.join(path,\'weights.npy\')).item()\n\t\texcept:\n\t\t\traise Exception(\'Not able to load Network weights\')\n\n\t\ttry:\t\n\t\t\tself.biases = np.load(os.path.join(path,\'biases.npy\')).item()\n\t\texcept:\n\t\t\traise Exception(\'Not able to load Network biases\')\n\n\t\ttry:\t\n\t\t\tself.batch_norm_params = np.load(os.path.join(path,\'bn_params.npy\')).item()\n\t\texcept:\n\t\t\traise Exception(\'Not able to load Network weights\')\n\n\n\tdef init_weights(self,n_hidden,dims):\n\t\t\'\'\'\n\t\tInitializes the weights of the network\n\t\tn_hidden = Number of hidden layers\n\t\tdims = Number of Hidden Dimensions\n\t\t\'\'\'\n\t\tself.weights[0]=np.random.randn(self.ip_dim,dims[0]) * (1.0/np.sqrt(self.ip_dim))\n\t\tself.biases[0] = np.zeros((dims[0],))\n\n\t\tfor i in range(1,n_hidden):\n\t\t\tself.weights[i] = np.random.randn(dims[i-1],dims[i])*(1.0/np.sqrt(dims[i-1]))\n\t\t\tself.biases[i] = np.zeros((dims[i],))\n\n\t\tself.weights[-1] = np.random.randn(dims[n_hidden-1],self.ncls)*(1.0/np.sqrt(dims[n_hidden-1]))\n\t\tself.biases[-1] = np.zeros((self.ncls,))\n\n\t\tfor i in range(n_hidden):\n\t\t\tself.batch_norm_params[i] = {\'gamma\':np.ones((dims[i],)),\\\n\t\t\t\t\t\t\t\t\t\t\'beta\':np.zeros((dims[i],)), \\\n\t\t\t\t\t\t\t\t\t\t\'running_mean\':np.zeros((dims[i],)),\\\n\t\t\t\t\t\t\t\t\t\t\'running_var\':np.zeros((dims[i],)),}\n\t\t\tself.bn_cache[i]=[]\n\n\n\tdef get_data(self):\n\t\t\'\'\'\n\t\tIterate over the training data for batch generation\n\t\t\'\'\'\n\t\tif self.counter > self.n_samples:\n\t\t\tself.counter = 0\n\n\t\tif(self.counter == 0):\n\t\t\tnp.random.shuffle(self.mnist_data)\n\t\treturn self.mnist_data[self.counter:self.counter+self.batch_size,:]\n\n\tdef forward(self,ip,labels=None):\n\t\t\'\'\'\n\t\tForward Pass\n\t\tip = Input image tensor\n\t\tlabels = If None, it will not compute loss\n\t\t\'\'\'\n\t\th1 = ip.dot(self.weights[0]) + self.biases[0]\n\t\th1_bn = self.batchnorm_forward(h1,0,mode=self.mode)\n\t\th1_relu = self.relu(h1_bn.copy())\n\t\t\n\t\th2 = h1_relu.dot(self.weights[1]) + self.biases[1]\n\t\th2_bn = self.batchnorm_forward(h2,1,mode=self.mode)\n\t\th2_relu = self.relu(h2_bn.copy())\n\n\t\t\n\t\tlogits = h2_relu.dot(self.weights[-1]) + self.biases[-1]\n\t\tprobs = self.softmax(logits)\n\t\tif labels is not None:\n\t\t\tloss = self.loss(labels,probs)\n\t\telse:\n\t\t\tloss = None\n\t\treturn (h1,h1_bn,h1_relu,h2,h2_bn,h2_relu,logits,probs,loss)\n\n\tdef loss(self,y,pred):\n\t\t\'\'\'\n\t\tCross Entropy  Loss\n\t\t\'\'\'\n\t\treturn -np.sum(y*np.log(pred))/(y.shape[0]+self.eps)\n\n\tdef softmax(self,ip):\n\t\t\'\'\'\n\t\tComputes Softmax of the Logits\n\t\t\'\'\'\n\t\tcentered_ip = ip - np.max(ip,axis=1).reshape(-1,1)\n\t\tprobs = np.exp(centered_ip)\n\t\treturn probs/np.sum(probs,axis=1).reshape(-1,1).astype(np.float32)\n\n\tdef relu(self,ip):\n\t\t\'\'\'\n\t\tRelU Activation\n\t\t\'\'\'\n\t\tx = ip\n\t\tx[np.where(x < 0)] = 0\n\t\treturn x\n\n\tdef batchnorm_forward(self,ip,index,mode=\'train\'):\n\t\t\'\'\'\n\t\tPerform batch normalization over ip (Nx D)\n\t\t\'\'\'\n\t\tgamma = self.batch_norm_params[index][\'gamma\']\n\t\tbeta = self.batch_norm_params[index][\'beta\']\n\t\trunning_mean = self.batch_norm_params[index][\'running_mean\']\n\t\trunning_var = self.batch_norm_params[index][\'running_var\']\n\t\tN,D = ip.shape\n\t\tif mode == \'train\':\n\n\t\t\tx = ip\n\t\t\tmu = np.mean(x,axis=0) # D,\n\t\t\txmu = x - mu\n\t\t\t\n\t\t\tdelta = (x-mu)**2\n\t\t\tvar = (np.mean(delta,axis=0))\n\t\t\tsqrtvar = np.sqrt(var+self.eps) \n\t\t\tinvvar = 1/sqrtvar\n\n\t\t\tx_hat = xmu*invvar\n\t\t\tx_hat_gamma = x_hat*gamma\n\t\t\tout = x_hat_gamma + beta\n\n\t\t\trunning_mean = 0.9*running_mean + 0.1*mu\n\t\t\trunning_var = 0.9*running_var + 0.1*var\n\t\t\tself.batch_norm_params[index][\'running_mean\'] = running_mean\n\t\t\tself.batch_norm_params[index][\'running_var\'] = running_var\n\t\t\tself.bn_cache[index] = [x,mu,xmu,delta,var,sqrtvar,invvar,x_hat,x_hat_gamma]\n\t\telse:\n\t\t\tx = ip\n\t\t\txmu = x - running_mean\n\t\t\tx_hat = xmu/np.sqrt(running_var+self.eps)\n\t\t\tout = x_hat*gamma + beta\n\n\t\treturn out\n\n\tdef batchnorm_backward(self,din,index):\n\t\tx,mu,xmu,delta,var,sqrtvar,invvar,x_hat,x_hat_gamma = self.bn_cache[index]\n\t\tgamma = self.batch_norm_params[index][\'gamma\']\n\t\tbeta = self.batch_norm_params[index][\'beta\']\n\n\t\tN,D = din.shape\n\n\t\tdbeta = np.sum(din,axis=0)\n\t\tdgamma = np.sum(din*x_hat,axis=0)\n\n\t\tdx_1 = gamma*invvar*din # del\n\t\tdx_2 = -gamma*invvar*np.sum(din,axis=0)/N\n\t\tdx_3 = -gamma*(xmu)*(var+self.eps)**(-1.5)*(np.sum(din*(xmu),axis=0))/float(N)\n\n\t\tdx = dx_1+dx_2+dx_3\n\t\treturn dx,dbeta,dgamma\n\n\n\tdef backward(self,cache,ip,labels):\n\t\t\'\'\'\n\t\tBackward Pass\n\t\t\'\'\'\n\t\th1,h1_bn,h1_relu, h2,h2_bn,h2_relu, logits, probs,__ = cache\n\t\tdy = (labels - probs)\n\t\tdW2 = h2_relu.T.dot(dy)\n\t\tdb2 = np.sum(dy,axis=0)\n\n\t\tdh2 = dy.dot(self.weights[-1].T)\n\t\tdh2[np.where(h2 == 0)] = 0\n\t\tdh2, dbeta2,dgamma2 = self.batchnorm_backward(dh2,1)\n\n\t\tdW1 = h1_relu.T.dot(dh2)\n\t\tdb1 = np.sum(dh2,axis=0)\n\n\t\tdh1 = dh2.dot(self.weights[1].T)\n\t\tdh1[np.where(h1 == 0)] = 0\n\t\tdh1, dbeta1,dgamma1 = self.batchnorm_backward(dh1,0)\n\n\t\tdW0 = ip.T.dot(dh1)\n\t\tdb0 = np.sum(dh1,axis=0)\n\n\t\treturn (dW0,db0,dW1,db1,dW2,db2,dbeta2,dgamma2,dbeta1,dgamma1)\n\n\tdef update(self,grads):\n\t\t\'\'\'\n\t\tWeight Update\n\t\t\'\'\'\n\t\t\n\t\tdW0,db0,dW1,db1,dW2,db2,dbeta2,dgamma2,dbeta1,dgamma1 = grads\n\n\t\tself.weights[0] += self.lr*(dW0)\n\t\tself.biases[0] += self.lr*(db0)\n\t\t\n\t\tself.weights[1] += self.lr*(dW1)\n\t\tself.biases[1] += self.lr*(db1)\n\n\t\tself.weights[-1] += self.lr*(dW2)\n\t\tself.biases[-1] += self.lr*(db2)\n\n\t\tself.batch_norm_params[0][\'gamma\'] += self.lr*dgamma1\n\t\tself.batch_norm_params[1][\'gamma\'] += self.lr*dgamma2\n\n\t\tself.batch_norm_params[0][\'beta\'] += self.lr*dbeta1\n\t\tself.batch_norm_params[1][\'beta\'] += self.lr*dbeta2\n\n\n\tdef accuracy(self,preds,labels):\n\t\t\'\'\'\n\t\tCompute Accuracy\n\t\t\'\'\'\n\t\tp_idx = np.argmax(preds,axis=1)\n\t\tl_idx = np.argmax(labels,axis=1)\n\n\t\tpos = len(np.where(p_idx-l_idx == 0)[0])\n\t\treturn pos/(float(preds.shape[0])+self.eps)\n\n\n\tdef train(self):\n\t\t\'\'\'\n\t\tStart Training the network\n\t\t\'\'\'\n\t\tit_per_epoch = self.n_samples/self.batch_size + 1\n\t\tit_num = 0\n\t\tprint ""Starting Training..............""\n\t\t\n\n\t\tfor i in range(self.nepochs):\n\t\t\tfor j in range(it_per_epoch):\n\n\t\t\t\tbatch_data = self.get_data()\n\t\t\t\tself.counter += self.batch_size\n\n\t\t\t\tbatch_imgs = batch_data[:,1:].astype(np.float32)/255.0 - 0.5\n\t\t\t\tbatch_labels = np.zeros((batch_imgs.shape[0],self.ncls))\n\t\t\t\tbatch_labels[np.arange(batch_labels.shape[0]),batch_data[:,0].astype(np.uint8).tolist()] = 1\n\n\t\t\t\tcache = self.forward(batch_imgs,batch_labels)\n\t\t\t\t\n\t\t\t\tgrads = self.backward(cache,batch_imgs,batch_labels)\n\n\t\t\t\tself.update(grads)\n\t\t\t\tif not it_num%self.save_iter:\n\t\t\t\t\tnew_acc = self.accuracy(cache[-2],batch_labels)\n\t\t\t\t\tnew_loss = cache[-1]\n\t\t\t\t\tprint ""Training Iteration===>%d, Loss ====>%.4f, Acc ====>%.4f""%(it_num,new_loss,new_acc)\n\t\t\t\tit_num +=1\n\t\t\t\t\n\t\t\t\tif it_num > self.maxiters:\n\t\t\t\t\tbreak\n\t\t\tif it_num > self.maxiters:\n\t\t\t\tbreak\n\n\t\tprint ""Done Training!""\n\t\tself.save_model()\n\t\tprint ""Model Saved!""\n\t\t\n\n\n\tdef test(self):\n\t\tit_per_epoch = self.n_samples/self.batch_size + 1\n\t\tit_num = 0\n\t\tfor j in range(it_per_epoch):\n\n\t\t\tbatch_data = self.get_data()\n\t\t\tself.counter += self.batch_size\n\n\t\t\tbatch_imgs = batch_data[:,1:].astype(np.float32)/255.0 - 0.5\n\t\t\tbatch_labels = np.zeros((batch_imgs.shape[0],self.ncls))\n\t\t\tbatch_labels[np.arange(batch_labels.shape[0]),batch_data[:,0].astype(np.uint8).tolist()] = 1\n\n\t\t\tcache = self.forward(batch_imgs,batch_labels)\n\t\t\tprint ""Testing Iteration===>%d, Acc ====>%.4f""%(it_num,self.accuracy(cache[-2],batch_labels))\n\t\t\tit_num +=1\n\t\t\t\n\n\tdef save_model(self):\n\t\t\'\'\'\n\t\tSave the trained mode at given model path\n\t\t\'\'\'\n\t\tif not os.path.exists(self.save_model_path):\n\t\t\tos.makedirs(self.save_model_path)\n\t\telse:\n\t\t\tmap(lambda x: os.unlink(os.path.join(self.save_model_path,x)), os.listdir(self.save_model_path))\n\n\t\tnp.save(os.path.join(self.save_model_path,\'bn_params.npy\'),self.batch_norm_params)\n\t\tnp.save(os.path.join(self.save_model_path,\'weights.npy\'),self.weights)\n\t\tnp.save(os.path.join(self.save_model_path,\'biases.npy\'),self.biases)\n\n\n\n\nif __name__ == \'__main__\':\n\t\n\tmode = sys.argv[1]\n\tdpath = sys.argv[2]\n\tmodel_path = sys.argv[3]\n\t\n\tobj = NN(mode=mode,dpath=dpath,model_path=model_path)\n\t\n\tif mode == \'train\':\n\t\tobj.train()\n\telse:\n\t\tobj.test()\n\t\n\n\n\n\n\n\n\n\n\n\n'"
run_on_image.py,2,"b'import sys\nimport os\nfrom nn import *\nimport cv2\n\ndef run_on_image(img_path,model_path):\n\n\tnet = NN(mode=\'test\',model_path=model_path)\n\n\timg = cv2.imread(img_path)\n\n\timg = img[:,:,0].astype(np.float32)/255.0 - 0.5\n\timg = img.reshape(1,784)\n\n\tcache = net.forward(img)\n\tprobs = cache[-2]\n\n\tlabel = np.argmax(probs,axis=1)\n\n\tprint ""Predicted Label {}"".format(label[0])\n\n\nif __name__ == \'__main__\':\n\n\timg_path = sys.argv[1]\n\tmodel_path = sys.argv[2]\n\trun_on_image(img_path,model_path)\n'"
