file_path,api_count,code
lstm_class_object.py,53,"b'import numpy as np\r\n\r\nclass LSTMPopulation(object):\r\n    def __init__(self, input_size, hidden_size):\r\n        self.input_size = input_size\r\n        self.hidden_size = hidden_size\r\n        self.WLSTM = np.random.randn(1 + input_size + hidden_size, 4 * hidden_size) / np.sqrt(input_size + hidden_size)\r\n        self.WLSTM[0,:] = 0\r\n        self.WpeepIFO = np.ones((3, hidden_size))\r\n        self.c0 = np.zeros((self.hidden_size))\r\n        self.h0 = np.zeros((self.hidden_size))\r\n        self.dcn = np.zeros((self.hidden_size))\r\n        self.dhn = np.zeros((self.hidden_size))\r\n        self.mWLSTM = np.zeros_like(self.WLSTM)\r\n        self.mWpeepIFO = np.zeros_like(self.WpeepIFO)\r\n        \r\n    def reset_states(self):\r\n        self.c0 = np.zeros((self.hidden_size))\r\n        self.h0 = np.zeros((self.hidden_size))\r\n        self.dcn = np.zeros((self.hidden_size))\r\n        self.dhn = np.zeros((self.hidden_size))\r\n        print (""Network states RESET"")\r\n        \r\n    def forward(self, X):\r\n        """"""\r\n        X is the input (n,input_size). n = length of sequence, and input_size = input size (input dimension)\r\n        """"""\r\n        n = X.shape[0]\r\n\r\n        # LSTM forward pass begins here\r\n        xphpbias = self.WLSTM.shape[0] # x (input) plus, hidden plus, bias\r\n        self.Hin = np.zeros((n, xphpbias))\r\n        self.Hout = np.zeros((n, self.hidden_size))\r\n    \r\n        self.IFOA = np.zeros((n, self.hidden_size * 4)) #before non-linearlity\r\n        self.IFOA_f = np.zeros((n, self.hidden_size * 4)) # after the non-linearity\r\n        self.C = np.zeros((n, self.hidden_size)) # Cell values/ cell contents\r\n\r\n        for t in xrange(n):\r\n            prev_h = self.Hout[t-1,:] if (t > 0) else self.h0\r\n            prev_c = self.C[t-1,:] if (t>0) else self.c0\r\n            self.Hin[t,0] = 1 # this is for the bias\r\n            self.Hin[t,1:1+self.input_size] = X[t, :]\r\n            self.Hin[t,1+self.input_size:] = prev_h\r\n            # Computing all gate activations \r\n\r\n            self.IFOA[t,:] = self.Hin[t,:].dot(self.WLSTM)\r\n            # Adding peephole weights connections\r\n            self.IFOA[t,:self.hidden_size] = self.IFOA[t,:self.hidden_size] + np.multiply(prev_c, self.WpeepIFO[0,:])       # input gate - adding peephole connections\r\n            self.IFOA[t,self.hidden_size:2*self.hidden_size] = self.IFOA[t,self.hidden_size:2*self.hidden_size] + np.multiply(prev_c, self.WpeepIFO[1,:])       # forget gate - adding peephole connections\r\n            \r\n            # Passing through the non-linearities - sigmoid for gates input and forget - output is below due to peephole connections \r\n            self.IFOA_f[t,0:2*self.hidden_size] = 1.0 / (1.0 + np.exp(-self.IFOA[t,0:2*self.hidden_size]))\r\n            self.IFOA_f[t,3*self.hidden_size:] = np.tanh(self.IFOA[t,3*self.hidden_size:]) # tanh non-linearity for the A gate (before the multiplicated input to the cell)\r\n            \r\n            # Computing the cell activation            \r\n            self.C[t,:] = self.IFOA_f[t,self.hidden_size:2*self.hidden_size]*prev_c + self.IFOA_f[t,:self.hidden_size]*self.IFOA_f[t,3*self.hidden_size:]\r\n\r\n            # Computing the output gate with peephole connections\r\n            self.IFOA[t,2*self.hidden_size:3*self.hidden_size] = self.IFOA[t,2*self.hidden_size:3*self.hidden_size] + np.multiply(self.C[t,:], self.WpeepIFO[2,:]) # output gate - adding peephole connections            \r\n            \r\n            self.IFOA_f[t,2*self.hidden_size:3*self.hidden_size] = 1.0 / (1.0 + np.exp(-self.IFOA[t,2*self.hidden_size:3*self.hidden_size]))\r\n            self.Hout[t,:] = self.IFOA_f[t,2*self.hidden_size:3*self.hidden_size]*np.tanh(self.C[t,:])\r\n        \r\n        self.c0 = self.C[t,:]\r\n        self.h0 = self.Hout[t,:]\r\n        \r\n        \r\n    def backward(self, dHout_temp):               \r\n        # backprop through the LSTM now\r\n        self.dIFOA = np.zeros_like(self.IFOA)\r\n        self.dIFOA_f = np.zeros_like(self.IFOA_f)\r\n        self.dWLSTM = np.zeros_like(self.WLSTM)\r\n        self.dWpeepIFO = np.zeros_like(self.WpeepIFO)\r\n        self.dC = np.zeros_like(self.C)\r\n        self.dHout = dHout_temp.copy()\r\n        self.dHin = np.zeros_like(self.Hin)\r\n        self.dh0 = np.zeros((self.hidden_size))\r\n        \r\n        n = self.Hin.shape[0]\r\n        \r\n        if self.dcn is not None: self.dC[n-1] += self.dcn.copy()\r\n        if self.dhn is not None: self.dHout[n-1] += self.dhn.copy()\r\n        \r\n#        print(dHout.shape, C.shape)\r\n        for t in reversed(xrange(n)):\r\n            self.dIFOA_f[t,2*self.hidden_size:3*self.hidden_size] = self.dHout[t,:]*np.tanh(self.C[t,:]) # backprop in to output gate\r\n            # backprop through the tanh non-linearity to get in to the cell, then will continue through it\r\n            self.dC[t,:] += (self.dHout[t,:] * self.IFOA_f[t,2*self.hidden_size:3*self.hidden_size]) * (1 - np.tanh(self.C[t,:]**2))\r\n                     \r\n            if (t>0):\r\n                self.dIFOA_f[t,self.hidden_size:2*self.hidden_size] = self.dC[t,:]*self.C[t-1,:] # backprop in to the forget gate\r\n                self.dC[t-1,:] += self.IFOA_f[t,self.hidden_size:2*self.hidden_size] * self.dC[t,:] # backprop through time for C (The recurrent connection to C from itself)\r\n            else:\r\n                self.dIFOA_f[t,self.hidden_size:2*self.hidden_size] = self.dC[t,:]*self.c0 # backprop in to forget gate\r\n                self.dc0 = self.IFOA_f[t,self.hidden_size:2*self.hidden_size] * self.dC[t,:]\r\n            \r\n            self.dIFOA_f[t,:self.hidden_size] = self.dC[t,:]*self.IFOA_f[t,3*self.hidden_size:] #backprop in to the input gate\r\n            self.dIFOA_f[t,3*self.hidden_size:] = self.dC[t,:]*self.IFOA_f[t,:self.hidden_size] #backprop in to the a gate                    \r\n\r\n            # backprop through the activation functions\r\n            # for input, forget and output gates - derivative of the sigmoid function\r\n            # for a - derivative of the tanh function                \r\n            \r\n            self.dIFOA[t,3*self.hidden_size:] =  self.dIFOA_f[t,3*self.hidden_size:] * (1 - self.IFOA_f[t,3*self.hidden_size:]**2)              \r\n            y = self.IFOA_f[t,:3*self.hidden_size]\r\n            self.dIFOA[t,:3*self.hidden_size] = (y*(1-y)) * self.dIFOA_f[t,:3*self.hidden_size] \r\n        \r\n            # backprop the input matrix multiplication            \r\n            self.dWLSTM += np.dot(self.Hin[t:t+1,:].T, self.dIFOA[t:t+1,:])\r\n            self.dHin[t,:] = self.dIFOA[t,:].dot(self.WLSTM.T) \r\n            \r\n            # backprop the peephole connections\r\n            if t>0:\r\n                self.dWpeepIFO[0,:] += np.multiply(self.dIFOA[t,:self.hidden_size], self.C[t-1,:])\r\n                self.dWpeepIFO[1,:] += np.multiply(self.dIFOA[t,self.hidden_size:2*self.hidden_size], self.C[t-1,:])  \r\n                self.dWpeepIFO[2,:] += np.multiply(self.dIFOA[t,2*self.hidden_size:3*self.hidden_size], self.C[t,:]) \r\n            else:\r\n                self.dWpeepIFO[0,:] += np.multiply(self.dIFOA[t,:self.hidden_size], self.c0)\r\n                self.dWpeepIFO[1,:] += np.multiply(self.dIFOA[t,self.hidden_size:2*self.hidden_size], self.c0)  \r\n                self.dWpeepIFO[2,:] += np.multiply(self.dIFOA[t,2*self.hidden_size:3*self.hidden_size], self.C[t,:])\r\n                    \r\n            if (t>0):\r\n                self.dHout[t-1,:] += self.dHin[t,1+self.input_size:]\r\n            else:\r\n                self.dh0 += self.dHin[t,1+self.input_size:] \r\n                \r\n    def get_hidden_output(self):      \r\n        return self.Hout\r\n\r\n\r\n    def train_network(self, learning_rate):\r\n        for param, dparam, mem in zip([self.WLSTM, self.WpeepIFO],\r\n                                  [self.dWLSTM, self.dWpeepIFO ],\r\n                                  [self.mWLSTM, self.mWpeepIFO]):\r\n            mem += dparam * dparam\r\n            param += -learning_rate * dparam / np.sqrt(mem + 1e-8)\r\n    \r\n    def sample_network(self, X, W_out, next_data_points):\r\n        self.reset_states()\r\n        n = X.shape[0]\r\n    \r\n        p = n + next_data_points\r\n          \r\n        # LSTM forward pass for the duration of X, and then prediction for another n duration\r\n        xphpbias = self.WLSTM.shape[0] # x (input) plus, hidden plus, bias\r\n        self.Hin = np.zeros((p, xphpbias))\r\n        self.Hout = np.zeros((p, self.hidden_size))\r\n        \r\n        self.IFOA = np.zeros((p, self.hidden_size * 4)) #before non-linearlity\r\n        self.IFOA_f = np.zeros((p, self.hidden_size * 4)) # after the non-linearity\r\n        self.C = np.zeros((p, self.hidden_size)) # Cell values/ cell contents\r\n                  \r\n        for t in xrange(p):\r\n            prev_h = self.Hout[t-1,:] if (t > 0) else self.h0\r\n            prev_c = self.C[t-1,:] if (t>0) else self.c0\r\n            \r\n            self.Hin[t,0] = 1 # this is for the bias\r\n            \r\n            self.Hin[t,1+self.input_size:] = prev_h\r\n            if (t<n):\r\n                self.Hin[t,1:1+self.input_size] = X[t, :]\r\n            else:\r\n                self.Hin[t,1:1+self.input_size] = (self.Hout[t-1,:].dot(W_out.T))[0]\r\n            # Computing all gate activations \r\n            self.IFOA[t,:] = self.Hin[t,:].dot(self.WLSTM)\r\n            \r\n            # Adding peephole weights connections\r\n            self.IFOA[t,:self.hidden_size] = self.IFOA[t,:self.hidden_size] + np.multiply(prev_c, self.WpeepIFO[0,:])       # input gate - adding peephole connections\r\n            self.IFOA[t,self.hidden_size:2*self.hidden_size] = self.IFOA[t,self.hidden_size:2*self.hidden_size] + np.multiply(prev_c, self.WpeepIFO[1,:])       # forget gate - adding peephole connections\r\n            \r\n            # Passing through the non-linearities - sigmoid for gates input and forget - output is below due to peephole connections \r\n            self.IFOA_f[t,0:2*self.hidden_size] = 1.0 / (1.0 + np.exp(-self.IFOA[t,0:2*self.hidden_size]))\r\n            self.IFOA_f[t,3*self.hidden_size:] = np.tanh(self.IFOA[t,3*self.hidden_size:]) # tanh non-linearity for the A gate (before the multiplicated input to the cell)\r\n            \r\n            # Computing the cell activation            \r\n            self.C[t,:] = self.IFOA_f[t,self.hidden_size:2*self.hidden_size]*prev_c + self.IFOA_f[t,:self.hidden_size]*self.IFOA_f[t,3*self.hidden_size:]\r\n\r\n            # Computing the output gate with peephole connections\r\n            self.IFOA[t,2*self.hidden_size:3*self.hidden_size] = self.IFOA[t,2*self.hidden_size:3*self.hidden_size] + np.multiply(self.C[t,:], self.WpeepIFO[2,:]) # output gate - adding peephole connections            \r\n            \r\n            self.IFOA_f[t,2*self.hidden_size:3*self.hidden_size] = 1.0 / (1.0 + np.exp(-self.IFOA[t,2*self.hidden_size:3*self.hidden_size]))\r\n            self.Hout[t,:] = self.IFOA_f[t,2*self.hidden_size:3*self.hidden_size]*np.tanh(self.C[t,:])\r\n        \r\n        return self.Hout\r\n  \r\n'"
lstm_execution.py,13,"b""import numpy as np\r\n#import pandas as pd\r\nimport matplotlib.pyplot as plt\r\nfrom lstm_class_object import LSTMPopulation\r\nimport sys\r\n\r\ndef normalise(signal):\r\n    mu = np.mean(signal)\r\n    variance = np.mean((signal - mu)**2)\r\n    signal_normalised = (signal - mu)/(np.sqrt(variance + 1e-8))\r\n    return signal_normalised\r\n    \r\nt_range = np.linspace(0,100,1000)\r\ntrain_df_roc_signal_unnormalised = np.sin(2*np.pi*300*t_range) + 0.5*np.sin(2*np.pi*t_range)\r\n\r\ntemp = train_df_roc_signal_unnormalised - min(train_df_roc_signal_unnormalised)\r\ntrain_df_roc_signal = (temp)/max(temp)\r\n\r\nplt.figure(1)\r\nplt.plot(train_df_roc_signal[0:100])\r\n\r\nseq_len = 24*3\r\ninput_size = 1\r\nhidden_size_a = 200\r\noutput_size = 1\r\nlearning_rate = 1e-3\r\nn, p = 0, 0\r\n\r\nW_out = np.random.randn(output_size, hidden_size_a) / np.sqrt(output_size)\r\nlstm_a = LSTMPopulation(input_size, hidden_size_a)\r\nsignal = np.zeros((seq_len,1))\r\ntarget = np.zeros((seq_len,output_size))\r\nmW_out = np.zeros_like(W_out)\r\n\r\nj=0\r\nk=0\r\nfor i in xrange(1000):\r\n    if j+seq_len+output_size >= len(train_df_roc_signal):\r\n        j=0\r\n        lstm_a.reset_states()\r\n    signal[:,0] = train_df_roc_signal[j:j+seq_len]\r\n    target[:,0] = train_df_roc_signal[j+1:j+1+seq_len]\r\n    lstm_a.forward(signal)\r\n    lstm_a_hidden_out = lstm_a.get_hidden_output()  \r\n    output = lstm_a_hidden_out.dot(W_out.T)   \r\n    error = output - target\r\n    dW_out = (error).T.dot(lstm_a_hidden_out)  \r\n    loss = np.mean(np.square(output - target))  \r\n    dh_out = (error).dot(W_out)\r\n    lstm_a.backward(dh_out)\r\n    lstm_a.train_network(learning_rate)        \r\n    for param, dparam, mem in zip([W_out],\r\n                              [dW_out],\r\n                              [mW_out]):\r\n        mem += dparam * dparam\r\n        param += -learning_rate * dparam / np.sqrt(mem + 1e-8)\r\n    \r\n    print (k, loss)\r\n\r\n    k += 1\r\n    j += 1\r\n\r\n# Testing phase\r\nfor ll in range(1):\r\n    index = 400+ll*100\r\n    plot_len = 24*10\r\n    next_vals = 12\r\n    \r\n    sample_signal = np.zeros((plot_len,1))\r\n    sample_signal[:,0] = train_df_roc_signal[index:index+plot_len] \r\n    sample_signal_plotting = train_df_roc_signal[index:index + plot_len + next_vals] \r\n    \r\n    dd = lstm_a.sample_network(sample_signal, W_out, next_vals)\r\n    sampled_output = dd.dot(W_out.T)\r\n    #y_out = dd.dot(W_out.T)\r\n    #sampled_output = 1.0 / (1.0 + np.exp(-y_out))\r\n    \r\n    plt.figure(2)\r\n    plt.plot(sampled_output[:,0])\r\n    plt.hold(True)\r\n    plt.plot(sample_signal_plotting[:], 'r')\r\n    plt.title('Prediction vs Actual Signal')\r\n    \r\n    plt.figure(3)\r\n    plt.plot(sampled_output[plot_len:plot_len+next_vals,0])\r\n    plt.hold(True)\r\n    plt.plot(sample_signal_plotting[plot_len:plot_len+next_vals], 'r')\r\n    plt.title('Prediction Mode - Blue (Prediction), Red (Actual)')\r\n    plt.hold(False)\r\n    plt.show()\r\n"""
