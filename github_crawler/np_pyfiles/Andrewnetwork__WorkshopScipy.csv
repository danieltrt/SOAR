file_path,api_count,code
TensorFlow-Workshop-March-2017/old_stuff/basics.py,0,"b""# To be interactively typed in order to explore TF basics.\n# Following Oriley's: Hello TensorFlow\n\n## Basic Exploration ##\nimport tensorflow as tf\n\ngraph = tf.get_default_graph()\n\ngraph.get_operations()\n\ninput_value = tf.constant(1.0)\n\noperations = graph.get_operations()\n\noperations\n\noperations[0].node_def\n\nsess = tf.Session()\n\n# TensorFlow manages it's own state of things and maintains a method of evaluating and executing code.  \nsess.run(input_value)\n\n## The simplest TensorFlow neuron ##\n\nweight = tf.Variable(0.8)\n\n# Display the operations added to the graph as a result. \nfor op in graph.get_operations(): print(op.name)\n\noutput_value = weight * input_value\n\nop = graph.get_operations()[-1]\nop.name\n\nfor op_input in op.inputs: print(op_input)\n\n# Generates an operation which initializes all our variables ( in this case just weight ).\n#if you add more variables you'll want to use tf.initialize_all_variables() again; a stale init wouldn't include the new variables.\n\ninit = tf.initialize_all_variables()\nsess.run(init)\n\nsess.run(output_value)\n\nx = tf.constant(1.0, name='input')\nw = tf.Variable(0.8, name='weight')\ny = tf.mul(w, x, name='output')\n\nsummary_writer = tf.train.SummaryWriter('log_simple_graph', sess.graph_def)\n\n# Command line: tensorboard --logdir=log_simple_graph\n#localhost:6006/#graphs\n\n## Training a sinngle Neuron ##\n\ny_ = tf.constant(0.0)\n\n# Defining the loss function as the squared diff between current output and desired. \n\nloss = (y - y_)**2\n\noptim = tf.train.GradientDescentOptimizer(learning_rate=0.025)\ngrads_and_vars = optim.compute_gradients(loss)\nsess.run(tf.initialize_all_variables())\nsess.run(grads_and_vars[1][0])\n\nsess.run(optim.apply_gradients(grads_and_vars))\n\nsess.run(w)\n\ntrain_step = tf.train.GradientDescentOptimizer(0.025).minimize(loss)\nfor i in range(100):\n\tprint('before step {}, y is {}'.format(i, sess.run(y)))\n\tsummary_str = sess.run(summary_y)\n\tsummary_writer.add_summary(summary_str, i)\n\tsess.run(train_step)\n\n\n\nsess.run(y)\n\n\n\n\n\n\n\n\n"""
TensorFlow-Workshop-March-2017/old_stuff/finalBasics.py,0,"b""# Source: https://www.oreilly.com/learning/hello-tensorflow\n# Annotated by: Andrew Ribeiro \n\nimport tensorflow as tf\n\nx = tf.constant(1.0, name='input')\nw = tf.Variable(0.8, name='weight')\ny = tf.mul(w, x, name='output')\ny_ = tf.constant(0.0, name='correct_value')\n\nloss = tf.pow(y - y_, 2, name='loss')\ntrain_step = tf.train.GradientDescentOptimizer(0.025).minimize(loss)\n\nfor value in [x, w, y, y_, loss]:\n\t    tf.scalar_summary(value.op.name, value)\n\nsummaries = tf.merge_all_summaries()\n\nsess = tf.Session()\nsummary_writer = tf.train.SummaryWriter('log_simple_stats', sess.graph)\n\nsess.run(tf.initialize_all_variables())\n\nfor i in range(100):\n\tsummary_writer.add_summary(sess.run(summaries), i)\n\tsess.run(train_step)\n"""
TensorFlow-Workshop-March-2017/old_stuff/input_data.py,0,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Functions for downloading and reading MNIST data.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gzip\nimport os\nimport tempfile\n\nimport numpy\nfrom six.moves import urllib\nfrom six.moves import xrange  # pylint: disable=redefined-builtin\nimport tensorflow as tf\nfrom tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n'"
TensorFlow-Workshop-March-2017/old_stuff/mnist_with_summaries.py,0,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \'License\');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \'AS IS\' BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""A simple MNIST classifier which displays summaries in TensorBoard.\n\n This is an unimpressive MNIST model, but it is a good example of using\ntf.name_scope to make a graph legible in the TensorBoard graph explorer, and of\nnaming summary tags so that they are grouped meaningfully in TensorBoard.\n\nIt demonstrates the functionality of every TensorBoard dashboard.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nflags = tf.app.flags\nFLAGS = flags.FLAGS\nflags.DEFINE_boolean(\'fake_data\', False, \'If true, uses fake data \'\n                     \'for unit testing.\')\nflags.DEFINE_integer(\'max_steps\', 1000, \'Number of steps to run trainer.\')\nflags.DEFINE_float(\'learning_rate\', 0.001, \'Initial learning rate.\')\nflags.DEFINE_float(\'dropout\', 0.9, \'Keep probability for training dropout.\')\nflags.DEFINE_string(\'data_dir\', \'/tmp/data\', \'Directory for storing data\')\nflags.DEFINE_string(\'summaries_dir\', \'/tmp/mnist_logs\', \'Summaries directory\')\n\n\ndef train():\n  # Import data\n  mnist = input_data.read_data_sets(FLAGS.data_dir,\n                                    one_hot=True,\n                                    fake_data=FLAGS.fake_data)\n\n  sess = tf.InteractiveSession()\n\n  # Create a multilayer model.\n\n  # Input placeholders\n  with tf.name_scope(\'input\'):\n    x = tf.placeholder(tf.float32, [None, 784], name=\'x-input\')\n    y_ = tf.placeholder(tf.float32, [None, 10], name=\'y-input\')\n\n  with tf.name_scope(\'input_reshape\'):\n    image_shaped_input = tf.reshape(x, [-1, 28, 28, 1])\n    tf.image_summary(\'input\', image_shaped_input, 10)\n\n  # We can\'t initialize these variables to 0 - the network will get stuck.\n  def weight_variable(shape):\n    """"""Create a weight variable with appropriate initialization.""""""\n    initial = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(initial)\n\n  def bias_variable(shape):\n    """"""Create a bias variable with appropriate initialization.""""""\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\n\n  def variable_summaries(var, name):\n    """"""Attach a lot of summaries to a Tensor.""""""\n    with tf.name_scope(\'summaries\'):\n      mean = tf.reduce_mean(var)\n      tf.scalar_summary(\'mean/\' + name, mean)\n      with tf.name_scope(\'stddev\'):\n        stddev = tf.sqrt(tf.reduce_sum(tf.square(var - mean)))\n      tf.scalar_summary(\'sttdev/\' + name, stddev)\n      tf.scalar_summary(\'max/\' + name, tf.reduce_max(var))\n      tf.scalar_summary(\'min/\' + name, tf.reduce_min(var))\n      tf.histogram_summary(name, var)\n\n  def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):\n    """"""Reusable code for making a simple neural net layer.\n\n    It does a matrix multiply, bias add, and then uses relu to nonlinearize.\n    It also sets up name scoping so that the resultant graph is easy to read,\n    and adds a number of summary ops.\n    """"""\n    # Adding a name scope ensures logical grouping of the layers in the graph.\n    with tf.name_scope(layer_name):\n      # This Variable will hold the state of the weights for the layer\n      with tf.name_scope(\'weights\'):\n        weights = weight_variable([input_dim, output_dim])\n        variable_summaries(weights, layer_name + \'/weights\')\n      with tf.name_scope(\'biases\'):\n        biases = bias_variable([output_dim])\n        variable_summaries(biases, layer_name + \'/biases\')\n      with tf.name_scope(\'Wx_plus_b\'):\n        preactivate = tf.matmul(input_tensor, weights) + biases\n        tf.histogram_summary(layer_name + \'/pre_activations\', preactivate)\n      activations = act(preactivate, \'activation\')\n      tf.histogram_summary(layer_name + \'/activations\', activations)\n      return activations\n\n  hidden1 = nn_layer(x, 784, 500, \'layer1\')\n\n  with tf.name_scope(\'dropout\'):\n    keep_prob = tf.placeholder(tf.float32)\n    tf.scalar_summary(\'dropout_keep_probability\', keep_prob)\n    dropped = tf.nn.dropout(hidden1, keep_prob)\n\n  y = nn_layer(dropped, 500, 10, \'layer2\', act=tf.nn.softmax)\n\n  with tf.name_scope(\'cross_entropy\'):\n    diff = y_ * tf.log(y)\n    with tf.name_scope(\'total\'):\n      cross_entropy = -tf.reduce_mean(diff)\n    tf.scalar_summary(\'cross entropy\', cross_entropy)\n\n  with tf.name_scope(\'train\'):\n    train_step = tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(\n        cross_entropy)\n\n  with tf.name_scope(\'accuracy\'):\n    with tf.name_scope(\'correct_prediction\'):\n      correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n    with tf.name_scope(\'accuracy\'):\n      accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    tf.scalar_summary(\'accuracy\', accuracy)\n\n  # Merge all the summaries and write them out to /tmp/mnist_logs (by default)\n  merged = tf.merge_all_summaries()\n  train_writer = tf.train.SummaryWriter(FLAGS.summaries_dir + \'/train\',\n                                        sess.graph)\n  test_writer = tf.train.SummaryWriter(FLAGS.summaries_dir + \'/test\')\n  tf.initialize_all_variables().run()\n\n  # Train the model, and also write summaries.\n  # Every 10th step, measure test-set accuracy, and write test summaries\n  # All other steps, run train_step on training data, & add training summaries\n\n  def feed_dict(train):\n    """"""Make a TensorFlow feed_dict: maps data onto Tensor placeholders.""""""\n    if train or FLAGS.fake_data:\n      xs, ys = mnist.train.next_batch(100, fake_data=FLAGS.fake_data)\n      k = FLAGS.dropout\n    else:\n      xs, ys = mnist.test.images, mnist.test.labels\n      k = 1.0\n    return {x: xs, y_: ys, keep_prob: k}\n\n  for i in range(FLAGS.max_steps):\n    if i % 10 == 0:  # Record summaries and test-set accuracy\n      summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(False))\n      test_writer.add_summary(summary, i)\n      print(\'Accuracy at step %s: %s\' % (i, acc))\n    else:  # Record train set summaries, and train\n      if i % 100 == 99:  # Record execution stats\n        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n        run_metadata = tf.RunMetadata()\n        summary, _ = sess.run([merged, train_step],\n                              feed_dict=feed_dict(True),\n                              options=run_options,\n                              run_metadata=run_metadata)\n        train_writer.add_run_metadata(run_metadata, \'step%03d\' % i)\n        train_writer.add_summary(summary, i)\n        print(\'Adding run metadata for\', i)\n      else:  # Record a summary\n        summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(True))\n        train_writer.add_summary(summary, i)\n  train_writer.close()\n  test_writer.close()\n\n\ndef main(_):\n  if tf.gfile.Exists(FLAGS.summaries_dir):\n    tf.gfile.DeleteRecursively(FLAGS.summaries_dir)\n  tf.gfile.MakeDirs(FLAGS.summaries_dir)\n  train()\n\n\nif __name__ == \'__main__\':\n  tf.app.run()\n'"
