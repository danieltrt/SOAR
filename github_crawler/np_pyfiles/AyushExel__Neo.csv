file_path,api_count,code
src/nn.py,31,"b'""""""\nClass containing functionality to build and train neural networks\nContains all activation and loss functions some other utility function.\n""""""\n\nimport numpy as np\nimport pdb\n\nfrom numpy import float64, ndarray\nfrom typing import List, Tuple, Union\nclass nn:\n    def __init__(self, layer_dimensions: List[int] = [], activations: List[str] = []) -> None:\n        """"""\n        Initializes networks\'s weights and other useful variables.\n\n        :param layer_dimensions:\n        :param activations: To store the activation for each layer\n        -Parameters contains weights of the layer in form {\'Wi\':[],\'bi\':[]}\n        -Cache contains intermediate results as [[A[i-1],Wi,bi],[Zi]], where i\n         is layer number.\n        -activations contains the names of activation function used for that layer\n        -cost_function  contains the name of cost function to be used\n        -lamb contains the regularization hyper-parameter\n        -grads contains the gradients calculated during back-prop in form {\'dA(i-1)\':[],\'dWi\':[],\'dbi\':[]}\n        -layer_type contains the info about the type of layer( fc, conv etc)\n        """"""\n        self.parameters = {}\n        self.cache = []\n        self.activations = activations\n        self.cost_function = \'\'\n        self.lamb = 0\n        self.grads = {}\n        self.layer_type = [\'\']\n        self.hyperparam = {}\n        self.initialize_parameters(layer_dimensions)\n        self.check_activations()\n \n\n    def initialize_parameters(self, layer_dimensions: List[int]) -> None:\n        """"""\n        Xavier initialization of weights of a network described by given layer\n        dimensions.\n\n        :param layer_dimensions: Dimensions to layers of the network\n        :return: None\n        """"""\n        num_layers = int(len(self.parameters)/2)\n\n        for i in range(1, len(layer_dimensions)):\n            self.parameters[""W"" + str(num_layers+i)] = (\n                np.sqrt(2/layer_dimensions[i - 1])*np.random.randn(layer_dimensions[i],\n                                layer_dimensions[i - 1])\n            )\n            self.parameters[""b"" + str(i+num_layers)] = np.zeros((layer_dimensions[i], 1))\n            self.layer_type.append(\'fc\')\n\n    def add_fcn(self,dims: List[int],activations: List[str]) -> None:\n        \'\'\'\n        Add fully connected layers in between the network\n        :param dims:list describing dimensions of fully connected networks\n        :param activations: activations of each layer\n        \'\'\'\n        self.initialize_parameters(dims)\n        for i in activations:\n            self.activations.append(i)\n\n    def check_activations(self) -> None:\n        \'\'\'\n        Checks if activations for all layers are present. Adds \'None\' if no activations are provided for a particular layer.\n        \n        :returns: None\n        \'\'\'\n        num_layers = int(len(self.parameters)/2)\n        while len(self.activations) < num_layers :\n            self.activations.append(None)\n        \n\n    @staticmethod\n    def __linear_forward(A_prev, W, b):\n        """"""\n        Linear forward to the current layer using previous activations.\n\n        :param A_prev: Previous Layer\'s activation\n        :param W: Weights for current layer\n        :param b: Biases for current layer\n        :return: Linear cache and current calculated layer\n        """"""\n        Z = W.dot(A_prev) + b\n        linear_cache = [A_prev, W, b]\n        return Z, linear_cache\n\n    def __activate(self, Z, n_layer=1):\n        """"""\n        Activate the given layer(Z) using the activation function specified by\n        \'type\'.\n\n        Note: This function treats 1 as starting index!\n              First layer\'s index is 1.\n\n        :param Z: Layer to activate\n        :param n_layer: Layer\'s index\n        :return: Activated layer and activation cache\n        """"""\n        act_cache = [Z]\n        act = None\n        if (self.activations[n_layer - 1]) == None:\n            act = Z\n        elif (self.activations[n_layer - 1]).lower() == ""relu"":\n            act = Z * (Z > 0)\n        elif (self.activations[n_layer - 1]).lower() == ""tanh"":\n            act = np.tanh(Z)\n        elif (self.activations[n_layer - 1]).lower() == ""sigmoid"":\n            act = 1 / (1 + np.exp(-Z))\n        elif (self.activations[n_layer - 1]).lower() == ""softmax"":\n            act = np.exp(Z-np.max(Z))\n            act = act/(act.sum(axis=0)+1e-10)\n        \n\n        # assert(act!=None)\n\n        return act, act_cache\n\n    def forward(self, net_input: ndarray) -> ndarray:\n        """"""\n        To forward propagate the entire Network.\n\n        :param net_input: Contains the input to the Network\n        :return: Output of the network\n        """"""\n        self.cache = [] \n        A = net_input\n        for i in range(1, int(len(self.parameters) / 2)):\n            W = self.parameters[""W"" + str(i)]\n            b = self.parameters[""b"" + str(i)]\n            Z = linear_cache = None\n            if self.layer_type[i] == \'fc\':\n                Z, linear_cache = self.__linear_forward(A, W, b)\n            elif self.layer_type[i] == \'conv\':\n                hyperparam = self.hyperparam[i]\n                Z , linear_cache = self.conv_forward(A,W,b,hyperparam)\n\n                #flatten the output if the next layer is fully connected\n            A, act_cache = self.__activate(Z, i)\n            if  self.layer_type[i]==\'conv\':\n                if  self.layer_type[i+1] == \'fc\':\n                    A = A.reshape((A.shape[1]*A.shape[2]*A.shape[3],A.shape[0]))\n \n\n            self.cache.append([linear_cache, act_cache])\n\n        # For Last Layer\n        W = self.parameters[""W"" + str(int(len(self.parameters) / 2))]\n        b = self.parameters[""b"" + str(int(len(self.parameters) / 2))]\n        Z, linear_cache = self.__linear_forward(A, W, b)\n        if len(self.activations) == len(self.parameters) / 2:\n            A, act_cache = self.__activate(Z, len(self.activations))\n            self.cache.append([linear_cache, act_cache])\n        else:\n            A = Z\n            self.cache.append([linear_cache, [None]])\n        \n        return A\n    \'\'\'\n    !!!!Only works for fully connected networks.!!!!!\n\n    def forward_upto(self, net_input, layer_num):\n        """"""\n        Calculates forward prop upto layer_num.\n\n        :param net_input: Contains the input to the Network\n        :param layer_num: Layer up to which forward prop is to be calculated\n        :return: Activations of layer layer_num\n        """"""\n        if layer_num == int(len(self.parameters) / 2):\n            return self.forward(net_input)\n        else:\n            A = net_input\n            for i in range(1, layer_num):\n                W = self.parameters[""W"" + str(i)]\n                b = self.parameters[""b"" + str(i)]\n                Z, linear_cache = self.__linear_forward(A, W, b)\n\n                A, act_cache = self.__activate(Z, i)\n                self.cache.append([linear_cache, act_cache])\n            return A\n    \'\'\' \n\n    def MSELoss(self,prediction: ndarray,mappings: ndarray) -> float64:\n        \'\'\'\n        Calculates the Mean Squared error with regularization cost(if provided) between output of the network and the real\n        mappings of a function.\n        Changes cost_function to appropriate value\n\n        :param prediction: Output of the neural net\n        :param mappings: Real outputs of a function\n        :return: Mean squared error b/w output and mappings\n        \'\'\'\n\n        self.cost_function = \'MSELoss\'\n        loss = np.square(prediction-mappings).mean()/2\n        regularization_cost = 0\n        if self.lamb != 0:\n            for params in range(len(self.cache)):  \n                regularization_cost = regularization_cost + np.sum(np.square(self.parameters[\'W\'+str(params+1)]))\n        regularization_cost = (self.lamb/(2*prediction.shape[1]))*regularization_cost\n        \n        return loss + regularization_cost\n\n    def CrossEntropyLoss(self,prediction: ndarray,mappings: ndarray) -> float64:\n        \'\'\'\n        Calculates the cross entropy loss between output of the network and the real mappings of a function\n        Changes cost_function to appropriate value\n\n        :param prediction: Output of the neural net\n        :param mappings: Real outputs of a function\n        :return: Mean squared error b/w output and mappings\n        \'\'\'\n        epsilon = 1e-8\n        self.cost_function = \'CrossEntropyLoss\'\n        loss = -(1/prediction.shape[1])*np.sum( mappings*np.log(prediction+epsilon) + (1-mappings)*np.log(1-prediction+epsilon) )\n        regularization_cost = 0\n        if self.lamb != 0:\n            for params in range(len(self.cache)):\n                regularization_cost = regularization_cost + np.sum(np.square(self.parameters[\'W\'+str(params+1)]))\n        regularization_cost = (self.lamb/(2*prediction.shape[1]))*regularization_cost\n\n        return loss + regularization_cost\n    \n    def output_backward(self,prediction: ndarray,mapping: ndarray) -> ndarray:\n        \'\'\'\n        Calculates the derivative of the output layer(dA)\n\n        :param prediction: Output of neural net\n        :param mapping: Correct output of the function\n        :param cost_type: Type of Cost function used\n        :return: Derivative of output layer, dA  \n        \'\'\'\n        dA = None\n        cost = self.cost_function\n        if cost.lower() == \'crossentropyloss\':\n            dA =  -(np.divide(mapping, prediction+1e-10) - np.divide(1 - mapping, 1 - prediction+1e-10))\n        \n        elif cost.lower() == \'mseloss\':   \n            dA =  (prediction-mapping)\n        \n        return dA\n    \n    def deactivate(self,dA: ndarray,n_layer: int) -> Union[ndarray, int]:\n        \'\'\'\n        Calculates the derivate of dA by deactivating the layer\n\n        :param dA: Activated derivative of the layer\n        :n_layer: Layer number to be deactivated\n        :return: deact=> derivative of activation \n        \'\'\'\n        act_cache = self.cache[n_layer-1][1]\n        dZ = act_cache[0]\n        deact = None\n        if self.activations[n_layer - 1] == None:\n            deact = 1\n        elif (self.activations[n_layer - 1]).lower() == ""relu"":\n            deact = 1* (dZ>0)\n        elif (self.activations[n_layer - 1]).lower() == ""tanh"":\n            deact = 1- np.square(dA)\n        elif (self.activations[n_layer - 1]).lower() == ""sigmoid"" or (self.activations[n_layer - 1]).lower()==\'softmax\':\n            s = 1/(1+np.exp(-dZ+1e-10))\n            deact = s*(1-s)\n\n        return deact\n    \n    def linear_backward(self,dA: ndarray,n_layer: int) -> Tuple[ndarray, ndarray, ndarray]:\n        \'\'\'\n        Calculates linear backward propragation for layer denoted by n_layer\n\n        :param dA: Derivative of cost w.r.t this layer\n        :param n_layer: layer number\n        :return : dZ,dW,db,dA_prev\n        \'\'\'\n        batch_size = dA.shape[1]\n        current_cache = self.cache[n_layer-1]\n        linear_cache = current_cache[0]\n        A_prev,W,b = linear_cache\n\n        dZ = dA*self.deactivate(dA,n_layer)\n        dW = (1/batch_size)*dZ.dot(A_prev.T) + (self.lamb/batch_size)*self.parameters[\'W\'+str(n_layer)]\n        db = (1/batch_size)*np.sum(dZ,keepdims=True,axis=1)\n        dA_prev = W.T.dot(dZ)\n\n        assert(dA_prev.shape == A_prev.shape)\n        assert(dW.shape == W.shape)\n        assert(db.shape == b.shape)\n        \n        return dW,db,dA_prev\n        \n        \n\n    def backward(self,prediction: ndarray,mappings: ndarray) -> None:\n        \'\'\'\n        Backward propagates through the network and stores useful calculations\n\n        :param prediction: Output of neural net\n        :param mapping: Correct output of the function\n        :return : None\n        \'\'\'\n        layer_num = len(self.cache)\n        doutput = self.output_backward(prediction,mappings)\n        self.grads[\'dW\'+str(layer_num)],self.grads[\'db\'+str(layer_num)],self.grads[\'dA\'+str(layer_num-1)] = self.linear_backward(doutput,layer_num)\n        temp = self.layer_type\n        self.layer_type = self.layer_type[1:]\n        \n        for l in reversed(range(layer_num-1)):\n            dW,db,dA_prev = None,None,None\n            if self.layer_type[l] == \'fc\':\n                dW,db,dA_prev = self.linear_backward(self.grads[\'dA\'+str(l+1)],l+1)\n            elif self.layer_type[l] == \'conv\':\n                dW,db,dA_prev = self.conv_backward((self.cache[l][1][0]),self.cache[l][0])\n            self.grads[\'dW\'+str(l+1)] = dW\n            self.grads[\'db\'+str(l+1)] = db\n            self.grads[\'dA\'+str(l)] = dA_prev\n        \n        self.layer_type = temp\n    \n    @staticmethod\n    def zero_pad(imgData: ndarray,pad: int) -> ndarray:\n        \'\'\'\n        Provides zero padding to the multi channel image data provided\n        :param imgData: image data to pad\n        :param pad    : amount of padding per layer\n\n        :return : image with desired padding\n        \'\'\'\n        X = np.pad(imgData,((0,0),(pad,pad),(pad,pad),(0,0)),\'constant\',constant_values = 0)\n        return X\n\n    def conv2d(self,in_planes: int,out_planes: int,kernel_size: int,activation: str,stride: int = 1,padding: int = 0) -> None:\n        \'\'\'\n        Add paramters for this layer in the parameters list\n\n        :return : None\n        \'\'\'\n        num_layers = int(len(self.parameters)/2)\n        self.parameters[\'W\'+str(num_layers+1)] = np.random.randn(kernel_size,kernel_size,in_planes,out_planes)\n        self.parameters[\'b\'+str(num_layers+1)] = np.random.randn(1,1,1,out_planes)\n        self.activations.append(activation)\n        self.layer_type.append(\'conv\')\n        self.hyperparam[num_layers+1] = list((stride,padding))\n\n    def conv_single(self,a_prev_slice: ndarray,W: ndarray,b: ndarray) -> float64:\n        \'\'\'\n        Apply convolution using W and b as filter on the activation slice of the previous layer\n\n        :param a_prev_slice: a slice of previous activated layer\n        :param W           : Filter\n        :param b           : bais\n        :return Z: scalar value resultant of the convolution\n        \'\'\'\n        Z  = np.multiply(a_prev_slice,W)\n        Z = np.sum(Z)\n        Z = Z + float(b) #to convert the value to float from matrix type\n        return Z\n    \n    def conv_forward(self,A_prev: ndarray,W: ndarray,b: ndarray,hyper_param: List[int]) -> Tuple[ndarray, Tuple[ndarray, ndarray, ndarray, List[int]]]:\n        \'\'\'\n        Implements forward pass of convolutional layer.\n        \n        :param A_prev:activations of previous layer\n        :param W: Filter\n        :param b: bias\n        :param hyper_param  : list of hyperparameters, stride and padding\n\n        :return: Z,cache\n        \'\'\'\n        m,h_prev,w_prev,nc_prev = A_prev.shape\n        f,f,nc_prev,nc = W.shape\n        stride,pad = hyper_param\n        #comupte the dimensions of the result using convolution formula => w/h = (w/h(prev) -f +2*pad)/stride +1\n        n_h = int(np.floor((h_prev-f+2*pad)/stride)) +1\n        n_w = int(np.floor((w_prev-f+2*pad)/stride)) +1\n        \n        Z = np.zeros((m,n_h,n_w,nc))\n        A_prev_pad = self.zero_pad(A_prev,pad)\n        for i in range(m):\n            prev_pad = A_prev_pad[i]\n\n            for h in range(n_h):\n                for w in range(n_w):\n                    for c in range(nc):\n                        vertstart = h*stride\n                        vertend = vertstart + f\n                        horstart = w*stride\n                        horend = horstart+f \n\n                        prev_slice = prev_pad[vertstart:vertend,horstart:horend,:]\n                        Z[i,h,w,c] = self.conv_single(prev_slice,W[:,:,:,c],b[:,:,:,c])\n        \n        cache = (A_prev,W,b,hyper_param)\n\n        return Z,cache\n\n    def pool_forward(self,A_prev,f,stride,type=\'max\'):\n        \'\'\'\n        To enable max and average pooling during the forward pass\n\n        :param A_prev: Activation of previous layer\n        :param   f   : filter size\n        :param stride: size of each stride\n        :param type  : type of pooling, max or average\n        \n        :returns A,cache:\n        \'\'\'\n        #Calculate the resultant dimensions:\n        n_h = int(1 + (A_prev.shape[1] - f) / stride)\n        n_w = int(1 + (A_prev.shape[2] - f) / stride)\n        n_c = A_prev.shape[3]\n\n        A = np.zeros((A_prev.shape[0],n_h,n_w,n_c))\n\n        for i in range(A.shape[0]):\n            for h in range(n_h):\n                for w in range(n_w):\n                    for c in range(n_c):\n                        vertstart = h*stride\n                        vertend = vertstart + f\n                        horstart = w*stride\n                        horend = horstart+f \n\n                        a_prev_slice = A_prev[i,vertstart:vertend,horstart:horend,c]\n\n                        if type == \'max\':\n                            A[i,h,w,c] = np.max(a_prev_slice)\n                        elif type == \'avg\':\n                            A[i,h,w,c] = np.mean(a_prev_slice)\n        \n        cache = (A_prev,[f,stride],type)\n        return A,cache\n\n\n\n\n    def conv_backward(self,dZ: ndarray, cache: Tuple[ndarray, ndarray, ndarray, List[int]]) -> Tuple[ndarray, ndarray, ndarray]:\n        """"""\n        Implement the backward propagation for a convolution function\n        \n        Arguments:\n        dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n        cache -- cache of values needed for the conv_backward(), output of conv_forward()\n        \n        Returns:\n        dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n                   numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n        dW -- gradient of the cost with respect to the weights of the conv layer (W)\n              numpy array of shape (f, f, n_C_prev, n_C)\n        db -- gradient of the cost with respect to the biases of the conv layer (b)\n              numpy array of shape (1, 1, 1, n_C)\n        """"""\n        \n\n        (A_prev, W, b, hparameters) = cache\n        \n        (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n        \n        (f, f, n_C_prev, n_C) = W.shape\n        \n        stride = hparameters[0]\n        pad = hparameters[1]\n        \n        (m, n_H, n_W, n_C) = dZ.shape\n        \n        dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))                           \n        dW = np.zeros((f, f, n_C_prev, n_C))\n        db = np.zeros((1, 1, 1, n_C))\n\n        A_prev_pad = self.zero_pad(A_prev, pad)\n        dA_prev_pad = self.zero_pad(dA_prev, pad)\n        \n        for i in range(m):                      \n            \n            a_prev_pad = A_prev_pad[i]\n            da_prev_pad = dA_prev_pad[i]\n            \n            for h in range(n_H):                  \n                for w in range(n_W):               \n                    for c in range(n_C):           \n                        \n                        vert_start = h * stride\n\n                        vert_end = vert_start + f\n                        horiz_start = w * stride\n\n                        horiz_end = horiz_start + f\n                        \n                        a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n\n                        da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n                        dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n                        db[:,:,:,c] += dZ[i, h, w, c]\n                        \n            dA_prev[i, :, :, :] =  da_prev_pad if pad == 0 else da_prev_pad[pad:-pad,pad:-pad,:]\n        \n        assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n        \n        return dA_prev, dW, db\n\n\n \n    \n    def create_mask(self,X):\n        \'\'\'\n        Creates mask of from a slice which sets max element index to 1 and others to 0\n\n        :param X: original matrix\n        :return :mask\n        \'\'\'\n        mask = (X==np.max(X))\n\n        return mask\n\n    def average_back(self,X,shape):\n        \'\'\'\n        Computes backward pass for average pooling layer\n\n        :param X: average pooled layer\n        :param shape: shape of the original matrix\n        \'\'\'\n        h,w = shape\n        X = X/(h*w)\n        return np.ones(shape)*X\n\n    def pool_backward(self,dA, cache, mode = ""max""):\n        """"""\n        Implements the backward pass of the pooling layer\n        \n        :param dA: gradient of cost with respect to the output of the pooling layer, same shape as A\n        :param cache: cache output from the forward pass of the pooling layer, contains the layer\'s input and hparameters \n        :param mode:the pooling mode you would like to use, defined as a string (""max"" or ""average"")\n        \n        Returns:\n        dA_prev  gradient of cost with respect to the input of the pooling layer, same shape as A_prev\n        """"""\n        \n        (A_prev, (stride,f),type) = cache\n        \n        m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n        m, n_H, n_W, n_C = dA.shape\n        \n        dA_prev = np.zeros(A_prev.shape)\n        \n        for i in range(m):                       \n            a_prev = A_prev[i]\n            for h in range(n_H):                   \n                for w in range(n_W):               \n                    for c in range(n_C):           \n                        \n                        vert_start = h*stride\n                        vert_end = vert_start + f\n                        horiz_start = w*stride\n                        horiz_end = horiz_start + f\n                        \n                        \n                        if type == ""max"":\n                        \n                            a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]\n                            mask = self.create_mask(a_prev_slice)\n                            dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += np.multiply(mask, dA[i, h, w, c])\n                        elif mode == ""average"":\n                            da = dA[i, h, w, c]\n                            shape = (f, f)\n                            dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += self.average_back(da, shape)\n                        \n    \n    \n        return dA_prev                           \n                            \n\n    def __str__(self) -> str:\n        \'\'\'\n        :Return: the network architecture and connectivity\n        \'\'\'\n        net_string = """"\n        for params in range(int(len(self.parameters)/2)):\n            weight = self.parameters[\'W\'+str(params+1)]\n            net_string = net_string + "" -> Linear("" + str(weight.shape[1]) +"" , "" + str(weight.shape[0]) + "")""\n            if self.activations[params] != None:\n                net_string = net_string + "" -> "" +  self.activations[params]\n        return net_string\n\n\n\n'"
src/optimizer.py,12,"b'""""""\nThis class contains various optimizer and helper functions in one place for better and modular understanding of overall \nlibrary.\n""""""\nimport nn\nimport numpy as np\n\nclass optimizer:\n    @staticmethod\n    def gradientDescentOptimizer(input,mappings,net,alpha=0.001,lamb=0, epoch=100,print_at=5,prnt=True,update=True):\n        """"""\n        Performs gradient descent on the given network setting the default value of epoch and alpha if not provided otherwise\n\n        :param input  : input for neural net\n        :param mapping: Correct output of the function\n        :param net    : nn.nn object which provides the network architecture\n        :param alpha  : Learning rate\n        :param lamb   : Regularization parameter\n        :param epoch  : Number of iterations\n        :param print_at: Print at multiples of \'print_at\'\n        :param prnt   : Print if prnt=true\n        """"""\n        net.lamb = lamb\n\n        for i in range(epoch):\n            net.cache = []\n            prediction = net.forward(input)\n            loss_function = (net.cost_function).lower()\n            loss,regularization_cost = None,0\n            if loss_function == \'mseloss\':\n                loss = net.MSELoss(prediction,mappings)\n            if loss_function == \'crossentropyloss\':\n                loss = net.CrossEntropyLoss(prediction,mappings)\n                \n            if prnt and i%print_at==0 :\n                print(\'Loss at \',i, \' \' ,loss)\n\n            net.backward(prediction,mappings)\n            if update:\n                net.parameters = optimizer.update_params(net.parameters,net.grads,alpha)\n\n    @staticmethod\n    def SGDOptimizer(input,mappings,net,mini_batch_size=64,alpha=0.001,lamb=0,momentum=None,epoch=5,print_at=5,prnt=True):\n        \'\'\'\n        Performs Stochaitic gradient descent on the given network\n        -Generates mini batches of given size using random permutation\n        -Uses gradient descent on each mini batch separately\n\n        :param input  : input for neural net\n        :param mapping: Correct output of the function\n        :param net    : nn.nn object which provides the network architecture\n        :param batch_size: Batch size to be used witch SGD\n        :param alpha  : Learning rate\n        :param lamb   : Regularization parameter\n        :param momentum: Momentum Hyper parameter\n        :param epoch  : Number of iterations\n        :param print_at: Print at multiples of \'print_at\'\n        :param prnt   : Print if prnt=true\n\n        :return : None\n        \'\'\'\n        batch_size = input.shape[1]\n        mini_batches = []\n        \n        permutation = list(np.random.permutation(batch_size))\n        shuffled_input = input[:,permutation]\n        shuffled_mappings = (mappings[:,permutation])\n\n        num_complete_batches = int(np.floor(batch_size/mini_batch_size))\n        \n        #Separate the complete mini_batches\n        for i in range(0,num_complete_batches):\n            mini_batch_input = shuffled_input[:,i*mini_batch_size:(i+1)*mini_batch_size]\n            mini_batch_mappings = shuffled_mappings[:,i*mini_batch_size:(i+1)*mini_batch_size]\n            mini_batch = (mini_batch_input,mini_batch_mappings)\n            mini_batches.append(mini_batch)\n        \n        #Separate the incomplete mini batch if any\n        if batch_size % mini_batch_size != 0:\n            mini_batch_input = shuffled_input[:,batch_size - num_complete_batches*mini_batch_size : batch_size]\n            mini_batch_mappings = shuffled_mappings[:,batch_size - num_complete_batches*mini_batch_size : batch_size]\n            mini_batch = (mini_batch_input,mini_batch_mappings)\n            mini_batches.append(mini_batch)\n        \n        #Initialize momentum velocity\n        velocity = {}\n        if momentum != None:\n            for i in range(int(len(net.parameters)/2)):\n                velocity[\'dW\'+str(i+1)] = np.zeros(net.parameters[\'W\'+str(i+1)].shape)\n                velocity[\'db\'+str(i+1)] = np.zeros(net.parameters[\'b\'+str(i+1)].shape)\n        \n\n        for i in range(1,epoch+1):\n\n            for batches in range(len(mini_batches)):\n\n                if momentum != None:\n                    optimizer.gradientDescentOptimizer(input,mappings,net,alpha,lamb,epoch=1,prnt=False,update=False)\n                    for j in range(int(len(net.parameters)/2)):\n                        velocity[\'dW\' + str(j+1)] = momentum*velocity[\'dW\'+str(j+1)] + (1-momentum)*net.grads[\'dW\'+str(j+1)]\n                        velocity[\'db\' + str(j+1)] = momentum*velocity[\'db\'+str(j+1)] + (1-momentum)*net.grads[\'db\'+str(j+1)]\n                    net.parameters = optimizer.update_params(net.parameters,velocity,alpha)\n                else:\n                    optimizer.gradientDescentOptimizer(input,mappings,net,alpha,lamb,epoch=1,prnt=False)\n\n            prediction = net.forward(input)\n            loss = None \n            loss_function = net.cost_function.lower()\n            if loss_function == \'mseloss\':\n                loss = net.MSELoss(prediction,mappings)\n            if loss_function == \'crossentropyloss\':\n                loss = net.CrossEntropyLoss(prediction,mappings)\n            \n            if i%print_at == 0:\n                print(\'Loss at \', i , \' \' , loss)\n    \n    @staticmethod\n    def AdamOptimizer(input,mappings,net,alpha=0.001,lamb=0,betas=(0.9,0.99),epoch=5,print_at=5,prnt=True):\n        \'\'\'\n        Performs Adam otimization on the given network.\n\n        :param input  : input for neural net\n        :param mapping: Correct output of the function\n        :param net    : nn.nn object which provides the network architecture\n        :param alpha  : Learning rate\n        :param lamb   : Regularization parameter\n        :param betas: Adam Hyper parameters\n        :param epoch  : Number of iterations\n        :param print_at: Print at multiples of \'print_at\'\n        :param prnt   : Print if prnt=true\n\n        :return : None\n        \'\'\'\n        batch_size = input.shape[1]\n\n        velocity, square = {},{}\n        for i in range(int(len(net.parameters)/2)):\n            velocity[\'dW\'+str(i+1)] = np.zeros(net.parameters[\'W\'+str(i+1)].shape)\n            velocity[\'db\'+str(i+1)] = np.zeros(net.parameters[\'b\'+str(i+1)].shape)\n            square[\'dW\'+str(i+1)] = np.zeros(net.parameters[\'W\'+str(i+1)].shape)\n            square[\'db\'+str(i+1)] = np.zeros(net.parameters[\'b\'+str(i+1)].shape)\n        \n        for i in range(1,epoch+1):\n            \n            optimizer.gradientDescentOptimizer(input,mappings,net,lamb,epoch=1,prnt=False,update=False)\n\n            for j in range(int(len(net.parameters)/2)):\n                velocity[\'dW\'+str(j+1)] = betas[0]*velocity[\'dW\'+str(j+1)] + (1-betas[0])*net.grads[\'dW\'+str(j+1)]\n                velocity[\'db\'+str(j+1)] = betas[0]*velocity[\'db\'+str(j+1)] + (1-betas[0])*net.grads[\'db\'+str(j+1)]\n                square[\'dW\'+str(j+1)] = betas[1]*square[\'dW\'+str(j+1)] + (1-betas[1])*np.power(net.grads[\'dW\'+str(j+1)],2)\n                square[\'db\'+str(j+1)] = betas[1]*square[\'db\'+str(j+1)] + (1-betas[1])*np.power(net.grads[\'db\'+str(j+1)],2)\n            \n            update = {}\n            for j in range(int(len(net.parameters)/2)):\n                update[\'dW\' + str(j+1)] = velocity[\'dW\'+ str(j+1)]/(np.sqrt(square[\'dW\'+str(j+1)])+1e-10)\n                update[\'db\' + str(j+1)] = velocity[\'db\'+ str(j+1)]/(np.sqrt(square[\'db\'+str(j+1)])+1e-10)\n            \n            net.parameters = optimizer.update_params(net.parameters,update,alpha)\n\n            prediction = net.forward(input)\n            loss = None \n            loss_function = net.cost_function.lower()\n            if loss_function == \'mseloss\':\n                loss = net.MSELoss(prediction,mappings)\n            if loss_function == \'crossentropyloss\':\n                loss = net.CrossEntropyLoss(prediction,mappings)\n            \n            if i%print_at == 0:\n                print(\'Loss at \', i , \' \' , loss)\n                \n    \n\n\n                \n\n\n\n        \n\n    @staticmethod\n    def update_params(params,updation,learning_rate):\n        \'\'\'\n        Updates the parameters using gradients and learning rate provided\n        \n        :param params   : Parameters of the network\n        :param updation    : updation valcues calculated using appropriate algorithms\n        :param learning_rate: Learning rate for the updation of values in params\n\n        :return : Updated params \n        \'\'\'\n        \n        for i in range(int(len(params)/2)):\n            params[\'W\' + str(i+1)] = params[\'W\' + str(i+1)] - learning_rate*updation[\'dW\' + str(i+1)]\n            params[\'b\' + str(i+1)] = params[\'b\' + str(i+1)] - learning_rate*updation[\'db\' + str(i+1)]\n        \n        return params\n        \n    \n\n        \n\n\n    \n\n\n    \n'"
src/test.py,7,"b'\'\'\'\nThis file contains the tests for evaluating the functions in nn.py\n\'\'\'\n\nimport sklearn.datasets as datasets\nimport nn\nimport numpy as np\nfrom optimizer import optimizer\n\ndef test_run():\n    """"""\n    Sample test run.\n\n    :return: None\n    """"""\n    \n    # test run for binary classification problem:\n    np.random.seed(3)\n    print(\'Running a binary classification test\')\n\n    #Generate sample binary classification data\n    data = datasets.make_classification(n_samples=30000,n_features=10,n_classes=2)\n    X= data[0].T\n    Y = (data[1].reshape(30000,1)).T\n    net = nn.nn([10,20,1],[\'relu\',\'sigmoid\'])\n    net.cost_function = \'CrossEntropyLoss\'\n    print(\'net architecture :\')\n    print(net)\n    #Optimize using standard gradient descenet\n    optim = optimizer.gradientDescentOptimizer\n    optim(X,Y,net,alpha=0.07,epoch=200,lamb=0.05,print_at=100)\n    output = net.forward(X)\n    #Convert the probabilities to output values\n    output = 1*(output>=0.5)\n    accuracy = np.sum(output==Y)/30000\n    print(\'for gradient descenet \\n accuracy = \' ,accuracy*100)\n\n    #Optimize using SGD without momentum\n    net = nn.nn([10,20,1],[\'relu\',\'sigmoid\'])\n    net.cost_function = \'CrossEntropyLoss\'\n    optim = optimizer.SGDOptimizer\n    optim(X,Y,net,128,alpha=0.07,epoch=5,lamb=0.05,print_at=1)\n    output = net.forward(X)\n    output = 1*(output>=0.5)\n    accuracy = np.sum(output==Y)/30000\n    print(\'for stochaistic gradient descenet without momentum\\n accuracy = \' ,accuracy*100)\n\n    #optimize using  SGD with momentum\n    net = nn.nn([10,20,1],[\'relu\',\'sigmoid\'])\n    net.cost_function = \'CrossEntropyLoss\'\n    optim = optimizer.SGDOptimizer\n    optim(X,Y,net,128,alpha=0.07,epoch=5,lamb=0.05,print_at=1,momentum=0.9)\n    output = net.forward(X)\n    output = 1*(output>=0.5)\n    accuracy = np.sum(output==Y)/30000\n    print(\'for stochaistic gradient descenet with momentum\\n accuracy = \' ,accuracy*100)\n\n    #optimize using  ADAM\n    net = nn.nn([10,20,1],[\'relu\',\'sigmoid\'])\n    net.cost_function = \'CrossEntropyLoss\'\n    optim = optimizer.AdamOptimizer\n    optim(X,Y,net,alpha=0.07,epoch=80,lamb=0.05,print_at=5)\n    output = net.forward(X)\n    output = 1*(output>=0.5)\n    accuracy = np.sum(output==Y)/30000\n    print(\'for Adam:\\n accuracy = \' ,accuracy*100)\n\n\n    \n    print(\'Running a regression test\')\n    #test run for regresssion problem:\n    #Generate sample regression data\n         \n    X = np.random.randn(1,60000)\n    Y = X**2 \n    net = nn.nn([1,10,1],[\'relu\'])\n    net.cost_function = \'MSELoss\'\n    print(\'net architecture :\')\n    print(net)\n\n    #Optimize using standard gradient descenet\n    print(\'for gradient descenet \')\n    optim = optimizer.gradientDescentOptimizer\n    optim(X,Y,net,alpha=0.3,epoch=200,lamb=0.05,print_at=100)\n\n    net = nn.nn([1,10,1],[\'relu\'])\n    net.cost_function = \'MSELoss\'\n    #Optimize using stochaistic gradient descenet without momentum\n    print(\'for stochaistic gradient descenet \')\n    optim = optimizer.SGDOptimizer\n    optim(X,Y,net,alpha=0.3,epoch=10,lamb=0.05,print_at=1)\n\n    net = nn.nn([1,10,1],[\'relu\'])\n    net.cost_function = \'MSELoss\'\n    #Optimize using stochaistic gradient descenet with momentum\n    print(\'for stochaistic gradient descenet with momentum \')\n    optim = optimizer.SGDOptimizer\n    optim(X,Y,net,alpha=0.3,epoch=10,lamb=0.05,print_at=1,momentum=0.9)\n\n    net = nn.nn([1,10,1],[\'relu\'])\n    net.cost_function = \'MSELoss\'\n    #Optimize using stochaistic gradient descenet with momentum\n    print(\'for Adam \')\n    optim = optimizer.AdamOptimizer\n    optim(X,Y,net,alpha=0.3,epoch=70,lamb=0.05,print_at=5)\n    #Sampe test for COnvolution layers => (NOTE: this is just test for bugs)\n    net = nn.nn()\n    net.conv2d(3,5,3,\'relu\',padding=1)\n    net.add_fcn([36*5,10,5],[\'relu\',\'relu\'])\n    \n    X = np.random.randn(10*6*6*3).reshape((10,6,6,3))\n    out = net.forward(X)\n    net.cost_function = \'CrossEntropyLoss\'\n    net.backward(out,out)\n \n\n   \n\nif __name__ == ""__main__"":\n    test_run()\n\n'"
