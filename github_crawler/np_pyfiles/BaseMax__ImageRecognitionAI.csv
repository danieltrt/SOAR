file_path,api_count,code
dataset.py,0,"b""import os, shutil\n\noriginal_dataset_dir = 'pic'\n\nbase_dir = 'dataset'\ntry:\n    os.mkdir(base_dir)\nexcept:\n    pass\n\ntrain_dir = os.path.join(base_dir, 'train')\ntry:\n    os.mkdir(train_dir)\nexcept:\n    pass\n\ntrain_cat_dir = []\nfor i in range(100):\n    train_cat_dir.append(os.path.join(train_dir, str(i)))\n    try:\n        os.mkdir(train_cat_dir[-1])\n    except:\n        pass\n\n\nfor i in range(100):\n    fnames = ['{}_{}.jpg'.format(i, (i*100)+j) for j in range(1,101)]\n    for fname in fnames:\n        src = os.path.join(original_dataset_dir, fname)\n        dst = os.path.join(train_cat_dir[i], fname)\n        shutil.copyfile(src, dst)\n"""
global.py,6,"b'#-----------------------------------\n# GLOBAL FEATURE EXTRACTION\n#-----------------------------------\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport mahotas\nimport cv2\nimport os\nimport h5py\n\n#--------------------\n# tunable-parameters\n#--------------------\nimages_per_class = 80\nfixed_size       = tuple((300, 300))\ntrain_path       = ""dataset/train""\nh5_data          = \'output/data.h5\'\nh5_labels        = \'output/labels.h5\'\nbins             = 8\n\n# feature-descriptor-1: Hu Moments\ndef fd_hu_moments(image):\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    feature = cv2.HuMoments(cv2.moments(image)).flatten()\n    return feature\n\n# feature-descriptor-2: Haralick Texture\ndef fd_haralick(image):\n    # convert the image to grayscale\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    # compute the haralick texture feature vector\n    haralick = mahotas.features.haralick(gray).mean(axis=0)\n    # return the result\n    return haralick\n\n# feature-descriptor-3: Color Histogram\ndef fd_histogram(image, mask=None):\n    # convert the image to HSV color-space\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n    # compute the color histogram\n    hist  = cv2.calcHist([image], [0, 1, 2], None, [bins, bins, bins], [0, 256, 0, 256, 0, 256])\n    # normalize the histogram\n    cv2.normalize(hist, hist)\n    # return the histogram\n    return hist.flatten()\n\n# get the training labels\ntrain_labels = os.listdir(train_path)\n\n# sort the training labels\ntrain_labels.sort()\nprint(train_labels)\n\n# empty lists to hold feature vectors and labels\nglobal_features = []\nlabels          = []\n\n# loop over the training data sub-folders\nfor training_name in train_labels:\n    # join the training data path and each species training folder\n    dir = os.path.join(train_path, training_name)\n\n    # get the current training label\n    current_label = training_name\n\n    # loop over the images in each sub-folder\n    for x in range(1,images_per_class+1):\n        # get the image file name\n        # file = dir + ""/"" + str(x) + "".jpg""\n        file = dir + ""/"" + ""{}_{}.jpg"".format(training_name, int(training_name)*100 + x)\n\n        # read the image and resize it to a fixed-size\n        image = cv2.imread(file)\n        image = cv2.resize(image, fixed_size)\n\n        ####################################\n        # Global Feature extraction\n        ####################################\n        fv_hu_moments = fd_hu_moments(image)\n        fv_haralick   = fd_haralick(image)\n        fv_histogram  = fd_histogram(image)\n\n        ###################################\n        # Concatenate global features\n        ###################################\n        global_feature = np.hstack([fv_histogram, fv_haralick, fv_hu_moments])\n\n        # update the list of labels and feature vectors\n        labels.append(current_label)\n        global_features.append(global_feature)\n\n    print(""[STATUS] processed folder: {}"".format(current_label))\n\nprint(""[STATUS] completed Global Feature Extraction..."")\n\n# get the overall feature vector size\nprint(""[STATUS] feature vector size {}"".format(np.array(global_features).shape))\n\n# get the overall training label size\nprint(""[STATUS] training Labels {}"".format(np.array(labels).shape))\n\n# encode the target labels\ntargetNames = np.unique(labels)\nle          = LabelEncoder()\ntarget      = le.fit_transform(labels)\nprint(""[STATUS] training labels encoded..."")\n\n# scale features in the range (0-1)\nscaler            = MinMaxScaler(feature_range=(0, 1))\nrescaled_features = scaler.fit_transform(global_features)\nprint(""[STATUS] feature vector normalized..."")\n\nprint(""[STATUS] target labels: {}"".format(target))\nprint(""[STATUS] target labels shape: {}"".format(target.shape))\n\n# save the feature vector using HDF5\nh5f_data = h5py.File(h5_data, \'w\')\nh5f_data.create_dataset(\'dataset_1\', data=np.array(rescaled_features))\n\nh5f_label = h5py.File(h5_labels, \'w\')\nh5f_label.create_dataset(\'dataset_1\', data=np.array(target))\n\nh5f_data.close()\nh5f_label.close()\n\nprint(""[STATUS] end of training.."")\n'"
train_test.py,5,"b'#-----------------------------------\n# TRAINING OUR MODEL\n#-----------------------------------\nimport h5py\nimport numpy as np\nimport os\nimport glob\nimport cv2\nimport warnings\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.externals import joblib\n\nwarnings.filterwarnings(\'ignore\')\n\n#--------------------\n# tunable-parameters\n#--------------------\nnum_trees = 100\ntest_size = 0.10\nseed      = 9\ntest_path = ""dataset/test""\nh5_data   = \'output/data.h5\'\nh5_labels = \'output/labels.h5\'\nscoring   = ""accuracy""\n\nif not os.path.exists(test_path):\n    os.makedirs(test_path)\n\n# create all the machine learning models\nmodels = []\nmodels.append((\'LR\', LogisticRegression(random_state=seed)))\nmodels.append((\'LDA\', LinearDiscriminantAnalysis()))\nmodels.append((\'KNN\', KNeighborsClassifier()))\nmodels.append((\'CART\', DecisionTreeClassifier(random_state=seed)))\nmodels.append((\'RF\', RandomForestClassifier(n_estimators=num_trees, random_state=seed)))\nmodels.append((\'NB\', GaussianNB()))\nmodels.append((\'SVM\', SVC(random_state=seed)))\n\n# variables to hold the results and names\nresults = []\nnames   = []\n\n# import the feature vector and trained labels\nh5f_data  = h5py.File(h5_data, \'r\')\nh5f_label = h5py.File(h5_labels, \'r\')\n\nglobal_features_string = h5f_data[\'dataset_1\']\nglobal_labels_string   = h5f_label[\'dataset_1\']\n\nglobal_features = np.array(global_features_string)\nglobal_labels   = np.array(global_labels_string)\n\nh5f_data.close()\nh5f_label.close()\n\n# verify the shape of the feature vector and labels\nprint(""[STATUS] features shape: {}"".format(global_features.shape))\nprint(""[STATUS] labels shape: {}"".format(global_labels.shape))\n\nprint(""[STATUS] training started..."")\n\n# split the training and testing data\n(trainDataGlobal, testDataGlobal, trainLabelsGlobal, testLabelsGlobal) = train_test_split(np.array(global_features),\n                                                                                          np.array(global_labels),\n                                                                                          test_size=test_size,\n                                                                                          random_state=seed)\n\nprint(""[STATUS] splitted train and test data..."")\nprint(""Train data  : {}"".format(trainDataGlobal.shape))\nprint(""Test data   : {}"".format(testDataGlobal.shape))\nprint(""Train labels: {}"".format(trainLabelsGlobal.shape))\nprint(""Test labels : {}"".format(testLabelsGlobal.shape))\n\n# 10-fold cross validation\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=seed)\n    cv_results = cross_val_score(model, trainDataGlobal, trainLabelsGlobal, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = ""%s: %f (%f)"" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n\n# boxplot algorithm comparison\nfig = pyplot.figure()\nfig.suptitle(\'Machine Learning algorithm comparison\')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\npyplot.show()\n\n#-----------------------------------\n# TESTING OUR MODEL\n#-----------------------------------\n\n# to visualize results\nimport matplotlib.pyplot as plt\n\n# create the model - Random Forests\nclf  = RandomForestClassifier(n_estimators=num_trees, random_state=seed)\n\n# fit the training data to the model\nclf.fit(trainDataGlobal, trainLabelsGlobal)\n\n# loop through the test images\nfor file in glob.glob(test_path + ""/*.jpg""):\n    # read the image\n    image = cv2.imread(file)\n\n    # resize the image\n    image = cv2.resize(image, fixed_size)\n\n    ####################################\n    # Global Feature extraction\n    ####################################\n    fv_hu_moments = fd_hu_moments(image)\n    fv_haralick   = fd_haralick(image)\n    fv_histogram  = fd_histogram(image)\n\n    ###################################\n    # Concatenate global features\n    ###################################\n    global_feature = np.hstack([fv_histogram, fv_haralick, fv_hu_moments])\n\n    # scale features in the range (0-1)\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    rescaled_feature = scaler.fit_transform(global_feature)\n\n    # predict label of test image\n    prediction = clf.predict(rescaled_feature.reshape(1,-1))[0]\n\n    # show predicted label on image\n    cv2.putText(image, train_labels[prediction], (20,30), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0,255,255), 3)\n\n    # display the output image\n    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n    plt.show()\n'"
