file_path,api_count,code
setup.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# flake8: noqa\n\nfrom setuptools import setup\nimport sys\nimport io\n\nwith io.open(\'README.rst\', encoding=\'utf-8\') as f:\n    long_description = f.read()\n\nwith open(\'bloscpack/version.py\') as f:\n    exec(f.read())\n\ninstall_requires = [\n    \'blosc\',\n    \'numpy\',\n    \'six\',\n    \'deprecated\',\n]\n\ntests_require = [\n    \'nose\',\n    \'cram>=0.6\',\n    \'mock\',\n    \'coverage\',\n    \'coveralls\',\n    \'twine\',\n    \'wheel\',\n]\n\nsetup(\n    name = ""bloscpack"",\n    version = __version__,\n    packages = [\'bloscpack\'],\n    entry_points = {\n        \'console_scripts\' : [\n            \'blpk = bloscpack.cli:main\',\n        ]\n    },\n    author = ""Valentin Haenel"",\n    author_email = ""valentin@haenel.co"",\n    description = ""Command line interface to and serialization format for Blosc"",\n    long_description=long_description,\n    license = ""MIT"",\n    keywords = (\'compression\', \'applied information theory\'),\n    url = ""https://github.com/blosc/bloscpack"",\n    install_requires = install_requires,\n    extras_require = dict(tests=tests_require),\n    tests_require = tests_require,\n    classifiers = [\'Development Status :: 4 - Beta\',\n                   \'Environment :: Console\',\n                   \'License :: OSI Approved :: MIT License\',\n                   \'Operating System :: Microsoft :: Windows\',\n                   \'Operating System :: POSIX\',\n                   \'Programming Language :: Python\',\n                   \'Topic :: Scientific/Engineering\',\n                   \'Topic :: System :: Archiving :: Compression\',\n                   \'Topic :: Utilities\',\n                   \'Programming Language :: Python\',\n                   \'Programming Language :: Python :: 2.7\',\n                   \'Programming Language :: Python :: 3.4\',\n                   \'Programming Language :: Python :: 3.5\',\n                   \'Programming Language :: Python :: 3.6\',\n                   \'Programming Language :: Python :: 3.7\',\n                  ],\n     )\n'"
bench/blpk_vs_gzip.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os.path as path\nimport os\nimport sys\nimport time\nimport subprocess\n\nimport bloscpack.pretty as bpp\nimport bloscpack.testutil as bpt\nfrom bloscpack import pack_file_to_file\n\nDROP_CACHES = False\n\n\ndef get_fs(file_name):\n    return bpp.pretty_size(path.getsize(file_name))\n\n\ndef get_ratio(file1, file2):\n    return path.getsize(file1)/path.getsize(file2)\n\n\ndef drop_caches():\n    if DROP_CACHES:\n        os.system(\'echo 3 > /proc/sys/vm/drop_caches\')\n\n\ndef am_root():\n    return os.geteuid() == 0\n\n\nif len(sys.argv) == 2 and sys.argv[1] in (\'-d\', \'--drop-caches\'):\n    if am_root():\n        print(\'will drop caches\')\n        DROP_CACHES = True\n    else:\n        print(\'error: need uid 0 (root) to drop caches\')\n        sys.exit(1)\n\nwith bpt.create_tmp_files() as (tdir, in_file, out_file, dcmp_file):\n    gz_out_file = path.join(tdir, \'file.gz\')\n\n    print(\'create the test data\', end=\'\')\n    bpt.create_array(100, in_file, progress=bpt.simple_progress)\n    print(\'\')\n\n    print(""Input file size: %s"" % get_fs(in_file))\n    drop_caches()\n\n    print(""Will now run bloscpack... "")\n    tic = time.time()\n    pack_file_to_file(in_file, out_file)\n    toc = time.time()\n    print(""Time: %.2f seconds"" % (toc - tic))\n    print(""Output file size: %s"" % get_fs(out_file))\n    print(""Ratio: %.2f"" % get_ratio(in_file, out_file))\n    drop_caches()\n\n    print(""Will now run gzip... "")\n    tic = time.time()\n    subprocess.call(\'gzip -c %s > %s\' % (in_file, gz_out_file), shell=True)\n    toc = time.time()\n    print(""Time: %.2f seconds"" % (toc - tic))\n    print(""Output file size: %s"" % get_fs(gz_out_file))\n    print(""Ratio: %.2f"" % get_ratio(in_file, gz_out_file))\n'"
bench/compression_time_vs_chunk_size.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n\nfrom __future__ import division\nfrom __future__ import print_function\n\n\nimport os.path as path\nimport time\n\n\nimport numpy\n\n\nimport bloscpack.testutil as bpt\nfrom bloscpack.sysutil import drop_caches, sync\nfrom bloscpack import pack_file_to_file, unpack_file_from_file\nfrom bloscpack.pretty import pretty_size\n\n\nwith bpt.create_tmp_files() as (tdir, in_file, out_file, dcmp_file):\n\n    print(\'create the test data\', end=\'\')\n    bpt.create_array(100, in_file, progress=bpt.simple_progress)\n    repeats = 3\n    print(""%s\\t%s\\t\\t%s\\t\\t%s"" %\n          (""chunk_size"", ""comp-time"", ""decomp-time"", ""ratio""))\n    for chunk_size in (int(2**i) for i in numpy.arange(19, 23.5, 0.5)):\n        cmp_times, dcmp_times = [], []\n        for _ in range(repeats):\n            drop_caches()\n            tic = time.time()\n            pack_file_to_file(in_file, out_file, chunk_size=chunk_size)\n            sync()\n            toc = time.time()\n            cmp_times.append(toc-tic)\n            drop_caches()\n            tic = time.time()\n            unpack_file_from_file(out_file, dcmp_file)\n            sync()\n            toc = time.time()\n            dcmp_times.append(toc-tic)\n        ratio = path.getsize(in_file)/path.getsize(out_file)\n        print(""%s\\t\\t%f\\t\\t%f\\t\\t%f"" %\n              (pretty_size(chunk_size),\n               sum(cmp_times)/repeats,\n               sum(dcmp_times)/repeats,\n               ratio,\n               )\n              )\n'"
bloscpack/__init__.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# vim :set ft=py:\n\n"""""" Command line interface to Blosc via python-blosc """"""\n\nfrom .version import __version__  # pragma: no cover\n\nfrom .args import (BloscArgs,\n                   BloscpackArgs,\n                   MetadataArgs,\n                   )\nfrom .file_io import (pack_file_to_file,\n                      unpack_file_from_file,\n                      pack_bytes_to_file,\n                      unpack_bytes_from_file,\n                      pack_bytes_to_bytes,\n                      unpack_bytes_from_bytes,\n                      )\n# deprecated\nfrom .file_io import (pack_file,\n                      unpack_file,\n                      pack_bytes_file,\n                      unpack_bytes_file,\n                      )\nfrom .numpy_io import (pack_ndarray_to_file,\n                       unpack_ndarray_from_file,\n                       pack_ndarray_to_bytes,\n                       unpack_ndarray_from_bytes,\n                       )\n# deprecated\nfrom .numpy_io import (pack_ndarray_file,\n                       unpack_ndarray_file,\n                       pack_ndarray_str,\n                       unpack_ndarray_str,\n                       )\n'"
bloscpack/abstract_io.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# vim :set ft=py:\n\n\nimport abc\n\n\nimport blosc\n\n\nfrom .args import (BloscArgs,\n                   BloscpackArgs,\n                   MetadataArgs,\n                   _handle_max_apps\n                   )\nfrom .headers import (BloscpackHeader,\n                      )\nfrom .exceptions import (ChecksumMismatch,\n                         )\nfrom .pretty import (double_pretty_size,\n                     )\nfrom . import log\n\n\ndef _compress_chunk_str(chunk, blosc_args):\n    return blosc.compress(chunk, **blosc_args)\n\n\nclass PlainSource(object):\n\n    _metaclass__ = abc.ABCMeta\n\n    def configure(self, chunk_size, last_chunk, nchunks):\n        self.chunk_size = chunk_size\n        self.last_chunk = last_chunk\n        self.nchunks = nchunks\n\n    @property\n    def compress_func(self):\n        return _compress_chunk_str\n\n    @abc.abstractmethod\n    def __iter__(self):\n        pass\n\n\nclass CompressedSource(object):\n\n    _metaclass__ = abc.ABCMeta\n\n    @abc.abstractmethod\n    def __iter__(self):\n        pass\n\n\nclass PlainSink(object):\n\n    _metaclass__ = abc.ABCMeta\n\n    @abc.abstractmethod\n    def put(self, chunk):\n        pass\n\n\nclass CompressedSink(object):\n\n    _metaclass__ = abc.ABCMeta\n\n    def configure(self, blosc_args, bloscpack_header):\n        self.blosc_args = blosc_args\n        self.bloscpack_header = bloscpack_header\n        self.checksum_impl = bloscpack_header.checksum_impl\n        self.offsets = bloscpack_header.offsets\n\n    @abc.abstractmethod\n    def write_bloscpack_header(self):\n        pass\n\n    @abc.abstractmethod\n    def write_metadata(self, metadata, metadata_args):\n        pass\n\n    @abc.abstractmethod\n    def init_offsets(self):\n        pass\n\n    @abc.abstractmethod\n    def finalize(self):\n        pass\n\n    @abc.abstractmethod\n    def put(self, i, compressed):\n        pass\n\n    def do_checksum(self, compressed):\n        if self.checksum_impl.size > 0:\n            # compute the checksum on the compressed data\n            digest = self.checksum_impl(compressed)\n            log.debug(\'checksum (%s): %s\' %\n                     (self.checksum_impl.name, repr(digest)))\n        else:\n            digest = \'\'\n            log.debug(\'no checksum\')\n        return digest\n\n\ndef pack(source, sink,\n         nchunks, chunk_size, last_chunk,\n         metadata=None,\n         blosc_args=None,\n         bloscpack_args=None,\n         metadata_args=None):\n    """""" Core packing function.  """"""\n\n    if not isinstance(source, PlainSource):\n        raise TypeError\n    if not isinstance(sink, CompressedSink):\n        raise TypeError\n\n    blosc_args = blosc_args or BloscArgs()\n    log.debug(blosc_args.pformat())\n    bloscpack_args = bloscpack_args or BloscpackArgs()\n    log.debug(bloscpack_args.pformat())\n    if metadata is not None:\n        metadata_args = metadata_args or MetadataArgs()\n        log.debug(metadata_args.pformat())\n    elif metadata_args is not None:\n        log.debug(\'metadata_args will be silently ignored\')\n\n    max_app_chunks = _handle_max_apps(bloscpack_args.offsets,\n            nchunks,\n            bloscpack_args.max_app_chunks)\n    # create the bloscpack header\n    bloscpack_header = BloscpackHeader(\n            offsets=bloscpack_args.offsets,\n            metadata=metadata is not None,\n            checksum=bloscpack_args.checksum,\n            typesize=blosc_args.typesize,\n            chunk_size=chunk_size,\n            last_chunk=last_chunk,\n            nchunks=nchunks,\n            max_app_chunks=max_app_chunks\n            )\n    log.debug(bloscpack_header.pformat())\n\n    source.configure(chunk_size, last_chunk, nchunks)\n    sink.configure(blosc_args, bloscpack_header)\n    sink.write_bloscpack_header()\n    if metadata is not None:\n        sink.write_metadata(metadata, metadata_args)\n    sink.init_offsets()\n\n    compress_func = source.compress_func\n    # read-compress-write loop\n    for i, chunk in enumerate(source):\n        if log.LEVEL == log.DEBUG:\n            log.debug(""Handle chunk \'%d\'%s"" %\n                    (i, \' (last)\' if i == nchunks - 1 else \'\'))\n        compressed = compress_func(chunk, blosc_args)\n        sink.put(i, compressed)\n        if log.LEVEL == log.DEBUG:\n            log.debug(""chunk handled, in: %s out: %s"" %\n                    (double_pretty_size(len(chunk)),\n                    double_pretty_size(len(compressed))))\n    sink.finalize()\n\n\ndef unpack(source, sink):\n    if not isinstance(source, CompressedSource):\n        raise TypeError\n    if not isinstance(sink, PlainSink):\n        raise TypeError\n    # read, decompress, write loop\n    for i, (compressed, digest) in enumerate(source):\n        if log.LEVEL == log.DEBUG:\n            log.debug(""decompressing chunk \'%d\'%s"" %\n                    (i, \' (last)\' if source.nchunks is not None\n                    and i == source.nchunks - 1 else \'\'))\n        if digest:\n            computed_digest = source.checksum_impl(compressed)\n            if digest != computed_digest:\n                raise ChecksumMismatch(\n                        ""Checksum mismatch detected in chunk, ""\n                        ""expected: \'%s\', received: \'%s\'"" %\n                        (repr(digest), repr(computed_digest)))\n            else:\n                if log.LEVEL == log.DEBUG:\n                    log.debug(\'checksum OK (%s): %s\' %\n                            (source.checksum_impl.name, repr(digest)))\n\n        len_decompressed = sink.put(compressed)\n        if log.LEVEL == log.DEBUG:\n            log.debug(""chunk handled, in: %s out: %s"" %\n                    (double_pretty_size(len(compressed)),\n                    double_pretty_size(len_decompressed)))\n'"
bloscpack/abstract_objects.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# vim :set ft=py:\n\n\nimport abc\nimport collections\nimport copy\nimport pprint\n\n\nfrom .pretty import (double_pretty_size,\n                     )\n\n\nclass MutableMappingObject(collections.MutableMapping):\n\n    _metaclass__ = abc.ABCMeta\n\n    @abc.abstractproperty\n    def attributes(self):\n        pass\n\n    @abc.abstractproperty\n    def bytes_attributes(self):\n        pass\n\n    @property\n    def _class_name(self):\n        return type(self).__name__\n\n    def __getitem__(self, key):\n        if key not in self.attributes:\n            raise KeyError(\'%s not in %s\' % (key, self._class_name))\n        return getattr(self, key)\n\n    def __setitem__(self, key, value):\n        if key not in self.attributes:\n            raise KeyError(\'%s not in %s\' % (key, self._class_name))\n        setattr(self, key, value)\n\n    def __delitem__(self, key):\n        raise NotImplementedError(\n            \'%s does not support __delitem__ or derivatives\'\n            % self._class_name)\n\n    def __len__(self):\n        return len(self.attributes)\n\n    def __iter__(self):\n        return iter(self.attributes)\n\n    def __str__(self):\n        return pprint.pformat(dict(self))\n\n    def __repr__(self):\n        args = ((""%s=%s"" % (arg, repr(value))) for arg, value in self.items())\n        return ""%s(%s)"" % (self._class_name, "", "".join(args))\n\n    def pformat(self, indent=4):\n        indent = "" "" * indent\n        # don\'t ask, was feeling functional\n        return ""%s:\\n%s%s"" % (self._class_name, indent, (""\\n%s"" % indent).join(((""%s: %s"" %\n            (key, (repr(value) if (key not in self.bytes_attributes or value == -1)\n                               else double_pretty_size(value)))\n             for key, value in self.items()))))\n\n    def copy(self):\n        return copy.copy(self)\n'"
bloscpack/append.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# vim :set ft=py:\n\n\nfrom __future__ import division\n\n\nimport os.path as path\nimport itertools\n\n\nimport blosc\n\n\nfrom .abstract_io import (_compress_chunk_str,\n                          )\nfrom .args import (BLOSC_ARGS,\n                   MetadataArgs,\n                   calculate_nchunks,\n                   _check_blosc_args,\n                   DEFAULT_META_CODEC,\n                   DEFAULT_META_LEVEL,\n                   METADATA_ARGS\n                   )\nfrom .checksums import (check_valid_checksum,\n                        CHECKSUMS_LOOKUP,\n                        )\nfrom .constants import (BLOSCPACK_HEADER_LENGTH,\n                        METADATA_HEADER_LENGTH,\n                        MAX_CLEVEL,\n                        )\nfrom .defaults import (DEFAULT_CLEVEL,\n                       DEFAULT_SHUFFLE,\n                       DEFAULT_CNAME,\n                       )\nfrom .exceptions import (NoMetadataFound,\n                         NoChangeInMetadata,\n                         ChecksumLengthMismatch,\n                         )\nfrom .file_io import (PlainFPSource,\n                      CompressedFPSink,\n                      _read_beginning,\n                      _read_compressed_chunk_fp,\n                      _write_offsets,\n                      _write_compressed_chunk,\n                      _read_bloscpack_header,\n                      _read_metadata,\n                      _write_metadata,\n                      )\nfrom .headers import check_range\nfrom .metacodecs import check_valid_codec\nfrom .serializers import check_valid_serializer\nfrom .exceptions import (NotEnoughSpace,\n                         NonUniformTypesize,\n                         )\nfrom .pretty import (double_pretty_size,\n                     )\nfrom . import log\n\n\ndef append_fp(original_fp, new_content_fp, new_size, blosc_args=None):\n    """""" Append from a file pointer to a file pointer.\n\n    Parameters\n    ----------\n    original : file_like\n        the original file_pointer\n    new_content : str\n        the file pointer for the data to be appended\n    new_size : int\n        the size of the new_content\n    blosc_args : dict\n        the blosc_args\n\n    Returns\n    -------\n    nchunks_written : int\n        the total number of new chunks written to the file\n\n    Notes\n    -----\n    The blosc_args argument can be supplied if different blosc arguments are\n    desired.\n\n    """"""\n    bloscpack_header, metadata, metadata_header, offsets = \\\n        _read_beginning(original_fp)\n    checksum_impl = bloscpack_header.checksum_impl\n    if not offsets:\n        raise RuntimeError(\'Appending to a file without offsets \'\n                           \'is not yet supported\')\n    if blosc_args is None:\n        blosc_args = dict(zip(BLOSC_ARGS, [None] * len(BLOSC_ARGS)))\n    # handle blosc_args\n    if blosc_args[\'typesize\'] is None:\n        if bloscpack_header.typesize == -1:\n            raise NonUniformTypesize(\'Non uniform type size, \'\n                                     \'can not append to file.\')\n        else:\n            # use the typesize from the bloscpack header\n            blosc_args[\'typesize\'] = bloscpack_header.typesize\n    if blosc_args[\'clevel\'] is None:\n        # use the default\n        blosc_args[\'clevel\'] = DEFAULT_CLEVEL\n    if blosc_args[\'shuffle\'] is None:\n        blosc_args[\'shuffle\'] = DEFAULT_SHUFFLE\n    if blosc_args[\'cname\'] is None:\n        blosc_args[\'cname\'] = DEFAULT_CNAME\n    _check_blosc_args(blosc_args)\n    offsets_pos = (BLOSCPACK_HEADER_LENGTH +\n                  (METADATA_HEADER_LENGTH + metadata_header[\'max_meta_size\'] +\n                      CHECKSUMS_LOOKUP[metadata_header[\'meta_checksum\']].size\n                   if metadata is not None else 0))\n    # seek to the final offset\n    original_fp.seek(offsets[-1], 0)\n    # decompress the last chunk\n    compressed, blosc_header, digest = _read_compressed_chunk_fp(original_fp,\n                                                         checksum_impl)\n    # TODO check digest\n    decompressed = blosc.decompress(compressed)\n    # figure out how many bytes we need to read to rebuild the last chunk\n    ultimo_length = len(decompressed)\n    bytes_to_read = bloscpack_header.chunk_size - ultimo_length\n    if new_size <= bytes_to_read:\n        # special case\n        # must squeeze data into last chunk\n        fill_up = new_content_fp.read(new_size)\n        # seek back to the position of the original last chunk\n        original_fp.seek(offsets[-1], 0)\n        # write the chunk that has been filled up\n        compressed = _compress_chunk_str(decompressed + fill_up, blosc_args)\n        digest = checksum_impl(compressed)\n        _write_compressed_chunk(original_fp, compressed, digest)\n        # return 0 to indicate that no new chunks have been written\n        # build the new header\n        bloscpack_header.last_chunk += new_size\n        # create the new header\n        raw_bloscpack_header = bloscpack_header.encode()\n        original_fp.seek(0)\n        original_fp.write(raw_bloscpack_header)\n        return 0\n\n    # figure out what is left over\n    new_new_size = new_size - bytes_to_read\n    # read those bytes\n    fill_up = new_content_fp.read(bytes_to_read)\n    # figure out how many chunks we will need\n    nchunks, chunk_size, last_chunk_size = \\\n            calculate_nchunks(new_new_size,\n                chunk_size=bloscpack_header.chunk_size)\n    # make sure that we actually have that kind of space\n    if nchunks > bloscpack_header.max_app_chunks:\n        raise NotEnoughSpace(\'not enough space\')\n    # seek back to the position of the original last chunk\n    original_fp.seek(offsets[-1], 0)\n    # write the chunk that has been filled up\n    compressed = _compress_chunk_str(decompressed + fill_up, blosc_args)\n    digest = checksum_impl(compressed)\n    _write_compressed_chunk(original_fp, compressed, digest)\n    # append to the original file, again original_fp should be adequately\n    # positioned\n    sink = CompressedFPSink(original_fp)\n    sink.configure(blosc_args, bloscpack_header)\n    # allocate new offsets\n    sink.offset_storage = list(itertools.repeat(-1, nchunks))\n    # read from the new input file, new_content_fp should be adequately\n    # positioned\n    source = PlainFPSource(new_content_fp)\n    source.configure(chunk_size, last_chunk_size, nchunks)\n    # read, compress, write loop\n    for i, chunk in enumerate(source):\n        log.debug(""Handle chunk \'%d\' %s"" % (i,\'(last)\' if i == nchunks -1\n            else \'\'))\n\n        compressed = _compress_chunk_str(chunk, blosc_args)\n        sink.put(i, compressed)\n\n    # build the new header\n    bloscpack_header.last_chunk = last_chunk_size\n    bloscpack_header.nchunks += nchunks\n    bloscpack_header.max_app_chunks -= nchunks\n    # create the new header\n    raw_bloscpack_header = bloscpack_header.encode()\n    original_fp.seek(0)\n    original_fp.write(raw_bloscpack_header)\n    # write the new offsets, but only those that changed\n    original_fp.seek(offsets_pos)\n    # FIXME: write only those that changed\n    _write_offsets(sink.output_fp, offsets + sink.offset_storage)\n    return nchunks\n\n\ndef append(orig_file, new_file, blosc_args=None):\n    """""" Append from a file pointer to a file pointer.\n\n    Parameters\n    ----------\n    orig_file : str\n        the name of the file to append to\n    new_file : str\n        the name of the file to append from\n    blosc_args : dict\n        the blosc_args\n\n    Notes\n    -----\n    The blosc_args argument can be supplied if different blosc arguments are\n    desired.\n    """"""\n    orig_size_before = path.getsize(orig_file)\n    new_size = path.getsize(new_file)\n    log.verbose(\'orig file size before append: %s\' %\n            double_pretty_size(orig_size_before))\n    log.verbose(\'new file size: %s\' % double_pretty_size(new_size))\n\n    with open(orig_file, \'r+b\') as orig_fp, open(new_file, \'rb\') as new_fp:\n        append_fp(orig_fp, new_fp, new_size, blosc_args)\n    orig_size_after = path.getsize(orig_file)\n    log.verbose(\'orig file size after append: %s\' %\n            double_pretty_size(orig_size_after))\n    log.verbose(\'Approximate compression ratio of appended data: %f\' %\n            ((orig_size_after-orig_size_before)/new_size))\n\n\ndef _seek_to_metadata(target_fp):\n    """""" Given a target file pointer, seek to the metadata section.\n\n    Parameters\n    ----------\n\n    target_fp : file like\n        the target file pointer\n\n    Returns\n    -------\n    metadata_position : int\n\n    Raises\n    ------\n    NoMetadataFound\n        if there is no metadata section in this file\n\n    """"""\n    bloscpack_header = _read_bloscpack_header(target_fp)\n    if not bloscpack_header.metadata:\n        raise NoMetadataFound(""unable to seek to metadata if it does not exist"")\n    else:\n        return target_fp.tell()\n\n\ndef _rewrite_metadata_fp(target_fp, new_metadata,\n                         magic_format=None, checksum=None,\n                         codec=DEFAULT_META_CODEC, level=DEFAULT_META_LEVEL):\n    """""" Rewrite the metadata section in a file pointer.\n\n    Parameters\n    ----------\n    target_fp : file like\n        the target file pointer to rewrite in\n    new_metadata: dict\n        the new metadata to save\n\n    See the notes in ``_recreate_metadata`` for a description of the keyword\n    arguments.\n\n    """"""\n    # cache the current position\n    current_pos = target_fp.tell()\n    # read the metadata section\n    old_metadata, old_metadata_header = _read_metadata(target_fp)\n    if old_metadata == new_metadata:\n        raise NoChangeInMetadata(\n                \'you requested to update metadata, but this has not changed\')\n    new_metadata_args = _recreate_metadata(old_metadata_header, new_metadata,\n            magic_format=magic_format, checksum=checksum,\n            codec=codec, level=level)\n    # seek back to where the metadata begins...\n    target_fp.seek(current_pos, 0)\n    # and re-write it\n    _write_metadata(target_fp, new_metadata, new_metadata_args)\n\n\ndef _recreate_metadata(old_metadata_header, new_metadata,\n                       magic_format=None, checksum=None,\n                       codec=DEFAULT_META_CODEC, level=DEFAULT_META_LEVEL):\n    """""" Update the metadata section.\n\n    Parameters\n    ----------\n    old_metadata_header: dict\n        the header of the old metadata\n    new_metadata: dict\n        the new metadata to save\n\n    See the notes below for a description of the keyword arguments.\n\n    Returns\n    -------\n    new_metadata_args: dict\n        the new arguments for ``_write_metadata``\n\n    Raises\n    ------\n    ChecksumLengthMismatch\n        if the new checksum has a different length than the old one\n    NoChangeInMetadata\n        if the metadata has not changed\n\n    Notes\n    -----\n    This create new ``metadata_args`` based on an old metadata_header, Since\n    the space has already been allocated, only certain metadata arguments can\n    be overridden. The keyword arguments specify which ones these are. If a\n    keyword argument value is \'None\' the existing argument which is obtained\n    from the header is used.  Otherwise the value from the keyword argument\n    takes precedence. Due to a policy of opportunistic compression, the \'codec\'\n    and \'level\' arguments are not \'None\' by default, to ensure that previously\n    uncompressed metadata, which might be favourably compressible as a result\n    of the enlargement process, will actually be compressed. As for the\n    \'checksum\' only a checksum with the same digest size can be used.\n\n    The ``metadata_args`` returned by this function are suitable to be passed\n    on to ``_write_metadata``.\n\n    """"""\n    # get the settings from the metadata header\n    metadata_args = MetadataArgs(**dict((k, old_metadata_header[k])\n                                        for k in METADATA_ARGS))\n    # handle and check validity of overrides\n    if magic_format is not None:\n        check_valid_serializer(magic_format)\n        metadata_args.magic_format = magic_format\n    if checksum is not None:\n        check_valid_checksum(checksum)\n        old_impl = CHECKSUMS_LOOKUP[old_metadata_header.meta_checksum]\n        new_impl = CHECKSUMS_LOOKUP[checksum]\n        if old_impl.size != new_impl.size:\n            raise ChecksumLengthMismatch(\n                    \'checksums have a size mismatch\')\n        metadata_args.meta_checksum = checksum\n    if codec is not None:\n        check_valid_codec(codec)\n        metadata_args.meta_codec = codec\n    if level is not None:\n        check_range(\'meta_level\', level, 0, MAX_CLEVEL)\n        metadata_args.meta_level = level\n    return metadata_args\n'"
bloscpack/args.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# vim :set ft=py:\n\n\nimport blosc\nfrom six import integer_types, string_types\n\n\nfrom .abstract_objects import (MutableMappingObject,\n                               )\nfrom .headers import (check_range,\n                      )\nfrom .defaults import (DEFAULT_TYPESIZE,\n                       DEFAULT_CLEVEL,\n                       DEFAULT_SHUFFLE,\n                       DEFAULT_CNAME,\n                       DEFAULT_CHUNK_SIZE,\n                       DEFAULT_CHECKSUM,\n                       DEFAULT_MAX_APP_CHUNKS,\n                       DEFAULT_OFFSETS,\n                       DEFAULT_MAGIC_FORMAT,\n                       DEFAULT_META_CHECKSUM,\n                       DEFAULT_META_CODEC,\n                       DEFAULT_META_LEVEL,\n                       DEFAULT_MAX_META_SIZE,\n                       )\nfrom .exceptions import (ChunkingException,\n                         )\nfrom .headers import (MAX_CHUNKS,\n                      )\nfrom .metacodecs import (CODECS_AVAIL,\n                         CODECS_LOOKUP,\n                         )\nfrom .pretty import (double_pretty_size,\n                     reverse_pretty,\n                     )\nfrom . import log\n\n# Bloscpack args\nBLOSCPACK_ARGS = (\'offsets\', \'checksum\', \'max_app_chunks\')\n_BLOSCPACK_ARGS_SET = set(BLOSCPACK_ARGS)  # cached\nDEFAULT_BLOSCPACK_ARGS = dict(zip(BLOSCPACK_ARGS,\n    (DEFAULT_OFFSETS, DEFAULT_CHECKSUM, DEFAULT_MAX_APP_CHUNKS)))\n\n\n# Blosc args\nBLOSC_ARGS = (\'typesize\', \'clevel\', \'shuffle\', \'cname\')\n_BLOSC_ARGS_SET = set(BLOSC_ARGS)  # cached\nDEFAULT_BLOSC_ARGS = dict(zip(BLOSC_ARGS,\n    (DEFAULT_TYPESIZE, DEFAULT_CLEVEL, DEFAULT_SHUFFLE, DEFAULT_CNAME)))\n\n\n# metadata args\nMETADATA_ARGS = (\'magic_format\', \'meta_checksum\',\n                 \'meta_codec\', \'meta_level\', \'max_meta_size\')\n_METADATA_ARGS_SET = set(METADATA_ARGS)  # cached\nDEFAULT_METADATA_ARGS = dict(zip(METADATA_ARGS,\n    (DEFAULT_MAGIC_FORMAT, DEFAULT_META_CHECKSUM,\n     DEFAULT_META_CODEC, DEFAULT_META_LEVEL, DEFAULT_MAX_META_SIZE)))\n\n\ndef _check_blosc_args(blosc_args):\n    """""" Check the integrity of the blosc arguments dict.\n\n    Parameters\n    ----------\n    blosc_args : dict\n        blosc args dictionary\n\n    Raises\n    ------\n    ValueError\n        if there are missing or unexpected keys present\n\n    Notes\n    -----\n    Check the value of the \'BLOSC_ARGS\' constant for the details of what\n    keys should be contained in the dictionary.\n\n    """"""\n    __check_args(\'blosc\', blosc_args, _BLOSC_ARGS_SET)\n\n\ndef _check_bloscpack_args(bloscpack_args):\n    """""" Check the integrity of the bloscpack arguments dict.\n\n    Parameters\n    ----------\n    bloscpack_args : dict\n        blosc args dictionary\n\n    Raises\n    ------\n    ValueError\n        if there are missing or unexpected keys present\n\n    Notes\n    -----\n    Check the value of the \'BLOSCPACK_ARGS\' constant for the details of what\n    keys should be contained in the dictionary.\n\n    """"""\n    __check_args(\'bloscpack\', bloscpack_args, _BLOSCPACK_ARGS_SET)\n\n\ndef _check_metadata_arguments(metadata_args):\n    """""" Check the integrity of the metadata arguments dict.\n\n    Parameters\n    ----------\n    metadata_args : dict\n        metadata args dictionary\n\n    Raises\n    ------\n    ValueError\n        if there are missing or unexpected keys present\n\n    Notes\n    -----\n    Check the value of the \'METADATA_ARGS\' constant for the details of what\n    keys should be contained in the dictionary.\n\n    """"""\n    __check_args(\'metadata\', metadata_args, _METADATA_ARGS_SET)\n\n\ndef __check_args(name, received, expected):\n    """""" Check an arg dict.\n\n    Parameters\n    ----------\n    name : str\n        the name of the arg dict\n    received : dict\n        the arg dict received\n    expected : set of str\n        the keys that should have been contained\n    """"""\n\n    received = set(received.keys())\n    missing = expected.difference(received)\n    if len(missing) != 0:\n        raise ValueError(""%s args was missing: \'%s\'"" % (name, repr(missing)))\n    extra = received.difference(expected)\n    if len(extra) != 0:\n        raise ValueError(""%s args had some extras: \'%s\'"" % (name, repr(extra)))\n\n\ndef calculate_nchunks(in_file_size, chunk_size=DEFAULT_CHUNK_SIZE):\n    """""" Determine chunking for an input file.\n\n    Parameters\n    ----------\n    in_file_size : int\n        the size of the input file\n    chunk_size : int or str\n        the desired chunk size\n\n    Returns\n    -------\n    nchunks, chunk_size, last_chunk_size\n\n    nchunks : int\n        the number of chunks\n    chunk_size : int\n        the size of each chunk in bytes\n    last_chunk_size : int\n        the size of the last chunk in bytes\n\n    Raises\n    ------\n    ChunkingException\n        if the resulting nchunks is larger than MAX_CHUNKS\n\n    """"""\n    if in_file_size < 0:\n            raise ValueError(""\'in_file_size\' must be strictly positive, not %d""\n                             % in_file_size)\n    elif in_file_size == 0:\n        return (1, 0, 0)\n        log.verbose(""Input was length zero, ignoring \'chunk_size\'"")\n    # convert a human readable description to an int\n    if isinstance(chunk_size, string_types):\n        chunk_size = reverse_pretty(chunk_size)\n    check_range(\'chunk_size\', chunk_size, 1, blosc.BLOSC_MAX_BUFFERSIZE)\n    # downcast\n    if chunk_size > in_file_size:\n        log.verbose(\n            ""Input was smaller than the given \'chunk_size\': %s using: %s""\n            % (double_pretty_size(chunk_size),\n               double_pretty_size(in_file_size)))\n        chunk_size = in_file_size\n    quotient, remainder = divmod(in_file_size, chunk_size)\n    # the user wants a single chunk\n    if chunk_size == in_file_size:\n        nchunks = 1\n        chunk_size = in_file_size\n        last_chunk_size = in_file_size\n    # no remainder, perfect fit\n    elif remainder == 0:\n        nchunks = quotient\n        last_chunk_size = chunk_size\n    # with a remainder\n    else:\n        nchunks = quotient + 1\n        last_chunk_size = remainder\n    if nchunks > MAX_CHUNKS:\n        raise ChunkingException(\n            ""nchunks: \'%d\' is greater than the MAX_CHUNKS: \'%d\'"" %\n            (nchunks, MAX_CHUNKS))\n    log.verbose(\'nchunks: %d\' % nchunks)\n    log.verbose(\'chunk_size: %s\' % double_pretty_size(chunk_size))\n    log.verbose(\'last_chunk_size: %s\' % double_pretty_size(last_chunk_size))\n    return nchunks, chunk_size, last_chunk_size\n\n\ndef _handle_max_apps(offsets, nchunks, max_app_chunks):\n    """""" Process and handle the \'max_app_chunks\' argument\n\n    Parameters\n    ----------\n    offsets: bool\n        if the offsets to the chunks are present\n    nchunks : int\n        the number of chunks\n    max_app_chunks : callable or int\n        the total number of possible append chunks\n\n    Returns\n    -------\n    max_app_chunks : int\n        the int value\n\n    Raises\n    ------\n    TypeError\n        if \'max_app_chunks\' is neither a callable or an int\n    ValueError\n        if \'max_app_chunks\' is a callable and returned either a non-int or a\n        negative int.\n\n    Notes\n    -----\n    The \'max_app_chunks\' parameter can either be a function of \'nchunks\'\n    (callable that takes a single int as argument and returns a single int) or\n    an int.  The sum of the resulting value and \'nchunks\' can not be larger\n    than MAX_CHUNKS.  The value of \'max_app_chunks\' must be \'0\' if there is not\n    offsets section or if nchunks is unknown (has the value \'-1\').\n\n    The function performs some silent optimisations. First, if there are no\n    offsets or \'nchunks\' is unknown any value for \'max_app_chunks\' will be\n    silently ignored. Secondly, if the resulting value of max_app_chunks would\n    be too large, i.e. the sum of \'nchunks\' and \'max_app_chunks\' is larger than\n    \'MAX_CHUNKS\', then \'max_app_chunks\' is automatically set to the maximum\n    permissible value.\n\n    """"""\n    # first check that the args are actually valid\n    check_range(\'nchunks\',        nchunks,       -1, MAX_CHUNKS)\n    # then check that we actually need to evaluate it\n    if offsets and nchunks != -1:\n        if hasattr(max_app_chunks, \'__call__\'):\n            # it\'s a callable all right\n            log.debug(""max_app_chunks is a callable"")\n            max_app_chunks = max_app_chunks(nchunks)\n            if not isinstance(max_app_chunks, integer_types):\n                raise ValueError(\n                        ""max_app_chunks callable returned a non integer ""\n                        ""of type \'%s\'"" % type(max_app_chunks))\n            # check that the result is still positive, might be quite large\n            if max_app_chunks < 0:\n                raise ValueError(\n                        \'max_app_chunks callable returned a negative integer\')\n        elif isinstance(max_app_chunks, integer_types):\n            # it\'s a plain int, check its range\n            log.debug(""max_app_chunks is an int"")\n            check_range(\'max_app_chunks\', max_app_chunks, 0, MAX_CHUNKS)\n        else:\n            raise TypeError(\'max_app_chunks was neither a callable or an int\')\n        # we managed to get a reasonable value, make sure it\'s not too large\n        if nchunks + max_app_chunks > MAX_CHUNKS:\n            max_app_chunks = MAX_CHUNKS - nchunks\n            log.debug(\n                    ""max_app_chunks was too large, setting to max value: %d""\n                    % max_app_chunks)\n    else:\n        if max_app_chunks is not None:\n            log.debug(\'max_app_chunks will be silently ignored\')\n        max_app_chunks = 0\n    log.debug(""max_app_chunks was set to: %d"" % max_app_chunks)\n    return max_app_chunks\n\n\nclass BloscArgs(MutableMappingObject):\n    """""" Object to hold Blosc arguments.\n\n    Parameters\n    ----------\n    typesize : int\n        The typesize used\n    clevel : int\n        Compression level\n    shuffle : boolean\n        Whether or not to activate the shuffle filter\n    cname: str\n        Name of the internal code to use\n\n    """"""\n\n    def __init__(self,\n                 typesize=DEFAULT_TYPESIZE,\n                 clevel=DEFAULT_CLEVEL,\n                 shuffle=DEFAULT_SHUFFLE,\n                 cname=DEFAULT_CNAME):\n        self.typesize = typesize\n        self.clevel = clevel\n        self.shuffle = shuffle\n        self.cname = cname\n\n        self._attrs = [\'typesize\',\n                       \'clevel\',\n                       \'shuffle\',\n                       \'cname\',\n                       ]\n\n    @property\n    def attributes(self):\n        return self._attrs\n\n    @property\n    def bytes_attributes(self):\n        return []\n\n\nclass BloscpackArgs(MutableMappingObject):\n    """""" Object to hold BloscPack arguments.\n\n    Parameters\n    ----------\n    offsets : boolean\n        Whether to include space for offsets\n    checksum : str\n        Name of the checksum to use or None/\'None\'\n    max_app_chunks : int or callable on number of chunks\n        How much space to reserve in the offsets for chunks to be appended.\n\n    """"""\n    def __init__(self,\n                 offsets=DEFAULT_OFFSETS,\n                 checksum=DEFAULT_CHECKSUM,\n                 max_app_chunks=DEFAULT_MAX_APP_CHUNKS):\n        self.offsets = offsets\n        # Special hack, accept Pythonic None as \'None\'.\n        self.checksum = \'None\' if checksum is None else checksum\n        self.max_app_chunks = max_app_chunks\n\n        self._attrs = [\'offsets\',\n                       \'checksum\',\n                       \'max_app_chunks\',\n                       ]\n\n    @property\n    def attributes(self):\n        return self._attrs\n\n    @property\n    def bytes_attributes(self):\n        return []\n\n\nclass MetadataArgs(MutableMappingObject):\n    """""" Object to hold the metadata arguments.\n\n    Parameters\n    ----------\n    magic_format : 8 bytes\n        Format identifier for the metadata\n    meta_checksum : str\n        Checksum to be used for the metadata\n    meta_codec : str\n        Codec to be used to compress the metadata\n    meta_level : int\n        Compression level for metadata\n    max_meta_size : int or callable on metadata size\n        How much space to reserve for additional metadata\n\n    """"""\n\n    def __init__(self,\n                 magic_format=DEFAULT_MAGIC_FORMAT,\n                 meta_checksum=DEFAULT_META_CHECKSUM,\n                 meta_codec=DEFAULT_META_CODEC,\n                 meta_level=DEFAULT_META_LEVEL,\n                 max_meta_size=DEFAULT_MAX_META_SIZE,\n                 ):\n        self.magic_format = magic_format\n        self.meta_checksum = meta_checksum\n        self.meta_codec = meta_codec\n        self.meta_level = meta_level\n        self.max_meta_size = max_meta_size\n\n        self._attrs = [\'magic_format\',\n                       \'meta_checksum\',\n                       \'meta_codec\',\n                       \'meta_level\',\n                       \'max_meta_size\',\n                       ]\n\n    @property\n    def should_compress(self):\n        return self.meta_codec != CODECS_AVAIL[0]\n\n    def nullify_codec(self):\n        self.meta_codec = CODECS_AVAIL[0]\n\n    @property\n    def meta_codec_impl(self):\n        return CODECS_LOOKUP[self.meta_codec]\n\n    @property\n    def meta_codec_name(self):\n        return self.meta_codec_impl.name\n\n    def effective_max_meta_size(self, meta_size):\n        if hasattr(self.max_meta_size, \'__call__\'):\n            max_meta_size = self.max_meta_size(meta_size)\n        elif isinstance(self.max_meta_size, integer_types):\n            max_meta_size = self.max_meta_size\n        log.debug(\'max meta size is deemed to be: %d\' % max_meta_size)\n        return max_meta_size\n\n    @property\n    def attributes(self):\n        return self._attrs\n\n    @property\n    def bytes_attributes(self):\n        return []\n'"
bloscpack/checksums.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# vim :set ft=py:\n\n\nimport hashlib\nimport struct\nimport zlib\n\n\nfrom .exceptions import (NoSuchChecksum,\n                         )\n\n\nclass Hash(object):\n    """""" Uniform hash object.\n\n    Parameters\n    ----------\n    name : str\n        the name of the hash\n    size : int\n        the length of the digest in bytes\n    function : callable\n        the hash function implementation\n\n    Notes\n    -----\n    The \'function\' argument should return the raw bytes as string.\n\n    """"""\n\n    def __init__(self, name, size, function):\n        self.name, self.size, self._function = name, size, function\n\n    def __call__(self, data):\n        return self._function(data)\n\n\ndef zlib_hash(func):\n    """""" Wrapper for zlib hashes. """"""\n    def hash_(data):\n        # The binary OR is recommended to obtain uniform hashes on all python\n        # versions and platforms. The result of the checksum is a \'uint32\'\n        # https://docs.python.org/3.4/library/zlib.html\n        return struct.pack(\'<I\', func(data) & 0xffffffff)\n    return 4, hash_\n\n\ndef hashlib_hash(func):\n    """""" Wrapper for hashlib hashes. """"""\n    def hash_(data):\n        return func(data).digest()\n    return func().digest_size, hash_\n\n\nCHECKSUMS = [Hash(\'None\', 0, lambda data: b\'\'),\n             Hash(\'adler32\', *zlib_hash(zlib.adler32)),\n             Hash(\'crc32\', *zlib_hash(zlib.crc32)),\n             Hash(\'md5\', *hashlib_hash(hashlib.md5)),\n             Hash(\'sha1\', *hashlib_hash(hashlib.sha1)),\n             Hash(\'sha224\', *hashlib_hash(hashlib.sha224)),\n             Hash(\'sha256\', *hashlib_hash(hashlib.sha256)),\n             Hash(\'sha384\', *hashlib_hash(hashlib.sha384)),\n             Hash(\'sha512\', *hashlib_hash(hashlib.sha512)),\n             ]\nCHECKSUMS_AVAIL = [c.name for c in CHECKSUMS]\nCHECKSUMS_LOOKUP = dict(((c.name, c) for c in CHECKSUMS))\n\n\ndef check_valid_checksum(checksum):\n    """""" Check the validity of a checksum.\n\n    Parameters\n    ----------\n    checksum : str\n        the string descriptor of the checksum\n\n    Raises\n    ------\n    NoSuchChecksum\n        if no such checksum exists.\n    """"""\n    if checksum not in CHECKSUMS_AVAIL:\n        raise NoSuchChecksum(""checksum \'%s\' does not exist"" % checksum)\n'"
bloscpack/cli.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# vim :set ft=py:\n\n\nimport argparse\nfrom os import path\nimport json\nimport pprint\n\n\nimport blosc\n\n\nfrom .args import (BloscArgs,\n                   BloscpackArgs,\n                   MetadataArgs,\n                   )\nfrom .append import (append,\n                     _seek_to_metadata,\n                     _rewrite_metadata_fp\n                     )\nfrom .checksums import (CHECKSUMS_AVAIL,\n                        )\nfrom .constants import (SUFFIXES,\n                        CNAME_AVAIL,\n                        EXTENSION,\n                        MIN_CLEVEL,\n                        MAX_CLEVEL,\n                        )\nfrom .defaults import (DEFAULT_TYPESIZE,\n                       DEFAULT_CLEVEL,\n                       DEFAULT_SHUFFLE,\n                       DEFAULT_CNAME,\n                       DEFAULT_CHUNK_SIZE,\n                       DEFAULT_CHECKSUM,\n                       DEFAULT_OFFSETS,\n                       )\nfrom .exceptions import (FileNotFound,\n                         ChunkingException,\n                         FormatVersionMismatch,\n                         ChecksumMismatch,\n                         )\nfrom .file_io import (pack_file_to_file,\n                      unpack_file_from_file,\n                      _read_beginning,\n                      _read_compressed_chunk_fp,\n                      )\nfrom .headers import (decode_blosc_flags,\n                      )\nfrom .pretty import (reverse_pretty,\n                     join_with_eol,\n                     )\nfrom .version import __version__\nfrom . import log\n\n\ndef check_files(in_file, out_file, args):\n    """""" Check files exist/don\'t exist.\n\n    Parameters\n    ----------\n    in_file : str:\n        the input file\n    out_file : str\n        the output file\n    args : parser args\n        any additional arguments from the parser\n\n    Raises\n    ------\n    FileNotFound\n        in case any of the files isn\'t found.\n\n    """"""\n    if not path.exists(in_file):\n        raise FileNotFound(""input file \'%s\' does not exist!"" % in_file)\n    if path.exists(out_file):\n        if not args.force:\n            raise FileNotFound(""output file \'%s\' exists!"" % out_file)\n        else:\n            log.verbose(""overwriting existing file: \'%s\'"" % out_file)\n    log.verbose(""input file is: \'%s\'"" % in_file)\n    log.verbose(""output file is: \'%s\'"" % out_file)\n\n\ndef _blosc_args_from_args(args):\n    return BloscArgs(typesize=args.typesize,\n                     clevel=args.clevel,\n                     shuffle=args.shuffle,\n                     cname=args.cname,\n                     )\n\n\ndef process_compression_args(args):\n    """""" Extract and check the compression args after parsing by argparse.\n\n    Parameters\n    ----------\n    args : argparse.Namespace\n        the parsed command line arguments\n\n    Returns\n    -------\n    in_file : str\n        the input file name\n    out_file : str\n        the out_file name\n    blosc_args : tuple of (int, int, bool)\n        typesize, clevel and shuffle\n    """"""\n    in_file = args.in_file\n    out_file = args.out_file or in_file + EXTENSION\n    return in_file, out_file, _blosc_args_from_args(args)\n\n\ndef process_decompression_args(args):\n    """""" Extract and check the decompression args after parsing by argparse.\n\n    Warning: may call sys.exit()\n\n    Parameters\n    ----------\n    args : argparse.Namespace\n        the parsed command line arguments\n\n    Returns\n    -------\n    in_file : str\n        the input file name\n    out_file : str\n        the out_file name\n    """"""\n    in_file = args.in_file\n    out_file = args.out_file\n    # remove the extension for output file\n    if args.no_check_extension:\n        if out_file is None:\n            log.error(\'--no-check-extension requires use of <out_file>\')\n    else:\n        if in_file.endswith(EXTENSION):\n            out_file = args.out_file or in_file[:-len(EXTENSION)]\n        else:\n            log.error(""input file \'%s\' does not end with \'%s\'"" %\n                      (in_file, EXTENSION))\n    return in_file, out_file\n\n\ndef process_append_args(args):\n    original_file = args.original_file\n    new_file = args.new_file\n    if not args.no_check_extension and not original_file.endswith(EXTENSION):\n        log.error(""original file \'%s\' does not end with \'%s\'"" %\n                  (original_file, EXTENSION))\n\n    return original_file, new_file\n\n\ndef process_metadata_args(args):\n    if args.metadata is not None:\n        try:\n            with open(args.metadata, \'r\') as metadata_file:\n                return json.loads(metadata_file.read().strip())\n        except IOError as ioe:\n            log.error(ioe.message)\n\n\ndef process_nthread_arg(args):\n    """""" Extract and set nthreads. """"""\n    if args.nthreads != blosc.ncores:\n        blosc.set_nthreads(args.nthreads)\n    log.verbose(\'using %d thread%s\' %\n                (args.nthreads, \'s\' if args.nthreads > 1 else \'\'))\n\n\ndef log_metadata(metadata):\n    log.normal(""Metadata:"")\n    log.normal(pprint.pformat(metadata, width=90))\n\n\nclass BloscPackCustomFormatter(argparse.HelpFormatter):\n    """""" Custom HelpFormatter.\n\n    Basically a combination and extension of ArgumentDefaultsHelpFormatter and\n    RawTextHelpFormatter. Adds default values to argument help, but only if the\n    default is not in [None, True, False]. Also retains all whitespace as it\n    is.\n\n    """"""\n    def _get_help_string(self, action):\n        help_ = action.help\n        if \'%(default)\' not in action.help \\\n                and action.default not in \\\n                [argparse.SUPPRESS, None, True, False]:\n            defaulting_nargs = [argparse.OPTIONAL, argparse.ZERO_OR_MORE]\n            if action.option_strings or action.nargs in defaulting_nargs:\n                help_ += \' (default: %(default)s)\'\n        return help_\n\n    def _split_lines(self, text, width):\n        return text.splitlines()\n\n    def _fill_text(self, text, width, indent):\n        return \'\'.join([indent + line for line in text.splitlines(True)])\n\n\ndef _inject_blosc_group(parser):\n    blosc_group = parser.add_argument_group(title=\'blosc settings\')\n    blosc_group.add_argument(\'-t\', \'--typesize\',\n                             metavar=\'<size>\',\n                             default=DEFAULT_TYPESIZE,\n                             type=int,\n                             help=\'typesize for blosc\')\n    blosc_group.add_argument(\'-l\', \'--clevel\',\n                             default=DEFAULT_CLEVEL,\n                             choices=range(MIN_CLEVEL, MAX_CLEVEL+1),\n                             metavar=\'[0, 9]\',\n                             type=int,\n                             help=\'compression level\')\n    blosc_group.add_argument(\'-s\', \'--no-shuffle\',\n                             action=\'store_false\',\n                             default=DEFAULT_SHUFFLE,\n                             dest=\'shuffle\',\n                             help=\'deactivate shuffle\')\n    blosc_group.add_argument(\'-c\', \'--codec\',\n                             metavar=\'<codec>\',\n                             type=str,\n                             choices=CNAME_AVAIL,\n                             default=DEFAULT_CNAME,\n                             dest=\'cname\',\n                             help=""codec to be used by Blosc: \\n%s""\n                                  % join_with_eol(CNAME_AVAIL))\n\n\ndef create_parser():\n    """""" Create and return the parser. """"""\n    parser = argparse.ArgumentParser(\n            #usage=\'%(prog)s [GLOBAL_OPTIONS] (compress | decompress)\n            # [COMMAND_OPTIONS] <in_file> [<out_file>]\',\n            description=\'command line de/compression with blosc\',\n            formatter_class=BloscPackCustomFormatter,\n            epilog=""Additional help for subcommands is available:\\n""+\n            ""  %(prog)s \'subcommand\' [ -h | --help ]"")\n\n    ## print version of bloscpack, python-blosc and blosc itself\n    version_str = ""bloscpack: \'%s\' "" % __version__ + \\\n                  ""python-blosc: \'%s\' "" % blosc.version.__version__ + \\\n                  ""blosc: \'%s\'"" % blosc.BLOSC_VERSION_STRING\n    parser.add_argument(\'--version\', action=\'version\', version=version_str)\n    output_group = parser.add_mutually_exclusive_group()\n    output_group.add_argument(\'-v\', \'--verbose\',\n                              action=\'store_true\',\n                              default=False,\n                              help=\'be verbose about actions\')\n    output_group.add_argument(\'-d\', \'--debug\',\n                              action=\'store_true\',\n                              default=False,\n                              help=\'print debugging output too\')\n    global_group = parser.add_argument_group(title=\'global options\')\n    global_group.add_argument(\'-f\', \'--force\',\n                              action=\'store_true\',\n                              default=False,\n                              help=\'disable overwrite checks for existing files\\n\' +\n                              \'(use with caution)\')\n\n    class CheckThreadOption(argparse.Action):\n        def __call__(self, parser, namespace, value, option_string=None):\n            if not 1 <= value <= blosc.BLOSC_MAX_THREADS:\n                log.error(\'%s must be 1 <= n <= %d\'\n                          % (option_string, blosc.BLOSC_MAX_THREADS))\n            setattr(namespace, self.dest, value)\n\n    global_group.add_argument(\'-n\', \'--nthreads\',\n                              metavar=\'[1, %d]\' % blosc.BLOSC_MAX_THREADS,\n                              action=CheckThreadOption,\n                              default=blosc.ncores,\n                              type=int,\n                              dest=\'nthreads\',\n                              help=\'set number of threads, \' +\n                                   \'(default: %(default)s (ncores))\')\n\n    subparsers = parser.add_subparsers(title=\'subcommands\',\n                                       metavar=\'\',\n                                       dest=\'subcommand\')\n    compress_parser = subparsers.add_parser(\'compress\',\n            formatter_class=BloscPackCustomFormatter,\n            help=\'perform compression on file\')\n    c_parser = subparsers.add_parser(\'c\',\n            formatter_class=BloscPackCustomFormatter,\n            help=""alias for \'compress\'"")\n\n    class CheckChunkSizeOption(argparse.Action):\n        def __call__(self, parser, namespace, value, option_string=None):\n            if value == \'max\':\n                value = blosc.BLOSC_MAX_BUFFERSIZE\n            else:\n                try:\n                    # try to get the value as bytes\n                    if value[-1] in SUFFIXES.keys():\n                        value = reverse_pretty(value)\n                    # seems to be intended to be a naked int\n                    else:\n                        value = int(value)\n                except ValueError as ve:\n                    log.error(\'%s error: %s\' % (option_string, str(ve)))\n                if value < 0:\n                    log.error(\'%s must be > 0\' % option_string)\n            setattr(namespace, self.dest, value)\n    for p in [compress_parser, c_parser]:\n        _inject_blosc_group(p)\n        bloscpack_group = p.add_argument_group(title=\'bloscpack settings\')\n        bloscpack_group.add_argument(\'-z\', \'--chunk-size\',\n                                     metavar=\'<size>\',\n                                     action=CheckChunkSizeOption,\n                                     type=str,\n                                     default=DEFAULT_CHUNK_SIZE,\n                                     dest=\'chunk_size\',\n                                     help=""set desired chunk size or \'max\'"")\n        checksum_format = join_with_eol(CHECKSUMS_AVAIL[0:3]) + \\\n                                        join_with_eol(CHECKSUMS_AVAIL[3:6]) + \\\n                                        join_with_eol(CHECKSUMS_AVAIL[6:])\n        checksum_help = \'set desired checksum:\\n\' + checksum_format\n        bloscpack_group.add_argument(\'-k\', \'--checksum\',\n                                     metavar=\'<checksum>\',\n                                     type=str,\n                                     choices=CHECKSUMS_AVAIL,\n                                     default=DEFAULT_CHECKSUM,\n                                     dest=\'checksum\',\n                                     help=checksum_help)\n        bloscpack_group.add_argument(\'-o\', \'--no-offsets\',\n                                     action=\'store_false\',\n                                     default=DEFAULT_OFFSETS,\n                                     dest=\'offsets\',\n                                     help=\'deactivate offsets\')\n        bloscpack_group.add_argument(\'-m\', \'--metadata\',\n                                     metavar=\'<metadata>\',\n                                     type=str,\n                                     dest=\'metadata\',\n                                     help=""file containing the metadata, must contain valid JSON"")\n\n\n    decompress_parser = subparsers.add_parser(\'decompress\',\n            formatter_class=BloscPackCustomFormatter,\n            help=\'perform decompression on file\')\n\n    d_parser = subparsers.add_parser(\'d\',\n            formatter_class=BloscPackCustomFormatter,\n            help=""alias for \'decompress\'"")\n\n    for p in [decompress_parser, d_parser]:\n        p.add_argument(\'-e\', \'--no-check-extension\',\n                       action=\'store_true\',\n                       default=False,\n                       dest=\'no_check_extension\',\n                       help=\'disable checking input file for extension (*.blp)\\n\' +\n                       \'(requires use of <out_file>)\')\n\n    for p, help_in, help_out in [(compress_parser,\n                                  \'file to be compressed\',\n                                  \'file to compress to\'),\n                                 (c_parser,\n                                  \'file to be compressed\',\n                                  \'file to compress to\'),\n                                 (decompress_parser,\n                                  \'file to be decompressed\',\n                                  \'file to decompress to\'),\n                                 (d_parser,\n                                  \'file to be decompressed\',\n                                  \'file to decompress to\'),\n                                 ]:\n        p.add_argument(\'in_file\',\n                       metavar=\'<in_file>\',\n                       type=str,\n                       default=None,\n                       help=help_in)\n        p.add_argument(\'out_file\',\n                       metavar=\'<out_file>\',\n                       type=str,\n                       nargs=\'?\',\n                       default=None,\n                       help=help_out)\n\n    append_parser = subparsers.add_parser(\'append\',\n            formatter_class=BloscPackCustomFormatter,\n            help=\'append data to a compressed file\')\n\n    a_parser = subparsers.add_parser(\'a\',\n            formatter_class=BloscPackCustomFormatter,\n            help=""alias for \'append\'"")\n\n    for p in (append_parser, a_parser):\n        _inject_blosc_group(p)\n        p.add_argument(\'original_file\',\n                       metavar=\'<original_file>\',\n                       type=str,\n                       help=""file to append to"")\n        p.add_argument(\'new_file\',\n                       metavar=\'<new_file>\',\n                       type=str,\n                       help=""file to append from"")\n        p.add_argument(\'-e\', \'--no-check-extension\',\n                       action=\'store_true\',\n                       default=False,\n                       dest=\'no_check_extension\',\n                       help=\'disable checking original file for extension (*.blp)\\n\')\n        p.add_argument(\'-m\', \'--metadata\',\n                       metavar=\'<metadata>\',\n                       type=str,\n                       dest=\'metadata\',\n                       help=""file containing the metadata, must contain valid JSON"")\n\n\n    info_parser = subparsers.add_parser(\'info\',\n            formatter_class=BloscPackCustomFormatter,\n            help=\'print information about a compressed file\')\n\n    i_parser = subparsers.add_parser(\'i\',\n            formatter_class=BloscPackCustomFormatter,\n            help=""alias for \'info\'"")\n\n    for p in (info_parser, i_parser):\n        p.add_argument(\'file_\',\n                       metavar=\'<file>\',\n                       type=str,\n                       default=None,\n                       help=""file to show info for"")\n    return parser\n\n\ndef main():\n    parser = create_parser()\n    log.set_prefix(parser.prog)\n    args = parser.parse_args()\n    if args.verbose:\n        log.LEVEL = log.VERBOSE\n    elif args.debug:\n        log.LEVEL = log.DEBUG\n    log.debug(\'command line argument parsing complete\')\n    log.debug(\'command line arguments are: \')\n    for arg, val in sorted(vars(args).items()):\n        log.debug(\'    %s: %s\' % (arg, str(val)))\n    process_nthread_arg(args)\n\n    # compression and decompression handled via subparsers\n    if args.subcommand in [\'compress\', \'c\']:\n        log.verbose(\'getting ready for compression\')\n        in_file, out_file, blosc_args = process_compression_args(args)\n        try:\n            check_files(in_file, out_file, args)\n        except FileNotFound as fnf:\n            log.error(str(fnf))\n        metadata = process_metadata_args(args)\n        bloscpack_args = BloscpackArgs(offsets=args.offsets,\n                                       checksum=args.checksum)\n        try:\n            pack_file_to_file(in_file, out_file,\n                              chunk_size=args.chunk_size,\n                              metadata=metadata,\n                              blosc_args=blosc_args,\n                              bloscpack_args=bloscpack_args,\n                              metadata_args=MetadataArgs())\n        except ChunkingException as ce:\n            log.error(str(ce))\n    elif args.subcommand in [\'decompress\', \'d\']:\n        log.verbose(\'getting ready for decompression\')\n        in_file, out_file = process_decompression_args(args)\n        try:\n            check_files(in_file, out_file, args)\n        except FileNotFound as fnf:\n            log.error(str(fnf))\n        try:\n            metadata = unpack_file_from_file(in_file, out_file)\n            if metadata:\n                log_metadata(metadata)\n        except FormatVersionMismatch as fvm:\n            log.error(fvm.message)\n        except ChecksumMismatch as csm:\n            log.error(csm.message)\n    elif args.subcommand in [\'append\', \'a\']:\n        log.verbose(\'getting ready for append\')\n        original_file, new_file = process_append_args(args)\n        try:\n            if not path.exists(original_file):\n                raise FileNotFound(""original file \'%s\' does not exist!"" %\n                                   original_file)\n            if not path.exists(new_file):\n                raise FileNotFound(""new file \'%s\' does not exist!"" %\n                                   new_file)\n        except FileNotFound as fnf:\n            log.error(str(fnf))\n        log.verbose(""original file is: \'%s\'"" % original_file)\n        log.verbose(""new file is: \'%s\'"" % new_file)\n        blosc_args = _blosc_args_from_args(args)\n        metadata = process_metadata_args(args)\n        append(original_file, new_file, blosc_args=blosc_args)\n        if metadata is not None:\n            with open(original_file, \'r+b\') as fp:\n                _seek_to_metadata(fp)\n                _rewrite_metadata_fp(fp, metadata)\n    elif args.subcommand in (\'info\', \'i\'):\n        try:\n            if not path.exists(args.file_):\n                raise FileNotFound(""file \'%s\' does not exist!"" %\n                                   args.file_)\n        except FileNotFound as fnf:\n            log.error(str(fnf))\n        try:\n            with open(args.file_, \'rb\') as fp:\n                bloscpack_header, metadata, metadata_header, offsets = \\\n                    _read_beginning(fp)\n                checksum_impl = bloscpack_header.checksum_impl\n                # get the header of the first chunk\n                _, blosc_header, _ = _read_compressed_chunk_fp(\n                    fp, checksum_impl)\n        except ValueError as ve:\n            log.error(str(ve) + ""\\n"" +\n                      ""This might not be a bloscpack compressed file."")\n        log.normal(bloscpack_header.pformat())\n        if offsets:\n            log.normal(""\'offsets\':"")\n            log.normal(""[%s,...]"" % ("","".join(str(o) for o in offsets[:5])))\n        if metadata is not None:\n            log_metadata(metadata)\n            log.normal(metadata_header.pformat())\n        log.normal(""First chunk blosc header:"")\n        log.normal(str(blosc_header))\n        log.normal(""First chunk blosc flags: "")\n        log.normal(str(decode_blosc_flags(blosc_header[\'flags\'])))\n    else:  # pragma: no cover\n        # in Python 3 subcommands are not mandatory by default\n        parser.print_usage()\n        log.error(\'too few arguments\', 2)\n    log.verbose(\'done\')\n'"
bloscpack/compat_util.py,0,b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# vim :set ft=py:\n\ntry:\n    from cStringIO import StringIO\nexcept ImportError:\n    from io import BytesIO as StringIO\n\ntry:\n    from collections import OrderedDict\nexcept ImportError:  # pragma: no cover\n    from ordereddict import OrderedDict\n'
bloscpack/constants.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# vim :set ft=py:\n\n\nimport blosc\n\n\nfrom .compat_util import (OrderedDict,\n                          )\n\n\n# miscellaneous\nFORMAT_VERSION = 3\nMAGIC = b\'blpk\'\nEXTENSION = \'.blp\'\n\n# header lengths\nBLOSC_HEADER_LENGTH = 16\nBLOSCPACK_HEADER_LENGTH = 32\nMETADATA_HEADER_LENGTH = 32\n\n# maximum/minimum values\nMAX_FORMAT_VERSION = 255\nMAX_CHUNKS = (2**63)-1\nMAX_META_SIZE = (2**32-1)  # uint32 max val\nMIN_CLEVEL = 0\nMAX_CLEVEL = 9\n\n# lookup table for human readable sizes\nSUFFIXES = OrderedDict((\n             (""B"", 2**0 ),\n             (""K"", 2**10),\n             (""M"", 2**20),\n             (""G"", 2**30),\n             (""T"", 2**40)))\n\n# Codecs available from Blosc\nCNAME_AVAIL = blosc.compressor_list()\nCNAME_MAPPING = {\n    0: \'blosclz\',\n    1: \'lz4\',\n    2: \'snappy\',\n    3: \'zlib\',\n    4: \'zstd\',\n}\n'"
bloscpack/defaults.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# vim :set ft=py:\n\n# blosc args\nDEFAULT_TYPESIZE = 8\nDEFAULT_CLEVEL = 7\nDEFAULT_SHUFFLE = True\nDEFAULT_CNAME = 'blosclz'\n\n# bloscpack args\nDEFAULT_OFFSETS = True\nDEFAULT_CHECKSUM = 'adler32'\nDEFAULT_MAX_APP_CHUNKS = lambda x: 10 * x\n\nDEFAULT_CHUNK_SIZE = '1M'\n\n# metadata args\nDEFAULT_MAGIC_FORMAT = b'JSON'\nDEFAULT_META_CHECKSUM = 'adler32'\nDEFAULT_META_CODEC = 'zlib'\nDEFAULT_META_LEVEL = 6\nDEFAULT_MAX_META_SIZE = lambda x: 10 * x\n"""
bloscpack/exceptions.py,0,b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# vim :set ft=py:\n\n\nclass FileNotFound(IOError):\n    pass\n\n\nclass NoSuchChecksum(ValueError):\n    pass\n\n\nclass NoSuchCodec(ValueError):\n    pass\n\n\nclass NoSuchSerializer(ValueError):\n    pass\n\n\nclass ChunkingException(BaseException):\n    pass\n\n\nclass ChunkSizeTypeSizeMismatch(ValueError):\n    pass\n\n\nclass ChecksumMismatch(RuntimeError):\n    pass\n\n\nclass FormatVersionMismatch(RuntimeError):\n    pass\n\n\nclass ChecksumLengthMismatch(RuntimeError):\n    pass\n\n\nclass NoMetadataFound(RuntimeError):\n    pass\n\n\nclass NoChangeInMetadata(RuntimeError):\n    pass\n\n\nclass MetadataSectionTooSmall(RuntimeError):\n    pass\n\n\nclass NonUniformTypesize(RuntimeError):\n    pass\n\n\nclass NotEnoughSpace(RuntimeError):\n    pass\n\n\nclass NotANumpyArray(RuntimeError):\n    pass\n\n\nclass ObjectNumpyArrayRejection(RuntimeError):\n    pass\n'
bloscpack/file_io.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# vim :set ft=py:\n\n\nfrom __future__ import division\n\nimport itertools\nimport os.path as path\n\nimport blosc\nimport six\nfrom six.moves import xrange\nfrom deprecated import deprecated\n\n\nfrom .args import (calculate_nchunks,\n                   _check_metadata_arguments,\n                   )\nfrom .abstract_io import (pack,\n                          unpack,\n                          )\nfrom .metacodecs import (CODECS_LOOKUP,\n                         )\nfrom .constants import (METADATA_HEADER_LENGTH,\n                        BLOSCPACK_HEADER_LENGTH,\n                        BLOSC_HEADER_LENGTH,\n                        FORMAT_VERSION,\n                        )\nfrom .compat_util import (StringIO,\n                          )\nfrom .checksums import (CHECKSUMS_AVAIL,\n                        CHECKSUMS_LOOKUP,\n                        )\nfrom .defaults import (DEFAULT_CHUNK_SIZE,\n                       )\nfrom .exceptions import (MetadataSectionTooSmall,\n                         FormatVersionMismatch,\n                         ChecksumMismatch,\n                         )\nfrom .headers import (decode_blosc_header,\n                      BloscpackHeader,\n                      MetadataHeader,\n                      decode_int64,\n                      encode_int64,\n                      )\nfrom .pretty import (double_pretty_size,\n                     pretty_size,\n                     )\nfrom .serializers import (SERIALIZERS_LOOKUP,\n                          )\nfrom .abstract_io import (PlainSource,\n                          PlainSink,\n                          CompressedSource,\n                          CompressedSink,\n                          )\nfrom . import log\n\n\ndef _write_metadata(output_fp, metadata, metadata_args):\n    """""" Write the metadata to a file pointer.\n\n    Parameters\n    ----------\n    output_fp : file like\n        the file pointer to write to\n    metadata : dict\n        the metadata to write\n    metadata_args : MetadataArgs\n        the metadata args\n\n    Returns\n    -------\n    metadata_total : int\n        the total number of bytes occupied by metadata header, metadata plus\n        preallocation and checksum\n\n    Notes\n    -----\n    The \'output_fp\' should point to the position in the file where the metadata\n    should be written.\n\n    """"""\n    _check_metadata_arguments(metadata_args)\n    metadata_total = 0\n    metadata_total += METADATA_HEADER_LENGTH\n    serializer_impl = SERIALIZERS_LOOKUP[metadata_args.magic_format]\n    metadata = serializer_impl.dumps(metadata)\n    meta_size = len(metadata)\n    if six.PY3 and isinstance(metadata, str):\n        metadata = metadata.encode()\n    if metadata_args.should_compress:\n        codec_impl = metadata_args.meta_codec_impl\n        metadata_compressed = codec_impl.compress(metadata,\n                metadata_args.meta_level)\n        meta_comp_size = len(metadata_compressed)\n        # be opportunistic, avoid compression if not beneficial\n        if meta_size < meta_comp_size:\n            log.debug(\'metadata compression requested, but it was not \'\n                      \'beneficial, deactivating \'\n                      ""(raw: \'%s\' vs. compressed: \'%s\') "" %\n                      (meta_size, meta_comp_size))\n            meta_comp_size = meta_size\n            metadata_args.nullify_codec()\n        else:\n            metadata = metadata_compressed\n    else:\n        meta_comp_size = meta_size\n    log.debug(""Raw %s metadata of size \'%s\': %s"" %\n              (\'compressed\' if metadata_args.should_compress else\n               \'uncompressed\', meta_comp_size, repr(metadata)))\n    max_meta_size = metadata_args.effective_max_meta_size(meta_size)\n    if meta_comp_size > max_meta_size:\n        raise MetadataSectionTooSmall(\n                \'metadata section is too small to contain the metadata \'\n                \'required: %d allocated: %d\' %\n                (meta_comp_size, max_meta_size))\n    metadata_total += meta_comp_size\n    # create metadata header\n    raw_metadata_header = MetadataHeader(\n            magic_format=metadata_args.magic_format,\n            meta_checksum=metadata_args.meta_checksum,\n            meta_codec=metadata_args.meta_codec_name,\n            meta_level=metadata_args.meta_level,\n            meta_size=meta_size,\n            max_meta_size=max_meta_size,\n            meta_comp_size=meta_comp_size).encode()\n    log.debug(\'raw_metadata_header: %s\' % repr(raw_metadata_header))\n    output_fp.write(raw_metadata_header)\n    output_fp.write(metadata)\n    prealloc = max_meta_size - meta_comp_size\n    for i in xrange(prealloc):\n        output_fp.write(b\'\\x00\')\n    metadata_total += prealloc\n    log.debug(""metadata has %d preallocated empty bytes"" % prealloc)\n    if metadata_args[\'meta_checksum\'] != CHECKSUMS_AVAIL[0]:\n        metadata_checksum_impl = CHECKSUMS_LOOKUP[metadata_args.meta_checksum]\n        metadata_digest = metadata_checksum_impl(metadata)\n        metadata_total += metadata_checksum_impl.size\n        output_fp.write(metadata_digest)\n        log.debug(""metadata checksum (%s): %s"" %\n                (metadata_args[\'meta_checksum\'], repr(metadata_digest)))\n    log.debug(""metadata section occupies a total of %s"" %\n            double_pretty_size(metadata_total))\n    return metadata_total\n\n\ndef _read_bloscpack_header(input_fp):\n    """""" Read the bloscpack header.\n\n    Parameters\n    ----------\n    input_fp : file like\n        a file pointer to read from\n\n    Returns\n    -------\n    bloscpack_header : BloscPackHeader\n        the decoded bloscpack header\n\n    Raises\n    ------\n    FormatVersionMismatch\n        if the received format version is not equal to the one this module can\n        decode\n\n    """"""\n    log.debug(\'reading bloscpack header\')\n    bloscpack_header_raw = input_fp.read(BLOSCPACK_HEADER_LENGTH)\n    log.debug(\'bloscpack_header_raw: %s\' %\n              repr(bloscpack_header_raw))\n    bloscpack_header = BloscpackHeader.decode(bloscpack_header_raw)\n    log.debug(""bloscpack header: %s"" % repr(bloscpack_header))\n    if FORMAT_VERSION != bloscpack_header.format_version:\n        raise FormatVersionMismatch(\n                ""format version of file was not \'%s\' as expected, but \'%d\'"" %\n                (FORMAT_VERSION, bloscpack_header.format_version))\n    return bloscpack_header\n\n\ndef _read_metadata(input_fp):\n    """""" Read the metadata and header from a file pointer.\n\n    Parameters\n    ----------\n    input_fp : file like\n        a file pointer to read from\n\n    Returns\n    -------\n    metadata : dict\n        the metadata\n    metadata_header : dict\n        the metadata contents as dict\n\n    Notes\n    -----\n    The \'input_fp\' should point to the position where the metadata starts. The\n    number of bytes to read will be determined from the metadata header.\n\n    """"""\n    raw_metadata_header = input_fp.read(METADATA_HEADER_LENGTH)\n    log.debug(""raw metadata header: %s"" % repr(raw_metadata_header))\n    metadata_header = MetadataHeader.decode(raw_metadata_header)\n    log.debug(metadata_header.pformat())\n    metadata = input_fp.read(metadata_header.meta_comp_size)\n    prealloc = metadata_header.max_meta_size - metadata_header.meta_comp_size\n    input_fp.seek(prealloc, 1)\n    if metadata_header.meta_checksum != \'None\':\n        metadata_checksum_impl = CHECKSUMS_LOOKUP[metadata_header.meta_checksum]\n        metadata_expected_digest = input_fp.read(metadata_checksum_impl.size)\n        metadata_received_digest = metadata_checksum_impl(metadata)\n        if metadata_received_digest != metadata_expected_digest:\n            raise ChecksumMismatch(\n                    ""Checksum mismatch detected in metadata ""\n                    ""expected: \'%s\', received: \'%s\'"" %\n                    (repr(metadata_expected_digest),\n                        repr(metadata_received_digest)))\n        else:\n            log.debug(\'metadata checksum OK (%s): %s\' %\n                    (metadata_checksum_impl.name,\n                        repr(metadata_received_digest)))\n    if metadata_header.meta_codec != \'None\':\n        metadata_codec_impl = CODECS_LOOKUP[metadata_header.meta_codec]\n        metadata = metadata_codec_impl.decompress(metadata)\n    log.verbose(""read %s metadata of size: \'%s\'"" %\n            # FIXME meta_codec?\n            (\'compressed\' if metadata_header.meta_codec != \'None\' else\n                \'uncompressed\', metadata_header.meta_comp_size))\n    serializer_impl = SERIALIZERS_LOOKUP[metadata_header.magic_format]\n    if six.PY3 and isinstance(metadata, bytes):\n        metadata = metadata.decode()\n    metadata = serializer_impl.loads(metadata)\n    return metadata, metadata_header\n\n\ndef _read_offsets(input_fp, bloscpack_header):\n    """""" Read the offsets from a file pointer.\n\n    Parameters\n    ----------\n    input_fp : file like\n        a file pointer to read from\n\n    Returns\n    -------\n    offsets : list of int\n        the offsets\n\n    Notes\n    -----\n    The \'input_fp\' should point to the position where the offsets start. Any\n    unused offsets will not be returned.\n\n    """"""\n    if bloscpack_header.offsets:\n        total_entries = bloscpack_header.total_prospective_chunks\n        offsets_raw = input_fp.read(8 * total_entries)\n        log.debug(\'Read raw offsets: %s\' % repr(offsets_raw))\n        offsets = [decode_int64(offsets_raw[j - 8:j]) for j in\n                   xrange(8, bloscpack_header.nchunks * 8 + 1, 8)]\n        log.debug(\'Offsets: %s\' % offsets)\n        return offsets\n    else:\n        return []\n\n\ndef _read_beginning(input_fp):\n    """""" Read the bloscpack_header, metadata, metadata_header and offsets.\n\n    Parameters\n    ----------\n    input_fp : file like\n        input file pointer\n\n    Returns\n    -------\n    bloscpack_header : dict\n    metadata : object\n    metadata_header : dict\n    offsets : list of ints\n\n    """"""\n    bloscpack_header = _read_bloscpack_header(input_fp)\n    metadata, metadata_header = _read_metadata(input_fp) \\\n            if bloscpack_header.metadata\\\n            else (None, None)\n    offsets = _read_offsets(input_fp, bloscpack_header)\n    return bloscpack_header, metadata, metadata_header, offsets\n\n\ndef _write_offsets(output_fp, offsets):\n    log.debug(""Writing \'%d\' offsets: \'%s\'"" %\n              (len(offsets), repr(offsets)))\n    # write the offsets encoded into the reserved space in the file\n    encoded_offsets = b"""".join([encode_int64(i) for i in offsets])\n    log.debug(""Raw offsets: %s"" % repr(encoded_offsets))\n    output_fp.write(encoded_offsets)\n\n\ndef _read_compressed_chunk_fp(input_fp, checksum_impl):\n    """""" Read a compressed chunk from a file pointer.\n\n    Parameters\n    ----------\n    input_fp : file like\n        the file pointer to read the chunk from\n    checksum_impl : Checksum\n        the checksum that has been used\n\n    Returns\n    -------\n    compressed : str\n        the compressed data\n    blosc_header : dict\n        the blosc header from the chunk\n    digest : str\n        the checksum of the chunk\n    """"""\n    # read blosc header\n    blosc_header_raw = input_fp.read(BLOSC_HEADER_LENGTH)\n    blosc_header = decode_blosc_header(blosc_header_raw)\n    if log.LEVEL == log.DEBUG:\n        log.debug(\'blosc_header: %s\' % repr(blosc_header))\n    ctbytes = blosc_header[\'ctbytes\']\n    # Seek back BLOSC_HEADER_LENGTH bytes in file relative to current\n    # position. Blosc needs the header too and presumably this is\n    # better than to read the whole buffer and then concatenate it...\n    input_fp.seek(-BLOSC_HEADER_LENGTH, 1)\n    # read chunk\n    compressed = input_fp.read(ctbytes)\n    digest = input_fp.read(checksum_impl.size) \\\n        if checksum_impl.size > 0 else None\n    return compressed, blosc_header, digest\n\n\ndef _write_compressed_chunk(output_fp, compressed, digest):\n    output_fp.write(compressed)\n    if len(digest) > 0:\n        output_fp.write(digest)\n\n\nclass PlainFPSource(PlainSource):\n\n    def __init__(self, input_fp):\n        self.input_fp = input_fp\n\n    def __iter__(self):\n        # if nchunks == 1 the last_chunk_size is the size of the single chunk\n        for num_bytes in ([self.chunk_size] *\n                          (self.nchunks - 1) +\n                          [self.last_chunk]):\n            yield self.input_fp.read(num_bytes)\n\n\nclass CompressedFPSource(CompressedSource):\n\n    def __init__(self, input_fp):\n\n        self.input_fp = input_fp\n        self.bloscpack_header, self.metadata, self.metadata_header, \\\n                self.offsets = _read_beginning(input_fp)\n        self.checksum_impl = self.bloscpack_header.checksum_impl\n        self.nchunks = self.bloscpack_header.nchunks\n\n    def __iter__(self):\n        for i in xrange(self.nchunks):\n            compressed, header, digest = _read_compressed_chunk_fp(self.input_fp, self.checksum_impl)\n            yield compressed, digest\n\n\nclass PlainFPSink(PlainSink):\n\n    def __init__(self, output_fp, nchunks=None):\n        self.output_fp = output_fp\n        self.nchunks = nchunks\n\n    def put(self, compressed):\n        decompressed = blosc.decompress(compressed)\n        self.output_fp.write(decompressed)\n        return len(decompressed)\n\n\nclass CompressedFPSink(CompressedSink):\n\n    def __init__(self, output_fp):\n        self.output_fp = output_fp\n        self.meta_total = 0\n\n    def write_bloscpack_header(self):\n        raw_bloscpack_header = self.bloscpack_header.encode()\n        self.output_fp.write(raw_bloscpack_header)\n\n    def write_metadata(self, metadata, metadata_args):\n        self.meta_total += _write_metadata(self.output_fp,\n                                           metadata,\n                                           metadata_args)\n\n    def init_offsets(self):\n        if self.offsets:\n            total_entries = self.bloscpack_header.total_prospective_chunks\n            self.offset_storage = list(itertools.repeat(-1,\n                                       self.bloscpack_header.nchunks))\n            self.output_fp.write(encode_int64(-1) * total_entries)\n\n    def finalize(self):\n        if self.offsets:\n            self.output_fp.seek(BLOSCPACK_HEADER_LENGTH + self.meta_total, 0)\n            _write_offsets(self.output_fp, self.offset_storage)\n\n    def put(self, i, compressed):\n        offset = self.output_fp.tell()\n        digest = self.do_checksum(compressed)\n        _write_compressed_chunk(self.output_fp, compressed, digest)\n        if self.offsets:\n            self.offset_storage[i] = offset\n        return offset, compressed, digest\n\n\ndef pack_file_to_file(in_file, out_file,\n                      chunk_size=DEFAULT_CHUNK_SIZE,\n                      metadata=None,\n                      blosc_args=None,\n                      bloscpack_args=None,\n                      metadata_args=None):\n    """""" Compress a file to a file.\n\n    Parameters\n    ----------\n    in_file : str\n        the name of the input file\n    out_file : str\n        the name of the output file\n    chunk_size : int\n        the desired chunk size in bytes\n    metadata : dict\n        the metadata dict\n    blosc_args : BloscArgs\n        blosc args\n    bloscpack_args : BloscpackArgs\n        bloscpack args\n    metadata_args : MetadataArgs\n        metadata args\n\n    Raises\n    ------\n\n    ChunkingException\n        if there was a problem caculating the chunks\n\n    # TODO document which arguments are silently ignored\n\n    """"""\n    in_file_size = path.getsize(in_file)\n    log.verbose(\'input file size: %s\' % double_pretty_size(in_file_size))\n    # calculate chunk sizes\n    nchunks, chunk_size, last_chunk_size = \\\n            calculate_nchunks(in_file_size, chunk_size)\n    with open(in_file, \'rb\') as input_fp, open(out_file, \'wb\') as output_fp:\n        source = PlainFPSource(input_fp)\n        sink = CompressedFPSink(output_fp)\n        pack(source, sink,\n                nchunks, chunk_size, last_chunk_size,\n                metadata=metadata,\n                blosc_args=blosc_args,\n                bloscpack_args=bloscpack_args,\n                metadata_args=metadata_args)\n    out_file_size = path.getsize(out_file)\n    log.verbose(\'output file size: %s\' % double_pretty_size(out_file_size))\n    log.verbose(\'compression ratio: %f\' % (in_file_size/out_file_size))\n\n\npack_file = deprecated(pack_file_to_file,\n                       version=""0.16.0"",\n                       reason=""Use \'pack_file_to_file\' instead"")\n\n\ndef unpack_file_from_file(in_file, out_file):\n    """""" Uncompress a file from a file.\n\n    Parameters\n    ----------\n    in_file : str\n        the name of the input file\n    out_file : str\n        the name of the output file\n\n    Returns\n    -------\n    metadata : bytes\n        the metadata contained in the file if present\n\n    Raises\n    ------\n\n    FormatVersionMismatch\n        if the file has an unmatching format version number\n    ChecksumMismatch\n        if any of the chunks fail to produce the correct checksum\n    """"""\n    in_file_size = path.getsize(in_file)\n    log.verbose(\'input file size: %s\' % pretty_size(in_file_size))\n    with open(in_file, \'rb\') as input_fp, open(out_file, \'wb\') as output_fp:\n        source = CompressedFPSource(input_fp)\n        sink = PlainFPSink(output_fp, source.nchunks)\n        unpack(source, sink)\n    out_file_size = path.getsize(out_file)\n    log.verbose(\'output file size: %s\' % pretty_size(out_file_size))\n    log.verbose(\'decompression ratio: %f\' % (out_file_size / in_file_size))\n    return source.metadata\n\n\nunpack_file = deprecated(unpack_file_from_file,\n                         version=""0.16.0"",\n                         reason=""Use \'unpack_file_from_file\' instead"")\n\n\ndef pack_bytes_to_file(bytes_, out_file,\n                       chunk_size=DEFAULT_CHUNK_SIZE,\n                       metadata=None,\n                       blosc_args=None,\n                       bloscpack_args=None,\n                       metadata_args=None):\n    """""" Compress bytes to file.\n\n    Parameters\n    ----------\n    bytes_ : bytes\n        the bytes to compress\n    out_file : str\n        the name of the output file\n    chunk_size : int\n        the desired chunk size in bytes\n    metadata : dict\n        the metadata dict\n    blosc_args : BloscArgs\n        blosc args\n    bloscpack_args : BloscpackArgs\n        bloscpack args\n    metadata_args : MetadataArgs\n        metadata args\n\n    Raises\n    ------\n\n    ChunkingException\n        if there was a problem caculating the chunks\n    """"""\n    bytes_size = len(bytes_)\n    log.verbose(\'input bytes size: %s\' % double_pretty_size(bytes_size))\n    # calculate chunk sizes\n    nchunks, chunk_size, last_chunk_size = \\\n            calculate_nchunks(bytes_size, chunk_size)\n    with open(out_file, \'wb\') as output_fp:\n        source = PlainFPSource(StringIO(bytes_))\n        sink = CompressedFPSink(output_fp)\n        pack(source, sink,\n             nchunks, chunk_size, last_chunk_size,\n             metadata=metadata,\n             blosc_args=blosc_args,\n             bloscpack_args=bloscpack_args,\n             metadata_args=metadata_args)\n    out_file_size = path.getsize(out_file)\n    log.verbose(\'output file size: %s\' % double_pretty_size(out_file_size))\n    log.verbose(\'compression ratio: %f\' % (bytes_size/out_file_size))\n\n\npack_bytes_file = deprecated(pack_bytes_to_file,\n                             version=""0.16.0"",\n                             reason=""Use \'pack_bytes_to_file\' instead"")\n\n\ndef unpack_bytes_from_file(compressed_file):\n    """""" Uncompress bytes from a file.\n\n    Parameters\n    ----------\n    compressed_file : str\n        the name of the input file\n\n    Returns\n    -------\n    bytes_ : bytes_\n        the decompressed bytes\n    metadata : bytes\n        the metadata contained in the file if present\n\n    Raises\n    ------\n\n    FormatVersionMismatch\n        if the file has an unmatching format version number\n    ChecksumMismatch\n        if any of the chunks fail to produce the correct checksum\n    """"""\n    sio = StringIO()\n    sink = PlainFPSink(sio)\n    with open(compressed_file, \'rb\') as fp:\n        source = CompressedFPSource(fp)\n        unpack(source, sink)\n        return sio.getvalue(), source.metadata\n\n\nunpack_bytes_file = deprecated(unpack_bytes_from_file,\n                               version=""0.16.0"",\n                               reason=""Use \'unpack_bytes_from_file\' instead"")\n\n\ndef pack_bytes_to_bytes(bytes_,\n                        chunk_size=DEFAULT_CHUNK_SIZE,\n                        metadata=None,\n                        blosc_args=None,\n                        bloscpack_args=None,\n                        metadata_args=None,\n                        ):\n\n    """""" Compress bytes to bytes_\n\n    Parameters\n    ----------\n    bytes_ : bytes\n        the bytes to compress\n    out_file : str\n        the name of the output file\n    chunk_size : int\n        the desired chunk size in bytes\n    metadata : dict\n        the metadata dict\n    blosc_args : BloscArgs\n        blosc args\n    bloscpack_args : BloscpackArgs\n        bloscpack args\n    metadata_args : MetadataArgs\n        metadata args\n\n    Returns\n    -------\n    bytes_ : bytes\n        the compressed bytes\n\n    Raises\n    ------\n\n    ChunkingException\n        if there was a problem caculating the chunks\n    """"""\n    bytes_size = len(bytes_)\n    log.verbose(\'input bytes size: %s\' % double_pretty_size(bytes_size))\n    nchunks, chunk_size, last_chunk_size = \\\n            calculate_nchunks(bytes_size, chunk_size)\n    source = PlainFPSource(StringIO(bytes_))\n    sio = StringIO()\n    sink = CompressedFPSink(sio)\n    pack(source, sink,\n         nchunks, chunk_size, last_chunk_size,\n         metadata=metadata,\n         blosc_args=blosc_args,\n         bloscpack_args=bloscpack_args,\n         metadata_args=metadata_args)\n    out_bytes_size = sio.tell()\n    log.verbose(\'output bytes size: %s\' % double_pretty_size(out_bytes_size))\n    log.verbose(\'compression ratio: %f\' % (bytes_size/out_bytes_size))\n    return sio.getvalue()\n\n\ndef unpack_bytes_from_bytes(bytes_):\n    """""" Uncompress bytes from bytes\n\n    Parameters\n    ----------\n    bytes_: bytes\n        input bytes\n\n    Returns\n    -------\n    bytes_ : bytes_\n        the decompressed bytes\n    metadata : bytes\n        the metadata contained in the file if present\n\n    Raises\n    ------\n\n    FormatVersionMismatch\n        if the file has an unmatching format version number\n    ChecksumMismatch\n        if any of the chunks fail to produce the correct checksum\n    """"""\n    source = CompressedFPSource(StringIO(bytes_))\n    sio = StringIO()\n    sink = PlainFPSink(sio)\n    unpack(source, sink)\n    return sio.getvalue(), source.metadata\n'"
bloscpack/headers.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# vim :set ft=py:\n\n\nimport struct\n\ntry:\n    from collections import OrderedDict\nexcept ImportError:\n    from ordereddict import OrderedDict\n\n\nimport blosc\nfrom six import PY3, integer_types, binary_type\n\nfrom .abstract_objects import (MutableMappingObject,\n                               )\nfrom .checksums import (CHECKSUMS_AVAIL,\n                        CHECKSUMS_LOOKUP,\n                        check_valid_checksum,\n                        )\nfrom .constants import (MAGIC,\n                        FORMAT_VERSION,\n                        MAX_FORMAT_VERSION,\n                        MAX_CHUNKS,\n                        MAX_CLEVEL,\n                        BLOSCPACK_HEADER_LENGTH,\n                        MAX_META_SIZE,\n                        CNAME_MAPPING,\n                        )\nfrom .defaults import (DEFAULT_OFFSETS,\n                       )\nfrom .metacodecs import (CODECS_AVAIL,\n                         check_valid_codec,\n                         )\nfrom .util import (memoryview,\n                   )\nfrom . import log\n\n\ndef check_range(name, value, min_, max_):\n    """""" Check that a variable is in range. """"""\n    if not isinstance(value, integer_types):\n        raise TypeError(""\'%s\' must be of type \'int\'"" % name)\n    elif not min_ <= value <= max_:\n        raise ValueError(\n            ""\'%s\' must be in the range %s <= n <= %s, not \'%s\'"" %\n            tuple(map(str, (name, min_, max_, value))))\n\n\ndef _check_str(name, value, max_len):\n    if not isinstance(value, binary_type):\n        raise TypeError(""\'%s\' must be of type \'str\'/\'bytes\'"" % name)\n    elif len(value) > max_len:\n        raise ValueError(""\'%s\' can be of max length \'%i\' but is: \'%s\'"" %\n                         (name, max_len, len(value)))\n\n\ndef _pad_with_nulls(data, len_):\n    """""" Pad string with null bytes.\n\n    Parameters\n    ----------\n    data : str/bytes\n        the string/bytes to pad\n    len_ : int\n        the final desired length\n    """"""\n    return data + (b\'\\x00\' * (len_ - len(data)))\n\n\ndef check_options(options):\n    """""" Check the options bitfield.\n\n    Parameters\n    ----------\n    options : str\n\n    Raises\n    ------\n    TypeError\n        if options is not a string\n    ValueError\n        either if any character in option is not a zero or a one, or if options\n        is not of length 8\n    """"""\n\n    if not isinstance(options, str):\n        raise TypeError(""\'options\' must be of type \'str\', not \'%s\'"" %\n                        type(options))\n    elif (not len(options) == 8 or\n            not all(map(lambda x: x in [\'0\', \'1\'], iter(options)))):\n        raise ValueError(\n            ""\'options\' must be string of 0s and 1s of length 8, not \'%s\'"" %\n            options)\n\n\ndef check_options_zero(options, indices):\n    for i in indices:\n        if options[i] != \'0\':\n            raise ValueError(\n                \'Element %i was non-zero when attempting to decode options\')\n\n\ndef decode_uint8(byte):\n    if PY3:\n        return byte\n    else:\n        return struct.unpack(\'<B\', byte)[0]\n\n\ndef decode_uint32(fourbyte):\n    return struct.unpack(\'<I\', fourbyte)[0]\n\n\ndef decode_int32(fourbyte):\n    return struct.unpack(\'<i\', fourbyte)[0]\n\n\ndef decode_int64(eightbyte):\n    return struct.unpack(\'<q\', eightbyte)[0]\n\n\ndef decode_bitfield(byte):\n    return bin(decode_uint8(byte))[2:].rjust(8, \'0\')\n\n\ndef decode_magic_string(str_):\n    if PY3:\n        return str_.strip(b\'\\x00\')\n    else:\n        return str_.strip(\'\\x00\')\n\n\ndef encode_uint8(byte):\n    return struct.pack(\'<B\', byte)\n\n\ndef encode_uint32(byte):\n    return struct.pack(\'<I\', byte)\n\n\ndef encode_int32(fourbyte):\n    return struct.pack(\'<i\', fourbyte)\n\n\ndef encode_int64(eightbyte):\n    return struct.pack(\'<q\', eightbyte)\n\n\ndef create_options(offsets=DEFAULT_OFFSETS, metadata=False):\n    """""" Create the options bitfield.\n\n    Parameters\n    ----------\n    offsets : bool\n    metadata : bool\n    """"""\n    return """".join([str(int(i)) for i in\n        [False, False, False, False, False, False, metadata, offsets]])\n\n\ndef decode_options(options):\n    """""" Parse the options bitfield.\n\n    Parameters\n    ----------\n    options : str\n        the options bitfield\n\n    Returns\n    -------\n    options : dict mapping str -> bool\n    """"""\n\n    check_options(options)\n    check_options_zero(options, range(6))\n    return {\'offsets\': bool(int(options[7])),\n            \'metadata\': bool(int(options[6])),\n            }\n\n\ndef create_metadata_options():\n    """""" Create the metadata options bitfield. """"""\n    return ""00000000""\n\n\ndef decode_metadata_options(options):\n    check_options(options)\n    check_options_zero(options, range(8))\n    return {}\n\n\ndef decode_blosc_header(buffer_):\n    """""" Read and decode header from compressed Blosc buffer.\n\n    Parameters\n    ----------\n    buffer_ : string of bytes\n        the compressed buffer\n\n    Returns\n    -------\n    settings : dict\n        a dict containing the settings from Blosc\n\n    Notes\n    -----\n    Please see the readme for a precise descripttion of the blosc header\n    format.\n\n    """"""\n    buffer_ = memoryview(buffer_)\n    return OrderedDict(((\'version\', decode_uint8(buffer_[0])),\n                        (\'versionlz\', decode_uint8(buffer_[1])),\n                        (\'flags\', decode_uint8(buffer_[2])),\n                        (\'typesize\', decode_uint8(buffer_[3])),\n                        (\'nbytes\', decode_uint32(buffer_[4:8])),\n                        (\'blocksize\', decode_uint32(buffer_[8:12])),\n                        (\'ctbytes\', decode_uint32(buffer_[12:16]))))\n\n\ndef decode_blosc_flags(byte_):\n    return OrderedDict(((\'byte_shuffle\',  bool(byte_ & 1)),\n                        (\'pure_memcpy\',   bool(byte_ >> 1 & 1)),\n                        (\'bit_shuffle\',   bool(byte_ >> 2 & 1)),\n                        (\'split_blocks\',  bool(byte_ >> 4 & 1)),\n                        (\'codec\',         CNAME_MAPPING[byte_ >> 5 & 7]),\n                        ))\n\n\nclass BloscpackHeader(MutableMappingObject):\n    """""" The Bloscpack header.\n\n    Parameters\n    ----------\n    format_version : int\n        the version format for the compressed file\n    offsets: bool\n        if the offsets to the chunks are present\n    metadata: bool\n        if the metadata is present\n    checksum : str\n        the checksum to be used\n    typesize : int\n        the typesize used for blosc in the chunks\n    chunk_size : int\n        the size of a regular chunk\n    last_chunk : int\n        the size of the last chunk\n    nchunks : int\n        the number of chunks\n    max_app_chunks : int\n        the total number of possible append chunks\n\n    Notes\n    -----\n    See the README distributed for details on the header format.\n\n    Raises\n    ------\n    ValueError\n        if any of the arguments have an invalid value\n    TypeError\n        if any of the arguments have the wrong type\n    """"""\n    def __init__(self,\n                 format_version=FORMAT_VERSION,\n                 offsets=False,\n                 metadata=False,\n                 checksum=\'None\',\n                 typesize=0,\n                 chunk_size=-1,\n                 last_chunk=-1,\n                 nchunks=-1,\n                 max_app_chunks=0):\n\n        check_range(\'format_version\', format_version, 0, MAX_FORMAT_VERSION)\n        check_valid_checksum(checksum)\n        check_range(\'typesize\',   typesize,    0, blosc.BLOSC_MAX_TYPESIZE)\n        check_range(\'chunk_size\', chunk_size, -1, blosc.BLOSC_MAX_BUFFERSIZE)\n        check_range(\'last_chunk\', last_chunk, -1, blosc.BLOSC_MAX_BUFFERSIZE)\n        check_range(\'nchunks\',    nchunks,    -1, MAX_CHUNKS)\n        check_range(\'max_app_chunks\', max_app_chunks, 0, MAX_CHUNKS)\n        if nchunks != -1:\n            check_range(\'nchunks + max_app_chunks\',\n                        nchunks + max_app_chunks, 0, MAX_CHUNKS)\n        elif max_app_chunks != 0:\n            raise ValueError(\n                ""\'max_app_chunks\' can not be non \'0\' if \'nchunks\' is \'-1\'"")\n        if chunk_size != -1 and last_chunk != -1 and last_chunk > chunk_size:\n            raise ValueError(\n                ""\'last_chunk\' (%d) is larger than \'chunk_size\' (%d)""\n                % (last_chunk, chunk_size))\n\n        self._attrs = [\'format_version\',\n                       \'offsets\',\n                       \'metadata\',\n                       \'checksum\',\n                       \'typesize\',\n                       \'chunk_size\',\n                       \'last_chunk\',\n                       \'nchunks\',\n                       \'max_app_chunks\']\n        self._bytes_attrs = [\'chunk_size\',\n                             \'last_chunk\']\n\n        self.format_version  = format_version\n        self.offsets         = offsets\n        self.metadata        = metadata\n        self.checksum        = checksum\n        self.typesize        = typesize\n        self.chunk_size      = chunk_size\n        self.last_chunk      = last_chunk\n        self.nchunks         = nchunks\n        self.max_app_chunks  = max_app_chunks\n\n    @property\n    def attributes(self):\n        return self._attrs\n\n    @property\n    def bytes_attributes(self):\n        return self._bytes_attrs\n\n    @property\n    def checksum_impl(self):\n        return CHECKSUMS_LOOKUP[self.checksum]\n\n    @property\n    def total_prospective_chunks(self):\n        return self.nchunks + self.max_app_chunks \\\n            if self.nchunks >= 0 else None\n\n    def encode(self):\n        """""" Encode the Bloscpack header.\n\n        Returns\n        -------\n\n        raw_bloscpack_header : string\n            the header as string of bytes\n        """"""\n        format_version = encode_uint8(self.format_version)\n        options = encode_uint8(int(\n            create_options(offsets=self.offsets, metadata=self.metadata),\n            2))\n        checksum = encode_uint8(CHECKSUMS_AVAIL.index(self.checksum))\n        typesize = encode_uint8(self.typesize)\n        chunk_size = encode_int32(self.chunk_size)\n        last_chunk = encode_int32(self.last_chunk)\n        nchunks = encode_int64(self.nchunks)\n        max_app_chunks = encode_int64(self.max_app_chunks)\n\n        raw_bloscpack_header = (MAGIC + format_version + options + checksum +\n                                typesize + chunk_size + last_chunk + nchunks +\n                                max_app_chunks)\n        log.debug(\'raw_bloscpack_header: %s\' % repr(raw_bloscpack_header))\n        return raw_bloscpack_header\n\n    @staticmethod\n    def decode(buffer_):\n        """""" Decode an encoded Bloscpack header.\n\n        Parameters\n        ----------\n        buffer_ : str of length BLOSCPACK_HEADER_LENGTH\n\n        Returns\n        -------\n        bloscpack_header : BloscPackHeader\n            the decoded Bloscpack header object\n\n        Raises\n        ------\n        ValueError\n            If the buffer_ is not equal to BLOSCPACK_HEADER_LENGTH or the the\n            first four bytes are not the Bloscpack magic.\n\n        """"""\n        if len(buffer_) != BLOSCPACK_HEADER_LENGTH:\n            raise ValueError(\n                ""attempting to decode a bloscpack header of length \'%d\', not \'%d\'""\n                % (len(buffer_), BLOSCPACK_HEADER_LENGTH))\n        elif buffer_[0:4] != MAGIC:\n            raise ValueError(\n                ""the magic marker %r is missing from the bloscpack "" % MAGIC +\n                ""header, instead we found: %r"" % buffer_[0:4])\n        options = decode_options(decode_bitfield(buffer_[5]))\n        return BloscpackHeader(\n            format_version=decode_uint8(buffer_[4]),\n            offsets=options[\'offsets\'],\n            metadata=options[\'metadata\'],\n            checksum=CHECKSUMS_AVAIL[decode_uint8(buffer_[6])],\n            typesize=decode_uint8(buffer_[7]),\n            chunk_size=decode_int32(buffer_[8:12]),\n            last_chunk=decode_int32(buffer_[12:16]),\n            nchunks=decode_int64(buffer_[16:24]),\n            max_app_chunks=decode_int64(buffer_[24:32]))\n\n\nclass MetadataHeader(MutableMappingObject):\n\n    def __init__(self,\n                 magic_format=b\'\',\n                 meta_options=""00000000"",\n                 meta_checksum=\'None\',\n                 meta_codec=\'None\',\n                 meta_level=0,\n                 meta_size=0,\n                 max_meta_size=0,\n                 meta_comp_size=0,\n                 user_codec=b\'\',\n                 ):\n        _check_str(\'magic-format\',     magic_format,  8)\n        check_options(meta_options)\n        check_valid_checksum(meta_checksum)\n        check_valid_codec(meta_codec)\n        check_range(\'meta_level\',      meta_level,     0, MAX_CLEVEL)\n        check_range(\'meta_size\',       meta_size,      0, MAX_META_SIZE)\n        check_range(\'max_meta_size\',   max_meta_size,  0, MAX_META_SIZE)\n        check_range(\'meta_comp_size\',  meta_comp_size, 0, MAX_META_SIZE)\n        _check_str(\'user_codec\',       user_codec,     8)\n\n        self.magic_format = magic_format\n        self.meta_options = meta_options\n        self.meta_checksum = meta_checksum\n        self.meta_codec = meta_codec\n        self.meta_level = meta_level\n        self.meta_size = meta_size\n        self.max_meta_size = max_meta_size\n        self.meta_comp_size = meta_comp_size\n        self.user_codec = user_codec\n\n        self._attrs = [\'magic_format\',\n                       \'meta_options\',\n                       \'meta_checksum\',\n                       \'meta_codec\',\n                       \'meta_level\',\n                       \'meta_size\',\n                       \'max_meta_size\',\n                       \'meta_comp_size\',\n                       \'user_codec\',\n                       ]\n\n        self._bytes_attrs = [\'meta_size\',\n                             \'max_meta_size\',\n                             \'meta_comp_size\',\n                             ]\n\n    @property\n    def attributes(self):\n        return self._attrs\n\n    @property\n    def bytes_attributes(self):\n        return self._bytes_attrs\n\n    def encode(self):\n\n        magic_format = _pad_with_nulls(self.magic_format, 8)\n        meta_options = encode_uint8(int(self.meta_options, 2))\n        meta_checksum = encode_uint8(CHECKSUMS_AVAIL.index(self.meta_checksum))\n        meta_codec = encode_uint8(CODECS_AVAIL.index(self.meta_codec))\n        meta_level = encode_uint8(self.meta_level)\n        meta_size = encode_uint32(self.meta_size)\n        max_meta_size = encode_uint32(self.max_meta_size)\n        meta_comp_size = encode_uint32(self.meta_comp_size)\n        user_codec = _pad_with_nulls(self.user_codec, 8)\n\n        return magic_format + meta_options + meta_checksum + meta_codec + \\\n                meta_level + meta_size + max_meta_size + meta_comp_size + \\\n                user_codec\n\n    @staticmethod\n    def decode(buffer_):\n        if len(buffer_) != 32:\n            raise ValueError(\n                ""attempting to decode a bloscpack metadata header of length \'%d\', not \'32\'""\n                % len(buffer_))\n        decoded= {\'magic_format\':        decode_magic_string(buffer_[:8]),\n                  \'meta_options\':        decode_bitfield(buffer_[8]),\n                  \'meta_checksum\':       CHECKSUMS_AVAIL[decode_uint8(buffer_[9])],\n                  \'meta_codec\':          CODECS_AVAIL[decode_uint8(buffer_[10])],\n                  \'meta_level\':          decode_uint8(buffer_[11]),\n                  \'meta_size\':           decode_uint32(buffer_[12:16]),\n                  \'max_meta_size\':       decode_uint32(buffer_[16:20]),\n                  \'meta_comp_size\':      decode_uint32(buffer_[20:24]),\n                  \'user_codec\':          decode_magic_string(buffer_[24:32])\n                  }\n        return MetadataHeader(**decoded)\n'"
bloscpack/log.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# vim :set ft=py:\n\n\nimport sys\n\n\nNORMAL  = \'NORMAL\'\nVERBOSE = \'VERBOSE\'\nDEBUG   = \'DEBUG\'\nVERBOSITY_LEVELS = (NORMAL, VERBOSE, DEBUG)\n\nLEVEL = NORMAL\nPREFIX = ""bloscpack.py""\n\n\ndef set_prefix(prefix):\n    global PREFIX\n    PREFIX = prefix\n\n\ndef set_level(level):\n    if level not in VERBOSITY_LEVELS:\n        raise ValueError(\'Log level must be one of: %s, not %s\' %\n                         (VERBOSITY_LEVELS, level))\n\n\ndef verbose(message, level=VERBOSE):\n    """""" Print message with desired verbosity level. """"""\n    if level not in VERBOSITY_LEVELS:\n        raise TypeError(""Desired level \'%s\' is not one of %s"" % (level,\n                        str(VERBOSITY_LEVELS)))\n    if VERBOSITY_LEVELS.index(level) <= VERBOSITY_LEVELS.index(LEVEL):\n        for line in [l for l in message.split(\'\\n\') if l != \'\']:\n            print(\'%s: %s\' % (PREFIX, line))\n\n\ndef debug(message):\n    """""" Print message with verbosity level ``DEBUG``. """"""\n    verbose(message, level=DEBUG)\n\n\ndef normal(message):\n    """""" Print message with verbosity level ``NORMAL``. """"""\n    verbose(message, level=NORMAL)\n\n\ndef error(message, exit_code=1):\n    """""" Print message and exit with desired code. """"""\n    for line in [l for l in message.split(\'\\n\') if l != \'\']:\n        print(\'%s: error: %s\' % (PREFIX, line))\n    sys.exit(exit_code)\n'"
bloscpack/memory_io.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# vim :set ft=py:\n\nimport blosc\nfrom six.moves import xrange\n\nfrom .abstract_io import (PlainSource,\n                          CompressedSource,\n                          PlainSink,\n                          CompressedSink,\n                          )\n\n\nclass PlainMemorySource(PlainSource):\n\n    def __init__(self, chunks):\n        self.chunks = chunks\n\n    def __iter__(self):\n        for c in self.chunks:\n            yield c\n\n\nclass CompressedMemorySource(CompressedSource):\n\n    @property\n    def metadata(self):\n        return self.compressed_memory_sink.metadata\n\n    def __init__(self, compressed_memory_sink):\n        self.compressed_memory_sink = compressed_memory_sink\n        self.checksum_impl = compressed_memory_sink.checksum_impl\n        self.checksum = compressed_memory_sink.checksum\n        self.nchunks = compressed_memory_sink.nchunks\n\n        self.chunks = compressed_memory_sink.chunks\n        if self.checksum:\n            self.checksums = compressed_memory_sink.checksums\n\n    def __iter__(self):\n        for i in xrange(self.nchunks):\n            compressed = self.chunks[i]\n            digest = self.checksums[i] if self.checksum else None\n            yield compressed, digest\n\n\nclass PlainMemorySink(PlainSink):\n\n    def __init__(self, nchunks=None):\n        if nchunks is not None:\n            self.have_chunks = True\n            self.chunks = [None] * nchunks\n            self.i = 0\n        else:\n            self.have_chunks = False\n            self.chunks = []\n\n    def put(self, compressed):\n        chunk = blosc.decompress(compressed)\n        if self.have_chunks:\n            self.chunks[self.i] = chunk\n            self.i += 1\n        else:\n            self.chunks.append(chunk)\n        return len(chunk)\n\n\nclass CompressedMemorySink(CompressedSink):\n\n    def configure(self, blosc_args, bloscpack_header):\n        self.blosc_args = blosc_args\n        self.bloscpack_header = bloscpack_header\n        self.checksum_impl = bloscpack_header.checksum_impl\n        self.checksum = bloscpack_header.checksum\n        self.nchunks = bloscpack_header.nchunks\n\n        self.chunks = [None] * self.bloscpack_header.nchunks\n        if self.checksum:\n            self.checksums = [None] * self.bloscpack_header.nchunks\n\n        self.metadata = None\n        self.metadata_args = None\n\n    def write_bloscpack_header(self):\n        # no op\n        pass\n\n    def write_metadata(self, metadata, metadata_args):\n        self.metadata = metadata\n        self.metadata_args = metadata_args\n\n    def init_offsets(self):\n        # no op\n        pass\n\n    def put(self, i, compressed):\n        self.chunks[i] = compressed\n        if self.checksum:\n            self.checksums[i] = self.do_checksum(compressed)\n'"
bloscpack/metacodecs.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# vim :set ft=py:\n\n\nimport zlib\n\n\nfrom .exceptions import NoSuchCodec\n\n\nclass MetaCodec(object):\n    """""" Uniform codec object.\n\n    Parameters\n    ----------\n    name : str\n        the name of the codec\n    compress : callable\n        a compression function taking data and level as args\n    decompress : callable\n        a decompression function taking data as arg\n\n    """"""\n\n    def __init__(self, name, compress, decompress):\n        self.name = name\n        self._compress = compress\n        self._decompress = decompress\n\n    def compress(self, data, level):\n        return self._compress(data, level)\n\n    def decompress(self, data):\n        return self._decompress(data)\n\nCODECS = [MetaCodec(\'None\', lambda data, level: data, lambda data: data),\n          MetaCodec(\'zlib\', zlib.compress, zlib.decompress)]\nCODECS_AVAIL = [c.name for c in CODECS]\nCODECS_LOOKUP = dict(((c.name, c) for c in CODECS))\n\n\ndef check_valid_codec(codec):\n    """""" Check the validity of a codec.\n\n    Parameters\n    ----------\n    codec : str\n        the string descriptor of the codec\n\n    Raises\n    ------\n    NoSuchCodec\n        if no such checksum exists.\n    """"""\n    if codec not in CODECS_AVAIL:\n        raise NoSuchCodec(""codec \'%s\' does not exist"" % codec)\n'"
bloscpack/numpy_io.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# vim :set ft=py:\n\n\nimport ast\n\nimport blosc\nimport numpy\nimport six\nfrom six.moves import xrange\nfrom deprecated import deprecated\n\n\nfrom .abstract_io import (pack,\n                          unpack,\n                          )\nfrom .compat_util import StringIO\nfrom .file_io import (CompressedFPSource,\n                      CompressedFPSink,\n                      )\nfrom .args import (BloscArgs,\n                   calculate_nchunks,\n                   )\nfrom .defaults import (DEFAULT_CHUNK_SIZE,\n                       )\nfrom .abstract_io import (PlainSource,\n                          PlainSink,\n                          )\nfrom .exceptions import (NotANumpyArray,\n                         ObjectNumpyArrayRejection,\n                         ChunkSizeTypeSizeMismatch,\n                         )\nfrom .pretty import (double_pretty_size,\n                     )\nfrom . import log\n\n\ndef _compress_chunk_ptr(chunk, blosc_args):\n    ptr, size = chunk\n    return blosc.compress_ptr(ptr, size, **blosc_args)\n\n\ndef _ndarray_meta(ndarray):\n    # Reagrding the dtype, quote from numpy/lib/format.py:dtype_to_descr\n    #\n    # The .descr attribute of a dtype object cannot be round-tripped\n    # through the dtype() constructor. Simple types, like dtype(\'float32\'),\n    # have a descr which looks like a record array with one field with \'\'\n    # as a name. The dtype() constructor interprets this as a request to\n    # give a default name.  Instead, we construct descriptor that can be\n    # passed to dtype().\n    return {\'dtype\': repr(ndarray.dtype.descr)\n            if ndarray.dtype.names is not None\n            else repr(ndarray.dtype.str),\n            \'shape\': ndarray.shape,\n            \'order\': \'F\' if numpy.isfortran(ndarray) else \'C\',\n            \'container\': \'numpy\',\n            }\n\n\nclass PlainNumpySource(PlainSource):\n\n    def __init__(self, ndarray):\n\n        self.metadata = _ndarray_meta(ndarray)\n        self.size = ndarray.size * ndarray.itemsize\n        # The following is guesswork\n        # non contiguous fortran array (if ever such a thing exists)\n        if numpy.isfortran(ndarray) and not ndarray.flags[\'F_CONTIGUOUS\']:\n            self.ndarray = numpy.asfortranarray(ndarray)\n        # non contiguous C array\n        elif not numpy.isfortran(ndarray) and not ndarray.flags[\'C_CONTIGUOUS\']:\n            self.ndarray = numpy.ascontiguousarray(ndarray)\n        # contiguous fortran or C array, do nothing\n        else:\n            self.ndarray = ndarray\n        self.ptr = self.ndarray.__array_interface__[\'data\'][0]\n\n    @property\n    def compress_func(self):\n        return _compress_chunk_ptr\n\n    def __iter__(self):\n        if self.chunk_size % self.ndarray.itemsize != 0:\n                raise ChunkSizeTypeSizeMismatch(\n                    ""chunk_size: \'%s\' is not divisible bytypesize: \'%i\'"" %\n                    (double_pretty_size(self.chunk_size), self.ndarray.itemsize)\n                )\n        self.nitems = int(self.chunk_size / self.ndarray.itemsize)\n        offset = self.ptr\n        for i in xrange(self.nchunks - 1):\n            yield offset, self.nitems\n            offset += self.chunk_size\n        yield offset, int(self.last_chunk / self.ndarray.itemsize)\n\n\ndef _conv(descr):\n    """""" Converts nested list of lists into list of tuples.\n\n    Needed for backwards compatability, see below.\n\n    Examples\n    --------\n     [[u\'a\', u\'f8\']] -> [(\'a\', \'f8\')]\n     [[u\'a\', u\'f8\', 2]] -> [(\'a\', \'f8\', 2)]\n     [[u\'a\', [[u\'b\', \'f8\']]]] -> [(\'a\', [(\'b\', \'f8\')])]\n\n    """"""\n    if isinstance(descr, list):\n        if isinstance(descr[0], list):\n            descr = [_conv(d) for d in descr]\n        else:\n            descr = tuple([_conv(d) for d in descr])\n    elif six.PY2 and isinstance(descr, unicode):  # pragma: no cover\n        descr = str(descr)\n    return descr\n\n\nclass PlainNumpySink(PlainSink):\n\n    def __init__(self, metadata):\n        self.metadata = metadata\n        if metadata is None or metadata[\'container\'] != \'numpy\':\n            raise NotANumpyArray\n        # The try except is a backwards compatability hack for the old way of\n        # serializing ndarray dtype which was used prior to 0.7.2. For basic\n        # dtyepes, the dtype \'descr\' was serialized directly to json and not\n        # via \'repr\'.  As such, it does not need to be evaluated, but instead\n        # is already a string that can be passed to the constructor. It will\n        # raise a SyntaxError in this case. For nested dtypes we have the\n        # problem, that it did compress the files but was unable to decompress\n        # them. In this case, it will raise a TypeError and the _conv function\n        # above is used to convert the dtype accordingly.\n        try:\n            dtype_ = ast.literal_eval(metadata[\'dtype\'])\n        except (ValueError, SyntaxError):\n            dtype_ = _conv(metadata[\'dtype\'])\n        self.ndarray = numpy.empty(metadata[\'shape\'],\n                                   dtype=numpy.dtype(dtype_),\n                                   order=metadata[\'order\'])\n        self.ptr = self.ndarray.__array_interface__[\'data\'][0]\n\n    def put(self, compressed):\n        bwritten = blosc.decompress_ptr(compressed, self.ptr)\n        self.ptr += bwritten\n        return bwritten\n\n\ndef pack_ndarray(ndarray, sink,\n                 chunk_size=DEFAULT_CHUNK_SIZE,\n                 blosc_args=None,\n                 bloscpack_args=None,\n                 metadata_args=None):\n    """""" Serialialize a Numpy array.\n\n    Parameters\n    ----------\n    ndarray : ndarray\n        the numpy array to serialize\n    sink : CompressedSink\n        the sink to serialize to\n    blosc_args : BloscArgs\n        blosc args\n    bloscpack_args : BloscpackArgs\n        bloscpack args\n    metadata_args : MetadataArgs\n        the args for the metadata\n\n    Notes\n    -----\n\n    The \'typesize\' value of \'blosc_args\' will be silently ignored and replaced\n    with the itemsize of the Numpy array\'s dtype.\n\n    """"""\n    if ndarray.dtype.hasobject:\n        raise ObjectNumpyArrayRejection\n    if blosc_args is None:\n        blosc_args = BloscArgs(typesize=ndarray.dtype.itemsize)\n    else:\n        log.debug(""Ignoring \'typesize\' in blosc_args"")\n        blosc_args.typesize = ndarray.dtype.itemsize\n    source = PlainNumpySource(ndarray)\n    nchunks, chunk_size, last_chunk_size = \\\n        calculate_nchunks(source.size, chunk_size)\n    pack(source, sink,\n         nchunks, chunk_size, last_chunk_size,\n         metadata=source.metadata,\n         blosc_args=blosc_args,\n         bloscpack_args=bloscpack_args,\n         metadata_args=metadata_args)\n    #out_file_size = path.getsize(file_pointer)\n    #log.verbose(\'output file size: %s\' % double_pretty_size(out_file_size))\n    #log.verbose(\'compression ratio: %f\' % (out_file_size/source.size))\n\n\ndef pack_ndarray_to_file(ndarray, filename,\n                         chunk_size=DEFAULT_CHUNK_SIZE,\n                         blosc_args=None,\n                         bloscpack_args=None,\n                         metadata_args=None):\n    """""" Serialialize a Numpy array to a file.\n\n    Parameters\n    ----------\n    ndarray : ndarray\n        the numpy array to serialize\n    filename : str\n        the file to compress to\n    blosc_args : BloscArgs\n        blosc args\n    bloscpack_args : BloscpackArgs\n        bloscpack args\n    metadata_args : MetadataArgs\n        the args for the metadata\n\n    Notes\n    -----\n    The \'typesize\' value of \'blosc_args\' will be silently ignored and replaced\n    with the itemsize of the Numpy array\'s dtype.\n\n    """"""\n    with open(filename, \'wb\') as fp:\n        sink = CompressedFPSink(fp)\n        pack_ndarray(ndarray, sink,\n                     chunk_size=chunk_size,\n                     blosc_args=blosc_args,\n                     bloscpack_args=bloscpack_args,\n                     metadata_args=metadata_args)\n\n\npack_ndarray_file = deprecated(pack_ndarray_to_file,\n                               version=\'0.16.0\',\n                               reason=""Use \'pack_ndarray_to_file\' instead.""\n                               )\n\n\ndef pack_ndarray_to_bytes(ndarray,\n                          chunk_size=DEFAULT_CHUNK_SIZE,\n                          blosc_args=None,\n                          bloscpack_args=None,\n                          metadata_args=None):\n    """""" Serialialize a Numpy array to bytes_\n\n    Parameters\n    ----------\n    ndarray : ndarray\n        the numpy array to serialize\n    filename : str\n        the file to compress to\n    blosc_args : BloscArgs\n        blosc args\n    bloscpack_args : BloscpackArgs\n        bloscpack args\n    metadata_args : MetadataArgs\n        the args for the metadata\n\n    Returns\n    -------\n    bytes_ : bytes\n        compressed bytes\n\n    Notes\n    -----\n    The \'typesize\' value of \'blosc_args\' will be silently ignored and replaced\n    with the itemsize of the Numpy array\'s dtype.\n\n    """"""\n    sio = StringIO()\n    sink = CompressedFPSink(sio)\n    pack_ndarray(ndarray, sink,\n                 chunk_size=chunk_size,\n                 blosc_args=blosc_args,\n                 bloscpack_args=bloscpack_args,\n                 metadata_args=metadata_args)\n    return sio.getvalue()\n\n\npack_ndarray_str = deprecated(pack_ndarray_to_bytes,\n                              version=\'0.16.0\',\n                              reason=""Use \'pack_ndarray_to_bytes\' instead.""\n                              )\n\n\ndef unpack_ndarray(source):\n    """""" Deserialize a Numpy array.\n\n    Parameters\n    ----------\n    source : CompressedSource\n        the source containing the serialized Numpy array\n\n    Returns\n    -------\n    ndarray : ndarray\n        the Numpy array\n\n    Raises\n    ------\n    NotANumpyArray\n        if the source doesn\'t seem to contain a Numpy array\n    """"""\n\n    sink = PlainNumpySink(source.metadata)\n    unpack(source, sink)\n    return sink.ndarray\n\n\ndef unpack_ndarray_from_file(filename):\n    """""" Deserialize a Numpy array from a file.\n\n    Parameters\n    ----------\n    filename : str\n        the file to decompress from\n\n    Returns\n    -------\n    ndarray : ndarray\n        the Numpy array\n\n    Raises\n    ------\n    NotANumpyArray\n        if the source doesn\'t seem to contain a Numpy array\n    """"""\n    source = CompressedFPSource(open(filename, \'rb\'))\n    return unpack_ndarray(source)\n\n\nunpack_ndarray_file = deprecated(unpack_ndarray_from_file,\n                                 version=\'0.16.0\',\n                                 reason=""Use \'pack_ndarray_from_file\' instead.""\n                                 )\n\n\ndef unpack_ndarray_from_bytes(bytes_):\n    """""" Deserialize a Numpy array from bytes.\n\n    Parameters\n    ----------\n    bytes_ : bytes\n        the bytes to decompress from\n\n    Returns\n    -------\n    ndarray : ndarray\n        the Numpy array\n\n    Raises\n    ------\n    NotANumpyArray\n        if the source doesn\'t seem to contain a Numpy array\n    """"""\n    sio = StringIO(bytes_)\n    source = CompressedFPSource(sio)\n    return unpack_ndarray(source)\n\n\nunpack_ndarray_str = deprecated(unpack_ndarray_from_bytes,\n                                version=\'0.16.0\',\n                                reason=""Use \'pack_ndarray_from_bytes\' instead.""\n                                )\n'"
bloscpack/pretty.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# vim :set ft=py:\n\nfrom __future__ import division\n\n\nfrom .constants import SUFFIXES\n\n\ndef pretty_size(size_in_bytes):\n    """""" Pretty print filesize.  """"""\n    if size_in_bytes == 0:\n        return ""0B""\n    for suf, lim in reversed(sorted(SUFFIXES.items(), key=lambda x: x[1])):\n        if size_in_bytes < lim:\n            continue\n        else:\n            return str(round(size_in_bytes/lim, 2))+suf\n\n\ndef double_pretty_size(size_in_bytes):\n    """""" Pretty print filesize including size in bytes. """"""\n    return (""%s (%dB)"" % (pretty_size(size_in_bytes), size_in_bytes))\n\n\ndef reverse_pretty(readable):\n    """""" Reverse pretty printed file size. """"""\n    # otherwise we assume it has a suffix\n    suffix = readable[-1]\n    if suffix not in SUFFIXES.keys():\n        raise ValueError(\n                ""\'%s\' is not a valid prefix multiplier, use one of: \'%s\'"" %\n                (suffix, SUFFIXES.keys()))\n    else:\n        return int(float(readable[:-1]) * SUFFIXES[suffix])\n\n\ndef join_with_eol(items):\n    return \', \'.join(items) + \'\\n\'\n'"
bloscpack/serializers.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# vim :set ft=py:\n\nimport json\n\n\nfrom .exceptions import NoSuchSerializer\n\n\nclass Serializer(object):\n    """""" Uniform serializer object.\n\n    Parameters\n    ----------\n    name : str\n        the name of the serializer\n    compress : callable\n        a compression function taking a dict as arg\n    decompress : callable\n        a decompression function taking serialized data as arg\n\n    """"""\n    def __init__(self, name, dumps, loads):\n        self.name = name\n        self._loads = loads\n        self._dumps = dumps\n\n    def dumps(self, dict_):\n        return self._dumps(dict_)\n\n    def loads(self, data):\n        return self._loads(data)\n\n\nSERIALIZERS = [Serializer(b\'JSON\',\n                  lambda x: json.dumps(x, separators=(\',\', \':\')),\n                  lambda x: json.loads(x))]\nSERIALIZERS_AVAIL = [s.name for s in SERIALIZERS]\nSERIALIZERS_LOOKUP = dict(((s.name, s) for s in SERIALIZERS))\n\n\ndef check_valid_serializer(serializer):\n    """""" Check the validity of a serializer.\n\n    Parameters\n    ----------\n    serializer : str\n        the magic format of the serializer\n\n    Raises\n    ------\n    NoSuchSerializer\n        if no such serializer exists.\n    """"""\n    if serializer not in SERIALIZERS_AVAIL:\n        raise NoSuchSerializer(""serializer \'%s\' does not exist"" % serializer)\n\n'"
bloscpack/sysutil.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# vim :set ft=py:\n\n\nimport os  # pragma: no cover\n\n\ndef drop_caches():  # pragma: no cover\n    if os.geteuid() == 0:\n        os.system('echo 3 > /proc/sys/vm/drop_caches')\n    else:\n        raise RuntimeError('Need root permission to drop caches')\n\n\ndef sync():  # pragma: no cover\n    os.system('sync')\n"""
bloscpack/testutil.py,1,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# vim :set ft=py:\n\n\nfrom __future__ import print_function\n\n\nimport atexit\nimport contextlib\nimport os.path as path\nimport shutil\nimport sys\nimport tempfile\n\n\nimport numpy as np\n\n\nfrom .defaults import (DEFAULT_CHUNK_SIZE,\n                       )\nfrom .pretty import (reverse_pretty\n                     )\n\n\ndef simple_progress(i):\n    if i % 10 == 0:\n        print(\'.\', end=\'\')\n    sys.stdout.flush()\n\n\ndef create_array(repeats, in_file, progress=False):\n    with open(in_file, \'wb\') as in_fp:\n        create_array_fp(repeats, in_fp, progress=progress)\n\n\ndef create_array_fp(repeats, in_fp, progress=False):\n    for i in range(repeats):\n        array_ = np.linspace(i, i+1, 2e6)\n        in_fp.write(array_.tostring())\n        if progress:\n            progress(i)\n    in_fp.flush()\n    if progress:\n        print(\'done\')\n\n\ndef atexit_tmpremover(dirname):\n    try:\n        shutil.rmtree(dirname)\n        print(""Removed temporary directory on abort: %s"" % dirname)\n    except OSError:\n        # if the temp dir was removed already, by the context manager\n        pass\n\n\n@contextlib.contextmanager\ndef create_tmp_files():\n    tdir = tempfile.mkdtemp(prefix=\'bloscpack-\')\n    in_file = path.join(tdir, \'file\')\n    out_file = path.join(tdir, \'file.blp\')\n    dcmp_file = path.join(tdir, \'file.dcmp\')\n    # register the temp dir remover, safeguard against abort\n    atexit.register(atexit_tmpremover, tdir)\n    yield tdir, in_file, out_file, dcmp_file\n    # context manager remover\n    shutil.rmtree(tdir)\n\n\ndef cmp_file(file1, file2):\n    """""" File comparison utility with a small chunksize """"""\n    with open(file1, \'rb\') as fp1, open(file2, \'rb\') as fp2:\n        cmp_fp(fp1, fp2)\n\n\ndef cmp_fp(fp1, fp2):\n    import nose.tools as nt  # nose is a testing dependency\n    chunk_size = reverse_pretty(DEFAULT_CHUNK_SIZE)\n    while True:\n        a = fp1.read(chunk_size)\n        b = fp2.read(chunk_size)\n        if a == b\'\' and b == b\'\':\n            return True\n        else:\n            nt.assert_equal(a, b)\n'"
bloscpack/util.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# vim :set ft=py:\n\n\nimport contextlib\nimport sys\n\n\nPYTHON_VERSION = sys.version_info[0:3]\nif sys.version_info < (2, 7, 5):  # pragma: no cover\n    memoryview = lambda x: x\nelse:\n    memoryview = memoryview\n'"
bloscpack/version.py,0,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# vim :set ft=py:\n\n__version__ = '0.17.0.dev0'\n__author__ = 'Valentin Haenel <valentin@haenel.co>'\n"""
test/test_append.py,1,"b'#!/usr/bin/env nosetests\n# -*- coding: utf-8 -*-\n# vim :set ft=py:\n\n\nimport blosc\nimport nose.tools as nt\nimport numpy as np\n\n\nfrom bloscpack.abstract_io import (pack,\n                                   unpack,\n                                   )\nfrom bloscpack.append import (append,\n                              append_fp,\n                              _recreate_metadata,\n                              _rewrite_metadata_fp,\n                              )\nfrom bloscpack.args import (BloscArgs,\n                            BloscpackArgs,\n                            calculate_nchunks,\n                            MetadataArgs,\n                            )\nfrom bloscpack.checksums import (CHECKSUMS_LOOKUP,\n                                 )\nfrom bloscpack.compat_util import StringIO\nfrom bloscpack.constants import (METADATA_HEADER_LENGTH,\n                                 )\nfrom bloscpack.exceptions import (NotEnoughSpace,\n                                  NoSuchSerializer,\n                                  NoSuchCodec,\n                                  ChecksumLengthMismatch,\n                                  NoChangeInMetadata,\n                                  MetadataSectionTooSmall,\n                                  )\nfrom bloscpack.file_io import (PlainFPSource,\n                               PlainFPSink,\n                               CompressedFPSource,\n                               CompressedFPSink,\n                               pack_file_to_file,\n                               unpack_file_from_file,\n                               _read_beginning,\n                               _read_compressed_chunk_fp,\n                               _write_metadata,\n                               _read_metadata,\n                               )\nfrom bloscpack.headers import (BloscpackHeader,\n                               MetadataHeader,\n                               )\nfrom bloscpack.serializers import (SERIALIZERS,\n                                   )\nfrom bloscpack.testutil import (create_array,\n                                create_array_fp,\n                                create_tmp_files,\n                                )\n\n\ndef prep_array_for_append(blosc_args=BloscArgs(),\n                          bloscpack_args=BloscpackArgs()):\n    orig, new, dcmp = StringIO(), StringIO(), StringIO()\n    create_array_fp(1, new)\n    new_size = new.tell()\n    new.seek(0)\n    chunking = calculate_nchunks(new_size)\n    source = PlainFPSource(new)\n    sink = CompressedFPSink(orig)\n    pack(source, sink, *chunking,\n         blosc_args=blosc_args,\n         bloscpack_args=bloscpack_args)\n    orig.seek(0)\n    new.seek(0)\n    return orig, new, new_size, dcmp\n\n\ndef reset_append_fp(original_fp, new_content_fp, new_size, blosc_args=None):\n    """""" like ``append_fp`` but with ``seek(0)`` on the file pointers. """"""\n    nchunks = append_fp(original_fp, new_content_fp, new_size,\n                        blosc_args=blosc_args)\n    original_fp.seek(0)\n    new_content_fp.seek(0)\n    return nchunks\n\n\ndef reset_read_beginning(input_fp):\n    """""" like ``_read_beginning`` but with ``seek(0)`` on the file pointer. """"""\n    ans = _read_beginning(input_fp)\n    input_fp.seek(0)\n    return ans\n\n\ndef test_append_fp():\n    orig, new, new_size, dcmp = prep_array_for_append()\n\n    # check that the header and offsets are as we expected them to be\n    orig_bloscpack_header, orig_offsets = reset_read_beginning(orig)[0:4:3]\n    expected_orig_bloscpack_header = BloscpackHeader(\n            format_version=3,\n            offsets=True,\n            metadata=False,\n            checksum=\'adler32\',\n            typesize=8,\n            chunk_size=1048576,\n            last_chunk=271360,\n            nchunks=16,\n            max_app_chunks=160,\n            )\n    expected_orig_offsets = [1440]\n    nt.assert_equal(expected_orig_bloscpack_header, orig_bloscpack_header)\n    nt.assert_equal(expected_orig_offsets[0], orig_offsets[0])\n\n    # perform the append\n    reset_append_fp(orig, new, new_size)\n\n    # check that the header and offsets are as we expected them to be after\n    # appending\n    app_bloscpack_header, app_offsets = reset_read_beginning(orig)[0:4:3]\n    expected_app_bloscpack_header = {\n            \'chunk_size\': 1048576,\n            \'nchunks\': 31,\n            \'last_chunk\': 542720,\n            \'max_app_chunks\': 145,\n            \'format_version\': 3,\n            \'offsets\': True,\n            \'checksum\': \'adler32\',\n            \'typesize\': 8,\n            \'metadata\': False\n    }\n    expected_app_offsets = [1440]\n    nt.assert_equal(expected_app_bloscpack_header, app_bloscpack_header)\n    nt.assert_equal(expected_app_offsets[0], app_offsets[0])\n\n    # now check by unpacking\n    source = CompressedFPSource(orig)\n    sink = PlainFPSink(dcmp)\n    unpack(source, sink)\n    dcmp.seek(0)\n    new.seek(0)\n    new_str = new.read()\n    dcmp_str = dcmp.read()\n    nt.assert_equal(len(dcmp_str), len(new_str * 2))\n    nt.assert_equal(dcmp_str, new_str * 2)\n\n    ## TODO\n    # * check additional aspects of file integrity\n    #   * offsets OK\n    #   * metadata OK\n\n\ndef test_append():\n    with create_tmp_files() as (tdir, in_file, out_file, dcmp_file):\n        create_array(1, in_file)\n        pack_file_to_file(in_file, out_file)\n        append(out_file, in_file)\n        unpack_file_from_file(out_file, dcmp_file)\n        in_content = open(in_file, \'rb\').read()\n        dcmp_content = open(dcmp_file, \'rb\').read()\n        nt.assert_equal(len(dcmp_content), len(in_content) * 2)\n        nt.assert_equal(dcmp_content, in_content * 2)\n\n\ndef test_append_into_last_chunk():\n    # first create an array with a single chunk\n    orig, new, dcmp = StringIO(), StringIO(), StringIO()\n    create_array_fp(1, new)\n    new_size = new.tell()\n    new.seek(0)\n    chunking = calculate_nchunks(new_size, chunk_size=new_size)\n    source = PlainFPSource(new)\n    sink = CompressedFPSink(orig)\n    pack(source, sink, *chunking)\n    orig.seek(0)\n    new.seek(0)\n    # append a few bytes, creating a new, smaller, last_chunk\n    new_content = new.read()\n    new.seek(0)\n    nchunks = reset_append_fp(orig, StringIO(new_content[:1023]), 1023)\n    bloscpack_header = reset_read_beginning(orig)[0]\n    nt.assert_equal(nchunks, 1)\n    nt.assert_equal(bloscpack_header[\'last_chunk\'], 1023)\n    # now append into that last chunk\n    nchunks = reset_append_fp(orig, StringIO(new_content[:1023]), 1023)\n    bloscpack_header = reset_read_beginning(orig)[0]\n    nt.assert_equal(nchunks, 0)\n    nt.assert_equal(bloscpack_header[\'last_chunk\'], 2046)\n\n    # now check by unpacking\n    source = CompressedFPSource(orig)\n    sink = PlainFPSink(dcmp)\n    unpack(source, sink)\n    dcmp.seek(0)\n    new.seek(0)\n    new_str = new.read()\n    dcmp_str = dcmp.read()\n    nt.assert_equal(len(dcmp_str), len(new_str) + 2046)\n    nt.assert_equal(dcmp_str, new_str + new_str[:1023] * 2)\n\n\ndef test_append_single_chunk():\n    orig, new, dcmp = StringIO(), StringIO(), StringIO()\n    create_array_fp(1, new)\n    new_size = new.tell()\n    new.seek(0)\n    chunking = calculate_nchunks(new_size, chunk_size=new_size)\n    source = PlainFPSource(new)\n    sink = CompressedFPSink(orig)\n    pack(source, sink, *chunking)\n    orig.seek(0)\n    new.seek(0)\n\n    # append a single chunk\n    reset_append_fp(orig, new, new_size)\n    bloscpack_header = reset_read_beginning(orig)[0]\n    nt.assert_equal(bloscpack_header[\'nchunks\'], 2)\n\n    # append a large content, that amounts to two chunks\n    new_content = new.read()\n    new.seek(0)\n    reset_append_fp(orig, StringIO(new_content * 2), new_size * 2)\n    bloscpack_header = reset_read_beginning(orig)[0]\n    nt.assert_equal(bloscpack_header[\'nchunks\'], 4)\n\n    # append half a chunk\n    reset_append_fp(orig, StringIO(new_content[:len(new_content)]), new_size//2)\n    bloscpack_header = reset_read_beginning(orig)[0]\n    nt.assert_equal(bloscpack_header[\'nchunks\'], 5)\n\n    # append a few bytes\n    reset_append_fp(orig, StringIO(new_content[:1023]), 1024)\n    # make sure it is squashed into the lat chunk\n    bloscpack_header = reset_read_beginning(orig)[0]\n    nt.assert_equal(bloscpack_header[\'nchunks\'], 5)\n\n\ndef test_double_append():\n    orig, new, new_size, dcmp = prep_array_for_append()\n    reset_append_fp(orig, new, new_size)\n    reset_append_fp(orig, new, new_size)\n    new_str = new.read()\n    source = CompressedFPSource(orig)\n    sink = PlainFPSink(dcmp)\n    unpack(source, sink)\n    dcmp.seek(0)\n    dcmp_str = dcmp.read()\n    nt.assert_equal(len(dcmp_str), len(new_str) * 3)\n    nt.assert_equal(dcmp_str, new_str * 3)\n\n\ndef test_append_metadata():\n    orig, new, dcmp = StringIO(), StringIO(), StringIO()\n    create_array_fp(1, new)\n    new_size = new.tell()\n    new.seek(0)\n\n    metadata = {""dtype"": ""float64"", ""shape"": [1024], ""others"": []}\n    chunking = calculate_nchunks(new_size, chunk_size=new_size)\n    source = PlainFPSource(new)\n    sink = CompressedFPSink(orig)\n    pack(source, sink, *chunking, metadata=metadata)\n    orig.seek(0)\n    new.seek(0)\n    reset_append_fp(orig, new, new_size)\n    source = CompressedFPSource(orig)\n    sink = PlainFPSink(dcmp)\n    ans = unpack(source, sink)\n    print(ans)\n    dcmp.seek(0)\n    new.seek(0)\n    new_str = new.read()\n    dcmp_str = dcmp.read()\n    nt.assert_equal(len(dcmp_str), len(new_str) * 2)\n    nt.assert_equal(dcmp_str, new_str * 2)\n\n\ndef test_append_fp_no_offsets():\n    bloscpack_args = BloscpackArgs(offsets=False)\n    orig, new, new_size, dcmp = prep_array_for_append(bloscpack_args=bloscpack_args)\n    nt.assert_raises(RuntimeError, append_fp, orig, new, new_size)\n\n\ndef test_append_fp_not_enough_space():\n    bloscpack_args = BloscpackArgs(max_app_chunks=0)\n    orig, new, new_size, dcmp = prep_array_for_append(bloscpack_args=bloscpack_args)\n    nt.assert_raises(NotEnoughSpace, append_fp, orig, new, new_size)\n\n\ndef test_mixing_clevel():\n    # the first set of chunks has max compression\n    blosc_args = BloscArgs(clevel=9)\n    orig, new, new_size, dcmp = prep_array_for_append()\n    # get the original size\n    orig.seek(0, 2)\n    orig_size = orig.tell()\n    orig.seek(0)\n    # get a backup of the settings\n    bloscpack_header, metadata, metadata_header, offsets = \\\n            reset_read_beginning(orig)\n    # compressed size of the last chunk, including checksum\n    last_chunk_compressed_size = orig_size - offsets[-1]\n\n    # do append\n    # use the typesize from the file and\n    # make the second set of chunks have no compression\n    blosc_args = BloscArgs(typesize=None, clevel=0)\n    nchunks = append_fp(orig, new, new_size, blosc_args=blosc_args)\n\n    # get the final size\n    orig.seek(0, 2)\n    final_size = orig.tell()\n    orig.seek(0)\n\n    # the original file minus the compressed size of the last chunk\n    discounted_orig_size = orig_size - last_chunk_compressed_size\n    # size of the appended data\n    #  * raw new size, since we have no compression\n    #  * uncompressed size of the last chunk\n    #  * nchunks + 1 times the blosc and checksum overhead\n    appended_size = new_size + bloscpack_header[\'last_chunk\'] + (nchunks+1) * (16 + 4)\n    # final size should be original plus appended data\n    nt.assert_equal(final_size, appended_size + discounted_orig_size)\n\n    # check by unpacking\n    source = CompressedFPSource(orig)\n    sink = PlainFPSink(dcmp)\n    unpack(source, sink)\n    dcmp.seek(0)\n    new.seek(0)\n    new_str = new.read()\n    dcmp_str = dcmp.read()\n    nt.assert_equal(len(dcmp_str), len(new_str * 2))\n    nt.assert_equal(dcmp_str, new_str * 2)\n\n\ndef test_append_mix_shuffle():\n    orig, new, new_size, dcmp = prep_array_for_append()\n    # use the typesize from the file\n    # deactivate shuffle\n    # crank up the clevel to ensure compression happens, otherwise the flags\n    # will be screwed later on\n    blosc_args = BloscArgs(typesize=None, shuffle=False, clevel=9)\n\n    # need to create something that will be compressible even without shuffle,\n    # the linspace used in \'new\' doesn\'t work anymore as of python-blosc 1.6.1\n    to_append = np.zeros(int(2e6))\n    to_append_fp = StringIO()\n    to_append_fp.write(to_append.tostring())\n    to_append_fp_size = to_append_fp.tell()\n    to_append_fp.seek(0)\n\n    # now do the append\n    reset_append_fp(orig, to_append_fp, to_append_fp_size, blosc_args=blosc_args)\n\n    # decompress \'orig\' so that we can examine it\n    source = CompressedFPSource(orig)\n    sink = PlainFPSink(dcmp)\n    unpack(source, sink)\n    orig.seek(0)\n    dcmp.seek(0)\n    new.seek(0)\n    new_str = new.read()\n    dcmp_str = dcmp.read()\n\n    # now sanity check the length and content of the decompressed\n    nt.assert_equal(len(dcmp_str), len(new_str) + to_append_fp_size)\n    nt.assert_equal(dcmp_str, new_str + to_append.tostring())\n\n    # now get the first and the last chunk and check that the shuffle doesn\'t\n    # match\n    bloscpack_header, offsets = reset_read_beginning(orig)[0:4:3]\n    orig.seek(offsets[0])\n    checksum_impl = CHECKSUMS_LOOKUP[bloscpack_header[\'checksum\']]\n    compressed_zero,  blosc_header_zero, digest = \\\n        _read_compressed_chunk_fp(orig, checksum_impl)\n    decompressed_zero = blosc.decompress(compressed_zero)\n    orig.seek(offsets[-1])\n    compressed_last,  blosc_header_last, digest = \\\n        _read_compressed_chunk_fp(orig, checksum_impl)\n    decompressed_last = blosc.decompress(compressed_last)\n    # first chunk has shuffle active\n    nt.assert_equal(blosc_header_zero[\'flags\'], 1)\n    # last chunk doesn\'t\n    nt.assert_equal(blosc_header_last[\'flags\'], 0)\n\n\ndef test_recreate_metadata():\n    old_meta_header = MetadataHeader(magic_format=b\'\',\n                                     meta_options=""00000000"",\n                                     meta_checksum=\'None\',\n                                     meta_codec=\'None\',\n                                     meta_level=0,\n                                     meta_size=0,\n                                     max_meta_size=0,\n                                     meta_comp_size=0,\n                                     user_codec=b\'\',\n                                     )\n    header_dict = old_meta_header\n    nt.assert_raises(NoSuchSerializer,\n                     _recreate_metadata,\n                     header_dict,\n                     \'\',\n                     magic_format=\'NOSUCHSERIALIZER\')\n    nt.assert_raises(NoSuchCodec,\n                     _recreate_metadata,\n                     header_dict,\n                     \'\',\n                     codec=\'NOSUCHCODEC\')\n    nt.assert_raises(ChecksumLengthMismatch,\n                     _recreate_metadata,\n                     header_dict,\n                     \'\',\n                     checksum=\'adler32\')\n\n\ndef test_rewrite_metadata():\n    test_metadata = {\'dtype\': \'float64\',\n                     \'shape\': [1024],\n                     \'others\': [],\n                     }\n    # assemble the metadata args from the default\n    metadata_args = MetadataArgs()\n    # avoid checksum and codec\n    metadata_args.meta_checksum = \'None\'\n    metadata_args.meta_codec = \'None\'\n    # preallocate a fixed size\n    metadata_args.max_meta_size = 1000  # fixed preallocation\n    target_fp = StringIO()\n    # write the metadata section\n    _write_metadata(target_fp, test_metadata, metadata_args)\n    # check that the length is correct\n    nt.assert_equal(METADATA_HEADER_LENGTH + metadata_args.max_meta_size,\n                    len(target_fp.getvalue()))\n\n    # now add stuff to the metadata\n    test_metadata[\'container\'] = \'numpy\'\n    test_metadata[\'data_origin\'] = \'LHC\'\n    # compute the new length\n    new_metadata_length = len(SERIALIZERS[0].dumps(test_metadata))\n    # jam the new metadata into the StringIO\n    target_fp.seek(0, 0)\n    _rewrite_metadata_fp(target_fp, test_metadata,\n                         codec=None, level=None)\n    # now seek back, read the metadata and make sure it has been updated\n    # correctly\n    target_fp.seek(0, 0)\n    result_metadata, result_header = _read_metadata(target_fp)\n    nt.assert_equal(test_metadata, result_metadata)\n    nt.assert_equal(new_metadata_length, result_header.meta_comp_size)\n\n    # make sure that NoChangeInMetadata is raised\n    target_fp.seek(0, 0)\n    nt.assert_raises(NoChangeInMetadata, _rewrite_metadata_fp,\n                     target_fp, test_metadata, codec=None, level=None)\n\n    # make sure that ChecksumLengthMismatch is raised, needs modified metadata\n    target_fp.seek(0, 0)\n    test_metadata[\'fluxcompensator\'] = \'back to the future\'\n    nt.assert_raises(ChecksumLengthMismatch, _rewrite_metadata_fp,\n                     target_fp, test_metadata,\n                     codec=None, level=None, checksum=\'sha512\')\n\n    # make sure if level is not None, this works\n    target_fp.seek(0, 0)\n    test_metadata[\'hoverboard\'] = \'back to the future 2\'\n    _rewrite_metadata_fp(target_fp, test_metadata,\n                         codec=None)\n\n    # len of metadata when dumped to json should be around 1105\n    for i in range(100):\n        test_metadata[str(i)] = str(i)\n    target_fp.seek(0, 0)\n    nt.assert_raises(MetadataSectionTooSmall, _rewrite_metadata_fp,\n                     target_fp, test_metadata, codec=None, level=None)\n'"
test/test_args.py,0,"b""#!/usr/bin/env nosetests\n# -*- coding: utf-8 -*-\n# vim :set ft=py:\n\n\nfrom unittest import TestCase\n\n\nimport nose.tools as nt\n\n\nfrom bloscpack.args import (DEFAULT_BLOSC_ARGS,\n                            DEFAULT_BLOSCPACK_ARGS,\n                            DEFAULT_METADATA_ARGS,\n                            DEFAULT_TYPESIZE,\n                            DEFAULT_CLEVEL,\n                            DEFAULT_SHUFFLE,\n                            DEFAULT_CNAME,\n                            DEFAULT_OFFSETS,\n                            DEFAULT_CHECKSUM,\n                            DEFAULT_MAX_APP_CHUNKS,\n                            calculate_nchunks,\n                            _handle_max_apps,\n                            _check_blosc_args,\n                            _check_bloscpack_args,\n                            _check_metadata_arguments,\n                            BloscArgs,\n                            BloscpackArgs,\n                            )\nfrom bloscpack.exceptions import ChunkingException\nfrom bloscpack.headers import MAX_CHUNKS\nfrom bloscpack.pretty import reverse_pretty\n\n\ndef test_check_blosc_arguments():\n    missing = DEFAULT_BLOSC_ARGS.copy()\n    missing.pop('typesize')\n    nt.assert_raises(ValueError, _check_blosc_args, missing)\n    extra = DEFAULT_BLOSC_ARGS.copy()\n    extra['wtf'] = 'wtf'\n    nt.assert_raises(ValueError, _check_blosc_args, extra)\n\n\ndef test_check_bloscpack_arguments():\n    missing = DEFAULT_BLOSCPACK_ARGS.copy()\n    missing.pop('offsets')\n    nt.assert_raises(ValueError, _check_bloscpack_args, missing)\n    extra = DEFAULT_BLOSCPACK_ARGS.copy()\n    extra['wtf'] = 'wtf'\n    nt.assert_raises(ValueError, _check_bloscpack_args, extra)\n\n\ndef test_check_bloscpack_arguments_accpets_None_as_checksum():\n    args = BloscpackArgs(checksum=None)\n    nt.assert_equal(args.checksum, 'None')\n\n\ndef test_check_metadata_arguments():\n    missing = DEFAULT_METADATA_ARGS.copy()\n    missing.pop('magic_format')\n    nt.assert_raises(ValueError, _check_metadata_arguments, missing)\n    extra = DEFAULT_METADATA_ARGS.copy()\n    extra['wtf'] = 'wtf'\n    nt.assert_raises(ValueError, _check_metadata_arguments, extra)\n\n\ndef test_calculate_nchunks():\n    # check for zero or negative chunk_size\n    nt.assert_raises(ValueError, calculate_nchunks,\n                     23, chunk_size=0)\n    nt.assert_raises(ValueError, calculate_nchunks,\n                     23, chunk_size=-1)\n\n    nt.assert_equal((9, 1, 1), calculate_nchunks(9, chunk_size=1))\n    nt.assert_equal((5, 2, 1), calculate_nchunks(9, chunk_size=2))\n    nt.assert_equal((3, 3, 3), calculate_nchunks(9, chunk_size=3))\n    nt.assert_equal((3, 4, 1), calculate_nchunks(9, chunk_size=4))\n    nt.assert_equal((2, 5, 4), calculate_nchunks(9, chunk_size=5))\n    nt.assert_equal((2, 6, 3), calculate_nchunks(9, chunk_size=6))\n    nt.assert_equal((2, 7, 2), calculate_nchunks(9, chunk_size=7))\n    nt.assert_equal((2, 8, 1), calculate_nchunks(9, chunk_size=8))\n    nt.assert_equal((1, 9, 9), calculate_nchunks(9, chunk_size=9))\n\n    # check downgrade\n    nt.assert_equal((1, 23, 23), calculate_nchunks(23, chunk_size=24))\n\n    # single byte file\n    nt.assert_equal((1, 1,  1),\n                    calculate_nchunks(1, chunk_size=1))\n\n    # check that a zero length input is handled correctly\n    nt.assert_equal((1, 0,  0),\n                    calculate_nchunks(0, chunk_size=1))\n    # check that the chunk_size is ignored in this case\n    nt.assert_equal((1, 0,  0),\n                    calculate_nchunks(0, chunk_size=512))\n    # in_file_size must be strictly positive\n    nt.assert_raises(ValueError, calculate_nchunks, -1)\n\n    # check overflow of nchunks due to chunk_size being too small\n    # and thus stuff not fitting into the header\n    nt.assert_raises(ChunkingException, calculate_nchunks,\n                     MAX_CHUNKS+1, chunk_size=1)\n\n    # check that strings are converted correctly\n    nt.assert_equal((6, 1048576, 209715),\n                    calculate_nchunks(reverse_pretty('5.2M')))\n    nt.assert_equal((3, 2097152, 1258291),\n                    calculate_nchunks(reverse_pretty('5.2M'),\n                                      chunk_size='2M'))\n\n\ndef test_handle_max_apps():\n    nt.assert_equals(_handle_max_apps(True, 10, 10), 10)\n    nt.assert_equals(_handle_max_apps(True, 10, lambda x: x*10), 100)\n    nt.assert_equals(_handle_max_apps(True, 1, lambda x: MAX_CHUNKS),\n                     MAX_CHUNKS-1)\n    nt.assert_equals(_handle_max_apps(True, 1, lambda x: MAX_CHUNKS+10),\n                     MAX_CHUNKS-1)\n    nt.assert_equals(_handle_max_apps(True, 1, MAX_CHUNKS),\n                     MAX_CHUNKS-1)\n    nt.assert_equals(_handle_max_apps(True, 10, MAX_CHUNKS),\n                     MAX_CHUNKS-10)\n    nt.assert_raises(TypeError, _handle_max_apps, True, 10, 10.0)\n    nt.assert_raises(ValueError, _handle_max_apps,\n                     True, 10, lambda x: -1)\n    nt.assert_raises(ValueError, _handle_max_apps,\n                     True, 10, lambda x: 1.0)\n\n\nclass TestBloscArgs(TestCase):\n\n    def test_init(self):\n        blosc_args = BloscArgs()\n        self.assertEqual(DEFAULT_TYPESIZE, blosc_args.typesize)\n        self.assertEqual(DEFAULT_CLEVEL, blosc_args.clevel)\n        self.assertEqual(DEFAULT_SHUFFLE, blosc_args.shuffle)\n        self.assertEqual(DEFAULT_CNAME, blosc_args.cname)\n\n\nclass TestBloscpackArgs(TestCase):\n\n    def test_init(self):\n        bloscpack_args = BloscpackArgs()\n        self.assertEqual(DEFAULT_OFFSETS, bloscpack_args.offsets)\n        self.assertEqual(DEFAULT_CHECKSUM, bloscpack_args.checksum)\n        self.assertEqual(DEFAULT_MAX_APP_CHUNKS, bloscpack_args.max_app_chunks)\n"""
test/test_checksums.py,0,"b'#!/usr/bin/env nosetests\n# -*- coding: utf-8 -*-\n# vim :set ft=py:\n\n\nimport nose.tools as nt\n\n\nfrom bloscpack import checksums\n\n\ndef test_checksusm_exist():\n    nt.assert_equal(len(checksums.CHECKSUMS), 9)\n    checksums_avail = [\'None\',\n                       \'adler32\',\n                       \'crc32\',\n                       \'md5\',\n                       \'sha1\',\n                       \'sha224\',\n                       \'sha256\',\n                       \'sha384\',\n                       \'sha512\']\n    nt.assert_equal(checksums.CHECKSUMS_AVAIL, checksums_avail)\n\n\ndef test_checksusm_are_sane():\n    # just make sure the hashes do actually compute something.\n    csum_targets = [\n        b\'\',\n        b\'\\x13\\x02\\xc1\\x03\',\n        b\'\\xbd\\xfa.\\xaa\',\n        b\'\\x04\\x8fD\\xd46\\xd5$M\\xd7c0\\xb1$mUC\',\n        b\'\\xae\\xea\\xddm\\x86t\\x86v\\r\\x96O\\x9fuPh\\x1a\\x01!#\\xe6\',\n        b\' (W\\xc8\\x1b\\x14\\x16w\\xec\\xc4\\xd7\\x89xU\\xc5\\x02*\\x15\\xb4q\' +\n        b\'\\xe09\\xd0$\\xe2+{\\x0e\',\n        b\'s\\x83U6N\\x81\\xa7\\xd8\\xd3\\xce)E/\\xa5N\\xde\\xda\\xa6\\x1c\\x90*\\xb0q&m=\' +\n        b\'\\xea6\\xc0\\x02\\x11-\',\n        b\'to\\xef\\xf2go\\x08\\xcf#\\x9e\\x05\\x8d~\\xa0R\\xc1\\x93/\\xa5\\x0b\\x8b9\' +\n        b\'\\x91E\\nKDYW\\x1d\\xff\\x84\\xbe\\x11\\x02X\\xd1)""(\\x0cO\\tJ=\\xf5f\\x94\',\n        b\'\\x12w\\xc9V/\\x84\\xe4\\x0cd\\xf0@\\xd2U:Ae\\xd9\\x9b\\xfbm\\xe2^*\\xdc\\x96KG\' +\n        b\'\\x06\\xa9\\xc7\\xee\\x02\\x1d\\xac\\x08\\xf3\\x9a*/\\x02\\x8b\\x89\\xa0\\x0b\' +\n        b\'\\xa5=r\\xd2\\x9b\\xf5Z\\xf0\\xe9z\\xb6d\\xa7\\x00\\x12<7\\x11\\x08e\',\n        ]\n    for i, csum in enumerate(checksums.CHECKSUMS):\n        digest = csum(b""\\x23\\x42\\xbe\\xef"")\n        yield nt.assert_equal, len(digest), csum.size\n        yield nt.assert_equal, digest, csum_targets[i]\n'"
test/test_cli.py,0,"b""#!/usr/bin/env nosetests\n# -*- coding: utf-8 -*-\n# vim :set ft=py:\n\n\nfrom mock import patch, Mock\nimport nose.tools as nt\n\nfrom bloscpack import cli\nfrom bloscpack.exceptions import FileNotFound\n\ndef test_parser():\n    # hmmm I guess we could override the error\n    parser = cli.create_parser()\n\n\n@patch('os.path.exists')\ndef test_non_existing_input_file_raises_exception(mock_exists):\n    args = Mock(force=False)\n    mock_exists.return_value = False\n    nt.assert_raises(FileNotFound,\n                     cli.check_files,\n                     'nosuchfile',\n                     'nosuchfile',\n                     args)\n\n\n@patch('os.path.exists')\ndef test_existing_output_file_raises_exception(mock_exists):\n    args = Mock(force=False)\n    mock_exists.side_effects = [True, True]\n    nt.assert_raises(FileNotFound,\n                     cli.check_files,\n                     'anyfile',\n                     'anyfile',\n                     args)\n\n\n@patch('os.path.exists')\ndef test_check_files_force_works(mock_exists):\n    args = Mock(force=True)\n    mock_exists.side_effects = [True, True]\n    cli.check_files('anyfile', 'anyfile', args)\n"""
test/test_file_io.py,2,"b'#!/usr/bin/env nosetests\n# -*- coding: utf-8 -*-\n# vim :set ft=py:\n\n\nfrom __future__ import print_function\n\n\nimport blosc\nimport nose.tools as nt\nfrom mock import patch\nimport numpy as np\n\nfrom bloscpack.args import (BloscArgs,\n                            BloscpackArgs,\n                            MetadataArgs,\n                            calculate_nchunks,\n                            )\nfrom bloscpack.compat_util import StringIO\nfrom bloscpack.constants import (MAX_FORMAT_VERSION,\n                                 BLOSCPACK_HEADER_LENGTH,\n                                 BLOSC_HEADER_LENGTH,\n                                 )\nfrom bloscpack.defaults import (DEFAULT_CHUNK_SIZE,\n                                )\nfrom bloscpack.exceptions import (FormatVersionMismatch,\n                                  ChecksumMismatch,\n                                  )\nfrom bloscpack.file_io import (PlainFPSource,\n                               PlainFPSink,\n                               CompressedFPSource,\n                               CompressedFPSink,\n                               pack_file_to_file,\n                               unpack_file_from_file,\n                               pack_bytes_to_file,\n                               unpack_bytes_from_file,\n                               pack_bytes_to_bytes,\n                               unpack_bytes_from_bytes,\n                               _read_bloscpack_header,\n                               _read_offsets,\n                               _read_beginning,\n                               _read_metadata,\n                               _write_metadata,\n                               )\nfrom bloscpack.headers import (decode_blosc_header,\n                               )\nfrom bloscpack.pretty import reverse_pretty\nfrom bloscpack.abstract_io import (pack, unpack)\nfrom bloscpack.testutil import (create_array,\n                                create_array_fp,\n                                create_tmp_files,\n                                cmp_fp,\n                                cmp_file,\n                                simple_progress,\n                                )\n\n\ndef test_offsets():\n    with create_tmp_files() as (tdir, in_file, out_file, dcmp_file):\n        create_array(1, in_file)\n        pack_file_to_file(in_file, out_file, chunk_size=\'2M\')\n        with open(out_file, \'r+b\') as input_fp:\n            bloscpack_header = _read_bloscpack_header(input_fp)\n            total_entries = bloscpack_header.total_prospective_chunks\n            offsets = _read_offsets(input_fp, bloscpack_header)\n            # First chunks should start after header and offsets\n            first = BLOSCPACK_HEADER_LENGTH + 8 * total_entries\n            # We assume that the others are correct\n            nt.assert_equal(offsets[0], first)\n            nt.assert_equal(736, offsets[0])\n            # try to read the second header\n            input_fp.seek(offsets[1], 0)\n            blosc_header_raw = input_fp.read(BLOSC_HEADER_LENGTH)\n            expected = {\'versionlz\': 1,\n                        \'version\':   2,\n                        \'flags\':     1,\n                        \'nbytes\':    2097152,\n                        \'typesize\':  8}\n            blosc_header = decode_blosc_header(blosc_header_raw)\n            blosc_header_slice = dict((k, blosc_header[k]) for k in expected.keys())\n            nt.assert_equal(expected, blosc_header_slice)\n\n    # now check the same thing again, but w/o any max_app_chunks\n    input_fp, output_fp = StringIO(), StringIO()\n    create_array_fp(1, input_fp)\n    nchunks, chunk_size, last_chunk_size = \\\n            calculate_nchunks(input_fp.tell(), chunk_size=\'2M\')\n    input_fp.seek(0, 0)\n    bloscpack_args = BloscpackArgs(max_app_chunks=0)\n    source = PlainFPSource(input_fp)\n    sink = CompressedFPSink(output_fp)\n    pack(source, sink,\n         nchunks, chunk_size, last_chunk_size,\n         bloscpack_args=bloscpack_args\n         )\n    output_fp.seek(0, 0)\n    bloscpack_header = _read_bloscpack_header(output_fp)\n    nt.assert_equal(0, bloscpack_header.max_app_chunks)\n    offsets = _read_offsets(output_fp, bloscpack_header)\n    nt.assert_equal(96, offsets[0])\n\n\ndef test_metadata():\n    test_metadata = {\'dtype\': \'float64\',\n                     \'shape\': [1024],\n                     \'others\': [],\n                     }\n    received_metadata = pack_unpack_fp(1, metadata=test_metadata)\n    nt.assert_equal(test_metadata, received_metadata)\n\n\ndef test_metadata_opportunisitic_compression():\n    # make up some metadata that can be compressed with benefit\n    test_metadata = (""{\'dtype\': \'float64\', \'shape\': [1024], \'others\': [],""\n                     ""\'original_container\': \'carray\'}"")\n    target_fp = StringIO()\n    _write_metadata(target_fp, test_metadata, MetadataArgs())\n    target_fp.seek(0, 0)\n    metadata, header = _read_metadata(target_fp)\n    nt.assert_equal(\'zlib\', header[\'meta_codec\'])\n\n    # now do the same thing, but use badly compressible metadata\n    test_metadata = ""abc""\n    target_fp = StringIO()\n    # default args say: do compression...\n    _write_metadata(target_fp, test_metadata, MetadataArgs())\n    target_fp.seek(0, 0)\n    metadata, header = _read_metadata(target_fp)\n    # but it wasn\'t of any use\n    nt.assert_equal(\'None\', header[\'meta_codec\'])\n\n\ndef test_disable_offsets():\n    in_fp, out_fp, dcmp_fp = StringIO(), StringIO(), StringIO()\n    create_array_fp(1, in_fp)\n    in_fp_size = in_fp.tell()\n    in_fp.seek(0)\n    bloscpack_args = BloscpackArgs(offsets=False)\n    source = PlainFPSource(in_fp)\n    sink = CompressedFPSink(out_fp)\n    pack(source, sink,\n         *calculate_nchunks(in_fp_size),\n         bloscpack_args=bloscpack_args)\n    out_fp.seek(0)\n    bloscpack_header, metadata, metadata_header, offsets = \\\n            _read_beginning(out_fp)\n    nt.assert_true(len(offsets) == 0)\n\n\n# this will cause a bug if we ever reach 255 format versions\n@patch(\'bloscpack.file_io.FORMAT_VERSION\', MAX_FORMAT_VERSION)\ndef test_invalid_format():\n    blosc_args = BloscArgs()\n    with create_tmp_files() as (tdir, in_file, out_file, dcmp_file):\n        create_array(1, in_file)\n        pack_file_to_file(in_file, out_file, blosc_args=blosc_args)\n        nt.assert_raises(FormatVersionMismatch,\n                         unpack_file_from_file, out_file, dcmp_file)\n\n\ndef test_file_corruption():\n    with create_tmp_files() as (tdir, in_file, out_file, dcmp_file):\n        create_array(1, in_file)\n        pack_file_to_file(in_file, out_file)\n        # now go in and modify a byte in the file\n        with open(out_file, \'r+b\') as input_fp:\n            # read offsets and header\n            _read_offsets(input_fp,\n                          _read_bloscpack_header(input_fp))\n            # read the blosc header of the first chunk\n            input_fp.read(BLOSC_HEADER_LENGTH)\n            # read four bytes\n            input_fp.read(4)\n            # read the fifth byte\n            fifth = input_fp.read(1)\n            # figure out what to replace it by\n            replace = b\'\\x00\' if fifth == b\'\\xff\' else b\'\\xff\'\n            # seek one byte back relative to current position\n            input_fp.seek(-1, 1)\n            # write the flipped byte\n            input_fp.write(replace)\n        # now attempt to unpack it\n        nt.assert_raises(ChecksumMismatch, unpack_file_from_file, out_file, dcmp_file)\n\n\ndef pack_unpack(repeats, chunk_size=None, progress=False):\n    with create_tmp_files() as (tdir, in_file, out_file, dcmp_file):\n        if progress:\n            print(""Creating test array"")\n        create_array(repeats, in_file, progress=progress)\n        if progress:\n            print(""Compressing"")\n        pack_file_to_file(in_file, out_file, chunk_size=chunk_size)\n        if progress:\n            print(""Decompressing"")\n        unpack_file_from_file(out_file, dcmp_file)\n        if progress:\n            print(""Verifying"")\n        cmp_file(in_file, dcmp_file)\n\n\ndef pack_unpack_fp(repeats, chunk_size=DEFAULT_CHUNK_SIZE,\n                   progress=False, metadata=None):\n    in_fp, out_fp, dcmp_fp = StringIO(), StringIO(), StringIO()\n    if progress:\n        print(""Creating test array"")\n    create_array_fp(repeats, in_fp, progress=progress)\n    in_fp_size = in_fp.tell()\n    if progress:\n        print(""Compressing"")\n    in_fp.seek(0)\n    nchunks, chunk_size, last_chunk_size = \\\n            calculate_nchunks(in_fp_size, chunk_size)\n    source = PlainFPSource(in_fp)\n    sink = CompressedFPSink(out_fp)\n    pack(source, sink,\n         nchunks, chunk_size, last_chunk_size,\n         metadata=metadata)\n    out_fp.seek(0)\n    if progress:\n        print(""Decompressing"")\n    source = CompressedFPSource(out_fp)\n    sink = PlainFPSink(dcmp_fp)\n    unpack(source, sink)\n    if progress:\n        print(""Verifying"")\n    cmp_fp(in_fp, dcmp_fp)\n    return source.metadata\n\n\ndef test_pack_unpack():\n    pack_unpack(1, chunk_size=reverse_pretty(\'1M\'))\n    pack_unpack(1, chunk_size=reverse_pretty(\'2M\'))\n    pack_unpack(1, chunk_size=reverse_pretty(\'4M\'))\n    pack_unpack(1, chunk_size=reverse_pretty(\'8M\'))\n\n\ndef test_pack_unpack_fp():\n    pack_unpack_fp(1, chunk_size=reverse_pretty(\'1M\'))\n    pack_unpack_fp(1, chunk_size=reverse_pretty(\'2M\'))\n    pack_unpack_fp(1, chunk_size=reverse_pretty(\'4M\'))\n    pack_unpack_fp(1, chunk_size=reverse_pretty(\'8M\'))\n\n\ndef test_pack_unpack_bytes_to_from_file():\n    array_ = np.linspace(0, 1e5)\n    input_bytes = array_.tostring()\n    with create_tmp_files() as (tdir, in_file, out_file, dcmp_file):\n        pack_bytes_to_file(input_bytes, out_file)\n        output_bytes, _ = unpack_bytes_from_file(out_file)\n    nt.assert_equal(input_bytes, output_bytes)\n\n\ndef test_pack_unpack_bytes_bytes():\n    a = np.linspace(0, 1e5)\n    b = a.tostring()\n    c = pack_bytes_to_bytes(b)\n    d, _ = unpack_bytes_from_bytes(c)\n    nt.assert_equal(b, d)\n\n\ndef pack_unpack_hard():\n    """""" Test on somewhat larger arrays, but be nice to memory. """"""\n    # Array is apprx. 1.5 GB large\n    # should make apprx 1536 chunks\n    pack_unpack(100, chunk_size=reverse_pretty(\'1M\'), progress=simple_progress)\n\n\ndef pack_unpack_extreme():\n    """""" Test on somewhat larer arrays, uses loads of memory. """"""\n    # this will create a huge array, and then use the\n    # blosc.BLOSC_MAX_BUFFERSIZE as chunk-szie\n    pack_unpack(300, chunk_size=blosc.BLOSC_MAX_BUFFERSIZE,\n                progress=simple_progress)\n'"
test/test_headers.py,6,"b""#!/usr/bin/env nosetests\n# -*- coding: utf-8 -*-\n# vim :set ft=py:\n\n\nimport struct\nimport sys\n\n\nimport nose.tools as nt\nfrom nose.plugins.skip import SkipTest\nimport blosc\nimport numpy as np\n\n\nfrom bloscpack.args import (BloscArgs,\n                            )\nfrom bloscpack.compat_util import OrderedDict\nfrom bloscpack.constants import (MAGIC,\n                                 FORMAT_VERSION,\n                                 MAX_FORMAT_VERSION,\n                                 MAX_META_SIZE,\n                                 MAX_CHUNKS,\n                                 )\nfrom bloscpack.pretty import reverse_pretty\nfrom bloscpack import checksums\nfrom bloscpack import exceptions\nfrom bloscpack.headers import (BloscpackHeader,\n                               MetadataHeader,\n                               create_options,\n                               decode_options,\n                               check_options,\n                               create_metadata_options,\n                               decode_metadata_options,\n                               check_range,\n                               decode_blosc_header,\n                               decode_blosc_flags,\n                               )\n\n\ndef test_check_range():\n    nt.assert_raises(TypeError,  check_range, 'test', 'a', 0, 1)\n    nt.assert_raises(ValueError, check_range, 'test', -1, 0, 1)\n    nt.assert_raises(ValueError, check_range, 'test', 2, 0, 1)\n\n\ndef test_create_options():\n\n    for expected_options, kwargs in [\n            ('00000001', {}),\n            ('00000001', {'offsets': True}),\n            ('00000000', {'offsets': False}),\n            ('00000001', {'metadata': False}),\n            ('00000011', {'metadata': True}),\n            ('00000000', {'offsets': False, 'metadata': False}),\n            ('00000010', {'offsets': False, 'metadata': True}),\n            ('00000001', {'offsets': True, 'metadata': False}),\n            ('00000011', {'offsets': True, 'metadata': True}),\n            ]:\n        yield nt.assert_equal, expected_options, create_options(**kwargs)\n\n\ndef test_decode_options():\n    for expected, input in [\n            ({'metadata': False, 'offsets': False}, '00000000'),\n            ({'metadata': False, 'offsets': True}, '00000001'),\n            ({'metadata': True, 'offsets': False}, '00000010'),\n            ({'metadata': True, 'offsets': True}, '00000011'),\n            ]:\n        yield nt.assert_equal, expected, decode_options(input)\n\n\ndef test_decode_options_exceptions():\n\n    for broken_input in [\n            '0000000',\n            '000000000',\n            '0000000a',\n            'abc',\n            '00000100',\n            '00001100',\n            '11111100',\n            ]:\n        yield nt.assert_raises, ValueError, decode_options, broken_input\n\n\ndef test_check_options_exceptions():\n    for broken_input in [\n            # check for non-string\n            0,\n            1,\n            ]:\n        yield nt.assert_raises, TypeError, check_options, broken_input\n    for broken_input in [\n            # check for lengths too small and too large\n            '0',\n            '1',\n            '0000000',\n            '000000000',\n            '1111111',\n            '111111111',\n            # check for non zeros and ones\n            '0000000a',\n            'aaaaaaaa',\n            ]:\n        yield nt.assert_raises, ValueError, check_options, broken_input\n\n\ndef test_create_metadata_options():\n    nt.assert_equal('00000000', create_metadata_options())\n\n\ndef test_decode_metadata_options():\n    nt.assert_equal({}, decode_metadata_options('00000000'))\n\n\ndef test_decode_metadata_options_exceptions():\n\n    for broken_input in [\n            '0000000',\n            '000000000',\n            '0000000a',\n            'abc',\n            '00000001',\n            '00001111',\n            '11111111',\n            ]:\n        yield nt.assert_raises, ValueError, decode_metadata_options, broken_input\n\n\ndef test_decode_blosc_header_basic():\n    array_ = np.linspace(0, 100, 2e4).tostring()\n    blosc_args = BloscArgs()\n    compressed = blosc.compress(array_, **blosc_args)\n    header = decode_blosc_header(compressed)\n    expected = {'versionlz': 1,\n                'version': 2,\n                'flags': 1,\n                'nbytes': len(array_),\n                'typesize': blosc_args.typesize}\n    header_slice = dict((k, header[k]) for k in expected.keys())\n    nt.assert_equal(expected, header_slice)\n\n\ndef test_decode_blosc_header_deactivate_shuffle():\n    array_ = np.ones(16000, dtype=np.uint8)\n    blosc_args = BloscArgs()\n    blosc_args.shuffle = False\n    compressed = blosc.compress(array_, **blosc_args)\n    header = decode_blosc_header(compressed)\n    expected = {'versionlz': 1,\n                'version': 2,\n                'flags': 0,  # no shuffle flag\n                'nbytes': len(array_),\n                'typesize': blosc_args.typesize}\n    header_slice = dict((k, header[k]) for k in expected.keys())\n    nt.assert_equal(expected, header_slice)\n\n\ndef test_decode_blosc_header_uncompressible_data():\n    array_ = np.asarray(np.random.randn(255),\n                        dtype=np.float32).tostring()\n    blosc_args = BloscArgs()\n    blosc_args.shuffle = True\n    compressed = blosc.compress(array_, **blosc_args)\n    header = decode_blosc_header(compressed)\n    expected = {'versionlz': 1,\n                'blocksize': 1016,\n                'ctbytes': len(array_) + 16,  # original + 16 header bytes\n                'version': 2,\n                'flags': 0x13,  # 1 for shuffle 2 for non-compressed 4 for small blocksize\n                'nbytes': len(array_),\n                'typesize': blosc_args.typesize}\n    nt.assert_equal(expected, header)\n\n\ndef test_decode_blosc_header_uncompressible_data_dont_split_false():\n    array_ = np.asarray(np.random.randn(256),\n                        dtype=np.float32).tostring()\n    blosc_args = BloscArgs()\n    blosc_args.shuffle = True\n    compressed = blosc.compress(array_, **blosc_args)\n    header = decode_blosc_header(compressed)\n    expected = {\n        'versionlz': 1,\n        'version': 2,\n        'blocksize': 1024,\n        'ctbytes': len(array_) + 16,  # original + 16 header bytes\n        'flags': 0x3,  # 1 for shuffle 2 for non-compressed\n        'nbytes': len(array_),\n        'typesize': blosc_args.typesize\n    }\n    nt.assert_equal(expected, header)\n\n\ndef test_decode_blosc_flags():\n\n    def gen_expected(new):\n        it = OrderedDict((\n            ('byte_shuffle', False),\n            ('pure_memcpy', False),\n            ('bit_shuffle', False),\n            ('split_blocks', False),\n            ('codec', 'blosclz'),\n        ))\n        it.update(new)\n        return it\n    for input_byte, new_params in [\n            (0b00000000, {}),\n            (0b00000001, {'byte_shuffle': True}),\n            (0b00000010, {'pure_memcpy': True}),\n            (0b00000100, {'bit_shuffle': True}),\n            (0b00010000, {'split_blocks': True}),\n            (0b00100000, {'codec': 'lz4'}),\n            (0b01000000, {'codec': 'snappy'}),\n            (0b01100000, {'codec': 'zlib'}),\n            (0b10000000, {'codec': 'zstd'}),\n            ]:\n        yield (nt.assert_equal,\n               decode_blosc_flags(input_byte),\n               gen_expected(new_params))\n\n\ndef test_BloscPackHeader_constructor_exceptions():\n    # uses nose test generators\n\n    def check(error_type, args_dict):\n        nt.assert_raises(error_type, BloscpackHeader, **args_dict)\n\n    for error_type, args_dict in [\n            (ValueError, {'format_version': -1}),\n            (ValueError, {'format_version': MAX_FORMAT_VERSION+1}),\n            (TypeError,  {'format_version': 'foo'}),\n            (ValueError, {'checksum': -1}),\n            (ValueError, {'checksum': len(checksums.CHECKSUMS)+1}),\n            (exceptions.NoSuchChecksum, {'checksum': 'foo'}),\n            (ValueError, {'typesize': -1}),\n            (ValueError, {'typesize': blosc.BLOSC_MAX_TYPESIZE+1}),\n            (TypeError,  {'typesize': 'foo'}),\n            (ValueError, {'chunk_size': blosc.BLOSC_MAX_BUFFERSIZE+1}),\n            (ValueError, {'chunk_size': -2}),\n            (TypeError,  {'chunk_size': 'foo'}),\n            (ValueError, {'last_chunk': blosc.BLOSC_MAX_BUFFERSIZE+1}),\n            (ValueError, {'last_chunk': -2}),\n            (TypeError,  {'last_chunk': 'foo'}),\n            (ValueError, {'nchunks': MAX_CHUNKS+1}),\n            (ValueError, {'nchunks': -2}),\n            (TypeError,  {'nchunks': 'foo'}),\n            (ValueError, {'max_app_chunks': MAX_CHUNKS+1}),\n            (ValueError, {'max_app_chunks': -1}),\n            (TypeError,  {'max_app_chunks': 'foo'}),\n            # sum of nchunks and max_app_chunks\n            (ValueError, {'nchunks': MAX_CHUNKS//2+1,\n                          'max_app_chunks': MAX_CHUNKS//2+1}),\n            # check that max_app_chunks is zero\n            (ValueError, {'nchunks': -1,\n                          'max_app_chunks': 1}),\n            # check constraint on last chunk, must be equal to or smaller than\n            # chunk_size\n            (ValueError, {'chunk_size': 1,\n                          'last_chunk': 2}),\n            ]:\n        yield check, error_type, args_dict\n\n\ndef test_BloscPackHeader_total_prospective_entries():\n\n    for expected, (nchunks, max_app_chunks) in [\n            (0, (0, 0)),\n            (1, (1, 0)),\n            (1, (0, 1)),\n            (None, (-1, 0)),\n            (65, (42, 23)),\n            (MAX_CHUNKS-1, (MAX_CHUNKS//2, MAX_CHUNKS//2)),\n            (MAX_CHUNKS, (MAX_CHUNKS-1, 1)),\n            (MAX_CHUNKS, (1, MAX_CHUNKS-1)),\n            ]:\n        header = BloscpackHeader(nchunks=nchunks,\n                                 max_app_chunks=max_app_chunks)\n        yield nt.assert_equal, expected, header.total_prospective_chunks\n\n\ndef test_BloscpackHeader_encode():\n\n    # the raw encoded header as produces w/o any kwargs\n    format_version = struct.pack('<B', FORMAT_VERSION)\n    raw = MAGIC + format_version + \\\n        b'\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff' + \\\n        b'\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\n\n    # modify the raw encoded header with the value starting at offset\n    def mod_raw(offset, replacement):\n        return raw[0:offset] + replacement + \\\n            raw[offset+len(replacement):]\n\n    # test with no arguments\n    yield nt.assert_equal, raw, BloscpackHeader().encode()\n\n    for offset, replacement, kwargs in [\n            (4, struct.pack('<B', 23), {'format_version': 23}),\n            # test with options\n            (5, b'\\x01', {'offsets': True}),\n            (5, b'\\x02', {'metadata': True}),\n            (5, b'\\x03', {'offsets': True, 'metadata': True}),\n            # test with checksum\n            (6, b'\\x01', {'checksum': 'adler32'}),\n            (6, b'\\x08', {'checksum': 'sha512'}),\n            # test with typesize\n            (7, b'\\x01', {'typesize': 1}),\n            (7, b'\\x02', {'typesize': 2}),\n            (7, b'\\x04', {'typesize': 4}),\n            (7, b'\\x10', {'typesize': 16}),\n            (7, b'\\xff', {'typesize': 255}),\n            # test with chunksize\n            (8, b'\\xff\\xff\\xff\\xff', {'chunk_size': -1}),\n            (8, b'\\x01\\x00\\x00\\x00', {'chunk_size': 1}),\n            (8, b'\\x00\\x00\\x10\\x00', {'chunk_size': reverse_pretty('1M')}),\n            (8, b'\\xef\\xff\\xff\\x7f', {'chunk_size': blosc.BLOSC_MAX_BUFFERSIZE}),\n            # test with last_chunk\n            (12, b'\\xff\\xff\\xff\\xff', {'last_chunk': -1}),\n            (12, b'\\x01\\x00\\x00\\x00', {'last_chunk': 1}),\n            (12, b'\\x00\\x00\\x10\\x00', {'last_chunk': reverse_pretty('1M')}),\n            (12, b'\\xef\\xff\\xff\\x7f', {'last_chunk': blosc.BLOSC_MAX_BUFFERSIZE}),\n            # test nchunks\n            (16, b'\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff', {'nchunks': -1}),\n            (16, b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00', {'nchunks': 0}),\n            (16, b'\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00', {'nchunks': 1}),\n            (16, b'\\x7f\\x00\\x00\\x00\\x00\\x00\\x00\\x00', {'nchunks': 127}),\n            (16, b'\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x7f', {'nchunks': MAX_CHUNKS}),\n            # test max_app_chunks\n            (16, b'\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00',\n                {'nchunks': 1, 'max_app_chunks': 0}),\n            (16, b'\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00',\n                {'nchunks': 1, 'max_app_chunks': 1}),\n            (16, b'\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x7f\\x00\\x00\\x00\\x00\\x00\\x00\\x00',\n                {'nchunks': 1, 'max_app_chunks': 127}),\n            # Maximum value is MAX_CHUNKS - 1 since nchunks is already 1\n            (16, b'\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xfe\\xff\\xff\\xff\\xff\\xff\\xff\\x7f',\n                {'nchunks': 1, 'max_app_chunks': MAX_CHUNKS-1}),\n            ]:\n        yield nt.assert_equal, mod_raw(offset, replacement), \\\n            BloscpackHeader(**kwargs).encode()\n\n\ndef test_BloscpackHeader_decode():\n\n    format_version = struct.pack('<B', FORMAT_VERSION)\n    raw = MAGIC + format_version + \\\n        b'\\x00\\x00\\x00\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff' + \\\n        b'\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\n\n    def mod_raw(offset, replacement):\n        return raw[0:offset] + replacement + \\\n            raw[offset+len(replacement):]\n\n    # check with no args\n    yield nt.assert_equal, BloscpackHeader(), BloscpackHeader.decode(raw)\n\n    for kwargs, offset, replacement in [\n            # check with format_version\n            ({'format_version': 23}, 4, b'\\x17'),\n            # check with options\n            ({'offsets': True}, 5, b'\\x01'),\n            ({'metadata': True}, 5, b'\\x02'),\n            ({'metadata': True, 'offsets': True}, 5, b'\\x03'),\n            # check with checksum\n            ({'checksum': 'adler32'}, 6, b'\\x01'),\n            ({'checksum': 'sha384'}, 6, b'\\x07'),\n            # check with typesize\n            ({'typesize': 1}, 7, b'\\x01'),\n            ({'typesize': 2}, 7, b'\\x02'),\n            ({'typesize': 4}, 7, b'\\x04'),\n            ({'typesize': 8}, 7, b'\\x08'),\n            ({'typesize': blosc.BLOSC_MAX_TYPESIZE}, 7, b'\\xff'),\n            # check with chunk_size\n            ({'chunk_size': 1},\n                8, b'\\x01\\x00\\x00\\x00'),\n            ({'chunk_size': reverse_pretty('1M')},\n                8, b'\\x00\\x00\\x10\\x00'),\n            ({'chunk_size': blosc.BLOSC_MAX_BUFFERSIZE},\n                8, b'\\xef\\xff\\xff\\x7f'),\n            # check with last_chunk\n            ({'last_chunk': 1},\n                12, b'\\x01\\x00\\x00\\x00'),\n            ({'last_chunk': reverse_pretty('1M')},\n                12, b'\\x00\\x00\\x10\\x00'),\n            ({'last_chunk': blosc.BLOSC_MAX_BUFFERSIZE},\n                12, b'\\xef\\xff\\xff\\x7f'),\n            # check with nchunks\n            ({'nchunks': 1},\n                16, b'\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00'),\n            ({'nchunks': reverse_pretty('1M')},\n                16, b'\\x00\\x00\\x10\\x00\\x00\\x00\\x00\\x00'),\n            ({'nchunks': MAX_CHUNKS},\n                16, b'\\xff\\xff\\xff\\xff\\xff\\xff\\xff\\x7f'),\n            # check with max_app_chunks\n            ({'nchunks': 1, 'max_app_chunks': 0},\n                16, b'\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'),\n            ({'nchunks': 1, 'max_app_chunks': 1},\n                16, b'\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00'),\n            ({'nchunks': 1, 'max_app_chunks': reverse_pretty('1M')},\n                16, b'\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x10\\x00\\x00\\x00\\x00\\x00'),\n            # Maximum value is MAX_CHUNKS - 1 since nchunks is already 1\n            ({'nchunks': 1, 'max_app_chunks': MAX_CHUNKS-1},\n                16, b'\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xfe\\xff\\xff\\xff\\xff\\xff\\xff\\x7f'),\n            ]:\n        yield (nt.assert_equal,\n               BloscpackHeader(**kwargs),\n               BloscpackHeader.decode(mod_raw(offset, replacement)))\n\n\ndef test_BloscpackHeader_accessor_exceptions():\n    if sys.version_info[0:2] < (2, 7):\n        raise SkipTest\n    bloscpack_header = BloscpackHeader()\n    nt.assert_raises_regexp(KeyError,\n                            'foo not in BloscpackHeader',\n                            bloscpack_header.__getitem__,\n                            'foo')\n    nt.assert_raises_regexp(KeyError,\n                            'foo not in BloscpackHeader',\n                            bloscpack_header.__setitem__,\n                            'foo', 'bar')\n    nt.assert_raises_regexp(NotImplementedError,\n                            'BloscpackHeader does not support __delitem__ or derivatives',\n                            bloscpack_header.__delitem__,\n                            'foo',)\n\n\ndef test_MetadataHeader_encode():\n    raw = b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\\\n          b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\\\n          b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\\\n          b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\n    yield nt.assert_equal, raw, MetadataHeader().encode()\n\n    def mod_raw(offset, value):\n        return raw[0:offset] + value + \\\n            raw[offset+len(value):]\n\n    for offset, replacement, kwargs in [\n            (0, b'JSON', {'magic_format': b'JSON'}),\n            (9, b'\\x01', {'meta_checksum': 'adler32'}),\n            (10, b'\\x01', {'meta_codec': 'zlib'}),\n            (11, b'\\x01', {'meta_level': 1}),\n            (12, b'\\x01', {'meta_size': 1}),\n            (12, b'\\xff\\xff\\xff\\xff', {'meta_size': MAX_META_SIZE}),\n            (16, b'\\x01', {'max_meta_size': 1}),\n            (16, b'\\xff\\xff\\xff\\xff', {'max_meta_size': MAX_META_SIZE}),\n            (20, b'\\x01', {'meta_comp_size': 1}),\n            (20, b'\\xff\\xff\\xff\\xff', {'meta_comp_size': MAX_META_SIZE}),\n            (24, b'sesame', {'user_codec': b'sesame'}),\n            ]:\n        yield nt.assert_equal, mod_raw(offset, replacement), \\\n            MetadataHeader(**kwargs).encode()\n\n\ndef test_MetadataHeader_decode():\n    no_arg_return = {'magic_format':        b'',\n                     'meta_options':        '00000000',\n                     'meta_checksum':       'None',\n                     'meta_codec':          'None',\n                     'meta_level':          0,\n                     'meta_size':           0,\n                     'max_meta_size':       0,\n                     'meta_comp_size':      0,\n                     'user_codec':          b'',\n                     }\n    no_arg_return = MetadataHeader(**no_arg_return)\n    no_arg_input = (b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\n                    b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\n                    b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\n                    b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00')\n\n    def copy_and_set_return(key, value):\n        copy_ = no_arg_return.copy()\n        copy_[key] = value\n        return copy_\n\n    def copy_and_set_input(offset, value):\n        return no_arg_input[0:offset] + value + \\\n            no_arg_input[offset+len(value):]\n\n    yield nt.assert_equal, no_arg_return, MetadataHeader.decode(no_arg_input)\n\n    for attribute, value, offset, replacement in [\n            ('magic_format', b'JSON', 0, b'JSON'),\n            ('meta_checksum', 'adler32', 9, b'\\x01'),\n            ('meta_codec', 'zlib', 10, b'\\x01'),\n            ('meta_level', 1, 11, b'\\x01'),\n            ('meta_size', 1, 12, b'\\x01\\x00\\x00\\x00'),\n            ('meta_size', MAX_META_SIZE, 12, b'\\xff\\xff\\xff\\xff'),\n            ('max_meta_size', 1, 16, b'\\x01\\x00\\x00\\x00'),\n            ('max_meta_size', MAX_META_SIZE, 16, b'\\xff\\xff\\xff\\xff'),\n            ('meta_comp_size', 1, 20, b'\\x01\\x00\\x00\\x00'),\n            ('meta_comp_size', MAX_META_SIZE, 20, b'\\xff\\xff\\xff\\xff'),\n            ('user_codec', b'sesame', 24, b'sesame'),\n            ]:\n        yield nt.assert_equal, copy_and_set_return(attribute, value), \\\n            MetadataHeader.decode(copy_and_set_input(offset, replacement))\n"""
test/test_log.py,0,"b""import nose.tools as nt\nfrom mock import patch\n\n\nfrom bloscpack import log\n\n\ndef test_verbose():\n    nt.assert_raises(TypeError, log.verbose, 'message', 'MAXIMUM')\n    log.set_level(log.DEBUG)\n    # should probably hijack the print statement\n    log.verbose('notification')\n    log.set_level(log.NORMAL)\n\n\n@patch('sys.exit')\ndef test_error(exit_mock):\n    log.error('error')\n    exit_mock.assert_called_once_with(1)\n\n\ndef test_set_level_exception():\n    nt.assert_raises(ValueError, log.set_level, 'NO_SUCH_LEVEL')\n"""
test/test_memory_io.py,0,"b'#!/usr/bin/env nosetests\n# -*- coding: utf-8 -*-\n# vim :set ft=py:\n\n\nfrom nose import tools as nt\n\n\nfrom bloscpack.abstract_io import (pack,\n                                   unpack,\n                                   )\nfrom bloscpack.args import (calculate_nchunks,\n                            )\nfrom bloscpack.compat_util import StringIO\nfrom bloscpack.defaults import (DEFAULT_CHUNK_SIZE,\n                                )\nfrom bloscpack.file_io import (PlainFPSource,\n                               CompressedFPSink,\n                               CompressedFPSource,\n                               PlainFPSink,\n                               )\nfrom bloscpack.memory_io import (CompressedMemorySink,\n                                 CompressedMemorySource,\n                                 PlainMemorySink,\n                                 PlainMemorySource,\n                                 )\nfrom bloscpack.pretty import (reverse_pretty,\n                              )\nfrom bloscpack.testutil import (create_array_fp,\n                                cmp_fp,\n                                )\n\n\ndef pack_unpack_mem(repeats, chunk_size=DEFAULT_CHUNK_SIZE,\n                    progress=False, metadata=None):\n    in_fp, out_fp, dcmp_fp = StringIO(), StringIO(), StringIO()\n    if progress:\n        print(""Creating test array"")\n    create_array_fp(repeats, in_fp, progress=progress)\n    in_fp_size = in_fp.tell()\n    if progress:\n        print(""Compressing"")\n    in_fp.seek(0)\n    nchunks, chunk_size, last_chunk_size = \\\n            calculate_nchunks(in_fp_size, chunk_size)\n    # let us play merry go round\n    source = PlainFPSource(in_fp)\n    sink = CompressedMemorySink()\n    pack(source, sink, nchunks, chunk_size, last_chunk_size, metadata=metadata)\n    source = CompressedMemorySource(sink)\n    sink = PlainMemorySink()\n    unpack(source, sink)\n    nt.assert_equal(metadata, source.metadata)\n    source = PlainMemorySource(sink.chunks)\n    sink = CompressedFPSink(out_fp)\n    pack(source, sink, nchunks, chunk_size, last_chunk_size, metadata=metadata)\n    out_fp.seek(0)\n    source = CompressedFPSource(out_fp)\n    sink = PlainFPSink(dcmp_fp)\n    unpack(source, sink)\n    nt.assert_equal(metadata, source.metadata)\n    in_fp.seek(0)\n    dcmp_fp.seek(0)\n    cmp_fp(in_fp, dcmp_fp)\n    return source.metadata\n\n\ndef test_pack_unpack_mem():\n    pack_unpack_mem(1, chunk_size=reverse_pretty(\'1M\'))\n    pack_unpack_mem(1, chunk_size=reverse_pretty(\'2M\'))\n    pack_unpack_mem(1, chunk_size=reverse_pretty(\'4M\'))\n    pack_unpack_mem(1, chunk_size=reverse_pretty(\'8M\'))\n\n    metadata = {""dtype"": ""float64"", ""shape"": [1024], ""others"": []}\n\n    pack_unpack_mem(1, chunk_size=reverse_pretty(\'1M\'), metadata=metadata)\n    pack_unpack_mem(1, chunk_size=reverse_pretty(\'2M\'), metadata=metadata)\n    pack_unpack_mem(1, chunk_size=reverse_pretty(\'4M\'), metadata=metadata)\n    pack_unpack_mem(1, chunk_size=reverse_pretty(\'8M\'), metadata=metadata)\n'"
test/test_metacodecs.py,0,"b'#!/usr/bin/env nosetests\n# -*- coding: utf-8 -*-\n# vim :set ft=py:\n\n\nimport nose.tools as nt\n\n\nfrom bloscpack.args import (DEFAULT_META_LEVEL,\n                            )\nfrom bloscpack.metacodecs import (CODECS,\n                                  CODECS_AVAIL\n                                  )\n\n\ndef test_codecs():\n    nt.assert_equal(CODECS_AVAIL, [\'None\', \'zlib\'])\n    random_str = b""4KzGCl7SxTsYLaerommsMWyZg1TXbV6wsR9Xk""\n    for i, c in enumerate(CODECS):\n        nt.assert_equal(random_str, c.decompress(\n            c.compress(random_str, DEFAULT_META_LEVEL)))\n'"
test/test_numpy_io.py,27,"b""#!/usr/bin/env nosetests\n# -*- coding: utf-8 -*-\n# vim :set ft=py:\n\n\nimport numpy as np\nimport numpy.testing as npt\nimport nose.tools as nt\nimport mock\n\n\nfrom bloscpack.abstract_io import (pack,\n                                   )\nfrom bloscpack.args import (BloscArgs,\n                            calculate_nchunks,\n                            )\nfrom bloscpack.compat_util import StringIO\nfrom bloscpack.exceptions import (NotANumpyArray,\n                                  ChunkSizeTypeSizeMismatch,\n                                  ObjectNumpyArrayRejection,\n                                  )\nfrom bloscpack.file_io import (PlainFPSource,\n                               CompressedFPSource,\n                               CompressedFPSink,\n                               )\nfrom bloscpack.headers import (decode_blosc_header,\n                               )\nfrom bloscpack.memory_io import CompressedMemorySource, CompressedMemorySink\nfrom bloscpack.numpy_io import (pack_ndarray,\n                                unpack_ndarray,\n                                pack_ndarray_to_bytes,\n                                unpack_ndarray_from_bytes,\n                                pack_ndarray_to_file,\n                                unpack_ndarray_from_file,\n                                _conv,\n                                )\nfrom bloscpack.testutil import (create_tmp_files,\n                                )\n\n\ndef roundtrip_numpy_memory(ndarray):\n    sink = CompressedMemorySink()\n    pack_ndarray(ndarray, sink)\n    source = CompressedMemorySource(sink)\n    b = unpack_ndarray(source)\n    return npt.assert_array_equal, ndarray, b\n\n\ndef roundtrip_numpy_str(ndarray):\n    s = pack_ndarray_to_bytes(ndarray)\n    b = unpack_ndarray_from_bytes(s)\n    return npt.assert_array_equal, ndarray, b\n\n\ndef roundtrip_numpy_file_pointers(ndarray):\n    sio = StringIO()\n    sink = CompressedFPSink(sio)\n    pack_ndarray(ndarray, sink)\n    sio.seek(0)\n    source = CompressedFPSource(sio)\n    b = unpack_ndarray(source)\n    return npt.assert_array_equal, ndarray, b\n\n\ndef roundtrip_numpy_file(ndarray):\n    with create_tmp_files() as (tdir, in_file, out_file, dcmp_file):\n        pack_ndarray_to_file(ndarray, out_file)\n        b = unpack_ndarray_from_file(out_file)\n        return npt.assert_array_equal, ndarray, b\n\n\ndef test_conv():\n    test_data = (\n        ([[u'a', u'f8']], [('a', 'f8')]),\n        ([[u'a', u'f8', 2]], [('a', 'f8', 2)]),\n        ([[u'a', [[u'b', 'f8']]]], [('a', [('b', 'f8')])]),\n    )\n    for input_, expected in test_data:\n        received = _conv(input_)\n        yield nt.assert_equal, expected, received\n\n\ndef test_unpack_exception():\n    a = np.arange(50)\n    sio = StringIO()\n    a_str = a.tostring()\n    source = PlainFPSource(StringIO(a_str))\n    sink = CompressedFPSink(sio)\n    pack(source, sink, *calculate_nchunks(len(a_str)))\n    nt.assert_raises(NotANumpyArray, unpack_ndarray_from_bytes, sio.getvalue())\n\n\ndef roundtrip_ndarray(ndarray):\n    yield roundtrip_numpy_memory(ndarray)\n    yield roundtrip_numpy_str(ndarray)\n    yield roundtrip_numpy_file_pointers(ndarray)\n    yield roundtrip_numpy_file(ndarray)\n\n\ndef test_numpy_dtypes_shapes_order():\n\n    # happy trail\n    a = np.arange(50)\n    for case in roundtrip_ndarray(a):\n        yield case\n\n    for dt in np.sctypes['int'] + np.sctypes['uint'] + np.sctypes['float']:\n        a = np.arange(64, dtype=dt)\n        for case in roundtrip_ndarray(a):\n            yield case\n        a = a.copy().reshape(8, 8)\n        for case in roundtrip_ndarray(a):\n            yield case\n        a = a.copy().reshape(4, 16)\n        for case in roundtrip_ndarray(a):\n            yield case\n        a = a.copy().reshape(4, 4, 4)\n        for case in roundtrip_ndarray(a):\n            yield case\n        a = np.asfortranarray(a)\n        nt.assert_true(np.isfortran(a))\n        for case in roundtrip_ndarray(a):\n            yield case\n\n    # Fixed width string arrays\n    a = np.array(['abc', 'def', 'ghi'])\n    for case in roundtrip_ndarray(a):\n        yield case\n\n    # This actually get's cast to a fixed width string array\n    a = np.array([(1, 'abc'), (2, 'def'), (3, 'ghi')])\n    for case in roundtrip_ndarray(a):\n        yield case\n\n    ## object arrays\n    #a = np.array([(1, 'abc'), (2, 'def'), (3, 'ghi')], dtype='object')\n    #for case in roundtrip_ndarray(a):\n    #    yield case\n\n    # structured array\n    a = np.array([('a', 1), ('b', 2)], dtype=[('a', 'S1'), ('b', 'f8')])\n    for case in roundtrip_ndarray(a):\n        yield case\n\n    # record array\n    a = np.array([(1, 'O', 1)],\n                 dtype=np.dtype([('step', 'int32'),\n                                ('symbol', '|S1'),\n                                ('index', 'int32')]))\n    for case in roundtrip_ndarray(a):\n        yield case\n\n    # and a nested record array\n    dt = [('year', '<i4'),\n          ('countries', [('c1', [('iso', 'a3'), ('value', '<f4')]),\n                         ('c2', [('iso', 'a3'), ('value', '<f4')])\n                         ])\n          ]\n    a = np.array([(2009, (('USA', 10.),\n                          ('CHN', 12.))),\n                  (2010, (('BRA', 10.),\n                          ('ARG', 12.)))],\n                 dt)\n    for case in roundtrip_ndarray(a):\n        yield case\n\n    # what about endianess\n    a = np.arange(10, dtype='>i8')\n    for case in roundtrip_ndarray(a):\n        yield case\n\n    # empty array\n    a = np.array([], dtype='f8')\n    for case in roundtrip_ndarray(a):\n        yield case\n\n\ndef test_reject_object_array():\n    a = np.array([(1, 'abc'), (2, 'def'), (3, 'ghi')], dtype='object')\n    nt.assert_raises(ObjectNumpyArrayRejection, roundtrip_numpy_memory, a)\n\n\ndef test_reject_nested_object_array():\n    a = np.array([(1, 'abc'), (2, 'def'), (3, 'ghi')],\n                  dtype=[('a', int), ('b', 'object')])\n    nt.assert_raises(ObjectNumpyArrayRejection, roundtrip_numpy_memory, a)\n\n\ndef test_backwards_compat():\n\n    def old_ndarray_meta(ndarray):\n        # This DOESN'T use 'repr', see also:\n        # bloscpack.numpy_io._ndarray_meta\n        return {'dtype': ndarray.dtype.descr\n                if ndarray.dtype.names is not None\n                else ndarray.dtype.str,\n                'shape': ndarray.shape,\n                'order': 'F' if np.isfortran(ndarray) else 'C',\n                'container': 'numpy',\n                }\n    test_data = [np.arange(10),\n                 np.array([('a', 1), ('b', 2)],\n                          dtype=[('a', 'S1'), ('b', 'f8')]),\n                 ]\n\n    with mock.patch('bloscpack.numpy_io._ndarray_meta', old_ndarray_meta):\n        for a in test_data:\n            # uses old version of _ndarray_meta\n            c = pack_ndarray_to_bytes(a)\n            # should not raise a SyntaxError\n            d = unpack_ndarray_from_bytes(c)\n            yield npt.assert_array_equal, a, d\n\n\ndef test_itemsize_chunk_size_mismatch():\n    a = np.arange(1000)\n    # typesize of the array is 8, let's glitch the typesize\n    for i in [1, 2, 3, 5, 6, 7, 9, 10, 11, 13, 14, 15]:\n            yield nt.assert_raises, \\\n                ChunkSizeTypeSizeMismatch, \\\n                pack_ndarray_to_bytes, a, i\n\n\ndef test_larger_arrays():\n    for dt in ('uint64', 'int64', 'float64'):\n        a = np.arange(2e4, dtype=dt)\n        for case in roundtrip_ndarray(a):\n            yield case\n\n\ndef huge_arrays():\n    for dt in ('uint64', 'int64', 'float64'):\n        # needs plenty of memory\n        a = np.arange(1e8, dtype=dt)\n        for case in roundtrip_ndarray(a):\n            yield case\n\n\ndef test_alternate_cname():\n    for cname, int_id in [\n            ('blosclz', 0),\n            ('lz4', 1),\n            ('lz4hc', 1),\n            ('zlib', 3),\n            ('zstd', 4),\n            ]:\n        blosc_args = BloscArgs(cname=cname)\n        array_ = np.linspace(0, 1, 2e6)\n        sink = CompressedMemorySink()\n        pack_ndarray(array_, sink, blosc_args=blosc_args)\n        blosc_header = decode_blosc_header(sink.chunks[0])\n        yield nt.assert_equal, blosc_header['flags'] >> 5, int_id\n\n\ndef test_typesize_is_set_correctly_with_default_blosc_args():\n    a = np.array([1, 2, 3], dtype='uint8')\n    sink = CompressedMemorySink()\n    pack_ndarray(a, sink)\n    expected_args = BloscArgs(typesize=1)\n    nt.assert_equal(expected_args, sink.blosc_args)\n\n\ndef test_typesize_is_set_correctly_with_custom_blosc_args():\n    a = np.array([1, 2, 3], dtype='uint8')\n    sink = CompressedMemorySink()\n    input_args = BloscArgs(clevel=9)\n    pack_ndarray(a, sink, blosc_args=input_args)\n    expected_args = BloscArgs(clevel=9, typesize=1)\n    nt.assert_equal(expected_args, sink.blosc_args)\n\n\ndef test_roundtrip_slice():\n    a = np.arange(100).reshape((10, 10))\n    s = a[3:5, 3:5]\n    for case in roundtrip_ndarray(s):\n        yield case\n"""
test/test_pretty.py,0,"b""#!/usr/bin/env nosetests\n# -*- coding: utf-8 -*-\n# vim :set ft=py:\n\n\nimport nose.tools as nt\n\n\nfrom bloscpack.pretty import (pretty_size,\n                              reverse_pretty,\n                              )\n\n\ndef test_pretty_filesieze():\n\n    nt.assert_equal('0B', pretty_size(0))\n    nt.assert_equal('9.0T', pretty_size(9898989898879))\n    nt.assert_equal('4.78G', pretty_size(5129898234))\n    nt.assert_equal('12.3M', pretty_size(12898234))\n    nt.assert_equal('966.7K', pretty_size(989898))\n    nt.assert_equal('128.0B', pretty_size(128))\n    nt.assert_equal(0, reverse_pretty('0B'))\n    nt.assert_equal(8, reverse_pretty('8B'))\n    nt.assert_equal(8192, reverse_pretty('8K'))\n    nt.assert_equal(134217728, reverse_pretty('128M'))\n    nt.assert_equal(2147483648, reverse_pretty('2G'))\n    nt.assert_equal(2199023255552, reverse_pretty('2T'))\n    # can't handle Petabytes, yet\n    nt.assert_raises(ValueError, reverse_pretty, '2P')\n"""
test/test_serializers.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# vim :set ft=py:\n\nimport nose.tools as nt\n\n\ntry:\n    from collections import OrderedDict\nexcept ImportError:  # pragma: no cover\n    from ordereddict import OrderedDict\n\n\nfrom bloscpack.serializers import (SERIALIZERS,\n                                   SERIALIZERS_AVAIL,\n                                   )\n\n\ndef test_serializers():\n    nt.assert_equal(SERIALIZERS_AVAIL, [b\'JSON\'])\n    output = \'{""dtype"":""float64"",""shape"":[1024],""others"":[]}\'\n    input_ = OrderedDict([(\'dtype\', ""float64""),\n                         (\'shape\', [1024]),\n                         (\'others\', [])])\n    for s in SERIALIZERS:\n        yield nt.assert_equal, output, s.dumps(input_)\n        yield nt.assert_equal, input_, s.loads(output)\n'"
test_cmdline/mktestarray.py,0,"b'#!/usr/bin/env python\n\n"""""" Utility for generating test data from cmdline interface tests.\n\nExample usage from cram file:\n\n  $ PYTHONPATH=$TESTDIR/../ ./$TESTDIR/mktestarray.py\n  $ ls\n  data.dat\n  meta.json\n""""""\n\n\nimport json\nimport os.path as path\n\nfrom bloscpack.numpy_io import _ndarray_meta\nimport numpy\n\nDATA_FILE = \'data.dat\'\nMETA_FILE = \'meta.json\'\n\n\ndef exists(filename):\n    return path.isfile(filename)\n\nif not exists(DATA_FILE) and not exists(META_FILE):\n    a = numpy.linspace(0, 100, int(2e7))\n    with open(DATA_FILE, \'wb\') as f:\n        f.write(a.tostring())\n    with open(META_FILE, \'w\') as m:\n        meta = dict(sorted(_ndarray_meta(a).items()))\n        m.write(json.dumps(meta))\n'"
test_cmdline/pyversion.py,0,"b""#!/usr/bin/env python\n\nimport sys\n\nif eval('sys.version_info[0:2] ' + sys.argv[1]):\n    sys.exit(0)\nelse:\n    sys.exit(1)\n"""
