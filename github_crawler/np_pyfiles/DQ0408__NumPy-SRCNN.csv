file_path,api_count,code
numpy_srcnn.py,35,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on 2018/11/20 21:51\n@author: Sucre\n@email: qian.dong.2018@gmail.com\n""""""\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom functools import reduce\nimport math\nimport time\nimport cv2\nimport os\n\n\nclass MSE(object):\n    def __init__(self):\n        print(""MSE"")\n\n    def cal_loss(self, x, y):\n        \'\'\'\n        :param x: train\xe8\xbe\x93\xe5\x85\xa5\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xae\xa1\xe7\xae\x97\xe5\xbe\x97\xe5\x88\xb0\xe7\x9a\x84out\n        :param y: target\n        :return: \xe5\x9d\x87\xe6\x96\xb9\xe5\xb7\xaeloss\n        \'\'\'\n        shape = x.shape\n        N = 1\n        for i in shape:\n            N = N * i\n        loss = np.sum(np.square(y - x)) / N\n        return loss\n\n    def gradient(self, x, y):\n        \'\'\'\n        :param x: train\xe8\xbe\x93\xe5\x85\xa5\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xae\xa1\xe7\xae\x97\xe5\xbe\x97\xe5\x88\xb0\xe7\x9a\x84out\n        :param y: target\n        :return: mse\xe5\xaf\xbc\xe6\x95\xb0\n        \'\'\'\n        dx = -(y - x)\n        return dx\n\n\nclass Relu(object):\n    def __init__(sel):\n        print(""Relu"")\n\n    def forward(self, x):\n        \'\'\'\n        :param x: \xe5\xbe\x85\xe6\xbf\x80\xe6\xb4\xbb\xe7\x9a\x84\xe5\x90\x91\xe9\x87\x8f\n        :return: \xe6\xbf\x80\xe6\xb4\xbb\xe5\x90\x8e\xe7\x9a\x84\xe5\x90\x91\xe9\x87\x8f\n        \'\'\'\n        self.x = x\n        return np.maximum(x, 0)\n\n    def backward(self):\n        pass\n\n\nclass Conv2D(object):\n    def __init__(self, shape, input_channels, output_channels, ksize=3, stride=1, method=\'SAME\'):\n        \'\'\'\n        :param shape: \xe6\x9c\x80\xe5\x85\x88\xe8\xbe\x93\xe5\x85\xa5\xe7\x9f\xa9\xe9\x98\xb5\xe7\x9a\x84shape\n        :param input_channels: \xe8\xbe\x93\xe5\x85\xa5\xe7\x9f\xa9\xe9\x98\xb5\xe7\x9a\x84channel\n        :param output_channels: \xe8\xbe\x93\xe5\x87\xba\xe7\x9f\xa9\xe9\x98\xb5\xe7\x9a\x84channel\n        :param ksize: kernel \xe7\x9a\x84\xe9\x95\xbf\xe5\xba\xa6\n        :param stride: \xe6\xad\xa5\xe9\x95\xbf\n        :param method: \xe5\x8d\xb7\xe7\xa7\xaf\xe7\x9a\x84\xe5\xbd\xa2\xe5\xbc\x8f\n        \'\'\'\n        self.output_channels = output_channels\n        self.input_channels = input_channels\n        self.batchsize = 1\n        self.stride = stride\n        self.ksize = ksize\n        self.method = method\n        weights_scale = math.sqrt(\n            reduce(lambda x, y: x * y, (1, 64, 64, 3)) / self.output_channels)  # \xe7\x94\xa8\xe4\xba\x8e\xe6\x8e\xa7\xe5\x88\xb6\xe6\x9d\x83\xe9\x87\x8d\xe7\x9a\x84\xe8\x8c\x83\xe5\x9b\xb4\xef\xbc\x8c\xe9\x98\xb2\xe6\xad\xa2\xe6\xa2\xaf\xe5\xba\xa6\xe6\xb6\x88\xe5\xa4\xb1/\xe7\x88\x86\xe7\x82\xb8\n        self.weights = np.random.standard_normal(\n            (ksize, ksize, self.input_channels, self.output_channels)) / weights_scale\n        self.bias = np.random.standard_normal(self.output_channels) / weights_scale\n        # He normal\xe5\x8f\x82\xe6\x95\xb0\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96  \xe6\xa0\x87\xe5\x87\x86\xe5\xb7\xae\xe5\xa4\xaa\xe5\xb0\x8f\xef\xbc\x8c\xe6\x94\xb6\xe6\x95\x9b\xe8\xbf\x87\xe6\x85\xa2\n        # fan_in = reduce(lambda x, y: x * y, shape)\n        # stddev = math.sqrt(2 / fan_in)\n        # self.weights = np.random.normal(0, stddev, (ksize, ksize, self.input_channels, self.output_channels))\n        # self.bias = np.random.normal(0, stddev, self.output_channels)\n        self.w_gradient = np.zeros(self.weights.shape)\n        self.b_gradient = np.zeros(self.bias.shape)\n\n    def forward(self, x):\n        \'\'\'\n        :param x: \xe8\xbe\x93\xe5\x85\xa5\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe7\x9a\x84\xe7\x9f\xa9\xe9\x98\xb5\n        :return: \xe8\xbe\x93\xe5\x87\xba\xe5\x8d\xb7\xe7\xa7\xaf\xe5\xb1\x82\xe7\x9a\x84\xe7\x9f\xa9\xe9\x98\xb5\n        \'\'\'\n        shape = x.shape\n        self.input_shape = shape\n        col_weights = self.weights.reshape([-1, self.output_channels])\n        if self.method == \'VALID\':\n            self.eta = np.zeros(\n                (shape[0], (shape[1] - self.ksize + 1) // self.stride, (shape[1] - self.ksize + 1) // self.stride,\n                 self.output_channels))\n        if self.method == \'SAME\':\n            self.eta = np.zeros((shape[0], shape[1] // self.stride, shape[2] // self.stride, self.output_channels))\n        if self.method == \'SAME\':\n            x = np.pad(x, (\n                (0, 0), (self.ksize // 2, self.ksize // 2), (self.ksize // 2, self.ksize // 2), (0, 0)),\n                       \'constant\', constant_values=0)\n        self.output_shape = self.eta.shape\n\n        self.col_image = []\n        conv_out = np.zeros(self.eta.shape)\n        for i in range(self.batchsize):\n            img_i = x[i][np.newaxis, :]\n            self.col_image_i = im2col(img_i, self.ksize, self.stride)\n            conv_out[i] = np.reshape(np.dot(self.col_image_i, col_weights) + self.bias, self.eta[0].shape)\n            self.col_image.append(self.col_image_i)\n        self.col_image = np.array(self.col_image)\n        return conv_out\n\n    def gradient(self, eta):\n        \'\'\'\n        \xe8\xae\xa1\xe7\xae\x97\xe7\xbd\x91\xe7\xbb\x9c\xe4\xb8\xad\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\n        \'\'\'\n        self.eta = eta\n        col_eta = np.reshape(eta, [self.batchsize, -1, self.output_channels])\n\n        for i in range(self.batchsize):\n            self.w_gradient += np.dot(self.col_image[i].T, col_eta[i]).reshape(self.weights.shape)\n        self.b_gradient += np.sum(col_eta, axis=(0, 1))\n\n        # deconv of padded eta with flippd kernel to get next_eta\n        if self.method == \'VALID\':\n            pad_eta = np.pad(self.eta, (\n                (0, 0), (self.ksize - 1, self.ksize - 1), (self.ksize - 1, self.ksize - 1), (0, 0)),\n                             \'constant\', constant_values=0)\n\n        if self.method == \'SAME\':\n            pad_eta = np.pad(self.eta, (\n                (0, 0), (self.ksize // 2, self.ksize // 2), (self.ksize // 2, self.ksize // 2), (0, 0)),\n                             \'constant\', constant_values=0)\n\n        flip_weights = np.flipud(np.fliplr(self.weights))\n        flip_weights = flip_weights.swapaxes(2, 3)\n        col_flip_weights = flip_weights.reshape([-1, self.input_channels])\n        col_pad_eta = np.array([im2col(pad_eta[i][np.newaxis, :], self.ksize, self.stride)\n                                for i in range(self.batchsize)])\n        next_eta = np.dot(col_pad_eta, col_flip_weights)\n        next_eta = np.reshape(next_eta, self.input_shape)\n        return next_eta\n\n    def backward(self, alpha=0.00001, weight_decay=0.0004):\n        \'\'\'\n        :param alpha: \xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\n        :param weight_decay: \xe8\xa1\xb0\xe9\x80\x80\xe7\x8e\x87\n        \xe6\x9b\xb4\xe6\x96\xb0\xe7\xbd\x91\xe7\xbb\x9c\xe4\xb8\xad\xe7\x9a\x84\xe6\x9d\x83\xe9\x87\x8d\n        \'\'\'\n        # self.weights *= (1 - weight_decay)\n        # self.bias *= (1 - weight_decay)\n        self.weights -= alpha * self.w_gradient\n        self.bias -= alpha * self.bias\n\n        self.w_gradient = np.zeros(self.weights.shape)\n        self.b_gradient = np.zeros(self.bias.shape)\n\n\ndef im2col(image, ksize, stride):\n    \'\'\'\n    :param image: \xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe7\x9f\xa9\xe9\x98\xb5\n    :param ksize: kernel\xe7\x9a\x84\xe9\x95\xbf\xe5\xba\xa6\n    :param stride: \xe6\xad\xa5\xe9\x95\xbf\n    :return: \xe8\xbd\xac\xe6\x8d\xa2\xe5\x90\x8e\xe7\x9a\x84image\xef\xbc\x8c\xe4\xbe\xbf\xe4\xba\x8e\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x8d\xb7\xe7\xa7\xaf\xe8\xbf\x90\xe7\xae\x97\n    \'\'\'\n    image_col = []\n    for i in range(0, image.shape[1] - ksize + 1, stride):\n        for j in range(0, image.shape[2] - ksize + 1, stride):\n            col = image[:, i:i + ksize, j:j + ksize, :].reshape([-1])\n            image_col.append(col)\n    image_col = np.array(image_col)\n    return image_col\n\n\ndef get_output(trains, targets, alpha, epoch, blurry_image):\n    \'\'\'\n    :param trains: \xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x867\xe4\xb8\xaa\xe6\xa8\xa1\xe7\xb3\x8a\xe5\x9b\xbe\xe7\x89\x87\n    :param targets: 7\xe4\xb8\xaa\xe9\xab\x98\xe6\xb8\x85\xe5\x9b\xbe\xe7\x89\x87\n    :param alpha: \xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\n    :param epoch: \xe8\xae\xad\xe7\xbb\x83\xe6\xac\xa1\xe6\x95\xb0\n    :param blurry_image: \xe5\xbe\x85\xe8\xb6\x85\xe5\x88\x86\xe8\xbe\xa8\xe9\x87\x8d\xe5\xbb\xba\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\n    :return: \xe9\x87\x8d\xe5\xbb\xba\xe5\x90\x8e\xe7\x9a\x84\xe9\xab\x98\xe6\xb8\x85\xe5\x9b\xbe\xe7\x89\x87\n    \'\'\'\n    shape = trains[0].shape\n    conv1 = Conv2D(shape, 3, 64)\n    conv2 = Conv2D(shape, 64, 64)\n    conv3 = Conv2D(shape, 64, 64)\n    conv4 = Conv2D(shape, 64, 64)\n    conv5 = Conv2D(shape, 64, 64)\n    conv6 = Conv2D(shape, 64, 64)\n    conv7 = Conv2D(shape, 64, 64)\n    conv8 = Conv2D(shape, 64, 64)\n    final_out = Conv2D(shape, 64, 3)\n    relu = Relu()\n    mse = MSE()\n    losses = []\n    # trains = [np.ones((1, 32, 32, 3)) for _ in range(7)]\n    # targets = [np.ones((1, 32, 32, 3)) for _ in range(7)]\n    for i in range(epoch):\n        for j in range(7):\n            conv1_out = relu.forward(conv1.forward(trains[j]))\n            conv2_out = relu.forward(conv2.forward(conv1_out))\n            conv3_out = relu.forward(conv3.forward(conv2_out))\n            conv4_out = relu.forward(conv4.forward(conv3_out))\n            conv5_out = relu.forward(conv5.forward(conv4_out))\n            conv6_out = relu.forward(conv6.forward(conv5_out))\n            conv7_out = relu.forward(conv7.forward(conv6_out))\n            conv8_out = relu.forward(conv8.forward(conv7_out))\n            out = final_out.forward(conv8_out)\n            train_loss = mse.cal_loss(out, targets[j])\n            losses.append(train_loss)\n            conv1.gradient(\n                conv2.gradient(\n                    conv3.gradient(\n                        conv4.gradient(\n                            conv5.gradient(\n                                conv6.gradient(\n                                    conv7.gradient(\n                                        conv8.gradient(\n                                            final_out.gradient(\n                                                mse.gradient(out, targets[j])))\n                                    )))))))\n            # alpha = alpha*(1-0.000005)**(i*7+j)\n            final_out.backward(alpha=alpha)\n            conv8.backward(alpha=alpha)\n            conv7.backward(alpha=alpha)\n            conv6.backward(alpha=alpha)\n            conv5.backward(alpha=alpha)\n            conv4.backward(alpha=alpha)\n            conv3.backward(alpha=alpha)\n            conv2.backward(alpha=alpha)\n            conv1.backward(alpha=alpha)\n            print(time.strftime(""%Y-%m-%d %H:%M:%S"", time.localtime()) +\n                  ""  epoch: %d  step: %d loss: %.4f alpha: %.1e"" % (i, j, train_loss, alpha))\n    conv1_out = relu.forward(conv1.forward(blurry_image))\n    conv2_out = relu.forward(conv2.forward(conv1_out))\n    conv3_out = relu.forward(conv3.forward(conv2_out))\n    conv4_out = relu.forward(conv4.forward(conv3_out))\n    conv5_out = relu.forward(conv5.forward(conv4_out))\n    conv6_out = relu.forward(conv6.forward(conv5_out))\n    conv7_out = relu.forward(conv7.forward(conv6_out))\n    conv8_out = relu.forward(conv8.forward(conv7_out))\n    sharp_image = final_out.forward(conv8_out)\n    plt.plot(losses)\n    plt.show()\n    return sharp_image\n\n\ndef get_trains_targets(img_name):\n    \'\'\'\n    :param img_name: \xe5\xbe\x85\xe9\x87\x8d\xe5\xbb\xba\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe5\x90\x8d\n    :return: [\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x867\xe4\xb8\xaa\xe6\xa8\xa1\xe7\xb3\x8a\xe5\x9b\xbe\xe7\x89\x87], [\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x847\xe4\xb8\xaa\xe9\xab\x98\xe6\xb8\x85\xe5\x9b\xbe\xe7\x89\x87]\n    \'\'\'\n    img = cv2.imread(img_name)\n    targets = []  # [(50,50)------(44,44)]\n    targets.append(img)\n    for i in range(7):\n        targets.append(cv2.resize(img, (img.shape[0] - i - 1, img.shape[1] - i - 1)))  # sharp image\n    trains = []  # [(50,50)------(44,44)]\n    for i in range(7):\n        trains.append(\n            cv2.resize(targets[i + 1], (targets[i + 1].shape[0] + 1, targets[i + 1].shape[1] + 1)))  # blurry image\n    targets = targets[0:-1]\n    trains = [train[np.newaxis, :, :, :] for train in trains]\n    targets = [target[np.newaxis, :, :, :] for target in targets]\n    return trains, targets\n\n\ndef main(img_name, alpha, resize_time, epoch):\n    \'\'\'\n    :param img_name: \xe5\xbe\x85\xe8\xb6\x85\xe6\xb8\x85\xe9\x87\x8d\xe5\xbb\xba\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe5\x90\x8d\n    :param alpha: \xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\n    :param resize_time: \xe9\x87\x8d\xe5\xbb\xba\xe6\xac\xa1\xe6\x95\xb0\xef\xbc\x8c\xe9\x87\x8d\xe5\xbb\xba\xe4\xb8\x80\xe6\xac\xa1\xef\xbc\x8cshape+1\n    :param epoch: \xe6\xaf\x8f\xe6\xac\xa1\xe9\x87\x8d\xe5\xbb\xba\xe7\x9a\x84\xe8\xae\xad\xe7\xbb\x83\xe6\xac\xa1\xe6\x95\xb0\n    :return: \xe8\xbe\x93\xe5\x87\xba\xe9\x87\x8d\xe5\xbb\xba\xe5\x90\x8e\xe7\x9a\x84\xe5\x9b\xbe\xe7\x89\x87\xe5\x88\xb0output_root\n    \'\'\'\n    output_root = ""./output_8_conv""\n    trains, targets = get_trains_targets(img_name)\n    blurry_image = cv2.resize(targets[0][0], (targets[0][0].shape[0] + 1, targets[0][0].shape[1] + 1))\n    blurry_image = blurry_image[np.newaxis, :, :, :]\n    sharp_one = get_output(trains, targets, epoch=epoch, blurry_image=blurry_image, alpha=alpha)  # 51, 51\n    img_out = sharp_one[0]\n    out_name = os.path.join(output_root, os.path.basename(img_name).split(\'.\')[0],\n                            str(img_out.shape[0]) + "".jpg"")\n    if not os.path.exists(os.path.dirname(out_name)):\n        os.makedirs(os.path.dirname(out_name))\n    cv2.imwrite(out_name, img_out)\n    for i in range(resize_time - 1):\n        trains = trains[0:-1]\n        trains.insert(0, blurry_image)\n        targets = targets[0:-1]\n        targets.insert(0, sharp_one)\n        blurry_image = cv2.resize(targets[0][0], (targets[0][0].shape[0] + 1, targets[0][0].shape[1] + 1))\n        blurry_image = blurry_image[np.newaxis, :, :, :]\n        sharp_one = get_output(trains, targets, epoch=epoch, blurry_image=blurry_image,\n                               alpha=alpha / (i + 1))  # 51+i+1, 51+i+1\n        img_out = sharp_one[0]\n        out_name = os.path.join(output_root, os.path.basename(img_name).split(\'.\')[0],\n                                str(img_out.shape[0]) + "".jpg"")\n        cv2.imwrite(out_name, img_out)\n\n\ndef t_cnn():\n    \'\'\'\n    \xe6\xb5\x8b\xe8\xaf\x95cnn\n    \'\'\'\n    trains = [np.ones((1, 32, 32, 3)) for _ in range(7)]\n    targets = [np.ones((1, 32, 32, 3)) + 1 for i in range(7)]\n    alpha = 1e-6\n    shape = (1, 32, 32, 3)\n    conv1 = Conv2D(shape, 3, 64)\n    final_out = Conv2D(shape, 64, 3)\n    relu = Relu()\n    mse = MSE()\n    losses = []\n    for i in range(10):\n        for j in range(7):\n            conv1_out = relu.forward(conv1.forward(trains[j]))\n            out = final_out.forward(conv1_out)\n            train_loss = mse.cal_loss(out, targets[j])\n            losses.append(train_loss)\n            conv1.gradient(final_out.gradient(mse.gradient(out, targets[j])))\n            final_out.backward(alpha=alpha)\n            conv1.backward(alpha=alpha)\n            print(time.strftime(""%Y-%m-%d %H:%M:%S"", time.localtime()) +\n                  ""  epoch: %d  step: %d loss: %.4f alpha: %.1e"" % (i, j, train_loss, alpha))\n    plt.plot(losses)\n    plt.title(""loss - epoch"")\n    plt.xlabel(\'epoch\')\n    plt.ylabel(\'loss\')\n    plt.show()\n\n\nif __name__ == ""__main__"":\n    # t_cnn()\n    try:\n        image_name = sys.argv[1]\n        alpha = float(sys.argv[2])\n        resize_time = int(sys.argv[3])\n        epoch = int(sys.argv[4])\n        main(image_name, alpha=alpha, resize_time=resize_time, epoch=epoch)\n    except:\n        main(\'./image0.jpg\', alpha=3e-11, resize_time=2, epoch=200)\n'"
