file_path,api_count,code
Free_Tools/__init__.py,1,"b'# -*- coding: utf-8 -*-\r\n""""""\r\n==================\r\nnpgeom.__init__.py\r\n==================\r\n\r\nAuthor :\r\n    Dan_Patterson@carleton.ca\r\n\r\nModified : 2019-09-06\r\n    Creation date during 2019 as part of ``arraytools``.\r\n\r\nPurpose : Tools for working with point and poly features as an array class\r\n    Requires npGeo to implement the array geometry class.\r\n\r\nNotes\r\n-----\r\n\r\nImport suggestion and package properties and methods.  The Geo class in npGeo\r\nprovides the base class for this package.  It is based on the numpy ndarray.\r\n\r\n>>> import npgeom as npg\r\n\r\n>>> npg.npg_io.__all__\r\n... [\'poly2array\', \'load_geojson\', \'arrays_to_Geo\', \'Geo_to_arrays\',\r\n...  \'array_ift\', \'_make_nulls_\', \'getSR\', \'fc_composition\', \'fc_data\',\r\n...  \'fc_geometry\', \'fc_shapes\', \'getSR\', \'shape_to_K\', \'array_poly\',\r\n...  \'geometry_fc\', \'prn_q\', \'_check\', \'prn_tbl\', \'prn_geo\']\r\n\r\n>>> npg.npGeo.__all__\r\n... [\'Geo\', \'Update_Geo\']\r\n\r\n>>> npg.npg_helpers.__all__\r\n... [\'_angles_\', \'_area_centroid_\', \'_ch_\', \'_ch_scipy\',\r\n...  \'_ch_simple_\', \'_nan_split_\', \'_o_ring_\', \'_pnts_on_line_\',\r\n...  \'_polys_to_segments_\', \'_polys_to_unique_pnts_\', \'_simplify_lines_\']\r\n\r\n**Import options for arcpy functions**\r\n\r\n>>> import arcgisscripting as ags\r\n... [\'ContingentFieldValue\', \'ContingentValue\', \'DatabaseSequence\', \'Describe\',\r\n... \'Domain\', \'Editor\', \'ExtendTable\', \'FeatureClassToNumPyArray\',\r\n... \'InsertCursor\', \'ListContingentValues\', \'ListDatabaseSequences\',\r\n... \'ListDomains\', \'ListFieldConflictFilters\', \'ListReplicas\', \'ListSubtypes\',\r\n... \'ListVersions\', \'NumPyArrayToFeatureClass\', \'NumPyArrayToTable\', \'Replica\',\r\n... \'SearchCursor\', \'TableToNumPyArray\', \'UpdateCursor\', \'Version\', \'Walk\'...]\r\n\r\n>>> ags.da.FeatureClassToNumPyArray(...)  # useage\r\n\r\nArcpy methods and properties needed::\r\n\r\n    arcpy.Point, arcpy.Polyline, arcpy.Polygon, arcpy.Array\r\n    arcpy.ListFields\r\n    arcpy.management.CopyFeatures\r\n    arcpy.da.Describe\r\n    arcpy.da.InsertCursor\r\n    arcpy.da.SearchCursor\r\n    arcpy.da.FeatureClassToNumPyArray\r\n""""""\r\n# pylint: disable=unused-import\r\n# pylint: disable=W0611\r\nimport numpy as np\r\n\r\nfrom . import (\r\n        npGeo, npg_io, npg_geom, npg_table, npg_create, npg_analysis,\r\n        smallest_circle, _tests_\r\n        )  # noqa\r\n\r\nfrom .npGeo import (Geo, Update_Geo, dirr, geo_info)  # noqa\r\n\r\nfrom .npg_io import (\r\n        poly2array, load_geojson, geojson_Geo, fc_json, arrays_to_Geo,\r\n        Geo_to_arrays, array_ift, _make_nulls_, getSR, shape_K,\r\n        fc_composition, fc_data, fc_geometry, fc_shapes, array_poly,\r\n        geometry_fc, prn_q, _check, prn_tbl, prn_geo,\r\n        shape_properties, flatten_to_points\r\n        )  # noqa\r\n\r\nfrom .npg_geom import (\r\n        _area_centroid_, _angles_, _rotate_, _ch_scipy_,\r\n        _ch_simple_, _ch_, _dist_along_, _percent_along_, _pnts_on_line_,\r\n        _pnt_on_segment_, _polys_to_unique_pnts_,\r\n        _simplify_lines_, _tri_pnts_,\r\n        )  # noqa\r\n\r\nfrom .npg_table import (\r\n        col_stats, crosstab_tbl, crosstab_rc, crosstab_array\r\n        )  # noqa\r\n\r\nfrom .npg_analysis import (\r\n        closest_n, distances, not_closer, n_check, n_near, n_spaced,\r\n        intersects, knn, knn0, mst, connect, concave\r\n        )  # noqa\r\n\r\n__all_io__ = [\r\n        \'__all_io__\',\r\n        \'arrays_to_Geo\', \'Geo_to_arrays\', \'_check\', \'_make_nulls_\',\r\n        \'array_ift\', \'array_poly\', \'fc_composition\', \'fc_data\',\r\n        \'fc_geometry\', \'fc_shapes\', \'geometry_fc\', \'getSR\', \'getSR\',\r\n        \'load_geojson\', \'poly2array\', \'prn_geo\', \'prn_q\', \'prn_tbl\',\r\n        \'shape_to_K\'\r\n        ]  # noqa\r\n__all_geo__ = [\r\n        \'__all_geo__\',\r\n        \'Geo\', \'Update_Geo\', \'dirr\', \'geo_info\'\r\n        ]  # noqa\r\n\r\n__all_geom__ = [\r\n        \'__all_geom__\',\r\n        \'_angles_\', \'_area_centroid_\', \'_ch_\', \'_ch_scipy_\', \'_ch_simple_\',\r\n        \'_dist_along_\', \'_percent_along_\', \'_pnt_on_poly_\', \'_pnt_on_segment_\',\r\n        \'_pnts_in_poly_\', \'_pnts_on_line_\', \'_polys_to_unique_pnts_\',\r\n        \'_rotate_\', \'_simplify_lines_\', \'_tri_pnts_\', \'ft\', \'np\', \'p_o_p\'\r\n        ]  # noqa\r\n\r\n__all_analysis__ = [\r\n        \'__all_analysis__\',\r\n        \'closest_n\', \'distances\', \'not_closer\', \'n_check\', \'n_near\',\r\n        \'n_spaced\', \'intersects\', \'knn\', \'knn0\', \'_dist_arr_\', \'_e_dist_\',\r\n        \'mst\', \'connect\', \'concave\'\r\n        ]  # noqa\r\n\r\n__all_table__ = [\r\n        \'__all_table__\',\r\n        \'crosstab_tbl\', \'crosstab_rc\', \'crosstab_array\', \'col_stats\',\r\n        \'group_stats\'\r\n        ]  # noqa\r\nargs = [__all_io__, __all_geo__, __all_geom__, __all_analysis__,\r\n        __all_table__]\r\n__all__ = np.concatenate([np.asarray(a) for a in args]).tolist()\r\n\r\n# __all__.sort()\r\n\r\n\r\nprint(""\\nUsage...\\n  import npgeom as npg"")\r\n'"
Free_Tools/npGeo.py,200,"b'# -*- coding: utf-8 -*-\r\n""""""\r\n=====\r\nnpGeo\r\n=====\r\n\r\nScript : npGeo.py\r\n    A geometry class and methods based on numpy.\r\n\r\nAuthor :\r\n    Dan_Patterson@carleton.ca\r\n\r\nModified : 2019-10-21\r\n    Initial creation period 2019-05.\r\n\r\nPurpose : geometry tools\r\n    A numpy geometry class, its properties and methods.\r\n\r\nNotes\r\n-----\r\n**Class instantiation**\r\n\r\nQuote from Subclassing ndarrays::\r\n\r\n    As you can see, the object can be initialized in the __new__ method or the\r\n    __init__ method, or both, and in fact ndarray does not have an __init__\r\n    method, because all the initialization is done in the __new__ method.\r\n\r\n----\r\n\r\n**Geo class notes**\r\n\r\nThe IDs can either be 0-based or in the case of some data-types, 1-based.\r\nNo assumption is made about IDs being sequential.  In the case of featureclass\r\ngeometry, the OID@ property is read.  For other geometries, provide ID values\r\nas appropriate.\r\n\r\nPoint, polyline, polygon features represented as numpy ndarrays.\r\nThe required inputs are created using ``fc_geometry(in_fc)`` or\r\n``Arrays_to_Geo``.\r\n\r\n**Attributes**\r\n\r\nNormal ndarray parameters including shape, ndim, dtype.\r\n\r\nshapes :\r\n    The points for polyline, polygons.\r\nparts :\r\n    Multipart shapes and/or outer and inner rings for holes.\r\nbits :\r\n    The final divisions to individual bits constituting the shape.\r\nis_multipart :\r\n    Array of booleans\r\npart_cnt :\r\n    ndarray of ids and counts.\r\npnt_cnt :\r\n    ndarray of ids and counts.\r\ngeometry properties :\r\n    Areas, centers, centroids and lengths are properties and not methods.\r\n\r\n**Comments**\r\n\r\nYou can use ``arrays_to_Geo`` fro npg_io.py to produce the required 2D array\r\nfrom lists of array-like objects of the same dimension, or a single array.\r\nThe IFT will be derived from breaks in the sequence and/or the presence of\r\nnull points within a sequence.\r\n\r\n>>> import npgeom as npg\r\n>>> g = npg.Geo(a, IFT)\r\n>>> g.__dict__.keys()\r\ndict_keys([\'IDs\', \'FT\', \'IFT\', \'K\', \'Info\', \'N\', \'X\', \'Y\', \'XY\', \'Z\'])\r\n>>> sorted(g.__dict__.keys())\r\n[\'FT\', \'IDs\', \'IFT\', \'Info\', \'K\', \'N\', \'X\', \'XY\', \'Y\', \'Z\']\r\n\r\n----\r\n\r\n**See Also**\r\n\r\n__init__.py :\r\n    General comments about the package.\r\nnpg_io.py :\r\n    Import and conversion routines for the Geo class.\r\nnpg_geom :\r\n    Methods/functions for working with the Geo class or used by it.\r\nnpg_table :\r\n    Methods/functions associated with tabular data.\r\n\r\n----\r\n\r\n**General notes**\r\n\r\nThe Geo class returns a 2D array of points which may consist of single or\r\nmultipart shapes with or without inner rings (holes).\r\n\r\nThe methods defined  in the Geo class allow one to operate on the parts of the\r\nshapes separately or in combination.  Since the coordinate data are represented\r\nas an Nx2 array, it is sometimes easier to perform calculations on the dataset\r\nall at once using numpy ``nan`` functions.  For example, to determine the\r\nminimum for the whole dataset:\r\n\r\n>>> np.nanmin(Geo, axis=0)\r\n\r\nAll null points (nan, nan) are omitted from the calculations.\r\n\r\n----\r\n\r\n**Working with np.ndarray and Geo class**\r\n\r\nTo check the difference between the np.ndarray and Geo class, use...\r\n\r\ngeo_info(g)\r\n\r\nGeo methods and properties\r\n    FT, IDs, IFT, Info, K, N, X, XY, Y, Z, __dict__, __module__,\r\n    aoi_extent, aoi_rectangle, areas, bit_IFT, bit_cnt, bit_ids, bits,\r\n    bounding_circles, centers, centroids, change_indices, close_polylines,\r\n    common_segments, convex_hulls, densify_by_distance,\r\n    densify_by_percent, extent_centers, extent_rectangles, extents,\r\n    fill_holes, first_bit, first_part, get, holes_to_shape, info,\r\n    is_clockwise, is_convex, is_multipart, lengths, maxs, means,\r\n    min_area_rect, mins, moveto_origin, multipart_to_singlepart, od_pairs,\r\n    outer_rings, part_IFT, part_cnt, part_ids, parts, pnt_ids,\r\n    pnt_on_poly, point_indices, point_info, polygon_angles,\r\n    polygons_to_polylines, polyline_angles, polylines_to_polygons,\r\n    polys_to_points, polys_to_segments, pull, rotate, shapes, shift,\r\n    shp_IFT, shp_cnt, shp_ids, sort_by_area, sort_by_extent,\r\n    sort_by_length, sort_coords, split_by, translate, triangulate,\r\n    unique_segments\r\n\r\nGeo.__dict_keys()\r\n    __module__, __doc__, __new__, __array_finalize__, __array_wrap__,\r\n    shapes, parts, bits, shp_IFT, part_IFT, bit_IFT, shp_ids, part_ids,\r\n    bit_ids, pnt_ids, shp_cnt, part_cnt, bit_cnt, first_bit, first_part,\r\n    get, pull, split_by, outer_rings, areas, lengths, centers, centroids,\r\n    aoi_extent, aoi_rectangle, extents, extent_centers, extent_rectangles,\r\n    maxs, mins, means, is_clockwise, is_convex, is_multipart,\r\n    polyline_angles, polygon_angles, moveto_origin, shift, translate,\r\n    rotate, bounding_circles, convex_hulls, min_area_rect, triangulate,\r\n    fill_holes, holes_to_shape, multipart_to_singlepart, od_pairs,\r\n    polylines_to_polygons, polygons_to_polylines, polys_to_points,\r\n    close_polylines, densify_by_distance, densify_by_percent, pnt_on_poly,\r\n    polys_to_segments, common_segments, unique_segments, change_indices,\r\n    point_indices, sort_by_area, sort_by_length, sort_by_extent,\r\n    sort_coords, info, point_info, __dict__\r\n\r\n----\r\n\r\n**Useage of methods**\r\n\r\n`g` is a Geo instance with 2 shapes.  Both approaches yield the same results.\r\n\r\n>>> Geo.centers(g)\r\narray([[ 5.  , 14.93],\r\n       [15.5 , 15.  ]])\r\n>>> g.centers()\r\narray([[ 5.  , 14.93],\r\n       [15.5 , 15.  ]])\r\n\r\nReferences\r\n----------\r\n`Subclassing ndarrays\r\n<https://docs.scipy.org/doc/numpy/user/basics.subclassing.html>`_.\r\n\r\n----\r\n\r\n**Sample file**\r\n\r\nSaved in the arraytools folder and on GitHub::\r\n\r\n    fname = \'C:/Git_Dan/arraytools/Data/geo_array.npz\'\r\n    npzfiles = np.load(fname)   # ---- the Geo, I(ds)F(rom)T(o) arrays\r\n    npzfiles.files              # ---- will show ==> [\'s2\', \'IFT\']\r\n    s2 = npzfiles[\'s2\']         # ---- slice by name from the npz file to get\r\n    IFT = npzfiles[\'IFT\']       #      each array\r\n""""""\r\n# pylint: disable=R0902  # Too many instance attributes\r\n# pylint: disable=R0904  # pylint issue\r\n# pylint: disable=C0103  # invalid-name\r\n# pylint: disable=R0914  # Too many local variables\r\n# pylint: disable=E1136  # s3lf.base is unsubscriptable\r\n# pylint: disable=W0105  # string statement has no effect\r\n# pylint: disable=W0201  # attribute defined outside __init__... none in numpy\r\n# pylint: disable=W0212  # Access to a protected member...\r\n# pylint: disable=W0621  # redefining name\r\n\r\nimport sys\r\nfrom textwrap import dedent\r\nimport numpy as np\r\n\r\nfrom numpy.lib.recfunctions import structured_to_unstructured as stu\r\nfrom numpy.lib.recfunctions import unstructured_to_structured as uts\r\nfrom numpy.lib.recfunctions import repack_fields\r\n\r\nimport npg_geom as geom\r\nimport smallest_circle as sc\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.1f}\'.format}\r\nnp.set_printoptions(edgeitems=5, linewidth=120, precision=2, suppress=True,\r\n                    threshold=200, formatter=ft)\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\nFLOATS = np.typecodes[\'AllFloat\']\r\nINTS = np.typecodes[\'AllInteger\']\r\nNUMS = FLOATS + INTS\r\nTwoPI = np.pi*2.\r\n\r\n__all__ = [\'Geo\', \'Update_Geo\', \'dirr\', \'geo_info\']\r\n\r\n\r\n# ----------------------------------------------------------------------------\r\n# ---- (1) ... Geo class, properties and methods ... -------------------------\r\n#\r\nclass Geo(np.ndarray):\r\n    """"""\r\n    **Geo class**\r\n\r\n    Create a Geo array based on the numpy ndarray.  The class focus is on\r\n    geometry properties and methods.  Construction of geometries can be made\r\n    using File Geodatabase Featureclasses (Esri) or GeoJSON data as the source\r\n    of the base geometries.\r\n\r\n    Construction from an ndarray, IFT, Kind and optional Info.\r\n\r\n    Parameters\r\n    ----------\r\n    arr : array-like\r\n        A 2D array sequence of points with shape (N, 2).\r\n    IFT : array-like\r\n        Defines, the I(d)F(rom)T(o) values identifying object parts if\r\n        ``arr`` represents polylines or polygons.  Shape (N, 3) required.\r\n    Kind : integer\r\n        Points (0), polylines/lines (1) and polygons (2).\r\n    Info : string (optional)\r\n        Optional information if needed.\r\n    """"""\r\n    # ----\r\n    def __new__(cls, arr=None, IFT=None, Kind=2, Info=""Geo array""):\r\n        """"""\r\n        See script header for construction notes.\r\n        """"""\r\n        msg = Geo.__doc__\r\n        arr = np.asarray(arr)\r\n        IFT = np.asarray(IFT)\r\n        if (arr.ndim != 2) or (IFT.ndim != 2):\r\n            print(dedent(msg))\r\n            return None\r\n        if (IFT.shape[-1] != 3) or (Kind not in (0, 1, 2)):\r\n            print(dedent(msg))\r\n            return None\r\n        # ----\r\n        self = arr.view(cls)     # view as Geo class\r\n        self.IFT = IFT\r\n        self.IDs = IFT[:, 0]\r\n        self.FT = IFT[:, 1:]\r\n        self.K = Kind\r\n        self.Info = Info\r\n        # --- other properties\r\n        self.N = len(np.unique(self.IDs))  # sample size, unique shapes\r\n        if self.shape[1] >= 2:             # X,Y and XY initialize\r\n            self.X = arr[:, 0]\r\n            self.Y = arr[:, 1]\r\n            self.XY = arr[:, :2]\r\n        if self.shape[1] >= 3:  # add Z, although not implemented\r\n            self.Z = arr[:, 2]  # directly, but kept for future additions\r\n        else:\r\n            self.Z = None\r\n        return self\r\n\r\n    def __array_finalize__(self, src_arr):\r\n        """"""The new object... this is where housecleaning takes place for\r\n        explicit, view casting or new from template...\r\n        ``src_arr`` is either None, any subclass of ndarray including our own\r\n        (words from documentation) OR another instance of our own array.\r\n        You can use the following with a dictionary instead of None:\r\n\r\n        >>> self.info = getattr(obj,\'info\',{})\r\n        """"""\r\n        if src_arr is None:\r\n            return\r\n        self.IFT = getattr(src_arr, \'IFT\', None)\r\n        self.IDs = getattr(src_arr, \'IDs\', None)\r\n        self.FT = getattr(src_arr, \'FT\', None)\r\n        self.K = getattr(src_arr, \'K\', None)\r\n        self.Info = getattr(src_arr, \'Info\', None)\r\n        self.N = getattr(src_arr, \'N\', None)\r\n        self.X = getattr(src_arr, \'X\', None)\r\n        self.Y = getattr(src_arr, \'Y\', None)\r\n        self.XY = getattr(src_arr, \'XY\', None)\r\n\r\n    def __array_wrap__(self, out_arr, context=None):\r\n        """"""Wrap it up""""""\r\n        return np.ndarray.__array_wrap__(self, out_arr, context)\r\n\r\n    # ------------------------------------------------------------------------\r\n    """"""To do list:\r\n    buffer\r\n    clip\r\n    contains\r\n    cut\r\n    dominant direction\r\n    """"""\r\n    # ------------------- End of class definition ----------------------------\r\n    # ---- geometry to object array by shape, part or bit\r\n    #\r\n    @property\r\n    def shapes(self):\r\n        """"""Shapes consist of points.  They can be singlepart or multipart.\r\n        Conversion will yield either an object array or ndarray.\r\n        """"""\r\n        uniq = np.unique(self.IDs)\r\n        c = [self.FT[self.IDs == i].ravel() for i in uniq]\r\n        c1 = [(i[0], i[-1]) for i in c]\r\n        return np.array([self.base[f:t] for f, t in c1])  # .squeeze()\r\n\r\n    @property\r\n    def parts(self):\r\n        """"""Deconstruct the 2D array into its parts, generally returning an\r\n        object array.  The reverse is np.vstack(self).\r\n        """"""\r\n        xy = self.base\r\n        if xy is None:\r\n            xy = self.XY.view(np.ndarray)\r\n        return np.asarray([xy[f:t] for f, t in self.FT])  # np.split equiv.\r\n\r\n    @property\r\n    def bits(self):\r\n        """"""Deconstruct the 2D array then parts of a piece if a piece contains\r\n        multiple parts.  Keeps all rings and removes nan.\r\n        """"""\r\n        fr_to = self.bit_IFT[:, 1:].tolist()\r\n        xy = self.base\r\n        if xy is None:\r\n            xy = self.XY.view(np.ndarray)\r\n        return np.asarray([xy[f:t] for f, t in fr_to])\r\n\r\n    # ---- IFT by shape, part, or bit\r\n    #\r\n    @property\r\n    def shp_IFT(self):\r\n        """"""Shape IFT values.""""""\r\n        i_too = self.shp_cnt\r\n        too = np.cumsum(i_too[:, 1])\r\n        fr = np.concatenate(([0], too[:-1]))\r\n        return np.array(list(zip(i_too[:, 0], fr, too)))\r\n\r\n    @property\r\n    def part_IFT(self):\r\n        """"""Just for completeness, return the IFT values of the geo object.\r\n        """"""\r\n        return self.IFT\r\n\r\n    @property\r\n    def bit_IFT(self):\r\n        """"""IFT for shape bits.\r\n        """"""\r\n        w = np.where(np.isnan(self.X))[0]\r\n        f = self.IFT[:, 1]\r\n        t = self.IFT[:, 2]\r\n        wn = np.digitize(w, t)\r\n        too = np.sort(np.concatenate((w, t)))\r\n        fr = np.sort(np.concatenate((w+1, f)))\r\n        ids = np.sort(np.concatenate((self.IDs[wn], self.IDs)))\r\n        new_ift = np.array(list(zip(ids, fr, too)))\r\n        return new_ift\r\n\r\n    # ---- IDs for shapes, parts, bits or points\r\n    @property\r\n    def shp_ids(self):\r\n        """"""Return the ID values of each shape.  Note, they may not be\r\n        sequential or continuous.\r\n        """"""\r\n        return self.shp_IFT[:, 0]\r\n\r\n    @property\r\n    def part_ids(self):\r\n        """"""Return the ID values of the shape parts.  See, shp_ids warning.\r\n        """"""\r\n        self.IFT[:, 0]\r\n\r\n    @property\r\n    def bit_ids(self):\r\n        """"""Return the ID values for each bit in a shape.  Ids are repeated for\r\n        each part or ring in a shape.  See, shp_ids warning.\r\n        """"""\r\n        return self.bit_IFT[:, 0]\r\n\r\n    @property\r\n    def pnt_ids(self):\r\n        """"""Feature id that each point belongs to.  Useful for slicing the\r\n        points of poly features.  See, shp_ids warning.\r\n        """"""\r\n        return np.concatenate([np.repeat(i[0], i[2] - i[1]) for i in self.IFT])\r\n\r\n    # ---- counts by shape, part and bit\r\n    @property\r\n    def shp_cnt(self):\r\n        """"""Point count by shape, including nulls""""""\r\n        idx = self.pnt_ids\r\n        seqs = np.split(idx, np.where(np.diff(idx) != 0)[0] + 1)\r\n        vals = [i[0] for i in seqs]\r\n        cnts = [len(i) for i in seqs]\r\n        return np.array(list(zip(vals, cnts)))\r\n\r\n    @property\r\n    def part_cnt(self):\r\n        """"""Part count for shapes. Returns IDs and count array""""""\r\n        return np.vstack(np.unique(self.IDs, return_counts=True)).T\r\n\r\n    @property\r\n    def bit_cnt(self):\r\n        """"""Point count for shape bits, by ID, excluding null points.""""""\r\n        b_ids = self.bit_ids\r\n        return np.array([(b_ids[i], len(p[~np.isnan(p[:, 0])]))\r\n                         for i, p in enumerate(self.bits)])\r\n\r\n    # ---- slicing, sampling equivalents\r\n    #\r\n    def first_bit(self, asGeo=True):\r\n        """"""Get the first bit of a multipart shape and/or shapes with holes""""""\r\n        b_ids = self.bit_ids\r\n        ids = np.unique(b_ids)\r\n        a_2d = [self.bits[b_ids == i][0] for i in ids]\r\n        if asGeo:\r\n            info = ""{} first bit"".format(str(self.Info))\r\n            id_too = np.array([len(i) for i in a_2d])\r\n            id_too = np.concatenate((ids[:, None], id_too[:, None]), axis=1)\r\n            return Update_Geo(a_2d, self.K, id_too, info)  # ---- update Geo\r\n        return a_2d\r\n\r\n    def first_part(self, asGeo=True):\r\n        """"""Return the first part of a multipart shape and/or a shape with\r\n        holes.\r\n        """"""\r\n        p_ids = self.IDs\r\n        ids = np.unique(p_ids)\r\n        a_2d = [self.parts[p_ids == i][0] for i in ids]\r\n        if asGeo:\r\n            info = ""{} first bit"".format(str(self.Info))\r\n            id_too = np.array([len(i) for i in a_2d])\r\n            id_too = np.concatenate((ids[:, None], id_too[:, None]), axis=1)\r\n            return Update_Geo(a_2d, 2, id_too, info)  # ---- update Geo\r\n        return a_2d\r\n\r\n    def get(self, ID=None, asGeo=True):\r\n        """"""Return a Geo or ndarray associated with the feature ID.  The ID must\r\n        exist, otherwise None is returned.\r\n\r\n        Parameters\r\n        ----------\r\n        ID : integer\r\n            A single integer value.\r\n        asGeo : Boolean\r\n            True, returns an updated Geo array.  False returns an ndarray or\r\n            object array.\r\n        """"""\r\n        if not isinstance(ID, (int)):\r\n            print(""Integer ID is required, see ``pull`` for multiple values."")\r\n            return None\r\n        if ID not in np.unique(self.IDs):\r\n            print(""ID not in possible values."")\r\n            return None\r\n        # f_t = self.IFT[self.IDs == ID]  # the parts associated with the ID\r\n        # s_e = f_t[:, 1:].ravel()        # slice the ft portion, ravel to 1D\r\n        # shp = self[s_e[0]: s_e[-1]]     # slice off the first and last\r\n        shp = self[self.pnt_ids == ID]\r\n        if asGeo:\r\n            f_t = self.IFT[self.IDs == ID]\r\n            return Geo(shp, IFT=f_t, Kind=self.K)\r\n        return np.asarray(shp)\r\n\r\n    def pull(self, ID_list=None, asGeo=True):\r\n        """"""Pull multiple shapes, in the order provided.  The original IDs are\r\n        kept but the point sequence is altered to reflect the new order.\r\n\r\n        Parameters\r\n        ----------\r\n        ID_list : array-like\r\n            A list, tuple or ndarray of ID values identifying which features\r\n            to pull from the input.\r\n        asGeo : Boolean\r\n            True, returns an updated Geo array.  False returns an ndarray or\r\n            object array.\r\n\r\n        Notes\r\n        -----\r\n        >>> a.pull(np.arange(3:8))  # get shapes over a range of values\r\n        >>> a.pull([1, 3, 5])  # get selected shapes\r\n        """"""\r\n        ID_list = np.asarray(ID_list)\r\n        if (ID_list.ndim and ID_list.size) == 0:\r\n            print(""An array/tuple/list of IDs are required, see ``get``."")\r\n            return None\r\n        if not np.all([a in self.IDs for a in ID_list]):\r\n            print(""Not all required IDs are in the list provided"")\r\n            return None\r\n        parts_ = np.vstack([self.IFT[self.IDs == i] for i in ID_list])\r\n        vals = [np.asarray(self.XY[p[1]:p[2]]) for p in parts_]\r\n        if asGeo:\r\n            ids = parts_[:, 0]\r\n            too = np.cumsum([len(i) for i in vals])\r\n            frum = np.concatenate(([0], too))\r\n            IFT = np.array(list(zip(ids, frum, too)))\r\n            vals = np.vstack(vals)\r\n            return Geo(vals, IFT, self.K)\r\n        return np.asarray(vals)\r\n\r\n    def split_by(self, by_part=False):\r\n        """"""Split points by shape or by parts for each shape. **keep for now**\r\n        Use self.parts or self.shapes directly.""""""\r\n        return self.parts if by_part else self.shapes\r\n\r\n    def outer_rings(self, asGeo=False):\r\n        """"""Collect the outer ring of a polygon shape.  Returns a list of\r\n        ndarrays or optionally a new Geo array.\r\n        """"""\r\n        if self.K != 2:\r\n            print(""Polygons required..."")\r\n            return None\r\n        if asGeo:\r\n            return self.first_bit(True)\r\n        return self.first_bit(False)\r\n\r\n    # ---- areas, centrality, lengths/perimeter for polylines/polygons\r\n    #\r\n    def areas(self, by_shape=True):\r\n        """"""Area for the sub arrays using _e_area for the calculations.  Uses\r\n        ``_area_part_`` to calculate the area.\r\n        ``by_shape=True`` reduces the redundancy in the areas which are\r\n        repeated for each part.\r\n        """"""\r\n        def _area_part_(a):\r\n            """"""Mini e_area, used by areas and centroids""""""\r\n            x0, y1 = (a.T)[:, 1:]\r\n            x1, y0 = (a.T)[:, :-1]\r\n            e0 = np.einsum(\'...i,...i->...i\', x0, y0)\r\n            e1 = np.einsum(\'...i,...i->...i\', x1, y1)\r\n            return np.nansum((e0 - e1)*0.5)\r\n        # ----\r\n        if self.K != 2:\r\n            print(""Polygons required"")\r\n            return None\r\n        subs = [_area_part_(i) for i in self.parts]   # call _area_part_\r\n        ids = self.IDs\r\n        totals = np.bincount(ids, weights=subs)[ids]  # weight by IDs\' area\r\n        if by_shape:\r\n            _, idx = np.unique(ids, return_index=True)\r\n            return totals[idx]\r\n        return totals\r\n\r\n    def lengths(self, by_shape=True):\r\n        """"""Polyline lengths or polygon perimeter.""""""\r\n        def _cal(a):\r\n            """"""Perform the calculation, mini-e_leng.""""""\r\n            diff = a[:-1] - a[1:]\r\n            return np.nansum(np.sqrt(np.einsum(\'ij,ij->i\', diff, diff)))\r\n        # ----\r\n        if self.K not in (1, 2):\r\n            print(""Polyline/polygon representation is required."")\r\n            return None\r\n        lengs = [_cal(i) for i in self.parts]\r\n        ids = self.IDs\r\n        totals = np.bincount(ids, weights=lengs)[ids]\r\n        if by_shape:\r\n            _, idx = np.unique(ids, return_index=True)\r\n            return totals[idx]\r\n        return totals\r\n\r\n    def centers(self, by_part=False, remove_dups=True):\r\n        """"""Return the center of an array\'s unique non-nan points.  Each part\r\n        of multipart features are determined separately if ``by_part = True``.\r\n        Mean per feature or part, optionally keep duplicates.""""""\r\n        if len(self.part_cnt) == 1:\r\n            chunks = [self]\r\n        else:\r\n            chunks = self.split_by(by_part)\r\n        if remove_dups:\r\n            chunks = [np.unique(i, axis=0) for i in chunks]\r\n        return np.asarray([np.nanmean(i, axis=0) for i in chunks])\r\n\r\n    @property\r\n    def centroids(self):\r\n        """"""Centroid of the polygons.  Uses ``_area_centroid_`` to calculate\r\n        values for each shape part.  The centroid is weighted by area for\r\n        multipart features.\r\n        """"""\r\n        # ----\r\n        def weighted(x_y, I, areas):\r\n            """"""Weighted coordinate by area, x_y is either the x or y.""""""\r\n            w = x_y * areas                # area weighted x or y\r\n            w1 = np.bincount(I, w)[I]      # weight divided by bin size\r\n            ar = np.bincount(I, areas)[I]  # areas per bin\r\n            return w1/ar\r\n        # ----\r\n        if self.K != 2:\r\n            print(""Polygons required."")\r\n            return None\r\n        centr = []\r\n        areas = []\r\n        ids = self.IDs\r\n        uni = np.unique(ids)\r\n        for ID in uni:\r\n            parts_ = self.FT[self.IDs == ID]\r\n            out = np.asarray([np.asarray(self.XY[p[0]:p[1]]) for p in parts_])\r\n            for prt in out:\r\n                area, cen = geom._area_centroid_(prt)  # ---- determine both\r\n                centr.append(cen)\r\n                areas.append(area)\r\n        centr = np.asarray(centr)\r\n        areas = np.asarray(areas)\r\n        xs = weighted(centr[:, 0], ids, areas)\r\n        ys = weighted(centr[:, 1], ids, areas)\r\n        return np.array(list(zip(xs, ys)))\r\n\r\n    # ---- methods section ---------------------------------------------------\r\n    # ---- extents and extent shapes\r\n    #\r\n    def aoi_extent(self):\r\n        """"""The full geographic extent of the dataset `aoi` (area of interest).\r\n        """"""\r\n        return np.concatenate((np.nanmin(self.XY, axis=0),\r\n                               np.nanmax(self.XY, axis=0)))\r\n\r\n    def aoi_rectangle(self):\r\n        """"""The polygon bounds derived from the aoi_extent.\r\n        """"""\r\n        L, B, R, T = self.aoi_extent()\r\n        return np.array([[L, B], [L, T], [R, T], [R, B], [L, B]])\r\n\r\n    def extents(self, by_part=False):\r\n        """"""Extents are returned as L(eft), B(ottom), R(ight), T(op).\r\n        """"""\r\n        def _extent_(i):\r\n            """"""Extent of a sub-array in an object array""""""\r\n            return np.concatenate((np.nanmin(i, axis=0), np.nanmax(i, axis=0)))\r\n        # ----\r\n        if self.N == 1:\r\n            by_part = True\r\n        return np.asarray([_extent_(i) for i in self.split_by(by_part)])\r\n\r\n    def extent_centers(self, by_part=False):\r\n        """"""Return extent centers""""""\r\n        ext = self.extents(by_part)\r\n        xs = (ext[:, 0] + ext[:, 2])/2.\r\n        ys = (ext[:, 1] + ext[:, 3])/2.\r\n        return np.asarray(list(zip(xs, ys)))\r\n\r\n    def extent_rectangles(self, by_part=False):\r\n        """"""Return extent polygons for for the whole shape of the shape by part.\r\n        Points are ordered clockwise from the bottom left, with the first and\r\n        last points the same.  Requires an Advanced license in Pro for\r\n        equivalent functionality.\r\n\r\n        See Also\r\n        --------\r\n        ``aoi_extent`` and ``aoi_rectangles``\r\n        """"""\r\n        ext_polys = []\r\n        for ext in self.extents(by_part):\r\n            L, B, R, T = ext\r\n            poly = np.array([[L, B], [L, T], [R, T], [R, B], [L, B]])\r\n            ext_polys.append(poly)\r\n        return np.asarray(ext_polys)\r\n\r\n    # ---- maxs, mins, means for all features\r\n    #\r\n    def maxs(self, by_part=False):\r\n        """"""Maximums per feature or part.""""""\r\n        if len(self.part_cnt) == 1:\r\n            return np.asarray(np.nanmax(self, axis=0))\r\n        return np.asarray([np.nanmax(i, axis=0)\r\n                           for i in self.split_by(by_part)])\r\n\r\n    def mins(self, by_part=False):\r\n        """"""Minimums per feature or part.""""""\r\n        if len(self.part_cnt) == 1:\r\n            return np.asarray(np.nanmin(self, axis=0))\r\n        return np.asarray([np.nanmin(i, axis=0)\r\n                           for i in self.split_by(by_part)])\r\n\r\n    def means(self, by_part=False, remove_dups=True):\r\n        """"""Mean per feature or part, optionally keep duplicates.""""""\r\n        if len(self.part_cnt) == 1:\r\n            chunks = [self]\r\n        else:\r\n            chunks = self.split_by(by_part)\r\n        if remove_dups:\r\n            chunks = [np.unique(i, axis=0) for i in chunks]\r\n        return np.asarray([np.nanmean(i, axis=0) for i in chunks])\r\n\r\n    # ==== Some of these methods use functions from npg_geom ================\r\n    #\r\n    # ---- **is** section, condition/case checking, kept to a minimum\r\n    def is_clockwise(self, is_closed_polyline=False):\r\n        """"""Utilize `shoelace` area calculation to determine whether polygon\r\n        rings are clockwise or not.  If the geometry represent a closed-loop\r\n        polyline, then set the `is_closed_polyline` to True.  Validity of the\r\n        geometry is not checked.\r\n        """"""\r\n        def _area_part_(a):\r\n            """"""Mini e_area, used by areas and centroids""""""\r\n            x0, y1 = (a.T)[:, 1:]\r\n            x1, y0 = (a.T)[:, :-1]\r\n            e0 = np.einsum(\'...i,...i->...i\', x0, y0)\r\n            e1 = np.einsum(\'...i,...i->...i\', x1, y1)\r\n            return np.nansum((e0 - e1)*0.5)\r\n        # ----\r\n        msg = ""Polygons or closed-loop polylines are required.""\r\n        if self.K not in (1, 2):\r\n            print(msg)\r\n            return None\r\n        if self.K == 1:\r\n            if not is_closed_polyline:\r\n                print(msg)\r\n                return None\r\n        ids = self.bit_ids\r\n        cw = np.asarray([1 if _area_part_(i) > 0. else 0\r\n                         for i in self.bits])\r\n        return uts(np.asarray(list(zip(ids, cw))), names=[\'IDs\', \'Clockwise\'])\r\n\r\n    @property\r\n    def is_convex(self):\r\n        """"""Return True for convex, False for concave.  Holes are excluded\r\n        The first part of multipart shapes are used. Duplicate start-end\r\n        points removed prior to cross product\r\n        """"""\r\n        def _x_(a):\r\n            """"""Cross product.  Concatenate version is slightly faster""""""\r\n            ba = a - np.roll(a, 1, 0)   # vector 1\r\n            bc = a - np.roll(a, -1, 0)  # vector 2\r\n            # ba = a - np.concatenate((a[-1][None, :], a[:-1]), axis=0)\r\n            # bc = a - np.concatenate((a[1:], a[0][None, :]), axis=0)\r\n            return np.cross(ba, bc)\r\n        # ----\r\n        if self.K != 2:\r\n            print(""Polygons are required."")\r\n            return None\r\n        f_bits = self.first_bit(False)\r\n        check = [_x_(p[:-1]) for p in f_bits]  # cross-product\r\n        return np.array([np.all(np.sign(i) >= 0) for i in check])\r\n\r\n    def is_multipart(self, as_structured=False):\r\n        """"""For each shape, returns whether it has multiple parts.  An ndarray\r\n        is returned with the first column being the shape number and the second\r\n        is coded as 1 for True and 0 for False.\r\n        """"""\r\n        partcnt = self.part_cnt\r\n        w = np.where(partcnt[:, 1] > 1, 1, 0)\r\n        ids = np.unique(self.IDs)\r\n        arr = np.array(list(zip(ids, w)))\r\n        if as_structured:\r\n            dt = np.dtype([(\'IDs\', \'<i4\'), (\'Parts\', \'<i4\')])\r\n            return uts(arr, dtype=dt)\r\n        return arr\r\n\r\n    # ---- Methods to determine angles, convexity and other properties that\r\n    #      enable you use methods by part or by whole.\r\n    #\r\n    def polyline_angles(self, fromNorth=False):\r\n        """"""Polyline/segment angles.""""""\r\n        s = self.polys_to_segments()\r\n        dxy = s[:, -2:] - s[:, :2]\r\n        ang = np.degrees(np.arctan2(dxy[:, 1], dxy[:, 0]))\r\n        if fromNorth:\r\n            ang = np.mod((450.0 - ang), 360.)\r\n        return ang\r\n\r\n    def polygon_angles(self, inside=True, in_deg=True):\r\n        """"""Sequential 3 point angles from a poly* shape.  The outer ring for\r\n        each part is used.  see ``_angles_`` and ``first_bit``.\r\n        """"""\r\n        f_bits = self.first_bit(False)\r\n        return [geom._angles_(p, inside, in_deg) for p in f_bits]\r\n\r\n    # ---- return altered geometry\r\n    #\r\n    def moveto(self, x=0, y=0):\r\n        """"""Shift the dataset so that the origin is the lower-left corner.\r\n        see also ``shift`` and ``translate``.\r\n        """"""\r\n        dx, dy = x, y\r\n        if dx == 0 and dy == 0:\r\n            dx, dy = np.nanmin(self.XY, axis=0)\r\n        return Geo(self.XY + [-dx, -dy], self.IFT)\r\n\r\n    def shift(self, dx=0, dy=0):\r\n        """"""see ``translate``""""""\r\n        return Geo(self.XY + [dx, dy], self.IFT)\r\n\r\n    def translate(self, dx=0, dy=0):\r\n        """"""Move/shift/translate by dx, dy to a new location.""""""\r\n        return Geo(self.XY + [dx, dy], self.IFT)\r\n\r\n    def rotate(self, as_group=True, angle=0.0, clockwise=False):\r\n        """"""Rotate shapes about the group center or individually.\r\n\r\n        Rotation is done by npg_geom._rotate_ and a new geo array is returned.\r\n        """"""\r\n        if clockwise:\r\n            angle = -angle\r\n        angle = np.radians(angle)\r\n        c, s = np.cos(angle), np.sin(angle)\r\n        R = np.array(((c, s), (-s, c)))\r\n        out = geom._rotate_(self, R, as_group, clockwise)\r\n        info = ""{} rotated"".format(self.Info)\r\n        out = np.vstack(out)\r\n        return Geo(out, self.IFT, self.K, info)\r\n\r\n    # ---- changes to geometry, derived from geometry, convex_hulls, minimum\r\n    #    area bounding rectangle  **see also** extent properties above\r\n    #\r\n    def bounding_circles(self, angle=5, return_xyr=False):\r\n        """"""Bounding circles for features.\r\n\r\n        Parameters\r\n        ----------\r\n        angle : number\r\n            Angles to form n-gon.  A value of 10 will yield 36 point circle.\r\n        return_xyr : boolean {optional}\r\n            Return circle center and radius.\r\n\r\n        Returns\r\n        -------\r\n        Circle points and optionally, the circle center and radius.\r\n        """"""\r\n        shps = [s[~np.isnan(s[:, 0])] for s in self.shapes]\r\n        xyr = [sc.small_circ(s) for s in shps]\r\n        circs = []\r\n        for vals in xyr:\r\n            x, y, r = vals\r\n            circs.append(sc.circle_mini(r, angle, x, y))\r\n        if return_xyr:\r\n            return xyr, circs\r\n        return circs\r\n\r\n    def convex_hulls(self, by_part=False, threshold=50):\r\n        """"""Convex hull for shapes.  Calls ``_ch_`` to control method used.\r\n\r\n        Parameters\r\n        ----------\r\n        by_part : boolean\r\n            False for whole shape.  True for shape parts if present.\r\n        threshold : integer\r\n            Points... less than threshold uses simple CH method, greater than,\r\n            uses scipy.\r\n        """"""\r\n        # ----\r\n        shps = self.parts if by_part else self.shapes\r\n        # ---- run convex hull, _ch_, on point groups\r\n        ch_out = [geom._ch_(s, threshold) for s in shps]\r\n        for i, c in enumerate(ch_out):  # check for closed\r\n            if np.all(c[0] != c[-1]):\r\n                ch_out[i] = np.vstack((c, c[0]))\r\n        return ch_out\r\n\r\n    def min_area_rect(self, as_structured=False):\r\n        """"""Determines the minimum area rectangle for a shape represented\r\n        by a list of points.  If the shape is a polygon, then only the outer\r\n        ring is used.  This is the MABR... minimum area bounding rectangle.\r\n       """"""\r\n        def _extent_area_(a):\r\n            """"""Area of an extent polygon.""""""\r\n            LBRT = np.concatenate((np.nanmin(a, axis=0), np.nanmax(a, axis=0)))\r\n            dx, dy = np.diff(LBRT.reshape(2, 2), axis=0).squeeze()\r\n            return dx * dy, LBRT\r\n\r\n        def _extents_(a):\r\n            """"""Extents are returned as L(eft), B(ottom), R(ight), T(op).""""""\r\n            def _sub_(i):\r\n                """"""Extent of a sub-array in an object array""""""\r\n                return np.concatenate((np.nanmin(i, axis=0),\r\n                                       np.nanmax(i, axis=0)))\r\n            p_ext = [_sub_(i) for i in a]\r\n            return np.asarray(p_ext)\r\n        # ----\r\n        chs = self.convex_hulls(False, 50)\r\n        ang_ = [geom._angles_(i) for i in chs]\r\n        xt = _extents_(chs)\r\n        cent_ = np.c_[np.mean(xt[:, 0::2], axis=1),\r\n                      np.mean(xt[:, 1::2], axis=1)]\r\n        rects = []\r\n        for i, p in enumerate(chs):\r\n            # ---- np.radians(np.unique(np.round(ang_[i], 2))) # --- round\r\n            uni_ = np.radians(np.unique(ang_[i]))\r\n            area_old, LBRT = _extent_area_(p)\r\n            for angle in uni_:\r\n                c, s = np.cos(angle), np.sin(angle)\r\n                R = np.array(((c, s), (-s, c)))\r\n                ch = np.einsum(\'ij,jk->ik\', p - cent_[i], R) + cent_[i]\r\n                area_, LBRT = _extent_area_(ch)\r\n                Xmin, Ymin, Xmax, Ymax = LBRT\r\n                vals = [area_, Xmin, Ymin, Xmax, Ymax]\r\n                if area_ < area_old:\r\n                    area_old = area_\r\n                    Xmin, Ymin, Xmax, Ymax = LBRT\r\n                    vals = [area_, Xmin, Ymin, Xmax, Ymax]   # min_area,\r\n            rects.append(vals)\r\n        rects = np.asarray(rects)\r\n        if as_structured:\r\n            dt = np.dtype([(\'Rect_area\', \'<f8\'), (\'Xmin\', \'<f8\'),\r\n                           (\'Ymin\', \'<f8\'), (\'Xmax\', \'<f8\'), (\'Ymax\', \'<f8\')])\r\n            return uts(rects, dtype=dt)\r\n        return rects\r\n\r\n    def triangulate(self, as_polygon=True):\r\n        """"""Delaunay triangulation for point groupings.""""""\r\n        out = [geom._tri_pnts_(s) for s in self.shapes]\r\n        kind = 2 if as_polygon else 1\r\n        return Update_Geo(out, K=kind)\r\n\r\n    #\r\n    # ---- conversions -------------------------------------------------------\r\n    #\r\n    def fill_holes(self):\r\n        """"""Fill holes in polygon shapes.  Returns a Geo class.""""""\r\n        a_2d = []\r\n        id_too = []\r\n        if self.K < 2:\r\n            print(""Polygon geometry required."")\r\n            return None\r\n        for i, p in enumerate(self.parts):\r\n            nan_check = np.isnan(p[:, 0])  # check the Xs for nan\r\n            if np.any(nan_check):          # split at first nan\r\n                w = np.where(np.isnan(p[:, 0]))[0]\r\n                p = np.split(p, w)[0]      # keep the outer ring\r\n            a_2d.append(np.array(p))\r\n            id_too.append([i, len(p)])\r\n        a_2d = np.vstack(a_2d)\r\n        id_too = np.asarray(id_too)\r\n        info = ""{} fill_holes"".format(self.Info)\r\n        return Update_Geo(a_2d, 2, id_too, info)  # run update\r\n\r\n    def holes_to_shape(self):\r\n        """"""Return holes in polygon shapes.  Returns a Geo class or None.""""""\r\n        a_2d = []\r\n        id_too = []\r\n        if self.K < 2:\r\n            print(""Polygon geometry required."")\r\n            return None\r\n        for i, p in enumerate(self.parts):  # work with the parts\r\n            nan_check = np.isnan(p[:, 0])   # check the Xs for nan\r\n            if np.any(nan_check):           # split at first nan\r\n                w = np.where(np.isnan(p[:, 0]))[0]\r\n                p_new = np.split(p, w)[1]     # keep the outer ring\r\n                p_new = p_new[1:][::-1]\r\n                a_2d.append(np.array(p_new))\r\n                id_too.append([i, len(p_new)])\r\n        if not a_2d:  # ---- if empty\r\n            return None\r\n        id_too = np.asarray(id_too)\r\n        return Update_Geo(a_2d, 2, id_too)  # run update\r\n\r\n    def multipart_to_singlepart(self, info=""""):\r\n        """"""Convert multipart shapes to singleparts and return a new Geo array.\r\n        """"""\r\n        ift = self.IFT\r\n        data = np.concatenate(self.parts, axis=0)  # np.vstack(self.parts)\r\n        ift[:, 0] = np.arange(len(self.parts))\r\n        return Geo(data, IFT=ift, Kind=self.K, Info=info)\r\n\r\n    def od_pairs(self):\r\n        """"""Construct origin-destination pairs for traversing around the\r\n        perimeter of polygons, along polylines or between point sequences.\r\n\r\n        Returns\r\n        -------\r\n        An object array of origin-destination pairs is returned.\r\n\r\n        See Also\r\n        --------\r\n        polys_to_segments\r\n        """"""\r\n        return np.asarray([np.c_[p[:-1], p[1:]] for p in self.bits])\r\n\r\n    def polylines_to_polygons(self):\r\n        """"""Return a polygon Geo type from a polyline Geo.  It is assumed that\r\n        the polylines form closed-loops, otherwise use ``close_polylines``.\r\n        """"""\r\n        if self.K == 2:\r\n            print(""Already classed as a polygon."")\r\n            return self\r\n        polygons = self.copy()\r\n        polygons.K = 2\r\n        return polygons\r\n\r\n    def polygons_to_polylines(self):\r\n        """"""Return a polyline Geo type from a polygon Geo.\r\n        """"""\r\n        if self.K == 1:\r\n            print(""Already classed as a polyline."")\r\n            return self\r\n        polylines = self.copy()\r\n        polylines.K = 1\r\n        return polylines\r\n\r\n    def polys_to_points(self, keep_order=True, as_structured=False):\r\n        """"""Convert all feature vertices to an ndarray of unique points.\r\n        NaN\'s are removed.  Optionally, retain point order.""""""\r\n        if as_structured:\r\n            return geom._polys_to_unique_pnts_(self, as_structured=True)\r\n        a = self[~np.isnan(self.X)]\r\n        uni, idx = np.unique(a, True, axis=0)\r\n        if keep_order:\r\n            uni = a[np.sort(idx)]\r\n        return np.asarray(uni[~np.isnan(uni[:, 0])])\r\n\r\n    def close_polylines(self, out_kind=1):\r\n        """"""Attempt to produce closed-loop polylines (1) or polygons (2)\r\n        from polylines.  Multipart features are converted to single part.\r\n        """"""\r\n        polys = []\r\n        for s in self.bits:  # shape as bits\r\n            if len(s) > 2:\r\n                if np.all(s[0] == s[-1]):\r\n                    polys.append(s)\r\n                else:\r\n                    polys.append(np.concatenate((s, s[..., :1, :]), axis=0))\r\n        return Update_Geo(polys, K=out_kind)\r\n\r\n    def densify_by_distance(self, spacing=1):\r\n        """"""Densify poly features by a specified distance.  Converts multipart\r\n        to singlepart features during the process.\r\n        Calls ``_pnts_on_line_`` for Geo bits.\r\n        """"""\r\n        polys = [geom._pnts_on_line_(a, spacing) for a in self.bits]\r\n        return Update_Geo(polys, K=self.K)\r\n\r\n    def densify_by_percent(self, percent=50):\r\n        """"""Densify poly features by a percentage for each segment.  Converts\r\n        multipart to singlepart features during the process.\r\n        Calls ``_percent_along``\r\n        """"""\r\n        polys = [geom._pnts_on_line_(a, spacing=percent, is_percent=True)\r\n                 for a in self.bits]\r\n        return Update_Geo(polys, K=self.K)\r\n\r\n    def pnt_on_poly(self, by_dist=True, val=1):\r\n        """"""Point on polyline/polygon by distance or percent. Emulates\r\n        `arcpy Polyline class, positionAlongLine (value, {use_percentage})\r\n        <https://pro.arcgis.com/en/pro-app/arcpy/classes/polyline.htm>`_.\r\n        """"""\r\n        if by_dist:\r\n            r = [geom._dist_along_(a, dist=val) for a in self.bits]\r\n            return Update_Geo(r, K=0)\r\n        r = [geom._percent_along_(a, percent=val) for a in self.bits]\r\n        return Update_Geo(r, K=0)\r\n\r\n    # ---- segments for poly* boundaries\r\n    #\r\n    def polys_to_segments(self, as_basic=True, as_3d=False):\r\n        """"""Segment poly* structures into o-d pairs from start to finish.\r\n\r\n        Parameters\r\n        ----------\r\n        as_basic : boolean\r\n            True returns the basic od pairs as an Nx5 array in the form\r\n            [X_orig\', Y_orig\', \'X_orig\', \'Y_orig\'] as an ndarray.\r\n            If False, the content is returned as a structured array with the\r\n            same content and ids and length.\r\n        as_3d : boolean\r\n            True, the point pairs are returned as a 3D array in the form\r\n            [[X_orig\', Y_orig\'], [\'X_orig\', \'Y_orig\']], without the distances.\r\n\r\n        Notes\r\n        -----\r\n        Any row containing np.nan is removed since this would indicate that the\r\n        shape contains the null_pnt separator.\r\n        Use ``prn_tbl`` if you want to see a well formatted output.\r\n        """"""\r\n        if self.K not in (1, 2):\r\n            print(""Poly* features required."")\r\n            return None\r\n        b_vals = self.bits\r\n#        fr_to = np.vstack([np.hstack((b[:-1], b[1:])) for b in b_vals])\r\n        fr_to = np.concatenate([np.concatenate((b[:-1], b[1:]), axis=1)\r\n                                for b in b_vals], axis=0)\r\n        #\r\n        # ---- shortcut to 3d from-to representation\r\n        if as_3d:\r\n            fr_to = fr_to[:, :4]\r\n            s0, s1 = fr_to.shape\r\n            return fr_to.reshape(s0, s1//2, s1//2)\r\n        #\r\n        # ---- basic return as ndarray used by common_segments\r\n        if as_basic:\r\n            return fr_to\r\n        #\r\n        # ----structured array section\r\n        # add bit ids and lengths to the output array\r\n        b_ids = self.bit_ids\r\n        segs = np.asarray([[b_ids[i], len(b) - 1]\r\n                           for i, b in enumerate(b_vals)])\r\n        s_ids = np.concatenate([np.repeat(i[0], i[1]) for i in segs])\r\n        dist = (np.sqrt(np.sum((fr_to[:, :2] - fr_to[:, 2:4])**2, axis=1)))\r\n        fr_to = np.hstack((fr_to, s_ids.reshape(-1, 1), dist.reshape(-1, 1)))\r\n        dt = np.dtype([(\'X_orig\', \'f8\'), (\'Y_orig\', \'f8\'),\r\n                       (\'X_dest\', \'f8\'), (\'Y_dest\', \'f8\'),\r\n                       (\'Orig_id\', \'i4\'), (\'Length\', \'f8\')])\r\n        fr_to = uts(fr_to, dtype=dt)\r\n        return repack_fields(fr_to)\r\n\r\n    def common_segments(self):\r\n        """"""Return the common segments in poly features.  Result is an array of\r\n        from-to pairs of points.  ft, tf pairs are evaluated to denote common\r\n        and duplicates.\r\n        """"""\r\n        h = self.polys_to_segments(as_basic=True)  # returns Nx5 array\r\n        if h is None:\r\n            return None\r\n        h_0 = uts(h)\r\n        names = h_0.dtype.names\r\n        h_1 = h_0[list(names[2:4] + names[:2])]  # x_to, y_to and x_fr, y_fr\r\n        idx = np.isin(h_0, h_1)\r\n        common = h_0[idx]\r\n        return stu(common)\r\n\r\n    def unique_segments(self):\r\n        """"""Return the unique segments in poly features as an array of\r\n        from-to pairs of points.\r\n        """"""\r\n        h = self.polys_to_segments()\r\n        if h is None:\r\n            return None\r\n        h_0 = uts(h)\r\n        names = h_0.dtype.names\r\n        h_1 = h_0[list(names[-2:] + names[:2])]\r\n        idx0 = ~np.isin(h_0, h_1)\r\n        uniq01 = np.concatenate((h_0[idx0], h_0[~idx0]), axis=0)\r\n        return stu(uniq01)\r\n\r\n    # ---- sort section\r\n    # Sorting the fc shape-related fields needs an advanced arcgis pro license.\r\n    # The following applies to the various sort options.\r\n    #\r\n    # Parameters\r\n    # ----------\r\n    # change_indices : boolean\r\n    #     True, returns a structured array of old and new IDs representing\r\n    #     the change in order based on the area sort option.  The actual\r\n    #     array is not returned.\r\n    def change_indices(self, new):\r\n        """"""Return the old and new indices derived from the application of a\r\n        function\r\n        """"""\r\n        if len(self.IDs) != len(new):\r\n            print(""Old and new ID lengths must be the same"")\r\n            return None\r\n        dt = np.dtype([(\'Orig_ID\', \'<i4\'), (\'New_ID\', \'<i4\')])\r\n        out = np.asarray(list(zip(self.IDs, new)), dtype=dt)\r\n        return out\r\n\r\n    def point_indices(self, as_structured=False):\r\n        """"""Returns the point ids and the feature that they belong to.\r\n        """"""\r\n        ids = np.arange(self.shape[0])\r\n        p_ids = [np.repeat(i[0], i[2] - i[1]) for i in self.IFT]\r\n        p_ids = np.concatenate(p_ids)\r\n        if as_structured:\r\n            dt = np.dtype([(\'Pnt_ID\', \'<i4\'), (\'Feature_ID\', \'<i4\')])\r\n            z = np.zeros((self.shape[0],), dtype=dt)\r\n            z[\'Pnt_ID\'] = ids\r\n            z[\'Feature_ID\'] = p_ids\r\n        else:\r\n            z = np.zeros((self.shape), dtype=np.int32)\r\n            z[:, 0] = ids\r\n            z[:, 1] = p_ids\r\n        return z\r\n\r\n    def sort_by_area(self, ascending=True, just_indices=False):\r\n        """"""Sort the geometry by ascending or descending order by shape area.\r\n\r\n        Parameters\r\n        ----------\r\n        ascending : boolean\r\n            True, in ascending order of shape area. False, in descending order.\r\n        just_indices : boolean\r\n            True, returns a structured array of old and new IDs representing\r\n            the change in order based on the area sort option.  The actual\r\n            array is not returned.\r\n        """"""\r\n        vals = self.areas(True)                 # shape area\r\n        idx = np.argsort(vals)\r\n        sorted_ids = self.shp_ids[idx]           # use shape IDs not part IDs\r\n        if not ascending:\r\n            sorted_ids = sorted_ids[::-1]\r\n        if just_indices:\r\n            return self.change_indices(sorted_ids)\r\n        sorted_array = self.pull(sorted_ids)\r\n        return sorted_array\r\n\r\n    def sort_by_length(self, ascending=True, just_indices=False):\r\n        """"""Sort the geometry by ascending or descending order""""""\r\n        areas = self.lengths\r\n        idx = np.argsort(areas)\r\n        sorted_ids = self.IDs[idx]\r\n        if not ascending:\r\n            sorted_ids = sorted_ids[::-1]\r\n        if just_indices:\r\n            return self.change_indices(sorted_ids)\r\n        sorted_array = self.pull(sorted_ids)\r\n        return sorted_array\r\n\r\n    def sort_by_extent(self, key=0, just_indices=False):\r\n        """"""Sort the feature geometry based on the conditions outlined in the\r\n        parameters.  The feature centers are used to determine sort order.\r\n\r\n        Parameters\r\n        ----------\r\n        sort_type : int\r\n\r\n        **Sorting... key - direction vector - azimuth**\r\n\r\n        +------+---------+-------+\r\n        | key  + vector  +azimuth|\r\n        +======+=========+=======+\r\n        |  0   | S to N  |    0  |\r\n        +------+---------+-------+\r\n        |  1   | SW - NE |   45  |\r\n        +------+---------+-------+\r\n        |  2   | W to E  |   90  |\r\n        +------+---------+-------+\r\n        |  3   | NW - SE |  135  |\r\n        +------+---------+-------+\r\n        |  4   | N to S  |  180  |\r\n        +------+---------+-------+\r\n        |  5   | NE - SW |  225  |\r\n        +------+---------+-------+\r\n        |  6   | E to W  |  270  |\r\n        +------+---------+-------+\r\n        |  7   | SE - NW |  315  |\r\n        +------+---------+-------+\r\n\r\n        Notes\r\n        -----\r\n        The key values are used to select the dominant direction vector.\r\n\r\n        >>> z = sin(a) * [X] + cos(a) * [Y]  # - sort by vector\r\n\r\n        `vector sort\r\n        <https://gis.stackexchange.com/questions/60134/sort-a-feature-table-by\r\n        -geographic-location>`_.\r\n        """"""\r\n        if key not in range(0, 8):\r\n            print(""Integer value between 0 and 7 inclusive required."")\r\n            return None\r\n        azim = np.array([0, 45, 90, 135, 180, 225, 270, 315])  # azimuths\r\n        val = np.radians(azim[key])              # sort angle in radians\r\n        ext = self.extent_centers()              # get the extent centers\r\n        ext_ids = self.shp_ids\r\n        xs = ext[:, 0]\r\n        ys = ext[:, 1]\r\n        z = np.sin(val) * xs + np.cos(val) * ys  # - sort by vector\r\n        idx = np.argsort(z)  # sort order\r\n        sorted_ids = ext_ids[idx]\r\n        if just_indices:\r\n            return self.change_indices(sorted_ids)\r\n        sorted_array = self.pull(sorted_ids)\r\n        return sorted_array\r\n\r\n    def sort_coords(self, x_ascending=True, y_ascending=True):\r\n        """"""Sort points by coordinates.\r\n\r\n        Parameters\r\n        ----------\r\n        x_ascending, y_ascending : boolean\r\n            If False, sort is done in decending order by axis.\r\n\r\n        See Also\r\n        --------\r\n        sort_by_extent :\r\n            For polygons or polylines.\r\n        """"""\r\n        if x_ascending:\r\n            if y_ascending:\r\n                return self[np.lexsort((self[:, 0], self[:, 1]))]\r\n            return self[np.lexsort((self[:, 0], -self[:, 1]))]\r\n        if y_ascending:\r\n            if x_ascending:\r\n                return self[np.lexsort((self[:, 1], self[:, 0]))]\r\n            return self[np.lexsort((self[:, 1], -self[:, 0]))]\r\n\r\n    # ---- info section\r\n    #\r\n    def info(self, prn=True, start=0, end=50):\r\n        """"""Convert an IFT array to full information.\r\n\r\n        Parameters\r\n        ----------\r\n        prn : boolean\r\n            If True, the top and bottom ``rows`` will be printed.\r\n            If False, the information will be returned and one can use\r\n            ``prn_tbl`` for more control over the tabular output.\r\n        start, end : integers\r\n            The start to end locations within the geo-array to print or view.\r\n\r\n        Notes\r\n        -----\r\n        Point count will include any null_pnts used to separate inner and\r\n        outer rings.\r\n\r\n        To see the data structure, use ``prn_geo``.\r\n        """"""\r\n        ift = self.IFT\r\n        ids = ift[:, 0]\r\n        frum = ift[:, 1]\r\n        too = ift[:, 2]\r\n        uni, cnts = np.unique(ids, return_counts=True)\r\n        part_count = np.concatenate([np.arange(i) for i in cnts])\r\n        pnts = np.array([len(p) for p in self.parts])\r\n        id_len2 = np.stack((ids, part_count, pnts, frum, too), axis=-1)\r\n        dt = np.dtype({\r\n            \'names\': [\'IDs\', \'Part\', \'Points\', \'From_pnt\', \'To_pnt\'],\r\n            \'formats\': [\'i4\', \'i4\', \'i4\', \'i4\', \'i4\']})\r\n        IFT_2 = uts(id_len2, dtype=dt)\r\n        frmt = ""-""*14 + \\\r\n            ""\\nShapes :{:>6.0f}\\nParts  :{:>6.0f}"" + \\\r\n            ""\\nPoints :{:>6.0f}\\n  min  :{:>6.0f}\\n  med  :{:>6.0f}"" + \\\r\n            ""\\n  max  :{:>6.0f}""\r\n        shps = len(uni)  # ---- zero-indexed, hence add 1\r\n        _, cnts = np.unique(IFT_2[\'Part\'], return_counts=True)\r\n        p0 = np.sum(cnts)\r\n        p3 = np.sum(IFT_2[\'Points\'])\r\n        p4 = np.min(IFT_2[\'Points\'])\r\n        p5 = np.median(IFT_2[\'Points\'])\r\n        p6 = np.max(IFT_2[\'Points\'])\r\n        msg = dedent(frmt).format(shps, p0, p3, p4, p5, p6)\r\n        if prn:\r\n            frmt = ""{:>8} ""*5\r\n            start, end = sorted([abs(int(i)) if isinstance(i, (int, float))\r\n                                 else 0\r\n                                 for i in [start, end]])\r\n            print(msg)\r\n            print(frmt.format(*IFT_2.dtype.names))\r\n            N = IFT_2.shape[0]\r\n            for i in range(min(N, end)):\r\n                print(frmt.format(*IFT_2[i]))\r\n            return None\r\n        return IFT_2\r\n\r\n    def point_info(self, by_part=True, with_null=False):\r\n        """"""Point count by feature or parts of feature.\r\n\r\n        Parameters\r\n        ----------\r\n        by part: boolean\r\n            True for each feature part or False for the whole feature.\r\n        with_null : boolean\r\n            True, to include nan/null points.\r\n        """"""\r\n        chunks = self.split_by(by_part)\r\n        if with_null:\r\n            return self.FT[:, 1] - self.FT[:, 0]\r\n        return np.array([len(i[~np.isnan(i[:, 0])]) for i in chunks])\r\n\r\n    #\r\n    # ----------------End of class definition-\r\n\r\n\r\n# ----------------------------------------------------------------------------\r\n# ---- (2) ... update Geo array, or create one from a list of arrays ... -----\r\n#\r\ndef Update_Geo(a_2d, K=None, id_too=None, Info=None):\r\n    """"""Create a new Geo array from a list of arrays.\r\n\r\n    Parameters\r\n    ----------\r\n    a_2d : list/tuple/array\r\n        Some form of nested 2D array-like structure that can be stacked.\r\n    K : integer\r\n        Points (0), polylines (1) or polygons (2).\r\n    id_too : array-like\r\n        If None, then the structure will be created.\r\n    Info : text (optional)\r\n        Provide any information that will help in identifying the array.\r\n\r\n    Returns\r\n    -------\r\n    A new Geo array is returned given the inputs.\r\n    """"""\r\n    if not isinstance(a_2d, (np.ndarray, list, tuple)):\r\n        return None\r\n    if K not in (0, 1, 2):\r\n        print(""Output type not specified, or not in (0, 1, 2)."")\r\n        return None\r\n    if id_too is None:  # create IFT structure\r\n        if K == 0:\r\n            ids = np.arange(0, len(a_2d))\r\n            frum = ids\r\n            too = np.arange(1, len(a_2d) + 1)\r\n        else:\r\n            id_too = [(i, len(a)) for i, a in enumerate(a_2d)]\r\n            a_2d = np.concatenate(a_2d, axis=0)  # np.vstack(a_2d)\r\n            id_too = np.array(id_too)\r\n            ids = id_too[:, 0]\r\n            too = np.cumsum(id_too[:, 1])\r\n            frum = np.concatenate(([0], too))\r\n    else:\r\n        a_2d = np.concatenate(a_2d, axis=0)  # np.vstack(a_2d)\r\n        ids = id_too[:, 0]\r\n        too = np.cumsum(id_too[:, 1])\r\n        frum = np.concatenate(([0], too))\r\n    IFT = np.array(list(zip(ids, frum, too)))\r\n    return Geo(a_2d, IFT, K, Info)\r\n\r\n\r\n# ----------------------------------------------------------------------------\r\n# ---- (3) dirr ... code section ... -----------------------------------------\r\n#\r\ndef dirr(obj, colwise=False, cols=3, prn=True):\r\n    """"""\r\n    A `dir` variant for a variety of objects.  The output is sorted by columns\r\n    or rows with `cols` of output to suite screen width.\r\n    """"""\r\n    from itertools import zip_longest as zl\r\n    if isinstance(obj, np.ndarray):\r\n        a = [\'... Geo class ...\']\r\n        a.extend(sorted(list(set(dir(obj)).difference(set(dir(np.ndarray))))))\r\n        a.extend([\'... np_geom ...\'] + sorted(geom.__all__))\r\n    else:\r\n        a = dir(obj)\r\n    w = max([len(i) for i in a])\r\n    frmt = ((""{{!s:<{}}} "".format(w)))*cols\r\n    csze = len(a) / cols  # split it\r\n    csze = int(csze) + (csze % 1 > 0)\r\n    if colwise:\r\n        a_0 = [a[i: i+csze] for i in range(0, len(a), csze)]\r\n        a_0 = list(zl(*a_0, fillvalue=""""))\r\n    else:\r\n        a_0 = [a[i: i+cols] for i in range(0, len(a), cols)]\r\n    if hasattr(obj, \'__module__\'):\r\n        args = [""-""*70, obj.__module__, obj.__class__]\r\n    else:\r\n        args = [""-""*70, type(obj), ""py version""]\r\n    txt_out = ""\\n{}\\n| dir({}) ...\\n|    {}\\n-------"".format(*args)\r\n    cnt = 0\r\n    for i in a_0:\r\n        cnt += 1\r\n        txt = ""\\n  ({:>03.0f})  "".format(cnt)\r\n        frmt = ((""{{!s:<{}}} "".format(w)))*len(i)\r\n        txt += frmt.format(*i)\r\n        txt_out += txt\r\n    if prn:\r\n        print(txt_out)\r\n        return None\r\n    return txt_out\r\n\r\n\r\ndef geo_info(g):\r\n    """"""print the difference between Geo methods and properties and those of the\r\n    ndarray.\r\n    """"""\r\n    from textwrap import indent, wrap\r\n    arr_set = set(dir(g.base))\r\n    geo_set = set(dir(g))\r\n    srt = sorted(list(geo_set.difference(arr_set)))\r\n    t = "", "".join([i for i in srt])\r\n    w = wrap(t, 70)\r\n    print("">>> geo_info(g)\\n... Geo methods and properties"")\r\n    for i in w:\r\n        print(indent(""{}"".format(i), prefix=""    ""))\r\n    print(""\\n>>> Geo.__dict_keys()"")\r\n    t = "", "".join(list(Geo.__dict__.keys()))\r\n    w = wrap(t, 70)\r\n    for i in w:\r\n        print(indent(""{}"".format(i), prefix=""    ""))\r\n\r\n\r\n""""""\r\nGeo class ...\r\n\r\nnpg.npGeo.__all__, npg.npg_io.__all__, npg.npg_geom.__all__,\r\nnpg.npg_analysis.__all__, npg.npg_table.__all__\r\n\r\n""""""\r\n# ===========================================================================\r\n#\r\nif __name__ == ""__main__"":\r\n    """"""optional location for parameters""""""\r\n    # print(""\\n...npGeo imported"")\r\n    # diff = sorted(list(set(dir(Geo)).difference(set(dir(np.ndarray)))))\r\n    """"""\r\n    [\'__dict__\', \'__module__\', \'aoi_extent\', \'aoi_rectangle\', \'areas\',\r\n     \'bit_ids\', \'bits\', \'bounding_circles\', \'centers\', \'centroids\',\r\n     \'close_polylines\', \'common_segments\', \'convex_hulls\',\r\n     \'densify_by_distance\', \'densify_by_percent\', \'extent_rectangles\',\r\n     \'extents\', \'fill_holes\', \'get\', \'holes_to_shape\', \'info\', \'is_clockwise\',\r\n     \'is_convex\', \'is_multipart\', \'lengths\', \'maxs\', \'means\', \'min_area_rect\',\r\n     \'mins\', \'moveto_origin\', \'multipart_to_singlepart\', \'od_pairs\',\r\n     \'outer_rings\', \'part_cnt\', \'parts\', \'pnt_cnt\', \'pnt_on_poly\',\r\n     \'point_info\', \'polygon_angles\', \'polygons_to_polylines\',\r\n     \'polyline_angles\', \'polylines_to_polygons\', \'polys_to_points\',\r\n     \'polys_to_segments\', \'pull\', \'rotate\', \'shapes\', \'shift\', \'split_by\',\r\n     \'translate\', \'triangulate\', \'unique_segments\']\r\n    """"""\r\n'"
Free_Tools/npg_analysis.py,69,"b'# -*- coding: utf-8 -*-\r\n""""""\r\n============\r\nnpg_analysis\r\n============\r\n\r\nScript :\r\n    npg_analysis.py\r\n\r\nAuthor :\r\n    Dan_Patterson@carleton.ca\r\n\r\nModified :\r\n    2019-09-07\r\n\r\nPurpose :\r\n    Analysis tools for the Geom class.\r\n\r\nNotes:\r\n\r\nReferences\r\n----------\r\nDerived from arraytools ``convex_hull, mst, near, n_spaced``\r\n\r\n""""""\r\n# pylint: disable=C0103  # invalid-name\r\n# pylint: disable=R0914  # Too many local variables\r\n# pylint: disable=R1710  # inconsistent-return-statements\r\n# pylint: disable=W0105  # string statement has no effect\r\n\r\n# import sys\r\nfrom textwrap import dedent\r\nimport numpy as np\r\nfrom numpy.lib.recfunctions import unstructured_to_structured as uts\r\nfrom numpy.lib.recfunctions import repack_fields\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=5, linewidth=120, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\n__all__ = [\r\n        \'closest_n\', \'distances\', \'not_closer\', \'n_check\', \'n_near\',\r\n        \'n_spaced\', \'intersects\', \'intersection_pnt\', \'knn\', \'knn0\',\r\n        \'_dist_arr_\', \'_e_dist_\', \'mst\', \'connect\', \'concave\'\r\n        ]\r\n\r\n\r\n# ===========================================================================\r\n# ---- def section: def code blocks go here ---------------------------------\r\ndef closest_n(a, N=3, ordered=True):\r\n    """"""A shell to `n_near`, see its doc string""""""\r\n    coords, dist, n_array = n_near(a, N=N, ordered=ordered)\r\n    return coords, dist, n_array\r\n\r\n\r\ndef distances(a, b):\r\n    """"""Distances for 2D arrays using einsum.  Based on a simplified version\r\n    of e_dist in arraytools.\r\n    """"""\r\n    diff = a[:, None] - b\r\n    return np.sqrt(np.einsum(\'ijk,ijk->ij\', diff, diff))\r\n\r\n\r\ndef not_closer(a, min_d=1, ordered=False):\r\n    """"""Find the points that are separated by a distance greater than\r\n     min_d.  This ensures a degree of point spacing\r\n\r\n    Parameters\r\n    ----------\r\n     `a` : coordinates\r\n         2D array of coordinates.\r\n     `min_d` : number\r\n         Minimum separation distance.\r\n     `ordered` : boolean\r\n         Order the input points.\r\n\r\n    Returns\r\n    -------\r\n    - b : points where the spacing condition is met\r\n    - c : the boolean array indicating which of the input points were valid.\r\n    - d : the distance matrix\r\n\r\n    """"""\r\n    if ordered:\r\n        a = a[np.argsort(a[:, 0])]\r\n    b = a.reshape(np.prod(a.shape[:-1]), 1, a.shape[-1])\r\n    diff = b - a\r\n    d = np.einsum(\'ijk,ijk->ij\', diff, diff)\r\n    d = np.sqrt(d).squeeze()\r\n    c = ~np.triu(d <= min_d, 1).any(0)\r\n    b = a[c]\r\n    return b, c, d\r\n\r\n\r\ndef n_check(a):  # N=3, order=True):\r\n    """"""n_check prior to running n_near analysis\r\n\r\n    Parameters\r\n    ----------\r\n    Two 2D array of X,Y coordinates required.  Parse your data to comply.\r\n    """"""\r\n    has_err = False\r\n    if isinstance(a, (list, tuple, np.ndarray)):\r\n        if (hasattr(a[0], \'__len__\')) and (len(a[0]) == 2):\r\n            return True\r\n        has_err = True\r\n    else:\r\n        has_err = True\r\n    if has_err:\r\n        print(n_check.__doc__)\r\n        return False\r\n\r\n\r\ndef n_near(a, N=3, ordered=True):\r\n    """"""Return the coordinates and distance to the nearest N points within\r\n      an 2D numpy array, \'a\', with optional ordering of the inputs.\r\n\r\n    Parameters\r\n    ----------\r\n    `a` : array\r\n        An ndarray of uniform int or float dtype.  Extract the fields\r\n        representing the x,y coordinates before proceeding.\r\n    `N` : number\r\n         Number of closest points to return.\r\n\r\n    Returns\r\n    -------\r\n    A structured array is returned containing an ID number.  The ID number\r\n    is the ID of the points as they were read.  The array will contain\r\n    (C)losest fields and distance fields\r\n    (C0_X, C0_Y, C1_X, C1_Y, Dist0, Dist1 etc) representing coordinates\r\n    and distance to the required \'closest\' points.\r\n    """"""\r\n    if not (isinstance(a, (np.ndarray)) and (N > 1)):\r\n        print(""\\nInput error...read the docs\\n\\n{}"".format(n_near.__doc__))\r\n        return a\r\n    rows, _ = a.shape\r\n    dt_near = [(\'Xo\', \'<f8\'), (\'Yo\', \'<f8\')]\r\n    dt_new = [(\'C{}\'.format(i) + \'{}\'.format(j), \'<f8\')\r\n              for i in range(N)\r\n              for j in [\'_X\', \'_Y\']]\r\n    dt_near.extend(dt_new)\r\n    dt_dist = [(\'Dist{}\'.format(i), \'<f8\') for i in range(N)]\r\n    # dt = [(\'ID\', \'<i4\')]  + dt_near + dt_dist # python 2.7\r\n    dt = [(\'ID\', \'<i4\'), *dt_near, *dt_dist]\r\n    n_array = np.zeros((rows,), dtype=dt)\r\n    n_array[\'ID\'] = np.arange(rows)\r\n    # ---- distance matrix calculation using einsum ----\r\n    if ordered:\r\n        a = a[np.argsort(a[:, 0])]\r\n    b = a.reshape(np.prod(a.shape[:-1]), 1, a.shape[-1])\r\n    diff = b - a\r\n    dist = np.einsum(\'ijk,ijk->ij\', diff, diff)\r\n    d = np.sqrt(dist).squeeze()\r\n    # ---- format for use in structured array output ----\r\n    # steps are outlined as follows....\r\n    #\r\n    kv = np.argsort(d, axis=1)       # sort \'d\' on last axis to get keys\r\n    coords = a[kv]                   # pull out coordinates using the keys\r\n    s0, s1, s2 = coords.shape\r\n    coords = coords.reshape((s0, s1*s2))\r\n    dist = np.sort(d)[:, 1:]         # slice sorted distances, skip 1st\r\n    # ---- construct the structured array ----\r\n    dt_names = n_array.dtype.names\r\n    s0, s1, s2 = (1, (N+1)*2 + 1, len(dt_names))\r\n    for i in range(0, s1):           # coordinate field names\r\n        nm = dt_names[i+1]\r\n        n_array[nm] = coords[:, i]\r\n    dist_names = dt_names[s1:s2]\r\n    for i in range(N):               # fill n_array with the results\r\n        nm = dist_names[i]\r\n        n_array[nm] = dist[:, i]\r\n    return coords, dist, n_array\r\n\r\n\r\ndef n_spaced(L=0, B=0, R=10, T=10, min_space=1, num=10, verbose=True):\r\n    """"""Produce num points within the bounds specified by the extent (L,B,R,T)\r\n\r\n    Parameters\r\n    ----------\r\n    L(eft), B, R, T(op) : numbers\r\n        Extent coordinates.\r\n    min_space : number\r\n        Minimum spacing between points.\r\n    num : number\r\n        Number of points... this value may not be reached if the extent\r\n        is too small and the spacing is large relative to it.\r\n    """"""\r\n    #\r\n    def _pnts(L, B, R, T, num):\r\n        """"""Create the points""""""\r\n        xs = (R-L) * np.random.random_sample(size=num) + L\r\n        ys = (T-B) * np.random.random_sample(size=num) + B\r\n        return np.array(list(zip(xs, ys)))\r\n\r\n    def _not_closer(a, min_space=1):\r\n        """"""Find the points that are greater than min_space in the extent.""""""\r\n        b = a.reshape(np.prod(a.shape[:-1]), 1, a.shape[-1])\r\n        diff = b - a\r\n        dist = np.einsum(\'ijk,ijk->ij\', diff, diff)\r\n        dist_arr = np.sqrt(dist).squeeze()\r\n        case = ~np.triu(dist_arr <= min_space, 1).any(0)\r\n        return a[case]\r\n    #\r\n    cnt = 1\r\n    n = num * 2  # check double the number required as a check\r\n    result = 0\r\n    frmt = ""Examined: {}  Found: {}  Need: {}""\r\n    a0 = []\r\n    while (result < num) and (cnt < 6):  # keep using random points\r\n        a = _pnts(L, B, R, T, num)\r\n        if cnt > 1:\r\n            a = np.concatenate((a0, a), axis=0)  # np.vstack((a0, a))\r\n        a0 = _not_closer(a, min_space)\r\n        result = len(a0)\r\n        if verbose:\r\n            print(dedent(frmt).format(n, result, num))\r\n        cnt += 1\r\n        n += n\r\n    # perform the final sample and calculation\r\n    use = min(num, result)\r\n    a0 = a0[:use]  # could use a0 = np.random.shuffle(a0)[:num]\r\n    a0 = a0[np.argsort(a0[:, 0])]\r\n    return a0\r\n\r\n\r\n# ==== intersection\r\n#\r\ndef intersects(*args):\r\n    """"""Line intersection check.  Two lines or 4 points that form the lines.\r\n\r\n    Parameters\r\n    ----------\r\n      intersects(line0, line1) or intersects(p0, p1, p2, p3)\r\n        p0, p1 -> line 1\r\n        p2, p3 -> line 2\r\n\r\n    Returns:\r\n    --------\r\n    boolean, if the segments do intersect\r\n\r\n    References:\r\n    -----------\r\n    `<https://stackoverflow.com/questions/563198/how-do-you-detect-where-two-\r\n    line-segments-intersect#565282>`_.\r\n    """"""\r\n    if len(args) == 2:\r\n        p0, p1, p2, p3 = *args[0], *args[1]\r\n    elif len(args) == 4:\r\n        p0, p1, p2, p3 = args\r\n    else:\r\n        raise AttributeError(""Pass 2, 2-pnt lines or 4 points to the function"")\r\n    #\r\n    # ---- First check\r\n    # Given 4 points, if there are < 4 unique, then the segments intersect\r\n    u, cnts = np.unique((p0, p1, p2, p3), return_counts=True, axis=0)\r\n    if len(u) < 4:\r\n        intersection_pnt = u[cnts > 1]\r\n        return True, intersection_pnt\r\n\r\n    s10_x = p1[0] - p0[0]\r\n    s10_y = p1[1] - p0[1]\r\n    s32_x = p3[0] - p2[0]\r\n    s32_y = p3[1] - p2[1]\r\n    s02_x = p0[0] - p2[0]\r\n    s02_y = p0[1] - p2[1]\r\n    #\r\n    # ---- Second check ----   np.cross(p1-p0, p3-p2)\r\n    denom = (s10_x * s32_y - s32_x * s10_y).item()\r\n    if denom == 0.0:  # collinear\r\n        return False, None\r\n    #\r\n    # ---- Third check ----  np.cross(p1-p0, p0-p2)\r\n    positive_denom = denom > 0.0  # denominator greater than zero\r\n    s_numer = (s10_x * s02_y - s10_y * s02_x).item()\r\n#    if (s_numer < 0) == positive_denom:\r\n#        return False\r\n    #\r\n    # ---- Fourth check ----  np.cross(p3-p2, p0-p2)\r\n    t_numer = s32_x * s02_y - s32_y * s02_x\r\n#    if (t_numer < 0) == positive_denom:\r\n#        return False\r\n    #\r\n    if ((s_numer > denom) == positive_denom) or \\\r\n       ((t_numer > denom) == positive_denom):\r\n        return False, None\r\n    #\r\n    # ---- check to see if the intersection point is one of the input points\r\n    # substitute p0 in the equation  These are the intersection points\r\n    t = t_numer / denom\r\n    intersection_point = [p0[0] + (t * s10_x), p0[1] + (t * s10_y)]\r\n    return True, intersection_point\r\n\r\n\r\ndef intersection_pnt(p0, p1, p2, p3):\r\n    """"""Returns the intersection point of a polygon segment (p0->p1) and a\r\n    clipping polygon segment (s->e.\r\n\r\n    `<https://en.wikipedia.org/wiki/Line\xe2\x80\x93line_intersection>`_.\r\n    """"""\r\n    x0, y0, x1, y1, x2, y2, x3, y3 = (*p0, *p1, *p2, *p3)\r\n    dc_x, dc_y = p2 - p3\r\n    dp_x, dp_y = p0 - p1\r\n    n1 = x2 * y3 - y2 * x3\r\n    n2 = x0 * y1 - y0 * x1\r\n    n3 = 1.0 / (dc_x * dp_y - dc_y * dp_x)\r\n    arr = np.array([(n1 * dp_x - n2 * dc_x), (n1 * dp_y - n2 * dc_y)])\r\n    return arr * n3\r\n\r\n\r\ndef knn(p, pnts, k=1, return_dist=True):\r\n    """"""\r\n    Calculates k nearest neighbours for a given point.\r\n\r\n    Parameters:\r\n    -----------\r\n    p :array\r\n        x,y reference point\r\n    pnts : array\r\n        Points array to examine\r\n    k : integer\r\n        The `k` in k-nearest neighbours\r\n\r\n    Returns\r\n    -------\r\n    Array of k-nearest points and optionally their distance from the source.\r\n    """"""\r\n\r\n    def _remove_self_(p, pnts):\r\n        """"""Remove a point which is duplicated or itself from the array\r\n        """"""\r\n        keep = ~np.all(pnts == p, axis=1)\r\n        return pnts[keep]\r\n\r\n    def _e_2d_(p, a):\r\n        """""" array points to point distance... mini e_dist\r\n        """"""\r\n        diff = a - p[np.newaxis, :]\r\n        return np.einsum(\'ij,ij->i\', diff, diff)\r\n\r\n    p = np.asarray(p)\r\n    k = max(1, min(abs(int(k)), len(pnts)))\r\n    pnts = _remove_self_(p, pnts)\r\n    d = _e_2d_(p, pnts)\r\n    idx = np.argsort(d)\r\n    if return_dist:\r\n        return pnts[idx][:k], d[idx][:k]\r\n    return pnts[idx][:k]\r\n\r\n\r\ndef knn0(pnts, p, k):\r\n    """"""Calculates `k` nearest neighbours for a given point, `p`, relative to\r\n     otherpoints.\r\n\r\n    Parameters\r\n    ----------\r\n    points : array\r\n        list of points\r\n    p : array-like\r\n        reference point, two numbers representing x, y\r\n    k : integer\r\n        number of neighbours\r\n\r\n    Returns\r\n    -------\r\n    list of the k nearest neighbours, based on squared distance\r\n    """"""\r\n    p = np.asarray(p)\r\n    pnts = np.asarray(pnts)\r\n    diff = pnts - p[np.newaxis, :]\r\n    d = np.einsum(\'ij,ij->i\', diff, diff)\r\n    idx = np.argsort(d)[:k]\r\n    return pnts[idx].tolist()\r\n\r\n\r\n# ---- minimum spanning tree\r\n#\r\ndef _dist_arr_(a, verbose=False):\r\n    """"""Minimum spanning tree prep... """"""\r\n    a = a[~np.isnan(a[:, 0])]\r\n    idx = np.lexsort((a[:, 1], a[:, 0]))  # sort X, then Y\r\n    # idx= np.lexsort((a[:, 0], a[:, 1]))  # sort Y, then X\r\n    a_srt = a[idx, :]\r\n    d = _e_dist_(a_srt)\r\n    if verbose:\r\n        frmt = """"""\\n    {}\\n    :Input array...\\n    {}\\n\\n    :Sorted array...\r\n        {}\\n\\n    :Distance...\\n    {}\r\n        """"""\r\n        args = [_dist_arr_.__doc__, a, a_srt, d]  # d.astype(\'int\')]\r\n        print(dedent(frmt).format(*args))\r\n    return d, idx, a_srt\r\n\r\n\r\ndef _e_dist_(a):\r\n    """"""Return a 2D square-form euclidean distance matrix.  For other\r\n    dimensions, use e_dist in ein_geom.py""""""\r\n    b = a.reshape(np.prod(a.shape[:-1]), 1, a.shape[-1])\r\n    diff = a - b\r\n    d = np.sqrt(np.einsum(\'ijk,ijk->ij\', diff, diff)).squeeze()\r\n    # d = np.triu(d)\r\n    return d\r\n\r\n\r\ndef mst(arr, calc_dist=True):\r\n    """"""Determine the minimum spanning tree for a set of points represented\r\n    by their inter-point distances. ie their `W`eights\r\n\r\n    Parameters\r\n    ----------\r\n    W : array, normally an interpoint distance array\r\n        Edge weights for example, distance, time, for a set of points.\r\n        W needs to be a square array or a np.triu perhaps\r\n\r\n    calc_dist : boolean\r\n        True, if W is a points array, calculate W as the interpoint distance.\r\n        False means that W is not a points array, but some other `weight`\r\n        representing the interpoint relationship\r\n\r\n    Returns\r\n    -------\r\n    pairs - the pair of nodes that form the edges\r\n    """"""\r\n    arr = np.unique(arr, True, False, False, axis=0)[0]\r\n    W = arr[~np.isnan(arr[:, 0])]\r\n    a_copy = np.copy(W)\r\n    if calc_dist:\r\n        W = _e_dist_(W)\r\n    if W.shape[0] != W.shape[1]:\r\n        raise ValueError(""W needs to be square matrix of edge weights"")\r\n    Np = W.shape[0]\r\n    pairs = []\r\n    pnts_seen = [0]  # Add the first point\r\n    n_seen = 1\r\n    # exclude self connections by assigning inf to the diagonal\r\n    diag = np.arange(Np)\r\n    W[diag, diag] = np.inf\r\n    #\r\n    while n_seen != Np:\r\n        new_edge = np.argmin(W[pnts_seen], axis=None)\r\n        new_edge = divmod(new_edge, Np)\r\n        new_edge = [pnts_seen[new_edge[0]], new_edge[1]]\r\n        pairs.append(new_edge)\r\n        pnts_seen.append(new_edge[1])\r\n        W[pnts_seen, new_edge[1]] = np.inf\r\n        W[new_edge[1], pnts_seen] = np.inf\r\n        n_seen += 1\r\n    pairs = np.array(pairs)\r\n    frum = a_copy[pairs[:, 0]]\r\n    too = a_copy[pairs[:, 1]]\r\n    fr_to = np.concatenate((frum, too), axis=1)  # np.vstack(pairs)\r\n    fr_to = uts(fr_to, names=[\'X_orig\', \'Y_orig\', \'X_dest\', \'Y_dest\'])\r\n    return repack_fields(fr_to)\r\n\r\n\r\ndef connect(a, dist_arr, edges):\r\n    """"""Return the full spanning tree, with points, connections and distance\r\n\r\n    Parameters\r\n    ----------\r\n    a : array\r\n        A point array\r\n    dist : array\r\n        The distance array, from _e_dist\r\n    edge : array\r\n        The edges derived from mst\r\n    """"""\r\n    a = a[~np.isnan(a[:, 0])]\r\n    p_f = edges[:, 0]\r\n    p_t = edges[:, 1]\r\n    d = dist_arr[p_f, p_t]\r\n    n = p_f.shape[0]\r\n    dt = [(\'Orig\', \'<i4\'), (\'Dest\', \'i4\'), (\'Dist\', \'<f8\')]\r\n    out = np.zeros((n,), dtype=dt)\r\n    out[\'Orig\'] = p_f\r\n    out[\'Dest\'] = p_t\r\n    out[\'Dist\'] = d\r\n    return out\r\n\r\n\r\n# ---- find\r\n#\r\n# ---- concave hull\r\ndef concave(points, k, pip_check=False):\r\n    """"""Calculates the concave hull for given points\r\n\r\n    Parameters\r\n    ----------\r\n    points : array-like\r\n        initially the input set of points with duplicates removes and\r\n        sorted on the Y value first, lowest Y at the top (?)\r\n    k : integer\r\n        initially the number of points to start forming the concave hull,\r\n        k will be the initial set of neighbors\r\n    pip_check : boolean\r\n        Whether to do the final point in polygon check.  Not needed for closely\r\n        spaced dense point patterns.\r\n    knn0, intersects, angle, point_in_polygon : functions\r\n        Functions used by `concave`\r\n\r\n    Notes:\r\n    ------\r\n    This recursively calls itself to check concave hull.\r\n\r\n    p_set : The working copy of the input points\r\n\r\n    70,000 points with final pop check removed, 1011 pnts on ch\r\n        23.1 s \xc2\xb1 1.13 s per loop (mean \xc2\xb1 std. dev. of 7 runs, 1 loop each)\r\n        2min 15s \xc2\xb1 2.69 s per loop (mean \xc2\xb1 std. dev. of 7 runs, 1 loop each)\r\n    """"""\r\n    PI = np.pi\r\n\r\n    def _angle_(p0, p1, prv_ang=0):\r\n        """"""Angle between two points and the previous angle, or zero.\r\n        """"""\r\n        ang = np.arctan2(p0[1] - p1[1], p0[0] - p1[0])\r\n        a0 = (ang - prv_ang)\r\n        a0 = a0 % (PI * 2) - PI\r\n        return a0\r\n\r\n    def _point_in_polygon_(pnt, poly):  # pnt_in_poly(pnt, poly):  #\r\n        """"""Point in polygon check. ## fix this and use pip from arraytools\r\n        """"""\r\n        x, y = pnt\r\n        N = len(poly)\r\n        for i in range(N):\r\n            x0, y0, xy = [poly[i][0], poly[i][1], poly[(i + 1) % N]]\r\n            c_min = min([x0, xy[0]])\r\n            c_max = max([x0, xy[0]])\r\n            if c_min < x <= c_max:\r\n                p = y0 - xy[1]\r\n                q = x0 - xy[0]\r\n                y_cal = (x - x0) * p / q + y0\r\n                if y_cal < y:\r\n                    return True\r\n        return False\r\n    # ----\r\n    k = max(k, 3)  # Make sure k >= 3\r\n    if isinstance(points, np.ndarray):  # Remove duplicates if not done already\r\n        p_set = np.unique(points, axis=0).tolist()\r\n    else:\r\n        pts = []\r\n        p_set = [pts.append(i) for i in points if i not in pts]  # Remove dupls\r\n        p_set = np.array(p_set)\r\n        del pts\r\n    if len(p_set) < 3:\r\n        raise Exception(""p_set length cannot be smaller than 3"")\r\n    if len(p_set) == 3:\r\n        return p_set  # Points are a polygon already\r\n    k = min(k, len(p_set) - 1)  # Make sure k neighbours can be found\r\n    frst_p = cur_p = min(p_set, key=lambda x: x[1])\r\n    hull = [frst_p]       # Initialize hull with first point\r\n    p_set.remove(frst_p)  # Remove first point from p_set\r\n    prev_ang = 0\r\n    # ----\r\n    while (cur_p != frst_p or len(hull) == 1) and len(p_set) != 0:\r\n        if len(hull) == 3:\r\n            p_set.append(frst_p)          # Add first point again\r\n        knn_pnts = knn0(p_set, cur_p, k)  # Find nearest neighbours\r\n        cur_pnts = sorted(knn_pnts, key=lambda x: -_angle_(x, cur_p, prev_ang))\r\n        its = True\r\n        i = -1\r\n        while its and i < len(cur_pnts) - 1:\r\n            i += 1\r\n            last_point = 1 if cur_pnts[i] == frst_p else 0\r\n            j = 1\r\n            its = False\r\n            while not its and j < len(hull) - last_point:\r\n                its = intersects(hull[-1], cur_pnts[i], hull[-j - 1], hull[-j])\r\n                j += 1\r\n        if its:  # All points intersect, try a higher number of neighbours\r\n            return concave(points, k + 1)\r\n        prev_ang = _angle_(cur_pnts[i], cur_p)\r\n        cur_p = cur_pnts[i]\r\n        hull.append(cur_p)  # Valid candidate was found\r\n        p_set.remove(cur_p)\r\n    if pip_check:\r\n        for point in p_set:\r\n            if not _point_in_polygon_(point, hull):\r\n                return concave(points, k + 1)\r\n    #\r\n    hull = np.array(hull)\r\n    return hull\r\n\r\n\r\ndef _demo():\r\n    """""" """"""\r\n    # L, R, B, T = [300000, 300100, 5025000, 5025100]\r\n    L, B, R, T = [1, 1, 10, 10]\r\n    tol = 1\r\n    N = 10\r\n    a = n_spaced(L, B, R, T, tol, num=N, verbose=True)\r\n    return a\r\n\r\n\r\n# ==== Processing finished ====\r\n# ===========================================================================\r\n#\r\nif __name__ == ""__main__"":\r\n    """"""optional location for parameters""""""\r\n    # print("""")\r\n'"
Free_Tools/npg_create.py,114,"b'# -*- coding: utf-8 -*-\r\n""""""\r\n==========\r\nnpg_create\r\n==========\r\n\r\nScript :\r\n    npg_create.py\r\n\r\nAuthor :\r\n    Dan_Patterson@carleton.ca\r\n\r\nModified :\r\n    2019-09-06\r\n\r\nPurpose :\r\n    Tools for creating arrays of various geometric shapes\r\n\r\nNotes\r\n-----\r\nOriginally part of the `arraytools` module.\r\n\r\nReferences\r\n\r\n""""""\r\n# pylint: disable=C0103  # invalid-name\r\n# pylint: disable=R0914  # Too many local variables\r\n# pylint: disable=R1710  # inconsistent-return-statements\r\n# pylint: disable=W0105  # string statement has no effect\r\n\r\nimport sys\r\nimport numpy as np\r\n\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=500, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n__all__ = [\'code_grid\', \'rot_matrix\',\r\n           \'arc_\', \'arc_sector\',\r\n           \'circle\', \'circle_mini\', \'circle_ring\',\r\n           \'ellipse\',\r\n           \'hex_flat\', \'hex_pointy\',\r\n           \'mesh_xy\',\r\n           \'pyramid\',\r\n           \'rectangle\',\r\n           \'triangle\',\r\n           \'pnt_from_dist_bearing\',\r\n           \'xy_grid\',\r\n           \'transect_lines\',\r\n           \'spiral_archim\',\r\n           \'repeat\', \'mini_weave\'\r\n           ]\r\n\r\n\r\ndef code_grid(cols=1, rows=1, zero_based=False, shaped=True, bottom_up=False):\r\n    """"""produce spreadsheet like labelling, either zero or 1 based\r\n    see: code_grid.py for more details\r\n    """"""\r\n    alph = list("" ABCDEFGHIJKLMNOPQRSTUVWXYZ"")\r\n    UC = [(""{}{}"").format(alph[i], alph[j]).strip()\r\n          for i in range(27)\r\n          for j in range(1, 27)]\r\n    z = [1, 0][zero_based]\r\n    rc = [1, 0][zero_based]\r\n    c = [""{}{:02.0f}"".format(UC[c], r)  # pull in the column heading\r\n         for r in range(z, rows + rc)   # label in the row letter\r\n         for c in range(cols)]          # label in the row number\r\n    c = np.asarray(c)\r\n    if shaped:\r\n        c = c.reshape(rows, cols)\r\n        if bottom_up:\r\n            c = np.flipud(c)\r\n    return c\r\n\r\n\r\n# ---- helpers ---- rot_matrix -----------------------------------------------\r\n#\r\ndef rot_matrix(angle=0, nm_3=False):\r\n    """"""Return the rotation matrix given points and rotation angle\r\n\r\n    Parameters\r\n    ----------\r\n    Rotation angle in degrees and whether the matrix will be used with\r\n    homogenous coordinates.\r\n\r\n    Returns\r\n    -------\r\n    rot_m : matrix\r\n        Rotation matrix for 2D transform.\r\n\r\n    Rotate around...  translate(-x, -y).rotate(theta).translate(x, y).\r\n    """"""\r\n    rad = np.deg2rad(angle)\r\n    c = np.cos(rad)\r\n    s = np.sin(rad)\r\n    rm = np.array([[c, -s, 0.],\r\n                   [s, c, 0.],\r\n                   [0., 0., 1.]])\r\n    if not nm_3:\r\n        rm = rm[:2, :2]\r\n    return rm\r\n\r\n\r\n# ---- arc_sector, convex hull, circle ellipse, hexagons, rectangles,\r\n#      triangle, xy-grid --\r\n#\r\ndef arc_(radius=100, start=0, stop=1, step=0.1, xc=0.0, yc=0.0):\r\n    """"""Create an arc from a specified radius, centre and start/stop angles\r\n\r\n    Parameters\r\n    ----------\r\n    radius : number\r\n        cirle radius from which the arc is obtained\r\n    start, stop, step : numbers\r\n        angles in degrees\r\n    xc, yc : number\r\n        center coordinates in projected units\r\n    as_list : boolean\r\n        False, returns an array.  True yields a list\r\n\r\n    Returns\r\n    -------\r\n      Points on the arc as an array\r\n\r\n    >>> # arc from 0 to 90 in 5 degree increments with radius 2 at (0, 0)\r\n    >>> a0 = arc_(radius=2, start=0, stop=90, step=5, xc=0.0, yc=0.0)\r\n    """"""\r\n    start, stop = sorted([start, stop])\r\n    angle = np.deg2rad(np.arange(start, stop, step))\r\n    x_s = radius*np.cos(angle)         # X values\r\n    y_s = radius*np.sin(angle)         # Y values\r\n    pnts = np.array([x_s, y_s]).T + [xc, yc]\r\n    return pnts\r\n\r\n\r\ndef arc_sector(outer=10, inner=9, start=1, stop=6, step=0.1):\r\n    """"""Form an arc sector bounded by a distance specified by two radii\r\n\r\n    Parameters\r\n    ----------\r\n    outer : number\r\n        outer radius of the arc sector\r\n    inner : number\r\n        inner radius\r\n    start : number\r\n        start angle of the arc\r\n    stop : number\r\n        end angle of the arc\r\n    step : number\r\n        the angle densification step\r\n\r\n    Requires\r\n    --------\r\n      `arc_` is used to produce the arcs, the top arc is rotated clockwise and\r\n      the bottom remains in the order produced to help form closed-polygons.\r\n    """"""\r\n    s_s = [start, stop]\r\n    s_s.sort()\r\n    start, stop = s_s\r\n    top = arc_(outer, start, stop, step, 0.0, 0.0)\r\n    top = top[::-1]\r\n    bott = arc_(inner, start, stop, step, 0.0, 0.0)\r\n    close = top[0]\r\n    pnts = np.concatenate((top, bott, [close]), axis=0)\r\n    return pnts\r\n\r\n\r\ndef circle(radius=100, clockwise=True, theta=1, rot=0.0, scale=1,\r\n           xc=0.0, yc=0.0):\r\n    """"""Produce a circle/ellipse depending on parameters.\r\n\r\n    Parameters\r\n    ----------\r\n    radius : number\r\n        In projected units.\r\n    clockwise : boolean\r\n        True for clockwise (outer rings), False for counter-clockwise\r\n        (for inner rings).\r\n    theta : number\r\n        Angle spacing. If theta=1, angles between -180 to 180, are returned\r\n        in 1 degree increments. The endpoint is excluded.\r\n    rot : number\r\n         Rotation angle in degrees... used if scaling is not equal to 1.\r\n    scale : number\r\n         For ellipses, change the scale to <1 or > 1. The resultant\r\n         y-values will favour the x or y-axis depending on the scaling.\r\n\r\n    Returns\r\n    -------\r\n    List of coordinates for the circle/ellipse\r\n\r\n    Notes\r\n    -----\r\n    You can also use np.linspace if you want to specify point numbers.\r\n    np.linspace(start, stop, num=50, endpoint=True, retstep=False)\r\n    np.linspace(-180, 180, num=720, endpoint=True, retstep=False)\r\n    """"""\r\n    if clockwise:\r\n        angles = np.deg2rad(np.arange(180.0, -180.0-theta, step=-theta))\r\n    else:\r\n        angles = np.deg2rad(np.arange(-180.0, 180.0+theta, step=theta))\r\n    x_s = radius*np.cos(angles)            # X values\r\n    y_s = radius*np.sin(angles) * scale    # Y values\r\n    pnts = np.array([x_s, y_s]).T\r\n    if rot != 0:\r\n        rot_mat = rot_matrix(angle=rot)\r\n        pnts = (np.dot(rot_mat, pnts.T)).T\r\n    pnts = pnts + [xc, yc]\r\n    return pnts\r\n\r\n\r\ndef circle_mini(radius=1.0, theta=10.0, xc=0.0, yc=0.0):\r\n    """"""Produce a circle/ellipse depending on parameters.\r\n\r\n    Parameters\r\n    ----------\r\n    radius : number\r\n        Distance from centre\r\n    theta : number\r\n        Angle of densification of the shape around 360 degrees\r\n\r\n    """"""\r\n    angles = np.deg2rad(np.arange(180.0, -180.0-theta, step=-theta))\r\n    x_s = radius*np.cos(angles) + xc    # X values\r\n    y_s = radius*np.sin(angles) + yc    # Y values\r\n    pnts = np.array([x_s, y_s]).T\r\n    return pnts\r\n\r\n\r\ndef circle_ring(outer=100, inner=0, theta=10, rot=0, scale=1, xc=0.0, yc=0.0):\r\n    """"""Create a multi-ring buffer around a center point (xc, yc).  AKA, a\r\n    buffer ring.\r\n\r\n    Parameters\r\n    ----------\r\n    outer : number\r\n        Outer radius.\r\n    inner : number\r\n        Inner radius.\r\n    theta : number\r\n        See below.\r\n    rot : number\r\n        Rotation angle, used for non-circles.\r\n    scale : number\r\n        Used to scale the y-coordinates.\r\n\r\n    Notes\r\n    -----\r\n    Angles to use to densify the circle::\r\n\r\n    - 360+ circle\r\n    - 120  triangle\r\n    - 90   square\r\n    - 72   pentagon\r\n    - 60   hexagon\r\n    - 45   octagon\r\n    - etc\r\n    """"""\r\n    top = circle(outer, clockwise=True, theta=theta, rot=rot, scale=scale,\r\n                 xc=xc, yc=yc)\r\n    if inner == 0.0:\r\n        return top\r\n    bott = circle(inner, clockwise=False, theta=theta, rot=rot, scale=scale,\r\n                  xc=xc, yc=yc)\r\n    return np.concatenate((top, bott), axis=0)\r\n\r\n\r\ndef circ_3pa(arr):\r\n    """"""same as circ3p but with 3 pnt arr""""""\r\n    p, q, r = arr\r\n    cx, cy, radius = circ_3p(p, q, r)\r\n    return cx, cy, radius\r\n\r\n\r\ndef circ_3p(p, q, r):\r\n    """"""Three point circle center and radius.  A check is made for three points\r\n    on a line.\r\n    """"""\r\n    temp = q[0] * q[0] + q[1] * q[1]\r\n    bc = (p[0] * p[0] + p[1] * p[1] - temp) / 2\r\n    cd = (temp - r[0] * r[0] - r[1] * r[1]) / 2\r\n    # three points on a line check\r\n    det = (p[0] - q[0]) * (q[1] - r[1]) - (q[0] - r[0]) * (p[1] - q[1])\r\n    if abs(det) < 1.0e-6:\r\n        return None, None, np.inf\r\n    # Center of circle\r\n    cx = (bc*(q[1] - r[1]) - cd*(p[1] - q[1])) / det\r\n    cy = ((p[0] - q[0]) * cd - (q[0] - r[0]) * bc) / det\r\n    radius = np.sqrt((cx - p[0])**2 + (cy - p[1])**2)\r\n    return cx, cy, radius\r\n\r\n\r\ndef ellipse(x_radius=1.0, y_radius=1.0, theta=10., xc=0.0, yc=0.0):\r\n    """"""Produce an ellipse depending on parameters.\r\n\r\n    Parameters\r\n    ----------\r\n    radius : number\r\n        Distance from centre in the X and Y directions.\r\n    theta : number\r\n        Angle of densification of the shape around 360 degrees.\r\n    """"""\r\n    angles = np.deg2rad(np.arange(180.0, -180.0-theta, step=-theta))\r\n    x_s = x_radius*np.cos(angles) + xc    # X values\r\n    y_s = y_radius*np.sin(angles) + yc    # Y values\r\n    pnts = np.concatenate((x_s, y_s), axis=0)\r\n    return pnts\r\n\r\n\r\ndef hex_flat(dx=1, dy=1, cols=1, rows=1):\r\n    """"""Generate the points for the flat-headed hexagon.\r\n\r\n    Parameters\r\n    ----------\r\n    dy_dx : number\r\n        The radius width, remember this when setting hex spacing.\r\n    dx : number\r\n        Increment in x direction, +ve moves west to east, left/right\r\n    dy : number\r\n        Increment in y direction, -ve moves north to south, top/bottom\r\n    """"""\r\n    f_rad = np.deg2rad([180., 120., 60., 0., -60., -120., -180.])\r\n    X = np.cos(f_rad) * dy\r\n    Y = np.sin(f_rad) * dy            # scaled hexagon about 0, 0\r\n    seed = np.array(list(zip(X, Y)))  # array of coordinates\r\n    dx = dx * 1.5\r\n    dy = dy * np.sqrt(3.)/2.0\r\n    hexs = [seed + [dx * i, dy * (i % 2)] for i in range(0, cols)]\r\n    m = len(hexs)\r\n    for j in range(1, rows):  # create the other rows\r\n        hexs += [hexs[h] + [0, dy * 2 * j] for h in range(m)]\r\n    return hexs\r\n\r\n\r\ndef hex_pointy(dx=1, dy=1, cols=1, rows=1):\r\n    """"""Pointy hex angles, convert to sin, cos, zip and send.  Also called\r\n    ``traverse hexagons`` by some.\r\n\r\n    Parameters\r\n    ----------\r\n    dy_dx - number\r\n        The radius width, remember this when setting hex spacing.\r\n    dx : number\r\n        Increment in x direction, +ve moves west to east, left/right.\r\n    dy : number\r\n        Increment in y direction, -ve moves north to south, top/bottom.\r\n    """"""\r\n    p_rad = np.deg2rad([150., 90, 30., -30., -90., -150., 150.])\r\n    X = np.cos(p_rad) * dx\r\n    Y = np.sin(p_rad) * dy      # scaled hexagon about 0, 0\r\n    seed = np.array(list(zip(X, Y)))\r\n    dx = dx * np.sqrt(3.)/2.0\r\n    dy = dy * 1.5\r\n    hexs = [seed + [dx * i * 2, 0] for i in range(0, cols)]\r\n    m = len(hexs)\r\n    for j in range(1, rows):  # create the other rows\r\n        hexs += [hexs[h] + [dx * (j % 2), dy * j] for h in range(m)]\r\n    return hexs\r\n\r\n\r\ndef mesh_xy(L=0, B=0, R=5, T=5, dx=1, dy=1, as_rec=True):\r\n    """"""Create a mesh of coordinates within the specified X, Y ranges\r\n\r\n    Parameters\r\n    ----------\r\n    L(eft), R(ight), dx : number\r\n        Coordinate min, max and delta x for X axis.\r\n    B(ott), T(op), dy  : number\r\n        Same as above for Y axis.\r\n    as_rec : boolean\r\n        Produce a structured array (or convert to a record array).\r\n\r\n    Returns\r\n    -------\r\n    -  A list of coordinates of X,Y pairs and an ID if as_rec is True.\r\n    -  A mesh grid X and Y coordinates is also produced.\r\n    """"""\r\n    dt = [(\'Pnt_num\', \'<i4\'), (\'X\', \'<f8\'), (\'Y\', \'<f8\')]\r\n    x = np.arange(L, R + dx, dx, dtype=\'float64\')\r\n    y = np.arange(B, T + dy, dy, dtype=\'float64\')\r\n    mesh = np.meshgrid(x, y, sparse=False)\r\n    if as_rec:\r\n        xs = mesh[0].ravel()\r\n        ys = mesh[1].ravel()\r\n        p = list(zip(np.arange(len(xs)), xs, ys))\r\n        pnts = np.array(p, dtype=dt)\r\n    else:\r\n        p = list(zip(mesh[0].ravel(), mesh[1].ravel()))\r\n        pnts = np.array(p)\r\n    return pnts, mesh\r\n\r\n\r\ndef pyramid(core=9, steps=10, incr=(1, 1), posi=True):\r\n    """"""Create a pyramid see pyramid_demo.py""""""\r\n    a = np.array([core])\r\n    a = np.atleast_2d(a)\r\n    for i in range(1, steps):\r\n        val = core - i\r\n        if posi and (val <= 0):\r\n            val = 0\r\n        a = np.lib.pad(a, incr, ""constant"", constant_values=(val, val))\r\n    return a\r\n\r\n\r\ndef rectangle(dx=1, dy=1, cols=1, rows=1):\r\n    """"""Create the array of pnts to construct a rectangle.\r\n\r\n    Parameters\r\n    ----------\r\n    dx : number\r\n        Increment in x direction, +ve moves west to east, left/right.\r\n    dy : number\r\n        Increment in y direction, -ve moves north to south, top/bottom.\r\n    rows, cols : ints\r\n        The number of rows and columns to produce.\r\n    """"""\r\n    X = [0.0, 0.0, dx, dx, 0.0]       # X, Y values for a unit square\r\n    Y = [0.0, dy, dy, 0.0, 0.0]\r\n    seed = np.array(list(zip(X, Y)))  # [dx0, dy0] keep for insets\r\n    a = [seed + [j * dx, i * dy]      # make the shapes\r\n         for i in range(0, rows)      # cycle through the rows\r\n         for j in range(0, cols)]     # cycle through the columns\r\n    a = np.asarray(a)\r\n    return a\r\n\r\n\r\ndef triangle(dx=1, dy=1, cols=1, rows=1):\r\n    """"""Create a row of meshed triangles.\r\n\r\n    Parameters\r\n    ----------\r\n    dx : number\r\n        Increment/width in x direction, +ve moves west to east, left/right.\r\n    dy : number\r\n        Increment/height in y direction, -ve moves north to south, top/bottom.\r\n    rows, cols : ints\r\n        The number of rows and columns to produce.\r\n    """"""\r\n    grid_type = \'triangle\'\r\n    a, dx, b = dx/2.0, dx, dx*1.5\r\n    Xu = [0.0, a, dx, 0.0]   # X, Y values for a unit triangle, point up\r\n    Yu = [0.0, dy, 0.0, 0.0]\r\n    Xd = [a, b, dx, a]       # X, Y values for a unit triangle, point down\r\n    Yd = [dy, dy, 0.0, dy]   # shifted by dx\r\n    seedU = np.vstack((Xu, Yu)).T  # np.array(list(zip(Xu, Yu)))\r\n    seedD = np.vstack((Xd, Yd)).T  # np.array(list(zip(Xd, Yd)))\r\n    seed = np.array([seedU, seedD])\r\n    a = [seed + [j * dx, i * dy]       # make the shapes\r\n         for i in range(0, rows)       # cycle through the rows\r\n         for j in range(0, cols)]      # cycle through the columns\r\n    a = np.asarray(a)\r\n    s1, s2, s3, s4 = a.shape\r\n    a = a.reshape(s1*s2, s3, s4)\r\n    return a, grid_type\r\n\r\n\r\ndef pnt_from_dist_bearing(orig=(0, 0), bearings=None, dists=None, prn=False):\r\n    """"""Point locations given distance and bearing from an origin.\r\n    Calculate the point coordinates from distance and angle.\r\n\r\n    References\r\n    ----------\r\n    `<https://community.esri.com/thread/66222>`_.\r\n\r\n    `<https://community.esri.com/blogs/dan_patterson/2018/01/21/\r\n    origin-distances-and-bearings-geometry-wanderings>`_.\r\n\r\n    Notes\r\n    -----\r\n    Planar coordinates are assumed.  Use Vincenty if you wish to work with\r\n    geographic coordinates.\r\n\r\n    Sample calculation::\r\n\r\n        bearings = np.arange(0, 361, 22.5)  # 17 bearings\r\n        dists = np.random.randint(10, 500, len(bearings)) * 1.0  OR\r\n        dists = np.full(bearings.shape, 100.)\r\n        data = dist_bearing(orig=orig, bearings=bearings, dists=dists)\r\n\r\n    Create a featureclass from the results::\r\n\r\n        shapeXY = [\'X_to\', \'Y_to\']\r\n        fc_name = \'C:/path/Geodatabase.gdb/featureclassname\'\r\n        arcpy.da.NumPyArrayToFeatureClass(\r\n            out, fc_name, [\'X_to\', \'Y_to\'], ""2951"")\r\n        # ... syntax\r\n        arcpy.da.NumPyArrayToFeatureClass(\r\n            in_array=out, out_table=fc_name, shape_fields=shapeXY,\r\n            spatial_reference=SR)\r\n    """"""\r\n    error = ""An origin with distances and bearings of equal size are required.""\r\n    orig = np.array(orig)\r\n    if bearings is None or dists is None:\r\n        raise ValueError(error)\r\n    iterable = np.all([isinstance(i, (list, tuple, np.ndarray))\r\n                       for i in [dists, bearings]])\r\n    if iterable:\r\n        if not (len(dists) == len(bearings)):\r\n            raise ValueError(error)\r\n    else:\r\n        raise ValueError(error)\r\n    rads = np.deg2rad(bearings)\r\n    dx = np.sin(rads) * dists\r\n    dy = np.cos(rads) * dists\r\n    x_t = np.cumsum(dx) + orig[0]\r\n    y_t = np.cumsum(dy) + orig[1]\r\n    stack = (x_t, y_t, dx, dy, dists, bearings)\r\n    names = [""X_to"", ""Y_to"", ""orig_dx"", ""orig_dy"", ""distance"", ""bearing""]\r\n    data = np.vstack(stack).T\r\n    N = len(names)\r\n    if prn:  # ---- just print the results ----------------------------------\r\n        frmt = ""Origin ({}, {})\\n"".format(*orig) + ""{:>10s}""*N\r\n        print(frmt.format(*names))\r\n        frmt = ""{: 10.2f}""*N\r\n        for i in data:\r\n            print(frmt.format(*i))\r\n        return data\r\n    # ---- produce a structured array from the output -----------------------\r\n    names = "", "".join(names)\r\n    kind = [""<f8""]*N\r\n    kind = "", "".join(kind)\r\n    out = data.transpose()\r\n    out = np.core.records.fromarrays(out, names=names, formats=kind)\r\n    return out\r\n\r\n\r\ndef xy_grid(x, y=None, top_left=True):\r\n    """"""Create a 2D array of locations from x, y values.  The values need not\r\n    be uniformly spaced just sequential. Derived from `meshgrid` in References.\r\n\r\n    Parameters\r\n    ----------\r\n    x, y : array-like\r\n        To form a mesh, there must at least be 2 values in each sequence\r\n    top_left: boolean\r\n        True, y\'s are sorted in descending order, x\'s in ascending\r\n\r\n    References\r\n    ----------\r\n    `<https://github.com/numpy/numpy/blob/master/numpy/lib/function_base.py>`_.\r\n    """"""\r\n    if y is None:\r\n        y = x\r\n    if x.ndim != 1:\r\n        return ""A 1D array required""\r\n    xs = np.sort(np.asanyarray(x))\r\n    ys = np.asanyarray(y)\r\n    if top_left:\r\n        ys = np.argsort(-ys)\r\n    xs = np.reshape(xs, newshape=((1,) + xs.shape))\r\n    ys = np.reshape(ys, newshape=(ys.shape + (1,)))\r\n    xy = [xs, ys]\r\n    xy = np.broadcast_arrays(*xy, subok=True)\r\n    shp = np.prod(xy[0].shape)\r\n    final = np.zeros((shp, 2), dtype=xs.dtype)\r\n    final[:, 0] = xy[0].ravel()\r\n    final[:, 1] = xy[1].ravel()\r\n    return final\r\n\r\n\r\ndef transect_lines(N=5, orig=None, dist=1, x_offset=0, y_offset=0,\r\n                   bearing=0, as_ndarray=True):\r\n    """"""Construct transect lines from origin-destination points given a\r\n    distance and bearing from the origin point.\r\n\r\n    Parameters\r\n    ----------\r\n    N : number\r\n        The number of transect lines.\r\n    orig : array-like\r\n         A single origin.  If None, the cartesian origin (0, 0) is used.\r\n    dist : number or array-like\r\n        The distance(s) from the origin\r\n    x_offset, y_offset : number\r\n        If the `orig` is a single location, you can construct offset lines\r\n        using these values.\r\n    bearing : number or array-like\r\n        If a single number, parallel lines are produced. An array of values\r\n        equal to the `orig` can be used.\r\n\r\n    Returns\r\n    -------\r\n    Two outputs are returned, the first depends on the `as_ndarray` setting.\r\n\r\n    1. True, a structured array. False - a recarray\r\n    2. An ndarray with the field names in case the raw data are required.\r\n\r\n    Notes\r\n    -----\r\n    It is easiest of you pick a `corner`, then use x_offset, y_offset to\r\n    control whether you are moving horizontally and vertically from the origin.\r\n    The bottom left is easiest, and positive offsets move east and north from.\r\n\r\n    Use XY to Line tool in ArcGIS Pro to convert the from/to pairs to a line.\r\n    See references.\r\n\r\n    Examples\r\n    --------\r\n    >>> out, data = transect_lines(N=5, orig=None,\r\n                                   dist=100, x_offset=10,\r\n                                   y_offset=0, bearing=45, as_ndarray=True)\r\n    >>> data\r\n    array([[  0.  ,   0.  ,  70.71,  70.71],\r\n           [ 10.  ,   0.  ,  80.71,  70.71],\r\n           [ 20.  ,   0.  ,  90.71,  70.71],\r\n           [ 30.  ,   0.  , 100.71,  70.71],\r\n           [ 40.  ,   0.  , 110.71,  70.71]])\r\n    >>> out\r\n    array([( 0., 0.,  70.71, 70.71), (10., 0.,  80.71, 70.71),\r\n    ...    (20., 0.,  90.71, 70.71), (30., 0., 100.71, 70.71),\r\n    ...    (40., 0., 110.71, 70.71)],\r\n    ...   dtype=[(\'X_from\', \'<f8\'), (\'Y_from\', \'<f8\'),\r\n    ...          (\'X_to\', \'<f8\'), (\'Y_to\', \'<f8\')])\r\n    ...\r\n    ... Create the table and the lines\r\n    >>> tbl = \'c:/folder/your.gdb/table_name\'\r\n    >>> # arcpy.da.NumPyArrayToTable(a, tbl)\r\n    >>> # arcpy.XYToLine_management(\r\n    ... #       in_table, out_featureclass,\r\n    ... #       startx_field, starty_field, endx_field, endy_field,\r\n    ... #       {line_type}, {id_field}, {spatial_reference}\r\n    ... This is general syntax, the first two are paths of source and output\r\n    ... files, followed by coordinates and options parameters.\r\n    ...\r\n    ... To create compass lines\r\n    >>> b = np.arange(0, 361, 22.5)\r\n    >>> a, data = transect_lines(N=10, orig=[299000, 4999000],\r\n                                 dist=100, x_offset=0, y_offset=0,\r\n                                 bearing=b, as_ndarray=True)\r\n\r\n    References\r\n    ----------\r\n    `<https://community.esri.com/blogs/dan_patterson/2019/01/17/transect-\r\n    lines-parallel-lines-offset-lines>`_.\r\n\r\n    `<http://pro.arcgis.com/en/pro-app/tool-reference/data-management\r\n    /xy-to-line.htm>`_.\r\n    """"""\r\n    def _array_struct_(a, fld_names=[\'X\', \'Y\'], kinds=[\'<f8\', \'<f8\']):\r\n        """"""Convert an array to a structured array""""""\r\n        dts = list(zip(fld_names, kinds))\r\n        z = np.zeros((a.shape[0],), dtype=dts)\r\n        for i in range(a.shape[1]):\r\n            z[fld_names[i]] = a[:, i]\r\n        return z\r\n    #\r\n    if orig is None:\r\n        orig = np.array([0., 0.])\r\n    args = [orig, dist, bearing]\r\n    arrs = [np.atleast_1d(i) for i in args]\r\n    orig, dist, bearing = arrs\r\n    # o_shp, d_shp, b_shp = [i.shape for i in arrs]\r\n    #\r\n    rads = np.deg2rad(bearing)\r\n    dx = np.sin(rads) * dist\r\n    dy = np.cos(rads) * dist\r\n    #\r\n    n = len(bearing)\r\n    N = [N, n][n > 1]  # either the number of lines or bearings\r\n    x_orig = np.arange(N) * x_offset + orig[0]\r\n    y_orig = np.arange(N) * y_offset + orig[1]\r\n    x_dest = x_orig + dx\r\n    y_dest = y_orig + dy\r\n    # ---- create the output array\r\n    names = [\'X_from\', \'Y_from\', \'X_to\', \'Y_to\']\r\n    cols = len(names)\r\n    kind = [\'<f8\']*cols\r\n    data = np.vstack([x_orig, y_orig, x_dest, y_dest]).T\r\n    if as_ndarray:  # **** add this as a flag\r\n        out = _array_struct_(data, fld_names=names, kinds=kind)\r\n    else:\r\n        out = data.transpose()\r\n        out = np.core.records.fromarrays(out, names=names, formats=kind)\r\n    return out, data\r\n\r\n\r\ndef spiral_archim(pnts, n, inward=False, clockwise=True):\r\n    """"""Create an Archimedes spiral in the range 0 to N points with \'n\' steps\r\n    between each incrementstep.  You could use np.linspace.\r\n\r\n    Parameters\r\n    ----------\r\n    N : integer\r\n        The range of the spiral.\r\n    n : integer\r\n        The number of points between steps.\r\n    scale : number\r\n        The size between points.\r\n    outward : boolean\r\n        Radiate the spiral from the center.\r\n\r\n    Notes\r\n    -----\r\n    When n is small relative to N, then you begin to form rectangular\r\n    spirals, like rotated rectangles.\r\n\r\n    With N = 360, n = 20 yields 360 points with 2n points (40) to complete each\r\n    360 degree loop of the spiral.\r\n    """"""\r\n    rnge = np.arange(0.0, pnts)\r\n    if inward:\r\n        rnge = rnge[::-1]\r\n    phi = rnge/n * np.pi\r\n    xs = phi * np.cos(phi)\r\n    ys = phi * np.sin(phi)\r\n    if clockwise:\r\n        xy = np.c_[ys, xs]\r\n    else:\r\n        xy = np.c_[xs, ys]\r\n    # wdth, hght = np.ptp(xy, axis=0)\r\n    return xs, ys, xy\r\n\r\n\r\ndef repeat(seed=None, corner=[0, 0], cols=1, rows=1, angle=0):\r\n    """"""Create the array of pnts to pass on to arcpy using numpy magic to\r\n    produce a fishnet of the desired in_shp.\r\n\r\n    Parameters\r\n    ----------\r\n    seed : use grid_array, hex_flat or hex_pointy.\r\n        You specify the width and height or its ratio when making the shapes.\r\n    corner : array-like\r\n        Lower left corner of the shape pattern.\r\n    dx, dy : numbers\r\n        Offset of the shapes... this is different.\r\n    rows, cols : integers\r\n        The number of rows and columns to produce.\r\n    angle : number\r\n        Rotation angle in degrees.\r\n    """"""\r\n    def rotate(pnts, angle=0):\r\n        """"""Rotate points about the origin in degrees, (+ve for clockwise).""""""\r\n        angle = np.deg2rad(angle)                 # convert to radians\r\n        s = np.sin(angle)\r\n        c = np.cos(angle)    # rotation terms\r\n        aff_matrix = np.array([[c, s], [-s, c]])  # rotation matrix\r\n        XY_r = np.dot(pnts, aff_matrix)           # numpy magic to rotate pnts\r\n        return XY_r\r\n    # ----\r\n    if seed is None:\r\n        a = rectangle(dx=1, dy=1, cols=3, rows=3)\r\n    else:\r\n        a = np.asarray(seed)\r\n    if angle != 0:\r\n        a = [rotate(p, angle) for p in a]      # rotate the scaled points\r\n    pnts = [p + corner for p in a]            # translate them\r\n    return pnts\r\n\r\n\r\ndef mini_weave(n):\r\n    """"""\r\n    n : segments\r\n       z is sliced to ensure compliance\r\n\r\n    >>> a = mini_weave(11)\r\n    >>> e_leng(a)\r\n    | total 14.142135623730953,\r\n    | segment [1.41, 1.41, 1.41, 1.41, 1.41, 1.41, 1.41, 1.41, 1.41, 1.41]\r\n    """"""\r\n    # root2 = np.full(n, np.sqrt(2.))\r\n    one_s = np.ones(n)\r\n    zero_s = np.zeros(n)\r\n#    x = np.arange(n)\r\n#    y = np.arange(n)\r\n#    z = np.asarray([*sum(zip(zero_s, root2), ())])\r\n    x = np.arange(n)\r\n    y = np.zeros(n)\r\n    z = np.asarray([*sum(zip(zero_s, one_s), ())])[:n]\r\n    a = np.vstack((x, y, z)).T\r\n    return a\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    print(""Script path {}"".format(script))\r\n'"
Free_Tools/npg_geom.py,99,"b'# -*- coding: utf-8 -*-\r\n""""""\r\n========\r\nnpg_geom\r\n========\r\n\r\nScript :\r\n    npg_geom.py\r\n\r\nAuthor :\r\n    Dan_Patterson@carleton.ca\r\n\r\nModified :\r\n    2019-09-06\r\n\r\nPurpose :\r\n    Geometry focused methods that work with Geo arrays or np.ndarrays.\r\n    In the case of the former, the methods may be being called from Geo methods\r\n    in such things as a list comprehension.\r\n\r\nNotes\r\n-----\r\n\r\nHow to flatten a searchcursor to points and/or None\r\n\r\n>>> in_fc = ""C:/Git_Dan/npgeom/npgeom.gdb/Polygons""\r\n>>> SR = npg.getSR(in_fc)\r\n>>> with arcpy.da.SearchCursor(in_fc, (\'OID@\', \'SHAPE@\'), None, SR) as c:\r\n...     pnts = [[[[p for p in arr] for arr in r[1]]] for r in c]\r\n>>> c.reset()  # don\'t forget to reset the cursor\r\n""""""\r\n# pylint: disable=R0904  # pylint issue\r\n# pylint: disable=C0103  # invalid-name\r\n# pylint: disable=E0611  # stifle the arcgisscripting\r\n# pylint: disable=R0914  # Too many local variables\r\n# pylint: disable=R1710  # inconsistent-return-statements\r\n# pylint: disable=W0105  # string statement has no effect\r\n# pylint: disable=W0201  # attribute defined outside __init__... none in numpy\r\n# pylint: disable=W0621  # redefining name\r\n\r\nimport sys\r\nimport numpy as np\r\n\r\n# from numpy.lib.recfunctions import unstructured_to_structured as uts\r\n# from numpy.lib.recfunctions import structured_to_unstructured as stu\r\n# from numpy.lib.recfunctions import repack_fields\r\n\r\nfrom scipy.spatial import ConvexHull as CH\r\nfrom scipy.spatial import Delaunay\r\n\r\nimport npgeom as npg\r\n\r\n# import npg_io\r\n# from npGeo_io import fc_data\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n__all__ = [\'_area_centroid_\', \'_angles_\',\r\n           \'_ch_scipy_\', \'_ch_simple_\', \'_ch_\',\r\n           \'_dist_along_\', \'_percent_along_\', \'_pnts_on_line_\',\r\n           \'_polys_to_unique_pnts_\',\r\n           \'_simplify_lines_\',\r\n           \'_pnts_in_poly_\', \'_pnt_on_poly_\', \'_pnt_on_segment_\', \'p_o_p\',\r\n           \'_rotate_\', \'_tri_pnts_\'\r\n           ]\r\n\r\n\r\ndef extent_to_poly(extent, kind=2):\r\n    """"""Create a polygon/polyline feature from an array of x,y values.  The\r\n    array returned is ordered clockwise with the first and last point repeated\r\n    to form a closed-loop.\r\n\r\n    Parameters\r\n    ----------\r\n    extent : array-like\r\n        The extent is specified as four float values in the form of\r\n        L(eft), B(ottom), R(ight), T(op) eg. np.array([5, 5, 10, 10])\r\n    kind : integer\r\n        A value of 1 for a polyline, or 2 for a polygon.\r\n    """"""\r\n    if len(extent) != 4:\r\n        print(""Check the docs...\\n{}"".format(extent_to_poly.__doc__))\r\n        return None\r\n    L, B, R, T = extent\r\n    L, R = min(L, R), max(L, R)\r\n    B, T = min(B, T), max(B, T)\r\n    ext = np.array([[L, B], [L, T], [R, T], [R, B], [L, B]])\r\n    return npg.Update_Geo([ext], K=kind)\r\n\r\n\r\n# ===== Workers with Geo and ndarrays. =======================================\r\n# ---- area and centroid helpers\r\ndef _area_centroid_(a):\r\n    """"""Calculate area and centroid for a singlepart polygon, `a`.  This is also\r\n    used to calculate area and centroid for a Geo array\'s parts.\r\n\r\n    Notes\r\n    -----\r\n    For multipart shapes, just use this syntax:\r\n\r\n    >>> # rectangle with hole\r\n    >>> a0 = np.array([[[0., 0.], [0., 10.], [10., 10.], [10., 0.], [0., 0.]],\r\n                      [[2., 2.], [8., 2.], [8., 8.], [2., 8.], [2., 2.]]])\r\n    >>> [npg._area_centroid_(i) for i in a0]\r\n    >>> [(100.0, array([ 5.00,  5.00])), (-36.0, array([ 5.00,  5.00]))]\r\n    """"""\r\n    x0, y1 = (a.T)[:, 1:]\r\n    x1, y0 = (a.T)[:, :-1]\r\n    e0 = np.einsum(\'...i,...i->...i\', x0, y0)\r\n    e1 = np.einsum(\'...i,...i->...i\', x1, y1)\r\n    t = e1 - e0\r\n    area = np.nansum((e0 - e1)*0.5)\r\n    x_c = np.nansum((x1 + x0) * t, axis=0) / (area * 6.0)\r\n    y_c = np.nansum((y1 + y0) * t, axis=0) / (area * 6.0)\r\n    return area, np.asarray([-x_c, -y_c])\r\n\r\n\r\n# ---- angle helper\r\n#\r\ndef _angles_(a, inside=True, in_deg=True):\r\n    """"""Worker for Geo.angles. sequential points, a, b, c for the first bit in\r\n    a shape, so interior holes are removed in polygons and the first part of\r\n    a multipart shape is used.  Use multipart_to_singlepart if you want to\r\n    process that type.\r\n\r\n    Parameters\r\n    ----------\r\n    inside : boolean\r\n        True, for interior angles.\r\n    in_deg : boolean\r\n        True for degrees, False for radians\r\n    """"""\r\n    #\r\n    dx, dy = a[0] - a[-1]\r\n    if np.allclose(dx, dy):     # closed loop, remove duplicate\r\n        a = a[:-1]\r\n    ba = a - np.roll(a, 1, 0)   # just as fastish as concatenate\r\n    bc = a - np.roll(a, -1, 0)  # but definitely cleaner\r\n    cr = np.cross(ba, bc)\r\n    dt = np.einsum(\'ij,ij->i\', ba, bc)\r\n    ang = np.arctan2(cr, dt)\r\n    TwoPI = np.pi*2.\r\n    if inside:\r\n        angles = np.where(ang < 0, ang + TwoPI, ang)\r\n    else:\r\n        angles = np.where(ang > 0, TwoPI - ang, ang)\r\n    if in_deg:\r\n        angles = np.degrees(angles)\r\n    return angles\r\n\r\n\r\n# ---- convex hull helpers\r\n#\r\ndef _ch_scipy_(points):\r\n    """"""Convex hull using scipy.spatial.ConvexHull. Remove null_pnts, calculate\r\n    the hull, derive the vertices and reorder clockwise.\r\n    """"""\r\n    p_nonan = points[~np.isnan(points[:, 0])]\r\n    out = CH(p_nonan)\r\n    return out.points[out.vertices][::-1]\r\n\r\n\r\ndef _ch_simple_(in_points):\r\n    """"""Calculates the convex hull for given points.  Removes null_pnts, finds\r\n    the unique points, then determines the hull from the remaining\r\n    """"""\r\n    def _x_(o, a, b):\r\n        """"""Cross-product for vectors o-a and o-b... a<--o-->b""""""\r\n        xo, yo = o\r\n        xa, ya = a\r\n        xb, yb = b\r\n        return (xa - xo)*(yb - yo) - (ya - yo)*(xb - xo)\r\n    # ----\r\n    points = in_points[~np.isnan(in_points[:, 0])]\r\n    _, idx = np.unique(points, return_index=True, axis=0)\r\n    points = points[idx]\r\n    if len(points) <= 3:\r\n        return in_points\r\n    # Build lower hull\r\n    lower = []\r\n    for p in points:\r\n        while len(lower) >= 2 and _x_(lower[-2], lower[-1], p) <= 0:\r\n            lower.pop()\r\n        lower.append(p)\r\n    # Build upper hull\r\n    upper = []\r\n    for p in reversed(points):\r\n        while len(upper) >= 2 and _x_(upper[-2], upper[-1], p) <= 0:\r\n            upper.pop()\r\n        upper.append(p)\r\n    ch = np.array(lower[:-1] + upper)[::-1]  # sort clockwise\r\n    if np.all(ch[0] != ch[-1]):\r\n        ch = np.concatenate((ch, ch[0]), axis=0)  # np.vstack((ch, ch[0]))\r\n    return ch\r\n\r\n\r\ndef _ch_(points, threshold=50):\r\n    """"""Perform a convex hull using either simple methods or scipy\'s.""""""\r\n    points = points[~np.isnan(points[:, 0])]\r\n    if len(points) > threshold:\r\n        return _ch_scipy_(points)\r\n    return _ch_simple_(points)\r\n\r\n\r\n# ---- distance, densification helpers\r\n#\r\ndef _dist_along_(a, dist=0):\r\n    """"""Add a point along a poly feature at a distance from the start point.\r\n\r\n    Requires\r\n    --------\r\n    val : number\r\n      `val` is assumed to be a value between 0 and to total length of the\r\n      poly feature.  If <= 0, the first point is returned.  If >= total\r\n      length the last point is returned.\r\n\r\n    Notes\r\n    -----\r\n    Determine the segment lengths and the cumulative length.  From the latter,\r\n    locate the desired distance relative to it and the indices of the start\r\n    and end points.\r\n\r\n    The coordinates of those points and the remaining distance is used to\r\n    derive the location of the point on the line.\r\n\r\n    See Also\r\n    --------\r\n    _percent_along : function\r\n        Similar to this function but measures distance as a percentage.\r\n    """"""\r\n    dxdy = a[1:, :] - a[:-1, :]                        # coordinate differences\r\n    leng = np.sqrt(np.einsum(\'ij,ij->i\', dxdy, dxdy))  # segment lengths\r\n    cumleng = np.concatenate(([0], np.cumsum(leng)))   # cumulative length\r\n    if dist <= 0:              # check for faulty distance or start point\r\n        return a[0]\r\n    if dist >= cumleng[-1]:    # check for greater distance than cumulative\r\n        return a[-1]\r\n    _end_ = np.digitize(dist, cumleng)\r\n    x1, y1 = a[_end_]\r\n    _start_ = _end_ - 1\r\n    x0, y0 = a[_start_]\r\n    t = (dist - cumleng[_start_]) / leng[_start_]\r\n    xt = x0 * (1. - t) + (x1 * t)\r\n    yt = y0 * (1. - t) + (y1 * t)\r\n    return np.array([xt, yt])\r\n\r\n\r\ndef _percent_along_(a, percent=0):\r\n    """"""Add a point along a poly feature at a distance from the start point.\r\n    The distance is specified as a percentage of the total poly feature length.\r\n\r\n    See Also\r\n    --------\r\n    _dist_along : function\r\n        Similar to this function but measures distance as a finite value from\r\n        the start point.\r\n\r\n    Requires\r\n    --------\r\n    Called by ``pnt_on_poly``.\r\n    """"""\r\n    if percent > 1.:\r\n        percent /= 100.\r\n    dxdy = a[1:, :] - a[:-1, :]                        # coordinate differences\r\n    leng = np.sqrt(np.einsum(\'ij,ij->i\', dxdy, dxdy))  # segment lengths\r\n    cumleng = np.concatenate(([0], np.cumsum(leng)))\r\n    perleng = cumleng / cumleng[-1]\r\n    if percent <= 0:              # check for faulty distance or start point\r\n        return a[0]\r\n    if percent >= perleng[-1]:    # check for greater distance than cumulative\r\n        return a[-1]\r\n    _end_ = np.digitize(percent, perleng)\r\n    x1, y1 = a[_end_]\r\n    _start_ = _end_ - 1\r\n    x0, y0 = a[_start_]\r\n    t = (percent - perleng[_start_])\r\n    xt = x0 * (1. - t) + (x1 * t)\r\n    yt = y0 * (1. - t) + (y1 * t)\r\n    return np.array([xt, yt])\r\n\r\n\r\ndef _pnts_on_line_(a, spacing=1, is_percent=False):  # densify by distance\r\n    """"""Add points, at a fixed spacing, to an array representing a line.\r\n    **See**  ``densify_by_distance`` for documentation.\r\n\r\n    Parameters\r\n    ----------\r\n    a : array\r\n        A sequence of `points`, x,y pairs, representing the bounds of a polygon\r\n        or polyline object.\r\n    spacing : number\r\n        Spacing between the points to be added to the line.\r\n    is_percent : boolean\r\n        Express the densification as a percent of the total length.\r\n\r\n    Requires\r\n    --------\r\n    Called by ``pnt_on_poly``.\r\n    """"""\r\n    N = len(a) - 1                                    # segments\r\n    dxdy = a[1:, :] - a[:-1, :]                       # coordinate differences\r\n    leng = np.sqrt(np.einsum(\'ij,ij->i\', dxdy, dxdy))  # segment lengths\r\n    if is_percent:                                    # as percentage\r\n        spacing = abs(spacing)\r\n        spacing = min(spacing/100, 1.)\r\n        steps = (sum(leng) * spacing) / leng          # step distance\r\n    else:\r\n        steps = leng/spacing                          # step distance\r\n    deltas = dxdy/(steps.reshape(-1, 1))              # coordinate steps\r\n    pnts = np.empty((N,), dtype=\'O\')                  # construct an `O` array\r\n    for i in range(N):              # cycle through the segments and make\r\n        num = np.arange(steps[i])   # the new points\r\n        pnts[i] = np.array((num, num)).T * deltas[i] + a[i]\r\n    a0 = a[-1].reshape(1, -1)        # add the final point and concatenate\r\n    return np.concatenate((*pnts, a0), axis=0)\r\n\r\n\r\n# ---- poly conversion helpers\r\n#\r\ndef _polys_to_unique_pnts_(a, as_structured=True):\r\n    """"""Derived from polys_to_points, but allowing for recreation of original\r\n    point order and unique points.  NaN\'s are removed.\r\n    """"""\r\n    good = a[~np.isnan(a.X)]\r\n    uni, idx, _, cnts = np.unique(good, True, True,\r\n                                  return_counts=True, axis=0)\r\n    if as_structured:\r\n        N = uni.shape[0]\r\n        dt = [(\'New_ID\', \'<i4\'), (\'Xs\', \'<f8\'), (\'Ys\', \'<f8\'), (\'Num\', \'<i4\')]\r\n        z = np.zeros((N,), dtype=dt)\r\n        z[\'New_ID\'] = idx\r\n        z[\'Xs\'] = uni[:, 0]\r\n        z[\'Ys\'] = uni[:, 1]\r\n        z[\'Num\'] = cnts\r\n        return z[np.argsort(z, order=\'New_ID\')]  # dump nan coordinates\r\n    return np.asarray(uni)\r\n\r\n\r\ndef _simplify_lines_(a, deviation=10):\r\n    """"""Simplify array\r\n    """"""\r\n    ang = _angles_(a, inside=True, in_deg=True)\r\n    idx = (np.abs(ang - 180.) >= deviation)\r\n    sub = a[1: -1]\r\n    p = sub[idx]\r\n    return a, p, ang\r\n\r\n\r\n# ---- points in or on geometries --------------------------------------------\r\n#\r\ndef _pnts_in_poly_(pnts, poly):\r\n    """"""Points in polygon, implemented using crossing number largely derived\r\n    from **pnpoly** in its various incarnations.\r\n\r\n    This version does a ``within extent`` test to pre-process the points.\r\n    Points meeting this condition are passed on to the crossing number section.\r\n\r\n    Parameters\r\n    ----------\r\n    pnts : array\r\n        point array\r\n    poly : polygon\r\n        Closed-loop as an array.  The last and first point will be the same in\r\n        a correctly formed polygon.\r\n\r\n    Notes\r\n    -----\r\n    Helpers from From arraytools.pntinply.\r\n    """"""\r\n    def _in_ext_(pnts, ext):\r\n        """""" Returns the points within an extent.""""""\r\n        LB, RT = ext\r\n        comp = np.logical_and(LB < pnts, pnts <= RT)\r\n        idx = np.logical_and(comp[..., 0], comp[..., 1])\r\n        return pnts[idx]\r\n\r\n    def _crossing_num_(pnts, poly):\r\n        """"""The implementation of pnply.""""""\r\n        xs = poly[:, 0]\r\n        ys = poly[:, 1]\r\n        dx = np.diff(xs)\r\n        dy = np.diff(ys)\r\n        ext = np.array([poly.min(axis=0), poly.max(axis=0)])\r\n        inside = _in_ext_(pnts, ext)\r\n        is_in = []\r\n        for pnt in inside:\r\n            cn = 0    # the crossing number counter\r\n            x, y = pnt\r\n            for i in range(len(poly)-1):      # edge from V[i] to V[i+1]\r\n                if np.logical_or((ys[i] <= y < ys[i+1]),\r\n                                 (ys[i] >= y > ys[i+1])):\r\n                    vt = (y - ys[i]) / dy[i]  # compute x-coordinate\r\n                    if x < (xs[i] + vt * dx[i]):\r\n                        cn += 1\r\n            is_in.append(cn % 2)  # either even or odd (0, 1)\r\n        return inside[np.nonzero(is_in)]\r\n    # ----\r\n    inside = _crossing_num_(pnts, poly)\r\n    return inside\r\n\r\n\r\ndef _pnt_on_poly_(pnt, poly):\r\n    """"""Find closest point location on a polygon/polyline.\r\n\r\n    See : ``p_o_p`` for batch running of multiple points to a polygon.\r\n\r\n    Parameters\r\n    ----------\r\n    pnt : 1D ndarray array\r\n        XY pair representing the point coordinates.\r\n    poly : 2D ndarray array\r\n        A sequence of XY pairs in clockwise order is expected.  The first and\r\n        last points may or may not be duplicates, signifying sequence closure.\r\n\r\n    Returns\r\n    -------\r\n    A list of [x, y, distance, angle] for the intersection point on the line.\r\n    The angle is relative to north from the origin point to the point on the\r\n    polygon.\r\n\r\n    Notes\r\n    -----\r\n    ``e_dist`` is represented by _e_2d and pnt_on_seg by its equivalent below.\r\n\r\n    ``_line_dir_`` is from it\'s equivalent line_dir included here.\r\n\r\n    This may be as simple as finding the closest point on the edge, but if\r\n    needed, an orthogonal projection onto a polygon/line edge will be done.\r\n    This situation arises when the distance to two sequential points is the\r\n    same.\r\n    """"""\r\n    def _e_2d_(a, p):\r\n        """""" array points to point distance... mini e_dist""""""\r\n        diff = a - p[None, :]\r\n        return np.sqrt(np.einsum(\'ij,ij->i\', diff, diff))\r\n\r\n    def _pnt_on_seg_(seg, pnt):\r\n        """"""mini pnt_on_seg function normally required by pnt_on_poly""""""\r\n        x0, y0, x1, y1, dx, dy = *pnt, *seg[0], *(seg[1] - seg[0])\r\n        dist_ = dx*dx + dy*dy  # squared length\r\n        u = ((x0 - x1)*dx + (y0 - y1)*dy)/dist_\r\n        u = max(min(u, 1), 0)  # u must be between 0 and 1\r\n        xy = np.array([dx, dy])*u + [x1, y1]\r\n        return xy\r\n\r\n    def _line_dir_(orig, dest):\r\n        """"""mini line direction function""""""\r\n        orig = np.atleast_2d(orig)\r\n        dest = np.atleast_2d(dest)\r\n        dxy = dest - orig\r\n        ang = np.degrees(np.arctan2(dxy[:, 1], dxy[:, 0]))\r\n        return ang\r\n    #\r\n    pnt = np.asarray(pnt)\r\n    poly = np.asarray(poly)\r\n    if np.all(poly[0] == poly[-1]):  # strip off any duplicate points\r\n        poly = poly[:-1]\r\n    # ---- determine the distances\r\n    d = _e_2d_(poly, pnt)   # abbreviated edist =>  d = e_dist(poly, pnt)\r\n    key = np.argsort(d)[0]  # dist = d[key]\r\n    if key == 0:  # np.vstack((poly[-1:], poly[:3]))\r\n        seg = np.concatenate((poly[-1:], poly[:3]), axis=0)\r\n    elif (key + 1) >= len(poly):  # np.vstack((poly[-2:], poly[:1]))\r\n        seg = np.concatenate((poly[-2:], poly[:1]), axis=0)\r\n    else:\r\n        seg = poly[key-1:key+2]       # grab the before and after closest\r\n    n1 = _pnt_on_seg_(seg[:-1], pnt)  # abbreviated pnt_on_seg\r\n    d1 = np.linalg.norm(n1 - pnt)\r\n    n2 = _pnt_on_seg_(seg[1:], pnt)   # abbreviated pnt_on_seg\r\n    d2 = np.linalg.norm(n2 - pnt)\r\n    if d1 <= d2:\r\n        dest = [n1[0], n1[1]]\r\n        ang = _line_dir_(pnt, dest)\r\n        ang = np.mod((450.0 - ang), 360.)\r\n        r = (pnt[0], pnt[1], n1[0], n1[1], d1.item(), ang.item())\r\n        return r\r\n    dest = [n2[0], n2[1]]\r\n    ang = _line_dir_(pnt, dest)\r\n    ang = np.mod((450.0 - ang), 360.)\r\n    r = (pnt[0], pnt[1], n2[0], n2[1], d2.item(), ang.item())\r\n    return r\r\n\r\n\r\ndef _pnt_on_segment_(pnt, seg):\r\n    """"""Orthogonal projection of a point onto a 2 point line segment.\r\n    Returns the intersection point, if the point is between the segment end\r\n    points, otherwise, it returns the distance to the closest endpoint.\r\n\r\n    Parameters\r\n    ----------\r\n    pnt : array-like\r\n        `x,y` coordinate pair as list or ndarray\r\n    seg : array-like\r\n        `from-to points`, of x,y coordinates as an ndarray or equivalent\r\n\r\n    Notes\r\n    -----\r\n    >>> seg = np.array([[0, 0], [10, 10]])  # p0, p1\r\n    >>> p = [10, 0]\r\n    >>> pnt_on_seg(seg, p)\r\n    array([5., 5.])\r\n\r\n    Generically, with crossproducts and norms\r\n\r\n    >>> d = np.linalg.norm(np.cross(p1-p0, p0-p))/np.linalg.norm(p1-p0)\r\n    """"""\r\n    x0, y0, x1, y1, dx, dy = *pnt, *seg[0], *(seg[1] - seg[0])\r\n    dist_ = dx*dx + dy*dy  # squared length\r\n    u = ((x0 - x1)*dx + (y0 - y1)*dy)/dist_\r\n    u = max(min(u, 1), 0)\r\n    xy = np.array([dx, dy])*u + [x1, y1]\r\n    d = xy - pnt\r\n    return xy, np.hypot(d[0], d[1])\r\n\r\n\r\ndef p_o_p(pnts, poly):\r\n    """"""Main runner to run multiple points to a polygon.\r\n    """"""\r\n    result = []\r\n    for p in pnts:\r\n        result.append(_pnt_on_poly_(p, poly))\r\n    result = np.asarray(result)\r\n    dt = [(\'X0\', \'<f8\'), (\'Y0\', \'<f8\'), (\'X1\', \'<f8\'), (\'Y1\', \'<f8\'),\r\n          (\'Dist\', \'<f8\'), (\'Angle\', \'<f8\')]\r\n    z = np.zeros((len(result),), dtype=dt)\r\n    names = z.dtype.names\r\n    for i, n in enumerate(names):\r\n        z[n] = result[:, i]\r\n    return z\r\n\r\n\r\n# ---- rotate helper\r\ndef _rotate_(geo_arr, R, as_group, clockwise):\r\n    """"""Rotation helper.\r\n\r\n    Parameters\r\n    ----------\r\n    geo_arr : array\r\n        The input geo array, which is split here.\r\n    as_group : boolean\r\n        False, rotated about the extent center.  True, rotated about each\r\n        shapes\' center.\r\n    R : array\r\n        The rotation matrix, passed on from Geo.rotate.\r\n    clockwise : boolean\r\n    """"""\r\n    shapes = geo_arr.shapes\r\n    out = []\r\n    if as_group:\r\n        uniqs = []\r\n        for chunk in shapes:\r\n            _, idx = np.unique(chunk, True, axis=0)\r\n            uniqs.append(chunk[np.sort(idx)])\r\n        cents = [np.nanmean(i, axis=0) for i in uniqs]\r\n        for i, chunk in enumerate(shapes):\r\n            ch = np.einsum(\'ij,jk->ik\', chunk-cents[i], R) + cents[i]\r\n            out.append(ch)\r\n        return out\r\n    cent = np.nanmean(geo_arr, axis=0)\r\n    for chunk in shapes:\r\n        ch = np.einsum(\'ij,jk->ik\', chunk-cent, R) + cent\r\n        out.append(ch)\r\n    return out\r\n\r\n\r\n# ---- triangulation, Delaunay helper\r\n#\r\ndef _tri_pnts_(pnts):\r\n    """"""Triangulate the points and return the triangles.\r\n\r\n    Parameters:\r\n    -----------\r\n    pnts : array\r\n        Points for a shape or a group of points in array format.\r\n        Either geo.shapes or np.ndarray.\r\n    out : array\r\n        An array of triangle points.\r\n\r\n    Notes:\r\n    ------\r\n    The simplices are ordered counterclockwise, this is reversed in this\r\n    implementation.\r\n\r\n    References\r\n    ----------\r\n    `<C:/Arc_projects/Polygon_lineTools/Scripts/triangulate.py>`_.\r\n    """"""\r\n    pnts = pnts[~np.isnan(pnts[:, 0])]  # strip nan points\r\n    pnts = np.unique(pnts, axis=0)    # get the unique points only\r\n    avg = np.mean(pnts, axis=0)\r\n    p = pnts - avg\r\n    tri = Delaunay(p)\r\n    simps = tri.simplices\r\n    # ---- indices holder, fill with indices, repeat first and roll CW\r\n    # translate the points back\r\n    z = np.zeros((len(simps), 4), dtype=\'int32\')\r\n    z[:, :3] = simps\r\n    z[:, 3] = simps[:, 0]\r\n    z = z[:, ::-1]\r\n    new_pnts = p[z] + avg\r\n    new_pnts = new_pnts.reshape(-1, 2)\r\n    return new_pnts\r\n\r\n# ---- Not included yet -----------------------------------------------------\r\n#\r\n\r\n\r\n# ===========================================================================\r\n# Extras used elsewhere\r\n\'\'\'\r\ndef _area_part_(a):\r\n    """"""Mini e_area, used by areas and centroids""""""\r\n    x0, y1 = (a.T)[:, 1:]\r\n    x1, y0 = (a.T)[:, :-1]\r\n    e0 = np.einsum(\'...i,...i->...i\', x0, y0)\r\n    e1 = np.einsum(\'...i,...i->...i\', x1, y1)\r\n    return np.nansum((e0 - e1)*0.5)\r\n\r\n\r\ndef pnt_in_list(pnt, pnts_list):\r\n    """"""Check to see if a point is in a list of points.\r\n\r\n    sum([(x, y) == tuple(i) for i in [p0, p1, p2, p3]]) > 0\r\n    """"""\r\n    is_in = np.any([np.isclose(pnt, i) for i in pnts_list])\r\n    return is_in\r\n\r\n\'\'\'\r\n# ===========================================================================\r\n#\r\nif __name__ == ""__main__"":\r\n    """"""optional location for parameters""""""\r\n    # optional controls here\r\n\r\n""""""\r\nDemo\r\n\r\nr = np.array([\'A\', \'A\', \'B\', \'B\', \'B\', \'A\', \'A\', \'C\', \'C\', \'A\'], dtype=\'<U1\')\r\nc = np.array([\'b\', \'a\', \'b\', \'a\', \'b\', \'b\', \'b\', \'a\', \'b\', \'a\'], dtype=\'<U1\')\r\nrc = np.array([""{}_{}"".format(*i) for i in zip(r, c)])\r\nu, idx, cnts = np.unique(rc, return_index=True, return_counts=True)\r\ndt = [(\'r_c\', u.dtype.str), (\'cnts\', \'<i4\')]\r\nctab = np.array(list(zip(u, cnts)), dtype=dt)\r\n""""""\r\n'"
Free_Tools/npg_io.py,97,"b'# -*- coding: utf-8 -*-\r\n""""""\r\n=========\r\nnpg_io.py\r\n=========\r\n\r\nScript :\r\n    .../npgeom/npg_io.py\r\n\r\nAuthor :\r\n    Dan_Patterson@carleton.ca\r\n\r\nModified : 2019-09-06\r\n    Creation date during 2019 as part of ``arraytools``.\r\n\r\nPurpose : Tools for working with point and poly features as an array class\r\n    Requires npGeo to implement the array geometry class.\r\n\r\n\r\nSee Also\r\n--------\r\n\r\n__init__ :\r\n    `__init__.py` has further information on arcpy related functionality.\r\nnpGeo :\r\n    A fuller description of the Geo class, its methods and properties is given\r\n    there.  This script focuses on getting arcpy or geojson geometry into\r\n    numpy arrays.\r\n\r\nReferences\r\n----------\r\n**General**\r\n\r\n`Subclassing ndarrays\r\n<https://docs.scipy.org/doc/numpy/user/basics.subclassing.html>`_.\r\n\r\n\r\n""""""\r\n# pylint: disable=C0330  # Wrong hanging indentation\r\n# pylint: disable=C0103  # invalid-name\r\n# pylint: disable=E0611  # stifle the arcgisscripting\r\n# pylint: disable=E1101  # ditto for arcpy\r\n# pylint: disable=R0914  # Too many local variables\r\n# pylint: disable=R1710  # inconsistent-return-statements\r\n# pylint: disable=W0105  # string statement has no effect\r\n# pylint: disable=W0621  # redefining name\r\n\r\nimport sys\r\nfrom textwrap import dedent, indent\r\nimport json\r\nimport numpy as np\r\nfrom numpy.lib.recfunctions import structured_to_unstructured as stu\r\nfrom numpy.lib.recfunctions import unstructured_to_structured as uts\r\nfrom npGeo import *\r\nimport arcpy.da\r\n\r\n\r\n__all__ = [\r\n    \'poly2array\',   # shape to array and conversion\r\n    \'load_geojson\', \'geojson_Geo\', \'fc_json\',\r\n    \'arrays_to_Geo\', \'Geo_to_arrays\',\r\n    \'array_ift\',\r\n    \'_make_nulls_\', \'getSR\', \'shape_K\', \'fc_composition\',\r\n    \'fc_data\', \'fc_geometry\', \'fc_shapes\',\r\n    \'array_poly\', \'geometry_fc\',                     # convert back to fc\r\n    \'prn_q\', \'_check\', \'prn_tbl\', \'prn_geo\',\r\n    \'shape_properties\', \'flatten_to_points\'\r\n    ]\r\n\r\n# ---- Constants -------------------------------------------------------------\r\n#\r\nscript = sys.argv[0]\r\n\r\nFLOATS = np.typecodes[\'AllFloat\']\r\nINTS = np.typecodes[\'AllInteger\']\r\nNUMS = FLOATS + INTS\r\n\r\nnull_pnt = (np.nan, np.nan)  # ---- a null point\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=160, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\n\r\n\r\n# ==== geometry =========================================================\r\n# These are the main geometry to array conversions\r\n#\r\n# ---- for polyline/polygon featureclasses\r\n\r\ndef poly2array(polys):\r\n    """"""Convert polyline or polygon shapes to arrays for use in the Geo class.\r\n\r\n    Parameters\r\n    ----------\r\n    polys : tuple, list\r\n        Polyline or polygons in a list/tuple\r\n    """"""\r\n    def _p2p_(poly):\r\n        """"""Convert a single ``poly`` shape to numpy arrays or object""""""\r\n        sub = []\r\n        for arr in poly:\r\n            pnts = [[pt.X, pt.Y] if pt else null_pnt for pt in arr]\r\n            sub.append(np.asarray(pnts))\r\n        return sub\r\n    # ----\r\n    if not isinstance(polys, (list, tuple)):\r\n        polys = [polys]\r\n    out = []\r\n    for poly in polys:\r\n        out.append(_p2p_(poly))\r\n    return out\r\n\r\n\r\n# ---- json section\r\n#\r\ndef load_geojson(pth, full=False, geometry=True):\r\n    """"""Load a geojson file and convert to a Geo Array.  The geojson is from the\r\n    Features to JSON tool listed in the references.\r\n\r\n    Requires\r\n    --------\r\n    pth : file path\r\n        Full file path to the geojson file.\r\n    full : boolean\r\n        True to return a formatted geojson file.\r\n    geometry : boolean\r\n        True returns just the geometry of the file.\r\n\r\n    Returns\r\n    -------\r\n    data : dictionary\r\n        The full geojson dictionary of the geometry and its attributes.  The\r\n        result is a nested dictionary::\r\n\r\n    >>> data\r\n    ... {\'type\':\r\n    ...  \'crs\': {\'type\': \'name\', \'properties\': {\'name\': \'EPSG:2951\'}},\r\n    ...  \'features\': [{\'type\': \'Feature\',\r\n    ...    \'id\': 1,\r\n    ...    \'geometry\': {\'type\':  \'MultiPolygon\',\r\n    ...     \'coordinates\': snip},  # coordinate values\r\n    ...     \'properties\': snip }}, # attribute values from table\r\n    ... {\'type\': ... repeat}\r\n\r\n    geometry : list\r\n        A list of lists representing the features, their parts *for multipart\r\n        features) and inner holes (for polygons).\r\n\r\n    References\r\n    ----------\r\n    `geojson specification in detail\r\n    <https://geojson.org/>`_.\r\n\r\n    `Features to JSON\r\n    <https://pro.arcgis.com/en/pro-app/tool-reference/conversion/\r\n    features-to-json.htm>`_.\r\n\r\n    `JSON to Features\r\n    <https://pro.arcgis.com/en/pro-app/tool-reference/conversion/\r\n    json-to-features.htm>`_.\r\n    """"""\r\n    # import json\r\n    with open(pth) as f:\r\n        data = json.load(f)\r\n    shapes = data[\'features\']\r\n    coords = [s[\'geometry\'][\'coordinates\'] for s in shapes]\r\n    if full and geometry:\r\n        return data, coords\r\n    if full:\r\n        return data\r\n    if geometry:\r\n        return coords\r\n\r\n\r\ndef geojson_Geo(pth, kind=2):\r\n    """"""GeoJSON file to Geo array using `array_ift`.\r\n\r\n    Parameters\r\n    ----------\r\n    pth : string\r\n        Full path to the geojson file.\r\n    kind : integer\r\n        Polygon, Polyline or Point type are identified as either 2, 1, or 0.\r\n    """"""\r\n    coords = load_geojson(pth)\r\n    a_2d, IFT = array_ift(coords)\r\n    a_2d = np.concatenate(a_2d, axis=0)  # np.vstack(a_2d)\r\n    return Geo(a_2d, IFT, kind)\r\n\r\n\r\ndef fc_json(in_fc, SR=None):\r\n    """"""Produce arrays from the json representation of fc_shapes shapes.\r\n    """"""\r\n    shapes = fc_shapes(in_fc, SR=SR)\r\n    arr = []\r\n    json_keys = [i for i in json.loads(shapes[0].JSON).keys()]\r\n    geom_key = json_keys[0]\r\n    for s in shapes:\r\n        arr.append(json.loads(s.JSON)[geom_key])\r\n    return arr\r\n\r\n\r\n# ===========================================================================\r\n# ---- Geo array construction\r\n#    Construct the Geo array from a list of ndarrays or an ndarray and\r\n#    deconstruct, the Geo array back to its origins\r\n#\r\ndef arrays_to_Geo(in_arrays, Kind=2, Info=None):\r\n    """"""Produce a Geo class object from a list/tuple of arrays.\r\n\r\n    Parameters\r\n    ----------\r\n    in_arrays : list\r\n        ``in_arrays`` can be created by adding existing 2D arrays to the list\r\n        or produced from the conversion of poly features to arrays using\r\n        ``poly2arrays``.\r\n    Kind : integer\r\n        Points (0), polylines (1) or polygons (2)\r\n\r\n    Requires\r\n    --------\r\n    npg_io.array_ift\r\n\r\n    Returns\r\n    -------\r\n    A ``Geo`` class object based on a 2D np.ndarray (a_2d) with an array of\r\n    indices (IFT) delineating geometry from-to points for each shape and its\r\n    parts.\r\n\r\n    See Also\r\n    --------\r\n    **fc_geometry** to produce ``Geo`` objects directly from arcgis pro\r\n    featureclasses.\r\n    """"""\r\n    a_2d, IFT = array_ift(in_arrays)     # ---- call npg_io.array_ift\r\n    return Geo(a_2d, IFT, Kind, Info)\r\n\r\n\r\ndef Geo_to_arrays(in_geo):\r\n    """"""Reconstruct the input arrays from the Geo array.""""""\r\n    return np.asarray([np.asarray(in_geo.get(i))\r\n                       for i in np.unique(in_geo.IDs).tolist()])\r\n\r\n\r\n# ---- produce the stacked array and IFT values from the output of poly2array\r\n#  or from the output of load_geojson\r\n#\r\ndef array_ift(in_arrays):\r\n    """"""Produce a 2D array stack of x,y points and an I(d) F(rom) T(o) list of\r\n    the coordinate pairs in the resultant.\r\n\r\n    Parameters\r\n    ----------\r\n    in_arrays : list, array\r\n        The input data as a list of lists or arrays or an array.\r\n\r\n    Notes\r\n    -----\r\n    Called by ``npgeo_io.arrays_to_Geo``.\r\n    Use ``fc_geometry`` to produce ``Geo`` objects directly from arcgis pro\r\n    featureclasses.\r\n    """"""\r\n    null_pnt = np.array([[np.nan, np.nan]])\r\n    id_too = []\r\n    a_2d = []\r\n    if isinstance(in_arrays, np.ndarray):\r\n        if in_arrays.ndim == 2:\r\n            in_arrays = [in_arrays]\r\n    for cnt, p in enumerate(in_arrays):\r\n        p = np.asarray(p)\r\n        kind = p.dtype.kind\r\n        # print(""id {} kind {}"".format(cnt, kind))\r\n        if kind == \'O\':\r\n            bits = []\r\n            for j in p:\r\n                for i in j:\r\n                    bits.append(np.asarray(i))\r\n                    bits.append(null_pnt)\r\n                bits = bits[:-1]\r\n                sub = np.concatenate(bits, axis=0)  # np.vstack(bits)\r\n                id_too.append([cnt, len(sub)])\r\n            # sub = stack\r\n        elif kind in NUMS:\r\n            sub = []\r\n            if len(p.shape) == 2:\r\n                id_too.append([cnt, len(p)])\r\n                sub.append(np.asarray(p))\r\n            elif len(p.shape) == 3:\r\n                id_too.extend([[cnt, len(k)] for k in p])\r\n                sub.append([np.asarray(j) for i in p for j in i])\r\n        subs = np.concatenate(sub, axis=0)  # np.vstack(sub)\r\n        a_2d.append(subs)\r\n    a_2d = np.concatenate(a_2d, axis=0)  # np.vstack(a_2d)\r\n    id_too = np.array(id_too)\r\n    ids = id_too[:, 0]\r\n    too = np.cumsum(id_too[:, 1])\r\n    frum = np.concatenate(([0], too))\r\n    IFT = np.array(list(zip(ids, frum, too)))\r\n    return a_2d, IFT\r\n\r\n\r\n# ===========================================================================\r\n# ---- featureclass section, arcpy dependent via arcgisscripting\r\n#\r\ndef _make_nulls_(in_fc, include_oid=True, int_null=-999):\r\n    """"""Return null values for a list of fields objects, excluding objectid\r\n    and geometry related fields.  Throw in whatever else you want.\r\n\r\n    Parameters\r\n    ----------\r\n    in_fc : featureclass or featureclass table\r\n        Uses arcpy.ListFields to get a list of featureclass/table fields.\r\n    int_null : integer\r\n        A default to use for integer nulls since there is no ``nan`` equivalent\r\n        Other options include\r\n\r\n    >>> np.iinfo(np.int32).min # -2147483648\r\n    >>> np.iinfo(np.int16).min # -32768\r\n    >>> np.iinfo(np.int8).min  # -128\r\n\r\n    >>> [i for i in cur.__iter__()]\r\n    >>> [[j if j else -999 for j in i] for i in cur.__iter__() ]\r\n    """"""\r\n    nulls = {\'Double\': np.nan, \'Single\': np.nan, \'Float\': np.nan,\r\n             \'Short\': int_null, \'SmallInteger\': int_null, \'Long\': int_null,\r\n             \'Integer\': int_null, \'String\': str(None), \'Text\': str(None),\r\n             \'Date\': np.datetime64(\'NaT\'), \'Geometry\': np.nan}\r\n    #\r\n    desc = arcpy.da.Describe(in_fc)\r\n    if desc[\'dataType\'] not in (\'FeatureClass\', \'Table\'):\r\n        print(""Only Featureclasses and tables are supported"")\r\n        return None, None\r\n    in_flds = desc[\'fields\']\r\n    good = [f for f in in_flds if f.editable and f.type != \'Geometry\']\r\n    fld_dict = {f.name: f.type for f in good}\r\n    fld_names = list(fld_dict.keys())\r\n    null_dict = {f: nulls[fld_dict[f]] for f in fld_names}\r\n    # ---- insert the OBJECTID field\r\n    if include_oid and desc[\'hasOID\']:\r\n        oid_name = \'OID@\'  # desc[\'OIDFieldName\']\r\n        oi = {oid_name: -999}\r\n        null_dict = dict(list(oi.items()) + list(null_dict.items()))\r\n        fld_names.insert(0, oid_name)\r\n    return null_dict, fld_names\r\n\r\n\r\ndef getSR(in_fc, verbose=False):\r\n    """"""Return the spatial reference of a featureclass""""""\r\n    desc = arcpy.da.Describe(in_fc)\r\n    SR = desc[\'spatialReference\']\r\n    if verbose:\r\n        print(""SR name: {}  factory code: {}"".format(SR.name, SR.factoryCode))\r\n    return SR\r\n\r\n\r\ndef shape_K(in_fc):\r\n    """"""The shape type represented by the featureclass.  Returns (kind, k)""""""\r\n    desc = arcpy.da.Describe(in_fc)\r\n    kind = desc[\'shapeType\']\r\n    if kind in (\'Polygon\', \'PolygonM\', \'PolygonZ\'):\r\n        return (kind, 2)\r\n    if kind == (\'Polyline\', \'PolylineM\', \'PolylineZ\'):\r\n        return (kind, 1)\r\n    if kind in (\'Point\', \'Multipoint\'):\r\n        return (kind, 0)\r\n\r\n\r\ndef fc_composition(in_fc, SR=None, prn=True, start=0, end=50):\r\n    """"""Featureclass geometry composition in terms of shapes, shape parts, and\r\n    point counts for each part.\r\n    """"""\r\n    if SR is None:\r\n        SR = getSR(in_fc)\r\n    with arcpy.da.SearchCursor(\r\n            in_fc, [\'OID@\', \'SHAPE@\'], spatial_reference=SR) as cur:\r\n        len_lst = []\r\n        for _, row in enumerate(cur):\r\n            p_id = row[0]\r\n            p = row[1]\r\n            parts = p.partCount\r\n            num_pnts = np.asarray([p[i].count for i in range(parts)])\r\n            IDs = np.repeat(p_id, parts)\r\n            part_count = np.arange(parts)\r\n            too = np.cumsum(num_pnts)\r\n            result = np.stack((IDs, part_count, num_pnts, too), axis=-1)\r\n            len_lst.append(result)\r\n    tmp = np.concatenate(len_lst, axis=0)  # np.vstack(len_lst)\r\n    too = np.cumsum(tmp[:, 2])\r\n    frum = np.concatenate(([0], too))\r\n    frum_too = np.array(list(zip(frum, too)))\r\n    fc_comp = np.hstack((tmp[:, :3], frum_too))  # axis=0)\r\n    dt = np.dtype({\'names\': [\'IDs\', \'Part\', \'Points\', \'From_pnt\', \'To_pnt\'],\r\n                   \'formats\': [\'i4\', \'i4\', \'i4\', \'i4\', \'i4\']})\r\n    fc = uts(fc_comp, dtype=dt)\r\n    frmt = ""\\nFeatureclass...  {}"" + \\\r\n        ""\\nShapes :{:>5.0f}\\nParts  :{:>5.0f}\\n  max  :{:>5.0f}"" + \\\r\n        ""\\nPoints :{:>5.0f}\\n  min  :{:>5.0f}\\n  med  :{:>5.0f}"" + \\\r\n        ""\\n  max  :{:>5.0f}""\r\n    if prn:  # \':>{}.0f\r\n        uni, cnts = np.unique(fc[\'IDs\'], return_counts=True)\r\n        a0, a1 = [fc[\'Part\'] + 1, fc[\'Points\']]\r\n        args = [in_fc, len(uni), np.sum(cnts), np.max(a0),\r\n                np.sum(a1), np.min(a1), int(np.median(a1)), np.max(a1)]\r\n        msg = dedent(frmt).format(*args)\r\n        print(msg)\r\n        # ---- to structured and print\r\n        frmt = ""{:>8} ""*5\r\n        start, end = sorted([abs(int(i)) if isinstance(i, (int, float))\r\n                             else 0 for i in [start, end]])\r\n        end = min([fc.shape[0], end])\r\n        print(frmt.format(*fc.dtype.names))\r\n        for i in range(start, end):\r\n            print(frmt.format(*fc[i]))\r\n        return None\r\n    return fc\r\n\r\n\r\n# ===========================================================================\r\n# ---- Create inputs for the Geo class\r\n#  fc_data - just the data\r\n#  fc_geometry - used to create the Geo class\r\n#  fc_shapes - returns an object array (usually) of the geometry\r\n\r\ndef fc_data(in_fc):\r\n    """"""Pull all editable attributes from a featureclass tables.  During the\r\n    process, <null> values are changed to an appropriate type.\r\n\r\n    Parameters\r\n    ----------\r\n    in_fc : text\r\n        Path to the input featureclass.\r\n\r\n    Notes\r\n    -----\r\n    The output objectid and geometry fields are renamed to\r\n    `OID_`, `X_cent`, `Y_cent`, where the latter two are the centroid values.\r\n    """"""\r\n    flds = [\'OID@\', \'SHAPE@X\', \'SHAPE@Y\']\r\n    null_dict, fld_names = _make_nulls_(in_fc, include_oid=True, int_null=-999)\r\n    if flds not in fld_names:\r\n        new_names = out_flds = fld_names\r\n    if fld_names[0] == \'OID@\':\r\n        out_flds = flds + fld_names[1:]\r\n        new_names = [\'OID_\', \'X_cent\', \'Y_cent\'] + out_flds[3:]\r\n    a = arcpy.da.FeatureClassToNumPyArray(\r\n            in_fc, out_flds, skip_nulls=False, null_value=null_dict\r\n            )\r\n    a.dtype.names = new_names\r\n    return np.asarray(a)\r\n\r\n\r\ndef _convert_polytypes_(in_fc, SR):\r\n    """"""A standalone version of _polytypes_ in fc_geometry.\r\n    Convert polylines/polygons geometry to array.\r\n\r\n    >>> cur = arcpy.da.SearchCursor( in_fc, (\'OID@\', \'SHAPE@\'), None, SR)\r\n    >>> ids = [r[0] for r in cur]\r\n    >>> arrs = [[j for j in r[1]] for r in cur]\r\n    """"""\r\n    # ----\r\n    null_pnt = (np.nan, np.nan)\r\n    id_len = []\r\n    a_2d = []\r\n    with arcpy.da.SearchCursor(in_fc, (\'OID@\', \'SHAPE@\'), None, SR) as cursor:\r\n        for row in cursor:\r\n            sub = []\r\n            IDs = []\r\n            num_pnts = []\r\n            p_id = row[0]\r\n            geom = row[1]\r\n            prt_cnt = geom.partCount\r\n            for arr in geom:\r\n                pnts = [[pt.X, pt.Y] if pt else null_pnt for pt in arr]\r\n                sub.append(np.asarray(pnts))\r\n                IDs.append(p_id)\r\n                num_pnts.append(len(pnts))\r\n            part_count = np.arange(prt_cnt)\r\n            result = np.stack((IDs, part_count, num_pnts), axis=-1)\r\n            id_len.append(result)\r\n            a_2d.extend([j for i in sub for j in i])\r\n    # ----\r\n    id_len = np.concatenate(id_len, axis=0)\r\n    a_2d = np.asarray(a_2d)\r\n    return id_len, a_2d\r\n\r\n\r\ndef fc_geometry(in_fc, SR=None, IFT_rec=False, true_curves=False, deg=5):\r\n    """"""Derive, arcpy geometry objects from a FeatureClass searchcursor.\r\n\r\n    Parameters\r\n    ----------\r\n    in_fc : text\r\n        Path to the input featureclass.  Points not supported.\r\n    SR : spatial reference\r\n        Spatial reference object, name or id\r\n    deg : integer\r\n        Used to densify curves found for circles and ellipses. Values of\r\n        1, 2, 5 and 10 deg(rees) are appropriate.  No error checking\r\n    IFT_rec : boolean\r\n        Return the ``IFT`` as a structured array as well.\r\n\r\n    Returns\r\n    -------\r\n    ``a_2d, IFT`` (ids_from_to), where a_2d are the points as a 2D array,\r\n    ``IFT``represent the id numbers (which are repeated for multipart shapes),\r\n    and the from-to pairs of the feature parts.\r\n\r\n    See Also\r\n    --------\r\n    Use ``array_ift`` to produce ``Geo`` objects directly pre-existing arrays,\r\n    or arrays derived form existing arcpy poly objects which originated from\r\n    esri featureclasses.\r\n\r\n    Notes\r\n    -----\r\n    Multipoint, polylines and polygons and its variants are supported.\r\n\r\n    **Point and Multipoint featureclasses**\r\n\r\n    >>> cent = arcpy.da.FeatureClassToNumPyArray(pnt_fc,\r\n                                             [\'OID@\', \'SHAPE@X\', \'SHAPE@Y\'])\r\n\r\n    For multipoints, use\r\n\r\n    >>> allpnts = arcpy.da.FeatureClassToNumPyArray(multipnt_fc,\r\n                            [\'OID@\', \'SHAPE@X\', \'SHAPE@Y\'],\r\n                            explode_to_points=True)\r\n\r\n    **IFT array structure**\r\n\r\n    To see the ``IFT`` output as a structured array, use the following.\r\n\r\n    >>> dt = np.dtype({\'names\': [\'ID\', \'From\', \'To\'], \'formats\': [\'<i4\']*3})\r\n    >>> z = IFT.view(dtype=dt).squeeze()\r\n    >>> prn_tbl(z)  To see the output in tabular form\r\n\r\n    **Flatten geometry tests**\r\n\r\n    >>> %timeit fc_geometry(in_fc2, SR)\r\n    105 ms \xc2\xb1 1.04 ms per loop (mean \xc2\xb1 std. dev. of 7 runs, 10 loops each)\r\n    ...\r\n    >>> cur = arcpy.da.SearchCursor(in_fc, \'SHAPE@\', None, SR)\r\n    >>> polys = [row[0] for row in cur]\r\n    >>> pts = [[(i.X, i.Y) if i else (np.nan, np.nan)\r\n                for i in itertools.chain.from_iterable(shp)]\r\n                for shp in polys]\r\n    7.28 ms \xc2\xb1 105 \xc2\xb5s per loop (mean \xc2\xb1 std. dev. of 7 runs, 100 loops each)\r\n    """"""\r\n    msg = """"""\r\n    Use arcpy.FeatureClassToNumPyArray for Point files.\r\n    MultiPoint, Polyline and Polygons and its variants are supported.\r\n    """"""\r\n\r\n    def _multipnt_(in_fc, SR):\r\n        """"""Convert multipoint geometry to array""""""\r\n        pnts = arcpy.da.FeatureClassToNumPyArray(\r\n                in_fc, [\'OID@\', \'SHAPE@X\', \'SHAPE@Y\'], spatial_reference=SR,\r\n                explode_to_points=True\r\n                )\r\n        id_len = np.vstack(np.unique(pnts[\'OID@\'], return_counts=True)).T\r\n        a_2d = stu(pnts[[\'SHAPE@X\', \'SHAPE@Y\']])  # ---- use ``stu`` to convert\r\n        return id_len, a_2d\r\n\r\n    def _polytypes_(in_fc, SR, true_curves, deg):\r\n        """"""Convert polylines/polygons geometry to array.\r\n\r\n        >>> cur = arcpy.da.SearchCursor( in_fc, (\'OID@\', \'SHAPE@\'), None, SR)\r\n        >>> ids = [r[0] for r in cur]\r\n        >>> arrs = [[j for j in r[1]] for r in cur]\r\n        """"""\r\n        def _densify_curves_(geom, deg=deg):\r\n            """"""Densify geometry for circle and ellipse (geom) at ``deg`` degree\r\n            increments. deg, angle = (1, 361), (2, 181), (5, 73)\r\n            """"""\r\n            if \'curve\' in geom.JSON:\r\n                return geom.densify(\'ANGLE\', 1, np.deg2rad(deg))\r\n            return geom\r\n        # ----\r\n        null_pnt = (np.nan, np.nan)\r\n        id_len = []\r\n        a_2d = []\r\n        with arcpy.da.SearchCursor(\r\n                in_fc, (\'OID@\', \'SHAPE@\'), None, SR) as cursor:\r\n            for row in cursor:\r\n                sub = []\r\n                IDs = []\r\n                num_pnts = []\r\n                p_id = row[0]\r\n                geom = row[1]\r\n                prt_cnt = geom.partCount\r\n                if true_curves:\r\n                    p_num = geom.pointCount  # ---- added\r\n                    if (prt_cnt == 1) and (p_num <= 4):\r\n                        geom = _densify_curves_(geom, deg=deg)\r\n                for arr in geom:\r\n                    pnts = [[pt.X, pt.Y] if pt else null_pnt for pt in arr]\r\n                    sub.append(np.asarray(pnts))\r\n                    IDs.append(p_id)\r\n                    num_pnts.append(len(pnts))\r\n                part_count = np.arange(prt_cnt)\r\n                result = np.stack((IDs, part_count, num_pnts), axis=-1)\r\n                id_len.append(result)\r\n                a_2d.extend([j for i in sub for j in i])\r\n        # ----\r\n        id_len = np.concatenate(id_len, axis=0)\r\n        a_2d = np.asarray(a_2d)\r\n        return id_len, a_2d\r\n    #\r\n    # ---- Check and process section ----------------------------------------\r\n    desc = arcpy.da.Describe(in_fc)\r\n    fc_kind = desc[\'shapeType\']\r\n    SR = desc[\'spatialReference\']\r\n    if fc_kind == ""Point"":\r\n        print(dedent(msg))\r\n        return None, None\r\n    if fc_kind == ""Multipoint"":\r\n        id_len, a_2d = _multipnt_(in_fc, SR)\r\n    else:\r\n        id_len, a_2d = _polytypes_(in_fc, SR, true_curves, deg)\r\n    # ---- Return and send out\r\n    ids = id_len[:, 0]\r\n    too = np.cumsum(id_len[:, 2])\r\n    frum = np.concatenate(([0], too))\r\n    from_to = np.concatenate((frum[:-1, None], too[:, None]), axis=1)\r\n    IFT = np.concatenate((ids[:, None], from_to), axis=1)\r\n    if IFT_rec:\r\n        id_len2 = np.concatenate((id_len, IFT[:, 1:]), axis=1)\r\n        dt = np.dtype(\r\n                {\'names\': [\'IDs\', \'Part\', \'Points\', \'From_pnt\', \'To_pnt\'],\r\n                 \'formats\': [\'i4\', \'i4\', \'i4\', \'i4\', \'i4\']})\r\n        IFT_2 = uts(id_len2, dtype=dt)\r\n        return a_2d, IFT, IFT_2\r\n    return a_2d, IFT\r\n\r\n\r\ndef fc_shapes(in_fc, SR=None):\r\n    """"""Featureclass to arcpy shapes.  Returns polygon, polyline, multipoint,\r\n    or points.\r\n    """"""\r\n    if SR is None:\r\n        SR = getSR(in_fc)\r\n    with arcpy.da.SearchCursor(in_fc, \'SHAPE@\', spatial_reference=SR) as cur:\r\n        out = [row[0] for row in cur]\r\n    return out\r\n\r\n\r\n# ===========================================================================\r\n# ---- back to featureclass\r\n#\r\ndef array_poly(a, p_type=None, sr=None, IFT=None):\r\n    """"""\r\n    Used by ``geometry_fc`` to assemble the poly features from array(s).\r\n    This can be used separately.\r\n\r\n    Parameters\r\n    ----------\r\n    a : array\r\n        Points array.\r\n    p_type : text\r\n        POLYGON or POLYLINE\r\n    sr : spatial reference\r\n        Spatial reference object, name or id.\r\n    IFT : array\r\n        An Nx3 array consisting of I(d)F(rom)T(o) points.\r\n\r\n    Notes\r\n    -----\r\n    Polyline or polygon features can be created from the array data.  The\r\n    features can be multipart with or without interior rings.\r\n\r\n    Outer rings are ordered clockwise, inner rings (holes) are ordered\r\n    counterclockwise.  For polylines, there is no concept of order.\r\n    Splitting is modelled after _nan_split_(arr).\r\n    """"""\r\n    def _arr_poly_(arr, SR, as_type):\r\n        """"""Slices the array where nan values appear, splitting them off during\r\n        the process.\r\n        """"""\r\n        subs = []\r\n        s = np.isnan(arr[:, 0])\r\n        if np.any(s):\r\n            w = np.where(s)[0]\r\n            ss = np.split(arr, w)\r\n            subs = [ss[0]]\r\n            subs.extend(i[1:] for i in ss[1:])\r\n        else:\r\n            subs.append(arr)\r\n        aa = []\r\n        for sub in subs:\r\n            aa.append([arcpy.Point(*pairs) for pairs in sub])\r\n        if as_type.upper() == \'POLYGON\':\r\n            poly = arcpy.Polygon(arcpy.Array(aa), SR)\r\n        elif as_type.upper() == \'POLYLINE\':\r\n            poly = arcpy.Polyline(arcpy.Array(aa), SR)\r\n        return poly\r\n    # ----\r\n    ids = IFT[:, 0]\r\n    from_to = IFT[:, 1:]\r\n    chunks = [a[f:t] for f, t in from_to]  # ---- _poly_pieces_ chunks input\r\n    polys = []\r\n    for i in chunks:\r\n        p = _arr_poly_(i, sr, p_type)  # ---- _arr_poly_ makes parts of chunks\r\n        polys.append(p)\r\n    out = list(zip(polys, ids))\r\n    return out\r\n\r\n\r\ndef geometry_fc(a, IFT, p_type=None, gdb=None, fname=None, sr=None):\r\n    """"""Reform poly features from the list of arrays created by ``fc_geometry``.\r\n\r\n    Parameters\r\n    ----------\r\n    a : array or list of arrays\r\n        Some can be object arrays, normally created by ``pnts_arr``\r\n    IFT : list/array\r\n        Identifies which feature each input belongs to.  This enables one to\r\n        account for multipart shapes\r\n    p_type : string\r\n        Uppercase geometry type eg POLYGON.\r\n    gdb : text\r\n        Geodatabase path and name.\r\n    fname : text\r\n        Featureclass name.\r\n    sr : spatial reference\r\n        name or object\r\n\r\n    Returns\r\n    -------\r\n    Singlepart and/or multipart featureclasses.\r\n\r\n    Notes\r\n    -----\r\n    The work is done by ``array_poly``.\r\n    """"""\r\n    if p_type is None:\r\n        p_type = ""POLYGON""\r\n    out = array_poly(a, p_type.upper(), sr=sr, IFT=IFT)   # call array_poly\r\n    name = gdb + ""/"" + fname\r\n    wkspace = arcpy.env.workspace = \'memory\'  # legacy is in_memory\r\n    arcpy.management.CreateFeatureclass(wkspace, fname, p_type,\r\n                                        spatial_reference=sr)\r\n    arcpy.management.AddField(fname, \'ID_arr\', \'LONG\')\r\n    with arcpy.da.InsertCursor(fname, [\'SHAPE@\', \'ID_arr\']) as cur:\r\n        for row in out:\r\n            cur.insertRow(row)\r\n    arcpy.management.CopyFeatures(fname, name)\r\n    return\r\n\r\n\r\n#\r\n# ============================================================================\r\n# ---- array dependent\r\ndef prn_q(a, edges=3, max_lines=25, width=120, decimals=2):\r\n    """"""Format a structured array by setting the width so it hopefully wraps.\r\n    """"""\r\n    width = min(len(str(a[0])), width)\r\n    with np.printoptions(edgeitems=edges, threshold=max_lines, linewidth=width,\r\n                         precision=decimals, suppress=True, nanstr=\'-n-\'):\r\n        print(""\\nArray fields/values...:"")\r\n        print(""  "".join([n for n in a.dtype.names]))\r\n        print(a)\r\n\r\n\r\n# ---- printing based on arraytools.frmts.py using prn_rec and dependencies\r\n#\r\ndef _check(a):\r\n    """"""Check dtype and max value for formatting information""""""\r\n    return a.shape, a.ndim, a.dtype.kind, np.nanmin(a), np.nanmax(a)\r\n\r\n\r\ndef prn_tbl(a, rows_m=20, names=None, deci=2, width=100):\r\n    """"""Format a structured array with a mixed dtype.  Derived from\r\n    arraytools.frmts and the prn_rec function therein.\r\n\r\n    Parameters\r\n    ----------\r\n    a : array\r\n        A structured/recarray\r\n    rows_m : integer\r\n        The maximum number of rows to print.  If rows_m=10, the top 5 and\r\n        bottom 5 will be printed.\r\n    names : list/tuple or None\r\n        Column names to print, or all if None.\r\n    deci : int\r\n        The number of decimal places to print for all floating point columns.\r\n    width : int\r\n        Print width in characters\r\n    """"""\r\n    def _ckw_(a, name, deci):\r\n        """"""array `a` c(olumns) k(ind) and w(idth)""""""\r\n        c_kind = a.dtype.kind\r\n        if (c_kind in FLOATS) and (deci != 0):  # float with decimals\r\n            c_max, c_min = np.round([np.nanmin(a), np.nanmax(a)], deci)\r\n            c_width = len(max(str(c_min), str(c_max), key=len))\r\n        elif c_kind in NUMS:      # int, unsigned int, float wih no decimals\r\n            c_width = len(max(str(np.nanmin(a)), str(np.nanmax(a)), key=len))\r\n        elif c_kind in (\'U\', \'S\', \'s\'):\r\n            c_width = len(max(a, key=len))\r\n        else:\r\n            c_width = len(str(a))\r\n        c_width = max(len(name), c_width) + deci\r\n        return [c_kind, c_width]\r\n\r\n    def _col_format(pairs, deci):\r\n        """"""Assemble the column format""""""\r\n        form_width = []\r\n        dts = []\r\n        for c_kind, c_width in pairs:\r\n            if c_kind in INTS:  # ---- integer type\r\n                c_format = \':>{}.0f\'.format(c_width)\r\n            elif c_kind in FLOATS:  # and np.isscalar(c[0]):  # float rounded\r\n                c_format = \':>{}.{}f\'.format(c_width, deci)\r\n            else:\r\n                c_format = ""!s:<{}"".format(c_width)\r\n            dts.append(c_format)\r\n            form_width.append(c_width)\r\n        return dts, form_width\r\n    # ----\r\n    dtype_names = a.dtype.names\r\n    if dtype_names is None:\r\n        print(""Structured/recarray required"")\r\n        return None\r\n    if names is None:\r\n        names = dtype_names\r\n    # ---- slice off excess rows, stack upper and lower slice using rows_m\r\n    if a.shape[0] > rows_m*2:\r\n        a = np.hstack((a[:rows_m], a[-rows_m:]))\r\n    # ---- get the column formats from ... _ckw_ and _col_format ----\r\n    pairs = [_ckw_(a[name], name, deci) for name in names]  # -- column info\r\n    dts, wdths = _col_format(pairs, deci)                   # format column\r\n    # ---- slice off excess columns\r\n    c_sum = np.cumsum(wdths)               # -- determine where to slice\r\n    N = len(np.where(c_sum < width)[0])    # columns that exceed ``width``\r\n    a = a[list(names[:N])]\r\n    # ---- Assemble the formats and print\r\n    tail = [\'\', \' ...\'][N < len(names)]\r\n    row_frmt = ""  "".join([(\'{\' + i + \'}\') for i in dts[:N]])\r\n    hdr = [""!s:<"" + ""{}"".format(wdths[i]) for i in range(N)]\r\n    hdr2 = ""  "".join([""{"" + hdr[i] + ""}"" for i in range(N)])\r\n    header = "" ... "" + hdr2.format(*names[:N]) + tail\r\n    header = ""{}\\n{}"".format(header, ""-""*len(header))\r\n    txt = [header]\r\n    for idx, i in enumerate(range(a.shape[0])):\r\n        if idx == rows_m:\r\n            txt.append(""..."")\r\n        else:\r\n            t = "" {:>03.0f} "".format(idx) + row_frmt.format(*a[i]) + tail\r\n            txt.append(t)\r\n    msg = ""\\n"".join([i for i in txt])\r\n    print(msg)\r\n    # return row_frmt, hdr2  # uncomment for testing\r\n\r\n\r\ndef prn_geo(a, rows_m=100, names=None, deci=2, width=100):\r\n    """"""Format a structured array with a mixed dtype.  Derived from\r\n    arraytools.frmts and the prn_rec function therein.\r\n\r\n    Parameters\r\n    ----------\r\n    a : array\r\n        A structured/recarray.\r\n    rows_m : integer\r\n        The maximum number of rows to print.  If rows_m=10, the top 5 and\r\n        bottom 5 will be printed.\r\n    names : list/tuple or None\r\n        Column names to print, or all if None.\r\n    deci : int\r\n        The number of decimal places to print for all floating point columns.\r\n    width : int\r\n        Print width in characters.\r\n\r\n    Notes\r\n    -----\r\n    >>> toos = s0.IFT[:,2]\r\n    >>> nans = np.where(np.isnan(s0[:,0]))[0]  # array([10, 21, 31, 41]...\r\n    >>> dn = np.digitize(nans, too)            # array([1, 2, 3, 4]...\r\n    >>> ift[:, 0][dn]                          # array([1, 1, 2, 2])\r\n    >>> np.sort(np.concatenate((too, nans)))\r\n    ... array([ 5, 10, 16, 21, 26, 31, 36, 41, 48, 57, 65], dtype=int64)\r\n    """"""\r\n    def _ckw_(a, name, deci):\r\n        """"""columns `a` kind and width""""""\r\n        c_kind = a.dtype.kind\r\n        if (c_kind in FLOATS) and (deci != 0):  # float with decimals\r\n            c_max, c_min = np.round([np.nanmin(a), np.nanmax(a)], deci)\r\n            c_width = len(max(str(c_min), str(c_max), key=len))\r\n        elif c_kind in NUMS:      # int, unsigned int, float wih no decimals\r\n            c_width = len(max(str(np.nanmin(a)), str(np.nanmax(a)), key=len))\r\n        else:\r\n            c_width = len(name)\r\n        c_width = max(len(name), c_width) + deci\r\n        return [c_kind, c_width]\r\n\r\n    def _col_format(pairs, deci):\r\n        """"""Assemble the column format""""""\r\n        form_width = []\r\n        dts = []\r\n        for c_kind, c_width in pairs:\r\n            if c_kind in INTS:  # ---- integer type\r\n                c_format = \':>{}.0f\'.format(c_width)\r\n            elif c_kind in FLOATS:  # and np.isscalar(c[0]):  # float rounded\r\n                c_format = \':>{}.{}f\'.format(c_width, deci[-1])\r\n            else:\r\n                c_format = ""!s:^{}"".format(c_width)\r\n            dts.append(c_format)\r\n            form_width.append(c_width)\r\n        return dts, form_width\r\n    # ----\r\n    if names is None:\r\n        names = [\'shape\', \'part\', \'X\', \'Y\']\r\n    # ---- slice off excess rows, stack upper and lower slice using rows_m\r\n    if not hasattr(a, \'IFT\'):\r\n        print(""Requires a Geo array"")\r\n        return None\r\n    ift = a.IFT\r\n    c = [np.repeat(ift[i, 0], ift[i, 2] - ift[i, 1])\r\n         for i, p in enumerate(ift[:, 0])]\r\n    c = np.concatenate(c)\r\n    # ---- p: __ shape end, p0: x parts, p1: o start of parts, pp: concatenate\r\n    p = np.where(np.diff(c, append=0) == 1, ""___"", """")\r\n    p0 = np.where(np.isnan(a[:, 0]), ""x"", """")\r\n    p1 = np.asarray(["""" if i not in ift[:, 2] else \'o\' for i in range(len(p))])\r\n    pp = np.asarray([p[i]+p0[i]+p1[i] for i in range(len(p))])\r\n    if a.shape[0] > rows_m:\r\n        a = a[:rows_m]\r\n        c = c[:rows_m]\r\n        p = p[:rows_m]\r\n    # ---- get the column formats from ... _ckw_ and _col_format ----\r\n    deci = [0, 0, deci, deci]\r\n    flds = [c, pp, a[:, 0], a[:, 1]]\r\n    pairs = [_ckw_(flds[n], names[n], deci[n])\r\n             for n, name in enumerate(names)]  # -- column info\r\n    dts, wdths = _col_format(pairs, deci)      # format column\r\n    # ---- slice off excess columns\r\n    c_sum = np.cumsum(wdths)               # -- determine where to slice the\r\n    N = len(np.where(c_sum < width)[0])    # columns that exceed ``width``\r\n    # ---- Assemble the formats and print\r\n    row_frmt = "" {:>03.0f} "" + ""  "".join([(\'{\' + i + \'}\') for i in dts[:N]])\r\n    hdr = [""!s:<"" + ""{}"".format(wdths[i]) for i in range(N)]\r\n    hdr2 = ""  "".join([""{"" + hdr[i] + ""}"" for i in range(N)])\r\n    header = "" pnt "" + hdr2.format(*names[:N])\r\n    header = ""\\n{}\\n{}"".format(header, ""-""*len(header))\r\n    txt = [header]\r\n    for i in range(a.shape[0]):\r\n        txt.append(row_frmt.format(i, c[i], pp[i], a[i, 0], a[i, 1]))\r\n    msg = ""\\n"".join([i for i in txt])\r\n    print(msg)\r\n    # return row_frmt, hdr2  # uncomment for testing\r\n\r\n\r\n# ==== Extras ===============================================================\r\n#\r\ndef shape_properties(a_shape, prn=True):\r\n    """"""Get some basic shape geometry properties\r\n    """"""\r\n    coords = a_shape.__geo_interface__[\'coordinates\']\r\n    sr = a_shape.spatialReference\r\n    props = [\'type\', \'isMultipart\', \'partCount\', \'pointCount\', \'area\',\r\n             \'length\', \'length3D\', \'centroid\', \'trueCentroid\', \'firstPoint\',\r\n             \'lastPoint\', \'labelPoint\']\r\n    props2 = [[\'Name\', sr.name], [\'Factory code\', sr.factoryCode]]\r\n    t = ""\\n"".join([""{!s:<12}: {}"".format(i, a_shape.__getattribute__(i))\r\n                   for i in props])\r\n    t = t + ""\\n"" + ""\\n"".join([""{!s:<12}: {}"".format(*i) for i in props2])\r\n    tc = \'{!r:}\'.format(np.array(coords))\r\n    tt = t + ""\\nCoordinates\\n"" + indent(tc, \'....\')\r\n    if prn:\r\n        print(tt)\r\n    else:\r\n        return tt\r\n\r\n\r\ndef gms(arr):\r\n    """"""Get maximum dimensions in a list/array\r\n\r\n    Returns\r\n    -------\r\n    A list with the format - [3, 2, 4, 10, 2]. Representing the maximum\r\n    expected value in each column::\r\n      [ID, parts, pieces, points, pair]\r\n    """"""\r\n    from collections import defaultdict\r\n\r\n    def get_dimensions(arr, level=0):\r\n        yield level, len(arr)\r\n        try:\r\n            for row in arr:\r\n                yield from get_dimensions(row, level + 1)\r\n        except TypeError:  # not an iterable\r\n            pass\r\n    # ----\r\n    dimensions = defaultdict(int)\r\n    for level, length in get_dimensions(arr):\r\n        dimensions[level] = max(dimensions[level], length)\r\n    return [value for _, value in sorted(dimensions.items())]\r\n\r\n\r\ndef flatten_to_points(iterable):\r\n    """"""Iteratively flattens an iterable containing potentially nested points\r\n    down to X,Y pairs with feature ID, part, subpart/ring and point number.\r\n\r\n    Requires\r\n    --------\r\n    iterable : list/array\r\n        See notes\r\n\r\n    Returns\r\n    -------\r\n    A structured array of coordinate geometry information as described by the\r\n    array dtype.\r\n\r\n    Notes\r\n    -----\r\n    `load_geojson`\'s `coords` output is suitable for input or any ndarray or\r\n    object array representing geometry coordinates.\r\n\r\n    I added a x[0] check to try to prevent the flattening of the\r\n    yield to beyond the final coordinate pair.\r\n\r\n    References\r\n    ----------\r\n    `Stefan Pochmann on flatten nested lists with indices\r\n    <https://stackoverflow.com/questions/48996063/python-flatten-nested-\r\n    lists-with-indices>`_.\r\n    """"""\r\n    def gen(iterable):\r\n        """"""Generator function to acquire the values.""""""\r\n        stack = [None, enumerate(iterable)]\r\n        pad = -1\r\n        N = 6\r\n        while stack:\r\n            for stack[-2], x in stack[-1]:\r\n                if isinstance(x[0], list):  # added [0] to check for pair\r\n                    stack += None, enumerate(x)\r\n                else:\r\n                    z = [*x, *stack[::2]]\r\n                    if len(z) < N:\r\n                        z.extend([pad]*(N-len(z)))\r\n                    yield z\r\n                break\r\n            else:\r\n                del stack[-2:]\r\n\r\n    # ----\r\n    z = list(gen(iterable))\r\n    # dt = np.dtype({\'names\': [\'Xs\', \'Ys\', \'a\', \'b\', \'c\', \'d\'],\r\n    #                \'formats\': [\'<f8\', \'<f8\', \'<i4\', \'<i4\', \'<i4\', \'<i4\']})\r\n    # z0 = np.vstack(list(z))\r\n    return z  # uts(z0, dtype=dt)\r\n\r\n\r\n# ===========================================================================\r\n# ---- main section\r\nif __name__ == ""__main__"":\r\n    """"""optional location for parameters""""""\r\n    print(""\\n{}"".format(script))\r\n\r\n""""""\r\nlists to dictionary\r\n\r\nlist1 =  [(\'84116\', 1750),(\'84116\', 1774),(\'84116\', 1783),(\'84116\',1792)]\r\nlist2 = [(\'84116\', 1783),(\'84116\', 1792),(\'84116\', 1847),(\'84116\', 1852),\r\n         (\'84116\', 1853)]\r\nLst12 = list1 + list2\r\ndt = [(\'Keys\', \'U8\'), (\'Vals\', \'<i4\')]\r\narr = np.asarray((list1 + list2), dtype=dt)\r\na0 =np.unique(arr)\r\nk = np.unique(arr[\'Keys\'])\r\n{i : a0[\'Vals\'][a0[\'Keys\'] == i].tolist() for i in k}\r\n\r\n""""""\r\n'"
Free_Tools/npg_table.py,55,"b'# -*- coding: utf-8 -*-\r\n""""""\r\n=========\r\nnpg_table\r\n=========\r\n\r\nScript :\r\n    npg_table.py\r\n\r\nAuthor :\r\n    Dan_Patterson@carleton.ca\r\n\r\nModified :\r\n    2019-10-16\r\n\r\nPurpose :\r\n    Tools for working with tabular data in the Geo class.\r\n""""""\r\n# pylint: disable=C0103  # invalid-name\r\n# pylint: disable=R0914  # Too many local variables\r\n# pylint: disable=R1710  # inconsistent-return-statements\r\n# pylint: disable=W0105  # string statement has no effect\r\n\r\nimport sys\r\nfrom textwrap import dedent\r\nimport numpy as np\r\nfrom numpy.lib.recfunctions import repack_fields\r\nfrom numpy.lib.recfunctions import append_fields\r\nfrom numpy.lib.recfunctions import structured_to_unstructured as stu\r\n# from numpy.lib.recfunctions import unstructured_to_structured as uts\r\n# from numpy.lib.recfunctions import _keep_fields\r\n\r\nimport npg_io\r\n# from npg_io import prn_tbl\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(\r\n        edgeitems=10, linewidth=80, precision=2, suppress=True, threshold=100,\r\n        formatter=ft\r\n        )\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n__all__ = [\r\n    \'_as_pivot\', \'crosstab_tbl\', \'crosstab_rc\', \'crosstab_array\',\r\n    \'calc_stats\', \'numeric_fields\', \'col_stats\',\r\n    \'group_stats\', \'find_a_in_b\', \'find_in\', \'group_sort\',\r\n    \'n_largest_vals\', \'n_smallest_vals\', \'split_sort_slice\'\r\n    ]\r\n\r\n\r\ndef _view_(a):\r\n    """"""Return a view of the array using the dtype and length\r\n\r\n    Notes\r\n    -----\r\n    The is a quick function.  The expectation is that the array contains a\r\n    uniform dtype (e.g \'f8\').  For example, coordinate values in the form\r\n    ``dtype([(\'X\', \'<f8\'), (\'Y\', \'<f8\')])`` maybe with a Z\r\n\r\n    See ``structured_to_unstructured`` in np.lib.recfunctions and the imports.\r\n    """"""\r\n    return stu(a)\r\n\r\n\r\n# ==== Crosstabulation tools =================================================\r\n# ---- fancy print/string formatter for crosstabulation and pivot\r\ndef _prn(r, c, a, stat_name=\'Total\'):\r\n    """"""Fancy print formatting.\r\n    """"""\r\n    r = r.tolist()\r\n    r.append(stat_name)\r\n    c = c.tolist()\r\n    c.append(stat_name)\r\n    r_sze = max(max([len(str(i)) for i in r]), 8)\r\n    c_sze = [max(len(str(i)), 5) for i in c]\r\n    f_0 = \'{{!s:<{}}} \'.format(r_sze)\r\n    f_1 = (\'{{!s:>{}}} \'*len(c)).format(*c_sze)\r\n    frmt = f_0 + f_1\r\n    hdr = \'Result\' + \'_\'*(r_sze-7)\r\n    txt = [frmt.format(hdr, *c)]\r\n    txt2 = txt + [frmt.format(r[i], *a[i]) for i in range(len(r))]\r\n    result = ""\\n"".join(txt2)\r\n    return result\r\n\r\n\r\ndef _as_pivot(a):\r\n    """"""Used by ``crosstab_tbl``. Present results in pivot table format.""""""\r\n    if a.dtype.fields is None:\r\n        print(""\\n...\\nStructured array with field names is required"")\r\n        return a\r\n    flds = list(a.dtype.names)\r\n    r = np.unique(a[flds[0]])\r\n    c = np.unique(a[flds[1]])\r\n    z = np.zeros((len(r)+1, len(c)+1), dtype=np.float)\r\n    rc = [[(np.where(r == i[0])[0]).item(),\r\n           (np.where(c == i[1])[0]).item()] for i in a]\r\n    for i in range(len(a)):\r\n        rr, cc = rc[i]\r\n        z[rr, cc] = a[i][2]\r\n    z[-1, :] = np.sum(z, axis=0)\r\n    z[:, -1] = np.sum(z, axis=1)\r\n    result = _prn(r, c, z, stat_name=\'Count\')\r\n    return result\r\n\r\n\r\n""""""\r\n#out_tbl is a featureclass table name\r\n#txt_file is a text file name\r\n#\r\n#    if not (out_tbl in [\'#\', \'\', None]):\r\n#        arcpy.da.NumPyArrayToTable(ctab, out_tbl)\r\n#    if not (txt_file in [\'#\', \'\', None]):\r\n#        with open(txt_file, \'w\') as f:\r\n#            f.write(""Crosstab for ... {}\\n"".format(in_tbl))\r\n#            f.write(result)\r\n""""""\r\n\r\n\r\n# (1) from featureclass table\r\ndef crosstab_tbl(in_tbl, flds=None, as_pivot=True):\r\n    """"""Derive the unique attributes in a table for all or selected fields.\r\n\r\n    Parameters\r\n    ----------\r\n    in_tbl : table\r\n        A featureclass or its table.\r\n    flds : fields\r\n        If None, then all fields in the table are used.\r\n        Make sure that you do not include sequential id fields or all table\r\n        records will be returned.\r\n\r\n    Notes\r\n    -----\r\n    None or <null> values in tables are converted to proper nodata values\r\n    depending on the field type.  This is handled by the call to fc_data which\r\n    uses _make_nulls_ to do the work.\r\n    """"""\r\n    a = npg_io.fc_data(in_tbl)\r\n    if flds is None:\r\n        flds = list(a.dtype.names)\r\n    uni, idx, cnts = np.unique(a[flds], True, False, True)\r\n    out_arr = append_fields(uni, ""Counts"", cnts, usemask=False)\r\n    if as_pivot:\r\n        return _as_pivot(out_arr)\r\n    return out_arr\r\n\r\n\r\n# (2) from two, 1D numpy ndarrays\r\ndef crosstab_rc(row, col, reclassed=False):\r\n    """"""Crosstabulate 2 data arrays, shape (N,), using np.unique.\r\n    scipy.sparse has similar functionality and is faster for large arrays.\r\n\r\n    Parameters\r\n    ----------\r\n    row, col : text\r\n        row and column array/field\r\n\r\n    Returns\r\n    -------\r\n    ctab : the crosstabulation result as row, col, count array\r\n    rc_ : similar to above, but the row/col unique pairs are combined.\r\n    """"""\r\n    dt = np.dtype([(\'row\', row.dtype), (\'col\', col.dtype)])\r\n    rc_zip = list(zip(row, col))\r\n    rc = np.asarray(rc_zip, dtype=dt)\r\n    u, idx, cnts = np.unique(rc, return_index=True, return_counts=True)\r\n    rcc_dt = u.dtype.descr\r\n    rcc_dt.append((\'Count\', \'<i4\'))\r\n    ctab = np.asarray(list(zip(u[\'row\'], u[\'col\'], cnts)), dtype=rcc_dt)\r\n    # ----\r\n    if reclassed:\r\n        rc2 = np.array([""{}_{}"".format(*i) for i in rc_zip])\r\n        u2, idx2, cnts2 = np.unique(rc2, return_index=True, return_counts=True)\r\n        dt = [(\'r_c\', u2.dtype.str), (\'cnts\', \'<i4\')]\r\n        rc_ = np.array(list(zip(u2, cnts2)), dtype=dt)\r\n        return rc_\r\n    return ctab\r\n\r\n\r\n# (3) from a structured array\r\ndef crosstab_array(a, flds=None):\r\n    """"""Frequency and crosstabulation for structured arrays.\r\n\r\n    Parameters\r\n    ----------\r\n    a : array\r\n       Input structured array.\r\n    flds : string or list\r\n       Fields/columns to use in the analysis.  For a single column, a string\r\n       is all that is needed.  Multiple columns require a list of field names.\r\n\r\n    Notes\r\n    -----\r\n    (1) Slice the input array by the classification fields.\r\n    (2) Sort the sliced array using the flds as sorting keys.\r\n    (3) Use unique on the sorted array to return the results.\r\n    (4) Reassemble the original columns and the new count data.\r\n    """"""\r\n    if flds is None:\r\n        return None\r\n    if isinstance(flds, (str)):\r\n        flds = [flds]\r\n    a = repack_fields(a[flds])  # need to repack fields\r\n    # a = _keep_fields(a, flds)  # alternative to repack_fields\r\n    idx = np.argsort(a, axis=0, order=flds)  # (2) sort\r\n    a_sort = a[idx]\r\n    uni, cnts = np.unique(a_sort, return_counts=True)  # (3) unique, count\r\n    dt = uni.dtype.descr\r\n    dt.append((\'Count\', \'<i4\'))\r\n    fr = np.empty_like(uni, dtype=dt)\r\n    names = fr.dtype.names\r\n    vals = list(zip(*uni)) + [cnts.tolist()]  # (4) reassemble\r\n    N = len(names)\r\n    for i in range(N):\r\n        fr[names[i]] = vals[i]\r\n    return fr\r\n\r\n\r\n# ==== Summarize tools ======================================================\r\n# (4) pivot table from 3 numpy ndarrays\r\ndef calc_stats(arr, axis=None, deci=4):\r\n    """"""Calculate stats for an array of number types, with nodata (nan, None)\r\n    in the column.\r\n\r\n    Notes\r\n    -----\r\n    See the args tuple for examples of nan functions.\r\n\r\n    >>> np.nansum(b, axis=0)   # by column\r\n    >>> np.nansum(b, axis=1)   # by row\r\n    >>> c_nan = np.count_nonzero(~np.isnan(b), axis=0) count nan if needed\r\n\r\n    [1, 0][True]  # ax = [1, 0][colwise]  colwise= True\r\n    """"""\r\n    if (axis is None) and (len(arr.shape) == 1):\r\n        ax = 0\r\n    else:\r\n        ax = axis\r\n    #\r\n    kind = arr.dtype.kind\r\n    arr_dt = arr.dtype\r\n    if kind == \'i\':\r\n        nulls = [np.iinfo(arr_dt).min, np.iinfo(arr_dt).max]\r\n    elif kind == \'f\':\r\n        nulls = [np.nan, np.finfo(arr_dt).min, np.finfo(arr_dt).max]\r\n    elif kind in (\'U\', \'S\'):\r\n        return None\r\n    #\r\n    nin = ~np.isin(arr, nulls)  # nin... Not In Nulls\r\n    a = arr[nin]\r\n    if len(arr.shape) > 1:\r\n        a = a.reshape(arr.shape)\r\n    mask = np.isnan(arr)\r\n    N = len(a)\r\n    cnt = np.sum(~mask, axis=ax, dtype=np.intp, keepdims=False)\r\n    n_sum = np.nansum(a, axis=ax)\r\n    n_min = np.nanmin(a, axis=ax)\r\n    n_max = np.nanmax(a, axis=ax)\r\n    n_mean = np.nanmean(a, axis=ax)\r\n    n_med = np.nanmedian(a, axis=ax)\r\n    n_std = np.nanstd(a, axis=ax)\r\n    n_var = np.nanvar(a, axis=ax)\r\n    s = [N, N-cnt, n_sum, n_min, n_max, n_mean, n_med, n_std, n_var]\r\n    s = [np.around(i, deci) for i in s]\r\n    return s\r\n\r\n\r\ndef numeric_fields(a, fields):\r\n    """"""Determine numeric fields in a structured/recarray.\r\n    """"""\r\n    num_flds = []\r\n    dt_names = a.dtype.names\r\n    dt_kind = a.dtype.kind\r\n    if fields is None:\r\n        if dt_names is None:\r\n            if dt_kind not in (\'i\', \'f\'):\r\n                return None\r\n        elif dt_kind in [\'V\']:\r\n            num_flds = [i for i in dt_names if a[i].dtype.kind in (\'i\', \'f\')]\r\n        else:\r\n            a = a.ravel()\r\n    elif isinstance(fields, (str)):\r\n        if a[fields].dtype.kind in (\'i\', \'f\'):\r\n            num_flds = fields\r\n    else:\r\n        num_flds = [i for i in fields if a[i].dtype.kind in (\'i\', \'f\')]\r\n    return num_flds\r\n\r\n\r\ndef col_stats(a, fields=None, deci=2, verbose=False):\r\n    """"""Calculate statistics for a structured/recarray with or without specified\r\n    fields.  Efforts have been made to check for all possible scenarios, but\r\n    human intelligence should prevail when one decides what to throw at it.\r\n\r\n    Parameters\r\n    ----------\r\n    a : array\r\n        A structured/recarray.\r\n    fields : list, string or None\r\n      - None, checks all fields or assumes that the input array is a singleton.\r\n      - String, a single field name, if the column names are known.\r\n      - List,  a list of field names.\r\n    deci : integer\r\n        An attempt to format floats with deci(mal) places.\r\n\r\n    Requires\r\n    --------\r\n    _numeric_fields_ : function\r\n        Returns the numeric fields in a structured/recarray.\r\n    _calc_stats : function\r\n        Performs the actual field calculations.\r\n    """"""\r\n    if isinstance(fields, str):\r\n        fields = [fields]\r\n    num_flds = numeric_fields(a, fields)\r\n    # ---- made it thus far\r\n    if len(num_flds) == 0:\r\n        num_flds = [\'array\']\r\n        s_lst = [calc_stats(a.ravel(), axis=None, deci=deci)]\r\n    else:\r\n        s_lst = [calc_stats(a[fld], deci=deci) for fld in num_flds]\r\n    #\r\n    dts = [(\'Statistic\', \'U10\')] + [(i, \'<f8\') for i in num_flds]\r\n    col_names = np.array([\'N (size)\', \'n (nans)\', \'sum\', \'min\', \'max\', \'mean\',\r\n                          \'median\', \'std\', \'var\'])\r\n    z = np.zeros((len(col_names),), dtype=dts)\r\n    z[\'Statistic\'] = col_names\r\n    N = len(num_flds)\r\n    for i in range(N):\r\n        fld = num_flds[i]\r\n        z[fld] = s_lst[i]\r\n    if verbose:\r\n        args = [""=""*25, ""Numeric fields""]\r\n        print(""\\n{}\\nStatistics for... a\\n{!s:>32}"".format(*args))\r\n        npg_io.prn_tbl(z)\r\n    return z\r\n\r\n\r\ndef group_stats(a, case_fld=None, num_flds=None, deci=2, verbose=False):\r\n    """"""Group column statistics.\r\n\r\n    Parameters\r\n    ----------\r\n    a : structured/recarray\r\n        Make sure that you know the field names in advance.\r\n    case_fld : string, list\r\n        String, summarized by the unique values in the case_fld.\r\n        List, to further fine-tune the selection or crosstabulation.\r\n    num_flds : string, list\r\n        You can limit the input fields accordingly, if you only need a few\r\n        know numeric fields.\r\n\r\n    Requires\r\n    --------\r\n    col_stats : function ... which requires\r\n      : _numeric_fields_ : function\r\n          returns the numeric fields in a structured/recarray\r\n      : _calc_stats : function\r\n          performs the actual field calculations\r\n    """"""\r\n    results = []\r\n    uniq, counts = np.unique(a[case_fld], return_counts=True)\r\n    n = len(uniq)\r\n    for i in range(n):\r\n        u = uniq[i]\r\n        if counts[i] >= 3:\r\n            sub = a[a[case_fld] == u]\r\n            z = col_stats(sub, fields=num_flds, deci=deci)\r\n            if verbose:\r\n                args = [""=""*25, u, ""Numeric fields""]\r\n                print(""\\n{}\\nStatistics for... a[{}]\\n{!s:>32}"".format(*args))\r\n                npg_io.prn_tbl(z)\r\n            results.append([u, z])\r\n        else:\r\n            print(""\\nToo few cases... ({}) for a[{}]..."".format(counts[i], u))\r\n    return results\r\n\r\n\r\ndef find_a_in_b(a, b, a_fields=None, b_fields=None):\r\n    """"""Find the indices of the elements in a smaller 2d array contained in\r\n    a larger 2d array. If the arrays are stuctured with field names,then these\r\n    need to be specified.  It should go without saying that the dtypes need to\r\n    be the same.\r\n\r\n    Parameters\r\n    ----------\r\n    a, b : 1D or 2D, ndarray or structured/record arrays\r\n        The arrays are arranged so that `a` is the smallest and `b` is the\r\n        largest.  If the arrays are stuctured with field names, then these\r\n        need to be specified.  It should go without saying that the dtypes\r\n        need to be the same.\r\n    a_fields, b_fields : list of field names\r\n        If the dtype has names, specify these in a list.  Both do not need\r\n        names.\r\n\r\n    Examples\r\n    --------\r\n    To demonstrate, a small array was made from the last 10 records of a larger\r\n    array to check that they could be found.\r\n\r\n    >>> a.dtype # ([(\'ID\', \'<i4\'), (\'X\', \'<f8\'), (\'Y\', \'<f8\'), (\'Z\', \'<f8\')])\r\n    >>> b.dtype # ([(\'X\', \'<f8\'), (\'Y\', \'<f8\')])\r\n    >>> a.shape, b.shape # ((69688,), (10,))\r\n    >>> find_a_in_b(a, b, flds, flds)\r\n    array([69678, 69679, 69680, 69681, 69682,\r\n           69683, 69684, 69685, 69686, 69687], dtype=int64)\r\n\r\n    References\r\n    ----------\r\n    This is a function from the arraytools.tbl module.\r\n\r\n    `<https://stackoverflow.com/questions/38674027/find-the-row-indexes-of-\r\n    several-values-in-a-numpy-array/38674038#38674038>`_.\r\n    """"""\r\n    def _view_(a):\r\n        """"""from the same name in arraytools""""""\r\n        return a.view((a.dtype[0], len(a.dtype.names)))\r\n    #\r\n    small, big = [a, b]\r\n    if a.size > b.size:\r\n        small, big = [b, a]\r\n    if a_fields is not None:\r\n        small = small[a_fields]\r\n        small = _view_(small)\r\n    if b_fields is not None:\r\n        big = big[b_fields]\r\n        big = _view_(big)\r\n    if a.ndim >= 1:  # last slice, if  [:2] instead, it returns both indices\r\n        indices = np.where((big == small[:, None]).all(-1))[1]\r\n    return indices\r\n\r\n\r\ndef find_in(a, col, what, where=\'in\', any_case=True, pull=\'all\'):\r\n    """"""Query a recarray/structured array for values\r\n\r\n    Parameters\r\n    ----------\r\n    a : recarray/structured array\r\n        Only text columns can be queried\r\n    col : column/field to query\r\n        Only 1 field can be queried at a time for the condition.\r\n    what : string or number\r\n        The query.  If a number, the field is temporarily converted to a\r\n        text representation for the query.\r\n    where : string\r\n        s, i, eq, en  st(arts with), in, eq(ual), en(ds with)\r\n    any_case : boolean\r\n        True, will find records regardless of ``case``, applies to text fields\r\n    extract : text or list\r\n        - `all`,  extracts all records where the column case is found\r\n        - `list`, extracts the records for only those fields in the list\r\n    Example\r\n    -------\r\n    >>> find_text(a, col=`FULLNAME`, what=`ABBEY`, pull=a.dtype.names[:2])\r\n    """"""\r\n    # ---- error checking section ----\r\n    e0 = """"""\r\n    Query error: You provided...\r\n    dtype: {}  col: {} what: {}  where: {}  any_case: {}  extract: {}\r\n    Required...\\n{}\r\n    """"""\r\n    if a is None:\r\n        return a\r\n    err1 = ""\\nField not found:\\nQuery fields: {}\\nArray fields: {}""\r\n    errors = [a.dtype.names is None,\r\n              col is None, what is None,\r\n              where.lower()[:2] not in (\'en\', \'eq\', \'in\', \'st\'),\r\n              col not in a.dtype.names]\r\n    if sum(errors) > 0:\r\n        arg = [a.dtype.kind, col, what, where, any_case, pull, find_in.__doc__]\r\n        print(dedent(e0).format(*arg))\r\n        return None\r\n    if isinstance(pull, (list, tuple)):\r\n        names = a.dtype.names\r\n        r = [i in names for i in pull]\r\n        if sum(r) != len(r):\r\n            print(err1.format(pull, names))\r\n            return None\r\n    # ---- query section\r\n    # convert column values and query to lowercase, if text, then query\r\n    c = a[col]\r\n    if c.dtype.kind in (\'i\', \'f\', \'c\'):\r\n        c = c.astype(\'U\')\r\n        what = str(what)\r\n    elif any_case:\r\n        c = np.char.lower(c)\r\n        what = what.lower()\r\n    where = where.lower()[0]\r\n    if where == \'i\':\r\n        q = np.char.find(c, what) >= 0   # ---- is in query ----\r\n    elif where == \'s\':\r\n        q = np.char.startswith(c, what)  # ---- startswith query ----\r\n    elif where == \'eq\':\r\n        q = np.char.equal(c, what)\r\n    elif where == \'en\':\r\n        q = np.char.endswith(c, what)    # ---- endswith query ----\r\n    if q.sum() == 0:\r\n        print(""none found"")\r\n        return None\r\n    if pull == \'all\':\r\n        return a[q]\r\n    pull = np.unique([col] + list(pull))\r\n    return a[q][pull]\r\n\r\n\r\ndef group_sort(a, group_fld, sort_fld, ascend=True):\r\n    """"""Group records in an structured array and sort on the sort_field.  The\r\n    order of the grouping field will be in ascending order, but the order of\r\n    the sort_fld can sort internally within the group.\r\n\r\n    Parameters\r\n    ----------\r\n    a : structured/recarray\r\n        Array must have field names to enable splitting on and sorting by\r\n    group_fld : text\r\n        The field/name in the dtype used to identify groupings of features\r\n    sort_fld : text\r\n        As above, but this field contains the numeric values that you want to\r\n        sort by.\r\n    ascend : boolean\r\n        **True**, sorts in ascending order, so you can slice for the lowest\r\n        `num` records. **False**, sorts in descending order if you want to\r\n        slice the top `num` records\r\n\r\n    Example\r\n    -------\r\n    >>> fn = ""C:/Git_Dan/arraytools/Data/pnts_in_poly.npy""\r\n    >>> a = np.load(fn)\r\n    >>> out = _split_sort_slice_(a, split_fld=\'Grid_codes\', val_fld=\'Norm\')\r\n    >>> arcpy.da.NumPyArrayToFeatureClass(out, out_fc, [\'Xs\', \'Ys\'], \'2951\')\r\n\r\n    References\r\n    ----------\r\n    `<https://community.esri.com/blogs/dan_patterson/2019/01/29/split-sort-\r\n    slice-the-top-x-in-y>`_.\r\n\r\n    `<https://community.esri.com/thread/227915-how-to-extract-top-five-max-\r\n    points>`_\r\n    """"""\r\n    ordered = split_sort_slice(a, split_fld=group_fld, order_fld=sort_fld)\r\n    final = []\r\n    if ascend:\r\n        for r in ordered:\r\n            final.extend(r)\r\n    else:\r\n        for r in ordered:\r\n            r = r[::-1]\r\n            final.extend(r)\r\n    return np.asarray(final)\r\n\r\n\r\ndef n_largest_vals(a, group_fld=None, val_fld=None, num=1):\r\n    """"""Run `split_sort_slice` to get the N largest values in the array.\r\n    """"""\r\n    ordered = split_sort_slice(a, split_fld=group_fld, order_fld=val_fld)\r\n    final = []\r\n    for r in ordered:\r\n        r = r[::-1]\r\n        num = min(num, r.size)\r\n        final.extend(r[:num])\r\n    return np.asarray(final)\r\n\r\n\r\ndef n_smallest_vals(a, group_fld=None, val_fld=None, num=1):\r\n    """"""Run `split_sort_slice` to get the N smallest values in the array.\r\n    """"""\r\n    ordered = split_sort_slice(a, split_fld=group_fld, order_fld=val_fld)\r\n    final = []\r\n    for r in ordered:\r\n        num = min(num, r.size)\r\n        final.extend(r[:num])\r\n    return np.asarray(final)\r\n\r\n\r\ndef split_sort_slice(a, split_fld=None, order_fld=None):\r\n    """"""Split a structured array into groups of common values based on the\r\n    split_fld, key field.  Once the array is split, the array is sorted on a\r\n    val_fld and sliced for the largest or smallest `num` records.\r\n\r\n    See Also\r\n    --------\r\n    Documentation is shown in `group_sort`\r\n\r\n    """"""\r\n    def _split_(a, fld):\r\n        """"""split unsorted array""""""\r\n        out = []\r\n        uni, _ = np.unique(a[fld], True)\r\n        for _, j in enumerate(uni):\r\n            key = (a[fld] == j)\r\n            out.append(a[key])\r\n        return out\r\n    #\r\n    err_0 = """"""\r\n    A structured/recarray with a split_field and a order_fld is required.\r\n    You provided\\n    array type  : {}""""""\r\n    err_1 = """"""\r\n    split_field : {}\r\n    order field : {}\r\n    """"""\r\n    if a.dtype.names is None:\r\n        print(err_0.format(type(a)))\r\n        return a\r\n    check = sum([i in a.dtype.names for i in [split_fld, order_fld]])\r\n    if check != 2:\r\n        print((err_0 + err_1).format(type(a), split_fld, order_fld))\r\n        return a\r\n    #\r\n    subs = _split_(a, split_fld)\r\n    ordered = []\r\n    for _, sub in enumerate(subs):\r\n        r = sub[np.argsort(sub, order=order_fld)]\r\n        ordered.append(r)\r\n    return ordered\r\n\r\n\r\n# ==== Processing finished ====\r\n# ===========================================================================\r\n#\r\nif __name__ == ""__main__"":\r\n    """"""optional location for parameters""""""\r\n    # msg = _demo_()\r\n'"
Free_Tools/smallest_circle.py,24,"b'""""""\r\n===============\r\nsmallest_circle\r\n===============\r\n\r\nScript :\r\n    smallest_circle.py for npgeom\r\n\r\nAuthor :\r\n    Dan_Patterson@carleton.ca\r\n\r\nModified :\r\n    2019-08-30\r\n\r\nPurpose :\r\n    Returns the smallest circle enclosing a shape in the form of a center and\r\n    radius.  Original in smallestCircle.py in Bounding Containers.\r\n\r\nRequires :\r\n    Must have at least two points.\r\n\r\nReferences\r\n----------\r\n\r\nde Berg et al., Computational Geometry with Applications, Springer-Verlag.\r\n\r\nWelzl, E. (1991), Smallest enclosing disks (balls and ellipsoids),\r\nLecture Notes in Computer Science, Vol. 555, pp. 359-370.\r\n\r\n`<https://stackoverflow.com/questions/27673463/smallest-enclosing-circle-in-\r\npython-error-in-the-code>`_.\r\n\r\n>>> cent = array([ 421645.83745955, 4596388.99204294])\r\n>>> Xc, Yc, radius = 421646.74552, 4596389.82475, 24.323246\r\n\r\nmean of points :\r\n    [  421645.83745955  4596388.99204294] ## correct mean\r\ntranslated to origin :\r\n    (0.9080813432488977, 0.8327111343034483, 24.323287017466253)\r\ndirect calculation :\r\n    (421646.74554089626, 4596389.8247540779, 24.323287017466253)\r\n""""""\r\n\r\nimport numpy as np\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 8.2f}\'.format}\r\n\r\nnp.set_printoptions(\r\n        edgeitems=5,\r\n        threshold=500,\r\n        floatmode=\'maxprec\',\r\n        precision=2, suppress=True, linewidth=120,\r\n        nanstr=\'nan\', infstr=\'inf\', sign=\'-\',\r\n        formatter=ft)\r\n\r\n\r\ndef circle_mini(radius=1.0, theta=10.0, xc=0.0, yc=0.0):\r\n    """"""Produce a circle/ellipse depending on parameters.\r\n\r\n    Parameters\r\n    ----------\r\n    radius : number\r\n        Distance from centre\r\n    theta : number\r\n        Angle of densification of the shape around 360 degrees\r\n\r\n    """"""\r\n    angles = np.deg2rad(np.arange(180.0, -180.0-theta, step=-theta))\r\n    x_s = radius*np.cos(angles) + xc    # X values\r\n    y_s = radius*np.sin(angles) + yc    # Y values\r\n    pnts = np.array([x_s, y_s]).T\r\n    return pnts\r\n\r\n\r\n# ---- smallest circle implementation ----------------------------------------\r\n# helpers : farthest, center, distance\r\ndef farthest(a, check=False):\r\n    """"""Distance matrix calculation for 2D points using einsum, yielding the\r\n    two points which have the greatest distance between them.\r\n    """"""\r\n    if check:\r\n        a = np.unique(a, axis=0)\r\n    b = a.reshape(a.shape[0], 1, a.shape[-1])\r\n    diff = a - b\r\n    dist_arr = np.sqrt(np.einsum(\'ijk,ijk->ij\', diff, diff))\r\n    t_low = np.tril(dist_arr)            # np.triu(dist_arr)\r\n    dist_order = np.unique(t_low)[::-1]  # largest to smallest unique dist\r\n    mx = dist_order[0]\r\n    r, c = np.where(t_low == mx)\r\n    return a[c][0], a[r][0], dist_order\r\n\r\n\r\ndef center(p0, p1):\r\n    """"""Center point between two points.""""""\r\n    return np.mean((p0, p1), axis=0)\r\n\r\n\r\ndef distance(p0, p1):\r\n    """"""Distance between two points.""""""\r\n    return np.hypot(*(p1 - p0))\r\n\r\n\r\ndef small_circ(a):\r\n    """"""Return the minimum area bounding circle for a points array.\r\n    The ``unique`` points are used since np.unique removes reduncant calls and\r\n    sorts the points in ascending order.\r\n\r\n    Notes\r\n    -----\r\n    This incarnation uses a mix of pure python and numpy functionality where\r\n    appropriate.  A simple check is first made to see if the farthest points\r\n    enclose the point set.  If not, then the search continues to attempt to\r\n    find 2, 3 or more points that form a circle to completely enclose all the\r\n    points.\r\n    """"""\r\n    a = np.unique(a, axis=0)\r\n    N = a.shape[0]\r\n    if N <= 1:\r\n        return a[0], 0.0, a\r\n    if N == 2:\r\n        cent = center(*a[:2])\r\n        radius = distance(cent, a[0])\r\n        return cent, radius, a[:2]\r\n    # ---- corner-cases/garbage checking over\r\n    p0, p1, _ = farthest(a, check=False)\r\n    cent = center(p0, p1)\r\n    radius = distance(cent, p0)\r\n    check = np.sqrt(np.einsum(\'ij,ij->i\', a-cent, a-cent)) <= radius\r\n    if not np.all(check):  # degenerate case found\r\n        for i in range(1, N):\r\n            ptP = a[i]\r\n            if distance(cent, ptP) > radius:\r\n                prev = i - 1\r\n                cent, radius = sub_1(a, prev, ptP)\r\n    check = np.sqrt(np.einsum(\'ij,ij->i\', a-cent, a-cent)) - radius\r\n#    pnts = a[np.isclose(check, 0.)]\r\n    return cent[0], cent[1], radius  # , pnts\r\n\r\n\r\n# -------------------------------------------------------------------\r\ndef sub_1(pnts, prev, ptQ):\r\n    """"""Stage 1 check.  Calls sub_2 to complete the search.""""""\r\n    N = prev\r\n    cent = center(pnts[0], ptQ)\r\n    radius = distance(cent, ptQ)\r\n    for i in range(1, N + 1):\r\n        ptP = pnts[i]\r\n        if distance(ptP, cent) > radius:\r\n            N = i - 1\r\n            cent, radius = sub_2(pnts, N, ptQ, ptP)\r\n    return cent, radius\r\n\r\n\r\n# -------------------------------------------------------------------\r\ndef sub_2(pnts, N, ptQ, ptP):\r\n    """"""Returns the {cent, radius} for the smallest disc enclosing the points\r\n    in the list with PointR and PointQ on its boundary.\r\n    """"""\r\n    if pnts.size == 0:\r\n        pnts = np.array([[1.0, 1.0]])  # check\r\n        N = 0\r\n        ptQ = np.array([0.0, 0.0])\r\n        ptR = np.array([1.0, 0.0])\r\n    else:\r\n        ptR = ptP\r\n    cent = center(ptR, ptQ)\r\n    radius = distance(cent, ptQ)\r\n    ptO = np.array([0.0, 0.0])\r\n    ptB = ptR - ptQ\r\n    c2 = (distance(ptR, ptO)**2 - distance(ptQ, ptO)**2)/2.0\r\n    for i in range(0, N + 1):\r\n        ptP = pnts[i]\r\n        if distance(ptP, cent) > radius:\r\n            if np.all([0.0, 0.0] == ptB):\r\n                cent = center(ptP, ptQ)\r\n                radius = distance(ptQ, cent)\r\n            else:\r\n                ptA = ptQ - ptP\r\n                xDelta = ptA[0] * ptB[1] - (ptA[1] * ptB[0])\r\n                if abs(xDelta) >= 1.0e-06:  # 0.0:\r\n                    c1 = (distance(ptQ, ptO)**2 - (distance(ptP, ptO)**2))/2.0\r\n                    x = (ptB[1] * c1 - (ptA[1] * c2)) / xDelta\r\n                    y = (ptA[0] * c2 - (ptB[0] * c1)) / xDelta\r\n                    cent = [x, y]\r\n                    radius = distance(cent, ptP)\r\n    return cent, radius\r\n\r\n\r\n# ===========================================================================\r\n#\r\nif __name__ == ""__main__"":\r\n    """"""optional location for parameters""""""\r\n'"
Free_Tools/tbx_tools.py,21,"b'# -*- coding: utf-8 -*-\r\n""""""\r\n=========\r\ntbx_tools\r\n=========\r\n\r\nScript :\r\n    tbx_tools.py for npgeom\r\n\r\nAuthor :\r\n    Dan_Patterson@carleton.ca\r\n\r\nModified :\r\n    2019-10-19\r\n\r\nPurpose :\r\n    Tools for working with ``free`` ArcGIS Pro functionality\r\n\r\nNotes :\r\n\r\nReferences\r\n----------\r\n\r\n**Advanced license tools**\r\n\r\nSome of the functions that you can replicate using this data class would\r\ninclude:\r\n\r\n**Containers**\r\n\r\n`1 Bounding circles\r\n<https://pro.arcgis.com/en/pro-app/tool-reference/data-management/minimum\r\n-bounding-geometry.htm>`_.  minimum area bounding circle\r\n\r\n`2 Convex hulls\r\n<https://pro.arcgis.com/en/pro-app/tool-reference/data-management/minimum\r\n-bounding-geometry.htm>`_.\r\n\r\n`3 Feature Envelope to Polygon\r\n<https://pro.arcgis.com/en/pro-app/tool-reference/data-management/feature\r\n-envelope-to-polygon.htm>`_.  axis oriented envelope\r\n\r\n**Conversion**\r\n\r\n`1 Feature to Point\r\n<https://pro.arcgis.com/en/pro-app/tool-reference/data-management/feature\r\n-to-point.htm>`_.  centroid for point clusters, polylines or polygons\r\n\r\n`2 Polygons to Polylines\r\n<https://pro.arcgis.com/en/pro-app/tool-reference/data-management/\r\nfeature-to-polygon.htm>`_.  Simple conversion.\r\n\r\n`3 Feature Vertices to Points\r\n<https://pro.arcgis.com/en/pro-app/tool-reference/data-management/feature\r\n-vertices-to-points.htm>`_.\r\n\r\n**Alter geometry**\r\n\r\n`Shift, move, translate features\r\n<https://pro.arcgis.com/en/pro-app/tool-reference/editing/\r\ntransform-features.htm>`_.\r\n\r\n`Sort Geometry\r\n<https://pro.arcgis.com/en/pro-app/tool-reference/data-management/sort.htm>`_.\r\n\r\n`Shift features\r\n<https://pro.arcgis.com/en/pro-app/tool-reference/editing/\r\ntransform-features.htm>`_.\r\n\r\n`4 Split Line at Vertices\r\n<https://pro.arcgis.com/en/pro-app/tool-reference/data-management/\r\nsplit-line-at-vertices.htm>`_.\r\n\r\n\r\n\r\n`8 Frequency\r\n<https://pro.arcgis.com/en/pro-app/tool-reference/analysis/frequency.htm>`_.\r\n\r\n**To do**\r\n\r\n`Feature to Line\r\n<https://pro.arcgis.com/en/pro-app/tool-reference/data-management/feature\r\n-to-line.htm>`_.\r\n\r\n`Find Identical\r\n<https://pro.arcgis.com/en/pro-app/tool-reference/data-management/\r\nfind-identical.htm>`_.\r\n\r\n`Unsplit line\r\n<https://pro.arcgis.com/en/pro-app/tool-reference/data-management/\r\nunsplit-line.htm>`_.\r\n""""""\r\n# pylint: disable=C0103  # invalid-name\r\n# pylint: disable-E0611  # arcpy.da or arcgisscripting.da issue\r\n# pylint: disable=E1101  # arcpy.da issue\r\n# pylint: disable=R0914  # Too many local variables\r\n# pylint: disable=R1710  # inconsistent-return-statements\r\n# pylint: disable=W0105  # string statement has no effect\r\n\r\nimport sys\r\nfrom textwrap import dedent\r\n# import importlib\r\nimport numpy as np\r\n# from numpy.lib.recfunctions import structured_to_unstructured as stu\r\nfrom numpy.lib.recfunctions import unstructured_to_structured as uts\r\nfrom numpy.lib.recfunctions import append_fields\r\n\r\nimport arcgisscripting as ags\r\nfrom arcgisscripting import da\r\nfrom arcpy import (\r\n        env, gp, AddMessage, Exists, ImportToolbox, Delete_management,\r\n        MultipartToSinglepart_management, XYToLine_management\r\n        )\r\n\r\nfrom npg_io import (getSR, shape_K, fc_data, fc_geometry, geometry_fc)\r\n\r\nfrom npGeo import Geo, Update_Geo\r\n\r\nfrom npg_geom import _polys_to_unique_pnts_\r\n\r\n\r\nenv.overwriteOutput = True\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\n# script = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\ntool_list = [\r\n        \'Bounding Circles\', \'Convex Hulls\', \'Extent Polys\',\r\n        \'Features to Points\', \'Polygons to Polylines\', \'Vertices to Points\',\r\n        \'Split at Vertices\',\r\n        \'Shift Features\', \'Rotate Features\', \'Fill Holes\',\r\n        \'Geometry Sort\', \'Area Sort\', \'Length Sort\', \'Extent Sort\',\r\n        \'Crosstabulate\', \'Attribute sort\'\r\n        ]\r\n\r\n\r\n# ===========================================================================\r\n# ---- def section: def code blocks go here ---------------------------------\r\nmsg0 = """"""\r\n----\r\nEither you failed to specify the geodatabase location and filename properly\r\nor you had flotsam, including spaces, in the path, like...\\n\r\n  {}\\n\r\nCreate a safe path and try again...\\n\r\n`Filenames and paths in Python`\r\n<https://community.esri.com/blogs/dan_patterson/2016/08/14/filenames-and\r\n-file-paths-in-python>`_.\r\n----\r\n""""""\r\n\r\nmsg_mp_sp = """"""\r\n----\r\nMultipart shapes have been converted to singlepart, so view any data\r\ncarried over during the extendtable join as representing those from\r\nthe original data.  Recalculate values where appropriate.\r\n----\r\n""""""\r\n\r\n\r\ndef tweet(msg):\r\n    """"""Print a message for both arcpy and python.\r\n    """"""\r\n    m = ""\\n{}\\n"".format(msg)\r\n    AddMessage(m)\r\n    print(m)\r\n\r\n\r\ndef check_path(fc):\r\n    """"""\r\n    ---- check_path ----\r\n\r\n    Checks for a file geodatabase and a filename. Files and/or paths containing\r\n    `flotsam` are flagged as being invalid.\r\n\r\n    Check the geodatabase location and filename properly.  Flotsam, in the\r\n    path or name, consists of one or more of these characters...::\r\n\r\n       \\\'!""#$%&\\\'()*+,-;<=>?@[]^`{|}~  including the `space`\r\n\r\n    Create a safe path and try again...\r\n\r\n    References\r\n    ----------\r\n    `Lexical analysis\r\n    <https://docs.python.org/3/reference/lexical_analysis.html>`_.\r\n\r\n    `Filenames and paths in Python\r\n    <https://community.esri.com/blogs/dan_patterson/2016/08/14/filenames-and\r\n    -file-paths-in-python>`_.\r\n    """"""\r\n    msg = dedent(check_path.__doc__)\r\n    _punc_ = \'!""#$%&\\\'()*+,-;<=>?@[]^`~}{ \'\r\n    flotsam = "" "".join([i for i in _punc_])  # "" ... plus the `space`""\r\n    fail = False\r\n    if ("".gdb"" not in fc) or np.any([i in fc for i in flotsam]):\r\n        fail = True\r\n    pth = fc.replace(""\\\\"", ""/"").split(""/"")\r\n    name = pth[-1]\r\n    if (len(pth) == 1) or (name[-4:] == "".gdb""):\r\n        fail = True\r\n    if fail:\r\n        tweet(msg)\r\n        return (None, None)\r\n    gdb = ""/"".join(pth[:-1])\r\n    return gdb, name\r\n\r\n\r\n# ---- Container tools -------------------------------------------------------\r\n# ---- (1) bounding circles\r\n#\r\ndef circles(in_fc, gdb, name, kind):\r\n    """"""Minimum area bounding circles.  Change `angle=2` to a smaller value for\r\n    denser points on circle perimeter.\r\n    `getSR`, `shape_k`  and `fc_geometry` are from npg_io.\r\n    """"""\r\n    SR = getSR(in_fc)\r\n    kind, k = shape_K(in_fc)\r\n    tmp, IFT = fc_geometry(in_fc, SR=SR, IFT_rec=False)\r\n    m = np.nanmin(tmp, axis=0)                   # shift to LB of whole extent\r\n    info = ""bounding circles""\r\n    a = tmp - m\r\n    g = Geo(a, IFT, k, info)                     # create the geo array\r\n    out = g.bounding_circles(angle=2, return_xyr=False)\r\n    circs = [arr + m for arr in out]\r\n    k = 1\r\n    if kind == \'Polygons\':\r\n        k = 2\r\n    circs = Update_Geo(circs, K=k, id_too=None, Info=info)\r\n    # produce the geometry\r\n    p = kind.upper()\r\n    geometry_fc(circs, circs.IFT, p_type=p, gdb=gdb, fname=name, sr=SR)\r\n    return ""{} completed"".format(""Circles"")\r\n\r\n\r\n# ---- (2) convex hulls\r\n#\r\ndef convex_hull_polys(in_fc, gdb, name, kind):\r\n    """"""Determine the convex hulls on a shape basis""""""\r\n    SR = getSR(in_fc)\r\n    kind, k = shape_K(in_fc)\r\n    tmp, IFT = fc_geometry(in_fc, SR=SR, IFT_rec=False)\r\n    info = ""convex hulls to polygons""\r\n    g = Geo(tmp, IFT, k, info)                   # create the geo array\r\n    ch_out = g.convex_hulls(by_part=False, threshold=50)\r\n    ch_out = Update_Geo(ch_out, K=k, id_too=None, Info=info)\r\n    # ---- produce the geometry\r\n    p = kind.upper()\r\n    geometry_fc(ch_out, ch_out.IFT, p_type=p, gdb=gdb, fname=name, sr=SR)\r\n    return ""{} completed"".format(""Convex Hulls"")\r\n\r\n\r\n# ---- (3) extent_poly section\r\n#\r\ndef extent_poly(in_fc, gdb, name, kind):\r\n    """"""Feature envelope to polygon demo.\r\n\r\n    Parameters\r\n    ----------\r\n    in_fc : string\r\n        Full geodatabase path and featureclass filename.\r\n    kind : integer\r\n        2 for polygons, 1 for polylines\r\n\r\n    References\r\n    ----------\r\n    `Feature Envelope to Polygon\r\n    <https://pro.arcgis.com/en/pro-app/tool-reference/data-management/feature\r\n    -envelope-to-polygon.htm>`_.\r\n\r\n    >>> data = fc_data(in_fc)\r\n    """"""\r\n    SR = getSR(in_fc)\r\n    kind, k = shape_K(in_fc)\r\n    tmp, IFT = fc_geometry(in_fc, SR=SR, IFT_rec=False)\r\n    m = np.nanmin(tmp, axis=0)                   # shift to LB of extent\r\n    info = ""extent to polygons""\r\n    a = tmp - m\r\n    g = Geo(a, IFT, k, Info=info)   # create the geo array\r\n    ext = g.extent_rectangles()   # create the extent array\r\n    ext = ext + m                 # shift back, construct the output features\r\n    ext = Update_Geo(ext, K=k, id_too=None, Info=info)\r\n    # ---- produce the geometry\r\n    p = kind.upper()\r\n    geometry_fc(ext, ext.IFT, p_type=p, gdb=gdb, fname=name, sr=SR)\r\n    return ""{} completed"".format(""Extents"")\r\n\r\n\r\n# ---- Conversion Tools ------------------------------------------------------\r\n# ---- (1) features to point\r\n#\r\ndef f2pnts(in_fc):\r\n    """"""Features to points.\r\n    `getSR`, `shape_K` and `fc_geometry` from `npGeo_io`\r\n    """"""\r\n    SR = getSR(in_fc)\r\n    kind, k = shape_K(in_fc)\r\n    tmp, ift = fc_geometry(in_fc, SR=SR, IFT_rec=False)\r\n    m = np.nanmin(tmp, axis=0)                   # shift to LB of whole extent\r\n    info = ""feature to points""\r\n    a = tmp - m\r\n    g = Geo(a, IFT=ift, Kind=k, Info=info)    # create the geo array\r\n    cent = g.centroids + m                       # create the centroids\r\n    dt = np.dtype([(\'Xs\', \'<f8\'), (\'Ys\', \'<f8\')])\r\n    cent = uts(cent, dtype=dt)\r\n    return cent, SR\r\n\r\n\r\n# ---- (2) polygon to polyline\r\n#\r\ndef pgon_to_pline(in_fc, gdb, name):\r\n    """"""Polygon to polyline conversion.  Multipart shapes are converted to\r\n    singlepart.  The singlepart geometry is used to produce the polylines.""""""\r\n#    gdb, name = check_path(out_fc)\r\n#    if gdb is None:\r\n#        return None\r\n    SR = getSR(in_fc)\r\n    tmp = MultipartToSinglepart_management(in_fc, r""memory\\in_fc_temp"")\r\n    a, IFT = fc_geometry(tmp, SR=SR, IFT_rec=False)\r\n    info = ""pgon to pline""\r\n    # create the geo array, convert it, then create the output featureclass\r\n    a = Geo(a, IFT=IFT, Kind=1, Info=info)       # create the geo array\r\n    geometry_fc(a, IFT, p_type=""POLYLINE"", gdb=gdb, fname=name, sr=SR)\r\n    out = ""{}/{}"".format(gdb, name)\r\n    if Exists(out):\r\n        d = fc_data(tmp)\r\n        import time\r\n        time.sleep(1.0)\r\n        da.ExtendTable(out, \'OBJECTID\', d, \'OID_\')\r\n    tweet(dedent(msg_mp_sp))\r\n    return\r\n\r\n\r\n# ---- (3) vertices to points\r\n#\r\ndef p_uni_pnts(in_fc):\r\n    """"""Implements `_polys_to_unique_pnts_` in ``npg_helpers``.\r\n    """"""\r\n    SR = getSR(in_fc)\r\n    tmp, IFT = fc_geometry(in_fc, SR=SR, IFT_rec=False)\r\n    info = ""unique points""\r\n    a = Geo(tmp, IFT=IFT, Kind=0, Info=info)     # create the geo array\r\n    out = _polys_to_unique_pnts_(a, as_structured=True)\r\n    return out, SR\r\n\r\n\r\n# ---- Alter Geometry --------------------------------------------------------\r\n# ---- (1) rotate features\r\ndef rotater(in_fc, gdb, name, as_group, angle, clockwise):\r\n    """"""Rotate features separately or as a group.\r\n    """"""\r\n    SR = getSR(in_fc)\r\n    kind, k = shape_K(in_fc)                        # geometry type, 0, 1, 2\r\n    a, IFT = fc_geometry(in_fc, SR=SR, IFT_rec=False)\r\n    tmp = MultipartToSinglepart_management(in_fc, r""memory\\in_fc_temp"")\r\n    info = ""rotate features""\r\n    a = Geo(a, IFT=IFT, Kind=k, Info=info)       # create the geo array\r\n    s = a.rotate(as_group=as_group, angle=angle, clockwise=clockwise)\r\n    p = kind.upper()\r\n    geometry_fc(s, s.IFT, p_type=p, gdb=gdb, fname=name, sr=SR)\r\n    out = ""{}/{}"".format(gdb, name)\r\n    if Exists(out):\r\n        import time\r\n        time.sleep(1.0)\r\n        d = fc_data(tmp)\r\n        da.ExtendTable(out, \'OBJECTID\', d, \'OID_\')\r\n    return\r\n\r\n\r\ndef fill_holes(in_fc, gdb, name):\r\n    """"""Fill holes in a featureclass.  See the Eliminate part tool.\r\n    """"""\r\n    SR = getSR(in_fc)\r\n    kind, k = shape_K(in_fc)                        # geometry type, 0, 1, 2\r\n    tmp = MultipartToSinglepart_management(in_fc, r""memory\\in_fc_temp"")\r\n    a, IFT = fc_geometry(tmp, SR=SR, IFT_rec=False)\r\n    info = ""fill holes""\r\n    a = Geo(a, IFT=IFT, Kind=k, Info=info)       # create the geo array\r\n    oring = a.outer_rings(True)\r\n    p = kind.upper()\r\n    geometry_fc(oring, oring.IFT, p_type=p, gdb=gdb, fname=name, sr=SR)\r\n    out = ""{}/{}"".format(gdb, name)\r\n    if Exists(out):\r\n        import time\r\n        time.sleep(1.0)\r\n        d = fc_data(tmp)\r\n        da.ExtendTable(out, \'OBJECTID\', d, \'OID_\')\r\n    return\r\n\r\n\r\n# ---- (2) sorty by area, length\r\n#\r\ndef sort_geom(in_fc, gdb, name, sort_kind):\r\n    """"""Sort features by area, length\r\n    """"""\r\n    SR = getSR(in_fc)\r\n    kind, k = shape_K(in_fc)                        # geometry type, 0, 1, 2\r\n    a, IFT = fc_geometry(in_fc, SR=SR, IFT_rec=False)\r\n    info = ""sort features""\r\n    a = Geo(a, IFT=IFT, Kind=k, Info=info)\r\n    if sort_kind == \'area\':\r\n        srt = a.sort_by_area(ascending=True, just_indices=False)\r\n    elif sort_kind == \'length\':\r\n        srt = a.sort_by_length(ascending=True, just_indices=False)\r\n    p = kind.upper()\r\n    geometry_fc(srt, srt.IFT, p_type=p, gdb=gdb, fname=name, sr=SR)\r\n    return\r\n\r\n\r\n# ---- (3) sorty by extent\r\n#\r\ndef sort_extent(in_fc, gdb, name, key):\r\n    """"""Sort features by extent, area, length\r\n    """"""\r\n    SR = getSR(in_fc)\r\n    kind, k = shape_K(in_fc)                        # geometry type, 0, 1, 2\r\n    a, IFT = fc_geometry(in_fc, SR=SR, IFT_rec=False)\r\n    info = ""sort features""\r\n    a = Geo(a, IFT=IFT, Kind=k, Info=info)\r\n    srt = a.sort_by_extent(key, just_indices=False)\r\n    p = kind.upper()\r\n    geometry_fc(srt, srt.IFT, p_type=p, gdb=gdb, fname=name, sr=SR)\r\n    return\r\n\r\n\r\n# ---- (4) move/shift/translate features\r\n#\r\ndef shifter(in_fc, gdb, name, dX, dY):\r\n    """"""Shift features to a new location by delta X and Y values.  Multipart\r\n    shapes are converted to singlepart shapes.\r\n    """"""\r\n    SR = getSR(in_fc)\r\n    desc = da.Describe(in_fc)\r\n    kind = desc[\'shapeType\']\r\n    kind, k = shape_K(in_fc)                        # geometry type, 0, 1, 2\r\n    a, IFT = fc_geometry(in_fc, SR=SR, IFT_rec=False)\r\n    tmp = MultipartToSinglepart_management(in_fc, r""memory\\in_fc_temp"")\r\n    info = ""shift features""\r\n    # create the geo array, shift it, then create the output featureclass\r\n    a = Geo(a, IFT=IFT, Kind=k, Info=info)\r\n    s = a.shift(dX, dY)\r\n    p = kind.upper()\r\n    geometry_fc(s, s.IFT, p_type=p, gdb=gdb, fname=name, sr=SR)\r\n    out = ""{}/{}"".format(gdb, name)\r\n    if Exists(out):\r\n        import time\r\n        time.sleep(1.0)\r\n        d = fc_data(tmp)\r\n        da.ExtendTable(out, \'OBJECTID\', d, \'OID_\')\r\n    return\r\n\r\n\r\n# ---- (5) split line at vertices\r\n#\r\ndef split_at_vertices(in_fc, out_fc):\r\n    """"""Unique segments retained when poly geometry is split at vertices.\r\n    """"""\r\n    gdb, _ = check_path(out_fc)\r\n    if gdb is None:\r\n        return None\r\n    SR = getSR(in_fc)\r\n    tmp, IFT = fc_geometry(in_fc, SR=SR, IFT_rec=False)\r\n    ag = Geo(tmp, IFT)\r\n    od = ag.polys_to_segments(as_basic=False, as_3d=False)\r\n    tmp = ""memory/tmp""\r\n    if Exists(tmp):\r\n        Delete_management(tmp)\r\n    ags.da.NumPyArrayToTable(od, tmp)\r\n    xyxy = list(od.dtype.names[:4])\r\n    args = [tmp, out_fc] + xyxy + [""GEODESIC"", ""Orig_id"", SR]\r\n    XYToLine_management(*args)\r\n    return\r\n\r\n\r\n# ---- Attribute tools -------------------------------------------------------\r\n# ---- (1) frequency and statistics\r\n#\r\ndef freq(a, cls_flds, stat_fld):\r\n    """"""Frequency and crosstabulation.\r\n\r\n    Parameters\r\n    ----------\r\n    a : array\r\n        A structured array.\r\n    flds : field\r\n        Fields to use in the analysis.\r\n\r\n    Notes\r\n    -----\r\n    1. Slice the input array by the classification fields.\r\n    2. Sort the sliced array using the flds as sorting keys.\r\n    3. Use unique on the sorted array to return the results and the counts.\r\n\r\n    >>> np.unique(ar, return_index=False, return_inverse=False,\r\n    ...           return_counts=True, axis=None)\r\n    """"""\r\n    if stat_fld is None:\r\n        a = a[cls_flds]  # (1) It is actually faster to slice the whole table\r\n    else:\r\n        all_flds = cls_flds + [stat_fld]\r\n        a = a[all_flds]\r\n    idx = np.argsort(a, axis=0, order=cls_flds)  # (2)\r\n    a_sort = a[idx]\r\n    uni, inv, cnts = np.unique(a_sort[cls_flds], False,\r\n                               True, return_counts=True)  # (3)\r\n    out_flds = ""Counts""\r\n    out_data = cnts\r\n    if stat_fld is not None:\r\n        splitter = np.where(np.diff(inv) == 1)[0] + 1\r\n        a0 = a_sort[stat_fld]\r\n        splits = np.split(a0, splitter)\r\n        sums = np.asarray([np.nansum(i.tolist()) for i in splits])\r\n        nans = np.asarray([np.sum(np.isnan(i.tolist())) for i in splits])\r\n        mins = np.asarray([np.nanmin(i.tolist()) for i in splits])\r\n        means = np.asarray([np.nanmean(i.tolist()) for i in splits])\r\n        maxs = np.asarray([np.nanmax(i.tolist()) for i in splits])\r\n        out_flds = [out_flds, stat_fld + ""_sums"", stat_fld + ""_NaN"",\r\n                    stat_fld + ""_min"", stat_fld + ""_mean"", stat_fld + ""_max""]\r\n        out_data = [out_data, sums, nans, mins, means, maxs]\r\n    out = append_fields(uni, names=out_flds, data=out_data, usemask=False)\r\n    return out\r\n\r\n\r\ndef attr_sort(a, oid_fld, sort_flds):\r\n    """"""Return old and new id values for the sorted array.\r\n    """"""\r\n    idx = np.argsort(a, order=sort_flds)\r\n    srted = a[idx]\r\n    dt = [(oid_fld, \'<i4\'), (\'Sorted_\', \'<i4\')]\r\n    out = np.zeros_like(srted, dtype=np.dtype(dt))  # create the new array\r\n    out[oid_fld] = srted[oid_fld]\r\n    out[\'Sorted_\'] = np.arange(0, out.shape[0])\r\n    return out\r\n\r\n\r\n# ===========================================================================\r\n# ---- main section: testing or tool run ------------------------------------\r\n#\r\nscript = sys.argv[0]\r\npth = ""/"".join(script.split(""/"")[:-1])\r\ntbx = pth + ""/Free_tools.tbx""\r\ntbx = ImportToolbox(tbx)\r\n\r\nfrmt = """"""\r\nSource script... {}\r\nUsing :\r\n    tool   : {}\r\n    input  : {}\r\n    output : {}\r\n""""""\r\n\r\n\r\ndef pick_tool(tool, in_fc, out_fc, gdb, name):\r\n    """"""Pick the tool and run the option.\r\n    """"""\r\n    # ---- Geometry tools ----------------------------------------------------\r\n    #\r\n    # ---- Containers\r\n    if tool in [\'Bounding Circles\', \'Convex Hulls\', \'Extent Polys\']:\r\n        kind = sys.argv[4].upper()\r\n        if tool == \'Bounding Circles\':           # ---- (1) bounding circles\r\n            circles(in_fc, gdb, name, kind)\r\n        elif tool == \'Convex Hulls\':             # ---- (2) convex hulls\r\n            convex_hull_polys(in_fc, gdb, name, kind)\r\n        elif tool == \'Extent Polys\':             # ---- (3) extent_poly\r\n            extent_poly(in_fc, gdb, name, kind)\r\n        tweet(""...\\n{} as {}"".format(tool, kind.title()))\r\n    #\r\n    # ---- Conversion\r\n    elif tool in [\'Features to Points\', \'Vertices to Points\']:\r\n        if tool == \'Features to Points\':         # ---- (1) features to point\r\n            out, SR = f2pnts(in_fc)\r\n        elif tool == \'Vertices to Points\':       # ---- (2) feature to vertices\r\n            out, SR = p_uni_pnts(in_fc)\r\n        ags.da.NumPyArrayToFeatureClass(out, out_fc, [\'Xs\', \'Ys\'], SR)\r\n        tweet(""...\\n{} as {}"".format(tool, \'Points\'))\r\n    elif tool == \'Polygons to Polylines\':        # ---- (3) polygon to polyline\r\n        tweet(""...\\nPolygons to Polylines...\\n"")\r\n        pgon_to_pline(in_fc, gdb, name)\r\n    elif tool == \'Split at Vertices\':            # ---- (4) split at vertices\r\n        tweet(""...\\n{} as {}"".format(tool, \'Lines\'))\r\n        split_at_vertices(in_fc, out_fc)\r\n    #\r\n    # ---- Alter geometry\r\n    elif tool == \'Rotate Features\':              # ---- (1) rotate\r\n        clockwise = False\r\n        as_group = False\r\n        rot_type = str(sys.argv[4])  # True, extent center, False, shape center\r\n        angle = float(sys.argv[5])\r\n        clockwise = str(sys.argv[6])\r\n        if rot_type == ""shape center"":\r\n            as_group = True\r\n        if clockwise.lower() == ""true"":\r\n            clockwise = True\r\n        tweet(""...\\n{} {}"".format(tool, \'Features\'))\r\n        rotater(in_fc, gdb, name, as_group, angle, clockwise)\r\n    elif tool == \'Shift Features\':               # ---- (5) shift\r\n        dX = float(sys.argv[4])\r\n        dY = float(sys.argv[5])\r\n        tweet(""...\\n{} {}"".format(tool, \'Features\'))\r\n        shifter(in_fc, gdb, name, dX=dX, dY=dY)\r\n    elif tool == \'Fill Holes\':\r\n        tweet(""not implemented yet"")\r\n        fill_holes(in_fc, gdb, name)\r\n    #\r\n    # ---- Sort geometry\r\n    elif tool in [\'Area Sort\', \'Length Sort\', \'Geometry Sort\']:\r\n        srt_type = tool.split("" "")[0].lower()\r\n        tweet(""...\\n{} as {}"".format(tool, \'input\'))\r\n        sort_geom(in_fc, gdb, name, srt_type)\r\n    elif tool == \'Extent Sort\':\r\n        srt_type = int(sys.argv[4][0])\r\n        tweet(""...\\n{} as {}"".format(tool, \'input\'))\r\n        sort_extent(in_fc, gdb, name, srt_type)\r\n    #\r\n    # ---- Attribute tools --------------------------------------------------\r\n    elif tool == \'Crosstabulate\':                # ---- (1) freq and stats\r\n        cls_flds = sys.argv[4]\r\n        stat_fld = sys.argv[5]\r\n        cls_flds = cls_flds.split("";"")  # multiple to list, singleton a list\r\n        if stat_fld in (None, \'NoneType\', """"):\r\n            stat_fld = None\r\n        a = ags.da.TableToNumPyArray(in_fc, ""*"")  # use the whole array\r\n        tweet(""result...\\n{}"".format(a))\r\n        out = freq(a, cls_flds, stat_fld)        # do freq analysis\r\n        if Exists(out_fc) and env.overwriteOutput:\r\n            Delete_management(out_fc)\r\n        ags.da.NumPyArrayToTable(out, out_fc)\r\n    elif tool == \'Attribute sort\':\r\n        sort_flds = str(sys.argv[4])  # just tool and in_fc, extend to existing\r\n        sort_flds = sort_flds.split("";"")\r\n        msg = """"""\\\r\n        ------------------\r\n        Sorting      : {}\r\n        Using fields : {}\r\n        Output field : {}\r\n        -----------------\r\n        """"""\r\n        tweet(dedent(msg).format(in_fc, sort_flds, ""Sorted_""))\r\n        oid_fld = da.Describe(in_fc)[\'OIDFieldName\']\r\n        flds = [oid_fld] + sort_flds\r\n        a = ags.da.TableToNumPyArray(in_fc, flds)\r\n        out = attr_sort(a, oid_fld, sort_flds)    # do the work... attr_sort\r\n        da.ExtendTable(in_fc, oid_fld, out, oid_fld, append_only=False)\r\n    else:\r\n        tweet(""tool {} not found"".format(tool))\r\n        return None\r\n    # ---- (\r\n\r\n\r\n# ==== testing or tool run ===================================================\r\n#\r\ndef _testing_():\r\n    """"""Run in spyder\r\n    """"""\r\n    in_fc = ""C:/Git_Dan/npgeom/npgeom.gdb/Polygons""\r\n    out_fc = ""C:/Git_Dan/npgeom/npgeom.gdb/x""\r\n    tool = \'ShiftFeatures\'  # None  #\r\n    info_ = gp.getParameterInfo(tool)\r\n    for param in info_:\r\n        print(""Name: {}, Type: {}, Value: {}"".format(\r\n            param.name, param.parameterType, param.value))\r\n    print(""Input {}\\nOutput {}"".format(in_fc, out_fc))\r\n    return info_, in_fc, out_fc  # in_fc, out_fc, tool, kind\r\n\r\n\r\ndef _tool_(tools=tool_list):\r\n    """"""Run from a tool in arctoolbox in ArcGIS Pro.  The tool checks to ensure\r\n    that the path to the output complies and that the desired tool actually\r\n    exists, so it can be parsed based on type.\r\n    """"""\r\n    tool = sys.argv[1]\r\n    in_fc = sys.argv[2]\r\n    out_fc = sys.argv[3]\r\n    tweet(""out_fc  {}"".format(out_fc))\r\n    if out_fc not in (None, \'None\'):\r\n        gdb, name = check_path(out_fc)               # ---- check the paths\r\n        if gdb is None:\r\n            tweet(msg0)\r\n            return None\r\n    else:\r\n        gdb = None\r\n        name = None\r\n    if tool not in tools:                        # ---- check the tool\r\n        tweet(""Tool {} not implemented"".format(tool))\r\n        return None\r\n    msg1 = ""Tool   : {}\\ninput  : {}\\noutput : {}""\r\n    tweet(msg1.format(tool, in_fc, out_fc))\r\n    pick_tool(tool, in_fc, out_fc, gdb, name)    # ---- run the tool\r\n    return  # tool, in_fc, out_fc\r\n\r\n\r\n# ===========================================================================\r\n# ---- main section: testing or tool run\r\n#\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    result = _testing_()\r\nelse:\r\n    testing = False\r\n    _tool_(tool_list)\r\n\r\n# ===========================================================================\r\n#\r\nif __name__ == ""__main__"":\r\n    """"""optional location for parameters""""""\r\n    info_ = _testing_()\r\n'"
all_scripts/_base_functions.py,45,"b'# -*- coding: utf-8 -*-\r\n""""""\r\n_base_functions\r\n===============\r\n\r\nScript :   _base_functions.py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified : 2018-11-23\r\n\r\nPurpose:  tools for working with numpy arrays\r\n\r\nUseage :\r\n--------\r\n`_base_functions` : some functions\r\n::\r\n  arr_info, art_info, even_odd, n_largest, n_smallest, num_to_mask,\r\n  num_to_nan, pad_even_odd, pad_nan, pad_zero, reshape_options, shape_to2D\r\n\r\nReferences\r\n----------\r\n\r\n`<https://docs.python.org/3/library/itertools.html#itertools-recipes>`_.\r\n\r\n`< >`_.\r\n\r\n`< >`_.\r\n\r\nFunctions\r\n---------\r\n**n largest, n_smallest**\r\n\r\n>>> a = np.arange(0, 9)  # array([0, 1, 2, 3, 4, 5, 6, 7, 8])\r\n>>> n_largest(a, num=2, by_row=True)\r\narray([[ 2,  3],\r\n       [ 6,  7],\r\n       [10, 11]])\r\n>>> n_largest(a, num=2, by_row=False)\r\narray([[ 4,  8],\r\n       [ 5,  9],\r\n       [ 6, 10],\r\n       [ 7, 11]])\r\n\r\n**num_to_nan, num_to_mask** : nan stuff\r\n\r\n>>> a = np.arange(6)  # array([0, 1, 2, 3, 4, 5])\r\n>>> num_to_nan(a, nums=[2, 3])\r\narray([ 0.,  1., nan, nan,  4.,  5.])\r\n>>> num_to_mask(a, nums=[2, 3]) ...\r\nmasked_array(data = [0 1 - - 4 5],\r\n             mask = [False False  True  True False False],\r\n       fill_value = 999999)\r\n\r\n---------------------------------------------------------------------\r\n""""""\r\n# pylint: disable=C0103\r\n# pylint: disable=R1710\r\n# pylint: disable=R0914\r\n\r\nimport sys\r\nfrom textwrap import dedent, indent\r\nimport numpy as np\r\n#from arcpytools import fc_info, tweet  #, prn_rec, _col_format\r\n#import arcpy\r\n\r\nepsilon = sys.float_info.epsilon  # note! for checking\r\n\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\n#import numpy.core.numerictypes as ntypes\r\n\r\ntype_keys = np.typecodes.keys()\r\ntype_vals = np.typecodes.values()\r\n\r\n\r\n__all__ = [\'arr_info\',      # (1) info functions\r\n           \'keep_ascii\',    # (2) chararray\r\n           \'is_float\',\r\n           \'keep_nums\',\r\n           \'del_punc\',\r\n           \'n_largest\',     # (3) ndarray... size-based\r\n           \'n_smallest\',\r\n           \'num_to_nan\',    # (4) masking\r\n           \'num_to_mask\',\r\n           \'even_odd\',      # (5) padding\r\n           \'pad_even_odd\',\r\n           \'pad_nan\',\r\n           \'pad_zero\',\r\n           \'shape_to2D\',    # (6) reshaping arrays\r\n           \'reshape_options\'\r\n           ]\r\n\r\n# ----------------------------------------------------------------------\r\n# ---- (1) info .... code section ----\r\ndef arr_info(a=None, prn=True):\r\n    """"""Returns basic information about an numpy array.\r\n\r\n    Requires:\r\n    --------\r\n    a : array\r\n        An array to return basic information on.\r\n    prn : Boolean\r\n        True to print, False to return as string.\r\n\r\n    Returns\r\n    -------\r\n    Example array information.\r\n\r\n    >>> a = np.arange(2. * 3.).reshape(2, 3) # quick float64 array\r\n    >>> arr_info(a)\r\n        Array information....\r\n         OWNDATA: if \'False\', data are a view\r\n        flags....\r\n        ... snip ...\r\n        array\r\n            |__shape (2, 3)\r\n            |__ndim  2\r\n            |__size  6\r\n            |__bytes\r\n            |__type  <class \'numpy.ndarray\'>\r\n            |__strides  (24, 8)\r\n        dtype      float64\r\n            |__kind  f\r\n            |__char  d\r\n            |__num   12\r\n            |__type  <class \'numpy.float64\'>\r\n            |__name  float64\r\n            |__shape ()\r\n            |__description\r\n                 |__name, itemsize\r\n                 |__[\'\', \'<f8\']\r\n    ---------------------\r\n    """"""\r\n    if a is None:\r\n        print(arr_info.__doc__)\r\n        return None\r\n    elif not isinstance(a, (np.ndarray, np.ma.core.MaskedArray)):\r\n        s = ""\\n... Requires a numpy ndarray or variant...\\n... Read the docs\\n""\r\n        print(s)\r\n        return None\r\n    frmt = """"""\r\n    :---------------------\r\n    :Array information....\r\n    : OWNDATA: if \'False\', data are a view\r\n    :flags....\r\n    {}\r\n    :array\r\n    :  |__shape {}\\n    :  |__ndim  {}\\n    :  |__size  {:,}\r\n    :  |__bytes {:,}\\n    :  |__type  {}\\n    :  |__strides  {}\r\n    :dtype      {}\r\n    :  |__kind  {}\\n    :  |__char  {}\\n    :  |__num   {}\r\n    :  |__type  {}\\n    :  |__name  {}\\n    :  |__shape {}\r\n    :  |__description\r\n    :  |  |__name, itemsize""""""\r\n    dt = a.dtype\r\n    flg = indent(a.flags.__str__(), prefix=\':   \')\r\n    info_ = [flg, a.shape, a.ndim, a.size,\r\n             a.nbytes, type(a), a.strides, dt,\r\n             dt.kind, dt.char, dt.num, dt.type, dt.name, dt.shape]\r\n    flds = sorted([[k, v] for k, v in dt.descr])\r\n    out = dedent(frmt).format(*info_) + ""\\n""\r\n    leader = """".join(["":     |__{}\\n"".format(i) for i in flds])\r\n    leader = leader + "":---------------------""\r\n    out = out + leader\r\n    if prn:\r\n        print(out)\r\n    else:\r\n        return out\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# ---- (2) chararray section ----\r\n# ----------------------------------------------------------------------\r\ndef keep_ascii(s):\r\n    """"""Remove non-ascii characters which may be bytes or unicode characters\r\n    """"""\r\n    if isinstance(s, bytes):\r\n        u = s.decode(""utf-8"")\r\n        u = """".join([[\'_\', i][ord(i) < 128] for i in u])\r\n        return u\r\n    return s\r\n\r\n\r\ndef is_float(a):\r\n    """"""float check""""""\r\n    try:\r\n        np.asarray(a, np.float_)\r\n        return True\r\n    except ValueError:\r\n        return False\r\n\r\n\r\ndef keep_nums(s):\r\n    """"""Remove all non-numbers and return an integer.\r\n    """"""\r\n    s = keep_ascii(s)\r\n    s = """".join([i for i in s if i.isdigit() or i == "" ""]).strip()\r\n    return int(s)\r\n\r\n\r\ndef del_punc(s, keep_under=False, keep_space=False):\r\n    """"""Remove punctuation with options to keep the underscore and spaces.\r\n    If keep_space is True, then they will not be replaced with an underscore.\r\n    False, will replace them.  Check for bytes as well.\r\n    """"""\r\n    s = keep_ascii(s)\r\n    repl = \' \'\r\n    punc = list(\'!""#$%&\\\'()*+,-./:;<=>?@[\\\\]^`{|}~\')\r\n    if keep_under:\r\n        punc.append(\'_\')\r\n    if not keep_space:\r\n        punc.append(\' \')\r\n        repl = \'\'\r\n    s = """".join([[i, repl][i in punc] for i in s])\r\n    return s\r\n\r\n\r\ndef del_punc_space(name, repl_with=\'_\'):\r\n    """"""delete punctuation and spaces and replace with \'_\'""""""\r\n    punc = list(\'!""#$%&\\\'()*+,-./:;<=>?@[\\\\]^`{|}~ \')\r\n    return """".join([[i, repl_with][i in punc] for i in name])\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# ---- ndarray section, change format or arrangement ----\r\n# ----------------------------------------------------------------------\r\n# ---- (3) size-based .... n largest, n_smallest\r\n#\r\ndef n_largest(a, num=1, by_row=True):\r\n    """"""Return the`\'num` largest entries in an array by row sorted by column, or\r\n    by column sorted by row.\r\n\r\n    Parameters:\r\n    -----------\r\n    a : ndarray\r\n        Array dimensions <=3 supported\r\n    num : integer\r\n        The number of elements to return\r\n    by_row : boolean\r\n        True for returns by row, False to determine by column\r\n    """"""\r\n    assert a.ndim <= 3, ""Only arrays with ndim <=3 supported""\r\n    if not by_row:\r\n        a = a.T\r\n    num = min(num, a.shape[-1])\r\n    if a.ndim == 1:\r\n        b = np.sort(a)[-num:]\r\n    elif a.ndim >= 2:\r\n        b = np.sort(a)[..., -num:]\r\n    else:\r\n        return None\r\n    return b\r\n\r\n\r\ndef n_smallest(a, num=1, by_row=True):\r\n    """"""Return the \'n\' smallest entries in an array by row sorted by column.\r\n    see `n_largest` for parameter description\r\n    """"""\r\n    assert a.ndim <= 3, ""Only arrays with ndim <=3 supported""\r\n    if not by_row:\r\n        a = a.T\r\n    num = min(num, a.shape[-1])\r\n    if a.ndim == 1:\r\n        b = np.sort(a)[:num]\r\n    elif a.ndim >= 2:\r\n        b = np.sort(a)[..., :num]\r\n    else:\r\n        return None\r\n    return b\r\n\r\n\r\n# ---- (4) masking ... num_to_nan, num_to_mask ... code section .... ----\r\n#\r\ndef num_to_nan(a, nums=None):\r\n    """"""Reverse of nan_to_num introduced in numpy 1.13\r\n\r\n    Example\r\n    -------\r\n    >>> a = np.arange(10)\r\n    >>> num_to_nan(a, num=[2, 3])\r\n    array([  0.,   1.,   nan,  nan,   4.,   5.,   6.,   7.,   8.,   9.])\r\n    """"""\r\n    a = a.astype(\'float64\')\r\n    if nums is None:\r\n        return a\r\n    if isinstance(nums, (list, tuple, np.ndarray)):\r\n        m = np.isin(a, nums, assume_unique=False, invert=False)\r\n        a[m] = np.nan\r\n    else:\r\n        a = np.where(a == nums, np.nan, a)\r\n    return a\r\n\r\n\r\ndef num_to_mask(a, nums=None, hardmask=True):\r\n    """"""Reverse of nan_to_num introduced in numpy 1.13\r\n\r\n    Example\r\n    -------\r\n    >>> a = np.arange(10)\r\n    >>> art.num_to_mask(a, nums=[1, 2, 4])\r\n    masked_array(data = [0 - - 3 - 5 6 7 8 9],\r\n                mask = [False  True  True False  True False\r\n                        False False False False], fill_value = 999999)\r\n    """"""\r\n    if nums is None:\r\n        ret = a\r\n    else:\r\n        m = np.isin(a, nums, assume_unique=False, invert=False)\r\n        nums = np.array(nums)\r\n        ret = np.ma.MaskedArray(a, mask=m, hard_mask=hardmask)\r\n    return ret\r\n\r\n\r\n# ---- (5) padding arrays  ... even_odd, pad_even_odd, pad_nan, pad_zero\r\n#\r\ndef even_odd(a):\r\n    """"""Even/odd from modulus.  Returns 0 for even, 1 for odd""""""\r\n    prod = np.cumprod(a.shape)[0]\r\n    return np.mod(prod, 2)\r\n\r\n\r\ndef pad_even_odd(a):\r\n    """"""To use when padding a strided array for window construction\r\n    """"""\r\n    p = even_odd(a)\r\n    ap = np.pad(a, pad_width=(1, p), mode=""constant"", constant_values=(0, 0))\r\n    return ap\r\n\r\n\r\ndef pad_nan(a, nan_edge=True):\r\n    """"""Pad a sliding array to allow for stats, padding uses np.nan\r\n    : see also: num_to_nan(a, num=None, copy=True)\r\n    """"""\r\n    a = a.astype(\'float64\')\r\n    if nan_edge:\r\n        cv = (np.NaN, np.NaN)\r\n        a = np.pad(a, pad_width=(1, 1), mode=""constant"", constant_values=cv)\r\n    return a\r\n\r\n\r\ndef pad_zero(a, n=1):\r\n    """"""To use when padding a strided array for window construction. n = number\r\n    : of zeros to pad arround the array\r\n    : see also: nun_to_nan (1.13)\r\n    """"""\r\n    ap = np.pad(a, pad_width=(n, n), mode=""constant"", constant_values=(0, 0))\r\n    return ap\r\n\r\n\r\ndef strip_whitespace(a):\r\n    """"""Strip unicode whitespace from an array\r\n\r\n    w = [\'\\t\', \'\\n\', \'\\x0b\', \'\\x0c\', \'\\r\', \'\\x1c\', \'\\x1d\', \'\\x1e\', \'\\x1f\',\r\n         \' \', \'\\x85\', \'\\xa0\', \'\\u1680\', \'\\u2000\', \'\\u2001\', \'\\u2002\', \'\\u2003\',\r\n         \'\\u2004\', \'\\u2005\', \'\\u2006\', \'\\u2007\', \'\\u2008\', \'\\u2009\', \'\\u200a\',\r\n         \'\\u2028\', \'\\u2029\', \'\\u202f\', \'\\u205f\', \'\\u3000\']\r\n    others = [\'\\u200B\', \'\\u200C\', \'\\u200D\', \'\\u2060\', \'\\uFEFF\']\r\n\r\n    References:\r\n    -----------\r\n    `<https://en.wikipedia.org/wiki/Whitespace_character>`_.\r\n    """"""\r\n    assert a.dtype.kind in (\'U\', \'S\'), ""Only numeric arrays supported""\r\n    return np.char.strip(a)\r\n\r\n\r\n# ---- (6) reshaping arrays ... shape_to2D, reshape_options\r\n#\r\ndef shape_to2D(a, stack2D=True, by_column=False):\r\n    """"""Reshape an ndim array to a 2D array for print formatting and other uses.\r\n\r\n    a : array\r\n        array ndim >= 3\r\n    stack2D : boolean\r\n      True, swaps, then stacks the last two dimensions row-wise.\r\n\r\n      False, the first two dimensions are raveled then vertically stacked\r\n\r\n    >>> shp = (2, 2, 3)\r\n    >>> a = np.arange(np.prod(shp)).reshape(shp)\r\n    array([[[ 0,  1,  2],\r\n            [ 3,  4,  5]],\r\n           [[ 6,  7,  8],\r\n            [ 9, 10, 11]]])\r\n\r\n    2D stack by row\r\n\r\n    >>> re_shape(a, stack2D=True,  by_column=False)  # np.hstack((a[0], a[1]))\r\n    array([[ 0,  1,  2,  6,  7,  8],\r\n           [ 3,  4,  5,  9, 10, 11]])\r\n\r\n    2D stack raveled by row\r\n\r\n    >>> re_shape(a, stack2D=False, by_column=False)\r\n    array([[ 0,  1,  2,  3,  4,  5],\r\n           [ 6,  7,  8,  9, 10, 11]])\r\n\r\n    2D stack by column\r\n\r\n    >>> re_shape(a, True, True)\r\n    array([[ 0,  3],\r\n           [ 1,  4],\r\n           [ 2,  5],  # note here a[0] is translated and stacked onto a[1]\r\n           [ 6,  9],\r\n           [ 7, 10],\r\n           [ 8, 11]])\r\n\r\n    >>>  re_shape(a, False, True)\r\n    array([[ 0,  6],\r\n           [ 1,  7],\r\n           [ 2,  8],  # note here a[0] becomes raveled and translated and\r\n           [ 3,  9],  # a[1] is stacked column-wise to it.\r\n           [ 4, 10],\r\n           [ 5, 11]])\r\n\r\n    For other shapes::\r\n\r\n        shp          re_shape(a).shape\r\n        (3, 4)       (4, 3)\r\n        (2, 3, 4)    (3, 8)\r\n        (2, 3, 4, 5) (3, 40)\r\n\r\n    np.transpose and np.swapaxes are related\r\n\r\n    >>> np.all(np.swapaxes(a, 0, 1) == np.transpose(a, (1, 0, 2)))\r\n    """"""\r\n    shp = a.shape\r\n    if stack2D:\r\n        out = np.swapaxes(a, 0, 1).reshape(shp[1], np.prod((shp[0],) + shp[2:]))\r\n    else:\r\n        m = 2\r\n        n = len(shp) - m\r\n        out = a.reshape(np.prod(shp[:n], dtype=\'int\'), np.prod(shp[-m:]))\r\n    if by_column:\r\n        out = out.T\r\n    return out\r\n\r\n\r\ndef reshape_options(a):\r\n    """"""Alternative shapes for a numpy array.\r\n\r\n    Parameters:\r\n    -----------\r\n    a : ndarray\r\n        The ndarray with ndim >= 2\r\n\r\n    Returns:\r\n    --------\r\n    An object array containing the shapes of equal or lower dimension,\r\n    excluding ndim=1\r\n\r\n    >>> a.shape # => (3, 2, 4)\r\n    array([(2, 12), (3, 8), (4, 6), (6, 4), (8, 3), (12, 2), (2, 3, 4),\r\n           (2, 4, 3), (3, 2, 4), (3, 4, 2), (4, 2, 3), (4, 3, 2)],\r\n          dtype=object)\r\n\r\n    Notes:\r\n    ------\r\n    >>> s = list(a.shape)\r\n    >>> case = np.array(list(chain.from_iterable(permutations(s, r)\r\n                        for r in range(len(s)+1)))[1:])\r\n    >>> prod = [np.prod(i) for i in case]\r\n    >>> match = np.where(prod == size)[0]\r\n\r\n    References:\r\n    -----------\r\n    `<https://docs.python.org/3/library/itertools.html#itertools-recipes>`\r\n    """"""\r\n    from itertools import permutations, chain\r\n    s = list(a.shape)\r\n    n = len(s) + 1\r\n    ch = list(chain.from_iterable(permutations(s, r) for r in range(n)))\r\n    case0 = np.array(ch[1:])\r\n    case1 = [i + (-1,) for i in case0]\r\n    new_shps = [a.reshape(i).shape for i in case1]\r\n    z = [i[::-1] for i in new_shps]\r\n    new_shps = new_shps + z\r\n    new_shps = [i for i in np.unique(new_shps) if 1 not in i]\r\n    new_shps = np.array(sorted(new_shps, key=len, reverse=False))\r\n    return new_shps\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# .... final code section producing the featureclass and extendtable\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    # parameters here\r\n# =============================================================================\r\n#     from arraytools.frmts import prn\r\n#     from arraytools.frmts import prn_nd\r\n#     a = np.arange(2*2*3*4).reshape(2*2,3,4)\r\n#     ns = reshape_options(a)\r\n#     print(""\\nBegin reshaping..."")\r\n#     for n in ns:\r\n#         if len(n) == len(a.shape):\r\n#             b = np.reshape(a, n)\r\n#             prn(b)\r\n# =============================================================================\r\nelse:\r\n    testing = False\r\n    # parameters here\r\n#\r\nif testing:\r\n    print(\'\\nScript source... {}\'.format(script))\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n'"
all_scripts/_common.py,10,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n_common.py\r\n==========\r\n\r\nScript :   _common.py\r\n\r\nAuthor :   Dan.Patterson@carleton.ca\r\n\r\nModified : 2018-09-13\r\n\r\nPurpose :\r\n    Common tools for working with numpy arrays and featureclasses\r\n\r\nRequires:\r\n---------\r\nnumpy and arcpy\r\n\r\nTools :\r\n-------\r\n    \'_describe\', \'_flatten\', \'fc_info\', \'flatten_shape\', \'fld_info\',\\\r\n    \'pack\', \'tweet\', \'unpack\'\r\n\r\nReferences :\r\n\r\n---------------------------------------------------------------------\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n__all__ = [\'tweet\',\r\n           \'de_punc\',\r\n           \'_describe\',\r\n           \'fc_info\',\r\n           \'fld_info\',\r\n           \'null_dict\',\r\n           ]\r\n\r\n\r\ndef tweet(msg):\r\n    """"""Print a message for both arcpy and python.\r\n\r\n    msg - a text message\r\n    """"""\r\n    m = ""\\n{}\\n"".format(msg)\r\n    arcpy.AddMessage(m)\r\n    print(m)\r\n\r\n\r\ndef de_punc(s, punc=None, no_spaces=True, char=\'_\'):\r\n    """"""Remove punctuation and/or spaces in strings and replace with\r\n    underscores or nothing\r\n\r\n    Parameters\r\n    ----------\r\n    s : string\r\n        input string to parse\r\n    punc : string\r\n        A string of characters to replace ie. \'@ ""!\\\'\\\\[]\'\r\n    no_spaces : boolean\r\n        True, replaces spaces with underscore.  False, leaves spaces\r\n    char : string\r\n        Replacement character\r\n    """"""\r\n    if (punc is None) or not isinstance(punc, str):\r\n        punc = \'!""#$%&\\\'()*+,-./:;<=>?@[\\\\]^`{|}~\'  # _ removed\r\n    if no_spaces:\r\n        punc = "" "" + punc\r\n    s = """".join([[i, char][i in punc] for i in s])\r\n    return s\r\n\r\n# ----------------------------------------------------------------------------\r\n# ---- Geometry objects and generic geometry/featureclass functions ----------\r\n# ----------------------------------------------------------------------------\r\ndef _describe(in_fc):\r\n    """"""Simply return the arcpy.da.Describe object.\r\n\r\n    **desc.keys()** an abbreviated list::\r\n\r\n    \'OIDFieldName\'... \'areaFieldName\', \'baseName\'... \'catalogPath\',\r\n    \'dataType\'... \'extent\', \'featureType\', \'fields\', \'file\'... \'hasM\',\r\n    \'hasOID\', \'hasZ\', \'indexes\'... \'lengthFieldName\'... \'name\', \'path\',\r\n    \'rasterFieldName\', ..., \'shapeFieldName\', \'shapeType\',\r\n    \'spatialReference\'\r\n\r\n    """"""\r\n    return arcpy.da.Describe(in_fc)\r\n\r\n\r\ndef fc_info(in_fc, prn=False):\r\n    """"""Return basic featureclass information, including the following...\r\n\r\n    Returns:\r\n    --------\r\n    shp_fld  :\r\n        field name which contains the geometry object\r\n    oid_fld  :\r\n        the object index/id field name\r\n    SR       :\r\n        spatial reference object (use SR.name to get the name)\r\n    shp_type :\r\n        shape type (Point, Polyline, Polygon, Multipoint, Multipatch)\r\n\r\n    Notes:\r\n    ------\r\n    Other useful parameters :\r\n        \'areaFieldName\', \'baseName\', \'catalogPath\',\'featureType\',\r\n        \'fields\', \'hasOID\', \'hasM\', \'hasZ\', \'path\'\r\n\r\n    Derive all field names :\r\n        all_flds = [i.name for i in desc[\'fields\']]\r\n    """"""\r\n    desc = _describe(in_fc)\r\n    args = [\'shapeFieldName\', \'OIDFieldName\', \'shapeType\', \'spatialReference\']\r\n    shp_fld, oid_fld, shp_type, SR = [desc[i] for i in args]\r\n    if prn:\r\n        frmt = ""FeatureClass:\\n   {}"".format(in_fc)\r\n        f = ""\\n{!s:<16}{!s:<14}{!s:<10}{!s:<10}""\r\n        frmt += f.format(*args)\r\n        frmt += f.format(shp_fld, oid_fld, shp_type, SR.name)\r\n        tweet(frmt)\r\n        return None\r\n    return shp_fld, oid_fld, shp_type, SR\r\n\r\n\r\ndef fld_info(in_fc, prn=False):\r\n    """"""Field information for a featureclass (in_fc).\r\n\r\n    Parameters:\r\n    -----------\r\n    prn : boolean\r\n        True - returns the values\r\n\r\n        False - simply prints the results\r\n\r\n    Field properties:\r\n    -----------------\r\n    \'aliasName\', \'baseName\', \'defaultValue\', \'domain\', \'editable\',\r\n    \'isNullable\', \'length\', \'name\', \'precision\', \'required\', \'scale\', \'type\'\r\n    """"""\r\n    flds = arcpy.ListFields(in_fc)\r\n    f_info = [(i.name, i.type, i.length, i.isNullable, i.required)\r\n              for i in flds]\r\n    f = ""{!s:<14}{!s:<12}{!s:>7} {!s:<10}{!s:<10}""\r\n    if prn:\r\n        frmt = ""FeatureClass:\\n   {}\\n"".format(in_fc)\r\n        args = [""Name"", ""Type"", ""Length"", ""Nullable"", ""Required""]\r\n        frmt += f.format(*args) + ""\\n""\r\n        frmt += ""\\n"".join([f.format(*i) for i in f_info])\r\n        tweet(frmt)\r\n        return None\r\n    return f_info\r\n\r\n\r\ndef null_dict(flds):\r\n    """"""Produce a null dictionary from a list of fields\r\n    These must be field objects and not just their name.\r\n    """"""\r\n    dump_flds = [""OBJECTID"",""Shape_Length"", ""Shape_Area"", ""Shape""]\r\n    flds_oth = [f for f in flds\r\n                if f.name not in dump_flds]\r\n#    oid_geom = [\'OBJECTID\', \'SHAPE@X\', \'SHAPE@Y\']\r\n    nulls = {\'Double\':np.nan,\r\n             \'Single\':np.nan,\r\n             \'Short\':np.iinfo(np.int16).min,\r\n             \'SmallInteger\':np.iinfo(np.int16).min,\r\n             \'Long\':np.iinfo(np.int32).min,\r\n             \'Float\':np.nan,\r\n             \'Integer\':np.iinfo(np.int32).min,\r\n             \'String\':str(None),\r\n             \'Text\':str(None)}\r\n    fld_dict = {i.name: i.type for i in flds_oth}\r\n    nulls = {f.name:nulls[fld_dict[f.name]] for f in flds_oth}\r\n    return nulls\r\n\r\n\r\ndef tbl_arr(pth):\r\n    """"""Convert featureclass/table to a structured ndarray\r\n\r\n    Requires\r\n    --------\r\n    pth : string\r\n        path to input featureclass or table\r\n\r\n    """"""\r\n    flds = arcpy.ListFields(pth)\r\n    nulls = null_dict(flds)\r\n    bad = [\'OID\', \'Geometry\', \'Shape_Length\', \'Shape_Area\']\r\n    f0 = [""OID@""]\r\n    f1 = [i.name for i in flds if i.type not in bad]\r\n    flds = f0 + f1\r\n    a = arcpy.da.TableToNumPyArray(pth,\r\n                          field_names=flds,\r\n                          skip_nulls=False,\r\n                          null_value=nulls)\r\n    dt = np.array(a.dtype.descr)\r\n    nmes = dt[:, 0]\r\n    sze = dt[:, 1]\r\n    cleaned = []\r\n    for i in nmes:\r\n        i = de_punc(i)  # run de_punc to remove punctuation\r\n        cleaned.append(i)\r\n    a.dtype = list(zip(cleaned, sze))\r\n    return a\r\n\r\n\r\ndef arr_csv(a):\r\n    """"""Format a structured/recarray to csv format\r\n    """"""\r\n    pass\r\n# ---- extras ----------------------------------------------------------------\r\n\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally... print the script source name. run the _demo """"""\r\n#    print(""Script... {}"".format(script))\r\n'"
all_scripts/_distance_tests.py,44,"b'# -*- coding: utf-8 -*-\r\n""""""\r\n\r\n=======\r\n\r\nScript :   .py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified : 2018-\r\n\r\nPurpose :  tools for working with numpy arrays and geometry\r\n\r\nResults:\r\n--------\r\n>>> a = np.random.rand(2, n, 3)  # 10, 100, 1000 \r\n::\r\n    1 linalg_norm\r\n    2 sqrt_sum\r\n    3 scipy_distance\r\n    4 sqrt_einsum(\r\n                          time\r\n    method   1      2      3         4\r\n    n = 10   6.25   5.64   94.5     3.18  \xc2\xb5s\r\n       100   8.57   7.65  938.0     4.25  \xc2\xb5s\r\n      1000  30.5   29.1    9.27 ms 14.0\r\n     10000 204    195     93 ms    71.6 \r\n\r\nReferences:\r\n-----------\r\n`<https://stackoverflow.com/questions/1401712/how-can-the-euclidean-distance\r\n-be-calculated-with-numpy>`_.\r\n""""""\r\nimport sys\r\nimport numpy as np\r\nfrom scipy.spatial import distance\r\n\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=140, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\ndef linalg_norm(data):\r\n    a, b = data\r\n    return np.linalg.norm(a-b, axis=1)\r\n\r\n\r\ndef sqrt_sum(data):\r\n    a, b = data\r\n    return np.sqrt(np.sum((a-b)**2, axis=1))\r\n\r\n\r\ndef scipy_distance(data):\r\n    a, b = data\r\n    return list(map(distance.euclidean, a, b))\r\n\r\n\r\ndef sqrt_einsum(data):\r\n    a, b = data\r\n    a_min_b = a - b\r\n    return np.sqrt(np.einsum(\'ij,ij->i\', a_min_b, a_min_b))\r\n\r\n# =============================================================================\r\n# np.random.RandomState(123)\r\n# \r\n# n = 10\r\n# a = np.random.rand(2, n, 3)\r\n# =============================================================================\r\n# ----------------------------------\r\ndef n_near(a, N=3, ordered=True):\r\n    """"""Return the coordinates and distance to the nearest N points within\r\n    :  an 2D numpy array, \'a\', with optional ordering of the inputs.\r\n    :Requires:\r\n    :--------\r\n    : a - an ndarray of uniform int or float dtype.  Extract the fields\r\n    :     representing the x,y coordinates before proceeding.\r\n    : N - number of closest points to return\r\n    :Returns:\r\n    :-------\r\n    :  A structured array is returned containing an ID number.  The ID number\r\n    :  is the ID of the points as they were read.  The array will contain\r\n    :  (C)losest fields and distance fields\r\n    :  (C0_X, C0_Y, C1_X, C1_Y, Dist0, Dist1 etc) representing coordinates\r\n    :  and distance to the required \'closest\' points.\r\n    """"""\r\n    if not (isinstance(a, (np.ndarray)) and (N >= 1)):\r\n        print(""\\nInput error...read the docs\\n\\n{}"".format(n_near.__doc__))\r\n        return a\r\n    rows, cols = a.shape\r\n    dt_near = [(\'Xo\', \'<f8\'), (\'Yo\', \'<f8\')]\r\n    dt_new = [(\'C{}\'.format(i) + \'{}\'.format(j), \'<f8\')\r\n              for i in range(N)\r\n              for j in [\'_X\', \'_Y\']]\r\n    dt_near.extend(dt_new)\r\n    dt_dist = [(\'Dist{}\'.format(i), \'<f8\') for i in range(N)]\r\n    dt = [(\'ID\', \'<i4\'), *dt_near, *dt_dist]\r\n    n_array = np.zeros((rows,), dtype=dt)\r\n    n_array[\'ID\'] = np.arange(rows)\r\n    # ---- distance matrix calculation using einsum ----\r\n    if ordered:\r\n        a = a[np.argsort(a[:, 0])]\r\n    b = a.reshape(np.prod(a.shape[:-1]), 1, a.shape[-1])\r\n    diff = b - a\r\n    dist = np.einsum(\'ijk,ijk->ij\', diff, diff)\r\n    d = np.sqrt(dist).squeeze()\r\n    # ---- format for use in structured array output ----\r\n    # steps are outlined as follows....\r\n    #\r\n    kv = np.argsort(d, axis=1)       # sort \'d\' on last axis to get keys\r\n    coords = a[kv]                   # pull out coordinates using the keys\r\n    s0, s1, s2 = coords.shape\r\n    coords = coords.reshape((s0, s1*s2))\r\n    dist = np.sort(d)[:, 1:]         # slice sorted distances, skip 1st\r\n    # ---- construct the structured array ----\r\n    dt_names = n_array.dtype.names\r\n    s0, s1, s2 = (1, (N+1)*2 + 1, len(dt_names))\r\n    for i in range(0, s1):           # coordinate field names\r\n        nm = dt_names[i+1]\r\n        n_array[nm] = coords[:, i]\r\n    dist_names = dt_names[s1:s2]\r\n    for i in range(N):               # fill n_array with the results\r\n        nm = dist_names[i]\r\n        n_array[nm] = dist[:, i]\r\n    return coords, dist, n_array\r\n\r\n\r\n\r\ndef kd_tree(a, as_cKD=True):\r\n    """"""Construct a KDTree from a point set to facilitate queries\r\n    """"""\r\n    from scipy.spatial import KDTree, cKDTree\r\n    if as_cKD:\r\n        t = cKDTree(a)\r\n    else:\r\n        t = KDTree(a)\r\n    return t\r\n\r\ndef _kd_data(as_cKD=True):\r\n    """""" testing data for KDTree\r\n    """"""\r\n#    a = np.random.randint(0, 100, size=(20,2))\r\n    xs = np.array([19.0, 82, 43, 96, 37, 10, 66, 23, 80, 13, 94, 91,\r\n                     61, 43, 67, 78, 76, 34, 56,  3]) * 1.0\r\n    ys = np.array([22.0, 91, 89, 40, 64, 94,  6, 94, 67, 18, 95, 66,\r\n                     87,  1, 41, 50, 20, 16, 84, 98])\r\n    a = np.array(list(zip(xs, ys)))\r\n    idx = a[:, 0].argsort()\r\n    a_s = a[idx]  # sort columns ascending\r\n    t = kd_tree(a_s, as_cKD=as_cKD)\r\n    return a, idx, a_s, t\r\n\r\ndef sparse_(t, dist=np.inf):\r\n    """"""form a sparse matrix from a tree\r\n    \'dok_matrix\', \'coo_matrix\', \'dict\', or \'ndarray\'. Default: \'dok_matrix\'.\r\n    a0 =t.sparse_distance_matrix(t, max_distance=np.inf,\r\n                                 output_type=\'dok_matrix\')\r\n    (0, 0)        0.0\r\n    (1, 0)        8.06225774829855\r\n    (2, 0)        80.62257748298549\r\n    (3, 0)        77.6659513557904\r\n     :  :\r\n    (17, 19)      26.476404589747453\r\n    (18, 19)      55.036351623268054\r\n    (19, 19)      0.0\r\n    \r\n    a0.toarray()\r\n    array([[ 0.  ,  8.06, 80.62 ...],  top left\r\n           [ 8.06,  0.  , 76.06 ...],\r\n           [80.62, 76.06,  0.   ...],\r\n           ...\r\n           [...  0.  , 29.15, 26.48],\r\n           [... 29.15,  0.  , 55.04],\r\n           [... 26.48, 55.04,  0.  ]]) bottom right\r\n\r\n    np.tril(a0.toarray()) as alternative\r\n    """"""\r\n    d = t.sparse_distance_matrix(t, dist, p=2)\r\n    return d\r\n\r\n\r\ndef xy_sort(a):\r\n    """"""Sort 2D array assumed to be coordinates, b x, then y, using argsort.\r\n    Returns the sorted array and the indices of the original positions in the\r\n    input array.\r\n\r\n    see: view_sort in arraytools.tools\r\n    """"""\r\n    a_view = a.view(a.dtype.descr * a.shape[1])\r\n    idx =np.argsort(a_view, axis=0, order=(a_view.dtype.names)).ravel()\r\n    a = np.ascontiguousarray(a[idx])\r\n    return a, idx\r\n\r\n\r\ndef nn_kdtree(a, N=3, sorted=True, to_tbl=True, as_cKD=True):\r\n    """"""Produce the N closest neighbours array with their distances using\r\n    scipy.spatial.KDTree as an alternative to einsum.\r\n\r\n    Parameters:\r\n    -----------\r\n    a : array\r\n        Assumed to be an array of point objects for which `nearest` is needed.\r\n    N : integer\r\n        Number of neighbors to return.  Note: the point counts as 1, so N=3 \r\n        returns the closest 2 points, plus itself.\r\n        For table output, max N is limited to 5 so that the tabular output\r\n        isn\'t ridiculous.\r\n    sorted : boolean\r\n        A nice option to facilitate things.  See `xy_sort`.  Its mini-version\r\n        is included in this function.\r\n    to_tbl : boolean\r\n        Produce a structured array output of coordinate pairs and distances.\r\n    as_cKD : boolean\r\n        Whether to use the `c` compiled or pure python version\r\n\r\n    References:\r\n    -----------\r\n    `<https://stackoverflow.com/questions/52366421/how-to-do-n-d-distance-\r\n    and-nearest-neighbor-calculations-on-numpy-arrays/52366706#52366706>`_.\r\n    \r\n    `<https://stackoverflow.com/questions/6931209/difference-between-scipy-\r\n    spatial-kdtree-and-scipy-spatial-ckdtree/6931317#6931317>`_.\r\n    """"""\r\n    def _xy_sort_(a):\r\n        """"""mini xy_sort""""""\r\n        a_view = a.view(a.dtype.descr * a.shape[1])\r\n        idx =np.argsort(a_view, axis=0, order=(a_view.dtype.names)).ravel()\r\n        a = np.ascontiguousarray(a[idx])\r\n        return a, idx\r\n    #\r\n    def xy_dist_headers(N):\r\n        """"""Construct headers for the optional table output""""""\r\n        vals = np.repeat(np.arange(N), 2)\r\n        names = [\'X_{}\', \'Y_{}\']*N + [\'d_{}\']*(N-1)\r\n        vals = (np.repeat(np.arange(N), 2)).tolist() + [i for i in range(1, N)]\r\n        n = [names[i].format(vals[i]) for i in range(len(vals))]\r\n        f = [\'<f8\']*N*2 + [\'<f8\']*(N-1) \r\n        return list(zip(n,f))\r\n    #    \r\n    from scipy.spatial import cKDTree, KDTree\r\n    #\r\n    idx_orig = []\r\n    if sorted:\r\n        a, idx_orig = _xy_sort_(a)\r\n    # ---- query the tree for the N nearest neighbors and their distance\r\n    if as_cKD:\r\n        t = cKDTree(a)\r\n    else:\r\n        t = KDTree(a)\r\n    dists, indices = t.query(a, N)\r\n    if to_tbl and (N<=5):\r\n        dt = xy_dist_headers(N)  # --- Format a structured array header\r\n        xys = a[indices]\r\n        new_shp = (xys.shape[0], np.prod(xys.shape[1:]))\r\n        xys = xys.reshape(new_shp)\r\n        ds = dists[:, 1:]  #[d[1:] for d in dists]\r\n        arr = np.concatenate((xys, ds), axis=1)\r\n        arr = arr.view(dtype=dt).squeeze()\r\n        return arr\r\n    else:\r\n        dists = dists.view(np.float64).reshape(dists.shape[0], -1)\r\n        return dists #np.array(indices) #, idx_orig]\r\n\r\n\r\ndef sequential_dist(a):\r\n    """"""sequential distances, an array of coordinates\r\n    """"""\r\n    diff = a[:-1] - a[1:]\r\n    dis = np.einsum(\'ij,ij->i\', diff, diff)\r\n    return dis\r\n\r\n\r\ndef _data_():\r\n    """"""data for memmap tests\r\n    """"""\r\n    f0 = r""C:\\GIS\\A_Tools_scripts\\Polygon_lineTools\\Data\\samplepoints3_2d.npy""\r\n    f1 = r""C:\\GIS\\A_Tools_scripts\\Polygon_lineTools\\Data\\samplepoints3_3d.npy""\r\n    a0 = np.load(f0)\r\n    shp0 = a0.shape\r\n    del a0\r\n    a1 = np.load(f1)\r\n    shp1 = a1.shape\r\n    del a1\r\n    a = np.memmap(f0, dtype=\'float64\', mode=\'r\', shape=shp0)\r\n    b = np.memmap(f1, dtype=\'float64\', mode=\'r\', shape=shp1)\r\n    return a, b\r\n\r\ndef _data_2():\r\n    """"""\r\n    """"""\r\n    f = r""\\points_2000_from_to.npy""\r\n    p = r""C:\\GIS\\A_Tools_scripts\\Polygon_lineTools\\Data""\r\n    fp = p + f\r\n    a = np.load(fp)\r\n    names = list(a.dtype.names[1:])\r\n    N = a.shape[0]\r\n    m = len(names)\r\n    a_s = a[names]\r\n    a_s = np.copy(a_s.view(np.float64).reshape(N, m))\r\n    xy0 = a_s[:, :2]\r\n    xy1 = a_s[:, 2:4]\r\n    uni0, idx0, inv0, cnts0 = np.unique(xy0, True, True, True, axis=0)\r\n    uni1, idx1, inv1, cnts1 = np.unique(xy1, True, True, True, axis=0)\r\n    uni_from = xy0[idx0]\r\n    uni_to = xy1[idx1]\r\n    return xy0, uni_from, uni_to\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n\r\n'"
all_scripts/a_io.py,33,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\na_io.py\r\n=======\r\n\r\nScript :   a_io.py\r\n\r\nAuthor :   Dan.Patterson@carleton.ca\r\n\r\nModified : 2018-12-30\r\n\r\nPurpose : Basic io tools for numpy arrays and arcpy\r\n\r\nNotes :\r\n::\r\n    1.  load_npy    - load numpy npy files\r\n    2.  save_npy    - save array to *.npy format\r\n    3.  load_txt    - read array created by save_txtt\r\n    4.  save_txt    - save array to npy format\r\n    5.  arr_json    - save to json format\r\n    6-9 dict<->array conversions\r\n    10. excel_np    - convert xls/xlsx files to numpy structured/recarray\r\n\r\n---------------------------------------------------------------------\r\n""""""\r\n# pylint: disable=C0103\r\n# pylint: disable=R1710\r\n# pylint: disable=R0914\r\n\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\n\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n__all__ = [\'dtype_info\',\r\n           \'load_npy\',\r\n           \'save_npy\',\r\n           \'load_txt\',\r\n           \'save_txt\',\r\n           \'arr_json\',\r\n           \'dict_arrays\',\r\n           \'iterable_dict\',\r\n           \'dict_struct\',\r\n           \'struct_dict\',\r\n           \'excel_np\'\r\n           ]\r\n\r\n\r\ndef dtype_info(a=None, as_string=False):\r\n    """"""Return dtype information for a structured/recarray\r\n    """"""\r\n    dt = a.dtype.descr\r\n    names = [i[0] for i in dt]\r\n    formats = [i[1] for i in a.dtype.descr]\r\n    if as_string:\r\n        names = "", "".join([i[1] for i in names])\r\n        formats = "", "".join([i[1] for i in formats])\r\n    return names, formats\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# (1) load_npy .... code section ---\r\ndef load_npy(f_name, all_info=False):\r\n    """"""load a well formed `npy` file representing a structured array\r\n\r\n    Returns\r\n    -------\r\n        The array, the description, field names and their size.\r\n    """"""\r\n    a = np.load(f_name)\r\n    if all_info:\r\n        desc = a.dtype.descr\r\n        nms = a.dtype.names\r\n        sze = [i[1] for i in a.dtype.descr]\r\n        return a, desc, nms, sze\r\n    #\r\n    return a\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# (2) read_npy .... code section ---\r\ndef save_npy(a, f_name):\r\n    """"""Save an array as an npy file.\r\n\r\n    The type of data in each column is arbitrary.  It will be cast to the\r\n    given dtype at runtime\r\n    """"""\r\n    np.save(f_name, a)\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# (3) read_txt .... code section ---\r\ndef load_txt(name=""arr.txt""):\r\n    """"""Read the structured/recarray created by save_txt.\r\n\r\n    dtype : data type\r\n        If `None`, it allows the structure to be read from the array.\r\n\r\n    delimiter : string\r\n        Use a comma delimiter by default.\r\n\r\n    names : boolean\r\n        If `True`, the first row contains the field names.\r\n    encoding :\r\n        Set to None to use system default\r\n    see np.genfromtxt for all *args and **kwargs.\r\n    """"""\r\n    a = np.genfromtxt(name, dtype=None, delimiter="","",\r\n                      names=True, autostrip=True, encoding=None)  # ,skip_header=1)\r\n    return a\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# (4) save_txt .... code section ---\r\ndef save_txt(a, name=""arr.txt"", sep="", "", dt_hdr=True):\r\n    """"""Save a NumPy structured, recarray to text.\r\n\r\n    Requires:\r\n    --------\r\n    a     : array\r\n        input array\r\n    fname : filename\r\n        output filename and path otherwise save to script folder\r\n    sep   : separator\r\n        column separater, include a space if needed\r\n    dt_hdr: boolean\r\n        if True, add dtype names to the header of the file\r\n    """"""\r\n    a_names = "", "".join(i for i in a.dtype.names)\r\n    hdr = ["""", a_names][dt_hdr]  # use """" or names from input array\r\n    s = np.array(a.tolist(), dtype=np.unicode_)\r\n    widths = [max([len(i) for i in s[:, j]])\r\n              for j in range(s.shape[1])]\r\n    frmt = sep.join([""%{}s"".format(i) for i in widths])\r\n    # vals = "", "".join([i[1] for i in a.dtype.descr])\r\n    np.savetxt(name, a, fmt=frmt, header=hdr, comments="""")\r\n    print(""\\nFile saved..."")\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# (5) arr_json .... code section ---\r\ndef arr_json(file_out, arr=None):\r\n    """"""Send an array out to json format. Use json_arr to read the file.\r\n    No error checking\r\n    """"""\r\n    import json\r\n    import codecs\r\n    json.dump(arr.tolist(), codecs.open(file_out, \'w\', encoding=\'utf-8\'),\r\n              sort_keys=True, indent=4)\r\n    # ----\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# (6 - 9) Dictionary - array section\r\n# dict_arrays, iterable_dict, struct_dict\r\ndef dict_arrays(d):\r\n    """"""Dictionary to arrays\r\n\r\n    Parameters:\r\n    -----------\r\n    d : dictionary\r\n        The dictionary to convert to arrays\r\n\r\n    Returns:\r\n    --------\r\n    A list, which can be converted to an array if needed.  It will probably\r\n    be an `object` array if the array types are mixed.\r\n\r\n    >>> d =  {\'A\': 1, \'B\': [1, 2], \'C\': (3.0, 4), \'D\': (5.0, 6.0),\r\n              \'E\': [\'a\', \'bb\'], \'F\': [\'ccc\', 7, 8.0], \'G\': [[1, 2], [3, 4]]}\r\n    >>> arr = np.asarray(dict_arrays(d))\r\n    >>> arr\r\n    array([array(1), array([1, 2]), array([3., 4.]), array([5., 6.]),\r\n           array([\'a\', \'bb\'], dtype=\'<U2\'),\r\n           array([\'ccc\', 7, 8.0], dtype=object),\r\n           array([[1, 2], [3, 4]], dtype=object)], dtype=object)\r\n    """"""\r\n#    def dtstr(v):\r\n    keys = d.keys()\r\n    dts = []\r\n    vals = []\r\n    for k in keys:\r\n        v = d[k]\r\n        if isinstance(v, int):\r\n            dts.append(\'<i4\')\r\n        elif isinstance(v, float):\r\n            dts.append(\'<f8\')\r\n        if isinstance(v, (list, tuple, np.ndarray)):\r\n            if all(isinstance(x, (int, float)) for x in v):\r\n                dts.append(np.array(v).dtype.str)\r\n            elif all(isinstance(x, (str)) for x in v):\r\n                m = len(max(v, key=len))\r\n                dts.append(\'<U\' + str(m))\r\n            else:\r\n                dts.append(\'O\')\r\n        vals.append(v)\r\n    arrs = [np.array(i[0], dtype=i[1]) for i in list(zip(vals, dts))]\r\n    return arrs\r\n\r\n\r\ndef iterable_dict(a, use_numbers=True):\r\n    """"""Iterable (list, tuple, np.ndarray) to dictionary\r\n\r\n    Parameters:\r\n    -----------\r\n    a : iterable\r\n        The iterable to convert to a dictionary.  Normally useful if it is a\r\n        list of lists or np.ndarray\r\n    use_numbers : boolean\r\n        True, the dictionary keys are assigned 0...n.  False, letters are\r\n        sliced from the lett_lst below\r\n\r\n    Returns:\r\n    --------\r\n    A dictionary.\r\n\r\n    >>> # see `arr` in `dict_arrays`\r\n    >>> iterable_dict(arr, use_numbers=False)\r\n    >>> {\'A\': array(1), \'B\': array([1, 2]), \'C\': array([3., 4.]),\r\n         \'D\': array([5., 6.]), \'E\': array([\'a\', \'bb\'], dtype=\'<U2\'),\r\n         \'F\': array([\'ccc\', 7, 8.0], dtype=object),\r\n         \'G\': array([[1, 2], [3, 4]], dtype=object)}\r\n\r\n    Simpler cases\r\n\r\n    >>> z = \'abcde\'\r\n    >>> iterable_dict(z)\r\n    {0: \'a\', 1: \'b\', 2: \'c\', 3: \'d\', 4: \'e\'}\r\n    >>> iterable_dict(z, False)\r\n    {\'A\': \'a\', \'B\': \'b\', \'C\': \'c\', \'D\': \'d\', \'E\': \'e\'}\r\n    >>> iterable_dict([1,\'a\', 2], False)\r\n    {\'A\': 1, \'B\': \'a\', \'C\': 2}\r\n\r\n    """"""\r\n    if use_numbers:\r\n        d = {i: a[i] for i, e in enumerate(a)}\r\n    else:\r\n        lett_lst = list(\'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\')\r\n        d = {lett_lst[i]: a[i] for i, e in enumerate(a)}\r\n    return d\r\n\r\n\r\ndef dict_struct(d):\r\n    """"""Dictionary to simple structured/recarray\r\n    """"""\r\n    if not isinstance(d, dict):\r\n        return d\r\n    lens = [len(i) for i in d.values()]\r\n    if max(lens) != min(lens):\r\n        return d\r\n    #\r\n    names = [k for k in d.keys()]\r\n    data = list(d.values())\r\n    dt_str = [i.dtype.str for i in data]\r\n    dt = list(zip(names, dt_str))\r\n    N = lens[0]\r\n    n = len(dt_str)\r\n    arr = np.zeros((N,), dtype=dt)\r\n    for i in range(n):\r\n        arr[names[i]] = data[i]\r\n    return arr\r\n\r\n\r\ndef struct_dict(a):\r\n    """"""Structured/recarray to dictionary\r\n    """"""\r\n    if not isinstance(a, np.ndarray):\r\n        return a\r\n    if a.dtype.names is None:\r\n        return a\r\n    names = a.dtype.names\r\n    return {i: a[i].tolist() for i in names}\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# (10) excel_np\r\ndef excel_np(path, sheet_num=0, int_null=-999):\r\n    """"""Read excel files to numpy structured/record arrays.  Your spreadsheet\r\n    must adhere to simple rules::\r\n      - first row must contain the field names for the output array\r\n      - no blank rows or columns, basically, no fluff or formatting\r\n      - if you have nodata values, put them in, since blank cells will be\r\n        \'corrected\' as best as possible.\r\n      - text and numbers in a column, results in a text column\r\n\r\n    Parameters:\r\n    -----------\r\n    path : text\r\n        Full path to the xls, xlsx file\r\n    sheet_num : integer\r\n        Sheets are numbered from 0.\r\n\r\n    int_null : integer\r\n        Integer value to use for nulls. Strings have `None`, floats `np.nan`\r\n        but integers have no equivalent so you have to provide one.\r\n        you could use np.iinfo(np.intXX).min where XX is 8, 16, 32 to reflect\r\n        the appropriate integer minimums\r\n\r\n    Returns:\r\n    --------\r\n    A numpy structured array is returned.  Excel only uses float or string\r\n    data, so attempts are made to coerse integer columns by comparing the\r\n    float vs int versions of the arrays.  A tad of a kludge, but it works.\r\n\r\n    The first row\'s data type is compared to its matching column data type.\r\n    If they match, then it is used as the dtype.  If there is a mismatch an\r\n    attempt is made to recover numeric data by assigning blanks etc in numeric\r\n    columns a value of np.nan.\r\n\r\n    String/text columns are check for empty cells, \'\', """" and that ever so\r\n    ugly invisible space.\r\n\r\n    Notes:\r\n    ------\r\n    >>> aString = open(\'c:/temp/test.xlsx\',\'rb\').read()\r\n    >>> book_ = open_workbook(file_contents=aString)\r\n    >>> dir(book_):\r\n        get_sheet, nsheets, sheet_by_index, sheet_by_name etc....\r\n\r\n    Now you can read a sheet\r\n\r\n    >>> sheet = book_.sheet_by_index(0)  # first sheet\r\n    >>> sheet.col_types(0)\r\n\r\n    References:\r\n    ----------\r\n    `<https://media.readthedocs.org/pdf/xlrd/latest/xlrd.pdf>`_.\r\n    """"""\r\n    def _values(sheet, rows, cols):\r\n        """"""return cell types for the above.  Skip the first row\r\n        Not used .... just kept for future reference\r\n        """"""\r\n        ar = []\r\n        for i in range(1, rows):\r\n            c = []\r\n            for j in range(cols):\r\n                c.append(sheet.cell_values(i, j))  # sheet.cell_types also\r\n            ar.append(c)\r\n        return ar\r\n\r\n    def isfloat(a):\r\n        """"""float check""""""\r\n        try:\r\n            i = float(a)\r\n            return i\r\n        except ValueError:\r\n            return np.nan\r\n\r\n    def punc_space(name):\r\n        """"""delete punctuation and spaces and replace with \'_\'""""""\r\n        punc = list(\'!""#$%&\\\'()*+,-./:;<=>?@[\\\\]^`{|}~ \')\r\n        return """".join([[i, \'_\'][i in punc] for i in name])\r\n\r\n    import xlrd\r\n    w = xlrd.open_workbook(path)        # xlrd.book.Book class\r\n    sheet = w.sheet_by_index(sheet_num) # sheet by number\r\n    # sheet = w.sheet_by_name(\'test\')   # case sensitive, not implemented\r\n    names = sheet.row_values(0)         # clean these up later\r\n    cols = sheet.ncols\r\n    rows = sheet.nrows\r\n    col_data = [sheet.col_values(i, 1, rows) for i in range(cols)]\r\n    row_guess = sheet.row_values(1)\r\n    row_dts = [np.asarray(i).dtype.kind for i in row_guess]\r\n    col_dts = [np.asarray(col_data[i]).dtype.kind\r\n               for i in range(cols)]\r\n    clean = []\r\n    for i in range(len(row_dts)):\r\n        c = col_data[i]\r\n        if row_dts[i] == col_dts[i]:    # same dtype... send to array\r\n            ar = np.asarray(c)\r\n        if row_dts[i] == \'f\':           # float? if so, substitute np.nan\r\n            ar = np.array([isfloat(i) for i in c])\r\n            is_nan = np.isnan(ar)       # find the nan values, then check\r\n            not_nan = ar[~is_nan]       # are the floats == ints?\r\n            if np.all(np.equal(not_nan, not_nan.astype(\'int\'))):  # integer?\r\n                ar[is_nan] = int_null   # assign the integer null\r\n                ar = ar.astype(\'int\')\r\n        elif row_dts[i] in (\'U\', \'S\'):  # unicode/string... send to array\r\n            ar = np.char.strip(ar)\r\n            ar = np.where(np.char.str_len(ar) == 0, \'None\', ar)\r\n        else:\r\n            ar = np.asarray(c)\r\n        clean.append(ar)\r\n    # ---- assemble the columns for the array ----\r\n    dt_str = [i.dtype.str for i in clean]\r\n    names = [i.strip() for i in names]      # clean up leading/trailing spaces\r\n    names = [punc_space(i) for i in names]  # replace punctuation and spaces\r\n    dts_name = list(zip(names, dt_str))\r\n    arr = np.empty((rows-1,), dtype=dts_name)\r\n    cnt = 0\r\n    for i in names:\r\n        arr[i] = clean[cnt]\r\n        cnt += 1\r\n    return arr\r\n\r\n\r\ndef openxl_np(path):\r\n    """"""read excel using openpyxl\r\n    A = np.array([[i.value for i in j] for j in ws[\'C1\':\'E38\']])\r\n\r\n    `<https://stackoverflow.com/questions/35823835/reading-excel-file-is-\r\n    magnitudes-slower-using-openpyxl-compared-to-xlrd>`_.\r\n\r\n    cols = sheet.max_column  # sheet.min_column  1 to max\r\n    rows = sheet.max_row  # sheet.min_row\r\n    """"""\r\n    import openpyxl as op\r\n    wb = op.load_workbook(path, data_only=True, guess_types=True,\r\n                          keep_links=False)\r\n    sheets = wb.sheetnames\r\n    sheet = wb[sheets[0]]\r\n    data = list(sheet.values)\r\n    #header = data[0]\r\n    return data\r\n\r\ndef _demo_npy():\r\n    """"""\r\n    """"""\r\n    _npy = ""/Data/sample_10K.npy""  # change to one in the Data folder\r\n    _npy = ""{}"".format(script.replace(""a_io.py"", _npy))\r\n    return _npy\r\n\r\ndef _demo_xlsx():\r\n    """"""\r\n    """"""\r\n#    _xlsx = ""/Data/test2.xlsx""  # page 0 or page 1\r\n    _xlsx = ""/Data/Test_10K.xlsx""\r\n    path = script.rpartition(""/"")[0] + _xlsx\r\n    arr = excel_np(path, 0)\r\n    return arr\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n#    fname = _demo()\r\n'"
all_scripts/aggregate_demo.py,4,"b'""""""\r\nScript:  aggregate_demo.py\r\nModified: 2016-01-30\r\nAuthor:   Dan.Patterson@carleton.ca\r\nPurpose:  To demonstrate aggregation of raster data without the spatial analyst\r\n    extension.  A sample raster is created and methods to convert an array to\r\n    a raster and vice versa are shown.\r\nNotes:\r\n- RasterToNumPyArray(in_raster, {lower_left_corner},\r\n                         {ncols}, {nrows}, {nodata_to_value})\r\n    arr = arcpy.RasterToNumPyArray(rast) #,arcpy.Point(300000,5025000),10)\r\n\r\n- NumPyArrayToRaster(in_array, {lower_left_corner}, {x_cell_size},\r\n                     {y_cell_size}, {value_to_nodata})\r\n    rast = arcpy.NumPyArrayToRaster(a, arcpy.Point(300000,5025000),10,10,-9999)\r\n    rast.save(r""F:\\Demos\\raster_ops\\test_agg"") # esri grid, or add tif, jpg etc\r\n\r\n""""""\r\nimport numpy as np\r\nfrom numpy.lib.stride_tricks import as_strided\r\nfrom textwrap import dedent\r\nimport arcpy\r\n\r\nnp.set_printoptions(edgeitems=3, linewidth=80, precision=2,\r\n                    suppress=True, threshold=200)\r\n\r\narcpy.env.overwriteOutput = True\r\n\r\n\r\ndef block_a(a, block=(3, 3)):\r\n    """"""Provide a 2D block view of a 2D array. No error checking made. Columns\r\n    and rows outside of the block are truncated.\r\n    """"""\r\n    a = np.ascontiguousarray(a)\r\n    r, c = block\r\n    shape = (a.shape[0]//r, a.shape[1]//c) + block\r\n    strides = (r*a.strides[0], c*a.strides[1]) + a.strides\r\n    b_a = as_strided(a, shape=shape, strides=strides)\r\n    return b_a\r\n\r\n\r\ndef agg_demo(n):\r\n    """"""Run the demo with a preset array shape and content.  See the header\r\n    """"""\r\n    a = np.random.randint(0, high=5, size=n*n).reshape((n, n))\r\n    rast = arcpy.NumPyArrayToRaster(a, x_cell_size=10)\r\n    agg_rast = arcpy.sa.Aggregate(rast, 2, ""MAXIMUM"")\r\n    agg_arr = arcpy.RasterToNumPyArray(agg_rast)\r\n    # ,arcpy.Point(300000,5025000),10)\r\n    # --- a_s is the strided array, a_agg_max is the strided array max\r\n    a_s = block_a(a, block=(2, 2))\r\n    a_agg_max = a_s.max(axis=(2, 3))\r\n    # ---\r\n    frmt = """"""\r\n    Input array... shape {} rows/cols\r\n    {}\\n\r\n    Arcpy.sa aggregate..\r\n    {}\\n\r\n    Numpy aggregate..\r\n    {}\\n\r\n    All close? {}\r\n    """"""\r\n    yup = np.allclose(agg_arr, a_agg_max)\r\n    print(dedent(frmt).format(a.shape, a, agg_arr, a_agg_max, yup))\r\n    return a, agg_arr, a_s, a_agg_max\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    """""" Returns the input array, it\'s aggregation raster from arcpy.sa,\r\n    the raster representation of the raster and the block representation\r\n    and the aggregation array.\r\n    """"""\r\n    n = 5000\r\n#    a, agg_arr, a_s, a_agg_max = agg_demo(n)\r\n\r\n\r\n\r\n\r\n'"
all_scripts/angles.py,17,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\nangles\r\n======\r\n\r\nScript:   angles.py\r\n\r\nAuthor:   Dan.Patterson@carleton.ca\r\n\r\nModified: 2018-03-31\r\n\r\nPurpose:  tools for working with numpy arrays\r\n\r\nUseage:\r\n\r\nReferences:\r\n-----------\r\n[1] https://en.wikipedia.org/wiki/Regular_polygon\r\n\r\nNotes:\r\n------\r\nsum of interior angles   (n-2) * 180, where n is the number of sides\r\n\r\nn = 3  180 triangle\r\n\r\nn = 4  360 rectangle\r\n\r\nn = 5  540 pentagram\r\n\r\nn = 6  720 hexagram``\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nfrom textwrap import dedent\r\nimport numpy as np\r\nimport arcpy\r\n\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\r\n      \'float\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\n# ---- arcpytools functions --------------------------------------------------\r\n# ---- You can remove these and import `arcpytools` ----\r\n# ------------------------------------------------------\r\ndef _describe(in_fc):\r\n    """"""Simply return the arcpy.da.Describe object\r\n\r\n    *desc.keys()* : an abbreviated list...\r\n    ::\r\n        \'OIDFieldName\'... \'areaFieldName\', \'baseName\'... \'catalogPath\',\r\n        ... \'dataType\'... \'extent\', \'featureType\', \'fields\', \'file\'... \'hasM\',\r\n        \'hasOID\', \'hasZ\', \'indexes\'... \'lengthFieldName\'... \'name\', \'path\',\r\n        \'rasterFieldName\', ..., \'shapeFieldName\', \'shapeType\',\r\n        \'spatialReference\',  ...\r\n    """"""\r\n    return arcpy.da.Describe(in_fc)\r\n\r\n\r\ndef fc_info(in_fc, prn=False):\r\n    """"""Return basic featureclass information, including...\r\n\r\n    Parameters\r\n    ----------\r\n    - ``shp_fld  :``\r\n        field name which contains the geometry object\r\n    - ``oid_fld  :``\r\n        the object index/id field name\r\n    - ``SR       :``\r\n        spatial reference object (use SR.name to get the name)\r\n    - ``shp_type :``\r\n        shape type (Point, Polyline, Polygon, Multipoint, Multipatch)\r\n    - ``others   :``\r\n        areaFieldName, baseName, catalogPath, featureType, fields,\r\n        hasOID, hasM, hasZ, path\r\n    - ``all_flds :``\r\n         [i.name for i in desc[\'fields\']]\r\n    """"""\r\n    desc = _describe(in_fc)\r\n    args = [\'shapeFieldName\', \'OIDFieldName\', \'shapeType\', \'spatialReference\']\r\n    shp_fld, oid_fld, shp_type, SR = [desc[i] for i in args]\r\n    if prn:\r\n        frmt = ""FeatureClass:\\n   {}"".format(in_fc)\r\n        f = ""\\n{!s:<16}{!s:<14}{!s:<10}{!s:<10}""\r\n        frmt += f.format(*args)\r\n        frmt += f.format(shp_fld, oid_fld, shp_type, SR.name)\r\n        tweet(frmt)\r\n    else:\r\n        return shp_fld, oid_fld, shp_type, SR\r\n\r\n\r\ndef tweet(msg):\r\n    """"""Produce a message for both arcpy and python::\r\n\r\n    `msg` - a text message\r\n    """"""\r\n    m = ""\\n{}\\n"".format(msg)\r\n    arcpy.AddMessage(m)\r\n    print(m)\r\n\r\n\r\n# ---- end of `arcpytools` functions -----------------------------------------\r\n#\r\n\r\ndef _geo_array(polys):\r\n    """"""Convert polygon objects to arrays\r\n    """"""\r\n    arrays = [np.asarray(pt.__geo_interface__[\'coordinates\']).squeeze()\r\n              for pt in polys]  # for p in pt]\r\n    return arrays\r\n\r\n\r\ndef _get_shapes(in_fc):\r\n    """"""Get shapes from a featureclass, in_fc, using SHAPE@ returning\r\n    :  [<Polygon object at....>, ... (<Polygon object at....>]\r\n    """"""\r\n    with arcpy.da.SearchCursor(in_fc, \'SHAPE@\') as cursor:\r\n        a = [row[0] for row in cursor]\r\n    return a\r\n\r\n\r\ndef arcpnts_poly(in_, out_type=\'Polygon\', SR=None):\r\n    """"""Convert arcpy Point lists to poly* features\r\n    : out_type - either \'Polygon\' or \'Polyline\'\r\n    :\r\n    """"""\r\n    s = []\r\n    for i in in_:\r\n        arr = arcpy.Array(i)\r\n        if out_type == \'Polyline\':\r\n            g = arcpy.Polyline(arr, SR)\r\n        elif out_type == \'Polygon\':\r\n            g = arcpy.Polygon(arr, SR)\r\n        elif out_type == \'Points\':\r\n            g = arcpy.arcpy.Multipoint(arr[0], SR)\r\n        s.append(g)\r\n    return s\r\n\r\n\r\ndef angle_seq(a):\r\n    """"""Sequential two point angle along a poly* feature\r\n    :\r\n    : angle = atan2(vector2.y, vector2.x) - atan2(vector1.y, vector1.x);\r\n    : Accepted answer from the poly_angles link\r\n    """"""\r\n    frum = a[:-1]\r\n    too = a[1:]\r\n    diff = too - frum\r\n    ang_ab = np.arctan2(diff[:, 1], diff[:, 0])\r\n    return np.degrees(ang_ab)\r\n\r\n\r\ndef angles_poly(a, inside=True, in_deg=True):\r\n    """"""Sequential angles from a poly* shape\r\n    : a - an array of points, derived from a polygon/polyline geometry\r\n    : inside - determine inside angles, outside if False\r\n    : in_deg - convert to degrees from radians\r\n    :\r\n    :Notes:\r\n    :-----\r\n    : 2 points - subtract 2nd and 1st points, effectively making the\r\n    :  calculation relative to the origin and x axis, aka... slope\r\n    : n points - sequential angle between 3 points\r\n    : - Check whether 1st and last points are duplicates.\r\n    :   \'True\' for polygons and closed loop polylines, it is checked using\r\n    :   np.allclose(a[0], a[-1])  # check first and last point\r\n    : - a rolling tuple is constructed to produce the point triplets\r\n    :   r = (-1,) + tuple(range(len(a))) + (0,)\r\n    :   for np.arctan2(np.linalg.norm(np.cross(ba, bc)), np.dot(ba, bc))\r\n    :\r\n    :Reference:\r\n    :---------\r\n    : https://stackoverflow.com/questions/21483999/\r\n    :         using-atan2-to-find-angle-between-two-vectors\r\n    :  *** keep to convert object to array\r\n    : a - a shape from the shape field\r\n    : a = p1.getPart()\r\n    : b =np.asarray([(i.X, i.Y) if i is not None else ()\r\n    :                for j in a for i in j])\r\n    """"""\r\n    # a = a.getPart()\r\n    # a = np.asarray([[i.X, i.Y] for j in a for i in j])\r\n    if len(a) < 2:\r\n        return None\r\n    elif len(a) == 2:  # **** check\r\n        ba = a[1] - a[0]\r\n        return np.arctan2(*ba[::-1])\r\n    else:\r\n        angles = []\r\n        dx, dy = a[0] - a[-1]\r\n        if np.allclose(dx, dy):  # closed loop\r\n            a = a[:-1]\r\n            r = (-1,) + tuple(range(len(a))) + (0,)\r\n        else:\r\n            r = tuple(range(len(a)))\r\n        for i in range(len(r)-2):\r\n            p0, p1, p2 = a[r[i]], a[r[i+1]], a[r[i+2]]\r\n            ba = p1 - p0\r\n            bc = p1 - p2\r\n            cr = np.cross(ba, bc)\r\n            dt = np.dot(ba, bc)\r\n            ang = np.arctan2(np.linalg.norm(cr), dt)\r\n            angles.append(ang)\r\n    if in_deg:\r\n        angles = np.degrees(angles)\r\n    return angles\r\n\r\n\r\ndef call_angles(a):\r\n    """"""Call angles for each shape\r\n    """"""\r\n    out = []\r\n    for i in a:\r\n        out.append(angles_poly(i, inside=True, in_deg=True))\r\n    return out\r\n\r\n\r\ndef prn_report(arrs, out):\r\n    """"""Print a report summarizing the output\r\n    """"""\r\n    hdr = """"""\r\n    :----------------------------------------------------------------------\r\n    :Angle report....\r\n    """"""\r\n    frmt = """"""\r\n    ({}) number of angles... ({})\r\n    :array points...\r\n    {}\\n\r\n    :interior angles {}\r\n    :sum interior... {}\r\n    """"""\r\n    print(dedent(""\\n{}"").format(hdr))\r\n    cnt = 0\r\n    for i in arrs:\r\n        args = [cnt, len(i), i, out[cnt], sum(out[cnt])]\r\n        prn = [str(i) for i in args]\r\n        print(dedent(frmt).format(*prn))\r\n        cnt += 1\r\n\r\n\r\n# ------------------------------------------------------------------------\r\n# (1) ---- Checks to see if running in test mode or from a tool ----------\r\ndef _demo():\r\n    """"""run when script is in demo mode""""""\r\n    pth = script.replace(\'/angles.py\', \'\')\r\n    in_fc = \'C:/Git_Dan/a_Data/arcpytools_demo.gdb/polylines\'\r\n#    in_fc = \'C:/Git_Dan/a_Data/arcpytools_demo.gdb/three_shapes\'\r\n#    in_fc = pth + \'/geom_data.gdb/three_shapes\'\r\n#    in_fc = r""C:\\Git_Dan\\a_Data\\testdata.gdb\\Carp_5x5""   # full 25 polygons\r\n#    in_fc = r""C:\\Git_Dan\\a_Data\\arcpytools_demo.gdb\\xy1000_tree""\r\n    out_fc = pth + \'/geom_data.gdb/x\'\r\n    out_type = \'Polygon\'\r\n    testing = True\r\n    return in_fc, out_fc, out_type, testing\r\n\r\n\r\ndef _tool():\r\n    """"""run when script is from a tool""""""\r\n    in_fc = sys.argv[1]  #\r\n    out_fc = sys.argv[2]  #\r\n    out_type = sys.argv[3]  # Polygon, Polyline are options\r\n    testing = False\r\n    return in_fc, out_fc, out_type, testing\r\n\r\n# ---- main block ------------------------------------------------------------\r\n#\r\n# (1) check to see if in demo or tool mode\r\n# (2) obtain fc information\r\n# (3) split the fc into two arrays, one geometry, the 2nd attributes\r\n# (4) obtain the shapes and densify\r\n# (5) optionally produce the output fc\r\n\r\n\r\nif len(sys.argv) == 1:\r\n    in_fc, out_fc, out_type, testing = _demo()\r\nelse:\r\n    in_fc, out_fc, out_type, testing = _tool()\r\n\r\nshp_fld, oid_fld, shp_type, SR = fc_info(in_fc)\r\n\r\n\r\n# ---- produce output --------------------------------------------------------\r\n\r\npolys = _get_shapes(in_fc)\r\narrs = _geo_array(polys)\r\nout = call_angles(arrs)\r\n# out = angles_poly(arrs, inside=True, in_deg=True)  # use for xy1000_tree only\r\n# a0 = np.array([[0., 0.], [0., 10.], [10., 10.], [10., 0.], [0., 0.]])\r\n# a1 = np.array([[20., 20.], [20., 30.], [30., 30.], [30., 20.], [20., 20.]])\r\n# p0, p1, p2 = polys\r\n# b = _get_attributes(in_fc)\r\n\r\n# out_shps = arcpnts_poly(out, out_type=out_type, SR=SR)\r\n# if not testing:\r\n#     if arcpy.Exists(out_fc):\r\n#         arcpy.Delete_management(out_fc)\r\n#     arcpy.CopyFeatures_management(out_shps, out_fc)\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n'"
all_scripts/apt.py,18,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\napt.py ... arcpy tools\r\n======================\r\n\r\nScript :   apt.py ... (arcpytools.py)\r\n\r\nAuthor :   Dan.Patterson@carleton.ca\r\n\r\nModified : 2018-09-12\r\n\r\nPurpose :  tools for working with arcpy and numpy arrays\r\n\r\nNotes\r\n-----\r\nFunctions\r\n---------\r\n\r\n-  _arr_common : common function for poly* features\r\n-  _shapes_fc : convert shapes to featureclass\r\n-  arr_pnts : array to points\r\n-  arr_polyline : array to polyline\r\n-  arr_polygon : array to polygon\r\n-  array_fc : array to featureclass\r\n-  array_struct : array to structured array\r\n-  pnts_arr : points to array\r\n-  polylines_arr : polylines to array\r\n-  polygons_arr : polygons to array\r\n-  fc_array : featureclass to array\r\n-  change_fld : convert arc to np field types\r\n-  tbl_arr : convert a table to an array\r\n-  tbl_2_np_array : shortcut to TableToNumPyArray\r\n-  to_fc : convert the results back to a featureclass\r\n\r\n**Common variables used in the functions**\r\n\r\n1. Array variables/properties\r\n  - a  : structured/recarray\r\n  - dt : array dtype properties\r\n\r\n2. Featureclass variables/properties\r\n\r\n  -  shps    - point geometry objects needed to create the featureclass\r\n  -  in_fc   - input featureclass\r\n  -  out_fc  - full path and name to the output container (gdb or folder)\r\n  -  SR      - spatial reference object (use SR.name to get the name)\r\n  -  shp_fld - field name which contains the geometry object\r\n  -  oid_fld - the object index/id field name\r\n\r\n3. Field options:\r\n\r\n    [\'OBJECTID\', \'Shape\'], [\'OID@\', \'SHAPE@XY\'],\r\n    [\'OID@\', \'SHAPE@X\', \'SHAPE@Y\'], [\'OID@\', \'SHAPE@JSON\']\r\n\r\n  >>> cur = arcpy.da.SearchCursor(in_fc, field_names, .....)\r\n  >>> cur = arcpy.da.SearchCursor(in_fc, [\'OBJECTID\', \'Shape\'], None, None,\r\n                                  True, (None, None))\r\n\r\nProcedures:\r\n-----------\r\n\r\n1. Split featureclass to array\r\n\r\n  - Split the geometry\r\n  - Split the attributes (TableToNumPyArray)\r\n\r\n2. Create featureclass\r\n\r\n  - geometry : arr_pnts, arr_polylines, arr_polygons\r\n  - join attributes : ExtendTable\r\n\r\nReferences:\r\n-----------\r\n\r\n(1) main link section:  http://pro.arcgis.com/en/pro-app/arcpy/data-access/\r\n\r\n(2) ...extendtable.htm\r\n\r\n>>> ExtendTable (in_table, table_match_field, in_array,\r\n                 array_match_field, {append_only})\r\n\r\n(3) ...featureclasstonumpyarray.htm\r\n\r\n>>> FeatureClassToNumPyArray(in_table, field_names, {where_clause},\r\n                            {spatial_reference}, {explode_to_points},\r\n                            {skip_nulls}, {null_value})\r\n\r\n(4) ...numpyarraytofeatureclass.htm\r\n\r\n>>>  NumPyArrayToFeatureClass(in_array, out_table, shape_fields,\r\n                              {spatial_reference})\r\n\r\n(5) ...tabletonumpyarray.htm\r\n\r\n>>> TableToNumPyArray(in_table, field_names, {where_clause},\r\n                     {skip_nulls}, {null_value})\r\n\r\n---------------------------------------------------------------------\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nfrom textwrap import dedent\r\nimport numpy as np\r\nfrom arraytools.fc_tools._common import fc_info, tweet\r\nimport arcpy\r\n\r\n\r\n__all__ = [\'_arr_common\', \'_split_array\', \'shapes_fc\',\r\n           \'arr_pnts\', \'obj_polyline\', \'obj_polyline\',\r\n           \'struct_polyline\', \'struct_polygon\',\r\n           \'array_fc\', \'array_struct\', \'arc_np\',\r\n           \'pnts_arr\', \'arr_polyline_fc\', \'arr_polygon_fc\',\r\n           \'shapes_fc\', \'_id_geom_array\',\r\n           \'polylines_arr\', \'polygons_arr\',\r\n           \'fc_array\', \'change_fld\',\r\n           \'tbl_arr\', \'to_fc\'\r\n           ]\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=3, linewidth=80, precision=3, suppress=True,\r\n                    threshold=20, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\n# ---- Array to featureclass section ----\r\n\r\ndef _arr_common(a, oid_fld, shp_fld):\r\n    """"""Common structure for polyline, polygon and multipoint features.\r\n    This functions pulls out all the array points that belong to a\r\n    feature so they can be used in shape construction.\r\n\r\n    Requires:\r\n    --------\r\n    a : structured/recarray\r\n\r\n    oid_fld : the object index/id field name\r\n        Normally the `OBJECTID` field, but any field that provides a means to\r\n        determine sequential objects.\r\n\r\n    shp_fld : the table field which contains the geometry\r\n        Usually the `Shape` field.\r\n    """"""\r\n    pts = []\r\n    keys = np.unique(a[oid_fld])\r\n    for k in keys:\r\n        w = np.where(a[oid_fld] == k)[0]\r\n        v = a[shp_fld][w[0]:w[-1] + 1]\r\n        pts.append(v)\r\n    return pts\r\n\r\n\r\n# @time_deco\r\ndef _split_array(a, fld=\'ID\'):\r\n    """"""Split a structured/recarray array, using unique values in a numeric id\r\n    `fld` assumed to be sorted in the correct order which indicates the\r\n    group a record belongs to.  From \'arraytools split_array\'.\r\n    """"""\r\n    return np.split(a, np.where(np.diff(a[fld]))[0] + 1)\r\n\r\n\r\ndef arr_pnts(a, out_fc, shp_fld, SR):\r\n    """"""Make point features from a structured array.\r\n\r\n    Requires:\r\n    --------\r\n    `a` : structured/recarray\r\n\r\n    `out_fc` : featureclass full path and name\r\n\r\n    `shp_fld` : the table field which contains the geometry\r\n\r\n    `SR` : the spatial reference/coordinate system of the geometry\r\n    """"""\r\n    msg0 = ""\\nCreated..\\n{}"".format(out_fc)\r\n    msg1 = ""\\nCan\'t overwrite {}... rename it"".format(out_fc)\r\n    try:\r\n        if arcpy.Exists(out_fc):\r\n            arcpy.Delete_management(out_fc)\r\n        arcpy.da.NumPyArrayToFeatureClass(a, out_fc, shp_fld, SR)\r\n        tweet(msg0)\r\n    except Exception as e:\r\n        tweet(msg1 + str(e))\r\n\r\n\r\ndef obj_polyline(pnts, SR=None):\r\n    """"""Object array of point geometry X, Y coordinates to a polyline, using\r\n    a known spatial reference (SR)\r\n    """"""\r\n    f = []\r\n    for pt in pnts:\r\n        f.append(arcpy.Polyline(arcpy.Array([arcpy.Point(*p)\r\n                                            for p in pt.tolist()]), SR))\r\n    return f\r\n\r\n\r\ndef obj_polygon(pnts, SR=None):\r\n    """"""Object array of point geometry X, Y coordinates to a polygon, using\r\n    a known spatial reference (SR)\r\n    """"""\r\n    f = []\r\n    for pt in pnts:\r\n        f.append(arcpy.Polygon(arcpy.Array([arcpy.Point(*p)\r\n                                            for p in pt.tolist()]), SR))\r\n    return f\r\n\r\n\r\ndef struct_polyline(a, oid_fld, shp_fld, SR):\r\n    """"""Make polyline features from a structured array.\r\n\r\n    Requires:\r\n    --------\r\n    `a`       : structured array\r\n\r\n    `out_fc`  : a featureclass path and name, or None\r\n\r\n    `oid_fld` : object id field, used to partition the shapes into groups\r\n\r\n    `shp_fld` : shape field(s)\r\n\r\n    `SR`      : spatial reference object, or name\r\n\r\n    Returns:\r\n    -------\r\n        Produces the featureclass optionally, but returns the polygons anyway.\r\n    """"""\r\n    pts = _arr_common(a, oid_fld, shp_fld)\r\n    f = []\r\n    for pt in pts:\r\n        f.append(arcpy.Polyline(arcpy.Array([arcpy.Point(*p)\r\n                                            for p in pt.tolist()]), SR))\r\n    return f\r\n\r\n\r\ndef struct_polygon(a, out_fc, oid_fld, shp_fld, SR):\r\n    """"""Make polygon features from a structured array.\r\n\r\n    Requires:\r\n    --------\r\n    `a` : structured array\r\n\r\n    `out_fc` : a featureclass path and name, or None\r\n\r\n    `oid_fld` : object id field, used to partition the shapes into groups\r\n\r\n    `shp_fld` : shape field(s)\r\n\r\n    `SR` : spatial reference object, or name\r\n\r\n    Returns:\r\n    -------\r\n        Produces the featureclass optionally, but returns the polygons anyway.\r\n    """"""\r\n    pts = _arr_common(a, oid_fld, shp_fld)\r\n    f = []\r\n    for pt in pts:\r\n        f.append(arcpy.Polygon(arcpy.Array([arcpy.Point(*p)\r\n                                            for p in pt.tolist()]), SR))\r\n    return f\r\n\r\n\r\ndef array_fc(a, out_fc=None, shp_fld=[\'Shape\'], SR=None):\r\n    """"""Array to featureclass/shapefile...optionally including all fields\r\n\r\n    Requires:\r\n    --------\r\n\r\n    a : structured or recarray\r\n\r\n    out_fc : the full path and filename to a featureclass or shapefile\r\n\r\n    shp_fld : shapefield name(s) ie. [\'Shape\'] or [\'X\', \'Y\']\r\n\r\n    References:\r\n    ----------\r\n        NumpyArrayToFeatureClass, ListFields for information and options\r\n\r\n    """"""\r\n    if not out_fc:\r\n        if arcpy.Exists(out_fc):\r\n            arcpy.Delete_management(out_fc)\r\n        arcpy.da.NumPyArrayToFeatureClass(a, out_fc, shp_fld, SR)\r\n    # return out_fc\r\n\r\n\r\ndef array_struct(a, fld_names=[\'X\', \'Y\'], dt=[\'<f8\', \'<f8\']):\r\n    """"""Convert an array to a structured array\r\n\r\n    Requires:\r\n    --------\r\n    `a` : an ndarray with shape at least (N, 2)\r\n\r\n    `dt` : dtype class\r\n\r\n    `names` : names for the fields\r\n    """"""\r\n    dts = [(fld_names[i], dt[i]) for i in range(len(fld_names))]\r\n    z = np.zeros((a.shape[0],), dtype=dts)\r\n    names = z.dtype.names\r\n    for i in range(a.shape[1]):\r\n        z[names[i]] = a[:, i]\r\n    return z\r\n\r\n\r\n# ---- Featureclass to array section ----\r\n\r\ndef arc_np(in_fc):\r\n    """"""Alternate constructor to create a structured array from a feature class.\r\n    This function only returns the geometry portion.  The OID field is\r\n    retained should you wish to associate attributes back to the results.\r\n    There is no point in carrying extra baggage around when you don\'t need it.\r\n\r\n    Requires:\r\n    --------\r\n    `in_fc` : the file path to the feature class\r\n\r\n    ArcGIS Pro is assumed, since the new arcpy.da.Describe is used which\r\n    returns a dictionary of properties.\r\n\r\n    Returns:\r\n    -------\r\n    A structured array with a specified dtype to facilitate geometry\r\n    reconstruction later.  The attributes are kept separate.\r\n\r\n    Alternatives:\r\n    ------------\r\n    SpatialDataFrame from ArcGIS API for Python... or GeoPandas\r\n    """"""\r\n    desc = arcpy.da.Describe(in_fc)\r\n    shp_type = desc[\'shapeType\']\r\n    prefix = desc[\'shapeFieldName\']\r\n    fields = [\'OID@\', prefix + \'@\']\r\n    dt = [(\'ID_num\', \'<i4\'), (\'Part_num\', \'<i4\'), (\'Xs\', \'<f8\'), (\'Ys\', \'<f8\')]\r\n    pnts = []\r\n    with arcpy.da.SearchCursor(in_fc, fields) as cursor:\r\n        for row in cursor:\r\n            oid, shp = row\r\n            for j in range(len(shp)):\r\n                pt = shp.getPart(j)\r\n                if shp_type in (\'Point\', \'point\'):\r\n                    pnts.extend([(oid, j, pt.X, pt.Y)])\r\n                else:\r\n                    p_list = [(oid, j, pnt.X, pnt.Y) for pnt in pt if pnt]\r\n                    pnts.extend(p_list)\r\n    a = np.asarray(pnts, dtype=dt)\r\n    return a\r\n\r\n\r\ndef pnts_arr(in_fc, as_struct=True, shp_fld=None, SR=None):\r\n    """"""Create points from an array.\r\n\r\n    Requires:\r\n    --------\r\n\r\n    `in_fc` :\r\n        input featureclass\r\n\r\n    `as_struct` : boolean\r\n        - True, returns a structured array with Id, X, Y fields\r\n        - False, returns an ndarray with dtype=\'<f8\'\r\n\r\n    `shp_fld` & `SR` :\r\n        if unspecified, it will be determined using fc_info\r\n        to return featureclass information.\r\n    """"""\r\n    if shp_fld is None or SR is None:\r\n        shp_fld, oid_fld, SR, shp_type = fc_info(in_fc)\r\n    a = arcpy.da.FeatureClassToNumPyArray(in_fc, ""*"", """", SR)\r\n    dt = [(\'Id\', \'<i4\'), (\'X\', \'<f8\'), (\'Y\', \'<f8\')]\r\n    if as_struct:\r\n        shps = np.array([i for i in a[[oid_fld, shp_fld]]], dtype=dt)\r\n    else:\r\n        shps = a[shp_fld]\r\n    return shps\r\n\r\n\r\n# ---- points to point, polyline or polygon featureclass----------------------\r\n#\r\ndef output_points(out_fc, SR, pnts):\r\n    """"""Produce the output point featureclass""""""\r\n    msg = \'\\nRead the script header... A projected coordinate system required\'\r\n    assert (SR is not None), msg\r\n    pnts_lst = []\r\n    for pnt in pnts:                 # create the point geometry\r\n        pnts_lst.append(arcpy.PointGeometry(arcpy.Point(*pnt), SR))\r\n    if arcpy.Exists(out_fc):     # overwrite any existing versions\r\n        arcpy.Delete_management(out_fc)\r\n    arcpy.CopyFeatures_management(pnts_lst, out_fc)\r\n    return out_fc\r\n\r\n\r\ndef output_polylines(out_fc, SR, pnts):\r\n    """"""Produce the output polygon featureclass.\r\n    :Requires:\r\n    :--------\r\n    : - A list of lists of points\r\n    :   aline = [[0, 0], [1, 1]]  # a list of points\r\n    :   aPolyline = [aline]       # a list of lists of points\r\n    """"""\r\n    msg = \'\\nRead the script header... A projected coordinate system required\'\r\n    assert (SR is not None), msg\r\n    polylines = []\r\n    for pair in pnts:\r\n        pl = arcpy.Polyline(arcpy.Array([arcpy.Point(*xy) for xy in pair]), SR)\r\n        polylines.append(pl)\r\n    if arcpy.Exists(out_fc):     # overwrite any existing versions\r\n        arcpy.Delete_management(out_fc)\r\n    arcpy.CopyFeatures_management(polylines, out_fc)\r\n    return\r\n\r\n\r\ndef output_polygons(out_fc, SR, pnts):\r\n    """"""Produce the output polygon featureclass.\r\n\r\n    Requires:\r\n    --------\r\n        - A list of lists of points\r\n        - aline = [[0, 0], [1, 1]]  # a list of points\r\n        - aPolygon = [aline]       # a list of lists of points\r\n    """"""\r\n    msg = \'\\nRead the script header... A projected coordinate system required\'\r\n    assert (SR is not None), msg\r\n    polygons = []\r\n    for pair in pnts:\r\n        pl = arcpy.Polygon(arcpy.Array([arcpy.Point(*xy) for xy in pair]), SR)\r\n        polygons.append(pl)\r\n    if arcpy.Exists(out_fc):     # overwrite any existing versions\r\n        arcpy.Delete_management(out_fc)\r\n    arcpy.CopyFeatures_management(polygons, out_fc)\r\n    return\r\n\r\n\r\n# ---- piece the geometry to featureclasses --------------------------------\r\ndef arr_polyline_fc(a, out_fc, oid_fld, shp_fld, SR):\r\n    """"""Make polyline featureclass from a structured array.""""""\r\n    f = obj_polyline(a, oid_fld, shp_fld, SR)\r\n    return shapes_fc(f, out_fc)\r\n\r\n\r\ndef arr_polygon_fc(a, out_fc, oid_fld, shp_fld, SR):\r\n    """"""Make polyline featureclass from a structured array.""""""\r\n    f = obj_polygon(a, oid_fld, shp_fld, SR)\r\n    return shapes_fc(f, out_fc)\r\n\r\n\r\ndef shapes_fc(shps, out_fc):\r\n    """"""Create a featureclass/shapefile from poly* shapes.\r\n\r\n    Requires:\r\n    ---------\r\n\r\n    - shps : geometry\r\n        Point geometry objects needed to create the featureclass\r\n    - out_fc : string\r\n        Full path and name to the output container (gdb or folder)\r\n    """"""\r\n    msg0 = ""\\nCreated..\\n{}"".format(out_fc)\r\n    msg1 = ""\\nCan\'t overwrite the {}... rename it"".format(out_fc)\r\n    arcpy.overwriteOutput = True\r\n    try:\r\n        if arcpy.Exists(out_fc):\r\n            arcpy.Delete_management(out_fc)\r\n        arcpy.CopyFeatures_management(shps, out_fc)\r\n        tweet(msg0)\r\n    except ValueError:\r\n        tweet(msg1)\r\n\r\n\r\n# ---- ******* main conversion section ***** ----\r\n# ----         ----------------------        ----\r\n\r\ndef _id_geom_array(in_fc):\r\n    """"""The main code segment which gets the id and shape information and\r\n    explodes the geometry to individual points\r\n    """"""\r\n    shp_fld, oid_fld, shp_type, SR = fc_info(in_fc)\r\n    a_flds = [oid_fld, shp_fld]\r\n    a = arcpy.da.FeatureClassToNumPyArray(in_fc, field_names=a_flds,\r\n                                          explode_to_points=True,\r\n                                          spatial_reference=SR)\r\n    dt = [(\'Idx\', \'<i4\'), (\'Xs\', \'<f8\'), (\'Ys\', \'<f8\')]\r\n    a.dtype = dt\r\n    return a\r\n\r\n\r\ndef polylines_arr(in_fc, as_struct=True):\r\n    """"""Create an array from polylines.\r\n    """"""\r\n    return _id_geom_array(in_fc)\r\n\r\n\r\ndef polygons_arr(in_fc, as_struct=True, shp_fld=None, SR=None):\r\n    """"""Create an array from polygons.\r\n    """"""\r\n    return _id_geom_array(in_fc)\r\n\r\n\r\ndef fc_array(in_fc, flds=""*"", allpnts=True):\r\n    """"""Convert a featureclass to an ndarray...with optional fields besides the\r\n    FID/OIDName and Shape fields.\r\n\r\n    Parameters:\r\n    -----------\r\n    in_fc : text\r\n        Full path to the geodatabase and the featureclass name\r\n    flds : text or list\r\n        - ``\'\'   : just an object id and shape field``\r\n        - ``\'*\'  : all fields in the featureclass or``\r\n        - ``list : specific fields [\'OBJECTID\',\'Shape\',\'SomeClass\', etc]``\r\n    allpnts : boolean\r\n        - True `explodes` geometry to individual points.\r\n        - False returns the centroid\r\n\r\n    Requires:\r\n    ---------\r\n        fc_info(in_fc) function\r\n\r\n    See also:\r\n    ---------\r\n        FeatureClassToNumPyArray, ListFields for more information in current\r\n        arcpy documentation\r\n    """"""\r\n    out_flds = []\r\n    shp_fld, oid_fld, shp_type, SR = fc_info(in_fc)  # get the base information\r\n    flds_all = arcpy.ListFields(in_fc)\r\n    flds_oth = [f for f in flds_all if f.type not in (\'OID\', \'Geometry\')]\r\n    fld_names = [f.name for f in flds_oth]\r\n    oid_geom = [oid_fld, \'SHAPE@X\', \'SHAPE@Y\']\r\n    nulls = {\'Double\':np.nan,\r\n             \'Single\':np.nan,\r\n             \'Short\':np.iinfo(np.int16).min,\r\n             \'SmallInteger\':np.iinfo(np.int16).min,\r\n             \'Long\':np.iinfo(np.int32).min,\r\n             \'Float\':np.nan,\r\n             \'Integer\':np.iinfo(np.int32).min,\r\n             \'String\':str(None),\r\n             \'Text\':str(None)}\r\n    fld_dict = {i.name: i.type for i in flds_oth}\r\n    null_dict = {f:nulls[fld_dict[f]] for f in fld_names}\r\n    if flds == """":                        # return just OID and Shape values\r\n        out_flds = oid_geom               # FID and Shape X, Y\r\n    elif flds == ""*"":                     # all fields\r\n        out_flds = oid_geom + fld_names\r\n    else:\r\n        out_flds = [oid_fld, \'SHAPE@X\', \'SHAPE@Y\']\r\n        for f in flds_oth:\r\n            if f.name in flds:\r\n                out_flds.append(f.name)\r\n    frmt = """"""\\nRunning \'fc_array\' with ....\r\n    \\nfeatureclass... {}\\nFields... {}\\nAll pnts... {}\\nSR... {}\r\n    """"""\r\n    args = [in_fc, out_flds, allpnts, SR.name]\r\n    msg = dedent(frmt).format(*args)\r\n    tweet(msg)\r\n    a = arcpy.da.FeatureClassToNumPyArray(in_fc,\r\n                                          field_names=out_flds,\r\n                                          where_clause="""",\r\n                                          spatial_reference=SR,\r\n                                          explode_to_points=allpnts,\r\n                                          skip_nulls=False,\r\n                                          null_value=null_dict)\r\n    # out it goes in array format\r\n    return a\r\n\r\n\r\ndef change_fld(flds):\r\n    """"""Convert the field types to array friendly ones\r\n    """"""\r\n    info = [(fld.type, fld.name, fld.length) for fld in flds]\r\n    dt = []\r\n    for i in info:\r\n        if i[0] in (\'OID\', \'Integer\', \'Long\', \'Short\', \'SmallInteger\'):\r\n            dt.append((i[1], \'<i4\'))\r\n        elif i[0] in (\'Double\', \'Single\', \'Float\'):\r\n            dt.append((i[1], \'<f8\'))\r\n        else:\r\n            dt.append((i[1], ""{}{}"".format(\'U\', i[2])))\r\n    return dt\r\n\r\n\r\ndef tbl_arr(in_fc):\r\n    """""" Convert a table or featureclass table (in_fc)to a numpy array\r\n    including the oid field but excluding the geometry field.\r\n    """"""\r\n    desc = arcpy.da.Describe(in_fc)  # use the new da.Describe method\r\n    dump = [\'shapeFieldName\', \'areaFieldName\', \'lengthFieldName\']\r\n    f_geo = [desc[i] for i in dump]\r\n    fields = [f for f in arcpy.ListFields(in_fc) if f.name not in f_geo]\r\n    f_names = [f.name for f in fields]\r\n    dt = change_fld(fields)\r\n    vals = []\r\n    with arcpy.da.SearchCursor(in_fc, field_names=f_names) as rows:\r\n        for row in rows:\r\n            vals.append(row)\r\n        del row\r\n        del rows\r\n    dt = [(i.replace(\'OBJECTID\', \'Idx\'), j) for i, j in dt]\r\n    a = np.asarray(vals, dtype=dt)\r\n    return a  # vals, az\r\n\r\n\r\n# ---- functions to convert between array and featureclass ----\r\n#\r\ndef to_fc(out_fc, a, b=None, dim=2, flds=[\'Id\', \'X\', \'Y\'], SR_code=None):\r\n    """"""Reconstruct a featureclass from a deconstructed pair of arrays.\r\n\r\n    This function reverses the functionality of to_array which splits a\r\n    featureclass into an array of geometry and one of attributes.\r\n\r\n    One can perform operations on one or the other or both, then reassemble\r\n    into a new file.\r\n\r\n    Requires:\r\n    --------\r\n    `out_fc` : filename\r\n        full path and name for the output featureclass\r\n    `a` : geometry array\r\n        the array of geometry objects\r\n    `b` : attribute array\r\n        the array of attributes for \'a\'\r\n    `dim` : geometry type\r\n        - 0 (point)\r\n        - 1 (polyline) or\r\n        - 2 (polygon)\r\n    `fld` : id and shape field(s)\r\n        normally [\'Id, \'X\', \'Y\'] if deconstructed using \'to_array\'\r\n    `SR_code` :\r\n        The spatial reference code of the output geometry\r\n\r\n        - 4326 for GCS WGS84\r\n        - 2951 for MTM 9\r\n\r\n    References:\r\n    ----------\r\n\r\n        Spatial reference http://spatialreference.org/\r\n    """"""\r\n    args = [to_fc.__module__, dedent(to_fc.__doc__)]\r\n    msg = ""\\n...to_fc ... in {} failed\\n{}"".format(*args)\r\n    try:\r\n        SR = arcpy.SpatialReference(SR_code)\r\n    except ValueError:\r\n        tweet(""Spatial reference is in error"")\r\n        tweet(msg)\r\n        return None\r\n    if (dim not in (0, 1, 2)) or (len(flds) not in (2, 3)):\r\n        tweet(msg)\r\n        return None\r\n    if len(flds) == 2:\r\n        oid_fld, shp_fld = flds\r\n    else:\r\n        oid_fld, shp_fld = flds[0], flds[1:3]\r\n    if dim == 0:\r\n        arr_pnts(a, out_fc, shp_fld, SR)\r\n#    geom = _split_array(a, fld=oid_fld)\r\n    geom = np.split(a, np.where(np.diff(a[oid_fld]))[0] + 1)\r\n    prts = [i[[\'Xs\', \'Ys\']].tolist() for i in geom]\r\n    if dim == 1:\r\n        arr_polyline_fc(geom, out_fc, oid_fld, shp_fld, SR)\r\n    else:\r\n        # arr_polygon_fc(geom, out_fc, oid_fld, shp_fld, SR)\r\n        # pts = _arr_common(a, oid_fld, shp_fld)\r\n        f = []\r\n        for pt in prts:  # g\r\n            f.append(arcpy.Polygon(\r\n                             arcpy.Array([arcpy.Point(*p) for p in pt]), SR))\r\n#    _shapes_fc(f, out_fc)\r\n#    arcpy.da.ExtendTable(out_fc, table_match_field=oid_fld,\r\n#                         in_array=b, array_match_field=oid_fld)\r\n    return f\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\n\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    from _common import fc_info, tweet\r\n    # from arcpytools import array_fc, array_struct, tweet\r\n#    print(""Script... {}"".format(script))\r\n#    _demo()\r\n#    in_fc0 = r""C:\\Git_Dan\\a_Data\\testdata.gdb\\Points_10""\r\n#    in_fc1 = r""C:\\Git_Dan\\a_Data\\testdata.gdb\\Polyline_connected""\r\n#    in_fc = r""C:\\Git_Dan\\a_Data\\testdata.gdb\\Carp_5x5km""   # full 25 polygons\r\n#    in_fc = r""C:\\Git_Dan\\a_Data\\testdata.gdb\\polygon""\r\n#    in_fc = r""C:\\Git_Dan\\a_Data\\arcpytools_demo.gdb\\Can_0_big_3""\r\n#    in_fc = r""C:\\Data\\Canada\\CAN_adm0.gdb\\CAN_0_sp""\r\n#    a0, _ = two_arrays(in_fc, both=False, split=False)\r\n\r\n#    fc_array(in_fc1, flds="""", allpnts=True)\r\n#    out_fc = r""C:\\Git_Dan\\a_Data\\testdata.gdb\\test_1""\r\n#    out_tbl = r""C:\\Git_Dan\\a_Data\\testdata.gdb\\join_tbl""\r\n'"
all_scripts/arc_io.py,4,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\narc_io\r\n======\r\n\r\nScript :   arc_io.py\r\n\r\nAuthor :   Dan.Patterson@carleton.ca\r\n\r\nModified : 2018-09-14\r\n\r\nPurpose : Basic io tools for numpy arrays and arcpy\r\n\r\nNotes :\r\n::\r\n    1.  load_npy    - load numpy npy files\r\n    2.  save_npy    - save array to *.npy format\r\n    3.  read_txt    - read array created by save_txtt\r\n    4.  save_txt    - save array to npy format\r\n    5.  arr_json    - save to json format\r\n    6.  array2raster - save array to raster\r\n    7.  rasters2nparray - batch rasters to numpy array\r\n\r\n---------------------------------------------------------------------\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport os\r\nimport numpy as np\r\nimport arcpy\r\n\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n__all__ = [\'array2raster\',\r\n           \'rasters2nparray\',\r\n           ]\r\n\r\n# ----------------------------------------------------------------------\r\n# (1) batch load and save to/from arrays and rasters\r\ndef array2raster(a, folder, fname, LL_corner, cellsize):\r\n    """"""It is easier if you have a raster to derive the values from.\r\n\r\n    >>> # Get one of the original rasters since they will have the same\r\n    >>> # extent and cell size needed to produce output\r\n    >>> r01 = rasters[1]\r\n    >>> rast = arcpy.Raster(r01)\r\n    >>> lower_left = rast.extent.lowerLeft\r\n    >>> # this is a Point object... ie LL = arcpy.Point(10, 10)\r\n    >>> cell_size = rast.meanCellHeight  # --- we will use this for x and y\r\n    >>> f = r\'c:/temp/result.tif\'  # --- don\'t forget the extention\r\n\r\n    Requires:\r\n    ---------\r\n\r\n    `arcpy` and `os` if not previously imported\r\n    """"""\r\n    if not os.path.exists(folder):\r\n        return None\r\n    r = arcpy.NumPyArrayToRaster(a, LL_corner, cellsize, cellsize)\r\n    f = os.path.join(folder, fname)\r\n    r.save(f)\r\n    print(""Array saved to...{}"".format(f))\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# (7) batch load and save to/from arrays and rasters\r\ndef rasters2nparray(folder=None, to3D=False):\r\n    """"""Batch the RasterToNumPyArray arcpy function to produce 3D or a list\r\n    of 2D arrays\r\n\r\n    NOTE:\r\n    ----\r\n    Edit the code... far simpler than accounting for everything.\r\n    There is a reasonable expectation that rasters exist in the folder.\r\n\r\n    Requires:\r\n    --------\r\n    modules :\r\n        os, arcpy if not already loaded\r\n    folder : folder\r\n        A folder on disk... a real one\r\n    to3D : boolean\r\n        If False, a list of arrays, if True a 3D array\r\n    """"""\r\n    arrs = []\r\n    if folder is None:\r\n        return None\r\n    if not os.path.exists(folder):\r\n        return None\r\n    arcpy.env.workspace = folder\r\n    rasters = arcpy.ListRasters(""*"", ""TIF"")\r\n    for raster in rasters:\r\n        arrs.append(arcpy.RasterToNumPyArray(raster))\r\n    if to3D:\r\n        return np.array(arrs)\r\n    else:\r\n        return arrs\r\n\r\n\r\ndef _demo():\r\n    """"""\r\n    : -\r\n    """"""\r\n    _npy = ""/Data/sample_20.npy""  # change to one in the Data folder\r\n    _npy = ""{}"".format(script.replace(\'/fc_tools/arc_io.py\',_npy))\r\n    return _npy\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n#    fname = _demo()\r\n'"
all_scripts/arcpytools.py,11,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   arcpytools.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2018-03-31\r\n:Purpose:  tools for working with numpy arrays\r\n:Useage:\r\n:\r\n:References:\r\n:\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nfrom textwrap import dedent\r\nimport numpy as np\r\nimport arcpy\r\n# from arcpytools import array_fc, array_struct, tweet\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\n\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\n__all__ = [\'_xyID\']\r\n\r\n\r\ndef tweet(msg):\r\n    """"""Produce a message for both arcpy and python\r\n    : msg - a text message\r\n    """"""\r\n    m = ""{}"".format(msg)\r\n    arcpy.AddMessage(m)\r\n    print(m)\r\n\r\n\r\ndef _describe(in_fc=None):\r\n    """"""Simply return the arcpy.da.Describe object\r\n    : desc.keys() an abbreviated list...\r\n    : [... \'OIDFieldName\'... \'areaFieldName\', \'baseName\'... \'catalogPath\',\r\n    :  ... \'dataType\'... \'extent\', \'featureType\', \'fields\', \'file\'... \'hasM\',\r\n    :  \'hasOID\', \'hasZ\', \'indexes\'... \'lengthFieldName\'... \'name\', \'path\',\r\n    :  \'rasterFieldName\', ..., \'shapeFieldName\', \'shapeType\',\r\n    :  \'spatialReference\',  ...]\r\n    """"""\r\n    if in_fc is not None:\r\n        return arcpy.da.Describe(in_fc)\r\n    else:\r\n        return None\r\n\r\ndef fc_info(in_fc, prn=False):\r\n    """"""Return basic featureclass information, including...\r\n\r\n    Parameters:\r\n    -----------\r\n    - shp_fld  :\r\n        field name which contains the geometry object\r\n    - oid_fld  :\r\n        the object index/id field name\r\n    - SR       :\r\n        spatial reference object (use SR.name to get the name)\r\n    - shp_type :\r\n        shape type (Point, Polyline, Polygon, Multipoint, Multipatch)\r\n    - others   :\r\n        \'areaFieldName\', \'baseName\', \'catalogPath\',\'featureType\',\'fields\',\r\n        \'hasOID\', \'hasM\', \'hasZ\', \'path\'\r\n\r\n\r\n     - all_flds :\r\n         [i.name for i in desc[\'fields\']]\r\n    """"""\r\n    desc = _describe(in_fc)\r\n    args = [\'shapeFieldName\', \'OIDFieldName\', \'shapeType\', \'spatialReference\']\r\n    shp_fld, oid_fld, shp_type, SR = [desc[i] for i in args]\r\n    if prn:\r\n        frmt = ""FeatureClass:\\n   {}"".format(in_fc)\r\n        f = ""\\n{!s:<16}{!s:<14}{!s:<10}{!s:<10}""\r\n        frmt += f.format(*args)\r\n        frmt += f.format(shp_fld, oid_fld, shp_type, SR.name)\r\n        tweet(frmt)\r\n    else:\r\n        return shp_fld, oid_fld, shp_type, SR\r\n\r\n\r\n# ---- geometry related -----------------------------------------------------\r\n#\r\ndef _xyID(in_fc, to_pnts=True):\r\n    """"""Convert featureclass geometry (in_fc) to a simple 2D structured array\r\n    :  with ID, X, Y values. Optionally convert to points, otherwise centroid.\r\n    """"""\r\n    flds = [\'OID@\', \'SHAPE@X\', \'SHAPE@Y\']\r\n    args = [in_fc, flds, None, None, to_pnts, (None, None)]\r\n    cur = arcpy.da.SearchCursor(*args)\r\n    a = cur._as_narray()\r\n    a.dtype = [(\'IDs\', \'<i4\'), (\'Xs\', \'<f8\'), (\'Ys\', \'<f8\')]\r\n    del cur\r\n    return a\r\n\r\n\r\ndef array_struct(a, fld_names=[\'X\', \'Y\'], dt=[\'<f8\', \'<f8\']):\r\n    """"""Convert an array to a structured array\r\n\r\n    Parameters:\r\n    -----------\r\n    - a : an ndarray with shape at least (N, 2)\r\n    -  dt : dtype class\r\n    -  names : names for the fields\r\n    """"""\r\n    dts = [(fld_names[i], dt[i]) for i in range(len(fld_names))]\r\n    z = np.zeros((a.shape[0],), dtype=dts)\r\n    names = z.dtype.names\r\n    for i in range(a.shape[1]):\r\n        z[names[i]] = a[:, i]\r\n    return z\r\n\r\n\r\ndef array_fc(a, out_fc, fld_names, SR):\r\n    """"""Array to featureclass/shapefile...optionally including all fields\r\n\r\n    Parameters:\r\n    -----------\r\n    - out_fc :  featureclass/shapefile... complete path\r\n    - fld_names : the Shapefield name ie [\'Shape\'] or [\'X\', \'Y\'s]\r\n    - SR : spatial reference of the output\r\n\r\n    See also :\r\n        NumpyArrayToFeatureClass, ListFields for information and options\r\n    """"""\r\n    if arcpy.Exists(out_fc):\r\n        arcpy.Delete_management(out_fc)\r\n    arcpy.da.NumPyArrayToFeatureClass(a, out_fc, fld_names, SR)\r\n    return out_fc\r\n\r\n\r\ndef fc_array(in_fc, flds, allpnts):\r\n    """"""Convert a featureclass to an ndarray...with optional fields besides the\r\n    FID/OIDName and Shape fields.\r\n\r\n    Parameters:\r\n    -----------\r\n    in_fc : text\r\n        Full path to the geodatabase and the featureclass name\r\n\r\n    flds : text or list\r\n        - ``\'\'   : just an object id and shape field``\r\n        - ``\'*\'  : all fields in the featureclass or``\r\n        - ``list : specific fields [\'OBJECTID\',\'Shape\',\'SomeClass\', etc]``\r\n\r\n    allpnts : boolean\r\n        - True `explodes` geometry to individual points.\r\n        - False returns the centroid\r\n\r\n    Requires:\r\n    ---------\r\n        fc_info(in_fc) function\r\n\r\n    See also:\r\n    ---------\r\n        FeatureClassToNumPyArray, ListFields for more information in current\r\n        arcpy documentation\r\n    """"""\r\n    out_flds = []\r\n    shp_fld, oid_fld, shp_type, SR = fc_info(in_fc)  # get the base information\r\n    fields = arcpy.ListFields(in_fc)      # all fields in the shapefile\r\n    if flds == """":                        # return just OID and Shape field\r\n        out_flds = [oid_fld, shp_fld]     # FID and Shape field required\r\n    elif flds == ""*"":                     # all fields\r\n        out_flds = [f.name for f in fields]\r\n    else:\r\n        out_flds = [oid_fld, shp_fld]\r\n        for f in fields:\r\n            if f.name in flds:\r\n                out_flds.append(f.name)\r\n    frmt = """"""\\nRunning \'fc_array\' with ....\r\n    \\nfeatureclass... {}\\nFields... {}\\nAll pnts... {}\\nSR... {}\r\n    """"""\r\n    args = [in_fc, out_flds, allpnts, SR.name]\r\n    msg = dedent(frmt).format(*args)\r\n    tweet(msg)\r\n    a = arcpy.da.FeatureClassToNumPyArray(in_fc, out_flds, """", SR, allpnts)\r\n    # out it goes in array format\r\n    return a, out_flds, SR\r\n\r\n\r\ndef arr2pnts(in_fc, as_struct=True, shp_fld=None, SR=None):\r\n    """"""Create points from an array.\r\n    :  in_fc - input featureclass\r\n    :  as_struct - if True, returns a structured array with X, Y fields,\r\n    :            - if False, returns an ndarray with dtype=\'<f8\'\r\n    :Notes: calls fc_info to return featureclass information\r\n    """"""\r\n    if shp_fld is None or SR is None:\r\n        shp_fld, oid_fld, SR = fc_info(in_fc)\r\n    a = arcpy.da.FeatureClassToNumPyArray(in_fc, ""*"", """", SR)\r\n    dt = [(\'X\', \'<f8\'), (\'Y\', \'<f8\')]\r\n    if as_struct:\r\n        shps = np.array([tuple(i) for i in a[shp_fld]], dtype=dt)\r\n    else:\r\n        shps = a[shp_fld]\r\n    return shps, shp_fld, SR\r\n\r\n\r\ndef arr2line(a, out_fc, SR=None):\r\n    """"""create lines from an array""""""\r\n    pass\r\n\r\n\r\ndef shapes2fc(shps, out_fc):\r\n    """"""Create a featureclass/shapefile from poly* shapes.\r\n    :  out_fc - full path and name to the output container (gdb or folder)\r\n    """"""\r\n    msg = ""\\nCan\'t overwrite the {}... rename"".format(out_fc)\r\n    try:\r\n        if arcpy.Exists(out_fc):\r\n            arcpy.Delete_management(out_fc)\r\n        arcpy.CopyFeatures_management(shps, out_fc)\r\n    except ValueError:\r\n        tweet(msg)\r\n\r\n\r\ndef arr2polys(a, out_fc, oid_fld, SR):\r\n    """"""Make poly* features from a structured array.\r\n    :  a - structured array\r\n    :  out_fc: a featureclass path and name, or None\r\n    :  oid_fld - object id field, used to partition the shapes into groups\r\n    :  SR - spatial reference object, or name\r\n    :Returns:\r\n    :-------\r\n    :  Produces the featureclass optionally, but returns the polygons anyway.\r\n    """"""\r\n    arcpy.overwriteOutput = True\r\n    pts = []\r\n    keys = np.unique(a[oid_fld])\r\n    for k in keys:\r\n        w = np.where(a[oid_fld] == k)[0]\r\n        v = a[\'Shape\'][w[0]:w[-1] + 1]\r\n        pts.append(v)\r\n    # Create a Polygon from an Array of Points, save to featueclass if needed\r\n    s = []\r\n    for pt in pts:\r\n        s.append(arcpy.Polygon(arcpy.Array([arcpy.Point(*p) for p in pt]), SR))\r\n    return s\r\n\r\n\r\ndef output_polylines(out_fc, SR, pnt_groups):\r\n    """"""Produce the output polygon featureclass.\r\n    :Requires:\r\n    :--------\r\n    : - A list of lists of points\r\n    :   aline = [[[0, 0], [1, 1]]]  # a list of points\r\n    :   aPolyline = [[aline]]       # a list of lists of points\r\n    """"""\r\n    msg = \'\\nRead the script header... A projected coordinate system required\'\r\n    assert (SR is not None), msg\r\n    polylines = []\r\n    for pnts in pnt_groups:\r\n        for pair in pnts:\r\n            arr = arcpy.Array([arcpy.Point(*xy) for xy in pair])\r\n            pl = arcpy.Polyline(arr, SR)\r\n            polylines.append(pl)\r\n    if arcpy.Exists(out_fc):     # overwrite any existing versions\r\n        arcpy.Delete_management(out_fc)\r\n    arcpy.CopyFeatures_management(polylines, out_fc)\r\n    return\r\n\r\n\r\ndef output_polygons(out_fc, SR, pnt_groups):\r\n    """"""Produce the output polygon featureclass.\r\n\r\n    Parameters:\r\n    -----------\r\n    out_fc : string\r\n        The path and name of the featureclass to be created.\r\n    SR : spatial reference of the output featureclass\r\n    pnts_groups :\r\n        The point groups, list of lists of points, to include parts rings.\r\n\r\n    Requires:\r\n    --------\r\n\r\n    - A list of lists of points.  Four points form a triangle is the minimum\r\n    -  aline = [[0, 0], [1, 1]]  # a list of points\r\n    -  aPolygon = [aline]        # a list of lists of points\r\n    """"""\r\n    msg = \'\\nRead the script header... A projected coordinate system required\'\r\n    assert (SR is not None), msg\r\n    polygons = []\r\n    for pnts in pnt_groups:\r\n        for pair in pnts:\r\n            arr = arcpy.Array([arcpy.Point(*xy) for xy in pair])\r\n            pl = arcpy.Polygon(arr, SR)\r\n            polygons.append(pl)\r\n    if arcpy.Exists(out_fc):     # overwrite any existing versions\r\n        arcpy.Delete_management(out_fc)\r\n    arcpy.CopyFeatures_management(polygons, out_fc)\r\n    return\r\n\r\n# ---- formatting, from arraytools ------------------------------------------\r\n#\r\n# ----------------------------------------------------------------------\r\n# (4) frmt_rec .... code section\r\n#  frmt_rec requires _col_format\r\ndef _col_format(a, c_name=""c00"", deci=0):\r\n    """"""Determine column format given a desired number of decimal places.\r\n    Used by frmt_struct.\r\n\r\n    `a` : column\r\n        A column in an array.\r\n    `c_name` : text\r\n        column name\r\n    `deci` : int\r\n        Desired number of decimal points if the data are numeric\r\n\r\n    Notes:\r\n    -----\r\n        The field is examined to determine whether it is a simple integer, a\r\n        float type or a list, array or string.  The maximum width is determined\r\n        based on this type.\r\n\r\n        Checks were also added for (N,) shaped structured arrays being\r\n        reformatted to (N, 1) shape which sometimes occurs to facilitate array\r\n        viewing.  A kludge at best, but it works for now.\r\n    """"""\r\n    a_kind = a.dtype.kind\r\n    if a_kind in (\'i\', \'u\'):  # ---- integer type\r\n        w_, m_ = [\':> {}.0f\', \'{:> 0.0f}\']\r\n        col_wdth = len(m_.format(a.max())) + 1\r\n        col_wdth = max(len(c_name), col_wdth) + 1  # + deci\r\n        c_fmt = w_.format(col_wdth, 0)\r\n    elif a_kind == \'f\' and np.isscalar(a[0]):  # ---- float type with rounding\r\n        w_, m_ = [\':> {}.{}f\', \'{:> 0.{}f}\']\r\n        a_max, a_min = np.round(np.sort(a[[0, -1]]), deci)\r\n        col_wdth = max(len(m_.format(a_max, deci)),\r\n                       len(m_.format(a_min, deci))) + 1\r\n        col_wdth = max(len(c_name), col_wdth) + 1\r\n        c_fmt = w_.format(col_wdth, deci)\r\n    # ---- lists, arrays, strings. Check for (N,) vs (N,1)\r\n    # I made some changes in how col_wdth is determined, old is commented\r\n    else:\r\n        if a.ndim == 1:  # ---- check for (N, 1) format of structured array\r\n            a = a[0]\r\n        dt = a.dtype.descr[0][1]\r\n        col_wdth = int("""".join([i for i in dt if i.isdigit()]))\r\n#       col_wdth = max([len(str(i)) for i in a])\r\n        col_wdth = max(len(c_name), col_wdth) + 1  # + deci\r\n        c_fmt = ""!s:>"" + ""{}"".format(col_wdth)\r\n    return c_fmt, col_wdth\r\n\r\n\r\ndef pd_(a, deci=2, use_names=True, prn=True):\r\n    """"""see help for `frmt_rec`...""""""\r\n    ret = frmt_rec(a, deci=deci, use_names=use_names, prn=prn)\r\n    return ret\r\n\r\n\r\ndef frmt_rec(a, deci=2, use_names=True, prn=True):\r\n    """"""Format a structured array with a mixed dtype.\r\n\r\n    NOTE : Can be called as `pd_(a, ... )` to emulate pandas dataframes\r\n        You should limit large arrays to a slice ie. a[:50]\r\n\r\n    Requires:\r\n    -------\r\n    `a` : array\r\n        A structured/recarray\r\n    `deci` : int\r\n        To facilitate printing, this value is the number of decimal\r\n        points to use for all floating point fields.\r\n    `use_names` : boolean\r\n        If no names are available, then create them\r\n    `prn` : boolean\r\n        True to print, False to return the string\r\n    Notes:\r\n    -----\r\n        `_col_format` : does the actual work of obtaining a representation of\r\n        the column format.\r\n\r\n        It is not really possible to deconstruct the exact number of decimals\r\n        to use for float values, so a decision had to be made to simplify.\r\n    """"""\r\n    dt_names = a.dtype.names\r\n    N = len(dt_names)\r\n    c_names = [[""C{:02.0f}"".format(i) for i in range(N)], dt_names][use_names]\r\n    # ---- get the column formats from ... _col_format ----\r\n    dts = []\r\n    wdths = []\r\n    pair = list(zip(dt_names, c_names))\r\n    for i in range(len(pair)):\r\n        fld, nme = pair[i]\r\n        c_fmt, col_wdth = _col_format(a[fld], c_name=nme, deci=deci)\r\n        dts.append(c_fmt)\r\n        wdths.append(col_wdth)\r\n    row_frmt = "" "".join([(\'{\' + i + \'}\') for i in dts])\r\n    hdr = [""!s:>"" + ""{}"".format(wdths[i]) for i in range(N)]\r\n    hdr2 = "" "".join([""{"" + hdr[i] + ""}"" for i in range(N)])\r\n    header = ""--n--"" + hdr2.format(*c_names)\r\n    header = ""\\n{}\\n{}"".format(header, ""-""*len(header))\r\n    txt = [header]\r\n    # ---- check for structured arrays reshaped to (N, 1) instead of (N,) ----\r\n    len_shp = len(a.shape)\r\n    idx = 0\r\n    for i in range(a.shape[0]):\r\n        if len_shp == 1:  # ---- conventional (N,) shaped array\r\n            row = "" {:03.0f} "".format(idx) + row_frmt.format(*a[i])\r\n        else:             # ---- reformatted to (N, 1)\r\n            row = "" {:03.0f} "".format(idx) + row_frmt.format(*a[i][0])\r\n        idx += 1\r\n        txt.append(row)\r\n    msg = ""\\n"".join([i for i in txt])\r\n    if prn:\r\n        print(msg)\r\n    else:\r\n        return msg\r\n\r\n# ----------------------------------------------------------------------\r\n# (5) form_ ... code section .....\r\n#  form_ requires make_row_format\r\ndef make_row_format(dim=3, cols=5, a_kind=\'f\', deci=1,\r\n                    a_max=10, a_min=-10, wdth=100, prnt=False):\r\n    """"""Format the row based on input parameters\r\n\r\n    `dim` - int\r\n        Number of dimensions\r\n    `cols` : int\r\n        Columns per dimension\r\n\r\n    `a_kind`, `deci`, `a_max` and `a_min` allow you to specify a data type,\r\n    number of decimals and maximum and minimum values to test formatting.\r\n    """"""\r\n    if a_kind not in [\'f\', \'i\']:\r\n        a_kind = \'f\'\r\n    w_, m_ = [[\':{}.0f\', \'{:0.0f}\'], [\':{}.{}f\', \'{:0.{}f}\']][a_kind == \'f\']\r\n    m_fmt = max(len(m_.format(a_max, deci)), len(m_.format(a_min, deci))) + 1\r\n    w_fmt = w_.format(m_fmt, deci)\r\n    suffix = \'  \'\r\n    while m_fmt*cols*dim > wdth:\r\n        cols -= 1\r\n        suffix = \'.. \'\r\n    row_sub = ((\'{\' + w_fmt + \'}\')*cols + suffix)\r\n    row_frmt = (row_sub*dim).strip()\r\n    if prnt:\r\n        frmt = ""Row format: dim cols: ({}, {})  kind: {} decimals: {}\\n\\n{}""\r\n        print(dedent(frmt).format(dim, cols, a_kind, deci, row_frmt))\r\n        a = np.random.randint(a_min, a_max+1, dim*cols)\r\n        col_hdr(wdth//10)  # run col_hdr to produce the column headers\r\n        print(row_frmt.format(*a))\r\n    else:\r\n        return row_frmt\r\n\r\n\r\ndef form_(a, deci=2, wdth=100, title=""Array"", prefix="". . "", prn=True):\r\n    """"""Alternate format to frmt_ function.\r\n    Inputs are largely the same.\r\n    """"""\r\n    def _piece(sub, i, frmt, linewidth):\r\n        """"""piece together 3D chunks by row""""""\r\n        s0 = sub.shape[0]\r\n        block = np.hstack([sub[j] for j in range(s0)])\r\n        txt = """"\r\n        if i is not None:\r\n            fr = ("":arr[{}"" + "", :{}""*len(a.shape[1:]) + ""]\\n"")\r\n            txt = fr.format(i, *sub.shape)\r\n        for line in block:\r\n            ln = frmt.format(*line)[:linewidth]\r\n            end = [""\\n"", ""...\\n""][len(ln) >= linewidth]\r\n            txt += indent(ln + end, "". . "")\r\n        return txt\r\n    # ---- main section ----\r\n    out = ""\\n{}... ndim: {}  shape: {}\\n"".format(title, a.ndim, a.shape)\r\n    linewidth = wdth\r\n    if a.ndim <= 1:\r\n        return a\r\n    elif a.ndim == 2:\r\n        a = a.reshape((1,) + a.shape)\r\n    # ---- pull the 1st and 3rd dimension for 3D and 4D arrays\r\n    frmt = make_row_format(dim=a.shape[-3],\r\n                           cols=a.shape[-1],\r\n                           a_kind=a.dtype.kind,\r\n                           deci=deci,\r\n                           a_max=a.max(),\r\n                           a_min=a.min(),\r\n                           wdth=wdth,\r\n                           prnt=False)\r\n    if a.ndim == 3:\r\n        s0, s1, s2 = a.shape\r\n        out += _piece(a, None, frmt, linewidth)  # ---- _piece ----\r\n    elif a.ndim == 4:\r\n        s0, s1, s2, _ = a.shape\r\n        for i in range(s0):\r\n            out = out + ""\\n"" + _piece(a[i], i, frmt, linewidth)  # ---- _piece\r\n    if prn:\r\n        print(out)\r\n    else:\r\n        return out\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n#    _demo()\r\n#    gdb_fc = [\'Data\', \'point_tools.gdb\', \'radial_pnts\']\r\n#    in_fc = ""/"".join(script.split(""/"")[:-2] + gdb_fc)\r\n#    result = fc_array(in_fc, flds="""", allpnts=True)  # a, out_flds, SR\r\n'"
all_scripts/arcpytools_old.py,17,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   arcpytools.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2017-08-03\r\n:Purpose:  tools for working with arcpy and numpy arrays\r\n:\r\n:Notes:\r\n:-----  common variables used in the functions ----\r\n:(1) array variables/properties\r\n:  a - structured/recarray\r\n:  dt = array dtype properties\r\n:\r\n:(2) featureclass variables/properties\r\n:  shps    - point geometry objects needed to create the featureclass\r\n:  in_fc   - input featureclass\r\n:  out_fc  - full path and name to the output container (gdb or folder)\r\n:  SR      - spatial reference object (use SR.name to get the name)\r\n:  shp_fld - field name which contains the geometry object\r\n:  oid_fld - the object index/id field name\r\n:\r\n:Procedures:\r\n:----------\r\n: (1) Split featureclass to array\r\n:    - split the geometry:\r\n:    - split the attributes: TableToNumPyArray\r\n: (2) Create featureclass\r\n:    - geometry: arr_pnts, arr_polylines, arr_polygons\r\n:    - join attributes: ExtendTable\r\n:    -\r\n:References:\r\n:----------\r\n:\r\n: (1) main link section:  http://pro.arcgis.com/en/pro-app/arcpy/data-access/\r\n:\r\n: (2) ...extendtable.htm\r\n:     ExtendTable (in_table, table_match_field, in_array,\r\n:                  array_match_field, {append_only})\r\n:\r\n: (3) ...featureclasstonumpyarray.htm\r\n:     FeatureClassToNumPyArray (in_table, field_names, {where_clause},\r\n:                               {spatial_reference}, {explode_to_points},\r\n:                               {skip_nulls}, {null_value})\r\n:\r\n: (4) ...numpyarraytofeatureclass.htm\r\n:     NumPyArrayToFeatureClass (in_array, out_table, shape_fields,\r\n                                {spatial_reference})\r\n: (5) ...tabletonumpyarray.htm\r\n:     TableToNumPyArray(in_table, field_names, {where_clause},\r\n                       {skip_nulls}, {null_value})\r\n:\r\n\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport time\r\nfrom functools import wraps\r\nfrom textwrap import dedent\r\nimport numpy as np\r\nimport arcpy\r\n\r\n\r\n# from arcpytools import array_fc, array_struct, tweet\r\n\r\n\r\n__all__ = [\'fc_info\',        # featureclass info\r\n           \'tweet\',          # tweet to python and arcpy\r\n           \'_arr_common\',    # common function for poly* features\r\n           \'_shapes_fc\',     # convert shapes to featureclass\r\n           \'arr_pnts\',       # array to points\r\n           \'arr_polyline\',   # to polyline\r\n           \'arr_polygon\',    # to polygon\r\n           \'array_fc\',       # to featureclass\r\n           \'array_struct\',   # array to structured array\r\n           \'pnts_arr\',       # points to array\r\n           \'polylines_arr\',  # polylines to array\r\n           \'polygons_arr\',   # polygons to array\r\n           \'fc_array\',       # featureclass to array\r\n           \'change_fld\',    # convert arc to np field types\r\n           \'tbl_arr\',        # convert a table to an array\r\n           \'two_arrays\',     # convert a featureclass to 2 arrays\r\n           \'to_fc\',          # convert the results back to a featureclass\r\n           ]\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\r\n      \'float\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=3, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\n# ---- General function section ----\r\n\r\ndef time_deco(func):  # timing originally\r\n    """"""timing decorator function\r\n    :print(""\\n  print results inside wrapper or use <return> ... "")\r\n    """"""\r\n    import time\r\n    from functools import wraps\r\n\r\n    @wraps(func)\r\n    def wrapper(*args, **kwargs):\r\n        t0 = time.perf_counter()        # start time\r\n        result = func(*args, **kwargs)  # ... run the function ...\r\n        t1 = time.perf_counter()        # end time\r\n        dt = t1 - t0\r\n        print(""\\nTiming function for... {}"".format(func.__name__))\r\n        print(""  Time: {: <8.2e}s for {:,} objects"".format(dt, len(result)))\r\n        return result                   # return the result of the function\r\n        return dt                       # return delta time\r\n    return wrapper\r\n\r\n\r\ndef fc_info(in_fc):\r\n    """"""Return basic featureclass information, including...\r\n    :\r\n    : shp_fld  - field name which contains the geometry object\r\n    : oid_fld  - the object index/id field name\r\n    : SR       - spatial reference object (use SR.name to get the name)\r\n    : shp_type - shape type (Point, Polyline, Polygon, Multipoint, Multipatch)\r\n    : - others: \'areaFieldName\', \'baseName\', \'catalogPath\',\'featureType\',\r\n    :   \'fields\', \'hasOID\', \'hasM\', \'hasZ\', \'path\'\r\n    : - all_flds =[i.name for i in desc[\'fields\']]\r\n    """"""\r\n    desc = arcpy.da.Describe(in_fc)\r\n    args = [\'shapeFieldName\', \'OIDFieldName\', \'spatialReference\', \'shapeType\']\r\n    shp_fld, oid_fld, SR, shp_type = [desc[i] for i in args]\r\n    return shp_fld, oid_fld, SR, shp_type\r\n\r\n\r\ndef fld_info(in_fc):\r\n    """"""Field information for a featureclass\r\n    :\r\n    """"""\r\n    flds = arcpy.ListFields(in_fc)\r\n    f_info = [(i.type, i.name, i.length) for i in flds]\r\n    return f_info\r\n\r\n\r\ndef change_fld(flds):\r\n    """"""Convert the field types to array friendly ones""""""\r\n    info = [(fld.type, fld.name, fld.length) for fld in flds]\r\n    dt = []\r\n    for i in info:\r\n        if i[0] in (\'OID\', \'Integer\', \'Long\', \'Short\'):\r\n            dt.append((i[1], \'<i4\'))\r\n        elif i[0] in (\'Double\', \'Single\', \'Float\'):\r\n            dt.append((i[1], \'<f8\'))\r\n        else:\r\n            dt.append(i[1], ""{}{}"".format(\'U\', i[2]))\r\n    return dt\r\n\r\n\r\ndef tweet(msg):\r\n    """"""Print a message for both arcpy and python.\r\n    : msg - a text message\r\n    """"""\r\n    m = ""\\n{}\\n"".format(msg)\r\n    arcpy.AddMessage(m)\r\n    print(m)\r\n    print(arcpy.GetMessages())\r\n\r\n\r\n# ---- Array to featureclass section ----\r\n\r\n@time_deco\r\ndef _split_array(a, fld=\'ID\'):\r\n    """"""Split a structured/recarray array, using unique values in a numeric id\r\n    :  \'fld\' assumed to be sorted in the correct order which indicates the\r\n    :  group a record belongs to.  From \'arraytools split_array\'.\r\n    """"""\r\n    return np.split(a, np.where(np.diff(a[fld]))[0] + 1)\r\n\r\n\r\ndef _arr_common(a, oid_fld, shp_fld):\r\n    """"""Common structure for polyline, polygon and multipoint features.\r\n    :  This functions pulls out all the array points that belong to a\r\n    :  feature so they can be used in shape construction.\r\n    :\r\n    :Requires:\r\n    :--------\r\n    : a - structured/recarray\r\n    : oid_fld -  the object index/id field name\r\n    : shp_fld - the table field which contains the geometry\r\n    """"""\r\n    arcpy.overwriteOutput = True\r\n    pts = []\r\n    keys = np.unique(a[oid_fld])\r\n    for k in keys:\r\n        w = np.where(a[oid_fld] == k)[0]\r\n        v = a[shp_fld][w[0]:w[-1] + 1]\r\n        pts.append(v)\r\n    return pts\r\n\r\n\r\ndef _shapes_fc(shps, out_fc):\r\n    """"""Create a featureclass/shapefile from poly* shapes.\r\n    :\r\n    :Requires:\r\n    :--------\r\n    :  shps   - point geometry objects needed to create the featureclass\r\n    :  out_fc - full path and name to the output container (gdb or folder)\r\n    """"""\r\n    msg0 = ""\\nCreated..\\n{}"".format(out_fc)\r\n    msg1 = ""\\nCan\'t overwrite the {}... rename it"".format(out_fc)\r\n    try:\r\n        if arcpy.Exists(out_fc):\r\n            arcpy.Delete_management(out_fc)\r\n        arcpy.CopyFeatures_management(shps, out_fc)\r\n        tweet(msg0)\r\n    except:\r\n        tweet(msg1)\r\n\r\n\r\ndef arr_pnts(a, out_fc, shp_fld, SR):\r\n    """"""Make point features from a structured array.\r\n    :\r\n    :Requires:\r\n    :--------\r\n    : a - structured/recarray\r\n    : out_fc - featureclass full path and name\r\n    : shp_fld - the table field which contains the geometry\r\n    : SR - the spatial reference/coordinate system of the geometry\r\n    """"""\r\n    msg0 = ""\\nCreated..\\n{}"".format(out_fc)\r\n    msg1 = ""\\nCan\'t overwrite {}... rename it"".format(out_fc)\r\n    try:\r\n        if arcpy.Exists(out_fc):\r\n            arcpy.Delete_management(out_fc)\r\n        arcpy.da.NumPyArrayToFeatureClass(a, out_fc, shp_fld, SR)\r\n        tweet(msg0)\r\n    except:\r\n        tweet(msg1)\r\n\r\n\r\ndef arr_polyline(a, out_fc, oid_fld, shp_fld, SR):\r\n    """"""Make polyline features from a structured array.\r\n    :\r\n    :Requires:\r\n    :--------\r\n    :  a - structured array\r\n    :  out_fc - a featureclass path and name, or None\r\n    :  oid_fld - object id field, used to partition the shapes into groups\r\n    :  shp_fld - shape field(s)\r\n    :  SR - spatial reference object, or name\r\n    :\r\n    :Returns:\r\n    :-------\r\n    :  Produces the featureclass optionally, but returns the polygons anyway.\r\n    """"""\r\n    pts = _arr_common(a, oid_fld, shp_fld)\r\n    f = []\r\n    for pt in pts:\r\n        f.append(arcpy.Polygon(arcpy.Array([arcpy.Point(*p)\r\n                                            for p in pt.tolist()]), SR))\r\n    _shapes_fc(f, out_fc)\r\n\r\n\r\ndef arr_polygon(a, out_fc, oid_fld, shp_fld, SR):\r\n    """"""Make polygon features from a structured array.\r\n    :\r\n    :Requires:\r\n    :--------\r\n    :  a - structured array\r\n    :  out_fc - a featureclass path and name, or None\r\n    :  oid_fld - object id field, used to partition the shapes into groups\r\n    :  shp_fld - shape field(s)\r\n    :  SR - spatial reference object, or name\r\n    :\r\n    :Returns:\r\n    :-------\r\n    :  Produces the featureclass optionally, but returns the polygons anyway.\r\n    """"""\r\n    pts = _arr_common(a, oid_fld, shp_fld)\r\n    f = []\r\n    for pt in pts:\r\n        f.append(arcpy.Polygon(arcpy.Array([arcpy.Point(*p)\r\n                                            for p in pt.tolist()]), SR))\r\n    _shapes_fc(f, out_fc)\r\n\r\n\r\ndef array_fc(a, out_fc=None, shp_fld=[\'Shape\'], SR=None):\r\n    """"""Array to featureclass/shapefile...optionally including all fields\r\n    :\r\n    :Requires:\r\n    :--------\r\n    : a - structured or recarray\r\n    : out_fc - the full path and filename to a featureclass or shapefile\r\n    : shp_fld - shapefield name(s) ie. [\'Shape\'] or [\'X\', \'Y\']\r\n    :\r\n    :References:\r\n    :----------\r\n    : - NumpyArrayToFeatureClass, ListFields for information and options\r\n    :\r\n    """"""\r\n    if not out_fc:\r\n        if arcpy.Exists(out_fc):\r\n            arcpy.Delete_management(out_fc)\r\n        arcpy.da.NumPyArrayToFeatureClass(a, out_fc, shp_fld, SR)\r\n    # return out_fc\r\n\r\n\r\ndef array_struct(a, fld_names=[\'X\', \'Y\'], dt=[\'<f8\', \'<f8\']):\r\n    """"""Convert an array to a structured array\r\n    :\r\n    :Requires:\r\n    :--------\r\n    :  a - an ndarray with shape at least (N, 2)\r\n    :  dt = dtype class\r\n    :  names - names for the fields\r\n    """"""\r\n    dts = [(fld_names[i], dt[i]) for i in range(len(fld_names))]\r\n    z = np.zeros((a.shape[0],), dtype=dts)\r\n    names = z.dtype.names\r\n    for i in range(a.shape[1]):\r\n        z[names[i]] = a[:, i]\r\n    return z\r\n\r\n\r\n# ---- Featureclass to array section ----\r\n\r\ndef arc_np(in_fc):\r\n    """"""Alternate constructor to create a structured array from a feature class.\r\n    :  This function only returns the geometry portion.  The OID field is\r\n    :  retained should you wish to associate attributes back to the results.\r\n    :  There is no point in carrying extra baggage around when you don\'t need\r\n    :  it.\r\n    :\r\n    :Requires:\r\n    :--------\r\n    :  in_fc - the file path to the feature class\r\n    :    ArcGIS Pro is assumed, since the new arcpy.da.Describe is used\r\n    :    which returns a dictionary of properties.\r\n    :\r\n    :Returns:\r\n    :-------\r\n    :  A structured array with a specified dtype to facilitate\r\n    :  geometry reconstruction later.  The attributes are kept separate.\r\n    :\r\n    :Alternatives:\r\n    :------------\r\n    :  SpatialDataFrame from ArcGIS API for Python... or GeoPandas\r\n    """"""\r\n    desc = arcpy.da.Describe(in_fc)\r\n    shp_type = desc[\'shapeType\']\r\n    prefix = desc[\'shapeFieldName\']\r\n    fields = [\'OID@\', prefix + \'@\']\r\n    dt = [(\'ID_num\', \'<i4\'), (\'Part_num\', \'<i4\'), (\'Xs\', \'<f8\'), (\'Ys\', \'<f8\')]\r\n    pnts = []\r\n    with arcpy.da.SearchCursor(in_fc, fields) as cursor:\r\n        for row in cursor:\r\n            oid, shp = row\r\n            for j in range(len(shp)):\r\n                pt = shp.getPart(j)\r\n                if shp_type in (\'Point\', \'point\'):\r\n                    pnts.extend([(oid, j, pt.X, pt.Y)])\r\n                else:\r\n                    p_list = [(oid, j, pnt.X, pnt.Y) for pnt in pt if pnt]\r\n                    pnts.extend(p_list)\r\n    a = np.asarray(pnts, dtype=dt)\r\n    return a\r\n\r\n\r\ndef pnts_arr(in_fc, as_struct=True, shp_fld=None, SR=None):\r\n    """"""Create points from an array.\r\n    :\r\n    :Requires:\r\n    :--------\r\n    :  in_fc - input featureclass\r\n    :  as_struct - if True, returns a structured array with Id, X, Y fields\r\n    :            - if False, returns an ndarray with dtype=\'<f8\'\r\n    :  shp_fld & SR - if unspecified, it will be determined using fc_info\r\n    :  to return featureclass information.\r\n    """"""\r\n    if shp_fld is None or SR is None:\r\n        shp_fld, oid_fld, SR, shp_type = fc_info(in_fc)\r\n    a = arcpy.da.FeatureClassToNumPyArray(in_fc, ""*"", """", SR)\r\n    dt = [(\'Id\', \'<i4\'), (\'X\', \'<f8\'), (\'Y\', \'<f8\')]\r\n    if as_struct:\r\n        shps = np.array([i for i in a[[oid_fld, shp_fld]]], dtype=dt)\r\n    else:\r\n        shps = a[shp_fld]\r\n    return shps\r\n\r\n# ---- ******* main conversion section ***** ----\r\n# ----         ----------------------        ----\r\n\r\n\r\ndef _id_geom_array(in_fc):\r\n    """"""The main code segment which gets the id and shape information and\r\n    :  explodes the geometry to individual points\r\n    """"""\r\n    shp_fld, oid_fld, SR, shp_type = fc_info(in_fc)\r\n    a_flds = [oid_fld, shp_fld]\r\n    a = arcpy.da.FeatureClassToNumPyArray(in_fc, field_names=a_flds,\r\n                                          explode_to_points=True,\r\n                                          spatial_reference=SR)\r\n    dt = [(\'Idx\', \'<i4\'), (\'Xs\', \'<f8\'), (\'Ys\', \'<f8\')]\r\n    a.dtype = dt\r\n    return a\r\n\r\n\r\ndef polylines_arr(in_fc, as_struct=True):\r\n    """"""Create an array from polylines.\r\n    """"""\r\n    return _id_geom_array(in_fc)\r\n\r\n\r\ndef polygons_arr(in_fc, as_struct=True, shp_fld=None, SR=None):\r\n    """"""Create an array from polygons.\r\n    """"""\r\n    return _id_geom_array(in_fc)\r\n\r\n\r\ndef fc_array(in_fc, flds="""", allpnts=True):\r\n    """"""Convert a featureclass to an ndarray of attribute, with the geometry\r\n    :  removed.\r\n    :\r\n    :Requires:\r\n    :--------\r\n    : input_fc - featureclass/shapefile complete path\r\n    : flds     - """"  just oid and shape fields\r\n    :          - ""*"" all fields or\r\n    :          - [\'Field1\', \'Field2\', etc] for specific fields\r\n    : allpnts  - True/False\r\n    :          - True to explode the geometry to individual points\r\n    :          - False for the centroid of the geometry\r\n    :References:\r\n    :----------\r\n    :  FeatureClassToNumPyArray, ListFields for more information\r\n    """"""\r\n    out_flds = []\r\n    shp_fld, oid_fld, SR, shp_type = fc_info(in_fc)  # get the base information\r\n    fields = arcpy.ListFields(in_fc)       # all fields in the shapefile\r\n    if flds == """":                         # return just OID and Shape field\r\n        out_flds = [oid_fld, shp_fld]      # FID and Shape field required\r\n    elif flds == ""*"":                      # all fields\r\n        out_flds = [f.name for f in fields]\r\n    else:                                  # oid, shape and specific fields\r\n        out_flds = [oid_fld, shp_fld]\r\n        for f in fields:\r\n            if f.name in flds:\r\n                out_flds.append(f.name)\r\n    frmt = """"""Creating array from featureclass using fc_array...with...\r\n    {}\\nFields...{}\\nAll pnts...{}\\nSR...{}\r\n    """"""\r\n    frmt = dedent(frmt)\r\n    args = [in_fc, out_flds, allpnts, SR.name]\r\n    msg = frmt.format(*args)\r\n    tweet(msg)\r\n    a = arcpy.da.FeatureClassToNumPyArray(in_fc, out_flds, """", SR, allpnts)\r\n    # out it goes in array format\r\n    return a\r\n\r\n\r\ndef tbl_arr(in_fc):\r\n    """""" Convert a table or featureclass table (in_fc)to a numpy array\r\n    :   including the oid field but excluding the geometry field.\r\n    """"""\r\n    desc = arcpy.da.Describe(in_fc)  # use the new da.Describe method\r\n    dump = [\'shapeFieldName\', \'areaFieldName\', \'lengthFieldName\']\r\n    f_geo = [desc[i] for i in dump]\r\n    fields = [f for f in arcpy.ListFields(in_fc) if f.name not in f_geo]\r\n    f_names = [f.name for f in fields]\r\n    dt = change_fld(fields)\r\n    vals = []\r\n    with arcpy.da.SearchCursor(in_fc, field_names=f_names) as rows:\r\n        for row in rows:\r\n            vals.append(row)\r\n            del row\r\n        del rows\r\n    dt = [(i.replace(\'OBJECTID\', \'Idx\'), j) for i, j in dt]\r\n    a = np.asarray(vals, dtype=dt)\r\n    return a  # vals, az\r\n\r\n\r\n# ---- functions to send to an array and send back to a featureclass ----\r\n# (1) To ndarray or recarry\r\n\r\n#@time_deco\r\ndef _to_ndarray(in_fc, to_pnts=True):\r\n    """"""Convert searchcursor shapes an ndarray quickly.\r\n    :\r\n    :Notes:\r\n    :-----\r\n    :  field_names\r\n    :    [\'OBJECTID\', \'Shape\'], [\'OID@\', \'Shape\'], [\'OID@\', \'Shape\']\r\n    :    [\'OID@\', \'SHAPE@JSON\']\r\n    :  cur = arcpy.da.SearchCursor(in_fc, field_names, .....)\r\n    :      =\r\n    :  cur = arcpy.da.SearchCursor(in_fc, [\'OBJECTID\', \'Shape\'], None, None,\r\n    :                              True, (None, None))\r\n    """"""\r\n    flds = [\'OID@\', \'SHAPE@X\', \'SHAPE@Y\']\r\n    cur = arcpy.da.SearchCursor(in_fc, flds, None, None, True, (None, None))\r\n    flds = cur.fields\r\n    dt = cur._dtype\r\n    a = cur._as_narray()\r\n    return a, flds, dt\r\n\r\n\r\n#@time_deco\r\ndef two_arrays(in_fc, both=True, split=True):\r\n    """"""Send to a numpy structured/array and split it into a geometry array\r\n    :  and an attribute array.\r\n    :\r\n    :Requires:\r\n    :--------\r\n    : fc_info(in_fc) - function needed to return fc properties\r\n    : split_geom\r\n    :    - True, split the points into their geometry groups as an object array\r\n    :    - False, a sequential array of shape = (N,)\r\n    : dtype0\r\n    :    - True  dt = [(\'Idx\', \'<i4\'), (\'Xs\', \'<f8\'), (\'Ys\', \'<f8\')]\r\n    :    - False dt = [(\'Idx\', \'<i4\'), (\'XY\', \'<f8\', (2,))]\r\n    """"""\r\n    shp_fld, oid_fld, SR, shp_type = fc_info(in_fc)\r\n    all_flds = arcpy.ListFields(in_fc)\r\n    b_flds = [i.name for i in all_flds]\r\n    a = arcpy.da.FeatureClassToNumPyArray(in_fc,\r\n                                          field_names=[oid_fld, shp_fld],\r\n                                          explode_to_points=True,\r\n                                          spatial_reference=SR)\r\n    dt = [(\'Idx\', \'<i4\'), (\'Xs\', \'<f8\'), (\'Ys\', \'<f8\')]  # option 1\r\n    dtb = [(\'Idx\', \'<i4\'), (\'Xc\', \'<f8\'), (\'Yc\', \'<f8\')]\r\n    a.dtype = dt\r\n    oid_fld = \'Idx\'\r\n    if split:\r\n        ids = np.unique(a[\'Idx\'])\r\n        w = np.where(np.diff(a[\'Idx\']))[0] + 1\r\n        a = np.split(a, w)\r\n        dt = [(\'Idx\', \'<i4\'), (\'Shp\', \'O\')]\r\n        a = np.array([[ids[i], a[i][[\'Xs\', \'Ys\']]] for i in range(len(ids))])\r\n#        a = [np.array([ids[i], a[i][[\'Xs\', \'Ys\']]], dtype=dt)\r\n#                      for i in range(len(ids))]\r\n#        a[np.where(a[\'Idx\']==1)][[\'Xs\', \'Ys\']]\r\n#        z = np.array((1, a[np.where(a[\'Idx\']==1)][[\'Xs\', \'Ys\']]), dtype=dt)\r\n    b = None\r\n    if both:\r\n        b = arcpy.da.FeatureClassToNumPyArray(in_fc, field_names=b_flds,\r\n                                              explode_to_points=False,\r\n                                              spatial_reference=SR)\r\n        dtb.extend(b.dtype.descr[2:])\r\n        b.dtype = dtb\r\n    return a, b\r\n\r\n\r\ndef to_fc(out_fc, a, b=None, dim=2, flds=[\'Id\', \'X\', \'Y\'], SR_code=None):\r\n    """"""Reconstruct a featureclass from a deconstructed pair of arrays.\r\n    :  This function reverses the functionality of to_array which splits a\r\n    :  featureclass into an array of geometry and one of attributes.\r\n    :  One can perform operations on one or the other or both, then reassemble\r\n    :  into a new file.\r\n    :\r\n    :Requires:\r\n    :--------\r\n    : out_fc - full path and name for the output featureclass\r\n    : a - the array of geometry objects\r\n    : b - the array of attributes for \'a\'\r\n    : dim - 0 (point), 1 (polyline) or 2 (polygon)\r\n    : fld - id and shape field(s), normally [\'Id, \'X\', \'Y\']\r\n    :       if deconstructed using \'to_array\'\r\n    : SR_code - the spatial reference code of the output geometry\r\n    :           ie 4326 for GCS WGS84\r\n    :              2951\r\n    :References:\r\n    :----------\r\n    : Spatial reference http://spatialreference.org/\r\n    """"""\r\n    args = [to_fc.__module__, dedent(to_fc.__doc__)]\r\n    msg = ""\\n...to_fc ... in {} failed\\n{}"".format(*args)\r\n    try:\r\n        SR = arcpy.SpatialReference(SR_code)\r\n    except ValueError:\r\n        tweet(""Spatial reference is in error"")\r\n        tweet(msg)\r\n        return None\r\n    if (dim not in (0, 1, 2)) or (len(flds) not in (2, 3)):\r\n        tweet(msg)\r\n        return None\r\n    if len(flds) == 2:\r\n        oid_fld, shp_fld = flds\r\n    else:\r\n        oid_fld, shp_fld = flds[0], flds[1:3]\r\n    if dim == 0:\r\n        arr_pnts(a, out_fc, shp_fld, SR)\r\n#    geom = _split_array(a, fld=oid_fld)\r\n    geom = np.split(a, np.where(np.diff(a[oid_fld]))[0] + 1)\r\n    prts = [i[[\'Xs\', \'Ys\']].tolist() for i in geom]\r\n    if dim == 1:\r\n        arr_polyline(geom, out_fc, oid_fld, shp_fld, SR)\r\n    else:\r\n        # arr_polygon(geom, out_fc, oid_fld, shp_fld, SR)\r\n        # pts = _arr_common(a, oid_fld, shp_fld)\r\n        f = []\r\n        for pt in prts:  # g\r\n            f.append(arcpy.Polygon(\r\n                             arcpy.Array([arcpy.Point(*p) for p in pt]), SR))\r\n#    _shapes_fc(f, out_fc)\r\n#    arcpy.da.ExtendTable(out_fc, table_match_field=oid_fld,\r\n#                         in_array=b, array_match_field=oid_fld)\r\n    return f\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\n\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n#    _demo()\r\n    # in_fc0 = r""C:\\Git_Dan\\a_Data\\testdata.gdb\\Points_10""\r\n    # in_fc1 = r""C:\\Git_Dan\\a_Data\\testdata.gdb\\Polyline_connected""\r\n\r\n    #in_fc = r""C:\\Git_Dan\\a_Data\\testdata.gdb\\polygon""\r\n    in_fc = r""C:\\Git_Dan\\a_Data\\arcpytools_demo.gdb\\Can_0_big_3""\r\n    #in_fc = r""C:\\Data\\Canada\\CAN_adm0.gdb\\CAN_0_sp""\r\n    a0, _ = two_arrays(in_fc, both=False, split=False)\r\n    a1, *_ = _to_ndarray(in_fc, to_pnts=True)\r\n\r\n    # fc_array(in_fc1, flds="""", allpnts=True)\r\n#    out_fc = r""C:\\Git_Dan\\a_Data\\testdata.gdb\\test_1""\r\n#    out_tbl = r""C:\\Git_Dan\\a_Data\\testdata.gdb\\join_tbl""\r\n'"
all_scripts/arr_moving.py,30,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\narr_moving\r\n==========\r\n\r\nScript :   arr_moving.py\r\n\r\nAuthor :   Dan.Patterson@carleton.ca\r\n\r\nModified : 2016-07-31\r\n\r\nPurpose :  Functions to import for use with numpy arrays.\r\n\r\nFunctions :\r\n  public  -  block, stride_, deline, rolling_stats (_check required)\r\n\r\n  private - _check, _pad, _demo\r\n\r\nNotes :\r\n  The following demonstration, shows how deline, stride, block\r\n  and rolling_stats work.\r\n\r\n\r\n**Rolling_stats example**\r\n\r\nStride the input array, \'a\' using a 3x3 window\r\n\r\n>>> c = stride(a, r_c=(3,3))\r\nMain array... \'a\'\r\nndim: 2 size: 54\r\nshape: (6, 9)\r\n[[ 0  1 ...,  7  8]\r\n [ 9 10 ..., 16 17]\r\n ...,\r\n [36 37 ..., 43 44]\r\n [45 46 ..., 52 53]]\r\n\r\n>>> print(deline(c[0][:2]))\r\nMain array...\r\nndim: 3 size: 18\r\nshape: (2, 3, 3)\r\n[[[ 0  1  2]\r\n  [ 9 10 11]\r\n  [18 19 20]]\r\na[1]....\r\n [[ 1  2  3]\r\n  [10 11 12]\r\n  [19 20 21]]]\r\n----- big clip\r\n\r\n>>> ax = tuple(np.arange(len(c.shape))[-2:]) # ax == (2, 3)\r\n>>> c_m =np.mean(c, ax)\r\n>>> print(deline(c_m))\r\nMain array...\r\nndim: 2 size: 28\r\nshape: (4, 7)\r\n[[ 10.0  11.0 ...,  15.0  16.0]\r\n [ 19.0  20.0 ...,  24.0  25.0]\r\n [ 28.0  29.0 ...,  33.0  34.0]\r\n [ 37.0  38.0 ...,  42.0  43.0]]\r\n\r\nWhich is what we expect.  The trick is to produce the\r\nstats on the last 2 entries in the array\'s shape.  If we do this\r\nwith a normal 2D array, like \'a\', we get...\r\n\r\n\r\n    np.mean(a,axis=(0,1)) == 26.5\r\n\r\n\r\nwhich for the whole 6*9 array rather than moving 3*3 window slices\r\nthrough it.  Striding or blocking a 2D array, results in a 4D\r\narray, statistics are calculate on the last 2 dimensions (2,3)\r\n\r\n\r\n**Block stats example**\r\n\r\nFollowing the same procedure above, the results are\r\n\r\n>>> d = block(a, r_c=(3,3))\r\n array([[[[ 0,  1,  2],\r\n          [ 9, 10, 11],\r\n          [18, 19, 20]],\r\n        .... snip .....\r\n        [[33, 34, 35],\r\n         [42, 43, 44],\r\n         [51, 52, 53]]]])\r\n>>> ax = tuple(np.arange(len(c.shape))[-2:]) which is (2,3) for 4D\r\n>>> d_m =np.mean(d, ax)\r\n>>> print(deline(d_m))\r\n Main array...\r\n ndim: 2 size: 6\r\n shape: (2, 3)\r\n [[ 10.0  13.0  16.0]\r\n  [ 37.0  40.0  43.0]]\r\n\r\n\r\n**Masked arrays**\r\n\r\n>>> m = np.where((a>19) & (a<27),1,0)\r\n>>> a_msk = np.ma.MaskedArray(a, mask=m, dtype=\'float\')  (fill value 1e 200)\r\n>>> a_msk = np.ma.MaskedArray(a, mask=m, dtype=\'float\',fill_value=np.nan)\r\n\r\nlast option useful for simplifying nodata values\r\n\r\n>>> a_msk\r\nmasked_array(data =\r\n [[0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0]\r\n  [9.0 10.0 11.0 12.0 13.0 14.0 15.0 16.0 17.0]\r\n  [18.0 19.0 -- -- -- -- -- -- --]\r\n  [27.0 28.0 29.0 30.0 31.0 32.0 33.0 34.0 35.0]\r\n  [36.0 37.0 38.0 39.0 40.0 41.0 42.0 43.0 44.0]\r\n  [45.0 46.0 47.0 48.0 49.0 50.0 51.0 52.0 53.0]],\r\n          mask =\r\n [[0 0 0 0 0 0 0 0 0]\r\n  [0 0 0 0 0 0 0 0 0]\r\n  [0 0 1 1 1 1 1 1 1]\r\n  [0 0 0 0 0 0 0 0 0]\r\n  [0 0 0 0 0 0 0 0 0]\r\n  [0 0 0 0 0 0 0 0 0]],\r\n        fill_value = nan)\r\n\r\n\r\n>>> np.mean(a)        => 26.5 ! nan values not accounted for, hence wrong\r\n>>> np.mean(a_msk)    => 27.021276595744681  accounted for\r\n>>> np.ma.mean(a_msk) => 27.021276595744681\r\n\r\n\r\nReferences:\r\n-----------\r\n\r\n\r\n""""""\r\n\r\n# ---- imports, formats, constants ----\r\n\r\nimport sys\r\nimport numpy as np\r\nfrom numpy.lib.stride_tricks import as_strided\r\nfrom textwrap import dedent\r\nfrom arraytools.frmts import deline\r\n\r\n__all__ = [\'_check\', \'block\', \'stride_\', \'rolling_stats\']\r\n__outside__ = [\'as_strided\', \'dedent\', \'deline\']\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\r\n      \'float\': \'{: 0.1f}\'.format}\r\nnp.set_printoptions(edgeitems=5, linewidth=80, precision=2,\r\n                    suppress=True, threshold=100, formatter=ft)\r\n\r\nscript = sys.argv[0]\r\n\r\n\r\n# ---- functions ----\r\n\r\ndef _check(a, r_c):\r\n    """"""Performs the array checks necessary for stride and block.\r\n\r\n    `a` : array-like\r\n        Array or list.\r\n    `r_c` : tuple/list/array of rows x cols.\r\n        Attempts will be made to produce a shape at least (1*c).  If a scalar\r\n        is given, the minimum shape will be (1*r) for 1D array or (1*c) for\r\n        2D array if r<c.  Be aware\r\n    """"""\r\n    if isinstance(r_c, (int, float)):\r\n        r_c = (1, int(r_c))\r\n    r, c = r_c\r\n    a = np.atleast_2d(a)\r\n    shp = a.shape\r\n    r, c = r_c = (min(r, a.shape[0]), min(c, shp[1]))\r\n    a = np.ascontiguousarray(a)\r\n    return a, shp, r, c, tuple(r_c)\r\n\r\n\r\ndef _pad(a, nan_edge=False):\r\n    """"""Pad a sliding array to allow for stats""""""\r\n    if nan_edge:\r\n        a = np.pad(a, pad_width=(1, 2), mode=""constant"",\r\n                   constant_values=(np.NaN, np.NaN))\r\n    else:\r\n        a = np.pad(a, pad_width=(1, 1), mode=""reflect"")\r\n    return a\r\n\r\n\r\ndef stride_(a, r_c=(3, 3)):\r\n    """"""Provide a 2D sliding/moving view of an array.\r\n    There is no edge correction for outputs.\r\n\r\n    Requires:\r\n    --------\r\n    `a` : array-like\r\n        Array or list, usually a 2D array.  Assumes the rows is >=1, it is\r\n        corrected as is the number of columns.\r\n    `r_c` : tuple/list/array of rows x cols.\r\n        Attempts will be made to produce a shape at least (1*c).  If a scalar\r\n        is given, the minimum shape will be (1*r) for 1D array or (1*c)\r\n        for 2D array if r<c.  Be aware\r\n\r\n    See also:\r\n    ---------\r\n        A more detailed version of `stride` is available in `tools.py`\r\n    """"""\r\n    a, shp, r, c, r_c = _check(a, r_c)\r\n    shp = (a.shape[0] - r + 1, a.shape[1] - c + 1) + r_c\r\n    strd = a.strides * 2\r\n    a_s = (as_strided(a, shape=shp, strides=strd)).squeeze()\r\n    return a_s\r\n\r\n\r\ndef block(a, r_c=(3, 3)):\r\n    """"""See _check and/or stride for documentation.  This function  moves in\r\n    increments of the block size, rather than sliding by one row and column.\r\n    """"""\r\n    a, shp, r, c, r_c = _check(a, r_c)\r\n    shp = (a.shape[0]//r, a.shape[1]//c) + r_c\r\n    strd = (r*a.strides[0], c*a.strides[1]) + a.strides\r\n    a_b = as_strided(a, shape=shp, strides=strd).squeeze()\r\n    return a_b\r\n\r\n\r\ndef rolling_stats(a, no_null=True, prn=True):\r\n    """"""Statistics on the last two dimensions of an array.\r\n\r\n    Requires:\r\n    --------\r\n    `a` : array\r\n        2D array\r\n    `no_null` : boolean\r\n        Whether to use masked values (nan) or not.\r\n    `prn` : boolean\r\n        To print the results or return the values.\r\n\r\n    Returns:\r\n    -------\r\n\r\n    The results return an array of 4 dimensions representing the original\r\n    array size and block size\r\n\r\n    eg.  original = 6x6 array   block=3x3 ...breaking the array into 4 chunks\r\n    """"""\r\n    a = np.asarray(a)\r\n    a = np.atleast_2d(a)\r\n    ax = None\r\n    if a.ndim > 1:\r\n        ax = tuple(np.arange(len(a.shape))[-2:])\r\n    if no_null:\r\n        a_min = a.min(axis=ax)\r\n        a_max = a.max(axis=ax)\r\n        a_mean = a.mean(axis=ax)\r\n        a_sum = a.sum(axis=ax)\r\n        a_std = a.std(axis=ax)\r\n        a_var = a.var(axis=ax)\r\n        a_ptp = a_max - a_min\r\n    else:\r\n        a_min = np.nanmin(a, axis=(ax))\r\n        a_max = np.nanmax(a, axis=(ax))\r\n        a_mean = np.nanmean(a, axis=(ax))\r\n        a_sum = np.nansum(a, axis=(ax))\r\n        a_std = np.nanstd(a, axis=(ax))\r\n        a_var = np.nanvar(a, axis=(ax))\r\n        a_ptp = a_max - a_min\r\n    if prn:\r\n        frmt = ""Minimum...\\n{}\\nMaximum...\\n{}\\nMean...\\n{}\\n"" +\\\r\n               ""Sum...\\n{}\\nStd...\\n{}\\nVar...\\n{}\\nRange...\\n{}""\r\n        frmt = dedent(frmt)\r\n        args = [a_min, a_max, a_mean, a_sum, a_std, a_var, a_ptp]\r\n        print(frmt.format(*args))\r\n    else:\r\n        return a_min, a_max, a_mean, a_sum, a_std, a_var, a_ptp\r\n\r\n\r\n# -----------------------------------\r\ndef _demo():\r\n    """"""\r\n    :Run demo of block, for a 2D array which yields a 3D array\r\n    :Run a stride of a 2D array which yields a 4D array\r\n    """"""\r\n    r = 6         # array rows\r\n    c = 9         # array columns\r\n    r_c = (3, 3)  # moving/block window size\r\n    a = np.arange(r*c).reshape(r, c)\r\n    b = block(a)\r\n    c = stride_(a, r_c)\r\n#    print(deline(a))\r\n#    print(deline(b))\r\n#    print(deline(c))\r\n    frmt = """"""\r\n    Rolling stats for \'a\' using a 3x3 rolling window\r\n    :array a...\r\n    :  ndim {}  size {}\r\n    :  print(deline(c[0][:2])\r\n    {}\r\n    :etc......\\n\r\n    :rolling mean:\r\n    :  c = stride(a, r_c=(3,3))\r\n    :  c.shape  # (4, 7, 3, 3)\r\n    :  ax = tuple(np.arange(len(c.shape))[-2:]) #(0,1,2,3) => (2,3)\r\n    :  c_m =np.mean(c, ax)\r\n    : ==> {}\r\n    """"""\r\n    c_m = np.mean(c, axis=(2, 3))\r\n    as0 = deline(c[0, :2])\r\n    as1 = deline(c_m)\r\n    args = [c.ndim, c.size, as0, as1]\r\n    print(dedent(frmt).format(*args))\r\n    return a, b, c\r\n\r\n\r\n# ----------------------\r\nif __name__ == ""__main__"":\r\n    """"""   """"""\r\n#    print(""Script... {}"".format(script))\r\n#    a, b, c = _demo()\r\n'"
all_scripts/arr_scatter.py,4,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\narr_scatter\r\n===========\r\n\r\nScript :   arr_scatter.py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified: 2018-04-14\r\n\r\nPurpose:\r\n--------\r\n    Sample scatterplot plotting\r\n\r\nNotes:\r\n------\r\n    >>> print(plt.style.available)\r\n    >>> import matplotlib.pyplot.figure as fig\r\n    # figure(num=None, figsize=None, dpi=None, facecolor=None, edgecolor=None,\r\n    #        frameon=True, FigureClass=<class \'matplotlib.figure.Figure\'>,\r\n    #        clear=False, **kwargs)\r\n    # matplotlib.pyplot.subplots\r\n    # subplots(nrows=1, ncols=1, sharex=False, sharey=False, squeeze=True,\r\n    #          subplot_kw=None, gridspec_kw=None, **fig_kw)\r\n\r\nReferences:\r\n-----------\r\n\r\n[1] https://matplotlib.org/users/customizing.html\r\n\r\n[2] https://matplotlib.org/api/_as_gen/matplotlib.pyplot.figure.html\r\n\r\n[3] https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplots.html\r\n\r\n""""""\r\n# ---- imports, formats, constants ----\r\n\r\nimport sys\r\nimport numpy as np\r\n# import matplotlib\r\n# matplotlib.use(\'QT5agg\')  # don\'t work Agg, WX, QTAgg, QT4Agg\r\nimport matplotlib.pyplot as plt\r\nfrom matplotlib.markers import MarkerStyle\r\n\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\r\n      \'float\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2,\r\n                    suppress=True, threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')\r\n\r\nscript = sys.argv[0]\r\n\r\n# ---- functions ----\r\n\r\n\r\ndef plot_pnts_(pnts, title=\'Title\', r_c=False, lbls=[\'X\', \'Y\'], params=True):\r\n    """"""Plot points for Nx2 array representing x,y or row,col data.\r\n\r\n    Requires:\r\n    --------\r\n    see _params() to specify special parameters\r\n\r\n    pnts : points\r\n        point or row/column array\r\n    r_c : boolean\r\n        If True, the y-axis is inverted to represent row-column formatting\r\n        rather than x,y formatting.\r\n\r\n    Returns:\r\n    --------\r\n    A scatterplot representing the data.  It is easier to modify the\r\n    script below than to provide a whole load of input parameters.\r\n\r\n    """"""\r\n    def scatter_params(plt, fig, ax, title=""Title"", lbls=[\'X\', \'Y\']):\r\n        """"""Default parameters for plots\r\n        :Notes:\r\n        :  ticklabel_format(useoffset), turns off scientific formatting\r\n        """"""\r\n        fig.set_figheight = 8\r\n        fig.set_figwidth = 6\r\n        fig.dpi = 200\r\n        x_label, y_label = lbls\r\n        font1 = {\'family\': \'sans-serif\', \'color\': \'black\',\r\n                 \'weight\': \'bold\', \'size\': 12}  # set size to other values\r\n    #    markers = [\'o\', \'s\', \'+\', \'*\', \'x\']\r\n        ax.set_aspect(\'equal\', adjustable=\'box\')\r\n    #    ax.margins(0.05)  # set a gap\r\n        ax.ticklabel_format(style=\'sci\', axis=\'both\', useOffset=False)\r\n        ax.set_xlabel(x_label, labelpad=12)\r\n        ax.xaxis.label_position = \'bottom\'\r\n        ax.xaxis.label.set_fontsize(14)\r\n    #    ax.get_xaxis().get_major_formatter().set_scientific(False)\r\n        ax.set_ylabel(y_label, labelpad=12)\r\n        ax.yaxis.label_position = \'left\'\r\n        ax.yaxis.label.set_fontsize(14)\r\n        plt.title(title + ""\\n"", loc=\'center\', fontdict=font1, size=16)\r\n        plt.tight_layout\r\n        plt.grid(True)\r\n    #\r\n    # ---- main plotting routine\r\n    fig, ax = plt.subplots(1, 1)\r\n    markers = MarkerStyle.filled_markers\r\n    # ---- optional parameter def ----\r\n    if params:  # use parameter def\r\n        scatter_params(plt, fig, ax, title, lbls)\r\n        x_min, y_min = pnts.min(axis=0) - [0.5, 0.5]\r\n        x_max, y_max = pnts.max(axis=0) + [0.5, 0.5]\r\n        #\r\n        lbl = np.arange(len(pnts))\r\n        for label, xpt, ypt in zip(lbl, pnts[:, 0], pnts[:, 1]):\r\n            plt.annotate(label, xy=(xpt, ypt), xytext=(2, 2), size=8,\r\n                         textcoords=\'offset points\', ha=\'left\', va=\'bottom\')\r\n        plt.xlim(x_min, x_max)\r\n        if r_c:\r\n            plt.ylim(y_max, y_min)\r\n        else:\r\n            plt.ylim(y_min, y_max)\r\n    # ---- enable multiple point files ----\r\n    if isinstance(pnts, (list, tuple)):\r\n        i = 0\r\n        for p in pnts:  # plot x, y using marker i.\r\n            plt.scatter(p[:, 0], p[:, 1], marker=markers[i])\r\n            i += 1\r\n    else:\r\n        plt.scatter(pnts[:, 0], pnts[:, 1])  # , marker=markers[0])\r\n#    plt.show()  # block=False\r\n#    plt.close()  # turn close back on in some IDEs\r\n#    plt.ion()\r\n    plt.show()\r\n\r\n\r\n# ----------------------------------------------------------------------------\r\n# ---- running script or testing code section ----\r\ndef _demo():\r\n    """"""Plot 20 points which have a minimum 1 unit point spacing\r\n    :\r\n    """"""\r\n    a = np.array([[0.4, 0.5], [1.2, 9.1], [1.2, 3.6], [1.9, 4.6],\r\n                  [2.9, 5.9], [4.2, 5.5], [4.3, 3.0], [5.1, 8.2],\r\n                  [5.3, 9.5], [5.5, 5.7], [6.1, 4.0], [6.5, 6.8],\r\n                  [7.1, 7.6], [7.3, 2.0], [7.4, 1.0], [7.7, 9.6],\r\n                  [8.5, 6.5], [9.0, 4.7], [9.6, 1.6], [9.7, 9.6]])\r\n    plt, ax = plot_pnts_(a, title=\'Points no closer than... test\',\r\n                         r_c=False, lbls=[\'X-values\', \'Y-values\'],\r\n                         params=True)\r\n    return a, plt, ax\r\n\r\n# ---------------------------------------------------------------------\r\nif __name__ == ""__main__"":\r\n    """"""Main section...   """"""\r\n#    print(""Script... {}"".format(script))\r\n#    a, plt, ax = _demo()\r\n'"
all_scripts/arrayprint2.py,59,"b'""""""Array printing function\r\n\r\n$Id: arrayprint.py,v 1.9 2005/09/13 13:58:44 teoliphant Exp $\r\n\r\n""""""\r\nfrom __future__ import division, absolute_import, print_function\r\n\r\n__all__ = [""array2string"", ""array_str"", ""array_repr"", ""set_string_function"",\r\n           ""set_printoptions"", ""get_printoptions"", ""format_float_positional"",\r\n           ""format_float_scientific""]\r\n__docformat__ = \'restructuredtext\'\r\n\r\n#\r\n# Written by Konrad Hinsen <hinsenk@ere.umontreal.ca>\r\n# last revision: 1996-3-13\r\n# modified by Jim Hugunin 1997-3-3 for repr\'s and str\'s (and other details)\r\n# and by Perry Greenfield 2000-4-1 for numarray\r\n# and by Travis Oliphant  2005-8-22 for numpy\r\n\r\n\r\n# Note: Both scalartypes.c.src and arrayprint.py implement strs for numpy\r\n# scalars but for different purposes. scalartypes.c.src has str/reprs for when\r\n# the scalar is printed on its own, while arrayprint.py has strs for when\r\n# scalars are printed inside an ndarray. Only the latter strs are currently\r\n# user-customizable.\r\n\r\nimport sys\r\nimport functools\r\nimport warnings\r\nimport numpy as np\r\nfrom numpy.core import multiarray, bool_\r\n\r\nif sys.version_info[0] >= 3:\r\n    try:\r\n        from _thread import get_ident\r\n    except ImportError:\r\n        from _dummy_thread import get_ident\r\nelse:\r\n    try:\r\n        from thread import get_ident\r\n    except ImportError:\r\n        from dummy_thread import get_ident\r\n\r\n\r\n_format_options = {\r\n    \'edgeitems\': 3,  # repr N leading and trailing items of each dimension\r\n    \'threshold\': 1000,  # total items > triggers array summarization\r\n    \'floatmode\': \'maxprec\',\r\n    \'precision\': 8,  # precision of floating point representations\r\n    \'suppress\': False,  # suppress printing small floating values in exp format\r\n    \'linewidth\': 75,\r\n    \'nanstr\': \'nan\',\r\n    \'infstr\': \'inf\',\r\n    \'sign\': \'-\',\r\n    \'formatter\': None,\r\n    \'legacy\': False}\r\n\r\n\r\ndef _make_options_dict(precision=None, threshold=None, edgeitems=None,\r\n                       linewidth=None, suppress=None, nanstr=None, infstr=None,\r\n                       sign=None, formatter=None, floatmode=None, legacy=None):\r\n    """""" make a dictionary out of the non-None arguments, plus sanity checks """"""\r\n\r\n    options = {k: v for k, v in locals().items() if v is not None}\r\n\r\n    if suppress is not None:\r\n        options[\'suppress\'] = bool(suppress)\r\n\r\n    modes = [\'fixed\', \'unique\', \'maxprec\', \'maxprec_equal\']\r\n    if floatmode not in modes + [None]:\r\n        raise ValueError(""floatmode option must be one of "" +\r\n                         "", "".join(\'""{}""\'.format(m) for m in modes))\r\n\r\n    if sign not in [None, \'-\', \'+\', \' \']:\r\n        raise ValueError(""sign option must be one of \' \', \'+\', or \'-\'"")\r\n\r\n    if legacy not in [None, False, \'1.13\']:\r\n        warnings.warn(""legacy printing option can currently only be \'1.13\' or ""\r\n                      ""`False`"", stacklevel=3)\r\n\r\n    return options\r\n\r\ndef set_printoptions(precision=None, threshold=None, edgeitems=None,\r\n                     linewidth=None, suppress=None, nanstr=None, infstr=None,\r\n                     formatter=None, sign=None, floatmode=None, **kwarg):\r\n    """"""\r\n    Set printing options.\r\n\r\n    These options determine the way floating point numbers, arrays and\r\n    other NumPy objects are displayed.\r\n\r\n    Parameters\r\n    ----------\r\n    precision : int, optional\r\n        Number of digits of precision for floating point output (default 8).\r\n    threshold : int, optional\r\n        Total number of array elements which trigger summarization\r\n        rather than full repr (default 1000).\r\n    edgeitems : int, optional\r\n        Number of array items in summary at beginning and end of\r\n        each dimension (default 3).\r\n    linewidth : int, optional\r\n        The number of characters per line for the purpose of inserting\r\n        line breaks (default 75).\r\n    suppress : bool, optional\r\n        If True, always print floating point numbers using fixed point\r\n        notation, in which case numbers equal to zero in the current precision\r\n        will print as zero.  If False, then scientific notation is used when\r\n        absolute value of the smallest number is < 1e-4 or the ratio of the\r\n        maximum absolute value to the minimum is > 1e3. The default is False.\r\n    nanstr : str, optional\r\n        String representation of floating point not-a-number (default nan).\r\n    infstr : str, optional\r\n        String representation of floating point infinity (default inf).\r\n    sign : string, either \'-\', \'+\', or \' \', optional\r\n        Controls printing of the sign of floating-point types. If \'+\', always\r\n        print the sign of positive values. If \' \', always prints a space\r\n        (whitespace character) in the sign position of positive values.  If\r\n        \'-\', omit the sign character of positive values. (default \'-\')\r\n    formatter : dict of callables, optional\r\n        If not None, the keys should indicate the type(s) that the respective\r\n        formatting function applies to.  Callables should return a string.\r\n        Types that are not specified (by their corresponding keys) are handled\r\n        by the default formatters.  Individual types for which a formatter\r\n        can be set are::\r\n\r\n            - \'bool\'\r\n            - \'int\'\r\n            - \'timedelta\' : a `numpy.timedelta64`\r\n            - \'datetime\' : a `numpy.datetime64`\r\n            - \'float\'\r\n            - \'longfloat\' : 128-bit floats\r\n            - \'complexfloat\'\r\n            - \'longcomplexfloat\' : composed of two 128-bit floats\r\n            - \'numpystr\' : types `numpy.string_` and `numpy.unicode_`\r\n            - \'object\' : `np.object_` arrays\r\n            - \'str\' : all other strings\r\n\r\n        Other keys that can be used to set a group of types at once are::\r\n\r\n            - \'all\' : sets all types\r\n            - \'int_kind\' : sets \'int\'\r\n            - \'float_kind\' : sets \'float\' and \'longfloat\'\r\n            - \'complex_kind\' : sets \'complexfloat\' and \'longcomplexfloat\'\r\n            - \'str_kind\' : sets \'str\' and \'numpystr\'\r\n    floatmode : str, optional\r\n        Controls the interpretation of the `precision` option for\r\n        floating-point types. Can take the following values:\r\n            - \'fixed\' : Always print exactly `precision` fractional digits,\r\n                    even if this would print more or fewer digits than\r\n                    necessary to specify the value uniquely.\r\n            - \'unique : Print the minimum number of fractional digits necessary\r\n                    to represent each value uniquely. Different elements may\r\n                    have a different number of digits. The value of the\r\n                    `precision` option is ignored.\r\n            - \'maxprec\' : Print at most `precision` fractional digits, but if\r\n                    an element can be uniquely represented with fewer digits\r\n                    only print it with that many.\r\n            - \'maxprec_equal\' : Print at most `precision` fractional digits,\r\n                    but if every element in the array can be uniquely\r\n                    represented with an equal number of fewer digits, use that\r\n                    many digits for all elements.\r\n    legacy : string or `False`, optional\r\n        If set to the string `\'1.13\'` enables 1.13 legacy printing mode. This\r\n        approximates numpy 1.13 print output by including a space in the sign\r\n        position of floats and different behavior for 0d arrays. If set to\r\n        `False`, disables legacy mode. Unrecognized strings will be ignored\r\n        with a warning for forward compatibility.\r\n\r\n        .. versionadded:: 1.14.0\r\n\r\n    See Also\r\n    --------\r\n    get_printoptions, set_string_function, array2string\r\n\r\n    Notes\r\n    -----\r\n    `formatter` is always reset with a call to `set_printoptions`.\r\n\r\n    Examples\r\n    --------\r\n    Floating point precision can be set:\r\n\r\n    >>> np.set_printoptions(precision=4)\r\n    >>> print(np.array([1.123456789]))\r\n    [ 1.1235]\r\n\r\n    Long arrays can be summarised:\r\n\r\n    >>> np.set_printoptions(threshold=5)\r\n    >>> print(np.arange(10))\r\n    [0 1 2 ..., 7 8 9]\r\n\r\n    Small results can be suppressed:\r\n\r\n    >>> eps = np.finfo(float).eps\r\n    >>> x = np.arange(4.)\r\n    >>> x**2 - (x + eps)**2\r\n    array([ -4.9304e-32,  -4.4409e-16,   0.0000e+00,   0.0000e+00])\r\n    >>> np.set_printoptions(suppress=True)\r\n    >>> x**2 - (x + eps)**2\r\n    array([-0., -0.,  0.,  0.])\r\n\r\n    A custom formatter can be used to display array elements as desired:\r\n\r\n    >>> np.set_printoptions(formatter={\'all\':lambda x: \'int: \'+str(-x)})\r\n    >>> x = np.arange(3)\r\n    >>> x\r\n    array([int: 0, int: -1, int: -2])\r\n    >>> np.set_printoptions()  # formatter gets reset\r\n    >>> x\r\n    array([0, 1, 2])\r\n\r\n    To put back the default options, you can use:\r\n\r\n    >>> np.set_printoptions(edgeitems=3,infstr=\'inf\',\r\n    ... linewidth=75, nanstr=\'nan\', precision=8,\r\n    ... suppress=False, threshold=1000, formatter=None)\r\n    """"""\r\n    legacy = kwarg.pop(\'legacy\', None)\r\n    if kwarg:\r\n        msg = ""set_printoptions() got unexpected keyword argument \'{}\'""\r\n        raise TypeError(msg.format(kwarg.popitem()[0]))\r\n\r\n    opt = _make_options_dict(precision, threshold, edgeitems, linewidth,\r\n                             suppress, nanstr, infstr, sign, formatter,\r\n                             floatmode, legacy)\r\n    # formatter is always reset\r\n    opt[\'formatter\'] = formatter\r\n    _format_options.update(opt)\r\n\r\n    # set the C variable for legacy mode\r\n    if _format_options[\'legacy\'] == \'1.13\':\r\n        set_legacy_print_mode(113)\r\n    elif _format_options[\'legacy\'] is False:\r\n        set_legacy_print_mode(0)\r\n\r\n\r\ndef get_printoptions():\r\n    """"""\r\n    Return the current print options.\r\n\r\n    Returns\r\n    -------\r\n    print_opts : dict\r\n        Dictionary of current print options with keys\r\n\r\n          - precision : int\r\n          - threshold : int\r\n          - edgeitems : int\r\n          - linewidth : int\r\n          - suppress : bool\r\n          - nanstr : str\r\n          - infstr : str\r\n          - formatter : dict of callables\r\n          - sign : str\r\n\r\n        For a full description of these options, see `set_printoptions`.\r\n\r\n    See Also\r\n    --------\r\n    set_printoptions, set_string_function\r\n\r\n    """"""\r\n    return _format_options.copy()\r\n\r\n\r\ndef _leading_trailing(a, edgeitems, index=()):\r\n    """"""\r\n    Keep only the N-D corners (leading and trailing edges) of an array.\r\n\r\n    Should be passed a base-class ndarray, since it makes no guarantees about\r\n    preserving subclasses.\r\n    """"""\r\n    axis = len(index)\r\n    if axis == a.ndim:\r\n        return a[index]\r\n\r\n    if a.shape[axis] > 2*edgeitems:\r\n        return concatenate((\r\n            _leading_trailing(a, edgeitems, index + np.index_exp[ :edgeitems]),\r\n            _leading_trailing(a, edgeitems, index + np.index_exp[-edgeitems:])\r\n        ), axis=axis)\r\n    else:\r\n        return _leading_trailing(a, edgeitems, index + np.index_exp[:])\r\n\r\n\r\ndef _object_format(o):\r\n    """""" Object arrays containing lists should be printed unambiguously """"""\r\n    if type(o) is list:\r\n        fmt = \'list({!r})\'\r\n    else:\r\n        fmt = \'{!r}\'\r\n    return fmt.format(o)\r\n\r\ndef repr_format(x):\r\n    return repr(x)\r\n\r\ndef str_format(x):\r\n    return str(x)\r\n\r\ndef _get_formatdict(data, **opt):\r\n    prec, fmode = opt[\'precision\'], opt[\'floatmode\']\r\n    supp, sign = opt[\'suppress\'], opt[\'sign\']\r\n    legacy = opt[\'legacy\']\r\n\r\n    # wrapped in lambdas to avoid taking a code path with the wrong type of data\r\n    formatdict = {\r\n        \'bool\': lambda: BoolFormat(data),\r\n        \'int\': lambda: IntegerFormat(data),\r\n        \'float\': lambda:\r\n            FloatingFormat(data, prec, fmode, supp, sign, legacy=legacy),\r\n        \'longfloat\': lambda:\r\n            FloatingFormat(data, prec, fmode, supp, sign, legacy=legacy),\r\n        \'complexfloat\': lambda:\r\n            ComplexFloatingFormat(data, prec, fmode, supp, sign, legacy=legacy),\r\n        \'longcomplexfloat\': lambda:\r\n            ComplexFloatingFormat(data, prec, fmode, supp, sign, legacy=legacy),\r\n        \'datetime\': lambda: DatetimeFormat(data, legacy=legacy),\r\n        \'timedelta\': lambda: TimedeltaFormat(data),\r\n        \'object\': lambda: _object_format,\r\n        \'void\': lambda: str_format,\r\n        \'numpystr\': lambda: repr_format,\r\n        \'str\': lambda: str}\r\n\r\n    # we need to wrap values in `formatter` in a lambda, so that the interface\r\n    # is the same as the above values.\r\n    def indirect(x):\r\n        return lambda: x\r\n\r\n    formatter = opt[\'formatter\']\r\n    if formatter is not None:\r\n        fkeys = [k for k in formatter.keys() if formatter[k] is not None]\r\n        if \'all\' in fkeys:\r\n            for key in formatdict.keys():\r\n                formatdict[key] = indirect(formatter[\'all\'])\r\n        if \'int_kind\' in fkeys:\r\n            for key in [\'int\']:\r\n                formatdict[key] = indirect(formatter[\'int_kind\'])\r\n        if \'float_kind\' in fkeys:\r\n            for key in [\'float\', \'longfloat\']:\r\n                formatdict[key] = indirect(formatter[\'float_kind\'])\r\n        if \'complex_kind\' in fkeys:\r\n            for key in [\'complexfloat\', \'longcomplexfloat\']:\r\n                formatdict[key] = indirect(formatter[\'complex_kind\'])\r\n        if \'str_kind\' in fkeys:\r\n            for key in [\'numpystr\', \'str\']:\r\n                formatdict[key] = indirect(formatter[\'str_kind\'])\r\n        for key in formatdict.keys():\r\n            if key in fkeys:\r\n                formatdict[key] = indirect(formatter[key])\r\n\r\n    return formatdict\r\n\r\ndef _get_format_function(data, **options):\r\n    """"""\r\n    find the right formatting function for the dtype_\r\n    """"""\r\n    dtype_ = data.dtype\r\n    if dtype_.fields is not None:\r\n        return StructureFormat.from_data(data, **options)\r\n\r\n    dtypeobj = dtype_.type\r\n    formatdict = _get_formatdict(data, **options)\r\n    if issubclass(dtypeobj, _nt.bool_):\r\n        return formatdict[\'bool\']()\r\n    elif issubclass(dtypeobj, _nt.integer):\r\n        if issubclass(dtypeobj, _nt.timedelta64):\r\n            return formatdict[\'timedelta\']()\r\n        else:\r\n            return formatdict[\'int\']()\r\n    elif issubclass(dtypeobj, _nt.floating):\r\n        if issubclass(dtypeobj, _nt.longfloat):\r\n            return formatdict[\'longfloat\']()\r\n        else:\r\n            return formatdict[\'float\']()\r\n    elif issubclass(dtypeobj, _nt.complexfloating):\r\n        if issubclass(dtypeobj, _nt.clongfloat):\r\n            return formatdict[\'longcomplexfloat\']()\r\n        else:\r\n            return formatdict[\'complexfloat\']()\r\n    elif issubclass(dtypeobj, (_nt.unicode_, _nt.string_)):\r\n        return formatdict[\'numpystr\']()\r\n    elif issubclass(dtypeobj, _nt.datetime64):\r\n        return formatdict[\'datetime\']()\r\n    elif issubclass(dtypeobj, _nt.object_):\r\n        return formatdict[\'object\']()\r\n    elif issubclass(dtypeobj, _nt.void):\r\n        return formatdict[\'void\']()\r\n    else:\r\n        return formatdict[\'numpystr\']()\r\n\r\n\r\ndef _recursive_guard(fillvalue=\'...\'):\r\n    """"""\r\n    Like the python 3.2 reprlib.recursive_repr, but forwards *args and **kwargs\r\n\r\n    Decorates a function such that if it calls itself with the same first\r\n    argument, it returns `fillvalue` instead of recursing.\r\n\r\n    Largely copied from reprlib.recursive_repr\r\n    """"""\r\n\r\n    def decorating_function(f):\r\n        repr_running = set()\r\n\r\n        @functools.wraps(f)\r\n        def wrapper(self, *args, **kwargs):\r\n            key = id(self), get_ident()\r\n            if key in repr_running:\r\n                return fillvalue\r\n            repr_running.add(key)\r\n            try:\r\n                return f(self, *args, **kwargs)\r\n            finally:\r\n                repr_running.discard(key)\r\n\r\n        return wrapper\r\n\r\n    return decorating_function\r\n\r\n\r\n# gracefully handle recursive calls, when object arrays contain themselves\r\n@_recursive_guard()\r\ndef _array2string(a, options, separator=\' \', prefix=""""):\r\n    # The formatter __init__s cannot deal with subclasses yet\r\n    data = asarray(a)\r\n\r\n    if a.size > options[\'threshold\']:\r\n        summary_insert = ""...""\r\n        data = _leading_trailing(data, options[\'edgeitems\'])\r\n    else:\r\n        summary_insert = """"\r\n\r\n    # find the right formatting function for the array\r\n    format_function = _get_format_function(data, **options)\r\n\r\n    # skip over ""[""\r\n    next_line_prefix = "" ""\r\n    # skip over array(\r\n    next_line_prefix += "" ""*len(prefix)\r\n\r\n    lst = _formatArray(a, format_function, options[\'linewidth\'],\r\n                       next_line_prefix, separator, options[\'edgeitems\'],\r\n                       summary_insert, options[\'legacy\'])\r\n    return lst\r\n\r\n\r\ndef array2string(a, max_line_width=None, precision=None,\r\n                 suppress_small=None, separator=\' \', prefix="""",\r\n                 style=np._NoValue, formatter=None, threshold=None,\r\n                 edgeitems=None, sign=None, floatmode=None, suffix="""",\r\n                 **kwarg):\r\n    """"""\r\n    Return a string representation of an array.\r\n\r\n    Parameters\r\n    ----------\r\n    a : ndarray\r\n        Input array.\r\n    max_line_width : int, optional\r\n        The maximum number of columns the string should span. Newline\r\n        characters splits the string appropriately after array elements.\r\n    precision : int, optional\r\n        Floating point precision. Default is the current printing\r\n        precision (usually 8), which can be altered using `set_printoptions`.\r\n    suppress_small : bool, optional\r\n        Represent very small numbers as zero. A number is ""very small"" if it\r\n        is smaller than the current printing precision.\r\n    separator : str, optional\r\n        Inserted between elements.\r\n    prefix : str, optional\r\n    suffix: str, optional\r\n        The length of the prefix and suffix strings are used to respectively\r\n        align and wrap the output. An array is typically printed as::\r\n\r\n          prefix + array2string(a) + suffix\r\n\r\n        The output is left-padded by the length of the prefix string, and\r\n        wrapping is forced at the column ``max_line_width - len(suffix)``.\r\n    style : _NoValue, optional\r\n        Has no effect, do not use.\r\n\r\n        .. deprecated:: 1.14.0\r\n    formatter : dict of callables, optional\r\n        If not None, the keys should indicate the type(s) that the respective\r\n        formatting function applies to.  Callables should return a string.\r\n        Types that are not specified (by their corresponding keys) are handled\r\n        by the default formatters.  Individual types for which a formatter\r\n        can be set are::\r\n\r\n            - \'bool\'\r\n            - \'int\'\r\n            - \'timedelta\' : a `numpy.timedelta64`\r\n            - \'datetime\' : a `numpy.datetime64`\r\n            - \'float\'\r\n            - \'longfloat\' : 128-bit floats\r\n            - \'complexfloat\'\r\n            - \'longcomplexfloat\' : composed of two 128-bit floats\r\n            - \'void\' : type `numpy.void`\r\n            - \'numpystr\' : types `numpy.string_` and `numpy.unicode_`\r\n            - \'str\' : all other strings\r\n\r\n        Other keys that can be used to set a group of types at once are::\r\n\r\n            - \'all\' : sets all types\r\n            - \'int_kind\' : sets \'int\'\r\n            - \'float_kind\' : sets \'float\' and \'longfloat\'\r\n            - \'complex_kind\' : sets \'complexfloat\' and \'longcomplexfloat\'\r\n            - \'str_kind\' : sets \'str\' and \'numpystr\'\r\n    threshold : int, optional\r\n        Total number of array elements which trigger summarization\r\n        rather than full repr.\r\n    edgeitems : int, optional\r\n        Number of array items in summary at beginning and end of\r\n        each dimension.\r\n    sign : string, either \'-\', \'+\', or \' \', optional\r\n        Controls printing of the sign of floating-point types. If \'+\', always\r\n        print the sign of positive values. If \' \', always prints a space\r\n        (whitespace character) in the sign position of positive values.  If\r\n        \'-\', omit the sign character of positive values.\r\n    floatmode : str, optional\r\n        Controls the interpretation of the `precision` option for\r\n        floating-point types. Can take the following values:\r\n            - \'fixed\' : Always print exactly `precision` fractional digits,\r\n                    even if this would print more or fewer digits than\r\n                    necessary to specify the value uniquely.\r\n            - \'unique : Print the minimum number of fractional digits necessary\r\n                    to represent each value uniquely. Different elements may\r\n                    have a different number of digits.  The value of the\r\n                    `precision` option is ignored.\r\n            - \'maxprec\' : Print at most `precision` fractional digits, but if\r\n                    an element can be uniquely represented with fewer digits\r\n                    only print it with that many.\r\n            - \'maxprec_equal\' : Print at most `precision` fractional digits,\r\n                    but if every element in the array can be uniquely\r\n                    represented with an equal number of fewer digits, use that\r\n                    many digits for all elements.\r\n    legacy : string or `False`, optional\r\n        If set to the string `\'1.13\'` enables 1.13 legacy printing mode. This\r\n        approximates numpy 1.13 print output by including a space in the sign\r\n        position of floats and different behavior for 0d arrays. If set to\r\n        `False`, disables legacy mode. Unrecognized strings will be ignored\r\n        with a warning for forward compatibility.\r\n\r\n        .. versionadded:: 1.14.0\r\n\r\n    Returns\r\n    -------\r\n    array_str : str\r\n        String representation of the array.\r\n\r\n    Raises\r\n    ------\r\n    TypeError\r\n        if a callable in `formatter` does not return a string.\r\n\r\n    See Also\r\n    --------\r\n    array_str, array_repr, set_printoptions, get_printoptions\r\n\r\n    Notes\r\n    -----\r\n    If a formatter is specified for a certain type, the `precision` keyword is\r\n    ignored for that type.\r\n\r\n    This is a very flexible function; `array_repr` and `array_str` are using\r\n    `array2string` internally so keywords with the same name should work\r\n    identically in all three functions.\r\n\r\n    Examples\r\n    --------\r\n    >>> x = np.array([1e-16,1,2,3])\r\n    >>> print(np.array2string(x, precision=2, separator=\',\',\r\n    ...                       suppress_small=True))\r\n    [ 0., 1., 2., 3.]\r\n\r\n    >>> x  = np.arange(3.)\r\n    >>> np.array2string(x, formatter={\'float_kind\':lambda x: ""%.2f"" % x})\r\n    \'[0.00 1.00 2.00]\'\r\n\r\n    >>> x  = np.arange(3)\r\n    >>> np.array2string(x, formatter={\'int\':lambda x: hex(x)})\r\n    \'[0x0L 0x1L 0x2L]\'\r\n\r\n    """"""\r\n    legacy = kwarg.pop(\'legacy\', None)\r\n    if kwarg:\r\n        msg = ""array2string() got unexpected keyword argument \'{}\'""\r\n        raise TypeError(msg.format(kwarg.popitem()[0]))\r\n\r\n    overrides = _make_options_dict(precision, threshold, edgeitems,\r\n                                   max_line_width, suppress_small, None, None,\r\n                                   sign, formatter, floatmode, legacy)\r\n    options = _format_options.copy()\r\n    options.update(overrides)\r\n\r\n    if options[\'legacy\'] == \'1.13\':\r\n        if a.shape == () and not a.dtype.names:\r\n            return style(a.item())\r\n    elif style is not np._NoValue:\r\n        # Deprecation 11-9-2017  v1.14\r\n        warnings.warn(""\'style\' argument is deprecated and no longer functional""\r\n                      "" except in 1.13 \'legacy\' mode"",\r\n                      DeprecationWarning, stacklevel=3)\r\n\r\n    if options[\'legacy\'] != \'1.13\':\r\n        options[\'linewidth\'] -= len(suffix)\r\n\r\n    # treat as a null array if any of shape elements == 0\r\n    if a.size == 0:\r\n        return ""[]""\r\n\r\n    return _array2string(a, options, separator, prefix)\r\n\r\n\r\ndef _extendLine(s, line, word, line_width, next_line_prefix, legacy):\r\n    needs_wrap = len(line) + len(word) > line_width\r\n    if legacy != \'1.13\':\r\n        s# don\'t wrap lines if it won\'t help\r\n        if len(line) <= len(next_line_prefix):\r\n            needs_wrap = False\r\n\r\n    if needs_wrap:\r\n        s += line.rstrip() + ""\\n""\r\n        line = next_line_prefix\r\n    line += word\r\n    return s, line\r\n\r\n\r\ndef _formatArray(a, format_function, line_width, next_line_prefix,\r\n                 separator, edge_items, summary_insert, legacy):\r\n    """"""formatArray is designed for two modes of operation:\r\n\r\n    1. Full output\r\n\r\n    2. Summarized output\r\n\r\n    """"""\r\n    def recurser(index, hanging_indent, curr_width):\r\n        """"""\r\n        By using this local function, we don\'t need to recurse with all the\r\n        arguments. Since this function is not created recursively, the cost is\r\n        not significant\r\n        """"""\r\n        axis = len(index)\r\n        axes_left = a.ndim - axis\r\n\r\n        if axes_left == 0:\r\n            return format_function(a[index])\r\n\r\n        # when recursing, add a space to align with the [ added, and reduce the\r\n        # length of the line by 1\r\n        next_hanging_indent = hanging_indent + \' \'\r\n        if legacy == \'1.13\':\r\n            next_width = curr_width\r\n        else:\r\n            next_width = curr_width - len(\']\')\r\n\r\n        a_len = a.shape[axis]\r\n        show_summary = summary_insert and 2*edge_items < a_len\r\n        if show_summary:\r\n            leading_items = edge_items\r\n            trailing_items = edge_items\r\n        else:\r\n            leading_items = 0\r\n            trailing_items = a_len\r\n\r\n        # stringify the array with the hanging indent on the first line too\r\n        s = \'\'\r\n\r\n        # last axis (rows) - wrap elements if they would not fit on one line\r\n        if axes_left == 1:\r\n            # the length up until the beginning of the separator / bracket\r\n            if legacy == \'1.13\':\r\n                elem_width = curr_width - len(separator.rstrip())\r\n            else:\r\n                elem_width = curr_width - max(len(separator.rstrip()), len(\']\'))\r\n\r\n            line = hanging_indent\r\n            for i in range(leading_items):\r\n                word = recurser(index + (i,), next_hanging_indent, next_width)\r\n                s, line = _extendLine(\r\n                    s, line, word, elem_width, hanging_indent, legacy)\r\n                line += separator\r\n\r\n            if show_summary:\r\n                s, line = _extendLine(\r\n                    s, line, summary_insert, elem_width, hanging_indent, legacy)\r\n                if legacy == \'1.13\':\r\n                    line += "", ""\r\n                else:\r\n                    line += separator\r\n\r\n            for i in range(trailing_items, 1, -1):\r\n                word = recurser(index + (-i,), next_hanging_indent, next_width)\r\n                s, line = _extendLine(\r\n                    s, line, word, elem_width, hanging_indent, legacy)\r\n                line += separator\r\n\r\n            if legacy == \'1.13\':\r\n                # width of the seperator is not considered on 1.13\r\n                elem_width = curr_width\r\n            word = recurser(index + (-1,), next_hanging_indent, next_width)\r\n            s, line = _extendLine(\r\n                s, line, word, elem_width, hanging_indent, legacy)\r\n\r\n            s += line\r\n\r\n        # other axes - insert newlines between rows\r\n        else:\r\n            s = \'\'\r\n            line_sep = separator.rstrip() + \'\\n\'*(axes_left - 1)\r\n\r\n            for i in range(leading_items):\r\n                nested = recurser(index + (i,), next_hanging_indent, next_width)\r\n                s += hanging_indent + nested + line_sep\r\n\r\n            if show_summary:\r\n                if legacy == \'1.13\':\r\n                    # trailing space, fixed number of newlines, and fixed separator\r\n                    s += hanging_indent + summary_insert + "", \\n""\r\n                else:\r\n                    s += hanging_indent + summary_insert + line_sep\r\n\r\n            for i in range(trailing_items, 1, -1):\r\n                nested = recurser(index + (-i,), next_hanging_indent, next_width)\r\n                s += hanging_indent + nested + line_sep\r\n\r\n            nested = recurser(index + (-1,), next_hanging_indent, next_width)\r\n            s += hanging_indent + nested\r\n\r\n        # remove the hanging indent, and wrap in []\r\n        s = \'[\' + s[len(hanging_indent):] + \']\'\r\n        return s\r\n\r\n    # invoke the recursive part with an initial index and prefix\r\n    return recurser(\r\n        index=(),\r\n        hanging_indent=next_line_prefix,\r\n        curr_width=line_width)\r\n\r\n\r\nclass FloatingFormat(object):\r\n    """""" Formatter for subtypes of np.floating """"""\r\n    def __init__(self, data, precision, floatmode, suppress_small, sign=False,\r\n                 **kwarg):\r\n        # for backcompatibility, accept bools\r\n        if isinstance(sign, bool):\r\n            sign = \'+\' if sign else \'-\'\r\n\r\n        self._legacy = kwarg.get(\'legacy\', False)\r\n        if self._legacy == \'1.13\':\r\n            sign = \'-\' if data.shape == () else \' \'\r\n\r\n        self.floatmode = floatmode\r\n        if floatmode == \'unique\':\r\n            self.precision = -1\r\n        else:\r\n            if precision < 0:\r\n                raise ValueError(\r\n                    ""precision must be >= 0 in {} mode"".format(floatmode))\r\n            self.precision = precision\r\n\r\n        self.suppress_small = suppress_small\r\n        self.sign = sign\r\n        self.exp_format = False\r\n        self.large_exponent = False\r\n\r\n        self.fillFormat(data)\r\n\r\n    def fillFormat(self, data):\r\n        # only the finite values are used to compute the number of digits\r\n        finite_vals = data[isfinite(data)]\r\n\r\n        # choose exponential mode based on the non-zero finite values:\r\n        abs_non_zero = absolute(finite_vals[finite_vals != 0])\r\n        if len(abs_non_zero) != 0:\r\n            max_val = np.max(abs_non_zero)\r\n            min_val = np.min(abs_non_zero)\r\n            with errstate(over=\'ignore\'):  # division can overflow\r\n                if max_val >= 1.e8 or (not self.suppress_small and\r\n                        (min_val < 0.0001 or max_val/min_val > 1000.)):\r\n                    self.exp_format = True\r\n\r\n        # do a first pass of printing all the numbers, to determine sizes\r\n        if len(finite_vals) == 0:\r\n            self.pad_left = 0\r\n            self.pad_right = 0\r\n            self.trim = \'.\'\r\n            self.exp_size = -1\r\n            self.unique = True\r\n        elif self.exp_format:\r\n            trim, unique = \'.\', True\r\n            if self.floatmode == \'fixed\' or self._legacy == \'1.13\':\r\n                trim, unique = \'k\', False\r\n            strs = (dragon4_scientific(x, precision=self.precision,\r\n                               unique=unique, trim=trim, sign=self.sign == \'+\')\r\n                    for x in finite_vals)\r\n            frac_strs, _, exp_strs = zip(*(s.partition(\'e\') for s in strs))\r\n            int_part, frac_part = zip(*(s.split(\'.\') for s in frac_strs))\r\n            self.exp_size = max(len(s) for s in exp_strs) - 1\r\n\r\n            self.trim = \'k\'\r\n            self.precision = max(len(s) for s in frac_part)\r\n\r\n            # for back-compatibility with np 1.13, use two spaces and full prec\r\n            if self._legacy == \'1.13\':\r\n                # undo addition of sign pos below\r\n                will_add_sign = all(finite_vals > 0) and self.sign == \' \'\r\n                self.pad_left = 3 - will_add_sign\r\n            else:\r\n                # this should be only 1 or 2. Can be calculated from sign.\r\n                self.pad_left = max(len(s) for s in int_part)\r\n            # pad_right is only needed for nan length calculation\r\n            self.pad_right = self.exp_size + 2 + self.precision\r\n\r\n            self.unique = False\r\n        else:\r\n            # first pass printing to determine sizes\r\n            trim, unique = \'.\', True\r\n            if self.floatmode == \'fixed\':\r\n                trim, unique = \'k\', False\r\n            strs = (dragon4_positional(x, precision=self.precision,\r\n                                       fractional=True,\r\n                                       unique=unique, trim=trim,\r\n                                       sign=self.sign == \'+\')\r\n                    for x in finite_vals)\r\n            int_part, frac_part = zip(*(s.split(\'.\') for s in strs))\r\n            self.pad_left = max(len(s) for s in int_part)\r\n            self.pad_right = max(len(s) for s in frac_part)\r\n            self.exp_size = -1\r\n\r\n            if self.floatmode in [\'fixed\', \'maxprec_equal\']:\r\n                self.precision = self.pad_right\r\n                self.unique = False\r\n                self.trim = \'k\'\r\n            else:\r\n                self.unique = True\r\n                self.trim = \'.\'\r\n\r\n        # account for sign = \' \' by adding one to pad_left\r\n        if all(finite_vals >= 0) and self.sign == \' \':\r\n            self.pad_left += 1\r\n\r\n        # if there are non-finite values, may need to increase pad_left\r\n        if data.size != finite_vals.size:\r\n            neginf = self.sign != \'-\' or any(data[isinf(data)] < 0)\r\n            nanlen = len(_format_options[\'nanstr\'])\r\n            inflen = len(_format_options[\'infstr\']) + neginf\r\n            offset = self.pad_right + 1  # +1 for decimal pt\r\n            self.pad_left = max(self.pad_left, nanlen - offset, inflen - offset)\r\n\r\n    def __call__(self, x):\r\n        if not np.isfinite(x):\r\n            with errstate(invalid=\'ignore\'):\r\n                if np.isnan(x):\r\n                    sign = \'+\' if self.sign == \'+\' else \'\'\r\n                    ret = sign + _format_options[\'nanstr\']\r\n                else:  # isinf\r\n                    sign = \'-\' if x < 0 else \'+\' if self.sign == \'+\' else \'\'\r\n                    ret = sign + _format_options[\'infstr\']\r\n                return \' \'*(self.pad_left + self.pad_right + 1 - len(ret)) + ret\r\n\r\n        if self.exp_format:\r\n            return dragon4_scientific(x,\r\n                                      precision=self.precision,\r\n                                      unique=self.unique,\r\n                                      trim=self.trim,\r\n                                      sign=self.sign == \'+\',\r\n                                      pad_left=self.pad_left,\r\n                                      exp_digits=self.exp_size)\r\n        else:\r\n            return dragon4_positional(x,\r\n                                      precision=self.precision,\r\n                                      unique=self.unique,\r\n                                      fractional=True,\r\n                                      trim=self.trim,\r\n                                      sign=self.sign == \'+\',\r\n                                      pad_left=self.pad_left,\r\n                                      pad_right=self.pad_right)\r\n\r\n# for back-compatibility, we keep the classes for each float type too\r\nclass FloatFormat(FloatingFormat):\r\n    def __init__(self, *args, **kwargs):\r\n        warnings.warn(""FloatFormat has been replaced by FloatingFormat"",\r\n                      DeprecationWarning, stacklevel=2)\r\n        super(FloatFormat, self).__init__(*args, **kwargs)\r\n\r\n\r\nclass LongFloatFormat(FloatingFormat):\r\n    def __init__(self, *args, **kwargs):\r\n        warnings.warn(""LongFloatFormat has been replaced by FloatingFormat"",\r\n                      DeprecationWarning, stacklevel=2)\r\n        super(LongFloatFormat, self).__init__(*args, **kwargs)\r\n\r\n\r\ndef format_float_scientific(x, precision=None, unique=True, trim=\'k\',\r\n                            sign=False, pad_left=None, exp_digits=None):\r\n    """"""\r\n    Format a floating-point scalar as a decimal string in scientific notation.\r\n\r\n    Provides control over rounding, trimming and padding. Uses and assumes\r\n    IEEE unbiased rounding. Uses the ""Dragon4"" algorithm.\r\n\r\n    Parameters\r\n    ----------\r\n    x : python float or numpy floating scalar\r\n        Value to format.\r\n    precision : non-negative integer, optional\r\n        Maximum number of fractional digits to print. May be omitted if\r\n        `unique` is `True`, but is required if unique is `False`.\r\n    unique : boolean, optional\r\n        If `True`, use a digit-generation strategy which gives the shortest\r\n        representation which uniquely identifies the floating-point number from\r\n        other values of the same type, by judicious rounding. If `precision`\r\n        was omitted, print all necessary digits, otherwise digit generation is\r\n        cut off after `precision` digits and the remaining value is rounded.\r\n        If `False`, digits are generated as if printing an infinite-precision\r\n        value and stopping after `precision` digits, rounding the remaining\r\n        value.\r\n    trim : one of \'k\', \'.\', \'0\', \'-\', optional\r\n        Controls post-processing trimming of trailing digits, as follows:\r\n            k : keep trailing zeros, keep decimal point (no trimming)\r\n            . : trim all trailing zeros, leave decimal point\r\n            0 : trim all but the zero before the decimal point. Insert the\r\n                zero if it is missing.\r\n            - : trim trailing zeros and any trailing decimal point\r\n    sign : boolean, optional\r\n        Whether to show the sign for positive values.\r\n    pad_left : non-negative integer, optional\r\n        Pad the left side of the string with whitespace until at least that\r\n        many characters are to the left of the decimal point.\r\n    exp_digits : non-negative integer, optional\r\n        Pad the exponent with zeros until it contains at least this many digits.\r\n        If omitted, the exponent will be at least 2 digits.\r\n\r\n    Returns\r\n    -------\r\n    rep : string\r\n        The string representation of the floating point value\r\n\r\n    See Also\r\n    --------\r\n    format_float_positional\r\n\r\n    Examples\r\n    --------\r\n    >>> np.format_float_scientific(np.float32(np.pi))\r\n    \'3.1415927e+00\'\r\n    >>> s = np.float32(1.23e24)\r\n    >>> np.format_float_scientific(s, unique=False, precision=15)\r\n    \'1.230000071797338e+24\'\r\n    >>> np.format_float_scientific(s, exp_digits=4)\r\n    \'1.23e+0024\'\r\n    """"""\r\n    precision = -1 if precision is None else precision\r\n    pad_left = -1 if pad_left is None else pad_left\r\n    exp_digits = -1 if exp_digits is None else exp_digits\r\n    return dragon4_scientific(x, precision=precision, unique=unique,\r\n                              trim=trim, sign=sign, pad_left=pad_left,\r\n                              exp_digits=exp_digits)\r\n\r\ndef format_float_positional(x, precision=None, unique=True,\r\n                            fractional=True, trim=\'k\', sign=False,\r\n                            pad_left=None, pad_right=None):\r\n    """"""\r\n    Format a floating-point scalar as a decimal string in positional notation.\r\n\r\n    Provides control over rounding, trimming and padding. Uses and assumes\r\n    IEEE unbiased rounding. Uses the ""Dragon4"" algorithm.\r\n\r\n    Parameters\r\n    ----------\r\n    x : python float or numpy floating scalar\r\n        Value to format.\r\n    precision : non-negative integer, optional\r\n        Maximum number of digits to print. May be omitted if `unique` is\r\n        `True`, but is required if unique is `False`.\r\n    unique : boolean, optional\r\n        If `True`, use a digit-generation strategy which gives the shortest\r\n        representation which uniquely identifies the floating-point number from\r\n        other values of the same type, by judicious rounding. If `precision`\r\n        was omitted, print out all necessary digits, otherwise digit generation\r\n        is cut off after `precision` digits and the remaining value is rounded.\r\n        If `False`, digits are generated as if printing an infinite-precision\r\n        value and stopping after `precision` digits, rounding the remaining\r\n        value.\r\n    fractional : boolean, optional\r\n        If `True`, the cutoff of `precision` digits refers to the total number\r\n        of digits after the decimal point, including leading zeros.\r\n        If `False`, `precision` refers to the total number of significant\r\n        digits, before or after the decimal point, ignoring leading zeros.\r\n    trim : one of \'k\', \'.\', \'0\', \'-\', optional\r\n        Controls post-processing trimming of trailing digits, as follows:\r\n            k : keep trailing zeros, keep decimal point (no trimming)\r\n            . : trim all trailing zeros, leave decimal point\r\n            0 : trim all but the zero before the decimal point. Insert the\r\n                zero if it is missing.\r\n            - : trim trailing zeros and any trailing decimal point\r\n    sign : boolean, optional\r\n        Whether to show the sign for positive values.\r\n    pad_left : non-negative integer, optional\r\n        Pad the left side of the string with whitespace until at least that\r\n        many characters are to the left of the decimal point.\r\n    pad_right : non-negative integer, optional\r\n        Pad the right side of the string with whitespace until at least that\r\n        many characters are to the right of the decimal point.\r\n\r\n    Returns\r\n    -------\r\n    rep : string\r\n        The string representation of the floating point value\r\n\r\n    See Also\r\n    --------\r\n    format_float_scientific\r\n\r\n    Examples\r\n    --------\r\n    >>> np.format_float_scientific(np.float32(np.pi))\r\n    \'3.1415927\'\r\n    >>> np.format_float_positional(np.float16(np.pi))\r\n    \'3.14\'\r\n    >>> np.format_float_positional(np.float16(0.3))\r\n    \'0.3\'\r\n    >>> np.format_float_positional(np.float16(0.3), unique=False, precision=10)\r\n    \'0.3000488281\'\r\n    """"""\r\n    precision = -1 if precision is None else precision\r\n    pad_left = -1 if pad_left is None else pad_left\r\n    pad_right = -1 if pad_right is None else pad_right\r\n    return dragon4_positional(x, precision=precision, unique=unique,\r\n                              fractional=fractional, trim=trim,\r\n                              sign=sign, pad_left=pad_left,\r\n                              pad_right=pad_right)\r\n\r\n\r\nclass IntegerFormat(object):\r\n    def __init__(self, data):\r\n        if data.size > 0:\r\n            max_str_len = max(len(str(np.max(data))),\r\n                              len(str(np.min(data))))\r\n        else:\r\n            max_str_len = 0\r\n        self.format = \'%{}d\'.format(max_str_len)\r\n\r\n    def __call__(self, x):\r\n        return self.format % x\r\n\r\n\r\nclass BoolFormat(object):\r\n    def __init__(self, data, **kwargs):\r\n        # add an extra space so "" True"" and ""False"" have the same length and\r\n        # array elements align nicely when printed, except in 0d arrays\r\n        self.truestr = \' True\' if data.shape != () else \'True\'\r\n\r\n    def __call__(self, x):\r\n        return self.truestr if x else ""False""\r\n\r\n\r\nclass ComplexFloatingFormat(object):\r\n    """""" Formatter for subtypes of np.complexfloating """"""\r\n    def __init__(self, x, precision, floatmode, suppress_small,\r\n                 sign=False, **kwarg):\r\n        # for backcompatibility, accept bools\r\n        if isinstance(sign, bool):\r\n            sign = \'+\' if sign else \'-\'\r\n\r\n        self.real_format = FloatingFormat(x.real, precision, floatmode,\r\n                                          suppress_small, sign=sign, **kwarg)\r\n        self.imag_format = FloatingFormat(x.imag, precision, floatmode,\r\n                                          suppress_small, sign=\'+\', **kwarg)\r\n\r\n    def __call__(self, x):\r\n        r = self.real_format(x.real)\r\n        i = self.imag_format(x.imag)\r\n        return r + i + \'j\'\r\n\r\n# for back-compatibility, we keep the classes for each complex type too\r\nclass ComplexFormat(ComplexFloatingFormat):\r\n    def __init__(self, *args, **kwargs):\r\n        warnings.warn(\r\n            ""ComplexFormat has been replaced by ComplexFloatingFormat"",\r\n            DeprecationWarning, stacklevel=2)\r\n        super(ComplexFormat, self).__init__(*args, **kwargs)\r\n\r\nclass LongComplexFormat(ComplexFloatingFormat):\r\n    def __init__(self, *args, **kwargs):\r\n        warnings.warn(\r\n            ""LongComplexFormat has been replaced by ComplexFloatingFormat"",\r\n            DeprecationWarning, stacklevel=2)\r\n        super(LongComplexFormat, self).__init__(*args, **kwargs)\r\n\r\n\r\nclass _TimelikeFormat(object):\r\n    def __init__(self, data):\r\n        non_nat = data[~isnat(data)]\r\n        if len(non_nat) > 0:\r\n            # Max str length of non-NaT elements\r\n            max_str_len = max(len(self._format_non_nat(np.max(non_nat))),\r\n                              len(self._format_non_nat(np.min(non_nat))))\r\n        else:\r\n            max_str_len = 0\r\n        if len(non_nat) < data.size:\r\n            # data contains a NaT\r\n            max_str_len = max(max_str_len, 5)\r\n        self._format = \'%{}s\'.format(max_str_len)\r\n        self._nat = ""\'NaT\'"".rjust(max_str_len)\r\n\r\n    def _format_non_nat(self, x):\r\n        # override in subclass\r\n        raise NotImplementedError\r\n\r\n    def __call__(self, x):\r\n        if isnat(x):\r\n            return self._nat\r\n        else:\r\n            return self._format % self._format_non_nat(x)\r\n\r\n\r\nclass DatetimeFormat(_TimelikeFormat):\r\n    def __init__(self, x, unit=None, timezone=None, casting=\'same_kind\',\r\n                 legacy=False):\r\n        # Get the unit from the dtype\r\n        if unit is None:\r\n            if x.dtype.kind == \'M\':\r\n                unit = datetime_data(x.dtype)[0]\r\n            else:\r\n                unit = \'s\'\r\n\r\n        if timezone is None:\r\n            timezone = \'naive\'\r\n        self.timezone = timezone\r\n        self.unit = unit\r\n        self.casting = casting\r\n        self.legacy = legacy\r\n\r\n        # must be called after the above are configured\r\n        super(DatetimeFormat, self).__init__(x)\r\n\r\n    def __call__(self, x):\r\n        if self.legacy == \'1.13\':\r\n            return self._format_non_nat(x)\r\n        return super(DatetimeFormat, self).__call__(x)\r\n\r\n    def _format_non_nat(self, x):\r\n        return ""\'%s\'"" % datetime_as_string(x,\r\n                                    unit=self.unit,\r\n                                    timezone=self.timezone,\r\n                                    casting=self.casting)\r\n\r\n\r\nclass TimedeltaFormat(_TimelikeFormat):\r\n    def _format_non_nat(self, x):\r\n        return str(x.astype(\'i8\'))\r\n\r\n\r\nclass SubArrayFormat(object):\r\n    def __init__(self, format_function):\r\n        self.format_function = format_function\r\n\r\n    def __call__(self, arr):\r\n        if arr.ndim <= 1:\r\n            return ""["" + "", "".join(self.format_function(a) for a in arr) + ""]""\r\n        return ""["" + "", "".join(self.__call__(a) for a in arr) + ""]""\r\n\r\n\r\nclass StructureFormat(object):\r\n    def __init__(self, format_functions):\r\n        self.format_functions = format_functions\r\n        self.num_fields = len(format_functions)\r\n\r\n    @classmethod\r\n    def from_data(cls, data, **options):\r\n        """"""\r\n        This is a second way to initialize StructureFormat, using the raw data\r\n        as input. Added to avoid changing the signature of __init__.\r\n        """"""\r\n        format_functions = []\r\n        for field_name in data.dtype.names:\r\n            format_function = _get_format_function(data[field_name], **options)\r\n            if data.dtype[field_name].shape != ():\r\n                format_function = SubArrayFormat(format_function)\r\n            format_functions.append(format_function)\r\n        return cls(format_functions)\r\n\r\n    def __call__(self, x):\r\n        s = ""(""\r\n        for field, format_function in zip(x, self.format_functions):\r\n            s += format_function(field) + "", ""\r\n        return (s[:-2] if 1 < self.num_fields else s[:-1]) + "")""\r\n\r\n\r\ndef _void_scalar_repr(x):\r\n    """"""\r\n    Implements the repr for structured-void scalars. It is called from the\r\n    scalartypes.c.src code, and is placed here because it uses the elementwise\r\n    formatters defined above.\r\n    """"""\r\n    return StructureFormat.from_data(array(x), **_format_options)(x)\r\n\r\n\r\n_typelessdata = [np.int_, np.float_, np.complex_, bool_]\r\nif issubclass(intc, int):\r\n    _typelessdata.append(intc)\r\nif issubclass(longlong, int):\r\n    _typelessdata.append(longlong)\r\n\r\n\r\ndef dtype_is_implied(dtype):\r\n    """"""\r\n    Determine if the given dtype is implied by the representation of its values.\r\n\r\n    Parameters\r\n    ----------\r\n    dtype : dtype\r\n        Data type\r\n\r\n    Returns\r\n    -------\r\n    implied : bool\r\n        True if the dtype is implied by the representation of its values.\r\n\r\n    Examples\r\n    --------\r\n    >>> np.core.arrayprint.dtype_is_implied(int)\r\n    True\r\n    >>> np.array([1, 2, 3], int)\r\n    array([1, 2, 3])\r\n    >>> np.core.arrayprint.dtype_is_implied(np.int8)\r\n    False\r\n    >>> np.array([1, 2, 3], np.int8)\r\n    array([1, 2, 3], dtype=np.int8)\r\n    """"""\r\n    dtype = np.dtype(dtype)\r\n    if _format_options[\'legacy\'] == \'1.13\' and dtype.type == bool_:\r\n        return False\r\n    return dtype.type in _typelessdata\r\n\r\n\r\ndef dtype_short_repr(dtype):\r\n    """"""\r\n    Convert a dtype to a short form which evaluates to the same dtype.\r\n\r\n    The intent is roughly that the following holds\r\n\r\n    >>> from numpy import *\r\n    >>> assert eval(dtype_short_repr(dt)) == dt\r\n    """"""\r\n    # handle these separately so they don\'t give garbage like str256\r\n    if issubclass(dtype.type, flexible):\r\n        if dtype.names:\r\n            return ""%s"" % str(dtype)\r\n        else:\r\n            return ""\'%s\'"" % str(dtype)\r\n\r\n    typename = dtype.name\r\n    # quote typenames which can\'t be represented as python variable names\r\n    if typename and not (typename[0].isalpha() and typename.isalnum()):\r\n        typename = repr(typename)\r\n\r\n    return typename\r\n\r\n\r\ndef array_repr(arr, max_line_width=None, precision=None, suppress_small=None):\r\n    """"""\r\n    Return the string representation of an array.\r\n\r\n    Parameters\r\n    ----------\r\n    arr : ndarray\r\n        Input array.\r\n    max_line_width : int, optional\r\n        The maximum number of columns the string should span. Newline\r\n        characters split the string appropriately after array elements.\r\n    precision : int, optional\r\n        Floating point precision. Default is the current printing precision\r\n        (usually 8), which can be altered using `set_printoptions`.\r\n    suppress_small : bool, optional\r\n        Represent very small numbers as zero, default is False. Very small\r\n        is defined by `precision`, if the precision is 8 then\r\n        numbers smaller than 5e-9 are represented as zero.\r\n\r\n    Returns\r\n    -------\r\n    string : str\r\n      The string representation of an array.\r\n\r\n    See Also\r\n    --------\r\n    array_str, array2string, set_printoptions\r\n\r\n    Examples\r\n    --------\r\n    >>> np.array_repr(np.array([1,2]))\r\n    \'array([1, 2])\'\r\n    >>> np.array_repr(np.ma.array([0.]))\r\n    \'MaskedArray([ 0.])\'\r\n    >>> np.array_repr(np.array([], np.int32))\r\n    \'array([], dtype=int32)\'\r\n\r\n    >>> x = np.array([1e-6, 4e-7, 2, 3])\r\n    >>> np.array_repr(x, precision=6, suppress_small=True)\r\n    \'array([ 0.000001,  0.      ,  2.      ,  3.      ])\'\r\n\r\n    """"""\r\n    if max_line_width is None:\r\n        max_line_width = _format_options[\'linewidth\']\r\n\r\n    if type(arr) is not ndarray:\r\n        class_name = type(arr).__name__\r\n    else:\r\n        class_name = ""array""\r\n\r\n    skipdtype = dtype_is_implied(arr.dtype) and arr.size > 0\r\n\r\n    prefix = class_name + ""(""\r\n    suffix = "")"" if skipdtype else "",""\r\n\r\n    if (_format_options[\'legacy\'] == \'1.13\' and\r\n            arr.shape == () and not arr.dtype.names):\r\n        lst = repr(arr.item())\r\n    elif arr.size > 0 or arr.shape == (0,):\r\n        lst = array2string(arr, max_line_width, precision, suppress_small,\r\n                           \', \', prefix, suffix=suffix)\r\n    else:  # show zero-length shape unless it is (0,)\r\n        lst = ""[], shape=%s"" % (repr(arr.shape),)\r\n\r\n    arr_str = prefix + lst + suffix\r\n\r\n    if skipdtype:\r\n        return arr_str\r\n\r\n    dtype_str = ""dtype={})"".format(dtype_short_repr(arr.dtype))\r\n\r\n    # compute whether we should put dtype on a new line: Do so if adding the\r\n    # dtype would extend the last line past max_line_width.\r\n    # Note: This line gives the correct result even when rfind returns -1.\r\n    last_line_len = len(arr_str) - (arr_str.rfind(\'\\n\') + 1)\r\n    spacer = "" ""\r\n    if _format_options[\'legacy\'] == \'1.13\':\r\n        if issubclass(arr.dtype.type, flexible):\r\n            spacer = \'\\n\' + \' \'*len(class_name + ""("")\r\n    elif last_line_len + len(dtype_str) + 1 > max_line_width:\r\n        spacer = \'\\n\' + \' \'*len(class_name + ""("")\r\n\r\n    return arr_str + spacer + dtype_str\r\n\r\ndef array_str(a, max_line_width=None, precision=None, suppress_small=None):\r\n    """"""\r\n    Return a string representation of the data in an array.\r\n\r\n    The data in the array is returned as a single string.  This function is\r\n    similar to `array_repr`, the difference being that `array_repr` also\r\n    returns information on the kind of array and its data type.\r\n\r\n    Parameters\r\n    ----------\r\n    a : ndarray\r\n        Input array.\r\n    max_line_width : int, optional\r\n        Inserts newlines if text is longer than `max_line_width`.  The\r\n        default is, indirectly, 75.\r\n    precision : int, optional\r\n        Floating point precision.  Default is the current printing precision\r\n        (usually 8), which can be altered using `set_printoptions`.\r\n    suppress_small : bool, optional\r\n        Represent numbers ""very close"" to zero as zero; default is False.\r\n        Very close is defined by precision: if the precision is 8, e.g.,\r\n        numbers smaller (in absolute value) than 5e-9 are represented as\r\n        zero.\r\n\r\n    See Also\r\n    --------\r\n    array2string, array_repr, set_printoptions\r\n\r\n    Examples\r\n    --------\r\n    >>> np.array_str(np.arange(3))\r\n    \'[0 1 2]\'\r\n\r\n    """"""\r\n    if (_format_options[\'legacy\'] == \'1.13\' and\r\n            a.shape == () and not a.dtype.names):\r\n        return str(a.item())\r\n\r\n    # the str of 0d arrays is a special case: It should appear like a scalar,\r\n    # so floats are not truncated by `precision`, and strings are not wrapped\r\n    # in quotes. So we return the str of the scalar value.\r\n    if a.shape == ():\r\n        return str(a[()])\r\n\r\n    return array2string(a, max_line_width, precision, suppress_small, \' \', """")\r\n\r\ndef set_string_function(f, repr=True):\r\n    """"""\r\n    Set a Python function to be used when pretty printing arrays.\r\n\r\n    Parameters\r\n    ----------\r\n    f : function or None\r\n        Function to be used to pretty print arrays. The function should expect\r\n        a single array argument and return a string of the representation of\r\n        the array. If None, the function is reset to the default NumPy function\r\n        to print arrays.\r\n    repr : bool, optional\r\n        If True (default), the function for pretty printing (``__repr__``)\r\n        is set, if False the function that returns the default string\r\n        representation (``__str__``) is set.\r\n\r\n    See Also\r\n    --------\r\n    set_printoptions, get_printoptions\r\n\r\n    Examples\r\n    --------\r\n    >>> def pprint(arr):\r\n    ...     return \'HA! - What are you going to do now?\'\r\n    ...\r\n    >>> np.set_string_function(pprint)\r\n    >>> a = np.arange(10)\r\n    >>> a\r\n    HA! - What are you going to do now?\r\n    >>> print(a)\r\n    [0 1 2 3 4 5 6 7 8 9]\r\n\r\n    We can reset the function to the default:\r\n\r\n    >>> np.set_string_function(None)\r\n    >>> a\r\n    array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\r\n\r\n    `repr` affects either pretty printing or normal string representation.\r\n    Note that ``__repr__`` is still affected by setting ``__str__``\r\n    because the width of each array element in the returned string becomes\r\n    equal to the length of the result of ``__str__()``.\r\n\r\n    >>> x = np.arange(4)\r\n    >>> np.set_string_function(lambda x:\'random\', repr=False)\r\n    >>> x.__str__()\r\n    \'random\'\r\n    >>> x.__repr__()\r\n    \'array([     0,      1,      2,      3])\'\r\n\r\n    """"""\r\n    if f is None:\r\n        if repr:\r\n            return multiarray.set_string_function(array_repr, 1)\r\n        else:\r\n            return multiarray.set_string_function(array_str, 0)\r\n    else:\r\n        return multiarray.set_string_function(f, repr)\r\n\r\nset_string_function(array_str, 0)\r\nset_string_function(array_repr, 1)'"
all_scripts/art_common.py,11,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nart_common\r\n===========\r\n\r\nScript :   art_common.py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified : 2018-09-24\r\n\r\nPurpose:  tools for working with numpy arrays\r\n\r\nart_common is a set of functions common to the implementation of array tools\r\nin testing model\r\n\r\nUseage :\r\n\r\nReferences\r\n----------\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/numpyarraytotable.htm>`_.\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/tabletonumpyarray.htm>`_.\r\n---------------------------------------------------------------------\r\n""""""\r\n\r\nimport sys\r\nfrom textwrap import dedent\r\nimport numpy as np\r\nfrom arcpy import AddMessage, ListFields, Raster\r\nfrom arcpy.da import Describe, TableToNumPyArray\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\n__all__ = [\'tweet\',\r\n           \'de_punc\',\r\n           \'_describe\',\r\n           \'fc_info\',\r\n           \'fld_info\',\r\n           \'null_dict\',\r\n           \'tbl_arr\'\r\n           ]\r\n\r\n\r\ndef tweet(msg):\r\n    """"""Print a message for both arcpy and python.\r\n\r\n    msg - a text message\r\n    """"""\r\n    m = ""\\n{}\\n"".format(msg)\r\n    AddMessage(m)\r\n    print(m)\r\n\r\n\r\ndef de_punc(s, punc=None, no_spaces=True, char=\'_\'):\r\n    """"""Remove punctuation and/or spaces in strings and replace with\r\n    underscores or nothing\r\n\r\n    Parameters\r\n    ----------\r\n    s : string\r\n        input string to parse\r\n    punc : string\r\n        A string of characters to replace ie. \'@ ""!\\\'\\\\[]\'\r\n    no_spaces : boolean\r\n        True, replaces spaces with underscore.  False, leaves spaces\r\n    char : string\r\n        Replacement character\r\n    """"""\r\n    if (punc is None) or not isinstance(punc, str):\r\n        punc = \'!""#$%&\\\'()*+,-./:;<=>?@[\\\\]^`{|}~\'  # _ removed\r\n    if no_spaces:\r\n        punc = "" "" + punc\r\n    s = """".join([[i, char][i in punc] for i in s])\r\n    return s\r\n\r\ndef _describe(in_fc):\r\n    """"""Simply return the arcpy.da.Describe object.\r\n\r\n    **desc.keys()** an abbreviated list::\r\n\r\n    \'OIDFieldName\'... \'areaFieldName\', \'baseName\'... \'catalogPath\',\r\n    \'dataType\'... \'extent\', \'featureType\', \'fields\', \'file\'... \'hasM\',\r\n    \'hasOID\', \'hasZ\', \'indexes\'... \'lengthFieldName\'... \'name\', \'path\',\r\n    \'rasterFieldName\', ..., \'shapeFieldName\', \'shapeType\',\r\n    \'spatialReference\'\r\n\r\n    """"""\r\n    return Describe(in_fc)\r\n\r\n\r\ndef fc_info(in_fc, prn=False):\r\n    """"""Return basic featureclass information, including the following...\r\n\r\n    Returns:\r\n    --------\r\n    shp_fld  :\r\n        field name which contains the geometry object\r\n    oid_fld  :\r\n        the object index/id field name\r\n    SR       :\r\n        spatial reference object (use SR.name to get the name)\r\n    shp_type :\r\n        shape type (Point, Polyline, Polygon, Multipoint, Multipatch)\r\n\r\n    Notes:\r\n    ------\r\n    Other useful parameters :\r\n        \'areaFieldName\', \'baseName\', \'catalogPath\',\'featureType\',\r\n        \'fields\', \'hasOID\', \'hasM\', \'hasZ\', \'path\'\r\n\r\n    Derive all field names :\r\n        all_flds = [i.name for i in desc[\'fields\']]\r\n    """"""\r\n    desc = _describe(in_fc)\r\n    args = [\'shapeFieldName\', \'OIDFieldName\', \'shapeType\', \'spatialReference\']\r\n    shp_fld, oid_fld, shp_type, SR = [desc[i] for i in args]\r\n    if prn:\r\n        frmt = ""FeatureClass:\\n   {}"".format(in_fc)\r\n        f = ""\\n{!s:<16}{!s:<14}{!s:<10}{!s:<10}""\r\n        frmt += f.format(*args)\r\n        frmt += f.format(shp_fld, oid_fld, shp_type, SR.name)\r\n        tweet(frmt)\r\n        return None\r\n    return shp_fld, oid_fld, shp_type, SR\r\n\r\n\r\ndef fld_info(in_fc, prn=False):\r\n    """"""Field information for a featureclass (in_fc).\r\n\r\n    Parameters:\r\n    -----------\r\n    prn : boolean\r\n        True - returns the values\r\n\r\n        False - simply prints the results\r\n\r\n    Field properties:\r\n    -----------------\r\n    \'aliasName\', \'baseName\', \'defaultValue\', \'domain\', \'editable\',\r\n    \'isNullable\', \'length\', \'name\', \'precision\', \'required\', \'scale\', \'type\'\r\n    """"""\r\n    flds = ListFields(in_fc)\r\n    f_info = [(i.name, i.type, i.length, i.isNullable, i.required)\r\n              for i in flds]\r\n    f = ""{!s:<14}{!s:<12}{!s:>7} {!s:<10}{!s:<10}""\r\n    if prn:\r\n        frmt = ""FeatureClass:\\n   {}\\n"".format(in_fc)\r\n        args = [""Name"", ""Type"", ""Length"", ""Nullable"", ""Required""]\r\n        frmt += f.format(*args) + ""\\n""\r\n        frmt += ""\\n"".join([f.format(*i) for i in f_info])\r\n        tweet(frmt)\r\n        return None\r\n    return f_info\r\n\r\n\r\ndef null_dict(flds):\r\n    """"""Produce a null dictionary from a list of fields\r\n    These must be field objects and not just their name.\r\n    """"""\r\n    dump_flds = [""OBJECTID"",""Shape_Length"", ""Shape_Area"", ""Shape""]\r\n    flds_oth = [f for f in flds\r\n                if f.name not in dump_flds]\r\n#    oid_geom = [\'OBJECTID\', \'SHAPE@X\', \'SHAPE@Y\']\r\n    nulls = {\'Double\':np.nan,\r\n             \'Single\':np.nan,\r\n             \'Short\':np.iinfo(np.int16).min,\r\n             \'SmallInteger\':np.iinfo(np.int16).min,\r\n             \'Long\':np.iinfo(np.int32).min,\r\n             \'Float\':np.nan,\r\n             \'Integer\':np.iinfo(np.int32).min,\r\n             \'String\':str(None),\r\n             \'Text\':str(None)}\r\n    fld_dict = {i.name: i.type for i in flds_oth}\r\n    nulls = {f.name:nulls[fld_dict[f.name]] for f in flds_oth}\r\n    return nulls\r\n\r\n\r\ndef tbl_arr(pth):\r\n    """"""Convert featureclass/table to a structured ndarray\r\n\r\n    Requires\r\n    --------\r\n    pth : string\r\n        path to input featureclass or table\r\n\r\n    """"""\r\n    flds = ListFields(pth)\r\n    nulls = null_dict(flds)\r\n    bad = [\'OID\', \'Geometry\', \'Shape_Length\', \'Shape_Area\']\r\n    f0 = [""OID@""]\r\n    f1 = [i.name for i in flds if i.type not in bad]\r\n    flds = f0 + f1\r\n    a = TableToNumPyArray(pth,\r\n                          field_names=flds,\r\n                          skip_nulls=False,\r\n                          null_value=nulls)\r\n    dt = np.array(a.dtype.descr)\r\n    nmes = dt[:, 0]\r\n    sze = dt[:, 1]\r\n    cleaned = []\r\n    for i in nmes:\r\n        i = de_punc(i)  # run de_punc to remove punctuation\r\n        cleaned.append(i)\r\n    a.dtype = list(zip(cleaned, sze))\r\n    return a\r\n\r\n# ---- raster section ----\r\n#\r\ndef rasterfile_info(fname, prn=False):\r\n    """"""Obtain raster stack information from the filename of an image\r\n    :\r\n    """"""\r\n    #\r\n    frmt = """"""\r\n    File path   - {}\r\n    Name        - {}\r\n    Spatial Ref - {}\r\n    Raster type - {}\r\n    Integer?    - {}\r\n    NoData      - {}\r\n    Min         - {}\r\n    Max         - {}\r\n    Mean        - {}\r\n    Std dev     - {}\r\n    Bands       - {}\r\n    Cell        - h {}   w {}\r\n    Lower Left  - X {}   Y {}\r\n    Upper Left  - X {}   Y {}\r\n    Extent      - h {}   w {}\r\n    """"""\r\n    desc = Describe(fname)\r\n    r_data_type = desc.datasetType  # \'RasterDataset\'\r\n    args = []\r\n    if r_data_type == \'RasterDataset\':\r\n        r = Raster(fname)\r\n        r.catalogPath            # full path name and file name\r\n        pth = r.path             # path only\r\n        name = r.name            # file name\r\n        SR = r.spatialReference\r\n        r_type = r.format        # \'TIFF\'\r\n        #\r\n        is_int = r.isInteger\r\n        nodata = r.noDataValue\r\n        r_max = r.maximum\r\n        r_min = r.minimum\r\n        r_mean = ""N/A""\r\n        r_std = ""N/A""\r\n        if not is_int:\r\n            r_mean = r.mean\r\n            r_std = r.standardDeviation\r\n        bands = r.bandCount\r\n        cell_hght = r.meanCellHeight\r\n        cell_wdth = r.meanCellWidth\r\n        extent = desc.Extent\r\n        LL = extent.lowerLeft  # Point (X, Y, #, #)\r\n        hght = r.height\r\n        wdth = r.width\r\n        UL = r.extent.upperLeft\r\n        args = [pth, name, SR.name, r_type, is_int, nodata, r_min, r_max,\r\n                r_mean, r_std, bands, cell_hght, cell_wdth, LL.X, LL.Y,\r\n                UL.X, UL.Y, hght, wdth]\r\n    if prn:\r\n        tweet(dedent(frmt).format(*args))\r\n    else:\r\n        return args\r\n\r\n# ----------------------------------------------------------------------\r\n# .... final code section producing the featureclass and extendtable\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    # parameters here\r\nelse:\r\n    testing = False\r\n    # parameters here\r\n#\r\nif testing:\r\n    print(\'\\nScript source... {}\'.format(script))\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n'"
all_scripts/ascii_to_raster.py,9,"b'# coding: utf-8\n""""""\nascii_to_raster\n===============\n\nScript :   ascii_to_raster.py\n\nAuthor :   Dan.Patterson@carleton.ca\n\nModified : 2018-11-20\n\nPurpose :  Convert an ascii file to a raster (tif)\n\n\n\nsyntax:\n    RasterToNumPyArray(in_raster, {lower_left_corner},\n                      {ncols}, {nrows}, {nodata_to_value})\n\nUseage:\n-------\ninRas : arcpy.Raster(\'C:/data/inRaster\')\n\nlowerLeft : arcpy.Point(inRas.extent.XMin,inRas.extent.YMin)\n\ncellSize : ras.meanCellWidth\n\nReturns:\n--------\nbands, rows, columns or a structured array\n\nNotes:\n------\n- rows is dim 0\n- cols is dim 1\n- depth is dim 2\n\nReferences:\n-----------\n`<http://desktop.arcgis.com/en/arcmap/latest/analyze/arcpy-functions/\nrastertonumpyarray-function.htm>`_.\n\nFrom my post : 2011-10-11\n\n`<http://gis.stackexchange.com/questions/16098/determining-min-and->`_.\nmax-values-in-an-ascii-raster-dataset-using-python/16101#16101\n\n>>> import numpy as np\n>>> ascii_file = ""c:/temp/Ascii_3x3_1nodata.asc""\n>>> an_array = np.mafromtxt(ascii_file, \'float\', \'#\', None, 6, None, \'-999\')\nNCOLS          3\nNROWS          3\nXLLCORNER      0\nYLLCORNER      0\nCELLSIZE       1\nNODATA_VALUE   -999\n0 1 2\n-999 4 5\n6 7 8\n\n""""""\nimport os\nimport numpy as np\nfrom textwrap import dedent, indent\nimport arcpy\n\narcpy.overwriteOutput = True\n\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\n      \'float\': \'{: 0.3f}\'.format}\nnp.set_printoptions(edgeitems=3, linewidth=80, precision=2, suppress=True,\n                    threshold=80, formatter=ft)\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\n\n# ---- Processing temporal ascii files ----\n# Header information\n# ncols 720\n# nrows 360\n# xllcorner -180\n# yllcorner -90\n# cellsize 0.5\n# NODATA_Value -9999\n# ---- Begin the process ----\n#\n# =============================================================================\n# save_output = False\n# cols = 720\n# rows = 360\n# ll_corner =  arcpy.Point(-180., -90.0)  # to position the bottom left corner\n# dx_dy = 0.5\n# nodata = \'-9999\'\n# #\n# # ---- create the basic workspace parameters, create masked arrays ----\n# #\n# out_file = r\'c:\\Data\\ascii_samples\\avg_yr.tif\'\n# folder = r\'C:\\Data\\ascii_samples\'\n# arcpy.env.workspace = folder\n# ascii_files = arcpy.ListFiles(""*.asc"")\n# a_s = [folder + \'\\{}\'.format(i) for i in ascii_files]\n# arrays = []\n#\n# for arr in a_s[:1]:\n#     a = np.mafromtxt(arr, dtype=\'int32\', comments=\'#\',\n#                      delimiter=\' \', skip_header=6,\n#                      missing_values=nodata, usemask=True)\n#     value_to_nodata = int(a.get_fill_value())\n#     out = a.view(np.ndarray, fill_value=value_to_nodata)\n#     r = arcpy.NumPyArrayToRaster(out, ll_corner, dx_dy, dx_dy)\n#     out_file = arr.replace("".asc"", "".tif"")\n#     if save_output:\n#         r.save(out_file)\n#     del r\n# =============================================================================\nfrom arcpy import NumPyArrayToRaster, Point\n\npath = r""C:\\Temp\\dem.txt""\nncols    =     317\nnrows     =    204\nxllcorner =    2697732\nyllcorner =    1210264\ncellsize  =    2\nNODATA_value = -9999\n\na = np.genfromtxt(path, np.float, delimiter=\' \', skip_header=6)\na0 = np.where(a==-9999., np.nan, a)\n\nLL = Point(xllcorner, yllcorner)\nout = NumPyArrayToRaster(a0, LL, 2.0, 2.0, np.nan)\n# out.save(r""C:\\Temp\\dem_np.tif"")\n# ----------------------------------------------------------------------\n# __main__ .... code section\nif __name__ == ""__main__"":\n    """"""Optionally...\n    : - print the script source name.\n    : - run the _demo\n    """"""\n\n#    print(""Script... {}"".format(script))\n#    _demo()\n'"
all_scripts/circlepnts.py,10,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   createcompass.py\r\n:Author   Dan_Patterson@carleton.ca\r\n:Modified: 2017-04-02\r\n: if north angle is needed, you can use this to convert\r\n:if fromNorth:\r\n:    ang = np.mod((450.0 - ang), 360.)\r\n""""""\r\n\r\n# --------------------------------------------------------------------------\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\nfrom arraytools import tweet\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2,\r\n                    suppress=True, threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\ndef _circle(radius=10, theta=22.5, xc=0.0, yc=0.0):\r\n    """"""Produce a circle/ellipse depending on parameters.\r\n    :  radius - distance from centre\r\n    :  theta - either a single value to form angles about a circle or\r\n    :        - a list or tuple of the desired angles\r\n    """"""\r\n    angles = np.deg2rad(np.arange(180.0, -180.0-theta, step=-theta))\r\n    x_s = radius*np.cos(angles) + xc    # X values\r\n    y_s = radius*np.sin(angles) + yc    # Y values\r\n    pnts = np.c_[x_s, y_s]\r\n    return pnts\r\n\r\n\r\n# --------------------------------------------------------------------------\r\ninFC = sys.argv[1]   # r""C:\\Data\\points\\points.gdb\\fishnet_label""\r\noutFC = sys.argv[2]  # r""C:\\Data\\points\\pnts2.shp""\r\nradius = float(sys.argv[3])  # radius = 2\r\ntheta = float(sys.argv[4])\r\na = arcpy.da.FeatureClassToNumPyArray(inFC, [""SHAPE@X"", ""SHAPE@Y""])\r\n\r\nfrmt = """"""... {} ...\r\n:Input featureclass : {}\r\n:Output featureclass: {}\r\n:Radius {}, angle step {}\r\n:Points:\r\n{}\r\n:\r\n""""""\r\nargs = [script, inFC, outFC, radius, theta, a]\r\nmsg = frmt.format(*args)\r\ntweet(msg)\r\n# ---- option (1) read X and Y separately, then reset the dtype names\r\n# ---- get the circle values, stack and set dtype\r\n# or a list like... theta = [0, 90, 180, 270]\r\n#    theta = [0, 90, 180, 270]\r\n\r\na.dtype = [(\'X\', \'<f8\'), (\'Y\', \'<f8\')]\r\nb = [_circle(radius, theta, a[\'X\'][i], a[\'Y\'][i])\r\n     for i in range(len(a))]\r\nc = np.vstack(b)\r\nc.dtype = a.dtype\r\nc = np.squeeze(c)\r\n\r\narcpy.da.NumPyArrayToFeatureClass(c, outFC, c.dtype.names)\r\narcpy.AddXY_management(outFC)\r\n\r\n# --------------------------------------------------------------------\r\nif __name__ == \'__main__\':\r\n    """"""produce some points around a centroid at angles and distances""""""\r\n# ---- option (2) read the centroid coordinates then reset the dtype name\r\n# ---- get the circle values, stack and set dtype\r\n#    a = arcpy.da.FeatureClassToNumPyArray(inFC, ""SHAPE@XY"")\r\n#    a.dtype = [(\'XY\', \'<f8\', (2,))]\r\n#    b = [_circle(radius, theta, a[\'XY\'][i,0], a[\'XY\'][i,1])\r\n#        for i in range(len(a))]\r\n'"
all_scripts/circular.py,25,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   circular.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2017-12-31\r\n:Purpose:  See the documentation for the functions\r\n:Notes:\r\n:\r\n:References:\r\n:\r\n""""""\r\n# pylint: disable=C0103\r\n# pylint: disable=R1710\r\n# pylint: disable=R0914\r\n\r\n# ---- imports, formats, constants ----\r\n\r\nimport sys\r\nimport numpy as np\r\n\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2,\r\n                    suppress=True, threshold=100, formatter=ft)\r\n\r\n_script = sys.argv[0]\r\n\r\n__all__ = [""plot_"",\r\n           ""rot_matrix"",\r\n           ""_arc"",\r\n           ""_circle"",\r\n           ""arc_sector"",\r\n           ""buffer_ring"",\r\n           ""multiring_buffer_demo"",\r\n           ""multi_sector_demo""\r\n           ]\r\n\r\n\r\n# ---- functions ----\r\n\r\ndef plot_(pnts):\r\n    """"""plot a circle, arc sector etc\r\n    """"""\r\n    import matplotlib.pyplot as plt\r\n    import matplotlib\r\n    from matplotlib.patches import Polygon\r\n    from matplotlib.collections import PatchCollection\r\n    fig, ax = plt.subplots()\r\n    patches = []\r\n    for i in pnts:  # Points need to form a closed loop\r\n        polygon = Polygon(i, closed=False)  # closed=True if 1st/last pnt !=\r\n        patches.append(polygon)\r\n    p = PatchCollection(patches, cmap=matplotlib.cm.jet, alpha=1.0)\r\n    colors = 100*np.random.rand(len(patches))\r\n    p.set_array(np.array(colors))\r\n    ax.add_collection(p)\r\n    plt.axis(\'equal\')\r\n    plt.show()\r\n#    plt.close()\r\n\r\n\r\ndef rot_matrix(angle=0, nm_3=False):\r\n    """"""Return the rotation matrix given points and rotation angle\r\n\r\n    Requires:\r\n    --------\r\n      - rotation angle in degrees and whether the matrix will be used with\r\n        homogenous coordinates\r\n\r\n    Returns:\r\n    -------\r\n      - rot_m - rotation matrix for 2D transform\r\n      - rotate around  translate(-x, -y).rotate(theta).translate(x, y)\r\n    """"""\r\n    rad = np.deg2rad(angle)\r\n    c = np.cos(rad)\r\n    s = np.sin(rad)\r\n    rm = np.array([[c, -s, 0.],\r\n                   [s, c, 0.],\r\n                   [0., 0., 1.]])\r\n    if not nm_3:\r\n        rm = rm[:2, :2]\r\n    return rm\r\n\r\n\r\ndef _arc(radius=100, start=0, stop=1, step=0.1, xc=0.0, yc=0.0):\r\n    """"""Create an arc from a specified radius, centre and start/stop angles\r\n\r\n    Requires:\r\n    ---------\r\n    `radius` : number\r\n        cirle radius from which the arc is obtained\r\n    `start`, `stop`, `step` : numbers\r\n        angles in degrees\r\n    `xc`, `yc` : number\r\n        center coordinates in projected units\r\n\r\n    Returns:\r\n    --------\r\n      points on the arc\r\n    """"""\r\n    start, stop = sorted([start, stop])\r\n    angle = np.deg2rad(np.arange(start, stop, step))\r\n    x_s = radius*np.cos(angle)         # X values\r\n    y_s = radius*np.sin(angle)         # Y values\r\n    pnts = np.c_[x_s, y_s]\r\n    pnts = pnts + [xc, yc]\r\n    p_lst = pnts.tolist()\r\n    return p_lst\r\n\r\n\r\ndef _circle(radius=100, clockwise=True, theta=1, rot=0.0, scale=1,\r\n            xc=0.0, yc=0.0):\r\n    """"""Produce a circle/ellipse depending on parameters.\r\n\r\n    Requires\r\n    --------\r\n    `radius` : number\r\n        in projected units\r\n    `clockwise` : boolean\r\n        True for clockwise (outer rings), False for counter-clockwise\r\n        (for inner rings)\r\n    `theta` : number\r\n        Angle spacing. If theta=1, angles between -180 to 180, are returned\r\n        in 1 degree increments. The endpoint is excluded.\r\n    `rot` : number\r\n         rotation angle in degrees... used if scaling is not equal to 1\r\n    `scale` : number\r\n         For ellipses, change the scale to <1 or > 1. The resultant\r\n         y-values will favour the x or y-axis depending on the scaling.\r\n\r\n    Returns:\r\n    -------\r\n      list of coordinates for the circle/ellipse\r\n\r\n    Notes:\r\n    ------\r\n     You can also use np.linspace if you want to specify point numbers.\r\n     np.linspace(start, stop, num=50, endpoint=True, retstep=False)\r\n     np.linspace(-180, 180, num=720, endpoint=True, retstep=False)\r\n    """"""\r\n    if clockwise:\r\n        angles = np.deg2rad(np.arange(180.0, -180.0-theta, step=-theta))\r\n    else:\r\n        angles = np.deg2rad(np.arange(-180.0, 180.0+theta, step=theta))\r\n    x_s = radius*np.cos(angles)            # X values\r\n    y_s = radius*np.sin(angles) * scale    # Y values\r\n    pnts = np.c_[x_s, y_s]\r\n    if rot != 0:\r\n        rot_mat = rot_matrix(angle=rot)\r\n        pnts = (np.dot(rot_mat, pnts.T)).T\r\n    pnts = pnts + [xc, yc]\r\n    return pnts\r\n\r\n\r\ndef arc_sector(outer=10, inner=9, start=1, stop=6, step=0.1):\r\n    """"""Form an arc sector bounded by a distance specified by two radii\r\n\r\n    `outer` : number\r\n        outer radius of the arc sector\r\n    `inner` : number\r\n        inner radius\r\n    `start` : number\r\n        start angle of the arc\r\n    `stop` : number\r\n        end angle of the arc\r\n    `step` : number\r\n        the angle densification step\r\n\r\n    Requires:\r\n    --------\r\n      `_arc` is used to produce the arcs, the top arc is rotated clockwise and\r\n      the bottom remains in the order produced to help form closed-polygons.\r\n    """"""\r\n    s_s = [start, stop]\r\n    s_s.sort()\r\n    start, stop = s_s\r\n    top = _arc(outer, start, stop, step, 0.0, 0.0)\r\n    top.reverse()\r\n    bott = _arc(inner, start, stop, step, 0.0, 0.0)\r\n    top = np.array(top)\r\n    bott = np.array(bott)\r\n    close = top[0]\r\n    pnts = np.asarray([i for i in [*top, *bott, close]])\r\n    return pnts\r\n\r\n\r\ndef buffer_ring(outer=100, inner=0, theta=10, rot=0, scale=1, xc=0.0, yc=0.0):\r\n    """"""Create a multi-ring buffer around a center point (xc, yc)\r\n     outer - outer radius\r\n     inner - inner radius\r\n     theta - angles to use to densify the circle...\r\n        - 360+ for circle\r\n        - 120 for triangle\r\n        - 90  for square\r\n        - 72  for pentagon\r\n        - 60  for hexagon\r\n        - 45  for octagon\r\n        - etc\r\n     rot - rotation angle, used for non-circles\r\n     scale - used to scale the y-coordinates\r\n\r\n    """"""\r\n    top = _circle(outer, clockwise=True, theta=theta, rot=rot,\r\n                  scale=scale, xc=xc, yc=yc)\r\n    if inner != 0.0:\r\n        bott = _circle(inner, clockwise=False, theta=theta, rot=rot,\r\n                       scale=scale, xc=xc, yc=yc)\r\n        pnts = np.asarray([i for i in [*top, *bott]])\r\n    else:\r\n        pnts = top\r\n    return pnts\r\n\r\n\r\n# ---- demo functions ----\r\n\r\ndef multiring_buffer_demo():\r\n    """"""Do a multiring buffer\r\n     rads - buffer radii\r\n     theta - angle density... 1 for 360 ngon, 120 for triangle\r\n     rot - rotation angle for ellipses and other shapes\r\n     scale - scale the y-values to produce ellipses\r\n    """"""\r\n    buffers = []\r\n    radii = [10, 20, 40, 80, 100]  # , 40, 60, 100]\r\n    theta = 10\r\n    rot = 22.5\r\n    scale = 0.7\r\n    for r in range(1, len(radii)):\r\n        ring = buffer_ring(radii[r], radii[r-1], theta, rot, scale)\r\n        buffers.append(ring)\r\n    plot_(buffers)\r\n    # return buffers\r\n\r\n\r\ndef multi_sector_demo():\r\n    """"""Produce multiple sectors  """"""\r\n    sectors = []\r\n    outer = 10\r\n    inner = 9\r\n    incr = np.arange(0, 91, 1)  # (0,361,5)\r\n    for outer in range(6, 10):\r\n        inner = outer - 1\r\n        for i in range(0, len(incr)):\r\n            st = incr[i]\r\n            end = incr[i-1]\r\n            arc = arc_sector(outer, inner, start=st, stop=end, step=0.1)\r\n            sectors.append(arc)\r\n    plot_(sectors)\r\n\r\n\r\ndef help_():\r\n    """"""Print the docs""""""\r\n    args = [\'_arc ....\', _arc.__doc__,\r\n            \'arc_sector ....\', arc_sector.__doc__,\r\n            \'_circle ....\', _circle.__doc__,\r\n            \'buffer_ring ....\', buffer_ring.__doc__,\r\n            \'rot_matrix ....\', rot_matrix.__doc__,\r\n            \'buffer_ring ....\', buffer_ring.__doc__]\r\n    frmt = ""-""*60 + ""\\ncircular.py ....\\n\\n"" + ""{}\\n""*len(args)\r\n    print(frmt.format(*args))\r\n    del frmt, args\r\n\r\n\r\n# ----------------------\r\nif __name__ == ""__main__"":\r\n    """"""Uncomment what you want to see""""""\r\n#    print(""Script... {}"".format(_script))\r\n#    circ_pnts = _circle(radius=1, theta=30, xc=5, yc=5)\r\n#    print(""\\ncircle points...\\n{}"".format(circ_pnts))\r\n#    arc_pnts = _arc(radius=10, start=0, stop=90.5, step=5, xc=0.0, yc=0.0)\r\n#    print(""\\narc points...\\n{}"".format(arc_pnts))\r\n#    pnts = arc_sector()\r\n#    pnts = buffer_ring()\r\n#    multi_sector_demo()\r\n#    multiring_buffer_demo()\r\n'"
all_scripts/closest.py,23,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   closest.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2017-04-11\r\n:\r\n:Purpose:  Determine the nearest points based on euclidean distance within\r\n:  a point file and then connect them\r\n:References:\r\n:----------\r\n: - see near.py documentation for documentation\r\n:\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\n\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\nfrom arcpytools import fc_info, tweet\r\n\r\n# from textwrap import dedent\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.1f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=120, precision=2,\r\n                    suppress=True, threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')\r\n\r\nscript = sys.argv[0]\r\n\r\narcpy.env.overwriteOutput = True\r\n# ---- functions ----\r\n\r\n\r\ndef n_near(a, N=3, ordered=True):\r\n    """"""Return the coordinates and distance to the nearest N points within\r\n    an 2D numpy array, `a`, with optional ordering of the inputs.\r\n\r\n    Requires:\r\n    ---------\r\n    ``a  :`` array\r\n        an ndarray of uniform int or float dtype.  Extract the fields\r\n        representing the x,y coordinates before proceeding.\r\n    ``N  :`` number\r\n        number of closest points to return\r\n\r\n    Returns:\r\n    -------\r\n    A structured array is returned containing an ID number.  The ID number\r\n    is the ID of the points as they were read.  The array will contain\r\n    (C)losest fields and distance fields\r\n\r\n    (C0_X, C0_Y, C1_X, C1_Y, Dist0, Dist1 etc) representing coordinates\r\n    and distance to the required \'closest\' points.\r\n    """"""\r\n    if not (isinstance(a, (np.ndarray)) and (N >= 1)):\r\n        print(""\\nInput error...read the docs\\n\\n{}"".format(n_near.__doc__))\r\n        return a\r\n    rows, cols = a.shape\r\n    dt_near = [(\'Xo\', \'<f8\'), (\'Yo\', \'<f8\')]\r\n    dt_new = [(\'C{}\'.format(i) + \'{}\'.format(j), \'<f8\')\r\n              for i in range(N)\r\n              for j in [\'_X\', \'_Y\']]\r\n    dt_near.extend(dt_new)\r\n    dt_dist = [(\'Dist{}\'.format(i), \'<f8\') for i in range(N)]\r\n    dt = [(\'ID\', \'<i4\'), *dt_near, *dt_dist]\r\n    n_array = np.zeros((rows,), dtype=dt)\r\n    n_array[\'ID\'] = np.arange(rows)\r\n    # ---- distance matrix calculation using einsum ----\r\n    if ordered:\r\n        a = a[np.argsort(a[:, 0])]\r\n    b = a.reshape(np.prod(a.shape[:-1]), 1, a.shape[-1])\r\n    diff = b - a\r\n    dist = np.einsum(\'ijk,ijk->ij\', diff, diff)\r\n    d = np.sqrt(dist).squeeze()\r\n    # ---- format for use in structured array output ----\r\n    # steps are outlined as follows....\r\n    #\r\n    kv = np.argsort(d, axis=1)       # sort \'d\' on last axis to get keys\r\n    coords = a[kv]                   # pull out coordinates using the keys\r\n    s0, s1, s2 = coords.shape\r\n    coords = coords.reshape((s0, s1*s2))\r\n    dist = np.sort(d)[:, 1:]         # slice sorted distances, skip 1st\r\n    # ---- construct the structured array ----\r\n    dt_names = n_array.dtype.names\r\n    s0, s1, s2 = (1, (N+1)*2 + 1, len(dt_names))\r\n    for i in range(0, s1):           # coordinate field names\r\n        nm = dt_names[i+1]\r\n        n_array[nm] = coords[:, i]\r\n    dist_names = dt_names[s1:s2]\r\n    for i in range(N):               # fill n_array with the results\r\n        nm = dist_names[i]\r\n        n_array[nm] = dist[:, i]\r\n    return coords, dist, n_array\r\n\r\n\r\ndef _uniq_by_row_col(a, axis=0):\r\n    """"""unique to emulate numpy 1.13 ...\r\n\r\n    Requires:\r\n    ---------\r\n    - a : array\r\n        an array of uniform dtype with ndim > 1\r\n    - axis : number\r\n        if 0, then unique rows are returned, if 1, then unique columns\r\n\r\n    References:\r\n    -----------\r\n    [1] https://github.com/numpy/numpy/blob/master/numpy/lib/arraysetops.py\r\n\r\n    [2] http://stackoverflow.com/questions/16970982/find-unique-rows-in-numpy-\\\r\n    array?noredirect=1&lq=1\r\n\r\n    Notes:\r\n    ------\r\n    Must reshape to a contiguous 2D array for this to work...\r\n\r\n    a.dtype.char : [\'AllInteger\'] + [\'Datetime\'] + \'S\') = \'bBhHiIlLqQpPMmS\'\r\n    """"""\r\n    a = np.asanyarray(a)\r\n    a = np.swapaxes(a, axis, 0)\r\n    orig_shape, _ = a.shape, a.dtype  # orig_shape, orig_dtype\r\n    a = a.reshape(orig_shape[0], -1)\r\n    a = np.ascontiguousarray(a)\r\n    if a.dtype.char in (\'bBhHiIlLqQpPMmS\'):\r\n        dt = np.dtype((np.void, a.dtype.itemsize * a.shape[1]))\r\n    else:\r\n        dt = [(\'f{i}\'.format(i=i), a.dtype) for i in range(a.shape[1])]\r\n    b = a.view(dt)\r\n    _, idx = np.unique(b, return_index=True)\r\n    unique_a = a[idx]\r\n    return unique_a\r\n\r\n\r\ndef connect(in_fc, out_fc, N=1, testing=False):\r\n    """"""Run the analysis to form the closest point pairs.\r\n\r\n    Calls n_near to produce the nearest features.\r\n    """"""\r\n    shp_fld, oid_fld, shp_type, SR = fc_info(in_fc)\r\n    a = arcpy.da.FeatureClassToNumPyArray(in_fc, ""*"", """", SR)\r\n    dt = \'<f8\'\r\n    b = np.array([tuple(i) for i in a[shp_fld]], dtype=dt)\r\n    coords, dist, n_array = n_near(b, N, ordered=True)  # ---- run n_near ----\r\n    fr_to = coords[:, :(N+1)*2]\r\n    frum = fr_to[:, :2]\r\n    twos = fr_to[:, 2:].reshape(-1, N, 2)\r\n    r = []\r\n    for i in range(len(frum)):\r\n        f = frum[i]\r\n        t = twos[i]\r\n        for j in range(len(t)):\r\n            r.append(np.array([f, t[j]]))\r\n    rr = np.array(r)\r\n    r0 = np.array([i[np.lexsort((i[:, 1], i[:, 0]))] for i in rr])  # slicesort\r\n    r1 = r0.reshape(-1, 4)\r\n    r2 = _uniq_by_row_col(r1, axis=0)  # use if np.version < 1.13\r\n    # r2 = unique_2d(r1)\r\n    r3 = r2[np.argsort(r2[..., 0])]\r\n    r3 = r3.reshape(-1, 2, 2)\r\n    if not testing:\r\n        s = []\r\n        for pt in r3:\r\n            arr = arcpy.Array([arcpy.Point(*p) for p in pt])\r\n            s.append(arcpy.Polyline(arr, SR))\r\n        if arcpy.Exists(out_fc):\r\n            arcpy.Delete_management(out_fc)\r\n        arcpy.CopyFeatures_management(s, out_fc)\r\n        return None\r\n    else:\r\n        return a, b, r0, r1, r2, r3\r\n\r\n\r\n# ---- Run the analysis ----\r\nfrmt = """"""\\n\r\n:Running ... {}\r\n:Using ..... {}\r\n:Finding ... {} closest points and forming connections\r\n:Producing.. {}\\n\r\n""""""\r\n\r\n\r\ndef tool():\r\n    """"""run the tool """"""\r\n    in_fc = sys.argv[1]\r\n    N = int(sys.argv[2])\r\n    out_fc = sys.argv[3]\r\n    args = [script, in_fc, N, out_fc]\r\n    tweet(frmt.format(*args))                    # call tweet\r\n    connect(in_fc, out_fc, N=N, testing=False)   # call connect\r\n\r\n\r\nif len(sys.argv) == 1:\r\n    in_fc = r""C:\\GIS\\array_projects\\data\\Pro_base.gdb\\small""\r\n    # out_fc = r""C:\\GIS\\array_projects\\data\\Pro_base.gdb\\ft3""\r\n    out_fc = None\r\n    N = 2\r\n    testing = True\r\n    a, b, r0, r1, r2, r3 = connect(in_fc, out_fc, N=N, testing=True)\r\n    args = [script, in_fc, N, out_fc]\r\n    tweet(frmt.format(*args))\r\nelse:\r\n    tool()\r\n\r\n\r\n# ---------------------------------------------------------------------\r\nif __name__ == ""__main__"":\r\n    """"""Main section...   """"""\r\n#    print(""Script... {}"".format(script))\r\n'"
all_scripts/closetbl.py,22,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\nclosetbl\r\n========\r\n\r\nScript : closetbl.py\r\n\r\nAuthor : Dan_Patterson@carleton.ca\r\n\r\nModified : 2018-03-31\r\n\r\nPurpose:\r\n    Determine the nearest points based on euclidean distance within a point\r\n    file.  Emulates Generate Near Table in ArcMap\r\n\r\nReferences:\r\n----------\r\n[1] http://desktop.arcgis.com/en/arcmap/latest/tools/analysis-toolbox/\r\ngenerate-near-table.htm\r\n\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\n\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\nfrom arcpytools import fc_info, tweet\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.1f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=120, precision=2,\r\n                    suppress=True, threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')\r\n\r\nscript = sys.argv[0]\r\n\r\n\r\n# ---- functions ----\r\ndef e_dist(a, b, metric=\'euclidean\'):\r\n    """"""Distance calculation for 1D, 2D and 3D points using einsum\r\n\r\n    - ``a, b   :`` list, tuple, array in 1,2 or 3D form\r\n    - ``metric :`` euclidean (\'e\',\'eu\'...), sqeuclidean (\'s\',\'sq\'...),\r\n    """"""\r\n    a = np.asarray(a)\r\n    b = np.atleast_2d(b)\r\n    a_dim = a.ndim\r\n    b_dim = b.ndim\r\n    if a_dim == 1:\r\n        a = a.reshape(1, 1, a.shape[0])\r\n    if a_dim >= 2:\r\n        a = a.reshape(np.prod(a.shape[:-1]), 1, a.shape[-1])\r\n    if b_dim > 2:\r\n        b = b.reshape(np.prod(b.shape[:-1]), b.shape[-1])\r\n    diff = a - b\r\n    dist_arr = np.einsum(\'ijk,ijk->ij\', diff, diff)\r\n    if metric[:1] == \'e\':\r\n        dist_arr = np.sqrt(dist_arr)\r\n    dist_arr = np.squeeze(dist_arr)\r\n    return dist_arr\r\n\r\n\r\ndef to_array(in_fc):\r\n    """"""Extract the shapes and produce a coordinate array.\r\n    """"""\r\n    shp_fld, oid_fld, shp_type, SR = fc_info(in_fc)\r\n    in_flds = [oid_fld, shp_fld]\r\n    a = arcpy.da.FeatureClassToNumPyArray(in_fc, in_flds)\r\n    a = a[shp_fld]\r\n    return a\r\n\r\n\r\ndef line_dir(orig, dest, fromNorth=False):\r\n    """"""Direction of a line given 2 points\r\n\r\n    - ``orig, dest :`` two points representing the start and end of a line.\r\n    - ``fromNorth  :`` True or False gives angle relative to x-axis)\r\n    """"""\r\n    orig = np.atleast_2d(orig)\r\n    dest = np.atleast_2d(dest)\r\n    dxy = dest - orig\r\n    ang = np.degrees(np.arctan2(dxy[:, 1], dxy[:, 0]))\r\n    if fromNorth:\r\n        ang = np.mod((450.0 - ang), 360.)\r\n    return ang\r\n\r\n\r\ndef near_tbl(a, b=None, N=1):\r\n    """"""Return the coordinates and distance to the nearest N points within\r\n    an 2D numpy array, `a`, with optional ordering of the inputs.\r\n\r\n    Parameters:\r\n    -----------\r\n\r\n    `e_dist`, `fc_info`, and `tweet` from arcpytools\r\n\r\n    -`a` : array\r\n        shape coordinates extracted from a point array\r\n    -`b` : array or None\r\n        if b is None, then within file differences are used, otherwise provide\r\n        another set of coordinates to do between file distances\r\n    -`N` : number\r\n        the closest N distances and angles to calculate\r\n\r\n    Returns:\r\n    -------\r\n\r\n    A structured array containing Origin, Dest and Dist_FT (from - to points)\r\n    """"""\r\n    # ---- Calculate the distance array ----\r\n    offset = False\r\n    if b is None:\r\n        b = np.copy(a)\r\n        offset = True\r\n    dist = e_dist(a, b, metric=\'euclidean\')\r\n    if dist.ndim == 1:\r\n        print(""do stuff"")\r\n        return dist\r\n    if offset:\r\n        np.fill_diagonal(dist, np.inf)\r\n    n, m = dist.shape\r\n    rows, cols = np.triu_indices(n, offset, m)  # shape and diag. offset\r\n    idx = dist[rows, cols].argsort()   # slicing with [:2] gives overall 2\r\n    r, c = rows[idx], cols[idx]\r\n    d = dist[r, c]\r\n    az0 = line_dir(a[r], b[c], fromNorth=True)\r\n    az1 = line_dir(b[c], a[r], fromNorth=True)\r\n    z0 = list(zip(r, c, a[r, 0], a[r, 1], b[c, 0], b[c, 1], d, az0))\r\n    z1 = list(zip(c, r, b[c, 0], b[c, 1], a[r, 0], a[r, 1], d, az1))\r\n    dt = [(\'Orig\', \'<i4\'), (\'Dest\', \'<i4\'),\r\n          (\'X_orig\', \'<f8\'), (\'Y_orig\', \'<f8\'),\r\n          (\'X_dest\', \'<f8\'), (\'Y_dest\', \'<f8\'),\r\n          (\'OD_dist\', \'<f8\'), (\'Azim_N\', \'<f8\')]\r\n    ft = np.array(z0 + z1, dtype=dt)\r\n    ft_idx = np.argsort(ft, order=(\'Orig\', \'OD_dist\'))  # sort by Orig first\r\n    ft2 = ft[ft_idx]\r\n    num_pnts = len(a)\r\n    nt = np.asanyarray([ft2[ft2[\'Orig\'] == i][:N] for i in range(num_pnts)])\r\n    nt = np.asanyarray([i for i in nt if len(i) > 0])\r\n    nt = nt.reshape((np.product(nt.shape),))\r\n    return nt\r\n\r\n\r\n# ---- Run the analysis ----\r\n#\r\ndef tool():\r\n    """""" run the tool""""""\r\n    in_fc = sys.argv[1]\r\n    N = int(sys.argv[2])\r\n    out_tbl = sys.argv[3]\r\n    args = [script, in_fc, N, out_tbl]\r\n    tweet(frmt.format(*args))           # call tweet\r\n    a = to_array(in_fc)                 # call to_array\r\n    nt = near_tbl(a, b=None, N=N)       # call near_tbl\r\n    tweet(""\\nnear table\\n{}"".format(nt.reshape(nt.shape[0], 1)))\r\n    arcpy.da.NumPyArrayToTable(nt, out_tbl)\r\n\r\n\r\nfrmt = """"""\\n\r\n:Running ... {}\r\n:Using ..... {}\r\n:Finding ... {} closest points\r\n:Producing.. {}\\n\r\n""""""\r\nif len(sys.argv) == 1:\r\n    fn = r\'C:\\GIS\\points\\points.gdb\\Fishnet_label\'\r\n    a = arcpy.da.FeatureClassToNumPyArray(fn, \'Shape\')\r\n    a = a[\'Shape\']\r\nelse:\r\n    tool()\r\n\r\n\r\n# ---------------------------------------------------------------------\r\nif __name__ == ""__main__"":\r\n    """"""Main section...   """"""\r\n\r\n#    print(""Script... {}"".format(script))\r\n'"
all_scripts/code_grid.py,5,"b'# -*- coding: utf-8 -*-\r\n""""""\r\ncode_grid\r\n=========\r\n\r\nScript :   code_grid.py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified : 2018-08-19\r\n\r\nPurpose:  produce a spreadsheet-like numbering system for \'grid\' cells\r\n\r\nThis use padding A01 to facilitate sorting.\r\nIf you want a different system change\r\n>>> >>> ""{}{}"".format(UC[c], r)    # A1 to whatever, no padding\r\n>>> ""{}{:02.0f}"".format(UC[c], r)  # A01 to ..99\r\n>>> ""{}{:03.0f}"".format(UC[c], r)  # A001 to A999\r\n>>> # etc\r\n---------------------------------------------------------------------\r\n""""""\r\n\r\nimport sys\r\nimport numpy as np\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\ndef code_grid(cols=1, rows=1, zero_based=False, shaped=True, bottom_up=False):\r\n    """"""produce spreadsheet like labelling, either zero or 1 based\r\n    :  zero - A0,A1  or ones - A1, A2..\r\n    :  dig = list(\'0123456789\')  # string.digits\r\n    : import string .... string.ascii_uppercase\r\n    """"""\r\n    alph = list("" ABCDEFGHIJKLMNOPQRSTUVWXYZ"")\r\n    UC = [(""{}{}"").format(alph[i], alph[j]).strip()\r\n          for i in range(27)\r\n          for j in range(1,27)]\r\n    z = [1, 0][zero_based]\r\n    rc = [1, 0][zero_based]\r\n    c = [""{}{:02.0f}"".format(UC[c], r) # pull in the column heading\r\n         for r in range(z, rows + rc)  # label in the row letter\r\n         for c in range(cols)]         # label in the row number\r\n    c = np.asarray(c)\r\n    if shaped:\r\n        c = c.reshape(rows, cols)\r\n        if bottom_up:\r\n            c = np.flipud(c)\r\n    return c\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n    c0 = code_grid(cols=5, rows=3, zero_based=False, shaped=True, bottom_up=False)\r\n    print(c0)'"
all_scripts/compass_angles.py,12,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   compass_angles.py\r\n:\r\n:Author:   Dan.Patterson@carleton.ca\r\n:\r\n:Modified: 2016-09-25\r\n:\r\n:Purpose:  Convert angles, in degrees, to compass designations.\r\n:\r\n:Functions:  help(<function name>) for help\r\n:  compass - the conversion function\r\n:  _demo  -  demo function ...\r\n:\r\n:Notes:\r\n:  Cardinal direction and degree ranges.........\r\n:  N 348.75- 11.25  NNE  11.25- 33.75  NE  33.75- 56.2   ENE  56.25- 78.75\r\n:  E  78.75-101.25  ESE 101.25-123.75  SE 123.75-146.25  SSE 146.25-168.75\r\n:  S 168.75-191.25  SSW 191.25-213.75  SW 213.75-236.25  WSW 236.25-258.75\r\n:  W 258.75-281.25  WNW 281.25-303.75  NW 303.75-326.25  NNW 326.25-348.75\r\n:\r\n:  np.arange(11.25, 360., 22.5)  #  Generator which yields...\r\n:  array([ 11.250,  33.750,  56.250 ... 303.750, 326.250,  348.750])\r\n:\r\n:References\r\n:\r\n""""""\r\n# ---- imports, formats, constants ----\r\n\r\nimport sys\r\nimport numpy as np\r\nfrom textwrap import dedent\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\r\n      \'float\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2,\r\n                    suppress=True, threshold=100, formatter=ft)\r\n\r\nscript = sys.argv[0]\r\n\r\n# ---- functions ----\r\n\r\n\r\ndef compass(angle):\r\n    """"""Return the compass direction based on supplied angle.\r\n\r\n    Requires:\r\n    --------\r\n    `angle` : number\r\n        Angle(s) in degrees, no check made for other formats.  a single value,\r\n        list or np.ndarray can be used as input.\r\n\r\n        Angles are assumed to be from compass north, alter to suit.\r\n\r\n    Returns:\r\n    -------\r\n        The compass direction.\r\n    Notes:\r\n    -----\r\n        Compass ranges can be altered to suit the desired format.\r\n        See various wiki\'s for other options.\r\n\r\n        This incarnation uses 22.5 degree ranges with the compass centered\r\n        on the range.\r\n\r\n        ie. N  between 348.75 and 11.25 degrees, range equals 22.5)\r\n\r\n    """"""\r\n    c = np.array([\'N\', \'NNE\', \'NE\', \'ENE\', \'E\', \'ESE\', \'SE\', \'SSE\',\r\n                  \'S\', \'SSW\', \'SW\', \'WSW\', \'W\', \'WNW\', \'NW\', \'NNW\', \'N\'])\r\n    a = np.arange(11.25, 360., 22.5)\r\n    if isinstance(angle, (float, int, list, np.ndarray)):\r\n        angle = np.atleast_1d(angle)\r\n    comp_dir = c[np.digitize(angle, a)]\r\n    if len(comp_dir) == 1:\r\n        comp_dir[0]\r\n    return comp_dir\r\n\r\n\r\ndef run_demo():\r\n    """"""A sample run of compass returning compass notations for 20 deg\r\n    increments.  Change to suit\r\n    """"""\r\n    angles = np.arange(0, 360, 20)\r\n    rose = compass(angles)\r\n    dt = [(""Angle"", ""<f8""), (""Code"", ""U5"")]\r\n    comp_rose = np.asarray(list(zip(angles, rose)), dtype=dt)\r\n    comp_rose[\'Angle\'] = angles\r\n    comp_rose[\'Code\'] = rose\r\n    print(""\\nCompass rose examples\\n{}"".format(comp_rose))\r\n    return comp_rose\r\n\r\n# n = np.arange(-10, 10.)\r\n# v = np.where(n >= 0, np.PZERO, np.NZERO)\r\n\r\n# a = ""Hello there my friend""\r\n# b = """".join([[i, ""\\n""][i == "" ""] for i in a])\r\n# print(b)\r\n\r\n\r\n# ----------------------\r\nif __name__ == ""__main__"":\r\n    """"""Main section...   """"""\r\n    # print(""Script... {}"".format(script))\r\n    comp_rose = run_demo()\r\n'"
all_scripts/concatenate_flds.py,18,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\nconcatenate_flds\r\n================\r\n\r\nScript : concatenate_flds.py\r\n\r\nAuthor :   Dan.Patterson@carleton.ca\r\n\r\nModified : 2018-03-31\r\n\r\nPurpose : tools for working with numpy arrays\r\n  Concatenate fields from fields in a geodatabase table.\r\n\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=3, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\ndef tweet(msg):\r\n    """"""Print a message for both arcpy and python.\r\n    : msg - a text message\r\n    """"""\r\n    m = ""\\n{}\\n"".format(msg)\r\n    arcpy.AddMessage(m)\r\n    print(m)\r\n    print(arcpy.GetMessages())\r\n\r\n\r\ndef _cleanup(arrs, strip_list):\r\n    """"""clean the arrays if needed""""""\r\n    cleaned = []\r\n    for ar in arrs:\r\n        if ar.dtype.kind in (\'f\', \'i\'):\r\n            tmp = ar.astype(np.unicode_)\r\n        else:\r\n            tmp = ar\r\n        for i in strip_list:\r\n            tmp = np.char.strip(tmp)\r\n            tmp = np.char.replace(tmp, str(i), """")\r\n        cleaned.append(tmp)\r\n    return cleaned\r\n\r\n\r\ndef concat_flds(arrs, sep=\'space\', name=None, strip_list=None, with_ids=True):\r\n    """"""Concatenate a sequence of arrays to string format and return a\r\n    structured array or ndarray\r\n\r\n    Parameters:\r\n    -----------\r\n    - `arrs` : a list of single arrays of the same length\r\n    - `sep` : the separator to separate the arrays\r\n    - `name` : used for structured array\r\n    """"""\r\n    def cleanup(arrs, strip_list):\r\n        """"""clean the arrays if needed""""""\r\n        cleaned = []\r\n        for ar in arrs:\r\n            if ar.dtype.kind in (\'f\', \'i\'):\r\n                tmp = ar.astype(np.unicode_)\r\n            else:\r\n                tmp = ar\r\n            for i in strip_list:\r\n                tmp = np.char.replace(tmp, str(i), """")\r\n                tmp = np.char.strip(tmp)\r\n            cleaned.append(tmp)\r\n        return cleaned\r\n    # ---- Main section\r\n    N = len(arrs)\r\n    if sep == \'space\':\r\n        sep = \' \'\r\n    elif sep == \'comma\':\r\n        sep = \', \'\r\n    elif sep == \'none\':\r\n        sep = \'\'\r\n    if N < 2:\r\n        return arrs\r\n    if strip_list is None:\r\n        cleaned = arrs\r\n    else:\r\n        cleaned = cleanup(arrs, strip_list)\r\n    a, b = cleaned[0], cleaned[1]\r\n    c = [""{}{}{}"".format(i, sep, j) for i, j in list(zip(a, b))]\r\n    if N > 2:\r\n        for i in range(2, len(cleaned)):\r\n            c = [""{}{}{}"".format(i, sep, j)\r\n                 for i, j in list(zip(c, cleaned[i]))]\r\n    c = np.asarray(c)\r\n    sze = c.dtype.str\r\n    if name is not None:\r\n        c.dtype = [(name, sze)]\r\n    else:\r\n        name = \'concat\'\r\n        c.dtype = [(name, sze)]\r\n    if with_ids:\r\n        tmp = np.copy(c)\r\n        dt = [(\'IDs\', \'<i8\'), (name, sze)]\r\n        c = np.empty((tmp.shape[0], ), dtype=dt)\r\n        c[\'IDs\'] = np.arange(1, tmp.shape[0] + 1)\r\n        c[name] = tmp\r\n    return c\r\n\r\n\r\n# ---- Run options: _demo or from _tool\r\n#\r\ndef _demo():\r\n    """"""Code to run if in demo mode\r\n    """"""\r\n    # in_tbl = r""C:\\Git_Dan\\arraytools\\Data\\numpy_demos.gdb\\sample_10k""\r\n    in_tbl = r""C:\\GIS\\Joe_address\\Joe_address\\Joe_address.gdb\\Addr_summary""\r\n    in_flds = [\'Street\', \'Len_range\', \'Test_txt\', \'Len_range2\']\r\n    nv = np.iinfo(np.int32).min  # use smallest int...it gets cast as needed\r\n    a = arcpy.da.TableToNumPyArray(in_tbl, in_flds,\r\n                                   skip_nulls=False,\r\n                                   null_value=nv)\r\n    a0 = a[\'Street\']\r\n    a1 = a[\'Len_range\']\r\n    a2 = a[\'Test_txt\']\r\n    a3 = a[\'Len_range2\']\r\n    arrs = [a0, a1, a2, a3]\r\n#    a = [np.arange(5, dtype=\'int\'),\r\n#            np.arange(10, 5, -1, dtype=\'float\'),\r\n#            np.array([\'a\', \'b\', \'c\', \'d\', \'e\'])]\r\n    strip_list = [nv, \'None\', None, """", "",""]\r\n    c = concat_flds(arrs, sep="" "", name=""Test"",\r\n                    strip_list=strip_list, with_ids=True)\r\n#    arcpy.da.ExtendTable(in_tbl, \'OBJECTID\', out_array, \'IDs\')\r\n    return arrs, c\r\n\r\n\r\ndef _tool():\r\n    """"""run when script is from a tool\r\n    """"""\r\n    in_tbl = sys.argv[1]\r\n    in_flds = sys.argv[2]\r\n    fld_sep = ""{}"".format(sys.argv[3])\r\n    strip_list = sys.argv[4]\r\n    out_fld = sys.argv[5]\r\n\r\n    if \';\' in in_flds:\r\n        in_flds = in_flds.split(\';\')\r\n    else:\r\n        in_flds = [in_flds]\r\n\r\n    desc = arcpy.da.Describe(in_tbl)\r\n    tbl_path = desc[\'path\']\r\n    fnames = [i.name for i in arcpy.ListFields(in_tbl)]\r\n    if out_fld in fnames:\r\n        out_fld += \'dup\'\r\n    out_fld = arcpy.ValidateFieldName(out_fld, tbl_path)\r\n    args = [in_tbl, in_flds, out_fld, tbl_path]\r\n    msg = ""in_tbl {}\\nin_fld {}\\nout_fld  {}\\ntbl_path  {}"".format(*args)\r\n    tweet(msg)\r\n    oid = \'OBJECTID\'\r\n    vals = [oid] + in_flds\r\n    nv = np.iinfo(np.int32).min  # use smallest int...it gets cast as needed\r\n    arr = arcpy.da.TableToNumPyArray(in_tbl, vals,\r\n                                     skip_nulls=False,\r\n                                     null_value=nv)\r\n    tweet(""{!r:}"".format(arr))\r\n    #\r\n    # ---- process arrays from the fields, concatenate, and ExtendTable ----\r\n    arrs = [arr[i] for i in in_flds]\r\n    out_array = concat_flds(arrs, sep=fld_sep, name=out_fld,\r\n                            strip_list=strip_list, with_ids=True)\r\n    arcpy.da.ExtendTable(in_tbl, \'OBJECTID\', out_array, \'IDs\')\r\n    del in_tbl, arr, out_array\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# .... final code section producing the featureclass and extendtable\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    arrs, c = _demo()\r\n    frmt = ""Result...\\n{}""\r\n    print(frmt.format(c))\r\nelse:\r\n    testing = False\r\n    _tool()\r\n#\r\nif not testing:\r\n    print(\'Concatenation done...\')\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n'"
all_scripts/conversion.py,20,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\nconversion\r\n==========\r\n\r\nScript :   conversion.py\r\n\r\nAuthor :   Dan.Patterson@carleton.ca\r\n\r\nModified : 2018-04-18\r\n\r\nPurpose :  conversion tools for working with numpy arrays\r\n\r\nFunctions\r\n---------\r\n\r\n>>> __all__ = [\'arr2tuple\',\r\n               \'recarray2dict\',\r\n               \'table2dict\',\r\n               \'read_npy\',\r\n               \'save_as_txt\',\r\n               \'arr_tif_tf\',    # array to tiff using tifffile\r\n               \'arr_tif_cb\',    # .... using CompositeBands\r\n               \'tifffile_arr\',  # tiff to array using tifffile\r\n               \'_demo_tif\'\r\n               ]\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport warnings\r\nfrom textwrap import dedent\r\nimport numpy as np\r\nfrom numpy.lib import format\r\n#import tifffile\r\n#from tifffile import imread, TiffFile\r\nimport arcpy\r\n\r\nwarnings.filterwarnings(\'ignore\')\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=3, linewidth=80, precision=2, suppress=True,\r\n                    threshold=200, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n__all__ = [\'arr2tuple\',\r\n           \'recarray2dict\',\r\n           \'table2dict\',\r\n           \'read_npy\',\r\n           \'save_as_txt\',\r\n           \'arr_tif_tf\',    # array to tiff using tifffile\r\n           \'arr_tif_cb\',    # .... using CompositeBands\r\n           \'tifffile_arr\',  # tiff to array using tifffile\r\n           \'_demo_tif\'\r\n           ]\r\n\r\n# ---- general functions -----------------------------------------------------\r\n#\r\ndef arr2tuple(a):\r\n    """"""Convert an array to tuples, a convenience function when array.tolist()\r\n    doesn\'t provide the necessary structure since lists and lists of lists\r\n    aren\'t hashable\r\n    """"""\r\n    return tuple(map(tuple, a.tolist()))\r\n\r\n\r\n# ---- array or table to dictionary ------------------------------------------\r\n#\r\ndef recarray2dict(a, flds=None, to_list=False):\r\n    """"""Return a numpy.recarray as dictionary\r\n\r\n    a :\r\n        a NumPy structured or recarray with/without mixed dtypes\r\n    to_list :\r\n        if True, the dictionary values are arrays, if False, the values\r\n        are returned as lists\r\n    """"""\r\n    if flds is None:\r\n        flds = a.dtype.names\r\n    if to_list:\r\n        tbl_dct = {nme: a[nme].tolist() for nme in flds}\r\n    else:\r\n        tbl_dct = {nme: a[nme] for nme in flds}\r\n    return tbl_dct\r\n\r\n\r\ndef table2dict(in_tbl, flds=None, to_list=True):\r\n    """"""Searchcursor to dictionary\r\n\r\n    in_tbl : table\r\n        a geodatabase table\r\n    flds : list or tuple\r\n        a list/tuple of field names, if None, then all the fields are used\r\n    to_list : boolean\r\n        True, then a list of the field values is returned.\r\n\r\n        False, an array subset from the array \'a\' is returned.\r\n    """"""\r\n    if flds is None:\r\n        flds = [f.name for f in arcpy.ListFields(in_tbl)]\r\n    elif not isinstance(flds, (list, tuple)):\r\n        flds = [flds]\r\n    a = arcpy.da.TableToNumPyArray(in_tbl, flds)\r\n    if to_list:\r\n        tbl_dct = {nme: a[nme].tolist() for nme in flds}\r\n    else:\r\n        tbl_dct = {nme: a[nme] for nme in flds}\r\n    return tbl_dct\r\n\r\n\r\n# ---- array to and from npy ------------------------------------------------\r\n#\r\ndef read_npy(fp, prn=False):\r\n    """""" Read an npy file quickly\r\n\r\n    fp : string\r\n        The file path: ""c:/temp/a01.npy""\r\n    prn : boolean\r\n        obtain full information if True\r\n\r\n    Requires:\r\n    ---------\r\n    from numpy.lib import format\r\n\r\n    Notes:\r\n    -------\r\n    shortcut ... np.load(""c:/temp/a01.npy"")\r\n    """"""\r\n    frmt = """"""\r\n    ---- npy reader ---------------------------------------------------------\r\n    File  {}\r\n    Shape {},  C-contig {},  dtype {}\r\n    Magic {}\r\n    -------------------------------------------------------------------------\r\n    """"""\r\n    with open(fp, \'rb\') as f:\r\n        major, minor = format.read_magic(f)\r\n        mag = format.magic(major, minor)\r\n        shp, is_fortran, dt = format.read_array_header_1_0(f)\r\n        count = np.multiply.reduce(shp, dtype=np.int64)\r\n        BUFFER_SIZE = 2**18\r\n        max_read_count = BUFFER_SIZE // min(BUFFER_SIZE, dt.itemsize)\r\n        array = np.ndarray(count, dtype=dt)\r\n        for i in range(0, count, max_read_count):\r\n            cnt = min(max_read_count, count - i)\r\n            read_size = int(cnt * dt.itemsize)\r\n            data = format._read_bytes(f, read_size, ""array data"")\r\n            array[i:i + cnt] = np.frombuffer(data, dtype=dt, count=cnt)\r\n        array.shape = shp\r\n    if prn:\r\n        print(dedent(frmt).format(fp, shp, (not is_fortran), dt, mag))\r\n    return array\r\n\r\n\r\ndef save_as_txt(fname, a):\r\n    """"""Save a numpy array as a text file determining the format from the\r\n    data type.\r\n\r\n    Reference:\r\n    ----------\r\n    from numpy savetxt\r\n\r\n    >>> savetxt(fname, X, fmt=\'%.18e\', delimiter=\' \',\r\n                newline=`\\\\n`, header=\'\', footer=\'\', comments=\'# \')\r\n\r\n    - fmt : \'%[flag]width[.precision]specifier\'\r\n    - fmt=\'%.18e\'\r\n    """"""\r\n    dt_kind = a.dtype.kind\r\n    l_sze = max(len(str(a.max())), len(str(a.min())))\r\n    frmt = \'%{}{}\'.format(l_sze, dt_kind)\r\n    hdr = ""dtype: {} shape: {}"".format(a.dtype.str, a.shape)\r\n    np.savetxt(fname, a, fmt=frmt, delimiter=\' \',\r\n               newline=\'\\n\', header=hdr, footer=\'\', comments=\'# \')\r\n\r\n\r\n# ---- array to and from tif files ------------------------------------------\r\n#\r\ndef arr_tif_tf(a, fname):\r\n    """"""Convert a NumPy array to a tiff using tifffile.py\r\n\r\n    Requires:\r\n    ---------\r\n    from tifffile import imread, imsave, TiffFile\r\n\r\n    help(TiffFile.geotiff_metadata)\r\n\r\n    imsave :\r\n        imsave(file, data=None, shape=None, dtype=None, bigsize=2**32-2**25,\r\n              **kwargs)\r\n\r\n    file - filename with *.tif\r\n\r\n    data - array\r\n\r\n    shape - if creating an empty array\r\n\r\n    >>> a = np.arange(4*5*6).reshape(4, 5, 6)\r\n    >>> imsave(\'c:/temp/temp.tif\', a)\r\n    >>> b =imread(\'c:/temp/temp.tif\')\r\n    >>> np.all(a == b)  # True\r\n\r\n    GeoTiff, World files:\r\n    ---------------------\r\n    [1]\r\n    https://github.com/blink1073/tifffile  source github page\r\n    [2]\r\n    http://trac.osgeo.org/geotiff/\r\n    [3]\r\n    https://en.wikipedia.org/wiki/World_file\r\n\r\n    imsave(\'temp.tif\', data, compress=6, metadata={\'axes\': \'TZCYX\'})\r\n\r\n    Parameters \xe2\x80\x98append\xe2\x80\x99, \xe2\x80\x98byteorder\xe2\x80\x99, \xe2\x80\x98bigtiff\xe2\x80\x99, \xe2\x80\x98software\xe2\x80\x99, and \xe2\x80\x98imagej\xe2\x80\x99,\r\n    are passed to the TiffWriter class.\r\n\r\n    Parameters \xe2\x80\x98photometric\xe2\x80\x99, \xe2\x80\x98planarconfig\xe2\x80\x99, \xe2\x80\x98resolution\xe2\x80\x99, \xe2\x80\x98compress\xe2\x80\x99,\r\n    \xe2\x80\x98colormap\xe2\x80\x99, \xe2\x80\x98tile\xe2\x80\x99, \xe2\x80\x98description\xe2\x80\x99, \xe2\x80\x98datetime\xe2\x80\x99, \xe2\x80\x98metadata\xe2\x80\x99, \xe2\x80\x98contiguous\xe2\x80\x99\r\n    and \xe2\x80\x98extratags\xe2\x80\x99 are passed to the TiffWriter.save function.\r\n    """"""\r\n    import warnings\r\n    warnings.filterwarnings(\'ignore\')\r\n    from tifffile import imsave\r\n    imsave(fname, a)\r\n\r\n\r\ndef arr_tif_cb(a, fname, LL_X=0, LL_Y=0, cell_size=1, no_data=None):\r\n    """"""Array to tif using esri compositebands\r\n    : a - array, at least 2D\r\n    : fname - full filename of tif to save\r\n    : LL_X, Y - coordinate of the lower left in real world coordinates\r\n    : cell_size - guess... things only make sense using projected coordinates\r\n    : no_data - specify the value if any, one will be assigned if None\r\n    """"""\r\n    arcpy.env.workspace = \'in_memory\'\r\n    pnt = arcpy.Point(LL_X, LL_Y)\r\n    rasters = []\r\n    if no_data is None:\r\n        if a.dtype.kind == \'i\':\r\n            no_data = np.iinfo(a.dtype.type).min\r\n        elif a.dtype.kind == \'f\':\r\n            no_data = np.finfo(a.dtype.type).min\r\n        else:\r\n            no_data = a.min() - 1\r\n    for i in range(a.shape[0]):\r\n        ai = a[i]\r\n        rast = arcpy.NumPyArrayToRaster(ai, pnt, cell_size, cell_size, no_data)\r\n        r_name = ""in_memory/a{:0>3}.tif"".format(i)\r\n        rasters.append(r_name)\r\n        rast.save(r_name)\r\n    rasters = "";"".join([i for i in rasters])\r\n    #\r\n    arcpy.management.CompositeBands(rasters, fname)\r\n    # ----\r\n    return\r\n\r\n\r\ndef tifffile_arr(fname, verbose=False):\r\n    """"""Convert tif to array using tifffile\r\n    :\r\n    :Source: tifffile # http://www.lfd.uci.edu/~gohlke/code/tifffile.py.html\r\n    :Requires: from tifffile import imread, TiffFile\r\n    :  fname - full tiff file path and filename\r\n    :  verbose - True to print all information gleaned\r\n    :Useage:\r\n    :(1) a = imread(fname) ... or\r\n    :(2) TiffFile.asarray(self, key=None, series=None, out=None, maxworkers=1)\r\n    :    key - int, slice, or sequence of page indices\r\n    :          Defines which pages to return as array.\r\n    :    series - int or TiffPageSeries\r\n    :    out - array if None or filename\r\n    :    maxworkers = default 1, number of threads to use to get data\r\n    :Extra info:\r\n    : kys = tif.__dict__.keys()\r\n    :     for k in kys:\r\n    :         print(""{!s:<15}: {!s:<30}"".format(k, tif.__dict__[k]))\r\n    """"""\r\n    with TiffFile(fname) as tif:\r\n        if verbose:\r\n            print(""\\nTiff file: {}\\nflags: {}"".format(fname, tif.flags))\r\n        if tif.is_shaped and verbose:\r\n            print(""Shape info: {}\\n"".format(tif.shaped_metadata))\r\n        if tif.is_geotiff and verbose:\r\n            print(""Geotiff info:"")\r\n            d = tif.geotiff_metadata\r\n            for key in d.keys():\r\n                print(""- {}:   {}"".format(key, d[key]))\r\n        #\r\n        a = tif.asarray()\r\n        #\r\n        if tif.is_tiled:\r\n            a = np.rollaxis(a, axis=2, start=0)\r\n    return a, tif  # uncomment and return tif for testing\r\n\r\n\r\ndef rast_np(fp):\r\n    """"""shortcut implementation to RasterToNumPyArray\r\n    : fp - file path and name\r\n    : LL_X, LL_Y - lower left coordinates\r\n    """"""\r\n    a = arcpy.RasterToNumPyArray(fp)\r\n    return a\r\n\r\n\r\ndef _demo_tif():\r\n    """"""Code to run if in demo mode\r\n    """"""\r\n#    a = np.arange(5*3*4, dtype=np.int8).reshape(5, 3, 4)  # int8\r\n#    a = np.arange(5*3*4, dtype=np.int16).reshape(5, 3, 4)  # int16\r\n#    a = np.arange(5*3*4, dtype=np.int32).reshape(5, 3, 4)  # int32\r\n#    a = np.arange(5*3*4, dtype=np.int64).reshape(5, 3, 4)  # int64\r\n#    a = np.arange(5*3*4, dtype=np.float16).reshape(5, 3, 4)  # float16\r\n#    a = np.arange(5*3*4, dtype=np.float32).reshape(5, 3, 4)  # float32\r\n    a = np.arange(5*3*4, dtype=np.float64).reshape(5, 3, 4)  # float64\r\n    return a\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo or uncomment other sections\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n#    a = _demo_tif()\r\n    # ---- tifffile section\r\n#    f = ""/Data/a30_tifffile.tif""\r\n#    f = ""/Data/rast_composite_9bands.tif""\r\n#    pth = script.split(""/"")[:-2]\r\n#    fname = ""/"".join(pth) + f\r\n    # ---- table section\r\n#    in_tbl = r""C:\\Git_Dan\\arraytools\\Data\\numpy_demos.gdb\\sample_10k""\r\n#    in_flds = [\'OBJECTID\', \'County\', \'Town\']\r\n#    a = arcpy.da.TableToNumPyArray(in_tbl, in_flds)\r\n'"
all_scripts/convert.py,0,"b'# -*- coding: utf-8 -*-\r\n""""""\r\n:Script:   convert.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2017-02-22\r\n:Purpose:\r\n\r\n:Requires:\r\n:\r\n:Notes:\r\n:\r\n:Functions:\r\n\r\n:References:\r\n\r\n""""""\r\n\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\nfrom arcpytools import fc_info, tweet, fc_array\r\n\r\n\'\'\'\r\ndef tweet(msg):\r\n    """"""Produce a message for both arcpy and python\r\n    : msg - a text message\r\n    """"""\r\n    m = ""\\n{}\\n"".format(msg)\r\n    arcpy.AddMessage(m)\r\n    print(m)\r\n    return None\r\n\r\n\r\ndef fc_info(in_fc):\r\n    """"""basic feature class information""""""\r\n    desc = arcpy.Describe(in_fc)\r\n    SR = desc.spatialReference      # spatial reference object\r\n    shp_fld = desc.shapeFieldName   # FID or OIDName, normally\r\n    oid_fld = desc.OIDFieldName     # Shapefield ...\r\n    return shp_fld, oid_fld, SR\r\n\r\n\r\n\r\ndef fc_array(in_fc, flds, allpnts):\r\n    """"""Convert featureclass to an ndarray...with optional fields besides the\r\n    :FID/OIDName and Shape fields.\r\n    :Syntax: read_shp(input_FC,other_flds, explode_to_points)\r\n    :   input_FC    shapefile\r\n    :   other_flds   ""*"", or specific fields [\'FID\',\'Shape\',\'SomeClass\', etc]\r\n    :   see:  FeatureClassToNumPyArray, ListFields for more information\r\n    """"""\r\n    out_flds = []\r\n    shp_fld, oid_fld, SR = fc_info(in_fc)  # get the base information\r\n    fields = arcpy.ListFields(in_fc)       # all fields in the shapefile\r\n    if flds == """":                   # return just OID and Shape field\r\n        out_flds = [oid_fld, shp_fld]     # FID and Shape field required\r\n    elif flds == ""*"":                # all fields\r\n        out_flds = [f.name for f in fields]\r\n    else:\r\n        out_flds = [oid_fld, shp_fld]\r\n        for f in fields:\r\n            if f.name in flds:\r\n                out_flds.append(f.name)\r\n    frmt = ""\\nRunning \'fc_array\' with...\\n{}\\nFields...{}\\nSR...{}""\r\n    args = [in_fc, out_flds, SR.name]\r\n    msg = frmt.format(*args)\r\n    tweet(msg)\r\n    a = arcpy.da.FeatureClassToNumPyArray(in_fc, out_flds, """", SR, allpnts)\r\n    # out it goes in array format\r\n    return a, out_flds, SR\r\n\'\'\'\r\n\r\nin_fc = sys.argv[1]\r\nfld_names = sys.argv[2]\r\npnt_type = sys.argv[3]\r\np4 = sys.argv[4]\r\nif pnt_type == \'all\':\r\n    allpnts = True\r\nelse:\r\n    allpnts = False\r\na, out_flds, SR = fc_array(in_fc, fld_names, allpnts)  # , fld_names, SR\r\nfrmt = """"""\r\nInput featureclass...\\n{}\\nSR...{}\\nFields...\\n{}\\nAll points? {}\r\nArray...\\n{!r:}\\n\r\n""""""\r\nmsg = frmt.format(in_fc, SR.name, out_flds, allpnts, a)\r\ntweet(msg)\r\narcpy.GetMessages()\r\n# del in_fc, fld_names, a, out_flds, SR\r\n\r\n\r\n# -------------------------------------------------------------------------\r\nif __name__ == ""__main__"":\r\n    """""" x  """"""\r\n    pass\r\n#    a, fld_names, SR = _demo()\r\n#    a, out_flds, SR = _demo()\r\n'"
all_scripts/cross_tab.py,28,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\ncross_tab\r\n=========\r\n\r\nScript :   cross_tab.py\r\n\r\nAuthor :   Dan.Patterson@carleton.ca\r\n\r\nModified : 2018-10-22\r\n\r\nPurpose :  Crosstabulate data\r\n\r\nNotes:\r\n\r\nReferences:\r\n-----------\r\n\r\n`<https://stackoverflow.com/questions/12983067/how-to-find-unique-vectors-of\r\n-a-2d-array-over-a-particular-axis-in-a-vectorized>`_.\r\n\r\n`<https://stackoverflow.com/questions/16970982/find-unique-rows-\r\nin-numpy-array>`_.\r\n\r\n`<http://stackoverflow.com/questions/38030054/create-adjacency-matrix-in-\r\npython-for-large-dataset>`_.\r\n\r\nnp.unique - in the newer version, they use flags to get the sums\r\n:\r\n""""""\r\nimport sys\r\nimport numpy as np\r\nfrom arraytools.fc_tools._common import * #(tweet, tbl_arr)\r\n# ---- others from above , de_punc, _describe, fc_info, fld_info, null_dict,\r\n#from arcpy import AddMessage, ListFields\r\n#from textwrap import indent\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\r\n      \'float\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2,\r\n                    suppress=True, threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')\r\n\r\nscript = sys.argv[0]\r\n\r\n\r\ndef crosstab(row, col, r_name=None, c_name=None, verbose=False):\r\n    """"""Crosstabulate 2 data arrays, shape (N,), using np.unique.\r\n    scipy.sparse has similar functionality and is faster for large arrays.\r\n\r\n    Requires:\r\n    --------\r\n    A 2D array of data with shape(N,) representing two variables\r\n\r\n    row : field/column\r\n        row variable\r\n    col : field/column\r\n        column variable\r\n\r\n    Useage:\r\n    ------\r\n    >>> float_min = np.finfo(np.float).min\r\n    >>> float_max = np.finfo(np.float).max\r\n    >>> int_min = np.iinfo(np.int_).min\r\n    >>> int_max = np.iinfo(np.int_).max\r\n    >>> f = r\'C:\\some\\path\\your.gdb\\your_featureclass\'\r\n    >>> null_dict = {\'Int_fld\': int_min, \'Float_fld\': float_min}  # None strings\r\n    >>> flds = [\'Int_field\', \'Txt01\', \'Txt02\']  # 2 text fields\r\n    >>> t = arcpy.da.TableToNumPyArray(in_table=f, field_names=flds,\r\n                                      skip_nulls=False)\r\n                                      # , null_value=null_dict) if needed\r\n    >>> rows = t[\'Txt01\']\r\n    >>> cols = t[\'Txt02\']\r\n    >>> ctab, a, result, r, c = crosstab(rows, cols, verbose=False)\r\n\r\n    Returns:\r\n    --------\r\n      ctab :\r\n          the crosstabulation result as row, col, count array\r\n      a :\r\n          the crosstabulation in a row, col, count, but filled out whether a\r\n          particular combination exists or not.\r\n      r, c :\r\n          unique values/names for the row and column variables\r\n    """"""\r\n    def _prn(r, c, r_name, c_name, a):\r\n        """"""fancy print formatting.\r\n        """"""\r\n        r = r.tolist()\r\n        r.append(\'Total\')\r\n        c = c.tolist()\r\n        c.append(\'Total\')\r\n        r_sze = max([len(str(i)) for i in r]) + 2\r\n        c_sze = [max(len(str(i)), 5) for i in c]\r\n        f_0 = \'{{!s:<{}}} \'.format(r_sze)\r\n        f_1 = (\'{{!s:>{}}} \'*len(c)).format(*c_sze)\r\n        frmt = f_0 + f_1\r\n        hdr = \'Row: {}\\nCol: {}\\n\'.format(r_name, c_name) + \'_\' * (r_sze)\r\n        txt = [frmt.format(hdr, *c)]\r\n        txt2 = txt + [frmt.format(r[i], *a[i]) for i in range(len(r))]\r\n        result = ""\\n"".join(txt2)\r\n        return result\r\n    #\r\n    r_name = [str(r_name), ""Row""][r_name is None]\r\n    c_name = [str(c_name), ""Col""][c_name is None]\r\n    dt = np.dtype([(r_name, row.dtype), (c_name, col.dtype)])\r\n    rc = np.asarray(list(zip(row, col)), dtype=dt)\r\n    r = np.unique(row)\r\n    c = np.unique(col)\r\n    u, idx, cnt = np.unique(rc, return_index=True, return_counts=True)\r\n    rcc_dt = u.dtype.descr\r\n    rcc_dt.append((\'Count\', \'<i4\'))\r\n    ctab = np.asarray(list(zip(u[r_name], u[c_name], cnt)), dtype=rcc_dt)\r\n    c0 = np.zeros((len(r), len(c)), dtype=np.int_)\r\n    rc = [[(np.where(r == i[0])[0]).item(),\r\n           (np.where(c == i[1])[0]).item()] for i in ctab]\r\n    for i in range(len(ctab)):\r\n        rr, cc = rc[i]\r\n        c0[rr, cc] = ctab[i][2]\r\n    tc = np.sum(c0, axis=0)\r\n    c1 = np.vstack((c0, tc))\r\n    tr = np.sum(c1, axis=1)\r\n    counts = np.hstack((c1, tr.reshape(tr.shape[0], 1)))\r\n    out_tbl = _prn(r, c, r_name, c_name, counts)\r\n    if verbose:\r\n        tweet(out_tbl)\r\n    return ctab, counts, out_tbl\r\n\r\ndef tabular_sum(a, r_name, c_name, val_name):\r\n    """"""array, row, col and value fields\r\n    """"""\r\n    rc = a[[r_name, c_name]]\r\n    sum_name = val_name +\'Sum\'\r\n    dt = rc.dtype.descr + [(sum_name, \'<i4\')]\r\n    uniq = np.unique(rc)\r\n    out_ = []\r\n    for u in uniq:\r\n        c0, c1 = u\r\n        idx = np.logical_and(a[r_name]==c0, a[c_name]==c1)\r\n        val = np.nansum(a[val_name][idx])\r\n        out_.append([c0, c1, val])\r\n    out_ = np.array(out_)\r\n    z = np.empty((len(out_),), dtype=dt)\r\n    z[r_name] = out_[:, 0]\r\n    z[c_name] = out_[:, 1]\r\n    z[sum_name] = out_[:, 2].astype(\'int32\')\r\n    return z\r\n#        cond =(np.where((a[row]==\'A\') & (a[\'Town\'] == \'A_\'), a[\'Time\'], 0))\r\n\r\n\r\n\r\n\r\nfrmt = """"""\\\r\nCrosstab results ....\r\n{}\\n\r\nThe array of counts/frequencies....\r\n{}\\n\r\nRow field:  {}\r\nCol field:  {}\\n\r\nRow and column headers...\r\n{}\r\n{}\\n\r\nAnd as a fancy output which can be saved to a csv file using\r\n....np.savetxt(\'c:/path/name.csv\', array, fmt= \'%s\', delimiter=\', \')\\n\r\n{}\r\n""""""\r\n# ---- crosstab from tool, uncomment for testing or tool use\r\n#if len(sys.argv) == 1:\r\n##    in_tbl = r""C:\\Git_Dan\\arraytools\\array_tools_testing\\array_tools.gdb\\pnts_2000""\r\n#    in_tbl = \'C:/Git_Dan/arraytools//Data/sample_10K.npy\'\r\n#    a = tbl_arr(in_tbl)  #arcpy.da.TableToNumPyArray(in_tbl, ""*"")\r\n#    row_fld = \'County\'\r\n#    col_fld = \'Town\'\r\n#    rows = a[row_fld]\r\n#    cols = a[col_fld]\r\n##    ctab, counts, out_tbl = crosstab(rows, cols, r_name=row_fld, c_name=col_fld, verbose=False)\r\n##    tweet(frmt.format(in_tbl, ctab, row_fld, col_fld, r, c, result))\r\n#else:\r\n#    in_tbl = sys.argv[1]\r\n#    row_fld = sys.argv[2]\r\n#    col_fld = sys.argv[3]\r\n#    flds = [row_fld, col_fld]\r\n#    t = tbl_arr(in_tbl)\r\n#    # arcpy.da.TableToNumPyArray(in_table=in_tbl, field_names=flds,\r\n#    #                            skip_nulls=False)  # , null_value=null_dict)\r\n#    rows = t[row_fld]\r\n#    cols = t[col_fld]\r\n#    ctab, a, result, r, c = crosstab(rows, cols, verbose=True)\r\n#    tweet(frmt.format(in_tbl, ctab, row_fld, col_fld, r, c, result))\r\n\r\nif __name__ == ""__main__"":\r\n    """"""run crosstabulation with data""""""\r\n#    ctab, counts, out_tbl = crosstab(a[\'County\'], a[\'Town\'], r_name=\'County\', c_name=\'Town\', verbose=False)\r\n#    ctab, a, result, r, c = _demo()\r\n'"
all_scripts/datamaker.py,32,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\ndatamaker\r\n=========\r\n\r\nScript:   datamaker.py\r\n\r\nAuthor:   Dan_Patterson@carleton.ca\r\n\r\nModified: 2018-11-03\r\n\r\nPurpose:  tools for working with numpy arrays\r\n\r\nUseage:\r\n\r\nReferences:\r\n\r\n`blog post_. https://community.esri.com/blogs/dan_patterson/2016/04/04/\r\nnumpy-lessons-6-creating-data-for-testing-purposes`\r\n\r\n---------------------------------------------------------------------\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nfrom functools import wraps\r\nimport numpy as np\r\nimport numpy.lib.recfunctions as rfn\r\n# Required imports\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\n# ----------------------------------------------------------------------------\r\n# ---- Required constants  ... see string module for others\r\nstr_opt = [\'0123456789\',\r\n           \'!""#$%&\\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\',\r\n           \'abcdefghijklmnopqrstuvwxyz\',\r\n           \'ABCDEFGHIJKLMNOPQRSTUVWXYZ\',\r\n           \'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\'\r\n           ]\r\n\r\n\r\n# ----------------------------------------------------------------------------\r\n# ---- decorators and helpers ----\r\ndef func_run(func):\r\n    """"""Prints basic function information and the results of a run.\r\n    :Required:  from functools import wraps\r\n    """"""\r\n    @wraps(func)\r\n    def wrapper(*args, **kwargs):\r\n        print(""\\nFunction... {}"".format(func.__name__))\r\n        print(""  args.... {}\\n  kwargs.. {}"".format(args, kwargs))\r\n        print(""  docs.... \\n{}"".format(func.__doc__))\r\n        result = func(*args, **kwargs)\r\n        print(""{!r:}\\n"".format(result))  # comment out if results not needed\r\n        return result                    # for optional use outside.\r\n    return wrapper\r\n\r\n\r\ndef time_deco(func):  # timing originally\r\n    """"""timing decorator function\r\n    print(""\\n  print results inside wrapper or use <return> ... "")\r\n    """"""\r\n    import time\r\n    from functools import wraps\r\n\r\n    @wraps(func)\r\n    def wrapper(*args, **kwargs):\r\n        t0 = time.perf_counter()        # start time\r\n        result = func(*args, **kwargs)  # ... run the function ...\r\n        t1 = time.perf_counter()        # end time\r\n        dt = t1 - t0\r\n        print(""\\nTiming function for... {}"".format(func.__name__))\r\n        if result is None:\r\n            result = 0\r\n        print(""  Time: {: <8.2e}s for {:,} objects"".format(dt, result))\r\n        return result                   # return the result of the function\r\n        return dt                       # return delta time\r\n    return wrapper\r\n\r\n\r\ndef strip_concatenate(in_flds, strip_list=["" "", "","", None]):\r\n    """"""Provide a list of fields ie [a, b, c] to strip spaces and remove nulls\r\n\r\n    - use: python parser\r\n    - syntax: strip_stuff(\'!a!, !b!, !c!]) assumed field names\r\n\r\n    """"""\r\n    fixed = []\r\n    fmt = []\r\n    for i in in_flds:\r\n        if i not in strip_list:\r\n            fixed.append(i)\r\n            fmt.append(""{}"")\r\n    frmt = "" "".join([f for f in fmt])\r\n    frmt.strip()\r\n    fixed = [str(i).strip() for i in fixed]\r\n    result = frmt.format(*fixed)\r\n    return result\r\n\r\n\r\n# ----------------------------------------------------------------------------\r\n# ---- functions\r\ndef concat_flds(a, flds=None, out_name=""Concat"", sep="" "", with_ids=True):\r\n    """"""Concatenate a sequence of fields to string format and return a\r\n    structured array or ndarray\r\n\r\n    Requires\r\n    --------\r\n\r\n    - arrs : a list single arrays of the same length\r\n    -  sep : the separator between lists\r\n    -  name : used for structured array\r\n    """"""\r\n    strip_list = ["" "", "","", None]\r\n    if (flds is None) or (a.dtype.names is None):\r\n        msg = ""Field/column names are required or need to exist in the array.""\r\n        print(msg)\r\n        return a\r\n    N = min(len(flds), len(a.dtype.names))\r\n    if N < 2:\r\n        print(""Two fields are required for concatenation"")\r\n        return a\r\n    s0 = [str(i) if i not in strip_list else \'\' for i in a[flds[0]]]\r\n    s1 = [str(i) if i not in strip_list else \'\' for i in a[flds[1]]]\r\n    c = [(""{}{}{}"".format(i, sep, j)).strip() for i, j in list(zip(s0, s1))]\r\n    if N > 2:\r\n        for i in range(2, len(flds)):\r\n            f = flds[i]\r\n            f = [str(i) if i not in strip_list else \'\' for i in a[flds[i]]]\r\n            c = [""{}{}{}"".format(i, sep, j) for i, j in list(zip(c, f))]\r\n    c = np.asarray(c)\r\n    sze = c.dtype.str\r\n    if out_name is not None:\r\n        c.dtype = [(out_name, sze)]\r\n    else:\r\n        out_name = \'f\'\r\n    if with_ids:\r\n        tmp = np.copy(c)\r\n        dt = [(\'IDs\', \'<i8\'), (out_name, sze)]\r\n        c = np.empty((tmp.shape[0], ), dtype=dt)\r\n        c[\'IDs\'] = np.arange(1, tmp.shape[0] + 1)\r\n        c[out_name] = tmp\r\n    return c\r\n\r\n\r\ndef colrow_txt(N=10, cols=2, rows=2, zero_based=True):\r\n    """"""  Produce spreadsheet like labels either 0- or 1-based.\r\n\r\n    Requires\r\n    --------\r\n    N : number\r\n        Number of records/rows to produce.\r\n    cols/rows : numbers\r\n        This combination will control the output of the values\r\n        cols=2, rows=2 - yields (A0, A1, B0, B1)\r\n        as optional classes regardless of the number of records being produced\r\n    zero-based : boolean\r\n        True for conventional array structure,\r\n        False for spreadsheed-style classes\r\n    """"""\r\n    if zero_based:\r\n        start = 0\r\n    else:\r\n        start = 1\r\n        rows = rows + 1\r\n    UC = (list(""ABCDEFGHIJKLMNOPQRSTUVWXYZ""))[:cols]  # see constants\r\n    dig = (list(\'0123456789\'))[start:rows]\r\n    cr_vals = [c + r for r in dig for c in UC]\r\n    colrow = np.random.choice(cr_vals, N)\r\n    return colrow\r\n\r\n\r\ndef rowcol_txt(N=10, rows=2, cols=2):\r\n    """"""  Produce array-like labels in a tuple format.\r\n    """"""\r\n    rc_vals = [""({},{})"".format(r, c)\r\n               for c in range(cols)\r\n               for r in range(rows)]\r\n    rowcol = np.random.choice(rc_vals, N)\r\n    return rowcol\r\n\r\n\r\ndef pnts_IdShape(N=10, x_min=0, x_max=10, y_min=0, y_max=10, simple=True):\r\n    """"""Create an array with a nested dtype which emulates a shapefile\'s\r\n    data structure.  This array is used to append other arrays to enable\r\n    import of the resultant into ArcMap.  Array construction, after hpaulj\r\n\r\n    http://stackoverflow.com/questions/32224220/\r\n        methods-of-creating-a-structured-array\r\n    """"""\r\n    Xs = np.random.randint(x_min, x_max, size=N)\r\n    Ys = np.random.randint(y_min, y_max, size=N)\r\n    IDs = np.arange(0, N)\r\n    c_stack = np.column_stack((IDs, Xs, Ys))\r\n    if simple:     # version 1  short version, optional form\r\n        dt = [(\'ID\', \'<i4\'), (\'X\', \'<f8\'), (\'Y\', \'<f8\')]\r\n        a = np.ones(N, dtype=dt)\r\n        a[\'ID\'] = c_stack[:, 0]\r\n        a[\'X\'] = c_stack[:, 1]         # this line too\r\n        a[\'Y\'] = c_stack[:, 2]\r\n    else:          # version 2\r\n        dt = [(\'ID\', \'<i4\'), (\'Shape\', ([(\'X\', \'<f8\'), (\'Y\', \'<f8\')]))]\r\n        a = np.ones(N, dtype=dt)\r\n        a[\'ID\'] = c_stack[:, 0]\r\n        a[\'Shape\'][\'X\'] = c_stack[:, 1]\r\n        a[\'Shape\'][\'Y\'] = c_stack[:, 2]\r\n    return a\r\n\r\n\r\ndef rand_text(N=10, cases=3, vals=str_opt[3]):\r\n    """"""Generate N samples from the letters of the alphabet denoted by the\r\n    number of cases.  If you want greater control on the text and\r\n    probability, see rand_case or rand_str.\r\n\r\n    vals:  see str_opt in required constants section\r\n    """"""\r\n    vals = list(vals)\r\n    txt_vals = np.random.choice(vals[:cases], N)\r\n    return txt_vals\r\n\r\n\r\ndef rand_str(N=10, low=1, high=10, vals=str_opt[3]):\r\n    """"""Returns N strings constructed from \'size\' random letters to form a\r\n    string\r\n\r\n    - create the cases as a list:  string.ascii_lowercase or ascii_uppercase\r\n    - determine how many letters. Ensure min <= max. Add 1 to max alleviate\r\n      low==high\r\n    - shuffle the case list each time through loop\r\n    """"""\r\n    vals = list(vals)\r\n    letts = np.arange(min([low, high]), max([low, high])+1)  # num letters\r\n    result = []\r\n    for i in range(N):\r\n        np.random.shuffle(vals)\r\n        size = np.random.choice(letts, 1)\r\n        result.append("""".join(vals[:size]))\r\n    result = np.array(result)\r\n    return result\r\n\r\n\r\ndef rand_case(N=10, cases=[""Aa"", ""Bb""], p_vals=[0.8, 0.2]):\r\n    """"""Generate N samples from a list of classes with an associated probability\r\n\r\n    - ensure: len(cases)==len(p_vals) and  sum(p_values) == 1\r\n    - small sample sizes will probably not yield the desired p-values\r\n    """"""\r\n    p = (np.array(p_vals))*N   # convert to integer\r\n    kludge = [np.repeat(cases[i], p[i]).tolist() for i in range(len(cases))]\r\n    case_vals = np.array([val for i in range(len(kludge))\r\n                          for val in kludge[i]])\r\n    np.random.shuffle(case_vals)\r\n    return case_vals\r\n\r\n\r\ndef rand_int(N=10, begin=0, end=10):\r\n    """"""Generate N random integers within the range begin - end\r\n    """"""\r\n    int_vals = np.random.randint(begin, end, size=(N))\r\n    return int_vals\r\n\r\n\r\ndef rand_float(N=10, begin=0, end=10):\r\n    """"""Generate N random floats within the range begin - end.\r\n\r\n    Technically, N random integers are produced then a random\r\n    amount within 0-1 is added to the value\r\n    """"""\r\n    float_vals = np.random.randint(begin, end-1, size=(N))\r\n    float_vals = float_vals + np.random.rand(N)\r\n    return float_vals\r\n\r\n\r\ndef blog_post():\r\n    """"""sample run""""""\r\n    N = 10000\r\n    id_shape = pnts_IdShape(N,\r\n                            x_min=300000,\r\n                            x_max=305000,\r\n                            y_min=5000000,\r\n                            y_max=5005000)\r\n    case1_fld = rand_case(N,\r\n                          cases=[\'A\', \'B\', \'C\', \'D\'],\r\n                          p_vals=[0.4, 0.3, 0.2, 0.1])\r\n    int_fld = rand_int(N, begin=0, end=10)\r\n    float_0 = rand_float(N, 5, 15)\r\n    float_1 = rand_float(N, 5, 20)\r\n    fld_names = [\'Case\', \'Observed\', \'Size\', \'Mass\']\r\n    fld_data = [case1_fld, int_fld, float_0, float_1]\r\n    arr = rfn.append_fields(id_shape, fld_names, fld_data, usemask=False)\r\n    return arr\r\n\r\n\r\ndef blog_post2(N=20):\r\n    """"""sample run\r\n    : import arcpy\r\n    : out_fc = r\'C:\\GIS\\A_Tools_scripts\\Graphing\\Graphing_tools\\\r\n    :            Graphing_tools.gdb\\data_01\'\r\n    : arcpy.da.NumPyArrayToFeatureClass(a, out_fc, [\'X\', \'Y\'])\r\n    """"""\r\n    ids = np.arange(1, N + 1)  # construct the base array of IDs to append to\r\n    ids = np.asarray(ids, dtype=[(\'Ids\', \'<i4\')])\r\n    int_fld = rand_int(N, begin=10, end=1000)\r\n    case1 = rand_case(N,\r\n                      cases=[\'N\', \'S\', \'E\', \'W\', \'\'],\r\n                      p_vals=[0.1, 0.1, 0.2, 0.2, 0.4])\r\n    case2 = rand_case(N,\r\n                      cases=[\'Maple\', \'Oak\', \'Elm\', \'Pine\', \'Spruce\'],\r\n                      p_vals=[0.3, 0.15, 0.2, 0.25, 0.1])\r\n    case3 = rand_case(N,\r\n                      cases=[\'Ave\', \'St\', \'Crt\'],\r\n                      p_vals=[0.3, 0.6, 0.1])\r\n    case4 = rand_case(N,\r\n                      cases=[\'Carp\', \'Almonte\', \'Arnprior\', \'Carleton Place\'],\r\n                      p_vals=[0.3, 0.3, 0.2, 0.2])\r\n    fld_names = [\'Str_Number\', \'Prefix\', \'Str_Name\', \'Str_Type\', \'Town\']\r\n    fld_data = [int_fld, case1, case2, case3, case4]\r\n    arr = rfn.append_fields(ids, fld_names, fld_data, usemask=False)\r\n    return arr\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""create ID,Shape,{txt_fld,int_fld...of any number}\r\n    """"""\r\n    a = blog_post2(N=20)\r\n#    a = np.array([\'a11\', \'b11\', \'c12\', \'a13\', \'b15\', \'c15\'])\r\n#\r\n#    check = np.array([\'11\', \'12\', \'13\'])\r\n#\r\n#    is_there = np.asarray([[np.char.rfind(i, val) for val in check]\r\n#                          for i in a]).max(axis=1)\r\n'"
all_scripts/densify_geom.py,14,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\ndensify_geom\r\n============\r\n\r\nScript : densify_geom.py\r\n\r\nAuthor : Dan.Patterson@carleton.ca\r\n\r\nModified : 2018-03-31\r\n\r\nPurpose : Densify geometry by a factor.\r\n\r\nNotes:\r\n------\r\n\r\nUses functions from \'arraytools\'.  These have been consolidated here.\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\n# ---- array functions -------------------------------------------------------\r\n#\r\ndef _flat_(a_list, flat_list=None):\r\n    """"""Change the isinstance as appropriate\r\n\r\n    Flatten an object using recursion\r\n    see: itertools.chain() for an alternate method of flattening.\r\n    """"""\r\n    if flat_list is None:\r\n        flat_list = []\r\n    for item in a_list:\r\n        if isinstance(item, (list, tuple, np.ndarray, np.void)):\r\n            _flat_(item, flat_list)\r\n        else:\r\n            flat_list.append(item)\r\n    return flat_list\r\n\r\n\r\ndef _O_nd(obj, out=None):\r\n    """"""Flatten type \'O\' arrays to ndarray, using recursion\r\n    :Note: append retains internal shape, extend will flatten\r\n    :  nested lists into a list\r\n    """"""\r\n    if out is None:\r\n        out = []\r\n    sub_out = []\r\n    for el in obj:\r\n        el = np.asarray(el)\r\n        if el.dtype.kind in (\'O\', \'V\'):\r\n            sub_out.append(_O_nd(el, out))  # ---- recursion needed ---\r\n        else:\r\n            out.extend(el)  # was append\r\n    return out\r\n\r\ndef _densify_2D(a, fact=2):\r\n    """"""Densify a 2D array using np.interp.\r\n    :fact - the factor to density the line segments by\r\n    :Notes\r\n    :-----\r\n    :original construction of c rather than the zero\'s approach\r\n    :  c0 = c0.reshape(n, -1)\r\n    :  c1 = c1.reshape(n, -1)\r\n    :  c = np.concatenate((c0, c1), 1)\r\n    """"""\r\n    # Y = a changed all the y\'s to a\r\n    a = np.squeeze(a)\r\n    n_fact = len(a) * fact\r\n    b = np.arange(0, n_fact, fact)\r\n    b_new = np.arange(n_fact - 1)     # Where you want to interpolate\r\n    c0 = np.interp(b_new, b, a[:, 0])\r\n    c1 = np.interp(b_new, b, a[:, 1])\r\n    n = c0.shape[0]\r\n    c = np.zeros((n, 2))\r\n    c[:, 0] = c0\r\n    c[:, 1] = c1\r\n    return c\r\n\r\n\r\n# ---- featureclass functions ------------------------------------------------\r\n#\r\ndef fc_info(in_fc, prn=False):\r\n    """"""Return basic featureclass information, including...\r\n    :\r\n    : shp_fld  - field name which contains the geometry object\r\n    : oid_fld  - the object index/id field name\r\n    : SR       - spatial reference object (use SR.name to get the name)\r\n    : shp_type - shape type (Point, Polyline, Polygon, Multipoint, Multipatch)\r\n    : - others: \'areaFieldName\', \'baseName\', \'catalogPath\',\'featureType\',\r\n    :   \'fields\', \'hasOID\', \'hasM\', \'hasZ\', \'path\'\r\n    : - all_flds =[i.name for i in desc[\'fields\']]\r\n    """"""\r\n    desc = arcpy.da.Describe(in_fc)\r\n    args = [\'shapeFieldName\', \'OIDFieldName\', \'shapeType\', \'spatialReference\']\r\n    shp_fld, oid_fld, shp_type, SR = [desc[i] for i in args]\r\n    if prn:\r\n        frmt = ""FeatureClass:\\n   {}"".format(in_fc)\r\n        f = ""\\n{!s:<16}{!s:<14}{!s:<10}{!s:<10}""\r\n        frmt += f.format(*args)\r\n        frmt += f.format(shp_fld, oid_fld, shp_type, SR.name)\r\n        tweet(frmt)\r\n    else:\r\n        return shp_fld, oid_fld, shp_type, SR\r\n\r\n\r\ndef tweet(msg):\r\n    """"""Print a message for both arcpy and python.\r\n    : msg - a text message\r\n    """"""\r\n    m = ""\\n{}\\n"".format(msg)\r\n    arcpy.AddMessage(m)\r\n    print(m)\r\n    print(arcpy.GetMessages())\r\n\r\n\r\ndef _get_shapes(in_fc):\r\n    """"""Get shapes from a featureclass, in_fc, using SHAPE@ returning\r\n    :  [<Polygon object at....>, ... (<Polygon object at....>]\r\n    """"""\r\n    with arcpy.da.SearchCursor(in_fc, \'SHAPE@\') as cursor:\r\n        a = [row[0] for row in cursor]\r\n    return a\r\n\r\n\r\ndef _ndarray(in_fc, to_pnts=True, flds=None, SR=None):\r\n    """"""Convert featureclass geometry (in_fc) to a structured ndarray including\r\n    options to select fields and specify a spatial reference.\r\n\r\n    Parameters:\r\n    -----------\r\n    - in_fc :\r\n        input featureclass\r\n    - to_pnts : boolean\r\n        True, convert the shape to points.\r\n        False, centroid returned.\r\n    - flds : `*` for all.\r\n        Others can include: \'Shape\',  [\'SHAPE@X\', \'SHAPE@Y\'], or specify\r\n    """"""\r\n    if flds is None:\r\n        flds = ""*""\r\n    if SR is None:\r\n        desc = arcpy.da.Describe(in_fc)\r\n        SR = desc[\'spatialReference\']\r\n    args = [in_fc, flds, None, SR, to_pnts, (None, None)]\r\n    cur = arcpy.da.SearchCursor(*args)\r\n    a = cur._as_narray()\r\n    del cur\r\n    return a\r\n\r\n\r\ndef _get_attributes(in_fc):\r\n    """"""Get the attributes of features, returns the centroid coordinates\r\n    as fields in the table.\r\n    """"""\r\n    dt_b = [(\'IDs\', \'<i4\'), (\'Xc\', \'<f8\'), (\'Yc\', \'<f8\')]\r\n    b = _ndarray(in_fc, to_pnts=False)\r\n    dt = [[n, t] for n, t in b.dtype.descr[2:]]\r\n    for i in dt:\r\n        if i[0] in [""Shape_Length"", ""Shape_Area"", ""Shape""]:\r\n            i[0] = i[0] + ""_orig""\r\n    dt = [tuple(i) for i in dt]\r\n    dt_b.extend(dt)\r\n    b.dtype = dt_b\r\n    return b\r\n\r\n\r\ndef obj_shapes(in_, SR):\r\n    """"""object array of coordinates to shapes""""""\r\n    s = []\r\n    for shps in in_:\r\n        tmp = []\r\n        if isinstance(shps, (list, tuple)):\r\n            for shp in shps:\r\n                shp = np.asarray(shp)\r\n                shp = shp.squeeze()\r\n                pnts = [arcpy.Point(*p) for p in shp]\r\n                tmp.append(pnts)\r\n            arr = arcpy.Array(pnts)\r\n        else:\r\n            arr = arcpy.Array([arcpy.Point(*p) for p in shps])\r\n        #\r\n        if out_type == \'Polyline\':\r\n            g = arcpy.Polyline(arr, SR)\r\n        elif out_type == \'Polygon\':\r\n            g = arcpy.Polygon(arr, SR)\r\n        s.append(g)\r\n    return s\r\n\r\n\r\ndef arcpnts_poly(in_, out_type=\'Polygon\', SR=None):\r\n    """"""Convert arcpy Point lists to poly* features\r\n\r\n    - out_type : either \'Polygon\' or \'Polyline\'\r\n    """"""\r\n    s = []\r\n    for i in in_:\r\n        for j in i:\r\n            if out_type == \'Polygon\':\r\n                g = arcpy.Polygon(arcpy.Array(j), SR)\r\n            elif out_type == \'Polyline\':\r\n                g = arcpy.Polyline(arcpy.Array(j), SR)\r\n            elif out_type == \'Points\':\r\n                j = _flat_(j)\r\n                g = arcpy.Multipoint(arcpy.Array(j), SR)  # check\r\n            s.append(g)\r\n    return s\r\n\r\n\r\ndef _convert(a, fact=2):\r\n    """"""Do the shape conversion for the array parts.  Calls to _densify_2D\r\n    """"""\r\n    out = []\r\n    parts = len(a)\r\n    for i in range(parts):\r\n        sub_out = []\r\n        p = np.asarray(a[i]).squeeze()\r\n        if p.ndim == 2:\r\n            shp = _densify_2D(p, fact=fact)  # call _densify_2D\r\n            arc_pnts = [arcpy.Point(*p) for p in shp]\r\n            sub_out.append(arc_pnts)\r\n            out.extend(sub_out)\r\n        else:\r\n            for i in range(len(p)):\r\n                pp = p[i]\r\n                shp = _densify_2D(pp, fact=fact)\r\n                arc_pnts = [arcpy.Point(*p) for p in shp]\r\n                sub_out.append(arc_pnts)\r\n            out.append(sub_out)\r\n    return out\r\n\r\n\r\ndef densify(polys, fact=2, sp_ref=None):\r\n    """"""Convert polygon objects to arrays, densify.\r\n    :\r\n    :Requires:\r\n    :--------\r\n    : _densify_2D - the function that is called for each shape part\r\n    : _unpack - unpack objects\r\n    """"""\r\n    # ---- main section ----\r\n    out = []\r\n    for poly in polys:\r\n        p = poly.__geo_interface__[\'coordinates\']\r\n        back = _convert(p, fact)\r\n        out.append(back)\r\n    return out\r\n\r\n\r\n# ------------------------------------------------------------------------\r\n# (1) ---- Checks to see if running in test mode or from a tool ----------\r\n\r\ndef _demo():\r\n    """"""run when script is in demo mode""""""\r\n    in_fc = \'C:\\\\all_scripts\\\\testfiles\\\\testdata.gdb\\Carp_5x5km\'\r\n#    out_fc = pth + \'/geom_data.gdb/x1\'\r\n    out_fc = None\r\n    fact = 2\r\n    out_type = \'Polygon\'  # \'Polyline\' or \'Points\'\r\n    testing = True\r\n    return in_fc, out_fc, fact, out_type, testing\r\n\r\n\r\ndef _tool():\r\n    """"""run when script is from a tool""""""\r\n    in_fc = sys.argv[1]  #\r\n    out_fc = sys.argv[2]  #\r\n    fact = int(sys.argv[3])  #\r\n    out_type = sys.argv[4]  # Polygon, Polyline are options\r\n    testing = False\r\n    return in_fc, out_fc, fact, out_type, testing\r\n\r\n\r\n# ---- main block ------------------------------------------------------------\r\n#\r\n# (1) check to see if in demo or tool mode\r\n# (2) obtain fc information\r\n# (3) convert multipart to singlepart\r\n# (4) split the fc into two arrays, one geometry, the 2nd attributes\r\n# (5) obtain the shapes and densify\r\n# (6) optionally produce the output fc\r\n# (7) join the attributes back\r\n\r\nif len(sys.argv) == 1:\r\n    in_fc, out_fc, fact, out_type, testing = _demo()\r\nelse:\r\n    in_fc, out_fc, fact, out_type, testing = _tool()\r\n\r\nshp_fld, oid_fld, shp_type, SR = fc_info(in_fc)\r\n\r\nif not testing:\r\n    temp = out_fc + ""tmp""\r\n    if arcpy.Exists(temp):\r\n        arcpy.Delete_management(temp)\r\n    arcpy.MultipartToSinglepart_management(in_fc, temp)\r\n    polys = _get_shapes(temp)\r\n    a = densify(polys, fact=fact, sp_ref=SR)\r\n    b = _get_attributes(temp)\r\n    out_shps = arcpnts_poly(a, out_type=out_type, SR=SR)\r\n    #\r\n    if arcpy.Exists(out_fc):\r\n        arcpy.Delete_management(out_fc)\r\n    arcpy.CopyFeatures_management(out_shps, out_fc)\r\n    arcpy.da.ExtendTable(out_fc, \'OBJECTID\', b, \'IDs\')\r\n    # ---- cleanup\r\n    arcpy.Delete_management(temp)\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n'"
all_scripts/densify_helper.py,7,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\ndensify_helper\r\n==============\r\n\r\nScript : densify_helper.py\r\n\r\nAuthor : Dan.Patterson@carleton.ca\r\n\r\nModified : 2018-03-31\r\n\r\nPurpose : tools for working with numpy arrays\r\n\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\n\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\n# ---- array functions -------------------------------------------------------\r\n#\r\n\r\n__all__ = [\'tweet\', \'_describe\', \'fc_info\', \'_xyID\', \'_ndarray\', \'_two_arrays\']\r\n\r\n\r\n# ---- Common functions used in other modules -------------------------------\r\n#\r\ndef tweet(msg):\r\n    """"""Produce a message for both arcpy and python::\r\n\r\n    `msg` - a text message\r\n    """"""\r\n    m = ""\\n{}\\n"".format(msg)\r\n    arcpy.AddMessage(m)\r\n    print(m)\r\n#    return None\r\n\r\n\r\ndef _describe(in_fc):\r\n    """"""Simply return the arcpy.da.Describe object\r\n\r\n    *desc.keys()* : an abbreviated list...\r\n    ::\r\n        \'OIDFieldName\'... \'areaFieldName\', \'baseName\'... \'catalogPath\',\r\n        ... \'dataType\'... \'extent\', \'featureType\', \'fields\', \'file\'... \'hasM\',\r\n        \'hasOID\', \'hasZ\', \'indexes\'... \'lengthFieldName\'... \'name\', \'path\',\r\n        \'rasterFieldName\', ..., \'shapeFieldName\', \'shapeType\',\r\n        \'spatialReference\',  ...\r\n    """"""\r\n    return arcpy.da.Describe(in_fc)\r\n\r\n\r\ndef fc_info(in_fc, prn=False):\r\n    """"""Return basic featureclass information, including...\r\n\r\n    Parameters\r\n    ----------\r\n    - ``shp_fld  :``\r\n        field name which contains the geometry object\r\n    - ``oid_fld  :``\r\n        the object index/id field name\r\n    - ``SR       :``\r\n        spatial reference object (use SR.name to get the name)\r\n    - ``shp_type :``\r\n        shape type (Point, Polyline, Polygon, Multipoint, Multipatch)\r\n    - ``others   :``\r\n        areaFieldName, baseName, catalogPath, featureType, fields,\r\n        hasOID, hasM, hasZ, path\r\n    - ``all_flds :``\r\n         [i.name for i in desc[\'fields\']]\r\n    """"""\r\n    desc = _describe(in_fc)\r\n    args = [\'shapeFieldName\', \'OIDFieldName\', \'shapeType\', \'spatialReference\']\r\n    shp_fld, oid_fld, shp_type, SR = [desc[i] for i in args]\r\n    if prn:\r\n        frmt = ""FeatureClass:\\n   {}"".format(in_fc)\r\n        f = ""\\n{!s:<16}{!s:<14}{!s:<10}{!s:<10}""\r\n        frmt += f.format(*args)\r\n        frmt += f.format(shp_fld, oid_fld, shp_type, SR.name)\r\n        tweet(frmt)\r\n    else:\r\n        return shp_fld, oid_fld, shp_type, SR\r\n\r\n\r\n#\r\n# ---- common functions, end ------------------------------------------------\r\n#\r\ndef _xyID(in_fc, to_pnts=True):\r\n    """"""Convert featureclass geometry (in_fc) to a simple 2D structured array\r\n    with ID, X, Y values. Optionally convert to points, otherwise centroid.\r\n    """"""\r\n    flds = [\'OID@\', \'SHAPE@X\', \'SHAPE@Y\']\r\n    args = [in_fc, flds, None, None, to_pnts, (None, None)]\r\n    cur = arcpy.da.SearchCursor(*args)\r\n    a = cur._as_narray()\r\n    a.dtype = [(\'IDs\', \'<i4\'), (\'Xs\', \'<f8\'), (\'Ys\', \'<f8\')]\r\n    del cur\r\n    return a\r\n\r\n\r\ndef _ndarray(in_fc, to_pnts=True, flds=None, SR=None):\r\n    """"""Convert featureclass geometry (in_fc) to a structured ndarray including\r\n    options to select fields and specify a spatial reference.\r\n\r\n    Parameters:\r\n    -----------\r\n    - in_fc :\r\n        input featureclass\r\n    - to_pnts : boolean\r\n        True, convert the shape to points.\r\n        False, centroid returned.\r\n    - flds : `*` for all.\r\n        Others can include: \'Shape\',  [\'SHAPE@X\', \'SHAPE@Y\'], or specify\r\n    """"""\r\n    if flds is None:\r\n        flds = ""*""\r\n    if SR is None:\r\n        desc = arcpy.da.Describe(in_fc)\r\n        SR = desc[\'spatialReference\']\r\n    args = [in_fc, flds, None, SR, to_pnts, (None, None)]\r\n    cur = arcpy.da.SearchCursor(*args)\r\n    a = cur._as_narray()\r\n    del cur\r\n    return a\r\n\r\n\r\ndef _two_arrays(in_fc, both=True, split=True):\r\n    """"""Send to a numpy structured/array and split it into a geometry array\r\n    and an attribute array.  They can be joined back later if needed.\r\n\r\n    Note:\r\n    -----\r\n        The geometry array is returned as an object array.  See the main\r\n        documentation.\r\n\r\n    Requires:\r\n    --------\r\n    _xyID :\r\n        function to get geometry array\r\n    _ndarray :\r\n        function to get the x, y, id array and attribute array\r\n    fc_info(in_fc) :\r\n        function needed to return fc properties parameters\r\n\r\n    Parameters:\r\n    -----------\r\n    both :\r\n        True, to return both arrays\r\n        False to return just geometry\r\n    split :\r\n      - True, split points by their geometry groups as an object array\r\n      - False, a sequential array with shape = (N,)\r\n\r\n    **variables**\r\n      - dt_a = [(\'IDs\', \'<i4\'), (\'Xs\', \'<f8\'), (\'Ys\', \'<f8\')]\r\n      - dt_b = [(\'IDs\', \'<i4\'), (\'Xc\', \'<f8\'), (\'Yc\', \'<f8\')]\r\n      - dt_b.extend(b.dtype.descr[2:])\r\n\r\n    Extend the dtype using the attribute dtype minus geometry and id\r\n    """"""\r\n    a = _xyID(in_fc, to_pnts=True)\r\n    shp_fld, oid_fld, SR, shp_type = fc_info(in_fc)\r\n    dt_a = [(\'IDs\', \'<i4\'), (\'Xs\', \'<f8\'), (\'Ys\', \'<f8\')]\r\n    dt_b = [(\'IDs\', \'<i4\'), (\'Xc\', \'<f8\'), (\'Yc\', \'<f8\')]\r\n    a.dtype = dt_a\r\n    b = None\r\n    if split:\r\n        ids = np.unique(a[\'IDs\'])\r\n        w = np.where(np.diff(a[\'IDs\']))[0] + 1\r\n        a = np.split(a, w)\r\n        a = np.array([[ids[i], a[i][[\'Xs\', \'Ys\']]] for i in range(len(ids))])\r\n    if both:\r\n        b = _ndarray(in_fc, to_pnts=False)\r\n        dt_b.extend(b.dtype.descr[2:])\r\n        b.dtype = dt_b\r\n    return a, b\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n#    _demo()\r\n'"
all_scripts/diamond_square.py,36,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   diamond_square.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2017-12-20\r\n:Purpose:\r\n:-------\r\n:The diamond-square algorithm is used to generate synthetic terrain samples.\r\n:These are used for slope, aspect and hillshade algorithm testing.\r\n:\r\n:Useage:\r\n:------\r\n:Specify a power (n) to get a square array of a certain size/shape (see below).\r\n:The default min/max and rows/columns are specified within d_s.  You can use\r\n:\'scale_range\' to change the values after if you like.  The number of rows/cols\r\n:are used to tile/replicate the base array in the x,y directions during array\r\n:creation, or you can use the np.fliplr, np.flipud to do this yourself later.\r\n:Roughness (r) can be specified if you want to add noise to the array.  Values\r\n:between 0-1 are accepted, but I suggest keeping them small\r\n:\r\n:Notes: relative timing (n= power, time, array shape)\r\n:  3   0.3 ms (9, 9)\r\n:  7  47.4 ms (129, 129)\r\n:  8 189.  ms (257, 257)\r\n   9 770.  ms (513, 513)\r\n: 10   3.1 s  (1025, 1025)\r\n:\r\n:References:\r\n:----------\r\n:  https://scipython.com/blog/cloud-images-using-the-diamond-square-algorithm/\r\n:  https://gist.github.com/CMCDragonkai/6444bf7ea41b4f43766abb9f4294cd69\r\n:  https://raw.githubusercontent.com/buckinha/DiamondSquare/master\r\n:        /DiamondSquare.py\r\n:  *** http://paulbourke.net/fractals/noise/\r\n:  *** https://en.m.wikipedia.org/wiki/Diamond-square_algorithm\r\n:       https://github.com/buckinha/DiamondSquare\r\n:       https://github.com/Crowgers/Diamond_Square\r\n:       https://github.com/elite174/TextureGen\r\n:     - https://joecrossdevelopment.wordpress.com/2012/04/30/\r\n:       2d-random-terrain-iterative-diamond-square-algorithm/\r\n:     - https://scipython.com/blog/cloud-images-using-the-diamond-square-\r\n:       algorithm/\r\n:**http://www-cs-students.stanford.edu/~amitp/\r\n:       game-programming/polygon-map-generation/\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# pylint: disable=C0103  # invalid-name\r\n# pylint: disable=R0914  # Too many local variables\r\n# pylint: disable=R1710  # inconsistent-return-statements\r\n# pylint: disable=W0105  # string statement has no effect\r\n\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\r\n      \'float\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=5, linewidth=80, precision=3, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n__all__ = [\'scale_range\',\r\n           \'normalize_to\',\r\n           \'n_p\',\r\n           \'d_s\',\r\n           \'_diamond_\',\r\n           \'_square_\']\r\n\r\n\r\ndef scale_range(a, new_min=0, new_max=1):\r\n    """"""scale an input array-like to a mininum and maximum number the input\r\n    :array must be of a floating point array if you have a non-floating\r\n    :point array, convert to floating using `astype(\'float\')` this works\r\n    :with n-dimensional arrays it will mutate in place.\r\n    :min and max can be integers\r\n    :Source:\r\n    :--- https://gist.github.com/CMCDragonkai/6444bf7ea41b4f43766abb9f4294cd69\r\n    def scale_range (input, min, max):\r\n        input += -(np.min(input))\r\n        input /= np.max(input) / (max - min)\r\n        input += min\r\n        return input\r\n    """"""\r\n    dt = a.dtype\r\n    a.dtype = \'float64\'\r\n    if (new_max - new_min) == 0.:\r\n        raise ValueError(""Max == Min {}=={}"".format(new_max, new_min))\r\n    a += -(np.min(a))\r\n    a /= np.max(a) / (new_max - new_min)\r\n    a += new_min\r\n    a = a.astype(dt)\r\n    return a\r\n\r\n\r\ndef normalize_to(a, upper=1, as_=float):\r\n    """"""normalize an array\r\n    :Normalised [0, 1]  a0 = (a - np.max(a))/-np.ptp(a)\r\n    :Normalised [-1,1]  a1 = 2*(a - np.max(a))/-np.ptp(a)-1\r\n    :Normalised [0,255] as integer\r\n    :  a2 = (255*(a - np.max(a))/-np.ptp(a)).astype(int)\r\n    :  a3 = (a - np.mean(a)) / np.std(a)\r\n    """"""\r\n    b = (upper*(a - np.max(a))/-np.ptp(a)).astype(as_)\r\n    return b\r\n\r\n\r\ndef n_p(r_c=[10, 10], pmax=10):\r\n    """"""Return the array to form based on desired size. This will be sliced\r\n    :  to desired dimensions after processing.\r\n    : r_c - rows, cols (y, x)\r\n    : pmax - maximum power to control maximum array size.\r\n    :    2**pmax + 1 = N\r\n    :    pairs (pmax, N): (10, 1025), (11, 2049), (12, 4097), (13, 8193)\r\n    """"""\r\n    max_size = max(r_c)\r\n    for p in range(1, pmax):\r\n        N = (2**p) + 1\r\n        if max_size <= N:\r\n            return N, p\r\n    return 2**pmax + 1, pmax\r\n\r\n\r\n# ---- main portion of the algorithm -----------------------------------------\r\n#\r\ndef d_s(twoPow=2, low=0, high=1, cols=1, rows=1, corner=None, r=0, img=False):\r\n    """"""Diamond square algorithm\r\n    : twoPow - 2**(x) this is the power to specify, returns rows/cols\r\n    :    (x, N) - (2, 5), (3, 9), (4, 17), (5, 33), (6, 65), (7, 129),\r\n    :             (8, 257), (9, 513), (10, 1025), (11, 2049)\r\n    : high/low - min, max of the array\r\n    : cols/rows - for replication in both directions\r\n    : corner - specify a corner if you want values to seed the algorithm\r\n    :          [UL, UR, LL, LR]\r\n    : r - roughness between 0 and 1\r\n    : img - True to show image, False otherwise\r\n    :Reference:\r\n    :---------\r\n    :  https://raw.githubusercontent.com/buckinha/DiamondSquare/master\r\n    :        /DiamondSquare.py\r\n    """"""\r\n    # seed the random number generator\r\n    np.random.seed(None)\r\n    if (r < 0.) or (r > 1.):\r\n        raise ValueError(""roughness, r, outside the acceptable range or 0-1"")\r\n    # ---- array, size (N, N), filled will NaN, with corner specification ----\r\n    p = twoPow\r\n    N = 2**twoPow + 1\r\n    arr = np.full((N, N), np.nan, dtype=\'float\', order=\'C\')\r\n    # ---- seed the corners\r\n    c_pnts = np.random.uniform(low, high, (2, 2))\r\n    if corner is None:\r\n        arr[0::N-1, 0::N-1] = c_pnts\r\n    elif len(corner) == 4:\r\n        c_pnts = np.asarray(corner).reshape(2, 2)\r\n        arr[0::N-1, 0::N-1] = c_pnts\r\n    else:\r\n        arr[0::N-1, 0::N-1] = c_pnts\r\n    # ---- run the algorithm\r\n    print(""Input template array with 4 corners defined...\\n{}"".format(arr))\r\n    for i in range(p):\r\n        r = r  # roughness**i  0**0 = 1 !!!! watch it\r\n        step_size = (N-1) // 2**(i)\r\n        _diamond_(arr, step_size, r)  # ---- diamond step\r\n        #print(""power di {}...\\narray...\\n{}"".format(i, arr))\r\n        _square_(arr, step_size, r)   # ---- square step\r\n        #print(""power sq {}...\\narray...\\n{}"".format(i, arr))\r\n    # ---- determine whether mirroring or graphing is desired ----\r\n    if cols == 2:\r\n        arr = np.c_[arr, np.fliplr(arr)]\r\n    if cols == 3:\r\n        arr = np.c_[arr, np.fliplr(arr), arr]\r\n    if rows == 2:\r\n        arr = np.r_[arr, np.flipud(arr)]\r\n    if img:\r\n        plt.imshow(arr, cmap=plt.cm.gist_earth)  # gist_earth, Blues, oceans)\r\n        plt.axis(\'off\')\r\n        plt.show()\r\n    return arr\r\n\r\n\r\ndef _diamond_(arr, step_size, r):\r\n    """"""Diamond step first.  Calculate to locate the diamond corners for\r\n    :  filling.\r\n    """"""\r\n    def shift_di(a, i, j, hs, r):\r\n        """"""hs - halfstep\r\n        :defines the midpoint displacement for the diamond step\r\n        """"""\r\n        T, L, B, R = [i-hs, j-hs, i+hs, j+hs]\r\n        ave = (a[T, L] + a[T, R] + a[B, L] + a[B, R])/4.0\r\n        ran = np.random.uniform(-r, r)\r\n        return (1.0 - ran) * ave\r\n    #\r\n    # ---- main diamond section\r\n    hs = step_size//2\r\n    x_steps = range(hs, arr.shape[0], step_size)\r\n    y_steps = range(hs, arr.shape[1], step_size)  # x_steps[:]\r\n    for i in x_steps:\r\n        for j in y_steps:\r\n            if np.isnan(arr[i, j]):  # == -1.0:  # ** checks for -1\r\n                arr[i, j] = shift_di(arr, i, j, hs, r)\r\n    # ---- end ----\r\n\r\n\r\ndef _square_(arr, step_size, r):\r\n    """"""Step the square with half-steps for shift_sq\r\n    :Shift the square and determine whether 3 or 4 values are used in the\r\n    :calculation for the average based on whether the square is on an edge.\r\n    :\r\n    """"""\r\n    def shift_sq(arr, i, j, hs, r):\r\n        """""" hs - half step """"""\r\n        div = 0\r\n        sum_ = 0\r\n        T, L, B, R = [i-hs, j-hs, i+hs, j+hs]\r\n        if i - hs >= 0:\r\n            sum_ += arr[T, j]  # top\r\n            div += 1\r\n        if i + hs < arr.shape[0]:\r\n            sum_ += arr[B, j]  # bottom\r\n            div += 1\r\n        if j - hs >= 0:\r\n            sum_ += arr[i, L]  # left\r\n            div += 1\r\n        if j + hs < arr.shape[0]:\r\n            sum_ += arr[i, R]  # right\r\n            div += 1\r\n        avg = sum_ / div\r\n        ran = np.random.uniform(-r, r)\r\n        return (1.0 - ran) * avg\r\n    #\r\n    hs = step_size//2\r\n    # ---- vertical step\r\n    steps_x_vert = range(hs, arr.shape[0], step_size)\r\n    steps_y_vert = range(0, arr.shape[1], step_size)\r\n    # ---- horizontal step\r\n    steps_x_horiz = range(0, arr.shape[0], step_size)\r\n    steps_y_horiz = range(hs, arr.shape[1], step_size)\r\n    for i in steps_x_horiz:\r\n        for j in steps_y_horiz:\r\n            arr[i, j] = shift_sq(arr, i, j, hs, r)\r\n    for i in steps_x_vert:\r\n        for j in steps_y_vert:\r\n            arr[i, j] = shift_sq(arr, i, j, hs, r)\r\n    # ---- end ----\r\n\r\n\r\ndef _demo(n, img=False):\r\n    np.random.RandomState(1)\r\n    # N = 2**n + 1\r\n    low = 0\r\n    high = 1.0\r\n#    c_pnts = [.25, .75, .5, 1.0]\r\n    c_pnts = [.25, 1.0, .25, 1.0]\r\n    a = d_s(twoPow=n, low=low, high=high, cols=1, rows=1, corner=c_pnts,\r\n            r=0, img=img)\r\n    return a\r\n\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n#    a = _demo(n=1, img=True)  # img... show plot if True\r\n""""""\r\nc_pnts = [.25, .75, .5, 1.0]  # used for peak\r\nc_pnts = [.25, 1.0, .25, 1.0] # used for vertical wave\r\nnp.save(""C:/Temp/ds_1.npy"", a)\r\n\r\na = d_s(twoPow=10, low=0, high=10., cols=100, rows=100, corner=c_pnts,\r\n            r=0, img=True)\r\na0 = np.fliplr(a)\r\naa0 = np.c_[a, a0]\r\na1 = np.flipud(aa0)\r\naa0a1 = np.r_[aa0, a1]\r\nnp.save(""c:/temp/ds_2.npy"", aa0a1)\r\nplt.imshow(aa0a1, cmap=plt.cm.gist_earth)\r\nplt.axis(\'off\')\r\nplt.show()\r\n\r\n#\r\npnts = np.random.randint(100, 1900, size=(20,2))\r\nsquares = [np.array([p, p+[50, 50]]) for p in pnts]\r\nzeros = np.zeros_like(aa0a1)\r\nfor i in squares:\r\n    zeros[i[0][0]:i[1][0], i[0][1]:i[1][1]] = 10.\r\n\r\nbergs = zeros + aa0a1\r\nplt.imshow(bergs, cmap=plt.cm.gist_earth)\r\nplt.axis(\'off\')\r\nplt.show()\r\n""""""\r\n""""""\r\nd0 = np.diff(bergs, 1, axis=0)\r\nd1 = np.diff(bergs, 1, axis=1)\r\nd0d = np.where(d0 > 1, d0, 0)\r\nd1d = np.where(d1 > 1, d1, 0)\r\nd0d1 = d0d[:2049, :2049] + d1d[:2049, :2049]\r\n""""""\r\n'"
all_scripts/excel2tbl.py,15,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nexcel2tbl\r\n===========\r\n\r\nScript :   excel2tbl.py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified : 2018-11-23\r\n\r\nPurpose:  tools for working with numpy arrays\r\n\r\nUseage :\r\n\r\nReferences\r\n----------\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/numpyarraytotable.htm>`_.\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/tabletonumpyarray.htm>`_.\r\n---------------------------------------------------------------------\r\n""""""\r\n# pylint: disable=C0103  # invalid-name\r\n# pylint: disable=R0914  # Too many local variables\r\n# pylint: disable=R1710  # inconsistent-return-statements\r\n# pylint: disable=W0105  # string statement has no effect\r\n\r\nimport sys\r\nimport numpy as np\r\nimport xlrd\r\nimport arcpy.da\r\nfrom arcpy import env\r\n\r\nenv.overwriteOutput = True\r\n#from arcpytools import fc_info, tweet  #, frmt_rec, _col_format\r\n#import arcpy\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n# ----------------------------------------------------------------------\r\n# ---- excel_np\r\ndef excel_np(path, sheet_num=0, int_null=-999):\r\n    """"""Read excel files to numpy structured/record arrays.  Your spreadsheet\r\n    must adhere to simple rules::\r\n      - first row must contain the field names for the output array\r\n      - no blank rows or columns, basically, no fluff or formatting\r\n      - if you have nodata values, put them in, since blank cells will be\r\n        \'corrected\' as best as possible.\r\n      - text and numbers in a column, results in a text column\r\n\r\n    See arraytools.a_io for excel_np for complete description\r\n    """"""\r\n    def isfloat(a):\r\n        """"""float check""""""\r\n        try:\r\n            i = float(a)\r\n            return i\r\n        except ValueError:\r\n            return np.nan\r\n\r\n    def punc_space(name):\r\n        """"""delete punctuation and spaces and replace with \'_\'""""""\r\n        punc = list(\'!""#$%&\\\'()*+,-./:;<=>?@[\\\\]^`{|}~ \')\r\n        return """".join([[i, \'_\'][i in punc] for i in name])\r\n\r\n    # import xlrd\r\n    w = xlrd.open_workbook(path)        # xlrd.book.Book class\r\n    sheets = len(w.sheets())\r\n    if sheet_num > sheets:\r\n        return None\r\n    sheet = w.sheet_by_index(sheet_num) # sheet by number\r\n    # sheet = w.sheet_by_name(\'test\')   # case sensitive, not implemented\r\n    names = sheet.row_values(0)         # clean these up later\r\n    cols = sheet.ncols\r\n    rows = sheet.nrows\r\n    col_data = [sheet.col_values(i, 1, rows) for i in range(cols)]\r\n    row_guess = sheet.row_values(1)\r\n    row_dts = [np.asarray(i).dtype.kind for i in row_guess]\r\n    col_dts = [np.asarray(col_data[i]).dtype.kind\r\n               for i in range(cols)]\r\n    clean = []\r\n    for i in range(len(row_dts)):\r\n        c = col_data[i]\r\n        if row_dts[i] == col_dts[i]:    # same dtype... send to array\r\n            ar = np.asarray(c)\r\n        if row_dts[i] == \'f\':           # float? if so, substitute np.nan\r\n            ar = np.array([isfloat(i) for i in c])\r\n            is_nan = np.isnan(ar)       # find the nan values, then check\r\n            not_nan = ar[~is_nan]       # are the floats == ints?\r\n            if np.all(np.equal(not_nan, not_nan.astype(\'int\'))):  # integer?\r\n                ar[is_nan] = int_null   # assign the integer null\r\n                ar = ar.astype(\'int\')\r\n        elif row_dts[i] in (\'U\', \'S\'):  # unicode/string... send to array\r\n            ar = np.char.strip(ar)\r\n            ar = np.where(np.char.str_len(ar) == 0, \'None\', ar)\r\n        else:\r\n            ar = np.asarray(c)\r\n        clean.append(ar)\r\n    # ---- assemble the columns for the array ----\r\n    dt_str = [i.dtype.str for i in clean]\r\n    names = [i.strip() for i in names]      # clean up leading/trailing spaces\r\n    names = [punc_space(i) for i in names]  # replace punctuation and spaces\r\n    dts_name = list(zip(names, dt_str))\r\n    arr = np.empty((rows-1,), dtype=dts_name)\r\n    cnt = 0\r\n    for i in names:\r\n        arr[i] = clean[cnt]\r\n        cnt += 1\r\n    return arr\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# .... final code section producing the featureclass and extendtable\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    in_excel = script.rpartition(""/"")[0] + ""/Data/test.xlsx""\r\n    sheet_num = 0\r\n    int_null = -999\r\n    arr = excel_np(in_excel, sheet_num=sheet_num, int_null=int_null)\r\n    print(""Array returned...\\n{}"".format(arr))\r\n    # parameters here\r\nelse:\r\n    testing = False\r\n    in_excel = sys.argv[1]\r\n    sheet_num = int(sys.argv[2])\r\n    int_null = sys.argv[3]\r\n    if int_null in (\'-2147483648\', \'-32768\', \'-128\', \'-9\', \'-1\'):\r\n        int_null == int(int_null)\r\n    else:\r\n        int_null = \'-2147483648\'\r\n    out_tbl = sys.argv[4]\r\n    arr = excel_np(in_excel, sheet_num=sheet_num, int_null=int_null)\r\n    if arr is None:\r\n        print(""not a sheet number"")\r\n    else:\r\n        arcpy.da.NumPyArrayToTable(arr, out_tbl)\r\n\r\n# parameters here\r\n#\r\nif testing:\r\n    print(\'\\nScript source... {}\'.format(script))\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n'"
all_scripts/extend_test.py,6,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   .py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2017-xx-xx\r\n:Purpose:  tools for working with numpy arrays\r\n:Useage:\r\n:\r\n:References:\r\n:\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\n\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\r\n      \'float\': \'{: 0.3f}\'.format}\r\n\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\ndef tweet(msg):\r\n    """"""Print a message for both arcpy and python.\r\n    : msg - a text message\r\n    """"""\r\n    m = ""\\n{}\\n"".format(msg)\r\n    arcpy.AddMessage(m)\r\n    print(m)\r\n    print(arcpy.GetMessages())\r\n\r\n\r\n#in_fc = r""C:\\Git_Dan\\a_Data\\testdata.gdb\\Polygon_pnts""\r\n# arcpy.env.workspace = ""C:/data/base.gdb""\r\nin_tbl = sys.argv[1]\r\nin_flds = sys.argv[2]\r\nout_fld = sys.argv[3]\r\n\r\nif \';\' in in_flds:\r\n    in_flds = in_flds.split(\';\')\r\nelse:\r\n    in_flds = [in_flds]\r\n\r\ndesc = arcpy.da.Describe(in_tbl)\r\ntbl_path = desc[\'path\']\r\nfnames = [i.name for i in arcpy.ListFields(in_tbl)]\r\nif out_fld in fnames:\r\n    out_fld += \'dup\'\r\nout_fld = arcpy.ValidateFieldName(out_fld, tbl_path)\r\nargs = [in_tbl, in_flds, out_fld, tbl_path]\r\nmsg = ""in_tbl {}\\nin_fld {}\\nout_fld  {}\\ntbl_path  {}"".format(*args)\r\ntweet(msg)\r\noid = \'OBJECTID\'\r\nvals = [oid] + in_flds\r\narr = arcpy.da.TableToNumPyArray(in_tbl, vals)\r\ntweet(""{!r:}"".format(arr))\r\nfor v in vals:\r\n    fix_null = np.where(arr[v] == \'None\', \'\', arr[v])\r\n    arr[v] = fix_null\r\n    tweet(""fixing vals...{}\\n{}"".format(v, arr[v]))\r\narr_sort = np.sort(arr, order=in_flds)\r\n# dt = [(oid, \'<i4\'), (vals + \'_rank2\', \'<i4\')]\r\ndt = [(oid, \'<i4\'), (out_fld, \'<i4\')]\r\nout_array = np.zeros((arr_sort.shape[0],), dtype=dt)\r\ndt_names = out_array.dtype.names\r\nout_array[dt_names[0]] = arr_sort[oid]\r\nout_array[dt_names[-1]] = np.arange(1, arr_sort.size + 1)  # gdb tables\r\narcpy.da.ExtendTable(in_tbl, \'OBJECTID\', out_array, \'OBJECTID\')\r\n\r\ndef _demo():\r\n    """"""\r\n    : -\r\n    """"""\r\n    pass\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n    _demo()\r\n\r\n'"
all_scripts/fc.py,28,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\nfc.py  featureclass.py\r\n======================\r\n\r\nScript   :   fc.py  (featureclass.py)\r\n\r\nAuthor   :   Dan_Patterson@carleton.ca\r\n\r\nModified : 2018-09-02\r\n\r\nPurpose  :\r\n    Tools for working with featureclass arcpy geometry objects and conversion\r\n    to numpy arrays.\r\n\r\nNotes:\r\n------\r\n- do not rely on the OBJECTID field for anything\r\n\r\n  http://support.esri.com/en/technical-article/000010834\r\n\r\nSee ... /arcpytools/Notes_etc/fc_py_output.txt\r\n\r\nfor sample output for the functions below\r\n\r\n-  _describe,  : arcpy describe object\r\n-  _get_shapes,  : actual arc* geometry\r\n-  _ndarray,  : a structured array\r\n-  _props  : detailed properties for an arcpy shape object\r\n-  _two_arrays,  : a geometry array and attribute array\r\n-  _xy,  : x,y coordinates only\r\n-  _xyID,  : x,y and ID\r\n-  _xy_idx,  : x,y and an index array\r\n-  change_fld,  : provide array happy field types\r\n-  fc_info,  : shape and oid fields, SR and geometry type\r\n-  fld_info,  : field type, namd and length\r\n\r\nGeneral\r\n-------\r\n\r\ncur._as_narray() and cur._dtype are methods of da.cursors\r\n\r\nfield_names\r\n  [\'OBJECTID\', \'Shape\'], [\'OID@\', \'Shape\'], [\'OID@\', \'SHAPE@WKT\']\r\n  [\'OID@\', \'SHAPE@JSON\'], [\'OID@\', \'SHAPE@X\', \'SHAPE@Y\']\r\n\r\n>>> cur = arcpy.da.SearchCursor(in_fc, field_names, .....)\r\n>>> cur = arcpy.da.SearchCursor(in_fc, [\'OBJECTID\', \'Shape\'], None, None,\r\n                                True, (None, None))\r\n\r\nPolygons\r\n--------\r\n\r\nArcpy polygon objects\r\n\r\n- information :\r\n    __doc__, __module__, __type_string__\r\n\r\n- conversion :\r\n    JSON, WKB, WKT, __geo_interface__, _fromGeoJson\r\n\r\n- properties :\r\n    \'area\', \'boundary\', \'centroid\', \'convexHull\', \'equals\', \'firstPoint,\r\n    \'extent\', \'isMultipart\', \'hullRectangle\', \'labelPoint\', \'lastPoint\',\r\n    \'length\', \'length3D\', \'partCount\', \'pointCount\', \'spatialReference\',\r\n    \'trueCentroid\', \'type\'\r\n\r\n- methods :\r\n    angleAndDistanceTo, buffer, clip, contains, crosses, cut, densify,\r\n    difference, disjoint, distanceTo, generalize, getArea, getGeohash,\r\n    getLength, getPart, intersect, measureOnLine, overlaps,\r\n    pointFromAngleAndDistance, positionAlongLine, projectAs,\r\n    queryPointAndDistance, segmentAlongLine, snapToLine, symmetricDifference,\r\n    touches, union, within\r\n\r\n\r\n**1. cursors**\r\n\r\nfrom arcgisscripting import da\r\n\r\ndir(da)  : the main data access link with underscore functions present\r\n\r\n   [\'Describe\', \'Domain\', \'Editor\', \'ExtendTable\', \'FeatureClassToNumPyArray\',\r\n   \'InsertCursor\', \'ListDomains\', \'ListFieldConflictFilters\', \'ListReplicas\',\r\n   \'ListSubtypes\', \'ListVersions\', \'NumPyArrayToFeatureClass\',\r\n   \'NumPyArrayToTable\', \'Replica\', \'SearchCursor\', \'TableToNumPyArray\',\r\n   \'UpdateCursor\', \'Version\', \'Walk\', \'__doc__\', \'__loader__\', \'__name__\',\r\n   \'__package__\', \'__spec__\', \'_internal_eq\', \'_internal_sd\', \'_internal_vb\']\r\n\r\ndir(da.SearchCursor)\r\n\r\n   [\'__class__\', \'__delattr__\', \'__dir__\', \'__doc__\', \'__enter__\', \'__eq__\',\r\n   \'__esri_toolinfo__\', \'__exit__\', \'__format__\', \'__ge__\', \'__getattribute__\',\r\n   \'__getitem__\', \'__gt__\', \'__hash__\', \'__init__\', \'__iter__\', \'__le__\',\r\n   \'__lt__\', \'__ne__\', \'__new__\', \'__next__\', \'__reduce__\', \'__reduce_ex__\',\r\n   \'__repr__\', \'__setattr__\', \'__sizeof__\', \'__str__\', \'__subclasshook__\',\r\n   \'_as_narray\', \'_dtype\', \'fields\', \'next\', \'reset\']\r\n\r\n\r\n**2. arcpy.da.SearchCursor**\r\n\r\n>>> cur = arcpy.da.SearchCursor(in_table, field_names, {where_clause},\r\n                                {spatial_reference}, {explode_to_points},\r\n                                {sql_clause})\r\n   - field_names\r\n     - flds = [i.name for i in arcpy.ListFields(in_fc)]  : fields, sort after\r\n     - flds = \'*\'  : all fields in order\r\n     - flds = [\'OBJECTID\', \'Shape\',...]  : specify the fields you want\r\n   - where_clause\r\n     - specify a where clause\r\n   - spatial_reference\r\n     - SR.name    \'NAD_1983_CSRS_MTM_9\'\r\n     - SR.PCSName \'NAD_1983_CSRS_MTM_9\'\r\n     - SR.PCSCode 2951\r\n   - explode_to_points\r\n     - True or False\r\n   - sql_clause\r\n     - specify one or (None, None)\r\n\r\nFor example....\r\n\r\n>>> args = [in_fc, [\'OBJECTID\', \'Shape\'], None, None,  True, (None, None)]\r\n>>> cur = arcpy.da.SearchCursor(*args)\r\n>>>  a = cur._as_narray()\r\n\r\n**3. JSON** --- example dictionary... a_polygon.JSON returns a string\r\n::\r\n   json.loads(a_polygon.JSON)  : 3 polygons\r\n   {\'rings\':  [[[300010, 5000010],... [300010, 5000010]],\r\n              [[300010, 5000010],... [300010, 5000010]],\r\n              [[300005, 5000008],... [300005, 5000008]]],\r\n    \'spatialReference\': {\'latestWkid\': 2951, \'wkid\': 2146}}\r\n\r\n**4. __geo_interface__**--- example return for a 2-part polygon\r\n::\r\n   a_polygon.__geo_interface__\r\n   {\'coordinates\': [[[(300010.0, 5000010.0),... (300010.0, 5000010.0)]],\r\n                    [[(300010.0, 5000010.0),... (300010.0, 5000010.0)],\r\n                     [(300005.0, 5000008.0),... (300005.0, 5000008.0)]]],\r\n    \'type\': \'MultiPolygon\'}\r\n\r\n\r\n**5. JSON and WKT** return strings, that is why you need json.loads or\r\n::\r\n   __geo_interface__\r\n   a_polygon.JSON\r\n   \'{\'rings\' ...snip... \'wkid\': 2146}}\'  : note the \' \' enclosure\r\n   WKT --- WKT returns a string like JSON\r\n   a_polygon.WKT\r\n   \'MULTIPOLYGON(((300010 5000010,... 300010 5000010)),\r\n                  ((300010 5000010,... 300010 5000010),\r\n                   (300005 5000008,... 300005 5000008)))\'\r\n\r\nOther examples:\r\n--------------\r\n\r\nThe following returned objects for each approach are:\r\n    p0 - list\r\n    p1 - ndarray\r\n    p2a, p2b = tuple of ndarrays\r\n    p3 - ndarray\r\n    p4 - ndarray\r\n\r\n>>> p0 = [i.__geo_interface__[\'coordinates\'] for i in in_polys]\r\n>>> p1 = _xyID(in_fc)\r\n>>> p2a, p2b = _xy_idx(in_fc)\r\n>>> p3 = _xy(in_fc)\r\n>>> p4 = arcpy.da.FeatureClassToNumPyArray(in_fc,\r\n                                           [""OID@"", ""Shape@X"", ""Shape@Y""],\r\n                                           explode_to_points=True,\r\n                                           spatial_reference=SR)\r\n\r\ngeometry is a square one of 5 shapes\r\n\r\n>>> p0[2]  # __geo_interface__\r\n[[[(307500.0, 5029300.0),\r\n   (308786.7818999998, 5029300.0),\r\n   (308792.1923000002, 5028500.0),\r\n   (307500.0, 5028500.0),\r\n   (307500.0, 5029300.0)]]]\r\n\r\n>>> p1[p1[\'IDs\'] == 3]\r\narray([(3, 307500.  , 5029300.), (3, 308786.78, 5029300.),\r\n       (3, 308792.19, 5028500.), (3, 307500.  , 5028500.),\r\n       (3, 307500.  , 5029300.)],\r\n      dtype=[(\'IDs\', \'<i4\'), (\'Xs\', \'<f8\'), (\'Ys\', \'<f8\')])\r\n\r\n>>> p2a, p2b = _xy_idx(in_fc)\r\n>>> idx = p2b[:,0] == 3\r\n>>> p2a[idx]\r\narray([[ 307500.  , 5029300.  ],\r\n       [ 308786.78, 5029300.  ],\r\n       [ 308792.19, 5028500.  ],\r\n       [ 307500.  , 5028500.  ],\r\n       [ 307500.  , 5029300.  ]])\r\n\r\n>>> p3  # just the coordinates regardless of the poly*\r\n\r\n>>> p4  # arcpy.da.FeatureClassToNumPyArray(......)\r\n>>> p4[p4[\'OID@\'] == 3]\r\narray([(3, 307500.   , 5029300.), (3, 308786.782, 5029300.),\r\n       (3, 308792.192, 5028500.), (3, 307500.   , 5028500.),\r\n       (3, 307500.   , 5029300.)],\r\n      dtype=[(\'OID@\', \'<i4\'), (\'Shape@X\', \'<f8\'), (\'Shape@Y\', \'<f8\')])\r\n\r\nTiming\r\n  p0  2.31 ms \xc2\xb1 27.2 \xc2\xb5s per loop (mean \xc2\xb1 std. dev. of 7 runs, 100 loops each)\r\n  p1  5 ms \xc2\xb1 42.5 \xc2\xb5s per loop (mean \xc2\xb1 std. dev. of 7 runs, 100 loops each)\r\n  p2  4.88 ms \xc2\xb1 143 \xc2\xb5s per loop (mean \xc2\xb1 std. dev. of 7 runs, 100 loops each)\r\n  p3  4.93 ms \xc2\xb1 135 \xc2\xb5s per loop (mean \xc2\xb1 std. dev. of 7 runs, 100 loops each)\r\n  p4  4.85 ms \xc2\xb1 110 \xc2\xb5s per loop (mean \xc2\xb1 std. dev. of 7 runs, 100 loops each)\r\nReferences:\r\n----------\r\n\r\narcpy.da.SearchCursor\r\n\r\nhttp://pro.arcgis.com/en/pro-app/arcpy/data-access/searchcursor-class.htm\r\n  ---------------------------------------------------------------------\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nfrom textwrap import indent\r\nimport numpy as np\r\nimport arcpy\r\n\r\nimport warnings\r\nwarnings.simplefilter(\'ignore\', FutureWarning)\r\n\r\nfrom arraytools.fc_tools._common import fc_info, tweet\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=3, linewidth=80, precision=3, suppress=True,\r\n                    threshold=50, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\n__all__ = [\'_cursor_array\',\r\n           \'_geo_array\',\r\n           \'_get_shapes\',\r\n           \'_ndarray\',\r\n           \'_two_arrays\',\r\n           \'_xy\', \'_xyID\', \'_xy_idx\',\r\n           \'orig_dest_pnts\',\r\n           \'obj_array\',\r\n           \'change_fld\',\r\n           \'_props\',\r\n           \'join_arr_fc\'\r\n           ]\r\n\r\n\r\n# ----------------------------------------------------------------------------\r\n# ---- Functions to get geometry and attributes ------------------------------\r\n# ----------------------------------------------------------------------------\r\n# (1) To ndarray/structured/recarry\r\n#\r\ndef _cursor_array(in_fc, full=True):\r\n    """"""Return the the points for a geometry object using a searchcursor.\r\n\r\n    in_fc :\r\n        the featureclass\r\n    full :\r\n        True: \'SHAPE@\', False: [\'SHAPE@X\', \'SHAPE@Y\' ]\r\n    """"""\r\n    shp = [[\'SHAPE@X\', \'SHAPE@Y\'], \'SHAPE@\'][full]\r\n    if full:\r\n        a = [np.asarray(row[0].__geo_interface__[\'coordinates\'])\r\n             for row in arcpy.da.SearchCursor(in_fc, shp)]\r\n    else:\r\n        a = [row for row in arcpy.da.SearchCursor(in_fc, shp,\r\n                                                  explode_to_points=True)]\r\n    a = np.asarray(a).squeeze()\r\n    return a\r\n\r\n\r\ndef _geo_array(polys):\r\n    """"""Convert polygon objects to arrays\r\n    """"""\r\n    arrays = [np.asarray(pt.__geo_interface__[\'coordinates\']).squeeze()\r\n              for pt in polys]  # for p in pt]\r\n    return arrays\r\n\r\n\r\ndef _get_shapes(in_fc):\r\n    """"""Get shapes from a featureclass, in_fc, using SHAPE@ returning\r\n       [<Polygon object at....>, ... (<Polygon object at....>]\r\n    """"""\r\n    with arcpy.da.SearchCursor(in_fc, \'SHAPE@\') as cursor:\r\n        a = [row[0] for row in cursor]\r\n    return a\r\n\r\n\r\ndef _ndarray(in_fc, to_pnts=True, flds=None, SR=None):\r\n    """"""Convert featureclass geometry (in_fc) to a structured ndarray including\r\n    options to select fields and specify a spatial reference.\r\n\r\n    Requires\r\n    --------\r\n    in_fc : string\r\n        input featureclass\r\n    to_pnts : boolean\r\n        True, convert the shape to points. False, centroid returned.\r\n    flds : string or list of strings\r\n      - \'*\' for all\r\n      - others : \'OID@\', \'Shape\',  [\'SHAPE@X\', \'SHAPE@Y\'], or specify\r\n    Note:\r\n    -----\r\n    You cannot use the \'SHAPE@\' field\r\n    Example:\r\n    --------\r\n    a = _ndarray(in_fc, True, [\'OID@\',\' SHAPE@X\', \'SHAPE@Y\', None]\r\n    """"""\r\n    if flds is None:\r\n        bad = [\'OID\', \'Geometry\', \'Shape_Length\', \'Shape_Area\']\r\n        f0 = [""OID@"", ""SHAPE@X"", ""SHAPE@Y""]\r\n        f1 = [i.name for i in arcpy.ListFields(in_fc)\r\n              if i.type not in bad]\r\n        flds = f0 + f1\r\n    if SR is None:\r\n        desc = arcpy.da.Describe(in_fc)\r\n        SR = desc[\'spatialReference\']\r\n    args = [in_fc, flds, """", SR, to_pnts, (None, None)]\r\n    cur = arcpy.da.SearchCursor(*args)\r\n    a = cur._as_narray()\r\n    del cur\r\n    return a\r\n\r\n\r\ndef _two_arrays(in_fc, both=True, split=True):\r\n    """"""Send to a numpy structured/array and split it into a geometry array\r\n    and an attribute array.  They can be joined back later if needed.\r\n\r\n    Note\r\n    ----\r\n        The geometry array is returned as an object array.  See the\r\n        main documentation\r\n\r\n    Requires:\r\n    --------\r\n\r\n    functions:\r\n        _xyID\r\n            function to get geometry array\r\n        _ndarray\r\n            function to get the x, y, id array and attribute array\r\n        fc_info(in_fc)\r\n            function needed to return fc properties\r\n    parameters:\r\n        both\r\n            True, to return both arrays, False to return just geometry\r\n        split\r\n            True, split points by their geometry groups as an object array;\r\n            False, a sequential array with shape = (N,)\r\n    variables:\r\n\r\n    >>> dt_a = [(\'IDs\', \'<i4\'), (\'Xs\', \'<f8\'), (\'Ys\', \'<f8\')]\r\n    >>> dt_b = [(\'IDs\', \'<i4\'), (\'Xc\', \'<f8\'), (\'Yc\', \'<f8\')]\r\n    >>> dt_b.extend(b.dtype.descr[2:])\r\n\r\n        Extend the dtype using the attribute dtype minus geometry and id\r\n    """"""\r\n    a = _xyID(in_fc, to_pnts=True)\r\n    shp_fld, oid_fld, SR, shp_type = fc_info(in_fc)\r\n    dt_a = [(\'IDs\', \'<i4\'), (\'Xs\', \'<f8\'), (\'Ys\', \'<f8\')]\r\n    dt_b = [(\'IDs\', \'<i4\'), (\'Xc\', \'<f8\'), (\'Yc\', \'<f8\')]\r\n    a.dtype = dt_a\r\n    b = None\r\n    if split:\r\n        ids = np.unique(a[\'IDs\'])\r\n        w = np.where(np.diff(a[\'IDs\']))[0] + 1\r\n        a = np.split(a, w)\r\n        a = np.array([[ids[i], a[i][[\'Xs\', \'Ys\']]] for i in range(len(ids))])\r\n    if both:\r\n        b = _ndarray(in_fc, to_pnts=False, flds=None, SR=None)\r\n        dt_b.extend(b.dtype.descr[2:])\r\n        b.dtype = dt_b\r\n    return a, b\r\n\r\n\r\ndef _xy(in_fc):\r\n    """"""Convert featureclass geometry (in_fc) to a simple 2D point array.\r\n    See _xyID if you need id values.\r\n    """"""\r\n    flds = [\'SHAPE@X\', \'SHAPE@Y\']\r\n    args = [in_fc, flds, None, None, True, (None, None)]\r\n    cur = arcpy.da.SearchCursor(*args)\r\n    a = cur._as_narray()\r\n    N = len(a)\r\n    a = a.view(dtype=\'float64\')\r\n    a = a.reshape(N, 2)\r\n    a = np.copy(a, order=\'C\')\r\n    del cur\r\n    return a\r\n\r\n\r\ndef _xyID(in_fc, to_pnts=True):\r\n    """"""Convert featureclass geometry (in_fc) to a simple 2D structured array\r\n    with ID, X, Y values. Optionally convert to points, otherwise centroid.\r\n    """"""\r\n    flds = [\'OID@\', \'SHAPE@X\', \'SHAPE@Y\']\r\n    args = [in_fc, flds, None, None, to_pnts, (None, None)]\r\n    cur = arcpy.da.SearchCursor(*args)\r\n    a = cur._as_narray()\r\n    a.dtype = [(\'IDs\', \'<i4\'), (\'Xs\', \'<f8\'), (\'Ys\', \'<f8\')]\r\n    del cur\r\n    return a\r\n\r\n\r\ndef _xy_idx(in_fc):\r\n    """"""Convert featureclass geometry (in_fc) to a simple 2D point array with\r\n    float64 data type and a separate index array to preserve id values\r\n    ***Best version comparied to _two_arrays\r\n    """"""\r\n    flds = [\'OID@\', \'SHAPE@X\', \'SHAPE@Y\']\r\n    args = [in_fc, flds, None, None, True, (None, None)]\r\n    cur = arcpy.da.SearchCursor(*args)\r\n    a = cur._as_narray()\r\n    N = len(a)\r\n    idx = np.zeros((N, 2), dtype=\'int\')\r\n    idx[:, 0] = a[\'OID@\']\r\n    id_n = np.cumsum(np.bincount(idx[:, 0]))\r\n    diff = np.diff(id_n, n=1)\r\n    s = [np.arange(i) for i in diff]\r\n    idx[:, 1] = np.hstack(s)\r\n    a = a[[\'SHAPE@X\', \'SHAPE@Y\']]\r\n    a = a.view(dtype=\'float64\')\r\n    a = a.reshape(N, 2)\r\n    del cur\r\n    return a, idx\r\n\r\n\r\ndef orig_dest_pnts(fc, SR):\r\n    """"""Convert sequential points to origin-destination pairs to enable\r\n    construction of a line.\r\n\r\n    Notes:\r\n    -----\r\n    a : array\r\n\r\n    >>> arcpy.da.FeatureClassToNumPyArray(\r\n            in_table=fc,\r\n            field_names=[""OID@"",""Shape@X"", ""Shape@Y""],\r\n            where_clause=None\r\n            spatial_reference=""2951"")\r\n            explode_to_points=False, skip_nulls=False,\r\n            null_value=None, sql_clause=(None, None)\r\n\r\n    out : from_to_pnts(fc)\r\n\r\n    >>> arcpy.da.ExtendTable(\r\n         in_table=fc,\r\n         table_match_field=\'OBJECTID\',  # normally\r\n         in_array = out,                # the array you created\r\n         array_match_field=\'IDs\',       # created by this script\r\n         append_only=True)\r\n\r\n    Sample:\r\n        fc = \'C:/Git_Dan/a_Data/arcpytools_demo.gdb/polylines_pnts\'\r\n\r\n        SR = \'2951\'\r\n    """"""\r\n    a = arcpy.da.FeatureClassToNumPyArray(fc, [""OID@"", ""Shape@X"", ""Shape@Y""],\r\n                                          spatial_reference=SR)\r\n    in_names = list(a.dtype.names)\r\n    kinds = [\'<i4\', \'<f8\', \'<f8\', \'<f8\', \'<f8\']\r\n    out_names = [\'IDs\', \'X_0\', \'Y_0\', \'X_1\', \'Y_1\']\r\n    dt = list(zip(out_names, kinds))\r\n    out = np.zeros(a.shape[0], dtype=dt)\r\n    arrs = [a[i] for i in in_names]\r\n    X_t = np.roll(a[in_names[-2]], -1)\r\n    Y_t = np.roll(a[in_names[-1]], -1)\r\n    arrs.append(X_t)\r\n    arrs.append(Y_t)\r\n    for i in range(len(out_names)):\r\n        out[out_names[i]] = arrs[i]\r\n    return out\r\n\r\n\r\ndef obj_array(in_fc):\r\n    """"""Convert an featureclass geometry to an object array.\r\n    The array must have an ID field.  Remove any other fields except\r\n    IDs, Xs and Ys or whatever is used by the featureclass.\r\n\r\n    Requires\r\n    --------\r\n        _xyID and a variant of group_pnts\r\n    """"""\r\n    def _group_pnts_(a, key_fld=\'IDs\', shp_flds=[\'Xs\', \'Ys\']):\r\n        """"""see group_pnts in tool.py""""""\r\n        returned = np.unique(a[key_fld], True, True, True)\r\n        uniq, idx, inv, cnt = returned\r\n        from_to = list(zip(idx, np.cumsum(cnt)))\r\n#        from_to = [[idx[i-1], idx[i]] for i in range(1, len(idx))]\r\n        subs = [a[shp_flds][i:j] for i, j in from_to]\r\n        groups = [sub.view(dtype=\'float\').reshape(sub.shape[0], -1)\r\n                  for sub in subs]\r\n        return groups\r\n    #\r\n    a = _xyID(in_fc)\r\n    a_s = _group_pnts_(a, key_fld=\'IDs\', shp_flds=[\'Xs\', \'Ys\'])\r\n    a_s = np.asarray(a_s)\r\n    return a_s\r\n\r\n\r\ndef change_fld(flds):\r\n    """"""Convert the field types to array friendly ones.\r\n    """"""\r\n    info = [(fld.type, fld.name, fld.length) for fld in flds]\r\n    dt = []\r\n    for i in info:\r\n        if i[0] in (\'OID\', \'Integer\', \'Long\', \'Short\'):\r\n            dt.append((i[1], \'<i4\'))\r\n        elif i[0] in (\'Double\', \'Single\', \'Float\'):\r\n            dt.append((i[1], \'<f8\'))\r\n        else:\r\n            dt.append(i[1], ""{}{}"".format(\'U\', i[2]))\r\n    return dt\r\n\r\n\r\ndef _props(a_shape, prn=True):\r\n    """"""Get some basic shape geometry properties.\r\n\r\n    Note:\r\n    ----\r\n        `a_shape`, is a single shape.\r\n        A searchcursor will return a list of geometries, so you should slice\r\n        even if there is only one shape.\r\n    """"""\r\n    if not hasattr(a_shape, \'__geo_interface__\'):\r\n        tweet(""Requires a \'shape\', your provided a {}"".format(type(a_shape)))\r\n        return None\r\n    coords = a_shape.__geo_interface__[\'coordinates\']\r\n    SR = a_shape.spatialReference\r\n    props = [\'type\', \'isMultipart\', \'partCount\', \'pointCount\', \'area\',\r\n             \'length\', \'length3D\', \'centroid\', \'trueCentroid\', \'firstPoint\',\r\n             \'lastPoint\', \'labelPoint\']\r\n    props2 = [[\'SR Name\', SR.name], [\'SR Factory code\', SR.factoryCode]]\r\n    t = ""\\n"".join([""{!s:<12}: {}"".format(i, a_shape.__getattribute__(i))\r\n                   for i in props])\r\n    t = t + ""\\n"" + ""\\n"".join([""{!s:<12}: {}"".format(*i) for i in props2])\r\n    tc = \'{!r:}\'.format(np.array(coords))\r\n    tt = t + ""\\nCoordinates\\n"" + indent(tc, \'....\')\r\n    if prn:\r\n        print(tt)\r\n    else:\r\n        return tt\r\n\r\n\r\n# (11)_join_array ... code section .....\r\ndef join_arr_fc(a, in_fc, out_fld=\'Result_\', OID_fld=\'OID@\'):\r\n    """"""Join an array to a featureclass table using matching fields, usually\r\n    an object id field.\r\n\r\n    Requires:\r\n    --------\r\n\r\n    a :\r\n        an array of numbers or text with ndim=1\r\n    out_fld :\r\n        field name for the results\r\n    in_fc :\r\n        input featureclass\r\n    in_flds :\r\n        list of fields containing the OID@ field as a minimum\r\n\r\n    ExtendTable (in_table, table_match_field,\r\n                 in_array, array_match_field, {append_only})\r\n\r\n    """"""\r\n    N = len(a)\r\n    dt_a = [(\'IDs\', \'<i4\'), (out_fld, a.dtype.str)]\r\n    out = np.zeros((N,), dtype=dt_a)\r\n    out[\'IDs\'] = [row[0] for row in arcpy.da.SearchCursor(in_fc, OID_fld)]\r\n    out[out_fld] = a\r\n    arcpy.da.ExtendTable(in_fc, OID_fld, out, \'IDs\', True)\r\n    return out\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n#\r\ndef _cross_3pnts(a):\r\n    """"""Requires 3 points on a plane:\r\n    """"""\r\n    a = np.asarray(a)\r\n    p0, p1, p2 = a\r\n    u, v = a[1:] - a[0]  # p1 - p0, p2 - p0\r\n    # u = unit_vector(u)\r\n    # v = unit_vector(v)\r\n    eq = np.cross(u, v)  # Cross product times one of the points\r\n    d = sum(eq * p0)\r\n    if d > 0.0:\r\n        eq /= d\r\n        d /= d\r\n    else:\r\n        d = 0.0\r\n    return eq, d\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n      - print the script source name.\r\n      - run the _demo\r\n    """"""\r\n    from _common import fc_info, tweet\r\n#    print(""Script... {}"".format(script))\r\n#    in_fc = r""C:\\Git_Dan\\a_Data\\testdata.gdb\\square2""\r\n#    in_fc = r""C:\\Git_Dan\\a_Data\\testdata.gdb\\Carp_5x5km""   # full 25 polygons\r\n#    in_fc = r""C:\\Git_Dan\\a_Data\\testdata.gdb\\Polygon""\r\n#    in_fc = r\'C:\\Git_Dan\\a_Data\\arcpytools_demo.gdb\\Can_geom_sp_LCC\'\r\n#    in_fc = r""C:\\Git_Dan\\a_Data\\arcpytools_demo.gdb\\Can_0_big_3""\r\n#    in_fc = r""C:\\Data\\Canada\\CAN_adm0.gdb\\CAN_0_sp""\r\n#    in_fc = r""C:\\Git_Dan\\a_Data\\testdata.gdb\\Polygon_pnts""\r\n#    flds = [\'OBJECTID\', \'Text_fld\']\r\n#    oid, vals = flds[0], flds[1:]\r\n#    arr = arcpy.da.TableToNumPyArray(in_fc, flds)\r\n#\r\n#    in_tbl = r""C:\\Git_Dan\\arraytools\\Data\\numpy_demos.gdb\\sample_10k""\r\n#    in_flds = [\'OBJECTID\', \'County\', \'Town\']\r\n#    a = arcpy.da.TableToNumPyArray(in_tbl, in_flds)\r\n#    c = concat_flds(a[\'County\'], a[\'Town\'], sep=\'...\', name=\'Locale\')\r\n#    c_id = np.zeros((len(c), ), dtype=[(\'IDs\', \'<i8\')])\r\n#    c_id[\'IDs\'] = np.arange(1, len(c) + 1)\r\n'"
all_scripts/field_statistics.py,26,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   field_statistics.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2018-02-11\r\n:Purpose:  Descriptive statistics for tables using numpy.\r\n:\r\n:References:\r\n:  https://github.com/numpy/numpy/blob/master/numpy/lib/nanfunctions.py\r\n:  _replace_nan(a, val) -  mask = np.isnan(a) - to get the mask\r\n:\r\n:  a = [1, 2, np.nan, 3, np.nan, 4]\r\n:  _, mask = _replace_nan(a, 0)  # for mean\r\n:  mask = array([False, False,  True, False,  True, False], dtype=bool)\r\n:\r\n: ---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\n\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\n# ---- skewness and kurtosis section -----------------------------------------\r\n\r\ndef skew_kurt(a, avg, var_x, std_x, col=True, mom=\'both\'):\r\n    """"""Momental and unbiased skewness\r\n\r\n    Emulates the nan functions approach to calculating these parameters\r\n    when data contains nan values.\r\n\r\n    Requires\r\n    ---------\r\n    a :\r\n        an array of float/double values where there are at least 3 non-nan\r\n        numbers in each column.  This is not checked since this situation\r\n        should never arise in real world data sets that have been checked.\r\n    moment :\r\n        both, skew or kurt  to return the moments\r\n\r\n    Notes:\r\n    -----\r\n        >>> a= np.arange(16.).reshape(4,4)\r\n        >>> mask = [0, 5, 10, 15]\r\n        >>> masked_array = np.where(a == mask, np.nan, a)\r\n    """"""\r\n#    a, mask = _replace_nan(a, 0.)  # produce a masked of the nan values\r\n    if len(a.shape) == 1:\r\n        ax = 0\r\n    else:\r\n        ax = [1, 0][col]\r\n#    # ---- mean section ----\r\n    mask = np.isnan(a)\r\n    cnt = np.sum(~mask, axis=ax, dtype=np.intp, keepdims=False)\r\n    diff = a - avg\r\n    sqrd = diff * diff\r\n    cubed = sqrd * diff\r\n    fourP = sqrd * sqrd\r\n    x_3 = np.nansum(cubed, axis=ax)\r\n    x_4 = np.nansum(fourP, axis=ax)\r\n    skew_m = x_3 / (cnt * (std_x**3))\r\n    kurt_m = x_4 / (cnt * (var_x * var_x))\r\n    # skew_u = skew_m*((cnt**2)/((cnt-1)*(cnt-2)))  # could add if needed\r\n    if mom == \'skew\':\r\n        return skew_m\r\n    elif mom == \'kurt\':\r\n        return kurt_m\r\n    elif mom == \'both\':\r\n        return skew_m, kurt_m\r\n\r\n\r\n# -----------------------------------------------------------------------------\r\n# ---- arraytools functions ----\r\n# ------------------------------\r\n\r\ndef tweet(msg):\r\n    """"""Produce a message (msg)for both arcpy and python\r\n    """"""\r\n    m = ""{}"".format(msg)\r\n    arcpy.AddMessage(m)\r\n    print(m)\r\n\r\n\r\n# (1) frmt_rec .... code section ... from frmts.py in arraytools\r\n#  frmt_rec requires _col_format\r\ndef _col_format(a, c_name=""c00"", deci=0):\r\n    """"""Determine column format given a desired number of decimal places.\r\n    Used by frmt_struct.\r\n\r\n    `a` : column\r\n        A column in an array.\r\n    `c_name` : text\r\n        column name\r\n    `deci` : int\r\n        Desired number of decimal points if the data are numeric\r\n\r\n    Notes:\r\n    -----\r\n        The field is examined to determine whether it is a simple integer, a\r\n        float type or a list, array or string.  The maximum width is determined\r\n        based on this type.\r\n\r\n        Checks were also added for (N,) shaped structured arrays being\r\n        reformatted to (N, 1) shape which sometimes occurs to facilitate array\r\n        viewing.  A kludge at best, but it works for now.\r\n    """"""\r\n    a_kind = a.dtype.kind\r\n    if a_kind in (\'i\', \'u\'):  # ---- integer type\r\n        w_, m_ = [\':> {}.0f\', \'{:> 0.0f}\']\r\n        col_wdth = len(m_.format(a.max())) + 1\r\n        col_wdth = max(len(c_name), col_wdth) + 1  # + deci\r\n        c_fmt = w_.format(col_wdth, 0)\r\n    elif a_kind == \'f\' and np.isscalar(a[0]):  # ---- float type with rounding\r\n        w_, m_ = [\':> {}.{}f\', \'{:> 0.{}f}\']\r\n        a_max, a_min = np.round(np.sort(a[[0, -1]]), deci)\r\n        col_wdth = max(len(m_.format(a_max, deci)),\r\n                       len(m_.format(a_min, deci))) + 1\r\n        col_wdth = max(len(c_name), col_wdth) + 1\r\n        c_fmt = w_.format(col_wdth, deci)\r\n    else:  # ---- lists, arrays, strings. Check for (N,) vs (N,1)\r\n        if a.ndim == 1:  # ---- check for (N, 1) format of structured array\r\n            a = a[0]\r\n        col_wdth = max([len(str(i)) for i in a])\r\n        col_wdth = max(len(c_name), col_wdth) + 1  # + deci\r\n        c_fmt = ""!s:>"" + ""{}"".format(col_wdth)\r\n    return c_fmt, col_wdth\r\n\r\n\r\ndef frmt_rec(a, deci=2, use_names=True, prn=True):\r\n    """"""Format a structured array with a mixed dtype.\r\n    :Requires\r\n    :-------\r\n    : a - a structured/recarray\r\n    : deci - to facilitate printing, this value is the number of decimal\r\n    :        points to use for all floating point fields.\r\n    : _col_format - does the actual work of obtaining a representation of\r\n    :  the column format.\r\n    :Notes\r\n    :-----\r\n    :  It is not really possible to deconstruct the exact number of decimals\r\n    :  to use for float values, so a decision had to be made to simplify.\r\n    """"""\r\n    dt_names = a.dtype.names\r\n    N = len(dt_names)\r\n    c_names = [[""C{:02.0f}"".format(i) for i in range(N)], dt_names][use_names]\r\n    # ---- get the column formats from ... _col_format ----\r\n    dts = []\r\n    wdths = []\r\n    pair = list(zip(dt_names, c_names))\r\n    for i in range(len(pair)):\r\n        fld, nme = pair[i]\r\n        c_fmt, col_wdth = _col_format(a[fld], c_name=nme, deci=deci)\r\n        dts.append(c_fmt)\r\n        wdths.append(col_wdth)\r\n    row_frmt = "" "".join([(\'{\' + i + \'}\') for i in dts])\r\n    hdr = [""!s:>"" + ""{}"".format(wdths[i]) for i in range(N)]\r\n    hdr2 = "" "".join([""{"" + hdr[i] + ""}"" for i in range(N)])\r\n    header = ""--n--"" + hdr2.format(*c_names)\r\n    header = ""\\n{}\\n{}"".format(header, ""-""*len(header))\r\n    txt = [header]\r\n    # ---- check for structured arrays reshaped to (N, 1) instead of (N,) ----\r\n    len_shp = len(a.shape)\r\n    idx = 0\r\n    for i in range(a.shape[0]):\r\n        if len_shp == 1:  # ---- conventional (N,) shaped array\r\n            row = "" {:03.0f} "".format(idx) + row_frmt.format(*a[i])\r\n        else:             # ---- reformatted to (N, 1)\r\n            row = "" {:03.0f} "".format(idx) + row_frmt.format(*a[i][0])\r\n        idx += 1\r\n        txt.append(row)\r\n    msg = ""\\n"".join([i for i in txt])\r\n    if prn:\r\n        print(msg)\r\n    else:\r\n        return msg\r\n\r\n\r\n# -----------------------------------------------------------------------------\r\n#\r\ndef cal_stats(in_fc, col_names):\r\n    """"""Calculate stats for an array of double types, with nodata (nan, None)\r\n    :  in the column.\r\n    :Requires:\r\n    :---------\r\n    : in_fc - input featureclass or table\r\n    : col_names - the columns... numeric (floating point, double)\r\n    :\r\n    :Notes:\r\n    :------  see the args tuple for examples of nan functions\r\n    :  np.nansum(b, axis=0)   # by column\r\n    :  np.nansum(b, axis=1)   # by row\r\n    :  c_nan = np.count_nonzero(~np.isnan(b), axis=0) count nan if needed\r\n    """"""\r\n    a = arcpy.da.FeatureClassToNumPyArray(in_fc, col_names)  # ""*"")\r\n    b = a.view(np.float).reshape(len(a), -1)\r\n    if len(a.shape) == 1:\r\n        ax = 0\r\n    else:\r\n        ax = [1, 0][True]  # ax = [1, 0][colwise]  colwise= True\r\n    mask = np.isnan(b)\r\n    cnt = np.sum(~mask, axis=ax, dtype=np.intp, keepdims=False)\r\n    n_sum = np.nansum(b, axis=0)\r\n    n_mean = np.nanmean(b, axis=0)\r\n    n_var = np.nanvar(b, axis=0)\r\n    n_std = np.nanstd(b, axis=0)\r\n    sk, kurt = skew_kurt(b, avg=n_mean, var_x=n_var, std_x=n_std,\r\n                         col=True, mom=\'both\')\r\n    args = (col_names, cnt, n_sum, np.nanmin(b, axis=0), np.nanmax(b, axis=0),\r\n            np.nanmedian(b, axis=0), n_mean, n_std, n_var, sk, kurt)\r\n    return col_names, args\r\n\r\n\r\ndef stats_tbl(col_names, args):\r\n    """"""Produce the output table\r\n    :   (\'N_\', \'<i4\'), (\'N_nan\', \'<i4\')\r\n    """"""\r\n    d = [(i, \'<f8\')\r\n         for i in [\'Sum\', \'Min\', \'Max\', \'Med\', \'Avg\',\r\n                   \'Std\', \'Var\', \'Skew\', \'Kurt\']]\r\n    dts = [(\'Field\', \'<U15\'), (\'N\', \'<i4\')] + d\r\n    rows = len(col_names)\r\n    cols = len(dts)\r\n    z = np.empty(shape=(rows,), dtype=dts)\r\n    for i in range(cols):\r\n        z[z.dtype.names[i]] = args[i]\r\n    return z\r\n\r\n\r\nif len(sys.argv) == 1:\r\n    in_fc = r\'C:\\GIS\\Tools_scripts\\Statistics\\Stats_demo_01.gdb\\pnts_2K_normal\'\r\n    flds = arcpy.ListFields(in_fc)\r\n    col_names = [fld.name for fld in flds if fld.type == \'Double\']\r\n    out_tbl = None\r\nelse:\r\n    in_fc = sys.argv[1]\r\n    col_names = sys.argv[2]\r\n    col_names = col_names.split(\';\')\r\n    out_tbl = sys.argv[3]\r\n\r\ncol_names, args = cal_stats(in_fc, col_names)  # calculate statistics\r\nz = stats_tbl(col_names, args)                 # produce the table\r\n\r\nmsg = frmt_rec(z, prn=False)  # fancy printout\r\ntweet(""\\n{}\\nSaving results to .... {}"".format(""-""*60, out_tbl))\r\ntweet(""Stats results...\\n{}"".format(msg))\r\n\r\nif not (out_tbl in (None, \'#\', \'\', \'None\')):\r\n    arcpy.da.NumPyArrayToTable(z, out_tbl)\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n'"
all_scripts/field_stats.py,27,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\nfield_stats\r\n===========\r\n\r\nScript :   field_stats.py\r\n\r\nAuthor :   Dan.Patterson@carleton.ca\r\n\r\nModified : 2018-03-21\r\n\r\nPurpose :  Descriptive statistics for tables using numpy.\r\n\r\nReferences:\r\n[1]\r\nhttps://github.com/numpy/numpy/blob/master/numpy/lib/nanfunctions.py\r\n\r\n  _replace_nan(a, val) -  mask = np.isnan(a) - to get the mask\r\n\r\n>>> a = [1, 2, np.nan, 3, np.nan, 4]\r\n>>> _, mask = _replace_nan(a, 0)  # for mean\r\n>>> mask = array([False, False,  True, False,  True, False], dtype=bool)\r\n\r\n: ---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\n\r\nimport sys\r\nimport numpy as np\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\n\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\nif \'prn\' not in locals().keys():\r\n    try:\r\n        from arraytools.frmts import prn\r\n    except:\r\n        prn = print\r\n\r\n\r\ndef freq(a, flds=None):\r\n    """"""Frequency and crosstabulation\r\n\r\n    `a` : array\r\n       input structured array\r\n\r\n    `flds` : string or list\r\n       fields/columns to use in the analysis\r\n\r\n    Notes\r\n    -----\r\n    (1) slice the input array by the classification fields\r\n    (2) sort the sliced array using the flds as sorting keys\r\n    (3) use unique on the sorted array to return the results\r\n    (4) a quick histogram to get the counts until numpy 1.12 can be used\r\n        then ship the results back.  only uni and vals is needed. The\r\n        rest is for testing and future work.\r\n    """"""\r\n    if flds is None:\r\n        return None\r\n    elif isinstance(flds, (str)):\r\n        flds = [flds]\r\n    a = a[flds]  # (1)\r\n    idx = np.argsort(a, axis=0, order=flds)  # (2)\r\n    a_sort = a[idx]\r\n    uniq, counts = np.unique(a_sort, return_counts=True)  # (3)\r\n    dt = uniq.dtype.descr + [(\'Count\', \'<i4\')]\r\n    fr = np.zeros_like(uniq, dtype=dt)\r\n    names = fr.dtype.names\r\n    vals = uniq, counts\r\n    for i in range(len(names)):\r\n        fr[names[i]] =  vals[i]\r\n    return fr\r\n\r\n\r\ndef summ(a, cls_flds, uniq, sum_flds):\r\n    """"""sum the input field\r\n    : a is the large array sliced by the classification fields\r\n    : uniq - unique classes\r\n    :\r\n    """"""\r\n    to_sum = a[cls_flds]\r\n    out_sum = []\r\n    for cl in uniq:\r\n        rows = a[to_sum == cl]\r\n        out_sum.append(np.nansum(rows[sum_flds]))  # use nansum\r\n    return out_sum\r\n\r\n# ---- skewness and kurtosis section -----------------------------------------\r\n\r\ndef skew_kurt(a, avg, var_x, std_x, col=True, mom=\'both\'):\r\n    """"""Momental and unbiased skewness\r\n\r\n    Emulates the nan functions approach to calculating these parameters\r\n    when data contains nan values.\r\n\r\n    Requires\r\n    ---------\r\n    a :\r\n        an array of float/double values where there are at least 3 non-nan\r\n        numbers in each column.  This is not checked since this situation\r\n        should never arise in real world data sets that have been checked.\r\n    moment :\r\n        both, skew or kurt  to return the moments\r\n\r\n    Notes:\r\n    -----\r\n        >>> a= np.arange(16.).reshape(4,4)\r\n        >>> mask = [0, 5, 10, 15]\r\n        >>> masked_array = np.where(a == mask, np.nan, a)\r\n    """"""\r\n#    a, mask = _replace_nan(a, 0.)  # produce a masked of the nan values\r\n    if len(a.shape) == 1:\r\n        ax = 0\r\n    else:\r\n        ax = [1, 0][col]\r\n#    # ---- mean section ----\r\n    mask = np.isnan(a)\r\n    cnt = np.sum(~mask, axis=ax, dtype=np.intp, keepdims=False)\r\n    diff = a - avg\r\n    sqrd = diff * diff\r\n    cubed = sqrd * diff\r\n    fourP = sqrd * sqrd\r\n    x_3 = np.nansum(cubed, axis=ax)\r\n    x_4 = np.nansum(fourP, axis=ax)\r\n    skew_m = x_3 / (cnt * (std_x**3))\r\n    kurt_m = x_4 / (cnt * (var_x * var_x))\r\n    # skew_u = skew_m*((cnt**2)/((cnt-1)*(cnt-2)))  # could add if needed\r\n    if mom == \'skew\':\r\n        return skew_m\r\n    elif mom == \'kurt\':\r\n        return kurt_m\r\n    elif mom == \'both\':\r\n        return skew_m, kurt_m\r\n\r\n\r\ndef cal_stats(a):\r\n    """"""Calculate stats for an array of double types, with nodata (nan, None)\r\n    in the column.\r\n\r\n    Notes\r\n    -----\r\n    see the args tuple for examples of nan functions::\r\n\r\n        >>> np.nansum(b, axis=0)   # by column\r\n        >>> np.nansum(b, axis=1)   # by row\r\n        >>> c_nan = np.count_nonzero(~np.isnan(b), axis=0) count nan if needed\r\n    """"""\r\n    if len(a.shape) == 1:\r\n        ax = 0\r\n    else:\r\n        ax = [1, 0][True]  # ax = [1, 0][colwise]  colwise= True\r\n    mask = np.isnan(a)\r\n    n = len(a)\r\n    cnt = np.sum(~mask, axis=ax, dtype=np.intp, keepdims=False)\r\n    n_sum = np.nansum(a, axis=0)\r\n    n_min = np.nanmin(a, axis=0)\r\n    n_max = np.nanmax(a, axis=0)\r\n    n_mean = np.nanmean(a, axis=0)\r\n    n_med = np.nanmedian(a, axis=0)\r\n    n_std = np.nanstd(a, axis=0)\r\n    n_var = np.nanvar(a, axis=0)\r\n    col_names = [\'N\', \'n\', \'sum\', \'min\', \'max\', \'mean\', \'median\',\r\n                 \'std\', \'var\', \'skew\', \'kurt\']\r\n    sk, kurt = skew_kurt(a, avg=n_mean, var_x=n_var, std_x=n_std,\r\n                         col=True, mom=\'both\')\r\n    args = [n, cnt, n_sum, n_min, n_max, n_mean, n_med, n_std, n_var, sk, kurt]\r\n    z = list(zip(col_names, args))\r\n    s = """".join([""\\n{:<6} {}"".format(*i) for i in z])\r\n    return s\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n'"
all_scripts/field_stats_orig.py,23,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   field_stats.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2017-07-08\r\n:Purpose:  Descriptive statistics for tables using numpy.\r\n:\r\n:References:\r\n:  https://github.com/numpy/numpy/blob/master/numpy/lib/nanfunctions.py\r\n:  _replace_nan(a, val) -  mask = np.isnan(a) - to get the mask\r\n:\r\n:  a = [1, 2, np.nan, 3, np.nan, 4]\r\n:  _, mask = _replace_nan(a, 0)  # for mean\r\n:  mask = array([False, False,  True, False,  True, False], dtype=bool)\r\n:\r\n: ---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\n\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\r\n      \'float\': \'{: 0.3f}\'.format}\r\n\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\n# ---- skewness and kurtosis section -----------------------------------------\r\n\r\ndef skew_kurt(a, avg, var_x, std_x, col=True, mom=\'both\'):\r\n    """"""Momental and unbiased skewness\r\n    :Emulates the nan functions approach to calculating these parameters\r\n    :when data contains nan values.\r\n    :Requires:\r\n    :---------\r\n    :  a - an array of float/double values where there are at least 3 non-nan\r\n    :      numbers in each column.  This is not checked since this situation\r\n    :      should never arise in real world data sets that have been checked.\r\n    :  moment - both, skew or kurt  to return the moments\r\n    :Notes:\r\n    :------\r\n    : a= np.arange(16.).reshape(4,4)\r\n    : mask = [0, 5, 10, 15]\r\n    : masked_array = np.where(a == mask, np.nan, a)\r\n    """"""\r\n#    a, mask = _replace_nan(a, 0.)  # produce a masked of the nan values\r\n    if len(a.shape) == 1:\r\n        ax = 0\r\n    else:\r\n        ax = [1, 0][col]\r\n#    # ---- mean section ----\r\n    mask = np.isnan(a)\r\n    cnt = np.sum(~mask, axis=ax, dtype=np.intp, keepdims=False)\r\n    diff = a - avg\r\n    sqrd = diff * diff\r\n    cubed = sqrd * diff\r\n    fourP = sqrd * sqrd\r\n    x_3 = np.nansum(cubed, axis=ax)\r\n    x_4 = np.nansum(fourP, axis=ax)\r\n    skew_m = x_3 / (cnt * (std_x**3))\r\n    kurt_m = x_4 / (cnt * (var_x * var_x))\r\n    # skew_u = skew_m*((cnt**2)/((cnt-1)*(cnt-2)))  # could add if needed\r\n    if mom == \'skew\':\r\n        return skew_m\r\n    elif mom == \'kurt\':\r\n        return kurt_m\r\n    elif mom == \'both\':\r\n        return skew_m, kurt_m\r\n\r\n\r\n# -----------------------------------------------------------------------------\r\n# functions\r\n\r\ndef tweet(msg):\r\n    """"""Produce a message (msg)for both arcpy and python\r\n    """"""\r\n    m = ""{}"".format(msg)\r\n    arcpy.AddMessage(m)\r\n    print(m)\r\n    print(arcpy.GetMessages())\r\n\r\n\r\ndef cal_stats(in_fc, col_names):\r\n    """"""Calculate stats for an array of double types, with nodata (nan, None)\r\n    :  in the column.\r\n    :Requires:\r\n    :---------\r\n    : in_fc - input featureclass or table\r\n    : col_names - the columns... numeric (floating point, double)\r\n    :\r\n    :Notes:\r\n    :------  see the args tuple for examples of nan functions\r\n    :  np.nansum(b, axis=0)   # by column\r\n    :  np.nansum(b, axis=1)   # by row\r\n    :  c_nan = np.count_nonzero(~np.isnan(b), axis=0) count nan if needed\r\n    """"""\r\n    a = arcpy.da.FeatureClassToNumPyArray(in_fc, col_names)  # ""*"")\r\n    b = a.view(np.float).reshape(len(a), -1)\r\n    if len(a.shape) == 1:\r\n        ax = 0\r\n    else:\r\n        ax = [1, 0][True]  # ax = [1, 0][colwise]  colwise= True\r\n    mask = np.isnan(b)\r\n    cnt = np.sum(~mask, axis=ax, dtype=np.intp, keepdims=False)\r\n    n_sum = np.nansum(b, axis=0)\r\n    n_mean = np.nanmean(b, axis=0)\r\n    n_var = np.nanvar(b, axis=0)\r\n    n_std = np.nanstd(b, axis=0)\r\n    sk, kurt = skew_kurt(b, avg=n_mean, var_x=n_var, std_x=n_std, col=True, mom=\'both\')\r\n    args = (col_names, n_sum, np.nanmin(b, axis=0), np.nanmax(b, axis=0),\r\n            np.nanmedian(b, axis=0), n_mean, n_std, n_var, sk, kurt)\r\n    return col_names, args\r\n\r\n\r\ndef stats_tbl(col_names, args):\r\n    """"""Produce the output table\r\n    :   (\'N_\', \'<i4\'), (\'N_nan\', \'<i4\')\r\n    """"""\r\n    d = [(i, \'<f8\')\r\n         for i in [\'Sum\', \'Min\', \'Max\', \'Med\', \'Avg\',\r\n                   \'Std\', \'Var\', \'Skew\', \'Kurt\']]\r\n    dts = [(\'Fld\', \'<U15\')] + d\r\n    rows = len(col_names)\r\n    cols = len(dts)\r\n    z = np.empty(shape=(rows,), dtype=dts)\r\n    for i in range(cols):\r\n        z[z.dtype.names[i]] = args[i]\r\n    return z\r\n\r\n\r\nif len(sys.argv) == 1:\r\n    in_fc = r\'C:\\GIS\\Tools_scripts\\Statistics\\Stats_demo_01.gdb\\pnts_2000_norm\'\r\n    flds = arcpy.ListFields(in_fc)\r\n    col_names = [fld.name for fld in flds if fld.type == \'Double\']\r\n    out_tbl = None\r\nelse:\r\n    in_fc = sys.argv[1]\r\n    col_names = sys.argv[2]\r\n    col_names = col_names.split(\';\')\r\n    out_tbl = sys.argv[3]\r\n\r\ncol_names, args = cal_stats(in_fc, col_names)  # calculate statistics\r\nz = stats_tbl(col_names, args)                 # produce the table\r\n\r\ntweet(""\\n{}\\nSaving results to .... {}"".format(""-""*60, out_tbl))\r\ntweet(""Stats results...\\n{}\\n{}"".format(z.dtype.names, z))\r\n\r\nif out_tbl is not None:\r\n    arcpy.da.NumPyArrayToTable(z, out_tbl)\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n'"
all_scripts/frequency.py,7,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\nfrequency\r\n=========\r\n\r\nScript :   frequency.py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified : 2018-04-06\r\n\r\nPurpose :\r\n    To supplant the Frequency tool for those that don\'t have an\r\n    advanced license.\r\n\r\nUseage :\r\n    Load the toolbox into Pro and run the tool script from there.\r\n\r\nReference:\r\n----------\r\n\r\n[1]\r\n`frequency tool`__:\r\n\r\n__http://desktop.arcgis.com/en/arcmap/latest/tools/analysis-toolbox/frequency.htm\r\n\r\nNotes:\r\n------\r\n\r\n- to_array = arcpy.da.TableToNumPyArray(r""C:\\folder\\sample.dbf"", ""*"")\r\n- arcpy.da.NumPyArrayToTable(from_array, r""C:\\folder_tbl\\test.gdb\\out"")\r\n\r\nDev Info:\r\n---------\r\n\r\n  tbx - C:\\GIS\\Tools_scripts\\Table_tools\\Table_tools.tbx\r\n  script - C:\\GIS\\Tools_scripts\\Table_tools\\Scripts\\frequency.py\r\n  arcpy.Tabletools.Frequency(""polygon_demo"",\r\n             r""C:\\GIS\\Tools_scripts\\Table_tools\\Table_tools.gdb\\f2"",\r\n             ""Test;main_part"", None)\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nimport numpy.lib.recfunctions as rfn\r\nfrom arraytools.fc_tools._common import tweet\r\nfrom arraytools.frmts import prn_rec\r\nimport arcpy\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\r\n      \'float\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\ndef freq(a, flds=None):\r\n    """"""Frequency and crosstabulation\r\n\r\n    `a` : array\r\n       input structured array\r\n\r\n    `flds` : string or list\r\n       fields/columns to use in the analysis\r\n\r\n    Notes\r\n    -----\r\n    (1) slice the input array by the classification fields\r\n    (2) sort the sliced array using the flds as sorting keys\r\n    (3) use unique on the sorted array to return the results\r\n    (4) a quick histogram to get the counts until numpy 1.12 can be used\r\n        then ship the results back.  only uni and vals is needed. The\r\n        rest is for testing and future work.\r\n    """"""\r\n    if flds is None:\r\n        return None\r\n    elif isinstance(flds, (str)):\r\n        flds = [flds]\r\n    a = a[flds]  # (1)\r\n    idx = np.argsort(a, axis=0, order=flds)  # (2)\r\n    a_sort = a[idx]\r\n    final = np.unique(a_sort, return_index=True, return_inverse=True,\r\n                      return_counts=True)  # (3)\r\n    uni = final[0]\r\n    first = final[1]\r\n    clas = final[2]\r\n    cases = np.arange(len(uni)).tolist()\r\n    cases.append(cases[-1] + 1)\r\n    count = np.histogram(clas, cases)  # (4)\r\n    dt = (uni.dtype.descr)\r\n    dt.append((\'count\', \'<i4\'))\r\n    vals = count[0]\r\n    return uni, first, clas, cases, count, vals\r\n\r\n\r\ndef summ(a, cls_flds, uniq, sum_flds):\r\n    """"""sum the input field\r\n    : a is the large array sliced by the classification fields\r\n    : uniq - unique classes\r\n    :\r\n    """"""\r\n    to_sum = a[cls_flds]\r\n    out_sum = []\r\n    for cl in uniq:\r\n        rows = a[to_sum == cl]\r\n        out_sum.append(np.nansum(rows[sum_flds]))  # use nansum\r\n    return out_sum\r\n\r\n\r\ndef _testing():\r\n    """"""Testing information\r\n    """"""\r\n    in_tbl = r""C:\\Git_Dan\\arraytools\\Data\\numpy_demos.gdb\\pnts_2K_normal""\r\n    out_tbl = None\r\n    cls_flds = \'Text01;Text02\'\r\n    sum_flds = \'C_0;C_1;C_2;Norm;Unif\'\r\n    return in_tbl, out_tbl, cls_flds, sum_flds\r\n\r\n\r\ndef _tool_():\r\n    """"""Tool information\r\n    """"""\r\n    in_tbl = sys.argv[1]           # input table\r\n    out_tbl = sys.argv[2]          # results table\r\n    cls_flds = sys.argv[3]         # classification field(s)\r\n    sum_flds = sys.argv[4]         # fields for doing sums on\r\n    return in_tbl, out_tbl, cls_flds, sum_flds\r\n\r\n# ----- Begin main code section ---------------------------------------------\r\n#\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    in_tbl, all_flds, cls_flds, sum_flds = _testing()\r\nelse:\r\n    testing = False\r\n    in_tbl, out_tbl, cls_flds, sum_flds = _tool_()\r\n\r\ncls_flds = cls_flds.split("";"")  # tidy up the two field inputs\r\nsum_flds = sum_flds.split("";"")\r\n\r\na = arcpy.da.TableToNumPyArray(in_tbl, ""*"")  # use the full array\'s data\r\n\r\nuni, first, clas, cases, count, vals = freq(a, cls_flds)  # do freq analysis\r\n\r\n# perform the summary results\r\nnew_vals = [vals]\r\nnew_names = [\'count\']\r\ntweet(""sum flds = {}"".format(sum_flds))\r\n\r\nif sum_flds[0] not in (\'#\', None, \'None\', """", \'\'):\r\n    for i in sum_flds:\r\n        fld_sums = summ(a, cls_flds, uni, i)  # do the sums using summ\r\n        new_names.append(\'sum_\' + i)\r\n        new_vals.append(fld_sums)\r\n\r\n## create the output array and return the table\r\n#b = rfn.append_fields(uni, names=new_names, data=new_vals, usemask=False)\r\n#\r\n#msg = ""\\nSummary array...\\n""\r\n#tweet(msg + prn_rec(b, prn=False))  # use arraytools.frmts.frmt_rec\r\n#\r\n#if not testing:\r\n#    arcpy.da.NumPyArrayToTable(b, out_tbl)\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Do nothing for now.\r\n    """"""\r\n#    in_tbl = r""C:\\GIS\\Tools_scripts\\Table_tools\\Table_tools.gdb\\polygon_demo""\r\n#    all_flds = ""*""\r\n#    cls_flds = \'Test;main_part\'\r\n#    sum_flds = \'Shape_Area;Shape_Length\'\r\n'"
all_scripts/frmts.py,59,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\nfrmts\r\n=====\r\n\r\nScript:   frmts.py\r\n\r\nAuthor:   Dan_Patterson@carleton.ca\r\n\r\nModified: 2018-11-02\r\n\r\nReferences:\r\n----------\r\n`np.set_printoptions` and `np.printoptions`\r\n\r\n`<https://github.com/numpy/numpy/blob/master/numpy/core/arrayprint.py>`_.\r\n\r\n>>> set_printoptions(precision=3, threshold=100, edgeitems=3, linewidth=80,\r\n                     suppress=True, nanstr=\'nan\', infstr=\'inf\',\r\n                     formatter=None, sign=None, floatmode=None, **kwarg)\r\n\r\n>>> with np.printoptions(precision=deci, linewidth=ln_wdth):\r\n        print(a)  # the original options will be reset after printing\r\n\r\nPurpose:\r\n--------\r\n\r\nThe prn2d function is used to provide a side-by-side view of 2, 3, and 4D\r\narrays.  Specifically, 3D and 4D arrays are useful and for testing\r\npurposes, seeing the dimensions in a different view can facilitate\r\nunderstanding.  For the best effect, the array shapes should be carefully\r\nconsidered. Some guidelines follow.  The middle \'r\' part of the shape is\r\nnot as affected as the combination of the \'d\' and \'c\' parts.  The array is\r\ntrimmed beyond the \'width\' parameter in prn2d.\r\n\r\nSample the 3D array shape so that the format (d, r, c)\r\nis within the 20-21 range for d*c ... for example::\r\n        integers          floats\r\n        2, r, 10  = 20    2, r, 8 = 16\r\n        3, r,  7  = 21    3, 4, 5 = 15\r\n        4, r,  5  = 20    4, r, 4 = 16\r\n        5, r,  4  = 20    5, r, 3 = 15\r\n\r\n>>> prn2d(a)  example for a =  np.arange(3*4*5).reshape(3, 4, 5)\r\n---------------------------------------------------\r\nArray...\r\n-shape (3, 4, 5), ndim 3\r\n  .  0  1  2  3  4    20 21 22 23 24    40 41 42 43 44\r\n  .  5  6  7  8  9    25 26 27 28 29    45 46 47 48 49\r\n  . 10 11 12 13 14    30 31 32 33 34    50 51 52 53 54\r\n  . 15 16 17 18 19    35 36 37 38 39    55 56 57 58 59\r\n  .   sub (0 )        : sub (1 )        : sub (2 )\r\n\r\nThe middle part of the shape should also be reasonable should you want\r\nto print the results:\r\n\r\nHow it works\r\n\r\n>>> a[...,0,:].flatten()\r\narray([ 0,  1,  2,  3,  4, 20, 21, 22, 23, 24, 40, 41, 42, 43, 44])\r\n\r\n>>> a[...,0,(0, 1, -2, -1)].flatten()\r\narray([ 0,  1,  3,  3, 20, 21, 23, 23, 40, 41, 43, 43])\r\n\r\n\r\nFunctions:\r\n=========\r\nhelp(<function name>) for help\r\n\r\n::\r\n\r\n    public  -  private...\r\n    deline  -  _pre\r\n    prn2d   - _check, _concat, _row_format\r\n    prn_ma - _fix\r\n    in_by   - _pre_num\r\n\r\n ... see __all__ for a complete listing\r\n\r\n1(a) col_hdr() :\r\n\r\nproduce column headers to align output for formatting purposes\r\n\r\n``.........1.........2.........3.........4.........5.........6.........\r\n123456789012345678901234567890123456789012345678901234567890123456789``\r\n\r\n----------------------------------------------------------------------\r\n\r\n\r\n1(b)  deline(a)::\r\n\r\n     shp = (2,3,4)\r\n     a = np.arange(np.prod(shp)).reshape(shp)\r\n     deline(a)\r\n\r\n     Main array...\r\n     ndim: 3 size: 24\r\n     shape: (2, 3, 4)\r\n     [[[ 0  1  2  3]\r\n       [ 4  5  6  7]\r\n       [ 8  9 10 11]]\r\n     a[1]....\r\n      [[12 13 14 15]\r\n       [16 17 18 19]\r\n       [20 21 22 23]]]\r\n\r\n(1c) in_by\r\n\r\nindent objects, added automatic support for arrays and optional line numbers\r\n::\r\n     a = np.arange(2*3*4).reshape(2,3,4)\r\n     print(art.in_by(a, hdr=\'---- header ----\', nums=True, prefix =""..""))\r\n     ---- header ----\r\n     00..[[[ 0  1  2  3]\r\n     01..  [ 4  5  6  7]\r\n     02..  [ 8  9 10 11]]\r\n     03..\r\n     04.. [[12 13 14 15]\r\n     05..  [16 17 18 19]\r\n     06..  [20 21 22 23]]]\r\n\r\n(1d)  redent(lines, spaces=4)\r\n::\r\n     a = np.arange(3*5).reshape(3,5)\r\n     >>> print(redent(a))\r\n     |    [[ 0  1  2  3  4]\r\n     |     [ 5  6  7  8  9]\r\n     |     [10 11 12 13 14]]\r\n\r\n(2) prn2d(a)\r\n::\r\n   a = np.arange(2*3*3).reshape(2,3,3)\r\n   array([[[ 0,  1,  2],\r\n           [ 3,  4,  5],\r\n           [ 6,  7,  8]],\r\n\r\n          [[ 9, 10, 11],\r\n           [12, 13, 14],\r\n           [15, 16, 17]]])\r\n   prn2d(a)\r\n   Array... shape (2, 3, 3), ndim 3, not masked\r\n    0,  1,  2     9, 10, 11\r\n    3,  4,  5    12, 13, 14\r\n    6,  7,  8    15, 16, 17\r\n   sub (0)       sub (1)\r\n\r\n(3) prn_ma\r\n::\r\n    :--------------------\r\n    :Masked array........\r\n    :  ndim: 2 size: 20\r\n    :  shape: (5, 4)\r\n    :\r\n    :... a[:5, :4] ...\r\n      -  1  2  3\r\n      4  5  6  7\r\n      8  -  -  -\r\n     12 13 14 15\r\n     16 17 18  -\r\n\r\n(4) pd and quick_prn\r\n    see code\r\n\r\n(5) prn_struct and prn_rec : main functions\r\n        _col_kind_width, _col_format,subsample\r\n\r\nprn_struct(b, edges=3, max_lines=10, width=100, deci=2)\r\n::\r\n    OBJECTID   f0   County  Town  Facility  Time\r\n    ----------------------------------------------\r\n             1    0 B       A_    Hall          26\r\n             2    1 C       C_    Hall          60\r\n             3    2 D       A_    Hall          42\r\n           ...  ...     ...   ...       ...\r\n            18   17 A       C_    Hall          59\r\n            19   18 C       C_    Hosp          37\r\n            20   19 B       B_    Hall          52\r\n\r\n    Array... shape: (20,)\r\n\r\nprn_rec(a, edges=5, max_rows=25, deci=2)\r\n::\r\n    Format ... C:/Git_Dan/arraytools/Data/sample_20.npy\r\n    record/structured array, with and without field names.\r\n    --n-- OBJECTID   f0  County  Town  Facility  Time``\r\n    -------------------------------------------------\r\n    000         1    0       B    A_      Hall    26\r\n    001         2    1       C    C_      Hall    60\r\n    002         3    2       D    A_      Hall    42\r\n\r\n\r\n(6) make_row_format\r\n::\r\n    make_row_format(dim=2, cols=3, a_kind=\'f\', deci=1,\r\n                    a_max=10, a_min=-10, prn=False)\r\n    \'{:6.1f}{:6.1f}{:6.1f}  {:6.1f}{:6.1f}{:6.1f}\'\r\n\r\nprn_\r\n::\r\n  prn_(a, deci=2, width=100, title=""Array"", prefix="". . "", prnt=True)\r\n\r\n  Array... ndim: 3  shape: (2, 3, 3)\r\n  . .   0  1  2    9 10 11\r\n  . .   3  4  5   12 13 14\r\n  . .   6  7  8   15 16 17\r\n\r\n(7)  prn_3d4d(a, deci=2, edgeitems=3, width=100, prnt=True)\r\n::\r\n    prn_3d4d(z)\r\n    Array... ndim 4  shape(1, 2, 3, 4)\r\n    |  0  1  2  3   12 13 14 15 |\r\n    |  4  5  6  7   16 17 18 19 |\r\n    |  8  9 10 11   20 21 22 23 |\r\n    |=> (0 2 3 4)\r\n\r\nNotes:\r\n=====\r\n\r\n**column numbering**\r\n\r\n>>> d = ((\'{:<10}\')*7).format(*\'0123456789\'), \'0123456789\'*7, \'-\'*70\r\n>>> s = \'\\n{}\\n{}\\n{}\'.format(args[0][1:], args[1][1:], args[2]) #*args)\r\n>>> print(s)\r\n             1         2         3         4         5         6\r\n    123456789012345678901234567890123456789012345678901234567890123456789\r\n\r\n\r\n**Getting default print options, then setting them back **\r\n\r\n>>> pr_opt = np.get_printoptions()\r\n>>> df_opt = "", "".join([""{}={}"".format(i, pr_opt[i]) for i in pr_opt])\r\n\r\n\r\n** Rearranging blocks into columns using np.c_[...] **\r\n\r\n>>>  a = np.arange(3*2*3).reshape(3, 2, 3)\r\n>>>  a_max = a.max()\r\n>>>  a_min = a.min()\r\n>>>  aa = np.c_[(a[0], a[1], a[2])]\r\n>>>  d, r, c = a.shape\r\n>>>  deci = 1\r\n>>>  a_kind = a.dtype.kind\r\n>>>  f = _format_row_test(d, r, c, a_kind, deci, a_max, a_min)\r\n::\r\nRow format given\r\nd 3, r 2, c 3\r\nkind i decimals 1\r\n{:3.0f}{:3.0f}{:3.0f}  {:3.0f}{:3.0f}{:3.0f}  {:3.0f}{:3.0f}{:3.0f}\r\n0123456789012345678901234567890123456789012345678901234567890123456789\r\n0         1         2         3         4         5         6\r\n\r\n>>>  r = `\\\\n`.join([f.format(*i) for i in aa])\r\n>>>  print(r)\r\n  0  1  2    6  7  8   12 13 14\r\n  3  4  5    9 10 11   15 16 17\r\n\r\n>>> # Now change dtype and decimals\r\n>>>  a_kind = \'f\'\r\n>>>  deci = 2\r\n>>>  f = _format_row_test(d, r, c, a_kind, deci, a_max, a_min)\r\n .... snip ....\r\n>>>  print(r)\r\n  0.00  1.00  2.00    6.00  7.00  8.00   12.00 13.00 14.00\r\n  3.00  4.00  5.00    9.00 10.00 11.00   15.00 16.00 17.00\r\n\r\n\r\n**all at once**\r\n\r\n>>> a\r\narray([[[ 0,  1,  2,  3],\r\n        [ 4,  5,  6,  7],\r\n        [ 8,  9, 10, 11]],\r\n       [[12, 13, 14, 15],\r\n        [16, 17, 18, 19],\r\n        [20, 21, 22, 23]]])\r\n\r\n>>>  s0, s1, s2 = a.shape\r\n>>>  b = a.swapaxes(2, 1).reshape(s0*s2, s1).T\r\n>>>  b\r\n  array([[ 0,  1,  2,  3, 12, 13, 14, 15],\r\n         [ 4,  5,  6,  7, 16, 17, 18, 19],\r\n         [ 8,  9, 10, 11, 20, 21, 22, 23]])\r\n\r\n\r\nMasked array info:\r\n------------------\r\n\r\n>>>  a.get_fill_value() # see default_filler dictionary\r\n>>>  a.set_fill_value(np.NaN)\r\n>>>  np.ma.maximum_fill_value(a)   -inf\r\n>>>  np.ma.minimum_fill_value(a)    inf\r\n>>>  default_filler =\r\n     {\'b\': True, \'c\': 1.e20 + 0.0j, \'f\': 1.e20, \'i\': 999999,\'O\': \'?\',\r\n      \'S\': b\'N/A\', \'u\': 999999,\'V\': \'???\',\'U\': sixu(\'N/A\')}\r\n\r\n\r\nOthers:\r\n------\r\n\r\n>>> b.transpose(1, 2, 0)[:,:,::-1]\r\n>>> # ** tip *** reorder from after transpose or even a swapaxes\r\n>>> # the ::-1 does the reversing... same as [...,::-1]\r\n\r\n\r\n----------\r\n""""""\r\n\r\n# pylint: disable=C0103\r\n# pylint: disable=R1710\r\n# pylint: disable=R0914\r\n\r\n# ---- imports, formats, constants ----\r\n\r\nimport sys\r\nfrom textwrap import dedent, indent\r\nimport numpy as np\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{!r: 0.3f}\'.format}\r\nedge = 3\r\nln_wdth = 100\r\nnp.set_printoptions(edgeitems=edge, linewidth=ln_wdth, precision=3,\r\n                    suppress=True, nanstr=\'-n-\', threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\npr_opt = np.get_printoptions()\r\ndf_opt = "", "".join([""{}={}"".format(i, pr_opt[i]) for i in pr_opt])\r\n\r\nscript = sys.argv[0]\r\n\r\n__all__ = [\'col_hdr\',            # column headers\r\n           \'xy_dist_headers\',    # table dtype format\r\n           \'deline\',             # remove excessive blank lines\r\n           \'in_by\',              # an indent variant with options\r\n           \'redent\',             # indent\r\n           \'_chunks\',            # take chunks of stuff\r\n           \'head_tail\',          # return the head/tail of a 1d array\r\n           \'_check\',             # helper functions\r\n           \'_slice_rows\',        #\r\n           \'_slice_cols\',        #\r\n           \'_slice_head_tail\',   #\r\n           \'_col_format\',        # printing section\r\n           \'prn_nd\',             # for ndarray\r\n           \'prn_ma\',             # for masked arrays\r\n           \'prn_rec\', \'pd_\',     # record/structured arrays\r\n           \'prn_struct\',         #\r\n           \'make_row_format\',    # a big helper function\r\n           \'prn_\',               # ndarray variant\r\n           \'prn\'        # ---- this def is used to call all the others ----\r\n           ]\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# (1) Short, or reused code section\r\n#\r\n# (1a) col_hdr ... code section .....\r\ndef col_hdr(num=8):\r\n    """"""Print numbers from 1 to 10*num to show column positions""""""\r\n    args = [((\'{:<10}\')*num).format(*\'0123456789\'),\r\n            \'0123456789\'*num, \'-\'*10*num]\r\n    s = ""\\n{}\\n{}\\n{}"".format(args[0][1:], args[1][1:], args[2])  # *args)\r\n    print(s)\r\n\r\n\r\ndef xy_dist_headers(N):\r\n    """"""Construct headers for the optional table output""""""\r\n    vals = np.repeat(np.arange(N), 2)\r\n    names = [\'X_{}\', \'Y_{}\']*N + [\'d_{}\']*(N-1)\r\n    vals = (np.repeat(np.arange(N), 2)).tolist() + [i for i in range(1, N)]\r\n    n = [names[i].format(vals[i]) for i in range(len(vals))]\r\n    f = [\'<f8\']*N*2 + [\'<f8\']*(N-1)\r\n    return list(zip(n, f))\r\n# ----------------------------------------------------------------------\r\n# (1b) deline ... code section .....\r\ndef deline(a, width=100, header=""Array..."", prefix=""  .""):\r\n    """"""Remove extraneous lines from array output.\r\n    More useful for long arrays with ndim >= 3\r\n\r\n    Requires:\r\n    --------\r\n    `a` : anything\r\n        anything that can be put into array form\r\n    `header` :\r\n        an optional header\r\n    `prefix` : text\r\n        could be just spaces or something like shown\r\n    """"""\r\n    def _pre(obj):\r\n        for line in obj.splitlines(False):\r\n            frmt = ""{}{}"".format(prefix, line)\r\n            yield frmt\r\n    # ----\r\n    if not isinstance(a, (list, tuple, np.ndarray)):\r\n        return a\r\n    a = np.asanyarray(a)\r\n    if a.dtype.kind not in (\'i\', \'u\', \'f\', \'c\'):\r\n        return a\r\n    header += "" shape: {} ndim: {}"".format(a.shape, a.ndim)\r\n    f1 = ("":arr[{}"" + "", :{}""*len(a.shape[1:]) + ""]"")\r\n    out = [header]\r\n    c = 0\r\n    for i in a:\r\n        a_s = f1.format(c, *i.shape)  # ---- uses f1 format above\r\n        out.append(a_s)\r\n        out.extend(_pre(str(i)))\r\n        c += 1\r\n    f = ""\\n"".join([i for i in out if i != prefix])\r\n    with np.printoptions(edgeitems=edge, linewidth=width):\r\n        print(f)\r\n    # ----\r\n\r\n\r\n# ---------------------------------------------------------------------------\r\n# (1c) in_by .... code section\r\ndef in_by(obj, hdr="""", line_nums=False, prefix=""   .""):\r\n    """"""A `textwrap.indent` variant for python 2.7 or a substitute for\r\n    any version of python.  The function stands for `indent by`.\r\n\r\n    Requires:\r\n    --------\r\n    `obj` : object that can be cast as string\r\n        obj to indent, List, tuple, ndarray converted to strings\r\n        first. You can use repr representation before using if needed.\r\n    `hdr` : text\r\n        optional header\r\n    `line_nums` : boolean\r\n        True to add line numbers\r\n    `prefix` : test\r\n        Text to use for indent ie \'  \' for 2 spaces or \'....\'\r\n\r\n    Reference:\r\n    ---------\r\n    [1] https://docs.python.org/3.7/library/textwrap.html for python > 3.3\r\n\r\n    Notes:\r\n    -----\r\n        Header and line numbers options added.\r\n    """"""\r\n    def _pre_num():\r\n        c = 0\r\n        for line in obj.splitlines(True):\r\n            if line_nums:\r\n                frmt = ""{:>02}{}{}"".format(c, prefix, line)\r\n            else:\r\n                frmt = ""{}{}"".format(prefix, line)\r\n            yield frmt\r\n            c += 1\r\n    #\r\n    if hdr != """":\r\n        hdr = ""\\n{}\\n"".format(hdr)\r\n    if isinstance(obj, (list, tuple, np.ndarray)):\r\n        obj = str(obj)\r\n    out = hdr + """".join(_pre_num())\r\n    print(out)\r\n    # ----\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# (1d) redent .... code section\r\ndef redent(lines, spaces=4):\r\n    """"""Strip and reindent by num_spaces, a sequence of lines\r\n    `lines` : text\r\n        Text or what can be made text\r\n        Use str() or repr() on the inputs if you want control on form\r\n\r\n    See also:\r\n    --------\r\n        See `in_by` for more options\r\n    """"""\r\n    lines = str(lines).splitlines()\r\n    sp = [len(ln) - len(ln.lstrip()) for ln in lines]\r\n    spn = "" ""*spaces\r\n    out = list(zip(lines, sp))\r\n    ret = ""\\n"".join([""{0}{1!s:>{2}}"".format(spn, *ln) for ln in out])\r\n    return ret\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# (1e) _chunks .... code section\r\ndef _chunks(s, n):\r\n    """"""Produce n-sized chunks from s.""""""\r\n    for start in range(0, len(s), n):\r\n        yield s[start:start+n]\r\n\r\n# ----------------------------------------------------------------------\r\n# (1f) head_tail  .... code section\r\n#\r\ndef head_tail(size=10, head=3, tail=None, fill=None):\r\n    """"""Slice `head` and `tail` elements of a 1D array of a given `size`.\r\n    Optionally, insert a middle `fill` element.\r\n\r\n    >>> head_tail(size=10, head=3, tail=None, fill=None)\r\n    [0, 1, 2, 7, 8, 9]\r\n    >>> head_tail(size=10, head=3, tail=None, fill=""..."")\r\n    [0, 1, 2, \'...\', 7, 8, 9]\r\n    """"""\r\n    if head is None:\r\n        head = 0\r\n    if tail is None:\r\n        tail = head\r\n    head, tail = [int(abs(i)) for i in [head, tail]]\r\n    r = np.arange(size).tolist()\r\n    if fill is None:\r\n        return r[:head] + r[-tail:]\r\n    return r[:head] + [fill] + r[-tail:]\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# ---- Format section ---- functions and constants must be used ----\r\n\r\n# (2) ---- constants and common functions----\r\n\r\nfloats = np.typecodes[\'AllFloat\']\r\nints = np.typecodes[\'AllInteger\']\r\nnums = floats + ints\r\n\r\n\r\ndef _check(a):\r\n    """"""Check dtype and max value for formatting information""""""\r\n    return a.shape, a.ndim, a.dtype.kind, np.nanmax(a), np.nanmin(a)\r\n\r\ndef _slice_rows(a, edge_rows=3):\r\n    """"""Split an array keeping `edge_rows` from the start and end of the array.\r\n    Used by `prn_struct` and `prn_rec`.\r\n    """"""\r\n    if a.shape[0] <= (edge_rows * 2):\r\n        return a\r\n    return np.hstack((a[:edge_rows], a[-edge_rows:]))  # top, bott\r\n\r\n\r\ndef _slice_cols(a, edge_cols=3):\r\n    """"""Split a structured array keeping `edge_cols from the start of an array.\r\n    Used by `prn_struct` and `prn_rec`.\r\n    """"""\r\n    names = a.dtype.names\r\n    return a[list(names[:edge_cols])]\r\n\r\n\r\ndef _slice_head_tail(a, edge_cols=3):\r\n    """"""Split a structuredarray keeping `edge_cols` from the start and end.\r\n    Used by `prn_struct` and `prn_rec`.\r\n    """"""\r\n    names = list(a.dtype.names)\r\n    shp = a.shape[0]\r\n    left = a[names[:edge_cols]]\r\n    right = a[names[-edge_cols:]]\r\n    dt_new = list(left.dtype.descr) + [(\'...\', \'3U\')] + list(right.dtype.descr)\r\n    z = np.zeros((shp,), dtype=dt_new)\r\n    for i in left.dtype.names:\r\n        z[i] = left[i]\r\n    z[\'...\'] = [\'...\'] * shp\r\n    for i in right.dtype.names:\r\n        z[i] = right[i]\r\n    del left, right, a\r\n    return z\r\n\r\n\r\ndef _row_format(a, sep=\'\', deci=0):\r\n    """"""Create format string from array parameters.\r\n\r\n    Parameters:\r\n    -----------\r\n    array parameters: _check function\r\n        a.shape, a.ndim, a.dtype.kind, a.max(), a.min()\r\n    sep : string\r\n        A separator for parts of the array\r\n    deci : integer\r\n        Number of decimal places to use for all floats\r\n\r\n    Requires:\r\n    ---------\r\n    constants : numbers\r\n        float and int as defined above\r\n    _check : function\r\n        Returns base array information\r\n    """"""\r\n    shp, ndim, kind, a_max, a_min = _check(a)  # get base array information\r\n    cols = shp[-1]\r\n    if kind in floats:\r\n        w_, m_ = [\':{}.{}f\', \'{:0.{}f}\']\r\n    elif kind in ints:\r\n        w_, m_ = [\':{}.0f\', \'{:0.0f}\']\r\n    else:\r\n        w_, m_ = [\'!s:>{}\', \'{}\']\r\n        deci = 0\r\n    m = max(len(m_.format(a_max, deci)), len(m_.format(a_min, deci))) + 1\r\n    w_fmt = w_.format(m, deci)\r\n    r_fmt = sep.join([\'{\' + w_fmt + \'}\' for i in range(cols)])\r\n    return r_fmt\r\n\r\n\r\ndef _col_kind_width(a, deci=0):\r\n    """"""Column properties for ndarray and recarray/structured array types.\r\n\r\n    Notes:\r\n        Used by _col_format, prn_rec and prn_struct.  It check the\r\n    length of the values in the field, rounded to `deci`mal places if needed,\r\n    That value to the field name length, if found, and returns the `max`.\r\n\r\n    sample output for a structured array :\r\n        [(\'i\', 5), (\'U\', 25), (\'i\', 5), (\'i\', 5)]\r\n\r\n    Requires:\r\n    ---------\r\n    constants : numbers\r\n        float and int as defined above\r\n    """"""\r\n    def _ckw_(a, name, deci):\r\n        """"""process for arrays arrays with named fields""""""\r\n        c_kind = a.dtype.kind\r\n        if (c_kind in floats) and (deci != 0):  # float with decimals\r\n            c_max, c_min = np.round([np.nanmin(a), np.nanmax(a)], deci)\r\n            c_width = len(max(str(c_min), str(c_max), key=len))\r\n        elif c_kind in nums:      # int, unsigned int, float wih no decimals\r\n            c_width = len(max(str(np.nanmin(a)), str(np.nanmax(a)), key=len))\r\n        elif c_kind in (\'U\', \'S\', \'s\'):\r\n            c_width = len(max(a, key=len))\r\n        else:\r\n            c_width = len(str(a))\r\n        c_width = max(len(name), c_width)\r\n        return [c_kind, c_width]\r\n    # ---- constants\r\n    # ---- call to _ckw_ ----\r\n    dtn = a.dtype.names\r\n    if dtn is None:  # ---- uniform dtype\r\n        return [_ckw_(a, name="""", deci=deci)]\r\n    return [_ckw_(a[name], name, deci=deci) for name in dtn]\r\n\r\n\r\ndef _col_format(c, c_name=""c00"", deci=0):\r\n    """"""Determine column format for an ndarray or structured array.  The\r\n    number of decimal places for float fields can be specified.\r\n    Used by prn_rec.\r\n\r\n    `c` : column\r\n        A column in an array.\r\n    `c_name` : text\r\n        column name for ndarrays of uniform dtype.  Ignored otherwise\r\n    `deci` : int\r\n        Desired number of decimal points if the data are numeric\r\n\r\n    Requires:\r\n    ---------\r\n    _col_kind_width : function\r\n        This function does the determination of column kind and width\r\n\r\n    Notes:\r\n    -----\r\n    To do all field `names`\r\n\r\n    >>> [_col_format(j) for j in [a[i] for i in names]]\r\n    [(\':> 6.0f\', 5), (\'!s:<26\', 25), (\':> 6.0f\', 5),\r\n     (\':> 6.0f\', 5), (\':> 4.0f\', 2)]\r\n    """"""\r\n    pairs = _col_kind_width(c, deci=deci)\r\n    form_width = []\r\n    for c_kind, c_width in pairs:\r\n        if c_kind in ints:  # ---- integer type\r\n            w_ = \':> {}.0f\'\r\n            c_width = max(len(c_name), c_width) + deci\r\n            c_format = w_.format(c_width, 0)\r\n        elif (c_kind in floats) and np.isscalar(c[0]):  # ---- float rounded\r\n            w_ = \':> {}.{}f\'\r\n            c_width = max(len(c_name), c_width) + deci\r\n            c_format = w_.format(c_width, deci)\r\n        else:\r\n            c_format = ""!s:<{}"".format(c_width)\r\n\r\n        form_width.append([c_format, c_width])\r\n    return form_width\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# (3) prn_nd .... code section\r\ndef prn_nd(a, deci=2, width=100, title=""Array"", prefix=""  ."", prnt=True):\r\n    """"""Format number arrays by row, and print\r\n\r\n    Parameters:\r\n    -----------\r\n    `a` : array\r\n        An array of int or float dtypes, 1, 2, 3 and 4D arrays tested.\r\n    `deci` - int\r\n        Decimal places for floating point numbers\r\n    `width` : int\r\n        Default width for onscreen and printing, output beyond this\r\n        length will be truncated with a warning.  Reshape to overcome.\r\n    `title` : string\r\n        The default title, change to provide more information.\r\n\r\n    Returns:\r\n    --------\r\n    Prints the array with the 1st dimension flattened-like by row\r\n\r\n    Notes:\r\n    -----\r\n    - `w_frmt` :  width formatter\r\n    - `m_frmt` :  max number formatter to get max. number of characters\r\n    """"""\r\n\r\n    def _concat(rows, r_fmt, width, prefix):\r\n        """"""print the subset to maximimum width""""""\r\n        end = ["""", ""....""][len(r_fmt.format(*rows[0])) > width]\r\n        txt = prefix\r\n        rw = [r_fmt.format(*v)[:width] + end for v in rows]\r\n        txt += (""\\n"" + prefix).join(rw)  # + ""\\n""\r\n        return txt\r\n\r\n    def d4_frmt(a_shp, a, txt, a_dim):\r\n        """"""Dealing with 4, 5 ?D arrays""""""\r\n        d4, d, r, c = a_shp\r\n        hdr = ""\\n"" + ""-""*25\r\n        fm = hdr + ""\\n-({}, + ({}, {}, {})""\r\n        if a_dim == 5:\r\n            fm = ""\\n--(.., {}, + ({}, {}, {})""\r\n        t = """"\r\n        for d3 in range(d4):\r\n            t += fm.format(d3, d, r, c) + ""\\n""\r\n            a_s = a[d3]\r\n            rows = [a_s[..., i, :].flatten() for i in range(r)]\r\n            t += _concat(rows, row_frmt, width, prefix)\r\n        return t\r\n    #\r\n    # ---- begin constructing the array format ----\r\n    txt = """"\r\n    a = np.asanyarray(a)\r\n    # ---- run _check ----\r\n    if a.ndim < 3:\r\n        if a.ndim == 2:\r\n            a = a.reshape((1,) + a.shape)\r\n        else:\r\n            return ""Array is not >= 2D""\r\n    #\r\n    a_shp, a_dim, a_kind, a_min, a_max = _check(a)  # get base array info\r\n    #\r\n    fv = """"\r\n    if np.ma.isMaskedArray(a):  # ----\r\n        a = np.ma.round(a, decimals=deci)\r\n        if a.dtype.kind in floats:\r\n            default_fill = np.ma.default_fill_value(a)\r\n            a.set_fill_value(default_fill)\r\n        else:\r\n            a.set_fill_value(np.iinfo(a.dtype).max)\r\n        fv = "", masked array, fill value {}"".format(a.get_fill_value())\r\n        #a = a.data\r\n    # ---- correct dtype, get formats ----\r\n    if (a_kind in nums) and (a_dim >= 3):\r\n        args = title, a_shp, a_dim, fv\r\n        txt = ""{}...\\n-shape {}, ndim {}{}"".format(*args)\r\n        d, r, c = a_shp[-3:]\r\n        row_frmt = _row_format(a, sep=\'\', deci=deci)\r\n        row_frmt = (row_frmt + ""  "") * d\r\n        if a_dim == 3:\r\n            rows = [a[..., i, :].flatten() for i in range(r)]\r\n            txt += ""\\n"" + _concat(rows, row_frmt, width, prefix)\r\n        elif a_dim == 4:\r\n            d4, d, r, c = a_shp\r\n            t = d4_frmt(a_shp, a, txt, a_dim)\r\n            txt += t\r\n        elif a_dim == 5:\r\n            d5, d4, d, r, c = a_shp\r\n            hdr = ""\\n"" + ""-""*25\r\n            for i in range(d5):\r\n                txt += hdr + \'\\n--({}, ..\'.format(i)\r\n                t = d4_frmt(a_shp[1:], a[i], txt, a_dim)\r\n                txt += t\r\n    else:\r\n        txt = ""Only integer and float arrays with ndim >= 2 supported""\r\n    if prnt:\r\n        with np.printoptions(precision=deci, linewidth=ln_wdth):\r\n            print(txt)\r\n    else:\r\n        return txt\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# (4) prn_ma .... code section\r\ndef prn_ma(a, edge=5, deci=2, width=100, prnt=True, prefix=""  .""):\r\n    """"""Format a masked array to preserve columns widths and style.\r\n\r\n    Parameters\r\n    ----------\r\n    `a` : masked array\r\n        A masked array\r\n    `prn` : Boolean\r\n        True to print\r\n    `prefix` : text\r\n        Can be """" for no indentation or ""   "" or the default\r\n\r\n    Returns\r\n    -------\r\n    Returns a print version of a masked array formatted with masked\r\n    values and appropriate spacing.\r\n    b = a.reshape(2,4,5) for 3d\r\n\r\n    Notes\r\n    -----\r\n    Get a string representation of the array.  Determine the maximum value\r\n    length of a string of the values in the array  and format each column\r\n    using that value.  Pad the result with a leader or replace the prefix\r\n    with \'\'\r\n    """"""\r\n    def _fix(v, tmp, prefix):\r\n        """""" sub array adjust""""""\r\n        r = [[\'[[\', "" ""], [\'[\', """"], [\']\', """"], [\']]\', """"]]\r\n        for i in r:\r\n            tmp = tmp.replace(i[0], i[1])\r\n        tmp0 = [i.strip().split(\' \') for i in tmp.split(\'\\n\')]\r\n        N = len(tmp0[0])\r\n        out = [""""]\r\n        for i in range(len(tmp0)):\r\n            N = len(tmp0[i])\r\n            out.append((frmt*N).format(*tmp0[i]))\r\n        jn = ""\\n"" + prefix\r\n        v += jn.join([i for i in out])\r\n        v += \'\\n\'\r\n        return v\r\n    # ---- main section ----\r\n#    np.set_printoptions(edgeitems=edge, linewidth=width, precision=3,\r\n#                    suppress=True, nanstr=\'-n-\', threshold=1000)\r\n    dim = a.ndim\r\n    shp = a.shape\r\n    a = np.ma.round(a, decimals=deci)\r\n    a_max = max(len(str(np.ma.max(a))), len(str(np.ma.min(a))))  # largest str\r\n    frmt = \'{:>\' + str(a_max + 1) + \'} \'\r\n    v = ""\\n:Masked array...\\n:shape: {}  ndim: {}\\n"".format(shp, dim)\r\n    if dim == 2:\r\n        v += ""\\n:.. a[:{}, :{}] ..."".format(*shp)\r\n        v = _fix(v, str(a), prefix)\r\n    elif dim == 3:\r\n        for d0 in range(shp[0]):  # dimension blocks\r\n            v += ""\\n:.. a[{}, :{}, :{}] ..."".format(d0, *a[d0].shape)\r\n            v = _fix(v, str(a[d0]), prefix)\r\n    if prnt:\r\n        print(v)\r\n    else:\r\n        return v\r\n\r\n# ----------------------------------------------------------------------\r\n# (5) pd and quick_prn\r\ndef pd_(a, deci=2, use_names=True, prnt=True):\r\n    """"""see help for `prn_rec`...""""""\r\n    ret = prn_rec(a, deci=deci, prnt=prnt)\r\n    return ret\r\n\r\ndef quick_prn(a, edges=3, max_lines=25, width=100, decimals=2):\r\n    """"""Format a structured array by setting the width so it hopefully wraps.\r\n    """"""\r\n    width = min(len(str(a[0])), width)\r\n    with np.printoptions(edgeitems=edges, threshold=max_lines, linewidth=width,\r\n                         precision=decimals, suppress=True, nanstr=\'-n-\'):\r\n        print(""\\nArray fields/values...:\\n{}\\n{}"".format(a.dtype.names, a))\r\n\r\ndef prn_q(a, rows=None, deci=2):\r\n    """"""Quick print a structured array.\r\n\r\n    rows : None or integer\r\n        None, prints all the rows. If an integer, prints [:rows] of the array.\r\n    deci : integer\r\n        Number of decimal places to use for float values.\r\n    """"""\r\n    cf = _col_format(a, deci=deci)  # ---- the big work done here\r\n    frmt = "" "".join([\'{\' + i[0] + \'}\' for i in cf])\r\n    hdr = "" "".join([\'{!s:<\' + str(i[1]) + \'}\' for i in cf])\r\n    if rows is None:\r\n        rows = a.shape[0]\r\n    print(hdr.format(*a.dtype.names))\r\n    for row in range(rows):\r\n        print(frmt.format(*a[row]))\r\n\r\n    # ----\r\n# ----------------------------------------------------------------------\r\n# (6) prn_rec and prn_struct .... code section\r\n#  both requires _c_kind_width, _col_format and subsample\r\n\r\ndef prn_struct(a, rows_m=25, cols_m=None, deci=2, width=100, prnt=True):\r\n    """"""Format a structured or recarray array.  See prn_rec for more details.\r\n    This variant adds row and column slicing with the `width` and\r\n    `max_lines` parameters.  Requires `subsample` and `_col_format`.\r\n    """"""\r\n    info = ""Array... shape: {}"".format(a.shape)\r\n    names = list(a.dtype.names)\r\n    dtn = list(a.dtype.names)\r\n    if cols_m is None:\r\n        cols_m = len(names)//2\r\n    rows_m = min(a.shape[0]//2, rows_m)\r\n    # ---- slice the rows\r\n    a = _slice_rows(a, edge_rows=rows_m)\r\n    form_width = _col_format(a, deci=deci)\r\n    dts = [i[0] for i in form_width]\r\n    wdths = [i[1] for i in form_width]\r\n    tot_width = sum(wdths)\r\n    if tot_width > width:  # ---- split wide arrays\r\n        cs = np.cumsum(wdths)\r\n        cols_m = min(np.sum(cs < width)//2, cols_m)\r\n        a = _slice_head_tail(a, cols_m)\r\n        dtn = list(a.dtype.names)\r\n        form_width = _col_format(a, deci=deci)  # _col_format again\r\n        dts = [i[0] for i in form_width]\r\n        wdths = [i[1] for i in form_width]\r\n    header = "" "".join([\'{\'+""!s:<{}"".format(i)+\'}\' for i in wdths])\r\n    h = header.format(*dtn)\r\n    print(""\\n{}\\n{}"".format(h, ""-""*len(h)))\r\n    dtf = "" "".join([\'{\' + i + \'}\' for i in dts])  #z[:, 0]])\r\n    if prnt:\r\n        for i in range(a.shape[0]): # <= rows_m:\r\n            print(dtf.format(*a[i]))\r\n        print(""\\n{}"".format(info))\r\n        print(""Head/tail rows: {}, columns: {}"".format(rows_m, cols_m))\r\n        return None\r\n    return a\r\n    # ---- done ----\r\n\r\n\r\ndef prn_rec(a, rows_m=25, cols_m=None, deci=2, width=100, prnt=True):\r\n    """"""Format a structured array with a mixed dtype.\r\n\r\n    NOTE : Can be called as `pd_(a, ... )` to emulate pandas dataframes\r\n        You should limit large arrays to a slice ie. a[:50]\r\n\r\n    Requires:\r\n    -------\r\n    `a` : array\r\n        A structured/recarray\r\n    `edges` : integer\r\n        Rows to keep from the start and end if max_rows is exceeded.\r\n    `deci` : int\r\n        To facilitate printing, this value is the number of decimal\r\n        points to use for all floating point fields.\r\n    `max_rows : integer\r\n        The number of rows to print before truncating to the `edges` option.\r\n        Change this to a larger value should you need to print whole arrays.\r\n    `subsample` : function\r\n        Requires this fucntion for sampling an array that exceeds max_rows.\r\n\r\n    Notes:\r\n    -----\r\n    `_col_format` : does the actual work of obtaining a representation of\r\n    the column format.\r\n\r\n    It is not really possible to deconstruct the exact number of decimals\r\n    to use for float values, so a decision had to be made to simplify.\r\n    """"""\r\n    names = a.dtype.names\r\n    if cols_m is None:\r\n        cols_m = len(names)\r\n    # slice off excess rows\r\n    if a.shape[0] > rows_m:\r\n        a = _slice_rows(a, edge_rows=rows_m)\r\n    # ---- get the column formats from ... _col_format ----\r\n    form_width = _col_format(a, deci=deci)\r\n    dts = [i[0] for i in form_width]\r\n    wdths = [i[1] for i in form_width]\r\n    # slice off excess columns\r\n    c_sum = np.cumsum(wdths)\r\n    N = len(np.where(c_sum < width)[0])\r\n    a = _slice_cols(a, edge_cols=N)\r\n    # slice the formats\r\n    tail = \'\'\r\n    if N < len(names):\r\n        tail = \' ...\'\r\n    row_frmt = "" "".join([(\'{\' + i + \'}\') for i in dts[:N]])\r\n    hdr = [""!s:<"" + ""{}"".format(wdths[i]) for i in range(N)]\r\n    hdr2 = "" "".join([""{"" + hdr[i] + ""}"" for i in range(N)])\r\n    header = "" id  "" + hdr2.format(*names[:N]) + tail\r\n    header = ""\\n{}\\n{}"".format(header, ""-""*len(header))\r\n    # ---- assemble the print string ----\r\n    txt = [header]\r\n    idx = 0\r\n    for i in range(a.shape[0]):\r\n        txt.append("" {:>03.0f} "".format(idx) + row_frmt.format(*a[i]) + tail)\r\n        idx += 1\r\n    msg = ""\\n"".join([i for i in txt])\r\n    if prnt:\r\n        print(msg)\r\n        return None\r\n    return msg\r\n\r\n# ----------------------------------------------------------------------\r\n# (7) prn_ ... code section .....\r\n#  prn_ requires make_row_format\r\ndef make_row_format(dim=3, cols=5, a_kind=\'f\', deci=1,\r\n                    a_max=10, a_min=-10, width=100, prnt=False):\r\n    """"""Format the row based on input parameters\r\n\r\n    `dim` - int\r\n        Number of dimensions\r\n    `cols` : int\r\n        Columns per dimension\r\n\r\n    `a_kind`, `deci`, `a_max` and `a_min` allow you to specify a data type,\r\n    number of decimals and maximum and minimum values to test formatting.\r\n    """"""\r\n    if a_kind not in nums:\r\n        a_kind = \'f\'\r\n    w_, m_ = [[\':{}.0f\', \'{:0.0f}\'], [\':{}.{}f\', \'{:0.{}f}\']][a_kind == \'f\']\r\n    m_fmt = max(len(m_.format(a_max, deci)), len(m_.format(a_min, deci))) + 1\r\n    w_fmt = w_.format(m_fmt, deci)\r\n    suffix = \'  \'\r\n    while m_fmt*cols*dim > width:\r\n        cols -= 1\r\n        suffix = \'.. \'\r\n    row_sub = ((\'{\' + w_fmt + \'}\')*cols + suffix)\r\n    row_frmt = (row_sub*dim).strip()\r\n    if prnt:\r\n        frmt = ""Row format: dim cols: ({}, {})  kind: {} decimals: {}\\n\\n{}""\r\n        print(dedent(frmt).format(dim, cols, a_kind, deci, row_frmt))\r\n        a = np.random.randint(a_min, a_max+1, dim*cols)\r\n        col_hdr(width//10)  # run col_hdr to produce the column headers\r\n        print(row_frmt.format(*a))\r\n    else:\r\n        return row_frmt\r\n\r\n\r\ndef prn_(a, deci=2, width=100, title=""Array"", prefix="". . "", prnt=True):\r\n    """"""Alternate format to prn_nd function.\r\n    Inputs are largely the same.\r\n    """"""\r\n    def _piece(sub, i, frmt, linewidth):\r\n        """"""piece together 3D chunks by row""""""\r\n        s0 = sub.shape[0]\r\n        block = np.hstack([sub[j] for j in range(s0)])\r\n        txt = """"\r\n        if i is not None:\r\n            fr = ("":arr[{}"" + "", :{}""*len(a.shape[1:]) + ""]\\n"")\r\n            txt = fr.format(i, *sub.shape)\r\n        for line in block:\r\n            ln = frmt.format(*line)[:linewidth]\r\n            end = [""\\n"", ""...\\n""][len(ln) >= linewidth]\r\n            txt += indent(ln + end, "". . "")\r\n        return txt\r\n    # ---- main section ----\r\n    out = ""\\n{}... ndim: {}  shape: {}\\n"".format(title, a.ndim, a.shape)\r\n    linewidth = width\r\n    if a.ndim <= 1:\r\n        return a\r\n    if a.ndim == 2:\r\n        a = a.reshape((1,) + a.shape)\r\n    # ---- pull the 1st and 3rd dimension for 3D and 4D arrays\r\n    frmt = make_row_format(dim=a.shape[-3],\r\n                           cols=a.shape[-1],\r\n                           a_kind=a.dtype.kind,\r\n                           deci=deci,\r\n                           a_max=a.max(),\r\n                           a_min=a.min(),\r\n                           width=width,\r\n                           prnt=False)\r\n    if a.ndim == 3:\r\n        s0, _, _ = a.shape\r\n        out += _piece(a, None, frmt, linewidth)  # ---- _piece ----\r\n    elif a.ndim == 4:\r\n        s0, _, _, _ = a.shape\r\n        for i in range(s0):\r\n            out = out + ""\\n"" + _piece(a[i], i, frmt, linewidth)  # ---- _piece\r\n    if prnt:\r\n        with np.printoptions(precision=deci, linewidth=width):\r\n            print(out)\r\n    else:\r\n        return out\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# (8)  ---- form prn_3d4d ----\r\ndef prn_3d4d(a, deci=2, edgeitems=3, width=100, prnt=True):\r\n    """"""Another variant for formatting arrays geared towards 3d and 4d\r\n    numeric and text arrays.  For object, structured arrays, see prn_rec\r\n    in arraytools.frmts\r\n    """"""\r\n    def _row_format(d, r, c, k, deci, a_min, a_max):\r\n        """"""abbreviated row format, see frmts.py in arraytools\r\n        """"""\r\n        if k in nums:\r\n            w_, m_ = [[\':{}.0f\', \'{:0.0f}\'], [\':{}.{}f\', \'{:0.{}f}\']][k == \'f\']\r\n        else:\r\n            w_, m_ = [\'!s:>{}\', \'{}\']\r\n            deci = 0\r\n        m = max(len(m_.format(a_max, deci)), len(m_.format(a_min, deci))) + 1\r\n        w_fmt = w_.format(m, deci)\r\n        r_fmt = ((\'{\' + w_fmt + \'}\') * c + \'\') * 1  # d\r\n        return r_fmt\r\n    #\r\n    def head_tail(s, e):\r\n        """"""Keep head and tail of array row/column indices [:e], [-e:]\r\n        if s = 10 and e = 3, then\r\n        h_t => array([ 0,  1,  2, -1,  5,  6,  7]) where -1 is a marker\r\n        """"""\r\n        h_t = np.arange(s)\r\n        if s > e*2:\r\n            h_t = np.concatenate([h_t[:e], [-1], h_t[-e:]])\r\n        return h_t\r\n    # ---- main section\r\n    # bail?\r\n    if (a.ndim not in (3, 4)) or (a.dtype.kind not in nums):\r\n        msg = ""Requires a 3D/4D numeric or text array. Kind in (i, f, U)))""\r\n        print(msg)\r\n        return msg\r\n    # (1) base information and reshape 3D to 4D array\r\n    if a.ndim == 3:\r\n        a = a.reshape((1,) + a.shape)\r\n    s4, s3, s2, s1 = a_shp = a.shape\r\n    # ---- start the format process ----\r\n    # (1) split the indices keeping the row, column edgeitems\r\n    e = edgeitems\r\n    s3_ = head_tail(s3, e)\r\n    s2_ = head_tail(s2, e)\r\n    # (2) assemble information for _row_format\r\n    d_, r_, c_ = a_shp[-3:]\r\n    if a.dtype.kind in (\'U\', \'S\', \'b\', \'O\', \'V\'):\r\n        n = int(a.dtype.str.lstrip(\'<^>|bOUSV\')) #+ 1\r\n        a_min = a_max = int(\'1\'*n)  # cheat to get a number len of string\r\n    elif a.dtype.kind in (\'i\', \'u\', \'f\'):\r\n        a_min = a.min()\r\n        a_max = a.max()\r\n    fm0 = _row_format(d_, r_, c_, a.dtype.kind, 2, a_min, a_max)  # d=1\r\n    split_0 = c_ > e*2  # boolean check\r\n    if split_0:  # e in place of c_\r\n        fm1 = _row_format(d_, r_, e, a.dtype.kind, 2, a_min, a_max)\r\n    # (3) process\r\n    t = ""Array... ndim {}  shape{}"".format(a.ndim, a.shape)\r\n    for k in range(s4):\r\n        for j in s2_:\r\n            row = []\r\n            for i in s3_:\r\n                r_ = a[k][i][j]\r\n                if split_0:\r\n                    sub = fm1.format(*r_[:e]) + "" ..."" + fm1.format(*r_[-e:])\r\n                else:\r\n                    sub = fm0.format(*r_)\r\n                if j == -1:\r\n                    row.append(""{!s:^{}}"".format("" . ."", len(sub)))\r\n                else:\r\n                    row.append(sub)\r\n            s = ""  "".join(row)\r\n            if len(s) > width:\r\n                s = s[:width] + ""...""\r\n            t += ""\\n|"" + s + "" |""\r\n        t += ""\\n|=> ({} {} {} {})\\n"".format(k, s3, s2, s1)\r\n    if prnt:\r\n        print(t)\r\n        return None\r\n    return t\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# (9)  ---- prn .... calls the other print methods\r\n#\r\ndef prn(a, rows=20, cols=3, deci=2, width=100, title="""", prnt=True):\r\n    """"""Calling function for all the array print options\r\n\r\n    References:\r\n    -----------\r\n    `<https://github.com/numpy/numpy/blob/\r\n    f85c71a6d16ab64695f07fde23e2c19104d36208/numpy/core/numerictypes.py>`_.\r\n    ::\r\n        prn_nd(a, deci=2, width=100, title=""Array"", prefix=""  ."", prnt=True)\r\n        prn_(a, deci=2, width=100, title=""Array"", prefix="". . "", prnt=True)\r\n        prn_ma(a, deci=2, prnt=True, prefix=""  ."")\r\n        prn_struct(a, rows_m=25, cols_m=None, deci=2, width=100, prnt=True)\r\n        prn_rec(a, rows_m=25, cols_m=None, deci=2, width=100, prnt=True)\r\n        prn_3d4d(a, deci=2, edgeitems=3, width=100, prnt=True)\r\n    """"""\r\n    #_kind_list = [\'b\', \'u\', \'i\', \'f\', \'c\', \'S\', \'U\', \'V\', \'O\', \'M\', \'m\']\r\n    kind = a.dtype.kind\r\n    dt = a.dtype\r\n    ndim = a.ndim\r\n    if title != """":\r\n        msg = ""\\n{}\\n{}"".format(\'-\'*30, title)\r\n        print(msg)\r\n    if np.ma.isMaskedArray(a):          # ---- use this for masked arrays\r\n        prn_ma(a, deci=deci, prnt=True, prefix=""  ."")\r\n    elif kind in (\'i\', \'u\', \'f\', \'c\'):  # ---- float-kind\r\n        v = ""{!s:<3} : {}"".format(kind, dt)\r\n        if ndim < 2:\r\n            print(a)\r\n        elif ndim == 2:\r\n            prn_(a)\r\n        elif ndim > 2:\r\n            if ndim <= 4:\r\n                prn_3d4d(a, deci=deci, edgeitems=cols, width=width, prnt=True)\r\n            else:\r\n                prn_nd(a, deci=deci, width=width, title=""Array"",\r\n                       prefix=""  ."", prnt=True)\r\n            #prn_(a)\r\n            #prn_3d4d(a, deci=deci, edgeitems=cols, width=width, prnt=True)\r\n    elif kind in (\'S\', \'U\'):            # ---- unicode, string\r\n        v = ""{!s:<3} : {}"".format(kind, dt)\r\n    elif kind in (\'V\', \'O\'):            # ---- void, object arrays\r\n        #v = ""{!s:<3} : {}"".format(kind, dt)\r\n        #prn_struct(a, rows_m=5, cols_m=None, deci=2, width=100, prnt=True)\r\n        prn_rec(a, rows_m=rows, cols_m=cols, deci=deci, width=width, prnt=prnt)\r\n    else:\r\n        v = ""{!s:<3} : {}"".format(kind, dt)\r\n        print(v)\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# (8)  ---- sample data ----\r\n\r\ndef _data():\r\n    """"""base file""""""\r\n    pth = _data.__code__.co_filename\r\n    pth = pth.replace(""frmts.py"", """")\r\n    a = np.load(pth + ""Data/points_2000.npy"")\r\n    b = np.load(pth + ""Data/sample_20.npy"")\r\n    c_d = np.load(pth + ""Data/masked_data.npy"")\r\n    c_m = np.load(pth + ""Data/masked_mask.npy"")\r\n    c = np.ma.MaskedArray(c_d, c_m)\r\n    d = np.load(pth + ""Data/ndim4.npy"")\r\n    return a, b, c, d\r\n\r\n# -------------------------\r\nif __name__ == ""__main__"":\r\n    """"""Main section...   """"""\r\n#    print(""Script... {}"".format(script))\r\n#    row_frmt = make_row_format()\r\n#    a, b, c, d = _data()\r\n'"
all_scripts/gaussian3d.py,7,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   .py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2017-xx-xx\r\n:Purpose:  tools for working with numpy arrays\r\n:Useage:\r\n:\r\n:References:\r\n:  https://stackoverflow.com/questions/40622203/how-to-plot-3d-gaussian-\r\n""    distribution-with-matplotlib\r\n:  https://stackoverflow.com/questions/25720600/generating-3d-gaussian-\r\n:    distribution-in-python\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\n\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\r\n      \'float\': \'{: 0.3f}\'.format}\r\n\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\nimport matplotlib.pyplot as plt\r\nfrom mpl_toolkits.mplot3d import Axes3D\r\nfrom scipy.stats import multivariate_normal\r\n\r\nx, y = np.mgrid[-1.0:1.0:30j, -1.0:1.0:30j]\r\n\r\n# Need an (N, 2) array of (x, y) pairs.\r\nxy = np.column_stack([x.flat, y.flat])\r\nmu = np.array([0.0, 0.0])\r\nsigma = np.array([.5, .5])\r\ncovariance = np.diag(sigma**2)\r\nz = multivariate_normal.pdf(xy, mean=mu, cov=covariance)\r\n# Reshape back to a (30, 30) grid.\r\nz = z.reshape(x.shape)\r\n\r\nfig = plt.figure()\r\n\r\nax = fig.add_subplot(111, projection=\'3d\')\r\n\r\n#ax.plot_surface(x,y,z)\r\nax.plot_wireframe(x,y,z)\r\n\r\nplt.show()\r\n\r\ndef _demo():\r\n    """"""\r\n    : -\r\n    """"""\r\n    pass\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n    _demo()\r\n\r\n'"
all_scripts/generate_plot.py,1,"b'import sys\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport matplotlib.lines as lines\r\n\r\ndef scatter(pts,show=True, label=False, connect=False, pairs=None,pos=1,row=1, col=1, new=False):\r\n    if(connect==True and pairs is None):\r\n        print(""ERROR: There must be a valid input for the \'pairs\' parameter to connect the points."")\r\n        return\r\n    if new==True:\r\n        plt.figure().add_subplot(row, col, pos)\r\n    else:\r\n        plt.subplot(row, col, pos)\r\n\r\n#    if show==True:\r\n#        plt.scatter(pts[:, 0], pts[:, 1])\r\n\r\n    if label==True:\r\n        lbl = np.arange(len(pts))\r\n        for label, xpt, ypt in zip(lbl, pts[:, 0], pts[:, 1]):\r\n            plt.annotate(label, xy=(xpt, ypt), xytext=(2, 2), size=8, textcoords=\'offset points\', ha=\'left\', va=\'bottom\')\r\n\r\n    if connect==True:\r\n        for pair in pairs:\r\n            i, j = pair\r\n            plt.plot([pts[i, 0], pts[j, 0]], [pts[i, 1], pts[j, 1]], c=\'r\')\r\n\r\n\r\ndef subplts(plots=1, by_col=True, max_rc=4):\r\n    """"""specify the num(ber) of subplots desired and return the rows\r\n    :  and columns to produce the subplots.\r\n    :  by_col - True for column oriented, False for row\r\n    :  max_rc - maximum number of rows or columns depending on by_col\r\n    """"""\r\n    row_col = (1, 1)\r\n    if by_col:\r\n        if plots <= max_rc:\r\n            row_col = (1, plots)\r\n        else:\r\n            row_col = (plots - max_rc, max_rc)\r\n    else:\r\n        if plots <= max_rc:\r\n            row_col = (plots, 1)\r\n        else:\r\n            row_col = (max_rc, plots - max_rc)\r\n    return row_col\r\n\r\n\r\ndef generate():\r\n    plt.show()\r\n    #plt.close()\r\n'"
all_scripts/geom.py,244,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\ngeom\r\n====\r\n\r\nScript :   geom.py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified : 2019-01-01\r\n\r\nPurpose :  tools for working with numpy arrays and geometry\r\n\r\nNotes:\r\n-----\r\n- Do not rely on the OBJECTID field for anything\r\n  http://support.esri.com/en/technical-article/000010834\r\n\r\n- When working with large coordinates, you should check to see whether a\r\n  translation about the origin (array - centre) produces different results.\r\n  This has been noted when calculating area using projected coordinates for the\r\n  Ontario.npy file.  The difference isn\'t huge, but subtracting the centre or\r\n  minimum from the coordinates produces area values which are equal but differ\r\n  slightly from those using the unaltered coordinates.\r\n\r\n\r\n**Functions**\r\n::\r\n    \'__all__\', \'__builtins__\', \'__cached__\', \'__doc__\', \'__file__\',\r\n    \'__loader__\', \'__name__\', \'__package__\', \'__spec__\', \'_arrs_\', \'_convert\',\r\n    \'_demo\', \'_densify_2D\', \'_flat_\', \'_new_view_\', \'_reshape_\', \'_test\',\r\n    \'_unpack\', \'_view_\', \'adjacency_edge\', \'angle_2pnts\', \'angle_between\',\r\n    \'angle_np\', \'angle_seq\', \'angles_poly\', \'areas\', \'as_strided\', \'azim_np\',\r\n    \'center_\', \'centers\', \'centroid_\', \'centroids\', \'circle\', \'convex\',\r\n    \'cross\', \'dedent\', \'densify\', \'dist_bearing\', \'dx_dy_np\', \'e_2d\',\r\n    \'e_area\', \'e_dist\', \'e_leng\', \'ellipse\', \'extent_\', \'fill_diagonal\', \'ft\',\r\n    \'hex_flat\', \'hex_pointy\', \'intersect_pnt\', \'knn\', \'lengths\', \'max_\',\r\n    \'min_\', \'nn_kdtree\', \'np\', \'p_o_p\', \'pnt_\', \'pnt_in_list\', \'pnt_on_poly\',\r\n    \'pnt_on_seg\', \'point_in_polygon\', \'radial_sort\', \'rectangle\',\r\n    \'remove_self\', \'rotate\', \'seg_lengths\', \'segment\', \'simplify\', \'stride\',\r\n    \'total_length\', \'trans_rot\', \'triangle\', \'xy_grid\'\r\n\r\nReferences:\r\n----------\r\nSee ein_geom.py for full details and examples\r\n\r\n`<https://www.redblobgames.com/grids/hexagons/>`_\r\n\r\n`<https://en.wikipedia.org/wiki/Centroid#Centroid_of_polygon>`_\r\n\r\n`<https://iliauk.com/2016/03/02/centroids-and-centres-numpy-r/>`_\r\n\r\n*includes KDTree as well*\r\n\r\n`<https://stackoverflow.com/questions/50751135/iterating-operation-with-two-\r\narrays-using-numpy>`_\r\n\r\n`<https://stackoverflow.com/questions/21483999/using-atan2-to-find-angle-\r\nbetween-two-vectors>`_\r\n\r\npoint in/on segment\r\n\r\n`<https://stackoverflow.com/questions/328107/how-can-you-determine-a-point\r\n-is-between-two-other-points-on-a-line-segment>`_.\r\n\r\nbenchmarking KDTree\r\n\r\n`<https://www.ibm.com/developerworks/community/blogs/jfp/entry/\r\nPython_Is_Not_C_Take_Two?lang=en>`_.\r\n\r\n`<https://iliauk.wordpress.com/2016/02/16/millions-of-distances-high-\r\nperformance-python/>`_.\r\n\r\n**cKDTree examples**\r\n\r\nquery_ball_point::\r\n\r\n    x, y = np.mgrid[0:3, 0:3]\r\n    pnts = np.c_[x.ravel(), y.ravel()]\r\n    t = cKDTree(pnts)\r\n    idx = t.query_ball_point([1, 0], 1)  # find points within x,y = (1,0)\r\n    pnts[idx]\r\n    array([[0, 0],\r\n    ...    [1, 0],\r\n    ...    [1, 1],\r\n    ...    [2, 0]])\r\n\r\n------\r\n""""""\r\n# pylint: disable=C0103  # invalid-name\r\n# pylint: disable=R0914  # Too many local variables\r\n# pylint: disable=R1710  # inconsistent-return-statements\r\n# pylint: disable=W0105  # string statement has no effect\r\n\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nfrom textwrap import dedent\r\nimport numpy as np\r\nfrom numpy.lib.stride_tricks import as_strided\r\n# from arraytools.fc import _xy\r\n\r\nEPSILON = sys.float_info.epsilon  # note! for checking\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.2f}\'.format}\r\nnp.set_printoptions(edgeitems=5, linewidth=120, precision=2, suppress=True,\r\n                    nanstr=\'nan\', infstr=\'inf\',\r\n                    threshold=200, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n# ---- order by appearance ----\r\n#\r\n__all__ = [\'_flat_\', \'_unpack\', \'segment\', \'stride\',\r\n           \'_new_view_\', \'_view_\', \'_reshape_\',\r\n           \'min_\', \'max_\', \'extent_\',\r\n           \'center_\', \'centroid_\', \'centers\', \'centroids\',\r\n           \'intersect_pnt\',\r\n           \'e_area\', \'e_dist\', \'e_leng\',\r\n           \'areas\', \'lengths\',\r\n           \'total_length\', \'seg_lengths\',\r\n           \'radial_sort\',\r\n           \'dx_dy_np\', \'angle_between\', \'angle_np\', \'azim_np\',\r\n           \'angle_2pnts\', \'angle_seq\', \'angles_poly\', \'dist_bearing\',\r\n           \'_densify_2D\', \'_convert\', \'densify\',\r\n           \'simplify\',\r\n           \'rotate\', \'trans_rot\',\r\n           \'convex\', \'circle\', \'ellipse\',\r\n           \'hex_flat\', \'hex_pointy\', \'rectangle\', \'triangle\',\r\n           \'xy_grid\',\r\n           \'pnt_in_list\', \'point_in_polygon\',\r\n           \'knn\', \'nn_kdtree\', \'cross\', \'e_2d\', \'fill_diagonal\', \'remove_self\',\r\n           \'pnt_on_seg\', \'pnt_on_poly\', \'adjacency_edge\'\r\n           ]\r\n\r\n\r\n# ---- array functions -------------------------------------------------------\r\n#\r\ndef _flat_(a_list, flat_list=None):\r\n    """"""Change the isinstance as appropriate.  Flatten an object using recursion\r\n\r\n    see: itertools.chain() for an alternate method of flattening.\r\n    """"""\r\n    if flat_list is None:\r\n        flat_list = []\r\n    for item in a_list:\r\n        if isinstance(item, (list, tuple, np.ndarray, np.void)):\r\n            _flat_(item, flat_list)\r\n        else:\r\n            flat_list.append(item)\r\n    return flat_list\r\n\r\n\r\ndef _unpack(iterable, param=\'__iter__\'):\r\n    """"""Unpack an iterable based on the param(eter) condition using recursion.\r\n    From `unpack` in `_common.py`\r\n    """"""\r\n    xy = []\r\n    for x in iterable:\r\n        if hasattr(x, param):\r\n            xy.extend(_unpack(x))\r\n        else:\r\n            xy.append(x)\r\n    return xy\r\n\r\n\r\ndef segment(a):\r\n    """"""Segment poly* structures into o-d pairs from start to finish\r\n\r\n    `a` : array\r\n        A 2D array of x,y coordinates representing polyline or polygons.\r\n    `fr_to` : array\r\n        Returns a 3D array of point pairs.\r\n    """"""\r\n    a = _new_view_(a)\r\n    s0, s1 = a.shape\r\n    fr_to = np.zeros((s0-1, s1, 2), dtype=a.dtype)\r\n    fr_to[..., 0] = a[:-1]\r\n    fr_to[..., 1] = a[1:]\r\n    return fr_to\r\n\r\n\r\ndef stride(a, win=(3, 3), stepby=(1, 1)):\r\n    """"""Provide a 2D sliding/moving view of an array.\r\n    There is no edge correction for outputs. Use the `_pad_` function first.\r\n\r\n    Note:\r\n    -----\r\n        Origin arraytools.tools  stride, see it for more information\r\n    """"""\r\n    err = """"""Array shape, window and/or step size error.\r\n    Use win=(3,) with stepby=(1,) for 1D array\r\n    or win=(3,3) with stepby=(1,1) for 2D array\r\n    or win=(1,3,3) with stepby=(1,1,1) for 3D\r\n    ----    a.ndim != len(win) != len(stepby) ----\r\n    """"""\r\n    assert (a.ndim == len(win)) and (len(win) == len(stepby)), err\r\n    shape = np.array(a.shape)  # array shape (r, c) or (d, r, c)\r\n    win_shp = np.array(win)    # window      (3, 3) or (1, 3, 3)\r\n    ss = np.array(stepby)      # step by     (1, 1) or (1, 1, 1)\r\n    newshape = tuple(((shape - win_shp) // ss) + 1) + tuple(win_shp)\r\n    newstrides = tuple(np.array(a.strides) * ss) + a.strides\r\n    a_s = as_strided(a, shape=newshape, strides=newstrides, subok=True).squeeze()\r\n    return a_s\r\n\r\n\r\n# ---- _view and _reshape_ are helper functions -----------------------------\r\n#\r\ndef _new_view_(a):\r\n    """"""View a structured array x,y coordinates as an ndarray to facilitate\r\n    some array calculations.\r\n\r\n    NOTE:  see _view_ for the same functionality\r\n    """"""\r\n    a = np.asanyarray(a)\r\n    if len(a.dtype) > 1:\r\n        shp = a.shape[0]\r\n        a = a.view(dtype=\'float64\')\r\n        a = a.reshape(shp, 2)\r\n    return a\r\n\r\n\r\ndef _view_(a):\r\n    """"""Return a view of the array using the dtype and length\r\n\r\n    Notes:\r\n    ------\r\n    The is a quick function.  The expectation is that they are coordinate\r\n    values in the form  dtype([(\'X\', \'<f8\'), (\'Y\', \'<f8\')])\r\n    """"""\r\n    return a.view((a.dtype[0], len(a.dtype.names)))\r\n\r\n\r\ndef _reshape_(a):\r\n    """"""Reshape arrays, structured or recarrays of coordinates to a 2D ndarray.\r\n\r\n    Notes\r\n    -----\r\n\r\n    1. The length of the dtype is checked. Only object (\'O\') and arrays with\r\n       a uniform dtype return 0.  Structured/recarrays will yield 1 or more.\r\n\r\n    2. dtypes are stripped and the array reshaped\r\n\r\n    >>> a = np.array([(341000., 5021000.), (341000., 5022000.),\r\n                      (342000., 5022000.), (341000., 5021000.)],\r\n                     dtype=[(\'X\', \'<f8\'), (\'Y\', \'<f8\')])\r\n        becomes...\r\n        a = np.array([[  341000.,  5021000.], [  341000.,  5022000.],\r\n                      [  342000.,  5022000.], [  341000.,  5021000.]])\r\n        a.dtype = dtype(\'float64\')\r\n\r\n    3. 3D arrays are collapsed to 2D\r\n\r\n    >>> a.shape = (2, 5, 2) => np.product(a.shape[:-1], 2) => (10, 2)\r\n\r\n    4. Object arrays are processed object by object but assumed to be of a\r\n       common dtype within, as would be expected from a gis package.\r\n    """"""\r\n    if not isinstance(a, np.ndarray):\r\n        raise ValueError(""\\nAn array is required..."")\r\n    shp = len(a.shape)\r\n    _len = len(a.dtype)\r\n    if a.dtype.kind == \'O\':\r\n        if len(a[0].shape) == 1:\r\n            return np.asarray([_view_(i) for i in a])\r\n        return _view_(a)\r\n    #\r\n    if _len == 0:\r\n        if shp == 1:\r\n            view = [_view_(i) for i in a]\r\n        elif shp == 2:\r\n            view = a\r\n        elif shp > 2:\r\n            tmp = a.reshape(np.product(a.shape[:-1]), 2)\r\n            view = tmp.view(\'<f8\')\r\n    elif _len == 1:\r\n        fld_name = a.dtype.names[0]  # assumes \'Shape\' field is the geometry\r\n        view = a[fld_name]\r\n    elif _len >= 2:\r\n        if shp == 1:\r\n            if len(a) == a.shape[0]:\r\n                view = _view_(a)\r\n            else:\r\n                view = np.asanyarray([_view_(i) for i in a])\r\n        else:\r\n            view = np.asanyarray([_view_(i) for i in a])\r\n    else:\r\n        view = a\r\n    return view\r\n\r\n\r\n# ---- extent, mins and maxs ------------------------------------------------\r\n# Note:\r\n#     The functions here use _reshape_ to ensure compatability with structured\r\n#  or recarrays.  ndarrays pass through _reshape_ untouched.\r\n#  _view_ or _new_view_ could also be used but are only suited for x,y\r\n#  structured/recarrays\r\n#\r\ndef min_(a):\r\n    """"""Array minimums\r\n    """"""\r\n    a = _reshape_(a)\r\n    if (a.dtype.kind == \'O\') or (len(a.shape) > 2):\r\n        mins = np.asanyarray([i.min(axis=0) for i in a])\r\n    else:\r\n        mins = a.min(axis=0)\r\n    return mins\r\n\r\n\r\ndef max_(a):\r\n    """"""Array maximums\r\n    """"""\r\n    a = _reshape_(a)\r\n    if (a.dtype.kind == \'O\') or (len(a.shape) > 2):\r\n        maxs = np.asanyarray([i.max(axis=0) for i in a])\r\n    else:\r\n        maxs = a.max(axis=0)\r\n    return maxs\r\n\r\n\r\ndef extent_(a):\r\n    """"""Array extent values\r\n    """"""\r\n    a = _reshape_(a)\r\n    if isinstance(a, (list, tuple)):\r\n        a = np.asanyarray(a)\r\n    if (a.dtype.kind == \'O\') or (len(a.shape) > 2):\r\n        mins = min_(a)\r\n        maxs = max_(a)\r\n        ret = np.hstack((mins, maxs))\r\n    else:\r\n        L, B = min_(a)\r\n        R, T = max_(a)\r\n        ret = np.asarray([L, B, R, T])\r\n    return ret\r\n\r\n# ---- centers --------------------------------------------------------------\r\ndef center_(a, remove_dup=True):\r\n    """"""Return the center of an array. If the array represents a polygon, then\r\n    a check is made for the duplicate first and last point to remove one.\r\n    """"""\r\n    if a.dtype.kind in (\'V\', \'O\'):\r\n        a = _new_view_(a)\r\n    if remove_dup:\r\n        if np.all(a[0] == a[-1]):\r\n            a = a[:-1]\r\n    return a.mean(axis=0)\r\n\r\n\r\ndef centroid_(a, a_6=None):\r\n    """"""Return the centroid of a closed polygon.\r\n\r\n    `a` : array\r\n        A 2D or more of point coordinates.  You need to keep the duplicate\r\n        first and last point.\r\n    `a_6` : number\r\n        If area has been precalculated, you can use its value.\r\n    `e_area` : function (required)\r\n        Contained in this module.\r\n    """"""\r\n    if a.dtype.kind in (\'V\', \'O\'):\r\n        a = _new_view_(a)\r\n    x, y = a.T\r\n    t = ((x[:-1] * y[1:]) - (y[:-1] * x[1:]))\r\n    if a_6 is None:\r\n        a_6 = e_area(a) * 6.0  # area * 6.0\r\n    x_c = np.sum((x[:-1] + x[1:]) * t) / a_6\r\n    y_c = np.sum((y[:-1] + y[1:]) * t) / a_6\r\n    return np.asarray([-x_c, -y_c])\r\n\r\n\r\ndef centers(a, remove_dup=True):\r\n    """"""batch centres (ie _center)\r\n    """"""\r\n    a = np.asarray(a)\r\n    if a.dtype == \'O\':\r\n        tmp = [_reshape_(i) for i in a]\r\n        return np.asarray([center_(i, remove_dup) for i in tmp])\r\n    if len(a.dtype) >= 1:\r\n        a = _reshape_(a)\r\n    if a.ndim == 2:\r\n        a = a.reshape((1,) + a.shape)\r\n    c = np.asarray([center_(i, remove_dup) for i in a]).squeeze()\r\n    return c\r\n\r\n\r\ndef centroids(a):\r\n    """"""batch centroids (ie _centroid)\r\n    """"""\r\n    a = np.asarray(a)\r\n    if a.dtype == \'O\':\r\n        tmp = [_reshape_(i) for i in a]\r\n        return np.asarray([centroid_(i) for i in tmp])\r\n    if len(a.dtype) >= 1:\r\n        a = _reshape_(a)\r\n    if a.ndim == 2:\r\n        a = a.reshape((1,) + a.shape)\r\n    c = np.asarray([centroid_(i) for i in a]).squeeze()\r\n    return c\r\n\r\n\r\n# ---- point functions ------------------------------------------------------\r\n#\r\ndef intersect_pnt(a, b=None):\r\n    """"""Returns the point of intersection of the segment passing through two\r\n    line segments (p0, p1) and (p2, p3)\r\n\r\n    Parameters:\r\n    -----------\r\n    a : array-like\r\n        1 segment  [p0, p1]\r\n        2 segments [p0, p1], [p2, p3] or\r\n        1 array-like np.array([p0, p1, p2, p3])\r\n    b : None or array-like\r\n        1 segment [p2, p3]  if `a` is [p0, p1], or ``None``\r\n\r\n    Notes:\r\n    ------\r\n    >>> s = np.array([[ 0,  0], [10, 10], [ 0,  5], [ 5,  0]])\r\n     s: array([[ 0,  0],    h: array([[  0.,   0.,   1.],\r\n               [10, 10],              [ 10.,  10.,   1.],\r\n               [ 0,  5],              [  0.,   5.,   1.],\r\n               [ 5,  0]])             [  5.,   0.,   1.]])\r\n\r\n    Reference:\r\n    ---------\r\n    `<https://stackoverflow.com/questions/3252194/numpy-and-line-\r\n    intersections>`_.\r\n    """"""\r\n    if (len(a) == 4) and (b is None):\r\n        s = a\r\n    elif (len(a) == 2) and (len(b) == 2):\r\n        s = np.vstack((a, b))\r\n    else:\r\n        raise AttributeError(""Use a 4 point array or 2, 2-pnt lines"")\r\n    h = np.hstack((s, np.ones((4, 1))))  # h for homogeneous\r\n    l1 = np.cross(h[0], h[1])            # get first line\r\n    l2 = np.cross(h[2], h[3])            # get second line\r\n    x, y, z = np.cross(l1, l2)           # point of intersection\r\n    if z == 0:                           # lines are parallel\r\n        return (float(\'inf\'), float(\'inf\'))\r\n    return (x/z, y/z)\r\n\r\n\r\ndef intersects(*args):\r\n    """"""Line intersection check.  Two lines or 4 points that form the lines.\r\n\r\n    Requires:\r\n    --------\r\n      intersects(line0, line1) or intersects(p0, p1, p2, p3)\r\n        p0, p1 -> line 1\r\n        p2, p3 -> line 2\r\n\r\n    Returns:\r\n    --------\r\n        boolean, if the segments do intersect\r\n\r\n    References:\r\n    -----------\r\n    `<https://stackoverflow.com/questions/563198/how-do-you-detect-where-two-\r\n    line-segments-intersect#565282>`_.\r\n\r\n    """"""\r\n    if len(args) == 2:\r\n        p0, p1, p2, p3 = *args[0], *args[1]\r\n    elif len(args) == 4:\r\n        p0, p1, p2, p3 = args\r\n    else:\r\n        raise AttributeError(""Pass 2, 2-pnt lines or 4 points to the function"")\r\n    #\r\n    # ---- First check ----   np.cross(p1-p0, p3-p2 )\r\n    p0_x, p0_y, p1_x, p1_y, p2_x, p2_y, p3_x, p3_y = *p0, *p1, *p2, *p3\r\n    s10_x = p1_x - p0_x\r\n    s10_y = p1_y - p0_y\r\n    s32_x = p3_x - p2_x\r\n    s32_y = p3_y - p2_y\r\n    denom = s10_x * s32_y - s32_x * s10_y\r\n    if denom == 0.0:\r\n        return False\r\n    #\r\n    # ---- Second check ----  np.cross(p1-p0, p0-p2 )\r\n    den_gt0 = denom > 0\r\n    s02_x = p0_x - p2_x\r\n    s02_y = p0_y - p2_y\r\n    s_numer = s10_x * s02_y - s10_y * s02_x\r\n    if (s_numer < 0) == den_gt0:\r\n        return False\r\n    #\r\n    # ---- Third check ----  np.cross(p3-p2, p0-p2)\r\n    t_numer = s32_x * s02_y - s32_y * s02_x\r\n    if (t_numer < 0) == den_gt0:\r\n        return False\r\n    #\r\n    if ((s_numer > denom) == den_gt0) or ((t_numer > denom) == den_gt0):\r\n        return False\r\n    #\r\n    # ---- check to see if the intersection point is one of the input points\r\n    t = t_numer / denom\r\n    # substitute p0 in the equation\r\n    x = p0_x + (t * s10_x)\r\n    y = p0_y + (t * s10_y)\r\n    # be careful that you are comparing tuples to tuples, lists to lists\r\n    if sum([(x, y) == tuple(i) for i in [p0, p1, p2, p3]]) > 0:\r\n        return False\r\n    return True\r\n\r\n\r\n# ---- distance, length and area --------------------------------------------\r\n# ----\r\ndef e_area(a, b=None):\r\n    """"""Area calculation, using einsum.\r\n\r\n    Some may consider this overkill, but consider a huge list of polygons,\r\n    many multipart, many with holes and even multiple version therein.\r\n\r\n    Requires:\r\n    --------\r\n    preprocessing :\r\n        use `_view_`, `_new_view_` or `_reshape_` with structured/recarrays\r\n    `a` : array\r\n        Either a 2D+ array of coordinates or arrays of x, y values\r\n    `b` : array, optional\r\n        If a < 2D, then the y values need to be supplied\r\n    Outer rings are ordered clockwise, inner holes are counter-clockwise\r\n\r\n    Notes:\r\n    -----\r\n    See ein_geom.py for examples\r\n\r\n    """"""\r\n    a = np.asarray(a)\r\n    if b is None:\r\n        xs = a[..., 0]\r\n        ys = a[..., 1]\r\n    else:\r\n        b = np.asarray(b)\r\n        xs, ys = a, b\r\n    x0 = np.atleast_2d(xs[..., 1:])\r\n    y0 = np.atleast_2d(ys[..., :-1])\r\n    x1 = np.atleast_2d(xs[..., :-1])\r\n    y1 = np.atleast_2d(ys[..., 1:])\r\n    e0 = np.einsum(\'...ij,...ij->...i\', x0, y0)\r\n    e1 = np.einsum(\'...ij,...ij->...i\', x1, y1)\r\n    area = abs(np.sum((e0 - e1)*0.5))\r\n    return area\r\n\r\n\r\ndef e_dist(a, b, metric=\'euclidean\'):\r\n    """"""Distance calculation for 1D, 2D and 3D points using einsum\r\n\r\n    preprocessing :\r\n        use `_view_`, `_new_view_` or `_reshape_` with structured/recarrays\r\n\r\n    Parameters:\r\n    -----------\r\n    `a`, `b` : array like\r\n        Inputs, list, tuple, array in 1, 2 or 3D form\r\n    `metric` : string\r\n        euclidean (\'e\', \'eu\'...), sqeuclidean (\'s\', \'sq\'...),\r\n\r\n    Notes:\r\n    -----\r\n    mini e_dist for 2d points array and a single point\r\n\r\n    >>> def e_2d(a, p):\r\n            diff = a - p[np.newaxis, :]  # a and p are ndarrays\r\n            return np.sqrt(np.einsum(\'ij,ij->i\', diff, diff))\r\n\r\n    """"""\r\n    a = np.asarray(a)\r\n    b = np.atleast_2d(b)\r\n    a_dim = a.ndim\r\n    b_dim = b.ndim\r\n    if a_dim == 1:\r\n        a = a.reshape(1, 1, a.shape[0])\r\n    if a_dim >= 2:\r\n        a = a.reshape(np.prod(a.shape[:-1]), 1, a.shape[-1])\r\n    if b_dim > 2:\r\n        b = b.reshape(np.prod(b.shape[:-1]), b.shape[-1])\r\n    diff = a - b\r\n    dist_arr = np.einsum(\'ijk,ijk->ij\', diff, diff)\r\n    if metric[:1] == \'e\':\r\n        dist_arr = np.sqrt(dist_arr)\r\n    dist_arr = np.squeeze(dist_arr)\r\n    return dist_arr\r\n\r\n\r\ndef e_leng(a):\r\n    """"""Length/distance between points in an array using einsum\r\n\r\n    Requires:\r\n    --------\r\n    preprocessing :\r\n        use `_view_`, `_new_view_` or `_reshape_` with structured/recarrays\r\n    `a` : array-like\r\n        A list/array coordinate pairs, with ndim = 3 and the minimum\r\n        shape = (1,2,2), eg. (1,4,2) for a single line of 4 pairs\r\n\r\n    The minimum input needed is a pair, a sequence of pairs can be used.\r\n\r\n    Returns:\r\n    -------\r\n    `length` : float\r\n        The total length/distance formed by the points\r\n    `d_leng` : float\r\n        The distances between points forming the array\r\n\r\n        (40.0, [array([[ 10.,  10.,  10.,  10.]])])\r\n\r\n    Notes:\r\n    ------\r\n    >>> diff = g[:, :, 0:-1] - g[:, :, 1:]\r\n    >>> # for 4D\r\n    >>> d = np.einsum(\'ijk..., ijk...->ijk...\', diff, diff).flatten()  # or\r\n    >>> d  = np.einsum(\'ijkl, ijkl->ijk\', diff, diff).flatten()\r\n    >>> d = np.sum(np.sqrt(d)\r\n    """"""\r\n    #\r\n#    d_leng = 0.0\r\n    # ----\r\n    def _cal(diff):\r\n        """""" perform the calculation, see above\r\n        """"""\r\n        d_leng = np.sqrt(np.einsum(\'ijk,ijk->ij\', diff, diff)).squeeze()\r\n        length = np.sum(d_leng.flatten())\r\n        return length, d_leng\r\n    # ----\r\n    diffs = []\r\n    a = np.atleast_2d(a)\r\n    if a.shape[0] == 1:\r\n        return 0.0\r\n    if a.ndim == 2:\r\n        a = np.reshape(a, (1,) + a.shape)\r\n    if a.ndim == 3:\r\n        diff = a[:, 0:-1] - a[:, 1:]\r\n        length, d_leng = _cal(diff)\r\n        diffs.append(d_leng)\r\n    if a.ndim == 4:\r\n        length = 0.0\r\n        for i in range(a.shape[0]):\r\n            diff = a[i][:, 0:-1] - a[i][:, 1:]\r\n            leng, d_leng = _cal(diff)\r\n            diffs.append(d_leng)\r\n            length += leng\r\n    return length, diffs[0]\r\n\r\n\r\n\r\n# ---- Batch calculations of e_area and e_leng ------------------------------\r\n#\r\ndef areas(a):\r\n    """"""Calls e_area to calculate areas for many types of nested objects.\r\n\r\n    This would include object arrays, list of lists and similar constructs.\r\n    Each part is considered separately.\r\n\r\n    Returns:\r\n    -------\r\n        A list with one or more areas.\r\n    """"""\r\n    a = np.asarray(a)\r\n    if a.dtype == \'O\':\r\n        tmp = [_reshape_(i) for i in a]\r\n        return [e_area(i) for i in tmp]\r\n    if len(a.dtype) >= 1:\r\n        a = _reshape_(a)\r\n    if a.ndim == 2:\r\n        a = a.reshape((1,) + a.shape)\r\n    a_s = [e_area(i) for i in a]\r\n    return a_s\r\n\r\n\r\ndef lengths(a, prn=False):\r\n    """"""Calls `e_leng` to calculate lengths for many types of nested objects.\r\n    This would include object arrays, list of lists and similar constructs.\r\n    Each part is considered separately.\r\n\r\n    Returns:\r\n    -------\r\n    A list with one or more lengths. `prn=True` for optional printing.\r\n    """"""\r\n    def _prn_(a_s):\r\n        """"""optional result printing""""""\r\n        hdr = ""{!s:<12}  {}\\n"".format(""Tot. Length"", ""Seg. Length"")\r\n        r = [""{:12.3f}  {!r:}"".format(*a_s[i]) for i in range(len(a_s))]\r\n        print(hdr + ""\\n"".join(r))\r\n    #\r\n    a = np.asarray(a)\r\n    if a.dtype == \'O\':\r\n        tmp = [_reshape_(i) for i in a]\r\n        a_s = [e_leng(i) for i in tmp]\r\n        if prn:\r\n            _prn_(a_s)\r\n        return a_s\r\n    if len(a.dtype) == 1:\r\n        a = _reshape_(a)\r\n    if len(a.dtype) > 1:\r\n        a = _reshape_(a)\r\n    if isinstance(a, (list, tuple)):\r\n        return [e_leng(i) for i in a]\r\n    if a.ndim == 2:\r\n        a = a.reshape((1,) + a.shape)\r\n    a_s = [e_leng(i) for i in a]\r\n    if prn:\r\n        _prn_(a_s)\r\n    return a_s\r\n\r\n\r\ndef total_length(a):\r\n    """"""Just return total length from \'length\' above\r\n    Returns:\r\n    -------\r\n        List of array(s) containing the total length for each object\r\n    """"""\r\n    a_s = lengths(a)\r\n    result = [i[0] for i in a_s]\r\n    return result[0]\r\n\r\n\r\ndef seg_lengths(a):\r\n    """"""Just return segment lengths from \'length above.\r\n    Returns:\r\n    -------\r\n        List of array(s) containing the segment lengths for each object\r\n    """"""\r\n    a_s = lengths(a)\r\n    result = [i[1] for i in a_s]\r\n    return result[0]\r\n\r\n\r\n# ---- sorting based on geometry --------------------------------------------\r\n#\r\ndef radial_sort(pnts, cent=None, as_azimuth=False):\r\n    """"""Sort about the point cloud center or from a given point\r\n\r\n    `pnts` : points\r\n        An array of points (x,y) as array or list\r\n    `cent` : coordinate\r\n        list, tuple, array of the center\'s x,y coordinates\r\n    >>> cent = [0, 0] or np.array([0, 0])\r\n\r\n    Returns:\r\n    -------\r\n        The angles in the range -180, 180 x-axis oriented\r\n\r\n    """"""\r\n    pnts = _new_view_(pnts)\r\n    if cent is None:\r\n        cent = center_(pnts, remove_dup=False)\r\n    ba = pnts - cent\r\n    ang_ab = np.arctan2(ba[:, 1], ba[:, 0])\r\n    ang_ab = np.degrees(ang_ab)\r\n    sort_order = np.argsort(ang_ab)\r\n    if as_azimuth:\r\n        ang_ab = np.where(ang_ab > 90, 450.0 - ang_ab, 90.0 - ang_ab)\r\n    return ang_ab, sort_order\r\n\r\n\r\n# ---- angle related functions ----------------------------------------------\r\n# ---- ndarrays and structured arrays ----\r\n#\r\ndef dx_dy_np(a):\r\n    """"""Sequential difference in the x/y pairs from a table/array\r\n\r\n    `a` : ndarray or structured array.\r\n        It is changed as necessary to a 2D array of x/y values.\r\n    _new_view_ : function\r\n        Does the array conversion to 2D from structured if necessary\r\n    """"""\r\n    a = _new_view_(a)\r\n    out = np.zeros_like(a)\r\n    diff = a[1:] - a[:-1]\r\n    out[1:] = diff\r\n    return out\r\n\r\n\r\ndef angle_np(a, as_degrees=True):\r\n    """"""Angle between successive points.\r\n\r\n    Requires: `dx_dy_np` and hence, `_new_view_`\r\n    """"""\r\n    diff = dx_dy_np(a)\r\n    angle = np.arctan2(diff[:, 1], diff[:, 0])\r\n    if as_degrees:\r\n        angle = np.rad2deg(angle)\r\n    return angle\r\n\r\n\r\ndef azim_np(a):\r\n    """"""Return the azimuth/bearing relative to North\r\n\r\n    Requires: `angle_np`, which calls `dx_dy_np` which calls `_new_view_`\r\n    """"""\r\n    s = angle_np(a, as_degrees=True)\r\n    azim = np.where(s <= 0, 90. - s,\r\n                    np.where(s > 90., 450.0 - s, 90.0 - s))\r\n    return azim\r\n\r\n\r\ndef angle_between(p0, p1, p2):\r\n    """"""angle between 3 sequential points\r\n\r\n    >>> p0, p1, p2 = np.array([[0, 0],[1, 1], [1, 0]])\r\n    angle_between(p0, p1, p2)\r\n    (45.0, -135.0, -90.0)\r\n    """"""\r\n    d1 = p0 - p1\r\n    d2 = p2 - p1\r\n    ang1 = np.arctan2(*d1[::-1])\r\n    ang2 = np.arctan2(*d2[::-1])\r\n    ang = (ang2 - ang1)  # % (2 * np.pi)\r\n    ang, ang1, ang2 = [np.degrees(i) for i in [ang, ang1, ang2]]\r\n    return ang, ang1, ang2\r\n\r\n\r\n# ---- ndarrays\r\ndef angle_2pnts(p0, p1):\r\n    """"""Two point angle. p0 represents the `from` point and p1 the `to` point.\r\n\r\n    >>> angle = atan2(vector2.y, vector2.x) - atan2(vector1.y, vector1.x)\r\n\r\n    Accepted answer from the poly_angles link\r\n    """"""\r\n    p0, p1 = [np.asarray(i) for i in [p0, p1]]\r\n    ba = p1 - p0\r\n    ang_ab = np.arctan2(*ba[::-1])\r\n    return np.rad2deg(ang_ab % (2 * np.pi))\r\n\r\n\r\ndef angle_seq(a):\r\n    """"""Sequential angles for a points list\r\n\r\n    >>> angle = atan2(vector2.y, vector2.x) - atan2(vector1.y, vector1.x)\r\n    Accepted answer from the poly_angles link\r\n    """"""\r\n    a = _new_view_(a)\r\n    ba = a[1:] - a[:-1]\r\n    ang_ab = np.arctan2(ba[:, 1], ba[:, 0])\r\n    return np.degrees(ang_ab % (2 * np.pi))\r\n\r\n\r\ndef angles_poly(a=None, inside=True, in_deg=True):\r\n    """"""Sequential 3 point angles from a poly* shape\r\n\r\n    a : array\r\n        an array of points, derived from a polygon/polyline geometry\r\n    inside : boolean\r\n        determine inside angles, outside if False\r\n    in_deg : bolean\r\n        convert to degrees from radians\r\n\r\n    Notes:\r\n    ------\r\n    General comments\r\n    ::\r\n        2 points - subtract 2nd and 1st points, effectively making the\r\n        calculation relative to the origin and x axis, aka... slope\r\n        n points - sequential angle between 3 points\r\n\r\n    Notes to keep\r\n    ::\r\n        *** keep to convert object to array\r\n        a - a shape from the shape field\r\n        a = p1.getPart()\r\n        b = np.asarray([(i.X, i.Y) if i is not None else ()\r\n                       for j in a for i in j])\r\n\r\n    Sample data\r\n\r\n    >>> a = np.array([[ 0, 0], [ 0, 100], [100, 100], [100,  80],\r\n                      [ 20,  80], [ 20, 20], [100, 20], [100, 0], [ 0, 0]])\r\n    >>> angles_poly(a)  # array([ 90.,  90.,  90., 270., 270.,  90.,  90.])\r\n    """"""\r\n    a = _new_view_(a)\r\n    if len(a) < 2:\r\n        return None\r\n    if len(a) == 2:\r\n        ba = a[1] - a[0]\r\n        return np.arctan2(*ba[::-1])\r\n    a0 = a[0:-2]\r\n    a1 = a[1:-1]\r\n    a2 = a[2:]\r\n    ba = a1 - a0\r\n    bc = a1 - a2\r\n    cr = np.cross(ba, bc)\r\n    dt = np.einsum(\'ij,ij->i\', ba, bc)\r\n    ang = np.arctan2(cr, dt)\r\n    two_pi = np.pi*2.\r\n    if inside:\r\n        ang = np.where(ang < 0, ang + two_pi, ang)\r\n    else:\r\n        ang = np.where(ang > 0, two_pi - ang, ang)\r\n    if in_deg:\r\n        angles = np.degrees(ang)\r\n    return angles\r\n\r\n\r\ndef dist_bearing(orig=(0, 0), bearings=None, dists=None, prn=False):\r\n    """"""Point locations given distance and bearing.\r\n    Now only distance and angle are known.  Calculate the point coordinates\r\n    from distance and angle\r\n\r\n    References:\r\n    ----------\r\n    `<https://community.esri.com/thread/66222>`_.\r\n\r\n    `<https://community.esri.com/blogs/dan_patterson/2018/01/21/\r\n    origin-distances-and-bearings-geometry-wanderings>`_.\r\n\r\n    Notes:\r\n    -----\r\n    Sample calculation\r\n    ::\r\n      bearings = np.arange(0, 361, 10.)  # 37 bearings\r\n      dists = np.random.randint(10, 500, len(bearings)) * 1.0\r\n      dists = np.ones((len(bearings),))\r\n      dists.fill(100.)\r\n      data = dist_bearing(orig=orig, bearings=bearings, dists=dists)\r\n\r\n    Create a featureclass from the results\r\n    ::\r\n       shapeXY = [\'X_f\', \'Y_f\']\r\n       fc_name = \'C:/path/Geodatabase.gdb/featureclassname\'\r\n       arcpy.da.NumPyArrayToFeatureClass(out, fc_name, [\'Xn\', \'Yn\'], ""2951"")\r\n       # ... syntax\r\n       arcpy.da.NumPyArrayToFeatureClass(\r\n                          in_array=out, out_table=fc_name,\r\n                          shape_fields=shapeXY, spatial_reference=SR)\r\n    """"""\r\n    orig = np.array(orig)\r\n    rads = np.deg2rad(bearings)\r\n    dx = np.sin(rads) * dists\r\n    dy = np.cos(rads) * dists\r\n    x_t = np.cumsum(dx) + orig[0]\r\n    y_t = np.cumsum(dy) + orig[1]\r\n    xy_f = np.array(list(zip(x_t[:-1], y_t[:-1])))\r\n    xy_f = np.vstack((orig, xy_f))\r\n    stack = (xy_f[:, 0], xy_f[:, 1], x_t, y_t, dx, dy, dists, bearings)\r\n    data = np.vstack(stack).T\r\n    names = [\'X_f\', \'Y_f\', ""X_t"", ""Yt"", ""dx"", ""dy"", ""dist"", ""bearing""]\r\n    N = len(names)\r\n    if prn:  # ---- just print the results ----------------------------------\r\n        frmt = ""Origin (0,0)\\n"" + ""{:>10s}""*N\r\n        print(frmt.format(*names))\r\n        frmt = ""{: 10.2f}""*N\r\n        for i in data:\r\n            print(frmt.format(*i))\r\n        return data\r\n    # ---- produce a structured array from the output ----------------\r\n    names = "", "".join(names)\r\n    kind = [""<f8""]*N\r\n    kind = "", "".join(kind)\r\n    out = data.transpose()\r\n    out = np.core.records.fromarrays(out, names=names, formats=kind)\r\n    return out\r\n\r\n\r\n# ---- densify functions -----------------------------------------------------\r\n#\r\ndef _densify_2D(a, fact=2):\r\n    """"""Densify a 2D array using np.interp.\r\n\r\n    `a` : array\r\n        Input polyline or polygon array coordinates\r\n    `fact` : number\r\n        The factor to density the line segments by\r\n\r\n    Notes:\r\n    -----\r\n        Original construction of c rather than the zero\'s approach.\r\n    Example\r\n    ::\r\n          c0 = c0.reshape(n, -1)\r\n          c1 = c1.reshape(n, -1)\r\n          c = np.concatenate((c0, c1), 1)\r\n    """"""\r\n    # Y = a changed all the y\'s to a\r\n    a = _new_view_(a)\r\n    a = np.squeeze(a)\r\n    n_fact = len(a) * fact\r\n    b = np.arange(0, n_fact, fact)\r\n    b_new = np.arange(n_fact - 1)     # Where you want to interpolate\r\n    c0 = np.interp(b_new, b, a[:, 0])\r\n    c1 = np.interp(b_new, b, a[:, 1])\r\n    n = c0.shape[0]\r\n    c = np.zeros((n, 2))\r\n    c[:, 0] = c0\r\n    c[:, 1] = c1\r\n    return c\r\n\r\n\r\ndef _convert(a, fact=2, check_arcpy=True):\r\n    """"""Do the shape conversion for the array parts.  Calls _densify_2D\r\n\r\n    Requires:\r\n    ---------\r\n    >>> import arcpy  # uncomment the first line below if using _convert\r\n    """"""\r\n    if check_arcpy:\r\n        #import arcpy\r\n        from arcpy.arcobjects import Point\r\n    out = []\r\n    parts = len(a)\r\n    for i in range(parts):\r\n        sub_out = []\r\n        p = np.asarray(a[i]).squeeze()\r\n        if p.ndim == 2:\r\n            shp = _densify_2D(p, fact=fact)  # call _densify_2D\r\n            arc_pnts = [Point(*p) for p in shp]\r\n            sub_out.append(arc_pnts)\r\n            out.extend(sub_out)\r\n        else:\r\n            for pp in p:\r\n                shp = _densify_2D(pp, fact=fact)\r\n                arc_pnts = [Point(*ps) for ps in shp]\r\n                sub_out.append(arc_pnts)\r\n            out.append(sub_out)\r\n    return out\r\n\r\n\r\ndef densify(polys, fact=2):\r\n    """"""Convert polygon objects to arrays, densify.\r\n\r\n    Requires:\r\n    --------\r\n    `_densify_2D` : function\r\n        the function that is called for each shape part\r\n    `_unpack` : function\r\n        unpack objects\r\n    """"""\r\n    # ---- main section ----\r\n    out = []\r\n    for poly in polys:\r\n        p = poly.__geo_interface__[\'coordinates\']\r\n        back = _convert(p, fact)\r\n        out.append(back)\r\n    return out\r\n\r\n\r\n# ---- simplify functions -----------------------------------------------------\r\n#\r\ndef simplify(a, deviation=10):\r\n    """"""Simplify array\r\n    """"""\r\n    angles = angles_poly(a, inside=True, in_deg=True)\r\n    idx = (np.abs(angles - 180.) >= deviation)\r\n    sub = a[1: -1]\r\n    p = sub[idx]\r\n    return a, p, angles\r\n\r\n\r\n# ---- Create geometries -----------------------------------------------------\r\n#\r\ndef pnt_(p=np.nan):\r\n    """"""Create a point object for null points, center points etc\r\n    ::\r\n        pnt_((1., 2.)\\n\r\n        pnt_(1) => array([1., 1.])\r\n    """"""\r\n    p = np.atleast_1d(p)\r\n    if p.dtype.kind not in (\'f\', \'i\'):\r\n        raise ValueError(""Numeric points supported, not {}"".format(p))\r\n    if np.any(np.isnan(p)):\r\n        p = np.array([np.nan, np.nan])\r\n    elif isinstance(p, (np.ndarray, list, tuple)):\r\n        if len(p) == 2:\r\n            p = np.array([p[0], p[1]])\r\n        else:\r\n            p = np.array([p[0], p[0]])\r\n    return p\r\n\r\n\r\ndef rotate(pnts, angle=0):\r\n    """"""Rotate points about the origin in degrees, (+ve for clockwise) """"""\r\n    pnts = _new_view_(pnts)\r\n    angle = np.deg2rad(angle)                 # convert to radians\r\n    s = np.sin(angle)\r\n    c = np.cos(angle)    # rotation terms\r\n    aff_matrix = np.array([[c, s], [-s, c]])  # rotation matrix\r\n    XY_r = np.dot(pnts, aff_matrix)           # numpy magic to rotate pnts\r\n    return XY_r\r\n\r\n\r\ndef trans_rot(a, angle=0.0, unique=True):\r\n    """"""Translate and rotate and array of points about the point cloud origin.\r\n\r\n    Requires:\r\n    ---------\r\n    a : array\r\n        2d array of x,y coordinates.\r\n    angle : double\r\n        angle in degrees in the range -180. to 180\r\n    unique : boolean\r\n        If True, then duplicate points are removed.  If False, then this would\r\n        be similar to doing a weighting on the points based on location.\r\n\r\n    Returns:\r\n    --------\r\n    Points rotated about the origin and translated back.\r\n\r\n    >>> a = np.array([[0, 0], [0, 1], [1, 1], [1, 0], [0, 0]])\r\n    >>> b = trans_rot(b, 45)\r\n    >>> b\r\n    array([[ 0.5,  1.5],\r\n           [ 1.5,  1.5],\r\n           [ 0.5, -0.5],\r\n           [ 1.5, -0.5]])\r\n\r\n    Notes:\r\n    ------\r\n    - if the points represent a polygon, make sure that the duplicate\r\n    - np.einsum(\'ij,kj->ik\', a - cent, R)  =  np.dot(a - cent, R.T).T\r\n    - ik does the rotation in einsum\r\n\r\n    >>> R = np.array(((c, s), (-s,  c)))  # clockwise about the origin\r\n    """"""\r\n    if unique:\r\n        a = np.unique(a, axis=0)\r\n    cent = a.mean(axis=0)\r\n    angle = np.radians(angle)\r\n    c, s = np.cos(angle), np.sin(angle)\r\n    R = np.array(((c, s), (-s, c)))\r\n    return  np.einsum(\'ij,kj->ik\', a - cent, R) + cent\r\n\r\n\r\n# ---- convex hull, circle ellipse, hexagons, rectangles, triangle, xy-grid --\r\n#\r\ndef convex(points):\r\n    """"""Calculates the convex hull for given points\r\n    :Input is a list of 2D points [(x, y), ...]\r\n    """"""\r\n    def _cross_(o, a, b):\r\n        """"""Cross-product for vectors o-a and o-b\r\n        """"""\r\n        xo, yo = o\r\n        xa, ya = a\r\n        xb, yb = b\r\n        return (xa - xo)*(yb - yo) - (ya - yo)*(xb - xo)\r\n    #\r\n    if isinstance(points, np.ndarray):\r\n        points = points.tolist()\r\n        points = [tuple(i) for i in points]\r\n    points = sorted(set(points))  # Remove duplicates\r\n    if len(points) <= 1:\r\n        return points\r\n    # Build lower hull\r\n    lower = []\r\n    for p in points:\r\n        while len(lower) >= 2 and _cross_(lower[-2], lower[-1], p) <= 0:\r\n            lower.pop()\r\n        lower.append(p)\r\n    # Build upper hull\r\n    upper = []\r\n    for p in reversed(points):\r\n        while len(upper) >= 2 and _cross_(upper[-2], upper[-1], p) <= 0:\r\n            upper.pop()\r\n        upper.append(p)\r\n    #print(""lower\\n{}\\nupper\\n{}"".format(lower, upper))\r\n    return np.array(lower[:-1] + upper)  # upper[:-1]) # for open loop\r\n\r\n\r\ndef circle(radius=1.0, theta=10.0, xc=0.0, yc=0.0):\r\n    """"""Produce a circle/ellipse depending on parameters.\r\n\r\n    `radius` : number\r\n        Distance from centre\r\n    `theta` : number\r\n        Angle of densification of the shape around 360 degrees\r\n    """"""\r\n    angles = np.deg2rad(np.arange(180.0, -180.0-theta, step=-theta))\r\n    x_s = radius*np.cos(angles) + xc    # X values\r\n    y_s = radius*np.sin(angles) + yc    # Y values\r\n    pnts = np.c_[x_s, y_s]\r\n    return pnts\r\n\r\n\r\ndef ellipse(x_radius=1.0, y_radius=1.0, theta=10., xc=0.0, yc=0.0):\r\n    """"""Produce an ellipse depending on parameters.\r\n\r\n    `radius` : number\r\n        Distance from centre in the X and Y directions\r\n    `theta` : number\r\n        Angle of densification of the shape around 360 degrees\r\n    """"""\r\n    angles = np.deg2rad(np.arange(180.0, -180.0-theta, step=-theta))\r\n    x_s = x_radius*np.cos(angles) + xc    # X values\r\n    y_s = y_radius*np.sin(angles) + yc    # Y values\r\n    pnts = np.c_[x_s, y_s]\r\n    return pnts\r\n\r\n\r\ndef hex_flat(dx=1, dy=1, cols=1, rows=1):\r\n    """"""Generate the points for the flat-headed hexagon\r\n\r\n    `dy_dx` : number\r\n        The radius width, remember this when setting hex spacing\r\n    `dx` : number\r\n        Increment in x direction, +ve moves west to east, left/right\r\n    `dy` : number\r\n        Increment in y direction, -ve moves north to south, top/bottom\r\n    """"""\r\n    f_rad = np.deg2rad([180., 120., 60., 0., -60., -120., -180.])\r\n    X = np.cos(f_rad) * dy\r\n    Y = np.sin(f_rad) * dy            # scaled hexagon about 0, 0\r\n    seed = np.array(list(zip(X, Y)))  # array of coordinates\r\n    dx = dx * 1.5\r\n    dy = dy * np.sqrt(3.)/2.0\r\n    hexs = [seed + [dx * i, dy * (i % 2)] for i in range(0, cols)]\r\n    m = len(hexs)\r\n    for j in range(1, rows):  # create the other rows\r\n        hexs += [hexs[h] + [0, dy * 2 * j] for h in range(m)]\r\n    return hexs\r\n\r\n\r\ndef hex_pointy(dx=1, dy=1, cols=1, rows=1):\r\n    """"""Pointy hex angles, convert to sin, cos, zip and send\r\n\r\n    `dy_dx` - number\r\n        The radius width, remember this when setting hex spacing\r\n    `dx` : number\r\n        Increment in x direction, +ve moves west to east, left/right\r\n    `dy` : number\r\n        Increment in y direction, -ve moves north to south, top/bottom\r\n    """"""\r\n    p_rad = np.deg2rad([150., 90, 30., -30., -90., -150., 150.])\r\n    X = np.cos(p_rad) * dx\r\n    Y = np.sin(p_rad) * dy      # scaled hexagon about 0, 0\r\n    seed = np.array(list(zip(X, Y)))\r\n    dx = dx * np.sqrt(3.)/2.0\r\n    dy = dy * 1.5\r\n    hexs = [seed + [dx * i * 2, 0] for i in range(0, cols)]\r\n    m = len(hexs)\r\n    for j in range(1, rows):  # create the other rows\r\n        hexs += [hexs[h] + [dx * (j % 2), dy * j] for h in range(m)]\r\n    return hexs\r\n\r\n\r\ndef rectangle(dx=1, dy=1, cols=1, rows=1):\r\n    """"""Create the array of pnts to pass on to arcpy using numpy magic\r\n\r\n    Parameters:\r\n    -----------\r\n    `dx` : number\r\n        Increment in x direction, +ve moves west to east, left/right\r\n    `dy` : number\r\n        Increment in y direction, -ve moves north to south, top/bottom\r\n    `rows`, `cols` : ints\r\n        Row and columns to produce\r\n    """"""\r\n    X = [0.0, 0.0, dx, dx, 0.0]       # X, Y values for a unit square\r\n    Y = [0.0, dy, dy, 0.0, 0.0]\r\n    seed = np.array(list(zip(X, Y)))  # [dx0, dy0] keep for insets\r\n    a = [seed + [j * dx, i * dy]       # make the shapes\r\n         for i in range(0, rows)   # cycle through the rows\r\n         for j in range(0, cols)]  # cycle through the columns\r\n    a = np.asarray(a)\r\n    return a\r\n\r\n\r\ndef triangle(dx=1, dy=1, cols=1, rows=1):\r\n    """"""Create a row of meshed triangles\r\n\r\n    Parameters:\r\n    -----------\r\n    see `rectangle`\r\n    """"""\r\n    grid_type = \'triangle\'\r\n    a, dx, b = dx/2.0, dx, dx*1.5\r\n    Xu = [0.0, a, dx, 0.0]   # X, Y values for a unit triangle, point up\r\n    Yu = [0.0, dy, 0.0, 0.0]\r\n    Xd = [a, b, dx, a]       # X, Y values for a unit triangle, point down\r\n    Yd = [dy, dy, 0.0, dy]   # shifted by dx\r\n    seedU = np.array(list(zip(Xu, Yu)))\r\n    seedD = np.array(list(zip(Xd, Yd)))\r\n    seed = np.array([seedU, seedD])\r\n    a = [seed + [j * dx, i * dy]       # make the shapes\r\n         for i in range(0, rows)       # cycle through the rows\r\n         for j in range(0, cols)]      # cycle through the columns\r\n    a = np.asarray(a)\r\n    s1, s2, s3, s4 = a.shape\r\n    a = a.reshape(s1*s2, s3, s4)\r\n    return a, grid_type\r\n\r\n\r\ndef xy_grid(x, y=None, top_left=True):\r\n    """"""Create a 2D array of locations from x, y values.  The values need not\r\n    be uniformly spaced just sequential. Derived from `meshgrid` in References.\r\n\r\n    Parameters:\r\n    -----------\r\n    xs, ys : array-like\r\n        To form a mesh, there must at least be 2 values in each sequence\r\n    top_left: boolean\r\n        True, y\'s are sorted in descending order, x\'s in ascending\r\n\r\n    References:\r\n    -----------\r\n    `<https://github.com/numpy/numpy/blob/master/numpy/lib/function_base.py>`_.\r\n    """"""\r\n    if y is None:\r\n        y = x\r\n    xs = np.sort(np.asanyarray(x))\r\n    ys = np.asanyarray(y)\r\n    if top_left:\r\n        ys = np.argsort(-ys)\r\n    xs = np.reshape(xs, newshape=((1,) + xs.shape))\r\n    ys = np.reshape(ys, newshape=(ys.shape + (1,)))\r\n    xy = [xs, ys]\r\n    xy = np.broadcast_arrays(*xy, subok=True)\r\n    shp = np.prod(xy[0].shape)\r\n    final = np.zeros((shp, 2), dtype=xs.dtype)\r\n    final[:, 0] = xy[0].ravel()\r\n    final[:, 1] = xy[1].ravel()\r\n    return final\r\n\r\n\r\ndef pnt_in_list(pnt, pnts_list):\r\n    """"""Check to see if a point is in a list of points\r\n    """"""\r\n    is_in = np.any([np.isclose(pnt, i) for i in pnts_list])\r\n    return is_in\r\n\r\n\r\ndef point_in_polygon(pnt, poly):  # pnt_in_poly(pnt, poly):  #\r\n    """"""Point is in polygon. ## fix this and use pip from arraytools\r\n    """"""\r\n    x, y = pnt\r\n    N = len(poly)\r\n    for i in range(N):\r\n        x0, y0, xy = [poly[i][0], poly[i][1], poly[(i + 1) % N]]\r\n        c_min = min([x0, xy[0]])\r\n        c_max = max([x0, xy[0]])\r\n        if c_min < x <= c_max:\r\n            p = y0 - xy[1]\r\n            q = x0 - xy[0]\r\n            y_cal = (x - x0) * p / q + y0\r\n            if y_cal < y:\r\n                return True\r\n    return False\r\n\r\n\r\ndef knn(p, pnts, k=1, return_dist=True):\r\n    """"""\r\n    Calculates k nearest neighbours for a given point.\r\n\r\n    Parameters:\r\n    -----------\r\n    p :array\r\n        x,y reference point\r\n    pnts : array\r\n        Points array to examine\r\n    k : integer\r\n        The `k` in k-nearest neighbours\r\n\r\n    Returns:\r\n    --------\r\n    Array of k-nearest points and optionally their distance from the source.\r\n    """"""\r\n    def _remove_self_(p, pnts):\r\n        """"""Remove a point which is duplicated or itself from the array\r\n        """"""\r\n        keep = ~np.all(pnts == p, axis=1)\r\n        return pnts[keep]\r\n    #\r\n    def _e_2d_(p, a):\r\n        """""" array points to point distance... mini e_dist\r\n        """"""\r\n        diff = a - p[np.newaxis, :]\r\n        return np.sqrt(np.einsum(\'ij,ij->i\', diff, diff))\r\n    #\r\n    p = np.asarray(p)\r\n    k = max(1, min(abs(int(k)), len(pnts)))\r\n    pnts = _remove_self_(p, pnts)\r\n    d = _e_2d_(p, pnts)\r\n    idx = np.argsort(d)\r\n    if return_dist:\r\n        return pnts[idx][:k], d[idx][:k]\r\n    return pnts[idx][:k]\r\n\r\n\r\ndef nn_kdtree(a, N=3, sorted_=True, to_tbl=True, as_cKD=True):\r\n    """"""Produce the N closest neighbours array with their distances using\r\n    scipy.spatial.KDTree as an alternative to einsum.\r\n\r\n    Parameters:\r\n    -----------\r\n    a : array\r\n        Assumed to be an array of point objects for which `nearest` is needed.\r\n    N : integer\r\n        Number of neighbors to return.  Note: the point counts as 1, so N=3\r\n        returns the closest 2 points, plus itself.\r\n        For table output, max N is limited to 5 so that the tabular output\r\n        isn\'t ridiculous.\r\n    sorted_ : boolean\r\n        A nice option to facilitate things.  See `xy_sort`.  Its mini-version\r\n        is included in this function.\r\n    to_tbl : boolean\r\n        Produce a structured array output of coordinate pairs and distances.\r\n    as_cKD : boolean\r\n        Whether to use the `c` compiled or pure python version\r\n\r\n    References:\r\n    -----------\r\n    `<https://stackoverflow.com/questions/52366421/how-to-do-n-d-distance-\r\n    and-nearest-neighbor-calculations-on-numpy-arrays/52366706#52366706>`_.\r\n\r\n    `<https://stackoverflow.com/questions/6931209/difference-between-scipy-\r\n    spatial-kdtree-and-scipy-spatial-ckdtree/6931317#6931317>`_.\r\n    """"""\r\n    def _xy_sort_(a):\r\n        """"""mini xy_sort""""""\r\n        a_view = a.view(a.dtype.descr * a.shape[1])\r\n        idx = np.argsort(a_view, axis=0, order=(a_view.dtype.names)).ravel()\r\n        a = np.ascontiguousarray(a[idx])\r\n        return a\r\n    #\r\n    def xy_dist_headers(N):\r\n        """"""Construct headers for the optional table output""""""\r\n        vals = np.repeat(np.arange(N), 2)\r\n        names = [\'X_{}\', \'Y_{}\']*N + [\'d_{}\']*(N-1)\r\n        vals = (np.repeat(np.arange(N), 2)).tolist() + [i for i in range(1, N)]\r\n        n = [names[i].format(vals[i]) for i in range(len(vals))]\r\n        f = [\'<f8\']*N*2 + [\'<f8\']*(N-1)\r\n        return list(zip(n, f))\r\n    #\r\n    from scipy.spatial import cKDTree, KDTree\r\n    #\r\n    if sorted_:\r\n        a = _xy_sort_(a)\r\n    # ---- query the tree for the N nearest neighbors and their distance\r\n    if as_cKD:\r\n        t = cKDTree(a)\r\n    else:\r\n        t = KDTree(a)\r\n    dists, indices = t.query(a, N)\r\n    if to_tbl and (N <= 5):\r\n        dt = xy_dist_headers(N)  # --- Format a structured array header\r\n        xys = a[indices]\r\n        new_shp = (xys.shape[0], np.prod(xys.shape[1:]))\r\n        xys = xys.reshape(new_shp)\r\n        ds = dists[:, 1:]  #[d[1:] for d in dists]\r\n        arr = np.concatenate((xys, ds), axis=1)\r\n        arr = arr.view(dtype=dt).squeeze()\r\n        return arr\r\n    dists = dists.view(np.float64).reshape(dists.shape[0], -1)\r\n    return dists\r\n\r\n\r\n# ---- mini stuff -----------------------------------------------------------\r\n#\r\ndef cross(o, a, b):\r\n    """"""Cross-product for vectors o-a and o-b\r\n    """"""\r\n    xo, yo = o\r\n    xa, ya = a\r\n    xb, yb = b\r\n    return (xa - xo)*(yb - yo) - (ya - yo)*(xb - xo)\r\n\r\n\r\ndef e_2d(p, a):\r\n    """""" array points to point distance... mini e_dist\r\n    """"""\r\n    p = np.asarray(p)\r\n    diff = a - p[np.newaxis, :]\r\n    return np.sqrt(np.einsum(\'ij,ij->i\', diff, diff))\r\n\r\n\r\ndef fill_diagonal(n=5, seq=None):\r\n    """"""Fill the diagonal of a square array with a sequence\r\n    """"""\r\n    aye = np.eye(n)\r\n    row_col = np.diag_indices_from(aye)\r\n    if seq is None:\r\n        seq = np.arange(n)\r\n    elif len(seq) > n:\r\n        seq = seq[:n]\r\n    elif len(seq) < n:\r\n        seq = np.concatenate((np.array(seq), np.zeros(n)))[:n]\r\n    aye[row_col] = seq\r\n    return aye\r\n\r\n\r\ndef remove_self(p, pnts):\r\n    """"""Remove a point which is duplicated or itself from the array\r\n    """"""\r\n    keep = ~np.all(pnts == p, axis=1)\r\n    return pnts[keep]\r\n\r\n\r\ndef pnt_on_seg(pnt, seg):\r\n    """"""Orthogonal projection of a point onto a 2 point line segment\r\n    Returns the intersection point, if the point is between the segment end\r\n     points, otherwise, it returns the distance to the closest endpoint.\r\n\r\n    Parameters:\r\n    -----------\r\n    pnt : array-like\r\n        `x,y` coordinate pair as list or ndarray\r\n    seg : array-like\r\n        `from-to points`, of x,y coordinates as an ndarray or equivalent\r\n\r\n    Notes:\r\n    ------\r\n    >>> seg = np.array([[0, 0], [10, 10]])  # p0, p1\r\n    >>> p = [10, 0]\r\n    >>> pnt_on_seg(seg, p)\r\n    array([5., 5.])\r\n\r\n    d = np.linalg.norm(np.cross(p1-p0, p0-p))/np.linalg.norm(p1-p0)\r\n    """"""\r\n    x0, y0, x1, y1, dx, dy = *pnt, *seg[0], *(seg[1] - seg[0])\r\n    dist_ = dx*dx + dy*dy  # squared length\r\n    u = ((x0 - x1)*dx + (y0 - y1)*dy)/dist_\r\n    u = max(min(u, 1), 0)\r\n    xy = np.array([dx, dy])*u + [x1, y1]\r\n    return xy, np.sqrt(dist_)\r\n\r\n\r\ndef pnt_on_poly(pnt, poly):\r\n    """"""Find closest point location on a polygon/polyline.\r\n\r\n    Parameters:\r\n    -----------\r\n    pnt : 1D ndarray array\r\n        XY pair representing the point coordinates.\r\n    poly : 2D ndarray array\r\n        A sequence of XY pairs in clockwise order is expected.  The first and\r\n        last points may or may not be duplicates, signifying sequence closeure.\r\n\r\n    Returns:\r\n    --------\r\n    A list of [x, y, distance] for the intersection point on the line\r\n\r\n    Requires:\r\n    ---------\r\n    e_dist is represented by _e_2d and pnt_on_seg by its equivalent below.\r\n\r\n    Notes:\r\n    ------\r\n    This may be as simple as finding the closest point on the edge, but if\r\n    needed, an orthogonal projection onto a polygon/line edge will be done.\r\n    This situation arises when the distance to two sequential points is the\r\n    same\r\n    """"""\r\n    def _e_2d_(a, p):\r\n        """""" array points to point distance... mini e_dist""""""\r\n        diff = a - p[np.newaxis, :]\r\n        return np.sqrt(np.einsum(\'ij,ij->i\', diff, diff))\r\n    #\r\n    def _pnt_on_seg_(seg, pnt):\r\n        """"""mini pnt_on_seg function normally required by pnt_on_poly""""""\r\n        x0, y0, x1, y1, dx, dy = *pnt, *seg[0], *(seg[1] - seg[0])\r\n        dist_ = dx*dx + dy*dy  # squared length\r\n        u = ((x0 - x1)*dx + (y0 - y1)*dy)/dist_\r\n        u = max(min(u, 1), 0)  # u must be between 0 and 1\r\n        xy = np.array([dx, dy])*u + [x1, y1]\r\n        return xy\r\n    #\r\n    pnt = np.asarray(pnt)\r\n    poly = np.asarray(poly)\r\n    if np.all(poly[0] == poly[-1]):  # strip off any duplicate\r\n        poly = poly[:-1]\r\n    # ---- determine the distances\r\n    d = _e_2d_(poly, pnt)  # abbreviated edist =>  d = e_dist(poly, pnt)\r\n    key = np.argsort(d)[0]         # dist = d[key]\r\n    if key == 0:\r\n        seg = np.vstack((poly[-1:], poly[:3]))\r\n    elif (key + 1) >= len(poly):\r\n        seg = np.vstack((poly[-2:], poly[:1]))\r\n    else:\r\n        seg = poly[key-1:key+2]    # grab the before and after closest\r\n    n1 = _pnt_on_seg_(seg[:-1], pnt)  # abbreviated pnt_on_seg\r\n    d1 = np.linalg.norm(n1 - pnt)\r\n    n2 = _pnt_on_seg_(seg[1:], pnt)   # abbreviated pnt_on_seg\r\n    d2 = np.linalg.norm(n2 - pnt)\r\n    if d1 <= d2:\r\n        return [n1[0], n1[1], np.asscalar(d1)]\r\n    return [n2[0], n2[1], np.asscalar(d2)]\r\n\r\n\r\ndef p_o_p(pnt, polys):\r\n    """""" main runner\r\n    """"""\r\n    result = []\r\n    for p in polys:\r\n        result.append(pnt_on_poly(p, pnt))\r\n    return result\r\n\r\n\r\ndef adjacency_edge():\r\n    """"""keep... adjacency-edge list association\r\n    : the columns are \'from\', the rows are \'to\' the cell value is the \'weight\'.\r\n    """"""\r\n    adj = np.random.randint(1, 4, size=(5, 5))\r\n    r, c = adj.shape\r\n    mask = ~np.eye(r, c, dtype=\'bool\')\r\n    # dt = [(\'Source\', \'<i4\'), (\'Target\', \'<i4\'), (\'Weight\', \'<f8\')]\r\n    m = mask.ravel()\r\n    XX, YY = np.meshgrid(np.arange(c), np.arange(r))\r\n    # mmm = np.stack((m, m, m), axis=1)\r\n    XX = np.ma.masked_array(XX, mask=m)\r\n    YY = np.ma.masked_array(YY, mask=m)\r\n    edge = np.stack((XX.ravel(), YY.ravel(), adj.ravel()), axis=1)\r\n    frmt = """"""\r\n    Adjacency edge list association...\r\n    {}\r\n    edge...\r\n    (col, row, value)\r\n    {}\r\n    """"""\r\n    print(dedent(frmt).format(adj, edge))\r\n\r\n\r\n# ---- Extras ----------------------------------------------------------------\r\n#\r\ndef _test(a0=None):\r\n    """"""testing stuff using a0""""""\r\n    import math\r\n    if a0 is None:\r\n        a0 = np.array([[10, 10], [10, 20], [20, 20], [10, 10]])\r\n    x0, y0 = p0 = a0[-2]\r\n    p1 = a0[-1]\r\n    dx, dy = p1 - p0\r\n    dist = math.hypot(dx, dy)\r\n    xc, yc = pc = p0 + (p1 - p0)/2.0\r\n    slope = math.atan2(dy, dx)\r\n    step = 2.\r\n    xn = x0 + math.cos(slope) * step  # dist / fact\r\n    yn = y0 + math.sin(slope) * step  # dist / fact\r\n    # better\r\n    start = 0\r\n    step = 2.0\r\n    stop = 10.0 + step/2\r\n    x2 = np.arange(start, stop + step, step)\r\n    return a0, dist, xc, yc, pc, slope, xn, yn, x2\r\n\r\n\r\n# ---- data and demos --------------------------------------------------------\r\n#\r\ndef _arrs_(prn=True):\r\n    """"""Sample arrays to test various cases\r\n    """"""\r\n    cw = np.array([[0, 0], [0, 100], [100, 100], [100, 80], [20, 80],\r\n                   [20, 20], [100, 20], [100, 0], [0, 0]])  # capital C\r\n    a0 = np.array([[10, 10], [10, 20], [20, 20], [10, 10]])\r\n    a1 = np.array([[20., 20.], [20., 30.], [30., 30.], [30., 20.], [20., 20.]])\r\n    a2 = np.array([(20., 20.), (20., 30.), (30., 30.), (30., 20.), (20., 20.)],\r\n                  dtype=[(\'X\', \'<f8\'), (\'Y\', \'<f8\')])\r\n    a3 = np.array([([20.0, 20.0],), ([20.0, 30.0],), ([30.0, 30.0],),\r\n                   ([30.0, 20.0],), ([20.0, 20.0],)],\r\n                  dtype=[(\'Shape\', \'<f8\', (2,))])\r\n    a_1a = np.asarray([a1, a1])\r\n    a_1b = np.asarray([a1, a1[:-1]])\r\n    a_2a = np.asarray([a2, a2])\r\n    a_2b = np.asarray([a2, a2[:-1]])\r\n    a_3a = np.asarray([a3, a3])\r\n    a_3b = np.asarray([a3, a3[:-1]])\r\n    a0 = a0 - [10, 10]\r\n    a1 = a1 - [10, 10]\r\n    a = [a0, a1, a2, a3, a_1a, a_1b, a_2a, a_2b, a_3a, a_3b]\r\n    sze = [i.size for i in a]\r\n    shp = [len(i.shape) for i in a]\r\n    dtn = [len(i.dtype) for i in a]\r\n    if prn:\r\n        a = [a0, a1, a2, a3, a_1a, a_1b, a_2a, a_2b, a_3a, a_3b, cw]\r\n        n = [\'a0\', \'a1\', \'a2\', \'a3\',\r\n             \'a_1a\', \'a_1b\',\r\n             \'a_2a\', \'a_2b\',\r\n             \'a_3a\', \'a_3b\']\r\n        args = [\'array\', \'kind\', \'size\', \'ndim\', \'shape\', \'dtype\']\r\n        frmt = ""{!s:<6} {!s:<5} {!s:<5} {!s:<4} {!s:<10} {!s:<20}""\r\n        print(frmt.format(*args))\r\n        cnt = 0\r\n        for i in a:\r\n            args = [n[cnt], i.dtype.kind, i.size, i.ndim, i.shape, i.dtype]\r\n            print(dedent(frmt).format(*args))\r\n            cnt += 1\r\n    a.extend([sze, shp, dtn])\r\n    return a\r\n\r\n\r\ndef _demo(prn=True):\r\n    """"""Demo the densify function using Ontario boundary polyline\r\n    :\r\n    : ---- Ontario boundary polyline, shape (49,874, 2) ----\r\n    :  x = ""...script location.../Data/Ontario.npy""\r\n#   : a = np.load(x)\r\n    : alternates... but slower\r\n    : def PolyArea(x, y):\r\n    :     return 0.5*np.abs(np.dot(x, np.roll(y,1))-np.dot(y, np.roll(x,1)))\r\n\r\n    """"""\r\n    x = ""/"".join(script.split(""/"")[:-1]) + ""/Data/Ontario.npy""\r\n    a = np.load(x)\r\n    fact = 2\r\n    b = _densify_2D(a, fact=fact)\r\n    t0, avl = e_leng(a)  # first is total, 2nd is all lengths\r\n    t1 = t0/1000.\r\n    min_l = avl.min()\r\n    avg_l = avl.mean()\r\n    max_l = avl.max()\r\n    ar = e_area(a)\r\n    ar1 = ar/1.0e04\r\n    ar2 = ar/1.0e06\r\n    if prn:\r\n        frmt = """"""\r\n        Original number of points... {:,}\r\n        Densified by a factor of ... {}\r\n        New point count ............ {:,}\r\n        Ontario perimeter .......... {:,.1f} m  {:,.2f} km\r\n        Segments lengths ... min  {:,.2f} m\r\n                             mean {:,.2f} m\r\n                             max  {:,.2f} m\r\n        Ontario area ............... {:,.2f} ha.  {:,.1f} sq.km.\r\n        """"""\r\n        args = [a.shape[0], fact, b.shape[0], t0, t1, min_l,\r\n                avg_l, max_l, ar1, ar2]\r\n        print(dedent(frmt).format(*args))\r\n    return a\r\n\r\n#temp = np.subtract(a, xyz)  # so we only have to compute this once\r\n#dist = np.linalg.norm(np.subtract(temp, np.multiply(np.dot(temp, n)[:, None], n)),\r\n#                      axis=-1)\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    from _common import fc_info\r\n#    from fc import _xyID, obj_array, _two_arrays\r\n#    from tools import group_pnts\r\n\r\n#    args = _arrs_(prn=False)  # prn=True to see array properties\r\n#    a0, a1, a2, a3, a_1a, a_1b, a_2a, a_2b, a_3a, a_3b = args[:10]\r\n#    sze, shp, dtn = args[10:]\r\n#    a = np.array([[0, 0.05], [1, 1.05], [2, 1.95], [3, 3.0],\r\n#                  [4, 4.1], [5, 5.2], [6, 5.9]])\r\n#    dist, xc, yc, pc, slope, xn, yn, x2 = _test(a0)\r\n#    fc = r""C:\\Git_Dan\\a_Data\\arcpytools_demo.gdb\\polylines_pnts""\r\n\r\n# for Ontario_LCC\r\n# total_length(a)  #: 6804096.2018476073  same as arcmap\r\n# areas(a)  #: [1074121438784.0]  1074121438405.34021\r\n\r\n#     ---- end\r\n#    from arraytools.fc_tools import fc\r\n#    in_fc = r\'C:\\Git_Dan\\a_Data\\arcpytools_demo.gdb\\Can_geom_sp_LCC\'\r\n#    in_fc = r\'C:\\Git_Dan\\a_Data\\arcpytools_demo.gdb\\Ontario_LCC\'\r\n#    in_fc = r""C:\\Git_Dan\\a_Data\\arcpytools_demo.gdb\\Carp_5x5""   # full 25 polygons\r\n#    a = fc._xy(in_fc)\r\n#    a_s = group_pnts(a, key_fld=\'IDs\', shp_flds=[\'Xs\', \'Ys\'])\r\n#    a_s = np.asarray(a_s)\r\n#    a_area = areas(a_s)\r\n#    a_tot_leng = total_length(a_s)\r\n#    a_seg_leng = seg_lengths(a_s)\r\n#    a_ng = angles_poly(a1)\r\n#    v = r\'C:\\Git_Dan\\arraytools\\Data\\sample_100K.npy\'  # 20, 1000, 10k, 100K\r\n#    oa = obj_array(in_fc)\r\n#    ta = _two_arrays(in_fc, both=True, split=True)\r\n'"
all_scripts/geomtester.py,5,"b'# -*- coding: utf-8 -*-\r\n""""""\r\n\r\n=======\r\n\r\nScript :   .py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified : 2018-\r\n\r\nPurpose :  tools for working with numpy arrays and geometry\r\n\r\nNotes:\r\n\r\nReferences:\r\n\r\n""""""\r\nimport sys\r\nimport numpy as np\r\n\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\nimport arcpy\r\nimport timeit\r\n\r\npoly_rings = [\r\n    [[15,0], [25,0], [25,10], [15,10], [15,0]],\r\n    [[18,13], [24,13], [24,18], [18,18], [18,13]\r\n]]\r\n\r\ndef FromArcPyArray():\r\n    aarr = arcpy.Array(\r\n        arcpy.Array(arcpy.Point(*xy) for xy in ring) for ring in poly_rings\r\n    )\r\n    return arcpy.Polygon(aarr)\r\n\r\ndef FromEsriJSON():\r\n    esri_json = {""type"":""Polygon"", ""rings"":poly_rings}\r\n    return arcpy.AsShape(esri_json, True)\r\n\r\ndef FromGeoJSON():\r\n    geojson = {""type"":""Polygon"", ""coordinates"":poly_rings}\r\n    return arcpy.AsShape(geojson)\r\n\r\ndef FromWKT():\r\n    wkt = ""MULTIPOLYGON({})"".format(\r\n        "","".join(""(({}))"".format(\r\n            "", "".join(""{} {}"".format(*xy) for xy in ring)\r\n        ) for ring in poly_rings)\r\n    )\r\n    return arcpy.FromWKT(wkt)\r\n\r\nfor ctor in [FromArcPyArray, FromEsriJSON, FromGeoJSON, FromWKT]:\r\n    pg = ctor()\r\n    print(""\\n"".join(\r\n        str(i) for i in [ctor.__name__, timeit.timeit(ctor, number=10000), """"]\r\n    ))\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n\'\'\'\r\npoly_rings = [\r\n    [[15,0], [25,0], [25,10], [15,10], [15,0]],\r\n    [[18,13], [24,13], [24,18], [18,18], [18,13]],\r\n    [[10,0], [5, 0]]]\r\n\r\nz = np.asarray(poly_rings)\r\n\r\nz\r\nOut[240]: \r\narray([list([[15, 0], [25, 0], [25, 10], [15, 10], [15, 0]]),\r\n       list([[18, 13], [24, 13], [24, 18], [18, 18], [18, 13]]),\r\n       list([[10, 0], [5, 0]])], dtype=object)\r\n\r\nwkt = ""MULTIPOLYGON({})"".format(\r\n    "","".join(""(({}))"".format(\r\n        "", "".join(""{} {}"".format(*xy) for xy in ring)\r\n    ) for ring in z))\r\n\r\narcpy.FromWKT(wkt)\r\nOut[242]: <Polygon object at 0x2a6592b6630[0x2a6580a4c88]>\r\n\r\nesri_json = {""type"":""Polygon"", ""rings"":z.tolist()}\r\n\r\narcpy.AsShape(esri_json, True)\r\nOut[244]: <Polygon object at 0x2a6581abc50[0x2a658115d00]>\r\n\r\n\r\n\r\nz = np.asarray([np.asarray(i) for i in poly_rings])\r\n\r\nz\r\nOut[246]: \r\narray([array([[15,  0],\r\n       [25,  0],\r\n       [25, 10],\r\n       [15, 10],\r\n       [15,  0]]),\r\n       array([[18, 13],\r\n       [24, 13],\r\n       [24, 18],\r\n       [18, 18],\r\n       [18, 13]]),\r\n       array([[10,  0],\r\n       [ 5,  0]])], dtype=object)\r\n\r\nzl = [i.tolist() for i in z]; esri_json = {""type"":""Polygon"", ""rings"":zl}\r\n\r\narcpy.AsShape(esri_json, True)\r\nOut[251]: <Polygon object at 0x2a6564607f0[0x2a658115d28]>\r\n\'\'\'\r\n'"
all_scripts/grid.py,76,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\ngrid\r\n====\r\n\r\nScript :   grid.py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified : 2018-11-23\r\n\r\nPurpose :  tools for working with numpy arrays\r\n\r\nRequires:\r\n---------\r\narraytools.tools - nd2struct, stride\r\n\r\nFunctions:\r\n----------\r\n>>> art.grid.__all__\r\n[\'check_shapes\', \'combine_\', \'expand_zone\', \'euc_dist\', \'euc_alloc\', \'expand_\',\r\n \'shrink_\', \'regions_\', \'expand_zone\', \'fill_arr\', \'reclass_vals\',\r\n \'reclass_ranges\', \'scale_up\']\r\n\r\nReferences:\r\n-----------\r\n\r\n`<https://community.esri.com/blogs/dan_patterson/2018/01/19/\r\ncombine-data-classification-from-raster-combinations>`_\r\n\r\n`<https://stackoverflow.com/questions/48035246/\r\nintersect-multiple-2d-np-arrays-for-determining-zones>`_\r\n\r\n\r\n---------------------------------------------------------------------\r\n""""""\r\n# pylint: disable=C0103\r\n# pylint: disable=R1710\r\n# pylint: disable=R0914\r\n\r\n# ---- imports, formats, constants ----\r\nimport sys\r\n#from textwrap import dedent, indent\r\nimport numpy as np\r\nfrom arraytools.tools import nd_rec, stride\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.2f}\'.format}\r\nnp.set_printoptions(edgeitems=5, linewidth=80, precision=2, suppress=True,\r\n                    threshold=500, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n__all__ = [\'check_shapes\',\r\n           \'combine_\',\r\n           \'euc_dist\',\r\n           \'euc_alloc\',\r\n           \'expand_\',\r\n           \'shrink_\',\r\n           \'regions_\',\r\n           \'expand_zone\',  # other functions\r\n           \'fill_arr\',\r\n           \'reclass_vals\',\r\n           \'reclass_ranges\',\r\n           \'scale_up\'\r\n           ]\r\n\r\n\r\n# ---- array checks and creation --------------------------------------------\r\n# ---- 3D arrays for stacked operations\r\n#\r\ndef check_shapes(arrs):\r\n    """"""Check the shapes of the arrays to ensure they are all equal\r\n    """"""\r\n    shps = [i.shape for i in arrs]\r\n    eq = np.all(np.array([shps[0] == i for i in shps[1:]]))\r\n    err = ""Arrays `arrs` need to have the same shape...""\r\n    if not eq:\r\n        raise ValueError(""{}\\n{}"".format(err, shps))\r\n\r\n\r\n# ---- array functions ----------------------------------------------------\r\n# (1) combine ----\r\ndef combine_(arrs, ret_classes=False):\r\n    """"""Combine arrays to produce a unique classification scheme\r\n\r\n    `arrs` : iterable\r\n        list, tuple of arrays of the same shape\r\n    `ret_classes` : array\r\n        a structured array with the class values for each array and the\r\n        last column is the new_class\r\n\r\n    Notes:\r\n    ------\r\n    You should mask any values prior to running this if you want to account\r\n    for nodata values.\r\n\r\n    """"""\r\n    err = ""\\n...A list of 2D arrays, or a 3D array is required, not...{}\\n""\r\n    check_shapes(arrs)\r\n    seq = [isinstance(i, (list, tuple)) for i in arrs]\r\n    is_seq = np.array(seq).all()\r\n    is_nd = np.array([isinstance(i, np.ndarray) for i in arrs]).all()\r\n    if is_seq:\r\n        indices = [np.unique(arr, return_inverse=True)[1] for arr in arrs]\r\n    elif is_nd:\r\n        if isinstance(arrs, np.ma.MaskedArray):\r\n            indices = [np.ma.unique(arrs[i], return_inverse=True)[1]\r\n                       for i in range(arrs.shape[0])]\r\n        else:\r\n            indices = [np.unique(arrs[i], return_inverse=True)[1]\r\n                       for i in range(len(arrs))]\r\n    else:\r\n        print(err.format(arrs))\r\n        return arrs\r\n    #\r\n    M = np.array([item.max()+1 for item in indices])\r\n    M = np.r_[1, M[:-1]]\r\n    strides = M.cumprod()\r\n    indices = np.stack(indices, axis=-1)\r\n    vals = (indices * strides).sum(axis=-1)\r\n    uniqs, cls_new = np.unique(vals, return_inverse=True)\r\n    combo = cls_new.reshape(arrs[0].shape)\r\n    if ret_classes:\r\n        classes = np.array([np.ravel(i) for i in arrs]).T\r\n        classes = np.c_[classes, cls_new]\r\n        classes = nd_rec(classes)    # call nd_rec\r\n        classes = np.unique(classes)\r\n        classes = classes[np.argsort(classes, order=classes.dtype.names)]\r\n        return combo, classes\r\n    return combo\r\n\r\n\r\ndef euc_dist(a, origins=0, cell_size=1):\r\n    """"""Calculate the euclidean distance and/or allocation\r\n\r\n    Parameters:\r\n    -----------\r\n    a : array\r\n        numpy float or integer array\r\n    origins : number, list or tuple\r\n        The locations to calculate distance for.  Anything that is not a mask\r\n        is an origin. If a single number is provided, a `mask` will be created\r\n        using it.  A list/tuple of values can be used for multiple value\r\n        masking.\r\n    cell_size : float, int\r\n        The cell size of the raster.  What does each cell represent on the\r\n        ground.  1.0 is assumed\r\n    """"""\r\n    from scipy import ndimage as nd\r\n    #\r\n    cell_size = abs(cell_size)\r\n    if cell_size == 0:\r\n        cell_size = 1\r\n    msk = (~np.isin(a, origins)).astype(\'int\')\r\n    dist = nd.distance_transform_edt(msk,\r\n                                     sampling=cell_size,\r\n                                     return_distances=True)\r\n    return dist\r\n\r\n\r\ndef euc_alloc(a, fill_zones=0):\r\n    """"""Calculate the euclidean distance and/or allocation\r\n\r\n    Parameters:\r\n    -----------\r\n    a : array\r\n        numpy float or integer array\r\n    fill_zones : number, list or tuple\r\n        These are the cells/zones to fill with the values of the closest cell.\r\n        If a single number is provided, a `mask` will be created using it.  A\r\n        list or tuple of values can be used to provide multiple value masking.\r\n    dist : boolean\r\n        True, the distance of the closest non-masked value to the masked cell\r\n    alloc : boolean\r\n        True, the value of the closest non-masked value to the masked cell\r\n    """"""\r\n    from scipy import ndimage as nd\r\n    #\r\n    msk = (np.isin(a, fill_zones)).astype(\'int\')\r\n    idx = nd.distance_transform_edt(msk,\r\n                                    return_distances=False,\r\n                                    return_indices=True)\r\n    alloc = a[tuple(idx)]\r\n    return alloc\r\n\r\n\r\ndef expand_(a, val=1, mask_vals=0, buff_dist=1):\r\n    """"""Expand/buffer a raster by cells (a distance)\r\n    """"""\r\n    from scipy import ndimage as nd\r\n    if isinstance(val, (list, tuple)):\r\n        m = np.isin(a, val, invert=True).astype(\'int\')\r\n    else:\r\n        m = np.where(a == val, 0, 1)\r\n    dist, idx = nd.distance_transform_edt(m, return_distances=True,\r\n                                          return_indices=True)\r\n    alloc = a[tuple(idx)]\r\n    a0 = np.where(dist <= buff_dist, alloc, a)  #0)\r\n    return a0\r\n\r\n\r\ndef shrink_(a, val=1, mask_vals=0, buff_dist=1):\r\n    """"""Expand/buffer a raster by a distance\r\n    """"""\r\n    from scipy import ndimage as nd\r\n    if isinstance(val, (list, tuple)):\r\n        m = np.isin(a, val, invert=False).astype(\'int\')\r\n    else:\r\n        m = np.where(a == val, 1, 0)\r\n    dist, idx = nd.distance_transform_edt(m, return_distances=True,\r\n                                          return_indices=True)\r\n    alloc = a[tuple(idx)]\r\n    m = np.logical_and(dist > 0, dist <= buff_dist)\r\n    a0 = np.where(m, alloc, a)  #0)\r\n    return a0\r\n\r\n\r\ndef regions_(a, cross=True):\r\n    """"""Delineate `regions` or `zones` in a raster.  This is analogous to\r\n    `regiongroup` in gis software.  In scipy.ndimage, a `label` is ascribed\r\n    to these groupings.  Any nonzero value will be considered a zone.\r\n    A `structure` is used to filter the raster to describe cell connectivity.\r\n\r\n    Parameters:\r\n    -----------\r\n    a : ndarray\r\n        pre-processing may be needed to assign values to `0` which will be\r\n        considered background/offsite\r\n    cross : boolean\r\n       - True, [[0,1,0], [1,1,1], [0,1,0]], diagonal cells not included\r\n       - False, [[1,1,1], [1,1,1], [1,1,1]], diagonals included\r\n\r\n    Notes:\r\n    ------\r\n    The use of `np.unique` will ensure that array values are queried and\r\n    returned in ascending order.\r\n\r\n    big sample 2000x2000  about 1 sec with 16 classes\r\n        aa = np.repeat(np.repeat(a, 500, axis=1), 500, axis=0)\r\n    """"""\r\n    from scipy import ndimage as nd\r\n    #\r\n    if (a.ndim != 2) or (a.dtype.kind != \'i\'):\r\n        msg = ""\\nA 2D array of integers is required, you provided\\n{}""\r\n        print(msg.format(a))\r\n        return a\r\n    if cross:\r\n        struct = np.array([[0, 1, 0], [1, 1, 1], [0, 1, 0]])\r\n    else:\r\n        struct = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\r\n    #\r\n    u = np.unique(a)\r\n    out = np.zeros_like(a, dtype=a.dtype)\r\n    details = []\r\n    is_first = True\r\n    for i in u:\r\n        z = np.where(a == i, 1, 0)\r\n        s, n = nd.label(z, structure=struct)\r\n        details.append([i, n])\r\n        m = np.logical_and(out == 0, s != 0)\r\n        if is_first:\r\n            out = np.where(m, s, out)\r\n            is_first = False\r\n            n_ = n\r\n        else:\r\n            out = np.where(m, s+n_, out)\r\n            n_ += n\r\n    details = np.array(details)\r\n    details = np.c_[(details, np.cumsum(details[:, 1]))]\r\n    return out, details\r\n\r\n\r\n# ---- Raster functions ------------------------------------\r\n#\r\ndef expand_zone(a, zone=None, win=2):\r\n    """"""Expand a value (zone) in a 2D array, normally assumed to represent a\r\n    raster surface.\r\n\r\n    zone : number\r\n        The value/class to expand into the surrounding cells\r\n    win : list/tuple\r\n        select a (2, 2) or (3, 3) moving window\r\n    """"""\r\n    msg = ""\\nYou need a zone that is within the range of values.""\r\n    if zone is None:\r\n        print(msg)\r\n        return a, None\r\n    if (zone < a.min()) or (zone > a.max()):\r\n        print(msg)\r\n        return a, None\r\n    if win not in (2, 3):\r\n        win = 2\r\n    p = [1, 0][win == 2]  # check for 2 or 3 in win\r\n    ap = np.pad(a, pad_width=(1, p), mode=""constant"", constant_values=(0, 0))\r\n    # n, m = ap.shape\r\n    if win == 2:\r\n        a_c = ap[1:, 1:]  # for 2x2 even\r\n    elif win == 3:\r\n        a_c = ap[1:-1, 1:-1]  # for 3x3 odd\r\n    a_s = stride(ap, win=(win, win), stepby=(win, win))  # stride the array\r\n    r, c = a_s.shape[:2]\r\n    out = []\r\n    x = a_s.shape[0]\r\n    y = a_s.shape[1]\r\n    for i in range(x):\r\n        for j in range(y):\r\n            if zone in a_s[i, j]:\r\n                out.append(1)\r\n            else:\r\n                out.append(0)\r\n    out1 = np.asarray(out).reshape(r, c)\r\n    out = np.repeat(np.repeat(out1, 2, axis=1), 2, axis=0)\r\n    dx, dy = np.array(out.shape) - np.array(a.shape)\r\n    if dx != 0:\r\n        out = out[:dx, :dy]\r\n    final = np.where(out == 1, zone, a_c)\r\n    return final\r\n\r\n\r\ndef fill_arr(a, win=(3, 3)):\r\n    """"""try filling an array\r\n    as in fill, sinks\r\n    """"""\r\n    #fd = np.array([[32, 64, 128], [16, 0, 1], [8, 4, 2]])  # flow direction\r\n#    if (zone < a.min()) or (zone > a.max()) or (zone is None):\r\n#        print(""\\nYou need a zone that is within the range of values."")\r\n#        return a, None\r\n    if win[0] == 3:\r\n        pr = 1\r\n    else:\r\n        pr = 0\r\n    ap = np.pad(a, pad_width=(1, pr), mode=""constant"", constant_values=(0, 0))\r\n    if win == (2, 2):\r\n        a_c = ap[1:, 1:]  # for 2x2 even\r\n    elif win == (3, 3):\r\n        a_c = ap[1:-1, 1:-1]   # for 3x3 odd\r\n    a_s = stride(a_c, win=win)  # stride the array\r\n    r, c = a_s.shape[:2]\r\n    out = []\r\n    x = a_s.shape[0]\r\n    y = a_s.shape[1]\r\n    for i in range(x):\r\n        for j in range(y):\r\n            # do stuff\r\n            sub = a_s[i, j].ravel()\r\n            edges = np.asarray([sub[:4], sub[5:]]).ravel()\r\n            e_min = edges[np.argmax(edges)]  # argmax or argmin???\r\n            if sub[4] < e_min:\r\n                out.append(e_min)\r\n            else:\r\n                out.append(sub[4])\r\n    out = np.asarray(out).reshape(r, c)\r\n    return out  # , a_s, ap, a_c\r\n\r\n\r\n# (xx) reclass_vals .... code section\r\ndef reclass_vals(a, old_vals=[], new_vals=[], mask=False, mask_val=None):\r\n    """"""Reclass an array of integer or floating point values.\r\n\r\n    Requires:\r\n    --------\r\n    old_vals : number(s)\r\n        list/array of values to reclassify\r\n    new_bins : number(s)\r\n        new class values for old value\r\n    mask : boolean\r\n        Does the raster contains nodata values or values to be masked\r\n    mask_val : number(s)\r\n        Values to use as the mask\r\n\r\n    Array dimensions will be squeezed.\r\n\r\n     >>> a = np.arange(10).reshape(2,5)\r\n     >>> a0 = np.arange(5)\r\n     >>> art.reclass_vals(a, a0, np.ones_like(a0))\r\n     # array([[0, 1, 2, 3, 4]   ==> array([[1, 1, 1, 1, 1],\r\n     #        [5, 6, 7, 8, 9]])           [5, 6, 7, 8, 9]])\r\n    """"""\r\n    a_rc = np.copy(a)\r\n    args = [old_vals, new_vals]\r\n    msg = ""\\nError....\\nLengths of old and new classes not equal \\n{}\\n{}\\n""\r\n    if len(old_vals) != len(new_vals):\r\n        print(msg.format(*args))\r\n        return a\r\n    old_new = np.array(list(zip(old_vals, new_vals)), dtype=\'int32\')\r\n    for pair in old_new:\r\n        q = (a == pair[0])\r\n        a_rc[q] = pair[1]\r\n    return a_rc\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# (15) reclass .... code section\r\ndef reclass_ranges(a, bins=[], new_bins=[], mask=False, mask_val=None):\r\n    """"""Reclass an array of integer or floating point values based on old and\r\n    new range values.\r\n\r\n    Requires:\r\n    --------\r\n\r\n    bins : list/array\r\n        Sequential list/array of the lower limits of each class include one\r\n        value higher to cover the upper range.\r\n    new_bins : number(s)\r\n        New class values for each bin\r\n    mask : boolean\r\n        Does the raster contains nodata values or values to be masked\r\n    mask_val : number(s)\r\n        Values to use as the mask\r\n\r\n    Array dimensions will be squeezed.\r\n\r\n    >>> z = np.arange(3*5).reshape(3,5)\r\n    >>> bins = [0, 5, 10, 15]\r\n    >>> new_bins = [1, 2, 3, 4]\r\n    >>> z_recl = reclass(z, bins, new_bins, mask=False, mask_val=None)\r\n       # ==> .... z                     ==> .... z_recl\r\n       array([[ 0,  1,  2,  3,  4],   array([[1, 1, 1, 1, 1],\r\n              [ 5,  6,  7,  8,  9],          [2, 2, 2, 2, 2],\r\n              [10, 11, 12, 13, 14]])         [3, 3, 3, 3, 3]])\r\n    """"""\r\n    a_rc = np.zeros_like(a)\r\n    if len(bins) < 2:  # or (len(new_bins <2)):\r\n        print(""Bins = {} new = {} won\'t work"".format(bins, new_bins))\r\n        return a\r\n    if len(new_bins) < 2:\r\n        new_bins = np.arange(1, len(bins)+2)\r\n    new_classes = list(zip(bins[:-1], bins[1:], new_bins))\r\n    for rc in new_classes:\r\n        q1 = (a >= rc[0])\r\n        q2 = (a < rc[1])\r\n        a_rc = a_rc + np.where(q1 & q2, rc[2], 0)\r\n    return a_rc\r\n\r\n\r\n# (16) scale .... code section\r\ndef scale_up(a, x=2, y=2, num_z=None):\r\n    """"""Scale the input array repeating the array values up by the\r\n    x and y factors.\r\n\r\n    Requires:\r\n    --------\r\n    a : array\r\n        An ndarray, 1D arrays will be upcast to 2D\r\n    x, y : numbers\r\n        Factors to scale the array in x (col) and y (row).  Scale factors\r\n        must be greater than 2\r\n    num_z : number\r\n        For 3D, produces the 3rd dimension, ie. if num_z = 3 with the\r\n        defaults, you will get an array with shape=(3, 6, 6).  If\r\n        num_z != None or 0, then the options are \'repeat\', \'random\'.\r\n        With \'repeat\' the extras are kept the same and you can add random\r\n        values to particular slices of the 3rd dimension, or multiply them.\r\n\r\n    Returns:\r\n    -------\r\n    >>> a = np.array([[0, 1, 2], [3, 4, 5]]\r\n    >>> b = scale(a, x=2, y=2)\r\n    array([[0, 0, 1, 1, 2, 2],\r\n           [0, 0, 1, 1, 2, 2],\r\n           [3, 3, 4, 4, 5, 5],\r\n           [3, 3, 4, 4, 5, 5]])\r\n\r\n    Notes:\r\n    -----\r\n    >>> a = np.arange(2*2).reshape(2,2)\r\n    array([[0, 1],\r\n           [2, 3]])\r\n    >>> f_(scale(a, x=2, y=2, num_z=2))\r\n    Array... shape (3, 4, 4), ndim 3, not masked\r\n    0, 0, 1, 1    0, 0, 1, 1    0, 0, 1, 1\r\n    0, 0, 1, 1    0, 0, 1, 1    0, 0, 1, 1\r\n    2, 2, 3, 3    2, 2, 3, 3    2, 2, 3, 3\r\n    2, 2, 3, 3    2, 2, 3, 3    2, 2, 3, 3\r\n    sub (0)       sub (1)       sub (2)\r\n\r\n    """"""\r\n    if (x < 1) or (y < 1):\r\n        print(""x or y scale < 1... \\n{}"".format(scale_up.__doc__))\r\n        return None\r\n    a = np.atleast_2d(a)\r\n    z0 = np.tile(a.repeat(x), y)  # repeat for x, then tile\r\n    z1 = np.hsplit(z0, y)         # split into y parts horizontally\r\n    z2 = np.vstack(z1)            # stack them vertically\r\n    if a.shape[0] > 1:            # if there are more, repeat\r\n        z3 = np.hsplit(z2, a.shape[0])\r\n        z3 = np.vstack(z3)\r\n    else:\r\n        z3 = np.vstack(z2)\r\n    if num_z not in (0, None):\r\n        d = [z3]\r\n        for i in range(num_z):\r\n            d.append(z3)\r\n        z3 = np.dstack(d)\r\n        z3 = np.rollaxis(z3, 2, 0)\r\n    return z3\r\n\r\n\r\n# ---- demo functions -------------------------------------------------------\r\n#\r\ndef _demo_combine():\r\n    """"""demo combine\r\n    dt = [(\'a\', \'<i8\'), (\'b\', \'<i8\'), (\'c\', \'<i8\'), (\'vals\', \'<i8\')]\r\n    """"""\r\n    a = np.array([[0, 0, 0, 4, 4, 4, 1, 1, 1],\r\n                  [0, 0, 0, 4, 4, 4, 1, 1, 1],\r\n                  [0, 0, 0, 4, 4, 4, 1, 1, 1],\r\n                  [2, 2, 2, 1, 1, 1, 2, 2, 2],\r\n                  [2, 2, 2, 1, 1, 1, 2, 2, 2],\r\n                  [2, 2, 2, 1, 1, 1, 2, 2, 2],\r\n                  [1, 1, 1, 4, 4, 4, 0, 0, 0],\r\n                  [1, 1, 1, 4, 4, 4, 0, 0, 0],\r\n                  [1, 1, 1, 4, 4, 4, 0, 0, 0]])\r\n\r\n    b = np.array([[0, 0, 0, 1, 1, 1, 2, 2, 2],\r\n                  [0, 0, 0, 1, 1, 1, 2, 2, 2],\r\n                  [0, 0, 0, 1, 1, 1, 2, 2, 2],\r\n                  [3, 3, 3, 4, 4, 4, 5, 5, 5],\r\n                  [3, 3, 3, 4, 4, 4, 5, 5, 5],\r\n                  [3, 3, 3, 4, 4, 4, 5, 5, 5],\r\n                  [0, 0, 0, 1, 1, 1, 2, 2, 2],\r\n                  [0, 0, 0, 1, 1, 1, 2, 2, 2],\r\n                  [0, 0, 0, 1, 1, 1, 2, 2, 2]])\r\n\r\n    c = np.array([[0, 0, 0, 3, 3, 3, 0, 0, 0],\r\n                  [0, 0, 0, 3, 3, 3, 0, 0, 0],\r\n                  [0, 0, 0, 3, 3, 3, 0, 0, 0],\r\n                  [1, 1, 1, 4, 4, 4, 1, 1, 1],\r\n                  [1, 1, 1, 4, 4, 4, 1, 1, 1],\r\n                  [1, 1, 1, 4, 4, 4, 1, 1, 1],\r\n                  [2, 2, 2, 5, 5, 5, 2, 2, 2],\r\n                  [2, 2, 2, 5, 5, 5, 2, 2, 2],\r\n                  [2, 2, 2, 5, 5, 5, 2, 2, 2]])\r\n#    ret = combine_(*[a, b, c])\r\n    return a, b, c  #, ret\r\n\r\n\r\ndef _demo_reclass():\r\n    """"""\r\n    : -\r\n    """"""\r\n    a = np.array([[9, 8, 2, 3, 4, 3, 5, 5, 2, 2],\r\n                  [4, 1, 4, 2, 4, 2, 4, 2, 3, 2],\r\n                  [5, 3, 5, 4, 5, 4, 5, 3, 1, 2],\r\n                  [5, 2, 3, 1, 4, 4, 3, 5, 4, 3],\r\n                  [2, 3, 2, 5, 5, 2, 5, 5, 4, 4],\r\n                  [5, 3, 4, 4, 2, 1, 3, 2, 4, 3],\r\n                  [3, 2, 3, 3, 3, 4, 3, 2, 4, 3],\r\n                  [4, 5, 2, 3, 2, 2, 3, 1, 4, 4],\r\n                  [3, 5, 5, 5, 2, 2, 4, 3, 4, 4],\r\n                  [4, 5, 4, 5, 3, 2, 4, 3, 1, 3]])\r\n#    f = np.array([[32, 64, 128], [16, 0, 1], [8, 4, 2]])\r\n#    out, out2 = expand_zone(a, zone=1, win=(3,3))\r\n    a_rc = reclass_vals(a,\r\n                        old_vals=[1, 3, 5],\r\n                        new_vals=[9, 5, 1],\r\n                        mask=False,\r\n                        mask_val=None)\r\n    return a_rc\r\n\r\ndef _demo_euclid():\r\n    """""" euclid functions""""""\r\n    a = np.array([[0, 1, 0, 0, 2, 0, 0, 0],   # note the block of 0\'s in the\r\n                  [1, 0, 0, 1, 1, 0, 0, 0],   # top right corner\r\n                  [0, 1, 0, 1, 1, 0, 0, 0],\r\n                  [0, 2, 0, 3, 0, 0, 0, 3],\r\n                  [0, 1, 2, 0, 0, 4, 2, 0],\r\n                  [4, 0, 0, 3, 2, 5, 1, 0],\r\n                  [1, 1, 0, 0, 0, 5, 0, 0],   # and the bottom right\r\n                  [0, 5, 0, 4, 0, 3, 0, 0]])\r\n    b = np.array(([0, 1, 1, 1, 1],  # from scipy help\r\n                  [0, 0, 1, 1, 1],\r\n                  [0, 1, 1, 1, 1],\r\n                  [0, 1, 1, 1, 0],\r\n                  [0, 1, 1, 0, 0]))\r\n    return a, b\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n# https://stackoverflow.com/questions/47861214/\r\n'"
all_scripts/h5py_testing.py,4,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   .py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2018-xx-xx\r\n:Purpose:  tools for working with numpy arrays\r\n:Useage:\r\n:\r\n:References:\r\n:\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nimport h5py\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\nv = r\'C:\\Git_Dan\\arraytools\\Data\\sample_1000.npy\'\r\na = np.load(v)\r\nv_h = r\'C:\\Git_Dan\\arraytools\\Data\\sample_100K.hdf5\'\r\na.dtype = [(\'OBJECTID\', \'<i4\'), (\'f0\', \'<i4\'), (\'County\', \'<S2\'),\r\n           (\'Town\', \'<S6\'), (\'Facility\', \'<S8\'), (\'Time\', \'<i4\')]\r\nshp = a.shape\r\nwith h5py.File(v_h, ""w"") as f:\r\n    dset = f.create_dataset(""sample100K"", shp, dtype=dt, data=a)\r\n\r\n\r\ndef _demo():\r\n    """"""\r\n    : -\r\n    """"""\r\n    pass\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n    _demo()\r\n\r\n'"
all_scripts/hulls.py,21,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\nhulls.py\r\n========\r\n\r\nScript:   hulls.py\r\n\r\nAuthor:   Dan.Patterson@carleton.ca\r\n\r\nModified: 2019-06-08\r\n\r\nPurpose:  working with numpy arrays to determine convex and concave hulls\r\n\r\nReferences:\r\n-----------\r\n`<https://community.esri.com/blogs/dan_patterson/2018/03/11/\r\nconcave-hulls-the-elusive-container>\'_.\r\n`<https://github.com/jsmolka/hull/blob/master/hull.py>\'_.\r\n`<https://stackoverflow.com/questions/563198/how-do-you-detect-where-two-\r\nline-segments-intersect#565282>\'_.\r\n`<http://www.codeproject.com/Tips/862988/Find-the-intersection-\r\npoint-of-two-line-segments>\'_.\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nfrom numpy.lib.recfunctions import structured_to_unstructured as stu\r\nfrom arcpytools_pnt import tweet, output_polylines, output_polygons\r\nimport arcpy\r\nimport warnings\r\nimport math\r\n\r\n\r\nwarnings.simplefilter(\'ignore\', FutureWarning)\r\n\r\narcpy.overwriteOutput = True\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\nPI = math.pi\r\n\r\n\r\ndef pnt_in_list(pnt, pnts_list):\r\n    """"""Check to see if a point is in a list of points\r\n    """"""\r\n    is_in = np.any([np.isclose(pnt, i) for i in pnts_list])\r\n    return is_in\r\n\r\n\r\ndef intersects(*args):\r\n    """"""Line intersection check.  Two lines or 4 points that form the lines.\r\n    :Requires:\r\n    :--------\r\n    :  intersects(line0, line1) or intersects(p0, p1, p2, p3)\r\n    :   p0, p1 -> line 1\r\n    :   p2, p3 -> line 2\r\n    :Returns: boolean, if the segments do intersect\r\n    :--------\r\n    :References:\r\n    :--------\r\n    : https://stackoverflow.com/questions/563198/how-do-you-detect-where-two-\r\n    :        line-segments-intersect#565282\r\n    """"""\r\n    if len(args) == 2:\r\n        p0, p1, p2, p3 = *args[0], *args[1]\r\n    elif len(args) == 4:\r\n        p0, p1, p2, p3 = args\r\n    else:\r\n        raise AttributeError(""Pass 2, 2-pnt lines or 4 points to the function"")\r\n    #\r\n    # ---- First check ----   np.cross(p1-p0, p3-p2 )\r\n    p0_x, p0_y, p1_x, p1_y, p2_x, p2_y, p3_x, p3_y = *p0, *p1, *p2, *p3\r\n    s10_x = p1_x - p0_x\r\n    s10_y = p1_y - p0_y\r\n    s32_x = p3_x - p2_x\r\n    s32_y = p3_y - p2_y\r\n    denom = s10_x * s32_y - s32_x * s10_y\r\n    if denom == 0.0:\r\n        return False\r\n    #\r\n    # ---- Second check ----  np.cross(p1-p0, p0-p2 )\r\n    den_gt0 = denom > 0\r\n    s02_x = p0_x - p2_x\r\n    s02_y = p0_y - p2_y\r\n    s_numer = s10_x * s02_y - s10_y * s02_x\r\n    if (s_numer < 0) == den_gt0:\r\n        return False\r\n    #\r\n    # ---- Third check ----  np.cross(p3-p2, p0-p2)\r\n    t_numer = s32_x * s02_y - s32_y * s02_x\r\n    if (t_numer < 0) == den_gt0:\r\n        return False\r\n    #\r\n    if ((s_numer > denom) == den_gt0) or ((t_numer > denom) == den_gt0):\r\n        return False\r\n    #\r\n    # ---- check to see if the intersection point is one of the input points\r\n    t = t_numer / denom\r\n    # substitute p0 in the equation\r\n    x = p0_x + (t * s10_x)\r\n    y = p0_y + (t * s10_y)\r\n    # be careful that you are comparing tuples to tuples, lists to lists\r\n    if sum([(x, y) == tuple(i) for i in [p0, p1, p2, p3]]) > 0:\r\n        return False\r\n    return True\r\n\r\n\r\ndef angle(p0, p1, prv_ang=0):\r\n    """"""Angle between two points and the previous angle, or zero.\r\n    """"""\r\n    ang = math.atan2(p0[1] - p1[1], p0[0] - p1[0])\r\n    a0 = (ang - prv_ang)\r\n    a0 = a0 % (PI * 2) - PI\r\n    return a0\r\n\r\n\r\ndef point_in_polygon(pnt, poly):  # pnt_in_poly(pnt, poly):  #\r\n    """"""Point is in polygon. ## fix this and use pip from arraytools\r\n    """"""\r\n    x, y = pnt\r\n    N = len(poly)\r\n    for i in range(N):\r\n        x0, y0, xy = [poly[i][0], poly[i][1], poly[(i + 1) % N]]\r\n        c_min = min([x0, xy[0]])\r\n        c_max = max([x0, xy[0]])\r\n        if c_min < x <= c_max:\r\n            p = y0 - xy[1]\r\n            q = x0 - xy[0]\r\n            y_cal = (x - x0) * p / q + y0\r\n            if y_cal < y:\r\n                return True\r\n    return False\r\n\r\n\r\ndef knn(pnts, p, k):\r\n    """"""\r\n    Calculates k nearest neighbours for a given point.\r\n\r\n    :param points: list of points\r\n    :param p: reference point\r\n    :param k: amount of neighbours\r\n    :return: list\r\n    """"""\r\n    s = sorted(pnts,\r\n               key=lambda x: math.sqrt((x[0]-p[0])**2 + (x[1]-p[1])**2))[0:k]\r\n    return s\r\n\r\n\r\ndef knn0(pnts, p, k):\r\n    """"""\r\n    Calculates k nearest neighbours for a given point.\r\n\r\n    points : array\r\n        list of points\r\n    p : two number array-like\r\n        reference point\r\n    k : integer\r\n        amount of neighbours\r\n    Returns:\r\n    --------\r\n    list of the k nearest neighbours, based on squared distance\r\n    """"""\r\n    p = np.asarray(p)\r\n    pnts = np.asarray(pnts)\r\n    diff = pnts - p[np.newaxis, :]\r\n    d = np.einsum(\'ij,ij->i\', diff, diff)\r\n    idx = np.argsort(d)[:k]\r\n#    s = [i.tolist() for i in pnts[idx]]\r\n    return pnts[idx].tolist()\r\n\r\n\r\ndef concave(points, k):\r\n    """"""Calculates the concave hull for given points\r\n    :Requires:\r\n    :--------\r\n    : points - initially the input set of points with duplicates removes and\r\n    :    sorted on the Y value first, lowest Y at the top (?)\r\n    : k - initially the number of points to start forming the concave hull,\r\n    :    k will be the initial set of neighbors\r\n    :Notes:  This recursively calls itself to check concave hull\r\n    : p_set - The working copy of the input points\r\n    :-----\r\n    """"""\r\n    k = max(k, 3)  # Make sure k >= 3\r\n    p_set = list(set(points[:]))  # Remove duplicates if not done already\r\n    if len(p_set) < 3:\r\n        raise Exception(""p_set length cannot be smaller than 3"")\r\n    elif len(p_set) == 3:\r\n        return p_set  # Points are a polygon already\r\n    k = min(k, len(p_set) - 1)  # Make sure k neighbours can be found\r\n\r\n    frst_p = cur_p = min(p_set, key=lambda x: x[1])\r\n    hull = [frst_p]  # Initialize hull with first point\r\n    p_set.remove(frst_p)  # Remove first point from p_set\r\n    prev_ang = 0\r\n\r\n    while (cur_p != frst_p or len(hull) == 1) and len(p_set) > 0:\r\n        if len(hull) == 3:\r\n            p_set.append(frst_p)  # Add first point again\r\n        knn_pnts = knn(p_set, cur_p, k)  # knn or knn0\r\n        cur_pnts = sorted(knn_pnts, key=lambda x: -angle(x, cur_p, prev_ang))\r\n\r\n        its = True\r\n        i = -1\r\n        while its and i < len(cur_pnts) - 1:\r\n            i += 1\r\n            last_point = 1 if cur_pnts[i] == frst_p else 0\r\n            j = 1\r\n            its = False\r\n            while not its and j < len(hull) - last_point:\r\n                its = intersects(hull[-1], cur_pnts[i], hull[-j - 1], hull[-j])\r\n                j += 1\r\n        if its:  # All points intersect, try a higher number of neighbours\r\n            return concave(points, k + 1)\r\n        prev_ang = angle(cur_pnts[i], cur_p)\r\n        cur_p = cur_pnts[i]\r\n        hull.append(cur_p)  # Valid candidate was found\r\n        p_set.remove(cur_p)\r\n\r\n    for point in p_set:\r\n        if not point_in_polygon(point, hull):\r\n            return concave(points, k + 1)\r\n    #\r\n    return hull\r\n\r\n\r\n# ---- convex hull ----------------------------------------------------------\r\n#\r\ndef cross(o, a, b):\r\n    """"""Cross-product for vectors o-a and o-b\r\n    """"""\r\n    xo, yo = o\r\n    xa, ya = a\r\n    xb, yb = b\r\n    return (xa - xo)*(yb - yo) - (ya - yo)*(xb - xo)\r\n#    return (a[0] - o[0]) * (b[1] - o[1]) - (a[1] - o[1]) * (b[0] - o[0])\r\n\r\n\r\ndef convex(points):\r\n    """"""Calculates the convex hull for given points\r\n    :Input is a list of 2D points [(x, y), ...]\r\n    """"""\r\n    points = sorted(set(points))  # Remove duplicates\r\n    if len(points) <= 1:\r\n        return points\r\n    # Build lower hull\r\n    lower = []\r\n    for p in points:\r\n        while len(lower) >= 2 and cross(lower[-2], lower[-1], p) <= 0:\r\n            lower.pop()\r\n        lower.append(p)\r\n    # Build upper hull\r\n    upper = []\r\n    for p in reversed(points):\r\n        while len(upper) >= 2 and cross(upper[-2], upper[-1], p) <= 0:\r\n            upper.pop()\r\n        upper.append(p)\r\n    print(""lower\\n{}\\nupper\\n{}"".format(lower, upper))\r\n    return np.array(lower[:-1] + upper)  # upper[:-1]) # for open loop\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# .... running script or testing code section\r\ndef _tool():\r\n    """"""run when script is from a tool\r\n    """"""\r\n    in_fc = sys.argv[1]\r\n    group_by = str(sys.argv[2])\r\n    k_factor = int(sys.argv[3])\r\n    hull_type = str(sys.argv[4])\r\n    out_type = str(sys.argv[5])\r\n    out_fc = sys.argv[6]\r\n    return in_fc, group_by, k_factor, hull_type, out_type, out_fc\r\n\r\n\r\ngdb_pth = ""/"".join(script.split(""/"")[:-2]) + ""/Data/Point_tools.gdb""\r\n\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    in_fc = gdb_pth + r""/r_sorted""\r\n    group_by = \'Group_\'\r\n    k_factor = 3\r\n    hull_type = \'concave\'  # \'convex\'\r\n    out_type = \'Polyline\'\r\n    out_fc = gdb_pth + r""/r_11""\r\nelse:\r\n    testing = False\r\n    in_fc, group_by, k_factor, hull_type, out_type, out_fc = _tool()\r\n\r\nmsg = """"""\\n\r\n-----------------------------------------------------------------------\r\n---- Concave/convex hull ----\r\nscript    {}\r\nTesting   {}\r\nin_fc     {}\r\ngroup_by  {}\r\nk_factor  {}\r\nhull_type {}\r\nout_type  {}\r\nout_fc    {}\r\n-----------------------------------------------------------------------\r\n\r\n""""""\r\nargs = [script, testing, in_fc, group_by, k_factor,\r\n        hull_type, out_type, out_fc]\r\ntweet(msg.format(*args))\r\n\r\ndesc = arcpy.da.Describe(in_fc)\r\nSR = desc[\'spatialReference\']\r\n#\r\n# (1) ---- get the points\r\nout_flds = [\'OID@\', \'SHAPE@X\', \'SHAPE@Y\'] + [group_by]\r\na = arcpy.da.FeatureClassToNumPyArray(in_fc, out_flds, """", SR, True)\r\n#\r\n# (2) ---- determine the unique groupings of the points\r\nuniq, idx, rev = np.unique(a[group_by], True, True)\r\ngroups = [a[np.where(a[group_by] == i)[0]] for i in uniq]\r\n#\r\n# (3) ---- for each group, perform the concave hull\r\nhulls = []\r\nfor i in range(0, len(groups)):\r\n    p = groups[i]\r\n    p = p[[\'SHAPE@X\', \'SHAPE@Y\']]\r\n    n = len(p)\r\n    p = stu(p)\r\n    #\r\n    # ---- point preparation section ------------------------------------\r\n    p = np.array(list(set([tuple(i) for i in p])))  # Remove duplicates\r\n    idx_cr = np.lexsort((p[:, 0], p[:, 1]))         # indices of sorted array\r\n    in_pnts = np.asarray([p[i] for i in idx_cr])    # p[idx_cr]  #\r\n    in_pnts = in_pnts.tolist()\r\n    in_pnts = [tuple(i) for i in in_pnts]\r\n    if hull_type == \'concave\':\r\n        cx = np.array(concave(in_pnts, k_factor))  # requires a list of tuples\r\n    else:\r\n        cx = np.array(convex(in_pnts))\r\n    hulls.append(cx.tolist())\r\n    # ----\r\n    #\r\nif out_type == \'Polyline\':\r\n    output_polylines(out_fc, SR, [hulls])\r\nelif out_type == \'Polygon\':\r\n    output_polygons(out_fc, SR, [hulls])\r\nelse:\r\n    for i in hulls:\r\n        print(""Hulls\\n{}"".format(np.array(i)))\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n'"
all_scripts/hulls_editing.py,54,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   .py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2018-xx-xx\r\n:Purpose:  tools for working with numpy arrays\r\n:Useage:\r\n:\r\n:References:\r\n: https://github.com/jsmolka/hull/blob/master/hull.py\r\n: https://stackoverflow.com/questions/563198/how-do-you-detect-where-two-\r\n:        line-segments-intersect#565282\r\n: http://www.codeproject.com/Tips/862988/Find-the-intersection-\r\n:       point-of-two-line-segments\r\n: considerCollinearOverlapAsIntersect => co_check\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nfrom arcpytools import output_polylines, output_polygons\r\nimport arcpy\r\nimport warnings\r\nimport math\r\n\r\n\r\nwarnings.simplefilter(\'ignore\', FutureWarning)\r\n\r\narcpy.overwriteOutput = True\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\nPI = math.pi\r\n\r\n\r\n## ---- Modified code from references\r\n##\r\ndef e_dist(a, b, metric=\'euclidean\'):\r\n    """"""Distance calculation for 1D, 2D and 3D points using einsum\r\n    : a, b   - list, tuple, array in 1,2 or 3D form\r\n    : metric - euclidean (\'e\',\'eu\'...), sqeuclidean (\'s\',\'sq\'...),\r\n    :-----------------------------------------------------------------------\r\n    """"""\r\n    a = np.asarray(a)\r\n    b = np.atleast_2d(b)\r\n    a_dim = a.ndim\r\n    b_dim = b.ndim\r\n    if a_dim == 1:\r\n        a = a.reshape(1, 1, a.shape[0])\r\n    if a_dim >= 2:\r\n        a = a.reshape(np.prod(a.shape[:-1]), 1, a.shape[-1])\r\n    if b_dim > 2:\r\n        b = b.reshape(np.prod(b.shape[:-1]), b.shape[-1])\r\n    diff = a - b\r\n    dist_arr = np.einsum(\'ijk,ijk->ij\', diff, diff)\r\n    if metric[:1] == \'e\':\r\n        dist_arr = np.sqrt(dist_arr)\r\n    dist_arr = np.squeeze(dist_arr)\r\n    return dist_arr\r\n\r\n\r\ndef nearest_n(pnts, pt, n):   # renamed tonearest_n(pnts, pt, n):\r\n    """"""n-nearest neighbours for a pnts list.\r\n    : pnts - points array (xs, ys)\r\n    : pt - reference point (px, py)\r\n    : n - nearest neighbours\r\n    : srted => sorted(sqrt((xs[0] - px[0])**2 + (ys[1] - py[1])**2))[0:k]\r\n    :Notes:\r\n    :------\r\n    :  for two 2d vectors U = (Ux, Uy) V = (Vx, Vy)\r\n    :  the crossproduct is    U x V = Ux*Vy - Uy*Vx\r\n    """"""\r\n    nn_idx = np.argsort(e_dist(pt, pnts))  # [:n]\r\n    p = pnts[nn_idx]\r\n    return p[:n]\r\n\r\n\r\ndef intersect_pnt(p0, p1, p2, p3):\r\n    """"""Returns the point of intersection of the segment passing through two\r\n    :  line segments (p0, p1) and (p2, p3)\r\n    :Notes:\r\n    :------\r\n    :         p0,            p1,             p2,            p3\r\n    : (array([0, 0]), array([10, 10]),array([0, 5]), array([5, 0]))\r\n    : s: array([[ 0,  0],    h: array([[  0.,   0.,   1.],\r\n    :           [10, 10],              [ 10.,  10.,   1.],\r\n    :           [ 0,  5],              [  0.,   5.,   1.],\r\n    :           [ 5,  0]])             [  5.,   0.,   1.]])\r\n    :Reference:\r\n    :---------\r\n    : https://stackoverflow.com/questions/3252194/numpy-and-line-intersections\r\n    """"""\r\n    s = np.vstack([p0, p1, p2, p3])      # s for stacked\r\n    h = np.hstack((s, np.ones((4, 1))))  # h for homogeneous\r\n    l1 = np.cross(h[0], h[1])            # get first line\r\n    l2 = np.cross(h[2], h[3])            # get second line\r\n    x, y, z = np.cross(l1, l2)           # point of intersection\r\n    if z == 0:                           # lines are parallel\r\n        return (float(\'inf\'), float(\'inf\'))\r\n    return (x/z, y/z)\r\n\r\n\r\ndef pnt_in_list(pnt, pnts_list):\r\n    """"""Check to see if a point is in a list of points\r\n    """"""\r\n    is_in = np.any([np.isclose(pnt, i) for i in pnts_list])\r\n    return is_in\r\n\r\n\r\ndef intersects(*args):\r\n    """"""Line intersection check.  Two lines or 4 points that form the lines.\r\n    :Requires:\r\n    :--------\r\n    :  intersects(line0, line1) or intersects(p0, p1, p2, p3)\r\n    :   p0, p1 -> line 1\r\n    :   p2, p3 -> line 2\r\n    :Returns: boolean, if the segments do intersect\r\n    :--------\r\n    :References:\r\n    :--------\r\n    : https://stackoverflow.com/questions/563198/how-do-you-detect-where-two-\r\n    :        line-segments-intersect#565282\r\n    """"""\r\n    if len(args) == 2:\r\n        p0, p1, p2, p3 = *args[0], *args[1]\r\n    elif len(args) == 4:\r\n        p0, p1, p2, p3 = args\r\n    else:\r\n        raise AttributeError(""Pass 2, 2-pnt lines or 4 points to the function"")\r\n    #\r\n    # ---- First check ----   np.cross(p1-p0, p3-p2 )\r\n    p0_x, p0_y, p1_x, p1_y, p2_x, p2_y, p3_x, p3_y = *p0, *p1, *p2, *p3\r\n    s10_x = p1_x - p0_x\r\n    s10_y = p1_y - p0_y\r\n    s32_x = p3_x - p2_x\r\n    s32_y = p3_y - p2_y\r\n    denom = s10_x * s32_y - s32_x * s10_y\r\n    if denom == 0.0:\r\n        return False\r\n    #\r\n    # ---- Second check ----  np.cross(p1-p0, p0-p2 )\r\n    den_gt0 = denom > 0\r\n    s02_x = p0_x - p2_x\r\n    s02_y = p0_y - p2_y\r\n    s_numer = s10_x * s02_y - s10_y * s02_x\r\n    if (s_numer < 0) == den_gt0:\r\n        return False\r\n    #\r\n    # ---- Third check ----  np.cross(p3-p2, p0-p2)\r\n    t_numer = s32_x * s02_y - s32_y * s02_x\r\n    if (t_numer < 0) == den_gt0:\r\n        return False\r\n    #\r\n    if ((s_numer > denom) == den_gt0) or ((t_numer > denom) == den_gt0):\r\n        return False\r\n    #\r\n    # ---- check to see if the intersection point is one of the input points\r\n    t = t_numer / denom\r\n    # substitute p0 in the equation\r\n    x = p0_x + (t * s10_x)\r\n    y = p0_y + (t * s10_y)\r\n    # be careful that you are comparing tuples to tuples, lists to lists\r\n    if sum([(x, y) == tuple(i) for i in [p0, p1, p2, p3]]) > 0:\r\n        return False\r\n    return True\r\n\r\n\r\ndef angle(p0, p1, prv_ang=0):\r\n    """"""Angle between two points and the previous angle, or zero.\r\n    """"""\r\n    ang = math.atan2(p0[1] - p1[1], p0[0] - p1[0])\r\n#    print(""ang {}  prev ang {}"".format(np.degrees(ang), np.degrees(prv_ang)))\r\n    a0 = (ang - prv_ang)\r\n    a0 = a0 % (PI * 2) - PI\r\n    return a0\r\n\r\n#def SortByAngle(kNearestPoints, currentPoint, prevPoint):\r\n#    \'\'\' Sorts the k nearest points given by angle \'\'\'\r\n#    angles = np.zeros(kNearestPoints.shape[0])\r\n#    i = 0\r\n#    for NearestPoint in kNearestPoints:\r\n#        # calculate the angle\r\n#        angle = np.arctan2(NearestPoint[1]-currentPoint[1],\r\n#                NearestPoint[0]-currentPoint[0]) - \\\r\n#                np.arctan2(prevPoint[1]-currentPoint[1],\r\n#                prevPoint[0]-currentPoint[0])\r\n#        angle = np.rad2deg(angle)\r\n#        # only positive angles\r\n#        angle = np.mod(angle+360,360)\r\n#        #print NearestPoint[0], NearestPoint[1], angle\r\n#        angles[i] = angle\r\n#        i=i+1\r\n#    return kNearestPoints[np.argsort(angles)]\r\n\r\ndef point_in_polygon(pnt, poly):  #pnt_in_poly(pnt, poly):  #\r\n    """"""Point is in polygon. ## fix this and use pip from arraytools\r\n    """"""\r\n    x, y = pnt\r\n    N = len(poly)\r\n    for i in range(N):\r\n        x0, y0, xy = [poly[i][0], poly[i][1], poly[(i + 1) % N]]\r\n        c_min = min([x0, xy[0]])\r\n        c_max = max([x0, xy[0]])\r\n        if c_min < x <= c_max:\r\n            p = y0 - xy[1]\r\n            q = x0 - xy[0]\r\n            y_cal = (x - x0) * p / q + y0\r\n            if y_cal < y:\r\n                return True\r\n    return False\r\n\r\n\r\ndef concave2(in_pnts, k):\r\n    """"""Calculates the concave hull for given points\r\n    :Requires:\r\n    :--------\r\n    : in_pnts - initially the input set of points with duplicates removes and\r\n    :    sorted on the Y value first, lowest Y at the top (?)\r\n    : k - initially the number of points to start forming the concave hull,\r\n    :    k will be the initial set of neighbors\r\n    :Notes:  This recursively calls itself to check concave hull\r\n    :-----\r\n    """"""\r\n    k = max(k, 3)  # Make sure k >= 3\r\n    pnts_ = in_pnts  # list(set(in_pnts[:]))  # Remove duplicates\r\n\r\n    if len(pnts_) < 3:\r\n        raise Exception(""Dataset length cannot be smaller than 3"")\r\n    elif len(pnts_) == 3:\r\n        return pnts_  # Points are a polygon already\r\n    k = min(k, len(pnts_) - 1)  # Make sure k neighbours can be found\r\n\r\n    # ---- get the point with the minimum y, throw it into the hull\r\n    # remove it from the input list\r\n    # first_pnt = cur_pnt = min(dataset, key=lambda x: x[1])\r\n    cur_pnt = pnts_[np.argmin(pnts_, axis=0)[1]]  #\r\n    first_pnt = cur_pnt\r\n    hull = [first_pnt]  # Initialize hull with first point\r\n    pnts_ = np.delete(pnts_, 0, axis=0)  # Remove first point from dataset\r\n    prv_ang = 0\r\n    # ----  need np.all since curr_pnt and first_pnt are arrays\r\n    while (np.all(cur_pnt != first_pnt) or len(hull) == 1) and len(pnts_) > 0:\r\n        if len(hull) == 3:  # Add first point again\r\n            pnts_ = np.append(pnts_, np.atleast_2d(first_pnt), axis=0)\r\n        nn_pnts = nearest_n(pnts_, cur_pnt, k)  # Find nearest neighbours\r\n#        print(""nn_pnts\\n{}"".format(nn_pnts))\r\n        #\r\n        c_points = sorted(nn_pnts, key=lambda x: -angle(x, cur_pnt, prv_ang))\r\n        #\r\n        is_True = True\r\n        i = -1\r\n        while is_True and i < len(c_points) - 1:\r\n            i += 1\r\n            last_point = 1 if np.all(c_points[i] == first_pnt) else 0\r\n            j = 1\r\n            is_True = False\r\n            while not is_True and j < len(hull) - last_point:\r\n                args = [hull[-1], c_points[i], hull[-j - 1], hull[-j]]\r\n                is_True = intersects(*args)\r\n                j += 1\r\n        if is_True:  # All intersect, try with higher number of neighbours\r\n            return concave(in_pnts, k + 1)\r\n        prv_ang = angle(c_points[i], cur_pnt)\r\n        cur_pnt = c_points[i]\r\n        hull.append(cur_pnt)  # Valid candidate was found\r\n#        pnts_.remove(cur_pnt)  # check\r\n        whr = np.where(pnts_ == cur_pnt)[0]\r\n        pnts_ = np.delete(pnts_, whr, axis=0)\r\n    # ---- final check again\r\n    for point in pnts_:  # final point in polygon check\r\n        if not pnt_in_poly(point, hull):\r\n            return concave(in_pnts, k + 1)\r\n    #\r\n    return hull\r\n#\r\n## --------------------------------------------------------------------------\r\ndef cross(o, a, b):\r\n    """"""\r\n    Calculates cross product.\r\n\r\n    :param o, a: vector\r\n    :param o, b: vector\r\n    :return: int\r\n    """"""\r\n    return (a[0] - o[0]) * (b[1] - o[1]) - (a[1] - o[1]) * (b[0] - o[0])\r\n\r\n\r\ndef convex(points):\r\n    """"""\r\n    Calculates the convex hull for given points\r\n    Input is a list of 2D points [(x, y), ...]\r\n\r\n    :param points: list of points\r\n    :return: list\r\n    """"""\r\n##    points = sorted(set(points))  # Remove duplicates\r\n    if len(points) <= 1:\r\n        return points\r\n\r\n    # Build lower hull\r\n    lower = []\r\n    for p in points:\r\n        while len(lower) >= 2 and cross(lower[-2], lower[-1], p) <= 0:\r\n            lower.pop()\r\n        lower.append(p)\r\n\r\n    # Build upper hull\r\n    upper = []\r\n    for p in reversed(points):\r\n        while len(upper) >= 2 and cross(upper[-2], upper[-1], p) <= 0:\r\n            upper.pop()\r\n        upper.append(p)\r\n\r\n    return np.array(lower[:-1] + upper[:-1])\r\n\r\n\r\n\r\n#import math\r\n#\r\n#\r\ndef knn(points, p, k):\r\n    """"""\r\n    Calculates k nearest neighbours for a given point.\r\n\r\n    :param points: list of points\r\n    :param p: reference point\r\n    :param k: amount of neighbours\r\n    :return: list\r\n    """"""\r\n    return sorted(points, key=lambda x: math.sqrt((x[0] - p[0]) ** 2 + (x[1] - p[1]) ** 2))[0:k]\r\n#\r\n#\r\n#def intersects(p1, p2, p3, p4):\r\n#    """"""\r\n#    Checks if lines p1, p2 and p3, p4 intersect.\r\n#\r\n#    :param p1, p2: line\r\n#    :param p3, p4: line\r\n#    :return: bool\r\n#    """"""\r\n#    p0_x, p0_y = p1\r\n#    p1_x, p1_y = p2\r\n#    p2_x, p2_y = p3\r\n#    p3_x, p3_y = p4\r\n#\r\n#    s10_x = p1_x - p0_x\r\n#    s10_y = p1_y - p0_y\r\n#    s32_x = p3_x - p2_x\r\n#    s32_y = p3_y - p2_y\r\n#\r\n#    denom = s10_x * s32_y - s32_x * s10_y\r\n#    if denom == 0:\r\n#        return False\r\n#\r\n#    denom_positive = denom > 0\r\n#    s02_x = p0_x - p2_x\r\n#    s02_y = p0_y - p2_y\r\n#    s_numer = s10_x * s02_y - s10_y * s02_x\r\n#    if (s_numer < 0) == denom_positive:\r\n#        return False\r\n#\r\n#    t_numer = s32_x * s02_y - s32_y * s02_x\r\n#    if (t_numer < 0) == denom_positive:\r\n#        return False\r\n#\r\n#    if ((s_numer > denom) == denom_positive) or ((t_numer > denom) == denom_positive):\r\n#        return False\r\n#\r\n#    t = t_numer / denom\r\n#    x = p0_x + (t * s10_x)\r\n#    y = p0_y + (t * s10_y)\r\n#\r\n#    if (x, y) in [p1, p2, p3, p4]:\r\n#        return False\r\n#\r\n#    return True\r\n#\r\n#\r\n#def angle(p1, p2, previous_angle=0):\r\n#    """"""\r\n#    Calculates angle between two points and previous angle.\r\n#\r\n#    :param p1: point\r\n#    :param p2: point\r\n#    :param previous_angle: previous angle\r\n#    :return: float\r\n#    """"""\r\n#    return (math.atan2(p1[1] - p2[1], p1[0] - p2[0]) - previous_angle) % (math.pi * 2) - math.pi\r\n#\r\n#\r\ndef point_in_polygon2(point, polygon):\r\n    """"""\r\n    Checks if point is in polygon.\r\n\r\n    :param point: point\r\n    :param polygon: polygon\r\n    :return: bool\r\n    """"""\r\n    size = len(polygon)\r\n    for i in range(size):\r\n        min_ = min([polygon[i][0], polygon[(i + 1) % size][0]])\r\n        max_ = max([polygon[i][0], polygon[(i + 1) % size][0]])\r\n        if min_ < point[0] <= max_:\r\n            p = polygon[i][1] - polygon[(i + 1) % size][1]\r\n            q = polygon[i][0] - polygon[(i + 1) % size][0]\r\n            point_y = (point[0] - polygon[i][0]) * p / q + polygon[i][1]\r\n            if point_y < point[1]:\r\n                return True\r\n    return False\r\n#\r\n#\r\ndef concave(points, k):\r\n    """"""\r\n    Calculates the concave hull for given points\r\n    Input is a list of 2D points [(x, y), ...]\r\n    k defines the number of of considered neighbours\r\n\r\n    :param points: list of points\r\n    :param k: considered neighbours\r\n    :return: list\r\n    """"""\r\n    k = max(k, 3)  # Make sure k >= 3\r\n    dataset = list(set(points[:]))  # Remove duplicates\r\n    if len(dataset) < 3:\r\n        raise Exception(""Dataset length cannot be smaller than 3"")\r\n    elif len(dataset) == 3:\r\n        return dataset  # Points are a polygon already\r\n    k = min(k, len(dataset) - 1)  # Make sure k neighbours can be found\r\n\r\n    first_point = current_point = min(dataset, key=lambda x: x[1])\r\n    hull = [first_point]  # Initialize hull with first point\r\n    dataset.remove(first_point)  # Remove first point from dataset\r\n    previous_angle = 0\r\n\r\n    while (current_point != first_point or len(hull) == 1) and len(dataset) > 0:\r\n        if len(hull) == 3:\r\n            dataset.append(first_point)  # Add first point again\r\n        kn_points = knn(dataset, current_point, k)  # Find nearest neighbours\r\n        c_points = sorted(kn_points, key=lambda x: -angle(x, current_point, previous_angle))\r\n\r\n        its = True\r\n        i = -1\r\n        while its and i < len(c_points) - 1:\r\n            i += 1\r\n            last_point = 1 if c_points[i] == first_point else 0\r\n            j = 1\r\n            its = False\r\n            while not its and j < len(hull) - last_point:\r\n                its = intersects(hull[-1], c_points[i], hull[-j - 1], hull[-j])\r\n                j += 1\r\n        if its:  # All points intersect, try again with higher number of neighbours\r\n            return concave(points, k + 1)\r\n        previous_angle = angle(c_points[i], current_point)\r\n        current_point = c_points[i]\r\n        hull.append(current_point)  # Valid candidate was found\r\n        dataset.remove(current_point)\r\n\r\n    for point in dataset:\r\n        if not point_in_polygon(point, hull):\r\n            return concave(points, k + 1)\r\n\r\n    return hull\r\n#\r\n\r\n#def cross(o, a, b):\r\n#    """"""\r\n#    Calculates cross product.\r\n#\r\n#    :param o, a: vector\r\n#    :param o, b: vector\r\n#    :return: int\r\n#    """"""\r\n#    return (a[0] - o[0]) * (b[1] - o[1]) - (a[1] - o[1]) * (b[0] - o[0])\r\n#\r\n#\r\n#def convex(points):\r\n#    """"""\r\n#    Calculates the convex hull for given points\r\n#    Input is a list of 2D points [(x, y), ...]\r\n#\r\n#    :param points: list of points\r\n#    :return: list\r\n#    """"""\r\n#    points = sorted(set(points))  # Remove duplicates\r\n#    if len(points) <= 1:\r\n#        return points\r\n#\r\n#    # Build lower hull\r\n#    lower = []\r\n#    for p in points:\r\n#        while len(lower) >= 2 and cross(lower[-2], lower[-1], p) <= 0:\r\n#            lower.pop()\r\n#        lower.append(p)\r\n#\r\n#    # Build upper hull\r\n#    upper = []\r\n#    for p in reversed(points):\r\n#        while len(upper) >= 2 and cross(upper[-2], upper[-1], p) <= 0:\r\n#            upper.pop()\r\n#        upper.append(p)\r\n#\r\n#    return lower[:-1] + upper[:-1]\r\n## ----------------------------------------------------------------------------\r\n##\r\ndef _tool():\r\n    """"""run when script is from a tool\r\n    """"""\r\n    in_fc = sys.argv[1]\r\n    group_by = str(sys.argv[2])\r\n    out_type = str(sys.argv[3])\r\n    out_fc = sys.argv[4]\r\n    return in_fc, from_north, cent, out_fc0, out_fc1\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# .... running script or testing code section\r\n\r\n\r\ngdb_pth = ""/"".join(script.split(""/"")[:-2]) + ""/Data/Point_tools.gdb""\r\n\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    in_fc = gdb_pth + r""/shapes""\r\n    group_by = \'ID\'\r\n    out_type = \'Polyline\'\r\n    out_fc = gdb_pth + r""/concave2""\r\n\r\nelse:\r\n    testing = False\r\n    in_fc, from_north, cent, out_fc0, out_fc1 = _tool()\r\n\r\n\r\ndesc = arcpy.da.Describe(in_fc)\r\nSR = desc[\'spatialReference\']\r\n#\r\n# (1) ---- get the points\r\nout_flds = [\'OID@\', \'SHAPE@X\', \'SHAPE@Y\'] + [group_by]\r\na = arcpy.da.FeatureClassToNumPyArray(in_fc, out_flds, """", SR, True)\r\n#\r\n# (2) ---- determine the unique groupings of the points\r\nuniq, idx, rev = np.unique(a[\'ID\'], True, True)\r\ngroups = [a[np.where(a[group_by] == i)[0]] for i in uniq]\r\n#\r\n# (3) ---- for each group, perform the concave hull\r\nhulls = []\r\nfor i in range(0, len(groups)):\r\n    p = groups[i]\r\n    n = len(p)\r\n    p = p[[\'SHAPE@X\', \'SHAPE@Y\']]\r\n    p = p.view(np.float64).reshape(n, 2)\r\n    idx_cr = np.lexsort((p[:, 0], p[:, 1]))       # indices of sorted array\r\n    in_pnts = np.asarray([p[i] for i in idx_cr])\r\n    in_pnts = in_pnts.tolist()\r\n    in_pnts = [tuple(i) for i in in_pnts]\r\n    cx = np.array(concave(in_pnts, 3))\r\n    hulls.append(cx.tolist())\r\nprint(""concave hull points...\\n{}"".format(hulls))\r\n\r\noutput_polylines(out_fc, SR, [hulls])\r\n# ----------------------------------------------------------------------------\r\ndef test():\r\n    """"""\r\n    cc = array([[442,  40],\r\n                [471, 187],\r\n                [433, 267],\r\n                [128, 261],\r\n                [ 33, 159],\r\n                [214,  49]])\r\n     """"""\r\n    p = [(207, 184), (393, 60), (197, 158), (197, 114), (128, 261),\r\n          (442, 40), (237, 159), (338, 75), (194, 93), (33, 159),\r\n          (393, 152), (433, 267), (324, 141), (384, 183), (273, 165),\r\n          (250, 257), (423, 198), (227, 68), (120, 184), (214, 49),\r\n          (256, 75), (379, 93), (312, 49), (471, 187), (366, 122)]\r\n    p = set(p)                                    # set removes duplicates\r\n    a = np.asarray(list(p))                       # convert to an array\r\n    idx_cr = np.lexsort((a[:, 0], a[:, 1]))       # indices of sorted array\r\n    in_pnts = np.asarray([a[i] for i in idx_cr])  # sorted array\r\n    cx = np.array(concave(in_pnts, 3))            # concave hull\r\n    cc = np.array(convex(in_pnts.tolist()))       # convex hull\r\n    return p, a, in_pnts, cx, cc\r\n\r\n\r\ndef test_main():\r\n    """""" """"""\r\n    in_fc = r""C:\\Git_Dan\\a_Data\\arcpytools_demo.gdb\\r_sorted""\r\n    out_fc = r""C:\\Git_Dan\\a_Data\\arcpytools_demo.gdb\\r_convex_hull""\r\n    desc = arcpy.da.Describe(in_fc)\r\n    SR = desc[\'spatialReference\']\r\n    flds = [\'SHAPE@X\', \'SHAPE@Y\']\r\n    args = [in_fc, flds, None, None, True, (None, None)]\r\n    cur = arcpy.da.SearchCursor(*args)\r\n    a = _xy(in_fc)                    # ---- get the points\r\n    a = uniq(a, axis=0)               # ---- get the unique points\r\n#    a_s = a[a[:, 1].argsort(axis=0)]  # sort to get lowest y-value\r\n    idx_cr = np.lexsort((a[:, 0], a[:, 1]))\r\n    in_pnts = np.asarray([a[i] for i in idx_cr])  # sort by column 1, then 0\r\n#    output_polylines(out_fc, SR, [pl])  # ***** it works\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n#    from tools import uniq\r\n#    in_fc = r""C:\\Git_Dan\\a_Data\\arcpytools_demo.gdb\\r_sorted""\r\n#    out_fc = r""C:\\Git_Dan\\a_Data\\arcpytools_demo.gdb\\r_convex_hull""\r\n#    desc = arcpy.da.Describe(in_fc)\r\n#    SR = desc[\'spatialReference\']\r\n#    flds = [\'SHAPE@X\', \'SHAPE@Y\']\r\n#    args = [in_fc, flds, None, None, True, (None, None)]\r\n#    cur = arcpy.da.SearchCursor(*args)\r\n#    a = _xy(in_fc)                    # ---- get the points\r\n#    a = uniq(a, axis=0)               # ---- get the unique points\r\n##    a_s = a[a[:, 1].argsort(axis=0)]  # sort to get lowest y-value\r\n#    idx_cr = np.lexsort((a[:, 0], a[:, 1]))\r\n#    in_pnts = np.asarray([a[i] for i in idx_cr])  # sort by column 1, then 0\r\n#    output_polylines(out_fc, SR, [pl])  # ***** it works\r\n\r\n#    in_pnts = [tuple(i) for i in a_s.tolist()]\r\n#    pnts = concave(in_pnts, 3)\r\n#    output_polylines(out_fc, SR, [pnts])\r\n#    ps = [(207, 184), (393, 60), (197, 158), (197, 114), (128, 261),\r\n#          (442, 40), (237, 159), (338, 75), (194, 93), (33, 159),\r\n#          (393, 152), (433, 267), (324, 141), (384, 183), (273, 165),\r\n#          (250, 257), (423, 198), (227, 68), (120, 184), (214, 49),\r\n#          (256, 75), (379, 93), (312, 49), (471, 187), (366, 122)]\r\n#    a = np.array(ps)\r\n#    a_s = a[a[:, 1].argsort(axis=0)]  # sort to get lowest y-value\r\n#    idx_cr = np.lexsort((a[:, 0], a[:, 1]))\r\n#    in_pnts = np.asarray([a[i] for i in idx_cr])  # sort by column 1, then 0\r\n##    cx = concave(in_pnts, 3)\r\n'"
all_scripts/hulls_original.py,47,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   .py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2018-xx-xx\r\n:Purpose:  tools for working with numpy arrays\r\n:Useage:\r\n:\r\n:References:\r\n: https://github.com/jsmolka/hull/blob/master/hull.py\r\n: https://stackoverflow.com/questions/563198/how-do-you-detect-where-two-\r\n:        line-segments-intersect#565282\r\n: http://www.codeproject.com/Tips/862988/Find-the-intersection-\r\n:       point-of-two-line-segments\r\n: considerCollinearOverlapAsIntersect => co_check\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport math\r\nimport numpy as np\r\nfrom fc import _xy\r\nfrom tools import uniq\r\nfrom geom import e_dist\r\nfrom apt import output_polylines\r\nimport arcpy\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\nPI = np.pi\r\n\r\n# ---- Modified code from references\r\n#\r\ndef nearest_n(pnts, pt, n):   # renamed tonearest_n(pnts, pt, n):\r\n    """"""n-nearest neighbours for a pnts list.\r\n    : pnts - points array (xs, ys)\r\n    : pt - reference point (px, py)\r\n    : n - nearest neighbours\r\n    : srted => sorted(sqrt((xs[0] - px[0])**2 + (ys[1] - py[1])**2))[0:k]\r\n    :Notes:\r\n    :------\r\n    :  for two 2d vectors U = (Ux, Uy) V = (Vx, Vy)\r\n    :  the crossproduct is    U x V = Ux*Vy - Uy*Vx\r\n    """"""\r\n    nn_idx = np.argsort(e_dist(pt, pnts))[:n]\r\n    return pnts[nn_idx]\r\n\r\n\r\ndef intersect_pnt(p0, p1, p2, p3):\r\n    """"""Returns the point of intersection of the segment passing through two\r\n    :  line segments (p0, p1) and (p2, p3)\r\n    :Notes:\r\n    :------\r\n    :         p0,            p1,             p2,            p3\r\n    : (array([0, 0]), array([10, 10]),array([0, 5]), array([5, 0]))\r\n    : s: array([[ 0,  0],    h: array([[  0.,   0.,   1.],\r\n    :           [10, 10],              [ 10.,  10.,   1.],\r\n    :           [ 0,  5],              [  0.,   5.,   1.],\r\n    :           [ 5,  0]])             [  5.,   0.,   1.]])\r\n    :Reference:\r\n    :---------\r\n    : https://stackoverflow.com/questions/3252194/numpy-and-line-intersections\r\n    """"""\r\n    s = np.vstack([p0, p1, p2, p3])      # s for stacked\r\n    h = np.hstack((s, np.ones((4, 1))))  # h for homogeneous\r\n    l1 = np.cross(h[0], h[1])            # get first line\r\n    l2 = np.cross(h[2], h[3])            # get second line\r\n    x, y, z = np.cross(l1, l2)           # point of intersection\r\n    if z == 0:                           # lines are parallel\r\n        return (float(\'inf\'), float(\'inf\'))\r\n    return (x/z, y/z)\r\n\r\n\r\ndef pnt_in_list(pnt, pnts_list):\r\n    """"""Check to see if a point is in a list of points\r\n    """"""\r\n    is_in = np.any([np.isclose(pnt, i) for i in pnts_list])\r\n    return is_in\r\n\r\n\r\ndef intersects(*args):\r\n    """"""Line intersection check.  Two lines or 4 points that form the lines.\r\n    :Requires:\r\n    :--------\r\n    :  intersects(line0, line1) or intersects(p0, p1, p2, p3)\r\n    :   p0, p1 -> line 1\r\n    :   p2, p3 -> line 2\r\n    :Returns: boolean, if the segments do intersect\r\n    :--------\r\n    :References:\r\n    :--------\r\n    : https://stackoverflow.com/questions/563198/how-do-you-detect-where-two-\r\n    :        line-segments-intersect#565282\r\n    """"""\r\n    if len(args) == 2:\r\n        p0, p1, p2, p3 = *args[0], *args[1]\r\n    elif len(args) == 4:\r\n        p0, p1, p2, p3 = args\r\n    else:\r\n        raise AttributeError(""Pass 2, 2-pnt lines or 4 points to the function"")\r\n    #\r\n    # ---- First check ----   np.cross(p1-p0, p3-p2 )\r\n    s10_x, s10_y = p1 - p0\r\n    s32_x, s32_y = p3 - p2\r\n    denom = s10_x * s32_y - s32_x * s10_y\r\n    if np.isclose(denom, 0.0):\r\n        return False\r\n    #\r\n    # ---- Second check ----  np.cross(p1-p0, p0-p2 )\r\n    den_gt0 = denom > 0\r\n    s02_x, s02_y = p0 - p2\r\n    s_numer = s10_x * s02_y - s10_y * s02_x\r\n    if (s_numer < 0) == den_gt0:\r\n        return False\r\n    #\r\n    # ---- Third check ----  np.cross(p3-p2, p0-p2)\r\n    t_numer = s32_x * s02_y - s32_y * s02_x\r\n    if (t_numer < 0) == den_gt0:\r\n        return False\r\n    #\r\n    if ((s_numer > denom) == den_gt0) or ((t_numer > denom) == den_gt0):\r\n        return False\r\n    #\r\n    # ---- check to see if the intersection point is one of the input points\r\n    t = t_numer / denom\r\n    x0, y0 = p0           # substitute p0 in the equation\r\n    x = x0 + (t * s10_x)\r\n    y = y0 + (t * s10_y)\r\n    if np.any([np.all([x, y] == i) for i in [p0, p1, p2, p3]]):\r\n        return False\r\n    return True\r\n\r\n\r\ndef angle(p0, p1, prv_ang=0):\r\n    """"""Angle between two points and the previous angle, or zero.\r\n    """"""\r\n#   PI = np.pi\r\n    a0 = (np.arctan2(p0[1] - p1[1], p0[0] - p1[0]) - prv_ang)\r\n    a0 = a0 % (PI * 2) - PI\r\n    return a0\r\n\r\n\r\ndef pnt_in_poly(pnt, poly):\r\n    """"""Point is in polygon. ## fix this and use pip from arraytools\r\n    """"""\r\n    poly = np.array(poly)\r\n    size = len(poly)\r\n    xs = poly[:, 0]\r\n    ys = poly[:, 1]\r\n    x, y = pnt\r\n    for i in range(size):\r\n        min_ = min([xs[i], poly[(i + 1) % size][0]])\r\n        max_ = max([xs[i], poly[(i + 1) % size][0]])\r\n        if min_ < x <= max_:\r\n            p = ys[i] - poly[(i + 1) % size][1]\r\n            q = xs[i] - poly[(i + 1) % size][0]\r\n            point_y = (x - xs[i]) * p / q + ys[i]\r\n            if point_y < y:\r\n                return True\r\n    return False\r\n\r\n\r\ndef concave(in_pnts, k):\r\n    """"""Calculates the concave hull for given points\r\n    :Requires:\r\n    :--------\r\n    : in_pnts - initially the input set of points with duplicates removes and\r\n    :    sorted on the Y value first, lowest Y at the top (?)\r\n    : k - initially the number of points to start forming the concave hull,\r\n    :    k will be the initial set of neighbors\r\n    :Notes:  This recursively calls itself to check concave hull\r\n    :-----\r\n    """"""\r\n    k = max(k, 3)  # Make sure k >= 3\r\n    pnts_ = in_pnts  # list(set(in_pnts[:]))  # Remove duplicates\r\n\r\n    if len(pnts_) < 3:\r\n        raise Exception(""Dataset length cannot be smaller than 3"")\r\n    elif len(pnts_) == 3:\r\n        return pnts_  # Points are a polygon already\r\n    k = min(k, len(pnts_) - 1)  # Make sure k neighbours can be found\r\n\r\n    # ---- get the point with the minimum y, throw it into the hull\r\n    # remove it from the input list\r\n    # first_pnt = cur_pnt = min(dataset, key=lambda x: x[1])\r\n    cur_pnt = pnts_[np.argmin(pnts_, axis=0)[1]]  #\r\n    first_pnt = cur_pnt\r\n    hull = [first_pnt]  # Initialize hull with first point\r\n    pnts_ = np.delete(pnts_, 0, axis=0)  # Remove first point from dataset\r\n    prv_ang = 0\r\n    # ----  need np.all since curr_pnt and first_pnt are arrays\r\n    while (np.all(cur_pnt != first_pnt) or len(hull) == 1) and len(pnts_) > 0:\r\n        if len(hull) == 3:  # Add first point again\r\n            pnts_ = np.append(pnts_, np.atleast_2d(first_pnt), axis=0)\r\n        nn_pnts = nearest_n(pnts_, cur_pnt, k)  # Find nearest neighbours\r\n#        print(""nn_pnts\\n{}"".format(nn_pnts))\r\n        #\r\n        c_points = sorted(nn_pnts, key=lambda x: -angle(x, cur_pnt, prv_ang))\r\n        #\r\n        is_True = True\r\n        i = -1\r\n        while is_True and i < len(c_points) - 1:\r\n            i += 1\r\n            last_point = 1 if np.all(c_points[i] == first_pnt) else 0\r\n            j = 1\r\n            is_True = False\r\n            while not is_True and j < len(hull) - last_point:\r\n                args = [hull[-1], c_points[i], hull[-j - 1], hull[-j]]\r\n                is_True = intersects(*args)\r\n                j += 1\r\n        if is_True:  # All intersect, try with higher number of neighbours\r\n            return concave(in_pnts, k + 1)\r\n        prv_ang = angle(c_points[i], cur_pnt)\r\n        cur_pnt = c_points[i]\r\n        hull.append(cur_pnt)  # Valid candidate was found\r\n#        pnts_.remove(cur_pnt)  # check\r\n        whr = np.where(pnts_ == cur_pnt)[0]\r\n        pnts_ = np.delete(pnts_, whr, axis=0)\r\n    # ---- final check again\r\n    for point in pnts_:  # final point in polygon check\r\n        if not pnt_in_poly(point, hull):\r\n            return concave(in_pnts, k + 1)\r\n    #\r\n    return hull\r\n\r\n# ------- original ----------------------------------------------------------\r\n#\r\n#def np_perp(a):\r\n#    """"""perpendicular""""""\r\n#    b = np.empty_like(a)\r\n#    b[0] = a[1]\r\n#    b[1] = -a[0]\r\n#    return b\r\n#\r\n#\r\n#def np_cross(a, b):\r\n#    """"""Cross product""""""\r\n#    return np.dot(a, np_perp(b))\r\n#\r\n#def knn(points, p, k):\r\n#    """"""\r\n#    Calculates k nearest neighbours for a given point.\r\n#\r\n#    :param points: list of points\r\n#    :param p: reference point\r\n#    :param k: amount of neighbours\r\n#    :return: list\r\n#    """"""\r\n#    return sorted(points, key=lambda x: math.sqrt((x[0] - p[0]) ** 2 + (x[1] - p[1]) ** 2))[0:k]\r\n#\r\n#\r\n#def np_seg_intersect(a, b, co_check = False):\r\n#    """"""Line segment intersections\r\n#    : https://stackoverflow.com/questions/563198/how-do-you-detect-where-\r\n#    :       two-line-segments-intersect/565282#565282\r\n#    : http://www.codeproject.com/Tips/862988/Find-the-intersection-\r\n#    :       point-of-two-line-segments\r\n#    : considerCollinearOverlapAsIntersect => co_check\r\n#    """"""\r\n#    def np_perp(a):\r\n#        """"""perpendicular""""""\r\n#        b = np.empty_like(a)\r\n#        b[0] = a[1]\r\n#        b[1] = -a[0]\r\n#        return b\r\n#\r\n#    def np_cross(a, b):\r\n#        """"""Cross product""""""\r\n#        return np.dot(a, np_perp(b))\r\n#\r\n#    r = a[1] - a[0]\r\n#    s = b[1] - b[0]\r\n#    v = b[0] - a[0]\r\n#    numer = np_cross(v, r)\r\n#    denom = np_cross(r, s)\r\n#    # If r x s = 0 and (q - p) x r = 0, then the two lines are collinear.\r\n#    if np.isclose(denom, 0) and np.isclose(numer, 0):\r\n#        # 1. If either  0 <= (q - p) * r <= r * r or 0 <= (p - q) * s <= * s\r\n#        # then the two lines are overlapping,\r\n#        if(co_check):\r\n#            vDotR = np.dot(v, r)\r\n#            aDotS = np.dot(-v, s)\r\n#            chk0 = (0 <= vDotR and vDotR <= np.dot(r, r))\r\n#            chk1 = (0 <= aDotS and aDotS <= np.dot(s, s))\r\n#            if chk0 or chk1:\r\n#                return True\r\n#        # 2. If neither\r\n#        #    0 <= (q - p) * r = r * r nor 0 <= (p - q) * s <= s * s\r\n#        # then the two lines are collinear but disjoint.\r\n#        # No need to implement this expression, it follows from\r\n#        # the expression above.\r\n#        return None\r\n#    if np.isclose(denom, 0.0) and not np.isclose(numer, 0.0):\r\n#        # Parallel and non intersecting\r\n#        return None\r\n#    u = numer / denom\r\n#    t = np_cross(v, s) / denom\r\n#    if u >= 0 and u <= 1 and t >= 0 and t <= 1:\r\n#        res = b[0] + (s*u)\r\n#        return res\r\n#    # Otherwise, the two line segments are not parallel but do not intersect.\r\n#    return None\r\n#def intersects(p1, p2, p3, p4):\r\n#    """"""\r\n#    Checks if lines p1, p2 and p3, p4 intersect.\r\n#\r\n#    :param p1, p2: line\r\n#    :param p3, p4: line\r\n#    :return: bool\r\n#    """"""\r\n#    p0_x, p0_y = p1\r\n#    p1_x, p1_y = p2\r\n#    p2_x, p2_y = p3\r\n#    p3_x, p3_y = p4\r\n#\r\n#    s10_x = p1_x - p0_x\r\n#    s10_y = p1_y - p0_y\r\n#    s32_x = p3_x - p2_x\r\n#    s32_y = p3_y - p2_y\r\n#\r\n#    denom = s10_x * s32_y - s32_x * s10_y\r\n#    if denom == 0:\r\n#        return False\r\n#\r\n#    denom_positive = denom > 0\r\n#    s02_x = p0_x - p2_x\r\n#    s02_y = p0_y - p2_y\r\n#    s_numer = s10_x * s02_y - s10_y * s02_x\r\n#    if (s_numer < 0) == denom_positive:\r\n#        return False\r\n#\r\n#    t_numer = s32_x * s02_y - s32_y * s02_x\r\n#    if (t_numer < 0) == denom_positive:\r\n#        return False\r\n#\r\n#    if ((s_numer > denom) == denom_positive) or ((t_numer > denom) == denom_positive):\r\n#        return False\r\n#\r\n#    t = t_numer / denom\r\n#    x = p0_x + (t * s10_x)\r\n#    y = p0_y + (t * s10_y)\r\n#\r\n#    if (x, y) in [p1, p2, p3, p4]:\r\n#        return False\r\n#\r\n#    return True\r\n#\r\n#\r\n#def angle(p1, p2, previous_angle=0):\r\n#    """"""\r\n#    Calculates angle between two points and previous angle.\r\n#\r\n#    :param p1: point\r\n#    :param p2: point\r\n#    :param previous_angle: previous angle\r\n#    :return: float\r\n#    """"""\r\n#    return (math.atan2(p1[1] - p2[1], p1[0] - p2[0]) - previous_angle) % (math.pi * 2) - math.pi\r\n#\r\n#\r\n#def point_in_polygon(point, polygon):\r\n#    """"""\r\n#    """"""\r\n#    size = len(polygon)\r\n#    for i in range(size):\r\n#        min_ = min([polygon[i][0], polygon[(i + 1) % size][0]])\r\n#        max_ = max([polygon[i][0], polygon[(i + 1) % size][0]])\r\n#        if min_ < point[0] <= max_:\r\n#            p = polygon[i][1] - polygon[(i + 1) % size][1]\r\n#            q = polygon[i][0] - polygon[(i + 1) % size][0]\r\n#            point_y = (point[0] - polygon[i][0]) * p / q + polygon[i][1]\r\n#            if point_y < point[1]:\r\n#                return True\r\n#    return False\r\n#\r\n#\r\n#def concave(points, k):\r\n#    """"""\r\n#    Calculates the concave hull for given points\r\n#    Input is a list of 2D points [(x, y), ...]\r\n#    k defines the number of of considered neighbours\r\n#\r\n#    :param points: list of points\r\n#    :param k: considered neighbours\r\n#    :return: list\r\n#    """"""\r\n#    k = max(k, 3)  # Make sure k >= 3\r\n#    dataset = list(set(points[:]))  # Remove duplicates\r\n#    if len(dataset) < 3:\r\n#        raise Exception(""Dataset length cannot be smaller than 3"")\r\n#    elif len(dataset) == 3:\r\n#        return dataset  # Points are a polygon already\r\n#    k = min(k, len(dataset) - 1)  # Make sure k neighbours can be found\r\n#\r\n#    first_point = current_point = min(dataset, key=lambda x: x[1])\r\n#    hull = [first_point]  # Initialize hull with first point\r\n#    dataset.remove(first_point)  # Remove first point from dataset\r\n#    previous_angle = 0\r\n#\r\n#    while (current_point != first_point or len(hull) == 1) and len(dataset) > 0:\r\n#        if len(hull) == 3:\r\n#            dataset.append(first_point)  # Add first point again\r\n#        kn_points = knn(dataset, current_point, k)  # Find nearest neighbours\r\n#        c_points = sorted(kn_points, key=lambda x: -angle(x, current_point, previous_angle))\r\n#\r\n#        is_True = True\r\n#        i = -1\r\n#        while is_True and i < len(c_points) - 1:\r\n#            i += 1\r\n#            last_point = 1 if c_points[i] == first_point else 0\r\n#            j = 1\r\n#            is_True = False\r\n#            while not is_True and j < len(hull) - last_point:\r\n#                is_True = intersects(hull[-1], c_points[i], hull[-j - 1], hull[-j])\r\n#                j += 1\r\n#        if is_True:  # All points intersect, try again with higher number of neighbours\r\n#            return concave(points, k + 1)\r\n#        previous_angle = angle(c_points[i], current_point)\r\n#        current_point = c_points[i]\r\n#        hull.append(current_point)  # Valid candidate was found\r\n#        dataset.remove(current_point)\r\n#\r\n#    for point in dataset:\r\n#        if not point_in_polygon(point, hull):\r\n#            return concave(points, k + 1)\r\n#\r\n#    return hull\r\n#\r\n\r\n# --------------------------------------------------------------------------\r\n#def cross(o, a, b):\r\n#    """"""\r\n#    Calculates cross product.\r\n#\r\n#    :param o, a: vector\r\n#    :param o, b: vector\r\n#    :return: int\r\n#    """"""\r\n#    return (a[0] - o[0]) * (b[1] - o[1]) - (a[1] - o[1]) * (b[0] - o[0])\r\n#\r\n#\r\n#def convex(points):\r\n#    """"""\r\n#    Calculates the convex hull for given points\r\n#    Input is a list of 2D points [(x, y), ...]\r\n#\r\n#    :param points: list of points\r\n#    :return: list\r\n#    """"""\r\n#    points = sorted(set(points))  # Remove duplicates\r\n#    if len(points) <= 1:\r\n#        return points\r\n#\r\n#    # Build lower hull\r\n#    lower = []\r\n#    for p in points:\r\n#        while len(lower) >= 2 and cross(lower[-2], lower[-1], p) <= 0:\r\n#            lower.pop()\r\n#        lower.append(p)\r\n#\r\n#    # Build upper hull\r\n#    upper = []\r\n#    for p in reversed(points):\r\n#        while len(upper) >= 2 and cross(upper[-2], upper[-1], p) <= 0:\r\n#            upper.pop()\r\n#        upper.append(p)\r\n#\r\n#    return lower[:-1] + upper[:-1]\r\n# ----------------------------------------------------------------------------\r\ndef test():\r\n    """""" """"""\r\n    a = [(207, 184), (393, 60), (197, 158), (197, 114), (128, 261),\r\n          (442, 40), (237, 159), (338, 75), (194, 93), (33, 159),\r\n          (393, 152), (433, 267), (324, 141), (384, 183), (273, 165),\r\n          (250, 257), (423, 198), (227, 68), (120, 184), (214, 49),\r\n          (256, 75), (379, 93), (312, 49), (471, 187), (366, 122)]\r\n    a = np.asarray(a)\r\n    idx_cr = np.lexsort((a[:, 0], a[:, 1]))\r\n    in_pnts = np.asarray([a[i] for i in idx_cr])\r\n    cx = concave(in_pnts, 3)\r\n    return cx\r\n\r\n\r\ndef test_main():\r\n    """""" """"""\r\n    in_fc = r""C:\\Git_Dan\\a_Data\\arcpytools_demo.gdb\\r_sorted""\r\n    out_fc = r""C:\\Git_Dan\\a_Data\\arcpytools_demo.gdb\\r_convex_hull""\r\n    desc = arcpy.da.Describe(in_fc)\r\n    SR = desc[\'spatialReference\']\r\n    flds = [\'SHAPE@X\', \'SHAPE@Y\']\r\n    args = [in_fc, flds, None, None, True, (None, None)]\r\n    cur = arcpy.da.SearchCursor(*args)\r\n    a = _xy(in_fc)                    # ---- get the points\r\n    a = uniq(a, axis=0)               # ---- get the unique points\r\n#    a_s = a[a[:, 1].argsort(axis=0)]  # sort to get lowest y-value\r\n    idx_cr = np.lexsort((a[:, 0], a[:, 1]))\r\n    in_pnts = np.asarray([a[i] for i in idx_cr])  # sort by column 1, then 0\r\n#    output_polylines(out_fc, SR, [pl])  # ***** it works\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n#    from tools import uniq\r\n#    in_fc = r""C:\\Git_Dan\\a_Data\\arcpytools_demo.gdb\\r_sorted""\r\n#    out_fc = r""C:\\Git_Dan\\a_Data\\arcpytools_demo.gdb\\r_convex_hull""\r\n#    desc = arcpy.da.Describe(in_fc)\r\n#    SR = desc[\'spatialReference\']\r\n#    flds = [\'SHAPE@X\', \'SHAPE@Y\']\r\n#    args = [in_fc, flds, None, None, True, (None, None)]\r\n#    cur = arcpy.da.SearchCursor(*args)\r\n#    a = _xy(in_fc)                    # ---- get the points\r\n#    a = uniq(a, axis=0)               # ---- get the unique points\r\n##    a_s = a[a[:, 1].argsort(axis=0)]  # sort to get lowest y-value\r\n#    idx_cr = np.lexsort((a[:, 0], a[:, 1]))\r\n#    in_pnts = np.asarray([a[i] for i in idx_cr])  # sort by column 1, then 0\r\n#    output_polylines(out_fc, SR, [pl])  # ***** it works\r\n\r\n#    in_pnts = [tuple(i) for i in a_s.tolist()]\r\n#    pnts = concave(in_pnts, 3)\r\n#    output_polylines(out_fc, SR, [pnts])\r\n#    ps = [(207, 184), (393, 60), (197, 158), (197, 114), (128, 261),\r\n#          (442, 40), (237, 159), (338, 75), (194, 93), (33, 159),\r\n#          (393, 152), (433, 267), (324, 141), (384, 183), (273, 165),\r\n#          (250, 257), (423, 198), (227, 68), (120, 184), (214, 49),\r\n#          (256, 75), (379, 93), (312, 49), (471, 187), (366, 122)]\r\n#    a = np.array(ps)\r\n#    a_s = a[a[:, 1].argsort(axis=0)]  # sort to get lowest y-value\r\n#    idx_cr = np.lexsort((a[:, 0], a[:, 1]))\r\n#    in_pnts = np.asarray([a[i] for i in idx_cr])  # sort by column 1, then 0\r\n##    cx = concave(in_pnts, 3)\r\n'"
all_scripts/image.py,27,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\nimage\r\n=====\r\n\r\nScript :   image.py\r\n\r\nAuthor :   Dan.Patterson@carleton.ca\r\n\r\nModified : 2019-01-02\r\n\r\nPurpose :  tools for working with numpy arrays as images\r\n\r\nUseage:\r\n\r\nFunctions:\r\n---------_\r\na_filter(a, mode=1, ignore_ndata=True)  # mode is a 3x3 filter\r\n\r\nReferences:\r\n:\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# pylint: disable=C0103\r\n# pylint: disable=R1710\r\n# pylint: disable=R0914\r\n\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport warnings\r\nimport numpy as np\r\nfrom arraytools.tools import stride\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=5, linewidth=80, precision=2,\r\n                    suppress=True, threshold=500,\r\n                    formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\nwarnings.filterwarnings(\'ignore\')\r\n\r\n__all__ = [\'_even_odd\',\r\n           \'_pad_even_odd\', \'_pad_nan\', \'_pad_zero\',\r\n           \'a_filter\',\r\n           \'plot_img\',\r\n           \'rgb_gray\', \'normalize\', \'equalize\']\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# (1) padding arrays\r\ndef _even_odd(a):\r\n    """"""Even/odd from modulus.  Returns 0 for even, 1 for odd""""""\r\n    prod = np.cumprod(a.shape)[0]\r\n    return np.mod(prod, 2)\r\n\r\n\r\ndef _pad_even_odd(a):\r\n    """"""To use when padding a strided array for window construction\r\n    """"""\r\n    p = _even_odd(a)\r\n    ap = np.pad(a, pad_width=(1, p), mode=""constant"", constant_values=(0, 0))\r\n    return ap\r\n\r\n\r\ndef _pad_nan(a, nan_edge=True):\r\n    """"""Pad a sliding array to allow for stats, padding uses np.nan\r\n    : see also: num_to_nan(a, num=None, copy=True)\r\n    """"""\r\n    a = a.astype(\'float64\')\r\n    if nan_edge:\r\n        cv = (np.NaN, np.NaN)\r\n        a = np.pad(a, pad_width=(1, 1), mode=""constant"", constant_values=cv)\r\n    return a\r\n\r\n\r\ndef _pad_zero(a, n=1):\r\n    """"""To use when padding a strided array for window construction. n = number\r\n    : of zeros to pad arround the array\r\n    : see also: nun_to_nan (1.13)\r\n    """"""\r\n    ap = np.pad(a, pad_width=(n, n), mode=""constant"", constant_values=(0, 0))\r\n    return ap\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# (1) filter array ---- convolution filters\r\n#\r\ndef a_filter(a, mode=1, pad_output=True, ignore_nodata=True, nodata=None):\r\n    """"""Various filters applied to an array.\r\n\r\n    Requires:\r\n    --------\r\n    stride : function\r\n        the `stride` function is used internally in this function\r\n    a : array\r\n        an array that will be strided using a 3x3 window\r\n    pad_output : boolean\r\n        True, produces a masked array padded so that the shape\r\n        is the same as the input\r\n    ignore_nodata : boolean\r\n        True, all values used, False, array contains nodata\r\n    nodata : None or number\r\n        None :\r\n            max int or float used\r\n        value :\r\n            use this value in integer or float form otherwise\r\n\r\n\r\n    mode :\r\n        a dictionary containing a choice from\r\n    ::\r\n\r\n        1.  `all_f`    : all 1\'s\r\n        2.  `no_cnt`   : no center\r\n        3.  `cross_f`  : cross filter, corners\r\n        4.  `plus_f`   : up down, left right\r\n        5.  `gradient` : `6 7 8 9 10` directional gradients\r\n        11. `lap_33`   : laplacian\r\n        12. `line_h`   : line detection, horizonal\r\n        13. `line_ld`  : line detection, left diagonal\r\n        14. `line_rd`  : line detection, right diagonal\r\n        15. `line_v`   : line detection, vertical\r\n        16. `high`     :\r\n        17. `sob_hor`  : Sobel horizontal\r\n        18. `sob_vert` : Sobel vertical\r\n        19. `emboss`   :\r\n        20. `sharp1`   : sharpen 1\r\n        22. `sharp2`   : sharpen 2\r\n        23. `sharp3`   : sharpen 3 highpass 3x3\r\n        24. `lowpass`  : lowpass filter\r\n\r\n    Notes:\r\n    -----\r\n        Only 3x3 filters covered here.  The output array is padded with np.nan\r\n        and the array is returned as a masked array.\r\n\r\n    >>> a0 = pyramid(core=4, steps=5, incr=(1, 1))\r\n    >>> a0 = a0 * 2  # multiply by a number to increase slope\r\n    >>> a0 = (pyramid(core=4, steps=5, incr=(1, 1)) + 1) * 2  # is also good!\r\n\r\n    References:\r\n    ----------\r\n    ..\r\n    [1]\r\n    http://desktop.arcgis.com/en/arcmap/latest/tools/spatial-analyst-toolbox/how-filter-works.htm\r\n\r\n    [2]\r\n    http://desktop.arcgis.com/en/arcmap/latest/manage-data/raster-and-images/convolution-function.htm\r\n\r\n    [3]\r\n    https://github.com/scikit-image/scikit-image/tree/master/skimage/filters\r\n    """"""\r\n    n = np.nan\r\n    all_f = [1, 1, 1, 1, 1, 1, 1, 1, 1]\r\n    no_cnt = [1, 1, 1, 1, n, 1, 1, 1, 1]\r\n    cross_f = [1, 0, 1, 0, 0, 0, 1, 0, 1]\r\n    plus_f = [0, 1, 0, 1, 0, 1, 0, 1, 0]\r\n    grad_e = [1, 0, -1, 2, 0, -2, 1, 0, -1]\r\n    grad_n = [-1, -2, -1, 0, 0, 0, 1, 2, 1]\r\n    grad_ne = [0, -1, -2, 1, 0, -1, 2, 1, 0]\r\n    grad_nw = [2, -1, 0, -1, 0, 1, 0, 1, 2]\r\n    grad_s = [1, 2, 1, 0, 0, 0, -1, -2, -1]\r\n    grad_w = [-1, 0, 1, -2, 0, 2, -1, 0, 1]\r\n    lap_33 = [0, -1, 0, -1, 4, -1, 0, -1, 0]\r\n    line_h = [-1, -1, -1, 2, 2, 2, -1, -1, -1]\r\n    line_ld = [2, -1, -1, -1, 2, -1, -1, -1, 2]\r\n    line_rd = [-1, -1, 2, -1, 2, -1, 2, -1, -1]\r\n    line_v = [-1, 0, -1, -1, 2, -1, -1, 2, -1]\r\n    high = [-0.7, -1.0, -0.7, -1.0, 6.8, -1.0, -0.7, -1.0, -0.7]  # arc\r\n    sob_hor = [1, 2, 1, 0, 0, 0, -1, -2, -1]   # sobel y  /4.0 weights\r\n    sob_vert = [1, 0, -1, 2, 0, -2, 1, 0, -1]  # sobel x  /4\r\n    emboss = [-1, -1, 0, -1, 0, 1, 0, 1, 1]\r\n    sharp1 = [0., -0.25, 0., -0.25, 2.0, -0.25, 0., -0.25, 0.]  # arc\r\n    sharp2 = [-0.25, -0.25, -0.25, -0.25, 3.0, -0.25, -0.25, -0.25, -0.25]\r\n    sharp3 = [-1, -1, -1, -1, 9, -1, -1, -1, -1]  # arc\r\n    lowpass = [1, 2, 1, 2, 4, 2, 1, 2, 1]  # arc\r\n    # ---- assemble the dictionary ----\r\n    d = {1: all_f, 2: no_cnt, 3: cross_f, 4: plus_f, 5: grad_e,\r\n         6: grad_n, 7: grad_ne, 8: grad_nw, 9: grad_s, 10: grad_w,\r\n         11: lap_33, 12: line_h, 13: line_ld, 14: line_rd, 15: line_v,\r\n         16: high, 17: sob_hor, 18: sob_vert, 19: emboss, 20: sharp1,\r\n         21: sharp2, 23: sharp3, 24: lowpass}\r\n    filter_ = np.array(d[mode]).reshape(3, 3)\r\n    # ---- stride the input array ----\r\n    a_strided = stride(a)\r\n    if ignore_nodata:\r\n        c = np.sum(a_strided * filter_, axis=(2, 3))\r\n    else:\r\n        c = np.nansum(a_strided * filter_, axis=(2, 3))\r\n    if pad_output:\r\n        pad_ = nodata\r\n        if nodata is None:\r\n            if c.dtype.name in (\'int\', \'int32\', \'int64\'):\r\n                pad_ = min([0, -1, a.min()-1])\r\n            else:\r\n                pad_ = min([0.0, -1.0, a.min()-1])\r\n        c = np.lib.pad(c, (1, 1), ""constant"", constant_values=(pad_, pad_))\r\n        m = np.where(c == pad_, 1, 0)\r\n        c = np.ma.array(c, mask=m, fill_value=None)\r\n    return c\r\n\r\n\r\ndef plot_img(img):\r\n    """"""plot image as gray scale""""""\r\n    import matplotlib.pyplot as plt\r\n    plt.imshow(img, cmap=plt.get_cmap(\'gray\'))\r\n#    plt.show()\r\n\r\n\r\ndef rgb_gray(a):\r\n    """"""Convert 3d array to 2d gray scale""""""\r\n    shp = a.shape\r\n    if shp[2] == 3:\r\n        r, g, b = a[:, :, 0], a[:, :, 1], a[:, :, 2]\r\n    else:\r\n        r, g, b = a\r\n    gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\r\n    return gray\r\n\r\n\r\ndef normalize(a):\r\n    """"""Normalize array to unsigned integer in range 0-255""""""\r\n    lmin = a.min()\r\n    lmax = a.max()\r\n    nor = np.floor((a - lmin) / (lmax - lmin)*255.)\r\n    nor = nor.astype(\'uint8\')\r\n    return nor\r\n\r\n\r\ndef equalize(a):\r\n    """"""Produce a raster equalization for a 2D array.\r\n    :Notes:\r\n    :------\r\n    : - derive the histogram, produce the ogive, then normalize it.\r\n    https://en.wikipedia.org/wiki/Histogram_equalization\r\n    """"""\r\n    hist, _ = np.histogram(a.flatten(), bins=256, range=[0, 256])\r\n    cdf = hist.cumsum()\r\n    cdf_m = np.ma.masked_equal(cdf, 0)\r\n    cdf_m = (cdf_m - cdf_m.min())*255 / (cdf_m.max() - cdf_m.min())\r\n    cdf = np.ma.filled(cdf_m, 0).astype(\'uint8\')\r\n    img2 = cdf[a]\r\n    return img2\r\n\r\n\r\ndef _demo():\r\n    """"""\r\n    :------------------------------------------------------------------\r\n    """"""\r\n    rows, cols = (4, 4)\r\n    a = np.arange(rows*cols, dtype=\'int32\').reshape(rows, cols)\r\n    x, y = (4, 4)\r\n    a = np.tile(a.repeat(x), y)\r\n    a = np.hsplit(a, y)         # split into y parts horizontally\r\n    a = np.vstack(a)\r\n    a = np.hsplit(a, a.shape[0])\r\n    a = np.vstack(a).copy(order=\'C\')\r\n    print(""array \'a\'\\n{}"".format(a))\r\n    print(""equalized \'a\'\\n{}"".format(equalize(a)))\r\n    return a\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n#    a = _demo()\r\n'"
all_scripts/line_ang_azim.py,10,"b'# -*- coding: UTF-8 -*-\n""""""\nline_ang_azim\n=============\n\nScript :  line_ang_azim.py\n\nAuthor :  Dan.Patterson@carleton.ca\n\nModified : 2018-04-06\n\nPurpose :\n\nFunctions:\n----------\n    help(<function name>) for help\n\n_demo  -  This function ...\n\nNotes:\n------\n- see help topic: np.info(np.arctan2)\n- np.arctan2(dy, dx) is the format which differs from excel\n- dx, dy - the differences in the respective coordinates x and y\n- 360 = 2*np.pi, aka the circle in radians\n\nResults:\n--------\n::\n\n  ---------------------------------------x-axis  compass azim\n  orig: [0, 0]: dest: [-1, 1]  line_dir:  135.0   NW    315\n  orig: [0, 0]: dest: [0, 1]   line_dir:   90.0   N       0 or 360\n  orig: [0, 0]: dest: [1, 1]   line_dir:   45.0   NE     45\n  orig: [0, 0]: dest: [1, 0]   line_dir:    0.0   E      90\n  orig: [0, 0]: dest: [1, -1]  line_dir:  -45.0   SE    135\n  orig: [0, 0]: dest: [0, -1]  line_dir:  -90.0   S     180\n  orig: [0, 0]: dest: [-1, -1] line_dir: -135.0   SW    225\n  orig: [0, 0]: dest: [-1, 0]  line_dir:  180.0   W     270``\n\nReferences:\n\n""""""\n# ---- imports, formats, constants ----\n\nimport sys\nimport numpy as np\n\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\n      \'float\': \'{: 0.3f}\'.format}\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2,\n                    suppress=True, threshold=100,\n                    formatter=ft)\nnp.ma.masked_print_option.set_display(\'-\')\n\nscript = sys.argv[0]\n\n# ---- functions ----\n\n\ndef line_dir(orig, dest, fromNorth=False):\n    """"""Direction of a line given 2 points\n\n    `orig`, `dest` : point coordinates\n        Two points representing the start and end of a line.\n    `fromNorth` : boolean\n        True or False gives angle relative to x-axis)\n    """"""\n    orig = np.atleast_2d(orig)\n    dest = np.atleast_2d(dest)\n    dxy = dest - orig\n    ang = np.degrees(np.arctan2(dxy[:, 1], dxy[:, 0]))\n    if fromNorth:\n        ang = np.mod((450.0 - ang), 360.)\n    return ang\n\n\ndef _demo(xc=0, yc=0, fromNorth=True):\n    """""" run the demo with the data below """"""\n    p0 = np.array([xc, yc])  # origin point\n    p1 = p0 + [-1, 1]   # NW\n    p2 = p0 + [0, 1]    # N\n    p3 = p0 + [1, 1]    # NE\n    p4 = p0 + [1, 0]    # E\n    p5 = p0 + [1, -1]   # SE\n    p6 = p0 + [0, -1]   # S\n    p7 = p0 + [-1, -1]  # SW\n    p8 = p0 + [-1, 0]   # W\n    #\n    od = [[p0, p1], [p0, p2], [p0, p3], [p0, p4],\n          [p0, p5], [p0, p6], [p0, p7], [p0, p8]]\n    for pair in od:\n        orig, dest = pair\n        ang = line_dir(orig, dest, fromNorth=fromNorth)\n        if fromNorth:\n            dir = ""From N.""\n        else:\n            dir = ""From x-axis""\n        args = [orig, dest, dir, ang]\n        print(""orig: {}: dest: {!s:<8} {}: {!s:>6}"".format(*args))\n    return od\n\n\n# ---------------------------------------------------------------------\nif __name__ == ""__main__"":\n    """"""Main section...   """"""\n#    print(""Script... {}"".format(script))\n    xc = 0  # 300000   # pick an origin x  0 or 300000 for example\n    yc = 0  # 5025000  # pick an origin y  0 or 5025000\n    od = _demo(xc, yc, fromNorth=True)\n'"
all_scripts/mesh_pnts.py,8,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\nmesh_pnts\r\n=========\r\n\r\n:Script :   mesh_pnts.py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified : 2017-03-11\r\n\r\nPurpose :  Just makes points on a grid as well as the meshgrid\r\n\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nfrom textwrap import dedent\r\n\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\n\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\ndef mesh_xy(L=0, B=0, R=5, T=5, dx=1, dy=1, as_rec=True):\r\n    """"""Create a mesh of coordinates within the specified X, Y ranges\r\n\r\n    Requires:\r\n    --------\r\n    L(eft), R(ight), dx : number\r\n        coordinate min, max and delta x for X axis\r\n    B(ott), T(op), dy  : number\r\n        same as above for Y axis\r\n    as_rec : boolean\r\n        Produce a structured array (or convert to a record array)\r\n\r\n    Returns:\r\n    -------\r\n    -  A list of coordinates of X,Y pairs and an ID if as_rec is True.\r\n    -  A mesh grid X and Y coordinates is also produced.\r\n    :-------------\r\n    """"""\r\n    dt = [(\'Pnt_num\', \'<i4\'), (\'X\', \'<f8\'), (\'Y\', \'<f8\')]\r\n    x = np.arange(L, R + dx, dx, dtype=\'float64\')\r\n    y = np.arange(B, T + dy, dy, dtype=\'float64\')\r\n    mesh = np.meshgrid(x, y, sparse=False)\r\n    if as_rec:\r\n        xs = mesh[0].ravel()\r\n        ys = mesh[1].ravel()\r\n        p = list(zip(np.arange(len(xs)), xs, ys))\r\n        pnts = np.array(p, dtype=dt)\r\n    else:\r\n        p = list(zip(mesh[0].ravel(), mesh[1].ravel()))\r\n        pnts = np.array(p)\r\n    return pnts, mesh\r\n\r\n\r\ndef _demo():\r\n    """"""A set of points and mesh using real world projected coordinates""""""\r\n    args = [300000, 5025000, 301000, 5026000, 250., 250., True]\r\n    L, B, R, T, dx, dy, as_rec = args\r\n    pnts, mesh = mesh_xy(L, B, R, T, dx, dy, as_rec)\r\n    frmt = """"""\\n\r\n    :Points...\r\n    {!r:}\\n\r\n    :Mesh...(Xs)\r\n    {!r:}\\n\r\n    :.......(Ys)\r\n    {!r:}\r\n    """"""\r\n    print(dedent(frmt).format(pnts, mesh[0], mesh[1]))\r\n#    return pnts, mesh\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\n\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n#    pnts, mesh = _demo()\r\n'"
all_scripts/movepnts.py,1,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:  movepnts.py\r\n:Author:  Dan.Patterson@carleton.ca\r\n:Modified: 2017-04-06\r\n:Notes:\r\n:- arcpy.da.FeatureClassToNumPyArray(in_table, field_names, {where_clause},\r\n:                                    {spatial_reference}, {explode_to_points},\r\n:                                    {skip_nulls}, {null_value})\r\n:- arcpy.da.NumPyArrayToFeatureClass(in_array, out_table, shape_fields,\r\n:                                    {spatial_reference})\r\n:- create multipart polygons: https://geonet.esri.com/message/461451\r\n: our house relative to 0,0 in MTM9\r\n: xy_shift = [341886,5023462]\r\n:\r\n:Spatial reference\r\n: NAD_1983_CSRS_MTM_9\r\n: WKID: 2951 Authority: EPSG\r\n: in_fc = r\'C:\\GIS\\Table_tools\\Table_tools.gdb\\polygon_demo\'\r\n: dx = 2\r\n: dy = 2\r\n: out_fc = r\'C:\\GIS\\Table_tools\\Table_tools.gdb\\bb\'\r\n""""""\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\nfrom arcpytools import fc_info, tweet\r\n\r\narcpy.env.overwriteOutput = True\r\n\r\n\r\n# ---- input parameters ----\r\nin_fc = sys.argv[1]\r\ndx = float(sys.argv[2])\r\ndy = float(sys.argv[3])\r\nout_fc = sys.argv[4]\r\nxy_shift = np.array([dx, dy], dtype=""<f8"")\r\nshp_field, OIDField, shp_type, SR = fc_info(in_fc)\r\n# ---- convert to array, shift and return ----\r\n# Apparently, there can be problems writing directly to a featureclass\r\n# so, write to in_memory changing the required field names, then copy out\r\narr = arcpy.da.FeatureClassToNumPyArray(in_fc, ""*"", """", SR, True)\r\narr[shp_field] = arr[shp_field] + xy_shift\r\nnms = [\'Feat_id\', \'XYs\'] + [i for i in arr.dtype.names[2:]]\r\narr.dtype.names = nms\r\ntemp_out = ""in_memory/temp2""\r\narcpy.da.NumPyArrayToFeatureClass(arr, temp_out, [\'XYs\'])\r\narcpy.CopyFeatures_management(temp_out, out_fc)\r\ndel temp_out\r\n# ---- the end ----\r\n'"
all_scripts/mst.py,31,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\nmst\r\n===\r\n\r\nScript:   mst.py\r\n\r\nAuthor:   Dan.Patterson@carleton.ca\r\n\r\nModified: 2018-04-09\r\n\r\nPurpose:\r\n--------\r\n  Produce a spanning tree from a point set.  I have yet to confirm\r\n  whether it constitutes a minimum spanning tree, since the implementation\r\n  doesn\'t specify whether Prim\'s algorithm is being used (see ref. 2)\r\n\r\nReferences:\r\n-----------\r\n[1]\r\nhttp://stackoverflow.com/questions/41903502/sort-two-dimensional-list-python\r\n\r\n[2]\r\nhttp://peekaboo-vision.blogspot.ca/2012/02/simplistic-minimum-spanning-\r\ntree-in.html\r\n\r\nalso referenced here...\r\n\r\n[3]\r\nhttp://stackoverflow.com/questions/34374839/minimum-spanning-tree-distance-\r\nand-graph\r\n\r\nNotes:\r\n-----\r\n\r\n>>> a\r\narray([[ 0,  0],  # constructed for minimum spanning tree example\r\n       [ 0,  8],\r\n       [10,  8],\r\n       [10,  0],\r\n       [ 3,  4],\r\n       [ 7,  4]])\r\n\r\n(1) sorting :\r\n  >>> np.lexsort((a[:,1], a[:,0])) sort by x, then y\r\n  >>> np.lexsort(a.T) >= np.lexsort((a[:,0], a[:,1])) sort y, x\r\n\r\n(2) Distances :\r\n\r\n- unsorted....\r\n>>> np.linalg.norm(a[1:] - a[:-1], axis=1)\r\narray([ 8.0,  10.0,  8.0,  8.1,  4.0])\r\n>>> np.sum(np.linalg.norm(a[1:] - a[:-1], axis=1)) => 38.0622...\r\n\r\n- sorted....\r\n>>> a_srt = a[np.lexsort(a.T),:]\r\n>>>   np.linalg.norm(a_srt[1:] - a_srt[:-1], axis=1)\r\narray([ 8.0,  5.0,  4.0,  5.0,  8.0])\r\n>>> np.sum(np.linalg.norm(a_srt[1:] - a_srt[:-1], axis=1)) => 30.0...\r\n\r\n(3) Near results...\r\n::\r\n  coords, dist, n_array = n_near(s, N=2)\r\n  ie   ID     Xo    Yo  C0_x C0_y   C1_x C1_y   Dist0 Dist1\r\n     ([(0,  0.0, 0.0,  3.0, 4.0,   0.0, 8.0,  5.0,  8.0),\r\n       (1,  0.0, 8.0,  3.0, 4.0,   0.0, 0.0,  5.0,  8.0),\r\n       (2,  3.0, 4.0,  7.0, 4.0,   0.0, 0.0,  4.0,  5.0),\r\n       (3,  7.0, 4.0,  3.0, 4.0,  10.0, 8.0,  4.0,  5.0),\r\n       (4, 10.0, 8.0,  7.0, 4.0,  10.0, 0.0,  5.0,  8.0),\r\n       (5, 10.0, 0.0,  7.0, 4.0,  10.0, 8.0,  5.0,  8.0)],\r\n      dtype=[(\'ID\', \'<i4\'),\r\n             (\'Xo\', \'<f8\'), (\'Yo\', \'<f8\'),\r\n             (\'C0_X\', \'<f8\'), (\'C0_Y\', \'<f8\'),\r\n             (\'C1_X\', \'<f8\'), (\'C1_Y\', \'<f8\'),\r\n             (\'Dist0\', \'<f8\'), (\'Dist1\', \'<f8\')])\r\n(4) Connections\r\n\r\n>>> o_d\r\narray([(0, 2, 5.0),\r\n       (2, 3, 4.0),\r\n       (2, 1, 5.0),\r\n       (3, 4, 5.0),\r\n       (3, 5, 5.0)],\r\n       dtype=[(\'Orig\', \'<i4\'), (\'Dest\', \'<i4\'), (\'Dist\', \'<f8\')])\r\n\r\n::\r\n\r\n  a[o_d[\'Orig\']]     a[o_d[\'Dest\']]\r\n  array([[ 0,  0],   array([[10,  8],\r\n         [10,  8],          [10,  0],\r\n         [10,  8],          [ 0,  8],\r\n         [10,  0],          [ 3,  4],\r\n         [10,  0]])         [ 7,  4]])\r\n\r\n(4) distance array\r\n\r\n>>> d\r\narray([[ 5.0,  8.0,  8.1,  10.0,  12.8],\r\n       [ 5.0,  8.0,  8.1,  10.0,  12.8],\r\n       [ 4.0,  5.0,  5.0,   8.1,   8.1],\r\n       [ 4.0,  5.0,  5.0,   8.1,   8.1],\r\n       [ 5.0,  8.0,  8.1,  10.0,  12.8],\r\n       [ 5.0,  8.0,  8.1,  10.0,  12.8]])\r\n\r\nBack to the original distance and sorted array, a_srt.\r\n  The distances are determined using the sorted points, the diagonal\r\n  distances are set to np.inf so that they have the maximal distance.\r\n  The distance values can be sorted to get their indices in the array\r\n  Then the array can be sliced to retrieve the points coordinates and the\r\n  distance array can be sliced to get the distances.\r\n\r\n>>> dix = np.arange(d.shape[0])\r\n>>> d[dix, dix] = np.inf\r\n\r\n- distance array, \'d\'\r\n\r\n>>> d\r\narray([[ inf,  8.0,  5.0,  8.1,  10.0,  12.8],\r\n       [ 8.0,  inf,  5.0,  8.1,  12.8,  10.0],\r\n       [ 5.0,  5.0,  inf,  4.0,  8.1,  8.1],\r\n       [ 8.1,  8.1,  4.0,  inf,  5.0,  5.0],\r\n       [ 10.0,  12.8,  8.1,  5.0,  inf,  8.0],\r\n       [ 12.8,  10.0,  8.1,  5.0,  8.0,  inf]])\r\n\r\n>>> np.argsort(d[0])  #=> array([2, 1, 3, 4, 5, 0])\r\n>>> a_srt[np.argsort(d[0])]\r\narray([[3, 4], [ 0, 8], [7, 4], [10, 0], [10, 8], [0, 0]])\r\n>>> d[0][np.argsort(d[0])]  # => array([ 5.0, 8.0, 8.1, 10.0, 12.8, inf])\r\n\r\n: ---------------------------------------------------------------------:\r\n""""""\r\n# pylint: disable=C0103\r\n# pylint: disable=R1710\r\n# pylint: disable=R0914\r\n\r\n#---- imports, formats, constants ----\r\n#\r\n\r\nimport sys\r\nfrom textwrap import dedent, indent\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\r\n      \'float\': \'{: 0.1f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=100, precision=2,\r\n                    suppress=True, threshold=120,\r\n                    formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')\r\n\r\nscript = sys.argv[0]\r\n\r\n# ---- functions ----\r\n\r\n\r\ndef dist_arr(a):\r\n    """"""Minimum spanning tree prep... see main header\r\n    : paths from given data set...\r\n    """"""\r\n    idx = np.lexsort((a[:, 1], a[:, 0]))  # sort X, then Y\r\n    # idx= np.lexsort((a[:, 0], a[:, 1]))  # sort Y, then X\r\n    a_srt = a[idx, :]\r\n    d = _e_dist(a_srt)\r\n    frmt = """"""\\n    {}\\n    :Input array...\\n    {}\\n\\n    :Sorted array...\r\n    {}\\n\\n    :Distance...\\n    {}\r\n    """"""\r\n    args = [dist_arr.__doc__, a, a_srt, d]  # d.astype(\'int\')]\r\n    print(dedent(frmt).format(*args))\r\n    return idx, a_srt, d\r\n\r\n\r\ndef _e_dist(a):\r\n    """"""Return a 2D square-form euclidean distance matrix.  For other\r\n    :  dimensions, use e_dist in ein_geom.py""""""\r\n    b = a.reshape(np.prod(a.shape[:-1]), 1, a.shape[-1])\r\n    diff = a - b\r\n    d = np.sqrt(np.einsum(\'ijk,ijk->ij\', diff, diff)).squeeze()\r\n    # d = np.triu(d)\r\n    return d\r\n\r\n\r\ndef mst(W, copy_W=True):\r\n    """"""Determine the minimum spanning tree for a set of points represented\r\n    :  by their inter-point distances... ie their \'W\'eights\r\n    :Requires:\r\n    :--------\r\n    :  W - edge weights (distance, time) for a set of points. W needs to be\r\n    :      a square array or a np.triu perhaps\r\n    :Returns:\r\n    :-------\r\n    :  pairs - the pair of nodes that form the edges\r\n    """"""\r\n    if copy_W:\r\n        W = W.copy()\r\n    if W.shape[0] != W.shape[1]:\r\n        raise ValueError(""W needs to be square matrix of edge weights"")\r\n    Np = W.shape[0]\r\n    pairs = []\r\n    pnts_seen = [0]  # Add the first point\r\n    n_seen = 1\r\n    # exclude self connections by assigning inf to the diagonal\r\n    diag = np.arange(Np)\r\n    W[diag, diag] = np.inf\r\n    #\r\n    while n_seen != Np:\r\n        new_edge = np.argmin(W[pnts_seen], axis=None)\r\n        new_edge = divmod(new_edge, Np)\r\n        new_edge = [pnts_seen[new_edge[0]], new_edge[1]]\r\n        pairs.append(new_edge)\r\n        pnts_seen.append(new_edge[1])\r\n        W[pnts_seen, new_edge[1]] = np.inf\r\n        W[new_edge[1], pnts_seen] = np.inf\r\n        n_seen += 1\r\n    return np.vstack(pairs)\r\n\r\n\r\ndef plot_mst(a, pairs):\r\n    """"""plot minimum spanning tree test """"""\r\n    plt.scatter(a[:, 0], a[:, 1])\r\n    ax = plt.axes()\r\n    ax.set_aspect(\'equal\')\r\n    for pair in pairs:\r\n        i, j = pair\r\n        plt.plot([a[i, 0], a[j, 0]], [a[i, 1], a[j, 1]], c=\'r\')\r\n    lbl = np.arange(len(a))\r\n    for label, xpt, ypt in zip(lbl, a[:, 0], a[:, 1]):\r\n        plt.annotate(label, xy=(xpt, ypt), xytext=(2, 2), size=8,\r\n                     textcoords=\'offset points\',\r\n                     ha=\'left\', va=\'bottom\')\r\n    plt.show()\r\n    # plt.close()\r\n\r\n\r\ndef connect(a, dist_arr, edges):\r\n    """"""Return the full spanning tree, with points, connections and distance\r\n    : a - point array\r\n    : dist - distance array, from _e_dist\r\n    : edge - edges, from mst\r\n    """"""\r\n    p_f = edges[:, 0]\r\n    p_t = edges[:, 1]\r\n    d = dist_arr[p_f, p_t]\r\n    n = p_f.shape[0]\r\n    dt = [(\'Orig\', \'<i4\'), (\'Dest\', \'i4\'), (\'Dist\', \'<f8\')]\r\n    out = np.zeros((n,), dtype=dt)\r\n    out[\'Orig\'] = p_f\r\n    out[\'Dest\'] = p_t\r\n    out[\'Dist\'] = d\r\n    return out\r\n\r\n\r\n# ---------------------------------------------------------------------\r\nif __name__ == ""__main__"":\r\n    """"""Main section...   """"""\r\n    # print(""Script... {}"".format(script))\r\n    #    a = np.random.randint(1, 10, size=(10,2))\r\n#    a = np.array([[0, 0], [0, 8], [10, 8], [10, 0], [3, 4], [7, 4]])\r\n#    idx, a_srt, d = dist_arr(a)     # return distance array and sorted pnts\r\n#    pairs = mst(d)                  # the orig-dest pairs for the mst\r\n#    plot_mst(a_srt, pairs)          # uncomment to plot\r\n#    o_d = connect(a_srt, d, pairs)  # produce an o-d structured array\r\n'"
all_scripts/n_spaced.py,20,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\nn_spaced\r\n========\r\n\r\nScript :   n_spaced.py\r\n\r\nAuthor :   Dan.Patterson@carleton.ca\r\n\r\nModified: 2018-04-09\r\n\r\nPurpose:\r\n--------\r\n  Produce a point set whose interpoint spacing is no closer than a specified\r\n  distance within a specified bounds.\r\n\r\nReferences:\r\n-----------\r\n`<http://stackoverflow.com/questions/6835531/sorting-a-python-array-\r\nrecarray-by-column>`_.\r\n\r\n""""""\r\n# pylint: disable=C0103  # invalid-name\r\n# pylint: disable=R0914  # Too many local variables\r\n# pylint: disable=R1710  # inconsistent-return-statements\r\n# pylint: disable=W0105  # string statement has no effect\r\n\r\n# ---- imports, formats, constants ------------------------------------------\r\n\r\nimport sys\r\nfrom textwrap import dedent\r\nimport numpy as np\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\r\n      \'float\': \'{: 0.1f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2,\r\n                    suppress=True, threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')\r\n\r\nscript = sys.argv[0]\r\n\r\n# ---- functions ------------------------------------------------------------\r\n#\r\ndef not_closer(a, min_d=1, ordered=False):\r\n    """"""Find the points that are separated by a distance greater than\r\n    min_d.  This ensures a degree of point spacing\r\n\r\n    Parameters:\r\n    --------\r\n    a : array\r\n      2D array of coordinates.\r\n    min_d : number\r\n      minimum separation distance\r\n    ordered : boolean\r\n      order the input points\r\n\r\n    Returns:\r\n    -------\r\n    - b : points where the spacing condition is met\r\n    - c : the boolean array indicating which of the input points were valid.\r\n    - d : the distance matrix\r\n    """"""\r\n    if ordered:\r\n        a = a[np.argsort(a[:, 0])]\r\n    b = a.reshape(np.prod(a.shape[:-1]), 1, a.shape[-1])\r\n    diff = b - a\r\n    d = np.einsum(\'ijk,ijk->ij\', diff, diff)\r\n    d = np.sqrt(d).squeeze()\r\n    c = ~(np.triu(d <= min_d, 1)).any(0)\r\n    b = a[c]\r\n    return b, c, d\r\n\r\n\r\ndef n_spaced(L=0, B=0, R=10, T=10, min_space=1, num=10, verbose=True):\r\n    """"""Produce num points within the bounds specified by the extent (L,B,R,T)\r\n\r\n    Parameters:\r\n    ---------\r\n\r\n    L(eft), B, R, T(op) : numbers\r\n      extent coordinates\r\n    min_space : number\r\n      minimum spacing between points.\r\n    num : number\r\n      number of points... this value may not be reached if the extent\r\n      is too small and the spacing is large relative to it.\r\n    """"""\r\n    #\r\n    def _pnts(L, B, R, T, num):\r\n        """"""Create the points""""""\r\n        xs = (R-L) * np.random.random_sample(size=num) + L\r\n        ys = (T-B) * np.random.random_sample(size=num) + B\r\n        return np.array(list(zip(xs, ys)))\r\n\r\n    def _not_closer(a, min_space=1):\r\n        """"""Find the points that are greater than min_space in the extent.""""""\r\n        b = a.reshape(np.prod(a.shape[:-1]), 1, a.shape[-1])\r\n        diff = b - a\r\n        dist = np.einsum(\'ijk,ijk->ij\', diff, diff)\r\n        dist_arr = np.sqrt(dist).squeeze()\r\n        case = ~(np.triu(dist_arr <= min_space, 1)).any(0)\r\n        return a[case]\r\n    #\r\n    cnt = 1\r\n    n = num * 2  # check double the number required as a check\r\n    result = 0\r\n    frmt = ""Examined: {}  Found: {}  Need: {}""\r\n    a0 = []\r\n    while (result < num) and (cnt < 6):  # keep using random points\r\n        a = _pnts(L, B, R, T, num)\r\n        if cnt > 1:\r\n            a = np.vstack((a0, a))\r\n        a0 = _not_closer(a, min_space)\r\n        result = len(a0)\r\n        if verbose:\r\n            print(dedent(frmt).format(n, result, num))\r\n        cnt += 1\r\n        n += n\r\n    # perform the final sample and calculation\r\n    use = min(num, result)\r\n    a0 = a0[:use]  # could use a0 = np.random.shuffle(a0)[:num]\r\n    a0 = a0[np.argsort(a0[:, 0])]\r\n    return a0\r\n\r\n\r\ndef _demo():\r\n    """""" """"""\r\n    # L, R, B, T = [300000, 300100, 5025000, 5025100]\r\n    L, B, R, T = [1, 1, 10, 10]\r\n    tol = 1\r\n    N = 10\r\n    a = n_spaced(L, B, R, T, tol, num=N, verbose=True)\r\n    return a\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    """""" run the demos, comment out what you don\'t want""""""\r\n    # print(""Script... {}"".format(script))\r\n#    a = np.array([[0, 0], [0, 2], [2, 2], [2, 0]], dtype=\'float64\')\r\n#    b = _demo()\r\n\r\n# z = np.zeros((3,), dtype=[(\'A\', \'int\', (2,)), (\'B\', \'float\')])\r\n# z[""A""] = np.arange(6).reshape(3,2)\r\n'"
all_scripts/ndset.py,27,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nndset\r\n=====\r\n\r\nScript :   ndset.py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified : 2018-12-10\r\n\r\nPurpose\r\n-------\r\nThis set of functions is largely directed to extending some of numpy set\r\nfunctions to apply to Nxd shaped arrays as well as structured and recarrays.\r\nThe functionality largely depends on using a `view` of the input array so that\r\neach row can be treated as a unique record in the array.\r\n\r\nIf you are working with arrays and wish to perform functions on certain columns\r\nthen you will have to preprocess/preselect.  You can only add so much to a\r\nfunction before it loses its readability and utility.\r\n\r\n**ndset**\r\n::\r\n  _view_as_, is_in, nd_diff, nd_diffxor, nd_intersect, nd_union, nd_uniq\r\n\r\nNotes:\r\n------\r\n_view_as_(a)\r\n    >>> a = np.array([[  0,   0], [  0, 100], [100, 100]])\r\n    >>> _view_as_(a)\r\n    ... array([[(  0,   0)],\r\n    ...        [(  0, 100)],\r\n    ...        [(100, 100)]], dtype=[(\'f0\', \'<i4\'), (\'f1\', \'<i4\')])\r\n\r\nis_in\r\n    >>> a = np.array([[  0,   0], [  0, 100], [100, 100]])\r\n    >>> look_for = np.array([[  0, 100], [100, 100]])\r\n    >>> is_in(a, look_for, reverse=False)\r\n    array([[  0, 100],\r\n    ...    [100, 100]])\r\n    >>> is_in(a, look_for, reverse=True)\r\n    array([[0, 0]])\r\n\r\nFor the following:\r\n    >>> a = np.array([[  0,   0], [  0, 100], [100, 100]])\r\n    >>> b = np.array([[  0, 100], [100, 100]])\r\n    >>> c = np.array([[ 20, 20], [100, 20], [100, 0], [ 0, 0]])\r\n\r\nnd_diff(a, b)\r\n    >>> nd_diff(a, b)\r\n    array([[0, 0]])\r\n\r\nnd_diffxor(a, b, uni=False)\r\n    >>> nd_diffxor(a, c, uni=False)\r\n    array([[  0, 100],\r\n           [ 20,  20],\r\n           [100,   0],\r\n           [100,  20],\r\n           [100, 100]])\r\n\r\nnd_intersect(a, b, invert=False)\r\n    >>> nd_intersect(a, b, invert=False)\r\n    array([[  0, 100],\r\n           [100, 100]])\r\n    >>> nd_intersect(a, c, invert=False)\r\n    array([[0, 0]])\r\n\r\nnd_union(a, b)\r\n    >>> nd_union(a, c)\r\n    array([[  0,   0],\r\n           [  0, 100],\r\n           [ 20,  20],\r\n           [100,   0],\r\n           [100,  20],\r\n           [100, 100]])\r\n\r\nnd_uniq(a, counts=False)\r\n    >>> d = np.array([[ 0, 0], [100, 100], [100, 100], [ 0, 0]])\r\n    nd_uniq(d)\r\n    array([[  0,   0],\r\n           [100, 100]])\r\n\r\nReferences:\r\n-----------\r\n`<https://community.esri.com/blogs/dan_patterson/2016/10/23/numpy-lessons-5-\r\nidentical-duplicate-unique-different>`_.\r\n\r\n`<https://github.com/numpy/numpy/blob/master/numpy/lib/arraysetops.py>`_.\r\n\r\n""""""\r\n# pylint: disable=C0103\r\n# pylint: disable=R1710\r\n# pylint: disable=R0914\r\n\r\nimport sys\r\nimport numpy as np\r\n\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\n__all__ = [\'_view_as_\',\r\n           \'_check_dtype_\',\r\n           \'nd_diff\',\r\n           \'nd_diffxor\',\r\n           \'nd_intersect\',\r\n           \'nd_isin\',\r\n           \'nd_merge\',\r\n           \'nd_union\',\r\n           \'nd_uniq\'\r\n           ]\r\n\r\n\r\ndef _view_as_(a):\r\n    """"""Key function to get uniform nd arrays to be viewed as structured arrays.\r\n    A bit of trickery, but it works for all set-like functionality\r\n\r\n    Parameters:\r\n    -----------\r\n    a : array\r\n        ndarray to be viewed\r\n\r\n    Returns:\r\n    --------\r\n    Array view as structured/recarray, with shape = (N, 1)\r\n\r\n    See main documentation under ``Notes``.\r\n    """"""\r\n    if not isinstance(a, np.ndarray):\r\n        print(""np.ndarray is required as input"")\r\n        return None\r\n    if len(a.shape) == 1:\r\n        a_view = np.zeros((1,), dtype=a.dtype.descr*2)\r\n        a_view[0] = tuple(a)\r\n        return a_view\r\n    if a.dtype.kind in (\'O\', \'V\'):\r\n        a = a.reshape(a.shape[0], 1)\r\n        return a\r\n    a_view = a.view(a.dtype.descr * a.shape[1])\r\n    return a_view\r\n\r\n\r\ndef _check_dtype_(a_view, b_view):\r\n    """"""Check for equivalency in the dtypes.  If they are not equal, flag and\r\n    return True or False\r\n    """"""\r\n    err = ""\\nData types are not equal, function failed.\\n1. {}\\n2. {}""\r\n    adtype = a_view.dtype.descr\r\n    bdtype = b_view.dtype.descr\r\n    if adtype != bdtype:\r\n        print(err.format(adtype, bdtype))\r\n        return False\r\n    return True\r\n\r\n\r\ndef nd_diff(a, b, invert=True):\r\n    """"""See nd_intersect.  This just returns the opposite/difference\r\n    """"""\r\n    diff = nd_intersect(a, b, invert=invert)\r\n    return diff\r\n\r\n\r\ndef nd_diffxor(a, b, uni=False):\r\n    """"""using setxor... it is slower than nd_diff, 36 microseconds vs 18.2\r\n    but this is faster for large sets\r\n    """"""\r\n    a_view = _view_as_(a)\r\n    b_view = _view_as_(b)\r\n    good = _check_dtype_(a_view, b_view)  # check dtypes\r\n    if not good:\r\n        return None\r\n    ab = np.setxor1d(a_view, b_view, assume_unique=uni)\r\n    return ab.view(a.dtype).reshape(-1, ab.shape[0]).squeeze()\r\n\r\n\r\ndef nd_intersect(a, b, invert=False):\r\n    """"""Intersect of two, 2D arrays using views and in1d\r\n\r\n    Parameters:\r\n    -----------\r\n    a, b : arrays\r\n        Arrays are assumed to have a shape = (N, 2)\r\n\r\n    `<https://github.com/numpy/numpy/blob/master/numpy/lib/arraysetops.py>`_.\r\n\r\n    `<https://stackoverflow.com/questions/9269681/intersection-of-2d-\r\n    numpy-ndarrays>`_.\r\n    """"""\r\n    a_view = _view_as_(a)\r\n    b_view = _view_as_(b)\r\n    good = _check_dtype_(a_view, b_view)  # check dtypes\r\n    if not good:\r\n        return None\r\n    if len(a) > len(b):\r\n        idx = np.in1d(a_view, b_view, assume_unique=False, invert=invert)\r\n        return a[idx]\r\n    idx = np.in1d(b_view, a_view, assume_unique=False, invert=invert)\r\n    return b[idx]\r\n\r\n\r\ndef nd_isin(a, look_for, reverse=False):\r\n    """"""Checks ndarray `a` for the presence of other records ndarray `look_for`\r\n\r\n    Parameters:\r\n    ----------\r\n    arr : array\r\n        the array to check for the elements\r\n    look_for : number, list or array\r\n        what to use for the good\r\n    reverse : boolean\r\n        Switch the query look_for to `True` to find those not in `a`\r\n    """"""\r\n    a_view = _view_as_(a)\r\n    b_view = _view_as_(look_for)\r\n    good = _check_dtype_(a_view, b_view)  # check dtypes\r\n    if not good:\r\n        return None\r\n    inv = False\r\n    if reverse:\r\n        inv = True\r\n    idx = np.in1d(a_view, b_view, assume_unique=False, invert=inv)\r\n    return a[idx]\r\n\r\n\r\ndef nd_merge(a, b):\r\n    """"""Merge views of 2 ndarrays or recarrays.  Duplicates are not removed, use\r\n    nd_union instead.\r\n\r\n    """"""\r\n    ab = None\r\n    if (a.dtype.kind in (\'f\', \'i\')) and (b.dtype.kind in (\'f\', \'i\')):\r\n        ab = np.concatenate((a, b), axis=0)\r\n    else:\r\n        a_view = _view_as_(a)\r\n        b_view = _view_as_(b)\r\n        good = _check_dtype_(a_view, b_view)  # check dtypes\r\n        if good:\r\n            ab = np.concatenate((a_view, b_view), axis=None)\r\n            ab = ab.view(a.dtype).reshape(-1, ab.shape[0]).squeeze()\r\n    return ab\r\n\r\n\r\ndef nd_union(a, b):\r\n    """"""Union view of arrays\r\n    """"""\r\n    a_view = _view_as_(a)\r\n    b_view = _view_as_(b)\r\n    good = _check_dtype_(a_view, b_view)  # check dtypes\r\n    if not good:\r\n        return None\r\n    ab = np.union1d(a_view, b_view)\r\n#    ab = np.unique(np.concatenate((a_view, b_view), axis=None))\r\n    return ab.view(a.dtype).reshape(ab.shape[0], -1).squeeze()\r\n\r\n\r\ndef nd_uniq(a, counts=False):\r\n    """"""Taken from, but modified for simple axis 0 and 1 and structured\r\n    arrays in (N, m) or (N,) format.\r\n\r\n    To enable determination of unique values in uniform arrays with\r\n    uniform dtypes.  np.unique in versions < 1.13 need to use this.\r\n\r\n    https://github.com/numpy/numpy/blob/master/numpy/lib/arraysetops.py\r\n    """"""\r\n    a_view = _view_as_(a)\r\n    if counts:\r\n        u, i, inv, cnts = np.unique(a_view, return_index=True,\r\n                                    return_inverse=True,\r\n                                    return_counts=counts)\r\n        uni = a[np.sort(i)]\r\n        return uni.squeeze(), cnts\r\n    u, i = np.unique(a_view, return_index=True, return_counts=False)\r\n    uni = a[np.sort(i)]\r\n    return uni.squeeze()\r\n\r\n\r\ndef _demo_data():\r\n    """"""some demo data""""""\r\n    a = np.load(r""C:\\Git_Dan\\arraytools\\Data\\sample_100K.npy"") #20.npy"")\r\n    a0 = a[[\'County\', \'Town\', \'Facility\']]\r\n    names = a0.dtype.names\r\n    return a, a0, names\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n'"
all_scripts/near.py,35,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\nnear\r\n====\r\n\r\nScript :   near.py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified: 2018-03-28\r\n\r\nPurpose :\r\n    Determine the nearest points based on euclidean distance within\r\n    a point file.\r\n\r\n    Also, a function to ensure points have a minimum spacing.\r\n\r\nReferences:\r\n----------\r\n\r\n**creating meshgrids from x,y data and plotting**\r\n\r\n[1]\r\n`2D array of values based on coordinates`__:\r\n\r\n__ http://stackoverflow.com/questions/30764955/python-numpy-create-2darray-of-values-based-on-coordinates\r\n\r\n[2]\r\n`2D histogram issues`__:\r\n\r\n__ https://github.com/numpy/numpy/issues/7317\r\n\r\n\r\n**distance calculations and related** .... (scipy, skikit-learn)\r\n\r\n[3]\r\n`scipy spatial distance`__:\r\n\r\n__ https://github.com/scipy/scipy/blob/v0.18.1/scipy/spatial/distance.py#L1744-L2211\r\n\r\n[4]\r\n`einsum and distance calculations`__:\r\n\r\n    __ http://stackoverflow.com/questions/32154475/einsum-and-distance-calculations\r\n\r\n[5]\r\n`optimizations for calculating squared euclidean distances`__:\r\n\r\n__ http://stackoverflow.com/questions/23983748/possible-optimizations-for-calculating-squared-euclidean-distance\r\n\r\n[6]\r\n`pairwise euclidean distances`__:\r\n\r\n__ http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.euclidean_distances.html\r\n\r\n[7]\r\n`euclidean distance between points`__:\r\n\r\n__ http://stackoverflow.com/questions/1871536/euclidean-distance-between-points-in-two-different-numpy-arrays-not-within\r\n\r\n---------------------------------------------------------------------\r\n""""""\r\n# ---- imports, formats, constants ----\r\n\r\nimport sys\r\nimport numpy as np\r\nfrom textwrap import dedent\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\r\n      \'float\': \'{: 0.1f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=120, precision=2,\r\n                    suppress=True, threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')\r\n\r\nscript = sys.argv[0]\r\n\r\n__all__ = [\'distances\',\r\n           \'not_closer\',\r\n           \'n_check\',\r\n           \'n_near\',\r\n           \'_pnts\',\r\n           \'_n_near_demo\',\r\n           \'_not_closer_demo\'\r\n           ]\r\n# ---- functions ----\r\n\r\n\r\ndef distances(a, b):\r\n    """"""A fast implementation for distance calculations\r\n\r\n    Requires:\r\n    --------\r\n    `a`, `b` - arrays\r\n        2D arrays of equal size!! ... can be the same array\r\n\r\n    Notes:\r\n    -----\r\n        Similar to my e_dist and scipy cdist\r\n    """"""\r\n    if (len(a) != len(b)):\r\n        print(""\\nInput array error...\\n{}"".format(distances.__doc__))\r\n        return None\r\n    d0 = np.subtract.outer(a[:, 0], b[:, 0])\r\n    d1 = np.subtract.outer(a[:, 1], b[:, 1])\r\n    return np.hypot(d0, d1)\r\n\r\n\r\ndef not_closer(a, min_d=1, ordered=False):\r\n    """"""Find the points that are separated by a distance greater than\r\n     min_d.  This ensures a degree of point spacing\r\n\r\n    Requires:\r\n    --------\r\n     `a` : coordinates\r\n         2D array of coordinates.\r\n     `min_d` : number\r\n         Minimum separation distance\r\n     `ordered` : boolean\r\n         Order the input points\r\n    """"""\r\n    if ordered:\r\n        a = a[np.argsort(a[:, 0])]\r\n    b = a.reshape(np.prod(a.shape[:-1]), 1, a.shape[-1])\r\n    diff = b - a\r\n    d = np.einsum(\'ijk,ijk->ij\', diff, diff)\r\n    d = np.sqrt(d).squeeze()\r\n    c = ~(np.triu(d <= min_d, 1)).any(0)\r\n    b = a[c]\r\n    return b, c, d\r\n\r\n\r\ndef n_check(a, N=3, order=True):\r\n    """"""n_check prior to running n_near analysis\r\n\r\n    Requires:\r\n    --------\r\n       Two 2D array of X,Y coordinates required.  Parse your data to comply.\r\n    """"""\r\n    has_err = False\r\n    if isinstance(a, (list, tuple, np.ndarray)):\r\n        if (hasattr(a[0], \'__len__\')) and (len(a[0]) == 2):\r\n            return True\r\n        else:\r\n            has_err = True\r\n    else:\r\n        has_err = True\r\n    if has_err:\r\n        print(n_check.__doc__)\r\n        return False\r\n\r\n\r\ndef n_near(a, N=3, ordered=True):\r\n    """"""Return the coordinates and distance to the nearest N points within\r\n      an 2D numpy array, \'a\', with optional ordering of the inputs.\r\n\r\n    Requires:\r\n    --------\r\n\r\n    `a` : array\r\n        An ndarray of uniform int or float dtype.  Extract the fields\r\n        representing the x,y coordinates before proceeding.\r\n\r\n    `N` : number\r\n         Number of closest points to return\r\n\r\n    Returns:\r\n    -------\r\n      A structured array is returned containing an ID number.  The ID number\r\n      is the ID of the points as they were read.  The array will contain\r\n      (C)losest fields and distance fields\r\n      (C0_X, C0_Y, C1_X, C1_Y, Dist0, Dist1 etc) representing coordinates\r\n      and distance to the required \'closest\' points.\r\n    """"""\r\n    if not (isinstance(a, (np.ndarray)) and (N > 1)):\r\n        print(""\\nInput error...read the docs\\n\\n{}"".format(n_near.__doc__))\r\n        return a\r\n    rows, cols = a.shape\r\n    dt_near = [(\'Xo\', \'<f8\'), (\'Yo\', \'<f8\')]\r\n    dt_new = [(\'C{}\'.format(i) + \'{}\'.format(j), \'<f8\')\r\n              for i in range(N)\r\n              for j in [\'_X\', \'_Y\']]\r\n    dt_near.extend(dt_new)\r\n    dt_dist = [(\'Dist{}\'.format(i), \'<f8\') for i in range(N)]\r\n    # dt = [(\'ID\', \'<i4\')]  + dt_near + dt_dist # python 2.7\r\n    dt = [(\'ID\', \'<i4\'), *dt_near, *dt_dist]\r\n    n_array = np.zeros((rows,), dtype=dt)\r\n    n_array[\'ID\'] = np.arange(rows)\r\n    # ---- distance matrix calculation using einsum ----\r\n    if ordered:\r\n        a = a[np.argsort(a[:, 0])]\r\n    b = a.reshape(np.prod(a.shape[:-1]), 1, a.shape[-1])\r\n    diff = b - a\r\n    dist = np.einsum(\'ijk,ijk->ij\', diff, diff)\r\n    d = np.sqrt(dist).squeeze()\r\n    # ---- format for use in structured array output ----\r\n    # steps are outlined as follows....\r\n    #\r\n    kv = np.argsort(d, axis=1)       # sort \'d\' on last axis to get keys\r\n    coords = a[kv]                   # pull out coordinates using the keys\r\n    s0, s1, s2 = coords.shape\r\n    coords = coords.reshape((s0, s1*s2))\r\n    dist = np.sort(d)[:, 1:]         # slice sorted distances, skip 1st\r\n    # ---- construct the structured array ----\r\n    dt_names = n_array.dtype.names\r\n    s0, s1, s2 = (1, (N+1)*2 + 1, len(dt_names))\r\n    for i in range(0, s1):           # coordinate field names\r\n        nm = dt_names[i+1]\r\n        n_array[nm] = coords[:, i]\r\n    dist_names = dt_names[s1:s2]\r\n    for i in range(N):               # fill n_array with the results\r\n        nm = dist_names[i]\r\n        n_array[nm] = dist[:, i]\r\n    return coords, dist, n_array\r\n\r\n\r\ndef _pnts(L, B, R, T, num, as_int=True, as_recarry=False):\r\n    """"""Create the points""""""\r\n    xs = (R-L) * np.random.random_sample(size=num) + L\r\n    ys = (T-B) * np.random.random_sample(size=num) + B\r\n    a = np.array(list(zip(xs, ys)))\r\n    if as_int:\r\n        a = a.astype(\'int32\')\r\n    a = a[np.argsort(a[:, 0])]\r\n    return a\r\n\r\n\r\ndef _n_near_demo():\r\n    """"""Demonstrate n_near function""""""\r\n    frmt = """"""\r\n    -----------------------------------------------------------------\r\n    Closest {} points for points in an array.  Results returned as\r\n      a structured array with coordinates and distance values.\r\n    {} ....\\n\r\n    Input points... array \'a\'\r\n    {}\\n\r\n    output array\r\n    """"""\r\n    vals = [[0.0, 0.0], [0, 0.5], [0, 1], [0, 1.5], [0, 2], [1, 2],\r\n            [2, 2], [2, 1], [2, 0], [1, 0], [1, 0]]\r\n    vals = [tuple(i) for i in vals]      # has to be tuples\r\n#    dt = np.dtype([(\'X\', \'<f8\'), (\'Y\', \'<f8\')])\r\n    a = np.array(vals, dtype=\'float64\')\r\n#    b = np.array(vals, dtype=dt)\r\n    N = 2\r\n    coords, dist, n_r = n_near(a, N=N, ordered=True)  # a, coords, dist,\r\n    args = [N, _n_near_demo.__doc__, a]\r\n    print(dedent(frmt).format(*args))\r\n    n = len(n_r[0])\r\n    names = n_r.dtype.names\r\n    frmt = ""{!s:>7}""*n\r\n    print(frmt.format(*names))\r\n    for i in n_r:  # .reshape(n_r.shape[0],-1)\r\n        frmt = ""{:> 7.2f}""*n\r\n        print(frmt.format(*i))\r\n    print("":""+""-""*66)\r\n    return a, coords, dist, n_r\r\n\r\n\r\ndef _not_closer_demo():\r\n    """"""Perform \'closest\' analysis and produce a histogram classed using\r\n      distance bands.  The histogram can be used to produce a 2D raster\r\n      representation of the point pattern.\r\n      np.histogram2d(x, y, bins=10, range=None, normed=False, weights=None)\r\n    """"""\r\n    a = np.array([[6, 79], [7, 24], [17, 11], [33, 47], [37, 46], [38, 42],\r\n                  [46, 98], [48, 66], [49, 21], [57, 40], [71, 74], [74, 86],\r\n                  [85, 20], [87, 98], [88,  5], [88, 56], [89, 95], [89, 55],\r\n                  [92, 97], [96, 93]], dtype=np.int32)\r\n    b, c, d = not_closer(a, ordered=False, min_d=20)\r\n    idx = np.arange(len(a))\r\n    e = np.c_[a, c, idx]\r\n    x_bin = np.arange(0, 101, 5)\r\n    y_bin = np.arange(0, 101, 5)\r\n    #\r\n    h, hx, hy = np.histogram2d(e[:, 1], e[:, 0], bins=(x_bin, y_bin),\r\n                               weights=e[:, -2])\r\n    h = h.astype(\'int64\')  # return the counts in the 10x10 cells\r\n    return a, b, c, d, e, h\r\n\r\n\r\n# ---------------------------------------------------------------------\r\nif __name__ == ""__main__"":\r\n    """"""Main section...   """"""\r\n#    print(""Script... {}"".format(script))\r\n#    a, coords, d, n_r = _n_near_demo()\r\n#    a, b, c, d, e, h = _not_closer_demo()\r\n'"
all_scripts/np2tbl.py,5,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\nnp2tbl\r\n======\r\n\r\nScript  : np2tbl.py\r\n\r\nAuthor  :   Dan.Patterson@carleton.ca\r\n\r\nModified: 2018-09-23\r\n\r\nPurpose:  tools for working with numpy arrays\r\n\r\nReferences:\r\n----------\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/numpyarraytotable.htm>`_.\r\n\r\nDerived from python snippet output...\r\n\r\n>>> in_arr = \'C:/Temp/x.npy\'\r\n>>> out_gdb = \'C:/GIS/Tools_scripts/Table_tools/Table_tools.gdb\'\r\n>>> out_name = \'sample_1000_npy\'\r\n>>> arcpy.Tabletools.NumPyArrayToTable(in_arr, out_gdb, out_name)\r\n\r\n---------------------------------------------------------------------\r\n""""""\r\n# pylint: disable=C0103  # invalid-name\r\n# pylint: disable=R0914  # Too many local variables\r\n# pylint: disable=R1710  # inconsistent-return-statements\r\n# pylint: disable=W0105  # string statement has no effect\r\n\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nfrom textwrap import dedent\r\nimport numpy as np\r\nfrom arcpy import (AddMessage, ListTables, ValidateTableName,\r\n                   MakeTableView_management)\r\nfrom arcpy.da import NumPyArrayToTable\r\nfrom arcpy.geoprocessing import env\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=5, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\ndef tweet(msg):\r\n    """"""Print a message for both arcpy and python.\r\n    : msg - a text message\r\n    """"""\r\n    m = ""\\n{}\\n"".format(msg)\r\n    AddMessage(m)\r\n    print(m)\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# .... final code section producing the featureclass and extendtable\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    pth = script.split(""/"")[:-2]\r\n    pth = ""/"".join(pth) + ""/Data/sample_20.npy""\r\n    a = np.load(pth)\r\n    frmt = ""Result...\\n{}""\r\n    print(frmt.format(a))\r\nelse:\r\n    testing = False\r\n    in_arr = sys.argv[1]\r\n    out_name = sys.argv[2]\r\n    out_gdb = sys.argv[3]\r\n    make_tbl_view = sys.argv[4]\r\n    env.workspace = out_gdb\r\n    tbls = ListTables()\r\n    out_name = ValidateTableName(out_name)\r\n    if tbls is not None:\r\n        if out_name in tbls:\r\n            out_name += \'_dup\'\r\n    out_tbl = out_gdb + ""/"" + out_name\r\n    # ---- call section for processing function\r\n    #\r\n    a = np.load(in_arr)\r\n    NumPyArrayToTable(a, out_tbl)  # create the table\r\n    if make_tbl_view in (True, \'True\', 1):\r\n        MakeTableView_management(out_tbl, out_name)\r\n    args = [in_arr, out_gdb, out_name]\r\n    msg = """"""\r\n    :------------------------------------------------------------\r\n\r\n    Input array... {}\r\n    Output gdb.... {}\r\n    Output name... {}\r\n\r\n    Conversion complete...\r\n    Add the table manually if you want to see it...\r\n\r\n    You need to refresh the geodatabase first since there is no\r\n    autorefresh\r\n\r\n    :------------------------------------------------------------\r\n    """"""\r\n    msg = dedent(msg).format(*args)\r\n    tweet(msg)\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n'"
all_scripts/npy2ras.py,7,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nnpy2ras\r\n=======\r\n\r\nScript :   npy2ras.py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified : 2018-09-26\r\n\r\nPurpose :  tools for working with numpy arrays\r\n\r\nReferences\r\n----------\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/functions/numpyarraytoraster\r\n-function.htm>`_.\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/numpyarraytotable.htm>`_.\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/tabletonumpyarray.htm>`_.\r\n---------------------------------------------------------------------\r\n""""""\r\n# pylint: disable=C0103  # invalid-name\r\n# pylint: disable=R0914  # Too many local variables\r\n# pylint: disable=R1710  # inconsistent-return-statements\r\n# pylint: disable=W0105  # string statement has no effect\r\n\r\nimport sys\r\nimport numpy as np\r\nfrom arcpy import Point\r\nfrom arcgisscripting import NumPyArrayToRaster\r\nimport arcpy.env as env\r\n\r\nenv.overwriteOutput = True\r\n\r\n#from arcpytools import fc_info, tweet  #, frmt_rec, _col_format\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n# ----------------------------------------------------------------------\r\n# .... final code section producing the featureclass and extendtable\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    pth = script.split(""/"")[:-1]\r\n    pth0 = ""/"".join(pth) + ""/Data/r00.npy""\r\n    LL_x = 300000.\r\n    LL_y = 5030000.\r\n    cell_sze = 10.\r\n    no_data = 255\r\n    in_arr = np.load(pth0)\r\n    pth1 = ""/"".join(pth) + ""/Data/r01.tif""\r\n    # parameters here\r\nelse:\r\n    testing = False\r\n    pth0 = sys.argv[1]\r\n    LL_x = float(sys.argv[2])\r\n    LL_y = float(sys.argv[3])\r\n    cell_sze = float(sys.argv[4])\r\n    pth1 = sys.argv[5]\r\n    if pth1[-4:] != "".tif"":\r\n        pth1 += "".tif""\r\n    in_arr = np.load(pth0)\r\n    # parameters here\r\n#\r\nto_pro = True  # ---- change to True to produce tif for ArcGIS PRO\r\n\r\ndt_kind = in_arr.dtype.kind\r\nif dt_kind in (\'u\', \'i\'):\r\n    no_data = np.iinfo(in_arr.dtype.str).max\r\nelif dt_kind in (\'f\'):\r\n    no_data = np.iinfo(in_arr.dtype.str).max\r\nelse:\r\n    no_data = None\r\nif to_pro:\r\n    ras = NumPyArrayToRaster(in_arr,\r\n                             lower_left_corner=Point(LL_x, LL_y),\r\n                             x_cell_size=cell_sze,\r\n                             value_to_nodata=no_data\r\n                             )\r\n    ras.save(pth1)\r\n\r\nif testing:\r\n    print(\'\\nScript source... {}\'.format(script))\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n'"
all_scripts/numpyarray2raster.py,9,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   numpyarray2raster.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2018-02-12\r\n:Purpose:  tools for working with numpy arrays\r\n:Useage:\r\n:\r\n: NumPyArrayToRaster(in_array, {lower_left_corner},\r\n:                              {x_cell_size}, {y_cell_size},\r\n:                              {value_to_nodata})\r\n:References:\r\n:  http://pro.arcgis.com/en/pro-app/arcpy/functions/\r\n:       numpyarraytoraster-function.htm\r\n\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport os\r\nfrom textwrap import dedent\r\nimport numpy as np\r\nimport arcpy\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n__all__ = [\'arr_tif_tf\',    # array to tiff using tifffile\r\n           \'arr_tif_PIL\',   # .... using PIL\r\n           \'arr_tif_cb\',    # .... using CompositeBands\r\n           \'tifffile_arr\'\r\n           ]\r\n\r\n\r\ndef tweet(msg):\r\n    """"""Print a message for both arcpy and python.\r\n    : msg - a text message\r\n    """"""\r\n    m = ""\\n{}\\n"".format(msg)\r\n    arcpy.AddMessage(m)\r\n    print(m)\r\n\r\n\r\ndef arr_tif_tf(a, fname):\r\n    """"""Convert a NumPy array to a tiff using tifffile.py\r\n    :\r\n    : - https://github.com/blink1073/tifffile  source github page\r\n    :Requires:  tifffile\r\n    :---------\r\n    : from tifffile import imread, imsave, TiffFile\r\n    : help(TiffFile.geotiff_metadata)\r\n    :\r\n    : imsave:\r\n    : imsave(file, data=None, shape=None, dtype=None, bigsize=2**32-2**25,\r\n    :       **kwargs)\r\n    :  file - filename with *.tif\r\n    :  data - array\r\n    :  shape - if creating an empty array\r\n    : a = np.arange(4*5*6).reshape(4, 5, 6)\r\n    : imsave(\'c:/temp/temp.tif\', a)\r\n    : b =imread(\'c:/temp/temp.tif\')\r\n    : np.all(a == b)  # True\r\n    :\r\n    : GeoTiff, World files:\r\n    :  http://trac.osgeo.org/geotiff/\r\n    :  https://en.wikipedia.org/wiki/World_file\r\n    """"""\r\n    import warnings\r\n    warnings.filterwarnings(\'ignore\')\r\n    from tifffile import imsave\r\n    imsave(fname, a)\r\n\r\n\r\ndef arr_tif_PIL(a, fname):\r\n    """"""convert an array to a tif using PIL\r\n    """"""\r\n    from PIL import Image\r\n    imgs = []\r\n    for i in a:\r\n        imgs.append(Image.fromarray(i))\r\n    imgs[0].save(fname, compression=""tiff_deflate"", save_all=True,\r\n                 append_images=imgs[1:])\r\n    # ---- done\r\n\r\n\r\ndef arr_tif_cb(a, fname, LL_X=0, LL_Y=0, cell_size=1, no_data=None):\r\n    """"""Array to tiff using esri compositebands\r\n    :\r\n    """"""\r\n    #  This section works\r\n    arcpy.env.workspace = \'in_memory\'\r\n    pnt = arcpy.Point(LL_X, LL_Y)\r\n    rasters = []\r\n    if no_data is None:\r\n        no_data = np.iinfo(a.dtype.type).min\r\n    for i in range(a.shape[0]):\r\n        ai = a[i]\r\n        rast = arcpy.NumPyArrayToRaster(ai, pnt, cell_size, cell_size, no_data)\r\n        r_name = ""in_memory/a{:0>3}.tif"".format(i)\r\n        rasters.append(r_name)\r\n        rast.save(r_name)\r\n    rasters = "";"".join([i for i in rasters])\r\n    # Mosaic dataset section\r\n    arcpy.management.CompositeBands(rasters, fname)\r\n    # ----\r\n\r\n\r\ndef tifffile_arr(fname):\r\n    """"""Convert tif to array using tifffile\r\n    :\r\n    : Source: tifffile # http://www.lfd.uci.edu/~gohlke/code/tifffile.py.html\r\n    :-------\r\n    : TiffFile.asarray(self, key=None, series=None, out=None, maxworkers=1)\r\n    :  key - int, slice, or sequence of page indices\r\n    :        Defines which pages to return as array.\r\n    :  series - int or TiffPageSeries\r\n    :  out - array if None or filename\r\n    :  maxworkers = default 1, number of threads to use to get data\r\n    #\r\n    : from tifffile import tifffile as tf ******\r\n    :\r\n    : tif = TiffFile(fp)  # *****\r\n    : tif.byteorder  # \'<\'\r\n    : tif.isnative   # True\r\n    """"""\r\n    from tifffile import TiffFile\r\n    with TiffFile(fname) as tif:\r\n        a = tif.asarray()\r\n        a = np.rollaxis(a,  axis=2, start=0)\r\n    return a\r\n\r\n\r\n# ---- Run options: _demo or from _tool\r\n#\r\ndef _demo():\r\n    """"""Code to run if in demo mode\r\n    """"""\r\n    a = np. array([\'1, 2, 3, 4, 5\', \'a, b, c\', \'6, 7, 8, 9\',\r\n                   \'d, e, f, g, h\', \'10, 11, 12, 13\'])\r\n    return a\r\n\r\n\r\nmsg0 = """"""\r\n: -----------------------------------------------------------\r\nScript....\r\n.... {}\r\nFailed because either...\r\n\r\n(1) Input raster needs to be an *.npy file.\r\n(2) The input and/or output path and/or filename has a space in it.\r\n(3) The output raster has to be a *.tif\r\n\r\nFix one or more conditions...\r\n\r\n...\r\n: -----------------------------------------------------------\r\n""""""\r\n\r\nmsg1 = """"""\r\n: -----------------------------------------------------------\r\nScript....\r\n.... {}\r\nCompleted....\r\n...\r\n: -----------------------------------------------------------\r\n""""""\r\n\r\n\r\ndef check_files(file_path, ext=""""):\r\n    """"""Check expected file paths and extensions, to ensure compliance with\r\n    :  tool specifications\r\n    """"""\r\n    is_good = True\r\n    head, tail = os.path.split(file_path)\r\n    if not os.path.exists(head):\r\n        return False\r\n    if "" "" in tail:\r\n        return False\r\n    if os.path.splitext(tail)[1] != ext:\r\n        return False\r\n    if "" "" in file_path:\r\n        return False\r\n    return is_good\r\n    # ----\r\n\r\n\r\ndef check_nodata(a, nodata):\r\n    """"""check for array dtype etc""""""\r\n    a_kind = a.dtype.str\r\n    if a_kind in (\'|i1\', \'<i2\', \'<i4\', \'<i8\'):\r\n        nodata = int(nodata)\r\n    elif a_kind in (\'<f2\', \'<f4\', \'<f8\'):\r\n        nodata = float(nodata)\r\n    else:\r\n        nodata = -9999\r\n    return nodata\r\n\r\n\r\ndef _tool():\r\n    """"""run when script is from a tool\r\n    """"""\r\n    in_npy = sys.argv[1]\r\n    x = float(sys.argv[2])\r\n    y = float(sys.argv[3])\r\n    cell_size = float(sys.argv[4])\r\n    no_data = sys.argv[5]\r\n    out_rast = sys.argv[6]\r\n    SR = sys.argv[7]\r\n    msg = """"""\r\n    : -----------------------------------------------------------\r\n    Script parameters....\r\n    in_npy {}\r\n    LL corner (x,y) ... {}, {}\r\n    cell size ... {}\r\n    nodata ...... {}\r\n    out raster .. {}\r\n    : -----------------------------------------------------------\r\n    """"""\r\n    args = [in_npy, x, y, cell_size, no_data, out_rast]\r\n    tweet(dedent(msg).format(*args))\r\n    #\r\n    # ---- main tool section\r\n    pnt = arcpy.Point(X=x, Y=y)\r\n    is_good1 = check_files(in_npy, ext="".npy"")\r\n    is_good2 = check_files(out_rast, ext="".tif"")\r\n    if is_good1 and is_good2:\r\n        a = np.load(in_npy)\r\n        no_data = check_nodata(a, no_data)  # check nodata value\r\n        rast = arcpy.NumPyArrayToRaster(a, pnt, cell_size, cell_size, no_data)\r\n        rast.save(out_rast)\r\n        if SR not in (\'#\', \' \', \'\', None, \'Unknown\'):\r\n            arcpy.DefineProjection_management(out_rast, SR)\r\n        tweet(dedent(msg1).format(script))\r\n    else:\r\n        raise AttributeError(msg0)\r\n        tweet(dedent(msg0).format(script))\r\n    # ----\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# .... final code section\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    a = _demo()\r\n    frmt = ""Result...\\n{}""\r\n    print(frmt.format(a))\r\nelse:\r\n    testing = False\r\n    _tool()\r\n#\r\nif not testing:\r\n    print(\'Done...\')\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n    a = _demo()\r\n'"
all_scripts/numpyarray2table.py,5,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   numpyarray2table.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2018-02-11\r\n:Purpose:  tools for working with numpy arrays\r\n:Useage:\r\n:\r\n:References:\r\n: - Derived from python snippet output...\r\n:  in_arr = \'C:/Temp/x.npy\'\r\n:  out_gdb = \'C:/GIS/Tools_scripts/Table_tools/Table_tools.gdb\'\r\n:  out_name = \'sample_1000_npy\'\r\n:  arcpy.Tabletools.NumPyArrayToTable(in_arr, out_gdb, out_name)\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nfrom textwrap import dedent\r\nimport numpy as np\r\nimport arcpy\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\ndef tweet(msg):\r\n    """"""Print a message for both arcpy and python.\r\n    : msg - a text message\r\n    """"""\r\n    m = ""\\n{}\\n"".format(msg)\r\n    arcpy.AddMessage(m)\r\n    print(m)\r\n\r\n\r\n# ---- Run options: _demo or from _tool\r\n#\r\ndef _demo():\r\n    """"""Code to run if in demo mode\r\n    """"""\r\n    a = np. array([\'1, 2, 3, 4, 5\', \'a, b, c\', \'6, 7, 8, 9\',\r\n                   \'d, e, f, g, h\', \'10, 11, 12, 13\'])\r\n    return a\r\n\r\n\r\ndef _tool():\r\n    """"""run when script is from a tool\r\n    """"""\r\n    in_arr = sys.argv[1]\r\n    out_gdb = sys.argv[2]\r\n    out_name = sys.argv[3]\r\n    #\r\n    arcpy.env.workspace = out_gdb\r\n    tbls = arcpy.ListTables()\r\n    out_name = arcpy.ValidateTableName(out_name)\r\n    if out_name in tbls:\r\n        out_name += \'_dup\'\r\n    #\r\n    # ---- call section for processing function\r\n    #\r\n    a = np.load(in_arr)\r\n    in_table = ""\\\\"".join([out_gdb, out_name])\r\n    # ---- where_clause= skip_nulls=  null_value=)\r\n    arcpy.da.NumPyArrayToTable(a, in_table)\r\n    arcpy.MakeTableView_management(in_table, out_name)\r\n    #\r\n    args = [in_arr, out_gdb, out_name]\r\n    msg = """"""\r\n    :------------------------------------------------------------\r\n    Input array... {}\r\n    Output gdb.... {}\r\n    Output name... {}\r\n\r\n    Conversion complete...\r\n    Add the table manually if you want to see it...\r\n    :------------------------------------------------------------\r\n    """"""\r\n    msg = dedent(msg).format(*args)\r\n    tweet(msg)\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# .... final code section producing the featureclass and extendtable\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    arrs = _demo()\r\n    frmt = ""Result...\\n{}""\r\n    print(frmt.format(arrs))\r\nelse:\r\n    testing = False\r\n    _tool()\r\n#\r\nif not testing:\r\n    tweet(\'\\nConversion done...\')\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n    a = _demo()\r\n'"
all_scripts/pip.py,25,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\npip\r\n===\r\n\r\nScript :   pip.py\r\n\r\nAuthor :   Dan.Patterson@carleton.ca\r\n\r\nModified : 2019-01-02\r\n\r\nPurpose:\r\n--------\r\n  Incarnations of point in polygon searches.  Includes, points in extent and\r\n  crossing number.\r\n\r\nReferences:\r\n----------\r\n\r\n`<https://stackoverflow.com/questions/33051244/numpy-filter-points-within-\r\nbounding-box/33051576#33051576>`_.\r\n\r\n`<https://wrf.ecse.rpi.edu//Research/Short_Notes/pnpoly.html>`_.  ** good\r\n\r\nNotes:\r\n------\r\nRemove points that are outside of the polygon extent, then filter those\r\nusing the crossing number approach to test whether a point is within.\r\n\r\n**Sample run**\r\n\r\n>>> a, ext = array_demo()\r\n>>> poly = extent_poly(ext)\r\n>>> p0 = np.array([341999, 5021999])  # just outside\r\n>>> p1 = np.mean(poly, axis=0)        # extent centroid\r\n>>> pnts - 10,000 points within the full extent, 401 points within the polygon\r\n\r\n(1) pnts_in_extent:\r\n\r\n>>> %timeit pnts_in_extent(pnts, ext, in_out=False)\r\n143 \xc2\xb5s \xc2\xb1 2.16 \xc2\xb5s per loop (mean \xc2\xb1 std. dev. of 7 runs, 10000 loops each)\r\n\r\n>>> %timeit pnts_in_extent(pnts, ext, in_out=True)\r\n274 \xc2\xb5s \xc2\xb1 9.12 \xc2\xb5s per loop (mean \xc2\xb1 std. dev. of 7 runs, 1000 loops each)\r\n\r\n(2) crossing_num with pnts_in_extent check (current version):\r\n\r\n>>> %timeit crossing_num(pnts, poly)\r\n9.68 ms \xc2\xb1 120 \xc2\xb5s per loop (mean \xc2\xb1 std. dev. of 7 runs, 100 loops each)\r\n\r\n(3) pure crossing_num:\r\n\r\n>>> %timeit crossing_num(pnts, poly)\r\n369 ms \xc2\xb1 19.1 ms per loop (mean \xc2\xb1 std. dev. of 7 runs, 1 loop each)\r\n\r\n""""""\r\n# pylint: disable=C0103  # invalid-name\r\n# pylint: disable=R0914  # Too many local variables\r\n# pylint: disable=R1710  # inconsistent-return-statements\r\n# pylint: disable=W0105  # string statement has no effect\r\n\r\n# ----10| ------20| ------30| ------40| ------50| ------60| ------70| ------80|\r\nimport numpy as np\r\nimport arcpy\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\n\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\n__all__ = [\'extent_poly\',\r\n           \'pnts_in_extent\',\r\n           \'crossing_num\']\r\n\r\n\r\ndef extent_poly(ext):\r\n    """"""Construct the extent rectangle from the extent points which are the\r\n    lower left and upper right points [LB, RT]\r\n    """"""\r\n    LB, RT = ext\r\n    L, B = LB\r\n    R, T = RT\r\n    box = [LB, [L, T], RT, [R, B], LB]\r\n    ext_rect = np.array(box)\r\n    return ext_rect\r\n\r\n\r\ndef pnts_in_extent(pnts, ext, in_out=True):\r\n    """"""Point(s) in polygon test using numpy and logical_and to find points\r\n    within a box/extent.\r\n\r\n    Requires:\r\n    --------\r\n    pnts : array\r\n      an array of points, ndim at least 2D\r\n    ext : numbers\r\n      the extent of the rectangle being tested as an array of the left bottom\r\n      (LB) and upper right (RT) coordinates\r\n    in_out : boolean\r\n      - True to return both the inside and outside points.\r\n      - False for inside only.\r\n\r\n    Notes:\r\n    ------\r\n    - comp : np.logical_and( great-eq LB, less RT)  condition check\r\n    - inside : np.where(np.prod(comp, axis=1) == 1) if both true, product = 1\r\n    - case : comp returns [True, False] so you take the product\r\n    - idx_in : indices derived using where since case will be 0 or 1\r\n    - inside : slice the pnts using idx_in\r\n    """"""\r\n    pnts = np.atleast_2d(pnts)  # account for single point\r\n    outside = None\r\n    LB, RT = ext\r\n    comp = np.logical_and((LB <= pnts), (pnts <= RT))\r\n    case = comp[..., 0] * comp[..., 1]\r\n    idx_in = np.where(case)[0]\r\n    inside = pnts[idx_in]\r\n    if in_out:\r\n        idx_out = np.where(~case)[0]  # invert case\r\n        outside = pnts[idx_out]\r\n    return inside, outside\r\n\r\n\r\ndef crossing_num(pnts, poly):\r\n    """"""Points in polygon implementation of crossing number largely from pnpoly\r\n    in its various incarnations.  This version also does a within extent\r\n    test to pre-process the points, keeping those within the extent to be\r\n    passed on to the crossing number section.\r\n\r\n    Requires:\r\n    ---------\r\n    pnts_in_extent : function\r\n      Method to limit the retained points to those within the polygon extent.\r\n      See \'pnts_in_extent\' for details\r\n    pnts : array\r\n      point array\r\n    poly : polygon\r\n      closed-loop as an array\r\n\r\n    """"""\r\n    xs = poly[:, 0]\r\n    ys = poly[:, 1]\r\n    dx = np.diff(xs)\r\n    dy = np.diff(ys)\r\n    ext = np.array([poly.min(axis=0), poly.max(axis=0)])\r\n    inside, outside = pnts_in_extent(pnts, ext, in_out=False)\r\n    is_in = []\r\n    for pnt in inside:\r\n        cn = 0    # the crossing number counter\r\n        x, y = pnt\r\n        for i in range(len(poly)-1):      # edge from V[i] to V[i+1]\r\n            u = ys[i] <= y < ys[i+1]\r\n            d = ys[i] >= y > ys[i+1]\r\n            if np.logical_or(u, d):       # compute x-coordinate\r\n                vt = (y - ys[i]) / dy[i]\r\n                if x < (xs[i] + vt * dx[i]):\r\n                    cn += 1\r\n        is_in.append(cn % 2)  # either even or odd (0, 1)\r\n    result = inside[np.nonzero(is_in)]\r\n    return result\r\n\r\n\r\ndef _demo():\r\n    """""" used in the testing\r\n    : polygon layers\r\n    : C:/Git_Dan/a_Data/testdata.gdb/Carp_5x5km  full 25 polygons\r\n    : C:/Git_Dan/a_Data/testdata.gdb/subpoly     centre polygon with \'ext\'\r\n    : C:/Git_Dan/a_Data/testdata.gdb/centre_4    above, but split into 4\r\n    """"""\r\n    ext = np.array([[342000, 5022000], [343000, 5023000]])\r\n    in_fc = r\'C:\\Git_Dan\\a_Data\\testdata.gdb\\xy_10k\'\r\n    SR = arcpy.SpatialReference(2951)\r\n    a = arcpy.da.FeatureClassToNumPyArray(in_fc, [\'SHAPE@X\', \'SHAPE@Y\'],\r\n                                          spatial_reference=SR)\r\n    pnts = a.view(dtype=np.float).reshape(len(a), 2)\r\n    poly = extent_poly(ext)\r\n    p0 = np.array([341999., 5021999.])\r\n    p1 = np.mean(poly, axis=0)\r\n    pnts = np.array([p0, p1])\r\n    return pnts, ext, poly, p0, p1\r\n\r\n\r\ndef _demo1():\r\n    """"""Simple check for known points""""""\r\n    ext = np.array([[342000, 5022000], [343000, 5023000]])\r\n    poly = extent_poly(ext)\r\n    p0 = np.array([341999, 5021999])\r\n    p1 = np.mean(poly, axis=0)\r\n    pnts = np.array([p0, p1])\r\n    return pnts, ext, poly, p0, p1\r\n\r\n\r\nif __name__ == ""__main__"":\r\n    """"""Make some points for testing, create an extent, time each option.\r\n    :\r\n    :Profiling functions\r\n    : %load_ext line_profiler\r\n    : %lprun -f pnts_in_extent pnts_in_extent(pnts, ext)  # -f means function\r\n    """"""\r\n#    pnts, ext, poly, p0, p1 = _demo1()\r\n'"
all_scripts/plot_arr.py,9,"b'# -*- coding: UTF-8 -*-\n""""""\n:Script:   plot_arr.py\n:Author:   Dan.Patterson@carleton.ca\n:Modified: 2018-08-18\n:Purpose:  To plot a 3D array as a graph with each dimension appearing\n:  row-wise, instead of down a column.  This produces a side-by-side\n:  comparison of the data.\n:\n:Functions:\n: - plot_grid\n:Notes:\n:  plt is matplotlib.pyplot from import\n:  fig = plt.gcf()        # get the current figure\n:  fig.get_size_inches()  # find out its size in inches\n:     array([ 9.225,  6.400])\n:  fig.bbox_inches\n:     Bbox(\'array([[ 0.000,  0.000],\\n       [ 9.225,  6.400]])\')\n:References:\n:----------\n:  https://matplotlib.org/2.0.0/api/figure_api.html\n:\n""""""\n# ---- imports, formats, constants ----\nimport sys\nimport numpy as np\nfrom textwrap import dedent\n\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\n# import matplotlib.colors as mc\n\n# local import\n\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\n      \'float_kind\': \'{: 0.2f}\'.format}\nnp.set_printoptions(edgeitems=3, linewidth=80, precision=2,\n                    suppress=True, threshold=20,\n                    formatter=ft)\n\nscript = sys.argv[0]\n\n\n# ---- functions -------------------------------------------------------------\n#\ndef plot_grid(a):\n    """"""Emulate a 3D array plotting each dimension sequentially.\n    :Requires:\n    :---------\n    : a - an ndarray.  if a.ndim < 3, an extra dimension is prepended to it.\n    :     The min and max are determined from the input data values.\n    :     This is used later on for interpolation.\n    :Returns:\n    :  A plot of the array dimensions.\n    """"""\n    def plot_3d(rows=1, cols=1):\n        """"""Set the parameters of the 3D array for plotting""""""\n        x = np.arange(cols)\n        y = np.arange(rows)\n        xg = np.arange(-0.5, cols + 0.5, 1)\n        yg = np.arange(-0.5, rows + 0.5, 1)\n        return [x, y, xg, yg]\n    #\n    a = a.squeeze()\n    if a.ndim < 3:\n        frmt = ""1D and 2D arrays not supported, read the docs\\n{}""\n        print(dedent(frmt).format(plot_grid.__doc__))\n        return None\n    # proceed with 3D array\n    n, rows, cols = a.shape\n    x, y, xg, yg = plot_3d(rows, cols)\n    w = (cols*n)//2\n    h = (rows + 1)//2\n    w = max(w, h)\n    h = min(w, h)\n    fig, axes = plt.subplots(1, n, sharex=True, sharey=True,\n                             dpi=150, figsize=(w, h))\n    fig.set_tight_layout(True)\n    fig.set_edgecolor(\'w\')\n    fig.set_facecolor(\'w\')\n    idx = 0\n    for ax in axes:\n        m_min = a.min()\n        m_max = a.max()\n        a_s = a[idx]\n        col_lbl = ""Cols: for "" + str(idx)\n        ax.set_aspect(\'equal\')\n        ax.set_adjustable(\'box\')  # box-forced\')  # deprecated prevents spaces\n        ax.set_xticks(xg, minor=True)\n        ax.set_yticks(yg, minor=True)\n        ax.set_xlabel(col_lbl, labelpad=12)\n        ax.xaxis.label_position = \'top\'\n        ax.xaxis.label.set_fontsize(12)\n        if idx == 0:\n            ax.set_ylabel(""Rows"", labelpad=2)  # was 12\n            ax.yaxis.label.set_fontsize(12)\n        ax.grid(which=\'minor\', axis=\'x\', linewidth=1, linestyle=\'-\', color=\'k\')\n        ax.grid(which=\'minor\', axis=\'y\', linewidth=1, linestyle=\'-\', color=\'k\')\n        t = [[x, y, a_s[y, x]]\n             for y in range(rows)\n             for x in range(cols)]\n        for i, (x_val, y_val, c) in enumerate(t):\n            ax.text(x_val, y_val, c, va=\'center\', ha=\'center\', fontsize=12)\n        ax.matshow(a[idx], cmap=cm.gray_r, interpolation=\'nearest\',\n                   vmin=m_min, vmax=m_max, alpha=0.2)\n        idx += 1\n    # ---- end of script ----------------------------------------------------\n\n\ndef _plt_(a):\n    """"""\n    :one array shows numbers, the alternate text\n    """"""\n    plot_grid(a)\n    print(""array... shape {} ndim {}\\n{}"".format(a.shape, a.ndim, a))\n\n\n# ----------------------------------------------------------------------------\nif __name__ == ""__main__"":\n    """"""   """"""\n#    print(""Script... {}"".format(script))\n    d, r, c = [3, 8, 4]\n    a = np.arange(d*r*c).reshape(d, r, c)\n    b = a * 1.0\n    c = np.random.randint(96, size=96).reshape(d, r, c)\n    c1 = c * 1.0\n    d = np.arange(2*3*3*4).reshape(2, 3, 3, 4)\n    e = np.arange(4*5).reshape(4, 5)\n#    _plt_(a)\n'"
all_scripts/polygon_a_sdf_rec_demo.py,20,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   .py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2017-xx-xx\r\n:Purpose:  tools for working with numpy arrays\r\n:Useage:\r\n:\r\n:References:\r\n:\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nfrom textwrap import dedent\r\nimport numpy as np\r\nimport arraytools as art\r\nimport arcpy\r\nfrom arcgis.geometry import _types\r\nfrom arcgis.features._data.geodataset import SpatialDataFrame as SDF\r\nimport pandas as pd\r\n\r\n\r\n# import json\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\r\n      \'float\': \'{: 0.2f}\'.format}\r\n\r\nnp.set_printoptions(edgeitems=3, linewidth=80, precision=2, suppress=True,\r\n                    threshold=10, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\ndef time_deco(func):  # timing originally\r\n    """"""timing decorator function\r\n    :print(""\\n  print results inside wrapper or use <return> ... "")\r\n    """"""\r\n    import time\r\n    from functools import wraps\r\n\r\n    @wraps(func)\r\n    def wrapper(*args, **kwargs):\r\n        t0 = time.perf_counter()        # start time\r\n        result = func(*args, **kwargs)  # ... run the function ...\r\n        t1 = time.perf_counter()        # end time\r\n        dt = t1-t0\r\n        print(""\\nTiming function for... {}\\n"".format(func.__name__))\r\n        print(""  Time: {: <8.2e}s for {:,} objects"".format(dt, len(result)))\r\n        return result                   # return the result of the function\r\n        return dt                       # return delta time\r\n    return wrapper\r\n\r\n\r\ndef fc_info(in_fc):\r\n    """"""Return basic featureclass information, including...\r\n    : SR - spatial reference object (use SR.name to get the name)\r\n    : shp_fld - field name which contains the geometry object\r\n    : oid_fld - the object index/id field name\r\n    : - others: \'areaFieldName\', \'baseName\', \'catalogPath\',\'featureType\',\r\n    :   \'fields\', \'hasOID\', \'hasM\', \'hasZ\', \'path\'\r\n    : - all_flds =[i.name for i in desc[\'fields\']]\r\n    """"""\r\n    desc = arcpy.da.Describe(in_fc)\r\n    args = [\'shapeFieldName\', \'OIDFieldName\', \'spatialReference\', \'shapeType\']\r\n    shp_fld, oid_fld, SR, shp_type = [desc[i] for i in args]\r\n    return shp_fld, oid_fld, SR, shp_type\r\n\r\n\r\ndef tweet(msg):\r\n    """"""Print a message for both arcpy and python.\r\n    : msg - a text message\r\n    """"""\r\n    m = ""\\n{}\\n"".format(msg)\r\n    arcpy.AddMessage(m)\r\n    print(m)\r\n    print(arcpy.GetMessages())\r\n\r\n\r\ndef flatten(a_list, flat_list=None):\r\n    """"""Change the isinstance as appropriate\r\n    :  Flatten an object using recursion\r\n    :  see: itertools.chain() for an alternate method of flattening.\r\n    """"""\r\n    if flat_list is None:\r\n        flat_list = []\r\n    for item in a_list:\r\n        if isinstance(item, list):\r\n            flatten(item, flat_list)\r\n        else:\r\n            flat_list.append(item)\r\n    return flat_list\r\n\r\n\r\n\r\ndef unpack(iterable, param=\'__iter__\'):\r\n    """"""Unpack an iterable based on the param(eter) condition using recursion.\r\n    :Notes:\r\n    : ---- see main docs for more information and options ----\r\n    : To produce an array from this, use the following after this is done.\r\n    :   out = np.array(xy).reshape(len(xy)//2, 2)\r\n    """"""\r\n    xy = []\r\n    for x in iterable:\r\n        if hasattr(x, \'__iter__\'):\r\n            xy.extend(unpack(x))\r\n        else:\r\n            xy.append(x)\r\n    return xy\r\n\r\n\r\n@time_deco\r\ndef get_geom(in_fc):\r\n    """"""just get the geometry object""""""\r\n    coords = [np.asarray(row[0].__geo_interface__[\'coordinates\'])\r\n              for row in arcpy.da.SearchCursor(in_fc, [\'SHAPE@\'])]  # shape@\r\n    # coords = [i.__geo_interface__[\'coordinates\'] for i in geoms]\r\n    return coords  # , g2\r\n\r\n@time_deco\r\ndef cursor_to_dicts(in_fc):  #cursor, field_names):\r\n    """"""use a searchcursor to get a list of a shape\'s attributes\r\n    :\r\n    :Reference:\r\n    :---------\r\n    :https://stackoverflow.com/questions/11869473/\r\n    :      loading-a-feature-class-in-a-list-using-arcpy-strange-behaviour\r\n    :      -of-searchcurso\r\n    def cursor_to_dicts(cursor, field_names):\r\n        for row in cursor:\r\n            row_dict = {}\r\n            for field in field_names:\r\n                val = row.getValue(field)\r\n                row_dict[field] = getattr(val, \'__geo_interface__\', val)\r\n            yield row_dict\r\n\r\n    fc = \'/path/to/fc\'\r\n    fields = [f.name for f in arcpy.ListFields(fc)]   # get field list\r\n    features = list(cursor_to_dicts(arcpy.SearchCursor(fc), fields))\r\n    :Useage:\r\n    :------\r\n    : flds = [f.name for f in arcpy.ListFields(fc)]  # get field list\r\n    : dct = list(cursor_to_dicts(arcpy.SearchCursor(fc), flds))\r\n    : \'Shape\': {\'coordinates\': [[[(300020.0, 5000000.0), ...snip...\r\n    :      (300020.0, 5000000.0)]]],\r\n    : \'type\': \'MultiPolygon\'}\r\n    : c[0][\'Shape\'][\'coordinates\']\r\n    :\r\n    : shps = [np.array(c[i][\'Shape\'][\'coordinates\']) for i in range(len(c))]\r\n    :---------------------------------------------------------\r\n    """"""\r\n    def yld(cursor, flds):\r\n        for row in cursor:\r\n            row_dict = {}\r\n            idx = row[0]\r\n            shp = row[1]\r\n            #if hasattr(val, \'__geo_interface__\'):\r\n            v = np.asarray(shp.__geo_interface__[\'coordinates\'])\r\n            row_dict[\'ID\'] = idx\r\n            row_dict[\'Shape\'] = v\r\n            yield row_dict\r\n\r\n    #flds = [f.name for f in arcpy.ListFields(in_fc)]   # get field list\r\n    flds = [\'OID@\', \'SHAPE@\']\r\n    cursor = arcpy.da.SearchCursor(in_fc, flds)\r\n    features = list(yld(cursor, flds))\r\n    return features\r\n\r\n\r\n\r\n\r\n@time_deco\r\ndef to_arr0(in_fc):\r\n    """"""Just get the geometry and id field\r\n    """"""\r\n    shp_fld, oid_fld, SR, shp_type = fc_info(in_fc)\r\n    flds = [\'OID@\', \'SHAPE@X\', \'SHAPE@Y\']\r\n    a = arcpy.da.FeatureClassToNumPyArray(in_fc,\r\n                                          field_names=flds,\r\n                                          spatial_reference=SR,\r\n                                          explode_to_points=True)\r\n    dt = [(\'Idx\', \'<i4\'), (\'Xs\', \'<f8\'), (\'Ys\', \'<f8\')]\r\n    a.dtype = dt\r\n    return a\r\n\r\n\r\n@time_deco\r\ndef to_arr1(in_fc, split_geom=True):\r\n    """"""Pulls out the geometry and index values from a featureclass.  Optionally\r\n    :  the full array can be split into an object array containing an array\r\n    :  of individual geometries.  Note, this is only useful for poly* objects\r\n    :\r\n    :Requires: numpy, arcpy and fc_info\r\n    :--------\r\n    :  in_fc - featureclass\r\n    :  split-geom - True, separate arrays are created for each geometry object\r\n    :\r\n    :Notes:\r\n    : - arcpy.da.SearchCursor(\r\n    :     in_fc, field_names, where_clause, spatial_reference,\r\n    :     explode_to_points, sql_clause=(None, None))\r\n    :-------------------------------------------------------------------------\r\n    """"""\r\n    shp_fld, oid_fld, SR, shp_type = fc_info(in_fc)\r\n#    flds = arcpy.ListFields(in_fc)\r\n#    fnames = [f.name for f in flds if f.type not in [\'OID\', \'Geometry\']]\r\n#    flds = [oid_fld, shp_fld]\r\n    g_flds = [\'OID@\', \'SHAPE@X\', \'SHAPE@Y\']  # \'SHAPE@\', SHAPE@Z etc\r\n    vals = []\r\n    with arcpy.da.SearchCursor(in_fc, g_flds, None, SR, True) as rows:\r\n        for row in rows:\r\n            vals.append(row)\r\n    del row, rows\r\n    # ---- construct the array ----\r\n    dt = [(\'Idx\', \'<i4\'), (\'Xs\', \'<f8\'), (\'Ys\', \'<f8\')]\r\n    a = np.array(vals, dtype=dt)\r\n    # ---- split out into an object array containing arrays of dt ----\r\n    if split_geom:\r\n        ids = np.unique(a[\'Idx\'])\r\n        w = np.where(np.diff(a[\'Idx\']))[0] + 1\r\n        a = np.split(a, w)  # a[[\'Xs\', \'Ys\']], w)\r\n        # dt = [(\'Idx\', \'<i4\'), (\'Shp\', \'O\')]\r\n        a = np.array([[ids[i], a[i][[\'Xs\', \'Ys\']]] for i in range(len(ids))])\r\n    return a\r\n\r\n\r\n@time_deco\r\ndef to_arr(in_fc, use_geo=False):\r\n    """"""Convert a featureclass to a structured or recarray using a searchcursor.\r\n    :\r\n    :Requires: import arcpy, numpy as np\r\n    :--------\r\n    : in_fc - featureclass\r\n    : use_geo - True .__geo_interface__\r\n    :         - list comprehension\r\n    :get the row information\r\n    : cycle through all geometries and get xy pairs\r\n    :\r\n    :References:\r\n    :----------\r\n    : - see the polygon, polyline etc classes in\r\n    :   C:\\ArcPro\\Resources\\ArcPy\\arcpy\\arcobjects\\geometry.py\r\n    """"""\r\n    shp_fld, oid_fld, SR, shp_type = fc_info(in_fc)  # get the base information\r\n    flds = arcpy.ListFields(in_fc)\r\n    fnames = [f.name for f in flds if f.type not in [\'OID\', \'Geometry\']]\r\n    geom_flds = [\'SHAPE@\', oid_fld] + fnames\r\n    flds = [shp_fld, oid_fld] + fnames\r\n    vals = []\r\n    geoms = []\r\n    coords = []\r\n    idx = flds.index(shp_fld)\r\n    with arcpy.da.SearchCursor(in_fc,\r\n                               field_names=geom_flds, where_clause=None,\r\n                               spatial_reference=SR,  explode_to_points=False,\r\n                               sql_clause=(None, None)) as rows:\r\n        for row in rows:\r\n            row = list(row)\r\n            geom = row.pop(idx)\r\n            vals.append(row)\r\n            geoms.append(geom)  # pop the geometry out\r\n            if use_geo:\r\n                xy = geom.__geo_interface__[\'coordinates\']\r\n            else:\r\n                xy = [np.array([(pt.X, pt.Y) for pt in arr if pt])\r\n                      for arr in geom]  # if pt else None\r\n            coords.append(np.asarray(xy))  # maybe dump the last as np.asarray\r\n            del row, geom, xy\r\n        del rows\r\n    return vals, coords, geoms\r\n\r\n\r\n@time_deco\r\ndef fc_sdf(in_fc, fields=None, sr=None, where_clause=None, sql_clause=None):\r\n    """"""Abbreviated version of featureclass to SpatialDataFrame\r\n    : in fileops.py... from_featureclass(filename, **kwargs)\r\n    : sdf - SpatialDataFrame is a class in\r\n    :     - arcgis.features._data.geodataset.geodataframe\r\n    :     - C:\\ArcPro\\bin\\Python\\envs\\arcgispro-py3\\lib\\site-packages\\\r\n            arcgis\\features\\_data\\geodataset.py\r\n    :\r\n    :  all_vals, vals, geoms = fc_sdf(in_fc)\r\n    :  p = all_vals[0][-1]\r\n    :  p0 = p.getPart(0)  # returns an array which you can cycle through\r\n    :\r\n    :Note:  These imports are at the top of the script\r\n    :  from arcgis.geometry import _types\r\n    :  from arcgis.features._data.geodataset import SpatialDataFrame as SDF\r\n    :  import pandas as pd\r\n    """"""\r\n    if not fields:\r\n        fields = [field.name for field in arcpy.ListFields(in_fc)\r\n                  if field.type not in [\'Geometry\']]\r\n    geom_fields = [\'SHAPE@\'] + fields\r\n    flds = [\'SHAPE\'] + fields\r\n    vals = []\r\n    all_vals = []\r\n    geoms = []\r\n    geom_idx = flds.index(\'SHAPE\')\r\n    with arcpy.da.SearchCursor(in_fc,\r\n                               field_names=geom_fields,\r\n                               where_clause=where_clause,\r\n                               sql_clause=sql_clause,\r\n                               spatial_reference=sr) as rows:\r\n\r\n        for row in rows:\r\n            all_vals.append(row)\r\n            row = list(row)\r\n            geoms.append(_types.Geometry(row.pop(geom_idx)))\r\n            vals.append(row)\r\n            del row\r\n        del rows\r\n    df = pd.DataFrame(data=vals, columns=fields)\r\n    sdf = SDF(data=df, geometry=geoms)\r\n    sdf.reset_index(drop=True, inplace=True)\r\n    del df\r\n    if sr is None:\r\n        sdf.sr = sr\r\n    else:\r\n        sdf.sr = sdf.geometry[0].spatialReference\r\n    rec = sdf.to_records()\r\n    shps = np.asarray([np.asarray(i[\'rings\']) for i in a[\'SHAPE\']])\r\n    return rec, shps, sdf, geoms  # df\r\n\r\n\r\ndef fc_array(in_fc, flds="""", allpnts=True):\r\n    """"""Convert a featureclass to an ndarray of attribute, with the geometry\r\n    :  removed.\r\n    :\r\n    :Requires:\r\n    :--------\r\n    : input_fc - featureclass/shapefile complete path\r\n    : flds     - """"  just oid and shape fields\r\n    :          - ""*"" all fields or\r\n    :          - [\'Field1\', \'Field2\', etc] for specific fields\r\n    : allpnts  - True/False\r\n    :          - True to explode the geometry to individual points\r\n    :          - False for the centroid of the geometry\r\n    :References:\r\n    :----------\r\n    :  FeatureClassToNumPyArray, ListFields for more information\r\n    """"""\r\n    out_flds = []\r\n    shp_fld, oid_fld, SR, shp_type = fc_info(in_fc)  # get the base information\r\n    fields = arcpy.ListFields(in_fc)       # all fields in the shapefile\r\n    if flds == """":                         # return just OID and Shape field\r\n        out_flds = [oid_fld, shp_fld]      # FID and Shape field required\r\n    elif flds == ""*"":                      # all fields\r\n        out_flds = [f.name for f in fields]\r\n    else:                                  # oid, shape and specific fields\r\n        out_flds = [oid_fld, shp_fld]\r\n        for f in fields:\r\n            if f.name in flds:\r\n                out_flds.append(f.name)\r\n    frmt = """"""Creating array from featureclass using fc_array...with...\r\n    {}\\nFields...{}\\nAll pnts...{}\\nSR...{}\r\n    """"""\r\n    frmt = dedent(frmt)\r\n    args = [in_fc, out_flds, allpnts, SR.name]\r\n    msg = frmt.format(*args)\r\n    tweet(msg)\r\n    a = arcpy.da.FeatureClassToNumPyArray(in_fc, out_flds, """", SR, allpnts)\r\n    # out it goes in array format\r\n    return a\r\n\r\n# =============================================================================\r\n# @time_deco\r\n# def to_arr1(geom):\r\n#     """"""making parts and rings\r\n#     """"""\r\n#     xy = []\r\n#     for pts in geom:\r\n#         sub = []\r\n#         for pt in pts:\r\n#             if pt:  # is not None:\r\n#                 sub.append([pt.X, pt.Y])\r\n#             else:\r\n#                 xy.append(sub)  #np.asarray(sub))\r\n#                 sub = []\r\n#         xy.append(sub)  #np.asarray(sub))\r\n#     return np.asarray(xy)\r\n# =============================================================================\r\n\r\ndef _demo():\r\n    """"""\r\n    : -\r\n    """"""\r\n    in_fc = r""C:\\Git_Dan\\a_Data\\testdata.gdb\\polygon""\r\n\r\n    vals, a0, geoms = to_arr(in_fc, use_geo=False)\r\n    _, a1, _ = to_arr(in_fc, use_geo=True)\r\n    # fc_array(in_fc, flds=""*"", allpnts=True)\r\n    a2 = to_arr0(in_fc)\r\n    a3 = np.array([to_arr1(g) for g in geoms])\r\n    sdf = SDF.from_featureclass(in_fc)\r\n    a4_rec = SDF.to_records(sdf)\r\n    a4_s = a4_rec[\'SHAPE\']\r\n    a4 = np.asarray([np.array(i[\'rings\']) for i in a4_s])\r\n    # a_rec1 = a_nd.view(np.recarray)\r\n    # print(""\\ndarray.... \\n{!r:}\\n\\nsdf..... \\n{!r:}\\n"".format(a_nd, sdf))\r\n    # print(""\\nrec0...... \\n{!r:}\\n\\nrec1.... \\n{!r:}\\n"".format(a_rec0, a_rec1))\r\n    return a0, a1, a2, a3, a4 #, a5\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n#    a_nd, sdf, a_rec0, a_rec1 = _demo()\r\n#    print(art.frmt_struct(a_nd, 1, True))\r\n#    print(art.frmt_struct(a_rec1, 1, True))\r\n\r\n    # pd.DataFrame.from_records(data, index=None, exclude=None, columns=None,\r\n    #                         coerce_float=False, nrows=None)\r\n\r\n    """"""\r\n    flds = a_nd.dtype.names\r\n    vals = a_nd.tolist()\r\n    df = pd.DataFrame(data=vals, columns=flds)\r\n    g = a_rec1.Shape\r\n    gg = [arcpy.Point(x,y) for x,y in g]\r\n    sdf2 = SDF(data=df, geometry=gg)  # SpatialDataFrame\r\n    #\r\n\r\n\r\n#    in_fc = r""C:\\Git_Dan\\a_Data\\testdata.gdb\\polygon""\r\n\r\n    geom_type = \'Polygon\'\r\n    row = None\r\n    geo = [json.loads(u[i][0].JSON)[\'rings\']  for i in range(len(u))]\r\n    attr = [u[i][1:]  for i in range(len(u))]\r\n    #geo = [json.loads(u[i][0].JSON)[\'rings\']  for i in range(len(u))]\r\n    # zz = {""type"" : geom_type, ""geometry"" : json.loads(z), ""attributes"":row}\r\n    # json.loads(u[0][0])\r\n    """"""\r\n\r\n    in_fc = r""C:\\Git_Dan\\a_Data\\testdata.gdb\\polygon""\r\n    #in_fc = r""C:\\Git_Dan\\a_Data\\arcpytools_demo.gdb\\Can_0_big_3""\r\n    #in_fc = r""C:\\Data\\Canada\\CAN_adm0.gdb\\CAN_0_sp""\r\n    #a, b, c = to_arr(in_fc)\r\n    """"""\r\n    in_fc\r\nOut[4]: \'C:\\\\Data\\\\Canada\\\\CAN_adm.gdb\\\\CAN_adm1\'\r\n%timeit to_arr(in_fc)\r\n2min 5s \xc2\xb1 2.33 s per loop (mean \xc2\xb1 std. dev. of 7 runs, 1 loop each)\r\n\r\n%timeit fc_sdf(in_fc)\r\n5min 53s \xc2\xb1 7.99 s per loop (mean \xc2\xb1 std. dev. of 7 runs, 1 loop each)\r\n\r\n%timeit -n1 -r1 to_arr(in_fc)  # with conversion to points\r\n1min 54s \xc2\xb1 0 ns per loop (mean \xc2\xb1 std. dev. of 1 run, 1 loop each)\r\n\r\n%timeit -n1 -r1 to_arr(in_fc)  # without conversion to points\r\n5.1 s \xc2\xb1 0 ns per loop (mean \xc2\xb1 std. dev. of 1 run, 1 loop each)\r\n""""""\r\n    """""" taken out of to_fc\r\n            desc = arcpy.da.Describe(in_fc)  # use the new da.Describe method\r\n        if hasattr(desc, \'areaFieldName\'):\r\n            afn = desc.areaFieldName\r\n            if afn in fields:\r\n                fields.remove(afn)\r\n        if hasattr(desc, \'lengthFieldName\'):\r\n            lfn = desc.lengthFieldName\r\n            if lfn in fields:\r\n                fields.remove(lfn)\r\n        del desc\r\n    """"""'"
all_scripts/polyline_demo.py,24,"b'# -*- coding: utf-8 -*-\r\n""""""\r\n:polyline_demo.py\r\n\r\n""""""\r\nimport numpy as np\r\nimport numpy.lib.recfunctions as rfn\r\nimport arcpy\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\r\n      \'float\': \'{: 0.2f}\'.format}\r\nnp.set_printoptions(edgeitems=3, linewidth=80, precision=3,\r\n                    suppress=True, threshold=100, formatter=ft)\r\n\r\n\r\ndef group_pnts(a, key_fld=\'ID\', keep_flds=[\'X\', \'Y\', \'Z\']):\r\n    """"""Group points for a feature that has been exploded to points by\r\n    :  arcpy.da.FeatureClassToNumPyArray.\r\n    :Requires:\r\n    :---------\r\n    : a - a structured array, assuming ID, X, Y, {Z} and whatever else\r\n    :   - the array is assumed to be sorted... which will be the case\r\n    :Returns:\r\n    :--------\r\n    : see np.unique descriptions below\r\n    :References:\r\n    :-----------\r\n    :  https://jakevdp.github.io/blog/2017/03/22/group-by-from-scratch/\r\n    :  http://esantorella.com/2016/06/16/groupby/\r\n    :Notes:\r\n    :------ split-apply-combine\r\n    """"""\r\n    returned = np.unique(a[key_fld],           # the unique id field\r\n                         return_index=True,    # first occurrence index\r\n                         return_inverse=True,  # indices needed to remake array\r\n                         return_counts=True)   # number in each group\r\n    uniq, idx, inv, cnt = returned\r\n    from_to = [[idx[i-1], idx[i]] for i in range(1, len(idx))]\r\n    subs = [a[keep_flds][i:j] for i, j in from_to]\r\n    groups = [sub.view(dtype=\'float\').reshape(sub.shape[0], -1)\r\n              for sub in subs]\r\n    return groups\r\n\r\n\r\ndef e_leng(a):\r\n    """"""Length/distance between points in an array using einsum\r\n    : Inputs\r\n    :   a list/array coordinate pairs, with ndim = 3 and the\r\n    :   Minimum shape = (1,2,2), eg. (1,4,2) for a single line of 4 pairs\r\n    :   The minimum input needed is a pair, a sequence of pairs can be used.\r\n    : Returns\r\n    :   d_arr  the distances between points forming the array\r\n    :   length the total length/distance formed by the points\r\n    :-----------------------------------------------------------------------\r\n    """"""\r\n    def cal(diff):\r\n        """""" perform the calculation\r\n        :diff = g[:, :, 0:-1] - g[:, :, 1:]\r\n        : for 4D\r\n        : d = np.einsum(\'ijk..., ijk...->ijk...\', diff, diff).flatten() or\r\n        :   = np.einsum(\'ijkl, ijkl->ijk\', diff, diff).flatten()\r\n        : d = np.sum(np.sqrt(d)\r\n        """"""\r\n        d_arr = np.sqrt(np.einsum(\'ijk,ijk->ij\', diff, diff))\r\n        d_leng = d_arr.flatten()\r\n        length = np.sum(d_leng)\r\n        return length, d_leng\r\n    # ----\r\n    diffs = []\r\n    a = np.atleast_2d(a)\r\n    if a.shape[0] == 1:\r\n        return 0.0\r\n    if a.ndim == 2:\r\n        a = np.reshape(a, (1,) + a.shape)\r\n    if a.ndim == 3:\r\n        diff = a[:, 0:-1] - a[:, 1:]\r\n        length, d_leng = cal(diff)\r\n        diffs.append(d_leng)\r\n    if a.ndim == 4:\r\n        length = 0.0\r\n        for i in range(a.shape[0]):\r\n            diff = a[i][:, 0:-1] - a[i][:, 1:]\r\n            leng, d_leng = cal(diff)\r\n            diffs.append(d_leng)\r\n            length += leng\r\n    return length, diffs\r\n\r\n\r\ndef report(subs):\r\n    """"""print out the data""""""\r\n    for i in range(len(subs)):\r\n        total, segs = e_leng(subs[i])\r\n        frmt = """"""\r\n        3D distances along polyline {}\r\n        Segment distances\r\n        {}\r\n        Total = sum of segments ? {}\r\n        """"""\r\n        print(frmt.format(total, segs, total == np.sum(segs)))\r\n\r\n\r\n# ---- inputs ---\r\nfc = r\'C:\\GIS\\Geometry_projects\\polyline_demo\\polyline_demo.gdb\\polylines\'\r\n\r\nflds = [\'OID@\', \'SHAPE@X\', \'SHAPE@Y\', \'SHAPE@Z\']\r\nSR = arcpy.Describe(fc).spatialreference\r\na = arcpy.da.FeatureClassToNumPyArray(fc,\r\n                                      field_names=flds,\r\n                                      spatial_reference=SR,\r\n                                      explode_to_points=True)\r\ndt = [(\'ID\', \'<i4\'), (\'X\', \'<f8\'), (\'Y\', \'<f8\'), (\'Z\', \'<f8\')]\r\na.dtype = dt                  # simplify the dtype\r\n      # get the unique id values\r\ngroups = group_pnts(a, key_fld=\'ID\', keep_flds=[\'X\', \'Y\', \'Z\'])\r\nreport(groups)  # print the results\r\n\r\n# ids = np.unique(a[\'ID\'])\r\n# p_lines = [a[a[\'ID\'] == i] for i in ids]                 # collect polylines\r\n# subs = [np.c_[p[\'X\'], p[\'Y\'], p[\'Z\']] for p in p_lines]  # stack coordinates\r\n""""""\r\n\r\nxyz_names = list(a.dtype.names[1:])\r\na[xyz_names]\r\nb = a[xyz_names].view(dtype=\'float\').reshape(a.shape[0], -1)\r\n\r\narray \'a\' must be sorted, which it will be when you explode a feature to\r\npoints\r\nuniq, idx, inv, cnt = np.unique(a[\'ID\'],             # the unique id field\r\n                                return_index=True,   # first occurrence index\r\n                                return_inverse=True, # indices needed to remake\r\n                                return_counts=True)  # number in each group\r\n\r\npnt_fc = r\'C:\\GIS\\Geometry_projects\\polyline_demo\\polyline_demo.gdb\\pnts\'\r\ndz = np.array([ln[-1][\'Z\'] - ln[0][\'Z\'] for ln in lines])\r\ndx = np.array([ln[-1][\'X\'] - ln[0][\'X\'] for ln in lines])\r\ndy = np.array([ln[-1][\'Y\'] - ln[0][\'Y\'] for ln in lines])\r\ndist = np.sqrt(dx**2 + dy**2)\r\nprint(""2D distances first point to last point \\n{}"".format(dist))\r\n\r\ndt = [(\'ID\', \'<i8\'), (\'X\', \'<f8\'), (\'Y\', \'<f8\'), (\'Z\', \'<f8\')]\r\nids = np.array([1,1,1,1,2,2,2,2])\r\nx_s = np.array([0,1,2,3,3,2,1,0])\r\ny_s = np.array([0,1,2,3,3,2,1,0])\r\nz_s = np.arange(8, dtype=\'float\')\r\na = np.zeros((8,), dtype=dt)\r\na[\'ID\'] = ids\r\na[\'X\'] = x_s\r\na[\'Y\'] = y_s\r\na[\'Z\'] = z_s\r\nids = np.unique(ids)\r\nlines = [a[a[\'ID\'] == i] for i in ids]\r\n\r\n""""""'"
all_scripts/py_tools.py,12,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\npy_tools\r\n========\r\n\r\nScript :   py_tools.py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified: 2018-10-15\r\n\r\n-------\r\n\r\nPurpose : tools for working with python, numpy and other python packages\r\n\r\n- iterables :\r\n    _flatten, flatten_shape, pack, unpack\r\n- folders :\r\n    get_dir, folders, sub-folders, dir_py\r\nUseage:\r\n\r\nReferences:\r\n\r\n------------------------------------------------------------------------------\r\n""""""\r\n# pylint: disable=C0103\r\n# pylint: disable=R1710\r\n# pylint: disable=R0914\r\n\r\n# ---- imports, formats, constants ---------------------------------------\r\nimport sys\r\nimport os\r\nfrom textwrap import dedent\r\nimport numpy as np\r\n\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\n__all__ = [\'comp_info\',\r\n           \'get_dir\', \'folders\', \'sub_folders\',  # basic folder functions\r\n           \'dir_py\',    # object and directory functions\r\n           \'_flatten\', \'flatten_shape\',  # iterables\r\n           \'pack\', \'unpack\',\r\n           \'combine_dicts\']\r\n\r\n\r\n# ---- (1) computer, python stuff ... code section ---------------------------\r\n#\r\ndef comp_info():\r\n    """"""Return information for the computer and python version\r\n    """"""\r\n    import platform\r\n    winv = platform.platform()\r\n    py_ver = platform.python_version()\r\n    plat = platform.architecture()\r\n    proc = platform.processor()\r\n    p_node = platform._node()\r\n    u_name = platform.uname()\r\n    ud = u_name._asdict()\r\n    udl = list(zip(ud.keys(), ud.values()))\r\n    frmt = """"""\r\n    ---------------------------\r\n    Computer/python information\r\n\r\n    Platform:        {}\r\n    python version:  {}\r\n    windows version: {}\r\n    processor:       {}\r\n    node:            {}\r\n    user/machine:    {}\\n\r\n    Alternate form....""""""\r\n    args = [winv, py_ver, plat, proc, p_node, u_name]\r\n    print(dedent(frmt).format(*args))\r\n    print(""\\n"".join([""{:<10}: {}"".format(*i) for i in udl]))\r\n\r\n\r\n# ---- (2) general file functions ... code section ---------------------------\r\n#\r\ndef get_dir(path):\r\n    """"""Get the directory list from a path, excluding geodatabase folders.\r\n    Used by.. folders\r\n\r\n    >>> get_dir(\'C:/Git_Dan/arraytools\')\r\n    [\'C:/Git_Dan/arraytools/.spyproject\',\r\n     \'C:/Git_Dan/arraytools/analysis\',\r\n     ... snip ...\r\n     \'C:/Git_Dan/arraytools/__pycache__\']\r\n    >>> # ---- common path prefix\r\n    >>> os.path.commonprefix(get_dir(\'C:/Git_Dan/arraytools\'))\r\n    \'C:/Git_Dan/arraytools/\'\r\n    """"""\r\n    if os.path.isfile(path):\r\n        path = os.path.dirname(path)\r\n    p = os.path.normpath(path)\r\n    full = [os.path.join(p, v) for v in os.listdir(p)]\r\n    dirlist = [val for val in full if os.path.isdir(val)]\r\n    return dirlist\r\n\r\n\r\ndef folders(path, first=True, prefix=""""):\r\n    """""" Print recursive listing of folders in a path.  Make sure you `raw`\r\n    format the path...\r\n    ::\r\n        r\'c:\\Temp\'  or \'c:/Temp\' or \'c:\\\\Temp\'\r\n\r\n    - Requires : _get_dir .... also, an example of path common prefix\r\n    """"""\r\n    if first:  # Detect outermost call, print a heading\r\n        print(""-""*30 + ""\\n|.... Folder listing for ....|\\n|--{}"".format(path))\r\n        prefix = ""|-""\r\n        first = False\r\n        cprev = path\r\n    dirlist = get_dir(path)\r\n    for d in dirlist:\r\n        fullname = os.path.join(path, d)  # Turn name into full pathname\r\n        if os.path.isdir(fullname):       # If a directory, recurse.\r\n            cprev = path\r\n            pad = \' \' * len(cprev)\r\n            n = d.replace(cprev, pad)\r\n            print(prefix + ""-"" + n)  # fullname) # os.path.relpath(fullname))\r\n            p = ""  ""\r\n            folders(fullname, first=False, prefix=p)\r\n    # ----\r\n\r\n\r\ndef sub_folders(path, combine=False):\r\n    """"""Print the folders in a path, excluding \'.\' folders\r\n    This is the best one.\r\n    """"""\r\n    import pathlib\r\n    print(""Path...\\n{}"".format(path))\r\n    if combine:\r\n        r = "" ""*len(path)\r\n    else:\r\n        r = """"\r\n    f = ""\\n"".join([(p._str).replace(path, r)\r\n                   for p in pathlib.Path(path).iterdir()\r\n                   if p.is_dir() and ""."" not in p._str])\r\n    print(""{}"".format(f))\r\n\r\n\r\ndef env_list(pth, ordered=False):\r\n    """"""List folders and files in a path\r\n    """"""\r\n    import os\r\n    d = []\r\n    for item in os.listdir(pth):\r\n        check = os.path.join(pth, item)\r\n        check = check.replace(""\\\\"", ""/"")\r\n        if os.path.isdir(check) and (""."" not in check):\r\n            d.append(check)\r\n    d = np.array(d)\r\n    if ordered:\r\n        d = d[np.argsort(d)]\r\n    return d\r\n\r\n\r\n# ---- (3) dirr ... code section ... -----------------------------------------\r\n#\r\ndef dir_py(obj, colwise=False, cols=4, prn=True):\r\n    """"""The non-numpy version of dirr\r\n    """"""\r\n    from itertools import zip_longest as zl\r\n    a = dir(obj)\r\n    w = max([len(i) for i in a])\r\n    frmt = ((""{{!s:<{}}} "".format(w)))*cols\r\n    csze = len(a) / cols  # split it\r\n    csze = int(csze) + (csze % 1 > 0)\r\n    if colwise:\r\n        a_0 = [a[i: i+csze] for i in range(0, len(a), csze)]\r\n        a_0 = list(zl(*a_0, fillvalue=""""))\r\n    else:\r\n        a_0 = [a[i: i+cols] for i in range(0, len(a), cols)]\r\n    if hasattr(obj, \'__name__\'):\r\n        args = [""-""*70, obj.__name__, obj]\r\n    else:\r\n        args = [""-""*70, type(obj), ""py version""]\r\n    txt_out = ""\\n{}\\n| dir({}) ...\\n|    {}\\n-------"".format(*args)\r\n    cnt = 0\r\n    for i in a_0:\r\n        cnt += 1\r\n        txt = ""\\n  ({:>03.0f})  "".format(cnt)\r\n        frmt = ((""{{!s:<{}}} "".format(w)))*len(i)\r\n        txt += frmt.format(*i)\r\n        txt_out += txt\r\n    if prn:\r\n        print(txt_out)\r\n    else:\r\n        return txt_out\r\n\r\n\r\n# ---- (4) iterables ---------------------------------------------------------\r\n#\r\ndef _flatten(a_list, flat_list=None):\r\n    """"""Change the isinstance as appropriate.\r\n\r\n    Flatten an object using recursion\r\n\r\n    see: itertools.chain() for an alternate method of flattening.\r\n    """"""\r\n    if flat_list is None:\r\n        flat_list = []\r\n    for item in a_list:\r\n        if hasattr(item, \'__iter__\'):\r\n            _flatten(item, flat_list)\r\n        else:\r\n            flat_list.append(item)\r\n    return flat_list\r\n\r\n\r\ndef flatten_shape(shp, completely=False):\r\n    """"""Flatten a array or geometry shape object using itertools.\r\n\r\n    Parameters:\r\n    -----------\r\n\r\n    shp :\r\n       an array or an array representing polygon, polyline, or point shapes\r\n    completely :\r\n       True returns points for all objects\r\n       False, returns Array for polygon or polyline objects\r\n\r\n    Notes:\r\n    ------\r\n    - for conventional array-like objects use `completely = False` to flatten\r\n      the object completely.\r\n    - for geometry objects, use `True` for polygon and polylines to retain their\r\n      parts, but for points, use `False` since you need to retain the x,y pair\r\n    - `__iter__` property: Polygon, Polyline, Array all have this property...\r\n      Points do not.\r\n    """"""\r\n    import itertools\r\n    if completely:\r\n        vals = [i for i in itertools.chain(shp)]\r\n    else:\r\n        vals = [i for i in itertools.chain.from_iterable(shp)]\r\n    return vals\r\n\r\n\r\ndef pack(a, param=\'__iter__\'):\r\n    """"""Pack an iterable into an ndarray or object array\r\n    """"""\r\n    if not hasattr(a, param):\r\n        return a\r\n    return np.asarray([np.asarray(i) for i in a])\r\n\r\n\r\ndef unpack(iterable, param=\'__iter__\'):\r\n    """"""Unpack an iterable based on the param(eter) condition using recursion.\r\n\r\n    Notes:\r\n    ------\r\n    - Use `flatten` for recarrays or structured arrays.\r\n    - See main docs for more information and options.\r\n    - To produce uniform array from this, use the following after this is done.\r\n    >>> out = np.array(xy).reshape(len(xy)//2, 2)\r\n\r\n    - To check whether unpack can be used.\r\n    >>> isinstance(x, (list, tuple, np.ndarray, np.void)) like in flatten above\r\n    """"""\r\n    xy = []\r\n    for x in iterable:\r\n        if hasattr(x, param):\r\n            xy.extend(unpack(x))\r\n        else:\r\n            xy.append(x)\r\n    return xy\r\n\r\ndef combine_dicts(ds):\r\n    """"""Combine dictionary values from multiple dictionaries and combine\r\n    their keys if needed.\r\n    Requires: import numpy as np\r\n    Returns: a new dictionary\r\n    """"""\r\n    a = np.array([(k, v)                 # key, value pairs\r\n                  for d in ds            # dict in dictionaries\r\n                  for k, v in d.items()  # get the key, values from items\r\n                  ])\r\n    ks, idx = np.unique(a[:, 0], True)\r\n    ks = ks[np.lexsort((ks, idx))]       # optional sort by appearance\r\n    uniq = [np.unique(a[a[:, 0] == i][:, 1]) for i in ks]\r\n    nd = ["" "".join(u.tolist()) for u in uniq]\r\n    new_d = dict(zip(ks, nd))\r\n    return new_d\r\n\r\n\r\ndef find_dups(a_list):\r\n    """"""Find dups in a list using an Ordered dictionary, return a list of\r\n    duplicated elements\r\n    """"""\r\n    from collections import OrderedDict\r\n    counter = OrderedDict()\r\n    for item in a_list:\r\n        if item in counter:\r\n            counter[item] += 1\r\n        else:\r\n            counter[item] = 1\r\n    return [item for item, counts in counter.items() if counts > 1]\r\n# ---- (5) demos  -------------------------------------------------------------\r\n#\r\n\r\n# ----------------------------------------------------------------------------\r\n# ---- __main__ .... code section --------------------------------------------\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n#    _demo()\r\n'"
all_scripts/query_reclass.py,7,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   query_reclass.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2017-11-16\r\n:Purpose:  tools for working with numpy arrays\r\n:Useage:  Used in arctoolbox for \'like\' sub-query style queries\r\n: - create a field in a geodatabase table and calculates values based on\r\n:   conditions being met.  Essentiall enables a reclassification into a new\r\n:   field.\r\n:\r\n:References:\r\n:----------\r\n: - SQL\r\n:   http://pro.arcgis.com/en/pro-app/help/mapping/navigation/\r\n:        sql-reference-for-elements-used-in-query-expressions.\r\n:        htm#GUID-68D21843-5274-4AF4-B7F3-165892232A43\r\n: - TableSelect\r\n:   http://pro.arcgis.com/en/pro-app/tool-reference/analysis/table-select.htm\r\n:   arcpy.analysis.TableSelect(""xy1000"", path_to_table,\r\n:                              ""Address_ LIKE \'%Street%\'"")\r\n: - CalculateField\r\n:   http://pro.arcgis.com/en/pro-app/tool-reference/data-management/\r\n:        calculate-field.htm\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nfrom textwrap import dedent\r\nimport numpy as np\r\nimport arcpy\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\r\n      \'float\': \'{: 0.3f}\'.format}\r\n\r\nnp.set_printoptions(edgeitems=5, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\n# ------------------------------------------------------------------------\r\n# ---- functions ----\r\ndef tweet(msg):\r\n    """"""Print a message for both arcpy and python.\r\n    : msg - a text message\r\n    """"""\r\n    m = ""{}"".format(msg)\r\n    arcpy.AddMessage(m)\r\n    print(m)\r\n    print(arcpy.GetMessages())\r\n\r\n\r\ndef _demo():\r\n    """"""run when script is standalone""""""\r\n    in_fc = r\'C:\\Git_Dan\\a_Data\\testdata.gdb\\xy1000\'\r\n    flds = [\'OID@\', \'Address_\']\r\n    in_fld = \'Address_\'\r\n    out_fld = \'Str_class\'\r\n    from_s = [\'Street\', \'Lane\', \'Court\']  # partial set\r\n    to_s = [10, 20, 30]\r\n    dt = [(\'IDs\', \'<i4\'), (\'Str_class\', \'<i4\')]\r\n    testing = True\r\n    return in_fc, flds, in_fld, out_fld, from_s, to_s, dt, testing\r\n\r\n\r\ndef _tool():\r\n    """"""run when script is from a tool""""""\r\n    in_fc = sys.argv[1]\r\n    in_fld = sys.argv[2]\r\n    out_fld = sys.argv[3]\r\n    from_s = sys.argv[4].split("","")\r\n    to_s = sys.argv[5].split("","")\r\n    testing = False\r\n    return in_fc, in_fld, out_fld, from_s, to_s, testing\r\n\r\n\r\n# ------------------------------------------------------------------------\r\n# (1) ---- Checks to see if running in test mode or from a tool ----------\r\nif len(sys.argv) == 1:\r\n    in_fc, flds, in_fld, out_fld, from_s, to_s, dt, testing = _demo()\r\nelse:\r\n    in_fc, in_fld, out_fld, from_s, to_s, testing = _tool()\r\n\r\n#\r\n# ------------------------------------------------------------------------\r\n# (2) ---- Create the array from the cursor, print inputs\r\n#\r\ndesc = arcpy.da.Describe(in_fc)\r\ntbl_path = desc[\'path\']\r\nfnames = [i.name for i in arcpy.ListFields(in_fc)]\r\nif out_fld in fnames:\r\n    out_fld += \'_dupl\'\r\nout_fld = arcpy.ValidateFieldName(out_fld)\r\nflds = [\'OBJECTID\', in_fld]\r\nargs = [in_fc, flds, None, None, False, (None, None)]\r\ncur = arcpy.da.SearchCursor(*args)\r\na = cur._as_narray()\r\n# ----\r\nargs = [""-""*60, in_fc, in_fld, out_fld, from_s, to_s, a]\r\nfrmt = """"""\r\n{}\\nInput table:  {}\\nin_fld:   {}\\nout_fld:  {}\\n\r\nFrom values  {}\\nTo values    {}\r\nInput array...\r\n{!r:}""""""\r\nmsg = frmt.format(*args)\r\ntweet(msg)\r\n#\r\n# ------------------------------------------------------------------------\r\n# (3) ----  Check the output dtype and produce the empty array -----------\r\n#\r\nto_s = np.asarray(to_s)\r\ndt_2s = to_s.dtype.str\r\ndt_k = to_s.dtype.kind\r\ndt = [(\'IDs\', \'<i4\'), (out_fld, dt_2s)]\r\nout = np.zeros((len(a),), dtype=dt)\r\nif dt_k in (\'i\', \'I\', \'l\', \'L\'):\r\n    fill_v = -9\r\nelif dt_k in (\'U\', \'S\'):\r\n    fill_v = None\r\nelse:\r\n    fill_v = np.nan\r\nout[out_fld].fill(fill_v)\r\nout[\'IDs\'] = np.arange(1, len(a) + 1, dtype=\'int32\')\r\ncnt = 0\r\nfor f in from_s:\r\n    idx = np.array([i for i, item in enumerate(a[in_fld]) if f in item])\r\n    out[out_fld][idx] = to_s[cnt]\r\n    cnt += 1\r\n#\r\n# ------------------------------------------------------------------------\r\n# (4) ---- Do the table joining ------------------------------------------\r\nif testing:\r\n    tweet(""Output array...\\n{!r:}"".format(out.reshape(out.shape[0], -1)))\r\nelse:\r\n    arcpy.da.ExtendTable(in_fc, \'OBJECTID\', out, \'OBJECTID\')\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n#    in_fc, flds, in_fld, out_fld, from_s, to_s, dt, testing = _demo()\r\n'"
all_scripts/radial_sort.py,23,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   radial_sort.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2018-02-27\r\n:Purpose:  tools for working with numpy arrays\r\n:Useage:\r\n:\r\n:References:\r\n:\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport os\r\nfrom textwrap import dedent\r\nimport numpy as np\r\nimport arcpy\r\nfrom arcpytools import _describe, fc_info, tweet\r\nimport warnings\r\n\r\nwarnings.simplefilter(\'ignore\', FutureWarning)\r\n\r\narcpy.env.overwriteOutput = True\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n# -------------------------------------------------------------------------\r\n\r\nmsg0 = """"""\r\n: -----------------------------------------------------------\r\nScript....\r\n.... {}\r\nNo output produced because either...\r\n - The input and/or output path and/or filename has a space in it.\r\n - The output path was no good.\r\n - A projected coordinate system is required for the inputs\r\nCopy and run with locally stored data or fix one or more conditions...\r\n...\r\n: -----------------------------------------------------------\r\n""""""\r\n\r\nmsg1 = """"""\r\n: -----------------------------------------------------------\r\nScript....\r\n.... {}\r\nCompleted....\r\n...\r\n: -----------------------------------------------------------\r\n""""""\r\n\r\n\r\ndef check_files(file_path, ext=""""):\r\n    """"""Check expected file paths and extensions, to ensure compliance with\r\n    :  tool specifications\r\n    """"""\r\n    is_good = True\r\n    head, tail = os.path.split(file_path)\r\n    if not os.path.exists(head):\r\n        return False\r\n    if "" "" in tail:\r\n        return False\r\n    if os.path.splitext(tail)[1] != ext:\r\n        return False\r\n    if "" "" in file_path:\r\n        return False\r\n    return is_good\r\n    # ----\r\n\r\n\r\ndef extent_cent(in_fc):\r\n    """"""Some basic featureclass properties\r\n    """"""\r\n    ext = arcpy.Describe(in_fc).extent\r\n    ext_poly = ext.polygon\r\n    cent = ext_poly.centroid\r\n    return cent\r\n\r\n\r\ndef _xyID(in_fc, to_pnts=True):\r\n    """"""Convert featureclass geometry (in_fc) to a simple 2D structured array\r\n    :  with ID, X, Y values. Optionally convert to points, otherwise centroid.\r\n    """"""\r\n    flds = [\'OID@\', \'SHAPE@X\', \'SHAPE@Y\']\r\n    args = [in_fc, flds, None, None, to_pnts, (None, None)]\r\n    cur = arcpy.da.SearchCursor(*args)\r\n    a = cur._as_narray()\r\n    a.dtype = [(\'IDs\', \'<i4\'), (\'Xs\', \'<f8\'), (\'Ys\', \'<f8\')]\r\n    del cur\r\n    return a\r\n\r\n\r\ndef _center(a, remove_dup=True):\r\n    """"""Return the center of an array. If the array represents a polygon, then\r\n    :  a check is made for the duplicate first and last point to remove one.\r\n    """"""\r\n    if remove_dup:\r\n        if np.all(a[0] == a[-1]):\r\n            a = a[:-1]\r\n    return a.mean(axis=0)\r\n\r\n\r\ndef e_dist(a, b, metric=\'euclidean\'):\r\n    """"""Distance calculation for 1D, 2D and 3D points using einsum\r\n    : a, b   - list, tuple, array in 1,2 or 3D form\r\n    : metric - euclidean (\'e\',\'eu\'...), sqeuclidean (\'s\',\'sq\'...),\r\n    :-----------------------------------------------------------------------\r\n    """"""\r\n    a = np.asarray(a)\r\n    b = np.atleast_2d(b)\r\n    a_dim = a.ndim\r\n    b_dim = b.ndim\r\n    if a_dim == 1:\r\n        a = a.reshape(1, 1, a.shape[0])\r\n    if a_dim >= 2:\r\n        a = a.reshape(np.prod(a.shape[:-1]), 1, a.shape[-1])\r\n    if b_dim > 2:\r\n        b = b.reshape(np.prod(b.shape[:-1]), b.shape[-1])\r\n    diff = a - b\r\n    dist_arr = np.einsum(\'ijk,ijk->ij\', diff, diff)\r\n    if metric[:1] == \'e\':\r\n        dist_arr = np.sqrt(dist_arr)\r\n    dist_arr = np.squeeze(dist_arr)\r\n    return dist_arr\r\n\r\n\r\ndef radial_sort(pnts, cent=None):\r\n    """"""Sort about the point cloud center or from a given point\r\n    : pnts - an array of points (x,y) as array or list\r\n    : cent - list, tuple, array of the center\'s x,y coordinates\r\n    :      - cent = [0, 0] or np.array([0, 0])\r\n    :Returns: the angles in the range -180, 180 x-axis oriented\r\n    """"""\r\n    pnts = np.asarray(pnts, dtype=np.float64)\r\n    if cent is None:\r\n        cent = _center(pnts, remove_dup=False)\r\n    ba = pnts - cent\r\n    ang_ab = np.arctan2(ba[:, 1], ba[:, 0])\r\n    ang_ab = np.degrees(ang_ab)\r\n    sort_order = np.argsort(ang_ab)\r\n    return ang_ab, sort_order\r\n\r\n\r\ndef output_points(out_fc, pnts):\r\n    """"""Produce the output point featureclass""""""\r\n    msg = \'\\nRead the script header... A projected coordinate system required\'\r\n    assert (SR is not None), msg  # and (SR.type == \'Projected\'), msg\r\n    pnts_lst = []\r\n    for pnt in pnts:                 # create the point geometry\r\n        pnts_lst.append(arcpy.PointGeometry(arcpy.Point(*pnt), SR))\r\n    if arcpy.Exists(out_fc):     # overwrite any existing versions\r\n        arcpy.Delete_management(out_fc)\r\n    arcpy.CopyFeatures_management(pnts_lst, out_fc)\r\n    return out_fc\r\n\r\n\r\ndef output_polylines(out_fc, SR, pnts):\r\n    """"""Produce the output polyline featureclass""""""\r\n    msg = \'\\nRead the script header... A projected coordinate system required\'\r\n    assert (SR is not None), msg  # and (SR.type == \'Projected\'), msg\r\n    polylines = []\r\n    for pair in pnts:                 # create the polyline geometry\r\n        pl = arcpy.Polyline(arcpy.Array([arcpy.Point(*xy) for xy in pair]), SR)\r\n        polylines.append(pl)\r\n    if arcpy.Exists(out_fc):     # overwrite any existing versions\r\n        arcpy.Delete_management(out_fc)\r\n    arcpy.CopyFeatures_management(polylines, out_fc)\r\n    return out_fc\r\n\r\n\r\ndef test_envs(in_fc, cent, out_fc0, out_fc1):\r\n    """""" test the required parameters\r\n    """"""\r\n    # (1) ---- check input feature and for projected data\r\n    if not arcpy.Exists(in_fc):\r\n        tweet(""\\nThis file doesn\'t exist.\\n"")\r\n        return False, []\r\n    shp_fld, oid_fld, shp_type, SR = fc_info(in_fc)\r\n    #\r\n    if SR.type != \'Projected\':\r\n        tweet(""\\nRadial sorts only make sense for projected data.\\n"")\r\n        return False, []\r\n    if shp_type != \'Point\':\r\n        tweet(""\\nYou need a point file.\\n"")\r\n        return False, []\r\n    #\r\n    # (2) ---- check the output files\r\n    if out_fc0 != ""#"":\r\n        is_good = check_files(out_fc0)\r\n        if not is_good:\r\n            tweet(""\\nWrong path or filename?....{}\\n"".format(out_fc0))\r\n            return False, []\r\n    if out_fc1 != ""#"":\r\n        is_good = check_files(out_fc1)\r\n        if not is_good:\r\n            tweet(""\\nWrong path or filename?....{}\\n"".format(out_fc1))\r\n            return False, []\r\n    #\r\n    # (3) check the center ....\r\n    if cent in (None, \'None\', "" "", """", ""#""):\r\n        cent = None\r\n    elif isinstance(cent, str):\r\n        for i in ["", "", "","", "";""]:\r\n            cent = cent.replace(i, "" "")\r\n        try:\r\n            cent = [float(i.strip()) for i in cent.split("" "")]\r\n            if len(cent) != 2:\r\n                cent = [cent[0], cent[0]]\r\n                tweet(""\\nBad center so I used... {} instead \\n"".format(cent))\r\n        except ValueError:\r\n            cent = None\r\n            tweet(""\\nCenter used... {}\\n"".format(cent))\r\n    # (4) all should be good\r\n    return True, [out_fc0, out_fc1, cent, SR]\r\n\r\n\r\ndef _tool():\r\n    """"""run when script is from a tool\r\n    """"""\r\n    in_fc = sys.argv[1]\r\n    cent = str(sys.argv[2])\r\n    from_north = str(sys.argv[3])\r\n    out_fc0 = sys.argv[4]\r\n    out_fc1 = sys.argv[5]\r\n    return in_fc, from_north, cent, out_fc0, out_fc1\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# .... running script or testing code section\r\n\r\ngdb_pth = ""/"".join(script.split(""/"")[:-2]) + ""/Data/Point_tools.gdb""\r\n\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    in_fc = gdb_pth + r""/radial_pnts""\r\n    cent = None\r\n    out_fc0 = gdb_pth + r""/radial""\r\n    out_fc1 = gdb_pth + r""/OD_01""\r\nelse:\r\n    testing = False\r\n    in_fc, from_north, cent, out_fc0, out_fc1 = _tool()\r\n\r\n#\r\n# (1) Run the test to see whether to continue\r\n#\r\nresults = test_envs(in_fc, cent, out_fc0, out_fc1)\r\ncleared, vals = results\r\n#\r\nif not cleared:\r\n    tweet(dedent(msg0).format(script))\r\nelse:\r\n    print(""\\nPassed all checks-------------"")\r\n    #\r\n    # ---- Process section ------------------------------\r\n    #\r\n    pnts_out, plys_out, cent, SR = vals\r\n    desc = _describe(in_fc)\r\n    arcpy.env.workspace = desc[\'path\']  # set the workspace to the gdb\r\n    arr = _xyID(in_fc, to_pnts=True)\r\n    indx = arr[\'IDs\']\r\n    pnts = arr[[\'Xs\', \'Ys\']]\r\n    pnts = pnts.view(np.float64).reshape(pnts.shape[0], 2)\r\n    if cent is None:\r\n        cent = np.mean(pnts, axis=0).tolist()\r\n    #\r\n    # (2) perform the radial sort ....\r\n    #\r\n    ang_ab, sort_order = radial_sort(pnts, cent=cent)  # angles and sort_order\r\n\r\n    # indx_sorted = indx[sort_order]\r\n    pnts_sorted = pnts[sort_order]\r\n    ang_sorted = ang_ab[sort_order]\r\n\r\n    dist_arr = e_dist(cent, pnts_sorted)\r\n    dist_indx = np.argsort(dist_arr)\r\n    dist_id = sort_order[dist_indx]  # indx[dist_indx]\r\n\r\n    pairs = [np.asarray([cent, pnt]) for pnt in pnts_sorted]\r\n\r\n    # ---- form the output results for use with extend table\r\n    #\r\n    dt = [(\'IDcent\', \'<i4\'), (\'Xp\', \'<f8\'), (\'Yp\', \'<f8\'), (\'Angle_\', \'<f8\'),\r\n          (\'Dist_\', \'<f8\'), (\'Orig_ID\', \'<i4\')]\r\n    Xsort = pnts_sorted[:, 0]\r\n    Ysort = pnts_sorted[:, 1]\r\n    ang_id = sort_order + 1  # the centroid ID for the sorted IDs\r\n    ext_tbl = np.empty(arr.shape, dtype=dt)\r\n    nms = ext_tbl.dtype.names\r\n    new_oid = np.arange(1, arr.shape[0]+1)  # to match OBJECTID values\r\n    vals = [new_oid, Xsort, Ysort, ang_sorted, dist_arr, ang_id]\r\n    for i in range(len(nms)):\r\n        ext_tbl[nms[i]] = vals[i]\r\n        ext_tbl2 = np.copy(ext_tbl)\r\n    # ---- create the output point file\r\n    tweet(""plys etc out {}, {}"".format(pnts_out, plys_out))\r\n    if pnts_out != ""#"":\r\n        output_points(out_fc0, pnts_sorted.tolist())\r\n        arcpy.da.ExtendTable(out_fc0, \'OBJECTID\', ext_tbl, \'IDcent\')\r\n    if plys_out != ""#"":\r\n        output_polylines(out_fc1, SR, pairs)\r\n        arcpy.da.ExtendTable(out_fc1, \'OBJECTID\', ext_tbl2, \'IDcent\')\r\nif not testing:\r\n    tweet(\'\\nDone....\')\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n#    _demo()\r\n'"
all_scripts/random_data_demo.py,44,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   random_data_demo.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2017-07-07\r\n:Purpose:\r\n:  Generate an array containing random data.  Optional fields include:\r\n:  ID,Shape, text, integer and float fields\r\n:Functions:\r\n:  colrow_txt, rowcol_txt, rand_text, rand_str, rand_case, rand_int,\r\n:  rand_float, pnts_IdShape\r\n:Requires:\r\n:  required imports\r\n:  required constants\r\n:Shape dtype options:\r\n:  dt_sub = np.dtype([(\'X\',\'<f8\'),(\'Y\',\'<f8\')]) # data type for X,Y fields\r\n:  dt = np.dtype([(\'ID\',\'<i4\'),(\'Shape\',dt_sub)])\r\n:\r\n:-To delete namespace use the following after unwrapping...\r\n: del [ __name__, __doc__, a, blog_post, colrow_txt, func_run, main,\r\n: pnts_IdShape, rand_case, rand_float, rand_int, rand_str, rand_text,\r\n: rfn, rowcol_txt, str_opt, str_opt, wraps]\r\n""""""\r\n\r\n# -----------------------------------------------------------------------------\r\n# Required imports\r\nimport arcpy\r\nfrom functools import wraps\r\nimport numpy as np\r\nimport numpy.lib.recfunctions as rfn\r\nnp.set_printoptions(edgeitems=5, linewidth=75, precision=2,\r\n                    suppress=True, threshold=5,\r\n                    formatter={\'bool\': lambda x: repr(x.astype(\'int32\')),\r\n                               \'float\': \'{: 0.2f}\'.format})\r\n\r\n# -----------------------------------------------------------------------------\r\n# Required constants  ... see string module for others\r\nstr_opt = [\'0123456789\',\r\n           \'!""#$%&\\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\',\r\n           \'abcdefghijklmnopqrstuvwxyz\',\r\n           \'ABCDEFGHIJKLMNOPQRSTUVWXYZ\',\r\n           \'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\'\r\n           ]\r\n# -----------------------------------------------------------------------------\r\n# decorator\r\n\r\ndef func_run(func):\r\n    """"""Prints basic function information and the results of a run.\r\n    :Required:  from functools import wraps\r\n    """"""\r\n    @wraps(func)\r\n    def wrapper(*args, **kwargs):\r\n        print(""\\nFunction... {}"".format(func.__name__))\r\n        print(""  args.... {}\\n  kwargs.. {}"".format(args, kwargs))\r\n        print(""  docs.... \\n{}"".format(func.__doc__))\r\n        result = func(*args, **kwargs)\r\n        print(""{!r:}\\n"".format(result))  # comment out if results not needed\r\n        return result  # for optional use outside.\r\n    return wrapper\r\n\r\n# -----------------------------------------------------------------------------\r\n# functions\r\n\r\n\r\n@func_run\r\ndef colrow_txt(N=10, cols=2, rows=2, zero_based=True):\r\n    """"""  Produce spreadsheet like labels either 0- or 1-based.\r\n    :N  - number of records/rows to produce.\r\n    :cols/rows - this combination will control the output of the values\r\n    :  cols=2, rows=2 - yields (A0,A1,B0,B1)\r\n    :  as optional classes regardless of the number of records being produced\r\n    :zero-based - True for conventional array structure,\r\n    :             False for spreadsheed-style classes\r\n    """"""\r\n    if zero_based:\r\n        start = 0\r\n    else:\r\n        start = 1\r\n        rows = rows + 1\r\n    UC = (list(""ABCDEFGHIJKLMNOPQRSTUVWXYZ""))[:cols]  # see constants\r\n    dig = (list(\'0123456789\'))[start:rows]\r\n    cr_vals = [c + r for r in dig for c in UC]\r\n    colrow = np.random.choice(cr_vals, N)\r\n    return colrow\r\n\r\n\r\n@func_run\r\ndef rowcol_txt(N=10,rows=2,cols=2):\r\n    """"""  Produce array-like labels in a tuple format.\r\n    """"""\r\n    rc_vals = [""({},{})"".format(r, c)\r\n               for c in range(cols)\r\n               for r in range(rows)]\r\n    rowcol = np.random.choice(rc_vals, N)\r\n    return rowcol\r\n\r\n\r\n@func_run\r\ndef rand_text(N=10, cases=3, vals=str_opt[3]):\r\n    """"""  Generate N samples from the letters of the alphabet denoted by the\r\n    :number of cases.  If you want greater control on the text and\r\n    :probability, see rand_case or rand_str.\r\n    :vals:  see str_opt in required constants section\r\n    """"""\r\n    vals = list(vals)\r\n    txt_vals = np.random.choice(vals[:cases], N)\r\n    return txt_vals\r\n\r\n\r\n@func_run\r\ndef rand_str(N=10,low=1,high=10,vals=str_opt[3]):\r\n    """"""  Returns N strings from \'size\' random letters.\r\n    :- create the cases as a list:  string.ascii_lowercase or ascii_uppercase\r\n    :- determine how many letters.\r\n    :  Ensure min <= max. Add 1 to max alleviate low==high\r\n    :- shuffle the case list each time through loop\r\n    """"""\r\n    vals = list(vals)\r\n    letts = np.arange(min([low, high]), max([low, high]) + 1)  # num letters\r\n    result = []\r\n    for i in range(N):\r\n        np.random.shuffle(vals)\r\n        size = np.random.choice(letts, 1)\r\n        result.append("""".join(vals[:size]))\r\n    result = np.array(result)\r\n    return result\r\n\r\n@func_run\r\ndef rand_case(N=10,cases=[""Aa"",""Bb""],p_vals=[0.8,0.2]):\r\n    """"""  Generate N samples from a list of classes with an associated probability\r\n    ensure: len(cases)==len(p_vals) and  sum(p_values) == 1\r\n    small sample sizes will probably not yield the desired p-values\r\n    """"""\r\n    p = (np.array(p_vals))*N   # convert to integer\r\n    kludge = [np.repeat(cases[i],p[i]).tolist() for i in range(len(cases))]\r\n    case_vals = np.array([ val for i in range(len(kludge)) for val in kludge[i]])\r\n    np.random.shuffle(case_vals)\r\n    return case_vals\r\n\r\n\r\n@func_run\r\ndef rand_int(N=10, begin=0, end=10):\r\n    """"""  Generate N random integers within the range begin - end\r\n    """"""\r\n    int_vals = np.random.random_integers(begin, end, size=(N))\r\n    return int_vals\r\n\r\n\r\n@func_run\r\ndef rand_float(N=10, begin=0, end=10, deci=2):\r\n    """"""  Generate N random floats within the range begin - end\r\n    :Technically, N random integers are produced then a random\r\n    :amount within 0-1 is added to the value\r\n    """"""\r\n    float_vals = np.random.randint(begin, end, size=(N))\r\n    float_vals = np.around(float_vals + np.random.rand(N), deci)\r\n    return float_vals\r\n\r\n\r\ndef rand_norm(N=10, avg_=10, st_dev=1, deci=2):\r\n    """"""  Generate N random floats within the range begin - end\r\n    :Technically, N random integers are produced then a random\r\n    :amount within 0-1 is added to the value\r\n    """"""\r\n    float_vals = np.random.normal(avg_, st_dev, size=(N))\r\n    float_vals = np.around(float_vals + np.random.rand(N), deci)\r\n    return float_vals\r\n\r\n\r\n\r\n@func_run\r\ndef pnts_IdShape(N=10, x_min=0, x_max=10, y_min=0, y_max=10, simple=True):\r\n    """"""  Create an array with a nested dtype which emulates a shapefile\'s\r\n    :data structure.  This array is used to append other arrays to enable\r\n    :import of the resultant into ArcMap.  Array construction, after hpaulj\r\n    :http://stackoverflow.com/questions/32224220/\r\n    :     methods-of-creating-a-structured-array\r\n    """"""\r\n    Xs = np.random.randint(x_min, x_max + 1, size=N)\r\n    Ys = np.random.randint(y_min, y_max + 1, size=N)\r\n    IDs = np.arange(0, N)\r\n    c_stack = np.column_stack((IDs, Xs, Ys))\r\n    if simple:     # version 1\r\n        dt = [(\'ID\', \'<i4\'), (\'Shape\', \'<f8\', (2,))]  # short version,\r\n        a = np.ones(N, dtype=dt)\r\n        a[\'ID\'] = c_stack[:, 0]\r\n        a[\'Shape\'] = c_stack[:, 1:]                   # this line too\r\n    else:          # version 2\r\n        dt = [(\'ID\', \'<i4\'), (\'Shape\', ([(\'X\', \'<f8\'), (\'Y\', \'<f8\')]))]\r\n        a = np.ones(N, dtype=dt)\r\n        a[\'Shape\'][\'X\'] = c_stack[:, 1]\r\n        a[\'Shape\'][\'Y\'] = c_stack[:, 2]\r\n        a[\'ID\'] = c_stack[:, 0]\r\n    return a  # IDs, Xs, Ys, a, dt\r\n\r\n\r\ndef main_demo():\r\n    """"""Run all the functions with their defaults\r\n    :  To make your own run func, copy and paste the function itself into\r\n    :   a func list.  The decorator handles the printing of results\r\n    """"""\r\n    N = 10\r\n    id_shape = pnts_IdShape(N, x_min=0, x_max=10, y_min=0, y_max=10)\r\n    colrow = colrow_txt(N, cols=5, rows=1, zero_based=True),\r\n    rowcol = rowcol_txt(N, rows=5, cols=1),\r\n    txt_fld = rand_text(N, cases=3, vals=str_opt[3]),\r\n    str_fld = rand_str(N, low=1, high=10, vals=str_opt[3]),\r\n    case1_fld = rand_case(N, cases=[\'cat\', \'dog\', \'fish\'],\r\n                          p_vals=[0.6, 0.3, 0.1]),\r\n    case2_fld = rand_case(N, cases=[""Aa"", ""Bb""], p_vals=[0.8, 0.2])\r\n    int_fld = rand_int(N, begin=0, end=10),\r\n    float_fld = rand_float(N, begin=0, end=10, deci=2)\r\n    #\r\n    print((""\\n"" + ""-""*60 + ""\\n""))\r\n    fld_names = [\'Colrow\', \'Rowcol\', \'txt_fld\', \'str_fld\',\r\n                 \'case1_fld\', \'case2_fld\', \'int_fld\', \'float_fld\']\r\n    fld_data = [colrow, rowcol, txt_fld, str_fld,\r\n                case1_fld, case2_fld, int_fld, float_fld]\r\n    arr = rfn.append_fields(id_shape, fld_names, fld_data, usemask=False)\r\n    print(""\\nArray generated....\\n{!r:}"".format(arr))\r\n    return fld_data\r\n\r\n\r\ndef blog_post():\r\n    """"""Sample run\r\n    |  import arcpy\r\n    |  a = blog_post()  # do the run if it isn\'t done\r\n    |  # ..... snip ..... the output\r\n    |  # ..... snip ..... now create the featureclass\r\n    |  SR_name = 32189  # u\'NAD_1983_CSRS_MTM_9\'\r\n    |  SR = arcpy.SpatialReference(SR_name)\r\n    |  output_shp =\'F:/Writing_Projects/NumPy_Lessons/Shapefiles/out.shp\'\r\n    |  arcpy.da.NumPyArrayToFeatureClass(a, output_shp, \'Shape\', SR)""""""\r\n    N = 10\r\n    #id_shape = pnts_IdShape(N, x_min=300000, x_max=300500,y_min=5000000,y_max=5000500)\r\n    IDs = np.arange(0,N)\r\n    a = np.ones(N, dtype=[(\'ID\', \'<i4\')])\r\n    a=IDs\r\n    case1_fld = rand_case(N, cases=[""A"", ""B"", ""C"", ""D""],\r\n                          p_vals=[0.4, 0.2, 0.2, 0.2])\r\n    case3_fld = rand_case(N, cases=[""wet "",""dry ""],\r\n                          p_vals=[0.5, 0.5])\r\n    case4_fld = rand_case(N, cases=[\'cat\', \'dog\', \'fish\'],\r\n                          p_vals=[0.6, 0.3, 0.1])\r\n    case2_fld = rand_case(N, cases=[""Aa"", ""Bb"", ""Cc"", ""Dd""],\r\n                          p_vals=[0.4, 0.2, 0.2, 0.2])\r\n    int_fld = rand_int(N, begin=0, end=10)\r\n    fld_names = [\'Place\', \'State\', \'Case\', \'Pet\', \'Number\']\r\n    fld_data = [case1_fld, case2_fld, case3_fld, case4_fld, int_fld]\r\n    arr = rfn.append_fields(a, fld_names, fld_data, usemask=False)\r\n    return arr\r\n\r\n\r\ndef run_samples():\r\n    """"""sample run""""""\r\n    N = 1000\r\n    # id_shape = pnts_IdShape(N, x_min=300000, x_max=300500,\r\n    #                         y_min=5000000, y_max=5000500)\r\n    IDs = np.arange(0,N)\r\n    a = np.ones(N, dtype=[(\'ID\', \'<i4\')])\r\n    a=IDs\r\n    case1_fld = rand_case(N,cases=[""A"", ""B"", ""C"", ""D""],\r\n                          p_vals=[0.3, 0.3, 0.3, 0.1])\r\n    case2_fld = rand_case(N,cases=[""A_ "", ""B_"", ""C_""],\r\n                          p_vals=[0.4, 0.3, 0.3])\r\n    case3_fld = rand_case(N,cases=[\'Hosp\', \'Hall\'],\r\n                          p_vals=[0.5, 0.5])\r\n    int_fld = rand_int(N, begin=1, end=60)\r\n    int_fld2 = rand_int(N, begin=1, end=300)\r\n    fld_names = [\'County\', \'Town\', \'Facility\', \'Time\', \'People\']\r\n    fld_data = [case1_fld, case2_fld, case3_fld, int_fld, int_fld2]\r\n    arr = rfn.append_fields(a, fld_names, fld_data, usemask=False)\r\n    return arr\r\n\r\n\r\ndef stats_demo(N=10, cols=10):\r\n    """"""Create points with 12 columns of random float values.\r\n    :  N - number of points\r\n    :  SR - GCS North American 1983 CSRS, WKID 4617\r\n    :  my link....\r\n    :  https://stackoverflow.com/questions/43442415/\r\n    :    cannot-perform-reduce-with-flexible-type\r\n    """"""\r\n    a = pnts_IdShape(N, x_min=300000, x_max=310000,\r\n                     y_min=5025000, y_max=5035000, simple=True)\r\n    fld_dt = a.dtype.descr\r\n    col_names = tuple([(\'C_\' + str(i)) for i in range(cols)])\r\n    xtra_dt = [(i, \'<f8\') for i in col_names]  # modify dtype if desired\r\n    #fld_dt.extend(xtra_dt)\r\n    fld_data = []\r\n    for i in range(cols):\r\n        vals = rand_norm(N, avg_=10, st_dev=1, deci=2)  # normal\r\n        #vals = rand_float(N, begin=0, end=10)  # uncomment for random\r\n        #vals = np.zeros(N)\r\n        #vals.fill(i)\r\n        vals[:100] = np.nan\r\n        np.random.shuffle(vals)\r\n        fld_data.append(vals)\r\n    b = rfn.append_fields(a, names=col_names, data=fld_data, usemask=False)\r\n    return a, b, col_names, fld_data\r\n\r\n\r\ndef cal_stats(in_fc, col_names):\r\n    """"""Calculate stats for an array with nodata (nan, None) in the columns\r\n    :in_fc - input featureclass or table\r\n    :col_names - the columns... numeric (floating point, double)\r\n    """"""\r\n    a = arcpy.da.FeatureClassToNumPyArray(in_fc, col_names)  # ""*"")\r\n    b = a.view(np.float).reshape(len(a), -1)\r\n    r_sum = np.nansum(b, axis=1)   # by row\r\n    c_sum = np.nansum(b, axis=0)   # by column\r\n    r_avg = np.nanmean(b, axis=1)  # by row\r\n    c_avg = np.nanmean(b, axis=0)  # by column\r\n    stck = np.vstack((col_names, c_sum, c_avg)).T\r\n    print(stck)\r\n    return stck, col_names, c_sum, c_avg\r\n\r\nif __name__ == \'__main__\':\r\n    """"""random_data_demo\r\n    Create a point shapefile containing fields:\r\n      ID, Shape, {txt_fld,int_fld...of any number}\r\n    """"""\r\n    print(""\\nRunning... {}\\n"".format(__file__))\r\n    # uncomment an option below\r\n    N = 2000\r\n    cols = 3\r\n    a, b, col_names, fld_data = stats_demo(N=N, cols=cols)\r\n    out_fc = r\'C:\\GIS\\Tools_scripts\\Statistics\\Stats_demo_01.gdb\\pnts_2000_norm\'\r\n    SR = arcpy.SpatialReference(\'NAD 1983 CSRS MTM  9\')\r\n    arcpy.da.NumPyArrayToFeatureClass(b, out_fc, [\'Shape\'], SR)\r\n#    stck, col_names, c_sum, c_avg = cal_stats(out_fc, col_names)\r\n#    dts = [(\'Column\', \'<U15\'), (\'Col_sum\', \'<f8\'), (\'Col_avg\', \'<f8\')]\r\n#    args = [col_names, c_sum, c_avg]\r\n#    z = np.empty(shape=(cols,), dtype=dts)\r\n#    for i in range(len(args)):\r\n#        z[z.dtype.names[i]] = args[i]\r\n#    c = arcpy.da.FeatureClassToNumPyArray(out_fc, col_names) # ""*"")\r\n#    good_flds = [i for i in c.dtype.names if c[i].dtype.kind in [\'f\']]\r\n#    by_col = [(i, np.nanmean(c[i])) for i in good_flds\r\n#              if c[i].dtype.kind in (\'i\', \'f\')]\r\n#    bc2 = c.view(np.float).reshape(len(c), -1)\r\n#    np.nanmean(bc2, axis=1)  # by row\r\n#    np.nanmean(bc2, axis=0)  # by column\r\n\r\n\r\n#    np.vstack((c[\'C_0\'], c[\'C_1\'])).T\r\n    #arr = run_samples()\r\n    #a = blog_post()\r\n    #returned = main_demo()  # print a complete function run...or specify one as in below\r\n    print((""\\n"" + ""-""*60 +""\\n""))\r\n'"
all_scripts/rank_field.py,7,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   rank_field.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2017-07-12\r\n:Purpose:  tools for working with arcpy and numpy arrays\r\n:  - sort a table based on a field or fields\r\n:References:\r\n:(1) FeatureClassToNumPyArray (in_table, field_names, {where_clause},\r\n:                           {spatial_reference}, {explode_to_points},\r\n:                           {skip_nulls}, {null_value})\r\n: - http://pro.arcgis.com/en/pro-app/arcpy/data-access/\r\n:        featureclasstonumpyarray.htm\r\n:   -SHAPE@TRUECENTROID \xe2\x80\x94A tuple of the feature\'s true centroid coordinates\r\n:   -SHAPE@X \xe2\x80\x94 A double of the feature\'s x-coordinate.\r\n:   -SHAPE@Y \xe2\x80\x94 A double of the feature\'s y-coordinate.\r\n:\r\n:(2) TableToNumPyArray (in_table, field_names, {where_clause},\r\n:                   {skip_nulls}, {null_value})\r\n: - http://pro.arcgis.com/en/pro-app/arcpy/data-access/tabletonumpyarray.htm\r\n:\r\n:(3) ExtendTable(in_table, table_match_field, in_array,\r\n:              array_match_field, {append_only})\r\n: - http://pro.arcgis.com/en/pro-app/arcpy/data-access/extendtable.htm\r\n:Notes:\r\n:-----\r\n:\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\r\n      \'float\': \'{: 0.3f}\'.format}\r\n\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\n# -----------------------------------------------------------------------------\r\n# functions from arraytools\r\ndef tweet(msg):\r\n    """"""Produce a message (msg)for both arcpy and python\r\n    """"""\r\n    m = ""{}"".format(msg)\r\n    arcpy.AddMessage(m)\r\n    print(m)\r\n    print(arcpy.GetMessages())\r\n\r\n\r\ndef fc_info(in_fc):\r\n    """"""basic feature class information""""""\r\n    desc = arcpy.Describe(in_fc)    # fix to use da.Describe\r\n    SR = desc.spatialReference      # spatial reference object\r\n    shp_fld = desc.shapeFieldName   # FID or OIDName, normally\r\n    oid_fld = desc.OIDFieldName     # Shapefield ...\r\n    return shp_fld, oid_fld, SR\r\n\r\n\r\n# -----------------------------------------------------------------------------\r\n# ---- Other defs ----\r\ndef rankmin(x):\r\n    """"""Returns a rank accounting for duplicates\r\n    :  The array must be sorted first\r\n    :  Warren W. solution at\r\n    : https://stackoverflow.com/questions/39059371/\r\n    :       can-numpys-argsort-give-equal-element-the-same-rank\r\n    """"""\r\n    u, inv, counts = np.unique(x, return_inverse=True, return_counts=True)\r\n    csum = np.zeros_like(counts)\r\n    csum[1:] = counts[:-1].cumsum()\r\n    return csum[inv]\r\n\r\n\r\n# ------------------------------------------------------------------------\r\n# ---- Checks to see if running in test mode or from a tool\r\nif len(sys.argv) == 1:\r\n    in_tbl = r\'C:\\GIS\\Tools_scripts\\Statistics\\Stats_demo_01.gdb\\pnts_2K_normal\'\r\n    desc = arcpy.da.Describe(in_tbl)\r\n    oid_fld = desc[\'OIDFieldName\']\r\n    flds = arcpy.ListFields(in_tbl)\r\n    # fld_names = [fld.name for fld in flds]\r\n    fld_names = [\'Rand_1_100\', oid_fld]\r\n    testing = True\r\n    rank_fld = \'Rand_1_100\'\r\n    rank_min = True\r\nelse:\r\n    in_tbl = sys.argv[1]\r\n    fld_names = sys.argv[2]\r\n    rank_fld = sys.argv[3]\r\n    rank_min = sys.argv[4]\r\n    #\r\n    desc = arcpy.da.Describe(in_tbl)\r\n    oid_fld = desc[\'OIDFieldName\']\r\n    testing = False\r\n\r\n# ------------------------------------------------------------------------\r\n# ---- Create the array, sort extend/join to the input table ----\r\nif rank_fld == \'\':\r\n    rank_fld = \'Rank\'\r\nelse:\r\n    no_good = \' !""#$%&\\\'()*+,-./:;<=>?@[\\\\]^`{|}~\'\r\n    rank_fld = """".join([i for i in rank_fld if i not in no_good])\r\n\r\nif isinstance(fld_names, (list, tuple)):\r\n    order_by = fld_names\r\nelif isinstance(fld_names, (str)):\r\n    if str(fld_names).find("";"") == -1:\r\n        order_by = [fld_names, oid_fld]\r\n    else:\r\n        order_by = fld_names.split("";"") + [oid_fld]\r\n\r\na = arcpy.da.TableToNumPyArray(in_tbl, field_names=order_by)\r\n\r\na_s = a[order_by]\r\nsrted = np.argsort(a_s, order=order_by)\r\n\r\ndt = [(oid_fld, \'<i4\'), (rank_fld, \'<i4\')]\r\nj_a = np.zeros(a.shape, dtype=dt)\r\nj_a[oid_fld] = a_s[srted][oid_fld]\r\n\r\nif rank_min in (\'true\', True):  # use regular or rankmin ranking method\r\n    r = a_s[srted][fld_names]\r\n    r = rankmin(r)\r\n    j_a[rank_fld] = r\r\nelse:\r\n    j_a[rank_fld] = np.arange(1, a.shape[0]+1)\r\n#\r\nif not testing:\r\n    arcpy.da.ExtendTable(in_table=in_tbl,\r\n                         table_match_field=oid_fld,\r\n                         in_array=j_a,\r\n                         array_match_field=oid_fld)\r\n\r\nfrmt = """"""\r\n{}\r\n:Script....{}\r\n:Ranking... {}\r\n:Using fields...\r\n:   {}\r\n{}\r\n""""""\r\nargs = [""-""*70, script, in_tbl, order_by, ""-""*70]\r\nmsg = frmt.format(*args)\r\ntweet(msg)\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n    pass\r\n'"
all_scripts/rank_flds.py,6,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   rank_flds.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2017-11-01\r\n:Purpose:  tools for working with numpy arrays\r\n:Useage:  Runk the toolbox, select the parameters\r\n:\r\n:References: sure are\r\n:\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\n\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\r\n      \'float\': \'{: 0.3f}\'.format}\r\n\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\ndef tweet(msg):\r\n    """"""Print a message for both arcpy and python.\r\n    : msg - a text message\r\n    """"""\r\n    m = ""\\n{}\\n"".format(msg)\r\n    arcpy.AddMessage(m)\r\n    print(m)\r\n    print(arcpy.GetMessages())\r\n\r\n\r\nin_tbl = sys.argv[1]\r\nin_flds = sys.argv[2]\r\nout_fld = sys.argv[3]\r\n\r\nif \';\' in in_flds:\r\n    in_flds = in_flds.split(\';\')\r\nelse:\r\n    in_flds = [in_flds]\r\n\r\ndesc = arcpy.da.Describe(in_tbl)\r\ntbl_path = desc[\'path\']\r\nfnames = [i.name for i in arcpy.ListFields(in_tbl)]\r\nif out_fld in fnames:\r\n    out_fld += \'dup\'\r\nout_fld = arcpy.ValidateFieldName(out_fld, tbl_path)\r\nargs = [in_tbl, in_flds, out_fld, tbl_path]\r\nmsg = ""in_tbl {}\\nin_fld {}\\nout_fld  {}\\ntbl_path  {}"".format(*args)\r\ntweet(msg)\r\noid = \'OBJECTID\'\r\nvals = [oid] + in_flds\r\narr = arcpy.da.TableToNumPyArray(in_tbl, vals)\r\ntweet(""{!r:}"".format(arr))\r\nfor v in vals:\r\n    fix_null = np.where(arr[v] == \'None\', \'\', arr[v])\r\n    arr[v] = fix_null\r\narr_sort = np.sort(arr, order=in_flds)\r\ndt = [(oid, \'<i4\'), (out_fld, \'<i4\')]\r\nout_array = np.zeros((arr_sort.shape[0],), dtype=dt)\r\ndt_names = out_array.dtype.names\r\nout_array[dt_names[0]] = arr_sort[oid]\r\nout_array[dt_names[-1]] = np.arange(1, arr_sort.size + 1)  # gdb tables\r\narcpy.da.ExtendTable(in_tbl, \'OBJECTID\', out_array, \'OBJECTID\')\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n'"
all_scripts/ras2npy.py,4,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nras2npy.py\r\n==========\r\n\r\nScript :   ras2npy.py  # raster to numpy array as *.npy file\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified : 2018-09-25\r\n\r\nPurpose:  tools for working with numpy arrays\r\n\r\nUseage :\r\n\r\nReferences\r\n----------\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/functions/rastertonumpyarray\r\n-function.htm>`_.\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/numpyarraytotable.htm>`_.\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/tabletonumpyarray.htm>`_.\r\n---------------------------------------------------------------------\r\n""""""\r\n# pylint: disable=C0103  # invalid-name\r\n# pylint: disable=R0914  # Too many local variables\r\n# pylint: disable=R1710  # inconsistent-return-statements\r\n# pylint: disable=W0105  # string statement has no effect\r\n\r\nimport sys\r\nimport os\r\n\r\nimport numpy as np\r\nfrom art_common import (tweet, rasterfile_info)\r\nfrom arcpy import Point, Raster, RasterToNumPyArray, env\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=5, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\nenv.overwriteOutput = True\r\n\r\n# ----------------------------------------------------------------------\r\n# .... final code section producing the featureclass and extendtable\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    pth = script.split(""/"")[:-2]\r\n    pth0 = ""/"".join(pth) + ""/Data/r00.tif""\r\n    r = Raster(pth0)\r\n    out_arr = ""/"".join(pth) + ""/Data/r01.npy""\r\n    frmt = ""Result...\\n{}""\r\n#    print(frmt.format(a))\r\nelse:\r\n    testing = False\r\n    pth = sys.argv[1]\r\n    out_arr = sys.argv[2]\r\n    r = Raster(pth)\r\n# parameters here\r\nLL = r.extent.lowerLeft\r\ncols = int(r.extent.width/r.meanCellWidth)\r\nrows = int(r.extent.height/r.meanCellWidth)\r\na = RasterToNumPyArray(r,\r\n                       lower_left_corner=Point(LL.X, LL.Y),\r\n                       ncols=cols,\r\n                       nrows=rows,\r\n                       nodata_to_value=r.noDataValue\r\n                       )\r\n#\r\n# ---- overwrite existing outputs\r\nif os.path.isfile(out_arr):\r\n    tweet(""\\nRemoving ... {}\\nbefore saving"".format(out_arr))\r\n    os.remove(out_arr)\r\nnp.save(out_arr, a)\r\nif testing:\r\n    tweet(\'\\nScript source... {}\'.format(script))\r\nprint(\'\\nCleaning up\')\r\ndel r, tweet, rasterfile_info, Point, Raster, RasterToNumPyArray\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n'"
all_scripts/raster_functions.py,21,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\nraster_functions\r\n================\r\n\r\nScript :   raster_functions.py\r\n\r\nAuthor :   Dan.Patterson@carleton.ca\r\n\r\nModified: 2018-03-28\r\n\r\nPurpose:  tools for working with numpy arrays\r\n\r\nUseage : See header for each function\r\n\r\nReferences:\r\n\r\n ---------------------------------------------------------------------\r\n""""""\r\n# pylint: disable=C0103  # invalid-name\r\n# pylint: disable=R0914  # Too many local variables\r\n# pylint: disable=R1710  # inconsistent-return-statements\r\n# pylint: disable=W0105  # string statement has no effect\r\n\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\n\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n__all__ = [\'ufunc_add\',\r\n           \'ufunc_sub\',\r\n           \'rotate_mesh\',\r\n           \'mesh_arr\'\r\n           ]\r\n\r\n\r\ndef ufunc_add(xx=None, yy=None, scale_x=1, scale_y=1):\r\n    """"""A ufunc that just permits adding a meshgrid xx and yy values with x, y\r\n    scaling\r\n\r\n    Parameters:\r\n    -----------\r\n    - xx : meshgrid xx values\r\n    - yy : meshgrid yy values\r\n    - scale_x : scale the xx range values prior to addition\r\n    - scale_y : same but for yy range values\r\n\r\n     >>> xx, yy = mesh_arr(x_ax=(0, 5., 1.0), y_ax=(0, 5., 1.0))\r\n     >>> ufunc_add(xx=xx, yy=yy)\r\n     array([[ 0.,  1.,  2.,  3.,  4.],\r\n            [ 1.,  2.,  3.,  4.,  5.],\r\n            [ 2.,  3.,  4.,  5.,  6.],\r\n            [ 3.,  4.,  5.,  6.,  7.],\r\n            [ 4.,  5.,  6.,  7.,  8.]])\r\n\r\n    Notes:\r\n    ------\r\n    Other examples\r\n    ::\r\n        z = xx * 1 + yy * 1  # same as above\r\n        z + np.flipud(z)\r\n        array([[  4.,   6.,   8.,  10.,  12.],\r\n               [  4.,   6.,   8.,  10.,  12.],\r\n               [  4.,   6.,   8.,  10.,  12.],\r\n               [  4.,   6.,   8.,  10.,  12.],\r\n               [  4.,   6.,   8.,  10.,  12.]])\r\n\r\n        z + np.fliplr(z)\r\n        array([[  4.,   4.,   4.,   4.,   4.],\r\n               [  6.,   6.,   6.,   6.,   6.],\r\n               [  8.,   8.,   8.,   8.,   8.],\r\n               [ 10.,  10.,  10.,  10.,  10.],\r\n               [ 12.,  12.,  12.,  12.,  12.]])\r\n\r\n    I suppose you have figured out that the documentation is larger than the\r\n    actual script code.\r\n\r\n    See `mesh_arr` for meshgrid construction.\r\n    """"""\r\n    z = xx * scale_x + yy * scale_y\r\n    return z\r\n\r\n\r\ndef ufunc_sub(xx=None, yy=None, scale_x=1, scale_y=1):\r\n    """"""A ufunc that just permits subtrace a meshgrid xx and yy values with x, y\r\n    scaling.\r\n\r\n    See:\r\n    ----\r\n    ufunc_add : for full details\r\n\r\n    Other examples\r\n    ::\r\n        z = xx * 1 - yy * 1  # see ufunc_add\r\n        array([[ 0.,  1.,  2.,  3.,  4.],\r\n               [-1.,  0.,  1.,  2.,  3.],\r\n               [-2., -1.,  0.,  1.,  2.],\r\n               [-3., -2., -1.,  0.,  1.],\r\n               [-4., -3., -2., -1.,  0.]])\r\n\r\n        z + np.flipud(z)\r\n        array([[-4., -2.,  0.,  2.,  4.],\r\n               [-4., -2.,  0.,  2.,  4.],\r\n               [-4., -2.,  0.,  2.,  4.],\r\n               [-4., -2.,  0.,  2.,  4.],\r\n               [-4., -2.,  0.,  2.,  4.]])\r\n\r\n        z + np.fliplr(z)\r\n        array([[ 4.,  4.,  4.,  4.,  4.],\r\n               [ 2.,  2.,  2.,  2.,  2.],\r\n               [ 0.,  0.,  0.,  0.,  0.],\r\n               [-2., -2., -2., -2., -2.],\r\n               [-4., -4., -4., -4., -4.]]\r\n\r\n    See `mesh_arr` for meshgrid construction.\r\n    """"""\r\n    z = xx * scale_x - yy * scale_y  # see other ufunc examples\r\n    return z\r\n\r\n\r\ndef rotate_mesh(x_ax=(0, 10., 1.0), y_ax=(0, 10., 1.0), rot_angle=0):\r\n    """"""Generate a meshgrid and rotate it by rot_angle degrees.\r\n\r\n    Parameters:\r\n    -----------\r\n    x_ax : tuple\r\n        x_min, x_max, dx of integers or floats\r\n    y_ax : tuple\r\n        y_min, y_max, dy\r\n    rot_angle : number\r\n        rotation angle in degrees\r\n\r\n    Returns:\r\n    --------\r\n    Rotated xx, yy meshgrid parameters with a clockwise rotation\r\n\r\n    References:\r\n    -----------\r\n    https://stackoverflow.com/questions/29708840/rotate-meshgrid-with-numpy\r\n\r\n    Extra:\r\n    ------\r\n    https://stackoverflow.com/questions/32544636/transform-image-data-in-3d/\\\r\n    32546099#32546099\r\n\r\n    https://stackoverflow.com/questions/31816754/\\\r\n    numpy-einsum-for-rotation-of-meshgrid\r\n\r\n    Similar to above, but rotation in 3d\r\n\r\n    >>> x, y, z = np.meshgrid(np.linspace(0, 1, 4),\r\n                              np.linspace(0, 1, 3),\r\n                              [.5], indexing=\'xy\')\r\n    >>> M = np.array([[ 0., -1.,  0.],\r\n                      [ 1.,  0.,  0.],\r\n                      [ 0.,  0.,  1.]])\r\n\r\n    Einsum is used to do the rotation given, x, y, z and a rotation matrix.\r\n\r\n    >>> xp, yp, zp = np.einsum(\'ij,jklm->iklm\', M, [x, y, z])\r\n\r\n    >>> xp.squeeze()\r\n    array([[ 0. ,  0. ,  0. ,  0. ],\r\n           [-0.5, -0.5, -0.5, -0.5],\r\n           [-1. , -1. , -1. , -1. ]])\r\n    >>> yp.squeeze()\r\n    array([[ 0. ,  0.3,  0.7,  1. ],\r\n           [ 0. ,  0.3,  0.7,  1. ],\r\n           [ 0. ,  0.3,  0.7,  1. ]])\r\n    >>> zp.squeeze()\r\n    array([[ 0.5,  0.5,  0.5,  0.5],\r\n           [ 0.5,  0.5,  0.5,  0.5],\r\n           [ 0.5,  0.5,  0.5,  0.5]])\r\n\r\n    plus more to come.\r\n    """"""\r\n    # 2D rotation matrix.  Clockwise rotation\r\n    ang_rad = np.radians(rot_angle)\r\n    rot_matrix = np.array([[np.cos(ang_rad), np.sin(ang_rad)],\r\n                           [-np.sin(ang_rad), np.cos(ang_rad)]])\r\n    xs = np.arange(*x_ax)\r\n    ys = np.arange(*y_ax)\r\n    x, y = np.meshgrid(xs, ys, indexing=\'xy\')\r\n    return np.einsum(\'ji, mni -> jmn\', rot_matrix, np.dstack([x, y]))\r\n\r\n\r\ndef mesh_arr(x_ax=(0, 10., 1.0), y_ax=(0, 10., 1.0)):\r\n    """"""Construct a mesh grid given the above ranges for x and y\r\n\r\n    Parameters:\r\n    -----------\r\n    x_ax : tuple of x_min, x_max, dx\r\n    y_ax : tuple of y_min, y_max, dy\r\n\r\n    Returns:\r\n    --------\r\n    A meshgrid\r\n\r\n    >>> xx, yy = mesh_arr(x_ax=(0, 5, 1), y_ax=(0, 3, 1))\r\n    xx\r\n    array([[0, 1, 2, 3, 4]])\r\n    yy\r\n    array([[0],\r\n           [1],\r\n           [2]])\r\n\r\n    See also:\r\n    ---------\r\n    `ufunc_add` and `ufunc_sub` have examples, including 90 degree rotations\r\n    in the x and y directions to produce altered meshgrids.\r\n    """"""\r\n    x = np.arange(*x_ax)\r\n    y = np.arange(*y_ax)\r\n    xx, yy = np.meshgrid(x, y, sparse=True, indexing=\'xy\')\r\n    return xx, yy\r\n\r\n\r\n#def slope_arr(dx, dy):\r\n#    """"""Create an array with a preferred slope\r\n#\r\n#    Parameters:\r\n#    -----------\r\n#    - dx : width in the x-direction ie. the columns of the array\r\n#    - dy : height in the y-direction ie. the rows of the array\r\n#\r\n#    """"""\r\n\r\n\r\ndef _demo():\r\n    """"""\r\n    : -\r\n    """"""\r\n    pass\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n    _demo()\r\n'"
all_scripts/rasters.py,74,"b'# -*- coding: UTF-8 -*-ct\r\n""""""\r\n:Script:   rasters.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2018-02-13\r\n:Purpose:  tools for working with numpy arrays\r\n:Useage:\r\n:\r\n:Requires:\r\n:  arraytools.tools - nd2struct, stride\r\n:References:\r\n: https://community.esri.com/blogs/dan_patterson/2018/01/19/\r\n:       combine-data-classification-from-raster-combinations\r\n: - combine\r\n: https://stackoverflow.com/questions/48035246/\r\n:       intersect-multiple-2d-np-arrays-for-determining-zones\r\n: original def find_labels(*arrs):\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# pylint: disable=C0103  # invalid-name\r\n# pylint: disable=R0914  # Too many local variables\r\n# pylint: disable=R1710  # inconsistent-return-statements\r\n# pylint: disable=W0105  # string statement has no effect\r\n\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nfrom tools import nd2struct, stride\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n__all__ = [\'combine_\',      # 3D array functions\r\n           \'check_stack\',\r\n           \'mask_stack\',    # statistical functions\r\n           \'stack_sum\', \'stack_cumsum\',\r\n           \'stack_prod\', \'stack_cumprod\', \'stack_min\', \'stack_mean\',\r\n           \'stack_median\', \'stack_max\', \'stack_std\', \'stack_var\',\r\n           \'stack_percentile\',\r\n           \'stack_stats\']\r\n\r\n\r\n# ---- array checks and creation --------------------------------------------\r\n# ---- 3D arrays for stacked operations\r\n#\r\ndef check_shapes(arrs):\r\n    """"""Check the shapes of the arrays to ensure they are all equal\r\n    """"""\r\n    shps = [i.shape for i in arrs]\r\n    eq = np.all(np.array([shps[0] == i for i in shps[1:]]))\r\n    err = ""Arrays arr not of the same shape...""\r\n    if not eq:\r\n        raise ValueError(""{}\\n{}"".format(err, shps))\r\n\r\n\r\ndef check_stack(arrs):\r\n    """"""Do the basic checking of the stack to ensure that a 3D array is\r\n    :  generated\r\n    """"""\r\n    err1 = ""Object, structured arrays not supported, current type...""\r\n    err2 = ""3D arrays supported current ndim...""\r\n    if isinstance(arrs, (list, tuple)):\r\n        arrs = np.array(arrs)\r\n    if arrs.dtype.kind in (\'O\', \'V\'):\r\n        raise ValueError(""{} {}"".format(err1, arrs.dtype.kind))\r\n    if arrs.ndim != 3:\r\n        raise ValueError(""{} {}"".format(err2, arrs.ndim))\r\n    return arrs\r\n\r\n\r\ndef mask_stack(arr, nodata=None):\r\n    """"""Produce masks for a 3d array""""""\r\n    if (nodata is None) or (arr.ndim < 2) or (arr.ndim > 3):\r\n        print(""\\n...mask_stack requires a 3d array and a nodata value\\n"")\r\n        return arr\r\n    m = (arr[:, ...] == nodata).any(0)\r\n    msk = [m for i in range(arr.shape[0])]\r\n    msk = np.array(msk)\r\n    a_m = np.ma.MaskedArray(arr, mask=msk)\r\n    return a_m\r\n\r\n\r\n# ---- 3D array functions ----------------------------------------------------\r\n# (1) combine ----\r\ndef combine_(*arrs, ret_classes=False):\r\n    """"""Combine arrays to produce a unique classification scheme\r\n    : arrs - list, tuple of arrays of the same shape\r\n    : ret_classes - a structured array with the class values for each array\r\n    :               and the last column is the new_class\r\n    :Notes:\r\n    :------\r\n    : You should mask any values prior to running this if you want to account\r\n    : for nodata values\r\n    :\r\n    :References:\r\n    :-----------\r\n    : https://stackoverflow.com/questions/48035246/\r\n    :       intersect-multiple-2d-np-arrays-for-determining-zones\r\n    : original: def find_labels(*arrs):\r\n    """"""\r\n    err = ""\\n...A list of 2D arrays, or a 3D array is required, not...{}\\n""\r\n    check_shapes(arrs)\r\n    seq = [isinstance(i, (list, tuple)) for i in arrs]\r\n    is_seq = np.array(seq).all()\r\n    is_nd = [isinstance(i, np.ndarray) for i in arrs]\r\n    is_nd.all()\r\n    if is_seq:\r\n        indices = [np.unique(arr, return_inverse=True)[1] for arr in arrs]\r\n    elif is_nd:\r\n        if isinstance(arrs, np.ma.MaskedArray):\r\n            indices = [np.ma.unique(arrs[i], return_inverse=True)[1]\r\n                       for i in range(arrs.shape[0])]\r\n        else:\r\n            indices = [np.unique(arrs[i], return_inverse=True)[1]\r\n                       for i in range(len(arrs))]\r\n    else:\r\n        print(err.format(arrs))\r\n        return arrs\r\n    #\r\n    M = np.array([item.max()+1 for item in indices])\r\n    M = np.r_[1, M[:-1]]\r\n    strides = M.cumprod()\r\n    indices = np.stack(indices, axis=-1)\r\n    vals = (indices * strides).sum(axis=-1)\r\n    uniqs, cls_new = np.unique(vals, return_inverse=True)\r\n    combo = cls_new.reshape(arrs[0].shape)\r\n    if ret_classes:\r\n        classes = np.array([np.ravel(i) for i in arrs]).T\r\n        classes = np.c_[classes, cls_new]\r\n        classes = nd2struct(classes)\r\n        classes = np.unique(classes)\r\n        classes = classes[np.argsort(classes, order=classes.dtype.names[-1])]\r\n        return combo, classes\r\n    return combo\r\n\r\n\r\n# ---- Statistics for stacked arrays (3D) ------------------------------------\r\n#\r\ndef stack_percentile(arrs, q=50, nodata=None):\r\n    """"""nanpercentile for an array stack with optional nodata masked\r\n    :  arrs - either a list, tuple of arrays or an array with ndim=3\r\n    :  q - the percentile\r\n    :  nodata - nodata value, numeric or np.nan (will upscale integers)\r\n    """"""\r\n    a = check_stack(arrs)\r\n    if nodata is not None:\r\n        a = mask_stack(a, nodata=nodata)\r\n    nan_per = np.nanpercentile(a, q=q, axis=0)\r\n    return nan_per\r\n\r\n\r\ndef stack_sum(arrs, nodata=None):\r\n    """"""see stack_stats""""""\r\n    a = check_stack(arrs)\r\n    if nodata is not None:\r\n        a = mask_stack(a, nodata=nodata)\r\n    return np.nansum(a, axis=0)\r\n\r\n\r\ndef stack_cumsum(arrs, nodata=None):\r\n    """"""see stack_stats""""""\r\n    a = check_stack(arrs)\r\n    if nodata is not None:\r\n        a = mask_stack(a, nodata=nodata)\r\n    return np.nancumsum(a, axis=0)\r\n\r\n\r\ndef stack_prod(arrs, nodata=None):\r\n    """"""see stack_stats""""""\r\n    a = check_stack(arrs)\r\n    if nodata is not None:\r\n        a = mask_stack(a, nodata=nodata)\r\n    return np.nanprod(a, axis=0)\r\n\r\n\r\ndef stack_cumprod(arrs, nodata=None):\r\n    """"""see stack_stats""""""\r\n    a = check_stack(arrs)\r\n    if nodata is not None:\r\n        a = mask_stack(a, nodata=nodata)\r\n    return np.nancumprod(a, axis=0)\r\n\r\n\r\ndef stack_min(arrs, nodata=None):\r\n    """"""see stack_stats""""""\r\n    a = check_stack(arrs)\r\n    if nodata is not None:\r\n        a = mask_stack(a, nodata=nodata)\r\n    return np.nanmin(a, axis=0)\r\n\r\n\r\ndef stack_mean(arrs, nodata=None):\r\n    """"""see stack_stats""""""\r\n    a = check_stack(arrs)\r\n    if nodata is not None:\r\n        a = mask_stack(a, nodata=nodata)\r\n    return np.nanmean(a, axis=0)\r\n\r\n\r\ndef stack_median(arrs, nodata=None):\r\n    """"""see stack_stats""""""\r\n    a = check_stack(arrs)\r\n    if nodata is not None:\r\n        a = mask_stack(a, nodata=nodata)\r\n    return np.nanmedian(a, axis=0)\r\n\r\n\r\ndef stack_max(arrs, nodata=None):\r\n    """"""see stack_stats""""""\r\n    a = check_stack(arrs)\r\n    if nodata is not None:\r\n        a = mask_stack(a, nodata=nodata)\r\n    return np.nanmax(a, axis=0)\r\n\r\n\r\ndef stack_std(arrs, nodata=None):\r\n    """"""see stack_stats""""""\r\n    a = check_stack(arrs)\r\n    if nodata is not None:\r\n        a = mask_stack(a, nodata=nodata)\r\n    return np.nanstd(a, axis=0)\r\n\r\n\r\ndef stack_var(arrs, nodata=None):\r\n    """"""see stack_stats""""""\r\n    a = check_stack(arrs)\r\n    if nodata is not None:\r\n        a = mask_stack(a, nodata=nodata)\r\n    return np.nanvar(a, axis=0)\r\n\r\n\r\ndef stack_stats(arrs, ax=0, nodata=None):\r\n    """"""All statistics for arrs\r\n    :\r\n    :  arrs - either a list, tuple of arrays or an array with ndim=3\r\n    :  ax - axis, either, 0 (by band) or (1,2) to get a single value for\r\n    :       each band\r\n    :  nodata - nodata value, numeric or np.nan (will upscale integers)\r\n    """"""\r\n    arrs = check_stack(arrs)\r\n    a_m = mask_stack(arrs, nodata=nodata)\r\n    nan_sum = np.nansum(a_m, axis=ax)\r\n    nan_min = np.nanmin(a_m, axis=ax)\r\n    nan_mean = np.nanmean(a_m, axis=ax)\r\n    nan_median = np.nanmean(a_m, axis=ax)\r\n    nan_max = np.nanmax(a_m, axis=ax)\r\n    nan_std = np.nanstd(a_m, axis=ax)\r\n    nan_var = np.nanvar(a_m, axis=ax)\r\n    stats = [nan_sum, nan_min, nan_mean, nan_median, nan_max, nan_std, nan_var]\r\n    if len(ax) == 1:\r\n        nan_cumsum = np.nancumsum(a_m, axis=ax)\r\n        stats.append(nan_cumsum)\r\n    return stats\r\n\r\n\r\ndef expand_zone(a, zone=None, win=2):\r\n    """"""Expand a value (zone) in a 2D array, normally assumed to represent a\r\n    :  raster surface.\r\n    :zone - the value/class to expand into the surrounding cells\r\n    :win - select a (2, 2) or (3, 3) moving window\r\n    :\r\n    """"""\r\n    msg = ""\\nYou need a zone that is within the range of values.""\r\n    if zone is None:\r\n        print(msg)\r\n        return a, None\r\n    if (zone < a.min()) or (zone > a.max()):\r\n        print(msg)\r\n        return a, None\r\n    if win not in (2, 3):\r\n        win = 2\r\n    p = [1, 0][win == 2]  # check for 2 or 3 in win\r\n    ap = np.pad(a, pad_width=(1, p), mode=""constant"", constant_values=(0, 0))\r\n    #n, m = ap.shape\r\n    if win == 2:\r\n        a_c = ap[1:, 1:]  # for 2x2 even\r\n    elif win == 3:\r\n        a_c = ap[1:-1, 1:-1]  # for 3x3 odd\r\n    a_s = stride(ap, win=(win, win), stepby=(win, win))  # stride the array\r\n    r, c = a_s.shape[:2]\r\n    out = []\r\n    x = a_s.shape[0]\r\n    y = a_s.shape[1]\r\n    for i in range(x):\r\n        for j in range(y):\r\n            if zone in a_s[i, j]:\r\n                out.append(1)\r\n            else:\r\n                out.append(0)\r\n    out1 = np.asarray(out).reshape(r, c)\r\n    out = np.repeat(np.repeat(out1, 2, axis=1), 2, axis=0)\r\n    dx, dy = np.array(out.shape) - np.array(a.shape)\r\n    if dx != 0:\r\n        out = out[:dx, :dy]\r\n    final = np.where(out == 1, zone, a_c)\r\n    return final\r\n\r\n\r\ndef fill_arr(a, win=(3, 3)):\r\n    """"""try filling an array""""""\r\n#    fd = np.array([[32, 64, 128], [16, 0, 1], [8, 4, 2]])  # flow direction\r\n#    if (zone < a.min()) or (zone > a.max()) or (zone is None):\r\n#        print(""\\nYou need a zone that is within the range of values."")\r\n#        return a, None\r\n    if win[0] == 3:\r\n        pr = 1\r\n    else:\r\n        pr = 0\r\n    ap = np.pad(a, pad_width=(1, pr), mode=""constant"", constant_values=(0, 0))\r\n    if win == (2, 2):\r\n        a_c = ap[1:, 1:]  # for 2x2 even\r\n        #w, h = win\r\n    elif win == (3, 3):\r\n        #w, h = win\r\n        a_c = ap[1:-1, 1:-1]   # for 3x3 odd\r\n    a_s = stride(a_c, win=win)  # stride the array\r\n    r, c = a_s.shape[:2]\r\n    out = []\r\n    x = a_s.shape[0]\r\n    y = a_s.shape[1]\r\n    for i in range(x):\r\n        for j in range(y):\r\n            # do stuff\r\n            sub = a_s[i, j].ravel()\r\n            edges = np.asarray([sub[:4], sub[5:]]).ravel()\r\n            e_min = edges[np.argmin(edges)]\r\n            if sub[4] < e_min:\r\n                out.append(e_min)\r\n            else:\r\n                out.append(sub[4])\r\n    out = np.asarray(out).reshape(r, c)\r\n    return out  # , a_s, ap, a_c\r\n\r\n\r\n# (xx) reclass_vals .... code section\r\ndef reclass_vals(a, old_vals=None, new_vals=None, mask=False, mask_val=None):\r\n    """"""Reclass an array of integer or floating point values.\r\n    :Requires:\r\n    :--------\r\n    : old_vals - list/array of values to reclassify\r\n    : new_bins - new class values for old value\r\n    : mask - whether the raster contains nodata values or values to\r\n    :        be masked with mask_val\r\n    : Array dimensions will be squeezed.\r\n    :Example:\r\n    :-------\r\n    :  array([[ 0,  1,  2,  3,  4],   array([[1, 1, 1, 1, 1],\r\n    :         [ 5,  6,  7,  8,  9],          [2, 2, 2, 2, 2],\r\n    :         [10, 11, 12, 13, 14]])         [3, 3, 3, 3, 3]])\r\n    """"""\r\n    err = ""Inputs are incorrect...old_vals: {}, new_vals: {}""\r\n    if old_vals is None or new_vals is None:\r\n        print(err.format(old_vals, new_vals))\r\n        return a\r\n    a_rc = np.copy(a)\r\n    args = [old_vals, new_vals]\r\n    if len(old_vals) != len(new_vals):\r\n        print(err.format(*args))\r\n        return a\r\n    old_new = np.array(list(zip(old_vals, new_vals)), dtype=\'int32\')\r\n    for pair in old_new:\r\n        q = (a == pair[0])\r\n        a_rc[q] = pair[1]\r\n    return a_rc\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# (15) reclass .... code section\r\ndef reclass_ranges(a, bins=None, new_bins=None, mask=False, mask_val=None):\r\n    """"""Reclass an array of integer or floating point values based on old and\r\n    :  new range values\r\n    :Requires:\r\n    :--------\r\n    : bins - sequential list/array of the lower limits of each class\r\n    :        include one value higher to cover the upper range.\r\n    : new_bins - new class values for each bin\r\n    : mask - whether the raster contains nodata values or values to\r\n    :        be masked with mask_val\r\n    : Array dimensions will be squeezed.\r\n    :Example:\r\n    :-------\r\n    :  z = np.arange(3*5).reshape(3,5)\r\n    :  bins = [0, 5, 10, 15]\r\n    :  new_bins = [1, 2, 3, 4]\r\n    :  z_recl = reclass(z, bins, new_bins, mask=False, mask_val=None)\r\n    :  ==> .... z                     ==> .... z_recl\r\n    :  array([[ 0,  1,  2,  3,  4],   array([[1, 1, 1, 1, 1],\r\n    :         [ 5,  6,  7,  8,  9],          [2, 2, 2, 2, 2],\r\n    :         [10, 11, 12, 13, 14]])         [3, 3, 3, 3, 3]])\r\n    """"""\r\n    err = ""Bins = {} new = {} won\'t work"".format(bins, new_bins)\r\n    if bins is None or new_bins is None:\r\n        print(err)\r\n    a_rc = np.zeros_like(a)\r\n    if len(bins) < 2:  # or (len(new_bins <2)):\r\n        print(err)\r\n        return a\r\n    if len(new_bins) < 2:\r\n        new_bins = np.arange(1, len(bins)+2)\r\n    new_classes = list(zip(bins[:-1], bins[1:], new_bins))\r\n    for rc in new_classes:\r\n        q1 = (a >= rc[0])\r\n        q2 = (a < rc[1])\r\n        a_rc = a_rc + np.where(q1 & q2, rc[2], 0)\r\n    return a_rc\r\n\r\n\r\n# (16) scale .... code section\r\ndef scale_up(a, x=2, y=2, num_z=None):\r\n    """"""Scale the input array repeating the array values up by the\r\n    :  x and y factors.\r\n    :Requires:\r\n    :--------\r\n    : a - an ndarray, 1D arrays will be upcast to 2D\r\n    : x, y - factors to scale the array in x (col) and y (row)\r\n    :      - scale factors must be greater than 2\r\n    : num_z - for 3D, produces the 3rd dimension, ie. if num_z = 3 with the\r\n    :    defaults, you will get an array with shape=(3, 6, 6)\r\n    : how - if num_z != None or 0, then the options are\r\n    :    \'repeat\', \'random\'.  With \'repeat\' the extras are kept the same\r\n    :     and you can add random values to particular slices of the 3rd\r\n    :     dimension, or multiply them etc etc.\r\n    :Returns:\r\n    :-------\r\n    : a = np.array([[0, 1, 2], [3, 4, 5]]\r\n    : b = scale(a, x=2, y=2)\r\n    :   =  array([[0, 0, 1, 1, 2, 2],\r\n    :             [0, 0, 1, 1, 2, 2],\r\n    :             [3, 3, 4, 4, 5, 5],\r\n    :             [3, 3, 4, 4, 5, 5]])\r\n    :Notes:\r\n    :-----\r\n    :  a=np.arange(2*2).reshape(2,2)\r\n    :  a = array([[0, 1],\r\n    :             [2, 3]])\r\n    :  f_(scale(a, x=2, y=2, num_z=2))\r\n    :  Array... shape (3, 4, 4), ndim 3, not masked\r\n    :   0, 0, 1, 1    0, 0, 1, 1    0, 0, 1, 1\r\n    :   0, 0, 1, 1    0, 0, 1, 1    0, 0, 1, 1\r\n    :   2, 2, 3, 3    2, 2, 3, 3    2, 2, 3, 3\r\n    :   2, 2, 3, 3    2, 2, 3, 3    2, 2, 3, 3\r\n    :   sub (0)       sub (1)       sub (2)\r\n    :--------\r\n    """"""\r\n    if (x < 1) or (y < 1):\r\n        print(""x or y scale < 1...\\n{}"".format(scale_up.__doc__))\r\n        return None\r\n    a = np.atleast_2d(a)\r\n    z0 = np.tile(a.repeat(x), y)  # repeat for x, then tile\r\n    z1 = np.hsplit(z0, y)         # split into y parts horizontally\r\n    z2 = np.vstack(z1)            # stack them vertically\r\n    if a.shape[0] > 1:            # if there are more, repeat\r\n        z3 = np.hsplit(z2, a.shape[0])\r\n        z3 = np.vstack(z3)\r\n    else:\r\n        z3 = np.vstack(z2)\r\n    if num_z not in (0, None):\r\n        d = [z3]\r\n        for i in range(num_z):\r\n            d.append(z3)\r\n        z3 = np.dstack(d)\r\n        z3 = np.rollaxis(z3, 2, 0)\r\n    return z3\r\n\r\n\r\ndef _demo_combine():\r\n    """"""demo combine\r\n    dt = [(\'a\', \'<i8\'), (\'b\', \'<i8\'), (\'c\', \'<i8\'), (\'vals\', \'<i8\')]\r\n    """"""\r\n    a = np.array([[0, 0, 0, 4, 4, 4, 1, 1, 1],\r\n                  [0, 0, 0, 4, 4, 4, 1, 1, 1],\r\n                  [0, 0, 0, 4, 4, 4, 1, 1, 1],\r\n                  [2, 2, 2, 1, 1, 1, 2, 2, 2],\r\n                  [2, 2, 2, 1, 1, 1, 2, 2, 2],\r\n                  [2, 2, 2, 1, 1, 1, 2, 2, 2],\r\n                  [1, 1, 1, 4, 4, 4, 0, 0, 0],\r\n                  [1, 1, 1, 4, 4, 4, 0, 0, 0],\r\n                  [1, 1, 1, 4, 4, 4, 0, 0, 0]])\r\n\r\n    b = np.array([[0, 0, 0, 1, 1, 1, 2, 2, 2],\r\n                  [0, 0, 0, 1, 1, 1, 2, 2, 2],\r\n                  [0, 0, 0, 1, 1, 1, 2, 2, 2],\r\n                  [3, 3, 3, 4, 4, 4, 5, 5, 5],\r\n                  [3, 3, 3, 4, 4, 4, 5, 5, 5],\r\n                  [3, 3, 3, 4, 4, 4, 5, 5, 5],\r\n                  [0, 0, 0, 1, 1, 1, 2, 2, 2],\r\n                  [0, 0, 0, 1, 1, 1, 2, 2, 2],\r\n                  [0, 0, 0, 1, 1, 1, 2, 2, 2]])\r\n\r\n    c = np.array([[0, 0, 0, 3, 3, 3, 0, 0, 0],\r\n                  [0, 0, 0, 3, 3, 3, 0, 0, 0],\r\n                  [0, 0, 0, 3, 3, 3, 0, 0, 0],\r\n                  [1, 1, 1, 4, 4, 4, 1, 1, 1],\r\n                  [1, 1, 1, 4, 4, 4, 1, 1, 1],\r\n                  [1, 1, 1, 4, 4, 4, 1, 1, 1],\r\n                  [2, 2, 2, 5, 5, 5, 2, 2, 2],\r\n                  [2, 2, 2, 5, 5, 5, 2, 2, 2],\r\n                  [2, 2, 2, 5, 5, 5, 2, 2, 2]])\r\n#    ret = combine_(*[a, b, c])\r\n    return a, b, c  #, ret\r\n\r\n\r\ndef _demo():\r\n    """"""\r\n    : -\r\n    """"""\r\n    a = np.array([[9, 8, 2, 3, 4, 3, 5, 5, 2, 2],\r\n                  [4, 1, 4, 2, 4, 2, 4, 2, 3, 2],\r\n                  [5, 3, 5, 4, 5, 4, 5, 3, 1, 2],\r\n                  [5, 2, 3, 1, 4, 4, 3, 5, 4, 3],\r\n                  [2, 3, 2, 5, 5, 2, 5, 5, 4, 4],\r\n                  [5, 3, 4, 4, 2, 1, 3, 2, 4, 3],\r\n                  [3, 2, 3, 3, 3, 4, 3, 2, 4, 3],\r\n                  [4, 5, 2, 3, 2, 2, 3, 1, 4, 4],\r\n                  [3, 5, 5, 5, 2, 2, 4, 3, 4, 4],\r\n                  [4, 5, 4, 5, 3, 2, 4, 3, 1, 3]])\r\n#    f = np.array([[32, 64, 128], [16, 0, 1], [8, 4, 2]])\r\n#    out, out2 = expand_zone(a, zone=1, win=(3,3))\r\n    a_rc = reclass_vals(a,\r\n                        old_vals=[1, 3, 5],\r\n                        new_vals=[9, 5, 1],\r\n                        mask=False,\r\n                        mask_val=None)\r\n    return a_rc\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n# https://stackoverflow.com/questions/47861214/\r\n# using-numpy-as-strided-to-retrieve-subarrays-centered-on-main-diagonal\r\n""""""\r\ntheta = inclination of sun from 90 in radians\r\ntheta2 = slope angle\r\nphi = ((450 - sun orientation from north in degrees) mod 360) * 180/pi\r\n""""""\r\n'"
all_scripts/rasterstats.py,33,"b'# -*- coding: UTF-8 -*-ct\r\n""""""\r\nrasterstats\r\n===========\r\n\r\nScript:   rasterstats.py\r\n\r\nAuthor:   Dan.Patterson@carleton.ca\r\n\r\nModified: 2018-03-29\r\n\r\nPurpose:  tools for working with numpy arrays\r\n\r\nRequires:\r\n---------\r\n    arraytools.tools - nd2struct, stride\r\n\r\nReferences:\r\n-----------\r\n  https://community.esri.com/blogs/dan_patterson/2018/02/06/cell-\\\r\nstatistics-made-easy-raster-data-over-time\r\n\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nfrom textwrap import dedent, indent\r\nimport numpy as np\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=5, linewidth=100, precision=2, suppress=True,\r\n                    threshold=150, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n__all__ = [\'check_shapes\', \'check_stack\', \'mask_stack\',\r\n           \'stack_sum\', \'stack_cumsum\',  # statistical functions\r\n           \'stack_prod\', \'stack_cumprod\',\r\n           \'stack_min\', \'stack_mean\',\r\n           \'stack_median\', \'stack_max\',\r\n           \'stack_std\', \'stack_var\',\r\n           \'stack_percentile\',\r\n           \'stack_stats\',\r\n           \'stack_stats_tbl\']\r\n\r\n\r\n# ---- array checks and creation --------------------------------------------\r\n# ---- 3D arrays for stacked operations\r\n#\r\ndef check_shapes(arrs):\r\n    """"""Check the shapes of the arrays to ensure they are all equal\r\n    """"""\r\n    shps = [i.shape for i in arrs]\r\n    eq = np.all(np.array([shps[0] == i for i in shps[1:]]))\r\n    err = ""Arrays arr not of the same shape...""\r\n    if not eq:\r\n        raise ValueError(""{}\\n{}"".format(err, shps))\r\n\r\n\r\ndef check_stack(arrs):\r\n    """"""Do the basic checking of the stack to ensure that a 3D array is\r\n    generated\r\n    """"""\r\n    err1 = ""Object, structured arrays not supported, current type...""\r\n    err2 = ""3D arrays supported current ndim...""\r\n    if isinstance(arrs, (list, tuple)):\r\n        arrs = np.array(arrs)\r\n    if arrs.dtype.kind in (\'O\', \'V\'):\r\n        raise ValueError(""{} {}"".format(err1, arrs.dtype.kind))\r\n    if arrs.ndim != 3:\r\n        raise ValueError(""{} {}"".format(err2, arrs.ndim))\r\n    return arrs\r\n\r\n\r\ndef mask_stack(arrs, nodata=None):\r\n    """"""Produce masks for a 3d array""""""\r\n    if nodata is None:\r\n        return arrs\r\n    m = (arrs[:, ...] == nodata).any(0)\r\n    msk = [m for i in range(arrs.shape[0])]\r\n    msk = np.array(msk)\r\n    a_m = np.ma.MaskedArray(arrs, mask=msk)\r\n    return a_m\r\n\r\n\r\n# ---- Statistics for stacked arrays (3D) ------------------------------------\r\n#\r\ndef stack_sum(arrs, nodata=None):\r\n    """"""see stack_stats""""""\r\n    a = check_stack(arrs)\r\n    if nodata is not None:\r\n        a = mask_stack(a, nodata=nodata)\r\n    return np.nansum(a, axis=0)\r\n\r\n\r\ndef stack_cumsum(arrs, nodata=None):\r\n    """"""see stack_stats""""""\r\n    a = check_stack(arrs)\r\n    if nodata is not None:\r\n        a = mask_stack(a, nodata=nodata)\r\n    return np.nancumsum(a, axis=0)\r\n\r\n\r\ndef stack_prod(arrs, nodata=None):\r\n    """"""see stack_stats""""""\r\n    a = check_stack(arrs)\r\n    if nodata is not None:\r\n        a = mask_stack(a, nodata=nodata)\r\n    return np.nanprod(a, axis=0)\r\n\r\n\r\ndef stack_cumprod(arrs, nodata=None):\r\n    """"""see stack_stats""""""\r\n    a = check_stack(arrs)\r\n    if nodata is not None:\r\n        a = mask_stack(a, nodata=nodata)\r\n    return np.nancumprod(a, axis=0)\r\n\r\n\r\ndef stack_min(arrs, nodata=None):\r\n    """"""see stack_stats""""""\r\n    a = check_stack(arrs)\r\n    if nodata is not None:\r\n        a = mask_stack(a, nodata=nodata)\r\n    return np.nanmin(a, axis=0)\r\n\r\n\r\ndef stack_mean(arrs, nodata=None):\r\n    """"""see stack_stats""""""\r\n    a = check_stack(arrs)\r\n    if nodata is not None:\r\n        a = mask_stack(a, nodata=nodata)\r\n    return np.nanmean(a, axis=0)\r\n\r\n\r\ndef stack_median(arrs, nodata=None):\r\n    """"""see stack_stats""""""\r\n    a = check_stack(arrs)\r\n    if nodata is not None:\r\n        a = mask_stack(a, nodata=nodata)\r\n    return np.nanmedian(a, axis=0)\r\n\r\n\r\ndef stack_max(arrs, nodata=None):\r\n    """"""see stack_stats""""""\r\n    a = check_stack(arrs)\r\n    if nodata is not None:\r\n        a = mask_stack(a, nodata=nodata)\r\n    return np.nanmax(a, axis=0)\r\n\r\n\r\ndef stack_std(arrs, nodata=None):\r\n    """"""see stack_stats""""""\r\n    a = check_stack(arrs)\r\n    if nodata is not None:\r\n        a = mask_stack(a, nodata=nodata)\r\n    return np.nanstd(a, axis=0)\r\n\r\n\r\ndef stack_var(arrs, nodata=None):\r\n    """"""see stack_stats""""""\r\n    a = check_stack(arrs)\r\n    if nodata is not None:\r\n        a = mask_stack(a, nodata=nodata)\r\n    return np.nanvar(a, axis=0)\r\n\r\n\r\ndef stack_percentile(arrs, q=50, nodata=None):\r\n    """"""nanpercentile for an array stack with optional nodata masked\r\n\r\n    -arrs :\r\n        either a list, tuple of arrays or an array with ndim=3\r\n    - q :\r\n        the percentile\r\n    - nodata :\r\n        nodata value, numeric or np.nan (will upscale integers)\r\n    """"""\r\n    a = check_stack(arrs)\r\n    if nodata is not None:\r\n        a = mask_stack(a, nodata=nodata)\r\n    nan_per = np.nanpercentile(a, q=q, axis=0)\r\n    return nan_per\r\n\r\n\r\ndef stack_stats(arrs, ax=0, nodata=None):\r\n    """"""All statistics for arrs.\r\n\r\n    - arrs :\r\n        either a list, tuple of arrays or an array with ndim=3\r\n    - ax :\r\n        axis, either, 0 (by band) or (1,2) to get a single value for each band\r\n    - nodata :\r\n        nodata value, numeric or np.nan (will upscale integers)\r\n    """"""\r\n    arrs = check_stack(arrs)\r\n    a_m = mask_stack(arrs, nodata=nodata)\r\n    nan_sum = np.nansum(a_m, axis=ax)\r\n    nan_min = np.nanmin(a_m, axis=ax)\r\n    nan_mean = np.nanmean(a_m, axis=ax)\r\n    nan_median = np.nanmean(a_m, axis=ax)\r\n    nan_max = np.nanmax(a_m, axis=ax)\r\n    nan_std = np.nanstd(a_m, axis=ax)\r\n    nan_var = np.nanvar(a_m, axis=ax)\r\n    stats = [nan_sum, nan_min, nan_mean, nan_median, nan_max, nan_std, nan_var]\r\n    if len(ax) == 1:\r\n        nan_cumsum = np.nancumsum(a_m, axis=ax)\r\n        stats.append(nan_cumsum)\r\n    return stats\r\n\r\n\r\ndef stack_stats_tbl(arrs, nodata=None):  # col_names, args):\r\n    """"""Produce the output table\r\n    :   (\'N_\', \'<i4\'), (\'N_nan\', \'<i4\')\r\n    """"""\r\n    stats = stack_stats(arrs, ax=(1, 2), nodata=nodata)\r\n    d = [(i, \'<f8\')\r\n         for i in [\'Sum\', \'Min\', \'Mean\', \'Med\', \'Max\', \'Std\', \'Var\']]\r\n    dts = [(\'Band\', \'<i4\'), (\'N\', \'<i4\'), (\'N_nan\', \'<i4\')] + d\r\n    N, r, c = arrs.shape\r\n    cols = len(dts)\r\n    z = np.empty(shape=(N,), dtype=dts)\r\n    z[z.dtype.names[0]] = np.arange(0, N)\r\n    z[z.dtype.names[1]] = np.array([r*c]*N)\r\n    z[z.dtype.names[2]] = np.count_nonzero(arrs == nodata, axis=(1, 2))\r\n    for i in range(cols-3):\r\n        z[z.dtype.names[i+3]] = stats[i]\r\n    return z\r\n\r\n\r\ndef _demo_stack():\r\n    """"""\r\n    demo stack :\r\n        Simply 31 layers shaped (31, 100, 150) with uniform values one for\r\n        each day, numbers from 1 to 31.\r\n    >>> stack_stats_tbl(stack)\r\n    array([( 0, 15000, 0,   15000.,   1.,   1.,   1.,   1.,  0.,  0.),\r\n           ( 1, 15000, 0,   30000.,   2.,   2.,   2.,   2.,  0.,  0.),\r\n           ( 2, 15000, 0,   45000.,   3.,   3.,   3.,   3.,  0.,  0.),\r\n           ( 3, 15000, 0,   60000.,   4.,   4.,   4.,   4.,  0.,  0.),\r\n           ( 4, 15000, 0,   75000.,   5.,   5.,   5.,   5.,  0.,  0.),\r\n           ( 5, 15000, 0,   90000.,   6.,   6.,   6.,   6.,  0.,  0.),\r\n           .... snip ....\r\n           (27, 15000, 0,  420000.,  28.,  28.,  28.,  28.,  0.,  0.),\r\n           (28, 15000, 0,  435000.,  29.,  29.,  29.,  29.,  0.,  0.),\r\n           (29, 15000, 0,  450000.,  30.,  30.,  30.,  30.,  0.,  0.),\r\n           (30, 15000, 0,  465000.,  31.,  31.,  31.,  31.,  0.,  0.)],\r\n          dtype=[(\'Band\', \'<i4\'), (\'N\', \'<i4\'), (\'N_nan\', \'<i4\'),\r\n                 (\'Sum\', \'<f8\'), (\'Min\', \'<f8\'), (\'Mean\', \'<f8\'),\r\n                 (\'Med\', \'<f8\'), (\'Max\', \'<f8\'), (\'Std\', \'<f8\'),\r\n                 (\'Var\', \'<f8\')])\r\n\r\n    """"""\r\n    fname = ""/"".join(script.split(""/"")[:-1]) + ""/Data/Arr_31_100_150.npy""\r\n    stack = np.load(fname)\r\n    return stack\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n    stack = _demo_stack()\r\n'"
all_scripts/rotatepnts.py,6,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\nScript :  movepnts.py\r\n\r\nAuthor :  Dan.Patterson@carleton.ca\r\n\r\nModified : 2018-07-21\r\n\r\nNotes:\r\n-----\r\n- arcpy.da.FeatureClassToNumPyArray(in_table, field_names, {where_clause},\r\n                                    {spatial_reference}, {explode_to_points},\r\n                                    {skip_nulls}, {null_value})\r\n- arcpy.da.NumPyArrayToFeatureClass(in_array, out_table, shape_fields,\r\n                                    {spatial_reference})\r\n\r\n- create multipart polygons: https://geonet.esri.com/message/461451\r\n our house relative to 0,0 in MTM9\r\n xy_shift = [341886,5023462]\r\n:\r\n:Spatial reference\r\n: NAD_1983_CSRS_MTM_9\r\n: WKID: 2951 Authority: EPSG\r\n: in_fc = r\'C:\\GIS\\Table_tools\\Table_tools.gdb\\polygon_demo\'\r\n: dx = 2\r\n: dy = 2\r\n: out_fc = r\'C:\\GIS\\Table_tools\\Table_tools.gdb\\bb\'\r\nC:\\GIS\\A_Tools_scripts\\PointTools\\Point_tools.gdb\\std_dist_center\r\n""""""\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\nfrom arcpytools import fc_info, tweet\r\n\r\nscript = sys.argv[0]\r\n\r\narcpy.env.overwriteOutput = True\r\n\r\ndef trans_rot(a, angle):\r\n    """"""Translate and rotate and array of points about the point cloud origin.\r\n\r\n    Requires:\r\n    ---------\r\n    a : array\r\n        2d array of x,y coordinates\r\n    theta : double\r\n        angle in degrees in the range -180. to 180        \r\n    Returns:\r\n    --------\r\n    Points rotated about the origin and translated back.\r\n    \r\n    Notes:\r\n    ------\r\n    np.einsum(\'ij,kj->ik\', a - cent, R).T  =  np.dot(a - cent, R.T).T\r\n    ik does the rotation in einsum\r\n    \r\n    R = np.array(((c, s), (-s,  c)))  - clockwise about the origin\r\n    """"""\r\n    cent = a.mean(axis=0)\r\n    angle = np.radians(angle)\r\n    c, s = np.cos(angle), np.sin(angle)\r\n    R = np.array(((c, s), (-s,  c)))\r\n    return  np.einsum(\'ij,kj->ik\', a - cent, R) + cent\r\n\r\n\r\n# ---- main section ----\r\n#\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    fc = ""/Point_tools.gdb/std_dist_center""\r\n    flder = ""/"".join(script.split(""/"")[:-2])\r\n    in_fc = flder + fc\r\n    angle = 30.0\r\n    out_fc = flder + ""/Point_tools.gdb/rot_std_dist2""\r\n\r\nelse:\r\n    testing = False\r\n    in_fc = sys.argv[1]\r\n    angle = float(sys.argv[2])\r\n    out_fc = sys.argv[3]\r\n    \r\n\r\n# ---- convert to array, shift and return ----\r\n# Apparently, there can be problems writing directly to a featureclass\r\n# so, write to in_memory changing the required field names, then copy out\r\n#\r\nshp_fld, oid_fld, shp_type, SR = fc_info(in_fc)\r\narr = arcpy.da.FeatureClassToNumPyArray(in_fc, ""*"", """", SR, True)\r\na = arr[shp_fld]\r\nnew_pnts = trans_rot(a, angle)\r\nnms = [\'Feat_id\', \'XYs\'] + [i for i in arr.dtype.names[2:]]\r\narr.dtype.names = nms\r\narr[\'XYs\'] = new_pnts\r\narcpy.da.NumPyArrayToFeatureClass(arr, out_fc, [\'XYs\'])\r\n\r\nmsg = """"""\r\n-------------------------------------\r\nInput points..... {}\r\nRotation angle... {}\r\nOutput points.... {}\r\n""""""\r\ntweet(msg.format(in_fc, angle, out_fc))\r\n# ---- the end ----\r\n'"
all_scripts/sampling_grid.py,25,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   sampling_grid.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2018-01-28\r\n:Purpose:  tools for working with numpy arrays\r\n:Source:\r\n: http://www.arcgis.com/home/item.html?id=ddb2deec9b5e4a09affe60de68f5ff4e\r\n:\r\n:References:\r\n:----------\r\n:Phish_Nyet.py\r\n:  https://community.esri.com/blogs/dan_patterson/2016/09/09/\r\n:        numpy-snippets-3-phishnyet-creating-sampling-grids-using-numpy\r\n:n-gons....\r\n:  https://community.esri.com/blogs/dan_patterson/2016/09/09/\r\n:        n-gons-regular-polygonal-shape-generation\r\n:\r\n:Purpose:\r\n:-------\r\n: - Produce a sampling grid with user defined parameters.\r\n: - create hexagon shapes in two forms, flat-topped and pointy-topped\r\n:\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nfrom textwrap import dedent\r\nimport numpy as np\r\nimport arcpy\r\n\r\narcpy.overwriteOutputs = True\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# ---- main functions ----\r\ndef code_grid(cols=1, rows=1, zero_based=False, shaped=True, bottom_up=False):\r\n    """"""produce spreadsheet like labelling, either zero or 1 based\r\n    :  zero - A0,A1  or ones - A1, A2..\r\n    :  dig = list(\'0123456789\')  # string.digits\r\n    : import string .... string.ascii_uppercase\r\n    """"""\r\n    UC = list(""ABCDEFGHIJKLMNOPQRSTUVWXYZ"")\r\n    z = [1, 0][zero_based]\r\n    rc = [1, 0][zero_based]\r\n    c = [UC[c] + str(r)                # pull in the column heading\r\n         for r in range(z, rows + rc)  # label in the row letter\r\n         for c in range(cols)]         # label in the row number\r\n    c = np.asarray(c)\r\n    if shaped:\r\n        c = c.reshape(rows, cols)\r\n        if bottom_up:\r\n            c = np.flipud(c)\r\n    return c\r\n\r\n\r\ndef rotate(pnts, angle=0):\r\n    """"""rotate points about the origin in degrees, (+ve for clockwise) """"""\r\n    angle = np.deg2rad(angle)                 # convert to radians\r\n    s = np.sin(angle)\r\n    c = np.cos(angle)    # rotation terms\r\n    aff_matrix = np.array([[c, s], [-s, c]])  # rotation matrix\r\n    XY_r = np.dot(pnts, aff_matrix)           # numpy magic to rotate pnts\r\n    return XY_r\r\n\r\n\r\ndef rectangle(dx=1, dy=1, cols=1, rows=1):\r\n    """"""Create the array of pnts to pass on to arcpy using numpy magic\r\n    :  dx - increment in x direction, +ve moves west to east, left/right\r\n    :  dy - increment in y direction, -ve moves north to south, top/bottom\r\n    """"""\r\n    X = [0.0, 0.0, dx, dx, 0.0]       # X, Y values for a unit square\r\n    Y = [0.0, dy, dy, 0.0, 0.0]\r\n    seed = np.array(list(zip(X, Y)))  # [dx0, dy0] keep for insets\r\n    a = [seed + [j * dx, i * dy]       # make the shapes\r\n         for i in range(0, rows)   # cycle through the rows\r\n         for j in range(0, cols)]  # cycle through the columns\r\n    a = np.asarray(a)\r\n    return a\r\n\r\n\r\ndef hex_flat(dx=1, dy=1, cols=1, rows=1):\r\n    """"""generate the points for the flat-headed hexagon\r\n    :dy_dx - the radius width, remember this when setting hex spacing\r\n    :  dx - increment in x direction, +ve moves west to east, left/right\r\n    :  dy - increment in y direction, -ve moves north to south, top/bottom\r\n    """"""\r\n    f_rad = np.deg2rad([180., 120., 60., 0., -60., -120., -180.])\r\n    X = np.cos(f_rad) * dy\r\n    Y = np.sin(f_rad) * dy            # scaled hexagon about 0, 0\r\n    seed = np.array(list(zip(X, Y)))  # array of coordinates\r\n    dx = dx * 1.5\r\n    dy = dy * np.sqrt(3.)/2.0\r\n    hexs = [seed + [dx * i, dy * (i % 2)] for i in range(0, cols)]\r\n    m = len(hexs)\r\n    for j in range(1, rows):  # create the other rows\r\n        hexs += [hexs[h] + [0, dy * 2 * j] for h in range(m)]\r\n    return hexs\r\n\r\n\r\ndef hex_pointy(dx=1, dy=1, cols=1, rows=1):\r\n    """"""pointy hex angles, convert to sin, cos, zip and send\r\n    :dy_dx - the radius width, remember this when setting hex spacing\r\n    :  dx - increment in x direction, +ve moves west to east, left/right\r\n    :  dy - increment in y direction, -ve moves north to south, top/bottom\r\n    """"""\r\n    p_rad = np.deg2rad([150., 90, 30., -30., -90., -150., 150.])\r\n    X = np.cos(p_rad) * dx\r\n    Y = np.sin(p_rad) * dy      # scaled hexagon about 0, 0\r\n    seed = np.array(list(zip(X, Y)))\r\n    dx = dx * np.sqrt(3.)/2.0\r\n    dy = dy * 1.5\r\n    hexs = [seed + [dx * i * 2, 0] for i in range(0, cols)]\r\n    m = len(hexs)\r\n    for j in range(1, rows):  # create the other rows\r\n        hexs += [hexs[h] + [dx * (j % 2), dy * j] for h in range(m)]\r\n    return hexs\r\n\r\n\r\ndef repeat(seed=None, corner=[0, 0], cols=1, rows=1, angle=0):\r\n    """"""Create the array of pnts to pass on to arcpy using numpy magic to\r\n    :  produce a fishnet of the desired in_shp.\r\n    :seed - use grid_array, hex_flat or hex_pointy.  You specify the width\r\n    :       and height or its ratio when making the shapes\r\n    :corner - lower left corner of the shape pattern\r\n    :dx, dy - offset of the shapes... this is different\r\n    :rows, cols - the number of rows and columns to produce\r\n    :angle - rotation angle in degrees\r\n    """"""\r\n    if seed is None:\r\n        a = rectangle(dx=1, dy=1, cols=3, rows=3)\r\n    else:\r\n        a = np.asarray(seed)\r\n    if angle != 0:\r\n        a = [rotate(p, angle) for p in a]      # rotate the scaled points\r\n    pnts = [p + corner for p in a]            # translate them\r\n    return pnts\r\n\r\n\r\ndef output_polygons(output_shp, SR, pnts):\r\n    """"""produce the output polygon shapefile""""""\r\n    msg = \'\\nRead the script header... A projected coordinate system required\'\r\n    assert (SR is not None), msg  # and (SR.type == \'Projected\'), msg\r\n    polygons = []\r\n    for pnt in pnts:                 # create the polygon geometry\r\n        pl = arcpy.Polygon(arcpy.Array([arcpy.Point(*xy) for xy in pnt]), SR)\r\n        polygons.append(pl)\r\n    if arcpy.Exists(output_shp):     # overwrite any existing versions\r\n        arcpy.Delete_management(output_shp)\r\n    arcpy.CopyFeatures_management(polygons, output_shp)\r\n    return output_shp\r\n\r\n\r\ndef extend_tbl(output_shp, rows, cols):\r\n    shp = rows*cols\r\n    code_fld = np.empty((shp,), dtype=[(\'IDs\', \'<i4\'), (\'Grid_codes\', \'<U3\')])\r\n    codes = code_grid(cols=cols, rows=rows, zero_based=False,\r\n                      shaped=True, bottom_up=False).ravel()\r\n    code_fld[\'IDs\'] = np.arange(1, shp+1)\r\n    code_fld[\'Grid_codes\'] = codes\r\n    arcpy.da.ExtendTable(output_shp, \'OBJECTID\', code_fld, \'IDS\')\r\n\r\n\r\nmsg = """"""\r\n: --------------------------------------------------------------------\r\n: output {}\r\n: SR  .. {}\r\n: type . {}\r\n: corner .. {}\r\n: size..... {} (dx, dy)\r\n: cols/rows {}\r\n: sample seed\r\n{}\r\n: --------------------------------------------------------------------\r\n""""""\r\n\r\n\r\ndef _demo(seed=None, out_fc=False, SR=None, corner=[0, 0], angle=0):\r\n    """"""Generate the grid using the specified or default parameters\r\n    """"""\r\n    corner = corner  # [300000.0, 5000000.0]\r\n    dx, dy = [1, 1]\r\n    cols, rows = [3, 3]\r\n    if seed is None:\r\n#        seed = rectangle(dx=1, dy=1, cols=3, rows=3)\r\n        seed = hex_pointy(dx=10, dy=10, cols=3, rows=3)\r\n#        seed = hex_flat(dx=10, dy=10, cols=3, rows=3)\r\n        seed_t = \'rectangle\'\r\n    if SR is None:\r\n        SR = 3857  # -- WGS84 Web Mercator (Auxiliary Sphere)\r\n    pnts = repeat(seed=seed, corner=corner, cols=3, rows=3, angle=0)\r\n    args = ["""", SR, seed_t, corner, [dx, dy], [cols, rows], seed[0]]\r\n    print(dedent(msg).format(*args))\r\n    return pnts\r\n\r\n\r\ndef _tool():\r\n    """"""run when script is from a tool""""""\r\n    out_fc = sys.argv[1]  #\r\n    SR = sys.argv[2]\r\n    seed_t = sys.argv[3]\r\n    xtent = [float(i) for i in sys.argv[4].split("" "")]\r\n    L, B, R, T = xtent\r\n    corn_x = L  # float(sys.argv[4])\r\n    corn_y = T  # float(sys.argv[5])\r\n    dx = float(sys.argv[5])\r\n    dy = float(sys.argv[6]) * -1.0\r\n    cols = int(sys.argv[7])\r\n    rows = int(sys.argv[8])\r\n    #\r\n    angle = float(sys.argv[9])\r\n    corner = [corn_x, corn_y]\r\n    if seed_t == \'rectangle\':\r\n        seed = rectangle(dx, dy, cols, rows)\r\n    elif seed_t == \'hex_pointy\':\r\n        seed = hex_pointy(dx, dy, cols, rows)\r\n    elif seed_t == \'hex_flat\':\r\n        seed = hex_flat(dx, dy, cols, rows)\r\n    else:\r\n        seed = rectangle(dx, dy, cols, rows)\r\n    # ----\r\n    msg = """"""\r\n    : --------------------------------------------------------------------\r\n    : output {}\r\n    : SR  .. {}\r\n    : extent .. {}\r\n    : type . {}\r\n    : corner .. {}\r\n    : size..... {} (dx, dy)\r\n    : cols/rows {}\r\n    : sample seed\r\n    {}\r\n    """"""\r\n    args = [out_fc, SR, xtent, seed_t, corner, [dx, dy],\r\n            [cols, rows], seed[0]]\r\n    arcpy.AddMessage(dedent(msg).format(*args))\r\n    arcpy.GetMessages()\r\n    pnts = repeat(seed=seed, corner=corner, cols=cols, rows=rows, angle=angle)\r\n    return out_fc, SR, pnts, rows, cols\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# .... final code section producing the featureclass and extendtable\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    pnts = _demo()\r\nelse:\r\n    testing = False\r\n    out_fc, SR, pnts, rows, cols = _tool()\r\n#\r\nif not testing:\r\n    output_shp = output_polygons(out_fc, SR, pnts)\r\n    extend_tbl(output_shp, rows, cols)\r\n    print(\'\\nSampling grid was created... {}\'.format(out_fc))\r\n\r\n# ----------------------------------------------------------------------\r\n#\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n'"
all_scripts/savetifftest.py,11,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   .py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2018-xx-xx\r\n:Purpose:  tools for working with numpy arrays\r\n:Useage:\r\n:\r\n:References:\r\n: https://www.lfd.uci.edu/~gohlke/code/tifffile.py.html\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nfrom textwrap import dedent\r\nimport numpy as np\r\nimport io\r\n\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\ndef save_as_txt(fname, a):\r\n    """"""Save a numpy array as a text file determining the format from the\r\n    :  data type\r\n    :\r\n    :Reference:  from numpy savetxt\r\n    :----------\r\n    : savetxt(fname, X, fmt=\'%.18e\', delimiter=\' \', newline=\'\\n\', header=\'\',\r\n    :         footer=\'\', comments=\'# \')\r\n    : fmt - \'%[flag]width[.precision]specifier\'\r\n    : fmt=\'%.18e\'\r\n    """"""\r\n    dt_kind = a.dtype.kind\r\n    l_sze = max(len(str(a.max())), len(str(a.min())))\r\n    frmt = \'%{}{}\'.format(l_sze, dt_kind)\r\n    hdr = ""dtype: {} shape: {}"".format(a.dtype.str, a.shape)\r\n    np.savetxt(fname, a, fmt=frmt, delimiter=\' \',\r\n               newline=\'\\n\', header=hdr, footer=\'\', comments=\'# \')\r\n\r\n\r\ndef _read_bytes(fp, size, error_template=""ran out of data""):\r\n    """"""\r\n    Read from file-like object until size bytes are read.\r\n    Raises ValueError if not EOF is encountered before size bytes are read.\r\n    Non-blocking objects only supported if they derive from io objects.\r\n\r\n    Required as e.g. ZipExtFile in python 2.6 can return less data than\r\n    requested.\r\n    Requires:  import io #####\r\n\r\n    _read_bytes(open(fp, \'rb\'), 100)\r\n    b""\\x93NUMPY\\x01\\x00F\\x00{\'descr\': \'<i4\',\r\n    \'fortran_order\': False, \'shape\': (10, 10), }        \\n\r\n\\x00\\x00\\x00\\x00\\x01\\x00\\x00\\x00\\x02\\x00\\x00\\x00\\x03\\x00\\x00\\x00\\x04\\x00\\x00\\x00""\r\n    """"""\r\n    data = bytes()\r\n    while True:\r\n        # io files (default in python3) return None or raise on\r\n        # would-block, python2 file will truncate, probably nothing can be\r\n        # done about that.  note that regular files can\'t be non-blocking\r\n        try:\r\n            r = fp.read(size - len(data))\r\n            data += r\r\n            if len(r) == 0 or len(data) == size:\r\n                break\r\n        except io.BlockingIOError:\r\n            pass\r\n    if len(data) != size:\r\n        msg = ""EOF: reading %s, expected %d bytes got %d""\r\n        raise ValueError(msg % (error_template, size, len(data)))\r\n    else:\r\n        return data\r\n\r\n\r\ndef read_npy(fp, prn=False):\r\n    """""" read an npy file quickly\r\n    : fp = file path\r\n    :\r\n    : file = ""c:/temp/a01.npy""\r\n    """"""\r\n    frmt = """"""\r\n    Magic {}\r\n    Shape {},  C-contig {}, dtype {}\r\n    """"""\r\n    from numpy.lib import format as format_\r\n    with open(fp, \'rb\') as f:\r\n        major, minor = format_.read_magic(f)\r\n        mag = format_.magic(major, minor)\r\n        shp, is_fortran, dt = format_.read_array_header_1_0(f)\r\n        count = np.multiply.reduce(shp, dtype=np.int64)\r\n        #data = f.readlines()\r\n\r\n        BUFFER_SIZE = 2**18\r\n        max_read_count = BUFFER_SIZE // min(BUFFER_SIZE, dt.itemsize)\r\n        array = np.ndarray(count, dtype=dt)\r\n        for i in range(0, count, max_read_count):\r\n            read_count = min(max_read_count, count - i)\r\n            read_size = int(read_count * dt.itemsize)\r\n            data = format_._read_bytes(f, read_size, ""array data"")\r\n            array[i:i+read_count] = np.frombuffer(data, dtype=dt,\r\n                                                  count=read_count)\r\n        array.shape = shp\r\n    if prn:\r\n        print(dedent(frmt).format(mag, shp, (not is_fortran), dt))\r\n    return array\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n\r\n# not used\r\ndef arr_tif_PIL(a, fname):\r\n    """"""convert an array to a tif using PIL\r\n    :  img = Image.fromarray(a, mode=\'F\') works for only 2D\r\n    """"""\r\n    from PIL import Image\r\n    imgs = []\r\n    for i in a:\r\n        imgs.append(Image.fromarray(i))\r\n    imgs[0].save(fname, compression=""tiff_deflate"", save_all=True,\r\n                 append_images=imgs[1:])\r\n    # ---- done\r\n\r\n\r\n#    print(""Script... {}"".format(script))\r\n#    fname = ""c:/temp/a01.txt""\r\n    a = np.arange(100).reshape(10,10)\r\n#    _demo()\r\n#    save_as_txt(fname, a)\r\n#    import pickle\r\n#\r\n#    file = ""c:/temp/a01.npy""\r\n#    fid = open(file, ""rb"")\r\n##    pk = pickle.load(fid)\r\n#    fid.close()\r\n#    ps = pickle.dumps(a)\r\n#    back = pickle.loads(ps)\r\n\r\n""""""\r\nThis works\r\n\r\nfile = ""c:/temp/a01.npy""\r\nfrom numpy.lib import format\r\nwith open(file, \'rb\') as f:\r\n    a = format.read_array(f)\r\n\r\nfrom numpy.lib import format\r\nwith open(file, \'rb\') as f:\r\n    major, minor = format.read_magic(f)\r\n    mag = format.magic(major, minor)\r\n    shp, is_fortran, dt = format.read_array_header_1_0(f)\r\n    count = np.multiply.reduce(shp, dtype=np.int64)\r\n    data = f.readlines()\r\n\r\n    BUFFER_SIZE = 2**18\r\n    max_read_count = BUFFER_SIZE // min(BUFFER_SIZE, dt.itemsize)\r\n    for i in range(0, count, max_read_count):\r\n        read_count = min(max_read_count, count - i)\r\n        read_size = int(read_count * dtype.itemsize)\r\n        data = _read_bytes(fp, read_size, ""array data"")\r\n        array[i:i+read_count] = np.frombuffer(data, dtype=dtype, count=read_count)\r\n\r\nfor i in data:\r\n    print(""\\n{}\\n"".format(i))\r\n\r\n\r\npickle stuff\r\n-------------\r\nnp.load(fp, mmap_mode=None, allow_pickle=True, fix_imports=True,\r\n        encoding=\'ASCII\'):\r\n\r\nown_fid = False\r\nif isinstance(file, basestring):\r\n    fid = open(file, ""rb"")\r\n    own_fid = True\r\nelif is_pathlib_path(file):\r\n    fid = file.open(""rb"")\r\n    own_fid = True\r\nelse:\r\n    fid = file\r\n...\r\nif it is an npy file, then the prefix will equal the magicprefix\r\n\r\nelif magic == format.MAGIC_PREFIX:  then\r\nformat.read_array(fid, allow_pickle=allow_pickle,\r\n                  pickle_kwargs=pickle_kwargs)\r\n\r\nelse... it isn\'t an nparray and you will have to read a pickle.\r\n\r\ntry: return pickle.load(fid, **pickle_kwargs)\r\n\r\n\r\nencoding=\'ASCII\'\r\nencoding = \'ASCII\' # \'latin1\' or \'bytes\'\r\npickle_kwargs = dict(encoding=encoding, fix_imports=fix_imports)\r\n\r\n""""""\r\n\r\n""""""\r\nfrom https://github.com/numpy/numpy/blob/master/numpy/lib/format.py\r\n\r\nMAGIC_PREFIX = b\'\\x93NUMPY\'\r\n\r\nMAGIC_LEN = len(MAGIC_PREFIX) + 2\r\n\r\nARRAY_ALIGN = 64 # plausible values are powers of 2 between 16 and 4096\r\n\r\nBUFFER_SIZE = 2**18  # size of buffer for reading npz files in bytes\r\n\r\nimport struct\r\nhlength_str = _read_bytes(open(fp, \'rb\'), struct.calcsize(hlength_type), ""array header length"")\r\n# b\'\\x93NUM\'\r\n\r\nheader_length = struct.unpack(hlength_type, hlength_str)[0]\r\n# 1297436307\r\n\r\n""""""\r\n'"
all_scripts/sequences.py,12,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\nsequences\r\n================\r\n\r\nScript:   sequential_funcs.py\r\nAuthor:   Dan.Patterson@carleton.ca\r\nModified: 2018-06-02\r\nPurpose :\r\n    Calculating sequential patterns for fields in geodatabase tables\r\nUseage :\r\n\r\nReferences:\r\n-----------\r\n  http://pro.arcgis.com/en/pro-app/arcpy/functions/\r\n       numpyarraytoraster-function.htm\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\nfrom arcpytools import fc_info, tweet, frmt_rec, _col_format\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\ndef sequences(data, stepsize=0):\r\n    """"""Return a list of arrays of sequences values denoted by stepsize\r\n\r\n    data :\r\n        List/array of values in 1D\r\n    stepsize :\r\n        Separation between the values.  If stepsize=0, sequences of equal\r\n        values will be searched.  If stepsize is 1, then sequences incrementing\r\n        by 1... etcetera.  Stepsize can be both positive or negative\r\n\r\n    >>> # check for incrementing sequence by 1\'s\r\n    d = [1, 2, 3, 4, 4, 5]\r\n    s, o = sequences(d, 1, True)\r\n    # s = [array([1, 2, 3, 4]), array([4, 5])]\r\n    # o = array([[1, 4, 4],\r\n    #            [4, 2, 6]])\r\n\r\n    Notes:\r\n    ------\r\n    For strings, use\r\n\r\n    >>> partitions = np.where(a[1:] != a[:-1])[0] + 1\r\n\r\n    Variants:\r\n    ---------\r\n    Change `N` in the expression to find other splits in the data\r\n\r\n    >>> np.split(data, np.where(np.abs(np.diff(data)) >= N)[0]+1)\r\n\r\n    References:\r\n    -----------\r\n\r\n    `<https://stackoverflow.com/questions/7352684/how-to-find-the-groups-of-\r\n    sequences-elements-from-an-array-in-numpy>`_.\r\n\r\n    `<https://stackoverflow.com/questions/50551776/python-chunk-array-on-\r\n    condition#50551924>`_.\r\n    """"""\r\n    #\r\n    a = np.array(data)\r\n    a_dt = a.dtype.kind\r\n    if a_dt in (\'U\', \'S\'):\r\n        seqs = np.split(a, np.where(a[1:] != a[:-1])[0] + 1)\r\n    elif a_dt in (\'i\', \'f\'):\r\n        seqs = np.split(a, np.where(np.diff(a) != stepsize)[0] + 1)\r\n    vals = [i[0] for i in seqs]\r\n    cnts = [len(i) for i in seqs]\r\n    seq_num = np.arange(len(cnts))\r\n    too = np.cumsum(cnts)\r\n    frum = np.zeros_like(too)\r\n    frum[1:] = too[:-1]\r\n    dt = [(\'ID\', \'<i4\'), (\'Value\', a.dtype.str), (\'Count\', \'<i4\'),\r\n          (\'From_\', \'<i4\'), (\'To_\', \'<i4\')]\r\n    out = np.array(list(zip(seq_num, vals, cnts, frum, too)), dtype=dt)\r\n    return out\r\n\r\n\r\n# ---- Run options: _demo or from _tool\r\n#\r\ndef _demo():\r\n    """"""Code to run if in demo mode\r\n    Requires:\r\n        arcpytools fc_info, tweet\r\n    """"""\r\n    tbl = ""Table_tools.gdb/pnts_2k_normal""\r\n    in_tbl = ""/"".join(script.split(""/"")[:-2] + [tbl])\r\n    #\r\n    _, oid_fld, _, _ = fc_info(in_tbl, prn=False)  # run fc_info\r\n    #\r\n    in_fld = \'SequenceTxt\'  #\'Sequences2\'  # \'SequenceTxt\'\r\n    stepsize = 0\r\n    in_flds = [oid_fld, in_fld]   # OBJECTID, plus another field\r\n    a = arcpy.da.TableToNumPyArray(in_tbl, in_flds, skip_nulls=False,\r\n                                   null_value=-1)\r\n#    a = [1, 1, 2, 2, 3, 3, 3, 4, 4, 5, 5, 5, 5, 4, 4, 3, 3, 3, 2, 1]\r\n    a = a[in_fld]\r\n    out_tbl = None\r\n    return in_tbl, a, in_fld, stepsize, out_tbl\r\n\r\n\r\ndef _tool():\r\n    """"""run when script is from a tool\r\n    """"""\r\n    in_tbl = sys.argv[1]\r\n    in_fld = sys.argv[2]\r\n    stepsize = int(sys.argv[3])\r\n    out_tbl = sys.argv[4]  # output field name\r\n    #\r\n    # ---- main tool section\r\n    _, oid_fld, _, _ = fc_info(in_tbl, prn=False)  # run fc_info\r\n    #\r\n    flds = [oid_fld, in_fld]\r\n    in_arr = arcpy.da.TableToNumPyArray(in_tbl, flds, skip_nulls=False,\r\n                                   null_value=-1)\r\n    a = in_arr[in_fld]  # do stuff with array\r\n    tweet(""{!r:}"".format(a))\r\n    return in_tbl, a, in_fld, stepsize, out_tbl\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# .... final code section producing the featureclass and extendtable\r\n#\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    in_tbl, a, in_fld, stepsize, out_tbl = _demo()\r\nelse:\r\n    testing = False\r\n    in_tbl, a, in_fld, stepsize, out_tbl = _tool()\r\n\r\n\r\nmsg = """"""\r\n---- sequences ------------------------------------------------------\r\nProcessing ... {}\r\ninput field .. {}\r\nstep size  ... {} (difference between adjacent values)\r\noutput table . {}\r\n\r\n----\r\nValue : value in the field\r\nCount : number of observations in that sequence\r\nFrom_ : start location of the sequence (includes this index)\r\nTo_   : end location of the sequence (up to but not including)\r\nNoData: -1\r\n""""""\r\n\r\ntweet(msg.format(in_tbl, in_fld, stepsize, out_tbl))\r\n\r\nout = sequences(a, stepsize=0)\r\nif out_tbl not in (""#"", """", "" "", None, \'None\'):\r\n    arcpy.da.NumPyArrayToTable(out, out_tbl)\r\nprn = frmt_rec(out[:50], 0, True, False)\r\ntweet(prn)\r\n#\r\n## ---- reassemble the table for extending ----\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n'"
all_scripts/sequential_funcs.py,24,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\nsequential_funcs\r\n================\r\n\r\nScript:   sequential_funcs.py\r\n\r\nAuthor:   Dan.Patterson@carleton.ca\r\n\r\nModified: 2018-06-04\r\n\r\nPurpose :\r\n    Calculating sequential values for fields in geodatabase tables\r\n\r\nUseage :\r\n\r\nReferences\r\n----------\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/numpyarraytotable.htm>`_.\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/tabletonumpyarray.htm>`_.\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\nfrom arcpytools import fc_info, tweet\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\ndef has_nulls(a):\r\n    """"""Check to see if nulls are in the array passed from the featureclass\r\n    """"""\r\n    #\r\n    a_kind = a.dtype.kind\r\n    if a_kind == \'i\':\r\n        m = a == np.iinfo(np.int32).min\r\n    elif a_kind == \'f\':\r\n        m = np.isnan(a)\r\n    else:\r\n        m = a == None\r\n    return m\r\n\r\n\r\ndef tbl_2_nparray(in_tbl, flds):\r\n    """"""Form the TableToNumPyArray to account for nulls for various dtypes.\r\n    This is essentially a shortcut to `arcpy.da.TableToNumPyArray`\r\n\r\n    Requires\r\n    --------\r\n    `in_tbl` :\r\n        table, or featureclass table name\r\n    `flds` :\r\n        list of field names\r\n    `skip_nulls` = False :\r\n        set within function\r\n    `null_value` :\r\n        determined from the dtype of the array...\r\n        otherwise you may as well do it manually\r\n\r\n    Source\r\n    ------\r\n    arraytools, apt.py module\r\n    """"""\r\n    nulls = {\'Double\':np.nan,\r\n             \'Integer\':np.iinfo(np.int32).min,\r\n             \'OID\':np.iinfo(np.int32).min,\r\n             \'String\':""None""}\r\n    #\r\n    fld_dict = {i.name: i.type for i in arcpy.ListFields(in_tbl)}\r\n    null_dict = {f:nulls[fld_dict[f]] for f in flds}\r\n    a = arcpy.da.TableToNumPyArray(in_table=in_tbl,\r\n                                   field_names=flds,\r\n                                   skip_nulls=False,\r\n                                   null_value=null_dict)\r\n    return a\r\n\r\n\r\ndef cum_sum(a):\r\n    """"""Cumulative sum""""""\r\n    return np.nancumsum(a)\r\n\r\ndef max_diff(a):\r\n    """"""diff from max""""""\r\n    return a - np.nanmax(a)\r\n\r\n\r\ndef mean_diff(a):\r\n    """"""diff from mean""""""\r\n    return a - np.nanmean(a)\r\n\r\n\r\ndef median_diff(a):\r\n    """"""diff from median""""""\r\n    return a - np.nanmedian(a)\r\n\r\n\r\ndef min_diff(a):\r\n    """"""diff from min""""""\r\n    return a - np.nanmin(a)\r\n\r\n\r\ndef percent(a):\r\n    """"""value percentage""""""\r\n    m = has_nulls(a)\r\n    if not np.alltrue(m):\r\n        a = np.ma.MaskedArray(a, mask=m)\r\n        return (a/(np.ma.sum(a) * 1.0)) * 100.\r\n    else:\r\n        return (a/(np.sum(a) * 1.)) * 100.\r\n\r\n\r\ndef seq_diff(a):\r\n    """"""Sequential diffs""""""\r\n    return a[1:] - a[:-1]\r\n\r\n\r\ndef seq_number(a):\r\n    """"""Sequentially number the class values in a field\r\n    """"""\r\n    uni = np.unique(a)\r\n    max_sze = [len(i) for i in uni]\r\n    out = np.chararray(len(a), max_sze + 5, True)\r\n    for u in uni:\r\n        idx = np.where(a == u)[0]\r\n        cnt = 0\r\n        for i in idx:\r\n            out[i] = ""{}{:02.0f}"".format(u, cnt)\r\n            cnt += 1\r\n    return out\r\n\r\n\r\ndef val_diff(a, val):\r\n    """"""diff from a value""""""\r\n    return a - val\r\n\r\n\r\ndef z_score(a):\r\n    """"""Z-scores""""""\r\n    return mean_diff(a)/np.nanstd(a)\r\n\r\n\r\ndef form_output(in_tbl, in_arr, out_fld=""Result_"", del_fld=True,\r\n                vals=None, idx=0, xtend=False):\r\n    """"""Form the output table given a field name and join field\r\n\r\n    Requires:\r\n    ---------\r\n\r\n    tbl :\r\n        input table\r\n    fld_name :\r\n        output field names, should contain OBJECTID and desired output field\r\n    vals :\r\n        values for output field\r\n    sze :\r\n        string representation of output field\r\n    idx :\r\n        index to start values from... usually 0 or 1 (ie for sequential)\r\n\r\n    """"""\r\n    desc = arcpy.da.Describe(in_tbl)\r\n    tbl_path = desc[\'path\']\r\n    oid_fld = desc[\'OIDFieldName\']   # \'OBJECTID\'\r\n    fnames = [i.name for i in arcpy.ListFields(in_tbl)]\r\n    if del_fld in (\'True\', \'true\', True, 1):\r\n        del_fld = True\r\n    else:\r\n        del_fld = False\r\n    if out_fld not in fnames:\r\n        out_fld = out_fld\r\n    elif out_fld in fnames and del_fld:\r\n        arcpy.DeleteField_management(in_tbl, out_fld)\r\n        tweet(""\\nDeleting field {}"".format(out_fld))\r\n    else:\r\n        out_fld += \'dup\'\r\n    out_fld = arcpy.ValidateFieldName(out_fld, tbl_path)\r\n    #\r\n    sze = vals.dtype.str\r\n    dt = [(\'IDs\', \'<i4\'), (out_fld, sze)]  # ie \'<f8\'\r\n    out_array = np.zeros((in_arr.shape[0],), dtype=dt)\r\n    out_array[\'IDs\'] = in_arr[oid_fld]\r\n    out_array[out_fld][idx:] = vals\r\n    if xtend:\r\n        arcpy.da.ExtendTable(in_tbl, oid_fld, out_array, \'IDs\')\r\n    return out_array\r\n\r\n\r\n# ---- Run options: _demo or from _tool\r\n#\r\ndef _demo():\r\n    """"""Code to run if in demo mode\r\n    Requires:\r\n        arcpytools fc_info, tweet\r\n    """"""\r\n    tbl = ""Table_tools.gdb/pnts_2k_normal""\r\n    in_tbl = ""/"".join(script.split(""/"")[:-2] + [tbl])\r\n    #\r\n    _, oid_fld, _, _ = fc_info(in_tbl, prn=False)  # run fc_info\r\n    #\r\n    in_fld = \'Unif\'  # \'Sequences2\'  #\'Ys\'\r\n    del_fld = True\r\n    out_fld = \'Result_fld\'\r\n    in_flds = [oid_fld, in_fld]   # OBJECTID, plus another field\r\n    in_arr = tbl_2_nparray(in_tbl, in_flds)\r\n    c = np.array([\'cumulative sum\', \'diff from max\',\r\n                  \'diff from mean\', \'diff from median\',\r\n                  \'diff from min\', \'diff from value\',\r\n                  \'percent\', \'sequential diff\',\r\n                  \'sequential number\',\r\n                  \'z_score\'])\r\n    func = \'percent\'  #np.random.choice(c)\r\n    xtend = False\r\n    val = None\r\n    return in_tbl, in_arr, in_fld, out_fld, del_fld, func, xtend, val\r\n\r\ndef _tool():\r\n    """"""run when script is from a tool\r\n    """"""\r\n    in_tbl = sys.argv[1]\r\n    in_fld = sys.argv[2]\r\n    func = sys.argv[3]\r\n    out_fld = sys.argv[4]  # output field name\r\n    del_fld = sys.argv[5]\r\n    val = sys.argv[6]\r\n    #\r\n    # ---- main tool section\r\n    _, oid_fld, _, _ = fc_info(in_tbl, prn=False)  # run fc_info\r\n    #\r\n    flds = [oid_fld, in_fld]\r\n    in_arr = tbl_2_nparray(in_tbl, flds)\r\n    tweet(""{!r:}"".format(in_arr))\r\n    xtend = True\r\n    return in_tbl, in_arr, in_fld, out_fld, del_fld, func, xtend, val\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# .... final code section producing the featureclass and extendtable\r\n#\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    in_tbl, in_arr, in_fld, out_fld, del_fld, func, xtend, val = _demo()\r\nelse:\r\n    testing = False\r\n    in_tbl, in_arr, in_fld, out_fld, del_fld, func, xtend, val = _tool()\r\n\r\na = in_arr[in_fld]  # do stuff with array\r\n\r\nif func == \'cumulative sum\':\r\n    result = cum_sum(a)  # sequential diff call\r\n    idx = 0\r\nelif func == \'diff from max\':\r\n    result = max_diff(a)\r\n    idx = 0\r\nelif func == \'diff from mean\':\r\n    result = mean_diff(a)\r\n    idx = 0\r\nelif func == \'diff from median\':\r\n    result = median_diff(a)\r\n    idx = 0\r\nelif func == \'diff from min\':\r\n    result = min_diff(a)\r\n    idx = 0\r\nelif func == \'diff from value\':\r\n    idx = 0\r\n    val_orig = val\r\n    try:    val = int(val)\r\n    except:    val = 0\r\n    try:    val = float(val)\r\n    except:    val = 0\r\n    finally:\r\n        frmt = ""Difference value entered... {!r:}... Value used... {!r:}""\r\n        tweet(frmt.format(val_orig, val))\r\n        pass\r\n    result = val_diff(a, val)\r\nelif func == \'percent\':\r\n    result = percent(a)\r\n    idx = 0\r\nelif func == \'sequential diff\':\r\n    result = seq_diff(a)  # sequential diff call\r\n    idx = 1\r\nelif func == \'sequential number\':\r\n    result = \'seq_number\'\r\n    idx = 0\r\nelif func == \'z_score\':\r\n    result = z_score(a)\r\n    idx = 0\r\nelse:\r\n    result = seq_diff(a)\r\n    idx = 1\r\n#\r\n# ---- reassemble the table for extending ----\r\nout_array = form_output(in_tbl,\r\n                        in_arr,\r\n                        out_fld=out_fld,\r\n                        del_fld=del_fld,\r\n                        vals=result,\r\n                        idx=idx,\r\n                        xtend=xtend)\r\nmsg = """"""\r\nProcessing... {}\r\nfunction..... {}\r\ninput field.. {}\r\noutput field. {}\r\n""""""\r\n\r\ntweet(msg.format(in_tbl, func, in_fld, out_fld))\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n'"
all_scripts/sequential_funcs_txt.py,13,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\nsequential_funcs_txt\r\n====================\r\n\r\nScript:   sequential_funcs_txt.py\r\n\r\nAuthor:   Dan.Patterson@carleton.ca\r\n\r\nModified: 2018-06-04\r\n\r\nPurpose :\r\n    Calculating sequential values for fields in geodatabase tables\r\n\r\nUseage :\r\n\r\nReferences\r\n----------\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/numpyarraytotable.htm>`_.\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/tabletonumpyarray.htm>`_.\r\n---------------------------------------------------------------------\r\n\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\nfrom arcpytools import fc_info, tweet\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\ndef has_nulls(a):\r\n    """"""Check to see if nulls are in the array passed from the featureclass\r\n    """"""\r\n    #\r\n    a_kind = a.dtype.kind\r\n    if a_kind == \'i\':\r\n        m = a == np.iinfo(np.int32).min\r\n    elif a_kind == \'f\':\r\n        m = np.isnan(a)\r\n    else:\r\n        m = a == None\r\n    return m\r\n\r\n\r\ndef tbl_2_nparray(in_tbl, flds):\r\n    """"""Form the TableToNumPyArray to account for nulls for various dtypes.\r\n    This is essentially a shortcut to `arcpy.da.TableToNumPyArray`\r\n\r\n    Requires\r\n    --------\r\n    `in_tbl` :\r\n        table, or featureclass table name\r\n    `flds` :\r\n        list of field names\r\n    `skip_nulls` = False :\r\n        set within function\r\n    `null_value` :\r\n        determined from the dtype of the array...\r\n        otherwise you may as well do it manually\r\n\r\n    Source\r\n    ------\r\n    arraytools, apt.py module\r\n    """"""\r\n    nulls = {\'Double\':np.nan,\r\n             \'Integer\':np.iinfo(np.int32).min,\r\n             \'OID\':np.iinfo(np.int32).min,\r\n             \'String\':""None""}\r\n    #\r\n    fld_dict = {i.name: i.type for i in arcpy.ListFields(in_tbl)}\r\n    null_dict = {f:nulls[fld_dict[f]] for f in flds}\r\n    a = arcpy.da.TableToNumPyArray(in_table=in_tbl,\r\n                                   field_names=flds,\r\n                                   skip_nulls=False,\r\n                                   null_value=null_dict)\r\n    return a\r\n\r\n\r\ndef seq_text(a):\r\n    """"""Sequentially number the text class values in a field\r\n    """"""\r\n    uni, counts = np.unique(a, False, False, True)\r\n    max_sze = max([len(i) for i in uni])\r\n    max_cnts = max([len(str(i)) for i in counts])\r\n    frmt = ""{}_{:0{}.0f}""\r\n    out = np.chararray(len(a), max_sze + 5, True)\r\n    for u in uni:\r\n        idx = np.where(a == u)[0]\r\n        cnt = 0\r\n        for i in idx:\r\n            out[i] = frmt.format(u, cnt, max_cnts)\r\n            cnt += 1\r\n    return out\r\n\r\n\r\ndef form_output(in_tbl, in_arr, out_fld=""Result_"", del_fld=True,\r\n                vals=None, idx=0, xtend=False):\r\n    """"""Form the output table given a field name and join field\r\n\r\n    Requires:\r\n    ---------\r\n\r\n    tbl :\r\n        input table\r\n    fld_name :\r\n        output field names, should contain OBJECTID and desired output field\r\n    vals :\r\n        values for output field\r\n    sze :\r\n        string representation of output field\r\n    idx :\r\n        index to start values from... usually 0 or 1 (ie for sequential)\r\n\r\n    """"""\r\n    desc = arcpy.da.Describe(in_tbl)\r\n    tbl_path = desc[\'path\']\r\n    oid_fld = desc[\'OIDFieldName\']   # \'OBJECTID\'\r\n    fnames = [i.name for i in arcpy.ListFields(in_tbl)]\r\n    if del_fld in (\'True\', \'true\', True, 1):\r\n        del_fld = True\r\n    else:\r\n        del_fld = False\r\n    if out_fld not in fnames:\r\n        out_fld = out_fld\r\n    elif out_fld in fnames and del_fld:\r\n        arcpy.DeleteField_management(in_tbl, out_fld)\r\n        tweet(""\\nDeleting field {}"".format(out_fld))\r\n    else:\r\n        out_fld += \'dup\'\r\n    out_fld = arcpy.ValidateFieldName(out_fld, tbl_path)\r\n    #\r\n    sze = vals.dtype.str\r\n    dt = [(\'IDs\', \'<i4\'), (out_fld, sze)]  # ie \'<f8\'\r\n    out_array = np.zeros((in_arr.shape[0],), dtype=dt)\r\n    out_array[\'IDs\'] = in_arr[oid_fld]\r\n    out_array[out_fld][idx:] = vals\r\n    if xtend:\r\n        arcpy.da.ExtendTable(in_tbl, oid_fld, out_array, \'IDs\')\r\n    return out_array\r\n\r\n\r\n# ---- Run options: _demo or from _tool\r\n#\r\ndef _demo():\r\n    """"""Code to run if in demo mode\r\n    Requires:\r\n        arcpytools fc_info, tweet\r\n    """"""\r\n    tbl = ""Table_tools.gdb/pnts_2k_normal""\r\n    in_tbl = ""/"".join(script.split(""/"")[:-2] + [tbl])\r\n    #\r\n    _, oid_fld, _, _ = fc_info(in_tbl, prn=False)  # run fc_info\r\n    #\r\n    in_fld = \'Text01\'  # \'Sequences2\'  #\'Ys\'\r\n    del_fld = True\r\n    out_fld = \'Result_fld\'\r\n    in_flds = [oid_fld, in_fld]   # OBJECTID, plus another field\r\n    in_arr = tbl_2_nparray(in_tbl, in_flds)\r\n    # c = np.array([\'sequential text\'])\r\n    func = \'sequential text\'\r\n    xtend = False\r\n    return in_tbl, in_arr, in_fld, out_fld, del_fld, func, xtend\r\n\r\ndef _tool():\r\n    """"""run when script is from a tool\r\n    """"""\r\n    in_tbl = sys.argv[1]\r\n    in_fld = sys.argv[2]\r\n    func = sys.argv[3]\r\n    out_fld = sys.argv[4]  # output field name\r\n    del_fld = sys.argv[5]\r\n    #\r\n    # ---- main tool section\r\n    _, oid_fld, _, _ = fc_info(in_tbl, prn=False)  # run fc_info\r\n    #\r\n    flds = [oid_fld, in_fld]\r\n    in_arr = tbl_2_np_array(in_tbl, flds)\r\n    tweet(""{!r:}"".format(in_arr))\r\n    xtend = True\r\n    return in_tbl, in_arr, in_fld, out_fld, del_fld, func, xtend\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# .... final code section producing the featureclass and extendtable\r\n#\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    in_tbl, in_arr, in_fld, out_fld, del_fld, func, xtend = _demo()\r\nelse:\r\n    testing = False\r\n    in_tbl, in_arr, in_fld, out_fld, del_fld, func, xtend = _tool()\r\n\r\na = in_arr[in_fld]  # do stuff with array\r\n\r\nif func == \'sequential text\':\r\n    result = seq_text(a)\r\n    idx = 0\r\nelse:\r\n    result = seq_text(a)\r\n    idx = 1\r\n#\r\n# ---- reassemble the table for extending ----\r\nout_array = form_output(in_tbl,\r\n                        in_arr,\r\n                        out_fld=out_fld,\r\n                        del_fld=del_fld,\r\n                        vals=result,\r\n                        idx=idx,\r\n                        xtend=xtend)\r\nmsg = """"""\r\nProcessing... {}\r\nfunction..... {}\r\ninput field.. {}\r\noutput field. {}\r\n""""""\r\n\r\ntweet(msg.format(in_tbl, func, in_fld, out_fld))\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n'"
all_scripts/shape_array.py,13,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   shape_array.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2017-08-16\r\n:Purpose:  Tools for working with arcpy geometry objects and conversion to\r\n:          numpy arrays.\r\n:Useage:\r\n:\r\n:Notes:\r\n:-----\r\n:(1) JSON --- example dictionary... a_polygon.JSON returns a string\r\n:  json.loads(a_polygon.JSON)  # 3 polygons\r\n:  {\'rings\': [[[300010, 5000010],... [300010, 5000010]],\r\n:             [[300010, 5000010],... [300010, 5000010]],\r\n:             [[300005, 5000008],... [300005, 5000008]]],\r\n:   \'spatialReference\': {\'latestWkid\': 2951, \'wkid\': 2146}}\r\n:\r\n:(2) __geo_interface__ --- example return for a 2-part polygon\r\n:  a_polygon.__geo_interface__\r\n:  {\'coordinates\': [[[(300010.0, 5000010.0),... (300010.0, 5000010.0)]],\r\n:                   [[(300010.0, 5000010.0),... (300010.0, 5000010.0)],\r\n:                    [(300005.0, 5000008.0),... (300005.0, 5000008.0)]]],\r\n:   \'type\': \'MultiPolygon\'}\r\n:\r\n:(3) JSON and WKT return strings, that is why you need json.loads or\r\n:  __geo_interface__\r\n:\r\n:  JSON\r\n:  a_polygon.JSON\r\n:  \'{\'rings\' ...snip... \'wkid\': 2146}}\'  # note the \' \' enclosure\r\n:  WKT --- WKT returns a string like JSON\r\n:  a_polygon.WKT\r\n:  \'MULTIPOLYGON(((300010 5000010,... 300010 5000010)),\r\n:                 ((300010 5000010,... 300010 5000010),\r\n:                  (300005 5000008,... 300005 5000008)))\'\r\n:\r\n:(4) cursors .....\r\n:  from arcgisscripting import da\r\n:\r\n:  dir(da)  # the main data access link with underscore functions present\r\n:\r\n:  [\'Describe\', \'Domain\', \'Editor\', \'ExtendTable\', \'FeatureClassToNumPyArray\',\r\n:  \'InsertCursor\', \'ListDomains\', \'ListFieldConflictFilters\', \'ListReplicas\',\r\n:  \'ListSubtypes\', \'ListVersions\', \'NumPyArrayToFeatureClass\',\r\n:  \'NumPyArrayToTable\', \'Replica\', \'SearchCursor\', \'TableToNumPyArray\',\r\n:  \'UpdateCursor\', \'Version\', \'Walk\', \'__doc__\', \'__loader__\', \'__name__\',\r\n:  \'__package__\', \'__spec__\', \'_internal_eq\', \'_internal_sd\', \'_internal_vb\']\r\n:\r\n:  dir(da.SearchCursor)\r\n:\r\n:  [\'__class__\', \'__delattr__\', \'__dir__\', \'__doc__\', \'__enter__\', \'__eq__\',\r\n:  \'__esri_toolinfo__\', \'__exit__\', \'__format__\', \'__ge__\', \'__getattribute__\',\r\n:  \'__getitem__\', \'__gt__\', \'__hash__\', \'__init__\', \'__iter__\', \'__le__\',\r\n:  \'__lt__\', \'__ne__\', \'__new__\', \'__next__\', \'__reduce__\', \'__reduce_ex__\',\r\n:  \'__repr__\', \'__setattr__\', \'__sizeof__\', \'__str__\', \'__subclasshook__\',\r\n:  \'_as_narray\', \'_dtype\', \'fields\', \'next\', \'reset\']\r\n\r\n\r\n:(5) arcpy.da.SearchCursor\r\n:  cur = arcpy.da.SearchCursor(in_table, field_names, {where_clause},\r\n:                             {spatial_reference}, {explode_to_points},\r\n:                             {sql_clause})\r\n:  field_names\r\n:    - flds = [i.name for i in arcpy.ListFields(in_fc)]  # fields, sort after\r\n:    - flds = ""*""  # all fields in order\r\n:    - flds = [\'OBJECTID\', \'Shape\',...]  # specify the fields you want\r\n:  where_clause\r\n:    - specify a where clause\r\n:  spatial_reference\r\n:    - SR.name    \'NAD_1983_CSRS_MTM_9\'\r\n:    - SR.PCSName \'NAD_1983_CSRS_MTM_9\'\r\n:    - SR.PCSCode 2951\r\n:  explode_to_points\r\n:    - True or False\r\n:  sql_clause\r\n:    - specify one or (None, None)\r\n:\r\n:  For example....\r\n:    args = [in_fc, [\'OBJECTID\', \'Shape\'], None, None,  True, (None, None)]\r\n:    cur = arcpy.da.SearchCursor(*args)\r\n:    a = cur._as_narray()\r\n:\r\n:Timing tests:\r\n:------------\r\n:  a.shape => (1814562, 2) 1,814,462 points, from 3 multipart arrays\r\n:\r\n:  a = _cursor_array(in_fc, full=True)  Time: 4.68e+01s for 3 objects\r\n:  a = _cursor_array(in_fc, full=False) Time: 2.29e+00s for 1,814,562 objects\r\n:\r\n:  a = _cursor_shp(in_fc, full=True)    Time: 1.26e+00s for 3 objects\r\n:  a = _cursor_shp(in_fc, full=False)   Time: 8.22e-01s for 3 objects\r\n:\r\n:  b = _geo_array(a)\r\n:\r\n: Timing function for... _geo_array\r\n:   Time: 4.50e+01s for 3 objects\r\n:\r\n:Notes:\r\n:Polygons\r\n: information - __doc__, __module__, __type_string__\r\n: conversion - JSON, WKB, WKT, __geo_interface__, _fromGeoJson\r\n: properties - \'area\', \'boundary\', \'centroid\', \'convexHull\', \'equals\',\r\n:    \'firstPoint, \'extent\', \'isMultipart\', \'hullRectangle\', \'labelPoint\',\r\n:    \'lastPoint\', \'length\', \'length3D\', \'partCount\', \'pointCount\',\r\n:    \'spatialReference\', \'trueCentroid\', \'type\'\r\n:\r\n: methods - angleAndDistanceTo, buffer, clip, contains, crosses, cut, densify,\r\n:    difference, disjoint, distanceTo, generalize, getArea, getGeohash,\r\n:    getLength, getPart, intersect, measureOnLine, overlaps,\r\n:    pointFromAngleAndDistance, positionAlongLine, projectAs,\r\n:    queryPointAndDistance, segmentAlongLine, snapToLine, symmetricDifference,\r\n:    touches, union, within\r\n:\r\n:\r\n:References:\r\n:----------\r\n: arcpy.da.SearchCursor\r\n:   http://pro.arcgis.com/en/pro-app/arcpy/data-access/searchcursor-class.htm\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport os\r\nfrom textwrap import dedent, indent\r\nimport numpy as np\r\nimport arcpy\r\n\r\nfrom arraytools import fc_info, time_deco\r\nimport arraytools as art\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\r\n      \'float\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=3, linewidth=80, precision=2, suppress=True,\r\n                    threshold=80, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n__imports__ = [\'time_deco\',\r\n               \'fc_info\']\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# ---- extras ----\r\n\r\n\r\ndef flatten(a_list, flat_list=None):\r\n    """"""Change the isinstance as appropriate\r\n    :  Flatten an object using recursion\r\n    :  see: itertools.chain() for an alternate method of flattening.\r\n    """"""\r\n    if flat_list is None:\r\n        flat_list = []\r\n    for item in a_list:\r\n        if isinstance(item, (list, tuple, np.ndarray, np.void)):\r\n            flatten(item, flat_list)\r\n        else:\r\n            flat_list.append(item)\r\n    return flat_list\r\n\r\n\r\ndef flatten_shape(shp, completely=False):\r\n    """"""Flatten a shape using itertools.\r\n    : shp - shape, polygon, polyline, point\r\n    : completely - True, returns points for all objects\r\n    :            - False, returns Array for polygon or polyline objects\r\n    : Notes:\r\n    :------\r\n    : __iter__ - Polygon, Polyline, Array all have this property... Points\r\n    :            do not.\r\n    """"""\r\n    import itertools\r\n    if completely:\r\n        vals = [i for i in itertools.chain(shp)]\r\n    else:\r\n        vals = [i for i in itertools.chain.from_iterable(shp)]\r\n    return vals\r\n\r\n\r\ndef unpack(iterable, param=\'__iter__\'):\r\n    """"""Unpack an iterable based on the param(eter) condition using recursion.\r\n    :Notes:\r\n    : - Use \'flatten\' for recarrays or structured arrays\'\r\n    : ---- see main docs for more information and options ----\r\n    : To produce uniform array from this, use the following after this is done.\r\n    :   out = np.array(xy).reshape(len(xy)//2, 2)\r\n    : isinstance(x, (list, tuple, np.ndarray, np.void)) like in flatten above\r\n    """"""\r\n    xy = []\r\n    for x in iterable:\r\n        if hasattr(x, \'__iter__\'):\r\n            xy.extend(unpack(x))\r\n        else:\r\n            xy.append(x)\r\n    return xy\r\n# ----------------------------------------------------------------------\r\n# list files in folder\r\n# import os\r\n# d = path  # the file/folder path\r\n# [os.path.join(d,o) for o in os.listdir(d)\r\n#                    if os.path.isdir(os.path.join(d,o))]\r\n\r\n\r\ndef get_dir(path):\r\n    """"""Get the directory list from a path, excluding geodatabase folders\r\n    """"""\r\n    if os.path.isfile(path):\r\n        path = os.path.dirname(path)\r\n    p = os.path.normpath(path)\r\n    full = [os.path.join(p, v) for v in os.listdir(p)]\r\n    dirlist = [val for val in full if os.path.isdir(val)]\r\n    dirlist.sort()\r\n    return dirlist\r\n\r\n\r\ndef print_folders(path, first=True, prefix=""""):\r\n    """""" Print recursive listing of contents of path """"""\r\n    if first:  # Detect outermost call, print a heading\r\n        print(""Folder listing for....\\n.... {}"".format(path))\r\n        prefix = ""|  ""\r\n        first = False\r\n    dirlist = get_dir(path)\r\n    cp = os.path.commonprefix(dirlist)\r\n#    print(""common prefix {}"".format(cp))\r\n    for d in dirlist:\r\n        fullname = os.path.join(path, d)   # Turn name into full pathname\r\n        if os.path.isdir(fullname):        # If a directory, recurse.\r\n            n = fullname.replace(cp, \'.\'*len(cp) + \'\\\\\')\r\n            # print(prefix + ""- "" + fullname)\r\n            print(prefix + ""- "" + n)  # fullname) # os.path.relpath(fullname))\r\n            p = ""   ""\r\n            print_folders(fullname, first=False, prefix=p)\r\n\r\n\r\n# ---- Cursor functions -----------------------------------------------------\r\n#\r\n@time_deco\r\ndef _to_ndarray(in_fc, flds=None, SR=None, to_pnts=True):\r\n    """"""Convert searchcursor shapes an ndarray quickly.\r\n    :\r\n    :Requires:\r\n    : in_fc - input featureclass\r\n    : SR - spatial reference, or WKID\r\n    :Notes:\r\n    :-----\r\n    :  field_names\r\n    :    [\'OBJECTID\', \'Shape\'], [\'OID@\', \'Shape\'], [\'OID@\', \'SHAPE@WKT\']\r\n    :    [\'OID@\', \'SHAPE@JSON\'], [\'OID@\', \'SHAPE@X\', \'SHAPE@Y\']\r\n    :  cur = arcpy.da.SearchCursor(in_fc, field_names, .....)\r\n    :      =\r\n    :  cur = arcpy.da.SearchCursor(in_fc, [\'OBJECTID\', \'Shape\'], None, None,\r\n    :                              True, (None, None))\r\n    """"""\r\n    flds = [\'OID@\', \'SHAPE@X\', \'SHAPE@Y\']\r\n    cur = arcpy.da.SearchCursor(in_fc, flds, None, \'2951\', True, (None, None))\r\n    flds = cur.fields\r\n    dt = cur._dtype\r\n    a = cur._as_narray()\r\n    return a, flds, dt\r\n\r\n\r\ndef _arr_json(file_out, arr=None):\r\n    """"""send an array out to json format\r\n    :use json_arr to read\r\n    :  no error checking\r\n    """"""\r\n    import json\r\n    import codecs\r\n    json.dump(arr.tolist(), codecs.open(file_out, \'w\', encoding=\'utf-8\'),\r\n              sort_keys=True, indent=4)\r\n    # ----\r\n\r\n\r\n@time_deco\r\ndef get_geom(in_fc):\r\n    """"""just get the geometry object""""""\r\n    coords = [np.asarray(row[0].__geo_interface__[\'coordinates\'])\r\n              for row in arcpy.da.SearchCursor(in_fc, [\'SHAPE@\'])]  # shape@\r\n    # coords = [i.__geo_interface__[\'coordinates\'] for i in geoms]\r\n    return coords  # , g2\r\n\r\n\r\n@time_deco\r\ndef _get_shapes(in_fc):\r\n    """"""Get the shapes from a featureclass.\r\n    :\r\n    :Requires:\r\n    :--------\r\n    : in_fc - the featureclass, SHAPE@ is used to pull the full geometry object\r\n    :\r\n    :Returns:\r\n    :-------\r\n    :  A list of polygon objects in the form\r\n    :  [<Polygon object at....>, ... (<Polygon object at....>]\r\n    """"""\r\n    a = []\r\n    with arcpy.da.SearchCursor(in_fc, \'SHAPE@\') as cursor:  # ""*""\r\n        for row in cursor:\r\n            a += row\r\n    return a, cursor\r\n\r\n\r\ndef _props(a_shape, prn=True):\r\n    """"""Get some basic shape geometry properties\r\n    """"""\r\n    coords = a_shape.__geo_interface__[\'coordinates\']\r\n    sr = a_shape.spatialReference\r\n    props = [\'type\', \'isMultipart\', \'partCount\', \'pointCount\', \'area\',\r\n             \'length\', \'length3D\', \'centroid\', \'trueCentroid\', \'firstPoint\',\r\n             \'lastPoint\', \'labelPoint\']\r\n    props2 = [[\'Name\', sr.name], [\'Factory code\', sr.factoryCode]]\r\n    t = ""\\n"".join([""{!s:<12}: {}"".format(i, a_shape.__getattribute__(i))\r\n                   for i in props])\r\n    t = t + ""\\n"" + ""\\n"".join([""{!s:<12}: {}"".format(*i) for i in props2])\r\n    tc = \'{!r:}\'.format(np.array(coords))\r\n    tt = t + ""\\nCoordinates\\n"" + indent(tc, \'....\')\r\n    if prn:\r\n        print(tt)\r\n    else:\r\n        return tt\r\n\r\n\r\n@time_deco\r\ndef _geo_array(polys):\r\n    """"""Convert the Polygon class, json to an array\r\n    :\r\n    """"""\r\n    arrays = [np.array(pt.__geo_interface__[\'coordinates\'])\r\n              for pt in polys]  # for p in pt]\r\n    return arrays\r\n\r\n\r\n@time_deco\r\ndef _cursor_shp(in_fc, full=True):\r\n    """"""Extract the point geometry from a featureclass\r\n    :\r\n    : in_fc - the featureclass\r\n    : full - True: \'SHAPE@\', False: [\'SHAPE@X\', \'SHAPE@Y\' ]\r\n    """"""\r\n    shp = [[\'SHAPE@X\', \'SHAPE@Y\'], \'SHAPE@\'][full]\r\n    if full:\r\n        a = [row[0].__geo_interface__[\'coordinates\']\r\n             for row in arcpy.da.SearchCursor(in_fc, shp)]\r\n    else:\r\n        a = [row for row in arcpy.da.SearchCursor(in_fc, shp,\r\n                                                  explode_to_points=True)]\r\n    return a\r\n\r\n\r\n#@time_deco\r\ndef _cursor_array(in_fc, full=True):\r\n    """"""Return the the points for a geometry object using a searchcursor.\r\n    :\r\n    : in_fc - the featureclass\r\n    : full - True: \'SHAPE@\', False: [\'SHAPE@X\', \'SHAPE@Y\' ]\r\n    """"""\r\n    shp = [[\'SHAPE@X\', \'SHAPE@Y\'], \'SHAPE@\'][full]\r\n    if full:\r\n        a = [np.asarray(row[0].__geo_interface__[\'coordinates\'])\r\n             for row in arcpy.da.SearchCursor(in_fc, shp)]\r\n    else:\r\n        a = [row for row in arcpy.da.SearchCursor(in_fc, shp,\r\n                                                  explode_to_points=True)]\r\n    a = np.array(a)\r\n    return a\r\n\r\n\r\ndef _cross_3pnts(a):\r\n    """"""Requires 3 points on a plane:\r\n    """"""\r\n    a = np.asarray(a)\r\n    p0, p1, p2 = a\r\n    u, v = a[1:] - a[0]  # p1 - p0, p2 - p0\r\n    #u = unit_vector(u)\r\n    #v = unit_vector(v)\r\n    eq = np.cross(u, v)  # Cross product times one of the points\r\n    d = sum(eq * p0)\r\n    if d > 0.0:\r\n        eq /= d\r\n        d /= d\r\n    else:\r\n        d = 0.0\r\n    return eq, d\r\n\r\n\r\ndef _demo():\r\n    """"""\r\n    : -\r\n    """"""\r\n    pass\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n#    _demo()\r\n    in_fc = r""C:\\Git_Dan\\a_Data\\testdata.gdb\\polygon""\r\n    # in_fc = r""C:\\Git_Dan\\a_Data\\arcpytools_demo.gdb\\Can_0_big_3""\r\n    # in_fc = r""C:\\Data\\Canada\\CAN_adm0.gdb\\CAN_0_sp""\r\n\r\n    #a0 = [[0., 0., 0.,], [4., 0., 3.], [4., 3., 3.]]\r\n    #a0 = np.array([[1.0, 1.0, 1.0], [1.0, 2.0, 0.0], [-1.0, 2.0, 1.0]])\r\n#    path = r\'C:\\Git_Dan\'\r\n#    pobj = [p for p in pathlib.Path(path).iterdir() if p.is_dir()]\r\n#    pstr = [p._str for p in pathlib.Path(path).iterdir() if p.is_dir()]\r\n    \'\'\'\r\n    import pathlib\r\n    p0 = pathlib.Path(r\'C:\\Temp\\a\\aa\\a0\\a00\')\r\n    p0 = \'C:/Git_Dan/arcpytools\'\r\n    p0._parts ... for a list or ...\r\n    p0.parts  ... for tuple\r\n    ... (\'C:\\\\\', \'Temp\', \'a\', \'aa\', \'a0\', \'a00\')\r\n    is_file, is_dir\r\n    Out[33]: [\'C:\\\\\', \'Git_Dan\', \'arcpytools\']\r\n    p0.parent\r\n\r\n    p0.root    # \'\\\\\'\r\n    p0.drive   # \'C:\'\r\n    p0.anchor  # \'C:\\\\\'\r\n    p0.stem    # \'a00\'\r\n    p0.parent  # WindowsPath(\'C:/Temp/a/aa/a0\')\r\n\r\n    os.walk(top[, topdown=True\r\n    \'\'\'\r\n'"
all_scripts/slope_calc_demo.py,56,"b'# coding: utf-8\n""""""\nScript:  slope_calc_demo.py\nAuthor:  Dan.Patterson@carleton.ca\nPurpose: Slope and aspect calculations using numpy\nReferences:\n\nWindows for slope:\nf_dxyz - as implemented in arcmap after Burrough\n   np.array([[1,2,1],\n             [2,0,2],\n             [1,2,1]], dtype=""float64"")\n\nf_plus   - maximum rise/fall after eppl7\n   np.array([[0,1,0],\n             [1,0,1],\n             [0,1,0]], dtype=""float64"")\nf_cross  -\n   np.array([[1,0,1],\n             [0,0,0],\n             [1,0,1]], dtype=""float64"")\nf_d8   t = np.sqrt(2.0)  f_plus + t*cross used for distance\n   np.array([[t,1,t],\n             [1,0,1],\n             [t,1,t]], dtype=""float64"")\n:-----------------------\nNotes:\n:\nSlope calculation   3rd order finite distance Horn (1981) see Burrough\n-----------------\n:    [a, b, c],    [1, 1, 1],\n:    [d, *, f],    [3, 3, 3],\n:    [g, h, i]])   [5, 5, 5]\n    [dz/dx] = ((c + 2f + i) - (a + 2d + g)) / (8 * x_cellsize)\n    [dz/dy] = ((g + 2h + i) - (a + 2b + c)) / (8 * y_cellsize)\n\n      rise_run = sqrt([dz/dx]**2 + [dz/dy]**2)\n               = sqrt((0)**2 + (2)**2) = sqrt(0 + 4) = 2\n      slope = atan( sqrt([dz/dx]**2 + [dz/dy]**2) ) in radians\n            = 63.434 degrees\n      [dz/dx] = ((c + 2f + i) - (a + 2d + g)) / (8 * x_cellsize)\n              = ((1 + 2*3 +5) - (1 + 2*3 +5)) / (8. * 1)\n              = 0.0\n\n      [dz/dy] = ((g + 2h + i) - (a + 2b + c)) / (8 * y_cellsize)\n              = ((5 + 2*5 +5) - (1 + 2*1 +1)) / (8. * 1)\n              = 2.0\n      rise_run = sqrt(([dz/dx]**2 + [dz/dy]**2)\n               = sqrt((0)**2 + (2.0)**2) = sqrt(4.0) = 2.0\n      slope = np.degrees(np.arctan(2)) = 63.43...\n:------------------\nAspect calculation:\n-------------------\n    [dz/dx] = ((c + 2f + i) - (a + 2d + g)) / 8\n    [dz/dy] = ((g + 2h + i) - (a + 2b + c)) / 8\n     aspect = 57.29578 * atan2 ([dz/dy], -[dz/dx])\n        if aspect < 0\n            cell = 90.0 - aspect  else if aspect > 90.0\n            cell = 450.0 - aspect\n         else\n            cell = 90.0 - aspect\n\n          [dz/dx] = ((c + 2f + i) - (a + 2d + g)) / 8.0\n                  = ((85 + 170 + 84)) - (101 + 202 + 101)) / 8.0\n                  = -8.125\n\n          [dz/dy] = ((g + 2h + i) - (a + 2b + c)) / 8.0\n                  = ((101 + 182 + 84) - (101 + 184 + 85)) / 8.0\n                  = -0.375\n\n        aspect = 57.29578 * atan2 ([dz/dy], -[dz/dx])\n                 = 57.29578 * atan2 (-0.375, 8.125)\n                 = -2.64\n\n        aspect rule:\n          if aspect < 0\n            cell = 90.0 - aspect  else if aspect > 90.0\n            cell = 360.0 - aspect + 90.0\n          else\n            cell = 90.0 - aspect\n:--------------------\n    interweave a list: this type works with both dtypes\n       [val for pair in zip(l1, l2) for val in pair]\n    interweave an array: only if they are the same dtype\n      intl = np.ravel(np.column_stack((a,b)))\n\n    years = [\'1999\', \'2000\', \'2001\', \'2002\', \'2003\',\n             \'2004\', \'2005\', \'2006\',\'2007\', \'2008\']\n    text = [z[0] +""_""+ pair[1] for z in zip(years[:-1], years[1:])]\n""""""\n# ---- begin with imports ----\nimport numpy as np\nfrom numpy.lib.stride_tricks import as_strided\nnp.set_printoptions(edgeitems=3, linewidth=80, precision=2,\n                    suppress=True, threshold=100)\nimport matplotlib.pyplot as plt\n\n__all__ = [""slide_a""]\n\n\ndef kernels(k=None):\n    """"""Kernels used for slope calculations\n\n    Requires:\n    ---------\n    - `f_dxyz`: default after ArcMap and Burrough\n    - `f_plus`:  maximum rise/fall after eppl7\n    - `f_cross`\n    - `\'f_d8`\n    """"""\n    if k == \'f_plus\':  # maximum rise/fall after eppl7\n        k = np.array([[0,1,0], [1,0,1], [0,1,0]], dtype=""float64"")\n    elif k == \'f_cross\':\n        k = np.array([[1,0,1], [0,0,0], [1,0,1]], dtype=""float64"")\n    elif k == \'f_d8\':\n        t = np.sqrt(2.0)  # f_plus + t*cross used for distance\n        k = np.array([[t,1,t], [1,0,1], [t,1,t]], dtype=""float64"")\n    else:  # f_dxyz or None or none of the above\n        k = np.array([[1,2,1], [2,0,2], [1,2,1]], dtype=""float64"")\n    return k\n\ndef slide_a(a, block=(3, 3)):\n    """"""Provide a 2D sliding/moving array view.  There is no edge\n    correction for outputs.\n    """"""\n    r, c = block  # 3x3 block default\n    a = np.ascontiguousarray(a)\n    shape = (a.shape[0] - r + 1, a.shape[1] - c + 1) + block\n    strides = a.strides * 2\n    s_a = as_strided(a, shape=shape, strides=strides)\n    return s_a\n\n\ndef angle2azim(val):\n    """"""correct x-oriented angle (in degrees) to N-oriented azimuth""""""\n    if val < 0:\n        az = 90.0 - val\n    elif val > 90.0:\n        az = 450.0 - val\n    else:\n        az = 90.0 - val\n    return az\n\n\ndef a2z(vals):\n    """"""a numpy version of angle2azim for single values""""""\n    out = np.where(vals < 0, 90. - vals,\n                   np.where(vals > 90, 450.0 - vals, 90.0 - vals))\n    return out\n\n\ndef slope_dem(a, cell_size=1, kernel=None):\n    """"""Return slope in degrees for an input array using the 2nd order\n    :  finite difference method\n    """"""\n    def cal_slope(win, cell_size):\n        """"""Calculate the slope for the window""""""\n        dzdx_s = (win[:, 2] - win[:, 0]).sum()/cell_size  # slope: col2 - col0\n        dzdy_s = (win[2] - win[0]).sum()/cell_size        # slope: row2 - row0\n        slope = np.sqrt(dzdx_s**2 + dzdy_s**2)\n        slope = np.rad2deg(np.arctan(slope))\n        return slope\n    # ---- read array and parse to calculate slope\n    a = np.ascontiguousarray(a)\n    ndim = a.ndim\n    shp = a.shape\n    kern = kernels(kernel)  # factor\n    cell_size = (8.0 * cell_size)   # cell size\n    a = a * kern                  # apply slope filter to array\n    if ndim == 2:\n        slope = cal_slope(a, cell_size)\n    elif ndim == 3:\n        slope = [cal_slope(a[i], cell_size)\n                 for i in range(a.shape[0])]  # shape (0,x)\n        slope = np.asarray(slope)\n    elif ndim == 4:\n        s0, s1, s2, s3 = shp\n        slope = [cal_slope(a[i][j], cell_size)\n                 for i in range(a.shape[0])   # shape (0,x,x,x)\n                 for j in range(a.shape[1])]  # shape (x,1,x,x)\n        slope = np.asarray(slope).reshape((s0, s1))\n    return slope\n\n\ndef slope_map():\n    """"""variant of slope """"""\n    f_123 = np.array([1, 2, 3]).reshape((-1, 1))\n    a0 = np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]])\n    a1 = np.array([[1, 1, 1], [3, 3, 3], [5, 5, 5]])\n    a2 = a0 * f_123\n    a3 = [np.rot90(a0, i) for i in range(4)]  # 4 3x3 arrays in each\n    a4 = [np.rot90(a1, i) for i in range(4)]\n    a5 = [np.rot90(a2, i) for i in range(4)]\n    #\n    b0 = np.array([[0, 1, 2], [1, 2, 3], [2, 3, 4]])\n    b1 = b0 * 2\n    b2 = b0 * f_123\n    b3 = [np.rot90(b0, i) for i in range(4)]\n    b4 = [np.rot90(b1, i) for i in range(4)]  # 16 3x3 arrays total\n    b5 = [np.rot90(b2, i) for i in range(4)]\n    #\n    arrs = np.array(a3 + b3 + a4 + b4 + a5 + b5)\n    print(""arrays shape {}"".format(arrs.shape))\n    a_s = np.array(arrs).reshape((6, 4, 3, 3))  # reshape to 4x4\n    return a_s, arrs\n\n\ndef aspect_dem(a):\n    """"""Return aspect relative to north """"""\n    a = np.asarray(a)\n    dzdx_a = (a[:, 2] - a[:, 0]).sum() / 8.0  # aspect: col2 - col0\n    dzdy_a = (a[2] - a[0]).sum() / 8.0        # aspect: row2 - row0\n    s = np.arctan2(dzdy_a, -dzdx_a)\n    s = np.rad2deg(s)\n    aspect = np.where(s < 0, 90. - s,\n                      np.where(s > 90, 450.0 - s, 90.0 - s))\n    return aspect\n\n\ndef aspect_demo():\n    """"""Rotate a simple slope and determine the slope""""""\n    a = np.array([[0, 1, 2], [1, 2, 3], [2, 3, 4]])\n    z = np.array([[1, 0, 1], [2, 2, 2], [3, 4, 3]])\n    j = 0\n    for i in range(4):\n        b = np.rot90(a, i)\n        c = aspect_dem(b)\n        d = np.rot90(z, i)\n        e = aspect_dem(d)\n        print(""\\n({}) Array aspect...{}\\n{}"".format(j, e, d))\n        j += 1\n        print(""\\n({}) Array aspect...{}\\n{}"".format(j, c, b))\n        j += 1\n    return a\n\n\ndef slope_demo():\n    """"""Rotate a simple slope and determine the slope\n       The z-values are as shown, dx is varied to produce the slope\n       values in both degrees and percent""""""\n    # a = np.array([[0, 1, 2], [1, 2, 3], [2, 3, 4]])\n    a = np.array([[1, 1, 1], [3, 3, 3], [5, 5, 5]])\n    j = 0\n    print(""Slope face...\\n{}"".format(a))\n    print("" N     dx    deg.    %"")\n    dx = [0.5, 1., 2, 4, 6, 8, 10, 12.5, 15, 20, 25, 40, 80, 100]\n    i = 1\n    for j in dx:\n        b = slope_dem(a, j)\n        print(""({:<2}) {:>5.1f}{:>7.1f} {:>6.1f}"".format(i, j, b, (200.0/j)))\n        i += 1\n    return a\n\n\ndef main_demo():\n    """"""Demo of the data set below""""""\n    data = np.array([[50, 45, 50],\n                     [30, 30, 30],\n                     [8, 10, 10]], dtype=""float64"")  # for slope\n    # data = np.array([[101,92,85],[101,92,85],[101,91,84]]) # for aspect\n    a = np.arange(36).reshape((6, 6))\n    za = np.array([[0, 1, 2, 3, 3, 3, 2, 1, 0],\n                   [1, 2, 3, 4, 4, 4, 3, 2, 1],\n                   [2, 3, 4, 5, 5, 5, 4, 3, 2],\n                   [3, 4, 5, 5, 5, 5, 5, 4, 3],\n                   [3, 4, 5, 5, 5, 5, 5, 4, 3],\n                   [3, 4, 5, 5, 5, 5, 5, 4, 3],\n                   [2, 3, 4, 5, 5, 5, 4, 3, 2],\n                   [1, 2, 3, 4, 4, 4, 3, 2, 1],\n                   [0, 1, 2, 3, 3, 3, 2, 1, 0]])\n    a = za\n    r, c = a.shape\n    data = slide_a(a, block=(3, 3))\n    slope = slope_dem(data, cell_size=1, kernel=None)\n    slope = np.array(slope)\n    aspect = [aspect_dem(data[i][j])\n              for i in range(data.shape[0])\n              for j in range(data.shape[1])]\n    aspect = np.array(aspect).reshape((r-2, c-2))\n    frmt = ""Dem...\\n{}\\nSlope...\\n{}\\nAspect\\n{}""\n    print(frmt.format(a, slope, aspect))\n    return a, slope, aspect\n\n\ndef plot_grid(a, title=""Grid""):\n    """""" """"""\n    import matplotlib.pyplot as plt\n    block = a\n    plt.legend(""hello"")\n    plt.matshow(block, cmap=plt.cm.gray)  # samplemat(d))\n    plt.show()\n    plt.close()\n\n\n# -------------------------------------------------------------------\nif __name__ == ""__main__"":\n    """"""run sample for slope and aspect determinations for dem data""""""\n    #\n#    a_s, arrs = slope_map()\n#    slope = slope_dem(a_s, cell_size=5)\n#    a = aspect_demo()\n#    a = slope_demo()\n    a, slope, aspect = main_demo()\n\n\n\n""""""\ncell = (8.0*5)  # cell size\nletters = [i for i in ""abcdefghi""]\nlabels = np.array(letters).reshape((3,3))\n#\n\nt = np.sqrt(2.0)\nf_d8 =np.array([[t,1,t], [1,1,1], [t,1,t]], dtype=""float64"")\n#\nprint(""array numbers and letters\\n{}\\n{}\\n"".format(data,labels))\n# ----- Now create the values from the input data and the filter\n\nlead = [""slope"",""aspect""] # np.array(lead)\nargs = [slope, aspect]                # np.array(args)\nfrmt = ""{: <10s} {:>8.3f}\\n""*len(args)\nout = [val for pair in zip(lead, args) for val in pair]\nprint(frmt.format(*out))\n""""""\n\n\n#vals = np.arange(-180.,180.,45.)\n#for v in vals:\n#    az = angle2azim(v)\n#    print(""angle {}  azimuth {}"".format(v,az))\n\n#block = three((3,3))\n#plt.matshow(block, cmap=plt.cm.gray) #samplemat(d))\n#plt.show()\n#plt.close()\n#years = [\'1999\', \'2000\', \'2001\', \'2002\', \'2003\', \'2004\', \'2005\', \'2006\',\'2007\', \'2008\']\n#text = [""{}_{}"".format(z[0], z[1]) for z in zip(years[:-1], years[1:])] #this type works with both dtypes\n#z=-1\n#zz = -(z + 180 % 360 - 180)  \' Convert to the range -180..180'"
all_scripts/sortpnts.py,6,"b'# -*- coding: utf-8 -*-\r\n""""""\r\n:Script:   sortpnts.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2017-02-28\r\n:Purpose: Sort points by X or Y in ascending or descending order\r\n""""""\r\n\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\nfrom arcpytools import fc_info, tweet\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.1f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=100, precision=2,\r\n                    suppress=True, threshold=120, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')\r\n\r\nscript = sys.argv[0]\r\n\r\n# ---- Convert the featureclass to points returning base information ----\r\n# ---- The \'Shape\' field is changed to X and Y to facilitate sorting etc.\r\nin_fc = sys.argv[1]\r\nsrt_order = sys.argv[2]\r\nascend = sys.argv[3]\r\nout_fc = sys.argv[4]\r\n\r\nshp_fld, oid_fld, shp_type, SR = fc_info(in_fc)\r\n\r\na = arcpy.da.FeatureClassToNumPyArray(in_fc, ""*"", """", SR)\r\ndt = [(\'X\', \'<f8\'), (\'Y\', \'<f8\')]\r\nshps = np.array([tuple(i) for i in a[shp_fld]], dtype=dt)\r\n\r\nif srt_order == \'X\':\r\n    idx = np.argsort(shps, order=(\'X\', \'Y\'))\r\nelse:\r\n    idx = np.argsort(shps, order=(\'Y\', \'X\'))\r\n\r\nshps = a[idx]\r\nif not ascend:\r\n    shps = shps[::-1]\r\n\r\narcpy.da.NumPyArrayToFeatureClass(shps, out_fc, shp_fld, SR)\r\n#\r\nfrmt = """"""\\n\\nScript.... {}\\nUsing..... {}\\nSR...{}\\nSorting by... {},\r\nascending... {}\\nProducing ... {}\\n""""""\r\nargs = [script, in_fc, SR.name, srt_order, ascend, out_fc]\r\ntweet(frmt.format(*args))\r\n\r\n# -------------------------------------------------------------------------\r\nif __name__ == ""__main__"":\r\n    """""" No demo  """"""\r\n#    in_fc = r""C:\\GIS\\Geometry_projects\\Spiral_sort\\Polygons\\Parcels.shp""\r\n'"
all_scripts/spaced.py,15,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   spaced.py\r\n:Author:   Dan_Patterson@carleton.ca\r\n:Modified: 2017-04-11\r\n:Purpose:  tools for working with numpy arrays\r\n:\r\n:Original sources:\r\n:----------------\r\n:n_spaced :  ...\\arraytools\\geom\\n_spaced.py\r\n: - n_spaced(L=0, B=0, R=10, T=10, min_space=1, num=10, verbose=True)\r\n:   Produce num points within the bounds specified by the extent (L,B,R,T)\r\n:   L(eft), B, R, T(op) - extent coordinates\r\n:   min_space - minimum spacing between points.\r\n:   num - number of points... this value may not be reached if the extent\r\n:   is too small and the spacing is large relative to it.\r\n:\r\n:arr_struct :  ...\\arcpytools.py\r\n: - array_struct(a, fld_names=[\'X\', \'Y\'], dt=[\'<f8\', \'<f8\']):\r\n:   Convert an array to a structured array\r\n:   a - an ndarray with shape at least (N,2)\r\n:   dt = dtype class\r\n:   names - names for the fields\r\n:\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nfrom textwrap import dedent\r\nimport numpy as np\r\n# import arcpy\r\nfrom arcpytools import array_fc, array_struct, tweet\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\n# ---------------------------------------------------------------------------\r\n# ---- from arraytools.geom ----\r\ndef n_spaced(L=0, B=0, R=10, T=10, min_space=1, num=10, verbose=True):\r\n    """"""Produce num points within the bounds specified by the extent (L,B,R,T)\r\n    :Requires:\r\n    :--------\r\n    :  L(eft), B, R, T(op) - extent coordinates\r\n    :  min_space - minimum spacing between points.\r\n    :  num - number of points... this value may not be reached if the extent\r\n    :        is too small and the spacing is large relative to it.\r\n    """"""\r\n    #\r\n    def _pnts(L, B, R, T, num):\r\n        """"""Create the points""""""\r\n        xs = (R-L) * np.random.random_sample(size=num) + L\r\n        ys = (T-B) * np.random.random_sample(size=num) + B\r\n        return np.array(list(zip(xs, ys)))\r\n\r\n    def _not_closer(a, min_space=1):\r\n        """"""Find the points that are greater than min_space in the extent.""""""\r\n        b = a.reshape(np.prod(a.shape[:-1]), 1, a.shape[-1])\r\n        diff = b - a\r\n        dist = np.einsum(\'ijk,ijk->ij\', diff, diff)\r\n        dist_arr = np.sqrt(dist).squeeze()\r\n        case = ~(np.triu(dist_arr <= min_space, 1)).any(0)\r\n        return a[case]\r\n    #\r\n    cnt = 1\r\n    n = num * 2  # check double the number required as a check\r\n    result = 0\r\n    frmt = ""Examined: {}  Found: {}  Need: {}""\r\n    a0 = []\r\n    while (result < num) and (cnt < 6):  # keep using random points\r\n        a = _pnts(L, B, R, T, num)\r\n        if cnt > 1:\r\n            a = np.vstack((a0, a))\r\n        a0 = _not_closer(a, min_space)\r\n        result = len(a0)\r\n        if verbose:\r\n            print(dedent(frmt).format(n, result, num))\r\n        cnt += 1\r\n        n += n\r\n    # perform the final sample and calculation\r\n    use = min(num, result)\r\n    a0 = a0[:use]  # could use a0 = np.random.shuffle(a0)[:num]\r\n    a0 = a0[np.argsort(a0[:, 0])]\r\n    return a0\r\n\r\n\r\n# ---- main section ---------------------------------------------------------\r\n#\r\naoi = sys.argv[1]  # \'340000 5020000 344999.999999999 5025000 NaN NaN NaN NaN\'\r\nmin_space = int(sys.argv[2])\r\nnum = int(sys.argv[3])\r\nSR = sys.argv[4]\r\nout_fc = sys.argv[5]\r\n\r\nfrmt = """"""\\n\r\nAOI extent for points...\r\n{}\r\nMinimum spacing.... {}\r\nNumber of points... {}\r\nSpatial reference.. {}\r\nOutput featureclass.. {}\\n\r\n""""""\r\nargs = [aoi, min_space, num, SR, out_fc]\r\nmsg = frmt.format(*args)\r\ntweet(msg)\r\n# ---- perform the point creation ----\r\naoi = aoi.split("" "")[:4]             # extent is returned as a string\r\next = [round(float(i)) for i in aoi]\r\nL, B, R, T = ext\r\na = n_spaced(L, B, R, T, min_space, num, verbose=False)\r\nall_flds = [\'X\', \'Y\', \'x_coord\', \'y_coord\']\r\nxy_flds = all_flds[:2]\r\nxy_dt = [\'<f8\', \'<f8\', \'float\', \'float\']\r\na = np.c_[(a, a)]\r\nz = array_struct(a, fld_names=all_flds, dt=xy_dt)\r\n# z = np.zeros((len(a)), dtype=[(\'X\', \'<f8\'), (\'Y\', \'<f8\')])\r\n# fld_names = (\'X\', \'Y\')\r\n# z[\'X\'] = a[:, 0]\r\n# z[\'Y\'] = a[:, 1]\r\nout_fc = array_fc(z, out_fc, xy_flds, SR)\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n    pass\r\n'"
all_scripts/spanning_tree.py,34,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   spanning_tree.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2017-02-27\r\n:\r\n:Original ... mst.py in my github\r\n:  extensive documentation is there.\r\n:\r\n:Purpose: Produce a spanning tree from a point set.  I have yet to confirm\r\n:  whether it constitutes a minimum spanning tree, since the implementation\r\n:  doesn\'t specify whether Prim\'s algorithm is being used (see ref. 2)\r\n:\r\n:References:\r\n:-----------\r\n:  http://stackoverflow.com/questions/41903502/\r\n:         sort-two-dimensional-list-python\r\n:  http://peekaboo-vision.blogspot.ca/2012/02/simplistic-minimum-\r\n:         spanning-tree-in.html\r\n:  also referenced here...\r\n:      http://stackoverflow.com/questions/34374839/minimum-spanning-tree-\r\n:           distance-and-graph\r\n:Notes:\r\n:-----\r\n: array \'a\' array([[ 0,  0],  constructed for minimum spanning tree example\r\n:                  [ 0,  8],\r\n:                  [10,  8],\r\n:                  [10,  0],\r\n:                  [ 3,  4],\r\n:                  [ 7,  4]])\r\n:(1)  sorting - np.lexsort((a[:,1], a[:,0])) sort by x, then y\r\n:               np.lexsort(a.T) >= np.lexsort((a[:,0], a[:,1])) sort y, x\r\n:(2) Distances\r\n:  unsorted....\r\n:     np.linalg.norm(a[1:] - a[:-1], axis=1)\r\n:         array([ 8.0,  10.0,  8.0,  8.1,  4.0])\r\n:     np.sum(np.linalg.norm(a[1:] - a[:-1], axis=1)) => 38.0622...\r\n:  sorted....\r\n:     a_srt = a[np.lexsort(a.T),:]\r\n:     np.linalg.norm(a_srt[1:] - a_srt[:-1], axis=1)\r\n:     array([ 8.0,  5.0,  4.0,  5.0,  8.0])\r\n:     np.sum(np.linalg.norm(a_srt[1:] - a_srt[:-1], axis=1)) => 30.0...\r\n:\r\n:(3) Near results...\r\n:------------\r\n:  coords, dist, n_array = n_near(s, N=2)\r\n:  ie   ID     Xo    Yo  C0_x C0_y   C1_x C1_y   Dist0 Dist1\r\n:     ([(0,  0.0, 0.0,  3.0, 4.0,   0.0, 8.0,  5.0,  8.0),\r\n:       (1,  0.0, 8.0,  3.0, 4.0,   0.0, 0.0,  5.0,  8.0),\r\n:       (2,  3.0, 4.0,  7.0, 4.0,   0.0, 0.0,  4.0,  5.0),\r\n:       (3,  7.0, 4.0,  3.0, 4.0,  10.0, 8.0,  4.0,  5.0),\r\n:       (4, 10.0, 8.0,  7.0, 4.0,  10.0, 0.0,  5.0,  8.0),\r\n:       (5, 10.0, 0.0,  7.0, 4.0,  10.0, 8.0,  5.0,  8.0)],\r\n:      dtype=[(\'ID\', \'<i4\'),\r\n:             (\'Xo\', \'<f8\'), (\'Yo\', \'<f8\'),\r\n:             (\'C0_X\', \'<f8\'), (\'C0_Y\', \'<f8\'),\r\n:             (\'C1_X\', \'<f8\'), (\'C1_Y\', \'<f8\'),\r\n:             (\'Dist0\', \'<f8\'), (\'Dist1\', \'<f8\')])\r\n:  Connections\r\n:   o_d    array([(0, 2, 5.0),\r\n:                 (2, 3, 4.0),\r\n:                 (2, 1, 5.0),\r\n:                 (3, 4, 5.0),\r\n:                 (3, 5, 5.0)],\r\n:                 dtype=[(\'Orig\', \'<i4\'), (\'Dest\', \'<i4\'), (\'Dist\', \'<f8\')])\r\n:\r\n:  a[o_d[\'Orig\']]     a[o_d[\'Dest\']]\r\n:  array([[ 0,  0],   array([[10,  8],\r\n:         [10,  8],          [10,  0],\r\n:         [10,  8],          [ 0,  8],\r\n:         [10,  0],          [ 3,  4],\r\n:         [10,  0]])         [ 7,  4]])\r\n:  distance array\r\n:  array([[ 5.0,  8.0,  8.1,  10.0,  12.8],\r\n:         [ 5.0,  8.0,  8.1,  10.0,  12.8],\r\n:         [ 4.0,  5.0,  5.0,   8.1,   8.1],\r\n:         [ 4.0,  5.0,  5.0,   8.1,   8.1],\r\n:         [ 5.0,  8.0,  8.1,  10.0,  12.8],\r\n:         [ 5.0,  8.0,  8.1,  10.0,  12.8]])\r\n:\r\n:Back to the original distance and sorted array, a_srt.\r\n:  The distances are determined using the sorted points, the diagonal\r\n:  distances are set to np.inf so that they have the maximal distance.\r\n:  The distance values can be sorted to get their indices in the array\r\n:  Then the array can be sliced to retrieve the points coordinates and the\r\n:  distance array can be sliced to get the distances.\r\n:\r\n:  dix = np.arange(d.shape[0])\r\n:   d[dix, dix] = np.inf\r\n:\r\n: - distance array, \'d\'\r\n:  array([[ inf,  8.0,  5.0,  8.1,  10.0,  12.8],\r\n:         [ 8.0,  inf,  5.0,  8.1,  12.8,  10.0],\r\n:         [ 5.0,  5.0,  inf,  4.0,  8.1,  8.1],\r\n:         [ 8.1,  8.1,  4.0,  inf,  5.0,  5.0],\r\n:         [ 10.0,  12.8,  8.1,  5.0,  inf,  8.0],\r\n:         [ 12.8,  10.0,  8.1,  5.0,  8.0,  inf]])\r\n:\r\n:  np.argsort(d[0]) => array([2, 1, 3, 4, 5, 0])\r\n:\r\n:  a_srt[np.argsort(d[0])]\r\n:    array([[3, 4], [ 0, 8], [7, 4], [10, 0], [10, 8], [0, 0]])\r\n:\r\n:  d[0][np.argsort(d[0])]  => array([ 5.0, 8.0, 8.1, 10.0, 12.8, inf])\r\n: ---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\n#\r\n\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\nfrom arcpytools import fc_info, tweet\r\nfrom textwrap import dedent\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.1f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=100, precision=2,\r\n                    suppress=True, threshold=120,\r\n                    formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')\r\n\r\nscript = sys.argv[0]\r\n\r\n# ---- process and functions ----\r\n# (1) run dist_arr which calls _e_dist\r\n# (2) perform the mst (minimum spanning tree, using Prims algorithm)\r\n# (3) connect the points and return the structured array and then the fc\r\n\r\n\r\ndef dist_arr(a):\r\n    """"""Minimum spanning tree prep... see main header\r\n    : paths from given data set...\r\n    """"""\r\n    # idx = np.lexsort(a.T)  # sort y, then x\r\n    idx = np.lexsort((a[:, 1], a[:, 0]))  # sort X, then Y\r\n    # idx= np.lexsort((a[:,0], a[:,1]))  # sort Y, then X\r\n    a_srt = a[idx, :]\r\n    d = _e_dist(a_srt)\r\n    frmt = """"""\\n    {}\\n    :Input array...\\n    {}\\n\\n    :Sorted array...\r\n    {}\\n\\n    :Distance...\\n    {}\r\n    """"""\r\n    args = [dist_arr.__doc__, a, a_srt, d]  # d.astype(\'int\')]\r\n    print(dedent(frmt).format(*args))\r\n    return idx, a_srt, d\r\n\r\n\r\ndef _e_dist(a):\r\n    """"""Return a 2D square-form euclidean distance matrix.  For other\r\n    :  dimensions, use e_dist in ein_geom.py""""""\r\n    b = a.reshape(np.prod(a.shape[:-1]), 1, a.shape[-1])\r\n    diff = a - b\r\n    d = np.sqrt(np.einsum(\'ijk,ijk->ij\', diff, diff)).squeeze()\r\n    # d = np.triu(d)\r\n    return d\r\n\r\n\r\ndef mst(W, copy_W=True):\r\n    """"""Determine the minimum spanning tree for a set of points represented\r\n    :  by their inter-point distances... ie their \'W\'eights\r\n    :Requires:\r\n    :--------\r\n    :  W - edge weights (distance, time) for a set of points. W needs to be\r\n    :      a square array or a np.triu perhaps\r\n    :Returns:\r\n    :-------\r\n    :  pairs - the pair of nodes that form the edges\r\n    """"""\r\n    if copy_W:\r\n        W = W.copy()\r\n    if W.shape[0] != W.shape[1]:\r\n        raise ValueError(""W needs to be square matrix of edge weights"")\r\n    Np = W.shape[0]\r\n    pairs = []\r\n    pnts_seen = [0]  # Add the first point\r\n    n_seen = 1\r\n    # exclude self connections by assigning inf to the diagonal\r\n    diag = np.arange(Np)\r\n    W[diag, diag] = np.inf\r\n    #\r\n    while n_seen != Np:\r\n        new_edge = np.argmin(W[pnts_seen], axis=None)\r\n        new_edge = divmod(new_edge, Np)\r\n        new_edge = [pnts_seen[new_edge[0]], new_edge[1]]\r\n        pairs.append(new_edge)\r\n        pnts_seen.append(new_edge[1])\r\n        W[pnts_seen, new_edge[1]] = np.inf\r\n        W[new_edge[1], pnts_seen] = np.inf\r\n        n_seen += 1\r\n    return np.vstack(pairs)\r\n\r\n\r\ndef connect(a, dist_arr, edges):\r\n    """"""Return the full spanning tree, with points, connections and distance\r\n    : a - point array\r\n    : dist - distance array, from _e_dist\r\n    : edge - edges, from mst\r\n    """"""\r\n    p_f = edges[:, 0]\r\n    p_t = edges[:, 1]\r\n    d = dist_arr[p_f, p_t]\r\n    n = p_f.shape[0]\r\n    dt = [(\'Orig\', \'<i4\'), (\'Dest\', \'i4\'), (\'Dist\', \'<f8\')]\r\n    out = np.zeros((n,), dtype=dt)\r\n    out[\'Orig\'] = p_f\r\n    out[\'Dest\'] = p_t\r\n    out[\'Dist\'] = d\r\n    return out\r\n\r\n\r\n# ---- main section ----\r\nin_fc = sys.argv[1]\r\nout_fc = sys.argv[2]\r\nshp_fld, oid_fld, shp_type, SR = fc_info(in_fc)\r\nout_flds = [oid_fld, shp_fld]\r\nfrmt = """"""\\nScript.... {}\\nUsing..... {}\\nSR...{}\\n""""""\r\nargs = [script, in_fc, SR.name]\r\nmsg = frmt.format(*args)\r\ntweet(msg)\r\na = arcpy.da.FeatureClassToNumPyArray(in_fc, shp_fld, """", SR)\r\nz = np.zeros((a.shape[0], 2))\r\nz[:, 0] = a[\'Shape\'][:, 0]\r\nz[:, 1] = a[\'Shape\'][:, 1]\r\nidx, a_srt, d = dist_arr(z)\r\npairs = mst(d)\r\no_d = connect(a_srt, d, pairs)\r\n\r\nos = a_srt[pairs[:, 0]]\r\nds = a_srt[pairs[:, 1]]\r\n\r\nfr_to = np.array(list(zip(os, ds)))\r\ns = []\r\nfor pt in fr_to:\r\n    s.append(arcpy.Polyline(arcpy.Array([arcpy.Point(*p) for p in pt]), SR))\r\n\r\nif arcpy.Exists(out_fc):\r\n    arcpy.Delete_management(out_fc)\r\narcpy.CopyFeatures_management(s, out_fc)\r\n\r\n\r\n# ---- demo section ----\r\ndef _demo():\r\n    """"""A sample run demonstrating the principles and workflow""""""\r\n    a = np.array([[0, 0], [0, 8], [10, 8], [10, 0], [3, 4], [7, 4]])\r\n    idx, a_srt, d = dist_arr(a)     # return distance array and sorted pnts\r\n    pairs = mst(d)                  # the orig-dest pairs for the mst\r\n    o_d = connect(a_srt, d, pairs)  # produce an o-d structured array\r\n    return a, d, pairs, o_d\r\n\r\n\r\n# ---------------------------------------------------------------------\r\nif __name__ == ""__main__"":\r\n    """"""Main section...   """"""\r\n#    print(""Script... {}"".format(script))\r\n#    a = np.random.randint(1, 10, size=(10,2))\r\n#    a, d, pairs, o_d = _demo()\r\n'"
all_scripts/spiral.py,29,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   spiral.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2018-02-28\r\n:Purpose:  tools for working with numpy arrays\r\n:Useage:\r\n:\r\n:References:\r\n:\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\n\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=5, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\nroot5 = 2.23606797749978967\r\ngolden = (1.0 + np.sqrt(5.0))/2  # 1.6180339887498948482..\r\n\r\n\r\n# ---- spiral shape generation -----------------------------------------------\r\n#\r\n# ---- arcpy related ----\r\n#\r\ndef extent_scale(ext_fc, scale_by=1.0):\r\n    """"""Scale up/down the extent defined by an extent featureclass by a\r\n    : factor (1 = 100%.  The scaling is done about the center point.\r\n    """"""\r\n    fc_ext = arcpy.Describe(ext_fc).extent\r\n    ext_w = fc_ext.width\r\n    ext_h = fc_ext.height\r\n    buff_dist = min(ext_w, ext_h) * scale_by\r\n    ext_poly = fc_ext.polygon\r\n    ext_buff = ext_poly.buffer(buff_dist)\r\n    new_ext = ext_buff.extent\r\n    cent = ext_poly.centroid\r\n    return new_ext, cent\r\n\r\n\r\ndef output_polylines(output_shp, SR, pnts):\r\n    """"""Produce the output polygon shapefile""""""\r\n    msg = \'\\nRead the script header... A projected coordinate system required\'\r\n    assert (SR is not None), msg  # and (SR.type == \'Projected\'), msg\r\n    polylines = []\r\n    for pair in pnts:                 # create the polygon geometry\r\n        pl = arcpy.Polyline(arcpy.Array([arcpy.Point(*xy) for xy in pair]), SR)\r\n        polylines.append(pl)\r\n    if arcpy.Exists(output_shp):     # overwrite any existing versions\r\n        arcpy.Delete_management(output_shp)\r\n    arcpy.CopyFeatures_management(polylines, output_shp)\r\n    return output_shp\r\n\r\n\r\ndef shapes(sides=5, radius=1.0):\r\n    """"""coordinates of a pentagram with the y-axis as the bisector and the base\r\n    :  on the x-axis\r\n    :  clockwise - a, b, c, d, e\r\n    The vertices will have coordinates (x+rsin\xce\xb8,y+rcos\xce\xb8)(x+rsin\xe2\x81\xa1\xce\xb8,y+rcos\xe2\x81\xa1\xce\xb8)\r\n    , where \xce\xb8 is an integer multiple of 2\xcf\x80/n or 360/n if you prefer degrees\r\n    to radians.)\r\n    """"""\r\n    rad_360 = np.radians(360.).astype(float)\r\n    step = rad_360/sides\r\n    st_end = np.arange(0.0, rad_360+step, step)\r\n    x = np.sin(st_end) * radius\r\n    y = np.cos(st_end) * radius\r\n    pnts = np.c_[x, y]\r\n    return pnts\r\n\r\n\r\n# ---- spiral examples ------------------------------------------------------\r\n#\r\ndef spiral_archim(N, n, clockwise=True, reverse=False):\r\n    """"""Create an Archimedes spiral in the range 0 to N points with \'n\' steps\r\n    : between each incrementstep.  You could use np.linspace\r\n    :Notes: When n is small relative to N, then you begin to form rectangular\r\n    :  spirals, like rotated rectangles\r\n    :Tried:  N = 1000, n = 30\r\n    """"""\r\n    rnge = np.arange(0.0, N+1.0)\r\n    if clockwise:\r\n        rnge = rnge[::-1]\r\n    phi = rnge/n * np.pi\r\n    xs = phi * np.cos(phi)\r\n    ys = phi * np.sin(phi)\r\n    if reverse:\r\n        tmp = np.copy(xs)\r\n        xs = ys\r\n        ys = tmp\r\n    xy = np.c_[xs, ys]\r\n    wdth, hght = np.ptp(xy, axis=0)\r\n    return xs, ys, xy\r\n\r\n\r\ndef spiral_sqr(ULx=-10, n_max=100):\r\n    """"""Create a square spiral from the centre in a clockwise direction\r\n    : ULx = upper left x coordinate, relative to center (0, 0)\r\n    : n-max = maximum number of iterations should ULx not be reached\r\n    :- see spirangle, Ulam spiral\r\n    """"""\r\n    def W(x, y, c):\r\n        x -= c[0]\r\n        return x, y, c\r\n\r\n    def S(x, y, c):\r\n        y -= c[1]\r\n        return x, y, c\r\n\r\n    def E(x, y, c):\r\n        x += c[2]\r\n        return x, y, c\r\n\r\n    def N(x, y, c):\r\n        y += c[3]\r\n        return x, y, c\r\n\r\n    c = np.array([1, 1, 2, 2])\r\n    pos = [0, 0, c]\r\n    n = 0\r\n    v = [pos]\r\n    cont = True\r\n    while cont:\r\n        p0 = W(*v[-1])\r\n        p1 = S(*p0)\r\n        p2 = E(*p1)\r\n        p3 = N(*p2)\r\n        c = c + 2\r\n        p3 = [p3[0], p3[1], c]\r\n        for i in [p0, p1, p2, p3]:\r\n            v.append(i)\r\n        # --- print(p0, p0[0])  # for testing\r\n        if (p0[0] <= ULx):      # bail option 1\r\n            cont = False\r\n        if n > n_max:           # bail option 2\r\n            cont = False\r\n        n = n+1\r\n    coords = np.asarray([np.array([i[0], i[1]]) for i in v])[:-3]\r\n    return coords\r\n\r\n\r\n# -------Excellent one-------------------------------------------------------\r\n#  https://stackoverflow.com/questions/36834505/\r\n#        creating-a-spiral-array-in-python\r\ndef spiral_cw(A):\r\n    A = np.array(A)\r\n    out = []\r\n    while(A.size):\r\n        out.append(A[0])        # take first row\r\n        A = A[1:].T[::-1]       # cut off first row and rotate counterclockwise\r\n    return np.concatenate(out)\r\n\r\n\r\ndef spiral_ccw(A):\r\n    A = np.array(A)\r\n    out = []\r\n    while(A.size):\r\n        out.append(A[0][::-1])    # first row reversed\r\n        A = A[1:][::-1].T         # cut off first row and rotate clockwise\r\n    return np.concatenate(out)\r\n\r\n\r\ndef base_spiral(nrow, ncol):\r\n    return spiral_ccw(np.arange(nrow*ncol).reshape(nrow, ncol))[::-1]\r\n\r\n\r\ndef to_spiral(A):\r\n    A = np.array(A)\r\n    B = np.empty_like(A)\r\n    B.flat[base_spiral(*A.shape)] = A.flat\r\n    return B\r\n\r\n\r\ndef from_spiral(A):\r\n    A = np.array(A)\r\n    return A.flat[base_spiral(*A.shape)].reshape(A.shape)\r\n# ---- end code section--------------------------------------\r\n\r\n\r\ndef _demo():\r\n    """""" demo to create an archimedes spiral ----\r\n    """"""\r\n    # ---- (1) basic parameters\r\n    scale_by = 1.10  # scale output extent so it is slightly bigger than needed\r\n    pnts_cnt = 1000  # points for spiral\r\n    pnts_div = 40.0  # divisions between points\r\n    pth = r""C:\\GIS\\Geometry_projects\\Spiral_sort\\Polygons\\spiral_sort.gdb""\r\n    ext_poly = r""\\extent_line""\r\n    out_sp = r""\\spiral""\r\n    #\r\n    ext_fc = pth + ext_poly\r\n    out_fc = pth + out_sp\r\n    #\r\n    desc = arcpy.da.Describe(ext_fc)  # get infor from extent poly\r\n    SR = desc[\'spatialReference\']\r\n    #\r\n    # ---- (2) create the spiral\r\n    xs, ys, xy = spiral_archim(pnts_cnt, pnts_div)  # (1) make a spiral\r\n    w, h = np.ptp(xy, axis=0)\r\n    #\r\n    ext, cent = extent_scale(ext_fc, scale_by=scale_by)  # (2) get extent info\r\n    x_c = cent.X  # cent is an arcpy point object\r\n    y_c = cent.Y\r\n    x_fac = ext.width/float(w)\r\n    y_fac = ext.height/float(h)\r\n    #\r\n    # ---- (3) put it all together and create the featureclass\r\n    a = np.array(xy)\r\n    a[:, 0] = a[:, 0] * x_fac + x_c\r\n    a[:, 1] = a[:, 1] * y_fac + y_c\r\n    pnts = a.tolist()\r\n    output_polylines(out_fc, SR, [pnts])\r\n    #\r\n    return a\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n#    _demo()\r\n#    pth = r""C:\\Git_Dan\\a_Data\\arcpytools_demo.gdb""\r\n#    in_fc = pth + r""\\r_extent""'"
all_scripts/split_field.py,10,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   .py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2017-xx-xx\r\n:Purpose:  tools for working with numpy arrays\r\n:Useage:\r\n:\r\n:References:\r\n:\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\n\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\ndef tweet(msg):\r\n    """"""Print a message for both arcpy and python.\r\n    : msg - a text message\r\n    """"""\r\n    m = ""\\n{}\\n"".format(msg)\r\n    arcpy.AddMessage(m)\r\n    print(m)\r\n    print(arcpy.GetMessages())\r\n\r\n\r\ndef sp(a, sep="",""):\r\n    """""" split stuff""""""\r\n    name = a.dtype.names\r\n    if name is not None:\r\n        a = a[name[0]]\r\n    shp = a.shape[0]\r\n    a0 = np.char.partition(a, sep="", "")\r\n    n_max = np.max([len(i) for i in a0])\r\n    out = [a0[:, 0]]\r\n    a0 = a0[:, -1]\r\n    for i in range(n_max+1):\r\n        a0 = np.char.partition(a0, \', \')\r\n        out.append(a0[:, 0])\r\n        a0 = a0[:, -1]\r\n    b = np.array(list(zip(out)))\r\n    b = b.squeeze().T\r\n    cnts = np.char.count(a, \', \')\r\n    f = np.empty((shp, max(cnts+1)), dtype=np.unicode)\r\n    for i in range(shp):\r\n        c = cnts[i]\r\n        f[i, :c+1] = [j.strip() for j in a[i].split(\',\')]\r\n    return f\r\n\r\n\r\n# ---- Run options: _demo or from _tool\r\n#\r\ndef _demo():\r\n    """"""Code to run if in demo mode\r\n    """"""\r\n    in_tbl = r""C:\\Git_Dan\\arraytools\\Data\\numpy_demos.gdb\\sample_10k""\r\n    in_fld = \'Test\'\r\n    a = arcpy.da.TableToNumPyArray(in_tbl, in_fld)\r\n#    a = np. array([\'1, 2, 3, 4, 5\', \'a, b, c\', \'6, 7, 8, 9\',\r\n#                   \'d, e, f, g, h\', \'10, 11, 12, 13\'])\r\n    return a\r\n\r\n\r\ndef _tool():\r\n    """"""run when script is from a tool\r\n    """"""\r\n    in_tbl = sys.argv[1]\r\n    in_flds = sys.argv[2]\r\n    out_fld = sys.argv[3]\r\n\r\n    if \';\' in in_flds:\r\n        in_flds = in_flds.split(\';\')\r\n    else:\r\n        in_flds = [in_flds]\r\n\r\n    desc = arcpy.da.Describe(in_tbl)\r\n    tbl_path = desc[\'path\']\r\n    fnames = [i.name for i in arcpy.ListFields(in_tbl)]\r\n    if out_fld in fnames:\r\n        out_fld += \'dup\'\r\n    out_fld = arcpy.ValidateFieldName(out_fld, tbl_path)\r\n    args = [in_tbl, in_flds, out_fld, tbl_path]\r\n    msg = ""in_tbl {}\\nin_fld {}\\nout_fld  {}\\ntbl_path  {}"".format(*args)\r\n    tweet(msg)\r\n    oid = \'OBJECTID\'\r\n    vals = [oid] + in_flds\r\n    arr = arcpy.da.TableToNumPyArray(in_tbl, vals)\r\n    tweet(""{!r:}"".format(arr))\r\n    arcpy.da.ExtendTable(in_tbl, \'OBJECTID\', arr, \'OBJECTID\')\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n    a = _demo()\r\n'"
all_scripts/split_polys.py,38,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\nscript\r\n======\r\n\r\nScript :   split_polys.py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified : 2018-06-14\r\n\r\nPurpose :  tools for working with numpy arrays\r\n\r\nUseage :\r\n\r\nReferences\r\n----------\r\n\r\n`<https://stackoverflow.com/questions/3252194/numpy-and-line-intersections>`_.\r\n\r\n""""""\r\n# pylint: disable=C0103  # invalid-name\r\n# pylint: disable=R0914  # Too many local variables\r\n# pylint: disable=R1710  # inconsistent-return-statements\r\n# pylint: disable=W0105  # string statement has no effect\r\n\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nfrom textwrap import dedent\r\nimport numpy as np\r\nfrom arraytools.fc_tools._common import * # fc_info\r\nfrom arraytools.geom import e_area, _extent\r\nfrom arraytools.fc_tools.apt import arc_np, _id_geom_array, output_polygons\r\nimport arcpy\r\n\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\n\r\nnp.set_printoptions(edgeitems=5, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n# ---- Split poly* geometry using several approaches ----\r\n#\r\n# -- helper functions\r\n\r\ndef _clip_ax(a, x_axis=True, ax_min=0, ax_max=1):\r\n    """"""Clip geometry x or y coordinates and return the result\r\n    a : array\r\n        Input array.  If polygon geometry, then coordinates are ordered\r\n        clockwise for outer rings with the first and last points identical.\r\n    x_axis : boolean\r\n        True for x-axis, False for y-axis\r\n    ax_min, ax_max : number\r\n        Number for the x or y coordinate minimum and maximum\r\n    """"""\r\n    b = np.zeros_like(a)\r\n    if x_axis:\r\n        b[:, 1] = a[:, 1]\r\n        b[:, 0] = np.clip(a[:, 0], ax_min, ax_max)\r\n    else:\r\n        b[:, 0] = a[:, 0]\r\n        b[:, 1] = np.clip(a[:, 1], ax_min, ax_max)\r\n    return b\r\n\r\n\r\ndef _uni_pnts(a):\r\n    """"""Return unique points forming a *poly feature\r\n    """"""\r\n    _, idx = np.unique(a, return_index=True, axis=0)\r\n    a = np.concatenate((a[np.sort(idx)], [a[0]]))\r\n    return a\r\n\r\n# ---- Split extent by a factor ----\r\n#\r\ndef split_ext(a, divisor=2, x_axis=True):\r\n    """"""Split the extent of a feature by a factor/percentage\r\n    a : array\r\n        Input array.\r\n    divisor : factor\r\n        The split factor representing the number of divisions to make over the\r\n        range of the geometry.\r\n    x_axis : boolean\r\n        Split the X-axis if True, otherwise the y_axis\r\n    """"""\r\n    #\r\n    L, B, R, T = _extent(a)\r\n    keep = []\r\n    if x_axis:\r\n        fac = np.linspace(L, R, num=divisor+1, endpoint=True)\r\n        f = fac[:-1]\r\n        t = fac[1:]\r\n        for i in range(divisor):\r\n            k = _clip_ax(a, x_axis=True, ax_min=f[i], ax_max=t[i])\r\n            k = _uni_pnts(k)\r\n            keep.append(k)\r\n    else:\r\n        fac = np.linspace(B, T, num=divisor+1, endpoint=True)\r\n        f = fac[:-1]\r\n        t = fac[1:]\r\n        for i in range(divisor):\r\n            k = _clip_ax(a, x_axis=False, ax_min=f[i], ax_max=t[i])\r\n            k = _uni_pnts(k)\r\n            keep.append(k)\r\n    return keep\r\n\r\n\r\n# ---- Split by area ----\r\n#\r\ndef split_area(a, pieces=4, step_size=10, tol=1.0, x_axis=True):\r\n    """"""Split the extent of a feature by a factor/percentage\r\n    a : array\r\n        Input array.\r\n    pieces : number\r\n        Number of pieces to split the poly* into\r\n    step : factor\r\n        The step is the distance to move in planar units.\r\n    tol : number\r\n        The percentage tolerance in the area\r\n    x_axis : boolean\r\n        Split the X-axis if True, otherwise the y_axis\r\n    """"""\r\n    xs = a[:, 0]\r\n    uni = np.unique(xs)\r\n    ax_min, ax_max = uni[0], uni[-1]\r\n    steps, incr = np.linspace(ax_min, ax_max, num=pieces+1,\r\n                              endpoint=True, retstep=True)\r\n    # ---- Do the work, clip the axes, get the unique points, calculate area\r\n    arrs = []\r\n    areas = []\r\n    for i in range(1, len(steps)):\r\n        sub = _clip_ax(a, True, steps[i-1], steps[i])  # ---- clip_ax\r\n        sub = _uni_pnts(sub)             # ---- uni_pnts\r\n        areas.append(e_area(sub))        # ---- e_area\r\n        arrs.append(sub)\r\n    tot_area = sum(areas)\r\n    cum_area = np.cumsum(areas)\r\n    area = tot_area/float(pieces)  # required area\r\n    bins = np.arange(1, pieces+1) * area\r\n    inds = np.digitize(cum_area, bins)\r\n    f = np.where(inds[:-1]-inds[1:] != 0)[0]\r\n    t = f + 2\r\n#    L, B, R, T = _extent(a)\r\n#    keep = []\r\n#    tot_area = e_area(a)\r\n#    area = tot_area/float(pieces)  # required area\r\n#    cal = 0.0\r\n#    tol = area*tol/100.\r\n#    check = np.abs(area-cal)\r\n#    if x_axis:\r\n#        n = 0\r\n#        step=step_size\r\n#        right = L + step\r\n#        while (check > tol) and (n < 20):\r\n#            k = clip_ax(a, x_axis=True, ax_min=L, ax_max=right)\r\n#            cal = e_area(k)\r\n#            #print(n, cal)\r\n#            check = area-cal\r\n#            print(""area {} check {}  right {}"".format(cal, check < tol, step))\r\n#            if check > 0.:\r\n#                right += step_size\r\n#            else:\r\n#                step_size /= 2.\r\n#                right -= step_size\r\n#                k = clip_ax(a, x_axis=True, ax_min=L, ax_max=right)\r\n#                cal = e_area(k)\r\n#                check = np.abs(area-cal)\r\n#                print(""....area-cal {} "".format(cal))\r\n##            step += step_size\r\n#            n += 1\r\n\r\n#    else:\r\n#        fac = np.linspace(B, T, num=divisor+1, endpoint=True)\r\n#        f = fac[:-1]\r\n#        t = fac[1:]\r\n#        for i in range(divisor):\r\n#            k = clip_ax(a, x_axis=False, ax_min=f[i], ax_max=t[i])\r\n#            _, idx = np.unique(k, return_index=True, axis=0)\r\n#            k = np.concatenate((k[np.sort(idx)], [k[0]]))\r\n#            keep.append(k)\r\n    return arrs, areas, f, t\r\n\r\n\r\ndef perp(a):\r\n    """"""Perpendicular to array""""""\r\n    b = np.empty_like(a)\r\n    b_dim = b.ndim\r\n    if b_dim == 1:\r\n        b[0] = -a[1]\r\n        b[1] = a[0]\r\n    elif b_dim == 2:\r\n        b[:, 0] = -a[:, 1]\r\n        b[:, 1] = a[:, 0]\r\n    return b\r\n\r\n\r\ndef seg_int(a, v):\r\n    """"""Returns the point of intersection of the line segments passing through\r\n    a1, a0 and v1, v0.\r\n\r\n    a0, a1 : points\r\n        [x, y] pairs for each line segment representing the start and end\r\n    v0, v1 : points\r\n        [x, y] pairs on the intersecting line\r\n    **** vertical line *****\r\n    v = np.array([[L + delta, B], [L + delta, T]]) # just need the extent\r\n    """"""\r\n    a0 = a[:-1]  # start points\r\n    a1 = a[1:]   # end points\r\n    b0, b1 = v   # start and end of the intersecting line\r\n    b_ = b0[0]\r\n    ox = a0[:, 0]\r\n    dx = a1[:, 0]\r\n#    f_t = np.array(list(zip(xs[:-1], xs[1:])))\r\n#    f_t = np.sort(f_t, axis=1)\r\n    idx0 = np.where((ox <= b_) & (b_ <= dx))[0]  # incrementing x\'s\r\n    idx1 = np.where((ox >= b_) & (b_ >= dx))[0]  # decreasing x\'s\r\n    idx_s = np.concatenate((idx0, idx1))\r\n    # ---- alternate\r\n    da = a1 - a0\r\n    db = b1 - b0\r\n    dp = a0 - b0\r\n    dap = perp(da)\r\n    denom = np.dot(dap, db)     # or dap @ db\r\n    # num = np.dot(dap, dp )\r\n    db2 = db.reshape(1, 2)\r\n    denom = np.einsum(\'ij,ij->i\', dap, db2)\r\n    num = np.einsum(\'ij,ij->i\', dap, dp)\r\n    int_pnts = (num/denom).reshape(num.shape[0], 1) * db + b0\r\n    ft_int = np.hstack((a0, a1, int_pnts))\r\n    return int_pnts, ft_int\r\n\r\n\r\ndef _demo():\r\n    """"""\r\n    Notes:\r\n    -----\r\n\r\n    The xs and ys form pairs with the first and last points being identical\r\n    The pairs are constructed using n-1 to ensure that you don\'t form a\r\n    line from identical points.\r\n    #\r\n    first split polygon as a sample of a multipart.  I had to add 0,0 and 0, 80\r\n    back in\r\n    l = [[[ 0., 0.], [ 0., 30.], [ 10., 30.], [ 10., 0.], [ 0.,  0.]],\r\n         [[ 0., 80.], [ 0., 100.], [ 10., 100.], [ 10., 73.75], [ 0., 80.]]]\r\n    """"""\r\n#    xs = [10., 20., 20., 0., 40., 70., 80., 80., 60., 40., 10.]\r\n#    ys = [0., 30., 50., 70., 70., 60., 30., 0., 10., 10., 0.]\r\n#    xs = [0., 0., 100., 100., 0.]  # simple square\r\n#    ys = [0., 100., 100., 0., 0.]\r\n\r\n    xs = [0., 0., 80., 0, 0., 100., 100., 0.]\r\n    ys = [0., 30., 30., 80., 100., 100., 0., 0.]\r\n    a = np.array(list(zip(xs, ys))) * 1.0  # --- must be floats\r\n    v = np.array([[50., 0], [50, 100.]])\r\n    ext = np.array([[0., 0], [0, 100.], [100, 100.], [100., 0.], [0., 0.]])\r\n\r\n#    return a, v\r\n\r\n#    fc = r""C:\\Temp\\junk.gdb\\smp""\r\n#\r\n#    a0 = arc_np(fc)\r\n##    b = _id_geom_array(fc)\r\n#    a = (a0[[\'Xs\', \'Ys\']].view(dtype=\'float64\')).reshape(a0.shape[0], 2)\r\n    arrs, areas, f, t = split_area(a, 5)\r\n    out = np.array(arrs)\r\n    out_fc = r""C:\\Temp\\junk.gdb\\s""\r\n    SR = \'NAD_1983_2011_StatePlane_Mississippi_East_FIPS_2301_Ft_US\'\r\n    polygons = []\r\n    frmt = """"""\r\n    area {}\r\n    extent ll {}\r\n    extent ur {}\r\n    width {}\r\n    height {}\r\n    centroid {}\r\n    """"""\r\n    for pair in out:\r\n        print(""pair ...\\n{}"".format(pair))\r\n        pnts = [arcpy.Point(*xy) for xy in pair]\r\n        print(""points ...\\n{}"".format(pnts))\r\n        pl = arcpy.Polygon(arcpy.Array(pnts), SR)\r\n        args = [pl.area, pl.extent.lowerLeft, pl.extent.upperRight,\r\n                pl.extent.width, pl.extent.height, pl.centroid]\r\n        print(dedent(frmt).format(*args))\r\n        polygons.append(pl)\r\n    return polygons, out\r\n#   arcpy.CopyFeatures_management(polygons, out_fc)\r\n#    output_polygons(out_fc, SR, out)\r\n#    # this works\r\n#    # z = [array_struct(i, fld_names=[\'X\', \'Y\'], dt=[\'<f8\', \'<f8\']) for i in out]\r\n#\r\n#    return a, out, out_fc, SR, polygons\r\n\r\n#    L, B, R, T = _extent(a)\r\n#    delta = 50\r\n#    v = np.array([[L + delta, B], [L + delta, T]]) # just need the extent\r\n#\r\n#    area = e_area(a)\r\n#    #\r\n##    arrs = split_ext(a, divisor=2, x_axis=True)\r\n#    args  = split_area(a, pieces=2, step_size=50, tol=0.05, x_axis=True)\r\n#    arrs, areas, f, t = args\r\n#    print(""full area {}"".format(area))\r\n##    for i in arrs:\r\n##        print(e_area(i))\r\n#    return a, arrs, areas, f, t, v\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n#    polygons, out = _demo()\r\n#    a, arrs, areas, f, t, v = _demo()\r\n#    a, out, out_fc, SR, polygons = _demo()\r\n""""""\r\nfor i in range(2, len(a)+1):\r\n    print(""{}, {}, {}"".format(a[i-2], seg_int(a[i-2:i], v), a[i-1]))\r\n\r\n[0. 0.], [[-n- inf]], [ 0. 30.]\r\n[ 0. 30.], [[10. 30.]], [80. 30.]\r\n[80. 30.], [[10.   73.75]], [ 0. 80.]\r\n[ 0. 80.], [[-n- inf]], [  0. 100.]\r\n[  0. 100.], [[-n- -n-]], [  0. 100.]\r\n[  0. 100.], [[ 10. 100.]], [100. 100.]\r\n[100. 100.], [[-n- inf]], [100.   0.]\r\n[100.   0.], [[10.  0.]], [0. 0.]\r\n""""""\r\n\r\n'"
all_scripts/split_polys_testing.py,50,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\nscript\r\n======\r\n\r\nScript :   split_polys.py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified : 2018-06-14\r\n\r\nPurpose :  tools for working with numpy arrays\r\n\r\nUseage :\r\n\r\nReferences\r\n----------\r\n\r\n`<https://stackoverflow.com/questions/3252194/numpy-and-line-intersections>`_.\r\n\r\n\r\n---------------------------------------------------------------------\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nfrom textwrap import dedent\r\nimport numpy as np\r\nfrom arcpytools_plt import tweet, fc_info, output_polygons\r\nimport arcpy\r\n\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=5, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\n# ---- from arrtools.fc\r\n#\r\ndef _ndarray(in_fc, to_pnts=True, flds=None, SR=None):\r\n    """"""Convert featureclass geometry (in_fc) to a structured ndarray including\r\n    options to select fields and specify a spatial reference.\r\n\r\n    Requires\r\n    --------\r\n    in_fc : string\r\n        input featureclass\r\n    to_pnts : boolean\r\n        True, convert the shape to points. False, centroid returned.\r\n    flds : string or list of strings\r\n      - \'*\' for all\r\n      - others : \'OID@\', \'Shape\',  [\'SHAPE@X\', \'SHAPE@Y\'], or specify\r\n\r\n    Example:\r\n    --------\r\n    a = _ndarray(in_fc, True, [\'OID@\',\' SHAPE@X\', \'SHAPE@Y\', None]\r\n    """"""\r\n    if flds is None:\r\n        flds = ""*""\r\n    if SR is None:\r\n        desc = arcpy.da.Describe(in_fc)\r\n        SR = desc[\'spatialReference\']\r\n    args = [in_fc, flds, None, SR, to_pnts, (None, None)]\r\n    cur = arcpy.da.SearchCursor(*args)\r\n    a = cur._as_narray()\r\n    del cur\r\n    return a\r\n\r\n# ---- from arraytools.geom\r\n#\r\ndef e_area(a, b=None):\r\n    """"""Area calculation, using einsum.\r\n\r\n    Some may consider this overkill, but consider a huge list of polygons,\r\n    many multipart, many with holes and even multiple version therein.\r\n\r\n    Requires:\r\n    --------\r\n    `a` : array\r\n        Either a 2D+ array of coordinates or arrays of x, y values\r\n    `b` : array, optional\r\n        If a < 2D, then the y values need to be supplied\r\n    Outer rings are ordered clockwise, inner holes are counter-clockwise\r\n\r\n    Notes:\r\n    -----\r\n        See ein_geom.py for examples\r\n\r\n    """"""\r\n    a = np.asanyarray(a)\r\n    if b is None:\r\n        xs = a[..., 0]\r\n        ys = a[..., 1]\r\n    else:\r\n        xs, ys = a, b\r\n    x0 = np.atleast_2d(xs[..., 1:])\r\n    y0 = np.atleast_2d(ys[..., :-1])\r\n    x1 = np.atleast_2d(xs[..., :-1])\r\n    y1 = np.atleast_2d(ys[..., 1:])\r\n    e0 = np.einsum(\'...ij,...ij->...i\', x0, y0)\r\n    e1 = np.einsum(\'...ij,...ij->...i\', x1, y1)\r\n    area = abs(np.sum((e0 - e1)*0.5))\r\n    return area\r\n\r\n\r\n# ---- Split poly* geometry using several approaches ----\r\n#\r\n# -- helper functions\r\ndef _clip_ax(a, x_axis=True, ax_min=0, ax_max=1):\r\n    """"""Clip geometry x or y coordinates and return the result\r\n    a : array\r\n        Input array of xy coordinates.  If polygon geometry, outer ring\r\n        coordinates are ordered clockwise with identical first-last points.\r\n    x_axis : boolean\r\n        True for x-axis, False for y-axis\r\n    ax_min, ax_max : number\r\n        Number for the x or y coordinate minimum and maximum\r\n    """"""\r\n    b = np.zeros_like(a)\r\n    if x_axis:\r\n        b[:, 1] = a[:, 1]\r\n        b[:, 0] = np.clip(a[:, 0], ax_min, ax_max)\r\n    else:\r\n        b[:, 0] = a[:, 0]\r\n        b[:, 1] = np.clip(a[:, 1], ax_min, ax_max)\r\n    return b\r\n\r\n\r\ndef _uni_pnts(a):\r\n    """"""Return unique points forming a *poly feature\r\n    """"""\r\n    _, idx = np.unique(a, return_index=True, axis=0)\r\n    a = np.concatenate((a[np.sort(idx)], [a[0]]))\r\n    return a\r\n\r\n# ---- Split extent by a factor ----\r\n#\r\ndef split_ext(a, divisor=2, x_axis=True):\r\n    """"""Split the extent of a feature by a factor/percentage\r\n    a : array\r\n        Input array.\r\n    divisor : factor\r\n        The split factor representing the number of divisions to make over the\r\n        range of the geometry.\r\n    x_axis : boolean\r\n        Split the X-axis if True, otherwise the y_axis\r\n\r\n    Notes:\r\n    ------\r\n    - Determine the extent of the features\r\n    - Split using `np.clip` to remove X or Y values beyond the desired range.\r\n    - Determine the unique points\r\n    """"""\r\n    #\r\n    L, B, R, T = [*a.min(axis=0), *a.max(axis=0)]\r\n    keep = []\r\n    if x_axis:\r\n        fac = np.linspace(L, R, num=divisor+1, endpoint=True)\r\n        f = fac[:-1]\r\n        t = fac[1:]\r\n        for i in range(divisor):\r\n            v = np.array([[f[i], B], [f[i], T]]) # just need the extent\r\n            k = _clip_ax(a, x_axis=True, ax_min=f[i], ax_max=t[i])\r\n            k = _uni_pnts(k)\r\n            keep.append(k)\r\n    else:\r\n        fac = np.linspace(B, T, num=divisor+1, endpoint=True)\r\n        f = fac[:-1]\r\n        t = fac[1:]\r\n        for i in range(divisor):\r\n            k = _clip_ax(a, x_axis=False, ax_min=f[i], ax_max=t[i])\r\n            k = _uni_pnts(k)\r\n            keep.append(k)\r\n    return keep\r\n\r\n\r\n# ---- Split by area ----\r\n#\r\ndef split_area(a, pieces=4, step_size=10, tol=1.0, x_axis=True):\r\n    """"""Split the extent of a feature by a factor/percentage\r\n    a : array\r\n        Input array.\r\n    pieces : number\r\n        Number of pieces to split the poly* into\r\n    step : factor\r\n        The step is the distance to move in planar units.\r\n    tol : number\r\n        The percentage tolerance in the area\r\n    x_axis : boolean\r\n        Split the X-axis if True, otherwise the y_axis\r\n    """"""\r\n    xs = a[:, 0]\r\n    uni = np.unique(xs)\r\n    ax_min, ax_max = uni[0], uni[-1]\r\n    steps, incr = np.linspace(ax_min, ax_max, num=pieces+1,\r\n                              endpoint=True, retstep=True)\r\n    # ---- Do the work, clip the axes, get the unique points, calculate area\r\n    arrs = []\r\n    areas = []\r\n    for i in range(1, len(steps)):\r\n        sub = _clip_ax(a, True, steps[i-1], steps[i])  # ---- clip_ax\r\n        sub = _uni_pnts(sub)             # ---- uni_pnts\r\n        areas.append(e_area(sub))        # ---- e_area\r\n        arrs.append(sub)\r\n    tot_area = sum(areas)\r\n    cum_area =  np.cumsum(areas)\r\n    area = tot_area/float(pieces)  # required area\r\n    bins = np.arange(1, pieces+1) * area\r\n    inds = np.digitize(cum_area, bins)\r\n    f = np.where(inds[:-1]-inds[1:] != 0)[0]\r\n    t = f + 2\r\n#    L, B, R, T = _extent(a)\r\n#    keep = []\r\n#    tot_area = e_area(a)\r\n#    area = tot_area/float(pieces)  # required area\r\n#    cal = 0.0\r\n#    tol = area*tol/100.\r\n#    check = np.abs(area-cal)\r\n#    if x_axis:\r\n#        n = 0\r\n#        step=step_size\r\n#        right = L + step\r\n#        while (check > tol) and (n < 20):\r\n#            k = clip_ax(a, x_axis=True, ax_min=L, ax_max=right)\r\n#            cal = e_area(k)\r\n#            #print(n, cal)\r\n#            check = area-cal\r\n#            print(""area {} check {}  right {}"".format(cal, check < tol, step))\r\n#            if check > 0.:\r\n#                right += step_size\r\n#            else:\r\n#                step_size /= 2.\r\n#                right -= step_size\r\n#                k = clip_ax(a, x_axis=True, ax_min=L, ax_max=right)\r\n#                cal = e_area(k)\r\n#                check = np.abs(area-cal)\r\n#                print(""....area-cal {} "".format(cal))\r\n##            step += step_size\r\n#            n += 1\r\n\r\n#    else:\r\n#        fac = np.linspace(B, T, num=divisor+1, endpoint=True)\r\n#        f = fac[:-1]\r\n#        t = fac[1:]\r\n#        for i in range(divisor):\r\n#            k = clip_ax(a, x_axis=False, ax_min=f[i], ax_max=t[i])\r\n#            _, idx = np.unique(k, return_index=True, axis=0)\r\n#            k = np.concatenate((k[np.sort(idx)], [k[0]]))\r\n#            keep.append(k)\r\n    return arrs, areas, f, t\r\n\r\n\r\ndef perp(a) :\r\n    b = np.empty_like(a)\r\n    b_dim = b.ndim\r\n    if b_dim == 1:\r\n        b[0] = -a[1]\r\n        b[1] = a[0]\r\n    elif b_dim == 2:\r\n        b[:, 0] = -a[:, 1]\r\n        b[:, 1] = a[:, 0]\r\n    return b\r\n\r\n\r\ndef seg_int(a, v) :\r\n    """"""Returns the point of intersection of the line segments passing through\r\n    a1, a0 and v1, v0.\r\n\r\n    a0, a1 : points\r\n        [x, y] pairs for each line segment representing the start and end\r\n    v0, v1 : points\r\n        [x, y] pairs on the intersecting line\r\n    **** vertical line *****\r\n    v = np.array([[L + delta, B], [L + delta, T]]) # just need the extent\r\n    """"""\r\n    a0 = a[:-1]  # start points\r\n    a1 = a[1:]   # end points\r\n    b0, b1 = v   # start and end of the intersecting line\r\n    b_ = b0[0]\r\n    ox = a0[:, 0]\r\n    dx = a1[:, 0]\r\n#    f_t = np.array(list(zip(xs[:-1], xs[1:])))\r\n#    f_t = np.sort(f_t, axis=1)\r\n    idx0 = np.where((ox <= b_) & (b_ <= dx))[0]  # incrementing x\'s\r\n    idx1 = np.where((ox >= b_) & (b_ >= dx))[0]  # decreasing x\'s\r\n    idx_s = np.concatenate((idx0, idx1))\r\n    # ---- alternate\r\n    da = a1 - a0\r\n    db = b1 - b0\r\n    dp = a0 - b0\r\n    dap = perp(da)\r\n    denom = np.dot(dap, db)     # or dap @ db\r\n    # num = np.dot(dap, dp )\r\n    db2 = db.reshape(1, 2)\r\n    denom = np.einsum(\'ij,ij->i\', dap, db2)\r\n    num = np.einsum(\'ij,ij->i\', dap, dp)\r\n    int_pnts = (num/denom).reshape(num.shape[0], 1) * db + b0\r\n    ft_int = np.hstack((a0, a1, int_pnts))\r\n    return int_pnts, ft_int\r\n\r\n\r\n# ---- Do the work or run the demo ------------------------------------------\r\n#\r\n\r\nfrmt = """"""\r\nInput features.... {}\r\nOutput features... {}\r\nNumber of splits . {}\r\nSplit types ...... {}\r\n""""""\r\n\r\n\r\ndef _poly_ext(p):\r\n    """"""poly* extent\r\n    """"""\r\n    L, B = p.extent.lowerLeft.X, p.extent.lowerLeft.Y\r\n    R, T = p.extent.upperRight.X, p.extent.upperRight.Y\r\n    return L, B, R, T\r\n\r\n\r\ndef _demo():\r\n    """"""\r\n    Notes:\r\n    -----\r\n    The xs and ys form pairs with the first and last points being identical\r\n    The pairs are constructed using n-1 to ensure that you don\'t form a\r\n    line from identical points.\r\n\r\n    First split polygon as a sample of a multipart.  I had to add 0,0 and 0, 80\r\n    back in\r\n\r\n    xs = [0., 0., 80., 0, 0., 100., 100., 0.]\r\n    ys = [0., 30., 30., 80., 100., 100., 0., 0.]\r\n    a = np.array(list(zip(xs, ys))) * 1.0  # --- must be floats\r\n    v = np.array([[50., 0], [50, 100.]])\r\n    ext = np.array([[0., 0], [0, 100.],[100, 100.], [100., 0.], [0., 0.]])\r\n    return a, v\r\n    """"""\r\n    in_pth = script.split(""/"")[:-2] + [""Polygon_lineTools.gdb""]\r\n    in_fc = ""/"".join(in_pth) + ""/r_concave_k3_polygon""\r\n    out_fc = ""/"".join(in_pth) + ""/c0""\r\n    split_type = \'extent\'\r\n    desc = arcpy.da.Describe(in_fc)\r\n    x_tent = str(desc[\'extent\'])\r\n    split_fac = 4\r\n    split_per = 25.\r\n    shp_fld, oid_fld, shp_type, SR = fc_info(in_fc)\r\n    do_work(in_fc, out_fc, split_type, x_tent, split_fac, split_per)\r\n    return None\r\n\r\n\r\ndef _tool():\r\n    """"""Split_polys tool script\r\n    """"""\r\n    testing = False\r\n    in_fc = sys.argv[1]\r\n    out_fc = sys.argv[2]\r\n    split_type = sys.argv[3]\r\n    # --- extent parameters\r\n    x_tent = sys.argv[4]\r\n    split_fac = sys.argv[5]\r\n    # ---- area parameters\r\n    split_per = sys.argv[4]\r\n\r\n    do_work(in_fc, out_fc, split_type, x_tent, split_fac, split_per)\r\n    return ""done""\r\n\r\n\r\ndef do_work(in_fc, out_fc, split_type, x_tent, split_fac, split_per):\r\n    """"""Do the actual work for either the demo or the tool\r\n\r\n    Requires:\r\n    --------\r\n    in_fc : feautureclass\r\n        polygon or polyline featureclass\r\n    out_fc : featureclass\r\n        same as input\r\n    split_type : choice of `extent` or `areas`\r\n        `extent` uses the bounds with `split_fac` to subdivide the range into\r\n        sub-polygons\r\n\r\n    **extent option**\r\n\r\n    x_tent : extent parameter\r\n        LBRT L(eft), B(ottom), R(ight), T(op) in the featureclass units\r\n    split_fac : integer\r\n        Integer representing the divisions, for example, a factor of 2 is the\r\n        same as half (1/2) or 50%\r\n\r\n    **area option**\r\n\r\n    split_per : double\r\n        Percentage of the total area representing sub-area size. 25% would\r\n        result in 4 sub-areas with approximately 25% of the total area.\r\n    """"""\r\n    shp_fld, oid_fld, shp_type, SR = fc_info(in_fc)\r\n    desc = arcpy.da.Describe(in_fc)\r\n    x_tent = x_tent.split("" "")[:4]\r\n    L, B, R, T = [float(i) for i in x_tent]\r\n    split_fac = float(split_fac)\r\n    #\r\n    if split_type == \'extent\':\r\n        dx = (R - L)/split_fac\r\n        lefts = np.arange(L+dx, R, step=dx) #[304000, 304500, 305000, 305500, 306000]\r\n        splitters = np.array([[[l, B], [l, T]] for l in lefts])\r\n        cutters = []\r\n        for s in splitters:\r\n            s = s.tolist()\r\n            c = arcpy.Polyline(arcpy.Array([arcpy.Point(*xy) for xy in s]), SR)\r\n            cutters.append(c)\r\n        polygons = []\r\n        with arcpy.da.SearchCursor(in_fc,  [""SHAPE@""]) as cursor:\r\n            for row in cursor:\r\n                poly = row[0]\r\n                p_right = poly\r\n                for i in cutters:\r\n                    pieces = p_right.cut(i)\r\n                    p_left = pieces[0]\r\n                    p_right = pieces[1]\r\n                    polygons.append(p_left)\r\n                if p_right is not None or len(p_right) > 0:\r\n                    polygons.append(p_right)\r\n    #\r\n    elif split_type == \'area\':\r\n#        ar= e_area(xy)\r\n        print(""None"")\r\n    #\r\n    # ---- create the output, overwite any existing versions\r\n    if arcpy.Exists(out_fc):\r\n        arcpy.Delete_management(out_fc)\r\n    arcpy.CopyFeatures_management(polygons, out_fc)\r\n\r\n\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    poly, cutter = _demo()\r\nelse:\r\n    testing = False\r\n    msg = _tool()\r\n\r\n#tweet(msg)\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n#    polygons, out = _demo()\r\n#    a, arrs, areas, f, t, v = _demo()\r\n#    a, out, out_fc, SR, polygons = _demo()\r\n""""""\r\nfor i in range(2, len(a)+1):\r\n    print(""{}, {}, {}"".format(a[i-2], seg_int(a[i-2:i], v), a[i-1]))\r\n\r\n[0. 0.], [[-n- inf]], [ 0. 30.]\r\n[ 0. 30.], [[10. 30.]], [80. 30.]\r\n[80. 30.], [[10.   73.75]], [ 0. 80.]\r\n[ 0. 80.], [[-n- inf]], [  0. 100.]\r\n[  0. 100.], [[-n- -n-]], [  0. 100.]\r\n[  0. 100.], [[ 10. 100.]], [100. 100.]\r\n[100. 100.], [[-n- inf]], [100.   0.]\r\n[100.   0.], [[10.  0.]], [0. 0.]\r\n""""""\r\n#    a0 = arc_np(fc)\r\n##    b = _id_geom_array(fc)\r\n#    a = (a0[[\'Xs\', \'Ys\']].view(dtype=\'float64\')).reshape(a0.shape[0], 2)\r\n#    arrs, areas, f, t = split_area(a, 5)\r\n#    out = np.array(arrs)\r\n#    out_fc = r""C:\\Temp\\junk.gdb\\s""\r\n#    SR = \'NAD_1983_2011_StatePlane_Mississippi_East_FIPS_2301_Ft_US\'\r\n#    polygons = []\r\n#    frmt = """"""\r\n#    area {}\r\n#    extent ll {}\r\n#    extent ur {}\r\n#    width {}\r\n#    height {}\r\n#    centroid {}\r\n#    """"""\r\n#    for pair in out:\r\n#        print(""pair ...\\n{}"".format(pair))\r\n#        pnts = [arcpy.Point(*xy) for xy in pair]\r\n#        print(""points ...\\n{}"".format(pnts))\r\n#        pl = arcpy.Polygon(arcpy.Array(pnts), SR)\r\n#        args = [pl.area, pl.extent.lowerLeft, pl.extent.upperRight,\r\n#                pl.extent.width, pl.extent.height, pl.centroid]\r\n#        print(dedent(frmt).format(*args))\r\n#        polygons.append(pl)\r\n#    return polygons, out\r\n#   arcpy.CopyFeatures_management(polygons, out_fc)\r\n#    output_polygons(out_fc, SR, out)\r\n#    # this works\r\n#    # z = [array_struct(i, fld_names=[\'X\', \'Y\'], dt=[\'<f8\', \'<f8\']) for i in out]\r\n#\r\n#    return a, out, out_fc, SR, polygons\r\n\r\n#    L, B, R, T = _extent(a)\r\n#    delta = 50\r\n#    v = np.array([[L + delta, B], [L + delta, T]]) # just need the extent\r\n#\r\n#    area = e_area(a)\r\n#    #\r\n##    arrs = split_ext(a, divisor=2, x_axis=True)\r\n#    args  = split_area(a, pieces=2, step_size=50, tol=0.05, x_axis=True)\r\n#    arrs, areas, f, t = args\r\n#    print(""full area {}"".format(area))\r\n##    for i in arrs:\r\n##        print(e_area(i))\r\n#    return a, arrs, areas, f, t, v\r\n\r\n'"
all_scripts/stackstats.py,34,"b'# -*- coding: UTF-8 -*-ct\r\n""""""\r\nstackstats\r\n===========\r\n\r\nScript:   stackstats.py\r\n\r\nAuthor:   Dan.Patterson@carleton.ca\r\n\r\nModified: 2018-11-23\r\n\r\nPurpose:  tools for working with numpy arrays\r\n\r\nRequires:\r\n---------\r\n    arraytools.tools - nd2struct, stride\r\n\r\nReferences:\r\n-----------\r\n\r\n`<https://community.esri.com/blogs/dan_patterson/2018/02/06/cell-\\\r\nstatistics-made-easy-raster-data-over-time>`_.\r\n\r\n""""""\r\n# pylint: disable=C0103\r\n# pylint: disable=R1710\r\n# pylint: disable=R0914\r\n\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=5, linewidth=100, precision=2, suppress=True,\r\n                    threshold=150, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n__all__ = [\'check_shapes\', \'check_stack\', \'mask_stack\',\r\n           \'stack_sum\', \'stack_cumsum\',  # statistical functions\r\n           \'stack_prod\', \'stack_cumprod\',\r\n           \'stack_min\', \'stack_mean\',\r\n           \'stack_median\', \'stack_max\',\r\n           \'stack_std\', \'stack_var\',\r\n           \'stack_percentile\',\r\n           \'stack_stats\',\r\n           \'stack_stats_tbl\']\r\n\r\n\r\n# ---- array checks and creation --------------------------------------------\r\n# ---- 3D arrays for stacked operations\r\n#\r\ndef check_shapes(arrs):\r\n    """"""Check the shapes of the arrays to ensure they are all equal\r\n    """"""\r\n    shps = [i.shape for i in arrs]\r\n    eq = np.all(np.array([shps[0] == i for i in shps[1:]]))\r\n    err = ""Arrays arr not of the same shape...""\r\n    if not eq:\r\n        raise ValueError(""{}\\n{}"".format(err, shps))\r\n\r\n\r\ndef check_stack(arrs):\r\n    """"""Do the basic checking of the stack to ensure that a 3D array is\r\n    generated\r\n    """"""\r\n    err1 = ""Object, structured arrays not supported, current type...""\r\n    err2 = ""3D arrays supported current ndim...""\r\n    if isinstance(arrs, (list, tuple)):\r\n        arrs = np.array(arrs)\r\n    if arrs.dtype.kind in (\'O\', \'V\'):\r\n        raise ValueError(""{} {}"".format(err1, arrs.dtype.kind))\r\n    if arrs.ndim != 3:\r\n        raise ValueError(""{} {}"".format(err2, arrs.ndim))\r\n    return arrs\r\n\r\n\r\ndef mask_stack(arrs, nodata=None):\r\n    """"""Produce masks for a 3d array""""""\r\n    if (nodata is None) or (arrs.ndim < 2) or (arrs.ndim > 3):\r\n        print(""\\n...mask_stack requires a 3d array and a nodata value\\n"")\r\n        return arrs\r\n    m = (arrs[:, ...] == nodata).any(0)\r\n    msk = [m for i in range(arrs.shape[0])]\r\n    msk = np.array(msk)\r\n    a_m = np.ma.MaskedArray(arrs, mask=msk)\r\n    return a_m\r\n\r\n\r\n# ---- Statistics for stacked arrays (3D) ------------------------------------\r\n#\r\ndef stack_sum(arrs, nodata=None):\r\n    """"""see stack_stats""""""\r\n    a = check_stack(arrs)\r\n    if nodata is not None:\r\n        a = mask_stack(a, nodata=nodata)\r\n    return np.nansum(a, axis=0)\r\n\r\n\r\ndef stack_cumsum(arrs, nodata=None):\r\n    """"""see stack_stats""""""\r\n    a = check_stack(arrs)\r\n    if nodata is not None:\r\n        a = mask_stack(a, nodata=nodata)\r\n    return np.nancumsum(a, axis=0)\r\n\r\n\r\ndef stack_prod(arrs, nodata=None):\r\n    """"""see stack_stats""""""\r\n    a = check_stack(arrs)\r\n    if nodata is not None:\r\n        a = mask_stack(a, nodata=nodata)\r\n    return np.nanprod(a, axis=0)\r\n\r\n\r\ndef stack_cumprod(arrs, nodata=None):\r\n    """"""see stack_stats""""""\r\n    a = check_stack(arrs)\r\n    if nodata is not None:\r\n        a = mask_stack(a, nodata=nodata)\r\n    return np.nancumprod(a, axis=0)\r\n\r\n\r\ndef stack_min(arrs, nodata=None):\r\n    """"""see stack_stats""""""\r\n    a = check_stack(arrs)\r\n    if nodata is not None:\r\n        a = mask_stack(a, nodata=nodata)\r\n    return np.nanmin(a, axis=0)\r\n\r\n\r\ndef stack_mean(arrs, nodata=None):\r\n    """"""see stack_stats""""""\r\n    a = check_stack(arrs)\r\n    if nodata is not None:\r\n        a = mask_stack(a, nodata=nodata)\r\n    return np.nanmean(a, axis=0)\r\n\r\n\r\ndef stack_median(arrs, nodata=None):\r\n    """"""see stack_stats""""""\r\n    a = check_stack(arrs)\r\n    if nodata is not None:\r\n        a = mask_stack(a, nodata=nodata)\r\n    return np.nanmedian(a, axis=0)\r\n\r\n\r\ndef stack_max(arrs, nodata=None):\r\n    """"""see stack_stats""""""\r\n    a = check_stack(arrs)\r\n    if nodata is not None:\r\n        a = mask_stack(a, nodata=nodata)\r\n    return np.nanmax(a, axis=0)\r\n\r\n\r\ndef stack_std(arrs, nodata=None):\r\n    """"""see stack_stats""""""\r\n    a = check_stack(arrs)\r\n    if nodata is not None:\r\n        a = mask_stack(a, nodata=nodata)\r\n    return np.nanstd(a, axis=0)\r\n\r\n\r\ndef stack_var(arrs, nodata=None):\r\n    """"""see stack_stats""""""\r\n    a = check_stack(arrs)\r\n    if nodata is not None:\r\n        a = mask_stack(a, nodata=nodata)\r\n    return np.nanvar(a, axis=0)\r\n\r\n\r\ndef stack_percentile(arrs, q=50, nodata=None):\r\n    """"""nanpercentile for an array stack with optional nodata masked\r\n\r\n    -arrs :\r\n        either a list, tuple of arrays or an array with ndim=3\r\n    - q :\r\n        the percentile\r\n    - nodata :\r\n        nodata value, numeric or np.nan (will upscale integers)\r\n    """"""\r\n    a = check_stack(arrs)\r\n    if nodata is not None:\r\n        a = mask_stack(a, nodata=nodata)\r\n    nan_per = np.nanpercentile(a, q=q, axis=0)\r\n    return nan_per\r\n\r\n\r\ndef stack_stats(arrs, ax=0, nodata=None):\r\n    """"""All statistics for arrs.\r\n\r\n    - arrs :\r\n        either a list, tuple of arrays or an array with ndim=3\r\n    - ax :\r\n        axis, either, 0 (by band) or (1,2) to get a single value for each band\r\n    - nodata :\r\n        nodata value, numeric or np.nan (will upscale integers)\r\n    """"""\r\n    arrs = check_stack(arrs)\r\n    a_m = mask_stack(arrs, nodata=nodata)\r\n    nan_sum = np.nansum(a_m, axis=ax)\r\n    nan_min = np.nanmin(a_m, axis=ax)\r\n    nan_mean = np.nanmean(a_m, axis=ax)\r\n    nan_median = np.nanmean(a_m, axis=ax)\r\n    nan_max = np.nanmax(a_m, axis=ax)\r\n    nan_std = np.nanstd(a_m, axis=ax)\r\n    nan_var = np.nanvar(a_m, axis=ax)\r\n    stats = [nan_sum, nan_min, nan_mean, nan_median, nan_max, nan_std, nan_var]\r\n    if np.isscalar(ax):\r\n        nan_cumsum = np.nancumsum(a_m, axis=ax)\r\n        stats.append(nan_cumsum)\r\n    return stats\r\n\r\n\r\ndef stack_stats_tbl(arrs, nodata=None):  # col_names, args):\r\n    """"""Produce the output table\r\n\r\n    Returns:\r\n    --------\r\n    Table of statistical results by band.  The dtype is shown below\r\n    dtype=[(\'Band\', \'<i4\'), (\'N\', \'<i4\'), (\'N_nan\', \'<i4\'), (\'Sum\', \'<f8\'),\r\n           (\'Min\', \'<f8\'), (\'Mean\', \'<f8\'), (\'Med\', \'<f8\'), (\'Max\', \'<f8\'),\r\n           (\'Std\', \'<f8\'), (\'Var\', \'<f8\')])\r\n    """"""\r\n    stats = stack_stats(arrs, ax=(1, 2), nodata=nodata)\r\n    d = [(i, \'<f8\')\r\n         for i in [\'Sum\', \'Min\', \'Mean\', \'Med\', \'Max\', \'Std\', \'Var\']]\r\n    dts = [(\'Band\', \'<i4\'), (\'N\', \'<i4\'), (\'N_nan\', \'<i4\')] + d\r\n    N, r, c = arrs.shape\r\n    cols = len(dts)\r\n    z = np.empty(shape=(N,), dtype=dts)\r\n    z[z.dtype.names[0]] = np.arange(0, N)\r\n    z[z.dtype.names[1]] = np.array([r*c]*N)\r\n    z[z.dtype.names[2]] = np.count_nonzero(arrs == nodata, axis=(1, 2))\r\n    for i in range(cols-3):\r\n        z[z.dtype.names[i+3]] = stats[i]\r\n    return z\r\n\r\n\r\ndef _demo_stack():\r\n    """"""\r\n    demo stack :\r\n        Simply 31 layers shaped (31, 100, 150) with uniform values one for\r\n        each day, numbers from 1 to 31.\r\n    >>> stack_stats_tbl(stack)\r\n    array([( 0, 15000, 0,   15000.,   1.,   1.,   1.,   1.,  0.,  0.),\r\n           ( 1, 15000, 0,   30000.,   2.,   2.,   2.,   2.,  0.,  0.),\r\n           ( 2, 15000, 0,   45000.,   3.,   3.,   3.,   3.,  0.,  0.),\r\n           ( 3, 15000, 0,   60000.,   4.,   4.,   4.,   4.,  0.,  0.),\r\n           ( 4, 15000, 0,   75000.,   5.,   5.,   5.,   5.,  0.,  0.),\r\n           ( 5, 15000, 0,   90000.,   6.,   6.,   6.,   6.,  0.,  0.),\r\n           .... snip ....\r\n           (27, 15000, 0,  420000.,  28.,  28.,  28.,  28.,  0.,  0.),\r\n           (28, 15000, 0,  435000.,  29.,  29.,  29.,  29.,  0.,  0.),\r\n           (29, 15000, 0,  450000.,  30.,  30.,  30.,  30.,  0.,  0.),\r\n           (30, 15000, 0,  465000.,  31.,  31.,  31.,  31.,  0.,  0.)],\r\n          dtype=[(\'Band\', \'<i4\'), (\'N\', \'<i4\'), (\'N_nan\', \'<i4\'),\r\n                 (\'Sum\', \'<f8\'), (\'Min\', \'<f8\'), (\'Mean\', \'<f8\'),\r\n                 (\'Med\', \'<f8\'), (\'Max\', \'<f8\'), (\'Std\', \'<f8\'),\r\n                 (\'Var\', \'<f8\')])\r\n\r\n    """"""\r\n    fname = ""/"".join(script.split(""/"")[:-1]) + ""/Data/Arr_31_100_150.npy""\r\n    stack = np.load(fname)\r\n    return stack\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n#    stack = _demo_stack()\r\n'"
all_scripts/strip_stuff.py,9,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   strip_stuff.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2018-02-10\r\n:Purpose:  tools for working with numpy arrays\r\n:Useage:\r\n:\r\n:References:\r\n:\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\npunc = [\'!\', \'""\', \'#\', \'$\', \'%\', \'&\', ""\'"", \'(\', \')\', \'*\', \'+\', \',\', \'-\', \'.\',\r\n        \'/\', \':\', \';\', \'<\', \'=\', \'>\', \'?\', \'@\', \'[\', \'\\\\\', \']\', \'^\', \'_\', \'`\',\r\n        \'{\', \'|\', \'}\', \'~\']\r\nwhitesp = [\' \', \'\\t\', \'\\n\', \'\\r\', \'\\x0b\', \'\\x0c\']\r\n\r\n\r\ndef tweet(msg):\r\n    """"""Print a message for both arcpy and python.\r\n    : msg - a text message\r\n    """"""\r\n    m = ""\\n{}\\n"".format(msg)\r\n    arcpy.AddMessage(m)\r\n    print(m)\r\n\r\n\r\ndef clean_fld(a, strip_list, new_value=""""):\r\n    """"""clean the arrays if needed""""""\r\n    tmp = np.copy(a)\r\n    for i in strip_list:\r\n        tmp = np.char.replace(tmp, str(i), new_value)\r\n    cleaned = tmp\r\n    return cleaned\r\n\r\n\r\n# ---- Run options: _demo or from _tool\r\n#\r\ndef _demo():\r\n    """"""Code to run if in demo mode\r\n    """"""\r\n    a = np. array([\'1, 2, 3, 4, 5\', \'a, b, c\', \'6, 7, 8, 9\',\r\n                   \'d, e, f, g, h\', \'10, 11, 12, 13\'])\r\n    cleaned = clean_fld(a, punc)\r\n    return a, cleaned\r\n\r\n\r\ndef _tool():\r\n    """"""run when script is from a tool\r\n    """"""\r\n    in_tbl = sys.argv[1]\r\n    in_fld = sys.argv[2]\r\n    out_fld = sys.argv[3]\r\n    all_punc = sys.argv[4]\r\n    all_white = sys.argv[5]\r\n    all_extra = sys.argv[6]\r\n    all_others = sys.argv[7]\r\n\r\n    a0 = [[], punc][all_punc in (True, \'True\', \'true\')]\r\n    a1 = [[], whitesp][all_white in (True, \'True\', \'true\')]\r\n    if len(all_others) == 1:\r\n        a2 = list(all_others)\r\n    elif len(all_others) > 1:\r\n        if "";"" in all_others:\r\n            a2 = all_others.replace("";"", ""xx"")\r\n            a2 = a2.split(\'xx\')[:-1]\r\n    else:\r\n        a2 = []\r\n    #\r\n    strip_list = a0 + a1 + a2\r\n    desc = arcpy.da.Describe(in_tbl)\r\n    tbl_path = desc[\'path\']\r\n    is_gdb_tbl = tbl_path[-4:] == \'.gdb\'\r\n    fnames = [i.name for i in arcpy.ListFields(in_tbl)]\r\n    if out_fld in fnames:\r\n        out_fld += \'dup\'\r\n    out_fld = arcpy.ValidateFieldName(out_fld, tbl_path)\r\n    args = [in_tbl, in_fld, out_fld, tbl_path]\r\n    msg = ""in_tbl {}\\nin_fld {}\\nout_fld  {}\\ntbl_path  {}"".format(*args)\r\n    tweet(msg)\r\n    tweet(""Removing .... {}"".format(strip_list))\r\n    oid = \'OBJECTID\'\r\n    vals = [oid, in_fld]\r\n    #\r\n    # ---- do the work\r\n    #\r\n    arr = arcpy.da.TableToNumPyArray(in_tbl, vals)\r\n    tweet(""{!r:}"".format(arr))\r\n    a0 = arr[in_fld]\r\n    #\r\n    cleaned = clean_fld(a0, strip_list)  # punc\r\n    #\r\n    if all_extra in (True, \'True\', \'true\'):\r\n        sps = [\'    \',  \'   \', \'  \']\r\n        for i in sps:\r\n            cleaned = np.char.replace(cleaned, i, "" "")\r\n    sze = cleaned.dtype.str\r\n    dt = [(\'IDs\', \'<i8\'), (out_fld, sze)]\r\n    out_array = np.empty((arr.shape[0], ), dtype=dt)\r\n    out_array[\'IDs\'] = np.arange(1, arr.shape[0] + 1)\r\n    out_array[out_fld] = cleaned\r\n    #\r\n    #\r\n    arcpy.da.ExtendTable(in_tbl, \'OBJECTID\', out_array, \'IDs\')\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# .... final code section producing the featureclass and extendtable\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    arrs, c = _demo()\r\n    frmt = ""Result...\\n{}""\r\n    print(frmt.format(c))\r\nelse:\r\n    testing = False\r\n    _tool()\r\n#\r\nif not testing:\r\n    print(\'Concatenation done...\')\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n    a, c = _demo()\r\n'"
all_scripts/surface.py,90,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\nsurface\r\n=======\r\n\r\nScript:   surface.py\r\n\r\nAuthor:   Dan.Patterson@carleton.ca\r\n\r\nModified: 2018-04-27\r\n\r\nPurpose :\r\n    Calculate slope, aspect, hillshade and other terrain derivatives\r\n    Plus basic array ramblings so I don\'t forget.\r\n\r\n---------------------------\r\n1. Truth test and slicing:\r\n---------------------------\r\n\r\nExample:\r\n\r\n>>> a = np.arange(3*3*3).reshape(3,3,3)\r\n>>> art.f_(a)\r\nArray... shape (3, 3, 3), ndim 3, not masked\r\n  0,  1,  2     9, 10, 11    18, 19, 20\r\n  3,  4,  5    12, 13, 14    21, 22, 23\r\n  6,  7,  8    15, 16, 17    24, 25, 26\r\n sub (0)       sub (1)       sub (2)\r\n\r\n>>> np.all((a[...,2], a[...,:,2], a[:,:,2])) # => True\r\na[:,:,2]\r\narray([[ 2,  5,  8],\r\n       [11, 14, 17],\r\n       [20, 23, 26]])\r\n\r\n\r\n2. Filters:\r\n----------\r\n- f_dxyz : as implemented in arcmap after Burrough\r\n\r\n  np.array([[1,2,1], [2,0,2], [1,2,1]], dtype=""float64"")\r\n\r\n- f_plus   - maximum rise/fall after eppl7\r\n\r\n  np.array([[0,1,0], [1,0,1], [0,1,0]], dtype=""float64"")\r\n\r\n-  f_cross  -\r\n\r\n   np.array([[1,0,1], [0,0,0], [1,0,1]], dtype=""float64"")\r\n\r\n-   f_d8   t = np.sqrt(2.0)  f_plus + t*cross used for distance\r\n\r\n   np.array([[t,1,t], [1,0,1], [t,1,t]], dtype=""float64"")\r\n\r\n\r\n3. Slope calculation:\r\n---------------------\r\n- 3rd order finite distance Horn (1981) see Burrough\r\n>>> filter = [[a, b, c], [d, e, f], [g, h, i]]\r\n    [dz/dx] = ((c + 2f + i) - (a + 2d + g)) / (8 * x_cellsize)\r\n    [dz/dy] = ((g + 2h + i) - (a + 2b + c)) / (8 * y_cellsize)\r\n    rise_run = sqrt([dz/dx]2 + [dz/dy]2)\r\n             = sqrt((0.05)2 + (-3.8)2)\r\n             = sqrt(0.0025 + 14.44) = 3.80032\r\n    slope_radians = ATAN(sqrt([dz/dx]2 + [dz/dy]2) )\r\n\r\n::\r\n\r\n    [dz/dx] = ((c + 2f + i) - (a + 2d + g) / (8 * x_cellsize)\r\n            = ((50 + 60 + 10) - (50 + 60 + 8)) / (8 * 5)\r\n            = (120 - 118) / 40 = 0.05\r\n\r\n    [dz/dy] = ((g + 2h + i) - (a + 2b + c)) / (8 * y_cellsize)\r\n            = ((8 + 20 + 10) - (50 + 90 + 50)) / (8 * 5)\r\n            = (38 - 190 ) / 40 = -3.8\r\n\r\n    rise_run = sqrt(([dz/dx]2 + [dz/dy]2)\r\n             = sqrt((0.05)2 + (-3.8)2)\r\n             = sqrt(0.0025 + 14.44) = 3.80032\r\n\r\n    slope_degrees = ATAN(rise_run) * 57.29578\r\n                  = ATAN(3.80032) * 57.29578\r\n                  = 1.31349 * 57.29578 = 75.25762\r\n\r\nOr via code:\r\n\r\n    >>> a= np.array([[50,45,50],[30,30,30],[8, 10, 10]]).reshape(1,1,3,3)\r\n    >>> f_dxyz = np.array([[1, 2, 1], [2, 0, 2], [1, 2, 1]], dtype=""float64"")\r\n    >>> cell_size = 5. * 8\r\n    >>> a_s = a * f_dxyz\r\n    >>> dz_dx = ((a_s[...,:,2] - a_s[...,:,0]).sum(axis=2)).squeeze()/cell_size\r\n    >>> dz_dy = ((a_s[...,2,:] - a_s[...,0,:]).sum(axis=2)).squeeze()/cell_size\r\n    dz_dx  # =>  0.050000000000000003\r\n    dz_dy  # => -3.7999999999999998\r\n    # after finishing all the math... = 75.25762\r\n\r\nNote:\r\n    These are all the same... since we are working with a 4d array\r\n\r\nThe same applies for dz_dy\r\n\r\n- dz_dx = ((a_s[...,:,2] - a_s[...,:,0]).sum(axis=-1))/cell_size\r\n- dz_dx = ((a_s[...,:,2] - a_s[...,:,0]).sum(axis=2)).squeeze()/cell_size\r\n- dz_dx = ((a_s[...,:,2] - a_s[...,:,0]).flatten()).sum()/cell_size\r\n\r\n\r\n4. Aspect calculation:\r\n----------------------\r\n\r\n- [dz/dx] = ((c + 2f + i) - (a + 2d + g)) / 8\r\n\r\n- [dz/dy] = ((g + 2h + i) - (a + 2b + c)) / 8\r\n\r\n- aspect = 57.29578 * atan2 ([dz/dy], -[dz/dx])\r\n\r\n-  aspect rule:  **** general way, but not the most efficient ****\r\n\r\n>>> if aspect < 0:       cell = 90.0 - aspect\r\n    elif aspect > 90.0:  cell = 360.0 - aspect + 90.0\r\n    else:                cell = 90.0 - aspect\r\n    return cell\r\n\r\nor ...\r\n\r\n>>> azimuth = np.mod((450.0 - 45), 360)      # => 45  **** best one ****\r\n\r\nor ...\r\n\r\n::\r\n\r\n   divmod((450.0 - 45), 360)[1]   => (1.0, 45.0)\r\n   [dz/dx] = ((c + 2f + i) - (a + 2d + g)) / 8\r\n           = ((85 + 170 + 84)) - (101 + 202 + 101)) / 8  = -8.125\r\n\r\n   [dz/dy] = ((g + 2h + i) - (a + 2b + c)) / 8\r\n           = ((101 + 182 + 84) - (101 + 184 + 85)) / 8   = -0.375\r\n\r\n   aspect = 57.29578 * atan2 ([dz/dy], -[dz/dx])\r\n          = 57.29578 * atan2 (-0.375, 8.125)    = -2.64:\r\n\r\n   a= np.array([[50,45,50],[30,30,30],[8, 10, 10]]).reshape(1,1,3,3)\r\n\r\n\r\n5. Hillshade:\r\n-------------\r\n\r\n::\r\n\r\n  Hillshade = 255.0 * ((cos(Zenith_rad) * cos(Slope_rad)) +\r\n                 (sin(Zenith_rad) * sin(Slope_rad) *\r\n                  cos(Azimuth_rad - Aspect_rad)))\r\n  Zenith_deg = 90 - Altitude\r\n  Convert to radians\r\n  Zenith_rad = Zenith * pi / 180.0\r\n\r\n  Azimuth_math = 360.0 - Azimuth + 90       # normally they are doing this\r\n  Note that if Azimuth_math >= 360.0, then: # to rotate back to the x-axis\r\n  Azimuth_math = Azimuth_math - 360.0       # I skip this and use North\r\n\r\n  Convert to radians\r\n  Azimuth_rad = Azimuth_math * pi / 180.0\r\n\r\n\r\nComputing slope and aspect\r\n\r\n6. Curvature:\r\n-------------\r\n\r\n::\r\n\r\n   Z1  Z2  Z3 - L cell width, L2=L^2, L3=L^3, L4=L^4\r\n   Z4  Z5  Z6\r\n   Z7  Z8  z9\r\n   from: http://desktop.arcgis.com/en/arcmap/latest/tools/\r\n               spatial-analyst-toolbox/how-curvature-works.htm\r\n   For each cell, a fourth-order polynomial of the form:\r\n   Z = Ax\xc2\xb2y\xc2\xb2 + Bx\xc2\xb2y + Cxy\xc2\xb2 + Dx\xc2\xb2 + Ey\xc2\xb2 + Fxy + Gx + Hy + I\r\n   is fit to a surface composed of a 3x3 window. The coefficients\r\n   a, b, c, and so on, are calculated from this surface.\r\n\r\n::\r\n\r\n   A = [(Z1 + Z3 + Z7 + Z9)/4  - (Z2 + Z4 + Z6 + Z8)/2 + Z5]/ L4\r\n   B = [(Z1 + Z3 - Z7 - Z9)/4 - (Z2 - Z8)/2] / L3\r\n   C = [(-Z1 + Z3 - Z7 + Z9)/4 + (Z4 - Z6)]/2] / L3\r\n   D = [(Z4 + Z6)/2 - Z5]/L2\r\n   E = [(Z2 + Z8)/2 - Z5]/L2\r\n   F = (-Z1 + Z3 + Z7 - Z9) / 4L2\r\n   G = (-Z4 + Z6)/2L\r\n   H = (Z2 - Z8)/2L\r\n   I = Z5\r\n   slope = sqrt(G^2 + H^2)  aspect = arctan(-H/-G)\r\n   profile curv.  =  2(DG^2 + EH^2 + FGH)/(G^2 + H^2)\r\n   planform curv. = -2(DH^2 + EG^2 - FGH)/(G^2 + H^2)\r\n   curvature... slope of the slope\r\n   Curvature = -2(D + E) * 100\r\n   or...  (100/L2)*(3*Z5 - [Z2+Z4+Z6+Z8+Z5])\r\n   Bill\'s shortcut\r\n   in comments http://gis.stackexchange.com/questions/37066/\r\n                     how-to-calculate-terrain-curvature\r\n\r\n\r\n6. Axis angles conversion:\r\n--------------------------\r\n\r\n::\r\n\r\n  axis angle and azimuth relationships..\r\n  an_az = np.array([[180, 270], [135, 315], [90, 0], [45, 45], [0, 90],\r\n                    [-45, 135], [-90, 180], [-135, 225],[-180, 270]])\r\n  an = an_az[:,0] => [180, 135, 90, 45, 0, -45, -90, -135, -180])\r\n  az = an_az[:,1] => [270, 315, 0, 45, 90, 135, 180, 225, 270]\r\n\r\n  azimuth np.mod((450.0 - an), 360)   **** best one ****\r\n\r\n\r\n7. single_demo:\r\n---------------\r\n\r\n-  pre-made orientations at 45 deg.  Example with a dx/dy of 2\r\n\r\n::\r\n\r\n     dx=2 - slope  45.0 asp:   0.0 hshade: 204.0\r\n     dx=2 - slope: 35.3 asp:  45.0 hshade: 137.0\r\n     dx=2 - slope: 45.0 asp:  90.0 hshade:   0.0\r\n     dx=2 - slope: 35.3 asp: 135.0 hshade:  19.0\r\n     dx=2 - slope: 45.0 asp: 180.0 hshade:   0.0\r\n     dx=2 - slope: 35.3 asp: 225.0 hshade: 137.0\r\n     dx=2 - slope: 45.0 asp: 270.0 hshade: 204.0\r\n     dx=2 - slope: 35.3 asp: 315.0 hshade: 254.0\r\n\r\n\r\ninterweave arrays:\r\n------------------\r\n\r\nVarious examples :\r\n\r\nlist comprehensions, column_stack, row_stack, r_, c_, vstack, hstack\r\n\r\n::\r\n\r\n  a = np.arange(5)\r\n  b = np.arange(5,0,-1)\r\n  (1)... [val for pair in zip(a, b) for val in pair] ....\r\n     =>   [0, 5, 1, 4, 2, 3, 3, 2, 4, 1]\r\n  (2)... np.ravel(np.column_stack((a, b))) ....\r\n     ... np.r_[a, b]  # note square brackets\r\n     =>   array([0, 5, 1, 4, 2, 3, 3, 2, 4, 1])\r\n  (3)... np.column_stack((a, b))\r\n     ... np.c_[a, b]  # note square brackets\r\n     =>  array([[0, 5],\r\n                [1, 4],\r\n                [2, 3],\r\n                [3, 2],\r\n                [4, 1]])\r\n  (4)... np.row_stack((a, b))\r\n     ... np.vstack((a, b))\r\n     =>  array([[0, 1, 2, 3, 4],\r\n                [5, 4, 3, 2, 1]])\r\n  (5)... np.hstack((a,b))\r\n         array([0, 1, 2, 3, 4, 5, 4, 3, 2, 1])\r\n  (6)... various inter-weave options\r\n   years = [\'2010\', \'2011\', \'2012\', \'2013\', \'2014\', \'2015\', \'2016\', \'2017\']\r\n   => [z[0] +""-""+ z[1] for z in zip(years[:-1], years[1:])]  # or ****\r\n   => [z[0] + ""-"" + z[1] for z in np.c_[years[:-1], years[1:]]]  # or\r\n   => [""{}-{}"".format(*z) for z in np.c_[years[:-1], years[1:]]]\r\n   => [\'2010-2011\', \'2011-2012\', \'2012-2013\', \'2013-2014\', \'2014-2015\',\r\n       \'2015-2016\', \'2016-2017\']\r\n\r\n\r\nMasked array:\r\n-------------\r\n\r\n::\r\n\r\n  \'nan\', \'nan_to_num\', \'nanargmax\', \'nanargmin\', \'nanmax\', \'nanmean\',\r\n  \'nanmin\', \'nanstd\', \'nansum\', \'nanvar\'\r\n  a_sf = np.array(a_s)*no_cnt\r\n  m = np.where(a_sf==-99, -99,0)\r\n  a_sm = np.ma.array(a_sf, mask = m, fill_value=mask_val)\r\n\r\n\r\nReferences:\r\n----------\r\n\r\n`<https://pro.arcgis.com/en/pro-app/tool-reference/spatial-analyst/\\\r\nhow-hillshade-works.htm>`_\r\n\r\n`<http://pangea.stanford.edu/~samuelj/musings/dems-in-python-pt-3-slope-/\r\nand-hillshades-.html>`_\r\n\r\n`<http://stackoverflow.com/questions/4936620/using-strides-for-an-/\r\nefficient-moving-average-filter>`_\r\n\r\n`<http://pangea.stanford.edu/~samuelj/musings/dems-in-python-pt-3-slope-/\r\nand-hillshades-.html>`_\r\n\r\n`<https://github.com/perrygeo/gdal_utils/blob/master/gis-bin/hillshade.py>`_\r\n\r\n`<http://matplotlib.org/examples/specialty_plots/topographic_hillshading.html>`_\r\n\r\n`<https://blogs.esri.com/esri/arcgis/2015/05/21/take-your-terrain-mapping-to/\r\n-new-heights/>`_\r\n\r\n`<http://gis.stackexchange.com/questions/146296/how-to-create-composite-\r\nhillshade>`_\r\n\r\n`<https://github.com/Blarghedy/TerrainGen-Python>`_\r\n\r\n`<http://vterrain.org/Elevation/Artificial/>`_\r\n\r\n`<http://geogratis.gc.ca/site/eng/extraction?id=2016_56ae834dd24892.15336554>`_\r\n\r\n`<http://www.jennessent.com/downloads/dem%20surface%20tools%20for%20arcgis.pdf>`_\r\n\r\n`<www.geog.ucsb.edu/~kclarke/G232/terrain/Zhang_etal_1999.pdf>`_\r\n\r\n\r\nRequires:\r\n---------\r\n  requires arraytools.tools and stride from there\r\n""""""\r\n# pylint: disable=C0103\r\n# pylint: disable=R1710\r\n# pylint: disable=R0914\r\n\r\n# ---- begin with imports ----\r\n\r\nimport sys\r\nfrom textwrap import dedent, indent\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n#from numpy.lib.stride_tricks import as_strided\r\nfrom arraytools.tools import stride\r\n\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\r\n      \'float\': \'{: 0.2f}\'.format}\r\nnp.set_printoptions(edgeitems=3, linewidth=100, precision=2,\r\n                    suppress=True, threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')\r\n\r\nscript = sys.argv[0]\r\n\r\n__all__ = [\'pad_a\',\r\n           \'kernels\',\r\n           \'stride\',\r\n           \'filter_a\',\r\n           \'slope_a\',\r\n           \'aspect_a\',\r\n           \'hillshade_a\']\r\n\r\n# ---- constants ----\r\nsurface_kernel = np.array([[1, 2, 1], [2, 0, 2], [1, 2, 1]])\r\nall_f = np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\r\nno_cnt = np.array([[1, 1, 1], [1, np.nan, 1], [1, 1, 1]])\r\ncross_f = np.array([[1, 0, 1], [0, 0, 0], [1, 0, 1]])\r\nplus_f = np.array([[0, 1, 0], [1, 0, 1], [0, 1, 0]])\r\n\r\n\r\n# ---- functions ----\r\n#\r\ndef pad_a(a):\r\n    """"""Pads the input array using mode=\'edge\' replicating the values\r\n    at the edges and corners.\r\n    """"""\r\n    a_pad = np.pad(a, 1, mode=\'edge\')\r\n    return a_pad\r\n\r\n\r\ndef kernels(k=None):\r\n    """"""Kernels used for slope calculations\r\n\r\n    Requires:\r\n    ---------\r\n    - `f_dxyz`: default after ArcMap and Burrough, surface_kernel as above\r\n    - `f_plus`:  maximum rise/fall after eppl7\r\n    - `f_cross`\r\n    - `\'f_d8`\r\n    """"""\r\n    if k == \'plus_f\':  # maximum rise/fall after eppl7\r\n        k = np.array([[0, 1, 0], [1, 0, 1], [0, 1, 0]], dtype=""float64"")\r\n    elif k == \'cross_f\':\r\n        k = np.array([[1, 0, 1], [0, 0, 0], [1, 0, 1]], dtype=""float64"")\r\n    elif k == \'f_d8\':\r\n        t = np.sqrt(2.0)  # f_plus + t*cross used for distance\r\n        k = np.array([[t, 1, t], [1, 0, 1], [t, 1, t]], dtype=""float64"")\r\n    else:  # f_dxyz or None or none of the above\r\n        k = np.array([[1, 2, 1], [2, 0, 2], [1, 2, 1]], dtype=""float64"")\r\n    return k\r\n\r\n\r\ndef filter_a(a_s, a_filter=surface_kernel, cell_size=1):\r\n    """"""Used by aspect, slope and hillshade to filter a raster/array\r\n\r\n    Requires:\r\n    --------\r\n    - a_s : a strided array with the shape r*c*3x3 is input\r\n    - filter : a 3x3 filter to apply to a_s\r\n    - cell_size - for slope, (actual size)*8; for aspect (8 is required)\r\n    """"""\r\n    cs = cell_size*8.0\r\n    f_dxyz = a_filter\r\n    if a_filter is None:\r\n        f_dxyz = np.array([[1, 2, 1], [2, 0, 2], [1, 2, 1]], dtype=""float64"")\r\n    a_s = a_s * f_dxyz\r\n    dz_dx = ((a_s[..., :, 2] - a_s[..., :, 0]).sum(axis=-1))/cs\r\n    dz_dy = ((a_s[..., 2, :] - a_s[..., 0, :]).sum(axis=-1))/cs\r\n    return dz_dx, dz_dy\r\n\r\n\r\n# @time_deco\r\ndef slope_a(a, cell_size=1, kern=None, degrees=True, verb=False, keep=False):\r\n    """"""Return slope in degrees for an input array using 3rd order\r\n    finite difference method for a 3x3 moing window view into the array.\r\n\r\n    Requires:\r\n    ---------\r\n    - a : an input 2d array. X and Y represent coordinates of the Z values\r\n    - cell_size : cell size, must be in the same units as X and Y\r\n    - kern : kernel to use\r\n    - degrees : True, returns degrees otherwise radians\r\n    - verb : True, to print results\r\n    - keep : False, to remove/squeeze extra dimensions\r\n    - filter :\r\n        np.array([[1, 2, 1], [2, 0, 2], [1, 2, 1]]) **current default\r\n\r\n    Notes:\r\n    ------\r\n\r\n    ::\r\n\r\n        dzdx: sum(col2 - col0)/8*cellsize\r\n        dzdy: sum(row2 - row0)/8*celsize\r\n        Assert the array is ndim=4 even if (1,z,y,x)\r\n        general         dzdx      +    dzdy     =    dxyz\r\n        [[a, b, c],  [[1, 0, 1],   [[1, 2, 1]       [[1, 2, 1]\r\n         [d, e, f]    [2, 0, 2], +  [0, 0, 0]   =    [2, 0, 2],\r\n         [g, h, i]    [1, 0, 1]]    [1, 2, 1]]       [1, 2, 1]]\r\n\r\n    """"""\r\n    frmt = """"""\\n    :----------------------------------------:\r\n    :{}\\n    :input array...\\n    {}\\n    :slope values...\\n    {!r:}\r\n    :----------------------------------------:\r\n    """"""\r\n    # ---- stride the data and calculate slope for 3x3 sliding windows ----\r\n    np.set_printoptions(edgeitems=10, linewidth=100, precision=1)\r\n    a_s = stride(a, win=(3, 3), stepby=(1, 1))\r\n    if a_s.ndim < 4:\r\n        new_shape = (1,) * (4-len(a_s.shape)) + a_s.shape\r\n        a_s = a_s.reshape(new_shape)\r\n    #\r\n    kern = kernels(kern)  # return the kernel if specified\r\n    # ---- default filter, apply the filter to the array ----\r\n    #\r\n    dz_dx, dz_dy = filter_a(a_s, a_filter=kern, cell_size=cell_size)\r\n    #\r\n    s = np.sqrt(dz_dx**2 + dz_dy**2)\r\n    if degrees:\r\n        s = np.rad2deg(np.arctan(s))\r\n    if not keep:\r\n        s = np.squeeze(s)\r\n    if verb:\r\n        p = ""    ""\r\n        args = [""Results for slope_a... "",\r\n                indent(str(a), p), indent(str(s), p)]\r\n        print(dedent(frmt).format(*args))\r\n    return s\r\n\r\n\r\ndef aspect_a(a, cell_size=1, flat=0.1, degrees=True, keepdims=False):\r\n    """"""Return the aspect of a slope in degrees from North.\r\n\r\n    Requires:\r\n    --------\r\n    - a :\r\n        an input 2d array. X and Y represent coordinates of the Z values\r\n    - cell_size :\r\n        needed to proper flat calculation\r\n    - flat :\r\n        degree value, e.g. flat surface <= 0.05 deg\r\n\r\n        0.05 deg => 8.7e-04 rad   0.10 deg => 1.7e-02 rad\r\n    """"""\r\n    if not isinstance(flat, (int, float)):\r\n        flat = 0.1\r\n    a_s = stride(a, win=(3, 3), stepby=(1, 1))\r\n    if a_s.ndim < 4:\r\n        new_shape = (1,) * (4-len(a_s.shape)) + a_s.shape\r\n        a_s = a_s.reshape(new_shape)\r\n    f_dxyz = np.array([[1, 2, 1], [2, 0, 2], [1, 2, 1]], dtype=""float64"")\r\n    a_s = a_s * f_dxyz\r\n    #\r\n    dz_dx, dz_dy = filter_a(a_s, a_filter=f_dxyz, cell_size=1)\r\n    #\r\n    asp = np.arctan2(dz_dy, -dz_dx)     # relative to East\r\n    # get the slope\r\n    s = np.sqrt((dz_dx*cell_size)**2 + (dz_dy*cell_size)**2)\r\n    asp = np.rad2deg(asp)\r\n    asp = np.mod((450.0 - asp), 360.)   # simplest way to get azimuth\r\n    asp = np.where(s <= flat, -1, asp)\r\n    if not keepdims:\r\n        asp = np.squeeze(asp)\r\n    if not degrees:\r\n        asp = np.deg2rad(asp)\r\n    return asp\r\n\r\n\r\ndef aspect_dem(a):\r\n    """"""Return aspect relative to north """"""\r\n    a = np.asarray(a)\r\n    dzdx_a = (a[:, 2] - a[:, 0]).sum() / 8.0  # aspect: col2 - col0\r\n    dzdy_a = (a[2] - a[0]).sum() / 8.0        # aspect: row2 - row0\r\n    s = np.degrees(np.arctan2(dzdy_a, -dzdx_a))\r\n    aspect = np.where(s < 0, 90. - s,\r\n                      np.where(s > 90, 450.0 - s, 90.0 - s))\r\n    return aspect\r\n\r\n\r\ndef angle2azim(val):\r\n    """"""correct x-oriented angle (in degrees) to N-oriented azimuth""""""\r\n    if val < 0:\r\n        az = 90.0 - val\r\n    elif val > 90.0:\r\n        az = 450.0 - val\r\n    else:\r\n        az = 90.0 - val\r\n    return az\r\n\r\n\r\ndef a2z(vals):\r\n    """"""a numpy version of angle2azim for single values""""""\r\n    out = np.where(vals < 0, 90. - vals,\r\n                   np.where(vals > 90, 450.0 - vals, 90.0 - vals))\r\n    return out\r\n\r\n\r\ndef hillshade_a(a, cell_size=1, sun_azim=315, sun_elev=45):\r\n    """"""Hillshade calculation as outlined in Burrough and implemented by\r\n    : esri in ArcMap and ArcGIS Pro.  All measures in radians.\r\n\r\n    - z, az :\r\n        sun\'s zenith angle and azimuth\r\n    - sl, asp :\r\n        surface properties, slope and aspect\r\n    - hillshade:\r\n        255.0 * ((cos(z) * cos(sl)) + (sin(z) * sin(sl) * cos(az-asp)))\r\n    """"""\r\n    s_azi = np.deg2rad(sun_azim)\r\n    s_elev = np.deg2rad(90.0 - sun_elev)\r\n    a_a = aspect_a(a, degrees=False)\r\n    a_s = slope_a(a, cell_size=cell_size, degrees=False)\r\n    out = 255*((np.cos(s_elev) * np.cos(a_s)) +\r\n               (np.sin(s_elev) * np.sin(a_s) * np.cos(s_azi - a_a)))\r\n    out = np.where(out < 0, 0, out)\r\n    return out.astype(\'int\')\r\n\r\n\r\n# ---- Demo section ----------------------------------------------------------\r\n#\r\ndef _slope_aspect_demo_(cell_size=2):\r\n    """"""Demo of the data set below""""""\r\n    a = np.array([[0, 1, 2, 3, 3, 3, 2, 1, 0],\r\n                  [1, 2, 3, 4, 4, 4, 3, 2, 1],\r\n                  [2, 3, 4, 5, 5, 5, 4, 3, 2],\r\n                  [3, 4, 5, 5, 5, 5, 5, 4, 3],\r\n                  [3, 4, 5, 5, 5, 5, 5, 4, 3],\r\n                  [3, 4, 5, 5, 5, 5, 5, 4, 3],\r\n                  [2, 3, 4, 5, 5, 5, 4, 3, 2],\r\n                  [1, 2, 3, 4, 4, 4, 3, 2, 1],\r\n                  [0, 1, 2, 3, 3, 3, 2, 1, 0]])\r\n    r, c = a.shape\r\n    data = stride(a, win=(3, 3), stepby=(1, 1))  # produce strided array\r\n    slope = slope_a(a, cell_size=cell_size)\r\n    slope = np.array(slope)\r\n    aspect = [aspect_a(data[i][j])\r\n              for i in range(data.shape[0])\r\n              for j in range(data.shape[1])]\r\n    aspect = np.array(aspect).reshape((r-2, c-2))\r\n    frmt = """"""\r\n    :---- Slope, Aspect Demo ------------------------------------------------\r\n    :Sample DEM with a cell size of {} units.\r\n    {}\\n\r\n    Slope (degrees) ...\r\n    {}\\n\r\n    Aspect (degrees) ...\r\n    {}\r\n    """"""\r\n    args = [cell_size]\r\n    pre = \'  ..\'\r\n    args.extend([indent(str(i), pre) for i in [a, slope, aspect]])\r\n    print(dedent(frmt).format(*args))\r\n    return a, slope, aspect\r\n\r\n\r\ndef pyramid(core=10, steps=10, stepby=2, incr=(1, 1), posi=True):\r\n    """"""Create a pyramid see pyramid_demo.py""""""\r\n    a = np.array([core])\r\n    a = np.atleast_2d(a)\r\n    for i in range(stepby, steps+1, stepby):\r\n        val = core-i\r\n        if posi and (val <= 0):\r\n            val = 0\r\n        a = np.lib.pad(a, incr, ""constant"", constant_values=(val, val))\r\n    return a\r\n\r\n\r\ndef _demo(cell_size=5, pad=False):\r\n    """"""Demonstration of calculations\r\n    :\r\n    """"""\r\n    frmt = """"""\r\n    :------------------------------------------------------------------\r\n    :{}{}\\n    :input array\\n    {}\\n    :slope values\\n    {}\r\n    :aspect values\\n    {}\\n    :\r\n    :------------------------------------------------------------------\r\n    """"""\r\n    ft0 = {\'float\': \'{: 0.1f}\'.format}\r\n    np.set_printoptions()\r\n    np.set_printoptions(linewidth=100, precision=1, formatter=ft0)\r\n    # p = ""    ""\r\n    # a = pyramid(core=5, steps=4, incr=(1,1), posi=True)\r\n    t = 1.0e-8\r\n    u = 1.0e-4\r\n    # n = np.nan\r\n    x = 4\r\n    y = 2\r\n    d0 = [[0, 0, 0, 2, 3, 4, 3, 2, 2, 4, 2, 3, 2],\r\n          [0, 0, 0, 2, 4, 5, 4, 3, 3, 3, 3, 2, 1],\r\n          [0, 0, 0, 2, 5, 6, 5, 4, 4, 2, 4, 1, 0]]\r\n    d1 = [[0, 0, 0, 0, 0, 0, 0, 2, 2, 2, y, x, x, 2],\r\n          [t, 0, u, u, t, 0, 0, 2, 2, 2, y, x, x, 2],\r\n          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, y, x, x, 2],\r\n          [1, 2, 3, 4, 5, 3, 3, 2, 1, 0, y, x, x, 2],\r\n          [2, 3, 4, 5, 6, 4, 4, 2, 1, 0, y, x, x, 2],\r\n          [3, 4, 5, 6, 3, 3, 3, 2, 1, 0, y, x, x, 2],\r\n          [2, 2, 4, 5, 2, 2, 2, 2, 0, 0, y, x, x, 2],\r\n          [1, 1, 2, 4, 1, 1, 1, 2, 0, 0, y, x, x, 2]]\r\n    ds = [[50, 45, 50], [30, 30, 30], [8, 10, 10]]  # for slope\r\n    da = [[101, 92, 85], [101, 92, 85], [101, 91, 84]]  # for aspect\r\n    ds = [d0, d1, ds, da]\r\n    for d in ds:\r\n        a = np.array(d, dtype=\'float64\')\r\n        if pad:\r\n            a = pad_a(a)\r\n        sl = slope_a(a, cell_size, kern=None, degrees=True,\r\n                     verb=False, keep=False)\r\n        asp = aspect_a(a, cell_size=cell_size, flat=0.1)\r\n        args = [""Surface properties for array shape "", a.shape, a, sl, asp]\r\n        print(dedent(frmt).format(*args))\r\n    return a, d0, d1\r\n\r\n\r\ndef circle_a(radius, pad=0, as_int=True):\r\n    """"""Create a circle\r\n    """"""\r\n    np.set_printoptions(edgeitems=10, linewidth=100, precision=1)\r\n    r = radius + pad\r\n    circ = np.zeros((2*r + 1, 2*r + 1))\r\n    y, x = np.ogrid[-r:r+1, -r:r+1]\r\n    mask = x**2 + y**2 <= radius**2\r\n    circ[mask] = 1\r\n    if as_int:\r\n        circ = circ.astype(\'int\')\r\n    return circ\r\n\r\n\r\ndef demo2():\r\n    """""" from 2.x """"""\r\n    cell_size = 1\r\n    a = pyramid(core=10, steps=10, stepby=1, incr=(1, 1), posi=True)\r\n    sa = slope_a(a, cell_size=cell_size)\r\n    dataExtent = [0, 21, 0, 21]\r\n    hs = hillshade_a(a, cell_size=cell_size, sun_azim=270,\r\n                     sun_elev=45.5)\r\n    # bilinear\r\n    f2 = plt.imshow(sa, interpolation=\'none\', cmap=\'coolwarm\',\r\n                    vmin=a.min(), vmax=a.max(), extent=dataExtent)\r\n    f1 = plt.imshow(hs, interpolation=\'bilinear\', cmap=\'gray\', alpha=0.7,\r\n                    extent=dataExtent)\r\n    plt.gca().invert_yaxis()\r\n    plt.show()\r\n#    plt.close()\r\n    # print(""\\npyramid\\n{}\\nslope:\\n{}\\nhillshade\\n{}"".format(a, sa, hs))\r\n    return f1, f2\r\n\r\n\r\ndef plot_(a, hs=None, interp=\'none\'):\r\n    """"""plot array simply with hillshade if available""""""\r\n    dataExtent = [0, a.shape[0], 0, a.shape[1]]\r\n    f1 = plt.imshow(a, interpolation=interp, cmap=\'coolwarm\',\r\n                    vmin=a.min(), vmax=a.max(), extent=dataExtent)  # bilinear\r\n    f1.show()\r\n    if hs is not None:\r\n        f2 = plt.imshow(hs, interpolation=\'bilinear\', cmap=\'gray\',\r\n                        alpha=0.5, extent=dataExtent)\r\n        f2.show()\r\n    plt.gca().invert_yaxis()\r\n#    plt.show()\r\n#    plt.close()\r\n\r\n\r\ndef circ_demo(rmax=50, plot=False):\r\n    """""" plot a circle and normalize the plot""""""\r\n    circs = []\r\n    for r in range(1, rmax):\r\n        p = rmax-r-1\r\n        c = circle_a(radius=r, pad=p, as_int=True)\r\n        circs.append(c)\r\n    cs = np.sum(circs, axis=0)\r\n    cs[rmax-1, rmax-1] = rmax\r\n    if plot:\r\n        # cn = cs/float(rmax)  # normalize relative to array max\r\n        plot_(cs)\r\n    return cs\r\n\r\n\r\ndef single_demo():\r\n    """"""Some finite single slope examples.\r\n    :\r\n    """"""\r\n    cellsizes = [1, 2, 5, 10]  # cell size\r\n    degrees = True\r\n    a = np.array([[0, 1, 2], [1, 2, 3], [2, 3, 4]])\r\n    a0 = [np.array([[0, 0, 0], [2, 2, 2], [4, 4, 4]]), \'North\']\r\n    a1 = [np.rot90(a, 3), \'NE\']\r\n    a2 = [np.rot90(a0[0], 3), \'E\']\r\n    a3 = [np.rot90(a, 2), \'SE\']\r\n    a4 = [np.rot90(a0[0], 2), \'S\']\r\n    a5 = [np.rot90(a, 1), \'SW\']\r\n    a6 = [np.rot90(a0[0], 1), \'W\']\r\n    a7 = [np.array([[0, 1, 2], [1, 2, 3], [2, 3, 4]]), \'NW\']\r\n    a_s = [a0, a1, a2, a3, a4, a5, a6, a7]\r\n    for r, t in a_s:\r\n        print(""\\nArray, facing...{}\\n{}"".format(t, r))\r\n        print(""   dx  slope  aspect hillshade"""""")\r\n        for cs in cellsizes:\r\n            sa = slope_a(r, cell_size=cs, degrees=degrees)\r\n            asp = aspect_a(r, cell_size=cs, degrees=degrees)\r\n            hs = hillshade_a(r, cell_size=cs)\r\n            frmt = ""{:>5.0f} {:6.1f} {:>7.1f} {:>7.1f}""\r\n            args = [cs]  # cs is a scalar, rest are arrays\r\n            args.extend([np.asscalar(i) for i in [sa, asp, hs]])\r\n            print(dedent(frmt).format(*args))\r\n\r\n\r\n# ---------------------------------------------------------------------\r\n#\r\nif __name__ == ""__main__"":\r\n    """"""Main section...   """"""\r\n#    print(""Script... {}"".format(script))\r\n    #\r\n#    axis = (-1,-2)\r\n#    a, d0, d1 = _demo(cell_size=5, degrees=True, pad=False, verbose=True)\r\n#    circ = circle_a(5, pad=3)\r\n#    a, sa, hs = demo2()  # demo2...........\r\n#    single_demo()\r\n\r\n#    a = np.array([[0, 1, 2, 3, 3, 3, 2, 1, 0],\r\n#                  [1, 2, 3, 4, 4, 4, 3, 2, 1],\r\n#                  [2, 3, 4, 5, 5, 5, 4, 3, 2],\r\n#                  [3, 4, 5, 5, 5, 5, 5, 4, 3],\r\n#                  [3, 4, 5, 5, 5, 5, 5, 4, 3],\r\n#                  [3, 4, 5, 5, 5, 5, 5, 4, 3],\r\n#                  [2, 3, 4, 5, 5, 5, 4, 3, 2],\r\n#                  [1, 2, 3, 4, 4, 4, 3, 2, 1],\r\n#                  [0, 1, 2, 3, 3, 3, 2, 1, 0]])\r\n'"
all_scripts/surface_plot.py,5,"b'# -*- coding: UTF-8 -*-\n""""""\n:Script:   surface_plot.py\n:Author:   Dan.Patterson@carleton.ca\n:Modified: 2017-12-31\n:References\n:  https://en.m.wikipedia.org/wiki/Great_Pyramid_of_Giza\n\n""""""\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom mpl_toolkits.mplot3d import Axes3D\n\n\ndef show_3d(a, surf_shw=True, wire_shw=True, cont_shw=False):\n    """"""Show a 3D array either as a surface, wireframe or contour or\n    :  a combination\n    : - a = np.random.random((nx, ny)) # original\n    """"""\n    nx, ny = a.shape\n    x = range(nx)\n    y = range(ny)\n    ax = plt.figure()  # should call this fig\n    ha = ax.add_subplot(111, projection=\'3d\')\n    X, Y = np.meshgrid(x, y)  # plot_surface expects x and y data to be 2D\n    if surf_shw:\n        surf = ha.plot_surface(X, Y, a, rstride=1, cstride=1,\n                               cmap=cm.coolwarm, linewidth=0,\n                               antialiased=False)\n        ax.colorbar(surf, shrink=0.5, aspect=5)  # ditto change to fig\n    if wire_shw:  # was ax and 5,5\n        ha.plot_wireframe(X, Y, a, rstride=1, cstride=1, color=\'black\')\n    if cont_shw:\n        ha.contourf(X, Y, a, extend3d=True, cmap=cm.coolwarm)\n    plt.show()\n\n\ndef pyramid(core=9, steps=11, incr=(1, 1), prn_arr=False):\n    """"""create a pyramid with a core value, a certain number of steps\n    :  decreasing by incr until done\n    """"""\n    a = np.array([core])\n    a = np.atleast_2d(a)\n    for i in range(1, steps):\n        val = max(0, core - i)\n        a = np.lib.pad(a, incr, ""constant"", constant_values=(val, val))\n    if prn_arr:\n        frmt = ""\\nSimple pyramid array... shape {}, ndim {} \\n{}""\n        print(frmt.format(a.shape, a.ndim, a))\n    return a\n\n\nif __name__ == ""__main__"":\n    """"""create a pyramid and display it""""""\n    a = pyramid()\n    show_3d(a, surf_shw=True, wire_shw=False, cont_shw=False)\n'"
all_scripts/table2numpy.py,9,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   table2numpyarray.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2018-02-11\r\n:Purpose:  tools for working with numpy arrays\r\n:Useage:\r\n:\r\n:References:\r\n:\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\ndef tweet(msg):\r\n    """"""Print a message for both arcpy and python.\r\n    : msg - a text message\r\n    """"""\r\n    m = ""\\n{}\\n"".format(msg)\r\n    arcpy.AddMessage(m)\r\n    print(m)\r\n\r\n\r\n# ---- Run options: _demo or from _tool\r\n#\r\ndef _demo():\r\n    """"""Code to run if in demo mode\r\n    """"""\r\n    a = np. array([\'1, 2, 3, 4, 5\', \'a, b, c\', \'6, 7, 8, 9\',\r\n                   \'d, e, f, g, h\', \'10, 11, 12, 13\'])\r\n    return a\r\n\r\n\r\ndef _tool():\r\n    """"""run when script is from a tool\r\n    """"""\r\n    in_tbl = sys.argv[1]\r\n    in_flds = sys.argv[2]\r\n    out_folder = sys.argv[3]  # output field name\r\n    out_filename = sys.argv[4]\r\n    out_name = ""\\\\"".join([out_folder, out_filename])\r\n    # ---- main tool section\r\n    desc = arcpy.da.Describe(in_tbl)\r\n    args = [in_tbl, in_flds, out_name]\r\n    msg = ""Input table.. {}\\nfields...\\n{}\\nOutput arr  {}"".format(*args)\r\n    tweet(msg)\r\n    #\r\n    # ---- call section for processing function\r\n    #\r\n    oid = \'OBJECTID\'\r\n    in_flds = in_flds.split("";"")\r\n    if oid in in_flds:\r\n        vals = in_flds\r\n    else:\r\n        vals = [oid] + in_flds\r\n    #\r\n    # ---- create the field dictionary\r\n    f_info = np.array([[i.name, i.type] for i in arcpy.ListFields(in_tbl)])\r\n    f_dict = {\'OBJECTID\': -1}\r\n    for f in in_flds:\r\n        if f in f_info[:, 0]:\r\n            n, t = f_info[f_info[:, 0] == f][0]\r\n            if t in (\'Integer\', \'Short\', \'Long\'):\r\n                t = np.iinfo(np.int32).min\r\n            elif t in (\'Double\', \'Float\'):\r\n                t = np.nan\r\n            elif t in (\'String\', \'Text\'):\r\n                t = str(None)\r\n            else:\r\n                t = np.iinfo(np.int32).min\r\n            f_dict[n] = t\r\n    # ---- where_clause= skip_nulls=  null_value=)\r\n    arr = arcpy.da.TableToNumPyArray(in_tbl, vals, ""#"", False, f_dict)\r\n    #\r\n    np.save(out_name, arr)\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# .... final code section producing the featureclass and extendtable\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    arrs= _demo()\r\n    frmt = ""Result...\\n{}""\r\n    print(frmt.format(arrs))\r\nelse:\r\n    testing = False\r\n    _tool()\r\n#\r\nif not testing:\r\n    print(\'Concatenation done...\')\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n    a = _demo()\r\n'"
all_scripts/table_shell_script.py,5,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   array2raster.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2018-02-12\r\n:Purpose:  tools for working with numpy arrays\r\n:Useage:\r\n:\r\n:References:\r\n:  http://pro.arcgis.com/en/pro-app/arcpy/functions/\r\n:       numpyarraytoraster-function.htm\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\n\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\ndef tweet(msg):\r\n    """"""Print a message for both arcpy and python.\r\n    : msg - a text message\r\n    """"""\r\n    m = ""\\n{}\\n"".format(msg)\r\n    arcpy.AddMessage(m)\r\n    print(m)\r\n\r\n\r\ndef your_func_here():\r\n    """""" this is the place""""""\r\n    pass\r\n\r\n\r\n# ---- Run options: _demo or from _tool\r\n#\r\ndef _demo():\r\n    """"""Code to run if in demo mode\r\n    """"""\r\n    a = np. array([\'1, 2, 3, 4, 5\', \'a, b, c\', \'6, 7, 8, 9\',\r\n                   \'d, e, f, g, h\', \'10, 11, 12, 13\'])\r\n    return a\r\n\r\n\r\ndef _tool():\r\n    """"""run when script is from a tool\r\n    """"""\r\n    in_tbl = sys.argv[1]\r\n    in_fld = sys.argv[2]\r\n    out_fld = sys.argv[3]  # output field name\r\n\r\n    # ---- main tool section\r\n    desc = arcpy.da.Describe(in_tbl)\r\n    tbl_path = desc[\'path\']\r\n    fnames = [i.name for i in arcpy.ListFields(in_tbl)]\r\n    if out_fld in fnames:\r\n        out_fld += \'dup\'\r\n    out_fld = arcpy.ValidateFieldName(out_fld, tbl_path)\r\n    args = [in_tbl, in_fld, out_fld, tbl_path]\r\n    msg = ""in_tbl {}\\nin_fld {}\\nout_fld  {}\\ntbl_path  {}"".format(*args)\r\n    tweet(msg)\r\n    #\r\n    # ---- call section for processing function\r\n    #\r\n    oid = \'OBJECTID\'\r\n    vals = [oid] + in_fld\r\n    in_arr = arcpy.da.TableToNumPyArray(in_tbl, vals)\r\n    tweet(""{!r:}"".format(arr))\r\n    #\r\n    a0 = in_arr[in_fld]\r\n    # do stuff here\r\n    sze = a0.dtype.str\r\n    # ---- reassemble the table for extending\r\n    dt = [(\'IDs\', \'<i8\'), (out_fld, sze)]\r\n    out_array = np.copy(in_arr.shape[0])\r\n    out_array[out_fld] = a0  # result goes here\r\n    out_array.dtype = dt\r\n    arcpy.da.ExtendTable(in_tbl, \'OBJECTID\', out_array, \'IDs\')\r\n\r\n# ----------------------------------------------------------------------\r\n# .... final code section producing the featureclass and extendtable\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    a = _demo()\r\n    frmt = ""Result...\\n{}""\r\n    print(frmt.format(a))\r\nelse:\r\n    testing = False\r\n    _tool()\r\n#\r\nif not testing:\r\n    print(\'Concatenation done...\')\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n    a = _demo()\r\n'"
all_scripts/table_to_text.py,3,"b'# -*- coding: UTF-8 -*-\n""""""\n:Script:   table_to_text.py\n:Author:   Dan.Patterson@carleton.ca\n:Modified: 2017-04-02\n:Purpose:\n:    To produce a formatted list/array format for ndarray, structured\n:    arrays, recarrays\n:References:\n:  http://pyopengl.sourceforge.net/pydoc/numpy.lib.recfunctions.html\n:  http://desktop.arcgis.com/en/arcmap/latest/analyze/arcpy-data-access/\n:        featureclasstonumpyarray.htm\n:\n""""""\nimport sys\nimport numpy as np\nimport arcpy\nfrom textwrap import dedent\n\nformatter = {\'float\': \'{:0.3f}\'.format,\n             \'float64\': \'{:0.3f}\'.format}\nnp.set_printoptions(edgeitems=3, linewidth=80, precision=2, suppress=True,\n                    threshold=200, formatter=formatter)\nscript = sys.argv[0]\n\n\ndef tbl_arr(in_tbl, in_flds=None):\n    """"""Convert a table to text\n    :Requires\n    :--------\n    :  in_tbl - a table from within arcmap\n    :  in_flds - either None, a list/tuple of field names.  If None or an\n    :            empty list or tuple, then all fields are returned.\n    """"""\n    if not isinstance(in_flds, (list, tuple, type(None), """")):\n        return ""Input is not correct""\n    if in_flds is None:\n        in_flds = ""*""\n    elif isinstance(in_flds, (list, tuple)):\n        if len(in_flds) == 0:\n            in_flds = ""*""\n    a = arcpy.da.TableToNumPyArray(in_tbl, in_flds)\n    return a\n\n\ndef tweet(msg):\n    """"""Produce a message for both arcpy and python\n    : msg - a text message\n    """"""\n    m = ""\\n{}\\n"".format(msg)\n    arcpy.AddMessage(m)\n    print(m)\n    print(arcpy.GetMessages())\n\n\n# ----------------------------------------------------------------------\n# (6) frmt_struct .... code section --- from frmts.py in arraytools\n#\ndef _col_format(a, nme=""fld"", deci=0):\n    """"""Determine column format given a desired number of decimal places.\n    :  Used by frmt_struct.\n    :  a - a column in an array\n    :  nme - column name\n    :  deci - desired number of decimal points if the data are numeric\n    :Notes:\n    :-----\n    :  The field is examined to determin whether it is a simple integer, a\n    :  float type or a list, array or string.  The maximum width is determined\n    :  based on this type.\n    """"""\n    a_kind = a.dtype.kind\n    a_nm = nme\n    if a_kind in (\'i\', \'u\'):                 # integer type\n        w_, m_ = [\':> {}.0f\', \'{:> 0.0f}\']\n        col_wdth = len(m_.format(a.max())) + 1\n        col_wdth = max(len(a_nm), col_wdth) + 1  # + deci\n        c_fmt = w_.format(col_wdth, 0)\n        # print(""name {} c_fmt {}, wdth {}"".format(a_nm, c_fmt, col_wdth))\n    elif a_kind == \'f\' and np.isscalar(a[0]):   # float type with rounding\n        w_, m_ = [\':> {}.{}f\', \'{:> 0.{}f}\']\n        a_max, a_min = np.round(np.sort(a[[0, -1]]), deci)\n        col_wdth = max(len(m_.format(a_max, deci)),\n                       len(m_.format(a_min, deci))) + 1\n        col_wdth = max(len(a_nm), col_wdth) + 1\n        c_fmt = w_.format(col_wdth, deci)\n    else:                                   # lists, arrays, strings\n        col_wdth = max([len(str(i)) for i in a])\n        col_wdth = max(len(a_nm), col_wdth) + 1  # + deci\n        c_fmt = ""!s:>"" + ""{}"".format(col_wdth)\n    return c_fmt, col_wdth\n\n\ndef frmt_struct(a, deci=2, f_names=True, prn=False):\n    """"""Format a structured array with a mixed dtype.\n    :Requires\n    :-------\n    : a - a structured/recarray\n    : deci - to facilitate printing, this value is the number of decimal\n    :        points to use for all floating point fields.\n    : _col_format - does the actual work of obtaining a representation of\n    :  the column format.\n    :Notes\n    :-----\n    :  It is not really possible to deconstruct the exact number of decimals\n    :  to use for float values, so a decision had to be made to simplify.\n    """"""\n    nms = a.dtype.names\n    N = len(nms)\n    title = [""ABCDEFGHIJKLMNOPQRSTUVWXYZ""[:N], nms][f_names]\n    # ---- get the column formats from ... _col_format ----\n    dts = []\n    wdths = []\n    for i in nms:\n        c_fmt, col_wdth = _col_format(a[i], nme=i, deci=deci)\n        dts.append(c_fmt)\n        wdths.append(max(col_wdth, len(i)))\n        # print(""name {} c_frmt {}, wdth {}"".format(i, c_fmt, col_wdth))\n    rf = "" "".join([(\'{\' + i + \'}\') for i in dts])\n    hdr = [""!s:>"" + ""{}"".format(wdths[i]) for i in range(N)]\n    hdr2 = "" "".join([""{"" + hdr[i] + ""}"" for i in range(N)])\n    header = hdr2.format(*title)\n    txt = [header]\n    for i in range(a.shape[0]):\n        row = rf.format(*a[i])\n        txt.append(row)\n    if prn:\n        for i in txt:\n            print(i)\n    msg = ""\\n"".join([i for i in txt])\n    return msg\n\n\n# ---- main section ----\n\nscript = sys.argv[0]\nin_tbl = sys.argv[1]\nin_flds = sys.argv[2]\nout_txt = str(sys.argv[3]).replace(""\\\\"", ""/"")\n\nin_flds = in_flds.split("";"")\n\nfrmt = """"""\\n\n:---------------------------------------------------------------------:\nRunning.... {}\nInput table ....... {}\nWith fields... {}\nOutput text file... {}\\n\n:---------------------------------------------------------------------:\n""""""\nargs = [script, in_tbl, in_flds, out_txt]\nmsg = dedent(frmt).format(*args)\ntweet(""Input parameters {}"".format(msg))\na = tbl_arr(in_tbl, in_flds)  # call tble_arr to get array\n# msg = frmt_rec(a, deci=2, f_names=True, max_rows=-1)  # send a message\nmsg = frmt_struct(a, deci=2, f_names=True, prn=False)\n\nf = open(out_txt, \'w\')\nprint(msg, file=f)\nf.close()\n\n\nif __name__ == ""__main__"":\n    """"""run sample""""""\n#    in_tbl = r""C:\\GIS\\Tools_scripts\\Table_tools\\Table_tools.gdb\\poly_pnts""\n#    in_flds = [\'OBJECTID\', \'Shape\', \'Id\', \'Area\', \'file_part\', \'X_c\']\n#    in_flds = None\n#    out_txt = r""c:\\temp\\x.txt""\n'"
all_scripts/table_xtras.py,10,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   .py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2017-xx-xx\r\n:Purpose:  tools for working with numpy arrays\r\n:Useage:\r\n:\r\n:References:\r\n:\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nimport numpy.lib.recfunctions as rfn\r\nimport arcpy\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\r\n      \'float\': \'{: 0.3f}\'.format}\r\n\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\ndef tweet(msg):\r\n    """"""Produce a message for both arcpy and python\r\n    : msg - a text message\r\n    """"""\r\n    m = ""\\n{}\\n"".format(msg)\r\n    arcpy.AddMessage(m)\r\n    print(m)\r\n    print(arcpy.GetMessages())\r\n\r\n\r\ndef tbl_txt(in_tbl, in_flds=None):\r\n    """"""Convert a table to text\r\n    :Requires\r\n    :--------\r\n    :  in_tbl - a table from within arcmap\r\n    :  in_flds - either None, a list/tuple of field names.  If None or an\r\n    :            empty list or tuple, then all fields are returned.\r\n    """"""\r\n    if not isinstance(in_flds, (list, tuple, type(None), """")):\r\n        return ""Input is not correct""\r\n    if in_flds is None:\r\n        in_flds = ""*""\r\n    elif isinstance(in_flds, (list, tuple)):\r\n        if len(in_flds) == 0:\r\n            in_flds = ""*""\r\n    a = arcpy.da.TableToNumPyArray(in_tbl, in_flds)\r\n    return a\r\n\r\n\r\ndef rotate_tbl(a, max_cols=20, line_wdth=79):\r\n    """"""Rotate a structured array to offer another view of the dsta\r\n    :  Be reasonable... the 1000 record table just isn\'t going to work.\r\n    :  The maximum number of rows can be specified and the column\r\n    :  widths are determined from the data therein.  Hopefully everything\r\n    :  will fit within the margin width... If not, reduce the number\r\n    :  of rows, or roll your own.\r\n    :\r\n    : max_cols - maximum number of columns to print\r\n    : line_wdth - slice the line after this width\r\n    :\r\n    :Notes:\r\n    : dt = "", "".join([""(\'C{}\', \'<U{}\')"".format(i, j) for i, j in enumerate(w)])\r\n    : w is the widths below and e is the empty object array\r\n    :arcpy.Tabletools.RotateTable(""polygon_demo"",\r\n    :                          ""OBJECTID;file_part;main_part;Test;Pnts;Shape"")\r\n    """"""\r\n    cut = min(a.shape[0], max_cols)\r\n    rc = (len(a[0]), cut + 1)\r\n    a = a[:cut]\r\n    e = np.empty(rc, dtype=np.object)\r\n    e[:, 0] = a.dtype.names\r\n    types = (list, tuple, np.ndarray)\r\n    u0 = [[[j, \'seq\'][isinstance(j, types)] for j in i] for i in a]\r\n    u = np.array(u0, dtype=np.unicode_)\r\n    e[:, 1:] = u[:].T\r\n    widths = [max([len(i) for i in e[:, j]]) for j in range(e.shape[1])]\r\n    f = [""{{!s: <{}}} "".format(width + 1) for width in widths]\r\n    txt = """".join(i for i in f)\r\n    txt = ""\\n"".join([txt.format(*e[i, :])[:line_wdth]\r\n                     for i in range(e.shape[0])])\r\n    txt = ""Attribute | Records....\\n{}"".format(txt)\r\n    tweet(txt)\r\n#    return txt, e, widths\r\n\r\n# ---- main section ----\r\n\'\'\'\r\nscript = sys.argv[0]\r\nif len(sys.argv) > 1:\r\n    in_tbl = sys.argv[1]\r\n    in_flds = sys.argv[2]\r\n#    out_txt = str(sys.argv[3]).replace(""\\\\"", ""/"")\r\nelse:\r\n    in_tbl = r""C:\\GIS\\Tools_scripts\\Table_tools\\Table_tools.gdb\\polygon_demo""\r\n    # in_flds = ""OBJECTID, Shape, Id, Area, file_part, X_c""\r\n# in_flds = in_flds.split("";"")\r\na = tbl_txt(in_tbl, in_flds=None)  # in_flds)\r\n# rotate_tbl(a)  # rotate table demo\r\nnms = a.dtype.names\r\nb = np.array([a[i] for i in nms if a[i].dtype.kind == \'i\'])  # int fields\r\n\'\'\'\r\n\r\n#f = \'C:/GIS/Tools_scripts/Data/sample_20.npy\'\r\n#f = \'C:/GIS/Tools_scripts/Data/sample_1000.npy\'\r\n#f = \'C:/GIS/Tools_scripts/Data/sample_10K.npy\'\r\n#f =  \'C:/GIS/Tools_scripts/Data/array_100K.npy\'\r\nf = \'C:/GIS/Tools_scripts/Data/sample_100K.npy\'\r\n\r\na = np.load(f)\r\nnms = a.dtype.names\r\nsze = [i[1] for i in a.dtype.descr]\r\nuni = np.unique(a[[\'Town\', \'County\']], return_counts=True)\r\nfinal = rfn.append_fields(uni[0], names=\'Count\', data=uni[1], usemask=False)\r\n#n_0 = [(nm, a[nm]) for nm in nms if a[nm].dtype.kind == \'i\']  # int fields\r\nsums = [(nm, a[nm].sum()) for nm in nms if a[nm].dtype.kind == \'i\']  # sums\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n\'\'\'\r\ndt = [(\'Game1\', \'<i4\'), (\'Game2\', \'<i4\'), (\'Game3\', \'<i4\'),\r\n       (\'Game4\', \'<i4\'), (\'Game5\', \'<i4\')]\r\na = np.array([(2, 6, 5, 2, 2),\r\n              (6, 4, 1, 8, 4),\r\n              (8, 3, 2, 1, 5),\r\n              (4, 9, 4, 7, 9)], dtype= dt)\r\nx.view(np.float64).reshape(len(x), -1))\r\n\r\nnms = a.dtype.names\r\nb = [(nm, a[nm]) for nm in nms if a[nm].dtype.kind == \'i\']  # int fields\r\nb = [(nm, a[nm].sum()) for nm in nms if a[nm].dtype.kind == \'i\']  # sums\r\nb = [(nm, a[nm].size) for nm in nms if a[nm].dtype.kind == \'i\']  # size\r\nb = [(nm, a[nm].min()) for nm in nms if a[nm].dtype.kind == \'i\']  # min\r\n\r\nc = [(nm, a[nm]) for nm in nms if a[nm].dtype.kind == \'f\']  # float fields\r\nc = [(nm, a[nm].mean(axis=0)) for nm in nms if a[nm].dtype.kind == \'f\']  #mean\r\nc =>\r\n[(\'Shape\', array([ 23.000,  8.788])),\r\n (\'X_min\', 19.0),\r\n (\'Y_min\', 5.0),\r\n (\'X_max\', 27.0),\r\n (\'Y_max\', 12.5),\r\n (\'Shape_Length\', 39.618033988749893),\r\n (\'Shape_Area\', 55.899999999999999)]\r\n\r\nor...\r\ntxt = ""\\n"".join([""{}: mean= {}"".format(*c[i]) for i in range(len(c))])\r\nprint(txt)\r\nShape: mean= [ 23.000  8.788]\r\nX_min: mean= 19.0\r\nY_min: mean= 5.0\r\nX_max: mean= 27.0\r\nY_max: mean= 12.5\r\nShape_Length: mean= 39.61803398874989\r\n\r\nNow compare to a[\'Shape\'][:,0].mean() => 23\r\n\r\n\'\'\'\r\n'"
all_scripts/tbl.py,38,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\ntbl\r\n===\r\n\r\nScript :   tbl.py   tools for working with text array in table form\r\n\r\nAuthor :   Dan.Patterson@carleton.ca\r\n\r\nModified : 2018-11-12\r\n\r\nPurpose :  Tabulate data\r\n\r\n- Unique counts on 2 or more variables.\r\n- Sums, mins, max etc on variable classes\r\n\r\nRequires:\r\n---------\r\n`frmts.py` is required since it uses print functions from there\r\n\r\n`prn` is used for fancy printing if it loads correctly\r\n\r\nUseage:\r\n-------\r\nTo convert esri geodatabase tables or shapefile tables to arrays, use the\r\nfollowing guidelines.\r\n\r\n>>> float_min = np.finfo(np.float).min\r\n>>> float_max = np.finfo(np.float).max\r\n>>> int_min = np.iinfo(np.int_).min\r\n>>> int_max = np.iinfo(np.int_).max\r\n>>> f = r\'C:\\some\\path\\your.gdb\\your_featureclass\'\r\n>>> null_dict = {\'Int_fld\': int_min, \'Float_fld\': float_min}  # None strings\r\n>>> flds = [\'Int_field\', \'Txt01\', \'Txt02\']  # 2 text fields\r\n>>> a = arcpy.da.TableToNumPyArray(in_table=f, field_names=flds,\r\n                                  skip_nulls=False,\r\n                                  null_value=null_dict)  # if needed\r\n>>> row = \'Txt01\'\r\n>>> col = \'Txt02\'\r\n>>> ctab = crosstab(a, row, col, verbose=False)\r\n\r\nNotes:\r\n------\r\nUseful tip:\r\n\r\n`...install folder.../Lib/site-packages/numpy/core/numerictypes.py`\r\n\r\n>>> # ""import string"" is costly to import!\r\n>>> # Construct the translation tables directly\r\n>>> #   ""A"" = chr(65), ""a"" = chr(97)\r\n>>> _all_chars = [chr(_m) for _m in range(256)]\r\n>>> _ascii_upper = _all_chars[65:65+26]\r\n>>> _ascii_lower = _all_chars[97:97+26]\r\n>>> _just_numbers = _all_chars[48:58]\r\n>>> LOWER_TABLE = """".join(_all_chars[:65] + _ascii_lower + _all_chars[65+26:])\r\n>>> UPPER_TABLE = """".join(_all_chars[:97] + _ascii_upper + _all_chars[97+26:])\r\n\r\n- np.char.split(s, \' \')\r\n- np.char.startswith(s, \'S\')\r\n- np.char.strip()\r\n- np.char.str_len(s)\r\n\r\n- np.sum(np.char.startswith(s, \' \'))  # check for leading spaces\r\n- np.sum(np.char.endswith(s0, \' \'))   # check for trailing spaces\r\n- s0 = np.char.rstrip(s0)\r\n\r\n**Partitioning**:\r\n::\r\n    lp = np.char.partition(s0, \' \')[:, 0]   # get the left-most partition\r\n    rp = np.char.rpartition(s0, \' \')[:, -1] # get the right-most partition\r\n    lpu, lpcnts= np.unique(lp, return_counts=True)\r\n    rpu, rpcnts= np.unique(rp, return_counts=True)\r\n\r\nQueries: d\r\n    np.char.find(c, query) >= 0\r\n\r\n\r\nReferences:\r\n-----------\r\n\r\n`<https://stackoverflow.com/questions/12983067/how-to-find-unique-vectors-of\r\n-a-2d-array-over-a-particular-axis-in-a-vectorized>`_.\r\n\r\n`<https://stackoverflow.com/questions/16970982/find-unique-rows-\r\nin-numpy-array>`_.\r\n\r\n`<http://stackoverflow.com/questions/38030054/create-adjacency-matrix-in-\r\npython-for-large-dataset>`_.\r\n\r\nnp.unique - in the newer version, they use flags to get the sums\r\n\r\n""""""\r\n# pylint: disable=C0103\r\n# pylint: disable=R1710\r\n# pylint: disable=R0914\r\n\r\nimport sys\r\nfrom textwrap import dedent\r\nimport numpy as np\r\n\r\n# ---- others from above , de_punc, _describe, fc_info, fld_info, null_dict,\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\r\n      \'float\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=5, linewidth=80, precision=2,\r\n                    suppress=True, threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')\r\n\r\nscript = sys.argv[0]\r\n\r\nif \'prn\' not in locals().keys():\r\n    try:\r\n        from arraytools.frmts import prn\r\n    except:\r\n        prn = print\r\n\r\n__all__ = [\'find_in\',\r\n           \'tbl_count\',\r\n           \'tbl_sum\']\r\n\r\n# ---- text columns... via char arrays\r\n#\r\n#def tbl_replace(a, col=None, from_=None, to_=None):\r\n#    """"""table replace\r\n#    """"""\r\n#    #np.char.replace(\r\n#    pass\r\n#    return None\r\n\r\ndef find_in(a, col=None, what=None, where=\'in\', any_case=True, pull=\'all\'):\r\n    """"""Query a recarray/structured array for values\r\n\r\n    a : recarray/structured array\r\n        Only text columns can be queried\r\n    col : column/field to query\r\n        Only 1 field can be queried at a time for the condition.\r\n    what : string or number\r\n        The query.  If a number, the field is temporarily converted to a\r\n        text representation for the query.\r\n    where : string\r\n        s, i, eq, en .... `st`(arts with), `in`, `eq`(ual), `en`(ds with)\r\n    any_case : boolean\r\n        True, will find records regardless of `case`, applies to text fields\r\n    extract: text or list\r\n        `all`:  extracts all records where the column case is found\r\n        `list`: extracts the records for only those fields in the list\r\n\r\n    >>> find_text(a, col=\'FULLNAME\', what=\'ABBEY\', pull=a.dtype.names[:2])\r\n    """"""\r\n    # ---- error checking section ----\r\n    e0 = """"""\r\n    :Query error: You provided...\r\n    :  dtype: {}  col: {} what: {}  where: {}  any_case: {}  extract: {}\r\n\r\n    :Required...\\n\r\n    {}\r\n    """"""\r\n    err1 = ""\\nField not found:\\nQuery fields: {}\\nArray fields: {}""\r\n    errors = [a.dtype.names is None,\r\n              col is None, what is None,\r\n              where.lower()[:2] not in (\'en\', \'eq\', \'in\', \'st\'),\r\n              col not in a.dtype.names]\r\n    if sum(errors) > 0:\r\n        arg = [a.dtype.kind, col, what, where, any_case, pull, find_in.__doc__]\r\n        print(dedent(e0).format(*arg))\r\n        return None\r\n    if isinstance(pull, (list, tuple)):\r\n        names = a.dtype.names\r\n        r = [i in names for i in pull]\r\n        if sum(r) != len(r):\r\n            print(err1.format(pull, names))\r\n            return None\r\n    # ---- query section\r\n    # convert column values and query to lowercase, if text, then query\r\n    c = a[col]\r\n    if c.dtype.kind in (\'i\', \'f\', \'c\'):\r\n        c = c.astype(\'U\')\r\n        what = str(what)\r\n    elif any_case:\r\n        c = np.char.lower(c)\r\n        what = what.lower()\r\n    where = where.lower()[0]\r\n    if where == \'i\':\r\n        q = np.char.find(c, what) >= 0   # ---- is in query ----\r\n    elif where == \'s\':\r\n        q = np.char.startswith(c, what)  # ---- startswith query ----\r\n    elif where == \'eq\':\r\n        q = np.char.equal(c, what)\r\n    elif where == \'en\':\r\n        q = np.char.endswith(c, what)    # ---- endswith query ----\r\n    if q.sum() == 0:\r\n        print(""none found"")\r\n        return None\r\n    if pull == \'all\':\r\n        return a[q]\r\n    pull = np.unique([col] + list(pull))\r\n    return a[q][pull]\r\n\r\n\r\ndef tbl_count(a, row=None, col=None, verbose=False):\r\n    """"""Crosstabulate 2 fields data arrays, shape (N,), using np.unique.\r\n    scipy.sparse has similar functionality and is faster for large arrays.\r\n\r\n    Requires:\r\n    --------\r\n    A 2D array of data with shape(N,) representing two variables.\r\n\r\n    row : field/column\r\n        The table column/field to use for the row variable\r\n    col : field/column\r\n        The table column/field to use for thecolumn variable\r\n\r\n    Notes:  See useage section above for converting Arc* tables to arrays.\r\n\r\n    Returns:\r\n    --------\r\n      ctab :\r\n          the crosstabulation result as row, col, count array\r\n      a :\r\n          the crosstabulation in a row, col, count, but filled out whether a\r\n          particular combination exists or not.\r\n      r, c :\r\n          unique values/names for the row and column variables\r\n    """"""\r\n    names = a.dtype.names\r\n    assert row in names, ""The.. {} ..column, not found in array."".format(row)\r\n    assert col in names, ""The.. {} ..column, not found in array."".format(col)\r\n    r_vals = a[row]\r\n    c_vals = a[col]\r\n    dt = np.dtype([(row, r_vals.dtype), (col, c_vals.dtype)])\r\n    rc = np.asarray(list(zip(r_vals, c_vals)), dtype=dt)\r\n    u, idx, cnt = np.unique(rc, return_index=True, return_counts=True)\r\n    rcc_dt = u.dtype.descr\r\n    rcc_dt.append((\'Count\', \'<i4\'))\r\n    ctab = np.asarray(list(zip(u[row], u[col], cnt)), dtype=rcc_dt)\r\n    if verbose:\r\n        prn(ctab)\r\n    else:\r\n        return ctab\r\n\r\n\r\ndef tbl_sum(a, row=None, col=None, val_fld=None):\r\n    """"""Tabular sum of values for two attributes\r\n\r\n    Parameters:\r\n    ----------\r\n    a : array\r\n        Structured/recarray\r\n    row, col : string\r\n        The fields to be used as the table rows and columns\r\n    val_fld : string\r\n        The field that will be summed for the unique combinations of\r\n        row/column classes\r\n\r\n    Returns:\r\n    --------\r\n    A table summarizing the sums for the row/column combinations.\r\n    """"""\r\n    # ---- Slice the input array using the row/column fields, determine the\r\n    # unique combinations of their attributes.  Create the output dtype\r\n    names = a.dtype.names\r\n    assert row in names, ""The.. {} ..column, not found in array."".format(row)\r\n    assert col in names, ""The.. {} ..column, not found in array."".format(col)\r\n    val_kind = a[val_fld].dtype.kind\r\n    if val_kind not in (\'i\', \'f\'):\r\n        print(""\\nThe value field must be numeric"")\r\n        return None\r\n    if val_kind == \'f\':\r\n        val_type = \'<f8\'\r\n    elif val_kind == \'i\':\r\n        val_type = \'<i4\'\r\n    rc = a[[row, col]]\r\n    sum_name = val_fld +\'_sum\'\r\n    dt = rc.dtype.descr + [(sum_name, val_type)]\r\n    uniq = np.unique(rc)\r\n    #\r\n    # ----\r\n    out_ = []\r\n    for u in uniq:\r\n        c0, c1 = u\r\n        idx = np.logical_and(a[row] == c0, a[col] == c1)\r\n        val = np.nansum(a[val_fld][idx])\r\n        out_.append([c0, c1, val])\r\n    out_ = np.array(out_)\r\n    z = np.empty((len(out_),), dtype=dt)\r\n    z[row] = out_[:, 0]\r\n    z[col] = out_[:, 1]\r\n    z[sum_name] = out_[:, 2].astype(val_kind)\r\n    return z\r\n\r\n\r\n# ---- crosstab from tool, uncomment for testing or tool use\r\ndef _demo():\r\n    """"""Load the sample file for testing\r\n    """"""\r\n    # script = sys.argv[0]  # the script path defined earlier\r\n    in_tbl = script.rpartition(""/"")[0] + \'/Data/sample_20.npy\'  # sample_20.npy\r\n    a = np.load(in_tbl)\r\n    ctab = tbl_count(a, row=\'County\', col=\'Town\', verbose=True)\r\n    return a, ctab\r\n\r\ndef _data():\r\n    """"""base file""""""\r\n    in_tbl = script.rpartition(""/"")[0] + \'/Data/points_2000.npy\'\r\n    a = np.load(in_tbl)\r\n    return a\r\n\r\nif __name__ == ""__main__"":\r\n    """"""run crosstabulation with data""""""\r\n#    ctab, counts, out_tbl = tab_count(a[\'County\'], a[\'Town\'],\r\n#    r_fld=\'County\', c_fld=\'Town\', verbose=False)\r\n#    ctab, a, result, r, c = _demo()\r\n'"
all_scripts/tbl2np.py,6,"b'# -*- coding: utf-8 -*-\r\n""""""\r\ntbl2np\r\n======\r\n\r\nScript :   tbl2np.py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified : 2018-09-24\r\n\r\nPurpose:  tools for working with numpy arrays\r\n\r\nUseage :\r\n\r\nReferences\r\n----------\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/numpyarraytotable.htm>`_.\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/tabletonumpyarray.htm>`_.\r\n---------------------------------------------------------------------\r\n""""""\r\n\r\n# pylint: disable=C0103  # invalid-name\r\n# pylint: disable=R0914  # Too many local variables\r\n# pylint: disable=R1710  # inconsistent-return-statements\r\n# pylint: disable=W0105  # string statement has no effect\r\n\r\nimport sys\r\nfrom textwrap import dedent\r\nimport numpy as np\r\nfrom art_common import (tweet, de_punc, _describe, fc_info, fld_info,\r\n                        null_dict, tbl_arr)\r\nfrom arcpy.da import Describe\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=5, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n# ----------------------------------------------------------------------\r\n# .... final code section producing the featureclass and extendtable\r\n\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    pth = script.split(""/"")[:-2]\r\n    pth = ""/"".join(pth) + ""/array_tools.gdb/pnts_2000""\r\n    a = tbl_arr(pth)\r\n    frmt = ""Result...\\n{}""\r\n    print(frmt.format(a))\r\nelse:\r\n    testing = False\r\n    in_tbl = sys.argv[1]\r\n    desc = Describe(in_tbl)\r\n    pth = desc[\'catalogPath\']\r\n#    out_folder = sys.argv[2]\r\n#    out_name = sys.argv[3]\r\n    out_arr = sys.argv[2]  # + ""/"" + out_name\r\n    # ---- call section for processing function\r\n    #\r\n    a = tbl_arr(pth)\r\n    np.save(out_arr, a)\r\n    args = [a, out_arr]\r\n    msg = """"""\r\n    :------------------------------------------------------------\r\n\r\n    Input table... {}\r\n    Output array.... {}\r\n\r\n    Conversion complete...\r\n    You can reload the array using np.load(drive:/path/name.py)\r\n\r\n    :------------------------------------------------------------\r\n    """"""\r\n    msg = dedent(msg).format(*args)\r\n    tweet(msg)\r\nif testing:\r\n    print(\'\\nScript source... {}\'.format(script))\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n'"
all_scripts/tblstats.py,34,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\ntblstats\r\n===========\r\n\r\nScript :   tblstats.py\r\n\r\nAuthor :   Dan.Patterson@carleton.ca\r\n\r\nModified : 2018-11-23\r\n\r\nPurpose :  Descriptive statistics for tables using numpy.\r\n\r\nReferences:\r\n-----------\r\n\r\n`<https://github.com/numpy/numpy/blob/master/numpy/lib/nanfunctions.py>`_.\r\n\r\n_replace_nan(a, val) -  mask = np.isnan(a) - to get the mask\r\n\r\n>>> a = [1, 2, np.nan, 3, np.nan, 4]\r\n>>> _, mask = _replace_nan(a, 0)  # for mean\r\n>>> mask = array([False, False,  True, False,  True, False], dtype=bool)\r\n\r\n""""""\r\n# pylint: disable=C0103\r\n# pylint: disable=R1710\r\n# pylint: disable=R0914\r\n\r\n# ---- imports, formats, constants ----\r\n\r\nimport sys\r\nimport numpy as np\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\n\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n__all__ = [\'freq\',\r\n           \'summ\',\r\n           \'skew_kurt\',         # called by _calc_stats\r\n           \'_calc_stats\',       # called by col_stats\r\n           \'_numeric_fields_\',  # called by col_stats\r\n           \'col_stats\',         # called by col_stats\r\n           \'group_stats\',       # called by col_stats\r\n           ]\r\n\r\nif \'prn\' not in locals().keys():\r\n    try:\r\n        from arraytools.frmts import prn\r\n        # print(""`prn` imported from arraytools"")\r\n    except:\r\n        prn = print\r\n\r\n\r\ndef freq(a, flds=None, to_array=True):\r\n    """"""Frequency and crosstabulation\r\n\r\n    `a` : array\r\n       input structured array\r\n\r\n    `flds` : string or list\r\n       fields/columns to use in the analysis\r\n\r\n    Notes\r\n    -----\r\n    (1) slice the input array by the classification fields\r\n    (2) sort the sliced array using the flds as sorting keys\r\n    (3) use unique on the sorted array to return the results\r\n    (4) reassemble the original columns and the new count data\r\n    """"""\r\n    if flds is None:\r\n        return None\r\n    if isinstance(flds, (str)):\r\n        flds = [flds]\r\n    a = a[flds]  # (1) slice\r\n    idx = np.argsort(a, axis=0, order=flds)  # (2) sort\r\n    a_sort = a[idx]\r\n    uniq, counts = np.unique(a_sort, return_counts=True)  # (3) unique, count\r\n    dt = uniq.dtype.descr + [(\'Count\', \'<i4\')]\r\n    fr = np.zeros_like(uniq, dtype=dt)\r\n    names = fr.dtype.names\r\n    vals = list(zip(*uniq)) + [counts.tolist()]  # (4) reassemble\r\n    for i in range(len(names)):\r\n        fr[names[i]] = vals[i]\r\n    if to_array:\r\n        return fr\r\n    else:\r\n        prn(fr)\r\n\r\n\r\ndef summ(a, cls_flds, uniq, sum_flds):\r\n    """"""sum the input field\r\n\r\n    `a` : array\r\n        large array sliced by the classification fields\r\n    `cls_fields` : fields\r\n        fields to slice the array with\r\n    `uniq` : string\r\n        unique values to sum on\r\n    `sum_flds` : string or list\r\n        The fields to do the sum on\r\n    """"""\r\n    to_sum = a[cls_flds]\r\n    out_sum = []\r\n    for cl in uniq:\r\n        rows = a[to_sum == cl]\r\n        out_sum.append(np.nansum(rows[sum_flds]))  # use nansum\r\n    return out_sum\r\n\r\n# ---- skewness and kurtosis section -----------------------------------------\r\n\r\ndef skew_kurt(a, avg, var_x, std_x, col=True, mom=\'both\'):\r\n    """"""Momental and unbiased skewness\r\n\r\n    Emulates the nan functions approach to calculating these parameters\r\n    when data contains nan values.\r\n\r\n    Requires\r\n    ---------\r\n    a : array\r\n        an array of float/double values where there are at least 3 non-nan\r\n        numbers in each column.  This is not checked since this situation\r\n        should never arise in real world data sets that have been checked.\r\n    moment : string\r\n        both, skew or kurt  to return the moments\r\n\r\n    Notes:\r\n    -----\r\n    >>> a= np.arange(16.).reshape(4,4)\r\n    >>> mask = [0, 5, 10, 15]\r\n    >>> masked_array = np.where(a == mask, np.nan, a)\r\n    """"""\r\n#    a, mask = _replace_nan(a, 0.)  # produce a masked of the nan values\r\n    if len(a.shape) == 1:\r\n        ax = 0\r\n    else:\r\n        ax = [1, 0][col]\r\n#    # ---- mean section ----\r\n    mask = np.isnan(a)\r\n    cnt = np.sum(~mask, axis=ax, dtype=np.intp, keepdims=False)\r\n    diff = a - avg\r\n    sqrd = diff * diff\r\n    cubed = sqrd * diff\r\n    fourP = sqrd * sqrd\r\n    x_3 = np.nansum(cubed, axis=ax)\r\n    x_4 = np.nansum(fourP, axis=ax)\r\n    skew_m = x_3 / (cnt * (std_x**3))\r\n    kurt_m = x_4 / (cnt * (var_x * var_x))\r\n    # skew_u = skew_m*((cnt**2)/((cnt-1)*(cnt-2)))  # could add if needed\r\n    if mom == \'skew\':\r\n        return skew_m\r\n    if mom == \'kurt\':\r\n        return kurt_m\r\n    if mom == \'both\':\r\n        return skew_m, kurt_m\r\n\r\n# ---- calculate field statistics section -----------------------------------\r\n#\r\ndef _calc_stats(arr, axis=None, deci=4):\r\n    """"""Calculate stats for an array of number types, with nodata (nan, None)\r\n    in the column.\r\n\r\n    Notes:\r\n    -----\r\n    see the args tuple for examples of nan functions\r\n\r\n    >>> np.nansum(b, axis=0)   # by column\r\n    >>> np.nansum(b, axis=1)   # by row\r\n    >>> c_nan = np.count_nonzero(~np.isnan(b), axis=0) count nan if needed\r\n\r\n    [1, 0][True]  # ax = [1, 0][colwise]  colwise= True\r\n    """"""\r\n    if (axis is None) and (len(arr.shape) == 1):\r\n        ax = 0\r\n    else:\r\n        ax = axis\r\n    #\r\n    kind = arr.dtype.kind\r\n    arr_dt = arr.dtype\r\n    if kind == \'i\':\r\n        nulls = [np.iinfo(arr_dt).min, np.iinfo(arr_dt).max]\r\n    elif kind == \'f\':\r\n        nulls = [np.nan, np.finfo(arr_dt).min, np.finfo(arr_dt).max]\r\n    elif kind in (\'U\', \'S\'):\r\n        return None\r\n    #\r\n    nin = ~np.isin(arr, nulls)  # nin... Not In Nulls\r\n    a = arr[nin]\r\n    if len(arr.shape) > 1:\r\n        a = a.reshape(arr.shape)\r\n    mask = np.isnan(arr)\r\n    N = len(a)\r\n    cnt = np.sum(~mask, axis=ax, dtype=np.intp, keepdims=False)\r\n    n_sum = np.nansum(a, axis=ax)\r\n    n_min = np.nanmin(a, axis=ax)\r\n    n_max = np.nanmax(a, axis=ax)\r\n    n_mean = np.nanmean(a, axis=ax)\r\n    n_med = np.nanmedian(a, axis=ax)\r\n    n_std = np.nanstd(a, axis=ax)\r\n    n_var = np.nanvar(a, axis=ax)\r\n    sk, kurt = skew_kurt(a, avg=n_mean, var_x=n_var, std_x=n_std,\r\n                         col=True, mom=\'both\')\r\n    s = [N, N-cnt, n_sum, n_min, n_max, n_mean, n_med, n_std, n_var, sk, kurt]\r\n    s = [np.around(i, deci)  for i in s]\r\n    return s\r\n\r\n\r\ndef _numeric_fields_(a, fields):\r\n    """"""Determine numeric fields in a structured/recarray\r\n    """"""\r\n    num_flds = []\r\n    dt_names = a.dtype.names\r\n    dt_kind = a.dtype.kind\r\n    if fields is None:\r\n        if dt_names is None:\r\n            if dt_kind not in (\'i\', \'f\'):\r\n                return None\r\n        elif dt_kind in [\'V\']:\r\n            num_flds = [i for i in dt_names if a[i].dtype.kind in (\'i\', \'f\')]\r\n        else:\r\n            a = a.ravel()\r\n    elif isinstance(fields, (str)):\r\n        if a[fields].dtype.kind in (\'i\', \'f\'):\r\n            num_flds = fields\r\n    else:\r\n        num_flds = [i for i in fields if a[i].dtype.kind in (\'i\', \'f\')]\r\n    return num_flds\r\n\r\n\r\ndef col_stats(a, fields=None, deci=2):\r\n    """"""Calculate statistics for a structured/recarray with or without specified\r\n    fields.  Efforts have been made to check for all possible scenarios, but\r\n    human intelligence should prevail when one decides what to throw at it.\r\n\r\n    >>> a.dtype.names  # to return a list of field names\r\n    >>> col_stats(a, fields=\'A\')  # run with field \'A\', must be integer/float\r\n    >>> col_stats(a, fields=[\'A\', \'B\'])  # both fields checked\r\n\r\n    Parameters:\r\n    -----------\r\n    a : array\r\n        A structured/recarray\r\n    fields : list, string or None\r\n      - None,  checks all fields or assumes that the input array is a singleton\r\n      - string, a single field name, if the column names are known\r\n      - list,  a list of field names\r\n    deci : integer\r\n        an attempt to format floats with deci(mal) places\r\n\r\n    Requires:\r\n    ---------\r\n    _numeric_fields_ : function\r\n        returns the numeric fields in a structured/recarray\r\n    _calc_stats : function\r\n        performs the actual field calculations\r\n    """"""\r\n    s_lst = []\r\n    num_flds = _numeric_fields_(a, fields)\r\n    # ---- made it thus far\r\n    if len(num_flds) == 0:\r\n        num_flds = [\'array\']\r\n        s_lst.append(_calc_stats(a.ravel(), axis=None, deci=deci))\r\n    else:\r\n        for fld in num_flds:\r\n            s_lst.append(_calc_stats(a[fld], deci=deci))\r\n    #\r\n    dts = [(\'Statistic\', \'U10\')] + [(i, \'<f8\') for i in num_flds]\r\n    col_names = np.array([\'N (size)\', \'n (nans)\', \'sum\', \'min\', \'max\', \'mean\',\r\n                          \'median\', \'std\', \'var\', \'skew\', \'kurt\'])\r\n    z = np.zeros((len(col_names),), dtype=dts)\r\n    z[\'Statistic\'] = col_names\r\n    for i in range(len(num_flds)):\r\n        fld = num_flds[i]\r\n        z[fld] = s_lst[i]\r\n    return z\r\n\r\n\r\ndef group_stats(a, case_fld=None, num_flds=None, deci=2):\r\n    """"""Group column statistics.\r\n\r\n    Parameters:\r\n    -----------\r\n    a : structured/recarray\r\n        Make sure that you know the field names in advance\r\n    case_fld : string, list\r\n        String,  summarized by the unique values in the case_fld.\r\n        List, to further fine-tune the selection or crosstabulation\r\n    num_flds : string, list\r\n        You can limit the input fields accordingly, if you only need a few\r\n        know numeric fields.\r\n\r\n    Requires:\r\n    ---------\r\n    col_stats : function ... which requires\r\n      : _numeric_fields_ : function\r\n          returns the numeric fields in a structured/recarray\r\n      : _calc_stats : function\r\n          performs the actual field calculations\r\n\r\n    """"""\r\n    results = []\r\n    uniq, counts = np.unique(a[case_fld], return_counts=True)\r\n    n = len(uniq)\r\n    for i in range(n):\r\n        if counts[i] >= 3:\r\n            u = uniq[i]\r\n            sub = a[a[case_fld] == u]\r\n            z = col_stats(sub, fields=num_flds, deci=deci)\r\n            prn(z, title=\'a[{}] stats...\'.format(u))\r\n            results.append(z)\r\n        else:\r\n            print(""\\nToo few cases... ({}) for a[{}]..."".format(counts[i], u))\r\n    return results\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n'"
all_scripts/tifffile.py,0,"b'##! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n# tifffile.py\n\n# Copyright (c) 2008-2018, Christoph Gohlke\n# Copyright (c) 2008-2018, The Regents of the University of California\n# Produced at the Laboratory for Fluorescence Dynamics\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n# * Redistributions of source code must retain the above copyright\n#   notice, this list of conditions and the following disclaimer.\n# * Redistributions in binary form must reproduce the above copyright\n#   notice, this list of conditions and the following disclaimer in the\n#   documentation and/or other materials provided with the distribution.\n# * Neither the name of the copyright holders nor the names of any\n#   contributors may be used to endorse or promote products derived\n#   from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n\n""""""Read image and meta data from (bio) TIFF\xc2\xae files. Save numpy arrays as TIFF.\n\nImage and metadata can be read from TIFF, BigTIFF, OME-TIFF, STK, LSM, NIH,\nSGI, ImageJ, MicroManager, FluoView, SEQ, and GEL files.\n\nTifffile is not a general-purpose TIFF library. Only a subset of the TIFF\nspecification is supported, mainly uncompressed and losslessly compressed\n2**(0 to 6) bit integer, 16, 32 and 64-bit float, grayscale and RGB(A) images,\nwhich are commonly used in bio-scientific imaging. Specifically, reading image\ntrees defined via SubIFDs, JPEG and CCITT compression, chroma subsampling,\nor IPTC and XMP metadata are not implemented.\n\nTIFF\xc2\xae, the tagged Image File Format, is a trademark and under control of\nAdobe Systems Incorporated. BigTIFF allows for files greater than 4 GB.\nSTK, LSM, FluoView, SGI, SEQ, GEL, and OME-TIFF, are custom extensions\ndefined by Molecular Devices (Universal Imaging Corporation), Carl Zeiss\nMicroImaging, Olympus, Silicon Graphics International, Media Cybernetics,\nMolecular Dynamics, and the Open Microscopy Environment consortium\nrespectively.\n\nFor command line usage run C{python -m tifffile --help}\n\n:Author:\n  `Christoph Gohlke <https://www.lfd.uci.edu/~gohlke/>`_\n\n:Organization:\n  Laboratory for Fluorescence Dynamics, University of California, Irvine\n\n:Version: 2018.02.16\n\nRequirements\n------------\n* `CPython 3.6 64-bit <https://www.python.org>`_\n* `Numpy 1.13 <http://www.numpy.org>`_\n* `Matplotlib 2.1 <https://www.matplotlib.org>`_ (optional for plotting)\n* `Tifffile.c 2018.02.10 <https://www.lfd.uci.edu/~gohlke/>`_\n  (recommended for faster decoding of PackBits and LZW encoded strings)\n* `Tifffile_geodb.py 2018.02.10 <https://www.lfd.uci.edu/~gohlke/>`_\n  (optional enums for GeoTIFF metadata)\n\nRevisions\n---------\n2018.02.16\n    Pass 2292 tests.\n    Do not use badly typed ImageDescription.\n    Coherce bad ASCII string tags to bytes.\n    Tuning of __str__ functions.\n    Fix reading \'undefined\' tag values (bug fix).\n    Read and write ZSTD compressed data.\n    Use hexdump to print byte strings.\n    Determine TIFF byte order from data dtype in imsave.\n    Add option to specify RowsPerStrip for compressed strips.\n    Allow memory map of arrays with non-native byte order.\n    Attempt to handle ScanImage <= 5.1 files.\n    Restore TiffPageSeries.pages sequence interface.\n    Use numpy.frombuffer instead of fromstring to read from binary data.\n    Parse GeoTIFF metadata.\n    Add option to apply horizontal differencing before compression.\n    Towards reading PerkinElmer QPTIFF (no test files).\n    Do not index out of bounds data in tifffile.c unpackbits and decodelzw.\n2017.09.29 (tentative)\n    Many backwards incompatible changes improving speed and resource usage:\n    Pass 2268 tests.\n    Add detail argument to __str__ function. Remove info functions.\n    Fix potential issue correcting offsets of large LSM files with positions.\n    Remove TiffFile sequence interface; use TiffFile.pages instead.\n    Do not make tag values available as TiffPage attributes.\n    Use str (not bytes) type for tag and metadata strings (WIP).\n    Use documented standard tag and value names (WIP).\n    Use enums for some documented TIFF tag values.\n    Remove \'memmap\' and \'tmpfile\' options; use out=\'memmap\' instead.\n    Add option to specify output in asarray functions.\n    Add option to concurrently decode image strips or tiles using threads.\n    Add TiffPage.asrgb function (WIP).\n    Do not apply colormap in asarray.\n    Remove \'colormapped\', \'rgbonly\', and \'scale_mdgel\' options from asarray.\n    Consolidate metadata in TiffFile _metadata functions.\n    Remove non-tag metadata properties from TiffPage.\n    Add function to convert LSM to tiled BIN files.\n    Align image data in file.\n    Make TiffPage.dtype a numpy.dtype.\n    Add \'ndim\' and \'size\' properties to TiffPage and TiffPageSeries.\n    Allow imsave to write non-BigTIFF files up to ~4 GB.\n    Only read one page for shaped series if possible.\n    Add memmap function to create memory-mapped array stored in TIFF file.\n    Add option to save empty arrays to TIFF files.\n    Add option to save truncated TIFF files.\n    Allow single tile images to be saved contiguously.\n    Add optional movie mode for files with uniform pages.\n    Lazy load pages.\n    Use lightweight TiffFrame for IFDs sharing properties with key TiffPage.\n    Move module constants to \'TIFF\' namespace (speed up module import).\n    Remove \'fastij\' option from TiffFile.\n    Remove \'pages\' parameter from TiffFile.\n    Remove TIFFfile alias.\n    Deprecate Python 2.\n    Require enum34 and futures packages on Python 2.7.\n    Remove Record class and return all metadata as dict instead.\n    Add functions to parse STK, MetaSeries, ScanImage, SVS, Pilatus metadata.\n    Read tags from EXIF and GPS IFDs.\n    Use pformat for tag and metadata values.\n    Fix reading some UIC tags (bug fix).\n    Do not modify input array in imshow (bug fix).\n    Fix Python implementation of unpack_ints.\n2017.05.23\n    Pass 1961 tests.\n    Write correct number of SampleFormat values (bug fix).\n    Use Adobe deflate code to write ZIP compressed files.\n    Add option to pass tag values as packed binary data for writing.\n    Defer tag validation to attribute access.\n    Use property instead of lazyattr decorator for simple expressions.\n2017.03.17\n    Write IFDs and tag values on word boundaries.\n    Read ScanImage metadata.\n    Remove is_rgb and is_indexed attributes from TiffFile.\n    Create files used by doctests.\n2017.01.12\n    Read Zeiss SEM metadata.\n    Read OME-TIFF with invalid references to external files.\n    Rewrite C LZW decoder (5x faster).\n    Read corrupted LSM files missing EOI code in LZW stream.\n2017.01.01\n    Add option to append images to existing TIFF files.\n    Read files without pages.\n    Read S-FEG and Helios NanoLab tags created by FEI software.\n    Allow saving Color Filter Array (CFA) images.\n    Add info functions returning more information about TiffFile and TiffPage.\n    Add option to read specific pages only.\n    Remove maxpages argument (backwards incompatible).\n    Remove test_tifffile function.\n2016.10.28\n    Pass 1944 tests.\n    Improve detection of ImageJ hyperstacks.\n    Read TVIPS metadata created by EM-MENU (by Marco Oster).\n    Add option to disable using OME-XML metadata.\n    Allow non-integer range attributes in modulo tags (by Stuart Berg).\n2016.06.21\n    Do not always memmap contiguous data in page series.\n2016.05.13\n    Add option to specify resolution unit.\n    Write grayscale images with extra samples when planarconfig is specified.\n    Do not write RGB color images with 2 samples.\n    Reorder TiffWriter.save keyword arguments (backwards incompatible).\n2016.04.18\n    Pass 1932 tests.\n    TiffWriter, imread, and imsave accept open binary file streams.\n2016.04.13\n    Correctly handle reversed fill order in 2 and 4 bps images (bug fix).\n    Implement reverse_bitorder in C.\n2016.03.18\n    Fix saving additional ImageJ metadata.\n2016.02.22\n    Pass 1920 tests.\n    Write 8 bytes double tag values using offset if necessary (bug fix).\n    Add option to disable writing second image description tag.\n    Detect tags with incorrect counts.\n    Disable color mapping for LSM.\n2015.11.13\n    Read LSM 6 mosaics.\n    Add option to specify directory of memory-mapped files.\n    Add command line options to specify vmin and vmax values for colormapping.\n2015.10.06\n    New helper function to apply colormaps.\n    Renamed is_palette attributes to is_indexed (backwards incompatible).\n    Color-mapped samples are now contiguous (backwards incompatible).\n    Do not color-map ImageJ hyperstacks (backwards incompatible).\n    Towards reading Leica SCN.\n2015.09.25\n    Read images with reversed bit order (FillOrder is LSB2MSB).\n2015.09.21\n    Read RGB OME-TIFF.\n    Warn about malformed OME-XML.\n2015.09.16\n    Detect some corrupted ImageJ metadata.\n    Better axes labels for \'shaped\' files.\n    Do not create TiffTag for default values.\n    Chroma subsampling is not supported.\n    Memory-map data in TiffPageSeries if possible (optional).\n2015.08.17\n    Pass 1906 tests.\n    Write ImageJ hyperstacks (optional).\n    Read and write LZMA compressed data.\n    Specify datetime when saving (optional).\n    Save tiled and color-mapped images (optional).\n    Ignore void bytecounts and offsets if possible.\n    Ignore bogus image_depth tag created by ISS Vista software.\n    Decode floating point horizontal differencing (not tiled).\n    Save image data contiguously if possible.\n    Only read first IFD from ImageJ files if possible.\n    Read ImageJ \'raw\' format (files larger than 4 GB).\n    TiffPageSeries class for pages with compatible shape and data type.\n    Try to read incomplete tiles.\n    Open file dialog if no filename is passed on command line.\n    Ignore errors when decoding OME-XML.\n    Rename decoder functions (backwards incompatible).\n2014.08.24\n    TiffWriter class for incremental writing images.\n    Simplify examples.\n2014.08.19\n    Add memmap function to FileHandle.\n    Add function to determine if image data in TiffPage is memory-mappable.\n    Do not close files if multifile_close parameter is False.\n2014.08.10\n    Pass 1730 tests.\n    Return all extrasamples by default (backwards incompatible).\n    Read data from series of pages into memory-mapped array (optional).\n    Squeeze OME dimensions (backwards incompatible).\n    Workaround missing EOI code in strips.\n    Support image and tile depth tags (SGI extension).\n    Better handling of STK/UIC tags (backwards incompatible).\n    Disable color mapping for STK.\n    Julian to datetime converter.\n    TIFF ASCII type may be NULL separated.\n    Unwrap strip offsets for LSM files greater than 4 GB.\n    Correct strip byte counts in compressed LSM files.\n    Skip missing files in OME series.\n    Read embedded TIFF files.\n2014.02.05\n    Save rational numbers as type 5 (bug fix).\n2013.12.20\n    Keep other files in OME multi-file series closed.\n    FileHandle class to abstract binary file handle.\n    Disable color mapping for bad OME-TIFF produced by bio-formats.\n    Read bad OME-XML produced by ImageJ when cropping.\n2013.11.03\n    Allow zlib compress data in imsave function (optional).\n    Memory-map contiguous image data (optional).\n2013.10.28\n    Read MicroManager metadata and little-endian ImageJ tag.\n    Save extra tags in imsave function.\n    Save tags in ascending order by code (bug fix).\n2012.10.18\n    Accept file like objects (read from OIB files).\n2012.08.21\n    Rename TIFFfile to TiffFile and TIFFpage to TiffPage.\n    TiffSequence class for reading sequence of TIFF files.\n    Read UltraQuant tags.\n    Allow float numbers as resolution in imsave function.\n2012.08.03\n    Read MD GEL tags and NIH Image header.\n2012.07.25\n    Read ImageJ tags.\n    ...\n\nNotes\n-----\nThe API is not stable yet and might change between revisions.\n\nTested on little-endian platforms only.\n\nOther Python packages and modules for reading bio-scientific TIFF files:\n\n*  `python-bioformats <https://github.com/CellProfiler/python-bioformats>`_\n*  `Imread <https://github.com/luispedro/imread>`_\n*  `PyLibTiff <https://github.com/pearu/pylibtiff>`_\n*  `SimpleITK <http://www.simpleitk.org>`_\n*  `PyLSM <https://launchpad.net/pylsm>`_\n*  `PyMca.TiffIO.py <https://github.com/vasole/pymca>`_ (same as fabio.TiffIO)\n*  `BioImageXD.Readers <http://www.bioimagexd.net/>`_\n*  `Cellcognition.io <http://cellcognition.org/>`_\n*  `pymimage <https://github.com/ardoi/pymimage>`_\n*  `pytiff <https://github.com/FZJ-INM1-BDA/pytiff>`_\n\nAcknowledgements\n----------------\n*   Egor Zindy, University of Manchester, for lsm_scan_info specifics.\n*   Wim Lewis for a bug fix and some LSM functions.\n*   Hadrien Mary for help on reading MicroManager files.\n*   Christian Kliche for help writing tiled and color-mapped files.\n\nReferences\n----------\n1)  TIFF 6.0 Specification and Supplements. Adobe Systems Incorporated.\n    http://partners.adobe.com/public/developer/tiff/\n2)  TIFF File Format FAQ. http://www.awaresystems.be/imaging/tiff/faq.html\n3)  MetaMorph Stack (STK) Image File Format.\n    http://support.meta.moleculardevices.com/docs/t10243.pdf\n4)  Image File Format Description LSM 5/7 Release 6.0 (ZEN 2010).\n    Carl Zeiss MicroImaging GmbH. BioSciences. May 10, 2011\n5)  The OME-TIFF format.\n    http://www.openmicroscopy.org/site/support/file-formats/ome-tiff\n6)  UltraQuant(r) Version 6.0 for Windows Start-Up Guide.\n    http://www.ultralum.com/images%20ultralum/pdf/UQStart%20Up%20Guide.pdf\n7)  Micro-Manager File Formats.\n    http://www.micro-manager.org/wiki/Micro-Manager_File_Formats\n8)  Tags for TIFF and Related Specifications. Digital Preservation.\n    http://www.digitalpreservation.gov/formats/content/tiff_tags.shtml\n9)  ScanImage BigTiff Specification - ScanImage 2016.\n    http://scanimage.vidriotechnologies.com/display/SI2016/\n    ScanImage+BigTiff+Specification\n10) CIPA DC-008-2016: Exchangeable image file format for digital still cameras:\n    Exif Version 2.31.\n    http://www.cipa.jp/std/documents/e/DC-008-Translation-2016-E.pdf\n\nExamples\n--------\n>>> # write and read numpy array\n>>> data = numpy.random.rand(5, 301, 219)\n>>> imsave(\'temp.tif\', data)\n>>> image = imread(\'temp.tif\')\n>>> numpy.testing.assert_array_equal(image, data)\n\n>>> # iterate over pages and tags\n>>> with TiffFile(\'temp.tif\') as tif:\n...     images = tif.asarray()\n...     for page in tif.pages:\n...         for tag in page.tags.values():\n...             _ = tag.name, tag.value\n...         image = page.asarray()\n\n""""""\n\nfrom __future__ import division, print_function\n\nimport sys\nimport os\nimport io\nimport re\nimport glob\nimport math\nimport zlib\nimport time\nimport json\nimport enum\nimport struct\nimport warnings\nimport binascii\nimport tempfile\nimport datetime\nimport threading\nimport collections\nimport multiprocessing\nimport concurrent.futures\n\nimport numpy\n\n# delay imports: mmap, pprint, fractions, xml, tkinter, matplotlib, lzma, zstd\n\n__version__ = \'2018.02.16\'\n__docformat__ = \'restructuredtext en\'\n__all__ = (\n    \'imsave\', \'imread\', \'imshow\', \'memmap\',\n    \'TiffFile\', \'TiffWriter\', \'TiffSequence\',\n    # utility functions used by oiffile or czifile\n    \'FileHandle\', \'lazyattr\', \'natural_sorted\', \'decode_lzw\', \'stripnull\',\n    \'create_output\', \'repeat_nd\', \'format_size\', \'product\', \'xml2dict\')\n\n\ndef imread(files, **kwargs):\n    """"""Return image data from TIFF file(s) as numpy array.\n\n    Refer to the TiffFile class and member functions for documentation.\n\n    Parameters\n    ----------\n    files : str, binary stream, or sequence\n        File name, seekable binary stream, glob pattern, or sequence of\n        file names.\n    kwargs : dict\n        Parameters \'multifile\' and \'is_ome\' are passed to the TiffFile class.\n        The \'pattern\' parameter is passed to the TiffSequence class.\n        Other parameters are passed to the asarray functions.\n        The first image series is returned if no arguments are provided.\n\n    Examples\n    --------\n    >>> # get image from first page\n    >>> imsave(\'temp.tif\', numpy.random.rand(3, 4, 301, 219))\n    >>> im = imread(\'temp.tif\', key=0)\n    >>> im.shape\n    (4, 301, 219)\n\n    >>> # get images from sequence of files\n    >>> ims = imread([\'temp.tif\', \'temp.tif\'])\n    >>> ims.shape\n    (2, 3, 4, 301, 219)\n\n    """"""\n    kwargs_file = parse_kwargs(kwargs, \'multifile\', \'is_ome\')\n    kwargs_seq = parse_kwargs(kwargs, \'pattern\')\n\n    if isinstance(files, basestring) and any(i in files for i in \'?*\'):\n        files = glob.glob(files)\n    if not files:\n        raise ValueError(\'no files found\')\n    if not hasattr(files, \'seek\') and len(files) == 1:\n        files = files[0]\n\n    if isinstance(files, basestring) or hasattr(files, \'seek\'):\n        with TiffFile(files, **kwargs_file) as tif:\n            return tif.asarray(**kwargs)\n    else:\n        with TiffSequence(files, **kwargs_seq) as imseq:\n            return imseq.asarray(**kwargs)\n\n\ndef imsave(file, data=None, shape=None, dtype=None, bigsize=2**32-2**25,\n           **kwargs):\n    """"""Write numpy array to TIFF file.\n\n    Refer to the TiffWriter class and member functions for documentation.\n\n    Parameters\n    ----------\n    file : str or binary stream\n        File name or writable binary stream, such as an open file or BytesIO.\n    data : array_like\n        Input image. The last dimensions are assumed to be image depth,\n        height, width, and samples.\n        If None, an empty array of the specified shape and dtype is\n        saved to file.\n        Unless \'byteorder\' is specified in \'kwargs\', the TIFF file byte order\n        is determined from the data\'s dtype or the dtype argument.\n    shape : tuple\n        If \'data\' is None, shape of an empty array to save to the file.\n    dtype : numpy.dtype\n        If \'data\' is None, data-type of an empty array to save to the file.\n    bigsize : int\n        Create a BigTIFF file if the size of data in bytes is larger than\n        this threshold and \'imagej\' or \'truncate\' are not enabled.\n        By default, the threshold is 4 GB minus 32 MB reserved for metadata.\n        Use the \'bigtiff\' parameter to explicitly specify the type of\n        file created.\n    kwargs : dict\n        Parameters \'append\', \'byteorder\', \'bigtiff\', \'software\', and \'imagej\',\n        are passed to TiffWriter().\n        Other parameters are passed to TiffWriter.save().\n\n    Returns\n    -------\n    If the image data are written contiguously, return offset and bytecount\n    of image data in the file.\n\n    Examples\n    --------\n    >>> # save a RGB image\n    >>> data = numpy.random.randint(0, 255, (256, 256, 3), \'uint8\')\n    >>> imsave(\'temp.tif\', data, photometric=\'rgb\')\n\n    >>> # save a random array and metadata, using compression\n    >>> data = numpy.random.rand(2, 5, 3, 301, 219)\n    >>> imsave(\'temp.tif\', data, compress=6, metadata={\'axes\': \'TZCYX\'})\n\n    """"""\n    tifargs = parse_kwargs(kwargs, \'append\', \'bigtiff\', \'byteorder\',\n                           \'software\', \'imagej\')\n    if data is None:\n        size = product(shape) * numpy.dtype(dtype).itemsize\n        byteorder = numpy.dtype(dtype).byteorder\n    else:\n        try:\n            size = data.nbytes\n            byteorder = data.dtype.byteorder\n        except Exception:\n            size = 0\n            byteorder = None\n    if size > bigsize and \'bigtiff\' not in tifargs and not (\n            tifargs.get(\'imagej\', False) or tifargs.get(\'truncate\', False)):\n        tifargs[\'bigtiff\'] = True\n    if \'byteorder\' not in tifargs:\n        tifargs[\'byteorder\'] = byteorder\n\n    with TiffWriter(file, **tifargs) as tif:\n        return tif.save(data, shape, dtype, **kwargs)\n\n\ndef memmap(filename, shape=None, dtype=None, page=None, series=0, mode=\'r+\',\n           **kwargs):\n    """"""Return memory-mapped numpy array stored in TIFF file.\n\n    Memory-mapping requires data stored in native byte order, without tiling,\n    compression, predictors, etc.\n    If \'shape\' and \'dtype\' are provided, existing files will be overwritten or\n    appended to depending on the \'append\' parameter.\n    Otherwise the image data of a specified page or series in an existing\n    file will be memory-mapped. By default, the image data of the first page\n    series is memory-mapped.\n    Call flush() to write any changes in the array to the file.\n    Raise ValueError if the image data in the file is not memory-mappable.\n\n    Parameters\n    ----------\n    filename : str\n        Name of the TIFF file which stores the array.\n    shape : tuple\n        Shape of the empty array.\n    dtype : numpy.dtype\n        Data-type of the empty array.\n    page : int\n        Index of the page which image data to memory-map.\n    series : int\n        Index of the page series which image data to memory-map.\n    mode : {\'r+\', \'r\', \'c\'}, optional\n        The file open mode. Default is to open existing file for reading and\n        writing (\'r+\').\n    kwargs : dict\n        Additional parameters passed to imsave() or TiffFile().\n\n    Examples\n    --------\n    >>> # create an empty TIFF file and write to memory-mapped image\n    >>> im = memmap(\'temp.tif\', shape=(256, 256), dtype=\'float32\')\n    >>> im[255, 255] = 1.0\n    >>> im.flush()\n    >>> im.shape, im.dtype\n    ((256, 256), dtype(\'float32\'))\n    >>> del im\n\n    >>> # memory-map image data in a TIFF file\n    >>> im = memmap(\'temp.tif\', page=0)\n    >>> im[255, 255]\n    1.0\n\n    """"""\n    if shape is not None and dtype is not None:\n        # create a new, empty array\n        kwargs.update(data=None, shape=shape, dtype=dtype, returnoffset=True,\n                      align=TIFF.ALLOCATIONGRANULARITY)\n        result = imsave(filename, **kwargs)\n        if result is None:\n            # TODO: fail before creating file or writing data\n            raise ValueError(\'image data are not memory-mappable\')\n        offset = result[0]\n    else:\n        # use existing file\n        with TiffFile(filename, **kwargs) as tif:\n            if page is not None:\n                page = tif.pages[page]\n                if not page.is_memmappable:\n                    raise ValueError(\'image data are not memory-mappable\')\n                offset, _ = page.is_contiguous\n                shape = page.shape\n                dtype = page.dtype\n            else:\n                series = tif.series[series]\n                if series.offset is None:\n                    raise ValueError(\'image data are not memory-mappable\')\n                shape = series.shape\n                dtype = series.dtype\n                offset = series.offset\n            dtype = tif.byteorder + dtype.char\n    return numpy.memmap(filename, dtype, mode, offset, shape, \'C\')\n\n\nclass lazyattr(object):\n    """"""Attribute whose value is computed on first access.""""""\n    # TODO: help() doesn\'t work\n    __slots__ = (\'func\',)\n\n    def __init__(self, func):\n        self.func = func\n        # self.__name__ = func.__name__\n        # self.__doc__ = func.__doc__\n        # self.lock = threading.RLock()\n\n    def __get__(self, instance, owner):\n        # with self.lock:\n        if instance is None:\n            return self\n        try:\n            value = self.func(instance)\n        except AttributeError as e:\n            raise RuntimeError(e)\n        if value is NotImplemented:\n            return getattr(super(owner, instance), self.func.__name__)\n        setattr(instance, self.func.__name__, value)\n        return value\n\n\nclass TiffWriter(object):\n    """"""Write numpy arrays to TIFF file.\n\n    TiffWriter instances must be closed using the \'close\' method, which is\n    automatically called when using the \'with\' context manager.\n\n    TiffWriter\'s main purpose is saving nD numpy array\'s as TIFF,\n    not to create any possible TIFF format. Specifically, JPEG compression,\n    SubIFDs, ExifIFD, or GPSIFD tags are not supported.\n\n    Examples\n    --------\n    >>> # successively append images to BigTIFF file\n    >>> data = numpy.random.rand(2, 5, 3, 301, 219)\n    >>> with TiffWriter(\'temp.tif\', bigtiff=True) as tif:\n    ...     for i in range(data.shape[0]):\n    ...         tif.save(data[i], compress=6)\n\n    """"""\n    def __init__(self, file, bigtiff=False, byteorder=None,\n                 software=\'tifffile.py\', append=False, imagej=False):\n        """"""Open a TIFF file for writing.\n\n        An empty TIFF file is created if the file does not exist, else the\n        file is overwritten with an empty TIFF file unless \'append\'\n        is true. Use bigtiff=True when creating files larger than 4 GB.\n\n        Parameters\n        ----------\n        file : str, binary stream, or FileHandle\n            File name or writable binary stream, such as an open file\n            or BytesIO.\n        bigtiff : bool\n            If True, the BigTIFF format is used.\n        byteorder : {\'<\', \'>\', \'=\', \'|\'}\n            The endianness of the data in the file.\n            By default, this is the system\'s native byte order.\n        software : str\n            Name of the software used to create the file.\n            Saved with the first page in the file only.\n            Must be 7-bit ASCII.\n        append : bool\n            If True and \'file\' is an existing standard TIFF file, image data\n            and tags are appended to the file.\n            Appending data may corrupt specifically formatted TIFF files\n            such as LSM, STK, ImageJ, NIH, or FluoView.\n        imagej : bool\n            If True, write an ImageJ hyperstack compatible file.\n            This format can handle data types uint8, uint16, or float32 and\n            data shapes up to 6 dimensions in TZCYXS order.\n            RGB images (S=3 or S=4) must be uint8.\n            ImageJ\'s default byte order is big-endian but this implementation\n            uses the system\'s native byte order by default.\n            ImageJ does not support BigTIFF format or LZMA compression.\n            The ImageJ file format is undocumented.\n\n        """"""\n        if append:\n            # determine if file is an existing TIFF file that can be extended\n            try:\n                with FileHandle(file, mode=\'rb\', size=0) as fh:\n                    pos = fh.tell()\n                    try:\n                        with TiffFile(fh) as tif:\n                            if (append != \'force\' and\n                                    any(getattr(tif, \'is_\'+a) for a in (\n                                        \'lsm\', \'stk\', \'imagej\', \'nih\',\n                                        \'fluoview\', \'micromanager\'))):\n                                raise ValueError(\'file contains metadata\')\n                            byteorder = tif.byteorder\n                            bigtiff = tif.is_bigtiff\n                            self._ifdoffset = tif.pages.next_page_offset\n                            if tif.pages:\n                                software = None\n                    except Exception as e:\n                        raise ValueError(\'cannot append to file: %s\' % str(e))\n                    finally:\n                        fh.seek(pos)\n            except (IOError, FileNotFoundError):\n                append = False\n\n        if byteorder in (None, \'=\', \'|\'):\n            byteorder = \'<\' if sys.byteorder == \'little\' else \'>\'\n        elif byteorder not in (\'<\', \'>\'):\n            raise ValueError(\'invalid byteorder %s\' % byteorder)\n        if imagej and bigtiff:\n            warnings.warn(\'writing incompatible BigTIFF ImageJ\')\n\n        self._byteorder = byteorder\n        self._software = software\n        self._imagej = bool(imagej)\n        self._truncate = False\n        self._metadata = None\n        self._colormap = None\n\n        self._descriptionoffset = 0\n        self._descriptionlen = 0\n        self._descriptionlenoffset = 0\n        self._tags = None\n        self._shape = None  # normalized shape of data in consecutive pages\n        self._datashape = None  # shape of data in consecutive pages\n        self._datadtype = None  # data type\n        self._dataoffset = None  # offset to data\n        self._databytecounts = None  # byte counts per plane\n        self._tagoffsets = None  # strip or tile offset tag code\n\n        if bigtiff:\n            self._bigtiff = True\n            self._offsetsize = 8\n            self._tagsize = 20\n            self._tagnoformat = \'Q\'\n            self._offsetformat = \'Q\'\n            self._valueformat = \'8s\'\n        else:\n            self._bigtiff = False\n            self._offsetsize = 4\n            self._tagsize = 12\n            self._tagnoformat = \'H\'\n            self._offsetformat = \'I\'\n            self._valueformat = \'4s\'\n\n        if append:\n            self._fh = FileHandle(file, mode=\'r+b\', size=0)\n            self._fh.seek(0, 2)\n        else:\n            self._fh = FileHandle(file, mode=\'wb\', size=0)\n            self._fh.write({\'<\': b\'II\', \'>\': b\'MM\'}[byteorder])\n            if bigtiff:\n                self._fh.write(struct.pack(byteorder+\'HHH\', 43, 8, 0))\n            else:\n                self._fh.write(struct.pack(byteorder+\'H\', 42))\n            # first IFD\n            self._ifdoffset = self._fh.tell()\n            self._fh.write(struct.pack(byteorder+self._offsetformat, 0))\n\n    def save(self, data=None, shape=None, dtype=None, returnoffset=False,\n             photometric=None, planarconfig=None, tile=None, contiguous=True,\n             align=16, truncate=False, compress=0, rowsperstrip=None,\n             predictor=False, colormap=None, description=None,\n             datetime=None, resolution=None, metadata={}, extratags=()):\n        """"""Write numpy array and tags to TIFF file.\n\n        The data shape\'s last dimensions are assumed to be image depth,\n        height (length), width, and samples.\n        If a colormap is provided, the data\'s dtype must be uint8 or uint16\n        and the data values are indices into the last dimension of the\n        colormap.\n        If \'shape\' and \'dtype\' are specified, an empty array is saved.\n        This option cannot be used with compression or multiple tiles.\n        Image data are written uncompressed in one strip per plane by default.\n        Dimensions larger than 2 to 4 (depending on photometric mode, planar\n        configuration, and SGI mode) are flattened and saved as separate pages.\n        The SampleFormat and BitsPerSample tags are derived from the data type.\n\n        Parameters\n        ----------\n        data : numpy.ndarray or None\n            Input image array.\n        shape : tuple or None\n            Shape of the empty array to save. Used only if \'data\' is None.\n        dtype : numpy.dtype or None\n            Data-type of the empty array to save. Used only if \'data\' is None.\n        returnoffset : bool\n            If True and the image data in the file is memory-mappable, return\n            the offset and number of bytes of the image data in the file.\n        photometric : {\'MINISBLACK\', \'MINISWHITE\', \'RGB\', \'PALETTE\', \'CFA\'}\n            The color space of the image data.\n            By default, this setting is inferred from the data shape and the\n            value of colormap.\n            For CFA images, DNG tags must be specified in \'extratags\'.\n        planarconfig : {\'CONTIG\', \'SEPARATE\'}\n            Specifies if samples are stored contiguous or in separate planes.\n            By default, this setting is inferred from the data shape.\n            If this parameter is set, extra samples are used to store grayscale\n            images.\n            \'CONTIG\': last dimension contains samples.\n            \'SEPARATE\': third last dimension contains samples.\n        tile : tuple of int\n            The shape (depth, length, width) of image tiles to write.\n            If None (default), image data are written in strips.\n            The tile length and width must be a multiple of 16.\n            If the tile depth is provided, the SGI ImageDepth and TileDepth\n            tags are used to save volume data.\n            Unless a single tile is used, tiles cannot be used to write\n            contiguous files.\n            Few software can read the SGI format, e.g. MeVisLab.\n        contiguous : bool\n            If True (default) and the data and parameters are compatible with\n            previous ones, if any, the image data are stored contiguously after\n            the previous one. Parameters \'photometric\' and \'planarconfig\'\n            are ignored. Parameters \'description\', datetime\', and \'extratags\'\n            are written to the first page of a contiguous series only.\n        align : int\n            Byte boundary on which to align the image data in the file.\n            Default 16. Use mmap.ALLOCATIONGRANULARITY for memory-mapped data.\n            Following contiguous writes are not aligned.\n        truncate : bool\n            If True, only write the first page including shape metadata if\n            possible (uncompressed, contiguous, not tiled).\n            Other TIFF readers will only be able to read part of the data.\n        compress : int or \'LZMA\', \'ZSTD\'\n            Values from 0 to 9 controlling the level of zlib compression.\n            If 0 (default), data are written uncompressed.\n            Compression cannot be used to write contiguous files.\n            If \'LZMA\' or \'ZSTD\', LZMA or ZSTD compression is used, which is\n            not available on all platforms.\n        rowsperstrip : int\n            The number of rows per strip used for compression.\n            Uncompressed data are written in one strip per plane.\n        predictor : bool\n            If True, apply horizontal differencing to integer type images\n            before compression.\n        colormap : numpy.ndarray\n            RGB color values for the corresponding data value.\n            Must be of shape (3, 2**(data.itemsize*8)) and dtype uint16.\n        description : str\n            The subject of the image. Must be 7-bit ASCII. Cannot be used with\n            the ImageJ format. Saved with the first page only.\n        datetime : datetime\n            Date and time of image creation. If None (default), the current\n            date and time is used. Saved with the first page only.\n        resolution : (float, float[, str]) or ((int, int), (int, int)[, str])\n            X and Y resolutions in pixels per resolution unit as float or\n            rational numbers. A third, optional parameter specifies the\n            resolution unit, which must be None (default for ImageJ),\n            \'INCH\' (default), or \'CENTIMETER\'.\n        metadata : dict\n            Additional meta data to be saved along with shape information\n            in JSON or ImageJ formats in an ImageDescription tag.\n            If None, do not write a second ImageDescription tag.\n            Strings must be 7-bit ASCII. Saved with the first page only.\n        extratags : sequence of tuples\n            Additional tags as [(code, dtype, count, value, writeonce)].\n\n            code : int\n                The TIFF tag Id.\n            dtype : str\n                Data type of items in \'value\' in Python struct format.\n                One of B, s, H, I, 2I, b, h, i, 2i, f, d, Q, or q.\n            count : int\n                Number of data values. Not used for string or byte string\n                values.\n            value : sequence\n                \'Count\' values compatible with \'dtype\'.\n                Byte strings must contain count values of dtype packed as\n                binary data.\n            writeonce : bool\n                If True, the tag is written to the first page only.\n\n        """"""\n        # TODO: refactor this function\n        fh = self._fh\n        byteorder = self._byteorder\n\n        if data is None:\n            if compress:\n                raise ValueError(\'cannot save compressed empty file\')\n            datashape = shape\n            datadtype = numpy.dtype(dtype).newbyteorder(byteorder)\n            datadtypechar = datadtype.char\n            data = None\n        else:\n            data = numpy.asarray(data, byteorder+data.dtype.char, \'C\')\n            if data.size == 0:\n                raise ValueError(\'cannot save empty array\')\n            datashape = data.shape\n            datadtype = data.dtype\n            datadtypechar = data.dtype.char\n\n        returnoffset = returnoffset and datadtype.isnative\n        datasize = product(datashape) * datadtype.itemsize\n\n        # just append contiguous data if possible\n        self._truncate = bool(truncate)\n        if self._datashape:\n            if (not contiguous\n                    or self._datashape[1:] != datashape\n                    or self._datadtype != datadtype\n                    or (compress and self._tags)\n                    or tile\n                    or not numpy.array_equal(colormap, self._colormap)):\n                # incompatible shape, dtype, compression mode, or colormap\n                self._write_remaining_pages()\n                self._write_image_description()\n                self._truncate = False\n                self._descriptionoffset = 0\n                self._descriptionlenoffset = 0\n                self._datashape = None\n                self._colormap = None\n                if self._imagej:\n                    raise ValueError(\n                        \'ImageJ does not support non-contiguous data\')\n            else:\n                # consecutive mode\n                self._datashape = (self._datashape[0] + 1,) + datashape\n                if not compress:\n                    # write contiguous data, write IFDs/tags later\n                    offset = fh.tell()\n                    if data is None:\n                        fh.write_empty(datasize)\n                    else:\n                        fh.write_array(data)\n                    if returnoffset:\n                        return offset, datasize\n                    return\n\n        input_shape = datashape\n        tagnoformat = self._tagnoformat\n        valueformat = self._valueformat\n        offsetformat = self._offsetformat\n        offsetsize = self._offsetsize\n        tagsize = self._tagsize\n\n        MINISBLACK = TIFF.PHOTOMETRIC.MINISBLACK\n        RGB = TIFF.PHOTOMETRIC.RGB\n        CFA = TIFF.PHOTOMETRIC.CFA\n        PALETTE = TIFF.PHOTOMETRIC.PALETTE\n        CONTIG = TIFF.PLANARCONFIG.CONTIG\n        SEPARATE = TIFF.PLANARCONFIG.SEPARATE\n\n        # parse input\n        if photometric is not None:\n            photometric = enumarg(TIFF.PHOTOMETRIC, photometric)\n        if planarconfig:\n            planarconfig = enumarg(TIFF.PLANARCONFIG, planarconfig)\n        if not compress:\n            compress = False\n            compresstag = 1\n            predictor = False\n        else:\n            if isinstance(compress, (tuple, list)):\n                compress, compresslevel = compress\n            elif isinstance(compress, int):\n                compress, compresslevel = \'ADOBE_DEFLATE\', int(compress)\n                if not 0 <= compresslevel <= 9:\n                    raise ValueError(\'invalid compression level %s\' % compress)\n            else:\n                compresslevel = None\n            compress = compress.upper()\n            compresstag = enumarg(TIFF.COMPRESSION, compress)\n\n        # prepare ImageJ format\n        if self._imagej:\n            if compress in (\'LZMA\', \'ZSTD\'):\n                raise ValueError(\n                    \'ImageJ cannot handle LZMA or ZSTD compression\')\n            if description:\n                warnings.warn(\'not writing description to ImageJ file\')\n                description = None\n            volume = False\n            if datadtypechar not in \'BHhf\':\n                raise ValueError(\n                    \'ImageJ does not support data type %s\' % datadtypechar)\n            ijrgb = photometric == RGB if photometric else None\n            if datadtypechar not in \'B\':\n                ijrgb = False\n            ijshape = imagej_shape(datashape, ijrgb)\n            if ijshape[-1] in (3, 4):\n                photometric = RGB\n                if datadtypechar not in \'B\':\n                    raise ValueError(\'ImageJ does not support data type %s \'\n                                     \'for RGB\' % datadtypechar)\n            elif photometric is None:\n                photometric = MINISBLACK\n                planarconfig = None\n            if planarconfig == SEPARATE:\n                raise ValueError(\'ImageJ does not support planar images\')\n            else:\n                planarconfig = CONTIG if ijrgb else None\n\n        # define compress function\n        if compress:\n            if compresslevel is None:\n                compressor, compresslevel = TIFF.COMPESSORS[compresstag]\n            else:\n                compressor, _ = TIFF.COMPESSORS[compresstag]\n                compresslevel = int(compresslevel)\n            if predictor:\n                if datadtype.kind not in \'iu\':\n                    raise ValueError(\n                        \'prediction not implemented for %s\' % datadtype)\n\n                def compress(data, level=compresslevel):\n                    # horizontal differencing\n                    diff = numpy.diff(data, axis=-2)\n                    data = numpy.insert(diff, 0, data[..., 0, :], axis=-2)\n                    return compressor(data, level)\n            else:\n                def compress(data, level=compresslevel):\n                    return compressor(data, level)\n\n        # verify colormap and indices\n        if colormap is not None:\n            if datadtypechar not in \'BH\':\n                raise ValueError(\'invalid data dtype for palette mode\')\n            colormap = numpy.asarray(colormap, dtype=byteorder+\'H\')\n            if colormap.shape != (3, 2**(datadtype.itemsize * 8)):\n                raise ValueError(\'invalid color map shape\')\n            self._colormap = colormap\n\n        # verify tile shape\n        if tile:\n            tile = tuple(int(i) for i in tile[:3])\n            volume = len(tile) == 3\n            if (len(tile) < 2 or tile[-1] % 16 or tile[-2] % 16 or\n                    any(i < 1 for i in tile)):\n                raise ValueError(\'invalid tile shape\')\n        else:\n            tile = ()\n            volume = False\n\n        # normalize data shape to 5D or 6D, depending on volume:\n        #   (pages, planar_samples, [depth,] height, width, contig_samples)\n        datashape = reshape_nd(datashape, 3 if photometric == RGB else 2)\n        shape = datashape\n        ndim = len(datashape)\n\n        samplesperpixel = 1\n        extrasamples = 0\n        if volume and ndim < 3:\n            volume = False\n        if colormap is not None:\n            photometric = PALETTE\n            planarconfig = None\n        if photometric is None:\n            photometric = MINISBLACK\n            if planarconfig == CONTIG:\n                if ndim > 2 and shape[-1] in (3, 4):\n                    photometric = RGB\n            elif planarconfig == SEPARATE:\n                if volume and ndim > 3 and shape[-4] in (3, 4):\n                    photometric = RGB\n                elif ndim > 2 and shape[-3] in (3, 4):\n                    photometric = RGB\n            elif ndim > 2 and shape[-1] in (3, 4):\n                photometric = RGB\n            elif self._imagej:\n                photometric = MINISBLACK\n            elif volume and ndim > 3 and shape[-4] in (3, 4):\n                photometric = RGB\n            elif ndim > 2 and shape[-3] in (3, 4):\n                photometric = RGB\n        if planarconfig and len(shape) <= (3 if volume else 2):\n            planarconfig = None\n            photometric = MINISBLACK\n        if photometric == RGB:\n            if len(shape) < 3:\n                raise ValueError(\'not a RGB(A) image\')\n            if len(shape) < 4:\n                volume = False\n            if planarconfig is None:\n                if shape[-1] in (3, 4):\n                    planarconfig = CONTIG\n                elif shape[-4 if volume else -3] in (3, 4):\n                    planarconfig = SEPARATE\n                elif shape[-1] > shape[-4 if volume else -3]:\n                    planarconfig = SEPARATE\n                else:\n                    planarconfig = CONTIG\n            if planarconfig == CONTIG:\n                datashape = (-1, 1) + shape[(-4 if volume else -3):]\n                samplesperpixel = datashape[-1]\n            else:\n                datashape = (-1,) + shape[(-4 if volume else -3):] + (1,)\n                samplesperpixel = datashape[1]\n            if samplesperpixel > 3:\n                extrasamples = samplesperpixel - 3\n        elif photometric == CFA:\n            if len(shape) != 2:\n                raise ValueError(\'invalid CFA image\')\n            volume = False\n            planarconfig = None\n            datashape = (-1, 1) + shape[-2:] + (1,)\n            if 50706 not in (et[0] for et in extratags):\n                raise ValueError(\'must specify DNG tags for CFA image\')\n        elif planarconfig and len(shape) > (3 if volume else 2):\n            if planarconfig == CONTIG:\n                datashape = (-1, 1) + shape[(-4 if volume else -3):]\n                samplesperpixel = datashape[-1]\n            else:\n                datashape = (-1,) + shape[(-4 if volume else -3):] + (1,)\n                samplesperpixel = datashape[1]\n            extrasamples = samplesperpixel - 1\n        else:\n            planarconfig = None\n            # remove trailing 1s\n            while len(shape) > 2 and shape[-1] == 1:\n                shape = shape[:-1]\n            if len(shape) < 3:\n                volume = False\n            datashape = (-1, 1) + shape[(-3 if volume else -2):] + (1,)\n\n        # normalize shape to 6D\n        assert len(datashape) in (5, 6)\n        if len(datashape) == 5:\n            datashape = datashape[:2] + (1,) + datashape[2:]\n        if datashape[0] == -1:\n            s0 = product(input_shape) // product(datashape[1:])\n            datashape = (s0,) + datashape[1:]\n        shape = datashape\n        if data is not None:\n            data = data.reshape(shape)\n\n        if tile and not volume:\n            tile = (1, tile[-2], tile[-1])\n\n        if photometric == PALETTE:\n            if (samplesperpixel != 1 or extrasamples or\n                    shape[1] != 1 or shape[-1] != 1):\n                raise ValueError(\'invalid data shape for palette mode\')\n\n        if photometric == RGB and samplesperpixel == 2:\n            raise ValueError(\'not a RGB image (samplesperpixel=2)\')\n\n        bytestr = bytes if sys.version[0] == \'2\' else (\n            lambda x: bytes(x, \'ascii\') if isinstance(x, str) else x)\n        tags = []  # list of (code, ifdentry, ifdvalue, writeonce)\n\n        strip_or_tile = \'Tile\' if tile else \'Strip\'\n        tagbytecounts = TIFF.TAG_NAMES[strip_or_tile + \'ByteCounts\']\n        tag_offsets = TIFF.TAG_NAMES[strip_or_tile + \'Offsets\']\n        self._tagoffsets = tag_offsets\n\n        def pack(fmt, *val):\n            return struct.pack(byteorder+fmt, *val)\n\n        def addtag(code, dtype, count, value, writeonce=False):\n            # Compute ifdentry & ifdvalue bytes from code, dtype, count, value\n            # Append (code, ifdentry, ifdvalue, writeonce) to tags list\n            code = int(TIFF.TAG_NAMES.get(code, code))\n            try:\n                tifftype = TIFF.DATA_DTYPES[dtype]\n            except KeyError:\n                raise ValueError(\'unknown dtype %s\' % dtype)\n            rawcount = count\n\n            if dtype == \'s\':\n                # strings\n                value = bytestr(value) + b\'\\0\'\n                count = rawcount = len(value)\n                rawcount = value.find(b\'\\0\\0\')\n                if rawcount < 0:\n                    rawcount = count\n                else:\n                    rawcount += 1  # length of string without buffer\n                value = (value,)\n            elif isinstance(value, bytes):\n                # packed binary data\n                dtsize = struct.calcsize(dtype)\n                if len(value) % dtsize:\n                    raise ValueError(\'invalid packed binary data\')\n                count = len(value) // dtsize\n            if len(dtype) > 1:\n                count *= int(dtype[:-1])\n                dtype = dtype[-1]\n            ifdentry = [pack(\'HH\', code, tifftype),\n                        pack(offsetformat, rawcount)]\n            ifdvalue = None\n            if struct.calcsize(dtype) * count <= offsetsize:\n                # value(s) can be written directly\n                if isinstance(value, bytes):\n                    ifdentry.append(pack(valueformat, value))\n                elif count == 1:\n                    if isinstance(value, (tuple, list, numpy.ndarray)):\n                        value = value[0]\n                    ifdentry.append(pack(valueformat, pack(dtype, value)))\n                else:\n                    ifdentry.append(pack(valueformat,\n                                         pack(str(count)+dtype, *value)))\n            else:\n                # use offset to value(s)\n                ifdentry.append(pack(offsetformat, 0))\n                if isinstance(value, bytes):\n                    ifdvalue = value\n                elif isinstance(value, numpy.ndarray):\n                    assert value.size == count\n                    assert value.dtype.char == dtype\n                    ifdvalue = value.tostring()\n                elif isinstance(value, (tuple, list)):\n                    ifdvalue = pack(str(count)+dtype, *value)\n                else:\n                    ifdvalue = pack(dtype, value)\n            tags.append((code, b\'\'.join(ifdentry), ifdvalue, writeonce))\n\n        def rational(arg, max_denominator=1000000):\n            # return nominator and denominator from float or two integers\n            from fractions import Fraction  # delayed import\n            try:\n                f = Fraction.from_float(arg)\n            except TypeError:\n                f = Fraction(arg[0], arg[1])\n            f = f.limit_denominator(max_denominator)\n            return f.numerator, f.denominator\n\n        if description:\n            # user provided description\n            addtag(\'ImageDescription\', \'s\', 0, description, writeonce=True)\n\n        # write shape and metadata to ImageDescription\n        self._metadata = {} if not metadata else metadata.copy()\n        if self._imagej:\n            description = imagej_description(\n                input_shape, shape[-1] in (3, 4), self._colormap is not None,\n                **self._metadata)\n        elif metadata or metadata == {}:\n            if self._truncate:\n                self._metadata.update(truncated=True)\n            description = json_description(input_shape, **self._metadata)\n        else:\n            description = None\n        if description:\n            # add 64 bytes buffer\n            # the image description might be updated later with the final shape\n            description = str2bytes(description, \'ascii\')\n            description += b\'\\0\'*64\n            self._descriptionlen = len(description)\n            addtag(\'ImageDescription\', \'s\', 0, description, writeonce=True)\n\n        if self._software:\n            addtag(\'Software\', \'s\', 0, self._software, writeonce=True)\n            self._software = None  # only save to first page in file\n        if datetime is None:\n            datetime = self._now()\n        addtag(\'DateTime\', \'s\', 0, datetime.strftime(\'%Y:%m:%d %H:%M:%S\'),\n               writeonce=True)\n        addtag(\'Compression\', \'H\', 1, compresstag)\n        if predictor:\n            addtag(\'Predictor\', \'H\', 1, 2)\n        addtag(\'ImageWidth\', \'I\', 1, shape[-2])\n        addtag(\'ImageLength\', \'I\', 1, shape[-3])\n        if tile:\n            addtag(\'TileWidth\', \'I\', 1, tile[-1])\n            addtag(\'TileLength\', \'I\', 1, tile[-2])\n            if tile[0] > 1:\n                addtag(\'ImageDepth\', \'I\', 1, shape[-4])\n                addtag(\'TileDepth\', \'I\', 1, tile[0])\n        addtag(\'NewSubfileType\', \'I\', 1, 0)\n        sampleformat = {\'u\': 1, \'i\': 2, \'f\': 3, \'c\': 6}[datadtype.kind]\n        addtag(\'SampleFormat\', \'H\', samplesperpixel,\n               (sampleformat,) * samplesperpixel)\n        addtag(\'PhotometricInterpretation\', \'H\', 1, photometric.value)\n        if colormap is not None:\n            addtag(\'ColorMap\', \'H\', colormap.size, colormap)\n        addtag(\'SamplesPerPixel\', \'H\', 1, samplesperpixel)\n        if planarconfig and samplesperpixel > 1:\n            addtag(\'PlanarConfiguration\', \'H\', 1, planarconfig.value)\n            addtag(\'BitsPerSample\', \'H\', samplesperpixel,\n                   (datadtype.itemsize * 8,) * samplesperpixel)\n        else:\n            addtag(\'BitsPerSample\', \'H\', 1, datadtype.itemsize * 8)\n        if extrasamples:\n            if photometric == RGB and extrasamples == 1:\n                addtag(\'ExtraSamples\', \'H\', 1, 1)  # associated alpha channel\n            else:\n                addtag(\'ExtraSamples\', \'H\', extrasamples, (0,) * extrasamples)\n        if resolution:\n            addtag(\'XResolution\', \'2I\', 1, rational(resolution[0]))\n            addtag(\'YResolution\', \'2I\', 1, rational(resolution[1]))\n            if len(resolution) > 2:\n                unit = resolution[2]\n                if unit is not None:\n                    unit = unit.upper()\n                unit = {None: 1, \'INCH\': 2, \'CM\': 3, \'CENTIMETER\': 3}[unit]\n            elif self._imagej:\n                unit = 1\n            else:\n                unit = 2\n            addtag(\'ResolutionUnit\', \'H\', 1, unit)\n\n        contiguous = not compress\n        if tile:\n            # one chunk per tile per plane\n            tiles = ((shape[2] + tile[0] - 1) // tile[0],\n                     (shape[3] + tile[1] - 1) // tile[1],\n                     (shape[4] + tile[2] - 1) // tile[2])\n            numtiles = product(tiles) * shape[1]\n            stripbytecounts = [\n                product(tile) * shape[-1] * datadtype.itemsize] * numtiles\n            addtag(tagbytecounts, offsetformat, numtiles, stripbytecounts)\n            addtag(tag_offsets, offsetformat, numtiles, [0] * numtiles)\n            contiguous = contiguous and product(tiles) == 1\n            if not contiguous:\n                # allocate tile buffer\n                chunk = numpy.empty(tile + (shape[-1],), dtype=datadtype)\n        elif contiguous:\n            # one strip per plane\n            stripbytecounts = [\n                product(datashape[2:]) * datadtype.itemsize] * shape[1]\n            addtag(tagbytecounts, offsetformat, shape[1], stripbytecounts)\n            addtag(tag_offsets, offsetformat, shape[1], [0] * shape[1])\n        else:\n            # compress rowsperstrip or ~64 KB chunks\n            rowsize = product(shape[-2:]) * datadtype.itemsize\n            if rowsperstrip is None:\n                rowsperstrip = 65536 // rowsize\n            if rowsperstrip < 1:\n                rowsperstrip = 1\n            elif rowsperstrip > shape[-3]:\n                rowsperstrip = shape[-3]\n            addtag(\'RowsPerStrip\', \'I\', 1, rowsperstrip)\n\n            numstrips = (shape[-3] + rowsperstrip - 1) // rowsperstrip\n            numstrips *= shape[1]\n            addtag(tagbytecounts, offsetformat, numstrips, [0] * numstrips)\n            addtag(tag_offsets, offsetformat, numstrips, [0] * numstrips)\n\n        if data is None and not contiguous:\n            raise ValueError(\'cannot write non-contiguous empty file\')\n\n        # add extra tags from user\n        for t in extratags:\n            addtag(*t)\n\n        # TODO: check TIFFReadDirectoryCheckOrder warning in files containing\n        #   multiple tags of same code\n        # the entries in an IFD must be sorted in ascending order by tag code\n        tags = sorted(tags, key=lambda x: x[0])\n\n        if not (self._bigtiff or self._imagej) and (\n                fh.tell() + datasize > 2**31-1):\n            raise ValueError(\'data too large for standard TIFF file\')\n\n        # if not compressed or multi-tiled, write the first IFD and then\n        # all data contiguously; else, write all IFDs and data interleaved\n        for pageindex in range(1 if contiguous else shape[0]):\n            # update pointer at ifd_offset\n            pos = fh.tell()\n            if pos % 2:\n                # location of IFD must begin on a word boundary\n                fh.write(b\'\\0\')\n                pos += 1\n            fh.seek(self._ifdoffset)\n            fh.write(pack(offsetformat, pos))\n            fh.seek(pos)\n\n            # write ifdentries\n            fh.write(pack(tagnoformat, len(tags)))\n            tag_offset = fh.tell()\n            fh.write(b\'\'.join(t[1] for t in tags))\n            self._ifdoffset = fh.tell()\n            fh.write(pack(offsetformat, 0))  # offset to next IFD\n\n            # write tag values and patch offsets in ifdentries, if necessary\n            for tagindex, tag in enumerate(tags):\n                if tag[2]:\n                    pos = fh.tell()\n                    if pos % 2:\n                        # tag value is expected to begin on word boundary\n                        fh.write(b\'\\0\')\n                        pos += 1\n                    fh.seek(tag_offset + tagindex*tagsize + offsetsize + 4)\n                    fh.write(pack(offsetformat, pos))\n                    fh.seek(pos)\n                    if tag[0] == tag_offsets:\n                        stripoffsetsoffset = pos\n                    elif tag[0] == tagbytecounts:\n                        strip_bytecounts_offset = pos\n                    elif tag[0] == 270 and tag[2].endswith(b\'\\0\\0\\0\\0\'):\n                        # image description buffer\n                        self._descriptionoffset = pos\n                        self._descriptionlenoffset = (\n                            tag_offset + tagindex * tagsize + 4)\n                    fh.write(tag[2])\n\n            # write image data\n            data_offset = fh.tell()\n            skip = align - data_offset % align\n            fh.seek(skip, 1)\n            data_offset += skip\n            if compress:\n                stripbytecounts = []\n            if contiguous:\n                if data is None:\n                    fh.write_empty(datasize)\n                else:\n                    fh.write_array(data)\n            elif tile:\n                for plane in data[pageindex]:\n                    for tz in range(tiles[0]):\n                        for ty in range(tiles[1]):\n                            for tx in range(tiles[2]):\n                                c0 = min(tile[0], shape[2] - tz*tile[0])\n                                c1 = min(tile[1], shape[3] - ty*tile[1])\n                                c2 = min(tile[2], shape[4] - tx*tile[2])\n                                chunk[c0:, c1:, c2:] = 0\n                                chunk[:c0, :c1, :c2] = plane[\n                                    tz*tile[0]:tz*tile[0]+c0,\n                                    ty*tile[1]:ty*tile[1]+c1,\n                                    tx*tile[2]:tx*tile[2]+c2]\n                                if compress:\n                                    t = compress(chunk)\n                                    stripbytecounts.append(len(t))\n                                    fh.write(t)\n                                else:\n                                    fh.write_array(chunk)\n                                    fh.flush()\n            elif compress:\n                # write one strip per rowsperstrip\n                assert data.shape[2] == 1  # not handling depth\n                numstrips = (shape[-3] + rowsperstrip - 1) // rowsperstrip\n                for plane in data[pageindex]:\n                    for i in range(numstrips):\n                        strip = plane[0, i*rowsperstrip: (i+1)*rowsperstrip]\n                        strip = compress(strip)\n                        stripbytecounts.append(len(strip))\n                        fh.write(strip)\n\n            # update strip/tile offsets and bytecounts if necessary\n            pos = fh.tell()\n            for tagindex, tag in enumerate(tags):\n                if tag[0] == tag_offsets:  # strip/tile offsets\n                    if tag[2]:\n                        fh.seek(stripoffsetsoffset)\n                        strip_offset = data_offset\n                        for size in stripbytecounts:\n                            fh.write(pack(offsetformat, strip_offset))\n                            strip_offset += size\n                    else:\n                        fh.seek(tag_offset + tagindex*tagsize + offsetsize + 4)\n                        fh.write(pack(offsetformat, data_offset))\n                elif tag[0] == tagbytecounts:  # strip/tile bytecounts\n                    if compress:\n                        if tag[2]:\n                            fh.seek(strip_bytecounts_offset)\n                            for size in stripbytecounts:\n                                fh.write(pack(offsetformat, size))\n                        else:\n                            fh.seek(tag_offset + tagindex*tagsize +\n                                    offsetsize + 4)\n                            fh.write(pack(offsetformat, stripbytecounts[0]))\n                    break\n            fh.seek(pos)\n            fh.flush()\n\n            # remove tags that should be written only once\n            if pageindex == 0:\n                tags = [tag for tag in tags if not tag[-1]]\n\n        self._shape = shape\n        self._datashape = (1,) + input_shape\n        self._datadtype = datadtype\n        self._dataoffset = data_offset\n        self._databytecounts = stripbytecounts\n\n        if contiguous:\n            # write remaining IFDs/tags later\n            self._tags = tags\n            # return offset and size of image data\n            if returnoffset:\n                return data_offset, sum(stripbytecounts)\n\n    def _write_remaining_pages(self):\n        """"""Write outstanding IFDs and tags to file.""""""\n        if not self._tags or self._truncate:\n            return\n\n        fh = self._fh\n        byteorder = self._byteorder\n        offsetformat = self._offsetformat\n        offsetsize = self._offsetsize\n        tagnoformat = self._tagnoformat\n        tagsize = self._tagsize\n        dataoffset = self._dataoffset\n        pagedatasize = sum(self._databytecounts)\n        pageno = self._shape[0] * self._datashape[0] - 1\n\n        def pack(fmt, *val):\n            return struct.pack(byteorder+fmt, *val)\n\n        # construct template IFD in memory\n        # need to patch offsets to next IFD and data before writing to disk\n        ifd = io.BytesIO()\n        ifd.write(pack(tagnoformat, len(self._tags)))\n        tagoffset = ifd.tell()\n        ifd.write(b\'\'.join(t[1] for t in self._tags))\n        ifdoffset = ifd.tell()\n        ifd.write(pack(offsetformat, 0))  # offset to next IFD\n        # tag values\n        for tagindex, tag in enumerate(self._tags):\n            offset2value = tagoffset + tagindex*tagsize + offsetsize + 4\n            if tag[2]:\n                pos = ifd.tell()\n                if pos % 2:  # tag value is expected to begin on word boundary\n                    ifd.write(b\'\\0\')\n                    pos += 1\n                ifd.seek(offset2value)\n                ifd.write(pack(offsetformat, pos + fh.tell()))\n                ifd.seek(pos)\n                ifd.write(tag[2])\n                if tag[0] == self._tagoffsets:\n                    # save strip/tile offsets for later updates\n                    stripoffset2offset = offset2value\n                    stripoffset2value = pos\n            elif tag[0] == self._tagoffsets:\n                # save strip/tile offsets for later updates\n                stripoffset2offset = None\n                stripoffset2value = offset2value\n        # size to word boundary\n        if ifd.tell() % 2:\n            ifd.write(b\'\\0\')\n\n        # check if all IFDs fit in file\n        pos = fh.tell()\n        if not self._bigtiff and pos + ifd.tell() * pageno > 2**32 - 256:\n            if self._imagej:\n                warnings.warn(\'truncating ImageJ file\')\n                return\n            raise ValueError(\'data too large for non-BigTIFF file\')\n\n        for _ in range(pageno):\n            # update pointer at IFD offset\n            pos = fh.tell()\n            fh.seek(self._ifdoffset)\n            fh.write(pack(offsetformat, pos))\n            fh.seek(pos)\n            self._ifdoffset = pos + ifdoffset\n            # update strip/tile offsets in IFD\n            dataoffset += pagedatasize  # offset to image data\n            if stripoffset2offset is None:\n                ifd.seek(stripoffset2value)\n                ifd.write(pack(offsetformat, dataoffset))\n            else:\n                ifd.seek(stripoffset2offset)\n                ifd.write(pack(offsetformat, pos + stripoffset2value))\n                ifd.seek(stripoffset2value)\n                stripoffset = dataoffset\n                for size in self._databytecounts:\n                    ifd.write(pack(offsetformat, stripoffset))\n                    stripoffset += size\n            # write IFD entry\n            fh.write(ifd.getvalue())\n\n        self._tags = None\n        self._datadtype = None\n        self._dataoffset = None\n        self._databytecounts = None\n        # do not reset _shape or _data_shape\n\n    def _write_image_description(self):\n        """"""Write meta data to ImageDescription tag.""""""\n        if (not self._datashape or self._datashape[0] == 1 or\n                self._descriptionoffset <= 0):\n            return\n\n        colormapped = self._colormap is not None\n        if self._imagej:\n            isrgb = self._shape[-1] in (3, 4)\n            description = imagej_description(\n                self._datashape, isrgb, colormapped, **self._metadata)\n        else:\n            description = json_description(self._datashape, **self._metadata)\n\n        # rewrite description and its length to file\n        description = description.encode(\'utf-8\')\n        description = description[:self._descriptionlen-1]\n        pos = self._fh.tell()\n        self._fh.seek(self._descriptionoffset)\n        self._fh.write(description)\n        self._fh.seek(self._descriptionlenoffset)\n        self._fh.write(struct.pack(self._byteorder+self._offsetformat,\n                                   len(description)+1))\n        self._fh.seek(pos)\n\n        self._descriptionoffset = 0\n        self._descriptionlenoffset = 0\n        self._descriptionlen = 0\n\n    def _now(self):\n        """"""Return current date and time.""""""\n        return datetime.datetime.now()\n\n    def close(self):\n        """"""Write remaining pages and close file handle.""""""\n        if not self._truncate:\n            self._write_remaining_pages()\n        self._write_image_description()\n        self._fh.close()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.close()\n\n\nclass TiffFile(object):\n    """"""Read image and metadata from TIFF file.\n\n    TiffFile instances must be closed using the \'close\' method, which is\n    automatically called when using the \'with\' context manager.\n\n    Attributes\n    ----------\n    pages : TiffPages\n        Sequence of TIFF pages in file.\n    series : list of TiffPageSeries\n        Sequences of closely related TIFF pages. These are computed\n        from OME, LSM, ImageJ, etc. metadata or based on similarity\n        of page properties such as shape, dtype, compression, etc.\n    byteorder : \'>\', \'<\'\n        The endianness of data in the file.\n        \'>\': big-endian (Motorola).\n        \'>\': little-endian (Intel).\n    is_flag : bool\n        If True, file is of a certain format.\n        Flags are: bigtiff, movie, shaped, ome, imagej, stk, lsm, fluoview,\n        nih, vista, \'micromanager, metaseries, mdgel, mediacy, tvips, fei,\n        sem, scn, svs, scanimage, andor, epics, pilatus, qptiff.\n\n    All attributes are read-only.\n\n    Examples\n    --------\n    >>> # read image array from TIFF file\n    >>> imsave(\'temp.tif\', numpy.random.rand(5, 301, 219))\n    >>> with TiffFile(\'temp.tif\') as tif:\n    ...     data = tif.asarray()\n    >>> data.shape\n    (5, 301, 219)\n\n    """"""\n    def __init__(self, arg, name=None, offset=None, size=None,\n                 multifile=True, movie=None, **kwargs):\n        """"""Initialize instance from file.\n\n        Parameters\n        ----------\n        arg : str or open file\n            Name of file or open file object.\n            The file objects are closed in TiffFile.close().\n        name : str\n            Optional name of file in case \'arg\' is a file handle.\n        offset : int\n            Optional start position of embedded file. By default, this is\n            the current file position.\n        size : int\n            Optional size of embedded file. By default, this is the number\n            of bytes from the \'offset\' to the end of the file.\n        multifile : bool\n            If True (default), series may include pages from multiple files.\n            Currently applies to OME-TIFF only.\n        movie : bool\n            If True, assume that later pages differ from first page only by\n            data offsets and byte counts. Significantly increases speed and\n            reduces memory usage when reading movies with thousands of pages.\n            Enabling this for non-movie files will result in data corruption\n            or crashes. Python 3 only.\n        kwargs : bool\n            \'is_ome\': If False, disable processing of OME-XML metadata.\n\n        """"""\n        if \'fastij\' in kwargs:\n            del kwargs[\'fastij\']\n            raise DeprecationWarning(\'the fastij option will be removed\')\n        for key, value in kwargs.items():\n            if key[:3] == \'is_\' and key[3:] in TIFF.FILE_FLAGS:\n                if value is not None and not value:\n                    setattr(self, key, bool(value))\n            else:\n                raise TypeError(\'unexpected keyword argument: %s\' % key)\n\n        fh = FileHandle(arg, mode=\'rb\', name=name, offset=offset, size=size)\n        self._fh = fh\n        self._multifile = bool(multifile)\n        self._files = {fh.name: self}  # cache of TiffFiles\n        try:\n            fh.seek(0)\n            try:\n                byteorder = {b\'II\': \'<\', b\'MM\': \'>\'}[fh.read(2)]\n            except KeyError:\n                raise ValueError(\'invalid TIFF file\')\n            sys_byteorder = {\'big\': \'>\', \'little\': \'<\'}[sys.byteorder]\n            self.isnative = byteorder == sys_byteorder\n\n            version = struct.unpack(byteorder+\'H\', fh.read(2))[0]\n            if version == 43:\n                # BigTiff\n                self.is_bigtiff = True\n                offsetsize, zero = struct.unpack(byteorder+\'HH\', fh.read(4))\n                if zero or offsetsize != 8:\n                    raise ValueError(\'invalid BigTIFF file\')\n                self.byteorder = byteorder\n                self.offsetsize = 8\n                self.offsetformat = byteorder+\'Q\'\n                self.tagnosize = 8\n                self.tagnoformat = byteorder+\'Q\'\n                self.tagsize = 20\n                self.tagformat1 = byteorder+\'HH\'\n                self.tagformat2 = byteorder+\'Q8s\'\n            elif version == 42:\n                self.is_bigtiff = False\n                self.byteorder = byteorder\n                self.offsetsize = 4\n                self.offsetformat = byteorder+\'I\'\n                self.tagnosize = 2\n                self.tagnoformat = byteorder+\'H\'\n                self.tagsize = 12\n                self.tagformat1 = byteorder+\'HH\'\n                self.tagformat2 = byteorder+\'I4s\'\n            else:\n                raise ValueError(\'not a TIFF file\')\n\n            # file handle is at offset to offset to first page\n            self.pages = TiffPages(self)\n\n            if self.is_lsm and (self.filehandle.size >= 2**32 or\n                                self.pages[0].compression != 1 or\n                                self.pages[1].compression != 1):\n                self._lsm_load_pages()\n                self._lsm_fix_strip_offsets()\n                self._lsm_fix_strip_bytecounts()\n            elif movie:\n                self.pages.useframes = True\n\n        except Exception:\n            fh.close()\n            raise\n\n    @property\n    def filehandle(self):\n        """"""Return file handle.""""""\n        return self._fh\n\n    @property\n    def filename(self):\n        """"""Return name of file handle.""""""\n        return self._fh.name\n\n    @lazyattr\n    def fstat(self):\n        """"""Return status of file handle as stat_result object.""""""\n        try:\n            return os.fstat(self._fh.fileno())\n        except Exception:  # io.UnsupportedOperation\n            return None\n\n    def close(self):\n        """"""Close open file handle(s).""""""\n        for tif in self._files.values():\n            tif.filehandle.close()\n        self._files = {}\n\n    def asarray(self, key=None, series=None, out=None, maxworkers=1):\n        """"""Return image data from multiple TIFF pages as numpy array.\n\n        By default, the data from the first series is returned.\n\n        Parameters\n        ----------\n        key : int, slice, or sequence of page indices\n            Defines which pages to return as array.\n        series : int or TiffPageSeries\n            Defines which series of pages to return as array.\n        out : numpy.ndarray, str, or file-like object; optional\n            Buffer where image data will be saved.\n            If None (default), a new array will be created.\n            If numpy.ndarray, a writable array of compatible dtype and shape.\n            If \'memmap\', directly memory-map the image data in the TIFF file\n            if possible; else create a memory-mapped array in a temporary file.\n            If str or open file, the file name or file object used to\n            create a memory-map to an array stored in a binary file on disk.\n        maxworkers : int\n            Maximum number of threads to concurrently get data from pages.\n            Default is 1. If None, up to half the CPU cores are used.\n            Reading data from file is limited to a single thread.\n            Using multiple threads can significantly speed up this function\n            if the bottleneck is decoding compressed data, e.g. in case of\n            large LZW compressed LSM files.\n            If the bottleneck is I/O or pure Python code, using multiple\n            threads might be detrimental.\n\n        """"""\n        if not self.pages:\n            return numpy.array([])\n        if key is None and series is None:\n            series = 0\n        if series is not None:\n            try:\n                series = self.series[series]\n            except (KeyError, TypeError):\n                pass\n            pages = series._pages\n        else:\n            pages = self.pages\n\n        if key is None:\n            pass\n        elif isinstance(key, inttypes):\n            pages = [pages[key]]\n        elif isinstance(key, slice):\n            pages = pages[key]\n        elif isinstance(key, collections.Iterable):\n            pages = [pages[k] for k in key]\n        else:\n            raise TypeError(\'key must be an int, slice, or sequence\')\n\n        if not pages:\n            raise ValueError(\'no pages selected\')\n\n        if self.is_nih:\n            result = stack_pages(pages, out=out, maxworkers=maxworkers,\n                                 squeeze=False)\n        elif key is None and series and series.offset:\n            typecode = self.byteorder + series.dtype.char\n            if out == \'memmap\' and pages[0].is_memmappable:\n                result = self.filehandle.memmap_array(\n                    typecode, series.shape, series.offset)\n            else:\n                if out is not None:\n                    out = create_output(out, series.shape, series.dtype)\n                self.filehandle.seek(series.offset)\n                result = self.filehandle.read_array(\n                    typecode, product(series.shape), out=out, native=True)\n        elif len(pages) == 1:\n            result = pages[0].asarray(out=out)\n        else:\n            result = stack_pages(pages, out=out, maxworkers=maxworkers)\n\n        if result is None:\n            return\n\n        if key is None:\n            try:\n                result.shape = series.shape\n            except ValueError:\n                try:\n                    warnings.warn(\'failed to reshape %s to %s\' % (\n                        result.shape, series.shape))\n                    # try series of expected shapes\n                    result.shape = (-1,) + series.shape\n                except ValueError:\n                    # revert to generic shape\n                    result.shape = (-1,) + pages[0].shape\n        elif len(pages) == 1:\n            result.shape = pages[0].shape\n        else:\n            result.shape = (-1,) + pages[0].shape\n        return result\n\n    @lazyattr\n    def series(self):\n        """"""Return related pages as TiffPageSeries.\n\n        Side effect: after calling this function, TiffFile.pages might contain\n        TiffPage and TiffFrame instances.\n\n        """"""\n        if not self.pages:\n            return []\n\n        useframes = self.pages.useframes\n        keyframe = self.pages.keyframe\n        series = []\n        for name in \'ome imagej lsm fluoview nih mdgel shaped\'.split():\n            if getattr(self, \'is_\' + name, False):\n                series = getattr(self, \'_%s_series\' % name)()\n                break\n        self.pages.useframes = useframes\n        self.pages.keyframe = keyframe\n        if not series:\n            series = self._generic_series()\n\n        # remove empty series, e.g. in MD Gel files\n        series = [s for s in series if sum(s.shape) > 0]\n\n        for i, s in enumerate(series):\n            s.index = i\n        return series\n\n    def _generic_series(self):\n        """"""Return image series in file.""""""\n        if self.pages.useframes:\n            # movie mode\n            page = self.pages[0]\n            shape = page.shape\n            axes = page.axes\n            if len(self.pages) > 1:\n                shape = (len(self.pages),) + shape\n                axes = \'I\' + axes\n            return [TiffPageSeries(self.pages[:], shape, page.dtype, axes,\n                                   stype=\'movie\')]\n\n        self.pages.clear(False)\n        self.pages.load()\n        result = []\n        keys = []\n        series = {}\n        compressions = TIFF.DECOMPESSORS\n        for page in self.pages:\n            if not page.shape:\n                continue\n            key = page.shape + (page.axes, page.compression in compressions)\n            if key in series:\n                series[key].append(page)\n            else:\n                keys.append(key)\n                series[key] = [page]\n        for key in keys:\n            pages = series[key]\n            page = pages[0]\n            shape = page.shape\n            axes = page.axes\n            if len(pages) > 1:\n                shape = (len(pages),) + shape\n                axes = \'I\' + axes\n            result.append(TiffPageSeries(pages, shape, page.dtype, axes,\n                                         stype=\'Generic\'))\n\n        return result\n\n    def _shaped_series(self):\n        """"""Return image series in ""shaped"" file.""""""\n        pages = self.pages\n        pages.useframes = True\n        lenpages = len(pages)\n\n        def append_series(series, pages, axes, shape, reshape, name):\n            page = pages[0]\n            if not axes:\n                shape = page.shape\n                axes = page.axes\n                if len(pages) > 1:\n                    shape = (len(pages),) + shape\n                    axes = \'Q\' + axes\n            size = product(shape)\n            resize = product(reshape)\n            if page.is_contiguous and resize > size and resize % size == 0:\n                # truncated file\n                axes = \'Q\' + axes\n                shape = (resize // size,) + shape\n            try:\n                axes = reshape_axes(axes, shape, reshape)\n                shape = reshape\n            except ValueError as e:\n                warnings.warn(str(e))\n            series.append(TiffPageSeries(pages, shape, page.dtype, axes,\n                                         name=name, stype=\'Shaped\'))\n\n        keyframe = axes = shape = reshape = name = None\n        series = []\n        index = 0\n        while True:\n            if index >= lenpages:\n                break\n            # new keyframe; start of new series\n            pages.keyframe = index\n            keyframe = pages[index]\n            if not keyframe.is_shaped:\n                warnings.warn(\'invalid shape metadata or corrupted file\')\n                return\n            # read metadata\n            axes = None\n            shape = None\n            metadata = json_description_metadata(keyframe.is_shaped)\n            name = metadata.get(\'name\', \'\')\n            reshape = metadata[\'shape\']\n            truncated = metadata.get(\'truncated\', False)\n            if \'axes\' in metadata:\n                axes = metadata[\'axes\']\n                if len(axes) == len(reshape):\n                    shape = reshape\n                else:\n                    axes = \'\'\n                    warnings.warn(\'axes do not match shape\')\n            # skip pages if possible\n            spages = [keyframe]\n            size = product(reshape)\n            npages, mod = divmod(size, product(keyframe.shape))\n            if mod:\n                warnings.warn(\'series shape does not match page shape\')\n                return\n            if 1 < npages <= lenpages - index:\n                size *= keyframe._dtype.itemsize\n                if truncated:\n                    npages = 1\n                elif not (keyframe.is_final and\n                          keyframe.offset + size < pages[index+1].offset):\n                    # need to read all pages for series\n                    for j in range(index+1, index+npages):\n                        page = pages[j]\n                        page.keyframe = keyframe\n                        spages.append(page)\n            append_series(series, spages, axes, shape, reshape, name)\n            index += npages\n\n        return series\n\n    def _imagej_series(self):\n        """"""Return image series in ImageJ file.""""""\n        # ImageJ\'s dimension order is always TZCYXS\n        # TODO: fix loading of color, composite, or palette images\n        self.pages.useframes = True\n        self.pages.keyframe = 0\n\n        ij = self.imagej_metadata\n        pages = self.pages\n        page = pages[0]\n\n        def is_hyperstack():\n            # ImageJ hyperstack store all image metadata in the first page and\n            # image data are stored contiguously before the second page, if any\n            if not page.is_final:\n                return False\n            images = ij.get(\'images\', 0)\n            if images <= 1:\n                return False\n            offset, count = page.is_contiguous\n            if (count != product(page.shape) * page.bitspersample // 8\n                    or offset + count*images > self.filehandle.size):\n                raise ValueError()\n            # check that next page is stored after data\n            if len(pages) > 1 and offset + count*images > pages[1].offset:\n                return False\n            return True\n\n        try:\n            hyperstack = is_hyperstack()\n        except ValueError:\n            warnings.warn(\'invalid ImageJ metadata or corrupted file\')\n            return\n        if hyperstack:\n            # no need to read other pages\n            pages = [page]\n        else:\n            self.pages.load()\n\n        shape = []\n        axes = []\n        if \'frames\' in ij:\n            shape.append(ij[\'frames\'])\n            axes.append(\'T\')\n        if \'slices\' in ij:\n            shape.append(ij[\'slices\'])\n            axes.append(\'Z\')\n        if \'channels\' in ij and not (page.photometric == 2 and not\n                                     ij.get(\'hyperstack\', False)):\n            shape.append(ij[\'channels\'])\n            axes.append(\'C\')\n        remain = ij.get(\'images\', len(pages))//(product(shape) if shape else 1)\n        if remain > 1:\n            shape.append(remain)\n            axes.append(\'I\')\n        if page.axes[0] == \'I\':\n            # contiguous multiple images\n            shape.extend(page.shape[1:])\n            axes.extend(page.axes[1:])\n        elif page.axes[:2] == \'SI\':\n            # color-mapped contiguous multiple images\n            shape = page.shape[0:1] + tuple(shape) + page.shape[2:]\n            axes = list(page.axes[0]) + axes + list(page.axes[2:])\n        else:\n            shape.extend(page.shape)\n            axes.extend(page.axes)\n        return [TiffPageSeries(pages, shape, page.dtype, axes, stype=\'ImageJ\')]\n\n    def _fluoview_series(self):\n        """"""Return image series in FluoView file.""""""\n        self.pages.useframes = True\n        self.pages.keyframe = 0\n        self.pages.load()\n        mm = self.fluoview_metadata\n        mmhd = list(reversed(mm[\'Dimensions\']))\n        axes = \'\'.join(TIFF.MM_DIMENSIONS.get(i[0].upper(), \'Q\')\n                       for i in mmhd if i[1] > 1)\n        shape = tuple(int(i[1]) for i in mmhd if i[1] > 1)\n        return [TiffPageSeries(self.pages, shape, self.pages[0].dtype, axes,\n                               name=mm[\'ImageName\'], stype=\'FluoView\')]\n\n    def _mdgel_series(self):\n        """"""Return image series in MD Gel file.""""""\n        # only a single page, scaled according to metadata in second page\n        self.pages.useframes = False\n        self.pages.keyframe = 0\n        self.pages.load()\n        md = self.mdgel_metadata\n        if md[\'FileTag\'] in (2, 128):\n            dtype = numpy.dtype(\'float32\')\n            scale = md[\'ScalePixel\']\n            scale = scale[0] / scale[1]  # rational\n            if md[\'FileTag\'] == 2:\n                # squary root data format\n                def transform(a):\n                    return a.astype(\'float32\')**2 * scale\n            else:\n                def transform(a):\n                    return a.astype(\'float32\') * scale\n        else:\n            transform = None\n        page = self.pages[0]\n        return [TiffPageSeries([page], page.shape, dtype, page.axes,\n                               transform=transform, stype=\'MDGel\')]\n\n    def _nih_series(self):\n        """"""Return image series in NIH file.""""""\n        self.pages.useframes = True\n        self.pages.keyframe = 0\n        self.pages.load()\n        page0 = self.pages[0]\n        if len(self.pages) == 1:\n            shape = page0.shape\n            axes = page0.axes\n        else:\n            shape = (len(self.pages),) + page0.shape\n            axes = \'I\' + page0.axes\n        return [\n            TiffPageSeries(self.pages, shape, page0.dtype, axes, stype=\'NIH\')]\n\n    def _ome_series(self):\n        """"""Return image series in OME-TIFF file(s).""""""\n        from xml.etree import cElementTree as etree  # delayed import\n        omexml = self.pages[0].description\n        try:\n            root = etree.fromstring(omexml)\n        except etree.ParseError as e:\n            # TODO: test badly encoded OME-XML\n            warnings.warn(\'ome-xml: %s\' % e)\n            try:\n                # might work on Python 2\n                omexml = omexml.decode(\'utf-8\', \'ignore\').encode(\'utf-8\')\n                root = etree.fromstring(omexml)\n            except Exception:\n                return\n\n        self.pages.useframes = True\n        self.pages.keyframe = 0\n        self.pages.load()\n\n        uuid = root.attrib.get(\'UUID\', None)\n        self._files = {uuid: self}\n        dirname = self._fh.dirname\n        modulo = {}\n        series = []\n        for element in root:\n            if element.tag.endswith(\'BinaryOnly\'):\n                # TODO: load OME-XML from master or companion file\n                warnings.warn(\'ome-xml: not an ome-tiff master file\')\n                break\n            if element.tag.endswith(\'StructuredAnnotations\'):\n                for annot in element:\n                    if not annot.attrib.get(\'Namespace\',\n                                            \'\').endswith(\'modulo\'):\n                        continue\n                    for value in annot:\n                        for modul in value:\n                            for along in modul:\n                                if not along.tag[:-1].endswith(\'Along\'):\n                                    continue\n                                axis = along.tag[-1]\n                                newaxis = along.attrib.get(\'Type\', \'other\')\n                                newaxis = TIFF.AXES_LABELS[newaxis]\n                                if \'Start\' in along.attrib:\n                                    step = float(along.attrib.get(\'Step\', 1))\n                                    start = float(along.attrib[\'Start\'])\n                                    stop = float(along.attrib[\'End\']) + step\n                                    labels = numpy.arange(start, stop, step)\n                                else:\n                                    labels = [label.text for label in along\n                                              if label.tag.endswith(\'Label\')]\n                                modulo[axis] = (newaxis, labels)\n\n            if not element.tag.endswith(\'Image\'):\n                continue\n\n            attr = element.attrib\n            name = attr.get(\'Name\', None)\n\n            for pixels in element:\n                if not pixels.tag.endswith(\'Pixels\'):\n                    continue\n                attr = pixels.attrib\n                dtype = attr.get(\'PixelType\', None)\n                axes = \'\'.join(reversed(attr[\'DimensionOrder\']))\n                shape = list(int(attr[\'Size\'+ax]) for ax in axes)\n                size = product(shape[:-2])\n                ifds = None\n                spp = 1  # samples per pixel\n                for data in pixels:\n                    if data.tag.endswith(\'Channel\'):\n                        attr = data.attrib\n                        if ifds is None:\n                            spp = int(attr.get(\'SamplesPerPixel\', spp))\n                            ifds = [None] * (size // spp)\n                        elif int(attr.get(\'SamplesPerPixel\', 1)) != spp:\n                            raise ValueError(\n                                ""Can\'t handle differing SamplesPerPixel"")\n                        continue\n                    if ifds is None:\n                        ifds = [None] * (size // spp)\n                    if not data.tag.endswith(\'TiffData\'):\n                        continue\n                    attr = data.attrib\n                    ifd = int(attr.get(\'IFD\', 0))\n                    num = int(attr.get(\'NumPlanes\', 1 if \'IFD\' in attr else 0))\n                    num = int(attr.get(\'PlaneCount\', num))\n                    idx = [int(attr.get(\'First\'+ax, 0)) for ax in axes[:-2]]\n                    try:\n                        idx = numpy.ravel_multi_index(idx, shape[:-2])\n                    except ValueError:\n                        # ImageJ produces invalid ome-xml when cropping\n                        warnings.warn(\'ome-xml: invalid TiffData index\')\n                        continue\n                    for uuid in data:\n                        if not uuid.tag.endswith(\'UUID\'):\n                            continue\n                        if uuid.text not in self._files:\n                            if not self._multifile:\n                                # abort reading multifile OME series\n                                # and fall back to generic series\n                                return []\n                            fname = uuid.attrib[\'FileName\']\n                            try:\n                                tif = TiffFile(os.path.join(dirname, fname))\n                                tif.pages.useframes = True\n                                tif.pages.keyframe = 0\n                                tif.pages.load()\n                            except (IOError, FileNotFoundError, ValueError):\n                                warnings.warn(\n                                    ""ome-xml: failed to read \'%s\'"" % fname)\n                                break\n                            self._files[uuid.text] = tif\n                            tif.close()\n                        pages = self._files[uuid.text].pages\n                        try:\n                            for i in range(num if num else len(pages)):\n                                ifds[idx + i] = pages[ifd + i]\n                        except IndexError:\n                            warnings.warn(\'ome-xml: index out of range\')\n                        # only process first UUID\n                        break\n                    else:\n                        pages = self.pages\n                        try:\n                            for i in range(num if num else len(pages)):\n                                ifds[idx + i] = pages[ifd + i]\n                        except IndexError:\n                            warnings.warn(\'ome-xml: index out of range\')\n\n                if all(i is None for i in ifds):\n                    # skip images without data\n                    continue\n\n                # set a keyframe on all IFDs\n                keyframe = None\n                for i in ifds:\n                    # try find a TiffPage\n                    if i and i == i.keyframe:\n                        keyframe = i\n                        break\n                if not keyframe:\n                    # reload a TiffPage from file\n                    for i, keyframe in enumerate(ifds):\n                        if keyframe:\n                            keyframe.parent.pages.keyframe = keyframe.index\n                            keyframe = keyframe.parent.pages[keyframe.index]\n                            ifds[i] = keyframe\n                            break\n                for i in ifds:\n                    if i is not None:\n                        i.keyframe = keyframe\n\n                dtype = keyframe.dtype\n                series.append(\n                    TiffPageSeries(ifds, shape, dtype, axes, parent=self,\n                                   name=name, stype=\'OME\'))\n        for serie in series:\n            shape = list(serie.shape)\n            for axis, (newaxis, labels) in modulo.items():\n                i = serie.axes.index(axis)\n                size = len(labels)\n                if shape[i] == size:\n                    serie.axes = serie.axes.replace(axis, newaxis, 1)\n                else:\n                    shape[i] //= size\n                    shape.insert(i+1, size)\n                    serie.axes = serie.axes.replace(axis, axis+newaxis, 1)\n            serie.shape = tuple(shape)\n        # squeeze dimensions\n        for serie in series:\n            serie.shape, serie.axes = squeeze_axes(serie.shape, serie.axes)\n        return series\n\n    def _lsm_series(self):\n        """"""Return main image series in LSM file. Skip thumbnails.""""""\n        lsmi = self.lsm_metadata\n        axes = TIFF.CZ_LSMINFO_SCANTYPE[lsmi[\'ScanType\']]\n        if self.pages[0].photometric == 2:  # RGB; more than one channel\n            axes = axes.replace(\'C\', \'\').replace(\'XY\', \'XYC\')\n        if lsmi.get(\'DimensionP\', 0) > 1:\n            axes += \'P\'\n        if lsmi.get(\'DimensionM\', 0) > 1:\n            axes += \'M\'\n        axes = axes[::-1]\n        shape = tuple(int(lsmi[TIFF.CZ_LSMINFO_DIMENSIONS[i]]) for i in axes)\n        name = lsmi.get(\'Name\', \'\')\n        self.pages.keyframe = 0\n        pages = self.pages[::2]\n        dtype = pages[0].dtype\n        series = [TiffPageSeries(pages, shape, dtype, axes, name=name,\n                                 stype=\'LSM\')]\n\n        if self.pages[1].is_reduced:\n            self.pages.keyframe = 1\n            pages = self.pages[1::2]\n            dtype = pages[0].dtype\n            cp, i = 1, 0\n            while cp < len(pages) and i < len(shape)-2:\n                cp *= shape[i]\n                i += 1\n            shape = shape[:i] + pages[0].shape\n            axes = axes[:i] + \'CYX\'\n            series.append(TiffPageSeries(pages, shape, dtype, axes, name=name,\n                                         stype=\'LSMreduced\'))\n\n        return series\n\n    def _lsm_load_pages(self):\n        """"""Load all pages from LSM file.""""""\n        self.pages.cache = True\n        self.pages.useframes = True\n        # second series: thumbnails\n        self.pages.keyframe = 1\n        keyframe = self.pages[1]\n        for page in self.pages[1::2]:\n            page.keyframe = keyframe\n        # first series: data\n        self.pages.keyframe = 0\n        keyframe = self.pages[0]\n        for page in self.pages[::2]:\n            page.keyframe = keyframe\n\n    def _lsm_fix_strip_offsets(self):\n        """"""Unwrap strip offsets for LSM files greater than 4 GB.\n\n        Each series and position require separate unwrapping (undocumented).\n\n        """"""\n        if self.filehandle.size < 2**32:\n            return\n\n        pages = self.pages\n        npages = len(pages)\n        series = self.series[0]\n        axes = series.axes\n\n        # find positions\n        positions = 1\n        for i in 0, 1:\n            if series.axes[i] in \'PM\':\n                positions *= series.shape[i]\n\n        # make time axis first\n        if positions > 1:\n            ntimes = 0\n            for i in 1, 2:\n                if axes[i] == \'T\':\n                    ntimes = series.shape[i]\n                    break\n            if ntimes:\n                div, mod = divmod(npages, 2*positions*ntimes)\n                assert mod == 0\n                shape = (positions, ntimes, div, 2)\n                indices = numpy.arange(product(shape)).reshape(shape)\n                indices = numpy.moveaxis(indices, 1, 0)\n        else:\n            indices = numpy.arange(npages).reshape(-1, 2)\n\n        # images of reduced page might be stored first\n        if pages[0].dataoffsets[0] > pages[1].dataoffsets[0]:\n            indices = indices[..., ::-1]\n\n        # unwrap offsets\n        wrap = 0\n        previousoffset = 0\n        for i in indices.flat:\n            page = pages[i]\n            dataoffsets = []\n            for currentoffset in page.dataoffsets:\n                if currentoffset < previousoffset:\n                    wrap += 2**32\n                dataoffsets.append(currentoffset + wrap)\n                previousoffset = currentoffset\n            page.dataoffsets = tuple(dataoffsets)\n\n    def _lsm_fix_strip_bytecounts(self):\n        """"""Set databytecounts to size of compressed data.\n\n        The StripByteCounts tag in LSM files contains the number of bytes\n        for the uncompressed data.\n\n        """"""\n        pages = self.pages\n        if pages[0].compression == 1:\n            return\n        # sort pages by first strip offset\n        pages = sorted(pages, key=lambda p: p.dataoffsets[0])\n        npages = len(pages) - 1\n        for i, page in enumerate(pages):\n            if page.index % 2:\n                continue\n            offsets = page.dataoffsets\n            bytecounts = page.databytecounts\n            if i < npages:\n                lastoffset = pages[i+1].dataoffsets[0]\n            else:\n                # LZW compressed strips might be longer than uncompressed\n                lastoffset = min(offsets[-1] + 2*bytecounts[-1], self._fh.size)\n            offsets = offsets + (lastoffset,)\n            page.databytecounts = tuple(offsets[j+1] - offsets[j]\n                                        for j in range(len(bytecounts)))\n\n    def __getattr__(self, name):\n        """"""Return \'is_flag\' attributes from first page.""""""\n        if name[3:] in TIFF.FILE_FLAGS:\n            if not self.pages:\n                return False\n            value = bool(getattr(self.pages[0], name))\n            setattr(self, name, value)\n            return value\n        raise AttributeError(""\'%s\' object has no attribute \'%s\'"" %\n                             (self.__class__.__name__, name))\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.close()\n\n    def __str__(self, detail=0, width=79):\n        """"""Return string containing information about file.\n\n        The detail parameter specifies the level of detail returned:\n\n        0: file only.\n        1: all series, first page of series and its tags.\n        2: large tag values and file metadata.\n        3: all pages.\n\n        """"""\n        info = [\n            ""TiffFile \'%s\'"",\n            format_size(self._fh.size),\n            {\'<\': \'LittleEndian\', \'>\': \'BigEndian\'}[self.byteorder]]\n        if self.is_bigtiff:\n            info.append(\'BigTiff\')\n        info.append(\'|\'.join(f.upper() for f in self.flags))\n        if len(self.pages) > 1:\n            info.append(\'%i Pages\' % len(self.pages))\n        if len(self.series) > 1:\n            info.append(\'%i Series\' % len(self.series))\n        if len(self._files) > 1:\n            info.append(\'%i Files\' % (len(self._files)))\n        info = \'  \'.join(info)\n        info = info.replace(\'    \', \'  \').replace(\'   \', \'  \')\n        info = info % snipstr(self._fh.name, max(12, width+2-len(info)))\n        if detail <= 0:\n            return info\n        info = [info]\n        info.append(\'\\n\'.join(str(s) for s in self.series))\n        if detail >= 3:\n            info.extend((TiffPage.__str__(p, detail=detail, width=width)\n                         for p in self.pages\n                         if p is not None))\n        else:\n            info.extend((TiffPage.__str__(s.pages[0], detail=detail,\n                                          width=width)\n                         for s in self.series\n                         if s.pages[0] is not None))\n        if detail >= 2:\n            for name in sorted(self.flags):\n                if hasattr(self, name + \'_metadata\'):\n                    m = getattr(self, name + \'_metadata\')\n                    if m:\n                        info.append(\n                            \'%s_METADATA\\n%s\' % (name.upper(),\n                                                 pformat(m, width=width,\n                                                         height=detail*12)))\n        return \'\\n\\n\'.join(info).replace(\'\\n\\n\\n\', \'\\n\\n\')\n\n    @lazyattr\n    def flags(self):\n        """"""Return set of file flags.""""""\n        return set(name.lower() for name in sorted(TIFF.FILE_FLAGS)\n                   if getattr(self, \'is_\' + name))\n\n    @lazyattr\n    def is_mdgel(self):\n        """"""File has MD Gel format.""""""\n        try:\n            return self.pages[0].is_mdgel or self.pages[1].is_mdgel\n        except IndexError:\n            return False\n\n    @property\n    def is_movie(self):\n        """"""Return if file is a movie.""""""\n        return self.pages.useframes\n\n    @lazyattr\n    def shaped_metadata(self):\n        """"""Return Tifffile metadata from JSON descriptions as dicts.""""""\n        if not self.is_shaped:\n            return\n        return tuple(json_description_metadata(s.pages[0].is_shaped)\n                     for s in self.series if s.stype.lower() == \'shaped\')\n\n    @lazyattr\n    def ome_metadata(self):\n        """"""Return OME XML as dict.""""""\n        if not self.is_ome:\n            return\n        return xml2dict(self.pages[0].description)[\'OME\']\n\n    @lazyattr\n    def qptiff_metadata(self):\n        """"""Return PerkinElmer-QPI-ImageDescription XML element as dict.""""""\n        if not self.is_qptiff:\n            return\n        root = \'PerkinElmer-QPI-ImageDescription\'\n        xml = self.pages[0].description.replace(\' \' + root + \' \', root)\n        return xml2dict(xml)[root]\n\n    @lazyattr\n    def lsm_metadata(self):\n        """"""Return LSM metadata from CZ_LSMINFO tag as dict.""""""\n        if not self.is_lsm:\n            return\n        return self.pages[0].tags[\'CZ_LSMINFO\'].value\n\n    @lazyattr\n    def stk_metadata(self):\n        """"""Return STK metadata from UIC tags as dict.""""""\n        if not self.is_stk:\n            return\n        page = self.pages[0]\n        tags = page.tags\n        result = {}\n        result[\'NumberPlanes\'] = tags[\'UIC2tag\'].count\n        if page.description:\n            result[\'PlaneDescriptions\'] = page.description.split(\'\\0\')\n            # result[\'plane_descriptions\'] = stk_description_metadata(\n            #    page.image_description)\n        if \'UIC1tag\' in tags:\n            result.update(tags[\'UIC1tag\'].value)\n        if \'UIC3tag\' in tags:\n            result.update(tags[\'UIC3tag\'].value)  # wavelengths\n        if \'UIC4tag\' in tags:\n            result.update(tags[\'UIC4tag\'].value)  # override uic1 tags\n        uic2tag = tags[\'UIC2tag\'].value\n        result[\'ZDistance\'] = uic2tag[\'ZDistance\']\n        result[\'TimeCreated\'] = uic2tag[\'TimeCreated\']\n        result[\'TimeModified\'] = uic2tag[\'TimeModified\']\n        try:\n            result[\'DatetimeCreated\'] = numpy.array(\n                [julian_datetime(*dt) for dt in\n                 zip(uic2tag[\'DateCreated\'], uic2tag[\'TimeCreated\'])],\n                dtype=\'datetime64[ns]\')\n            result[\'DatetimeModified\'] = numpy.array(\n                [julian_datetime(*dt) for dt in\n                 zip(uic2tag[\'DateModified\'], uic2tag[\'TimeModified\'])],\n                dtype=\'datetime64[ns]\')\n        except ValueError as e:\n            warnings.warn(\'stk_metadata: %s\' % e)\n        return result\n\n    @lazyattr\n    def imagej_metadata(self):\n        """"""Return consolidated ImageJ metadata as dict.""""""\n        if not self.is_imagej:\n            return\n        page = self.pages[0]\n        result = imagej_description_metadata(page.is_imagej)\n        if \'IJMetadata\' in page.tags:\n            try:\n                result.update(page.tags[\'IJMetadata\'].value)\n            except Exception:\n                pass\n        return result\n\n    @lazyattr\n    def fluoview_metadata(self):\n        """"""Return consolidated FluoView metadata as dict.""""""\n        if not self.is_fluoview:\n            return\n        result = {}\n        page = self.pages[0]\n        result.update(page.tags[\'MM_Header\'].value)\n        # TODO: read stamps from all pages\n        result[\'Stamp\'] = page.tags[\'MM_Stamp\'].value\n        # skip parsing image description; not reliable\n        # try:\n        #     t = fluoview_description_metadata(page.image_description)\n        #     if t is not None:\n        #         result[\'ImageDescription\'] = t\n        # except Exception as e:\n        #     warnings.warn(\n        #         ""failed to read FluoView image description: %s"" % e)\n        return result\n\n    @lazyattr\n    def nih_metadata(self):\n        """"""Return NIH Image metadata from NIHImageHeader tag as dict.""""""\n        if not self.is_nih:\n            return\n        return self.pages[0].tags[\'NIHImageHeader\'].value\n\n    @lazyattr\n    def fei_metadata(self):\n        """"""Return FEI metadata from SFEG or HELIOS tags as dict.""""""\n        if not self.is_fei:\n            return\n        tags = self.pages[0].tags\n        if \'FEI_SFEG\' in tags:\n            return tags[\'FEI_SFEG\'].value\n        if \'FEI_HELIOS\' in tags:\n            return tags[\'FEI_HELIOS\'].value\n\n    @lazyattr\n    def sem_metadata(self):\n        """"""Return SEM metadata from CZ_SEM tag as dict.""""""\n        if not self.is_sem:\n            return\n        return self.pages[0].tags[\'CZ_SEM\'].value\n\n    @lazyattr\n    def mdgel_metadata(self):\n        """"""Return consolidated metadata from MD GEL tags as dict.""""""\n        for page in self.pages[:2]:\n            if \'MDFileTag\' in page.tags:\n                tags = page.tags\n                break\n        else:\n            return\n        result = {}\n        for code in range(33445, 33453):\n            name = TIFF.TAGS[code]\n            if name not in tags:\n                continue\n            result[name[2:]] = tags[name].value\n        return result\n\n    @lazyattr\n    def andor_metadata(self):\n        """"""Return Andor tags as dict.""""""\n        return self.pages[0].andor_tags\n\n    @lazyattr\n    def epics_metadata(self):\n        """"""Return EPICS areaDetector tags as dict.""""""\n        return self.pages[0].epics_tags\n\n    @lazyattr\n    def tvips_metadata(self):\n        """"""Return TVIPS tag as dict.""""""\n        if not self.is_tvips:\n            return\n        return self.pages[0].tags[\'TVIPS\'].value\n\n    @lazyattr\n    def metaseries_metadata(self):\n        """"""Return MetaSeries metadata from image description as dict.""""""\n        if not self.is_metaseries:\n            return\n        return metaseries_description_metadata(self.pages[0].description)\n\n    @lazyattr\n    def pilatus_metadata(self):\n        """"""Return Pilatus metadata from image description as dict.""""""\n        if not self.is_pilatus:\n            return\n        return pilatus_description_metadata(self.pages[0].description)\n\n    @lazyattr\n    def micromanager_metadata(self):\n        """"""Return consolidated MicroManager metadata as dict.""""""\n        if not self.is_micromanager:\n            return\n        # from file header\n        result = read_micromanager_metadata(self._fh)\n        # from tag\n        result.update(self.pages[0].tags[\'MicroManagerMetadata\'].value)\n        return result\n\n    @lazyattr\n    def scanimage_metadata(self):\n        """"""Return ScanImage non-varying frame and ROI metadata as dict.""""""\n        if not self.is_scanimage:\n            return\n        result = {}\n        try:\n            framedata, roidata = read_scanimage_metadata(self._fh)\n            result[\'FrameData\'] = framedata\n            result.update(roidata)\n        except ValueError:\n            pass\n        # TODO: scanimage_artist_metadata\n        try:\n            result[\'Description\'] = scanimage_description_metadata(\n                self.pages[0].description)\n        except Exception as e:\n            warnings.warn(\'scanimage_description_metadata failed: %s\' % e)\n        return result\n\n    @property\n    def geotiff_metadata(self):\n        """"""Return GeoTIFF metadata from first page as dict.""""""\n        if not self.is_geotiff:\n            return\n        return self.pages[0].geotiff_tags\n\n\nclass TiffPages(object):\n    """"""Sequence of TIFF image file directories.""""""\n    def __init__(self, parent):\n        """"""Initialize instance from file. Read first TiffPage from file.\n\n        The file position must be at an offset to an offset to a TiffPage.\n\n        """"""\n        self.parent = parent\n        self.pages = []  # cache of TiffPages, TiffFrames, or their offsets\n        self.complete = False  # True if offsets to all pages were read\n        self._tiffpage = TiffPage  # class for reading tiff pages\n        self._keyframe = None\n        self._cache = True\n\n        # read offset to first page\n        fh = parent.filehandle\n        self._nextpageoffset = fh.tell()\n        offset = struct.unpack(parent.offsetformat,\n                               fh.read(parent.offsetsize))[0]\n\n        if offset == 0:\n            # warnings.warn(\'file contains no pages\')\n            self.complete = True\n            return\n        if offset >= fh.size:\n            warnings.warn(\'invalid page offset (%i)\' % offset)\n            self.complete = True\n            return\n\n        # always read and cache first page\n        fh.seek(offset)\n        page = TiffPage(parent, index=0)\n        self.pages.append(page)\n        self._keyframe = page\n\n    @property\n    def cache(self):\n        """"""Return if pages/frames are currenly being cached.""""""\n        return self._cache\n\n    @cache.setter\n    def cache(self, value):\n        """"""Enable or disable caching of pages/frames. Clear cache if False.""""""\n        value = bool(value)\n        if self._cache and not value:\n            self.clear()\n        self._cache = value\n\n    @property\n    def useframes(self):\n        """"""Return if currently using TiffFrame (True) or TiffPage (False).""""""\n        return self._tiffpage == TiffFrame and TiffFrame is not TiffPage\n\n    @useframes.setter\n    def useframes(self, value):\n        """"""Set to use TiffFrame (True) or TiffPage (False).""""""\n        self._tiffpage = TiffFrame if value else TiffPage\n\n    @property\n    def keyframe(self):\n        """"""Return index of current keyframe.""""""\n        return self._keyframe.index\n\n    @keyframe.setter\n    def keyframe(self, index):\n        """"""Set current keyframe. Load TiffPage from file if necessary.""""""\n        if self._keyframe.index == index:\n            return\n        if self.complete or 0 <= index < len(self.pages):\n            page = self.pages[index]\n            if isinstance(page, TiffPage):\n                self._keyframe = page\n                return\n            elif isinstance(page, TiffFrame):\n                # remove existing frame\n                self.pages[index] = page.offset\n        # load TiffPage from file\n        useframes = self.useframes\n        self._tiffpage = TiffPage\n        self._keyframe = self[index]\n        self.useframes = useframes\n\n    @property\n    def next_page_offset(self):\n        """"""Return offset where offset to a new page can be stored.""""""\n        if not self.complete:\n            self._seek(-1)\n        return self._nextpageoffset\n\n    def load(self):\n        """"""Read all remaining pages from file.""""""\n        fh = self.parent.filehandle\n        keyframe = self._keyframe\n        pages = self.pages\n        if not self.complete:\n            self._seek(-1)\n        for i, page in enumerate(pages):\n            if isinstance(page, inttypes):\n                fh.seek(page)\n                page = self._tiffpage(self.parent, index=i, keyframe=keyframe)\n                pages[i] = page\n\n    def clear(self, fully=True):\n        """"""Delete all but first page from cache. Set keyframe to first page.""""""\n        pages = self.pages\n        if not self._cache or len(pages) < 1:\n            return\n        self._keyframe = pages[0]\n        if fully:\n            # delete all but first TiffPage/TiffFrame\n            for i, page in enumerate(pages[1:]):\n                if not isinstance(page, inttypes):\n                    pages[i+1] = page.offset\n        elif TiffFrame is not TiffPage:\n            # delete only TiffFrames\n            for i, page in enumerate(pages):\n                if isinstance(page, TiffFrame):\n                    pages[i] = page.offset\n\n    def _seek(self, index):\n        """"""Seek file to offset of specified page.""""""\n        pages = self.pages\n        if not pages:\n            return\n\n        fh = self.parent.filehandle\n        if fh.closed:\n            raise RuntimeError(\'FileHandle is closed\')\n\n        if self.complete or 0 <= index < len(pages):\n            page = pages[index]\n            offset = page if isinstance(page, inttypes) else page.offset\n            fh.seek(offset)\n            return\n\n        offsetformat = self.parent.offsetformat\n        offsetsize = self.parent.offsetsize\n        tagnoformat = self.parent.tagnoformat\n        tagnosize = self.parent.tagnosize\n        tagsize = self.parent.tagsize\n        unpack = struct.unpack\n\n        page = pages[-1]\n        offset = page if isinstance(page, inttypes) else page.offset\n\n        while True:\n            # read offsets to pages from file until index is reached\n            fh.seek(offset)\n            # skip tags\n            try:\n                tagno = unpack(tagnoformat, fh.read(tagnosize))[0]\n                if tagno > 4096:\n                    raise ValueError(\'suspicious number of tags\')\n            except Exception:\n                warnings.warn(\'corrupted tag list at offset %i\' % offset)\n                del pages[-1]\n                self.complete = True\n                break\n            self._nextpageoffset = offset + tagnosize + tagno * tagsize\n            fh.seek(self._nextpageoffset)\n\n            # read offset to next page\n            offset = unpack(offsetformat, fh.read(offsetsize))[0]\n            if offset == 0:\n                self.complete = True\n                break\n            if offset >= fh.size:\n                warnings.warn(\'invalid page offset (%i)\' % offset)\n                self.complete = True\n                break\n\n            pages.append(offset)\n            if 0 <= index < len(pages):\n                break\n\n        if index >= len(pages):\n            raise IndexError(\'list index out of range\')\n\n        page = pages[index]\n        fh.seek(page if isinstance(page, inttypes) else page.offset)\n\n    def __bool__(self):\n        """"""Return True if file contains any pages.""""""\n        return len(self.pages) > 0\n\n    def __len__(self):\n        """"""Return number of pages in file.""""""\n        if not self.complete:\n            self._seek(-1)\n        return len(self.pages)\n\n    def __getitem__(self, key):\n        """"""Return specified page(s) from cache or file.""""""\n        pages = self.pages\n        if not pages:\n            raise IndexError(\'list index out of range\')\n        if key is 0:\n            return pages[key]\n\n        if isinstance(key, slice):\n            start, stop, _ = key.indices(2**31-1)\n            if not self.complete and max(stop, start) > len(pages):\n                self._seek(-1)\n            return [self[i] for i in range(*key.indices(len(pages)))]\n\n        if self.complete and key >= len(pages):\n            raise IndexError(\'list index out of range\')\n\n        try:\n            page = pages[key]\n        except IndexError:\n            page = 0\n        if not isinstance(page, inttypes):\n            return page\n\n        self._seek(key)\n        page = self._tiffpage(self.parent, index=key, keyframe=self._keyframe)\n        if self._cache:\n            pages[key] = page\n        return page\n\n    def __iter__(self):\n        """"""Return iterator over all pages.""""""\n        i = 0\n        while True:\n            try:\n                yield self[i]\n                i += 1\n            except IndexError:\n                break\n\n\nclass TiffPage(object):\n    """"""TIFF image file directory (IFD).\n\n    Attributes\n    ----------\n    index : int\n        Index of page in file.\n    dtype : numpy.dtype or None\n        Data type (native byte order) of the image in IFD.\n    shape : tuple\n        Dimensions of the image in IFD.\n    axes : str\n        Axes label codes:\n        \'X\' width, \'Y\' height, \'S\' sample, \'I\' image series|page|plane,\n        \'Z\' depth, \'C\' color|em-wavelength|channel, \'E\' ex-wavelength|lambda,\n        \'T\' time, \'R\' region|tile, \'A\' angle, \'P\' phase, \'H\' lifetime,\n        \'L\' exposure, \'V\' event, \'Q\' unknown, \'_\' missing\n    tags : dict\n        Dictionary of tags in IFD. {tag.name: TiffTag}\n    colormap : numpy.ndarray\n        Color look up table, if exists.\n\n    All attributes are read-only.\n\n    Notes\n    -----\n    The internal, normalized \'_shape\' attribute is 6 dimensional:\n\n    0 : number planes/images  (stk, ij).\n    1 : planar samplesperpixel.\n    2 : imagedepth Z  (sgi).\n    3 : imagelength Y.\n    4 : imagewidth X.\n    5 : contig samplesperpixel.\n\n    """"""\n    # default properties; will be updated from tags\n    imagewidth = 0\n    imagelength = 0\n    imagedepth = 1\n    tilewidth = 0\n    tilelength = 0\n    tiledepth = 1\n    bitspersample = 1\n    samplesperpixel = 1\n    sampleformat = 1\n    rowsperstrip = 2**32-1\n    compression = 1\n    planarconfig = 1\n    fillorder = 1\n    photometric = 0\n    predictor = 1\n    extrasamples = 1\n    colormap = None\n    software = \'\'\n    description = \'\'\n    description1 = \'\'\n\n    def __init__(self, parent, index, keyframe=None):\n        """"""Initialize instance from file.\n\n        The file handle position must be at offset to a valid IFD.\n\n        """"""\n        self.parent = parent\n        self.index = index\n        self.shape = ()\n        self._shape = ()\n        self.dtype = None\n        self._dtype = None\n        self.axes = \'\'\n        self.tags = {}\n\n        self.dataoffsets = ()\n        self.databytecounts = ()\n\n        # read TIFF IFD structure and its tags from file\n        fh = parent.filehandle\n        self.offset = fh.tell()  # offset to this IFD\n        try:\n            tagno = struct.unpack(parent.tagnoformat,\n                                  fh.read(parent.tagnosize))[0]\n            if tagno > 4096:\n                raise ValueError(\'suspicious number of tags\')\n        except Exception:\n            raise ValueError(\'corrupted tag list at offset %i\' % self.offset)\n\n        tagsize = parent.tagsize\n        data = fh.read(tagsize * tagno)\n        tags = self.tags\n        index = -tagsize\n        for _ in range(tagno):\n            index += tagsize\n            try:\n                tag = TiffTag(self.parent, data[index:index+tagsize])\n            except TiffTag.Error as e:\n                warnings.warn(str(e))\n                continue\n            tagname = tag.name\n            if tagname not in tags:\n                name = tagname\n                tags[name] = tag\n            else:\n                # some files contain multiple tags with same code\n                # e.g. MicroManager files contain two ImageDescription tags\n                i = 1\n                while True:\n                    name = \'%s%i\' % (tagname, i)\n                    if name not in tags:\n                        tags[name] = tag\n                        break\n            name = TIFF.TAG_ATTRIBUTES.get(name, \'\')\n            if name:\n                if (name[:3] in \'sof des\' and not isinstance(tag.value, str)):\n                    pass  # wrong string type for software, description\n                else:\n                    setattr(self, name, tag.value)\n\n        if not tags:\n            return  # found in FIBICS\n\n        # consolidate private tags; remove them from self.tags\n        if self.is_andor:\n            self.andor_tags\n        elif self.is_epics:\n            self.epics_tags\n\n        if self.is_lsm or (self.index and self.parent.is_lsm):\n            # correct non standard LSM bitspersample tags\n            self.tags[\'BitsPerSample\']._fix_lsm_bitspersample(self)\n\n        if self.is_vista or (self.index and self.parent.is_vista):\n            # ISS Vista writes wrong ImageDepth tag\n            self.imagedepth = 1\n\n        if self.is_stk and \'UIC1tag\' in tags and not tags[\'UIC1tag\'].value:\n            # read UIC1tag now that plane count is known\n            uic1tag = tags[\'UIC1tag\']\n            fh.seek(uic1tag.valueoffset)\n            tags[\'UIC1tag\'].value = read_uic1tag(\n                fh, self.parent.byteorder, uic1tag.dtype,\n                uic1tag.count, None, tags[\'UIC2tag\'].count)\n\n        if \'IJMetadata\' in tags:\n            # decode IJMetadata tag\n            try:\n                tags[\'IJMetadata\'].value = imagej_metadata(\n                    tags[\'IJMetadata\'].value,\n                    tags[\'IJMetadataByteCounts\'].value,\n                    self.parent.byteorder)\n            except Exception as e:\n                warnings.warn(str(e))\n\n        if \'BitsPerSample\' in tags:\n            tag = tags[\'BitsPerSample\']\n            if tag.count == 1:\n                self.bitspersample = tag.value\n            else:\n                # LSM might list more items than samplesperpixel\n                value = tag.value[:self.samplesperpixel]\n                if any((v-value[0] for v in value)):\n                    self.bitspersample = value\n                else:\n                    self.bitspersample = value[0]\n\n        if \'SampleFormat\' in tags:\n            tag = tags[\'SampleFormat\']\n            if tag.count == 1:\n                self.sampleformat = tag.value\n            else:\n                value = tag.value[:self.samplesperpixel]\n                if any((v-value[0] for v in value)):\n                    self.sampleformat = value\n                else:\n                    self.sampleformat = value[0]\n\n        if \'ImageLength\' in tags:\n            if \'RowsPerStrip\' not in tags or tags[\'RowsPerStrip\'].count > 1:\n                self.rowsperstrip = self.imagelength\n            # self.stripsperimage = int(math.floor(\n            #    float(self.imagelength + self.rowsperstrip - 1) /\n            #    self.rowsperstrip))\n\n        # determine dtype\n        dtype = self.sampleformat, self.bitspersample\n        dtype = TIFF.SAMPLE_DTYPES.get(dtype, None)\n        if dtype is not None:\n            dtype = numpy.dtype(dtype)\n        self.dtype = self._dtype = dtype\n\n        # determine shape of data\n        imagelength = self.imagelength\n        imagewidth = self.imagewidth\n        imagedepth = self.imagedepth\n        samplesperpixel = self.samplesperpixel\n\n        if self.is_stk:\n            assert self.imagedepth == 1\n            uictag = tags[\'UIC2tag\'].value\n            planes = tags[\'UIC2tag\'].count\n            if self.planarconfig == 1:\n                self._shape = (\n                    planes, 1, 1, imagelength, imagewidth, samplesperpixel)\n                if samplesperpixel == 1:\n                    self.shape = (planes, imagelength, imagewidth)\n                    self.axes = \'YX\'\n                else:\n                    self.shape = (\n                        planes, imagelength, imagewidth, samplesperpixel)\n                    self.axes = \'YXS\'\n            else:\n                self._shape = (\n                    planes, samplesperpixel, 1, imagelength, imagewidth, 1)\n                if samplesperpixel == 1:\n                    self.shape = (planes, imagelength, imagewidth)\n                    self.axes = \'YX\'\n                else:\n                    self.shape = (\n                        planes, samplesperpixel, imagelength, imagewidth)\n                    self.axes = \'SYX\'\n            # detect type of series\n            if planes == 1:\n                self.shape = self.shape[1:]\n            elif numpy.all(uictag[\'ZDistance\'] != 0):\n                self.axes = \'Z\' + self.axes\n            elif numpy.all(numpy.diff(uictag[\'TimeCreated\']) != 0):\n                self.axes = \'T\' + self.axes\n            else:\n                self.axes = \'I\' + self.axes\n        elif self.photometric == 2 or samplesperpixel > 1:  # PHOTOMETRIC.RGB\n            if self.planarconfig == 1:\n                self._shape = (\n                    1, 1, imagedepth, imagelength, imagewidth, samplesperpixel)\n                if imagedepth == 1:\n                    self.shape = (imagelength, imagewidth, samplesperpixel)\n                    self.axes = \'YXS\'\n                else:\n                    self.shape = (\n                        imagedepth, imagelength, imagewidth, samplesperpixel)\n                    self.axes = \'ZYXS\'\n            else:\n                self._shape = (1, samplesperpixel, imagedepth,\n                               imagelength, imagewidth, 1)\n                if imagedepth == 1:\n                    self.shape = (samplesperpixel, imagelength, imagewidth)\n                    self.axes = \'SYX\'\n                else:\n                    self.shape = (\n                        samplesperpixel, imagedepth, imagelength, imagewidth)\n                    self.axes = \'SZYX\'\n        else:\n            self._shape = (1, 1, imagedepth, imagelength, imagewidth, 1)\n            if imagedepth == 1:\n                self.shape = (imagelength, imagewidth)\n                self.axes = \'YX\'\n            else:\n                self.shape = (imagedepth, imagelength, imagewidth)\n                self.axes = \'ZYX\'\n\n        # dataoffsets and databytecounts\n        if \'TileOffsets\' in tags:\n            self.dataoffsets = tags[\'TileOffsets\'].value\n        elif \'StripOffsets\' in tags:\n            self.dataoffsets = tags[\'StripOffsets\'].value\n        else:\n            self.dataoffsets = (0,)\n\n        if \'TileByteCounts\' in tags:\n            self.databytecounts = tags[\'TileByteCounts\'].value\n        elif \'StripByteCounts\' in tags:\n            self.databytecounts = tags[\'StripByteCounts\'].value\n        else:\n            self.databytecounts = (\n                product(self.shape) * (self.bitspersample // 8),)\n            if self.compression != 1:\n                warnings.warn(\'required ByteCounts tag is missing\')\n\n        assert len(self.shape) == len(self.axes)\n\n    def asarray(self, out=None, squeeze=True, lock=None, reopen=True,\n                maxsize=64*2**30, validate=True):\n        """"""Read image data from file and return as numpy array.\n\n        Raise ValueError if format is unsupported.\n\n        Parameters\n        ----------\n        out : numpy.ndarray, str, or file-like object; optional\n            Buffer where image data will be saved.\n            If None (default), a new array will be created.\n            If numpy.ndarray, a writable array of compatible dtype and shape.\n            If \'memmap\', directly memory-map the image data in the TIFF file\n            if possible; else create a memory-mapped array in a temporary file.\n            If str or open file, the file name or file object used to\n            create a memory-map to an array stored in a binary file on disk.\n        squeeze : bool\n            If True, all length-1 dimensions (except X and Y) are\n            squeezed out from the array.\n            If False, the shape of the returned array might be different from\n            the page.shape.\n        lock : {RLock, NullContext}\n            A reentrant lock used to syncronize reads from file.\n            If None (default), the lock of the parent\'s filehandle is used.\n        reopen : bool\n            If True (default) and the parent file handle is closed, the file\n            is temporarily re-opened and closed if no exception occurs.\n        maxsize: int or None\n            Maximum size of data before a ValueError is raised.\n            Can be used to catch DOS. Default: 64 GB.\n        validate : bool\n            If True (default), validate various parameters.\n            If None, only validate parameters and return None.\n\n        """"""\n        self_ = self\n        self = self.keyframe  # self or keyframe\n\n        if not self._shape or product(self._shape) == 0:\n            return\n\n        tags = self.tags\n\n        if validate or validate is None:\n            if maxsize and product(self._shape) > maxsize:\n                raise ValueError(\'data are too large %s\' % str(self._shape))\n            if self.dtype is None:\n                raise ValueError(\'data type not supported: %s%i\' % (\n                    self.sampleformat, self.bitspersample))\n            if self.compression not in TIFF.DECOMPESSORS:\n                raise ValueError(\n                    \'cannot decompress %s\' % self.compression.name)\n            if \'SampleFormat\' in tags:\n                tag = tags[\'SampleFormat\']\n                if tag.count != 1 and any((i-tag.value[0] for i in tag.value)):\n                    raise ValueError(\n                        \'sample formats do not match %s\' % tag.value)\n            if self.is_chroma_subsampled:\n                # TODO: implement chroma subsampling\n                raise NotImplementedError(\'chroma subsampling not supported\')\n            if validate is None:\n                return\n\n        fh = self_.parent.filehandle\n        lock = fh.lock if lock is None else lock\n        with lock:\n            closed = fh.closed\n            if closed:\n                if reopen:\n                    fh.open()\n                else:\n                    raise IOError(\'file handle is closed\')\n\n        dtype = self._dtype\n        shape = self._shape\n        imagewidth = self.imagewidth\n        imagelength = self.imagelength\n        imagedepth = self.imagedepth\n        bitspersample = self.bitspersample\n        typecode = self.parent.byteorder + dtype.char\n        lsb2msb = self.fillorder == 2\n        offsets, bytecounts = self_.offsets_bytecounts\n        istiled = self.is_tiled\n\n        if istiled:\n            tilewidth = self.tilewidth\n            tilelength = self.tilelength\n            tiledepth = self.tiledepth\n            tw = (imagewidth + tilewidth - 1) // tilewidth\n            tl = (imagelength + tilelength - 1) // tilelength\n            td = (imagedepth + tiledepth - 1) // tiledepth\n            shape = (shape[0], shape[1],\n                     td*tiledepth, tl*tilelength, tw*tilewidth, shape[-1])\n            tileshape = (tiledepth, tilelength, tilewidth, shape[-1])\n            runlen = tilewidth\n        else:\n            runlen = imagewidth\n\n        if out == \'memmap\' and self.is_memmappable:\n            with lock:\n                result = fh.memmap_array(typecode, shape, offset=offsets[0])\n        elif self.is_contiguous:\n            if out is not None:\n                out = create_output(out, shape, dtype)\n            with lock:\n                fh.seek(offsets[0])\n                result = fh.read_array(typecode, product(shape), out=out)\n            if out is None and not result.dtype.isnative:\n                # swap byte order and dtype without copy\n                result.byteswap(True)\n                result = result.newbyteorder()\n            if lsb2msb:\n                reverse_bitorder(result)\n        else:\n            result = create_output(out, shape, dtype)\n            if self.planarconfig == 1:\n                runlen *= self.samplesperpixel\n            if bitspersample in (8, 16, 32, 64, 128):\n                if (bitspersample * runlen) % 8:\n                    raise ValueError(\'data and sample size mismatch\')\n\n                def unpack(x, typecode=typecode):\n                    if self.predictor == 3:  # PREDICTOR.FLOATINGPOINT\n                        # the floating point horizontal differencing decoder\n                        # needs the raw byte order\n                        typecode = dtype.char\n                    try:\n                        # read only numpy array\n                        return numpy.frombuffer(x, typecode)\n                    except ValueError:\n                        # strips may be missing EOI\n                        # warnings.warn(\'unpack: %s\' % e)\n                        xlen = ((len(x) // (bitspersample // 8)) *\n                                (bitspersample // 8))\n                        return numpy.frombuffer(x[:xlen], typecode)\n\n            elif isinstance(bitspersample, tuple):\n                def unpack(x):\n                    return unpack_rgb(x, typecode, bitspersample)\n            else:\n                def unpack(x):\n                    return unpack_ints(x, typecode, bitspersample, runlen)\n\n            decompress = TIFF.DECOMPESSORS[self.compression]\n            if self.compression == 7:  # COMPRESSION.JPEG\n                if \'JPEGTables\' in tags:\n                    table = tags[\'JPEGTables\'].value\n                else:\n                    table = b\'\'\n\n                def decompress(x):\n                    return decode_jpeg(x, table, self.photometric)\n\n            if istiled:\n                writable = None\n                tw, tl, td, pl = 0, 0, 0, 0\n                for tile in buffered_read(fh, lock, offsets, bytecounts):\n                    if lsb2msb:\n                        tile = reverse_bitorder(tile)\n                    tile = decompress(tile)\n                    tile = unpack(tile)\n                    try:\n                        tile.shape = tileshape\n                    except ValueError:\n                        # incomplete tiles; see gdal issue #1179\n                        warnings.warn(\'invalid tile data\')\n                        t = numpy.zeros(tileshape, dtype).reshape(-1)\n                        s = min(tile.size, t.size)\n                        t[:s] = tile[:s]\n                        tile = t.reshape(tileshape)\n                    if self.predictor == 2:  # PREDICTOR.HORIZONTAL\n                        if writable is None:\n                            writable = tile.flags[\'WRITEABLE\']\n                        if writable:\n                            numpy.cumsum(tile, axis=-2, dtype=dtype, out=tile)\n                        else:\n                            tile = numpy.cumsum(tile, axis=-2, dtype=dtype)\n                    elif self.predictor == 3:  # PREDICTOR.FLOATINGPOINT\n                        raise NotImplementedError()\n                    result[0, pl, td:td+tiledepth,\n                           tl:tl+tilelength, tw:tw+tilewidth, :] = tile\n                    del tile\n                    tw += tilewidth\n                    if tw >= shape[4]:\n                        tw, tl = 0, tl + tilelength\n                        if tl >= shape[3]:\n                            tl, td = 0, td + tiledepth\n                            if td >= shape[2]:\n                                td, pl = 0, pl + 1\n                result = result[...,\n                                :imagedepth, :imagelength, :imagewidth, :]\n            else:\n                strip_size = self.rowsperstrip * self.imagewidth\n                if self.planarconfig == 1:\n                    strip_size *= self.samplesperpixel\n                result = result.reshape(-1)\n                index = 0\n                for strip in buffered_read(fh, lock, offsets, bytecounts):\n                    if lsb2msb:\n                        strip = reverse_bitorder(strip)\n                    strip = decompress(strip)\n                    strip = unpack(strip)\n                    size = min(result.size, strip.size, strip_size,\n                               result.size - index)\n                    result[index:index+size] = strip[:size]\n                    del strip\n                    index += size\n\n        result.shape = self._shape\n\n        if self.predictor != 1 and not (istiled and not self.is_contiguous):\n            if self.parent.is_lsm and self.compression == 1:\n                pass  # work around bug in LSM510 software\n            elif self.predictor == 2:  # PREDICTOR.HORIZONTAL\n                numpy.cumsum(result, axis=-2, dtype=dtype, out=result)\n            elif self.predictor == 3:  # PREDICTOR.FLOATINGPOINT\n                result = decode_floats(result)\n\n        if squeeze:\n            try:\n                result.shape = self.shape\n            except ValueError:\n                warnings.warn(\'failed to reshape from %s to %s\' % (\n                    str(result.shape), str(self.shape)))\n\n        if closed:\n            # TODO: file should remain open if an exception occurred above\n            fh.close()\n        return result\n\n    def asrgb(self, uint8=False, alpha=None, colormap=None,\n              dmin=None, dmax=None, *args, **kwargs):\n        """"""Return image data as RGB(A).\n\n        Work in progress.\n\n        """"""\n        data = self.asarray(*args, **kwargs)\n        self = self.keyframe  # self or keyframe\n        photometric = self.photometric\n        PHOTOMETRIC = TIFF.PHOTOMETRIC\n\n        if photometric == PHOTOMETRIC.PALETTE:\n            colormap = self.colormap\n            if (colormap.shape[1] < 2**self.bitspersample or\n                    self.dtype.char not in \'BH\'):\n                raise ValueError(\'cannot apply colormap\')\n            if uint8:\n                if colormap.max() > 255:\n                    colormap >>= 8\n                colormap = colormap.astype(\'uint8\')\n            if \'S\' in self.axes:\n                data = data[..., 0] if self.planarconfig == 1 else data[0]\n            data = apply_colormap(data, colormap)\n\n        elif photometric == PHOTOMETRIC.RGB:\n            if \'ExtraSamples\' in self.tags:\n                if alpha is None:\n                    alpha = TIFF.EXTRASAMPLE\n                extrasamples = self.extrasamples\n                if self.tags[\'ExtraSamples\'].count == 1:\n                    extrasamples = (extrasamples,)\n                for i, exs in enumerate(extrasamples):\n                    if exs in alpha:\n                        if self.planarconfig == 1:\n                            data = data[..., [0, 1, 2, 3+i]]\n                        else:\n                            data = data[:, [0, 1, 2, 3+i]]\n                        break\n            else:\n                if self.planarconfig == 1:\n                    data = data[..., :3]\n                else:\n                    data = data[:, :3]\n            # TODO: convert to uint8?\n\n        elif photometric == PHOTOMETRIC.MINISBLACK:\n            raise NotImplementedError()\n        elif photometric == PHOTOMETRIC.MINISWHITE:\n            raise NotImplementedError()\n        elif photometric == PHOTOMETRIC.SEPARATED:\n            raise NotImplementedError()\n        else:\n            raise NotImplementedError()\n        return data\n\n    def aspage(self):\n        return self\n\n    @property\n    def keyframe(self):\n        return self\n\n    @keyframe.setter\n    def keyframe(self, index):\n        return\n\n    @lazyattr\n    def offsets_bytecounts(self):\n        """"""Return simplified offsets and bytecounts.""""""\n        if self.is_contiguous:\n            offset, byte_count = self.is_contiguous\n            return [offset], [byte_count]\n        return clean_offsets_counts(self.dataoffsets, self.databytecounts)\n\n    @lazyattr\n    def is_contiguous(self):\n        """"""Return offset and size of contiguous data, else None.\n\n        Excludes prediction and fill_order.\n\n        """"""\n        if (self.compression != 1\n                or self.bitspersample not in (8, 16, 32, 64)):\n            return\n        if \'TileWidth\' in self.tags:\n            if (self.imagewidth != self.tilewidth or\n                    self.imagelength % self.tilelength or\n                    self.tilewidth % 16 or self.tilelength % 16):\n                return\n            if (\'ImageDepth\' in self.tags and \'TileDepth\' in self.tags and\n                    (self.imagelength != self.tilelength or\n                     self.imagedepth % self.tiledepth)):\n                return\n\n        offsets = self.dataoffsets\n        bytecounts = self.databytecounts\n        if len(offsets) == 1:\n            return offsets[0], bytecounts[0]\n        if self.is_stk or all((offsets[i] + bytecounts[i] == offsets[i+1] or\n                               bytecounts[i+1] == 0)  # no data/ignore offset\n                              for i in range(len(offsets)-1)):\n            return offsets[0], sum(bytecounts)\n\n    @lazyattr\n    def is_final(self):\n        """"""Return if page\'s image data are stored in final form.\n\n        Excludes byte-swapping.\n\n        """"""\n        return (self.is_contiguous and self.fillorder == 1 and\n                self.predictor == 1 and not self.is_chroma_subsampled)\n\n    @lazyattr\n    def is_memmappable(self):\n        """"""Return if page\'s image data in file can be memory-mapped.""""""\n        return (self.parent.filehandle.is_file and self.is_final and\n                # (self.bitspersample == 8 or self.parent.isnative) and\n                self.is_contiguous[0] % self.dtype.itemsize == 0)  # aligned?\n\n    def __str__(self, detail=0, width=79):\n        """"""Return string containing information about page.""""""\n        if self.keyframe != self:\n            return TiffFrame.__str__(self, detail)\n        attr = \'\'\n        for name in (\'memmappable\', \'final\', \'contiguous\'):\n            attr = getattr(self, \'is_\'+name)\n            if attr:\n                attr = name.upper()\n                break\n        info = \'  \'.join(s for s in (\n            \'x\'.join(str(i) for i in self.shape),\n            \'%s%s\' % (TIFF.SAMPLEFORMAT(self.sampleformat).name,\n                      self.bitspersample),\n            \'|\'.join(i for i in (\n                TIFF.PHOTOMETRIC(self.photometric).name,\n                \'TILED\' if self.is_tiled else \'\',\n                self.compression.name if self.compression != 1 else \'\',\n                self.planarconfig.name if self.planarconfig != 1 else \'\',\n                self.predictor.name if self.predictor != 1 else \'\',\n                self.fillorder.name if self.fillorder != 1 else \'\')\n                     if i),\n            attr,\n            \'|\'.join((f.upper() for f in self.flags))\n            ) if s)\n        info = \'TiffPage %i @%i  %s\' % (self.index, self.offset, info)\n        if detail <= 0:\n            return info\n        info = [info]\n        tags = self.tags\n        tlines = []\n        vlines = []\n        for tag in sorted(tags.values(), key=lambda x: x.code):\n            value = tag.__str__(width=width+1)\n            tlines.append(value[:width].strip())\n            if detail > 1 and len(value) > width:\n                name = tag.name.upper()\n                if detail <= 2 and (\'COUNTS\' in name or \'OFFSETS\' in name):\n                    value = pformat(tag.value, width=width, height=detail*4)\n                else:\n                    value = pformat(tag.value, width=width, height=detail*12)\n                vlines.append(\'%s\\n%s\' % (tag.name, value))\n        info.append(\'\\n\'.join(tlines))\n        if detail > 1:\n            info.append(\'\\n\\n\'.join(vlines))\n        return \'\\n\\n\'.join(info)\n\n    @lazyattr\n    def flags(self):\n        """"""Return set of flags.""""""\n        return set((name.lower() for name in sorted(TIFF.FILE_FLAGS)\n                    if getattr(self, \'is_\' + name)))\n\n    @property\n    def ndim(self):\n        """"""Return number of array dimensions.""""""\n        return len(self.shape)\n\n    @property\n    def size(self):\n        """"""Return number of elements in array.""""""\n        return product(self.shape)\n\n    @lazyattr\n    def andor_tags(self):\n        """"""Return consolidated metadata from Andor tags as dict.\n\n        Remove Andor tags from self.tags.\n\n        """"""\n        if not self.is_andor:\n            return\n        tags = self.tags\n        result = {\'Id\': tags[\'AndorId\'].value}\n        for tag in list(self.tags.values()):\n            code = tag.code\n            if not 4864 < code < 5031:\n                continue\n            value = tag.value\n            name = tag.name[5:] if len(tag.name) > 5 else tag.name\n            result[name] = value\n            del tags[tag.name]\n        return result\n\n    @lazyattr\n    def epics_tags(self):\n        """"""Return consolidated metadata from EPICS areaDetector tags as dict.\n\n        Remove areaDetector tags from self.tags.\n\n        """"""\n        if not self.is_epics:\n            return\n        result = {}\n        tags = self.tags\n        for tag in list(self.tags.values()):\n            code = tag.code\n            if not 65000 <= code < 65500:\n                continue\n            value = tag.value\n            if code == 65000:\n                result[\'timeStamp\'] = datetime.datetime.fromtimestamp(\n                    float(value))\n            elif code == 65001:\n                result[\'uniqueID\'] = int(value)\n            elif code == 65002:\n                result[\'epicsTSSec\'] = int(value)\n            elif code == 65003:\n                result[\'epicsTSNsec\'] = int(value)\n            else:\n                key, value = value.split(\':\', 1)\n                result[key] = astype(value)\n            del tags[tag.name]\n        return result\n\n    @lazyattr\n    def geotiff_tags(self):\n        """"""Return consolidated metadata from GeoTIFF tags as dict.""""""\n        if not self.is_geotiff:\n            return\n        tags = self.tags\n\n        gkd = tags[\'GeoKeyDirectoryTag\'].value\n        if gkd[0] != 1:\n            warnings.warn(\'invalid GeoKeyDirectoryTag\')\n            return {}\n\n        result = {\n            \'KeyDirectoryVersion\': gkd[0],\n            \'KeyRevision\': gkd[1],\n            \'KeyRevisionMinor\': gkd[2],\n            # \'NumberOfKeys\': gkd[3],\n        }\n        # deltags = [\'GeoKeyDirectoryTag\']\n        geokeys = TIFF.GEO_KEYS\n        geocodes = TIFF.GEO_CODES\n        for index in range(gkd[3]):\n            keyid, tagid, count, offset = gkd[4 + index * 4: index * 4 + 8]\n            keyid = geokeys.get(keyid, keyid)\n            if tagid == 0:\n                value = offset\n            else:\n                tagname = TIFF.TAGS[tagid]\n                # deltags.append(tagname)\n                value = tags[tagname].value[offset: offset + count]\n                if tagid == 34737 and count > 1 and value[-1] == \'|\':\n                    value = value[:-1]\n                value = value if count > 1 else value[0]\n            if keyid in geocodes:\n                try:\n                    value = geocodes[keyid](value)\n                except Exception:\n                    pass\n            result[keyid] = value\n\n        if \'IntergraphMatrixTag\' in tags:\n            value = tags[\'IntergraphMatrixTag\'].value\n            value = numpy.array(value)\n            if len(value) == 16:\n                value = value.reshape((4, 4)).tolist()\n            result[\'IntergraphMatrix\'] = value\n        if \'ModelPixelScaleTag\' in tags:\n            value = numpy.array(tags[\'ModelPixelScaleTag\'].value).tolist()\n            result[\'ModelPixelScale\'] = value\n        if \'ModelTiepointTag\' in tags:\n            value = tags[\'ModelTiepointTag\'].value\n            value = numpy.array(value).reshape((-1, 6)).squeeze().tolist()\n            result[\'ModelTiepoint\'] = value\n        if \'ModelTransformationTag\' in tags:\n            value = tags[\'ModelTransformationTag\'].value\n            value = numpy.array(value).reshape((4, 4)).tolist()\n            result[\'ModelTransformation\'] = value\n        elif False:\n            # if \'ModelPixelScaleTag\' in tags and \'ModelTiepointTag\' in tags:\n            sx, sy, sz = tags[\'ModelPixelScaleTag\'].value\n            tiepoints = tags[\'ModelTiepointTag\'].value\n            transforms = []\n            for tp in range(0, len(tiepoints), 6):\n                i, j, k, x, y, z = tiepoints[tp:tp+6]\n                transforms.append([\n                    [sx,  0.0, 0.0, x - i * sx],\n                    [0.0, -sy, 0.0, y + j * sy],\n                    [0.0, 0.0,  sz, z - k * sz],\n                    [0.0, 0.0, 0.0, 1.0]])\n            if len(tiepoints) == 6:\n                transforms = transforms[0]\n            result[\'ModelTransformation\'] = transforms\n        return result\n\n    @property\n    def is_tiled(self):\n        """"""Page contains tiled image.""""""\n        return \'TileWidth\' in self.tags\n\n    @property\n    def is_reduced(self):\n        """"""Page is reduced image of another image.""""""\n        return (\'NewSubfileType\' in self.tags and\n                self.tags[\'NewSubfileType\'].value & 1)\n\n    @property\n    def is_chroma_subsampled(self):\n        """"""Page contains chroma subsampled image.""""""\n        return (\'YCbCrSubSampling\' in self.tags and\n                self.tags[\'YCbCrSubSampling\'].value != (1, 1))\n\n    @lazyattr\n    def is_imagej(self):\n        """"""Return ImageJ description if exists, else None.""""""\n        for description in (self.description, self.description1):\n            if not description:\n                return\n            if description[:7] == \'ImageJ=\':\n                return description\n\n    @lazyattr\n    def is_shaped(self):\n        """"""Return description containing array shape if exists, else None.""""""\n        for description in (self.description, self.description1):\n            if not description:\n                return\n            if description[:1] == \'{\' and \'""shape"":\' in description:\n                return description\n            if description[:6] == \'shape=\':\n                return description\n\n    @property\n    def is_mdgel(self):\n        """"""Page contains MDFileTag tag.""""""\n        return \'MDFileTag\' in self.tags\n\n    @property\n    def is_mediacy(self):\n        """"""Page contains Media Cybernetics Id tag.""""""\n        return (\'MC_Id\' in self.tags and\n                self.tags[\'MC_Id\'].value[:7] == b\'MC TIFF\')\n\n    @property\n    def is_stk(self):\n        """"""Page contains UIC2Tag tag.""""""\n        return \'UIC2tag\' in self.tags\n\n    @property\n    def is_lsm(self):\n        """"""Page contains CZ_LSMINFO tag.""""""\n        return \'CZ_LSMINFO\' in self.tags\n\n    @property\n    def is_fluoview(self):\n        """"""Page contains FluoView MM_STAMP tag.""""""\n        return \'MM_Stamp\' in self.tags\n\n    @property\n    def is_nih(self):\n        """"""Page contains NIH image header.""""""\n        return \'NIHImageHeader\' in self.tags\n\n    @property\n    def is_sgi(self):\n        """"""Page contains SGI image and tile depth tags.""""""\n        return \'ImageDepth\' in self.tags and \'TileDepth\' in self.tags\n\n    @property\n    def is_vista(self):\n        """"""Software tag is \'ISS Vista\'.""""""\n        return self.software == \'ISS Vista\'\n\n    @property\n    def is_metaseries(self):\n        """"""Page contains MDS MetaSeries metadata in ImageDescription tag.""""""\n        if self.index > 1 or self.software != \'MetaSeries\':\n            return False\n        d = self.description\n        return d.startswith(\'<MetaData>\') and d.endswith(\'</MetaData>\')\n\n    @property\n    def is_ome(self):\n        """"""Page contains OME-XML in ImageDescription tag.""""""\n        if self.index > 1 or not self.description:\n            return False\n        d = self.description\n        return d[:14] == \'<?xml version=\' and d[-6:] == \'</OME>\'\n\n    @property\n    def is_scn(self):\n        """"""Page contains Leica SCN XML in ImageDescription tag.""""""\n        if self.index > 1 or not self.description:\n            return False\n        d = self.description\n        return d[:14] == \'<?xml version=\' and d[-6:] == \'</scn>\'\n\n    @property\n    def is_micromanager(self):\n        """"""Page contains Micro-Manager metadata.""""""\n        return \'MicroManagerMetadata\' in self.tags\n\n    @property\n    def is_andor(self):\n        """"""Page contains Andor Technology tags.""""""\n        return \'AndorId\' in self.tags\n\n    @property\n    def is_pilatus(self):\n        """"""Page contains Pilatus tags.""""""\n        return (self.software[:8] == \'TVX TIFF\' and\n                self.description[:2] == \'# \')\n\n    @property\n    def is_epics(self):\n        """"""Page contains EPICS areaDetector tags.""""""\n        return (self.description == \'EPICS areaDetector\' or\n                self.software == \'EPICS areaDetector\')\n\n    @property\n    def is_tvips(self):\n        """"""Page contains TVIPS metadata.""""""\n        return \'TVIPS\' in self.tags\n\n    @property\n    def is_fei(self):\n        """"""Page contains SFEG or HELIOS metadata.""""""\n        return \'FEI_SFEG\' in self.tags or \'FEI_HELIOS\' in self.tags\n\n    @property\n    def is_sem(self):\n        """"""Page contains Zeiss SEM metadata.""""""\n        return \'CZ_SEM\' in self.tags\n\n    @property\n    def is_svs(self):\n        """"""Page contains Aperio metadata.""""""\n        return self.description[:20] == \'Aperio Image Library\'\n\n    @property\n    def is_scanimage(self):\n        """"""Page contains ScanImage metadata.""""""\n        return (self.description[:12] == \'state.config\' or\n                self.software[:22] == \'SI.LINE_FORMAT_VERSION\' or\n                \'scanimage.SI.\' in self.description[-256:])\n\n    @property\n    def is_qptiff(self):\n        """"""Page contains PerkinElmer tissue images metadata.""""""\n        # The ImageDescription tag contains XML with a top-level\n        # <PerkinElmer-QPI-ImageDescription> element\n        return self.software[:15] == \'PerkinElmer-QPI\'\n\n    @property\n    def is_geotiff(self):\n        """"""Page contains GeoTIFF metadata.""""""\n        return \'GeoKeyDirectoryTag\' in self.tags\n\n\nclass TiffFrame(object):\n    """"""Lightweight TIFF image file directory (IFD).\n\n    Only a limited number of tag values are read from file, e.g. StripOffsets,\n    and StripByteCounts. Other tag values are assumed to be identical with a\n    specified TiffPage instance, the keyframe.\n\n    TiffFrame is intended to reduce resource usage and speed up reading data\n    from file, not for introspection of metadata.\n\n    Not compatible with Python 2.\n\n    """"""\n    __slots__ = (\'keyframe\', \'parent\', \'index\', \'offset\',\n                 \'dataoffsets\', \'databytecounts\')\n\n    is_mdgel = False\n    tags = {}\n\n    def __init__(self, parent, index, keyframe):\n        """"""Read specified tags from file.\n\n        The file handle position must be at the offset to a valid IFD.\n\n        """"""\n        self.keyframe = keyframe\n        self.parent = parent\n        self.index = index\n\n        unpack = struct.unpack\n        fh = parent.filehandle\n        self.offset = fh.tell()\n        try:\n            tagno = unpack(parent.tagnoformat, fh.read(parent.tagnosize))[0]\n            if tagno > 4096:\n                raise ValueError(\'suspicious number of tags\')\n        except Exception:\n            raise ValueError(\'corrupted page list at offset %i\' % self.offset)\n\n        # tags = {}\n        tagcodes = {273, 279, 324, 325}  # TIFF.FRAME_TAGS\n        tagsize = parent.tagsize\n        codeformat = parent.tagformat1[:2]\n\n        data = fh.read(tagsize * tagno)\n        index = -tagsize\n        for _ in range(tagno):\n            index += tagsize\n            code = unpack(codeformat, data[index:index+2])[0]\n            if code not in tagcodes:\n                continue\n            try:\n                tag = TiffTag(parent, data[index:index+tagsize])\n            except TiffTag.Error as e:\n                warnings.warn(str(e))\n                continue\n            if code == 273 or code == 324:\n                setattr(self, \'dataoffsets\', tag.value)\n            elif code == 279 or code == 325:\n                setattr(self, \'databytecounts\', tag.value)\n            # elif code == 270:\n            #     tagname = tag.name\n            #     if tagname not in tags:\n            #         tags[tagname] = bytes2str(tag.value)\n            #     elif \'ImageDescription1\' not in tags:\n            #         tags[\'ImageDescription1\'] = bytes2str(tag.value)\n            # else:\n            #     tags[tag.name] = tag.value\n\n    def aspage(self):\n        """"""Return TiffPage from file.""""""\n        self.parent.filehandle.seek(self.offset)\n        return TiffPage(self.parent, index=self.index, keyframe=None)\n\n    def asarray(self, *args, **kwargs):\n        """"""Read image data from file and return as numpy array.""""""\n        # TODO: fix TypeError on Python 2\n        #   ""TypeError: unbound method asarray() must be called with TiffPage\n        #   instance as first argument (got TiffFrame instance instead)""\n        kwargs[\'validate\'] = False\n        return TiffPage.asarray(self, *args, **kwargs)\n\n    def asrgb(self, *args, **kwargs):\n        """"""Read image data from file and return RGB image as numpy array.""""""\n        kwargs[\'validate\'] = False\n        return TiffPage.asrgb(self, *args, **kwargs)\n\n    @property\n    def offsets_bytecounts(self):\n        """"""Return simplified offsets and bytecounts.""""""\n        if self.keyframe.is_contiguous:\n            return self.dataoffsets[:1], self.keyframe.is_contiguous[1:]\n        return clean_offsets_counts(self.dataoffsets, self.databytecounts)\n\n    @property\n    def is_contiguous(self):\n        """"""Return offset and size of contiguous data, else None.""""""\n        if self.keyframe.is_contiguous:\n            return self.dataoffsets[0], self.keyframe.is_contiguous[1]\n\n    @property\n    def is_memmappable(self):\n        """"""Return if page\'s image data in file can be memory-mapped.""""""\n        return self.keyframe.is_memmappable\n\n    def __getattr__(self, name):\n        """"""Return attribute from keyframe.""""""\n        if name in TIFF.FRAME_ATTRS:\n            return getattr(self.keyframe, name)\n        raise AttributeError(""\'%s\' object has no attribute \'%s\'"" %\n                             (self.__class__.__name__, name))\n\n    def __str__(self, detail=0):\n        """"""Return string containing information about frame.""""""\n        info = \'  \'.join(s for s in (\n            \'x\'.join(str(i) for i in self.shape),\n            str(self.dtype)))\n        return \'TiffFrame %i @%i  %s\' % (self.index, self.offset, info)\n\n\nclass TiffTag(object):\n    """"""TIFF tag structure.\n\n    Attributes\n    ----------\n    name : string\n        Name of tag.\n    code : int\n        Decimal code of tag.\n    dtype : str\n        Datatype of tag data. One of TIFF DATA_FORMATS.\n    count : int\n        Number of values.\n    value : various types\n        Tag data as Python object.\n    ImageSourceData : int\n        Location of value in file.\n\n    All attributes are read-only.\n\n    """"""\n    __slots__ = (\'code\', \'count\', \'dtype\', \'value\', \'valueoffset\')\n\n    class Error(Exception):\n        pass\n\n    def __init__(self, parent, tagheader, **kwargs):\n        """"""Initialize instance from tag header.""""""\n        fh = parent.filehandle\n        byteorder = parent.byteorder\n        unpack = struct.unpack\n        offsetsize = parent.offsetsize\n\n        self.valueoffset = fh.tell() + offsetsize + 4\n        code, type_ = unpack(parent.tagformat1, tagheader[:4])\n        count, value = unpack(parent.tagformat2, tagheader[4:])\n\n        try:\n            dtype = TIFF.DATA_FORMATS[type_]\n        except KeyError:\n            raise TiffTag.Error(\'unknown tag data type %i\' % type_)\n\n        fmt = \'%s%i%s\' % (byteorder, count * int(dtype[0]), dtype[1])\n        size = struct.calcsize(fmt)\n        if size > offsetsize or code in TIFF.TAG_READERS:\n            self.valueoffset = offset = unpack(parent.offsetformat, value)[0]\n            if offset < 8 or offset > fh.size - size:\n                raise TiffTag.Error(\'invalid tag value offset\')\n            # if offset % 2:\n            #     warnings.warn(\'tag value does not begin on word boundary\')\n            fh.seek(offset)\n            if code in TIFF.TAG_READERS:\n                readfunc = TIFF.TAG_READERS[code]\n                value = readfunc(fh, byteorder, dtype, count, offsetsize)\n            elif type_ == 7 or (count > 1 and dtype[-1] == \'B\'):\n                value = read_bytes(fh, byteorder, dtype, count, offsetsize)\n            elif code in TIFF.TAGS or dtype[-1] == \'s\':\n                value = unpack(fmt, fh.read(size))\n            else:\n                value = read_numpy(fh, byteorder, dtype, count, offsetsize)\n        elif dtype[-1] == \'B\' or type_ == 7:\n            value = value[:size]\n        else:\n            value = unpack(fmt, value[:size])\n\n        process = (code not in TIFF.TAG_READERS and code not in TIFF.TAG_TUPLE\n                   and type_ != 7)\n        if process and dtype[-1] == \'s\' and isinstance(value[0], bytes):\n            # TIFF ASCII fields can contain multiple strings,\n            #   each terminated with a NUL\n            value = value[0]\n            try:\n                value = bytes2str(stripascii(value).strip())\n            except UnicodeDecodeError:\n                warnings.warn(\'tag %i: coercing invalid ASCII to bytes\' % code)\n                dtype = \'1B\'\n        else:\n            if code in TIFF.TAG_ENUM:\n                t = TIFF.TAG_ENUM[code]\n                try:\n                    value = tuple(t(v) for v in value)\n                except ValueError as e:\n                    warnings.warn(str(e))\n            if process:\n                if len(value) == 1:\n                    value = value[0]\n\n        self.code = code\n        self.dtype = dtype\n        self.count = count\n        self.value = value\n\n    @property\n    def name(self):\n        return TIFF.TAGS.get(self.code, str(self.code))\n\n    def _fix_lsm_bitspersample(self, parent):\n        """"""Correct LSM bitspersample tag.\n\n        Old LSM writers may use a separate region for two 16-bit values,\n        although they fit into the tag value element of the tag.\n\n        """"""\n        if self.code == 258 and self.count == 2:\n            # TODO: test this case; need example file\n            warnings.warn(\'correcting LSM bitspersample tag\')\n            tof = parent.offsetformat[parent.offsetsize]\n            self.valueoffset = struct.unpack(tof, self._value)[0]\n            parent.filehandle.seek(self.valueoffset)\n            self.value = struct.unpack(\'<HH\', parent.filehandle.read(4))\n\n    def __str__(self, detail=0, width=79):\n        """"""Return string containing information about tag.""""""\n        height = 1 if detail <= 0 else 8 * detail\n        tcode = \'%i%s\' % (self.count * int(self.dtype[0]), self.dtype[1])\n        line = \'TiffTag %i %s  %s @%i  \' % (\n            self.code, self.name, tcode, self.valueoffset)[:width]\n\n        if self.code in TIFF.TAG_ENUM:\n            if self.count == 1:\n                value = TIFF.TAG_ENUM[self.code](self.value).name\n            else:\n                value = pformat(tuple(v.name for v in self.value))\n        else:\n            value = pformat(self.value, width=width, height=height)\n\n        if detail <= 0:\n            line += value\n            line = line[:width]\n        else:\n            line += \'\\n\' + value\n        return line\n\n\nclass TiffPageSeries(object):\n    """"""Series of TIFF pages with compatible shape and data type.\n\n    Attributes\n    ----------\n    pages : list of TiffPage\n        Sequence of TiffPages in series.\n    dtype : numpy.dtype\n        Data type (native byte order) of the image array in series.\n    shape : tuple\n        Dimensions of the image array in series.\n    axes : str\n        Labels of axes in shape. See TiffPage.axes.\n    offset : int or None\n        Position of image data in file if memory-mappable, else None.\n\n    """"""\n    def __init__(self, pages, shape, dtype, axes,\n                 parent=None, name=None, transform=None, stype=None):\n        """"""Initialize instance.""""""\n        self.index = 0\n        self._pages = pages  # might contain only first of contiguous pages\n        self.shape = tuple(shape)\n        self.axes = \'\'.join(axes)\n        self.dtype = numpy.dtype(dtype)\n        self.stype = stype if stype else \'\'\n        self.name = name if name else \'\'\n        self.transform = transform\n        if parent:\n            self.parent = parent\n        elif pages:\n            self.parent = pages[0].parent\n        else:\n            self.parent = None\n        if len(pages) == 1:\n            self._len = int(product(self.shape) // product(pages[0].shape))\n        else:\n            self._len = len(pages)\n\n    def asarray(self, out=None):\n        """"""Return image data from series of TIFF pages as numpy array.""""""\n        if self.parent:\n            result = self.parent.asarray(series=self, out=out)\n            if self.transform is not None:\n                result = self.transform(result)\n            return result\n\n    @lazyattr\n    def offset(self):\n        """"""Return offset to series data in file, if any.""""""\n        if not self._pages:\n            return\n\n        pos = 0\n        for page in self._pages:\n            if page is None:\n                return\n            if not page.is_final:\n                return\n            if not pos:\n                pos = page.is_contiguous[0] + page.is_contiguous[1]\n                continue\n            if pos != page.is_contiguous[0]:\n                return\n            pos += page.is_contiguous[1]\n\n        page = self._pages[0]\n        offset = page.is_contiguous[0]\n        if (page.is_imagej or page.is_shaped) and len(self._pages) == 1:\n            # truncated files\n            return offset\n        if pos == offset + product(self.shape) * self.dtype.itemsize:\n            return offset\n\n    @property\n    def ndim(self):\n        """"""Return number of array dimensions.""""""\n        return len(self.shape)\n\n    @property\n    def size(self):\n        """"""Return number of elements in array.""""""\n        return int(product(self.shape))\n\n    @property\n    def pages(self):\n        """"""Return sequence of all pages in series.""""""\n        # a workaround to keep the old interface working\n        return self\n\n    def __len__(self):\n        """"""Return number of TiffPages in series.""""""\n        return self._len\n\n    def __getitem__(self, key):\n        """"""Return specified TiffPage.""""""\n        if len(self._pages) == 1 and 0 < key < self._len:\n            index = self._pages[0].index\n            return self.parent.pages[index + key]\n        return self._pages[key]\n\n    def __iter__(self):\n        """"""Return iterator over TiffPages in series.""""""\n        if len(self._pages) == self._len:\n            for page in self._pages:\n                yield page\n        else:\n            pages = self.parent.pages\n            index = self._pages[0].index\n            for i in range(self._len):\n                yield pages[index + i]\n\n    def __str__(self):\n        """"""Return string with information about series.""""""\n        s = \'  \'.join(s for s in (\n            snipstr(""\'%s\'"" % self.name, 20) if self.name else \'\',\n            \'x\'.join(str(i) for i in self.shape),\n            str(self.dtype),\n            self.axes,\n            self.stype,\n            \'%i Pages\' % len(self.pages),\n            (\'Offset=%i\' % self.offset) if self.offset else \'\') if s)\n        return \'TiffPageSeries %i  %s\' % (self.index, s)\n\n\nclass TiffSequence(object):\n    """"""Sequence of TIFF files.\n\n    The image data in all files must match shape, dtype, etc.\n\n    Attributes\n    ----------\n    files : list\n        List of file names.\n    shape : tuple\n        Shape of image sequence. Excludes shape of image array.\n    axes : str\n        Labels of axes in shape.\n\n    Examples\n    --------\n    >>> # read image stack from sequence of TIFF files\n    >>> imsave(\'temp_C001T001.tif\', numpy.random.rand(64, 64))\n    >>> imsave(\'temp_C001T002.tif\', numpy.random.rand(64, 64))\n    >>> tifs = TiffSequence(\'temp_C001*.tif\')\n    >>> tifs.shape\n    (1, 2)\n    >>> tifs.axes\n    \'CT\'\n    >>> data = tifs.asarray()\n    >>> data.shape\n    (1, 2, 64, 64)\n\n    """"""\n    _patterns = {\n        \'axes\': r""""""\n            # matches Olympus OIF and Leica TIFF series\n            _?(?:(q|l|p|a|c|t|x|y|z|ch|tp)(\\d{1,4}))\n            _?(?:(q|l|p|a|c|t|x|y|z|ch|tp)(\\d{1,4}))?\n            _?(?:(q|l|p|a|c|t|x|y|z|ch|tp)(\\d{1,4}))?\n            _?(?:(q|l|p|a|c|t|x|y|z|ch|tp)(\\d{1,4}))?\n            _?(?:(q|l|p|a|c|t|x|y|z|ch|tp)(\\d{1,4}))?\n            _?(?:(q|l|p|a|c|t|x|y|z|ch|tp)(\\d{1,4}))?\n            _?(?:(q|l|p|a|c|t|x|y|z|ch|tp)(\\d{1,4}))?\n            """"""}\n\n    class ParseError(Exception):\n        pass\n\n    def __init__(self, files, imread=TiffFile, pattern=\'axes\',\n                 *args, **kwargs):\n        """"""Initialize instance from multiple files.\n\n        Parameters\n        ----------\n        files : str, or sequence of str\n            Glob pattern or sequence of file names.\n            Binary streams are not supported.\n        imread : function or class\n            Image read function or class with asarray function returning numpy\n            array from single file.\n        pattern : str\n            Regular expression pattern that matches axes names and sequence\n            indices in file names.\n            By default, the pattern matches Olympus OIF and Leica TIFF series.\n\n        """"""\n        if isinstance(files, basestring):\n            files = natural_sorted(glob.glob(files))\n        files = list(files)\n        if not files:\n            raise ValueError(\'no files found\')\n        if not isinstance(files[0], basestring):\n            raise ValueError(\'not a file name\')\n        self.files = files\n\n        if hasattr(imread, \'asarray\'):\n            # redefine imread\n            _imread = imread\n\n            def imread(fname, *args, **kwargs):\n                with _imread(fname) as im:\n                    return im.asarray(*args, **kwargs)\n\n        self.imread = imread\n\n        self.pattern = self._patterns.get(pattern, pattern)\n        try:\n            self._parse()\n            if not self.axes:\n                self.axes = \'I\'\n        except self.ParseError:\n            self.axes = \'I\'\n            self.shape = (len(files),)\n            self._startindex = (0,)\n            self._indices = tuple((i,) for i in range(len(files)))\n\n    def __str__(self):\n        """"""Return string with information about image sequence.""""""\n        return \'\\n\'.join([\n            self.files[0],\n            \' size: %i\' % len(self.files),\n            \' axes: %s\' % self.axes,\n            \' shape: %s\' % str(self.shape)])\n\n    def __len__(self):\n        return len(self.files)\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.close()\n\n    def close(self):\n        pass\n\n    def asarray(self, out=None, *args, **kwargs):\n        """"""Read image data from all files and return as numpy array.\n\n        The args and kwargs parameters are passed to the imread function.\n\n        Raise IndexError or ValueError if image shapes do not match.\n\n        """"""\n        im = self.imread(self.files[0], *args, **kwargs)\n        shape = self.shape + im.shape\n        result = create_output(out, shape, dtype=im.dtype)\n        result = result.reshape(-1, *im.shape)\n        for index, fname in zip(self._indices, self.files):\n            index = [i-j for i, j in zip(index, self._startindex)]\n            index = numpy.ravel_multi_index(index, self.shape)\n            im = self.imread(fname, *args, **kwargs)\n            result[index] = im\n        result.shape = shape\n        return result\n\n    def _parse(self):\n        """"""Get axes and shape from file names.""""""\n        if not self.pattern:\n            raise self.ParseError(\'invalid pattern\')\n        pattern = re.compile(self.pattern, re.IGNORECASE | re.VERBOSE)\n        matches = pattern.findall(self.files[0])\n        if not matches:\n            raise self.ParseError(\'pattern does not match file names\')\n        matches = matches[-1]\n        if len(matches) % 2:\n            raise self.ParseError(\'pattern does not match axis name and index\')\n        axes = \'\'.join(m for m in matches[::2] if m)\n        if not axes:\n            raise self.ParseError(\'pattern does not match file names\')\n\n        indices = []\n        for fname in self.files:\n            matches = pattern.findall(fname)[-1]\n            if axes != \'\'.join(m for m in matches[::2] if m):\n                raise ValueError(\'axes do not match within the image sequence\')\n            indices.append([int(m) for m in matches[1::2] if m])\n        shape = tuple(numpy.max(indices, axis=0))\n        startindex = tuple(numpy.min(indices, axis=0))\n        shape = tuple(i-j+1 for i, j in zip(shape, startindex))\n        if product(shape) != len(self.files):\n            warnings.warn(\'files are missing. Missing data are zeroed\')\n\n        self.axes = axes.upper()\n        self.shape = shape\n        self._indices = indices\n        self._startindex = startindex\n\n\nclass FileHandle(object):\n    """"""Binary file handle.\n\n    A limited, special purpose file handler that can:\n\n    * handle embedded files (for CZI within CZI files)\n    * re-open closed files (for multi file formats, such as OME-TIFF)\n    * read and write numpy arrays and records from file like objects\n\n    Only \'rb\' and \'wb\' modes are supported. Concurrently reading and writing\n    of the same stream is untested.\n\n    When initialized from another file handle, do not use it unless this\n    FileHandle is closed.\n\n    Attributes\n    ----------\n    name : str\n        Name of the file.\n    path : str\n        Absolute path to file.\n    size : int\n        Size of file in bytes.\n    is_file : bool\n        If True, file has a filno and can be memory-mapped.\n\n    All attributes are read-only.\n\n    """"""\n    __slots__ = (\'_fh\', \'_file\', \'_mode\', \'_name\', \'_dir\', \'_lock\',\n                 \'_offset\', \'_size\', \'_close\', \'is_file\')\n\n    def __init__(self, file, mode=\'rb\', name=None, offset=None, size=None):\n        """"""Initialize file handle from file name or another file handle.\n\n        Parameters\n        ----------\n        file : str, binary stream, or FileHandle\n            File name or seekable binary stream, such as an open file\n            or BytesIO.\n        mode : str\n            File open mode in case \'file\' is a file name. Must be \'rb\' or \'wb\'.\n        name : str\n            Optional name of file in case \'file\' is a binary stream.\n        offset : int\n            Optional start position of embedded file. By default, this is\n            the current file position.\n        size : int\n            Optional size of embedded file. By default, this is the number\n            of bytes from the \'offset\' to the end of the file.\n\n        """"""\n        self._fh = None\n        self._file = file\n        self._mode = mode\n        self._name = name\n        self._dir = \'\'\n        self._offset = offset\n        self._size = size\n        self._close = True\n        self.is_file = False\n        self._lock = NullContext()\n        self.open()\n\n    def open(self):\n        """"""Open or re-open file.""""""\n        if self._fh:\n            return  # file is open\n\n        if isinstance(self._file, basestring):\n            # file name\n            self._file = os.path.realpath(self._file)\n            self._dir, self._name = os.path.split(self._file)\n            self._fh = open(self._file, self._mode)\n            self._close = True\n            if self._offset is None:\n                self._offset = 0\n        elif isinstance(self._file, FileHandle):\n            # FileHandle\n            self._fh = self._file._fh\n            if self._offset is None:\n                self._offset = 0\n            self._offset += self._file._offset\n            self._close = False\n            if not self._name:\n                if self._offset:\n                    name, ext = os.path.splitext(self._file._name)\n                    self._name = \'%s@%i%s\' % (name, self._offset, ext)\n                else:\n                    self._name = self._file._name\n            if self._mode and self._mode != self._file._mode:\n                raise ValueError(\'FileHandle has wrong mode\')\n            self._mode = self._file._mode\n            self._dir = self._file._dir\n        elif hasattr(self._file, \'seek\'):\n            # binary stream: open file, BytesIO\n            try:\n                self._file.tell()\n            except Exception:\n                raise ValueError(\'binary stream is not seekable\')\n            self._fh = self._file\n            if self._offset is None:\n                self._offset = self._file.tell()\n            self._close = False\n            if not self._name:\n                try:\n                    self._dir, self._name = os.path.split(self._fh.name)\n                except AttributeError:\n                    self._name = \'Unnamed binary stream\'\n            try:\n                self._mode = self._fh.mode\n            except AttributeError:\n                pass\n        else:\n            raise ValueError(\'The first parameter must be a file name, \'\n                             \'seekable binary stream, or FileHandle\')\n\n        if self._offset:\n            self._fh.seek(self._offset)\n\n        if self._size is None:\n            pos = self._fh.tell()\n            self._fh.seek(self._offset, 2)\n            self._size = self._fh.tell()\n            self._fh.seek(pos)\n\n        try:\n            self._fh.fileno()\n            self.is_file = True\n        except Exception:\n            self.is_file = False\n\n    def read(self, size=-1):\n        """"""Read \'size\' bytes from file, or until EOF is reached.""""""\n        if size < 0 and self._offset:\n            size = self._size\n        return self._fh.read(size)\n\n    def write(self, bytestring):\n        """"""Write bytestring to file.""""""\n        return self._fh.write(bytestring)\n\n    def flush(self):\n        """"""Flush write buffers if applicable.""""""\n        return self._fh.flush()\n\n    def memmap_array(self, dtype, shape, offset=0, mode=\'r\', order=\'C\'):\n        """"""Return numpy.memmap of data stored in file.""""""\n        if not self.is_file:\n            raise ValueError(\'Cannot memory-map file without fileno\')\n        return numpy.memmap(self._fh, dtype=dtype, mode=mode,\n                            offset=self._offset + offset,\n                            shape=shape, order=order)\n\n    def read_array(self, dtype, count=-1, sep=\'\', chunksize=2**25, out=None,\n                   native=False):\n        """"""Return numpy array from file.\n\n        Work around numpy issue #2230, ""numpy.fromfile does not accept\n        StringIO object"" https://github.com/numpy/numpy/issues/2230.\n\n        """"""\n        fh = self._fh\n        dtype = numpy.dtype(dtype)\n        size = self._size if count < 0 else count * dtype.itemsize\n\n        if out is None:\n            try:\n                result = numpy.fromfile(fh, dtype, count, sep)\n            except IOError:\n                # ByteIO\n                data = fh.read(size)\n                result = numpy.frombuffer(data, dtype, count).copy()\n            if native and not result.dtype.isnative:\n                # swap byte order and dtype without copy\n                result.byteswap(True)\n                result = result.newbyteorder()\n            return result\n\n        # Read data from file in chunks and copy to output array\n        shape = out.shape\n        size = min(out.nbytes, size)\n        out = out.reshape(-1)\n        index = 0\n        while size > 0:\n            data = fh.read(min(chunksize, size))\n            datasize = len(data)\n            if datasize == 0:\n                break\n            size -= datasize\n            data = numpy.frombuffer(data, dtype)\n            out[index:index+data.size] = data\n            index += data.size\n\n        if hasattr(out, \'flush\'):\n            out.flush()\n        return out.reshape(shape)\n\n    def read_record(self, dtype, shape=1, byteorder=None):\n        """"""Return numpy record from file.""""""\n        rec = numpy.rec\n        try:\n            record = rec.fromfile(self._fh, dtype, shape, byteorder=byteorder)\n        except Exception:\n            dtype = numpy.dtype(dtype)\n            if shape is None:\n                shape = self._size // dtype.itemsize\n            size = product(sequence(shape)) * dtype.itemsize\n            data = self._fh.read(size)\n            record = rec.fromstring(data, dtype, shape, byteorder=byteorder)\n        return record[0] if shape == 1 else record\n\n    def write_empty(self, size):\n        """"""Append size bytes to file. Position must be at end of file.""""""\n        if size < 1:\n            return\n        self._fh.seek(size-1, 1)\n        self._fh.write(b\'\\x00\')\n\n    def write_array(self, data):\n        """"""Write numpy array to binary file.""""""\n        try:\n            data.tofile(self._fh)\n        except Exception:\n            # BytesIO\n            self._fh.write(data.tostring())\n\n    def tell(self):\n        """"""Return file\'s current position.""""""\n        return self._fh.tell() - self._offset\n\n    def seek(self, offset, whence=0):\n        """"""Set file\'s current position.""""""\n        if self._offset:\n            if whence == 0:\n                self._fh.seek(self._offset + offset, whence)\n                return\n            elif whence == 2 and self._size > 0:\n                self._fh.seek(self._offset + self._size + offset, 0)\n                return\n        self._fh.seek(offset, whence)\n\n    def close(self):\n        """"""Close file.""""""\n        if self._close and self._fh:\n            self._fh.close()\n            self._fh = None\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.close()\n\n    def __getattr__(self, name):\n        """"""Return attribute from underlying file object.""""""\n        if self._offset:\n            warnings.warn(\n                ""FileHandle: \'%s\' not implemented for embedded files"" % name)\n        return getattr(self._fh, name)\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def dirname(self):\n        return self._dir\n\n    @property\n    def path(self):\n        return os.path.join(self._dir, self._name)\n\n    @property\n    def size(self):\n        return self._size\n\n    @property\n    def closed(self):\n        return self._fh is None\n\n    @property\n    def lock(self):\n        return self._lock\n\n    @lock.setter\n    def lock(self, value):\n        self._lock = threading.RLock() if value else NullContext()\n\n\nclass NullContext(object):\n    """"""Null context manager.\n\n    >>> with NullContext():\n    ...     pass\n\n    """"""\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        pass\n\n\nclass OpenFileCache(object):\n    """"""Keep files open.""""""\n\n    __slots__ = (\'files\', \'past\', \'lock\', \'size\')\n\n    def __init__(self, size, lock=None):\n        """"""Initialize open file cache.""""""\n        self.past = []  # FIFO of opened files\n        self.files = {}  # refcounts of opened files\n        self.lock = NullContext() if lock is None else lock\n        self.size = int(size)\n\n    def open(self, filehandle):\n        """"""Re-open file if necessary.""""""\n        with self.lock:\n            if filehandle in self.files:\n                self.files[filehandle] += 1\n            elif filehandle.closed:\n                filehandle.open()\n                self.files[filehandle] = 1\n                self.past.append(filehandle)\n\n    def close(self, filehandle):\n        """"""Close openend file if no longer used.""""""\n        with self.lock:\n            if filehandle in self.files:\n                self.files[filehandle] -= 1\n                # trim the file cache\n                index = 0\n                size = len(self.past)\n                while size > self.size and index < size:\n                    filehandle = self.past[index]\n                    if self.files[filehandle] == 0:\n                        filehandle.close()\n                        del self.files[filehandle]\n                        del self.past[index]\n                        size -= 1\n                    else:\n                        index += 1\n\n    def clear(self):\n        """"""Close all opened files if not in use.""""""\n        with self.lock:\n            for filehandle, refcount in list(self.files.items()):\n                if refcount == 0:\n                    filehandle.close()\n                    del self.files[filehandle]\n                    del self.past[self.past.index(filehandle)]\n\n\nclass LazyConst(object):\n    """"""Class whose attributes are computed on first access from its methods.""""""\n    def __init__(self, cls):\n        self._cls = cls\n        self.__doc__ = getattr(cls, \'__doc__\')\n\n    def __getattr__(self, name):\n        func = getattr(self._cls, name)\n        if not callable(func):\n            return func\n        try:\n            value = func()\n        except TypeError:\n            # Python 2 unbound method\n            value = func.__func__()\n        setattr(self, name, value)\n        return value\n\n\n@LazyConst\nclass TIFF(object):\n    """"""Namespace for module constants.""""""\n\n    def TAGS():\n        # TIFF tag codes and names\n        return {\n            254: \'NewSubfileType\',\n            255: \'SubfileType\',\n            256: \'ImageWidth\',\n            257: \'ImageLength\',\n            258: \'BitsPerSample\',\n            259: \'Compression\',\n            262: \'PhotometricInterpretation\',\n            263: \'Threshholding\',\n            264: \'CellWidth\',\n            265: \'CellLength\',\n            266: \'FillOrder\',\n            269: \'DocumentName\',\n            270: \'ImageDescription\',\n            271: \'Make\',\n            272: \'Model\',\n            273: \'StripOffsets\',\n            274: \'Orientation\',\n            277: \'SamplesPerPixel\',\n            278: \'RowsPerStrip\',\n            279: \'StripByteCounts\',\n            280: \'MinSampleValue\',\n            281: \'MaxSampleValue\',\n            282: \'XResolution\',\n            283: \'YResolution\',\n            284: \'PlanarConfiguration\',\n            285: \'PageName\',\n            286: \'XPosition\',\n            287: \'YPosition\',\n            288: \'FreeOffsets\',\n            289: \'FreeByteCounts\',\n            290: \'GrayResponseUnit\',\n            291: \'GrayResponseCurve\',\n            292: \'T4Options\',\n            293: \'T6Options\',\n            296: \'ResolutionUnit\',\n            297: \'PageNumber\',\n            300: \'ColorResponseUnit\',\n            301: \'TransferFunction\',\n            305: \'Software\',\n            306: \'DateTime\',\n            315: \'Artist\',\n            316: \'HostComputer\',\n            317: \'Predictor\',\n            318: \'WhitePoint\',\n            319: \'PrimaryChromaticities\',\n            320: \'ColorMap\',\n            321: \'HalftoneHints\',\n            322: \'TileWidth\',\n            323: \'TileLength\',\n            324: \'TileOffsets\',\n            325: \'TileByteCounts\',\n            326: \'BadFaxLines\',\n            327: \'CleanFaxData\',\n            328: \'ConsecutiveBadFaxLines\',\n            330: \'SubIFDs\',\n            332: \'InkSet\',\n            333: \'InkNames\',\n            334: \'NumberOfInks\',\n            336: \'DotRange\',\n            337: \'TargetPrinter\',\n            338: \'ExtraSamples\',\n            339: \'SampleFormat\',\n            340: \'SMinSampleValue\',\n            341: \'SMaxSampleValue\',\n            342: \'TransferRange\',\n            343: \'ClipPath\',\n            344: \'XClipPathUnits\',\n            345: \'YClipPathUnits\',\n            346: \'Indexed\',\n            347: \'JPEGTables\',\n            351: \'OPIProxy\',\n            400: \'GlobalParametersIFD\',\n            401: \'ProfileType\',\n            402: \'FaxProfile\',\n            403: \'CodingMethods\',\n            404: \'VersionYear\',\n            405: \'ModeNumber\',\n            433: \'Decode\',\n            434: \'DefaultImageColor\',\n            435: \'T82Options\',\n            512: \'JPEGProc\',\n            513: \'JPEGInterchangeFormat\',\n            514: \'JPEGInterchangeFormatLength\',\n            515: \'JPEGRestartInterval\',\n            517: \'JPEGLosslessPredictors\',\n            518: \'JPEGPointTransforms\',\n            519: \'JPEGQTables\',\n            520: \'JPEGDCTables\',\n            521: \'JPEGACTables\',\n            529: \'YCbCrCoefficients\',\n            530: \'YCbCrSubSampling\',\n            531: \'YCbCrPositioning\',\n            532: \'ReferenceBlackWhite\',\n            559: \'StripRowCounts\',\n            700: \'XMP\',\n            4864: \'AndorId\',  # TODO: Andor Technology 4864 - 5030\n            4869: \'AndorTemperature\',\n            4876: \'AndorExposureTime\',\n            4878: \'AndorKineticCycleTime\',\n            4879: \'AndorAccumulations\',\n            4881: \'AndorAcquisitionCycleTime\',\n            4882: \'AndorReadoutTime\',\n            4884: \'AndorPhotonCounting\',\n            4885: \'AndorEmDacLevel\',\n            4890: \'AndorFrames\',\n            4896: \'AndorHorizontalFlip\',\n            4897: \'AndorVerticalFlip\',\n            4898: \'AndorClockwise\',\n            4899: \'AndorCounterClockwise\',\n            4904: \'AndorVerticalClockVoltage\',\n            4905: \'AndorVerticalShiftSpeed\',\n            4907: \'AndorPreAmpSetting\',\n            4908: \'AndorCameraSerial\',\n            4911: \'AndorActualTemperature\',\n            4912: \'AndorBaselineClamp\',\n            4913: \'AndorPrescans\',\n            4914: \'AndorModel\',\n            4915: \'AndorChipSizeX\',\n            4916: \'AndorChipSizeY\',\n            4944: \'AndorBaselineOffset\',\n            4966: \'AndorSoftwareVersion\',\n            # Private tags\n            32781: \'ImageID\',\n            32932: \'WangAnnotation\',\n            32995: \'Matteing\',\n            32996: \'DataType\',\n            32997: \'ImageDepth\',\n            32998: \'TileDepth\',\n            33300: \'ImageFullWidth\',\n            33301: \'ImageFullLength\',\n            33302: \'TextureFormat\',\n            33303: \'TextureWrapModes\',\n            33304: \'FieldOfViewCotangent\',\n            33305: \'MatrixWorldToScreen\',\n            33306: \'MatrixWorldToCamera\',\n            33421: \'CFARepeatPatternDim\',\n            33422: \'CFAPattern\',\n            33432: \'Copyright\',\n            33445: \'MDFileTag\',\n            33446: \'MDScalePixel\',\n            33447: \'MDColorTable\',\n            33448: \'MDLabName\',\n            33449: \'MDSampleInfo\',\n            33450: \'MDPrepDate\',\n            33451: \'MDPrepTime\',\n            33452: \'MDFileUnits\',\n            33550: \'ModelPixelScaleTag\',\n            33628: \'UIC1tag\',  # Metamorph  Universal Imaging Corp STK\n            33629: \'UIC2tag\',\n            33630: \'UIC3tag\',\n            33631: \'UIC4tag\',\n            33723: \'IPTC\',\n            33918: \'INGRPacketDataTag\',\n            33919: \'INGRFlagRegisters\',\n            33920: \'IntergraphMatrixTag\',  # IrasBTransformationMatrix\n            33922: \'ModelTiepointTag\',\n            33923: \'LeicaMagic\',\n            34118: \'CZ_SEM\',  # Zeiss SEM\n            34122: \'IPLAB\',  # number of images\n            34264: \'ModelTransformationTag\',\n            34361: \'MM_Header\',\n            34362: \'MM_Stamp\',\n            34363: \'MM_Unknown\',\n            34377: \'Photoshop\',\n            34386: \'MM_UserBlock\',\n            34412: \'CZ_LSMINFO\',\n            34665: \'ExifIFD\',\n            34675: \'ICCProfile\',\n            34680: \'FEI_SFEG\',  #\n            34682: \'FEI_HELIOS\',  #\n            34683: \'FEI_TITAN\',  #\n            34710: \'MarCCD\',  # offset to MarCCD header\n            34732: \'ImageLayer\',\n            34735: \'GeoKeyDirectoryTag\',\n            34736: \'GeoDoubleParamsTag\',\n            34737: \'GeoAsciiParamsTag\',\n            34853: \'GPSIFD\',\n            34908: \'HylaFAXFaxRecvParams\',\n            34909: \'HylaFAXFaxSubAddress\',\n            34910: \'HylaFAXFaxRecvTime\',\n            34911: \'FaxDcs\',\n            # 36864: \'TVX ?\',  # TODO: Pilatus/CHESS/TV6 36864 .. 37120\n            # 36865: \'TVX_NumExposure\',\n            # 36866: \'TVX_NumBackground\',\n            # 36867: \'TVX_ExposureTime\',\n            # 36868: \'TVX_BackgroundTime\',\n            # 36870: \'TVX ?\',\n            # 36873: \'TVX_SubBpp\',\n            # 36874: \'TVX_SubWide\',\n            # 36875: \'TVX_SubHigh\',\n            # 36876: \'TVX_BlackLevel\',\n            # 36877: \'TVX_DarkCurrent\',\n            # 36878: \'TVX_ReadNoise\',\n            # 36879: \'TVX_DarkCurrentNoise\',\n            # 36880: \'TVX_BeamMonitor\',\n            # 37120: \'TVX_UserVariables\',  # A/D values\n            37439: \'StoNits\',\n            37679: \'MODIText\',  # Microsoft Office Document Imaging\n            37680: \'MODIOLEPropertySetStorage\',\n            37681: \'MODIPositioning\',\n            37706: \'TVIPS\',  # offset to TemData structure\n            37707: \'TVIPS1\',\n            37708: \'TVIPS2\',  # same TemData structure as undefined\n            37724: \'ImageSourceData\',\n            40001: \'MC_IpWinScal\',  # Media Cybernetics\n            40100: \'MC_IdOld\',\n            40965: \'InteroperabilityIFD\',\n            42112: \'GDAL_METADATA\',\n            42113: \'GDAL_NODATA\',\n            43314: \'NIHImageHeader\',\n            50215: \'OceScanjobDescription\',\n            50216: \'OceApplicationSelector\',\n            50217: \'OceIdentificationNumber\',\n            50218: \'OceImageLogicCharacteristics\',\n            50288: \'MC_Id\',  # Media Cybernetics\n            50289: \'MC_XYPosition\',\n            50290: \'MC_ZPosition\',\n            50291: \'MC_XYCalibration\',\n            50292: \'MC_LensCharacteristics\',\n            50293: \'MC_ChannelName\',\n            50294: \'MC_ExcitationWavelength\',\n            50295: \'MC_TimeStamp\',\n            50296: \'MC_FrameProperties\',\n            50341: \'EpsonPrintImageMatching\',\n            50495: \'PCO_RAW\',  # TODO: PCO CamWare\n            50560: \'USPTO_OriginalContentType\',  # US Patent Office\n            50561: \'USPTO_RotationCode\',\n            50706: \'DNGVersion\',  # DNG 50706 .. 51112\n            50707: \'DNGBackwardVersion\',\n            50708: \'UniqueCameraModel\',\n            50709: \'LocalizedCameraModel\',\n            50710: \'CFAPlaneColor\',\n            50711: \'CFALayout\',\n            50712: \'LinearizationTable\',\n            50713: \'BlackLevelRepeatDim\',\n            50714: \'BlackLevel\',\n            50715: \'BlackLevelDeltaH\',\n            50716: \'BlackLevelDeltaV\',\n            50717: \'WhiteLevel\',\n            50718: \'DefaultScale\',\n            50719: \'DefaultCropOrigin\',\n            50720: \'DefaultCropSize\',\n            50721: \'ColorMatrix1\',\n            50722: \'ColorMatrix2\',\n            50723: \'CameraCalibration1\',\n            50724: \'CameraCalibration2\',\n            50725: \'ReductionMatrix1\',\n            50726: \'ReductionMatrix2\',\n            50727: \'AnalogBalance\',\n            50728: \'AsShotNeutral\',\n            50729: \'AsShotWhiteXY\',\n            50730: \'BaselineExposure\',\n            50731: \'BaselineNoise\',\n            50732: \'BaselineSharpness\',\n            50733: \'BayerGreenSplit\',\n            50734: \'LinearResponseLimit\',\n            50735: \'CameraSerialNumber\',\n            50736: \'LensInfo\',\n            50737: \'ChromaBlurRadius\',\n            50738: \'AntiAliasStrength\',\n            50739: \'ShadowScale\',\n            50740: \'DNGPrivateData\',\n            50741: \'MakerNoteSafety\',\n            50778: \'CalibrationIlluminant1\',\n            50779: \'CalibrationIlluminant2\',\n            50780: \'BestQualityScale\',\n            50781: \'RawDataUniqueID\',\n            50784: \'AliasLayerMetadata\',\n            50827: \'OriginalRawFileName\',\n            50828: \'OriginalRawFileData\',\n            50829: \'ActiveArea\',\n            50830: \'MaskedAreas\',\n            50831: \'AsShotICCProfile\',\n            50832: \'AsShotPreProfileMatrix\',\n            50833: \'CurrentICCProfile\',\n            50834: \'CurrentPreProfileMatrix\',\n            50838: \'IJMetadataByteCounts\',\n            50839: \'IJMetadata\',\n            51023: \'FibicsXML\',  #\n            51123: \'MicroManagerMetadata\',\n            # 65000: \'DimapDocument\',  # Dimap_Document XML\n            65200: \'FlexXML\',  #\n            65563: \'PerSample\',\n        }\n\n    def TAG_NAMES():\n        return {v: c for c, v in TIFF.TAGS.items()}\n\n    def TAG_READERS():\n        # Map TIFF tag codes to import functions\n        return {\n            320: read_colormap,\n            # 700: read_bytes,  # read_utf8,\n            # 34377: read_bytes,\n            33723: read_bytes,\n            # 34675: read_bytes,\n            33628: read_uic1tag,  # Universal Imaging Corp STK\n            33629: read_uic2tag,\n            33630: read_uic3tag,\n            33631: read_uic4tag,\n            34118: read_cz_sem,  # Carl Zeiss SEM\n            34361: read_mm_header,  # Olympus FluoView\n            34362: read_mm_stamp,\n            34363: read_numpy,  # MM_Unknown\n            34386: read_numpy,  # MM_UserBlock\n            34412: read_cz_lsminfo,  # Carl Zeiss LSM\n            34680: read_fei_metadata,  # S-FEG\n            34682: read_fei_metadata,  # Helios NanoLab\n            37706: read_tvips_header,  # TVIPS EMMENU\n            37724: read_bytes,  # ImageSourceData\n            33923: read_bytes,  # read_leica_magic\n            43314: read_nih_image_header,\n            # 40001: read_bytes,\n            40100: read_bytes,\n            50288: read_bytes,\n            50296: read_bytes,\n            50839: read_bytes,\n            51123: read_json,\n            34665: read_exif_ifd,\n            34853: read_gps_ifd,\n            40965: read_interoperability_ifd,\n        }\n\n    def TAG_TUPLE():\n        # Tags whose values must be stored as tuples\n        return frozenset((273, 279, 324, 325, 530, 531, 34736))\n\n    def TAG_ATTRIBUTES():\n        #  Map tag codes to TiffPage attribute names\n        return {\n            \'ImageWidth\': \'imagewidth\',\n            \'ImageLength\': \'imagelength\',\n            \'BitsPerSample\': \'bitspersample\',\n            \'Compression\': \'compression\',\n            \'PlanarConfiguration\': \'planarconfig\',\n            \'FillOrder\': \'fillorder\',\n            \'PhotometricInterpretation\': \'photometric\',\n            \'ColorMap\': \'colormap\',\n            \'ImageDescription\': \'description\',\n            \'ImageDescription1\': \'description1\',\n            \'SamplesPerPixel\': \'samplesperpixel\',\n            \'RowsPerStrip\': \'rowsperstrip\',\n            \'Software\': \'software\',\n            \'Predictor\': \'predictor\',\n            \'TileWidth\': \'tilewidth\',\n            \'TileLength\': \'tilelength\',\n            \'ExtraSamples\': \'extrasamples\',\n            \'SampleFormat\': \'sampleformat\',\n            \'ImageDepth\': \'imagedepth\',\n            \'TileDepth\': \'tiledepth\',\n        }\n\n    def TAG_ENUM():\n        return {\n            # 254: TIFF.FILETYPE,\n            255: TIFF.OFILETYPE,\n            259: TIFF.COMPRESSION,\n            262: TIFF.PHOTOMETRIC,\n            263: TIFF.THRESHHOLD,\n            266: TIFF.FILLORDER,\n            274: TIFF.ORIENTATION,\n            284: TIFF.PLANARCONFIG,\n            290: TIFF.GRAYRESPONSEUNIT,\n            # 292: TIFF.GROUP3OPT,\n            # 293: TIFF.GROUP4OPT,\n            296: TIFF.RESUNIT,\n            300: TIFF.COLORRESPONSEUNIT,\n            317: TIFF.PREDICTOR,\n            338: TIFF.EXTRASAMPLE,\n            339: TIFF.SAMPLEFORMAT,\n            # 512: TIFF.JPEGPROC,\n            # 531: TIFF.YCBCRPOSITION,\n        }\n\n    def FILETYPE():\n        class FILETYPE(enum.IntFlag):\n            # Python 3.6 only\n            UNDEFINED = 0\n            REDUCEDIMAGE = 1\n            PAGE = 2\n            MASK = 4\n        return FILETYPE\n\n    def OFILETYPE():\n        class OFILETYPE(enum.IntEnum):\n            UNDEFINED = 0\n            IMAGE = 1\n            REDUCEDIMAGE = 2\n            PAGE = 3\n        return OFILETYPE\n\n    def COMPRESSION():\n        class COMPRESSION(enum.IntEnum):\n            NONE = 1  # Uncompressed\n            CCITTRLE = 2  # CCITT 1D\n            CCITT_T4 = 3  # \'T4/Group 3 Fax\',\n            CCITT_T6 = 4  # \'T6/Group 4 Fax\',\n            LZW = 5\n            OJPEG = 6  # old-style JPEG\n            JPEG = 7\n            ADOBE_DEFLATE = 8\n            JBIG_BW = 9\n            JBIG_COLOR = 10\n            JPEG_99 = 99\n            KODAK_262 = 262\n            NEXT = 32766\n            SONY_ARW = 32767\n            PACKED_RAW = 32769\n            SAMSUNG_SRW = 32770\n            CCIRLEW = 32771\n            SAMSUNG_SRW2 = 32772\n            PACKBITS = 32773\n            THUNDERSCAN = 32809\n            IT8CTPAD = 32895\n            IT8LW = 32896\n            IT8MP = 32897\n            IT8BL = 32898\n            PIXARFILM = 32908\n            PIXARLOG = 32909\n            DEFLATE = 32946\n            DCS = 32947\n            APERIO_JP2000_YCBC = 33003  # Leica Aperio\n            APERIO_JP2000_RGB = 33005  # Leica Aperio\n            JBIG = 34661\n            SGILOG = 34676\n            SGILOG24 = 34677\n            JPEG2000 = 34712\n            NIKON_NEF = 34713\n            JBIG2 = 34715\n            MDI_BINARY = 34718  # \'Microsoft Document Imaging\n            MDI_PROGRESSIVE = 34719  # \'Microsoft Document Imaging\n            MDI_VECTOR = 34720  # \'Microsoft Document Imaging\n            JPEG_LOSSY = 34892\n            LZMA = 34925\n            ZSTD = 34926\n            OPS_PNG = 34933  # Objective Pathology Services\n            OPS_JPEGXR = 34934  # Objective Pathology Services\n            KODAK_DCR = 65000\n            PENTAX_PEF = 65535\n            # def __bool__(self): return self != 1  # Python 3.6 only\n        return COMPRESSION\n\n    def PHOTOMETRIC():\n        class PHOTOMETRIC(enum.IntEnum):\n            MINISWHITE = 0\n            MINISBLACK = 1\n            RGB = 2\n            PALETTE = 3\n            MASK = 4\n            SEPARATED = 5  # CMYK\n            YCBCR = 6\n            CIELAB = 8\n            ICCLAB = 9\n            ITULAB = 10\n            CFA = 32803  # Color Filter Array\n            LOGL = 32844\n            LOGLUV = 32845\n            LINEAR_RAW = 34892\n        return PHOTOMETRIC\n\n    def THRESHHOLD():\n        class THRESHHOLD(enum.IntEnum):\n            BILEVEL = 1\n            HALFTONE = 2\n            ERRORDIFFUSE = 3\n        return THRESHHOLD\n\n    def FILLORDER():\n        class FILLORDER(enum.IntEnum):\n            MSB2LSB = 1\n            LSB2MSB = 2\n        return FILLORDER\n\n    def ORIENTATION():\n        class ORIENTATION(enum.IntEnum):\n            TOPLEFT = 1\n            TOPRIGHT = 2\n            BOTRIGHT = 3\n            BOTLEFT = 4\n            LEFTTOP = 5\n            RIGHTTOP = 6\n            RIGHTBOT = 7\n            LEFTBOT = 8\n        return ORIENTATION\n\n    def PLANARCONFIG():\n        class PLANARCONFIG(enum.IntEnum):\n            CONTIG = 1\n            SEPARATE = 2\n        return PLANARCONFIG\n\n    def GRAYRESPONSEUNIT():\n        class GRAYRESPONSEUNIT(enum.IntEnum):\n            _10S = 1\n            _100S = 2\n            _1000S = 3\n            _10000S = 4\n            _100000S = 5\n        return GRAYRESPONSEUNIT\n\n    def GROUP4OPT():\n        class GROUP4OPT(enum.IntEnum):\n            UNCOMPRESSED = 2\n        return GROUP4OPT\n\n    def RESUNIT():\n        class RESUNIT(enum.IntEnum):\n            NONE = 1\n            INCH = 2\n            CENTIMETER = 3\n            # def __bool__(self): return self != 1  # Python 3.6 only\n        return RESUNIT\n\n    def COLORRESPONSEUNIT():\n        class COLORRESPONSEUNIT(enum.IntEnum):\n            _10S = 1\n            _100S = 2\n            _1000S = 3\n            _10000S = 4\n            _100000S = 5\n        return COLORRESPONSEUNIT\n\n    def PREDICTOR():\n        class PREDICTOR(enum.IntEnum):\n            NONE = 1\n            HORIZONTAL = 2\n            FLOATINGPOINT = 3\n            # def __bool__(self): return self != 1  # Python 3.6 only\n        return PREDICTOR\n\n    def EXTRASAMPLE():\n        class EXTRASAMPLE(enum.IntEnum):\n            UNSPECIFIED = 0\n            ASSOCALPHA = 1\n            UNASSALPHA = 2\n        return EXTRASAMPLE\n\n    def SAMPLEFORMAT():\n        class SAMPLEFORMAT(enum.IntEnum):\n            UINT = 1\n            INT = 2\n            IEEEFP = 3\n            VOID = 4\n            COMPLEXINT = 5\n            COMPLEXIEEEFP = 6\n        return SAMPLEFORMAT\n\n    def DATATYPES():\n        class DATATYPES(enum.IntEnum):\n            NOTYPE = 0\n            BYTE = 1\n            ASCII = 2\n            SHORT = 3\n            LONG = 4\n            RATIONAL = 5\n            SBYTE = 6\n            UNDEFINED = 7\n            SSHORT = 8\n            SLONG = 9\n            SRATIONAL = 10\n            FLOAT = 11\n            DOUBLE = 12\n            IFD = 13\n            UNICODE = 14\n            COMPLEX = 15\n            LONG8 = 16\n            SLONG8 = 17\n            IFD8 = 18\n        return DATATYPES\n\n    def DATA_FORMATS():\n        # Map TIFF DATATYPES to Python struct formats\n        return {\n            1: \'1B\',   # BYTE 8-bit unsigned integer.\n            2: \'1s\',   # ASCII 8-bit byte that contains a 7-bit ASCII code;\n                       #   the last byte must be NULL (binary zero).\n            3: \'1H\',   # SHORT 16-bit (2-byte) unsigned integer\n            4: \'1I\',   # LONG 32-bit (4-byte) unsigned integer.\n            5: \'2I\',   # RATIONAL Two LONGs: the first represents the numerator\n                       #   of a fraction; the second, the denominator.\n            6: \'1b\',   # SBYTE An 8-bit signed (twos-complement) integer.\n            7: \'1B\',   # UNDEFINED An 8-bit byte that may contain anything,\n                       #   depending on the definition of the field.\n            8: \'1h\',   # SSHORT A 16-bit (2-byte) signed (twos-complement)\n                       #   integer.\n            9: \'1i\',   # SLONG A 32-bit (4-byte) signed (twos-complement)\n                       #   integer.\n            10: \'2i\',  # SRATIONAL Two SLONGs: the first represents the\n                       #   numerator of a fraction, the second the denominator.\n            11: \'1f\',  # FLOAT Single precision (4-byte) IEEE format.\n            12: \'1d\',  # DOUBLE Double precision (8-byte) IEEE format.\n            13: \'1I\',  # IFD unsigned 4 byte IFD offset.\n            # 14: \'\',  # UNICODE\n            # 15: \'\',  # COMPLEX\n            16: \'1Q\',  # LONG8 unsigned 8 byte integer (BigTiff)\n            17: \'1q\',  # SLONG8 signed 8 byte integer (BigTiff)\n            18: \'1Q\',  # IFD8 unsigned 8 byte IFD offset (BigTiff)\n        }\n\n    def DATA_DTYPES():\n        # Map numpy dtypes to TIFF DATATYPES\n        return {\'B\': 1, \'s\': 2, \'H\': 3, \'I\': 4, \'2I\': 5, \'b\': 6,\n                \'h\': 8, \'i\': 9, \'2i\': 10, \'f\': 11, \'d\': 12, \'Q\': 16, \'q\': 17}\n\n    def SAMPLE_DTYPES():\n        # Map TIFF SampleFormats and BitsPerSample to numpy dtype\n        return {\n            (1, 1): \'?\',  # bitmap\n            (1, 2): \'B\',\n            (1, 3): \'B\',\n            (1, 4): \'B\',\n            (1, 5): \'B\',\n            (1, 6): \'B\',\n            (1, 7): \'B\',\n            (1, 8): \'B\',\n            (1, 9): \'H\',\n            (1, 10): \'H\',\n            (1, 11): \'H\',\n            (1, 12): \'H\',\n            (1, 13): \'H\',\n            (1, 14): \'H\',\n            (1, 15): \'H\',\n            (1, 16): \'H\',\n            (1, 17): \'I\',\n            (1, 18): \'I\',\n            (1, 19): \'I\',\n            (1, 20): \'I\',\n            (1, 21): \'I\',\n            (1, 22): \'I\',\n            (1, 23): \'I\',\n            (1, 24): \'I\',\n            (1, 25): \'I\',\n            (1, 26): \'I\',\n            (1, 27): \'I\',\n            (1, 28): \'I\',\n            (1, 29): \'I\',\n            (1, 30): \'I\',\n            (1, 31): \'I\',\n            (1, 32): \'I\',\n            (1, 64): \'Q\',\n            (2, 8): \'b\',\n            (2, 16): \'h\',\n            (2, 32): \'i\',\n            (2, 64): \'q\',\n            (3, 16): \'e\',\n            (3, 32): \'f\',\n            (3, 64): \'d\',\n            (6, 64): \'F\',\n            (6, 128): \'D\',\n            (1, (5, 6, 5)): \'B\',\n        }\n\n    def COMPESSORS():\n        # Map COMPRESSION to compress functions and default compression levels\n        compressors = {\n            8: (zlib.compress, 6),\n            32946: (zlib.compress, 6),\n        }\n        # TODO: import lzma and zstd on demand\n        try:\n            try:\n                import lzma  # delayed import\n            except ImportError:\n                import backports.lzma as lzma  # delayed import\n            compressors[34925] = (lambda x, y: lzma.compress(x)), 0\n        except ImportError:\n            pass\n        try:\n            import zstd  # delayed import\n            compressors[34926] = zstd.compress, 9\n        except ImportError:\n            pass\n        return compressors\n\n    def DECOMPESSORS():\n        # Map COMPRESSION to decompress functions\n        decompressors = {\n            None: identityfunc,\n            1: identityfunc,\n            5: decode_lzw,\n            # 7: decode_jpeg,\n            8: zlib.decompress,\n            32946: zlib.decompress,\n            32773: decode_packbits,\n        }\n        # TODO: import lzma and zstd on demand\n        try:\n            try:\n                import lzma  # delayed import\n            except ImportError:\n                import backports.lzma as lzma  # delayed import\n            decompressors[34925] = lzma.decompress\n        except ImportError:\n            pass\n        try:\n            import zstd  # delayed import\n            decompressors[34926] = zstd.decompress\n        except ImportError:\n            pass\n        return decompressors\n\n    def FRAME_ATTRS():\n        # Attributes that a TiffFrame shares with its keyframe\n        return set(\'shape ndim size dtype axes is_final\'.split())\n\n    def FILE_FLAGS():\n        # TiffFile and TiffPage \'is_\\*\' attributes\n        exclude = set(\'reduced final memmappable contiguous \'\n                      \'chroma_subsampled\'.split())\n        return set(a[3:] for a in dir(TiffPage)\n                   if a[:3] == \'is_\' and a[3:] not in exclude)\n\n    def FILE_EXTENSIONS():\n        # TIFF file extensions\n        return tuple(\'tif tiff ome.tif lsm stk qptiff pcoraw \'\n                     \'gel seq svs bif tf8 tf2 btf\'.split())\n\n    def FILEOPEN_FILTER():\n        # String for use in Windows File Open box\n        return [(\'%s files\' % ext.upper(), \'*.%s\' % ext)\n                for ext in TIFF.FILE_EXTENSIONS] + [(\'allfiles\', \'*\')]\n\n    def AXES_LABELS():\n        # TODO: is there a standard for character axes labels?\n        axes = {\n            \'X\': \'width\',\n            \'Y\': \'height\',\n            \'Z\': \'depth\',\n            \'S\': \'sample\',  # rgb(a)\n            \'I\': \'series\',  # general sequence, plane, page, IFD\n            \'T\': \'time\',\n            \'C\': \'channel\',  # color, emission wavelength\n            \'A\': \'angle\',\n            \'P\': \'phase\',  # formerly F    # P is Position in LSM!\n            \'R\': \'tile\',  # region, point, mosaic\n            \'H\': \'lifetime\',  # histogram\n            \'E\': \'lambda\',  # excitation wavelength\n            \'L\': \'exposure\',  # lux\n            \'V\': \'event\',\n            \'Q\': \'other\',\n            \'M\': \'mosaic\',  # LSM 6\n        }\n        axes.update(dict((v, k) for k, v in axes.items()))\n        return axes\n\n    def ANDOR_TAGS():\n        # Andor Technology tags #4864 - 5030\n        return set(range(4864, 5030))\n\n    def EXIF_TAGS():\n        return {\n            33434: \'ExposureTime\',\n            33437: \'FNumber\',\n            34850: \'ExposureProgram\',\n            34852: \'SpectralSensitivity\',\n            34855: \'ISOSpeedRatings\',\n            34856: \'OECF\',\n            34858: \'TimeZoneOffset\',\n            34859: \'SelfTimerMode\',\n            34864: \'SensitivityType\',\n            34865: \'StandardOutputSensitivity\',\n            34866: \'RecommendedExposureIndex\',\n            34867: \'ISOSpeed\',\n            34868: \'ISOSpeedLatitudeyyy\',\n            34869: \'ISOSpeedLatitudezzz\',\n            36864: \'ExifVersion\',\n            36867: \'DateTimeOriginal\',\n            36868: \'DateTimeDigitized\',\n            36873: \'GooglePlusUploadCode\',\n            36880: \'OffsetTime\',\n            36881: \'OffsetTimeOriginal\',\n            36882: \'OffsetTimeDigitized\',\n            37121: \'ComponentsConfiguration\',\n            37122: \'CompressedBitsPerPixel\',\n            37377: \'ShutterSpeedValue\',\n            37378: \'ApertureValue\',\n            37379: \'BrightnessValue\',\n            37380: \'ExposureBiasValue\',\n            37381: \'MaxApertureValue\',\n            37382: \'SubjectDistance\',\n            37383: \'MeteringMode\',\n            37384: \'LightSource\',\n            37385: \'Flash\',\n            37386: \'FocalLength\',\n            37390: \'FocalPlaneXResolution\',\n            37391: \'FocalPlaneYResolution\',\n            37392: \'FocalPlaneResolutionUnit\',\n            37393: \'ImageNumber\',\n            37394: \'SecurityClassification\',\n            37395: \'ImageHistory\',\n            37396: \'SubjectArea\',\n            37398: \'EPStandardID\',\n            37399: \'SensingMethod\',\n            37500: \'MakerNote\',\n            37510: \'UserComment\',\n            37520: \'SubsecTime\',\n            37521: \'SubsecTimeOriginal\',\n            37522: \'SubsecTimeDigitized\',\n            37888: \'Temperature\',\n            37889: \'Humidity\',\n            37890: \'Pressure\',\n            37891: \'WaterDepth\',\n            37892: \'Acceleration\',\n            37893: \'CameraElevationAngle\',\n            40960: \'FlashpixVersion\',\n            40961: \'ColorSpace\',\n            40962: \'PixelXDimension\',\n            40963: \'PixelYDimension\',\n            40964: \'RelatedSoundFile\',\n            40965: \'InteroperabilityTag\',\n            41483: \'FlashEnergy\',\n            41484: \'SpatialFrequencyResponse\',\n            41486: \'FocalPlaneXResolution\',\n            41487: \'FocalPlaneYResolution\',\n            41488: \'FocalPlaneResolutionUnit\',\n            41492: \'SubjectLocation\',\n            41493: \'ExposureIndex\',\n            41495: \'SensingMethod\',\n            41728: \'FileSource\',\n            41729: \'SceneType\',\n            41730: \'CFAPattern\',\n            41985: \'CustomRendered\',\n            41986: \'ExposureMode\',\n            41987: \'WhiteBalance\',\n            41988: \'DigitalZoomRatio\',\n            41989: \'FocalLengthIn35mmFilm\',\n            41990: \'SceneCaptureType\',\n            41991: \'GainControl\',\n            41992: \'Contrast\',\n            41993: \'Saturation\',\n            41994: \'Sharpness\',\n            41995: \'DeviceSettingDescription\',\n            41996: \'SubjectDistanceRange\',\n            42016: \'ImageUniqueID\',\n            42032: \'CameraOwnerName\',\n            42033: \'BodySerialNumber\',\n            42034: \'LensSpecification\',\n            42035: \'LensMake\',\n            42036: \'LensModel\',\n            42037: \'LensSerialNumber\',\n            42240: \'Gamma\',\n            59932: \'Padding\',\n            59933: \'OffsetSchema\',\n            65000: \'OwnerName\',\n            65001: \'SerialNumber\',\n            65002: \'Lens\',\n            65100: \'RawFile\',\n            65101: \'Converter\',\n            65102: \'WhiteBalance\',\n            65105: \'Exposure\',\n            65106: \'Shadows\',\n            65107: \'Brightness\',\n            65108: \'Contrast\',\n            65109: \'Saturation\',\n            65110: \'Sharpness\',\n            65111: \'Smoothness\',\n            65112: \'MoireFilter\',\n        }\n\n    def GPS_TAGS():\n        return {\n            0: \'GPSVersionID\',\n            1: \'GPSLatitudeRef\',\n            2: \'GPSLatitude\',\n            3: \'GPSLongitudeRef\',\n            4: \'GPSLongitude\',\n            5: \'GPSAltitudeRef\',\n            6: \'GPSAltitude\',\n            7: \'GPSTimeStamp\',\n            8: \'GPSSatellites\',\n            9: \'GPSStatus\',\n            10: \'GPSMeasureMode\',\n            11: \'GPSDOP\',\n            12: \'GPSSpeedRef\',\n            13: \'GPSSpeed\',\n            14: \'GPSTrackRef\',\n            15: \'GPSTrack\',\n            16: \'GPSImgDirectionRef\',\n            17: \'GPSImgDirection\',\n            18: \'GPSMapDatum\',\n            19: \'GPSDestLatitudeRef\',\n            20: \'GPSDestLatitude\',\n            21: \'GPSDestLongitudeRef\',\n            22: \'GPSDestLongitude\',\n            23: \'GPSDestBearingRef\',\n            24: \'GPSDestBearing\',\n            25: \'GPSDestDistanceRef\',\n            26: \'GPSDestDistance\',\n            27: \'GPSProcessingMethod\',\n            28: \'GPSAreaInformation\',\n            29: \'GPSDateStamp\',\n            30: \'GPSDifferential\',\n            31: \'GPSHPositioningError\',\n        }\n\n    def IOP_TAGS():\n        return {\n            1: \'InteroperabilityIndex\',\n            2: \'InteroperabilityVersion\',\n            4096: \'RelatedImageFileFormat\',\n            4097: \'RelatedImageWidth\',\n            4098: \'RelatedImageLength\',\n        }\n\n    def GEO_KEYS():\n        return {\n            1024: \'GTModelTypeGeoKey\',\n            1025: \'GTRasterTypeGeoKey\',\n            1026: \'GTCitationGeoKey\',\n            2048: \'GeographicTypeGeoKey\',\n            2049: \'GeogCitationGeoKey\',\n            2050: \'GeogGeodeticDatumGeoKey\',\n            2051: \'GeogPrimeMeridianGeoKey\',\n            2052: \'GeogLinearUnitsGeoKey\',\n            2053: \'GeogLinearUnitSizeGeoKey\',\n            2054: \'GeogAngularUnitsGeoKey\',\n            2055: \'GeogAngularUnitsSizeGeoKey\',\n            2056: \'GeogEllipsoidGeoKey\',\n            2057: \'GeogSemiMajorAxisGeoKey\',\n            2058: \'GeogSemiMinorAxisGeoKey\',\n            2059: \'GeogInvFlatteningGeoKey\',\n            2060: \'GeogAzimuthUnitsGeoKey\',\n            2061: \'GeogPrimeMeridianLongGeoKey\',\n            2062: \'GeogTOWGS84GeoKey\',\n            3059: \'ProjLinearUnitsInterpCorrectGeoKey\',  # GDAL\n            3072: \'ProjectedCSTypeGeoKey\',\n            3073: \'PCSCitationGeoKey\',\n            3074: \'ProjectionGeoKey\',\n            3075: \'ProjCoordTransGeoKey\',\n            3076: \'ProjLinearUnitsGeoKey\',\n            3077: \'ProjLinearUnitSizeGeoKey\',\n            3078: \'ProjStdParallel1GeoKey\',\n            3079: \'ProjStdParallel2GeoKey\',\n            3080: \'ProjNatOriginLongGeoKey\',\n            3081: \'ProjNatOriginLatGeoKey\',\n            3082: \'ProjFalseEastingGeoKey\',\n            3083: \'ProjFalseNorthingGeoKey\',\n            3084: \'ProjFalseOriginLongGeoKey\',\n            3085: \'ProjFalseOriginLatGeoKey\',\n            3086: \'ProjFalseOriginEastingGeoKey\',\n            3087: \'ProjFalseOriginNorthingGeoKey\',\n            3088: \'ProjCenterLongGeoKey\',\n            3089: \'ProjCenterLatGeoKey\',\n            3090: \'ProjCenterEastingGeoKey\',\n            3091: \'ProjFalseOriginNorthingGeoKey\',\n            3092: \'ProjScaleAtNatOriginGeoKey\',\n            3093: \'ProjScaleAtCenterGeoKey\',\n            3094: \'ProjAzimuthAngleGeoKey\',\n            3095: \'ProjStraightVertPoleLongGeoKey\',\n            3096: \'ProjRectifiedGridAngleGeoKey\',\n            4096: \'VerticalCSTypeGeoKey\',\n            4097: \'VerticalCitationGeoKey\',\n            4098: \'VerticalDatumGeoKey\',\n            4099: \'VerticalUnitsGeoKey\',\n        }\n\n    def GEO_CODES():\n        try:\n            from .tifffile_geodb import GEO_CODES  # delayed import\n        except (ImportError, ValueError):\n            try:\n                from tifffile_geodb import GEO_CODES  # delayed import\n            except (ImportError, ValueError):\n                GEO_CODES = {}\n        return GEO_CODES\n\n    def CZ_LSMINFO():\n        return [\n            (\'MagicNumber\', \'u4\'),\n            (\'StructureSize\', \'i4\'),\n            (\'DimensionX\', \'i4\'),\n            (\'DimensionY\', \'i4\'),\n            (\'DimensionZ\', \'i4\'),\n            (\'DimensionChannels\', \'i4\'),\n            (\'DimensionTime\', \'i4\'),\n            (\'DataType\', \'i4\'),  # DATATYPES\n            (\'ThumbnailX\', \'i4\'),\n            (\'ThumbnailY\', \'i4\'),\n            (\'VoxelSizeX\', \'f8\'),\n            (\'VoxelSizeY\', \'f8\'),\n            (\'VoxelSizeZ\', \'f8\'),\n            (\'OriginX\', \'f8\'),\n            (\'OriginY\', \'f8\'),\n            (\'OriginZ\', \'f8\'),\n            (\'ScanType\', \'u2\'),\n            (\'SpectralScan\', \'u2\'),\n            (\'TypeOfData\', \'u4\'),  # TYPEOFDATA\n            (\'OffsetVectorOverlay\', \'u4\'),\n            (\'OffsetInputLut\', \'u4\'),\n            (\'OffsetOutputLut\', \'u4\'),\n            (\'OffsetChannelColors\', \'u4\'),\n            (\'TimeIntervall\', \'f8\'),\n            (\'OffsetChannelDataTypes\', \'u4\'),\n            (\'OffsetScanInformation\', \'u4\'),  # SCANINFO\n            (\'OffsetKsData\', \'u4\'),\n            (\'OffsetTimeStamps\', \'u4\'),\n            (\'OffsetEventList\', \'u4\'),\n            (\'OffsetRoi\', \'u4\'),\n            (\'OffsetBleachRoi\', \'u4\'),\n            (\'OffsetNextRecording\', \'u4\'),\n            # LSM 2.0 ends here\n            (\'DisplayAspectX\', \'f8\'),\n            (\'DisplayAspectY\', \'f8\'),\n            (\'DisplayAspectZ\', \'f8\'),\n            (\'DisplayAspectTime\', \'f8\'),\n            (\'OffsetMeanOfRoisOverlay\', \'u4\'),\n            (\'OffsetTopoIsolineOverlay\', \'u4\'),\n            (\'OffsetTopoProfileOverlay\', \'u4\'),\n            (\'OffsetLinescanOverlay\', \'u4\'),\n            (\'ToolbarFlags\', \'u4\'),\n            (\'OffsetChannelWavelength\', \'u4\'),\n            (\'OffsetChannelFactors\', \'u4\'),\n            (\'ObjectiveSphereCorrection\', \'f8\'),\n            (\'OffsetUnmixParameters\', \'u4\'),\n            # LSM 3.2, 4.0 end here\n            (\'OffsetAcquisitionParameters\', \'u4\'),\n            (\'OffsetCharacteristics\', \'u4\'),\n            (\'OffsetPalette\', \'u4\'),\n            (\'TimeDifferenceX\', \'f8\'),\n            (\'TimeDifferenceY\', \'f8\'),\n            (\'TimeDifferenceZ\', \'f8\'),\n            (\'InternalUse1\', \'u4\'),\n            (\'DimensionP\', \'i4\'),\n            (\'DimensionM\', \'i4\'),\n            (\'DimensionsReserved\', \'16i4\'),\n            (\'OffsetTilePositions\', \'u4\'),\n            (\'\', \'9u4\'),  # Reserved\n            (\'OffsetPositions\', \'u4\'),\n            # (\'\', \'21u4\'),  # must be 0\n        ]\n\n    def CZ_LSMINFO_READERS():\n        # Import functions for CZ_LSMINFO sub-records\n        # TODO: read more CZ_LSMINFO sub-records\n        return {\n            \'ScanInformation\': read_lsm_scaninfo,\n            \'TimeStamps\': read_lsm_timestamps,\n            \'EventList\': read_lsm_eventlist,\n            \'ChannelColors\': read_lsm_channelcolors,\n            \'Positions\': read_lsm_floatpairs,\n            \'TilePositions\': read_lsm_floatpairs,\n            \'VectorOverlay\': None,\n            \'InputLut\': None,\n            \'OutputLut\': None,\n            \'TimeIntervall\': None,\n            \'ChannelDataTypes\': None,\n            \'KsData\': None,\n            \'Roi\': None,\n            \'BleachRoi\': None,\n            \'NextRecording\': None,\n            \'MeanOfRoisOverlay\': None,\n            \'TopoIsolineOverlay\': None,\n            \'TopoProfileOverlay\': None,\n            \'ChannelWavelength\': None,\n            \'SphereCorrection\': None,\n            \'ChannelFactors\': None,\n            \'UnmixParameters\': None,\n            \'AcquisitionParameters\': None,\n            \'Characteristics\': None,\n        }\n\n    def CZ_LSMINFO_SCANTYPE():\n        # Map CZ_LSMINFO.ScanType to dimension order\n        return {\n            0: \'XYZCT\',  # \'Stack\' normal x-y-z-scan\n            1: \'XYZCT\',  # \'Z-Scan\' x-z-plane Y=1\n            2: \'XYZCT\',  # \'Line\'\n            3: \'XYTCZ\',  # \'Time Series Plane\' time series x-y  XYCTZ ? Z=1\n            4: \'XYZTC\',  # \'Time Series z-Scan\' time series x-z\n            5: \'XYTCZ\',  # \'Time Series Mean-of-ROIs\'\n            6: \'XYZTC\',  # \'Time Series Stack\' time series x-y-z\n            7: \'XYCTZ\',  # Spline Scan\n            8: \'XYCZT\',  # Spline Plane x-z\n            9: \'XYTCZ\',  # Time Series Spline Plane x-z\n            10: \'XYZCT\',  # \'Time Series Point\' point mode\n        }\n\n    def CZ_LSMINFO_DIMENSIONS():\n        # Map dimension codes to CZ_LSMINFO attribute\n        return {\n            \'X\': \'DimensionX\',\n            \'Y\': \'DimensionY\',\n            \'Z\': \'DimensionZ\',\n            \'C\': \'DimensionChannels\',\n            \'T\': \'DimensionTime\',\n            \'P\': \'DimensionP\',\n            \'M\': \'DimensionM\',\n        }\n\n    def CZ_LSMINFO_DATATYPES():\n        # Description of CZ_LSMINFO.DataType\n        return {\n            0: \'varying data types\',\n            1: \'8 bit unsigned integer\',\n            2: \'12 bit unsigned integer\',\n            5: \'32 bit float\',\n        }\n\n    def CZ_LSMINFO_TYPEOFDATA():\n        # Description of CZ_LSMINFO.TypeOfData\n        return {\n            0: \'Original scan data\',\n            1: \'Calculated data\',\n            2: \'3D reconstruction\',\n            3: \'Topography height map\',\n        }\n\n    def CZ_LSMINFO_SCANINFO_ARRAYS():\n        return {\n            0x20000000: \'Tracks\',\n            0x30000000: \'Lasers\',\n            0x60000000: \'DetectionChannels\',\n            0x80000000: \'IlluminationChannels\',\n            0xa0000000: \'BeamSplitters\',\n            0xc0000000: \'DataChannels\',\n            0x11000000: \'Timers\',\n            0x13000000: \'Markers\',\n        }\n\n    def CZ_LSMINFO_SCANINFO_STRUCTS():\n        return {\n            # 0x10000000: \'Recording\',\n            0x40000000: \'Track\',\n            0x50000000: \'Laser\',\n            0x70000000: \'DetectionChannel\',\n            0x90000000: \'IlluminationChannel\',\n            0xb0000000: \'BeamSplitter\',\n            0xd0000000: \'DataChannel\',\n            0x12000000: \'Timer\',\n            0x14000000: \'Marker\',\n        }\n\n    def CZ_LSMINFO_SCANINFO_ATTRIBUTES():\n        return {\n            # Recording\n            0x10000001: \'Name\',\n            0x10000002: \'Description\',\n            0x10000003: \'Notes\',\n            0x10000004: \'Objective\',\n            0x10000005: \'ProcessingSummary\',\n            0x10000006: \'SpecialScanMode\',\n            0x10000007: \'ScanType\',\n            0x10000008: \'ScanMode\',\n            0x10000009: \'NumberOfStacks\',\n            0x1000000a: \'LinesPerPlane\',\n            0x1000000b: \'SamplesPerLine\',\n            0x1000000c: \'PlanesPerVolume\',\n            0x1000000d: \'ImagesWidth\',\n            0x1000000e: \'ImagesHeight\',\n            0x1000000f: \'ImagesNumberPlanes\',\n            0x10000010: \'ImagesNumberStacks\',\n            0x10000011: \'ImagesNumberChannels\',\n            0x10000012: \'LinscanXySize\',\n            0x10000013: \'ScanDirection\',\n            0x10000014: \'TimeSeries\',\n            0x10000015: \'OriginalScanData\',\n            0x10000016: \'ZoomX\',\n            0x10000017: \'ZoomY\',\n            0x10000018: \'ZoomZ\',\n            0x10000019: \'Sample0X\',\n            0x1000001a: \'Sample0Y\',\n            0x1000001b: \'Sample0Z\',\n            0x1000001c: \'SampleSpacing\',\n            0x1000001d: \'LineSpacing\',\n            0x1000001e: \'PlaneSpacing\',\n            0x1000001f: \'PlaneWidth\',\n            0x10000020: \'PlaneHeight\',\n            0x10000021: \'VolumeDepth\',\n            0x10000023: \'Nutation\',\n            0x10000034: \'Rotation\',\n            0x10000035: \'Precession\',\n            0x10000036: \'Sample0time\',\n            0x10000037: \'StartScanTriggerIn\',\n            0x10000038: \'StartScanTriggerOut\',\n            0x10000039: \'StartScanEvent\',\n            0x10000040: \'StartScanTime\',\n            0x10000041: \'StopScanTriggerIn\',\n            0x10000042: \'StopScanTriggerOut\',\n            0x10000043: \'StopScanEvent\',\n            0x10000044: \'StopScanTime\',\n            0x10000045: \'UseRois\',\n            0x10000046: \'UseReducedMemoryRois\',\n            0x10000047: \'User\',\n            0x10000048: \'UseBcCorrection\',\n            0x10000049: \'PositionBcCorrection1\',\n            0x10000050: \'PositionBcCorrection2\',\n            0x10000051: \'InterpolationY\',\n            0x10000052: \'CameraBinning\',\n            0x10000053: \'CameraSupersampling\',\n            0x10000054: \'CameraFrameWidth\',\n            0x10000055: \'CameraFrameHeight\',\n            0x10000056: \'CameraOffsetX\',\n            0x10000057: \'CameraOffsetY\',\n            0x10000059: \'RtBinning\',\n            0x1000005a: \'RtFrameWidth\',\n            0x1000005b: \'RtFrameHeight\',\n            0x1000005c: \'RtRegionWidth\',\n            0x1000005d: \'RtRegionHeight\',\n            0x1000005e: \'RtOffsetX\',\n            0x1000005f: \'RtOffsetY\',\n            0x10000060: \'RtZoom\',\n            0x10000061: \'RtLinePeriod\',\n            0x10000062: \'Prescan\',\n            0x10000063: \'ScanDirectionZ\',\n            # Track\n            0x40000001: \'MultiplexType\',  # 0 After Line; 1 After Frame\n            0x40000002: \'MultiplexOrder\',\n            0x40000003: \'SamplingMode\',  # 0 Sample; 1 Line Avg; 2 Frame Avg\n            0x40000004: \'SamplingMethod\',  # 1 Mean; 2 Sum\n            0x40000005: \'SamplingNumber\',\n            0x40000006: \'Acquire\',\n            0x40000007: \'SampleObservationTime\',\n            0x4000000b: \'TimeBetweenStacks\',\n            0x4000000c: \'Name\',\n            0x4000000d: \'Collimator1Name\',\n            0x4000000e: \'Collimator1Position\',\n            0x4000000f: \'Collimator2Name\',\n            0x40000010: \'Collimator2Position\',\n            0x40000011: \'IsBleachTrack\',\n            0x40000012: \'IsBleachAfterScanNumber\',\n            0x40000013: \'BleachScanNumber\',\n            0x40000014: \'TriggerIn\',\n            0x40000015: \'TriggerOut\',\n            0x40000016: \'IsRatioTrack\',\n            0x40000017: \'BleachCount\',\n            0x40000018: \'SpiCenterWavelength\',\n            0x40000019: \'PixelTime\',\n            0x40000021: \'CondensorFrontlens\',\n            0x40000023: \'FieldStopValue\',\n            0x40000024: \'IdCondensorAperture\',\n            0x40000025: \'CondensorAperture\',\n            0x40000026: \'IdCondensorRevolver\',\n            0x40000027: \'CondensorFilter\',\n            0x40000028: \'IdTransmissionFilter1\',\n            0x40000029: \'IdTransmission1\',\n            0x40000030: \'IdTransmissionFilter2\',\n            0x40000031: \'IdTransmission2\',\n            0x40000032: \'RepeatBleach\',\n            0x40000033: \'EnableSpotBleachPos\',\n            0x40000034: \'SpotBleachPosx\',\n            0x40000035: \'SpotBleachPosy\',\n            0x40000036: \'SpotBleachPosz\',\n            0x40000037: \'IdTubelens\',\n            0x40000038: \'IdTubelensPosition\',\n            0x40000039: \'TransmittedLight\',\n            0x4000003a: \'ReflectedLight\',\n            0x4000003b: \'SimultanGrabAndBleach\',\n            0x4000003c: \'BleachPixelTime\',\n            # Laser\n            0x50000001: \'Name\',\n            0x50000002: \'Acquire\',\n            0x50000003: \'Power\',\n            # DetectionChannel\n            0x70000001: \'IntegrationMode\',\n            0x70000002: \'SpecialMode\',\n            0x70000003: \'DetectorGainFirst\',\n            0x70000004: \'DetectorGainLast\',\n            0x70000005: \'AmplifierGainFirst\',\n            0x70000006: \'AmplifierGainLast\',\n            0x70000007: \'AmplifierOffsFirst\',\n            0x70000008: \'AmplifierOffsLast\',\n            0x70000009: \'PinholeDiameter\',\n            0x7000000a: \'CountingTrigger\',\n            0x7000000b: \'Acquire\',\n            0x7000000c: \'PointDetectorName\',\n            0x7000000d: \'AmplifierName\',\n            0x7000000e: \'PinholeName\',\n            0x7000000f: \'FilterSetName\',\n            0x70000010: \'FilterName\',\n            0x70000013: \'IntegratorName\',\n            0x70000014: \'ChannelName\',\n            0x70000015: \'DetectorGainBc1\',\n            0x70000016: \'DetectorGainBc2\',\n            0x70000017: \'AmplifierGainBc1\',\n            0x70000018: \'AmplifierGainBc2\',\n            0x70000019: \'AmplifierOffsetBc1\',\n            0x70000020: \'AmplifierOffsetBc2\',\n            0x70000021: \'SpectralScanChannels\',\n            0x70000022: \'SpiWavelengthStart\',\n            0x70000023: \'SpiWavelengthStop\',\n            0x70000026: \'DyeName\',\n            0x70000027: \'DyeFolder\',\n            # IlluminationChannel\n            0x90000001: \'Name\',\n            0x90000002: \'Power\',\n            0x90000003: \'Wavelength\',\n            0x90000004: \'Aquire\',\n            0x90000005: \'DetchannelName\',\n            0x90000006: \'PowerBc1\',\n            0x90000007: \'PowerBc2\',\n            # BeamSplitter\n            0xb0000001: \'FilterSet\',\n            0xb0000002: \'Filter\',\n            0xb0000003: \'Name\',\n            # DataChannel\n            0xd0000001: \'Name\',\n            0xd0000003: \'Acquire\',\n            0xd0000004: \'Color\',\n            0xd0000005: \'SampleType\',\n            0xd0000006: \'BitsPerSample\',\n            0xd0000007: \'RatioType\',\n            0xd0000008: \'RatioTrack1\',\n            0xd0000009: \'RatioTrack2\',\n            0xd000000a: \'RatioChannel1\',\n            0xd000000b: \'RatioChannel2\',\n            0xd000000c: \'RatioConst1\',\n            0xd000000d: \'RatioConst2\',\n            0xd000000e: \'RatioConst3\',\n            0xd000000f: \'RatioConst4\',\n            0xd0000010: \'RatioConst5\',\n            0xd0000011: \'RatioConst6\',\n            0xd0000012: \'RatioFirstImages1\',\n            0xd0000013: \'RatioFirstImages2\',\n            0xd0000014: \'DyeName\',\n            0xd0000015: \'DyeFolder\',\n            0xd0000016: \'Spectrum\',\n            0xd0000017: \'Acquire\',\n            # Timer\n            0x12000001: \'Name\',\n            0x12000002: \'Description\',\n            0x12000003: \'Interval\',\n            0x12000004: \'TriggerIn\',\n            0x12000005: \'TriggerOut\',\n            0x12000006: \'ActivationTime\',\n            0x12000007: \'ActivationNumber\',\n            # Marker\n            0x14000001: \'Name\',\n            0x14000002: \'Description\',\n            0x14000003: \'TriggerIn\',\n            0x14000004: \'TriggerOut\',\n        }\n\n    def NIH_IMAGE_HEADER():\n        return [\n            (\'FileID\', \'a8\'),\n            (\'nLines\', \'i2\'),\n            (\'PixelsPerLine\', \'i2\'),\n            (\'Version\', \'i2\'),\n            (\'OldLutMode\', \'i2\'),\n            (\'OldnColors\', \'i2\'),\n            (\'Colors\', \'u1\', (3, 32)),\n            (\'OldColorStart\', \'i2\'),\n            (\'ColorWidth\', \'i2\'),\n            (\'ExtraColors\', \'u2\', (6, 3)),\n            (\'nExtraColors\', \'i2\'),\n            (\'ForegroundIndex\', \'i2\'),\n            (\'BackgroundIndex\', \'i2\'),\n            (\'XScale\', \'f8\'),\n            (\'Unused2\', \'i2\'),\n            (\'Unused3\', \'i2\'),\n            (\'UnitsID\', \'i2\'),  # NIH_UNITS_TYPE\n            (\'p1\', [(\'x\', \'i2\'), (\'y\', \'i2\')]),\n            (\'p2\', [(\'x\', \'i2\'), (\'y\', \'i2\')]),\n            (\'CurveFitType\', \'i2\'),  # NIH_CURVEFIT_TYPE\n            (\'nCoefficients\', \'i2\'),\n            (\'Coeff\', \'f8\', 6),\n            (\'UMsize\', \'u1\'),\n            (\'UM\', \'a15\'),\n            (\'UnusedBoolean\', \'u1\'),\n            (\'BinaryPic\', \'b1\'),\n            (\'SliceStart\', \'i2\'),\n            (\'SliceEnd\', \'i2\'),\n            (\'ScaleMagnification\', \'f4\'),\n            (\'nSlices\', \'i2\'),\n            (\'SliceSpacing\', \'f4\'),\n            (\'CurrentSlice\', \'i2\'),\n            (\'FrameInterval\', \'f4\'),\n            (\'PixelAspectRatio\', \'f4\'),\n            (\'ColorStart\', \'i2\'),\n            (\'ColorEnd\', \'i2\'),\n            (\'nColors\', \'i2\'),\n            (\'Fill1\', \'3u2\'),\n            (\'Fill2\', \'3u2\'),\n            (\'Table\', \'u1\'),  # NIH_COLORTABLE_TYPE\n            (\'LutMode\', \'u1\'),  # NIH_LUTMODE_TYPE\n            (\'InvertedTable\', \'b1\'),\n            (\'ZeroClip\', \'b1\'),\n            (\'XUnitSize\', \'u1\'),\n            (\'XUnit\', \'a11\'),\n            (\'StackType\', \'i2\'),  # NIH_STACKTYPE_TYPE\n            # (\'UnusedBytes\', \'u1\', 200)\n        ]\n\n    def NIH_COLORTABLE_TYPE():\n        return (\'CustomTable\', \'AppleDefault\', \'Pseudo20\', \'Pseudo32\',\n                \'Rainbow\', \'Fire1\', \'Fire2\', \'Ice\', \'Grays\', \'Spectrum\')\n\n    def NIH_LUTMODE_TYPE():\n        return (\'PseudoColor\', \'OldAppleDefault\', \'OldSpectrum\', \'GrayScale\',\n                \'ColorLut\', \'CustomGrayscale\')\n\n    def NIH_CURVEFIT_TYPE():\n        return (\'StraightLine\', \'Poly2\', \'Poly3\', \'Poly4\', \'Poly5\', \'ExpoFit\',\n                \'PowerFit\', \'LogFit\', \'RodbardFit\', \'SpareFit1\',\n                \'Uncalibrated\', \'UncalibratedOD\')\n\n    def NIH_UNITS_TYPE():\n        return (\'Nanometers\', \'Micrometers\', \'Millimeters\', \'Centimeters\',\n                \'Meters\', \'Kilometers\', \'Inches\', \'Feet\', \'Miles\', \'Pixels\',\n                \'OtherUnits\')\n\n    def NIH_STACKTYPE_TYPE():\n        return (\'VolumeStack\', \'RGBStack\', \'MovieStack\', \'HSVStack\')\n\n    def TVIPS_HEADER_V1():\n        # TVIPS TemData structure from EMMENU Help file\n        return [\n            (\'Version\', \'i4\'),\n            (\'CommentV1\', \'a80\'),\n            (\'HighTension\', \'i4\'),\n            (\'SphericalAberration\', \'i4\'),\n            (\'IlluminationAperture\', \'i4\'),\n            (\'Magnification\', \'i4\'),\n            (\'PostMagnification\', \'i4\'),\n            (\'FocalLength\', \'i4\'),\n            (\'Defocus\', \'i4\'),\n            (\'Astigmatism\', \'i4\'),\n            (\'AstigmatismDirection\', \'i4\'),\n            (\'BiprismVoltage\', \'i4\'),\n            (\'SpecimenTiltAngle\', \'i4\'),\n            (\'SpecimenTiltDirection\', \'i4\'),\n            (\'IlluminationTiltDirection\', \'i4\'),\n            (\'IlluminationTiltAngle\', \'i4\'),\n            (\'ImageMode\', \'i4\'),\n            (\'EnergySpread\', \'i4\'),\n            (\'ChromaticAberration\', \'i4\'),\n            (\'ShutterType\', \'i4\'),\n            (\'DefocusSpread\', \'i4\'),\n            (\'CcdNumber\', \'i4\'),\n            (\'CcdSize\', \'i4\'),\n            (\'OffsetXV1\', \'i4\'),\n            (\'OffsetYV1\', \'i4\'),\n            (\'PhysicalPixelSize\', \'i4\'),\n            (\'Binning\', \'i4\'),\n            (\'ReadoutSpeed\', \'i4\'),\n            (\'GainV1\', \'i4\'),\n            (\'SensitivityV1\', \'i4\'),\n            (\'ExposureTimeV1\', \'i4\'),\n            (\'FlatCorrected\', \'i4\'),\n            (\'DeadPxCorrected\', \'i4\'),\n            (\'ImageMean\', \'i4\'),\n            (\'ImageStd\', \'i4\'),\n            (\'DisplacementX\', \'i4\'),\n            (\'DisplacementY\', \'i4\'),\n            (\'DateV1\', \'i4\'),\n            (\'TimeV1\', \'i4\'),\n            (\'ImageMin\', \'i4\'),\n            (\'ImageMax\', \'i4\'),\n            (\'ImageStatisticsQuality\', \'i4\'),\n        ]\n\n    def TVIPS_HEADER_V2():\n        return [\n            (\'ImageName\', \'V160\'),  # utf16\n            (\'ImageFolder\', \'V160\'),\n            (\'ImageSizeX\', \'i4\'),\n            (\'ImageSizeY\', \'i4\'),\n            (\'ImageSizeZ\', \'i4\'),\n            (\'ImageSizeE\', \'i4\'),\n            (\'ImageDataType\', \'i4\'),\n            (\'Date\', \'i4\'),\n            (\'Time\', \'i4\'),\n            (\'Comment\', \'V1024\'),\n            (\'ImageHistory\', \'V1024\'),\n            (\'Scaling\', \'16f4\'),\n            (\'ImageStatistics\', \'16c16\'),\n            (\'ImageType\', \'i4\'),\n            (\'ImageDisplaType\', \'i4\'),\n            (\'PixelSizeX\', \'f4\'),  # distance between two px in x, [nm]\n            (\'PixelSizeY\', \'f4\'),  # distance between two px in y, [nm]\n            (\'ImageDistanceZ\', \'f4\'),\n            (\'ImageDistanceE\', \'f4\'),\n            (\'ImageMisc\', \'32f4\'),\n            (\'TemType\', \'V160\'),\n            (\'TemHighTension\', \'f4\'),\n            (\'TemAberrations\', \'32f4\'),\n            (\'TemEnergy\', \'32f4\'),\n            (\'TemMode\', \'i4\'),\n            (\'TemMagnification\', \'f4\'),\n            (\'TemMagnificationCorrection\', \'f4\'),\n            (\'PostMagnification\', \'f4\'),\n            (\'TemStageType\', \'i4\'),\n            (\'TemStagePosition\', \'5f4\'),  # x, y, z, a, b\n            (\'TemImageShift\', \'2f4\'),\n            (\'TemBeamShift\', \'2f4\'),\n            (\'TemBeamTilt\', \'2f4\'),\n            (\'TilingParameters\', \'7f4\'),  # 0: tiling? 1:x 2:y 3: max x\n                                          # 4: max y 5: overlap x 6: overlap y\n            (\'TemIllumination\', \'3f4\'),  # 0: spotsize 1: intensity\n            (\'TemShutter\', \'i4\'),\n            (\'TemMisc\', \'32f4\'),\n            (\'CameraType\', \'V160\'),\n            (\'PhysicalPixelSizeX\', \'f4\'),\n            (\'PhysicalPixelSizeY\', \'f4\'),\n            (\'OffsetX\', \'i4\'),\n            (\'OffsetY\', \'i4\'),\n            (\'BinningX\', \'i4\'),\n            (\'BinningY\', \'i4\'),\n            (\'ExposureTime\', \'f4\'),\n            (\'Gain\', \'f4\'),\n            (\'ReadoutRate\', \'f4\'),\n            (\'FlatfieldDescription\', \'V160\'),\n            (\'Sensitivity\', \'f4\'),\n            (\'Dose\', \'f4\'),\n            (\'CamMisc\', \'32f4\'),\n            (\'FeiMicroscopeInformation\', \'V1024\'),\n            (\'FeiSpecimenInformation\', \'V1024\'),\n            (\'Magic\', \'u4\'),\n        ]\n\n    def MM_HEADER():\n        # Olympus FluoView MM_Header\n        MM_DIMENSION = [\n            (\'Name\', \'a16\'),\n            (\'Size\', \'i4\'),\n            (\'Origin\', \'f8\'),\n            (\'Resolution\', \'f8\'),\n            (\'Unit\', \'a64\')]\n        return [\n            (\'HeaderFlag\', \'i2\'),\n            (\'ImageType\', \'u1\'),\n            (\'ImageName\', \'a257\'),\n            (\'OffsetData\', \'u4\'),\n            (\'PaletteSize\', \'i4\'),\n            (\'OffsetPalette0\', \'u4\'),\n            (\'OffsetPalette1\', \'u4\'),\n            (\'CommentSize\', \'i4\'),\n            (\'OffsetComment\', \'u4\'),\n            (\'Dimensions\', MM_DIMENSION, 10),\n            (\'OffsetPosition\', \'u4\'),\n            (\'MapType\', \'i2\'),\n            (\'MapMin\', \'f8\'),\n            (\'MapMax\', \'f8\'),\n            (\'MinValue\', \'f8\'),\n            (\'MaxValue\', \'f8\'),\n            (\'OffsetMap\', \'u4\'),\n            (\'Gamma\', \'f8\'),\n            (\'Offset\', \'f8\'),\n            (\'GrayChannel\', MM_DIMENSION),\n            (\'OffsetThumbnail\', \'u4\'),\n            (\'VoiceField\', \'i4\'),\n            (\'OffsetVoiceField\', \'u4\'),\n        ]\n\n    def MM_DIMENSIONS():\n        # Map FluoView MM_Header.Dimensions to axes characters\n        return {\n            \'X\': \'X\',\n            \'Y\': \'Y\',\n            \'Z\': \'Z\',\n            \'T\': \'T\',\n            \'CH\': \'C\',\n            \'WAVELENGTH\': \'C\',\n            \'TIME\': \'T\',\n            \'XY\': \'R\',\n            \'EVENT\': \'V\',\n            \'EXPOSURE\': \'L\',\n        }\n\n    def UIC_TAGS():\n        # Map Universal Imaging Corporation MetaMorph internal tag ids to\n        # name and type\n        from fractions import Fraction  # delayed import\n\n        return [\n            (\'AutoScale\', int),\n            (\'MinScale\', int),\n            (\'MaxScale\', int),\n            (\'SpatialCalibration\', int),\n            (\'XCalibration\', Fraction),\n            (\'YCalibration\', Fraction),\n            (\'CalibrationUnits\', str),\n            (\'Name\', str),\n            (\'ThreshState\', int),\n            (\'ThreshStateRed\', int),\n            (\'tagid_10\', None),  # undefined\n            (\'ThreshStateGreen\', int),\n            (\'ThreshStateBlue\', int),\n            (\'ThreshStateLo\', int),\n            (\'ThreshStateHi\', int),\n            (\'Zoom\', int),\n            (\'CreateTime\', julian_datetime),\n            (\'LastSavedTime\', julian_datetime),\n            (\'currentBuffer\', int),\n            (\'grayFit\', None),\n            (\'grayPointCount\', None),\n            (\'grayX\', Fraction),\n            (\'grayY\', Fraction),\n            (\'grayMin\', Fraction),\n            (\'grayMax\', Fraction),\n            (\'grayUnitName\', str),\n            (\'StandardLUT\', int),\n            (\'wavelength\', int),\n            (\'StagePosition\', \'(%i,2,2)u4\'),  # N xy positions as fract\n            (\'CameraChipOffset\', \'(%i,2,2)u4\'),  # N xy offsets as fract\n            (\'OverlayMask\', None),\n            (\'OverlayCompress\', None),\n            (\'Overlay\', None),\n            (\'SpecialOverlayMask\', None),\n            (\'SpecialOverlayCompress\', None),\n            (\'SpecialOverlay\', None),\n            (\'ImageProperty\', read_uic_image_property),\n            (\'StageLabel\', \'%ip\'),  # N str\n            (\'AutoScaleLoInfo\', Fraction),\n            (\'AutoScaleHiInfo\', Fraction),\n            (\'AbsoluteZ\', \'(%i,2)u4\'),  # N fractions\n            (\'AbsoluteZValid\', \'(%i,)u4\'),  # N long\n            (\'Gamma\', \'I\'),  # \'I\' uses offset\n            (\'GammaRed\', \'I\'),\n            (\'GammaGreen\', \'I\'),\n            (\'GammaBlue\', \'I\'),\n            (\'CameraBin\', \'2I\'),\n            (\'NewLUT\', int),\n            (\'ImagePropertyEx\', None),\n            (\'PlaneProperty\', int),\n            (\'UserLutTable\', \'(256,3)u1\'),\n            (\'RedAutoScaleInfo\', int),\n            (\'RedAutoScaleLoInfo\', Fraction),\n            (\'RedAutoScaleHiInfo\', Fraction),\n            (\'RedMinScaleInfo\', int),\n            (\'RedMaxScaleInfo\', int),\n            (\'GreenAutoScaleInfo\', int),\n            (\'GreenAutoScaleLoInfo\', Fraction),\n            (\'GreenAutoScaleHiInfo\', Fraction),\n            (\'GreenMinScaleInfo\', int),\n            (\'GreenMaxScaleInfo\', int),\n            (\'BlueAutoScaleInfo\', int),\n            (\'BlueAutoScaleLoInfo\', Fraction),\n            (\'BlueAutoScaleHiInfo\', Fraction),\n            (\'BlueMinScaleInfo\', int),\n            (\'BlueMaxScaleInfo\', int),\n            # (\'OverlayPlaneColor\', read_uic_overlay_plane_color),\n        ]\n\n    def PILATUS_HEADER():\n        # PILATUS CBF Header Specification, Version 1.4\n        # Map key to [value_indices], type\n        return {\n            \'Detector\': ([slice(1, None)], str),\n            \'Pixel_size\': ([1, 4], float),\n            \'Silicon\': ([3], float),\n            \'Exposure_time\': ([1], float),\n            \'Exposure_period\': ([1], float),\n            \'Tau\': ([1], float),\n            \'Count_cutoff\': ([1], int),\n            \'Threshold_setting\': ([1], float),\n            \'Gain_setting\': ([1, 2], str),\n            \'N_excluded_pixels\': ([1], int),\n            \'Excluded_pixels\': ([1], str),\n            \'Flat_field\': ([1], str),\n            \'Trim_file\': ([1], str),\n            \'Image_path\': ([1], str),\n            # optional\n            \'Wavelength\': ([1], float),\n            \'Energy_range\': ([1, 2], float),\n            \'Detector_distance\': ([1], float),\n            \'Detector_Voffset\': ([1], float),\n            \'Beam_xy\': ([1, 2], float),\n            \'Flux\': ([1], str),\n            \'Filter_transmission\': ([1], float),\n            \'Start_angle\': ([1], float),\n            \'Angle_increment\': ([1], float),\n            \'Detector_2theta\': ([1], float),\n            \'Polarization\': ([1], float),\n            \'Alpha\': ([1], float),\n            \'Kappa\': ([1], float),\n            \'Phi\': ([1], float),\n            \'Phi_increment\': ([1], float),\n            \'Chi\': ([1], float),\n            \'Chi_increment\': ([1], float),\n            \'Oscillation_axis\': ([slice(1, None)], str),\n            \'N_oscillations\': ([1], int),\n            \'Start_position\': ([1], float),\n            \'Position_increment\': ([1], float),\n            \'Shutter_time\': ([1], float),\n            \'Omega\': ([1], float),\n            \'Omega_increment\': ([1], float)\n        }\n\n    def REVERSE_BITORDER_BYTES():\n        # Bytes with reversed bitorder\n        return (\n            b\'\\x00\\x80@\\xc0 \\xa0`\\xe0\\x10\\x90P\\xd00\\xb0p\\xf0\\x08\\x88H\\xc8(\'\n            b\'\\xa8h\\xe8\\x18\\x98X\\xd88\\xb8x\\xf8\\x04\\x84D\\xc4$\\xa4d\\xe4\\x14\'\n            b\'\\x94T\\xd44\\xb4t\\xf4\\x0c\\x8cL\\xcc,\\xacl\\xec\\x1c\\x9c\\\\\\xdc<\\xbc|\'\n            b\'\\xfc\\x02\\x82B\\xc2""\\xa2b\\xe2\\x12\\x92R\\xd22\\xb2r\\xf2\\n\\x8aJ\\xca*\'\n            b\'\\xaaj\\xea\\x1a\\x9aZ\\xda:\\xbaz\\xfa\\x06\\x86F\\xc6&\\xa6f\\xe6\\x16\'\n            b\'\\x96V\\xd66\\xb6v\\xf6\\x0e\\x8eN\\xce.\\xaen\\xee\\x1e\\x9e^\\xde>\\xbe~\'\n            b\'\\xfe\\x01\\x81A\\xc1!\\xa1a\\xe1\\x11\\x91Q\\xd11\\xb1q\\xf1\\t\\x89I\\xc9)\'\n            b\'\\xa9i\\xe9\\x19\\x99Y\\xd99\\xb9y\\xf9\\x05\\x85E\\xc5%\\xa5e\\xe5\\x15\'\n            b\'\\x95U\\xd55\\xb5u\\xf5\\r\\x8dM\\xcd-\\xadm\\xed\\x1d\\x9d]\\xdd=\\xbd}\'\n            b\'\\xfd\\x03\\x83C\\xc3#\\xa3c\\xe3\\x13\\x93S\\xd33\\xb3s\\xf3\\x0b\\x8bK\'\n            b\'\\xcb+\\xabk\\xeb\\x1b\\x9b[\\xdb;\\xbb{\\xfb\\x07\\x87G\\xc7\\\'\\xa7g\\xe7\'\n            b\'\\x17\\x97W\\xd77\\xb7w\\xf7\\x0f\\x8fO\\xcf/\\xafo\\xef\\x1f\\x9f_\'\n            b\'\\xdf?\\xbf\\x7f\\xff\')\n\n    def REVERSE_BITORDER_ARRAY():\n        # Numpy array of bytes with reversed bitorder\n        return numpy.frombuffer(TIFF.REVERSE_BITORDER_BYTES, dtype=\'uint8\')\n\n    def ALLOCATIONGRANULARITY():\n        # alignment for writing contiguous data to TIFF\n        import mmap  # delayed import\n        return mmap.ALLOCATIONGRANULARITY\n\n\ndef read_tags(fh, byteorder, offsetsize, tagnames,\n              customtags=None, maxifds=None):\n    """"""Read tags from chain of IFDs and return as list of dicts.\n\n    The file handle position must be at a valid IFD header.\n\n    """"""\n    if offsetsize == 4:\n        offsetformat = byteorder+\'I\'\n        tagnosize = 2\n        tagnoformat = byteorder+\'H\'\n        tagsize = 12\n        tagformat1 = byteorder+\'HH\'\n        tagformat2 = byteorder+\'I4s\'\n    elif offsetsize == 8:\n        offsetformat = byteorder+\'Q\'\n        tagnosize = 8\n        tagnoformat = byteorder+\'Q\'\n        tagsize = 20\n        tagformat1 = byteorder+\'HH\'\n        tagformat2 = byteorder+\'Q8s\'\n    else:\n        raise ValueError(\'invalid offset size\')\n\n    if customtags is None:\n        customtags = {}\n    if maxifds is None:\n        maxifds = 2**32\n\n    result = []\n    unpack = struct.unpack\n    offset = fh.tell()\n    while len(result) < maxifds:\n        # loop over IFDs\n        try:\n            tagno = unpack(tagnoformat, fh.read(tagnosize))[0]\n            if tagno > 4096:\n                raise ValueError(\'suspicious number of tags\')\n        except Exception:\n            warnings.warn(\'corrupted tag list at offset %i\' % offset)\n            break\n\n        tags = {}\n        data = fh.read(tagsize * tagno)\n        pos = fh.tell()\n        index = 0\n        for _ in range(tagno):\n            code, type_ = unpack(tagformat1, data[index:index+4])\n            count, value = unpack(tagformat2, data[index+4:index+tagsize])\n            index += tagsize\n            name = tagnames.get(code, str(code))\n            try:\n                dtype = TIFF.DATA_FORMATS[type_]\n            except KeyError:\n                raise TiffTag.Error(\'unknown tag data type %i\' % type_)\n\n            fmt = \'%s%i%s\' % (byteorder, count * int(dtype[0]), dtype[1])\n            size = struct.calcsize(fmt)\n            if size > offsetsize or code in customtags:\n                offset = unpack(offsetformat, value)[0]\n                if offset < 8 or offset > fh.size - size:\n                    raise TiffTag.Error(\'invalid tag value offset %i\' % offset)\n                fh.seek(offset)\n                if code in customtags:\n                    readfunc = customtags[code][1]\n                    value = readfunc(fh, byteorder, dtype, count, offsetsize)\n                elif type_ == 7 or (count > 1 and dtype[-1] == \'B\'):\n                    value = read_bytes(fh, byteorder, dtype, count, offsetsize)\n                elif code in tagnames or dtype[-1] == \'s\':\n                    value = unpack(fmt, fh.read(size))\n                else:\n                    value = read_numpy(fh, byteorder, dtype, count, offsetsize)\n            elif dtype[-1] == \'B\' or type_ == 7:\n                value = value[:size]\n            else:\n                value = unpack(fmt, value[:size])\n\n            if code not in customtags and code not in TIFF.TAG_TUPLE:\n                if len(value) == 1:\n                    value = value[0]\n            if type_ != 7 and dtype[-1] == \'s\' and isinstance(value, bytes):\n                # TIFF ASCII fields can contain multiple strings,\n                #   each terminated with a NUL\n                try:\n                    value = bytes2str(stripascii(value).strip())\n                except UnicodeDecodeError:\n                    warnings.warn(\n                        \'tag %i: coercing invalid ASCII to bytes\' % code)\n\n            tags[name] = value\n\n        result.append(tags)\n        # read offset to next page\n        fh.seek(pos)\n        offset = unpack(offsetformat, fh.read(offsetsize))[0]\n        if offset == 0:\n            break\n        if offset >= fh.size:\n            warnings.warn(\'invalid page offset %i\' % offset)\n            break\n        fh.seek(offset)\n\n    if result and maxifds == 1:\n        result = result[0]\n    return result\n\n\ndef read_exif_ifd(fh, byteorder, dtype, count, offsetsize):\n    """"""Read EXIF tags from file and return as dict.""""""\n    exif = read_tags(fh, byteorder, offsetsize, TIFF.EXIF_TAGS, maxifds=1)\n    for name in (\'ExifVersion\', \'FlashpixVersion\'):\n        try:\n            exif[name] = bytes2str(exif[name])\n        except Exception:\n            pass\n    if \'UserComment\' in exif:\n        idcode = exif[\'UserComment\'][:8]\n        try:\n            if idcode == b\'ASCII\\x00\\x00\\x00\':\n                exif[\'UserComment\'] = bytes2str(exif[\'UserComment\'][8:])\n            elif idcode == b\'UNICODE\\x00\':\n                exif[\'UserComment\'] = exif[\'UserComment\'][8:].decode(\'utf-16\')\n        except Exception:\n            pass\n    return exif\n\n\ndef read_gps_ifd(fh, byteorder, dtype, count, offsetsize):\n    """"""Read GPS tags from file and return as dict.""""""\n    return read_tags(fh, byteorder, offsetsize, TIFF.GPS_TAGS, maxifds=1)\n\n\ndef read_interoperability_ifd(fh, byteorder, dtype, count, offsetsize):\n    """"""Read Interoperability tags from file and return as dict.""""""\n    tag_names = {1: \'InteroperabilityIndex\'}\n    return read_tags(fh, byteorder, offsetsize, tag_names, maxifds=1)\n\n\ndef read_bytes(fh, byteorder, dtype, count, offsetsize):\n    """"""Read tag data from file and return as byte string.""""""\n    dtype = \'B\' if dtype[-1] == \'s\' else byteorder+dtype[-1]\n    count *= numpy.dtype(dtype).itemsize\n    data = fh.read(count)\n    if len(data) != count:\n        warnings.warn(\'failed to read all bytes: %i, %i\' % (len(data), count))\n    return data\n\n\ndef read_utf8(fh, byteorder, dtype, count, offsetsize):\n    """"""Read tag data from file and return as unicode string.""""""\n    return fh.read(count).decode(\'utf-8\')\n\n\ndef read_numpy(fh, byteorder, dtype, count, offsetsize):\n    """"""Read tag data from file and return as numpy array.""""""\n    dtype = \'b\' if dtype[-1] == \'s\' else byteorder+dtype[-1]\n    return fh.read_array(dtype, count)\n\n\ndef read_colormap(fh, byteorder, dtype, count, offsetsize):\n    """"""Read ColorMap data from file and return as numpy array.""""""\n    cmap = fh.read_array(byteorder+dtype[-1], count)\n    cmap.shape = (3, -1)\n    return cmap\n\n\ndef read_json(fh, byteorder, dtype, count, offsetsize):\n    """"""Read JSON tag data from file and return as object.""""""\n    data = fh.read(count)\n    try:\n        return json.loads(unicode(stripnull(data), \'utf-8\'))\n    except ValueError:\n        warnings.warn(""invalid JSON \'%s\'"" % data)\n\n\ndef read_mm_header(fh, byteorder, dtype, count, offsetsize):\n    """"""Read FluoView mm_header tag from file and return as dict.""""""\n    mmh = fh.read_record(TIFF.MM_HEADER, byteorder=byteorder)\n    mmh = recarray2dict(mmh)\n    mmh[\'Dimensions\'] = [\n        (bytes2str(d[0]).strip(), d[1], d[2], d[3], bytes2str(d[4]).strip())\n        for d in mmh[\'Dimensions\']]\n    d = mmh[\'GrayChannel\']\n    mmh[\'GrayChannel\'] = (\n        bytes2str(d[0]).strip(), d[1], d[2], d[3], bytes2str(d[4]).strip())\n    return mmh\n\n\ndef read_mm_stamp(fh, byteorder, dtype, count, offsetsize):\n    """"""Read FluoView mm_stamp tag from file and return as numpy.ndarray.""""""\n    return fh.read_array(byteorder+\'f8\', 8)\n\n\ndef read_uic1tag(fh, byteorder, dtype, count, offsetsize, planecount=None):\n    """"""Read MetaMorph STK UIC1Tag from file and return as dict.\n\n    Return empty dictionary if planecount is unknown.\n\n    """"""\n    assert dtype in (\'2I\', \'1I\') and byteorder == \'<\'\n    result = {}\n    if dtype == \'2I\':\n        # pre MetaMorph 2.5 (not tested)\n        values = fh.read_array(\'<u4\', 2*count).reshape(count, 2)\n        result = {\'ZDistance\': values[:, 0] / values[:, 1]}\n    elif planecount:\n        for _ in range(count):\n            tagid = struct.unpack(\'<I\', fh.read(4))[0]\n            if tagid in (28, 29, 37, 40, 41):\n                # silently skip unexpected tags\n                fh.read(4)\n                continue\n            name, value = read_uic_tag(fh, tagid, planecount, offset=True)\n            result[name] = value\n    return result\n\n\ndef read_uic2tag(fh, byteorder, dtype, planecount, offsetsize):\n    """"""Read MetaMorph STK UIC2Tag from file and return as dict.""""""\n    assert dtype == \'2I\' and byteorder == \'<\'\n    values = fh.read_array(\'<u4\', 6*planecount).reshape(planecount, 6)\n    return {\n        \'ZDistance\': values[:, 0] / values[:, 1],\n        \'DateCreated\': values[:, 2],  # julian days\n        \'TimeCreated\': values[:, 3],  # milliseconds\n        \'DateModified\': values[:, 4],  # julian days\n        \'TimeModified\': values[:, 5]}  # milliseconds\n\n\ndef read_uic3tag(fh, byteorder, dtype, planecount, offsetsize):\n    """"""Read MetaMorph STK UIC3Tag from file and return as dict.""""""\n    assert dtype == \'2I\' and byteorder == \'<\'\n    values = fh.read_array(\'<u4\', 2*planecount).reshape(planecount, 2)\n    return {\'Wavelengths\': values[:, 0] / values[:, 1]}\n\n\ndef read_uic4tag(fh, byteorder, dtype, planecount, offsetsize):\n    """"""Read MetaMorph STK UIC4Tag from file and return as dict.""""""\n    assert dtype == \'1I\' and byteorder == \'<\'\n    result = {}\n    while True:\n        tagid = struct.unpack(\'<H\', fh.read(2))[0]\n        if tagid == 0:\n            break\n        name, value = read_uic_tag(fh, tagid, planecount, offset=False)\n        result[name] = value\n    return result\n\n\ndef read_uic_tag(fh, tagid, planecount, offset):\n    """"""Read a single UIC tag value from file and return tag name and value.\n\n    UIC1Tags use an offset.\n\n    """"""\n    def read_int(count=1):\n        value = struct.unpack(\'<%iI\' % count, fh.read(4*count))\n        return value[0] if count == 1 else value\n\n    try:\n        name, dtype = TIFF.UIC_TAGS[tagid]\n    except IndexError:\n        # unknown tag\n        return \'_TagId%i\' % tagid, read_int()\n\n    Fraction = TIFF.UIC_TAGS[4][1]\n\n    if offset:\n        pos = fh.tell()\n        if dtype not in (int, None):\n            off = read_int()\n            if off < 8:\n                if dtype is str:\n                    return name, \'\'\n                warnings.warn(""invalid offset for uic tag \'%s\': %i"" %\n                              (name, off))\n                return name, off\n            fh.seek(off)\n\n    if dtype is None:\n        # skip\n        name = \'_\' + name\n        value = read_int()\n    elif dtype is int:\n        # int\n        value = read_int()\n    elif dtype is Fraction:\n        # fraction\n        value = read_int(2)\n        value = value[0] / value[1]\n    elif dtype is julian_datetime:\n        # datetime\n        value = julian_datetime(*read_int(2))\n    elif dtype is read_uic_image_property:\n        # ImagePropertyEx\n        value = read_uic_image_property(fh)\n    elif dtype is str:\n        # pascal string\n        size = read_int()\n        if 0 <= size < 2**10:\n            value = struct.unpack(\'%is\' % size, fh.read(size))[0][:-1]\n            value = bytes2str(stripnull(value))\n        elif offset:\n            value = \'\'\n            warnings.warn(""corrupt string in uic tag \'%s\'"" % name)\n        else:\n            raise ValueError(\'invalid string size: %i\' % size)\n    elif dtype == \'%ip\':\n        # sequence of pascal strings\n        value = []\n        for _ in range(planecount):\n            size = read_int()\n            if 0 <= size < 2**10:\n                string = struct.unpack(\'%is\' % size, fh.read(size))[0][:-1]\n                string = bytes2str(stripnull(string))\n                value.append(string)\n            elif offset:\n                warnings.warn(""corrupt string in uic tag \'%s\'"" % name)\n            else:\n                raise ValueError(\'invalid string size: %i\' % size)\n    else:\n        # struct or numpy type\n        dtype = \'<\' + dtype\n        if \'%i\' in dtype:\n            dtype = dtype % planecount\n        if \'(\' in dtype:\n            # numpy type\n            value = fh.read_array(dtype, 1)[0]\n            if value.shape[-1] == 2:\n                # assume fractions\n                value = value[..., 0] / value[..., 1]\n        else:\n            # struct format\n            value = struct.unpack(dtype, fh.read(struct.calcsize(dtype)))\n            if len(value) == 1:\n                value = value[0]\n\n    if offset:\n        fh.seek(pos + 4)\n\n    return name, value\n\n\ndef read_uic_image_property(fh):\n    """"""Read UIC ImagePropertyEx tag from file and return as dict.""""""\n    # TODO: test this\n    size = struct.unpack(\'B\', fh.read(1))[0]\n    name = struct.unpack(\'%is\' % size, fh.read(size))[0][:-1]\n    flags, prop = struct.unpack(\'<IB\', fh.read(5))\n    if prop == 1:\n        value = struct.unpack(\'II\', fh.read(8))\n        value = value[0] / value[1]\n    else:\n        size = struct.unpack(\'B\', fh.read(1))[0]\n        value = struct.unpack(\'%is\' % size, fh.read(size))[0]\n    return dict(name=name, flags=flags, value=value)\n\n\ndef read_cz_lsminfo(fh, byteorder, dtype, count, offsetsize):\n    """"""Read CZ_LSMINFO tag from file and return as dict.""""""\n    assert byteorder == \'<\'\n    magic_number, structure_size = struct.unpack(\'<II\', fh.read(8))\n    if magic_number not in (50350412, 67127628):\n        raise ValueError(\'invalid CZ_LSMINFO structure\')\n    fh.seek(-8, 1)\n\n    if structure_size < numpy.dtype(TIFF.CZ_LSMINFO).itemsize:\n        # adjust structure according to structure_size\n        lsminfo = []\n        size = 0\n        for name, dtype in TIFF.CZ_LSMINFO:\n            size += numpy.dtype(dtype).itemsize\n            if size > structure_size:\n                break\n            lsminfo.append((name, dtype))\n    else:\n        lsminfo = TIFF.CZ_LSMINFO\n\n    lsminfo = fh.read_record(lsminfo, byteorder=byteorder)\n    lsminfo = recarray2dict(lsminfo)\n\n    # read LSM info subrecords at offsets\n    for name, reader in TIFF.CZ_LSMINFO_READERS.items():\n        if reader is None:\n            continue\n        offset = lsminfo.get(\'Offset\' + name, 0)\n        if offset < 8:\n            continue\n        fh.seek(offset)\n        try:\n            lsminfo[name] = reader(fh)\n        except ValueError:\n            pass\n    return lsminfo\n\n\ndef read_lsm_floatpairs(fh):\n    """"""Read LSM sequence of float pairs from file and return as list.""""""\n    size = struct.unpack(\'<i\', fh.read(4))[0]\n    return fh.read_array(\'<2f8\', count=size)\n\n\ndef read_lsm_positions(fh):\n    """"""Read LSM positions from file and return as list.""""""\n    size = struct.unpack(\'<I\', fh.read(4))[0]\n    return fh.read_array(\'<2f8\', count=size)\n\n\ndef read_lsm_timestamps(fh):\n    """"""Read LSM time stamps from file and return as list.""""""\n    size, count = struct.unpack(\'<ii\', fh.read(8))\n    if size != (8 + 8 * count):\n        warnings.warn(\'invalid LSM TimeStamps block\')\n        return []\n    # return struct.unpack(\'<%dd\' % count, fh.read(8*count))\n    return fh.read_array(\'<f8\', count=count)\n\n\ndef read_lsm_eventlist(fh):\n    """"""Read LSM events from file and return as list of (time, type, text).""""""\n    count = struct.unpack(\'<II\', fh.read(8))[1]\n    events = []\n    while count > 0:\n        esize, etime, etype = struct.unpack(\'<IdI\', fh.read(16))\n        etext = bytes2str(stripnull(fh.read(esize - 16)))\n        events.append((etime, etype, etext))\n        count -= 1\n    return events\n\n\ndef read_lsm_channelcolors(fh):\n    """"""Read LSM ChannelColors structure from file and return as dict.""""""\n    result = {\'Mono\': False, \'Colors\': [], \'ColorNames\': []}\n    pos = fh.tell()\n    (size, ncolors, nnames,\n     coffset, noffset, mono) = struct.unpack(\'<IIIIII\', fh.read(24))\n    if ncolors != nnames:\n        warnings.warn(\'invalid LSM ChannelColors structure\')\n        return result\n    result[\'Mono\'] = bool(mono)\n    # Colors\n    fh.seek(pos + coffset)\n    colors = fh.read_array(\'uint8\', count=ncolors*4).reshape((ncolors, 4))\n    result[\'Colors\'] = colors.tolist()\n    # ColorNames\n    fh.seek(pos + noffset)\n    buffer = fh.read(size - noffset)\n    names = []\n    while len(buffer) > 4:\n        size = struct.unpack(\'<I\', buffer[:4])[0]\n        names.append(bytes2str(buffer[4:3+size]))\n        buffer = buffer[4+size:]\n    result[\'ColorNames\'] = names\n    return result\n\n\ndef read_lsm_scaninfo(fh):\n    """"""Read LSM ScanInfo structure from file and return as dict.""""""\n    block = {}\n    blocks = [block]\n    unpack = struct.unpack\n    if struct.unpack(\'<I\', fh.read(4))[0] != 0x10000000:\n        # not a Recording sub block\n        warnings.warn(\'invalid LSM ScanInfo structure\')\n        return block\n    fh.read(8)\n    while True:\n        entry, dtype, size = unpack(\'<III\', fh.read(12))\n        if dtype == 2:\n            # ascii\n            value = bytes2str(stripnull(fh.read(size)))\n        elif dtype == 4:\n            # long\n            value = unpack(\'<i\', fh.read(4))[0]\n        elif dtype == 5:\n            # rational\n            value = unpack(\'<d\', fh.read(8))[0]\n        else:\n            value = 0\n        if entry in TIFF.CZ_LSMINFO_SCANINFO_ARRAYS:\n            blocks.append(block)\n            name = TIFF.CZ_LSMINFO_SCANINFO_ARRAYS[entry]\n            newobj = []\n            block[name] = newobj\n            block = newobj\n        elif entry in TIFF.CZ_LSMINFO_SCANINFO_STRUCTS:\n            blocks.append(block)\n            newobj = {}\n            block.append(newobj)\n            block = newobj\n        elif entry in TIFF.CZ_LSMINFO_SCANINFO_ATTRIBUTES:\n            name = TIFF.CZ_LSMINFO_SCANINFO_ATTRIBUTES[entry]\n            block[name] = value\n        elif entry == 0xffffffff:\n            # end sub block\n            block = blocks.pop()\n        else:\n            # unknown entry\n            block[\'Entry0x%x\' % entry] = value\n        if not blocks:\n            break\n    return block\n\n\ndef read_tvips_header(fh, byteorder, dtype, count, offsetsize):\n    """"""Read TVIPS EM-MENU headers and return as dict.""""""\n    result = {}\n    header = fh.read_record(TIFF.TVIPS_HEADER_V1, byteorder=byteorder)\n    for name, typestr in TIFF.TVIPS_HEADER_V1:\n        result[name] = header[name].tolist()\n    if header[\'Version\'] == 2:\n        header = fh.read_record(TIFF.TVIPS_HEADER_V2, byteorder=byteorder)\n        if header[\'Magic\'] != int(0xaaaaaaaa):\n            warnings.warn(\'invalid TVIPS v2 magic number\')\n            return {}\n        # decode utf16 strings\n        for name, typestr in TIFF.TVIPS_HEADER_V2:\n            if typestr.startswith(\'V\'):\n                s = header[name].tostring().decode(\'utf16\', errors=\'ignore\')\n                result[name] = stripnull(s, null=\'\\0\')\n            else:\n                result[name] = header[name].tolist()\n        # convert nm to m\n        for axis in \'XY\':\n            header[\'PhysicalPixelSize\' + axis] /= 1e9\n            header[\'PixelSize\' + axis] /= 1e9\n    elif header.version != 1:\n        warnings.warn(\'unknown TVIPS header version\')\n        return {}\n    return result\n\n\ndef read_fei_metadata(fh, byteorder, dtype, count, offsetsize):\n    """"""Read FEI SFEG/HELIOS headers and return as dict.""""""\n    result = {}\n    section = {}\n    data = bytes2str(fh.read(count))\n    for line in data.splitlines():\n        line = line.strip()\n        if line.startswith(\'[\'):\n            section = {}\n            result[line[1:-1]] = section\n            continue\n        try:\n            key, value = line.split(\'=\')\n        except ValueError:\n            continue\n        section[key] = astype(value)\n    return result\n\n\ndef read_cz_sem(fh, byteorder, dtype, count, offsetsize):\n    """"""Read Zeiss SEM tag and return as dict.""""""\n    result = {\'\': ()}\n    key = None\n    data = bytes2str(fh.read(count))\n    for line in data.splitlines():\n        if line.isupper():\n            key = line.lower()\n        elif key:\n            try:\n                name, value = line.split(\'=\')\n            except ValueError:\n                continue\n            value = value.strip()\n            unit = \'\'\n            try:\n                v, u = value.split()\n                number = astype(v, (int, float))\n                if number != v:\n                    value = number\n                    unit = u\n            except Exception:\n                number = astype(value, (int, float))\n                if number != value:\n                    value = number\n                if value in (\'No\', \'Off\'):\n                    value = False\n                elif value in (\'Yes\', \'On\'):\n                    value = True\n            result[key] = (name.strip(), value)\n            if unit:\n                result[key] += (unit,)\n            key = None\n        else:\n            result[\'\'] += (astype(line, (int, float)),)\n    return result\n\n\ndef read_nih_image_header(fh, byteorder, dtype, count, offsetsize):\n    """"""Read NIH_IMAGE_HEADER tag from file and return as dict.""""""\n    a = fh.read_record(TIFF.NIH_IMAGE_HEADER, byteorder=byteorder)\n    a = a.newbyteorder(byteorder)\n    a = recarray2dict(a)\n    a[\'XUnit\'] = a[\'XUnit\'][:a[\'XUnitSize\']]\n    a[\'UM\'] = a[\'UM\'][:a[\'UMsize\']]\n    return a\n\n\ndef read_scanimage_metadata(fh):\n    """"""Read ScanImage BigTIFF v3 static and ROI metadata from open file.\n\n    Return non-varying frame data as dict and ROI group data as JSON.\n\n    The settings can be used to read image data and metadata without parsing\n    the TIFF file.\n\n    Raise ValueError if file does not contain valid ScanImage v3 metadata.\n\n    """"""\n    fh.seek(0)\n    try:\n        byteorder, version = struct.unpack(\'<2sH\', fh.read(4))\n        if byteorder != b\'II\' or version != 43:\n            raise Exception\n        fh.seek(16)\n        magic, version, size0, size1 = struct.unpack(\'<IIII\', fh.read(16))\n        if magic != 117637889 or version != 3:\n            raise Exception\n    except Exception:\n        raise ValueError(\'not a ScanImage BigTIFF v3 file\')\n\n    frame_data = matlabstr2py(bytes2str(fh.read(size0)[:-1]))\n    roi_data = read_json(fh, \'<\', None, size1, None) if size1 > 1 else {}\n    return frame_data, roi_data\n\n\ndef read_micromanager_metadata(fh):\n    """"""Read MicroManager non-TIFF settings from open file and return as dict.\n\n    The settings can be used to read image data without parsing the TIFF file.\n\n    Raise ValueError if the file does not contain valid MicroManager metadata.\n\n    """"""\n    fh.seek(0)\n    try:\n        byteorder = {b\'II\': \'<\', b\'MM\': \'>\'}[fh.read(2)]\n    except IndexError:\n        raise ValueError(\'not a MicroManager TIFF file\')\n\n    result = {}\n    fh.seek(8)\n    (index_header, index_offset, display_header, display_offset,\n     comments_header, comments_offset, summary_header, summary_length\n     ) = struct.unpack(byteorder + \'IIIIIIII\', fh.read(32))\n\n    if summary_header != 2355492:\n        raise ValueError(\'invalid MicroManager summary header\')\n    result[\'Summary\'] = read_json(fh, byteorder, None, summary_length, None)\n\n    if index_header != 54773648:\n        raise ValueError(\'invalid MicroManager index header\')\n    fh.seek(index_offset)\n    header, count = struct.unpack(byteorder + \'II\', fh.read(8))\n    if header != 3453623:\n        raise ValueError(\'invalid MicroManager index header\')\n    data = struct.unpack(byteorder + \'IIIII\'*count, fh.read(20*count))\n    result[\'IndexMap\'] = {\'Channel\': data[::5],\n                          \'Slice\': data[1::5],\n                          \'Frame\': data[2::5],\n                          \'Position\': data[3::5],\n                          \'Offset\': data[4::5]}\n\n    if display_header != 483765892:\n        raise ValueError(\'invalid MicroManager display header\')\n    fh.seek(display_offset)\n    header, count = struct.unpack(byteorder + \'II\', fh.read(8))\n    if header != 347834724:\n        raise ValueError(\'invalid MicroManager display header\')\n    result[\'DisplaySettings\'] = read_json(fh, byteorder, None, count, None)\n\n    if comments_header != 99384722:\n        raise ValueError(\'invalid MicroManager comments header\')\n    fh.seek(comments_offset)\n    header, count = struct.unpack(byteorder + \'II\', fh.read(8))\n    if header != 84720485:\n        raise ValueError(\'invalid MicroManager comments header\')\n    result[\'Comments\'] = read_json(fh, byteorder, None, count, None)\n\n    return result\n\n\ndef read_metaseries_catalog(fh):\n    """"""Read MetaSeries non-TIFF hint catalog from file.\n\n    Raise ValueError if the file does not contain a valid hint catalog.\n\n    """"""\n    # TODO: implement read_metaseries_catalog\n    raise NotImplementedError()\n\n\ndef imagej_metadata(data, bytecounts, byteorder):\n    """"""Return IJMetadata tag value as dict.\n\n    The \'info\' string can have multiple formats, e.g. OIF or ScanImage,\n    that might be parsed into dicts using the matlabstr2py or\n    oiffile.SettingsFile functions.\n\n    """"""\n    def readstring(data, byteorder):\n        return data.decode(\'utf-16\' + {\'>\': \'be\', \'<\': \'le\'}[byteorder])\n\n    def readdouble(data, byteorder):\n        return struct.unpack(byteorder+(\'d\' * (len(data) // 8)), data)\n\n    def readbytes(data, byteorder):\n        return numpy.frombuffer(data, \'uint8\')\n\n    metadata_types = {  # big-endian\n        b\'info\': (\'Info\', readstring),\n        b\'labl\': (\'Labels\', readstring),\n        b\'rang\': (\'Ranges\', readdouble),\n        b\'luts\': (\'LUTs\', readbytes),\n        b\'roi \': (\'ROI\', readbytes),\n        b\'over\': (\'Overlays\', readbytes)}\n    metadata_types.update(  # little-endian\n        dict((k[::-1], v) for k, v in metadata_types.items()))\n\n    if not bytecounts:\n        raise ValueError(\'no ImageJ metadata\')\n\n    if not data[:4] in (b\'IJIJ\', b\'JIJI\'):\n        raise ValueError(\'invalid ImageJ metadata\')\n\n    header_size = bytecounts[0]\n    if header_size < 12 or header_size > 804:\n        raise ValueError(\'invalid ImageJ metadata header size\')\n\n    ntypes = (header_size - 4) // 8\n    header = struct.unpack(byteorder+\'4sI\'*ntypes, data[4:4+ntypes*8])\n    pos = 4 + ntypes * 8\n    counter = 0\n    result = {}\n    for mtype, count in zip(header[::2], header[1::2]):\n        values = []\n        name, func = metadata_types.get(mtype, (bytes2str(mtype), read_bytes))\n        for _ in range(count):\n            counter += 1\n            pos1 = pos + bytecounts[counter]\n            values.append(func(data[pos:pos1], byteorder))\n            pos = pos1\n        result[name.strip()] = values[0] if count == 1 else values\n    return result\n\n\ndef imagej_description_metadata(description):\n    """"""Return metatata from ImageJ image description as dict.\n\n    Raise ValueError if not a valid ImageJ description.\n\n    >>> description = \'ImageJ=1.11a\\\\nimages=510\\\\nhyperstack=true\\\\n\'\n    >>> imagej_description_metadata(description)  # doctest: +SKIP\n    {\'ImageJ\': \'1.11a\', \'images\': 510, \'hyperstack\': True}\n\n    """"""\n    def _bool(val):\n        return {\'true\': True, \'false\': False}[val.lower()]\n\n    result = {}\n    for line in description.splitlines():\n        try:\n            key, val = line.split(\'=\')\n        except Exception:\n            continue\n        key = key.strip()\n        val = val.strip()\n        for dtype in (int, float, _bool):\n            try:\n                val = dtype(val)\n                break\n            except Exception:\n                pass\n        result[key] = val\n\n    if \'ImageJ\' not in result:\n        raise ValueError(\'not a ImageJ image description\')\n    return result\n\n\ndef imagej_description(shape, rgb=None, colormaped=False, version=\'1.11a\',\n                       hyperstack=None, mode=None, loop=None, **kwargs):\n    """"""Return ImageJ image description from data shape.\n\n    ImageJ can handle up to 6 dimensions in order TZCYXS.\n\n    >>> imagej_description((51, 5, 2, 196, 171))  # doctest: +SKIP\n    ImageJ=1.11a\n    images=510\n    channels=2\n    slices=5\n    frames=51\n    hyperstack=true\n    mode=grayscale\n    loop=false\n\n    """"""\n    if colormaped:\n        raise NotImplementedError(\'ImageJ colormapping not supported\')\n    shape = imagej_shape(shape, rgb=rgb)\n    rgb = shape[-1] in (3, 4)\n\n    result = [\'ImageJ=%s\' % version]\n    append = []\n    result.append(\'images=%i\' % product(shape[:-3]))\n    if hyperstack is None:\n        hyperstack = True\n        append.append(\'hyperstack=true\')\n    else:\n        append.append(\'hyperstack=%s\' % bool(hyperstack))\n    if shape[2] > 1:\n        result.append(\'channels=%i\' % shape[2])\n    if mode is None and not rgb:\n        mode = \'grayscale\'\n    if hyperstack and mode:\n        append.append(\'mode=%s\' % mode)\n    if shape[1] > 1:\n        result.append(\'slices=%i\' % shape[1])\n    if shape[0] > 1:\n        result.append(\'frames=%i\' % shape[0])\n        if loop is None:\n            append.append(\'loop=false\')\n    if loop is not None:\n        append.append(\'loop=%s\' % bool(loop))\n    for key, value in kwargs.items():\n        append.append(\'%s=%s\' % (key.lower(), value))\n\n    return \'\\n\'.join(result + append + [\'\'])\n\n\ndef imagej_shape(shape, rgb=None):\n    """"""Return shape normalized to 6D ImageJ hyperstack TZCYXS.\n\n    Raise ValueError if not a valid ImageJ hyperstack shape.\n\n    >>> imagej_shape((2, 3, 4, 5, 3), False)\n    (2, 3, 4, 5, 3, 1)\n\n    """"""\n    shape = tuple(int(i) for i in shape)\n    ndim = len(shape)\n    if 1 > ndim > 6:\n        raise ValueError(\'invalid ImageJ hyperstack: not 2 to 6 dimensional\')\n    if rgb is None:\n        rgb = shape[-1] in (3, 4) and ndim > 2\n    if rgb and shape[-1] not in (3, 4):\n        raise ValueError(\'invalid ImageJ hyperstack: not a RGB image\')\n    if not rgb and ndim == 6 and shape[-1] != 1:\n        raise ValueError(\'invalid ImageJ hyperstack: not a non-RGB image\')\n    if rgb or shape[-1] == 1:\n        return (1, ) * (6 - ndim) + shape\n    return (1, ) * (5 - ndim) + shape + (1,)\n\n\ndef json_description(shape, **metadata):\n    """"""Return JSON image description from data shape and other meta data.\n\n    Return UTF-8 encoded JSON.\n\n    >>> json_description((256, 256, 3), axes=\'YXS\')  # doctest: +SKIP\n    b\'{""shape"": [256, 256, 3], ""axes"": ""YXS""}\'\n\n    """"""\n    metadata.update(shape=shape)\n    return json.dumps(metadata)  # .encode(\'utf-8\')\n\n\ndef json_description_metadata(description):\n    """"""Return metatata from JSON formated image description as dict.\n\n    Raise ValuError if description is of unknown format.\n\n    >>> description = \'{""shape"": [256, 256, 3], ""axes"": ""YXS""}\'\n    >>> json_description_metadata(description)  # doctest: +SKIP\n    {\'shape\': [256, 256, 3], \'axes\': \'YXS\'}\n    >>> json_description_metadata(\'shape=(256, 256, 3)\')\n    {\'shape\': (256, 256, 3)}\n\n    """"""\n    if description[:6] == \'shape=\':\n        # old style \'shaped\' description; not JSON\n        shape = tuple(int(i) for i in description[7:-1].split(\',\'))\n        return dict(shape=shape)\n    if description[:1] == \'{\' and description[-1:] == \'}\':\n        # JSON description\n        return json.loads(description)\n    raise ValueError(\'invalid JSON image description\', description)\n\n\ndef fluoview_description_metadata(description, ignoresections=None):\n    """"""Return metatata from FluoView image description as dict.\n\n    The FluoView image description format is unspecified. Expect failures.\n\n    >>> descr = (\'[Intensity Mapping]\\\\nMap Ch0: Range=00000 to 02047\\\\n\'\n    ...          \'[Intensity Mapping End]\')\n    >>> fluoview_description_metadata(descr)\n    {\'Intensity Mapping\': {\'Map Ch0: Range\': \'00000 to 02047\'}}\n\n    """"""\n    if not description.startswith(\'[\'):\n        raise ValueError(\'invalid FluoView image description\')\n    if ignoresections is None:\n        ignoresections = {\'Region Info (Fields)\', \'Protocol Description\'}\n\n    result = {}\n    sections = [result]\n    comment = False\n    for line in description.splitlines():\n        if not comment:\n            line = line.strip()\n        if not line:\n            continue\n        if line[0] == \'[\':\n            if line[-5:] == \' End]\':\n                # close section\n                del sections[-1]\n                section = sections[-1]\n                name = line[1:-5]\n                if comment:\n                    section[name] = \'\\n\'.join(section[name])\n                if name[:4] == \'LUT \':\n                    a = numpy.array(section[name], dtype=\'uint8\')\n                    a.shape = -1, 3\n                    section[name] = a\n                continue\n            # new section\n            comment = False\n            name = line[1:-1]\n            if name[:4] == \'LUT \':\n                section = []\n            elif name in ignoresections:\n                section = []\n                comment = True\n            else:\n                section = {}\n            sections.append(section)\n            result[name] = section\n            continue\n        # add entry\n        if comment:\n            section.append(line)\n            continue\n        line = line.split(\'=\', 1)\n        if len(line) == 1:\n            section[line[0].strip()] = None\n            continue\n        key, value = line\n        if key[:4] == \'RGB \':\n            section.extend(int(rgb) for rgb in value.split())\n        else:\n            section[key.strip()] = astype(value.strip())\n    return result\n\n\ndef pilatus_description_metadata(description):\n    """"""Return metatata from Pilatus image description as dict.\n\n    Return metadata from Pilatus pixel array detectors by Dectris, created\n    by camserver or TVX software.\n\n    >>> pilatus_description_metadata(\'# Pixel_size 172e-6 m x 172e-6 m\')\n    {\'Pixel_size\': (0.000172, 0.000172)}\n\n    """"""\n    result = {}\n    if not description.startswith(\'# \'):\n        return result\n    for c in \'#:=,()\':\n        description = description.replace(c, \' \')\n    for line in description.split(\'\\n\'):\n        if line[:2] != \'  \':\n            continue\n        line = line.split()\n        name = line[0]\n        if line[0] not in TIFF.PILATUS_HEADER:\n            try:\n                result[\'DateTime\'] = datetime.datetime.strptime(\n                    \' \'.join(line), \'%Y-%m-%dT%H %M %S.%f\')\n            except Exception:\n                result[name] = \' \'.join(line[1:])\n            continue\n        indices, dtype = TIFF.PILATUS_HEADER[line[0]]\n        if isinstance(indices[0], slice):\n            # assumes one slice\n            values = line[indices[0]]\n        else:\n            values = [line[i] for i in indices]\n        if dtype is float and values[0] == \'not\':\n            values = [\'NaN\']\n        values = tuple(dtype(v) for v in values)\n        if dtype == str:\n            values = \' \'.join(values)\n        elif len(values) == 1:\n            values = values[0]\n        result[name] = values\n    return result\n\n\ndef svs_description_metadata(description):\n    """"""Return metatata from Aperio image description as dict.\n\n    The Aperio image description format is unspecified. Expect failures.\n\n    >>> svs_description_metadata(\'Aperio Image Library v1.0\')\n    {\'Aperio Image Library\': \'v1.0\'}\n\n    """"""\n    if not description.startswith(\'Aperio Image Library \'):\n        raise ValueError(\'invalid Aperio image description\')\n    result = {}\n    lines = description.split(\'\\n\')\n    key, value = lines[0].strip().rsplit(None, 1)  # \'Aperio Image Library\'\n    result[key.strip()] = value.strip()\n    if len(lines) == 1:\n        return result\n    items = lines[1].split(\'|\')\n    result[\'\'] = items[0].strip()  # TODO: parse this?\n    for item in items[1:]:\n        key, value = item.split(\' = \')\n        result[key.strip()] = astype(value.strip())\n    return result\n\n\ndef stk_description_metadata(description):\n    """"""Return metadata from MetaMorph image description as list of dict.\n\n    The MetaMorph image description format is unspecified. Expect failures.\n\n    """"""\n    description = description.strip()\n    if not description:\n        return []\n    try:\n        description = bytes2str(description)\n    except UnicodeDecodeError:\n        warnings.warn(\'failed to parse MetaMorph image description\')\n        return []\n    result = []\n    for plane in description.split(\'\\x00\'):\n        d = {}\n        for line in plane.split(\'\\r\\n\'):\n            line = line.split(\':\', 1)\n            if len(line) > 1:\n                name, value = line\n                d[name.strip()] = astype(value.strip())\n            else:\n                value = line[0].strip()\n                if value:\n                    if \'\' in d:\n                        d[\'\'].append(value)\n                    else:\n                        d[\'\'] = [value]\n        result.append(d)\n    return result\n\n\ndef metaseries_description_metadata(description):\n    """"""Return metatata from MetaSeries image description as dict.""""""\n    if not description.startswith(\'<MetaData>\'):\n        raise ValueError(\'invalid MetaSeries image description\')\n\n    from xml.etree import cElementTree as etree  # delayed import\n    root = etree.fromstring(description)\n    types = {\'float\': float, \'int\': int,\n             \'bool\': lambda x: asbool(x, \'on\', \'off\')}\n\n    def parse(root, result):\n        # recursive\n        for child in root:\n            attrib = child.attrib\n            if not attrib:\n                result[child.tag] = parse(child, {})\n                continue\n            if \'id\' in attrib:\n                i = attrib[\'id\']\n                t = attrib[\'type\']\n                v = attrib[\'value\']\n                if t in types:\n                    result[i] = types[t](v)\n                else:\n                    result[i] = v\n        return result\n\n    adict = parse(root, {})\n    if \'Description\' in adict:\n        adict[\'Description\'] = adict[\'Description\'].replace(\'&#13;&#10;\', \'\\n\')\n    return adict\n\n\ndef scanimage_description_metadata(description):\n    """"""Return metatata from ScanImage image description as dict.""""""\n    return matlabstr2py(description)\n\n\ndef scanimage_artist_metadata(artist):\n    """"""Return metatata from ScanImage artist tag as dict.""""""\n    try:\n        return json.loads(artist)\n    except ValueError:\n        warnings.warn(""invalid JSON \'%s\'"" % artist)\n\n\ndef _replace_by(module_function, package=__package__, warn=None, prefix=\'_\'):\n    """"""Try replace decorated function by module.function.""""""\n    def _warn(e, warn):\n        if warn is None:\n            warn = \'\\n  Functionality might be degraded or be slow.\\n\'\n        elif warn is True:\n            warn = \'\'\n        elif not warn:\n            return\n        warnings.warn(\'%s%s\' % (e, warn))\n\n    try:\n        from importlib import import_module\n    except ImportError as e:\n        _warn(e, warn)\n        return identityfunc\n\n    def decorate(func, module_function=module_function, warn=warn):\n        module, function = module_function.split(\'.\')\n        try:\n            if package:\n                module = import_module(\'.\' + module, package=package)\n            else:\n                module = import_module(module)\n        except Exception as e:\n            _warn(e, warn)\n            return func\n        try:\n            func, oldfunc = getattr(module, function), func\n        except Exception as e:\n            _warn(e, warn)\n            return func\n        globals()[prefix + func.__name__] = oldfunc\n        return func\n\n    return decorate\n\n\ndef decode_floats(data):\n    """"""Decode floating point horizontal differencing.\n\n    The TIFF predictor type 3 reorders the bytes of the image values and\n    applies horizontal byte differencing to improve compression of floating\n    point images. The ordering of interleaved color channels is preserved.\n\n    Parameters\n    ----------\n    data : numpy.ndarray\n        The image to be decoded. The dtype must be a floating point.\n        The shape must include the number of contiguous samples per pixel\n        even if 1.\n\n    """"""\n    shape = data.shape\n    dtype = data.dtype\n    if len(shape) < 3:\n        raise ValueError(\'invalid data shape\')\n    if dtype.char not in \'dfe\':\n        raise ValueError(\'not a floating point image\')\n    littleendian = data.dtype.byteorder == \'<\' or (\n        sys.byteorder == \'little\' and data.dtype.byteorder == \'=\')\n    # undo horizontal byte differencing\n    data = data.view(\'uint8\')\n    data.shape = shape[:-2] + (-1,) + shape[-1:]\n    numpy.cumsum(data, axis=-2, dtype=\'uint8\', out=data)\n    # reorder bytes\n    if littleendian:\n        data.shape = shape[:-2] + (-1,) + shape[-2:]\n    data = numpy.swapaxes(data, -3, -2)\n    data = numpy.swapaxes(data, -2, -1)\n    data = data[..., ::-1]\n    # back to float\n    data = numpy.ascontiguousarray(data)\n    data = data.view(dtype)\n    data.shape = shape\n    return data\n\n\ndef decode_jpeg(encoded, tables=b\'\', photometric=None,\n                ycbcrsubsampling=None, ycbcrpositioning=None):\n    """"""Decode JPEG encoded byte string (using _czifile extension module).""""""\n    from czifile import _czifile\n    image = _czifile.decode_jpeg(encoded, tables)\n    if photometric == 2 and ycbcrsubsampling and ycbcrpositioning:\n        # TODO: convert YCbCr to RGB\n        pass\n    return image.tostring()\n\n\n@_replace_by(\'_tifffile.decode_packbits\')\ndef decode_packbits(encoded):\n    """"""Decompress PackBits encoded byte string.\n\n    PackBits is a simple byte-oriented run-length compression scheme.\n\n    """"""\n    func = ord if sys.version[0] == \'2\' else identityfunc\n    result = []\n    result_extend = result.extend\n    i = 0\n    try:\n        while True:\n            n = func(encoded[i]) + 1\n            i += 1\n            if n < 129:\n                result_extend(encoded[i:i+n])\n                i += n\n            elif n > 129:\n                result_extend(encoded[i:i+1] * (258-n))\n                i += 1\n    except IndexError:\n        pass\n    return b\'\'.join(result) if sys.version[0] == \'2\' else bytes(result)\n\n\n@_replace_by(\'_tifffile.decode_lzw\')\ndef decode_lzw(encoded):\n    """"""Decompress LZW (Lempel-Ziv-Welch) encoded TIFF strip (byte string).\n\n    The strip must begin with a CLEAR code and end with an EOI code.\n\n    This implementation of the LZW decoding algorithm is described in (1) and\n    is not compatible with old style LZW compressed files like quad-lzw.tif.\n\n    """"""\n    len_encoded = len(encoded)\n    bitcount_max = len_encoded * 8\n    unpack = struct.unpack\n\n    if sys.version[0] == \'2\':\n        newtable = [chr(i) for i in range(256)]\n    else:\n        newtable = [bytes([i]) for i in range(256)]\n    newtable.extend((0, 0))\n\n    def next_code():\n        """"""Return integer of \'bitw\' bits at \'bitcount\' position in encoded.""""""\n        start = bitcount // 8\n        s = encoded[start:start+4]\n        try:\n            code = unpack(\'>I\', s)[0]\n        except Exception:\n            code = unpack(\'>I\', s + b\'\\x00\'*(4-len(s)))[0]\n        code <<= bitcount % 8\n        code &= mask\n        return code >> shr\n\n    switchbitch = {  # code: bit-width, shr-bits, bit-mask\n        255: (9, 23, int(9*\'1\'+\'0\'*23, 2)),\n        511: (10, 22, int(10*\'1\'+\'0\'*22, 2)),\n        1023: (11, 21, int(11*\'1\'+\'0\'*21, 2)),\n        2047: (12, 20, int(12*\'1\'+\'0\'*20, 2)), }\n    bitw, shr, mask = switchbitch[255]\n    bitcount = 0\n\n    if len_encoded < 4:\n        raise ValueError(\'strip must be at least 4 characters long\')\n\n    if next_code() != 256:\n        raise ValueError(\'strip must begin with CLEAR code\')\n\n    code = 0\n    oldcode = 0\n    result = []\n    result_append = result.append\n    while True:\n        code = next_code()  # ~5% faster when inlining this function\n        bitcount += bitw\n        if code == 257 or bitcount >= bitcount_max:  # EOI\n            break\n        if code == 256:  # CLEAR\n            table = newtable[:]\n            table_append = table.append\n            lentable = 258\n            bitw, shr, mask = switchbitch[255]\n            code = next_code()\n            bitcount += bitw\n            if code == 257:  # EOI\n                break\n            result_append(table[code])\n        else:\n            if code < lentable:\n                decoded = table[code]\n                newcode = table[oldcode] + decoded[:1]\n            else:\n                newcode = table[oldcode]\n                newcode += newcode[:1]\n                decoded = newcode\n            result_append(decoded)\n            table_append(newcode)\n            lentable += 1\n        oldcode = code\n        if lentable in switchbitch:\n            bitw, shr, mask = switchbitch[lentable]\n\n    if code != 257:\n        warnings.warn(\'unexpected end of LZW stream (code %i)\' % code)\n\n    return b\'\'.join(result)\n\n\n@_replace_by(\'_tifffile.unpack_ints\')\ndef unpack_ints(data, dtype, itemsize, runlen=0):\n    """"""Decompress byte string to array of integers of any bit size <= 32.\n\n    This Python implementation is slow and only handles itemsizes 1, 2, 4, 8,\n    16, 32, and 64.\n\n    Parameters\n    ----------\n    data : byte str\n        Data to decompress.\n    dtype : numpy.dtype or str\n        A numpy boolean or integer type.\n    itemsize : int\n        Number of bits per integer.\n    runlen : int\n        Number of consecutive integers, after which to start at next byte.\n\n    Examples\n    --------\n    >>> unpack_ints(b\'a\', \'B\', 1)\n    array([0, 1, 1, 0, 0, 0, 0, 1], dtype=uint8)\n    >>> unpack_ints(b\'ab\', \'B\', 2)\n    array([1, 2, 0, 1, 1, 2, 0, 2], dtype=uint8)\n\n    """"""\n    if itemsize == 1:  # bitarray\n        data = numpy.frombuffer(data, \'|B\')\n        data = numpy.unpackbits(data)\n        if runlen % 8:\n            data = data.reshape(-1, runlen + (8 - runlen % 8))\n            data = data[:, :runlen].reshape(-1)\n        return data.astype(dtype)\n\n    dtype = numpy.dtype(dtype)\n    if itemsize in (8, 16, 32, 64):\n        return numpy.frombuffer(data, dtype)\n    if itemsize not in (1, 2, 4, 8, 16, 32):\n        raise ValueError(\'itemsize not supported: %i\' % itemsize)\n    if dtype.kind not in \'biu\':\n        raise ValueError(\'invalid dtype\')\n\n    itembytes = next(i for i in (1, 2, 4, 8) if 8 * i >= itemsize)\n    if itembytes != dtype.itemsize:\n        raise ValueError(\'dtype.itemsize too small\')\n    if runlen == 0:\n        runlen = (8 * len(data)) // itemsize\n    skipbits = runlen * itemsize % 8\n    if skipbits:\n        skipbits = 8 - skipbits\n    shrbits = itembytes*8 - itemsize\n    bitmask = int(itemsize*\'1\'+\'0\'*shrbits, 2)\n    dtypestr = \'>\' + dtype.char  # dtype always big-endian?\n\n    unpack = struct.unpack\n    size = runlen * (len(data)*8 // (runlen*itemsize + skipbits))\n    result = numpy.empty((size,), dtype)\n    bitcount = 0\n    for i in range(size):\n        start = bitcount // 8\n        s = data[start:start+itembytes]\n        try:\n            code = unpack(dtypestr, s)[0]\n        except Exception:\n            code = unpack(dtypestr, s + b\'\\x00\'*(itembytes-len(s)))[0]\n        code <<= bitcount % 8\n        code &= bitmask\n        result[i] = code >> shrbits\n        bitcount += itemsize\n        if (i+1) % runlen == 0:\n            bitcount += skipbits\n    return result\n\n\ndef unpack_rgb(data, dtype=\'<B\', bitspersample=(5, 6, 5), rescale=True):\n    """"""Return array from byte string containing packed samples.\n\n    Use to unpack RGB565 or RGB555 to RGB888 format.\n\n    Parameters\n    ----------\n    data : byte str\n        The data to be decoded. Samples in each pixel are stored consecutively.\n        Pixels are aligned to 8, 16, or 32 bit boundaries.\n    dtype : numpy.dtype\n        The sample data type. The byteorder applies also to the data stream.\n    bitspersample : tuple\n        Number of bits for each sample in a pixel.\n    rescale : bool\n        Upscale samples to the number of bits in dtype.\n\n    Returns\n    -------\n    result : ndarray\n        Flattened array of unpacked samples of native dtype.\n\n    Examples\n    --------\n    >>> data = struct.pack(\'BBBB\', 0x21, 0x08, 0xff, 0xff)\n    >>> print(unpack_rgb(data, \'<B\', (5, 6, 5), False))\n    [ 1  1  1 31 63 31]\n    >>> print(unpack_rgb(data, \'<B\', (5, 6, 5)))\n    [  8   4   8 255 255 255]\n    >>> print(unpack_rgb(data, \'<B\', (5, 5, 5)))\n    [ 16   8   8 255 255 255]\n\n    """"""\n    dtype = numpy.dtype(dtype)\n    bits = int(numpy.sum(bitspersample))\n    if not (bits <= 32 and all(i <= dtype.itemsize*8 for i in bitspersample)):\n        raise ValueError(\'sample size not supported: %s\' % str(bitspersample))\n    dt = next(i for i in \'BHI\' if numpy.dtype(i).itemsize*8 >= bits)\n    data = numpy.frombuffer(data, dtype.byteorder+dt)\n    result = numpy.empty((data.size, len(bitspersample)), dtype.char)\n    for i, bps in enumerate(bitspersample):\n        t = data >> int(numpy.sum(bitspersample[i+1:]))\n        t &= int(\'0b\'+\'1\'*bps, 2)\n        if rescale:\n            o = ((dtype.itemsize * 8) // bps + 1) * bps\n            if o > data.dtype.itemsize * 8:\n                t = t.astype(\'I\')\n            t *= (2**o - 1) // (2**bps - 1)\n            t //= 2**(o - (dtype.itemsize * 8))\n        result[:, i] = t\n    return result.reshape(-1)\n\n\n@_replace_by(\'_tifffile.reverse_bitorder\')\ndef reverse_bitorder(data):\n    """"""Reverse bits in each byte of byte string or numpy array.\n\n    Decode data where pixels with lower column values are stored in the\n    lower-order bits of the bytes (FillOrder is LSB2MSB).\n\n    Parameters\n    ----------\n    data : byte string or ndarray\n        The data to be bit reversed. If byte string, a new bit-reversed byte\n        string is returned. Numpy arrays are bit-reversed in-place.\n\n    Examples\n    --------\n    >>> reverse_bitorder(b\'\\\\x01\\\\x64\')\n    b\'\\\\x80&\'\n    >>> data = numpy.array([1, 666], dtype=\'uint16\')\n    >>> reverse_bitorder(data)\n    >>> data\n    array([  128, 16473], dtype=uint16)\n\n    """"""\n    try:\n        view = data.view(\'uint8\')\n        numpy.take(TIFF.REVERSE_BITORDER_ARRAY, view, out=view)\n    except AttributeError:\n        return data.translate(TIFF.REVERSE_BITORDER_BYTES)\n    except ValueError:\n        raise NotImplementedError(\'slices of arrays not supported\')\n\n\ndef apply_colormap(image, colormap, contig=True):\n    """"""Return palette-colored image.\n\n    The image values are used to index the colormap on axis 1. The returned\n    image is of shape image.shape+colormap.shape[0] and dtype colormap.dtype.\n\n    Parameters\n    ----------\n    image : numpy.ndarray\n        Indexes into the colormap.\n    colormap : numpy.ndarray\n        RGB lookup table aka palette of shape (3, 2**bits_per_sample).\n    contig : bool\n        If True, return a contiguous array.\n\n    Examples\n    --------\n    >>> image = numpy.arange(256, dtype=\'uint8\')\n    >>> colormap = numpy.vstack([image, image, image]).astype(\'uint16\') * 256\n    >>> apply_colormap(image, colormap)[-1]\n    array([65280, 65280, 65280], dtype=uint16)\n\n    """"""\n    image = numpy.take(colormap, image, axis=1)\n    image = numpy.rollaxis(image, 0, image.ndim)\n    if contig:\n        image = numpy.ascontiguousarray(image)\n    return image\n\n\ndef reorient(image, orientation):\n    """"""Return reoriented view of image array.\n\n    Parameters\n    ----------\n    image : numpy.ndarray\n        Non-squeezed output of asarray() functions.\n        Axes -3 and -2 must be image length and width respectively.\n    orientation : int or str\n        One of TIFF.ORIENTATION names or values.\n\n    """"""\n    ORIENTATION = TIFF.ORIENTATION\n    orientation = enumarg(ORIENTATION, orientation)\n\n    if orientation == ORIENTATION.TOPLEFT:\n        return image\n    elif orientation == ORIENTATION.TOPRIGHT:\n        return image[..., ::-1, :]\n    elif orientation == ORIENTATION.BOTLEFT:\n        return image[..., ::-1, :, :]\n    elif orientation == ORIENTATION.BOTRIGHT:\n        return image[..., ::-1, ::-1, :]\n    elif orientation == ORIENTATION.LEFTTOP:\n        return numpy.swapaxes(image, -3, -2)\n    elif orientation == ORIENTATION.RIGHTTOP:\n        return numpy.swapaxes(image, -3, -2)[..., ::-1, :]\n    elif orientation == ORIENTATION.RIGHTBOT:\n        return numpy.swapaxes(image, -3, -2)[..., ::-1, :, :]\n    elif orientation == ORIENTATION.LEFTBOT:\n        return numpy.swapaxes(image, -3, -2)[..., ::-1, ::-1, :]\n\n\ndef repeat_nd(a, repeats):\n    """"""Return read-only view into input array with elements repeated.\n\n    Zoom nD image by integer factors using nearest neighbor interpolation\n    (box filter).\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    repeats : sequence of int\n        The number of repetitions to apply along each dimension of input array.\n\n    Example\n    -------\n    >>> repeat_nd([[1, 2], [3, 4]], (2, 2))\n    array([[1, 1, 2, 2],\n           [1, 1, 2, 2],\n           [3, 3, 4, 4],\n           [3, 3, 4, 4]])\n\n    """"""\n    a = numpy.asarray(a)\n    reshape = []\n    shape = []\n    strides = []\n    for i, j, k in zip(a.strides, a.shape, repeats):\n        shape.extend((j, k))\n        strides.extend((i, 0))\n        reshape.append(j * k)\n    return numpy.lib.stride_tricks.as_strided(\n        a, shape, strides, writeable=False).reshape(reshape)\n\n\ndef reshape_nd(data_or_shape, ndim):\n    """"""Return image array or shape with at least ndim dimensions.\n\n    Prepend 1s to image shape as necessary.\n\n    >>> reshape_nd(numpy.empty(0), 1).shape\n    (0,)\n    >>> reshape_nd(numpy.empty(1), 2).shape\n    (1, 1)\n    >>> reshape_nd(numpy.empty((2, 3)), 3).shape\n    (1, 2, 3)\n    >>> reshape_nd(numpy.empty((3, 4, 5)), 3).shape\n    (3, 4, 5)\n    >>> reshape_nd((2, 3), 3)\n    (1, 2, 3)\n\n    """"""\n    is_shape = isinstance(data_or_shape, tuple)\n    shape = data_or_shape if is_shape else data_or_shape.shape\n    if len(shape) >= ndim:\n        return data_or_shape\n    shape = (1,) * (ndim - len(shape)) + shape\n    return shape if is_shape else data_or_shape.reshape(shape)\n\n\ndef squeeze_axes(shape, axes, skip=\'XY\'):\n    """"""Return shape and axes with single-dimensional entries removed.\n\n    Remove unused dimensions unless their axes are listed in \'skip\'.\n\n    >>> squeeze_axes((5, 1, 2, 1, 1), \'TZYXC\')\n    ((5, 2, 1), \'TYX\')\n\n    """"""\n    if len(shape) != len(axes):\n        raise ValueError(\'dimensions of axes and shape do not match\')\n    shape, axes = zip(*(i for i in zip(shape, axes)\n                        if i[0] > 1 or i[1] in skip))\n    return tuple(shape), \'\'.join(axes)\n\n\ndef transpose_axes(image, axes, asaxes=\'CTZYX\'):\n    """"""Return image with its axes permuted to match specified axes.\n\n    A view is returned if possible.\n\n    >>> transpose_axes(numpy.zeros((2, 3, 4, 5)), \'TYXC\', asaxes=\'CTZYX\').shape\n    (5, 2, 1, 3, 4)\n\n    """"""\n    for ax in axes:\n        if ax not in asaxes:\n            raise ValueError(\'unknown axis %s\' % ax)\n    # add missing axes to image\n    shape = image.shape\n    for ax in reversed(asaxes):\n        if ax not in axes:\n            axes = ax + axes\n            shape = (1,) + shape\n    image = image.reshape(shape)\n    # transpose axes\n    image = image.transpose([axes.index(ax) for ax in asaxes])\n    return image\n\n\ndef reshape_axes(axes, shape, newshape, unknown=\'Q\'):\n    """"""Return axes matching new shape.\n\n    Unknown dimensions are labelled \'Q\'.\n\n    >>> reshape_axes(\'YXS\', (219, 301, 1), (219, 301))\n    \'YX\'\n    >>> reshape_axes(\'IYX\', (12, 219, 301), (3, 4, 219, 1, 301, 1))\n    \'QQYQXQ\'\n\n    """"""\n    shape = tuple(shape)\n    newshape = tuple(newshape)\n    if len(axes) != len(shape):\n        raise ValueError(\'axes do not match shape\')\n\n    size = product(shape)\n    newsize = product(newshape)\n    if size != newsize:\n        raise ValueError(\'cannot reshape %s to %s\' % (shape, newshape))\n    if not axes or not newshape:\n        return \'\'\n\n    lendiff = max(0, len(shape) - len(newshape))\n    if lendiff:\n        newshape = newshape + (1,) * lendiff\n\n    i = len(shape)-1\n    prodns = 1\n    prods = 1\n    result = []\n    for ns in newshape[::-1]:\n        prodns *= ns\n        while i > 0 and shape[i] == 1 and ns != 1:\n            i -= 1\n        if ns == shape[i] and prodns == prods*shape[i]:\n            prods *= shape[i]\n            result.append(axes[i])\n            i -= 1\n        else:\n            result.append(unknown)\n\n    return \'\'.join(reversed(result[lendiff:]))\n\n\ndef stack_pages(pages, out=None, maxworkers=1, *args, **kwargs):\n    """"""Read data from sequence of TiffPage and stack them vertically.\n\n    Additional parameters are passsed to the TiffPage.asarray function.\n\n    """"""\n    npages = len(pages)\n    if npages == 0:\n        raise ValueError(\'no pages\')\n\n    if npages == 1:\n        return pages[0].asarray(out=out, *args, **kwargs)\n\n    page0 = next(p for p in pages if p is not None)\n    page0.asarray(validate=None)  # ThreadPoolExecutor swallows exceptions\n    shape = (npages,) + page0.keyframe.shape\n    dtype = page0.keyframe.dtype\n    out = create_output(out, shape, dtype)\n\n    if maxworkers is None:\n        maxworkers = multiprocessing.cpu_count() // 2\n    page0.parent.filehandle.lock = maxworkers > 1\n\n    filecache = OpenFileCache(size=max(4, maxworkers),\n                              lock=page0.parent.filehandle.lock)\n\n    def func(page, index, out=out, filecache=filecache,\n             args=args, kwargs=kwargs):\n        """"""Read, decode, and copy page data.""""""\n        if page is not None:\n            filecache.open(page.parent.filehandle)\n            out[index] = page.asarray(lock=filecache.lock, reopen=False,\n                                      validate=False, *args, **kwargs)\n            filecache.close(page.parent.filehandle)\n\n    if maxworkers < 2:\n        for i, page in enumerate(pages):\n            func(page, i)\n    else:\n        with concurrent.futures.ThreadPoolExecutor(maxworkers) as executor:\n            executor.map(func, pages, range(npages))\n\n    filecache.clear()\n    page0.parent.filehandle.lock = None\n\n    return out\n\n\ndef clean_offsets_counts(offsets, counts):\n    """"""Return cleaned offsets and byte counts.\n\n    Remove zero offsets and counts. Use to sanitize _offsets and _bytecounts\n    tag values for strips or tiles.\n\n    """"""\n    offsets = list(offsets)\n    counts = list(counts)\n    assert len(offsets) == len(counts)\n    j = 0\n    for i, (o, b) in enumerate(zip(offsets, counts)):\n        if o > 0 and b > 0:\n            if i > j:\n                offsets[j] = o\n                counts[j] = b\n            j += 1\n        elif b > 0 and o <= 0:\n            raise ValueError(\'invalid offset\')\n        else:\n            warnings.warn(\'empty byte count\')\n    if j == 0:\n        j = 1\n    return offsets[:j], counts[:j]\n\n\ndef buffered_read(fh, lock, offsets, bytecounts, buffersize=2**26):\n    """"""Return iterator over blocks read from file.""""""\n    length = len(offsets)\n    i = 0\n    while i < length:\n        data = []\n        with lock:\n            size = 0\n            while size < buffersize and i < length:\n                fh.seek(offsets[i])\n                bytecount = bytecounts[i]\n                data.append(fh.read(bytecount))\n                size += bytecount\n                i += 1\n        for block in data:\n            yield block\n\n\ndef create_output(out, shape, dtype, mode=\'w+\', suffix=\'.memmap\'):\n    """"""Return numpy array where image data of shape and dtype can be copied.\n\n    The \'out\' parameter may have the following values or types:\n\n    None\n        An empty array of shape and dtype is created and returned.\n    numpy.ndarray\n        An existing writable array of compatible dtype and shape. A view of\n        the same array is returned after verification.\n    \'memmap\' or \'memmap:tempdir\'\n        A memory-map to an array stored in a temporary binary file on disk\n        is created and returned.\n    str or open file\n        The file name or file object used to create a memory-map to an array\n        stored in a binary file on disk. The created memory-mapped array is\n        returned.\n\n    """"""\n    if out is None:\n        return numpy.zeros(shape, dtype)\n    if isinstance(out, str) and out[:6] == \'memmap\':\n        tempdir = out[7:] if len(out) > 7 else None\n        with tempfile.NamedTemporaryFile(dir=tempdir, suffix=suffix) as fh:\n            return numpy.memmap(fh, shape=shape, dtype=dtype, mode=mode)\n    if isinstance(out, numpy.ndarray):\n        if product(shape) != product(out.shape):\n            raise ValueError(\'incompatible output shape\')\n        if not numpy.can_cast(dtype, out.dtype):\n            raise ValueError(\'incompatible output dtype\')\n        return out.reshape(shape)\n    return numpy.memmap(out, shape=shape, dtype=dtype, mode=mode)\n\n\ndef matlabstr2py(string):\n    """"""Return Python object from Matlab string representation.\n\n    Return str, bool, int, float, list (Matlab arrays or cells), or\n    dict (Matlab structures) types.\n\n    Use to access ScanImage metadata.\n\n    >>> matlabstr2py(\'1\')\n    1\n    >>> matlabstr2py(""[\'x y z\' true false; 1 2.0 -3e4; NaN Inf @class]"")\n    [[\'x y z\', True, False], [1, 2.0, -30000.0], [nan, inf, \'@class\']]\n    >>> d = matlabstr2py(""SI.hChannels.channelType = {\'stripe\' \'stripe\'}\\\\n""\n    ...                  ""SI.hChannels.channelsActive = 2"")\n    >>> d[\'SI.hChannels.channelType\']\n    [\'stripe\', \'stripe\']\n\n    """"""\n    # TODO: handle invalid input\n    # TODO: review unboxing of multidimensional arrays\n\n    def lex(s):\n        # return sequence of tokens from matlab string representation\n        tokens = [\'[\']\n        while True:\n            t, i = next_token(s)\n            if t is None:\n                break\n            if t == \';\':\n                tokens.extend((\']\', \'[\'))\n            elif t == \'[\':\n                tokens.extend((\'[\', \'[\'))\n            elif t == \']\':\n                tokens.extend((\']\', \']\'))\n            else:\n                tokens.append(t)\n            s = s[i:]\n        tokens.append(\']\')\n        return tokens\n\n    def next_token(s):\n        # return next token in matlab string\n        length = len(s)\n        if length == 0:\n            return None, 0\n        i = 0\n        while i < length and s[i] == \' \':\n            i += 1\n        if i == length:\n            return None, i\n        if s[i] in \'{[;]}\':\n            return s[i], i + 1\n        if s[i] == ""\'"":\n            j = i + 1\n            while j < length and s[j] != ""\'"":\n                j += 1\n            return s[i: j+1], j + 1\n        if s[i] == \'<\':\n            j = i + 1\n            while j < length and s[j] != \'>\':\n                j += 1\n            return s[i: j+1], j + 1\n        j = i\n        while j < length and not s[j] in \' {[;]}\':\n            j += 1\n        return s[i:j], j\n\n    def value(s, fail=False):\n        # return Python value of token\n        s = s.strip()\n        if not s:\n            return s\n        if len(s) == 1:\n            try:\n                return int(s)\n            except Exception:\n                if fail:\n                    raise ValueError()\n                return s\n        if s[0] == ""\'"":\n            if fail and s[-1] != ""\'"" or ""\'"" in s[1:-1]:\n                raise ValueError()\n            return s[1:-1]\n        if s[0] == \'<\':\n            if fail and s[-1] != \'>\' or \'<\' in s[1:-1]:\n                raise ValueError()\n            return s\n        if fail and any(i in s for i in "" \';[]{}""):\n            raise ValueError()\n        if s[0] == \'@\':\n            return s\n        if s in (\'true\', \'True\'):\n            return True\n        if s in (\'false\', \'False\'):\n            return False\n        if s[:6] == \'zeros(\':\n            return numpy.zeros([int(i) for i in s[6:-1].split(\',\')]).tolist()\n        if s[:5] == \'ones(\':\n            return numpy.ones([int(i) for i in s[5:-1].split(\',\')]).tolist()\n        if \'.\' in s or \'e\' in s:\n            try:\n                return float(s)\n            except Exception:\n                pass\n        try:\n            return int(s)\n        except Exception:\n            pass\n        try:\n            return float(s)  # nan, inf\n        except Exception:\n            if fail:\n                raise ValueError()\n        return s\n\n    def parse(s):\n        # return Python value from string representation of Matlab value\n        s = s.strip()\n        try:\n            return value(s, fail=True)\n        except ValueError:\n            pass\n        result = add2 = []\n        levels = [add2]\n        for t in lex(s):\n            if t in \'[{\':\n                add2 = []\n                levels.append(add2)\n            elif t in \']}\':\n                x = levels.pop()\n                if len(x) == 1 and isinstance(x[0], (list, str)):\n                    x = x[0]\n                add2 = levels[-1]\n                add2.append(x)\n            else:\n                add2.append(value(t))\n        if len(result) == 1 and isinstance(result[0], (list, str)):\n            result = result[0]\n        return result\n\n    if \'\\r\' in string or \'\\n\' in string:\n        # structure\n        d = {}\n        for line in string.splitlines():\n            line = line.strip()\n            if not line or line[0] == \'%\':\n                continue\n            k, v = line.split(\'=\', 1)\n            k = k.strip()\n            if any(c in k for c in "" \';[]{}<>""):\n                continue\n            d[k] = parse(v)\n        return d\n    return parse(string)\n\n\ndef stripnull(string, null=b\'\\x00\'):\n    """"""Return string truncated at first null character.\n\n    Clean NULL terminated C strings. For unicode strings use null=\'\\\\0\'.\n\n    >>> stripnull(b\'string\\\\x00\')\n    b\'string\'\n    >>> stripnull(\'string\\\\x00\', null=\'\\\\0\')\n    \'string\'\n\n    """"""\n    i = string.find(null)\n    return string if (i < 0) else string[:i]\n\n\ndef stripascii(string):\n    """"""Return string truncated at last byte that is 7-bit ASCII.\n\n    Clean NULL separated and terminated TIFF strings.\n\n    >>> stripascii(b\'string\\\\x00string\\\\n\\\\x01\\\\x00\')\n    b\'string\\\\x00string\\\\n\'\n    >>> stripascii(b\'\\\\x00\')\n    b\'\'\n\n    """"""\n    # TODO: pythonize this\n    i = len(string)\n    while i:\n        i -= 1\n        if 8 < byte2int(string[i]) < 127:\n            break\n    else:\n        i = -1\n    return string[:i+1]\n\n\ndef asbool(value, true=(b\'true\', u\'true\'), false=(b\'false\', u\'false\')):\n    """"""Return string as bool if possible, else raise TypeError.\n\n    >>> asbool(b\' False \')\n    False\n\n    """"""\n    value = value.strip().lower()\n    if value in true:  # might raise UnicodeWarning/BytesWarning\n        return True\n    if value in false:\n        return False\n    raise TypeError()\n\n\ndef astype(value, types=None):\n    """"""Return argument as one of types if possible.\n\n    >>> astype(\'42\')\n    42\n    >>> astype(\'3.14\')\n    3.14\n    >>> astype(\'True\')\n    True\n    >>> astype(b\'Neee-Wom\')\n    \'Neee-Wom\'\n\n    """"""\n    if types is None:\n        types = int, float, asbool, bytes2str\n    for typ in types:\n        try:\n            return typ(value)\n        except (ValueError, AttributeError, TypeError, UnicodeEncodeError):\n            pass\n    return value\n\n\ndef format_size(size, threshold=1536):\n    """"""Return file size as string from byte size.\n\n    >>> format_size(1234)\n    \'1234 B\'\n    >>> format_size(12345678901)\n    \'11.50 GiB\'\n\n    """"""\n    if size < threshold:\n        return ""%i B"" % size\n    for unit in (\'KiB\', \'MiB\', \'GiB\', \'TiB\', \'PiB\'):\n        size /= 1024.0\n        if size < threshold:\n            return ""%.2f %s"" % (size, unit)\n\n\ndef identityfunc(arg):\n    """"""Single argument identity function.\n\n    >>> identityfunc(\'arg\')\n    \'arg\'\n\n    """"""\n    return arg\n\n\ndef nullfunc(*args, **kwargs):\n    """"""Null function.\n\n    >>> nullfunc(\'arg\', kwarg=\'kwarg\')\n\n    """"""\n    return\n\n\ndef sequence(value):\n    """"""Return tuple containing value if value is not a sequence.\n\n    >>> sequence(1)\n    (1,)\n    >>> sequence([1])\n    [1]\n\n    """"""\n    try:\n        len(value)\n        return value\n    except TypeError:\n        return (value,)\n\n\ndef product(iterable):\n    """"""Return product of sequence of numbers.\n\n    Equivalent of functools.reduce(operator.mul, iterable, 1).\n    Multiplying numpy integers might overflow.\n\n    >>> product([2**8, 2**30])\n    274877906944\n    >>> product([])\n    1\n\n    """"""\n    prod = 1\n    for i in iterable:\n        prod *= i\n    return prod\n\n\ndef natural_sorted(iterable):\n    """"""Return human sorted list of strings.\n\n    E.g. for sorting file names.\n\n    >>> natural_sorted([\'f1\', \'f2\', \'f10\'])\n    [\'f1\', \'f2\', \'f10\']\n\n    """"""\n    def sortkey(x):\n        return [(int(c) if c.isdigit() else c) for c in re.split(numbers, x)]\n\n    numbers = re.compile(r\'(\\d+)\')\n    return sorted(iterable, key=sortkey)\n\n\ndef excel_datetime(timestamp, epoch=datetime.datetime.fromordinal(693594)):\n    """"""Return datetime object from timestamp in Excel serial format.\n\n    Convert LSM time stamps.\n\n    >>> excel_datetime(40237.029999999795)\n    datetime.datetime(2010, 2, 28, 0, 43, 11, 999982)\n\n    """"""\n    return epoch + datetime.timedelta(timestamp)\n\n\ndef julian_datetime(julianday, milisecond=0):\n    """"""Return datetime from days since 1/1/4713 BC and ms since midnight.\n\n    Convert Julian dates according to MetaMorph.\n\n    >>> julian_datetime(2451576, 54362783)\n    datetime.datetime(2000, 2, 2, 15, 6, 2, 783)\n\n    """"""\n    if julianday <= 1721423:\n        # no datetime before year 1\n        return None\n\n    a = julianday + 1\n    if a > 2299160:\n        alpha = math.trunc((a - 1867216.25) / 36524.25)\n        a += 1 + alpha - alpha // 4\n    b = a + (1524 if a > 1721423 else 1158)\n    c = math.trunc((b - 122.1) / 365.25)\n    d = math.trunc(365.25 * c)\n    e = math.trunc((b - d) / 30.6001)\n\n    day = b - d - math.trunc(30.6001 * e)\n    month = e - (1 if e < 13.5 else 13)\n    year = c - (4716 if month > 2.5 else 4715)\n\n    hour, milisecond = divmod(milisecond, 1000 * 60 * 60)\n    minute, milisecond = divmod(milisecond, 1000 * 60)\n    second, milisecond = divmod(milisecond, 1000)\n\n    return datetime.datetime(year, month, day,\n                             hour, minute, second, milisecond)\n\n\ndef byteorder_isnative(byteorder):\n    """"""Return if byteorder matches the system\'s byteorder.\n\n    >>> byteorder_isnative(\'=\')\n    True\n\n    """"""\n    if byteorder == \'=\' or byteorder == sys.byteorder:\n        return True\n    keys = {\'big\': \'>\', \'little\': \'<\'}\n    return keys.get(byteorder, byteorder) == keys[sys.byteorder]\n\n\ndef recarray2dict(recarray):\n    """"""Return numpy.recarray as dict.""""""\n    # TODO: subarrays\n    result = {}\n    for descr, value in zip(recarray.dtype.descr, recarray):\n        name, dtype = descr[:2]\n        if dtype[1] == \'S\':\n            value = bytes2str(stripnull(value))\n        elif value.ndim < 2:\n            value = value.tolist()\n        result[name] = value\n    return result\n\n\ndef xml2dict(xml, sanitize=True, prefix=None):\n    """"""Return XML as dict.\n\n    >>> xml2dict(\'<?xml version=""1.0"" ?><root attr=""name""><key>1</key></root>\')\n    {\'root\': {\'key\': 1, \'attr\': \'name\'}}\n\n    """"""\n    from xml.etree import cElementTree as etree  # delayed import\n\n    at = tx = \'\'\n    if prefix:\n        at, tx = prefix\n\n    def astype(value):\n        # return value as int, float, bool, or str\n        for t in (int, float, asbool):\n            try:\n                return t(value)\n            except Exception:\n                pass\n        return value\n\n    def etree2dict(t):\n        # adapted from https://stackoverflow.com/a/10077069/453463\n        key = t.tag\n        if sanitize:\n            key = key.rsplit(\'}\', 1)[-1]\n        d = {key: {} if t.attrib else None}\n        children = list(t)\n        if children:\n            dd = collections.defaultdict(list)\n            for dc in map(etree2dict, children):\n                for k, v in dc.items():\n                    dd[k].append(astype(v))\n            d = {key: {k: astype(v[0]) if len(v) == 1 else astype(v)\n                       for k, v in dd.items()}}\n        if t.attrib:\n            d[key].update((at + k, astype(v)) for k, v in t.attrib.items())\n        if t.text:\n            text = t.text.strip()\n            if children or t.attrib:\n                if text:\n                    d[key][tx + \'value\'] = astype(text)\n            else:\n                d[key] = astype(text)\n        return d\n\n    return etree2dict(etree.fromstring(xml))\n\n\ndef hexdump(bytestr, width=75, height=24, snipat=-2, modulo=2, ellipsis=\'...\'):\n    """"""Return hexdump representation of byte string.\n\n    >>> hexdump(binascii.unhexlify(\'49492a00080000000e00fe0004000100\'))\n    \'49 49 2a 00 08 00 00 00 0e 00 fe 00 04 00 01 00 II*.............\'\n\n    """"""\n    size = len(bytestr)\n    if size < 1 or width < 2 or height < 1:\n        return \'\'\n    if height == 1:\n        addr = b\'\'\n        bytesperline = min(modulo * (((width - len(addr)) // 4) // modulo),\n                           size)\n        if bytesperline < 1:\n            return \'\'\n        nlines = 1\n    else:\n        addr = b\'%%0%ix: \' % len(b\'%x\' % size)\n        bytesperline = min(modulo * (((width - len(addr % 1)) // 4) // modulo),\n                           size)\n        if bytesperline < 1:\n            return \'\'\n        width = 3*bytesperline + len(addr % 1)\n        nlines = (size - 1) // bytesperline + 1\n\n    if snipat is None or snipat == 1:\n        snipat = height\n    elif 0 < abs(snipat) < 1:\n        snipat = int(math.floor(height * snipat))\n    if snipat < 0:\n        snipat += height\n\n    if height == 1 or nlines == 1:\n        blocks = [(0, bytestr[:bytesperline])]\n        addr = b\'\'\n        height = 1\n        width = 3 * bytesperline\n    elif height is None or nlines <= height:\n        blocks = [(0, bytestr)]\n    elif snipat <= 0:\n        start = bytesperline * (nlines - height)\n        blocks = [(start, bytestr[start:])]  # (start, None)\n    elif snipat >= height or height < 3:\n        end = bytesperline * height\n        blocks = [(0, bytestr[:end])]  # (end, None)\n    else:\n        end1 = bytesperline * snipat\n        end2 = bytesperline * (height - snipat - 1)\n        blocks = [(0, bytestr[:end1]),\n                  (size-end1-end2, None),\n                  (size-end2, bytestr[size-end2:])]\n\n    ellipsis = str2bytes(ellipsis)\n    result = []\n    for start, bytestr in blocks:\n        if bytestr is None:\n            result.append(ellipsis)  # \'skip %i bytes\' % start)\n            continue\n        hexstr = binascii.hexlify(bytestr)\n        strstr = re.sub(br\'[^\\x20-\\x7f]\', b\'.\', bytestr)\n        for i in range(0, len(bytestr), bytesperline):\n            h = hexstr[2*i:2*i+bytesperline*2]\n            r = (addr % (i + start)) if height > 1 else addr\n            r += b\' \'.join(h[i:i+2] for i in range(0, 2*bytesperline, 2))\n            r += b\' \' * (width - len(r))\n            r += strstr[i:i+bytesperline]\n            result.append(r)\n    result = b\'\\n\'.join(result)\n    if sys.version_info[0] == 3:\n        result = result.decode(\'ascii\')\n    return result\n\n\ndef isprintable(string):\n    """"""Return if all characters in string are printable.\n\n    >>> isprintable(\'abc\')\n    True\n\n    >>> isprintable(b\'\\01\')\n    False\n\n    """"""\n    string = string.strip()\n    if len(string) < 1:\n        return True\n    if sys.version_info[0] == 3:\n        try:\n            return string.isprintable()\n        except Exception:\n            pass\n        try:\n            return string.decode(\'utf-8\').isprintable()\n        except Exception:\n            pass\n    else:\n        if string.isalnum():\n            return True\n        printable = (\'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRST\'\n                     \'UVWXYZ!""#$%&\\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0c\')\n        return all(c in printable for c in string)\n\n\ndef clean_whitespace(string, compact=False):\n    """"""Return string with compressed whitespace.""""""\n    for a, b in ((\'\\r\\n\', \'\\n\'), (\'\\r\', \'\\n\'), (\'\\n\\n\', \'\\n\'),\n                 (\'\\t\', \' \'), (\'  \', \' \')):\n        string = string.replace(a, b)\n    if compact:\n        for a, b in ((\'\\n\', \' \'), (\'[ \', \'[\'),\n                     (\'  \', \' \'), (\'  \', \' \'), (\'  \', \' \')):\n            string = string.replace(a, b)\n    return string.strip()\n\n\ndef pformat_xml(xml):\n    """"""Return pretty formatted XML.""""""\n    try:\n        import lxml.etree as etree  # delayed import\n        if not isinstance(xml, bytes):\n            xml = xml.encode(\'utf-8\')\n        xml = etree.parse(io.BytesIO(xml))\n        xml = etree.tostring(xml, pretty_print=True, xml_declaration=True,\n                             encoding=xml.docinfo.encoding)\n        xml = bytes2str(xml)\n    except Exception:\n        if isinstance(xml, bytes):\n            xml = bytes2str(xml)\n        xml = xml.replace(\'><\', \'>\\n<\')\n    return xml.replace(\'  \', \' \').replace(\'\\t\', \' \')\n\n\ndef pformat(arg, width=79, height=24, compact=True):\n    """"""Return pretty formatted representation of object as string.\n\n    Whitespace might be altered.\n\n    """"""\n    if height is None or height < 1:\n        height = 1024\n    if width is None or width < 1:\n        width = 256\n\n    npopt = numpy.get_printoptions()\n    numpy.set_printoptions(threshold=100, linewidth=width)\n\n    if isinstance(arg, basestring):\n        if arg[:5].lower() in (\'<?xml\', b\'<?xml\'):\n            if height == 1:\n                arg = arg[:4*width]\n            else:\n                arg = pformat_xml(arg)\n        elif isinstance(arg, bytes):\n            if isprintable(arg):\n                arg = bytes2str(arg)\n                arg = clean_whitespace(arg)\n            else:\n                numpy.set_printoptions(**npopt)\n                return hexdump(arg, width=width, height=height, modulo=1)\n        arg = arg.rstrip()\n    elif isinstance(arg, numpy.record):\n        arg = arg.pprint()\n    else:\n        import pprint  # delayed import\n        compact = {} if sys.version_info[0] == 2 else dict(compact=compact)\n        arg = pprint.pformat(arg, width=width, **compact)\n\n    numpy.set_printoptions(**npopt)\n\n    if height == 1:\n        arg = clean_whitespace(arg, compact=True)\n        return arg[:width]\n\n    argl = list(arg.splitlines())\n    if len(argl) > height:\n        arg = \'\\n\'.join(argl[:height//2] + [\'...\'] + argl[-height//2:])\n    return arg\n\n\ndef snipstr(string, width=79, snipat=0.5, ellipsis=\'...\'):\n    """"""Return string cut to specified length.\n\n    >>> snipstr(\'abcdefghijklmnop\', 8)\n    \'abc...op\'\n\n    """"""\n    if ellipsis is None:\n        if isinstance(string, bytes):\n            ellipsis = b\'...\'\n        else:\n            ellipsis = u\'\\u2026\'  # does not print on win-py3.5\n    esize = len(ellipsis)\n\n    splitlines = string.splitlines()\n    # TODO: finish and test multiline snip\n\n    result = []\n    for line in splitlines:\n        if line is None:\n            result.append(ellipsis)\n            continue\n        linelen = len(line)\n        if linelen <= width:\n            result.append(string)\n            continue\n\n        split = snipat\n        if split is None or split == 1:\n            split = linelen\n        elif 0 < abs(split) < 1:\n            split = int(math.floor(linelen * split))\n        if split < 0:\n            split += linelen\n            if split < 0:\n                split = 0\n\n        if esize == 0 or width < esize + 1:\n            if split <= 0:\n                result.append(string[-width:])\n            else:\n                result.append(string[:width])\n        elif split <= 0:\n            result.append(ellipsis + string[esize-width:])\n        elif split >= linelen or width < esize + 4:\n            result.append(string[:width-esize] + ellipsis)\n        else:\n            splitlen = linelen - width + esize\n            end1 = split - splitlen // 2\n            end2 = end1 + splitlen\n            result.append(string[:end1] + ellipsis + string[end2:])\n\n    if isinstance(string, bytes):\n        return b\'\\n\'.join(result)\n    else:\n        return \'\\n\'.join(result)\n\n\ndef enumarg(enum, arg):\n    """"""Return enum member from its name or value.\n\n    >>> enumarg(TIFF.PHOTOMETRIC, 2)\n    <PHOTOMETRIC.RGB: 2>\n    >>> enumarg(TIFF.PHOTOMETRIC, \'RGB\')\n    <PHOTOMETRIC.RGB: 2>\n\n    """"""\n    try:\n        return enum(arg)\n    except Exception:\n        try:\n            return enum[arg.upper()]\n        except Exception:\n            raise ValueError(\'invalid argument %s\' % arg)\n\n\ndef parse_kwargs(kwargs, *keys, **keyvalues):\n    """"""Return dict with keys from keys|keyvals and values from kwargs|keyvals.\n\n    Existing keys are deleted from kwargs.\n\n    >>> kwargs = {\'one\': 1, \'two\': 2, \'four\': 4}\n    >>> kwargs2 = parse_kwargs(kwargs, \'two\', \'three\', four=None, five=5)\n    >>> kwargs == {\'one\': 1}\n    True\n    >>> kwargs2 == {\'two\': 2, \'four\': 4, \'five\': 5}\n    True\n\n    """"""\n    result = {}\n    for key in keys:\n        if key in kwargs:\n            result[key] = kwargs[key]\n            del kwargs[key]\n    for key, value in keyvalues.items():\n        if key in kwargs:\n            result[key] = kwargs[key]\n            del kwargs[key]\n        else:\n            result[key] = value\n    return result\n\n\ndef update_kwargs(kwargs, **keyvalues):\n    """"""Update dict with keys and values if keys do not already exist.\n\n    >>> kwargs = {\'one\': 1, }\n    >>> update_kwargs(kwargs, one=None, two=2)\n    >>> kwargs == {\'one\': 1, \'two\': 2}\n    True\n\n    """"""\n    for key, value in keyvalues.items():\n        if key not in kwargs:\n            kwargs[key] = value\n\n\ndef lsm2bin(lsmfile, binfile=None, tile=(256, 256), verbose=True):\n    """"""Convert [MP]TZCYX LSM file to series of BIN files.\n\n    One BIN file containing \'ZCYX\' data are created for each position, time,\n    and tile. The position, time, and tile indices are encoded at the end\n    of the filenames.\n\n    """"""\n    verbose = print_ if verbose else nullfunc\n\n    if binfile is None:\n        binfile = lsmfile\n    elif binfile.lower() == \'none\':\n        binfile = None\n    if binfile:\n        binfile += \'_(z%ic%iy%ix%i)_m%%ip%%it%%03iy%%ix%%i.bin\'\n\n    verbose(\'\\nOpening LSM file... \', end=\'\', flush=True)\n    start_time = time.time()\n\n    with TiffFile(lsmfile) as lsm:\n        if not lsm.is_lsm:\n            verbose(\'\\n\', lsm, flush=True)\n            raise ValueError(\'not a LSM file\')\n        series = lsm.series[0]  # first series contains the image data\n        shape = series.shape\n        axes = series.axes\n        dtype = series.dtype\n        size = product(shape) * dtype.itemsize\n\n        verbose(\'%.3f s\' % (time.time() - start_time))\n        # verbose(lsm, flush=True)\n        verbose(\'Image\\n  axes:  %s\\n  shape: %s\\n  dtype: %s\\n  size:  %s\'\n                % (axes, shape, dtype, format_size(size)), flush=True)\n        if not series.axes.endswith(\'TZCYX\'):\n            raise ValueError(\'not a *TZCYX LSM file\')\n\n        verbose(\'Copying image from LSM to BIN files\', end=\'\', flush=True)\n        start_time = time.time()\n        tiles = shape[-2] // tile[-2], shape[-1] // tile[-1]\n        if binfile:\n            binfile = binfile % (shape[-4], shape[-3], tile[0], tile[1])\n        shape = (1,) * (7-len(shape)) + shape\n        # cache for ZCYX stacks and output files\n        data = numpy.empty(shape[3:], dtype=dtype)\n        out = numpy.empty((shape[-4], shape[-3], tile[0], tile[1]),\n                          dtype=dtype)\n        # iterate over Tiff pages containing data\n        pages = iter(series.pages)\n        for m in range(shape[0]):  # mosaic axis\n            for p in range(shape[1]):  # position axis\n                for t in range(shape[2]):  # time axis\n                    for z in range(shape[3]):  # z slices\n                        data[z] = next(pages).asarray()\n                    for y in range(tiles[0]):  # tile y\n                        for x in range(tiles[1]):  # tile x\n                            out[:] = data[...,\n                                          y*tile[0]:(y+1)*tile[0],\n                                          x*tile[1]:(x+1)*tile[1]]\n                            if binfile:\n                                out.tofile(binfile % (m, p, t, y, x))\n                            verbose(\'.\', end=\'\', flush=True)\n        verbose(\' %.3f s\' % (time.time() - start_time))\n\n\ndef imshow(data, title=None, vmin=0, vmax=None, cmap=None,\n           bitspersample=None, photometric=\'RGB\',\n           interpolation=None, dpi=96, figure=None, subplot=111, maxdim=32768,\n           **kwargs):\n    """"""Plot n-dimensional images using matplotlib.pyplot.\n\n    Return figure, subplot and plot axis.\n    Requires pyplot already imported C{from matplotlib import pyplot}.\n\n    Parameters\n    ----------\n    bitspersample : int or None\n        Number of bits per channel in integer RGB images.\n    photometric : {\'MINISWHITE\', \'MINISBLACK\', \'RGB\', or \'PALETTE\'}\n        The color space of the image data.\n    title : str\n        Window and subplot title.\n    figure : matplotlib.figure.Figure (optional).\n        Matplotlib to use for plotting.\n    subplot : int\n        A matplotlib.pyplot.subplot axis.\n    maxdim : int\n        maximum image width and length.\n    kwargs : optional\n        Arguments for matplotlib.pyplot.imshow.\n\n    """"""\n    isrgb = photometric in (\'RGB\',)  # \'PALETTE\'\n    if isrgb and not (data.shape[-1] in (3, 4) or (\n            data.ndim > 2 and data.shape[-3] in (3, 4))):\n        isrgb = False\n        photometric = \'MINISWHITE\'\n\n    data = data.squeeze()\n    if photometric in (\'MINISWHITE\', \'MINISBLACK\', None):\n        data = reshape_nd(data, 2)\n    else:\n        data = reshape_nd(data, 3)\n\n    dims = data.ndim\n    if dims < 2:\n        raise ValueError(\'not an image\')\n    elif dims == 2:\n        dims = 0\n        isrgb = False\n    else:\n        if isrgb and data.shape[-3] in (3, 4):\n            data = numpy.swapaxes(data, -3, -2)\n            data = numpy.swapaxes(data, -2, -1)\n        elif not isrgb and (data.shape[-1] < data.shape[-2] // 8 and\n                            data.shape[-1] < data.shape[-3] // 8 and\n                            data.shape[-1] < 5):\n            data = numpy.swapaxes(data, -3, -1)\n            data = numpy.swapaxes(data, -2, -1)\n        isrgb = isrgb and data.shape[-1] in (3, 4)\n        dims -= 3 if isrgb else 2\n\n    if isrgb:\n        data = data[..., :maxdim, :maxdim, :maxdim]\n    else:\n        data = data[..., :maxdim, :maxdim]\n\n    if photometric == \'PALETTE\' and isrgb:\n        datamax = data.max()\n        if datamax > 255:\n            data = data >> 8  # possible precision loss\n        data = data.astype(\'B\')\n    elif data.dtype.kind in \'ui\':\n        if not (isrgb and data.dtype.itemsize <= 1) or bitspersample is None:\n            try:\n                bitspersample = int(math.ceil(math.log(data.max(), 2)))\n            except Exception:\n                bitspersample = data.dtype.itemsize * 8\n        elif not isinstance(bitspersample, inttypes):\n            # bitspersample can be tuple, e.g. (5, 6, 5)\n            bitspersample = data.dtype.itemsize * 8\n        datamax = 2**bitspersample\n        if isrgb:\n            if bitspersample < 8:\n                data = data << (8 - bitspersample)\n            elif bitspersample > 8:\n                data = data >> (bitspersample - 8)  # precision loss\n            data = data.astype(\'B\')\n    elif data.dtype.kind == \'f\':\n        datamax = data.max()\n        if isrgb and datamax > 1.0:\n            if data.dtype.char == \'d\':\n                data = data.astype(\'f\')\n                data /= datamax\n            else:\n                data = data / datamax\n    elif data.dtype.kind == \'b\':\n        datamax = 1\n    elif data.dtype.kind == \'c\':\n        data = numpy.absolute(data)\n        datamax = data.max()\n\n    if not isrgb:\n        if vmax is None:\n            vmax = datamax\n        if vmin is None:\n            if data.dtype.kind == \'i\':\n                dtmin = numpy.iinfo(data.dtype).min\n                vmin = numpy.min(data)\n                if vmin == dtmin:\n                    vmin = numpy.min(data > dtmin)\n            if data.dtype.kind == \'f\':\n                dtmin = numpy.finfo(data.dtype).min\n                vmin = numpy.min(data)\n                if vmin == dtmin:\n                    vmin = numpy.min(data > dtmin)\n            else:\n                vmin = 0\n\n    pyplot = sys.modules[\'matplotlib.pyplot\']\n\n    if figure is None:\n        pyplot.rc(\'font\', family=\'sans-serif\', weight=\'normal\', size=8)\n        figure = pyplot.figure(dpi=dpi, figsize=(10.3, 6.3), frameon=True,\n                               facecolor=\'1.0\', edgecolor=\'w\')\n        try:\n            figure.canvas.manager.window.title(title)\n        except Exception:\n            pass\n        size = len(title.splitlines()) if title else 1\n        pyplot.subplots_adjust(bottom=0.03*(dims+2), top=0.98-size*0.03,\n                               left=0.1, right=0.95, hspace=0.05, wspace=0.0)\n    subplot = pyplot.subplot(subplot)\n\n    if title:\n        try:\n            title = unicode(title, \'Windows-1252\')\n        except TypeError:\n            pass\n        pyplot.title(title, size=11)\n\n    if cmap is None:\n        if data.dtype.kind in \'ubf\' or vmin == 0:\n            cmap = \'viridis\'\n        else:\n            cmap = \'coolwarm\'\n        if photometric == \'MINISWHITE\':\n            cmap += \'_r\'\n\n    image = pyplot.imshow(numpy.atleast_2d(data[(0,) * dims].squeeze()),\n                          vmin=vmin, vmax=vmax, cmap=cmap,\n                          interpolation=interpolation, **kwargs)\n\n    if not isrgb:\n        pyplot.colorbar()  # panchor=(0.55, 0.5), fraction=0.05\n\n    def format_coord(x, y):\n        # callback function to format coordinate display in toolbar\n        x = int(x + 0.5)\n        y = int(y + 0.5)\n        try:\n            if dims:\n                return \'%s @ %s [%4i, %4i]\' % (\n                    curaxdat[1][y, x], current, y, x)\n            return \'%s @ [%4i, %4i]\' % (data[y, x], y, x)\n        except IndexError:\n            return \'\'\n\n    def none(event):\n        return \'\'\n\n    subplot.format_coord = format_coord\n    image.get_cursor_data = none\n    image.format_cursor_data = none\n\n    if dims:\n        current = list((0,) * dims)\n        curaxdat = [0, data[tuple(current)].squeeze()]\n        sliders = [pyplot.Slider(\n            pyplot.axes([0.125, 0.03*(axis+1), 0.725, 0.025]),\n            \'Dimension %i\' % axis, 0, data.shape[axis]-1, 0, facecolor=\'0.5\',\n            valfmt=\'%%.0f [%i]\' % data.shape[axis]) for axis in range(dims)]\n        for slider in sliders:\n            slider.drawon = False\n\n        def set_image(current, sliders=sliders, data=data):\n            # change image and redraw canvas\n            curaxdat[1] = data[tuple(current)].squeeze()\n            image.set_data(curaxdat[1])\n            for ctrl, index in zip(sliders, current):\n                ctrl.eventson = False\n                ctrl.set_val(index)\n                ctrl.eventson = True\n            figure.canvas.draw()\n\n        def on_changed(index, axis, data=data, current=current):\n            # callback function for slider change event\n            index = int(round(index))\n            curaxdat[0] = axis\n            if index == current[axis]:\n                return\n            if index >= data.shape[axis]:\n                index = 0\n            elif index < 0:\n                index = data.shape[axis] - 1\n            current[axis] = index\n            set_image(current)\n\n        def on_keypressed(event, data=data, current=current):\n            # callback function for key press event\n            key = event.key\n            axis = curaxdat[0]\n            if str(key) in \'0123456789\':\n                on_changed(key, axis)\n            elif key == \'right\':\n                on_changed(current[axis] + 1, axis)\n            elif key == \'left\':\n                on_changed(current[axis] - 1, axis)\n            elif key == \'up\':\n                curaxdat[0] = 0 if axis == len(data.shape)-1 else axis + 1\n            elif key == \'down\':\n                curaxdat[0] = len(data.shape)-1 if axis == 0 else axis - 1\n            elif key == \'end\':\n                on_changed(data.shape[axis] - 1, axis)\n            elif key == \'home\':\n                on_changed(0, axis)\n\n        figure.canvas.mpl_connect(\'key_press_event\', on_keypressed)\n        for axis, ctrl in enumerate(sliders):\n            ctrl.on_changed(lambda k, a=axis: on_changed(k, a))\n\n    return figure, subplot, image\n\n\ndef _app_show():\n    """"""Block the GUI. For use as skimage plugin.""""""\n    pyplot = sys.modules[\'matplotlib.pyplot\']\n    pyplot.show()\n\n\ndef askopenfilename(**kwargs):\n    """"""Return file name(s) from Tkinter\'s file open dialog.""""""\n    try:\n        from Tkinter import Tk\n        import tkFileDialog as filedialog\n    except ImportError:\n        from tkinter import Tk, filedialog\n    root = Tk()\n    root.withdraw()\n    root.update()\n    filenames = filedialog.askopenfilename(**kwargs)\n    root.destroy()\n    return filenames\n\n\ndef main(argv=None):\n    """"""Command line usage main function.""""""\n    if float(sys.version[0:3]) < 2.7:\n        print(\'This script requires Python version 2.7 or better.\')\n        print(\'This is Python version %s\' % sys.version)\n        return 0\n    if argv is None:\n        argv = sys.argv\n\n    import optparse  # TODO: use argparse\n\n    parser = optparse.OptionParser(\n        usage=\'usage: %prog [options] path\',\n        description=\'Display image data in TIFF files.\',\n        version=\'%%prog %s\' % __version__)\n    opt = parser.add_option\n    opt(\'-p\', \'--page\', dest=\'page\', type=\'int\', default=-1,\n        help=\'display single page\')\n    opt(\'-s\', \'--series\', dest=\'series\', type=\'int\', default=-1,\n        help=\'display series of pages of same shape\')\n    opt(\'--nomultifile\', dest=\'nomultifile\', action=\'store_true\',\n        default=False, help=\'do not read OME series from multiple files\')\n    opt(\'--noplots\', dest=\'noplots\', type=\'int\', default=8,\n        help=\'maximum number of plots\')\n    opt(\'--interpol\', dest=\'interpol\', metavar=\'INTERPOL\', default=\'bilinear\',\n        help=\'image interpolation method\')\n    opt(\'--dpi\', dest=\'dpi\', type=\'int\', default=96,\n        help=\'plot resolution\')\n    opt(\'--vmin\', dest=\'vmin\', type=\'int\', default=None,\n        help=\'minimum value for colormapping\')\n    opt(\'--vmax\', dest=\'vmax\', type=\'int\', default=None,\n        help=\'maximum value for colormapping\')\n    opt(\'--debug\', dest=\'debug\', action=\'store_true\', default=False,\n        help=\'raise exception on failures\')\n    opt(\'--doctest\', dest=\'doctest\', action=\'store_true\', default=False,\n        help=\'runs the docstring examples\')\n    opt(\'-v\', \'--detail\', dest=\'detail\', type=\'int\', default=2)\n    opt(\'-q\', \'--quiet\', dest=\'quiet\', action=\'store_true\')\n\n    settings, path = parser.parse_args()\n    path = \' \'.join(path)\n\n    if settings.doctest:\n        import doctest\n        doctest.testmod(optionflags=doctest.ELLIPSIS)\n        return 0\n    if not path:\n        path = askopenfilename(title=\'Select a TIFF file\',\n                               filetypes=TIFF.FILEOPEN_FILTER)\n        if not path:\n            parser.error(\'No file specified\')\n\n    if any(i in path for i in \'?*\'):\n        path = glob.glob(path)\n        if not path:\n            print(\'no files match the pattern\')\n            return 0\n        # TODO: handle image sequences\n        path = path[0]\n\n    if not settings.quiet:\n        print(\'\\nReading file structure...\', end=\' \')\n    start = time.time()\n    try:\n        tif = TiffFile(path, multifile=not settings.nomultifile)\n    except Exception as e:\n        if settings.debug:\n            raise\n        else:\n            print(\'\\n\', e)\n            sys.exit(0)\n    if not settings.quiet:\n        print(\'%.3f ms\' % ((time.time()-start) * 1e3))\n\n    if tif.is_ome:\n        settings.norgb = True\n\n    images = []\n    if settings.noplots > 0:\n        if not settings.quiet:\n            print(\'Reading image data... \', end=\' \')\n\n        def notnone(x):\n            return next(i for i in x if i is not None)\n\n        start = time.time()\n        try:\n            if settings.page >= 0:\n                images = [(tif.asarray(key=settings.page),\n                           tif[settings.page], None)]\n            elif settings.series >= 0:\n                images = [(tif.asarray(series=settings.series),\n                           notnone(tif.series[settings.series]._pages),\n                           tif.series[settings.series])]\n            else:\n                images = []\n                for i, s in enumerate(tif.series[:settings.noplots]):\n                    try:\n                        images.append((tif.asarray(series=i),\n                                       notnone(s._pages),\n                                       tif.series[i]))\n                    except ValueError as e:\n                        images.append((None, notnone(s.pages), None))\n                        if settings.debug:\n                            raise\n                        else:\n                            print(\'\\nSeries %i failed: %s... \' % (i, e),\n                                  end=\'\')\n            if not settings.quiet:\n                print(\'%.3f ms\' % ((time.time()-start) * 1e3))\n        except Exception as e:\n            if settings.debug:\n                raise\n            else:\n                print(e)\n\n    if not settings.quiet:\n        print()\n        print(TiffFile.__str__(tif, detail=int(settings.detail)))\n        print()\n    tif.close()\n\n    if images and settings.noplots > 0:\n        try:\n            import matplotlib\n            matplotlib.use(\'TkAgg\')\n            from matplotlib import pyplot\n        except ImportError as e:\n            warnings.warn(\'failed to import matplotlib.\\n%s\' % e)\n        else:\n            for img, page, series in images:\n                if img is None:\n                    continue\n                vmin, vmax = settings.vmin, settings.vmax\n                if \'GDAL_NODATA\' in page.tags:\n                    try:\n                        vmin = numpy.min(\n                            img[img > float(page.tags[\'GDAL_NODATA\'].value)])\n                    except ValueError:\n                        pass\n                if tif.is_stk:\n                    try:\n                        vmin = tif.stk_metadata[\'MinScale\']\n                        vmax = tif.stk_metadata[\'MaxScale\']\n                    except KeyError:\n                        pass\n                    else:\n                        if vmax <= vmin:\n                            vmin, vmax = settings.vmin, settings.vmax\n                if series:\n                    title = \'%s\\n%s\\n%s\' % (str(tif), str(page), str(series))\n                else:\n                    title = \'%s\\n %s\' % (str(tif), str(page))\n                photometric = \'MINISBLACK\'\n                if page.photometric not in (3,):\n                    photometric = TIFF.PHOTOMETRIC(page.photometric).name\n                imshow(img, title=title, vmin=vmin, vmax=vmax,\n                       bitspersample=page.bitspersample,\n                       photometric=photometric,\n                       interpolation=settings.interpol,\n                       dpi=settings.dpi)\n            pyplot.show()\n\n\nif sys.version_info[0] == 2:\n    inttypes = int, long  # noqa\n\n    def print_(*args, **kwargs):\n        """"""Print function with flush support.""""""\n        flush = kwargs.pop(\'flush\', False)\n        print(*args, **kwargs)\n        if flush:\n            sys.stdout.flush()\n\n    def bytes2str(b, encoding=None, errors=None):\n        """"""Return string from bytes.""""""\n        return b\n\n    def str2bytes(s, encoding=None):\n        """"""Return bytes from string.""""""\n        return s\n\n    def byte2int(b):\n        """"""Return value of byte as int.""""""\n        return ord(b)\n\n    class FileNotFoundError(IOError):\n        pass\n\n    TiffFrame = TiffPage  # noqa\nelse:\n    inttypes = int\n    basestring = str, bytes\n    unicode = str\n    print_ = print\n\n    def bytes2str(b, encoding=None, errors=\'strict\'):\n        """"""Return unicode string from encoded bytes.""""""\n        if encoding is not None:\n            return b.decode(encoding, errors)\n        try:\n            return b.decode(\'utf-8\', errors)\n        except UnicodeDecodeError:\n            return b.decode(\'cp1252\', errors)\n\n    def str2bytes(s, encoding=\'cp1252\'):\n        """"""Return bytes from unicode string.""""""\n        return s.encode(encoding)\n\n    def byte2int(b):\n        """"""Return value of byte as int.""""""\n        return b\n\nif __name__ == \'__main__\':\n    sys.exit(main())\n\n'"
all_scripts/tools.py,179,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\narraytools tools\r\n================\r\n\r\nScript :   tools.py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified : 2018-11-23\r\n\r\nPurpose :  tools for working with numpy arrays\r\n\r\nUseage:\r\n-------\r\n\r\n>>> import arraytools as art\r\n\r\n- `tools.py` and other scripts are part of the arraytools package.\r\n- Access in other programs using .... art.func(params) ....\r\n\r\n**Requires**\r\n-------------\r\n  see import section and __init__.py in the `arraytools` folder\r\n\r\n**Notes**\r\n---------\r\n**Basic array information**\r\n\r\n- `np.typecodes`\r\n- `np.sctypeDict`\r\n- `np.sctypes`\r\n\r\n>>> for i in np.sctypes:\r\n    print(""{!s:<8} : {}"".format(i, np.sctypes[i]))\r\n\r\n**Classes available**\r\n::\r\n\r\nint      : class\r\n    \'numpy.int8\', \'numpy.int16\', \'numpy.int32\', \'numpy.int64\'\r\nuint     : class\r\n    \'numpy.uint8\', \'numpy.uint16\', \'numpy.uint32, \'numpy.uint64\'\r\nfloat    : class\r\n    \'numpy.float16\', \'numpy.float32\', \'numpy.float64\'\r\ncomplex  : class\r\n    \'numpy.complex64\', \'numpy.complex128\'\r\nothers   : class\r\n    \'bool\', \'object\', \'bytes\', \'str\', \'numpy.void\'\r\n\r\n**Typecodes**\r\n\r\n`np.typecodes.items()`  ... `np.typecodes[\'AllInteger\']`\r\n::\r\n    All: \'?bhilqpBHILQPefdgFDGSUVOMm\'\r\n    |__AllFloat: \'efdgFDG\'\r\n       |__Float: \'efdg\'\r\n       |__Complex: \'FDG\'\r\n    |__AllInteger: \'bBhHiIlLqQpP\'\r\n    |  |__UnsignedInteger: \'BHILQP\'\r\n    |  |__Integer: \'bhilqp\'\r\n    |__Datetime\': \'Mm\'\r\n    |__Character\': \'c\'\r\n    |__Other:  \'U\', \'?\', \'S\', \'O\', \'V\'  Determined from the above\r\n\r\n`np.sctypes.keys` and `np.sctypes.values`\r\n::\r\n    numpy classes\r\n    |__complex  complex64, complex128, complex256\r\n    |__float    float16, float32, float64, float128\r\n    |__int      int8, int16, int32, int64\r\n    |__uint     uint8, uint16, uint32, uint63\r\n    |__others   bool, object, str, void\r\n                   ?,   O,    S U,  V\r\n\r\n**Numbers**\r\n::\r\n   np.inf, -np.inf\r\n   np.iinfo(np.int8).min  or .max = -128, 128\r\n   np.iinfo(np.int16).min or .max = -32768, 32768\r\n   np.iinfo(np.int32).min or .max = -2147483648, max=2147483647\r\n   np.finfo(np.float64)\r\n   np.finfo(resolution=1e-15, min=-1.7976931348623157e+308,\r\n            max=1.7976931348623157e+308, dtype=float64)\r\n\r\n\r\n**Functions**\r\n-------------\r\nTool function examples follow...\r\n\r\n**1. arr2xyz** : convert 2/3D arrays into xyz values (ie 2D)\r\n\r\narr2xyz(a, verbose=False)** : convert an array to x,y,z values, using\r\nrow/column values for x and y\r\n::\r\n    a= np.arange(2*3).reshape(2,3)\r\n    arr2xyz(a)\r\n    array([[0, 0, 0],\r\n           [1, 0, 1],\r\n           [2, 0, 2],\r\n           [0, 1, 3],\r\n           [1, 1, 4],\r\n           [2, 1, 5]])\r\n\r\n**2. make_blocks** : create array blocks\r\n\r\n>>> make_blocks(rows=2, cols=4, r=2, c=2, dt=\'int\')\r\narray([[0, 0, 1, 1, 2, 2, 3, 3],\r\n       [0, 0, 1, 1, 2, 2, 3, 3],\r\n       [4, 4, 5, 5, 6, 6, 7, 7],\r\n       [4, 4, 5, 5, 6, 6, 7, 7]])\r\n\r\n**3. group_vals(seq, stepsize=1)**\r\n::\r\n    seq = [1, 2, 4, 5, 8, 9, 10]\r\n    stepsize = 1\r\n    [array([1, 2]), array([4, 5]), array([ 8,  9, 10])]\r\n\r\n**4. reclass(z, bins, new_bins, mask=False, mask_val=None)**\r\n\r\nReclass an array using existing class breaks (bins) and new bins both must be\r\nin ascending order.\r\n::\r\n      z = np.arange(3*5).reshape(3,5)\r\n      bins = [0, 5, 10, 15]\r\n      new_bins = [1, 2, 3, 4]\r\n      z_recl = reclass(z, bins, new_bins, mask=False, mask_val=None)\r\n      ==> .... z                     ==> .... z_recl\r\n      array([[ 0,  1,  2,  3,  4],   array([[1, 1, 1, 1, 1],\r\n             [ 5,  6,  7,  8,  9],          [2, 2, 2, 2, 2],\r\n             [10, 11, 12, 13, 14]])         [3, 3, 3, 3, 3]])\r\n\r\n**5. scale(a, x=2, y=2)** : scale an array by x, y factors\r\n::\r\n      a = np.array([[0, 1, 2], [3, 4, 5]]\r\n      b = scale(a, x=2, y=2)\r\n        =  array([[0, 0, 1, 1, 2, 2],\r\n                  [0, 0, 1, 1, 2, 2],\r\n                  [3, 3, 4, 4, 5, 5],\r\n                  [3, 3, 4, 4, 5, 5]])\r\n\r\nusing scale with np.tile\r\n::\r\n      art.scale(a, 2,2)         np.tile(art.scale(a, 2, 2), (2, 2))\r\n      array([[0, 0, 1, 1],      array([[0, 0, 1, 1, 0, 0, 1, 1],\r\n             [0, 0, 1, 1],             [0, 0, 1, 1, 0, 0, 1, 1],\r\n             [2, 2, 3, 3],             [2, 2, 3, 3, 2, 2, 3, 3],\r\n             [2, 2, 3, 3]])            [2, 2, 3, 3, 2, 2, 3, 3],\r\n                                       [0, 0, 1, 1, 0, 0, 1, 1],\r\n                                       [0, 0, 1, 1, 0, 0, 1, 1],\r\n                                       [2, 2, 3, 3, 2, 2, 3, 3],\r\n                                       [2, 2, 3, 3, 2, 2, 3, 3]])\r\n\r\n**6. split_array(a, fld=\'Id\')**\r\n::\r\n     array \'b\'\r\n     array([(0, 1, 2, 3), (4, 5, 6, 7), (8, 9, 10, 11)],\r\n           dtype=[(\'A\', \'<i4\'), (\'B\', \'<i4\'), (\'C\', \'<i4\'), (\'D\', \'<i4\')])\r\n    - split_array(b, fld=\'A\')\r\n    [array([(0, 1, 2, 3)],\r\n          dtype=[(\'A\', \'<i4\'), (\'B\', \'<i4\'), (\'C\', \'<i4\'), (\'D\', \'<i4\')]),\r\n     array([(4, 5, 6, 7)],\r\n          dtype=[(\'A\', \'<i4\'), (\'B\', \'<i4\'), (\'C\', \'<i4\'), (\'D\', \'<i4\')]),\r\n     array([(8, 9, 10, 11)],\r\n          dtype=[(\'A\', \'<i4\'), (\'B\', \'<i4\'), (\'C\', \'<i4\'), (\'D\', \'<i4\')])]\r\n\r\n**7.  make_flds(n=1, as_type=names=None, default=""col"")** : example\r\n\r\n>>> from numpy.lib._iotools import easy_dtype as easy\r\n>>> make_flds(n=1, as_type=\'float\', names=None, def_name=""col"")\r\ndtype([(\'col_00\', \'<f8\')])\r\n\r\n>>> make_flds(n=2, as_type=\'int\', names=[\'f01\', \'f02\'], def_name=""col"")\r\ndtype([(\'f01\', \'<i8\'), (\'f02\', \'<i8\')])\r\n\r\n**8.  nd_rec** : ndarray to structured array or recarray\r\n\r\n**9.  nd_struct** :\r\n\r\n**10. nd2struct(a)**\r\n\r\nKeep the dtype the same\r\n::\r\n    aa = nd2struct(a)       # produce a structured array from inputs\r\n    aa.reshape(-1,1)   # structured array\r\n    array([[(0, 1, 2, 3, 4)],\r\n           [(5, 6, 7, 8, 9)],\r\n           [(10, 11, 12, 13, 14)],\r\n           [(15, 16, 17, 18, 19)]],\r\n       dtype=[(\'A\', \'<i4\'), ... snip ... , (\'E\', \'<i4\')])\r\n\r\nUpcast the dtype\r\n::\r\n    a_f = nd2struct(a.astype(\'float\'))  # note astype allows a view\r\n    array([(0.0, 1.0, 2.0, 3.0, 4.0), ... snip... ,\r\n           (15.0, 16.0, 17.0, 18.0, 19.0)],\r\n          dtype=[(\'A\', \'<f8\'), ... snip ... , (\'E\', \'<f8\')])\r\n\r\n**11. np2rec** : shell around above\r\n\r\n**12. rc_vals(a)**\r\n\r\n**13. xy_vals(a) ... array to x, y, values**\r\n\r\n**14. array_cols**\r\n\r\n**15. change_arr(a, order=[], prn=False)** : merely a convenience function\r\n::\r\n    a = np.arange(4*5).reshape((4, 5))\r\n    change(a, [2, 1, 0, 3, 4])\r\n    array([[ 2,  1,  0,  3,  4],\r\n           [ 7,  6,  5,  8,  9],\r\n           [12, 11, 10, 13, 14],\r\n           [17, 16, 15, 18, 19]])\r\n\r\n**shortcuts**\r\n::\r\n    b = a[:, [2, 1, 0, 3, 4]]    # reorder the columns, keeping the rows\r\n    c = a[:, [0, 2, 3]]          # delete columns 1 and 4\r\n    d = a[[0, 1, 3, 4], :]       # delete row 2, keeping the columns\r\n    e = a[[0, 1, 3], [1, 2, 3]]  # keep [0, 1], [1, 2], [3, 3]\r\n                                   => ([ 1, 7, 18])\r\n\r\n**16. concat_arrs**\r\n\r\n**17. pad__(a, pad_with=None, size=(1, 1))**\r\n\r\n**18. stride(a, r_c=(3, 3))**\r\n\r\nProduce a strided array using a window of r_c shape.\r\n\r\nCalls _check(a, r_c, subok=False) to check for array compliance\r\n::\r\n      a =np.arange(15).reshape(3,5)\r\n      s = stride(a)    stride     ====>   slide    =====>\r\n      array([[[ 0,  1,  2],  [[ 1,  2,  3],  [[ 2,  3,  4],\r\n              [ 5,  6,  7],   [ 6,  7,  8],   [ 7,  8,  9],\r\n              [10, 11, 12]],  [11, 12, 13]],  [12, 13, 14]]])\r\n\r\n`pad_`  to pad an array prior to striding or blocking\r\n\r\n`block`  calls stride with non-overlapping blocks with no padding\r\n\r\n\r\n**19. block(a, win=(3, 3))**\r\n\r\n**20. sliding_window_view**\r\n\r\n**21.  block_arr(a, win=[3, 3], nodata=-1)**\r\n\r\nBlock an array given an input array, a window and a nodata value.\r\n::\r\n    a = np.arange(16).reshape(4,4)\r\n    block_arr(a, win=[4, 3], nodata=-1)\r\n    array([[ 0,  1,  2,  3, -1, -1],\r\n           [ 4,  5,  6,  7, -1, -1],\r\n           [ 8,  9, 10, 11, -1, -1],\r\n           [12, 13, 14, 15, -1, -1]]),\r\n    masked_array(data =\r\n        [[[0 1 2]\r\n          [4 5 6]\r\n          [8 9 10]\r\n          [12 13 14]]\r\n\r\n         [[3 -- --]\r\n          [7 -- --]\r\n          [11 -- --]\r\n          [15 -- --]]],\r\n    mask .... snipped ....\r\n\r\n**22. rolling_stats() : stats for a strided array**\r\n\r\n    min, max, mean, sum, std, var, ptp\r\n\r\n**23. find(a, func, this=None, count=0, keep=[], prn=False, r_lim=2)**\r\n\r\n    func - (cumsum, eq, neq, ls, lseq, gt, gteq, btwn, btwni, byond)\r\n           (        ==,  !=,  <,   <=,  >,   >=,  >a<, =>a<=,  <a> )\r\n\r\n**23a. _func(fn, a, this)**\r\n\r\n    called by \'find\' see details there\r\n    (cumsum, eq, neq, ls, lseq, gt, gteq, btwn, btwni, byond)\r\n\r\nNote  see ``find1d_demo.py`` for examples\r\n\r\n\r\n**24. group_pnts(a, key_fld=\'ID\', keep_flds=[\'X\', \'Y\', \'Z\'])**\r\n\r\n**25. uniq(ar, return_index=False, return_inverse=False, return_counts=False,\r\n          axis=0)**\r\n\r\n**26. is_in(find_in, using, not_in=False)**\r\n\r\n**27. running_count**\r\n\r\n**28. sequences(data, stepsize)**\r\n\r\n**29. sort_rows_by_col(a, col=0, descending=False)**\r\n\r\nSort 2d ndarray by column\r\n::\r\n      a                           col_sort(a, col=1, descending=False)\r\n      array([[2, 3, 2, 2],        array([[2, 1, 2, 4],\r\n             [1, 4, 1, 3],               [2, 3, 2, 2],\r\n             [2, 1, 2, 4]])              [1, 4, 1, 3]])\r\n\r\n**30. sort_cols_by_row**\r\n\r\n**31. radial_sort(pnts, cent=None)**\r\n\r\n**32. pack_last_axis**\r\n\r\n\r\nReferences:\r\n----------\r\ngeneral\r\n\r\n`<https://github.com/numpy/numpy>`_.\r\n`<https://github.com/numpy/numpy/blob/master/numpy/lib/_iotools.py>`_.\r\n\r\nstriding\r\n\r\n`<https://github.com/numpy/numpy/blob/master/numpy/lib/stride_tricks.py>`_.\r\n`<http://www.johnvinyard.com/blog/?p=268>`_.\r\n\r\nfor strided arrays\r\n\r\n`<https://stackoverflow.com/questions/47469947/as-strided-linking-stepsize-\r\nstrides-of-conv2d-with-as-strided-strides-paramet#47470711>`_.\r\n\r\n`<https://stackoverflow.com/questions/48097941/strided-convolution-of-2d-in-\r\nnumpy>`_.\r\n\r\n`<https://stackoverflow.com/questions/45960192/using-numpy-as-strided-\r\nfunction-to-create-patches-tiles-rolling-or-sliding-w>`_.\r\n\r\nnumpy  # stride for convolve 4d\r\n\r\n`<https://stackoverflow.com/questions/2828059/sorting-arrays-in-numpy-by-\r\ncolumn>`_.\r\n\r\nFunctions\r\n---------\r\nAlphabetical listing\r\n::\r\n\r\n \'_func\', \'_tools_help_\', \'arr2xyz\', \'arrays_cols\', \'block\', \'block_arr\',\r\n \'change_arr\', \'concat_arrs\', \'find\', \'group_pnts\', \'group_vals\', \'is_in\',\r\n \'make_blocks\', \'make_flds\', \'n_largest\', \'n_smallest\', \'nd2rec\',\r\n \'nd2struct\', \'nd_rec\', \'nd_struct\', \'num_to_mask\', \'num_to_nan\',\r\n \'pack_last_axis\', \'pad_\', \'radial_sort\', \'rc_vals\', \'reclass\',\r\n \'rolling_stats\', \'running_count\', \'scale\', \'sequences\',\r\n \'sliding_window_view\', \'sort_cols_by_row\', \'sort_rows_by_col\',\r\n \'split_array\', \'stride\', \'uniq\', \'view_sort\', \'xy_sort\',\'xy_vals\'\r\n\r\n---------------------------------------------------------------------\r\n""""""\r\n# pylint: disable=C0103\r\n# pylint: disable=R1710\r\n# pylint: disable=R0914\r\n\r\n# ---- imports, formats, constants -------------------------------------------\r\nimport sys\r\nfrom textwrap import dedent, indent\r\nimport warnings\r\nimport numpy as np\r\nfrom numpy.lib.stride_tricks import as_strided\r\n\r\nwarnings.simplefilter(\'ignore\', FutureWarning)\r\n\r\n__all__ = [\'_tools_help_\',\r\n           \'arr2xyz\', \'make_blocks\',     # (1-6) ndarrays ... make arrays,\r\n           \'group_vals\', \'reclass\',      #     change shape, arangement\r\n           \'scale\', \'split_array\',\r\n           \'make_flds\', \'nd_rec\',        # (7-14) structured/recdarray\r\n           \'nd_struct\', \'nd2struct\',\r\n           \'nd2rec\', \'rc_vals\', \'xy_vals\',\r\n           \'arrays_struct\',\r\n           \'change_arr\', \'concat_arrs\',  # (15-16) change/modify arrays\r\n           \'pad_\', \'stride\', \'block\',    # (17-22) stride, block and pad\r\n           \'sliding_window_view\',\r\n           \'block_arr\', \'rolling_stats\',\r\n           \'_func\', \'find\', \'group_pnts\', # (23-28) querying, analysis\r\n           \'uniq\', \'is_in\',\r\n           \'running_count\', \'sequences\',\r\n           \'sort_cols_by_row\',            # (29-31) column and row sorting\r\n           \'sort_rows_by_col\',\r\n           \'radial_sort\',\r\n           \'view_sort\', \'xy_sort\',\r\n           \'pack_last_axis\'  # extras -------\r\n           ]\r\n\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 8.2f}\'.format}\r\n\r\nnp.set_printoptions(\r\n        edgeitems=3,\r\n        threshold=120,\r\n        floatmode=\'maxprec\',\r\n        precision=2, suppress=True, linewidth=100,\r\n        nanstr=\'nan\', infstr=\'inf\', sign=\'-\',\r\n        formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\n\r\n# ---- (1) ndarrays ... code section .... ----\r\n# ---- make arrays, change shape, arrangement\r\n# ---- arr2xyz, makeblocks, rc_vals, xy_vals ----\r\n#\r\ndef arr2xyz(a, keep_masked=False, verbose=False):\r\n    """"""Produce an array such that the row, column values are used for x,y\r\n    and array values for z.  Masked arrays are sorted.\r\n\r\n    Returns\r\n    --------\r\n    A mesh grid with values, dimensions and shapes are changed so\r\n    that ndim=2, ie shape(3,4,5), ndim=3 becomes shape(12,5), ndim=2\r\n\r\n    >>> a = np.arange(9).reshape(3, 3)\r\n    array([[0, 1, 2],\r\n           [3, 4, 5],\r\n           [6, 7, 8]])\r\n    >>> arr2xyz(am, keep_masked=True)   # keep the masked values...\r\n    masked_array(data =\r\n    [[0 0 0]\r\n     [1 0 -]\r\n     [2 0 2]\r\n     [0 1 -]\r\n     [1 1 4]\r\n     [2 1 -]\r\n     [0 2 6]\r\n     [1 2 7]\r\n     [2 2 8]],\r\n             mask =\r\n     [[False False False]... snip\r\n     [False False False]],\r\n           fill_value = 999999)\r\n    >>>\r\n    >>> arr2xyz(am, keep_masked=False)  # remove the masked values\r\n    array([[0, 0, 0],\r\n           [2, 0, 2],\r\n           [1, 1, 4],\r\n           [0, 2, 6],\r\n           [1, 2, 7],\r\n           [2, 2, 8]])\r\n\r\n    See also\r\n    --------\r\n    `xy_vals(a)` and `rc_vals(a)` for simpler versions.\r\n\r\n    `num_to_mask(a)` and  `num_to_nan(a)` if you want structured arrays,\r\n    to produce masks prior to conversion.\r\n\r\n    """"""\r\n    if a.ndim == 1:\r\n        a = a.reshape(a.shape[0], 1)\r\n    if a.ndim > 2:\r\n        a = a.reshape(np.product(a.shape[:-1]), a.shape[-1])\r\n    r, c = a.shape\r\n    XX, YY = np.meshgrid(np.arange(c), np.arange(r))\r\n    XX = XX.ravel()\r\n    YY = YY.ravel()\r\n    if isinstance(np.ma.getmask(a), np.ndarray):\r\n        tbl = np.ma.vstack((XX, YY, a.ravel()))\r\n        tbl = tbl.T\r\n        if not keep_masked:\r\n            m = tbl[:, 2].mask\r\n            tbl = tbl[~m].data\r\n    else:\r\n        tbl = np.stack((XX, YY, a.ravel()), axis=1)\r\n    if verbose:\r\n        frmt = """"""\r\n        ----------------------------\r\n        Meshgrid demo: array to x,y,z table\r\n        :Formulation...\r\n        :  XX,YY = np.meshgrid(np.arange(x.shape[1]),np.arange(x.shape[0]))\r\n        :Input table\r\n        {!r:<}\r\n        :Raveled array, using x.ravel()\r\n        {!r:<}\r\n        :XX in mesh: columns shape[1]\r\n        {!r:<}\r\n        :YY in mesh: rows shape[0]\r\n        {!r:<}\r\n        :Output:\r\n        {!r:<}\r\n        :-----------------------------\r\n        """"""\r\n        print(dedent(frmt).format(a, a.ravel(), XX, YY, tbl))\r\n    else:\r\n        return tbl\r\n\r\n\r\ndef make_blocks(rows=3, cols=3, r=2, c=2, dt=\'int\'):\r\n    """"""Make a block array with rows * cols containing r*c sub windows.\r\n    Specify the rows, columns, then the block size as r, c and dtype\r\n    Use `scale`, if you want specific values during array construction.\r\n\r\n    Requires\r\n    --------\r\n    rows : integer\r\n        rows in initial array\r\n    cols : integer\r\n        columns in the initial array\r\n    r : integer\r\n        rows in sub window\r\n    c : integer\r\n        columns in sub window\r\n    dt : np.dtype\r\n        array data type\r\n\r\n    Returns\r\n    --------\r\n    The defaults produce an 8 column by 8 row array numbered from\r\n    0 to (rows*cols) - 1\r\n\r\n    >>> array.shape = (rows * r, cols * c)  # (6, 6)\r\n\r\n    >>> make_blocks(rows=3, cols=3, r=2, c=2, dt=\'int\')\r\n    array([[0, 0, 1, 1, 2, 2],\r\n           [0, 0, 1, 1, 2, 2],\r\n           [3, 3, 4, 4, 5, 5],\r\n           [3, 3, 4, 4, 5, 5],\r\n           [6, 6, 7, 7, 8, 8],\r\n           [6, 6, 7, 7, 8, 8]])\r\n\r\n    """"""\r\n    a = np.arange(rows*cols, dtype=dt).reshape(rows, cols)\r\n    a = scale(a, x=r, y=c)\r\n    return a\r\n\r\n\r\ndef group_vals(seq, delta=0, oper=\'!=\'):\r\n    """"""Group consecutive values separated by no more than delta\r\n\r\n    Parameters\r\n    ----------\r\n    seq : array, list tuple\r\n        sequence of values\r\n    delta :\r\n        difference between consecutive values\r\n    oper :\r\n        \'eq\', \'==\', \'ne\', \'!=\', \'gt\', \'>\', \'lt\', \'<\'\r\n\r\n    Reference\r\n    ---------\r\n    `<https://stackoverflow.com/questions/7352684/\r\n    how-to-find-the-groups-of-consecutive-elements-from-an-array-in-numpy>`_.\r\n\r\n    Notes\r\n    -----\r\n    >>> a = [1, 1, 1, 2, 2, 3, 1, 1, 1]\r\n    >>> group_vals(a, delta=0, oper=\'!=\')  # sequential difference !=0\r\n    [array([1, 1, 1]), array([2, 2]), array([3]), array([1, 1, 1])]\r\n\r\n    See also\r\n    --------\r\n    split_array : form structured or recarrays\r\n    """"""\r\n    valid = (\'eq\', \'==\', \'ne\', \'!=\', \'gt\', \'>\', \'lt\', \'<\')\r\n    if oper not in valid:\r\n        raise ValueError(""operand not in {}"".format(valid))\r\n    elif oper in (\'==\', \'eq\'):\r\n        s = np.split(seq, np.where(np.diff(seq) == delta)[0]+1)\r\n    elif oper in (\'!=\', \'ne\'):\r\n        s = np.split(seq, np.where(np.diff(seq) != delta)[0]+1)\r\n    elif oper in (\'>\', \'gt\'):\r\n        s = np.split(seq, np.where(np.diff(seq) > delta)[0]+1)\r\n    elif oper in (\'<\', \'lt\'):\r\n        s = np.split(seq, np.where(np.diff(seq) < delta)[0]+1)\r\n    else:\r\n        s = seq\r\n    return s\r\n\r\n\r\ndef reclass(a, bins=None, new_bins=None, mask_=False, mask_val=None):\r\n    """"""Reclass an array of integer or floating point values.\r\n\r\n    Requires:\r\n    --------\r\n    bins : list/tuple\r\n        sequential list/array of the lower limits of each class\r\n        include one value higher to cover the upper range.\r\n    new_bins : list/tuple\r\n        new class values for each bin\r\n    mask : boolean\r\n        whether the raster contains nodata values or values to\r\n        be masked with mask_val\r\n    mask_val: number\r\n        value to be masked\r\n\r\n    Array dimensions will be squeezed.\r\n\r\n    Example\r\n    -------\r\n    inputs::\r\n\r\n        z = np.arange(3*5).reshape(3,5)\r\n        bins = [0, 5, 10, 15]\r\n        new_bins = [1, 2, 3, 4]\r\n        z_recl = reclass(z, bins, new_bins, mask=False, mask_val=None)\r\n\r\n    outputs::\r\n\r\n        ==> .... z                     ==> .... z_recl\r\n        array([[ 0,  1,  2,  3,  4],   array([[1, 1, 1, 1, 1],\r\n               [ 5,  6,  7,  8,  9],          [2, 2, 2, 2, 2],\r\n               [10, 11, 12, 13, 14]])         [3, 3, 3, 3, 3]])\r\n\r\n    """"""\r\n    a_rc = np.zeros_like(a)\r\n    c_0 = isinstance(bins, (list, tuple))\r\n    c_1 = isinstance(new_bins, (list, tuple))\r\n    err = ""Bins = {} new = {} won\'t work"".format(bins, new_bins)\r\n    if not c_0 or not c_1:\r\n        print(err)\r\n        return a\r\n    if len(bins) < 2:  # or (len(new_bins <2)):\r\n        print(err)\r\n        return a\r\n    if len(new_bins) < 2:\r\n        new_bins = np.arange(1, len(bins)+2)\r\n    new_classes = list(zip(bins[:-1], bins[1:], new_bins))\r\n    for rc in new_classes:\r\n        q1 = (a >= rc[0])\r\n        q2 = (a < rc[1])\r\n        a_rc = a_rc + np.where(q1 & q2, rc[2], 0)\r\n    return a_rc\r\n\r\n\r\ndef scale(a, x=2, y=2, num_z=None):\r\n    """"""Scale the input array repeating the array values up by the\r\n    x and y factors.\r\n\r\n    Parameters:\r\n    ----------\r\n    `a` : An ndarray, 1D arrays will be upcast to 2D.\r\n\r\n    `x y` : Factors to scale the array in x (col) and y (row).  Scale factors\r\n    must be greater than 2.\r\n\r\n    `num_z` : For 3D, produces the 3rd dimension, ie. if num_z = 3 with the\r\n    defaults, you will get an array with shape=(3, 6, 6),\r\n\r\n    Examples:\r\n    --------\r\n    >>> a = np.array([[0, 1, 2], [3, 4, 5]]\r\n    >>> b = scale(a, x=2, y=2)\r\n    array([[0, 0, 1, 1, 2, 2],\r\n           [0, 0, 1, 1, 2, 2],\r\n           [3, 3, 4, 4, 5, 5],\r\n           [3, 3, 4, 4, 5, 5]])\r\n\r\n    Notes:\r\n    -----\r\n    >>> a = np.arange(2*2).reshape(2,2)\r\n    array([[0, 1],\r\n           [2, 3]])\r\n\r\n    >>> frmt_(scale(a, x=2, y=2, num_z=2))\r\n    Array... shape (3, 4, 4), ndim 3, not masked\r\n      0, 0, 1, 1    0, 0, 1, 1    0, 0, 1, 1\r\n      0, 0, 1, 1    0, 0, 1, 1    0, 0, 1, 1\r\n      2, 2, 3, 3    2, 2, 3, 3    2, 2, 3, 3\r\n      2, 2, 3, 3    2, 2, 3, 3    2, 2, 3, 3\r\n      sub (0)       sub (1)       sub (2)\r\n\r\n    """"""\r\n    if (x < 1) or (y < 1):\r\n        print(""x or y scale < 1... read the docs\\n{}"".format(scale.__doc__))\r\n        return None\r\n    a = np.atleast_2d(a)\r\n    z0 = np.tile(a.repeat(x), y)  # repeat for x, then tile\r\n    z1 = np.hsplit(z0, y)         # split into y parts horizontally\r\n    z2 = np.vstack(z1)            # stack them vertically\r\n    if a.shape[0] > 1:            # if there are more, repeat\r\n        z3 = np.hsplit(z2, a.shape[0])\r\n        z3 = np.vstack(z3)\r\n    else:\r\n        z3 = np.vstack(z2)\r\n    if num_z not in (0, None):\r\n        d = [z3]\r\n        for i in range(num_z):\r\n            d.append(z3)\r\n        z3 = np.dstack(d)\r\n        z3 = np.rollaxis(z3, 2, 0)\r\n    return z3\r\n\r\n\r\ndef split_array(a, fld=\'ID\'):\r\n    """"""Split a structured or recarray array using unique values in the\r\n    `fld` field.  It is assumed that there is a sequential ordering to\r\n    the values in the field.  If there is not, use np.where in conjunction\r\n    with np.unique or sort the array first.\r\n\r\n    Parameters\r\n    ----------\r\n    `a` : A structured or recarray.\r\n\r\n    `fld` : A numeric field assumed to be sorted which indicates which group\r\n    a record belongs to.\r\n\r\n    Returns\r\n    -------\r\n    A list of arrays split on the categorizing field\r\n\r\n    """"""\r\n    return np.split(a, np.where(np.diff(a[fld]))[0] + 1)\r\n\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# ---- (2) structured/recdarray section, change format or arrangement ----\r\n# ----------------------------------------------------------------------\r\n# ---- make_flds, nd_rec, nd_struct, nd_struct, np2rec, rc_vals, xy_vals\r\n#\r\ndef make_flds(n=2, as_type=\'float\', names=None, def_name=""col""):\r\n    """"""Create float or integer fields for statistics and their names.\r\n\r\n    Requires\r\n    --------\r\n    n : integer\r\n        number of fields to create excluding the names field\r\n    def_name : string\r\n        base name to use, numeric values will be produced for each dimension\r\n        for the 3D array, ie Values_00... Values_nn\r\n\r\n    Returns\r\n    -------\r\n    a dtype : which contains the necessary fields to contain the values.\r\n\r\n    >>> from numpy.lib._iotools import easy_dtype as easy\r\n    >>> make_flds(n=1, as_type=\'float\', names=None, def_name=""col"")\r\n    dtype([(\'col_00\', \'<f8\')])\r\n\r\n    >>> make_flds(n=2, as_type=\'int\', names=[\'f01\', \'f02\'], def_name=""col"")\r\n    dtype([(\'f01\', \'<i8\'), (\'f02\', \'<i8\')])\r\n\r\n    Don\'t forget the above, a cool way to create fields quickly\r\n\r\n    """"""\r\n    from numpy.lib._iotools import easy_dtype as easy\r\n    if as_type in [\'float\', \'f8\', \'<f8\']:\r\n        as_type = \'<f8\'\r\n    elif as_type in [\'int\', \'i4\', \'i8\', \'<i4\', \'<i8\']:\r\n        as_type = \'<i8\'\r\n    else:\r\n        as_type = \'str\'\r\n    f = "","".join([as_type for i in range(n)])\r\n    if names is None:\r\n        names = "", "".join([""{}_{:>02}"".format(def_name, i) for i in range(n)])\r\n        dt = easy(f, names=names, defaultfmt=def_name)\r\n    else:\r\n        dt = easy(f, names=names)\r\n    return dt\r\n\r\n\r\ndef nd_rec(a, flds=None, types=None):\r\n    """"""Change a uniform array to an array of mixed dtype as a recarray\r\n\r\n    Requires:\r\n    ---------\r\n    flds : string or None\r\n        flds=\'a, b, c\'\r\n    types : string or None\r\n        types=\'U8, f8, i8\'\r\n\r\n    See also:\r\n    ---------\r\n    nd_struct : alternate using lists rather than string inputs\r\n\r\n    Notes:\r\n    -----\r\n    The a.T turns the columns to rows so that each row can be assigned a\r\n    separate data type.\r\n\r\n    Example::\r\n\r\n       a = np.arange(9).reshape(3, 3)\r\n       a_r = nd_rec(a, flds=\'a, b, c\', types=\'U8, f8, i8\')\r\n       a_r\r\n       rec.array([(\'0\',  1., 2), (\'3\',  4., 5), (\'6\',  7., 8)],\r\n          dtype=[(\'a\', \'<U8\'), (\'b\', \'<f8\'), (\'c\', \'<i8\')])\r\n\r\n    """"""\r\n    _, c = a.shape\r\n    if flds is None:\r\n        flds = list(""ABCDEFGHIJKLMNOPQRSTUVWXYZ"")[:c]\r\n        flds = "", "".join([n for n in flds])\r\n    if types is None:\r\n        types = a.dtype.str  # a.dtype.descr[0][1]\r\n        types = "", "".join([""{}"".format(types) for i in range(c)])\r\n    a_r = np.core.records.fromarrays(a.transpose(),\r\n                                     names=flds,\r\n                                     formats=types)\r\n    return a_r\r\n\r\n\r\ndef nd_struct(a, flds=None, types=None):\r\n    """"""""Change an array with uniform dtype to an array of mixed dtype as a\r\n    structured array.\r\n\r\n    Requires:\r\n    ---------\r\n    flds : list or None\r\n        flds=[\'A\', \'B\', \'C\']\r\n    types : list or None\r\n        types=[\'U8\', \'f8\', \'i8\']\r\n\r\n    See also:\r\n    ---------\r\n    nd_rec : alternate using strings rather than list inputs\r\n\r\n    Example::\r\n\r\n        a = np.arange(9).reshape(3, 3)\r\n        a_s = nd_struct(a, flds=[\'A\', \'B\', \'C\'], types=[\'U8\', \'f8\', \'i8\'])\r\n        a_s\r\n        array([(\'0\',  1., 2), (\'3\',  4., 5), (\'6\',  7., 8)],\r\n              dtype=[(\'A\', \'<U8\'), (\'B\', \'<f8\'), (\'C\', \'<i8\')])\r\n\r\n    Timing of nd_rec and nd_struct\r\n\r\n    >>> %timeit nd_rec(a, flds=\'a, b, c\', types=\'U8, f8, i8\')\r\n    465 \xc2\xb5s \xc2\xb1 53 \xc2\xb5s per loop (mean \xc2\xb1 std. dev. of 7 runs, 1000 loops each)\r\n\r\n    >>> %timeit nd_struct(a, flds=[\'A\', \'B\', \'C\'], types=[\'U8\', \'f8\', \'i8\'])\r\n    253 \xc2\xb5s \xc2\xb1 27.1 \xc2\xb5s per loop (mean \xc2\xb1 std. dev. of 7 runs, 1000 loops each)\r\n    """"""\r\n    _, c = a.shape\r\n    dt_base = [a.dtype.str] * c  # a.dtype.descr[0][1]\r\n    if flds is None:\r\n        flds = list(""ABCDEFGHIJKLMNOPQRSTUVWXYZ"")[:c]\r\n    if types is None:\r\n        types = dt_base\r\n    dt0 = np.dtype(list(zip(flds, dt_base)))\r\n    dt1 = list(zip(flds, types))\r\n    a_s = a.view(dtype=dt0).squeeze(axis=-1).astype(dt1)\r\n    return a_s\r\n\r\n\r\ndef nd2struct(a, fld_names=None):\r\n    """"""Return a view of an ndarray as structured array with a uniform dtype/\r\n\r\n    Parameters\r\n    ----------\r\n    a : array\r\n        ndarray with a uniform dtype.\r\n    fld_names : list\r\n        A list of strings one for each column/field.  If none are provided,\r\n        then the field names are assigned from an alphabetical list up to 26\r\n        fields.  The dtype of the input array is retained, but can be upcast.\r\n\r\n    Examples\r\n    --------\r\n    >>> a = np.arange(2*3).reshape(2,3)\r\n    array([[0, 1, 2],\r\n           [3, 4, 5]])  # dtype(\'int64\')\r\n    >>> b = nd2struct(a)\r\n    array([(0, 1, 2), (3, 4, 5)],\r\n          dtype=[(\'A\', \'<i8\'), (\'B\', \'<i8\'), (\'C\', \'<i8\')])\r\n    >>> c = nd2struct(a.astype(np.float64))\r\n    array([( 0.,  1.,  2.), ( 3.,  4.,  5.)],\r\n          dtype=[(\'A\', \'<f8\'), (\'B\', \'<f8\'), (\'C\', \'<f8\')])\r\n\r\n    See Also\r\n    --------\r\n    pack_last_axis(arr, names=None) at the end\r\n\r\n    """"""\r\n    if a.dtype.names:  # return if a structured array already\r\n        return a\r\n    alph = ""ABCDEFGHIJKLMNOPQRSTUVWXYZ""\r\n    if a.ndim != 2:\r\n        frmt = ""Wrong array shape... read the docs..\\n{}""\r\n        print(frmt.format(nd2struct.__doc__))\r\n        return a\r\n    _, cols = a.shape\r\n    if fld_names is None:\r\n        names = list(alph)[:cols]\r\n    elif (len(fld_names) == cols) and (cols < 26):\r\n        names = fld_names\r\n    else:  # from... pack_last_axis\r\n        names = [\'f{:02.0f}\'.format(i) for i in range(cols)]\r\n    return a.view([(n, a.dtype) for n in names]).squeeze(-1)\r\n\r\n\r\ndef nd2rec(a, fld_names=None):\r\n    """"""Shell to nd2struct but yielding a recarray.\r\n    """"""\r\n    a = nd2struct(a, fld_names=fld_names)\r\n    return a.view(type=np.recarray)\r\n\r\n\r\ndef rc_vals(a):\r\n    """"""Convert array to rcv, for 2D arrays.  See xy_val for details.\r\n    """"""\r\n    r, c = a.shape\r\n    n = r * c\r\n    x, y = np.meshgrid(np.arange(c), np.arange(r))\r\n    dt = [(\'Row\', \'<i8\'), (\'Col\', \'<i8\'), (\'Val\', a.dtype.str)]\r\n    out = np.zeros((n,), dtype=dt)\r\n    out[\'Row\'] = x.ravel()\r\n    out[\'Col\'] = y.ravel()\r\n    out[\'Val\'] = a.ravel()\r\n    return out\r\n\r\n\r\ndef xy_vals(a):\r\n    """"""Convert array to xyz, for 2D arrays\r\n\r\n    Parameters:\r\n    -----------\r\n    a : array\r\n        2D array of values\r\n    Returns:\r\n    --------\r\n    Triplets of x, y and vals as an nx3 array\r\n\r\n    >>> a = np.random.randint(1,5,size=(2,4))\r\n    >>> a\r\n    array([[4, 1, 4, 3],\r\n           [2, 3, 1, 3]])\r\n    >>> xy_val(a)\r\n    array([(0, 0, 4), (1, 0, 1), (2, 0, 4), (3, 0, 3),\r\n           (0, 1, 2), (1, 1, 3), (2, 1, 1), (3, 1, 3)],\r\n          dtype=[(\'X\', \'<i8\'), (\'Y\', \'<i8\'), (\'Val\', \'<i4\')])\r\n    """"""\r\n    r, c = a.shape\r\n    n = r * c\r\n    x, y = np.meshgrid(np.arange(c), np.arange(r))\r\n    dt = [(\'X\', \'<i8\'), (\'Y\', \'<i8\'), (\'Val\', a.dtype.str)]\r\n    out = np.zeros((n,), dtype=dt)\r\n    out[\'X\'] = x.ravel()\r\n    out[\'Y\'] = y.ravel()\r\n    out[\'Val\'] = a.ravel()\r\n    return out\r\n\r\n\r\n# ---- arrays_cols ----\r\ndef arrays_struct(arrs):\r\n    """"""Stack arrays of any dtype to form a structured array, stacked in\r\n    columns format.\r\n    """"""\r\n    if len(arrs) < 2:\r\n        return arrs\r\n    out_dt = [i.dtype.descr[0] for i in arrs]\r\n    N = arrs[0].shape[0]\r\n    out = np.empty((N,), dtype=out_dt)\r\n    names = np.dtype(out_dt).names\r\n    for i in range(len(names)):\r\n        out[names[i]] = arrs[i]\r\n    return out\r\n\r\n\r\n# ----------------------------------------------------------------------------\r\n# ---- (3) change/modify arrays ... code section ----\r\n# ---- change_arr, scale, split_array, concat_arrs\r\n#\r\ndef change_arr(a, order=None, prn=False):\r\n    """"""Reorder and/or drop columns in an ndarray or structured array.\r\n\r\n    Fields not included will be dropped in the output array.\r\n\r\n    Parameters\r\n    ----------\r\n    order : list of fields\r\n        fields in the order that you want them\r\n    prn : boolean\r\n        True, prints additional information prior to returning the array\r\n\r\n    Notes:\r\n    ------\r\n    *reorder fields : [\'a\', \'c\', \'b\']*\r\n        For a structured/recarray, the desired field order is required.\r\n        An ndarray, not using named fields, will require the numerical\r\n        order of the fields.\r\n\r\n    *remove fields : [\'a\', \'c\']*   ...  `b` is dropped\r\n        To remove fields, simply leave them out of the list.  The\r\n        order of the remaining fields will be reflected in the output.\r\n        This is a convenience function.... see the module header for\r\n        one-liner syntax.\r\n\r\n    Tip :\r\n        Use... `arraytools._base_functions.arr_info(a, verbose=True)`\r\n        This gives field names which can be copied for use here.\r\n\r\n    """"""\r\n    if order is None or (not isinstance(order, (list, tuple))):\r\n        print(""Order not given in a list or tuple"")\r\n        return a\r\n    names = a.dtype.names\r\n    if names is None:\r\n        b = a[:, order]\r\n    else:\r\n        out_flds = []\r\n        out_flds = [i for i in order if i in names]\r\n        if prn:\r\n            missing = [i for i in names if i not in order]\r\n            missing.extend([i for i in order if i not in out_flds])\r\n            frmt = """"""\r\n            : change(a)\r\n            : - field(s) {}\r\n            : - not found, missing or removed.\r\n            """"""\r\n            print(dedent(frmt).format(missing))\r\n        b = a[out_flds]\r\n    return b\r\n\r\n\r\ndef concat_arrs(arrs, sep="" "", name=None, with_ids=True):\r\n    """"""Concatenate a sequence of arrays to string format and return a\r\n    structured array or ndarray\r\n\r\n    arrs : list\r\n        A list of single arrays of the same length\r\n    sep : string\r\n        The separator between lists\r\n    name : string\r\n        A default name used for constructing the array field names.\r\n    """"""\r\n    N = len(arrs)\r\n    if N < 2:\r\n        return arrs\r\n    a, b = arrs[0], arrs[1]\r\n    c = [""{}{}{}"".format(i, sep, j) for i, j in list(zip(a, b))]\r\n    if N > 2:\r\n        for i in range(2, len(arrs)):\r\n            c = [""{}{}{}"".format(i, sep, j) for i, j in list(zip(c, arrs[i]))]\r\n    c = np.asarray(c)\r\n    sze = c.dtype.str\r\n    if name is not None:\r\n        c.dtype = [(name, sze)]\r\n    else:\r\n        name = \'f\'\r\n    if with_ids:\r\n        tmp = np.copy(c)\r\n        dt = [(\'IDs\', \'<i8\'), (name, sze)]\r\n        c = np.empty((tmp.shape[0], ), dtype=dt)\r\n        c[\'IDs\'] = np.arange(1, tmp.shape[0] + 1)\r\n        c[name] = tmp\r\n    return c\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# ---- (4) stride, block and pad .... code section\r\n# ----  pad_, stride, sliding_window_view, block\r\n#\r\ndef pad_(a, pad_with=None, size=(1, 1)):\r\n    """"""To use when padding a strided array for window construction.\r\n\r\n    Parameters:\r\n    ----------\r\n    pad_with : number\r\n        Options for number types\r\n    - ints : 0, +/-128, +/-32768 `np.iinfo(np.int16).min or max 8, 16, 32`.\r\n    - float : 0., np.nan, np.inf, `-np.inf` or `np.finfo(float64).min or max`\r\n    size : list/tuple\r\n        Size of padding on sides in cells.\r\n    - 2D : 1 cell => (1,1)\r\n    - 3D : 1 cell => (1,1,1)\r\n    """"""\r\n    if pad_with is None:\r\n        return a\r\n    #\r\n    new_shape = tuple(i+2 for i in a.shape)\r\n    tmp = np.zeros(new_shape, dtype=a.dtype)\r\n    tmp.fill(pad_with)\r\n    if tmp.ndim == 2:\r\n        tmp[1:-1, 1:-1] = a\r\n    elif tmp.ndim == 3:\r\n        tmp[1:-1, 1:-1, 1:-1] = a\r\n    a = np.copy(tmp, order=\'C\')\r\n    del tmp\r\n    return a\r\n\r\n\r\ndef pad_sides(a, TL=(0, 0), RB=(0, 0), value=0):\r\n    """"""Pad an array\'s T(op), L(eft), B(ottom) and R(ight) sides with `value`.\r\n\r\n    Parameters:\r\n    -----------\r\n    `pad_by` : tuple of integers\r\n        Pad the T, B rows and L, R columns.\r\n    `value` : integer\r\n        Value to use on all axes\r\n\r\n    >>> a np.array([[0, 1, 2], [3, 4, 5]])\r\n    >>> pad_sides(a, (0, 1, 0, 1), -1)\r\n    array([[ 0,  1,  2, -1],\r\n           [ 3,  4,  5, -1],\r\n           [-1, -1, -1, -1]])\r\n    >>> pad_sides(a, (1, 0, 0, 2), -1)\r\n    array([[-1, -1, -1, -1, -1],\r\n           [ 0,  1,  2, -1, -1],\r\n           [ 3,  4,  5, -1, -1]])\r\n    """"""\r\n    L, T = TL\r\n    R, B = RB\r\n    a = np.pad(a, pad_width=((T, B), (L, R)), mode=\'constant\',\r\n               constant_values=value)\r\n    return a\r\n\r\n\r\ndef needed(a, win=(3, 3)):\r\n    """"""pad size for right bottom padding given array shape and window size\r\n    """"""\r\n    shp = a.shape\r\n    RB = np.remainder(shp, win)\r\n    return RB\r\n\r\n\r\ndef stride(a, win=(3, 3), stepby=(1, 1)):\r\n    """"""Provide a 2D sliding/moving view of an array.\r\n    There is no edge correction for outputs. Use the `pad_` function first.\r\n\r\n    Requires\r\n    --------\r\n    as_strided : function\r\n        from numpy.lib.stride_tricks import as_strided\r\n    a : array or list\r\n        Usually a 2D array.  Assumes rows >=1, it is corrected as is the\r\n        number of columns.\r\n    win, stepby : array-like\r\n        tuple/list/array of window strides by dimensions\r\n    ::\r\n\r\n        - 1D - (3,)       (1,)       3 elements, step by 1\r\n        - 2D - (3, 3)     (1, 1)     3x3 window, step by 1 rows and col.\r\n        - 3D - (1, 3, 3)  (1, 1, 1)  1x3x3, step by 1 row, col, depth\r\n\r\n    Examples\r\n    --------\r\n    >>> a = np.arange(10)\r\n    >>> # stride(a, (3,), (1,)) 3 value moving window, step by 1\r\n    >>> stride(a, (3,), (2,))\r\n    array([[0, 1, 2],\r\n           [2, 3, 4],\r\n           [4, 5, 6],\r\n           [6, 7, 8]])\r\n    >>> a = np.arange(6*6).reshape(6, 6)\r\n    #    stride(a, (3, 3), (1, 1))  sliding window\r\n    #    stride(a, (3, 3), (3, 3))  block an array\r\n\r\n    Notes:\r\n    -----\r\n    - np.product(a.shape) == a.size   # shape product equals array size\r\n    - To check if the base array and the strided version share memory\r\n    - np.may_share_memory(a, a_s)     # True\r\n\r\n    ----------------------------------------------------------\r\n    """"""\r\n    err = """"""Array shape, window and/or step size error.\r\n    Use win=(3,) with stepby=(1,) for 1D array\r\n    or win=(3,3) with stepby=(1,1) for 2D array\r\n    or win=(1,3,3) with stepby=(1,1,1) for 3D\r\n    ----    a.ndim != len(win) != len(stepby) ----\r\n    """"""\r\n    from numpy.lib.stride_tricks import as_strided\r\n    a_ndim = a.ndim\r\n    if isinstance(win, int):\r\n        win = (win,) * a_ndim\r\n    if isinstance(stepby, int):\r\n        stepby = (stepby,) * a_ndim\r\n    assert (a_ndim == len(win)) and (len(win) == len(stepby)), err\r\n    shp = np.array(a.shape)    # array shape (r, c) or (d, r, c)\r\n    win_shp = np.array(win)    # window      (3, 3) or (1, 3, 3)\r\n    ss = np.array(stepby)      # step by     (1, 1) or (1, 1, 1)\r\n    newshape = tuple(((shp - win_shp) // ss) + 1) + tuple(win_shp)\r\n    newstrides = tuple(np.array(a.strides) * ss) + a.strides\r\n    a_s = as_strided(a, shape=newshape, strides=newstrides, subok=True).squeeze()\r\n    return a_s\r\n\r\n\r\n# ---- sliding_window_view .... new ----\r\ndef sliding_window_view(x, shape=None):\r\n    """"""Create rolling window views of the 2D array with the given shape.\r\n    proposed for upcoming numpy version.\r\n    """"""\r\n    if shape is None:\r\n        shape = x.shape\r\n    o = np.array(x.shape) - np.array(shape) + 1  # output shape\r\n    strides = x.strides\r\n    view_shape = np.concatenate((o, shape), axis=0)\r\n    view_strides = np.concatenate((strides, strides), axis=0)\r\n    return np.lib.stride_tricks.as_strided(x, view_shape, view_strides)\r\n\r\n\r\ndef block(a, win=(3, 3)):\r\n    """"""Calls stride with step_by equal to win size.\r\n    No padding of the array, so this works best when win size is divisible\r\n    in both directions\r\n\r\n    Note:\r\n        see block_arr if you want padding\r\n    """"""\r\n    a_b = stride(a, win=win, stepby=win)\r\n    return a_b\r\n\r\n\r\ndef block_arr(a, win=[3, 3], nodata=-1, as_masked=False):\r\n    """"""Block array into window sized chunks padding to the right and bottom\r\n    to accommodate array and window shape.\r\n\r\n    Parameters\r\n    ----------\r\n    `a` : array\r\n        2D array\r\n    `win` : [integer, integer]\r\n        [rows, cols], aka y,x, m,n sized window\r\n    `nodata` : number\r\n        to use for the mask\r\n\r\n    Returns\r\n    -------\r\n    The padded array and the masked array blocked.\r\n\r\n    Reference\r\n    ---------\r\n    `<http://stackoverflow.com/questions/40275876/how-to-reshape-this-image-\r\n    array-in-python>`_.\r\n\r\n    >>> def block_2(a, blocks=2)\r\n            B = blocks # Blocksize\r\n            m, n = a.shape\r\n            out = a.reshape(m//B, B, n//B, B).swapaxes(1, 2).reshape(-1, B, B)\r\n            return out\r\n\r\n    """"""\r\n    s = np.array(a.shape)\r\n    if len(win) != 2:\r\n        print(""\\n....... Read the docs .....\\n{}"".format(block_arr.__doc__))\r\n        return None\r\n    win = np.asarray(win)\r\n    m = divmod(s, win)\r\n    s2 = win*m[0] + win*(m[1] != 0)\r\n    ypad, xpad = s2 - a.shape\r\n    pad = ((0, ypad), (0, xpad))\r\n    p_with = ((nodata, nodata), (nodata, nodata))\r\n    b = np.pad(a, pad_width=pad, mode=\'constant\', constant_values=p_with)\r\n    w_y, w_x = win       # Blocksize\r\n    y, x = b.shape       # padded array\r\n    c = b.reshape((y//w_y, w_y, x//w_x, w_x))\r\n    c = c.swapaxes(1, 2).reshape(-1, w_y, w_x)\r\n    if as_masked:\r\n        c = np.ma.masked_equal(c, nodata)\r\n        c.set_fill_value(nodata)\r\n    return c\r\n\r\n\r\ndef rolling_stats(a, no_null=True, prn=True):\r\n    """"""Statistics on the last two dimensions of an array.\r\n\r\n    Requires\r\n    --------\r\n    a : array\r\n        2D array  **Note, use \'stride\' above to obtain rolling stats\r\n    no_null : boolean\r\n        Whether to use masked values (nan) or not.\r\n    prn : boolean\r\n        To print the results or return the values.\r\n\r\n    Returns\r\n    -------\r\n    The results return an array of 4 dimensions representing the original\r\n    array size and block size.  An original 6x6 array will be broken into\r\n    block 4 3x3 chunks.\r\n    """"""\r\n    a = np.asarray(a)\r\n    a = np.atleast_2d(a)\r\n    ax = None\r\n    if a.ndim > 1:\r\n        ax = tuple(np.arange(len(a.shape))[-2:])\r\n    if no_null:\r\n        a_min = a.min(axis=ax)\r\n        a_max = a.max(axis=ax)\r\n        a_mean = a.mean(axis=ax)\r\n        a_med = np.median(a, axis=ax)\r\n        a_sum = a.sum(axis=ax)\r\n        a_std = a.std(axis=ax)\r\n        a_var = a.var(axis=ax)\r\n        a_ptp = a_max - a_min\r\n    else:\r\n        a_min = np.nanmin(a, axis=(ax))\r\n        a_max = np.nanmax(a, axis=(ax))\r\n        a_mean = np.nanmean(a, axis=(ax))\r\n        a_med = np.nanmedian(a, axis=(ax))\r\n        a_sum = np.nansum(a, axis=(ax))\r\n        a_std = np.nanstd(a, axis=(ax))\r\n        a_var = np.nanvar(a, axis=(ax))\r\n        a_ptp = a_max - a_min\r\n    if prn:\r\n        s = [\'Min\', \'Max\', \'Mean\', \'Med\', \'Sum\', \'Std\', \'Var\', \'Range\']\r\n        frmt = ""...\\n{}\\n"".join([i for i in s])\r\n        v = [a_min, a_max, a_mean, a_med, a_sum, a_std, a_var, a_ptp]\r\n        args = [indent(str(i), \'... \') for i in v]\r\n        print(frmt.format(*args))\r\n    else:\r\n        return a_min, a_max, a_mean, a_med, a_sum, a_std, a_var, a_ptp\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# ---- (5) querying, working with arrays ----\r\n# ----------------------------------------------------------------------\r\n# ---- _func, find, group_pnts, group_vals, reclass\r\n#\r\ndef _func(fn, a, this):\r\n    """"""Called by \'find\' see details there\r\n    (cumsum, eq, neq, ls, lseq, gt, gteq, btwn, btwni, byond)\r\n    """"""\r\n    #\r\n    fn = fn.lower().strip()\r\n    if fn in [\'cumsum\', \'csum\', \'cu\']:\r\n        v = np.where(np.cumsum(a) <= this)[0]\r\n    elif fn in [\'eq\', \'e\', \'==\']:\r\n        v = np.where(np.in1d(a, this))[0]\r\n    elif fn in [\'neq\', \'ne\', \'!=\']:\r\n        v = np.where(~np.in1d(a, this))[0]  # (a, this, invert=True)\r\n    elif fn in [\'ls\', \'les\', \'<\']:\r\n        v = np.where(a < this)[0]\r\n    elif fn in [\'lseq\', \'lese\', \'<=\']:\r\n        v = np.where(a <= this)[0]\r\n    elif fn in [\'gt\', \'grt\', \'>\']:\r\n        v = np.where(a > this)[0]\r\n    elif fn in [\'gteq\', \'gte\', \'>=\']:\r\n        v = np.where(a >= this)[0]\r\n    elif fn in [\'btwn\', \'btw\', \'>a<\']:\r\n        low, upp = this\r\n        v = np.where((a >= low) & (a < upp))[0]\r\n    elif fn in [\'btwni\', \'btwi\', \'=>a<=\']:\r\n        low, upp = this\r\n        v = np.where((a >= low) & (a <= upp))[0]\r\n    elif fn in [\'byond\', \'bey\', \'<a>\']:\r\n        low, upp = this\r\n        v = np.where((a < low) | (a > upp))[0]\r\n    return v\r\n\r\n\r\n# @time_deco\r\ndef find(a, func, this=None, count=0, keep=None, prn=False, r_lim=2):\r\n    """"""Find the conditions that are met in an array, defined by `func`.\r\n    `this` is the condition being looked for.  The other parameters are defined\r\n    in the Parameters section.\r\n\r\n    >>> a = np.arange(10)\r\n    >>> find(a, \'gt\', this=5)\r\n    array([6, 7, 8, 9])\r\n\r\n    Parameters\r\n    ----------\r\n    `a` :\r\n        Array or array like.\r\n    `func` :\r\n        `(cumsum, eq, neq, ls, lseq, gt, gteq, btwn, btwni, byond)`\r\n        (        ==,  !=,  <,   <=,  >,   >=,  >a<, =>a<=,  <a> )\r\n    `count` :\r\n        only used for recursive functions\r\n    `keep` :\r\n        for future use\r\n    `verbose` :\r\n        True for test printing\r\n    `max_depth` :\r\n        prevent recursive functions running wild, it can be varied\r\n\r\n    Recursive functions:\r\n    -------------------\r\n    cumsum :\r\n        An example of using recursion to split a list/array of data\r\n        parsing the results into groups that sum to this.  For example,\r\n        split input into groups where the total population is less than\r\n        a threshold (this).  The default is to use a sequential list,\r\n        however, the inputs could be randomized prior to running.\r\n\r\n    Returns\r\n    -------\r\n        A 1D or 2D array meeting the conditions\r\n\r\n    """"""\r\n    a = np.asarray(a)              # ---- ensure array format\r\n    if keep is None:\r\n        keep = []\r\n    this = np.asarray(this)\r\n    # masked = np.ma.is_masked(a)    # ---- check for masked array\r\n    if prn:                        # ---- optional print\r\n        print(""({}) Input values....\\n  {}"".format(count, a))\r\n    ix = _func(func, a, this)      # ---- sub function -----\r\n    if ix is not None:\r\n        keep.append(a[ix])         # ---- slice and save\r\n        if len(ix) > 1:\r\n            a = a[(len(ix)):]      # ---- use remainder\r\n        else:\r\n            a = a[(len(ix)+1):]\r\n    if prn:                        # optional print\r\n        print(""  Remaining\\n  {}"".format(a))\r\n    # ---- recursion functions check and calls ----\r\n    if func in [\'cumsum\']:  # functions that support recursion\r\n        if (len(a) > 0) and (count < r_lim):  # recursive call\r\n            count += 1\r\n            find(a, func, this, count, keep, prn, r_lim)\r\n        elif count == r_lim:\r\n            frmt = """"""Recursion check... count {} == {} recursion limit\r\n                   Warning...increase recursion limit, reduce sample size\\n\r\n                   or changes conditions""""""\r\n            print(dedent(frmt).format(count, r_lim))\r\n    # ---- end recursive functions ----\r\n    # print(""keep for {} : {}"".format(func,keep))\r\n    #\r\n    if len(keep) == 1:   # for most functions, this is it\r\n        final = keep[0]\r\n    else:                # for recursive functions, there will be more\r\n        temp = []\r\n        incr = 0\r\n        for i in keep:\r\n            temp.append(np.vstack((i, np.array([incr]*len(i)))))\r\n            incr += 1\r\n        temp = (np.hstack(temp)).T\r\n        dt = [(\'orig\', \'<i8\'), (\'class\', \'<i8\')]\r\n        final = np.zeros((temp.shape[0],), dtype=dt)\r\n        final[\'orig\'] = temp[:, 0]\r\n        final[\'class\'] = temp[:, 1]\r\n        # ---- end recursive section\r\n    return final\r\n\r\n\r\ndef find_closest(a, close_to=1):\r\n    """"""Change values in an ndarray to match the closest in `close_to`.  This\r\n    may be a scalar, or array-like for multiple cases.\r\n\r\n    a : array\r\n        an ndarray of integer or floating point values\r\n    close_to : number or array-like\r\n        If a number, it will return a close_to or 0.  If array-like, then the\r\n        closest value in close_to will be returned\r\n\r\n    >>> a = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\r\n    >>> find_closest(a, close_to=[3, 6, 9])\r\n    array([3, 3, 3, 3, 3, 6, 6, 6, 9, 9])\r\n    This behaviour differs from np.digitize\r\n\r\n    >>> np.digitize(a, bins=[3, 6, 9])\r\n    array([0, 0, 0, 1, 1, 1, 2, 2, 2, 3], dtype=int64)\r\n\r\n    >>> a = np.array([[3, 2, 4, 3, 3],\r\n                      [1, 2, 3, 4, 1],\r\n                      [2, 4, 4, 1, 1]])\r\n    >>> find_closest(a, close_to=[0, 4])\r\n    array([[4, 0, 4, 4, 4],\r\n           [0, 0, 4, 4, 0],\r\n           [0, 4, 4, 0, 0]])\r\n\r\n    """"""\r\n    shp = a.shape\r\n    if np.isscalar(close_to):\r\n        z = a.ravel()\r\n        val = z[np.abs(z - close_to).argmin()]\r\n        r = np.where(a == val, a, 0)\r\n    else:\r\n        close_to = np.atleast_1d(close_to)\r\n        r = close_to[np.argmin(np.abs(a.ravel()[:, np.newaxis] - close_to), axis=1)]\r\n        r = r.reshape(shp)\r\n    return r\r\n\r\n\r\ndef group_pnts(a, key_fld=\'IDs\', shp_flds=[\'Xs\', \'Ys\']):\r\n    """"""Group points for a feature that has been exploded to points by\r\n    `arcpy.da.FeatureClassToNumPyArray`.\r\n\r\n    Parameters:\r\n    ---------\r\n    `a` : array\r\n        A structured array, assuming ID, X, Y, {Z} and whatever else\r\n        the array is assumed to be sorted... which will be the case\r\n    `key_fld` : string\r\n        Normally this is the `IDs` or similar\r\n    `shp_flds` : strings\r\n        The fields that are used to produce the geometry.\r\n\r\n    Returns:\r\n    -------\r\n    See np.unique descriptions below\r\n\r\n    References:\r\n    ----------\r\n    `<https://jakevdp.github.io/blog/2017/03/22/group-by-from-scratch/>`_.\r\n    `<http://esantorella.com/2016/06/16/groupby/>`_.\r\n\r\n    Notes:\r\n    -----\r\n    split-apply-combine .... that is the general rule\r\n\r\n    """"""\r\n    returned = np.unique(a[key_fld],           # the unique id field\r\n                         return_index=True,    # first occurrence index\r\n                         return_inverse=True,  # indices needed to remake array\r\n                         return_counts=True)   # number in each group\r\n    uni, idx, inv, cnt = returned\r\n#    from_to = [[idx[i-1], idx[i]] for i in range(1, len(idx))]\r\n    from_to = list(zip(idx, np.cumsum(cnt)))\r\n    subs = [a[shp_flds][i:j] for i, j in from_to]\r\n    groups = [sub.view(dtype=\'float\').reshape(sub.shape[0], -1)\r\n              for sub in subs]\r\n    return groups\r\n\r\n\r\n# ---- (6) analysis .... code section ----\r\n# ---- uniq, is_in\r\n#\r\ndef uniq(ar, return_index=False, return_inverse=False,\r\n         return_counts=False, axis=None):\r\n    """"""Taken from, but modified for simple axis 0 and 1 and structured\r\n    arrays in (N, m) or (N,) format.\r\n\r\n    To enable determination of unique values in uniform arrays with\r\n    uniform dtypes.  np.unique in versions < 1.13 need to use this.\r\n\r\n    https://github.com/numpy/numpy/blob/master/numpy/lib/arraysetops.py\r\n    """"""\r\n    ar = np.asanyarray(ar)\r\n    if np.version.version > \'1.13\':\r\n        return np.unique(ar, return_index, return_inverse,\r\n                         return_counts, axis=axis)\r\n\r\n\r\ndef is_in(arr, look_for, keep_shape=True, binary=True, not_in=False):\r\n    """"""Similar to `np.isin` for numpy versions < 1.13, but with additions to\r\n    return the original shaped array with an `int` dtype\r\n\r\n    Parameters:\r\n    ----------\r\n    arr : array\r\n        the array to check for the elements\r\n    look_for : number, list or array\r\n        what to use for the check\r\n    keep_shape : boolean\r\n        True, returns the array\'s original shape.  False, summarizes all axes\r\n    not_in : boolean\r\n        Switch the query look_for True\r\n\r\n    Note:\r\n    ----\r\n    >>> from numpy.lib import NumpyVersion\r\n    >>> if NumpyVersion(np.__version__) < \'1.13.0\'):\r\n        # can add for older versions later\r\n    """"""\r\n    arr = np.asarray(arr)\r\n    shp = arr.shape\r\n    look_for = np.asarray(look_for)\r\n    uni = False\r\n    inv = False\r\n    if not_in:\r\n        inv = True\r\n    r = np.in1d(arr, look_for, assume_unique=uni, invert=inv)\r\n    if keep_shape:\r\n        r = r.reshape(shp)\r\n    if binary:\r\n        r = r.astype(\'int\')\r\n    return r\r\n\r\n\r\ndef running_count(a, to_label=False):\r\n    """"""Perform a running count on a 1D array identifying the order number\r\n    of the value in the sequence.\r\n\r\n    Parameters\r\n    ----------\r\n    `a` : array\r\n        1D array of values, int, float or string\r\n    `to_label` : boolean\r\n        Return the output as a concatenated string of value-sequence numbers if\r\n        True, or if False, return a structured array with a specified dtype.\r\n\r\n    Examples:\r\n    ---------\r\n    >>> a = np.random.randint(1, 10, 10)\r\n    >>> #  [3, 5, 7, 5, 9, 2, 2, 2, 6, 4] #\r\n    >>> running_count(a, False)\r\n    array([(3, 1), (5, 1), (7, 1), (5, 2), (9, 1), (2, 1), (2, 2),\r\n           (2, 3), (6, 1), (4, 1)],\r\n          dtype=[(\'Value\', \'<i4\'), (\'Count\', \'<i4\')])\r\n    >>> running_count(a, True)\r\n    array([\'3_001\', \'5_001\', \'7_001\', \'5_002\', \'9_001\', \'2_001\', \'2_002\',\r\n           \'2_003\', \'6_001\', \'4_001\'],\r\n          dtype=\'<U5\')\r\n\r\n    >>> b = np.array(list(""zabcaabbdedbz""))\r\n    >>> #  [\'z\', \'a\', \'b\', \'c\', \'a\', \'a\', \'b\', \'b\', \'d\', \'e\', \'d\',\'b\', \'z\'] #\r\n    >>> running_count(b, False)\r\n    array([(\'z\', 1), (\'a\', 1), (\'b\', 1), (\'c\', 1), (\'a\', 2), (\'a\', 3),\r\n           (\'b\', 2), (\'b\', 3), (\'d\', 1), (\'e\', 1), (\'d\', 2), (\'b\', 4),\r\n           (\'z\', 2)], dtype=[(\'Value\', \'<U1\'), (\'Count\', \'<i4\')])\r\n    >>> running_count(b, True)\r\n    array([\'z_001\', \'a_001\', \'b_001\', \'c_001\', \'a_002\', \'a_003\', \'b_002\',\r\n           \'b_003\', \'d_001\', \'e_001\', \'d_002\', \'b_004\', \'z_002\'], dtype=\'<U5\')\r\n\r\n    """"""\r\n    dt = [(\'Value\', a.dtype.str), (\'Count\', \'<i4\')]\r\n    z = np.zeros((a.shape[0],), dtype=dt)\r\n    idx = a.argsort(kind=\'mergesort\')\r\n    s_a = a[idx]\r\n    neq = np.where(s_a[1:] != s_a[:-1])[0] + 1\r\n    run = np.ones(a.shape, int)\r\n    run[neq[0]] -= neq[0]\r\n    run[neq[1:]] -= np.diff(neq)\r\n    out = np.empty_like(run)\r\n    out[idx] = run.cumsum()\r\n    z[\'Value\'] = a\r\n    z[\'Count\'] = out\r\n    if to_label:\r\n        z = np.array([""{}_{:0>3}"".format(*i) for i in list(zip(a, out))])\r\n    return z\r\n\r\n\r\ndef sequences(data, stepsize=0):\r\n    """"""Return an array of sequence information denoted by stepsize\r\n\r\n    data :\r\n        List/array of values in 1D\r\n    stepsize :\r\n        Separation between the values.  If stepsize=0, sequences of equal\r\n        values will be searched.  If stepsize is 1, then sequences incrementing\r\n        by 1... etcetera.  Stepsize can be both positive or negative\r\n\r\n    >>> # check for incrementing sequence by 1\'s\r\n    >>> d = [1, 2, 3, 4, 4, 5]\r\n    >>> s = sequences(d, 1)\r\n    array([(0, 1, 4, 0, 4), (1, 4, 2, 4, 6)],\r\n          dtype=[(\'ID\', \'<i4\'), (\'Value\', \'<i4\'), (\'Count\', \'<i4\'),\r\n                 (\'From_\', \'<i4\'), (\'To_\', \'<i4\')])\r\n    >>> prn_rec(s)  # prn_rec in frmts.py\r\n     id  ID   Value   Count   From_   To_\r\n    ---------------------------------------\r\n     000    0       1       4       0     4\r\n     001    1       4       2       4     6\r\n\r\n    Notes:\r\n    ------\r\n    For strings, use\r\n\r\n    >>> partitions = np.where(a[1:] != a[:-1])[0] + 1\r\n\r\n    Variants:\r\n    ---------\r\n    Change `N` in the expression to find other splits in the data\r\n\r\n    >>> np.split(data, np.where(np.abs(np.diff(data)) >= N)[0]+1)\r\n\r\n    References:\r\n    -----------\r\n    https://stackoverflow.com/questions/7352684/how-to-find-the-groups-of-\r\n    sequences-elements-from-an-array-in-numpy\r\n    """"""\r\n    #\r\n    a = np.array(data)\r\n    a_dt = a.dtype.kind\r\n    dt = [(\'ID\', \'<i4\'), (\'Value\', a.dtype.str), (\'Count\', \'<i4\'),\r\n          (\'From_\', \'<i4\'), (\'To_\', \'<i4\')]\r\n    if a_dt in (\'U\', \'S\'):\r\n        seqs = np.split(a, np.where(a[1:] != a[:-1])[0] + 1)\r\n    elif a_dt in (\'i\', \'f\'):\r\n        seqs = np.split(a, np.where(np.diff(a) != stepsize)[0] + 1)\r\n    vals = [i[0] for i in seqs]\r\n    cnts = [len(i) for i in seqs]\r\n    seq_num = np.arange(len(cnts))\r\n    too = np.cumsum(cnts)\r\n    frum = np.zeros_like(too)\r\n    frum[1:] = too[:-1]\r\n    out = np.array(list(zip(seq_num, vals, cnts, frum, too)), dtype=dt)\r\n    return out\r\n\r\n\r\n# ---- (7) sorting,  column and row sorting .... code section ---------------\r\n# ---- sort_rows_by_col, sort_cols_by_row, radial_sort ----\r\ndef sort_rows_by_col(a, col=0, descending=False):\r\n    """"""Sort a 2D array by column.\r\n\r\n    >>> sort_rows_by_col(a, 0, True)\r\n    >>> a =array([[0, 1, 2],    array([[6, 7, 8],\r\n                  [3, 4, 5],           [3, 4, 5],\r\n                  [6, 7, 8]])          [0, 1, 2]])\r\n    """"""\r\n    a = np.asarray(a)\r\n    shp = a.shape[0]\r\n    if not (0 <= abs(col) <= shp):\r\n        raise ValueError(""column ({}) in range (0 to {})"".format(col, shp))\r\n    a_s = a[a[:, col].argsort()]\r\n    if descending:\r\n        a_s = a_s[::-1]\r\n    return a_s\r\n\r\n\r\ndef sort_cols_by_row(a, col=0, descending=False):\r\n    """"""Sort the rows of an array in the order of their column values\r\n    :  Uses lexsort """"""\r\n    ret = a[np.lexsort(np.transpose(a)[::-1])]\r\n    if descending:\r\n        ret = np.flipud(ret)\r\n    return ret\r\n\r\n\r\ndef radial_sort(pnts, cent=None):\r\n    """"""Sort about the point cloud center or from a given point\r\n\r\n    Requires:\r\n    ---------\r\n    pnts : array-like, 2D\r\n        an array of points (x,y) as array or list\r\n    cent : floats\r\n        list, tuple, array of the center\'s x,y coordinates\r\n        cent = [0, 0] or np.array([0, 0])\r\n\r\n    Returns:\r\n    --------\r\n    The angles in the range -180, 180 x-axis oriented, and the sort order.\r\n    """"""\r\n    pnts = np.asarray(pnts, dtype=\'float64\')\r\n    if cent is None:\r\n        cent = pnts.mean(axis=0)\r\n    ba = pnts - cent\r\n    ang_ab = np.arctan2(ba[:, 1], ba[:, 0])\r\n    ang_ab = np.degrees(ang_ab)\r\n    sort_order = np.argsort(ang_ab)\r\n    return ang_ab, sort_order\r\n\r\n\r\ndef view_sort(a):\r\n    """"""Sort 2D arrays assumed to be coordinates and other baggage, in the order\r\n    that they appear in the row.  It is best used for sorting x,y coorinate,\r\n    using argsort.\r\n\r\n    Returns:\r\n    --------\r\n    The sorted array and the indices of their original positions in the\r\n    input array.\r\n    """"""\r\n    a_view = a.view(a.dtype.descr * a.shape[1])\r\n    idx = np.argsort(a_view, axis=0, order=(a_view.dtype.names)).ravel()\r\n    a = np.ascontiguousarray(a[idx])\r\n    return a, idx\r\n\r\ndef xy_sort(a):\r\n    """"""Formally called `view_sort`.  See the documentation there\r\n    """"""\r\n    return view_sort(a)\r\n\r\n\r\n# ---- extras *****\r\n\r\ndef pack_last_axis(arr, names=None):\r\n    """"""used in nd2struct\r\n    Then you could do:\r\n    >>> pack_last_axis(uv).tolist()\r\n    to get a list of tuples.\r\n    """"""\r\n    if arr.dtype.names:\r\n        return arr\r\n    names = names or [\'f{}\'.format(i) for i in range(arr.shape[-1])]\r\n    return arr.view([(n, arr.dtype) for n in names]).squeeze(-1)\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# ----  _help .... code section\r\ndef _tools_help_():\r\n    """"""arraytools.tools help...\r\n\r\n    Function list follows:\r\n    """"""\r\n    _hf = """"""\r\n    :-------------------------------------------------------------------:\r\n    : ---- arrtools functions  (loaded as \'art\') ----\r\n    : ---- from tools.py\r\n    (1)  arr2xyz(a, verbose=False)\r\n         array (col, rows) to (x, y) and array values for z.\r\n    (2)  make_blocks(rows=3, cols=3, r=2, c=2, dt=\'int\')\r\n         make arrays consisting of blocks\r\n    (3)  group_vals(seq, delta=1, oper=\'!=\')\r\n    (4)  reclass(a, bins=[], new_bins=[], mask=False, mask_val=None)\r\n         reclass an array\r\n    (5)  scale(a, x=2, y=2, num_z=None)\r\n         scale an array up in size by repeating values\r\n    (6)  split_array(a, fld=\'ID\')\r\n         split an array using an index field\r\n    (7)  make_flds(n=1, as_type=\'float\', names=None, def_name=\'col\')\r\n         make structured/recarray fields\r\n    (8) nd_rec\r\n    (9) nd_struct\r\n    (10) nd2struct(a)\r\n         convert an ndarray to a structured array with fields\r\n    (11) nd2rec\r\n    (12) rc_vals\r\n    (13) xy_vals\r\n    (14) array_cols\r\n    (15) change_arr(a, order=[], prn=False)\r\n         reorder and/or drop columns\r\n    (16) concat_arrs\r\n    (17) pad__\r\n    (18) stride(a, r_c=(3, 3))\r\n         stride an array for moving window functions\r\n    (19) block\r\n    (20) sliding_window_view\r\n    (21) block_arr(a, win=[3, 3], nodata=-1)\r\n         break an array up into blocks\r\n    (22) rolling_stats((a0, no_null=True, prn=True))\r\n    (23) _func, find(a, func, this=None, count=0, keep=[], prn=False, r_lim=2)\r\n         find elements in an array using...\r\n         func - (cumsum, eq, neq, ls, lseq, gt, gteq, btwn, btwni, byond)\r\n               (      , ==,  !=,  <,   <=,  >,   >=,  >a<, =>a<=,  <a> )\r\n    (24)  group_pnts(a, key_fld=\'ID\', keep_flds=[\'X\', \'Y\', \'Z\'])\r\n    (25) uniq(ar, return_index=False, return_inverse=False,\r\n              return_counts=False, axis=0)\r\n    (26) is_in\r\n    (27) running_count\r\n    (28) sequences(data, stepsize)\r\n    (29) sort_rows_by_col\r\n    (30) sort_cols_by_row\r\n    (31) radial_sort\r\n    (32) pack_last_axis\r\n    ---  _tools_help_  this function\r\n    :-------------------------------------------------------------------:\r\n    """"""\r\n    print(dedent(_hf))\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# ---- _demo .... code section\r\n# @run_deco\r\ndef _demo_tools():\r\n    """"""\r\n    : - Run examples of the existing functions.\r\n    """"""\r\n    a = np.arange(3*4).reshape(3, 4).copy()\r\n    b = nd2struct(a)\r\n    c = np.arange(2*3*4).reshape(2, 3, 4)\r\n    d = np.arange(9*6).reshape(9, 6)\r\n    bloc = block_arr(a, win=[2, 2], nodata=-1)  # for block\r\n    chng = change_arr(b, order=[\'B\', \'C\', \'A\'], prn=False)\r\n    scal = scale(a, 2)\r\n    m_blk = make_blocks(rows=3, cols=3, r=2, c=2, dt=\'int\')\r\n    m_fld = str(make_flds(n=3, as_type=\'int\', names=[""A"", ""B"", ""C""]))\r\n    spl = split_array(b, fld=\'A\')\r\n    stri = stride(a, (3, 3))\r\n    rsta = rolling_stats(d, no_null=True, prn=False)\r\n#    arr = np.load(data_path + \'/sample_20.npy\')\r\n#    row = arr[\'County\']\r\n#    col = arr[\'Town\']\r\n#    ctab, a0, result, r0, c0 = crosstab(row, col)\r\n#    arr = arr.reshape(arr.shape[0], -1)\r\n    frmt = """"""\r\n: ----- _demo {}\r\n:\r\n:Input ndarray, \'a\' ...\r\n{!r:}\\n\r\n:Input ndarray, \'b\' ...\r\n{!r:}\\n\r\n:Input ndarray, \'c\' ...\r\n{!r:}\\n\r\n:Input ndarray, \'d\' ...\r\n{!r:}\\n\r\n:---- Functions by number  ---------------------------------------------\r\n:(1)  arr2xyz(a, verbose=False)\r\n{}\\n\r\n:(2)  block_arr(a, win=[2, 2], nodata=-1)\r\n{}\\n\r\n:(3) change_arr(b, order=[\'B\', \'C\', \'A\'], prn=False\r\n:    Array \'b\', reordered with 2 fields dropped...\r\n{!r:}\\n\r\n:(5) scale() ... scale an array up by an integer factor...\r\n{}\\n\r\n:(7) make_flds() ... create default field names ...\r\n{}\\n\r\n:(8) split_array() ... split an array according to an index field\r\n{}\\n\r\n:(9) stride() ... stride an array ....\r\n{}\\n\r\n:(10) make_blocks(rows=3, cols=3, r=2, c=2, dt=\'int\')\r\n{}\\n\r\n:(11) nd_struct() ... make a structured array from another array ...\r\n{!r:}\\n\r\n:(12) rolling_stats()... stats for a strided array ...\r\n:    min, max, mean, sum, std, var, ptp\r\n{}\\n\r\n""""""\r\n    args = [""-""*62, a, b, c, d,\r\n            arr2xyz(a), bloc, chng.reshape(a.shape[0], -1), scal,  # 1 -5\r\n            m_fld, spl, stri, m_blk, nd2struct(a), rsta]  # 6- 12\r\n    print(frmt.format(*args))\r\n    # del args, d, e\r\n\r\n\r\ndef pyramid(core=9, steps=10, incr=(1, 1), posi=True):\r\n    """"""Create a pyramid see pyramid_demo.py""""""\r\n    a = np.array([core])\r\n    a = np.atleast_2d(a)\r\n    for i in range(1, steps):\r\n        val = core - i\r\n        if posi and (val <= 0):\r\n            val = 0\r\n        a = np.lib.pad(a, incr, ""constant"", constant_values=(val, val))\r\n    return a\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# ---- __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))  # script =  sys.argv[0]\r\n#    data_path = script.replace(\'tools.py\', \'Data\')\r\n#    def f():\r\n#        pass\r\n#    print(f.__code__.co_filename)\r\n#    _demo_tools()\r\n'"
all_scripts/triangulate.py,9,"b'# -*- coding: utf-8 -*-\r\n""""""\r\ntriangulate\r\n===========\r\n\r\nScript :   triangulate.py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified : 2018-08-24\r\n\r\nPurpose:  triangulate poly* features\r\n\r\nUseage :\r\n\r\nReferences\r\n----------\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/numpyarraytotable.htm>`_.\r\n\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/tabletonumpyarray.htm>`_.\r\n\r\n`<https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/\r\nscipy.spatial.Delaunay.html>`_.\r\n\r\n`<https://tereshenkov.wordpress.com/2017/11/28/building-concave-hulls-alpha-\r\nshapes-with-pyqt-shapely-and-arcpy/>`_.\r\n---------------------------------------------------------------------\r\n""""""\r\n# pylint: disable=C0103  # invalid-name\r\n# pylint: disable=R0914  # Too many local variables\r\n# pylint: disable=R1710  # inconsistent-return-statements\r\n# pylint: disable=W0105  # string statement has no effect\r\n\r\nimport sys\r\nimport numpy as np\r\nfrom scipy.spatial import Delaunay, Voronoi\r\nfrom scipy.spatial import voronoi_plot_2d, delaunay_plot_2d\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\n# ----\r\n#\r\ndef v_plot(vor):\r\n    """"""from Voronoi help""""""\r\n#    import matplotlib.pyplot as plt\r\n    voronoi_plot_2d(vor)\r\n#    plt.show()\r\n\r\n\r\ndef d_plot(tri):\r\n    """"""delaunay plots, requires new points and simplices\r\n    See : matplotlib.pyplot.triplot ... for additional options""""""\r\n#    import matplotlib.pyplot as plt\r\n    delaunay_plot_2d(tri)\r\n#    plt.show()\r\n\r\n\r\ndef Vor_pnts(pnts, testing=True, plot=True):\r\n    """"""Requires a set of points deemed to be a cluster to delineate as a\r\n    Voronoi diagram. You can do multiple point groupings by using this within\r\n    a loop to return the geometries.\r\n    """"""\r\n    avg = np.mean(pnts, axis=0)\r\n    p = pnts - avg\r\n    tri = Voronoi(p)\r\n    out = []\r\n    for region in tri.regions:\r\n        if not -1 in region:\r\n            polygon = np.array([tri.vertices[i] + avg for i in region])\r\n            out.append(polygon)\r\n            if testing:\r\n                print(""{}"".format(polygon.T))\r\n    if plot:\r\n        voronoi_plot_2d(tri)\r\n    return out\r\n\r\n\r\ndef Del_pnts(pnts, testing=False, plot=True):\r\n    """"""Triangulate the points and return the triangles\r\n\r\n    Parameters:\r\n    -----------\r\n    pnts : np.array\r\n        Points in array format.\r\n    out : array\r\n        an array of triangle points\r\n\r\n    Notes:\r\n    ------\r\n    >>> pnts = pnts.reshape((1,) + pnts.shape)  # a 3D set of points (ndim=3)\r\n    >>> [pnts]  # or pass in as a list\r\n    """"""\r\n    pnts = np.unique(pnts, axis=0)  # get the unique points only\r\n    avg = np.mean(pnts, axis=0)\r\n    p = pnts - avg\r\n    tri = Delaunay(p)\r\n    simps = tri.simplices\r\n    del_pnts = [p[s]+avg for s in simps]\r\n    if testing:\r\n        print(""{}"".format(del_pnts))\r\n    if plot:\r\n        delaunay_plot_2d(tri)\r\n    return del_pnts, pnts, simps\r\n\r\n\r\n# ---- Do the work\r\n#\r\n\r\n#pnts = np.array([[0, 0], [0, 100], [100, 100], [100, 80], [20,  80],\r\n#                 [20, 20], [100, 20], [100, 0], [0, 0]])\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n'"
all_scripts/utils.py,9,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nutils\r\n=====\r\n\r\nScript :   utils.py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified : 2018-11-22\r\n\r\nPurpose:  tools for working with numpy arrays\r\n\r\nUseage:\r\n-------\r\n\r\n**doc_func(func=None)** : see get_func and get_modu\r\n\r\n**get_func** : retrieve function information\r\n::\r\n    get_func(func, line_nums=True, verbose=True)\r\n    print(art.get_func(art.main))\r\n\r\n    Function: .... main ....\r\n    Line number... 1334\r\n    Docs:\r\n    Do nothing\r\n    Defaults: None\r\n    Keyword Defaults: None\r\n    Variable names:\r\n    Source code:\r\n       0  def main():\r\n       1   \'\'\'Do nothing\'\'\'\r\n       2      pass\r\n\r\n**get_modu** : retrieve module info\r\n\r\n    get_modu(obj, code=False, verbose=True)\r\n\r\n**info(a, prn=True)** : retrieve array information\r\n::\r\n    - array([(0, 1, 2, 3, 4), (5, 6, 7, 8, 9),\r\n             (10, 11, 12, 13, 14), (15, 16, 17, 18, 19)],\r\n      dtype=[(\'A\', \'<i8\'), (\'B\', \'<i8\')... snip ..., (\'E\', \'<i8\')])\r\n    ---------------------\r\n    Array information....\r\n    array\r\n      |__shape (4,)\r\n      |__ndim  1\r\n      |__size  4\r\n      |__type  <class \'numpy.ndarray\'>\r\n    dtype      [(\'A\', \'<i8\'), (\'B\', \'<i8\') ... , (\'E\', \'<i8\')]\r\n      |__kind  V\r\n      |__char  V\r\n      |__num   20\r\n      |__type  <class \'numpy.void\'>\r\n      |__name  void320\r\n      |__shape ()\r\n      |__description\r\n         |__name, itemsize\r\n         |__[\'A\', \'<i8\']\r\n         |__[\'B\', \'<i8\']\r\n         |__[\'C\', \'<i8\']\r\n         |__[\'D\', \'<i8\']\r\n         |__[\'E\', \'<i8\']\r\n\r\nReferences\r\n----------\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/numpyarraytotable.htm>`_.\r\n\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/tabletonumpyarray.htm>`_.\r\n\r\n---------------------------------------------------------------------\r\n""""""\r\n# pylint: disable=C0103\r\n# pylint: disable=R1710\r\n# pylint: disable=R0914\r\n\r\nimport sys\r\nfrom textwrap import dedent, indent, wrap\r\nimport warnings\r\nimport numpy as np\r\n\r\nwarnings.simplefilter(\'ignore\', FutureWarning)\r\n\r\n#from arcpytools import fc_info, tweet  #, frmt_rec, _col_format\r\n#import arcpy\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\n__all__ = [\'time_deco\',\r\n           \'run_deco\',\r\n           \'doc_func\',\r\n           \'get_func\',\r\n           \'get_modu\',\r\n           \'dirr\',\r\n           \'wrapper\',\r\n           \'_utils_help_\'\r\n           ]\r\n\r\n# ---- decorators and helpers ------------------------------------------------\r\n#\r\ndef time_deco(func):  # timing originally\r\n    """"""Timing decorator function\r\n\r\n    Requires:\r\n    ---------\r\n    The following import.  Uncomment the import or move it inside the script.\r\n\r\n    >>> from functools import wraps\r\n\r\n    Useage::\r\n\r\n        @time_deco  # on the line above the function\r\n        def some_func():\r\n            \'\'\'do stuff\'\'\'\r\n            return None\r\n\r\n    """"""\r\n    import time\r\n    from functools import wraps\r\n\r\n    @wraps(func)\r\n    def wrapper(*args, **kwargs):\r\n        t_0 = time.perf_counter()        # start time\r\n        result = func(*args, **kwargs)   # ... run the function ...\r\n        t_1 = time.perf_counter()        # end time\r\n        dt = t_1 - t_0\r\n        print(""\\nTiming function for... {}"".format(func.__name__))\r\n        if result is None:\r\n            result = 0\r\n        print(""  Time: {: <8.2e}s for {:,} objects"".format(dt, result))\r\n        # return result                   # return the result of the function\r\n        return dt                       # return delta time\r\n    return wrapper\r\n\r\n\r\ndef run_deco(func):\r\n    """"""Prints basic function information and the results of a run.\r\n\r\n    Requires:\r\n    ---------\r\n    The following import.  Uncomment the import or move it inside the script.\r\n\r\n    >>> from functools import wraps\r\n\r\n    Useage::\r\n\r\n        @run_deco  # on the line above the function\r\n        def some_func():\r\n            \'\'\'do stuff\'\'\'\r\n            return None\r\n\r\n    """"""\r\n    from functools import wraps\r\n\r\n    @wraps(func)\r\n    def wrapper(*args, **kwargs):\r\n        """"""wrapper function""""""\r\n        frmt = ""\\n"".join([""Function... {}"", ""  args.... {}"",\r\n                          ""  kwargs.. {}"", ""  docs.... {}""])\r\n        ar = [func.__name__, args, kwargs, func.__doc__]\r\n        print(dedent(frmt).format(*ar))\r\n        result = func(*args, **kwargs)\r\n        print(""{!r:}\\n"".format(result))  # comment out if results not needed\r\n        return result                    # for optional use outside.\r\n    return wrapper\r\n\r\n\r\n# ----------------------------------------------------------------------------\r\n# ---- (1) doc_func ... code section ... ----\r\ndef doc_func(func=None, verbose=True):\r\n    """"""(doc_func)...Documenting code using inspect\r\n\r\n    Requires:\r\n    ---------\r\n    >>> import inspect  # module\r\n\r\n    Returns\r\n    -------\r\n    A listing of the source code with line numbers\r\n\r\n    Parameters\r\n    ----------\r\n    func : function\r\n        Function name to document, without quotes\r\n    verbose : Boolean\r\n        True prints the result, False returns a string of the result.\r\n\r\n    Notes\r\n    -----\r\n\r\n    Source code for...\r\n    ::\r\n\r\n        module level\r\n        - inspect.getsourcelines(sys.modules[__name__])[0]\r\n\r\n        function level\r\n        - as a list => inspect.getsourcelines(num_41)[0]\r\n        - as a string => inspect.getsource(num_41)\r\n\r\n        file level\r\n        - script = sys.argv[0]\r\n\r\n    """"""\r\n    def demo_func():\r\n        """"""dummy...\r\n        : Demonstrates retrieving and documenting module and function info.\r\n        """"""\r\n        def sub():\r\n            """"""sub in dummy""""""\r\n            pass\r\n        return None\r\n    #\r\n    import inspect\r\n    if func is None:\r\n        func = demo_func\r\n    if not inspect.isfunction(func):\r\n        out = ""\\nError... `{}` is not a function, but is of type... {}\\n""\r\n        print(out.format(func.__name__, type(func)))\r\n        return None\r\n    script = sys.argv[0]  # a useful way to get a file\'s name\r\n    lines, line_num = inspect.getsourcelines(func)\r\n    code = """".join([""{:4d}  {}"".format(idx+line_num, line)\r\n                    for idx, line in enumerate(lines)])\r\n    nmes = [\'args\', \'varargs\', \'varkw\', \'defaults\', \'kwonlyargs\',\r\n            \'kwonlydefaults\', \'annotations\']\r\n    f = inspect.getfullargspec(func)\r\n    f_args = ""\\n"".join([str(i) for i in list(zip(nmes, list(f)))])\r\n    args = [line_num, code,\r\n            inspect.getcomments(func),\r\n            inspect.isfunction(func),\r\n            inspect.ismethod(func),\r\n            inspect.getmodulename(script),\r\n            f_args]\r\n    frmt = """"""\r\n    :----------------------------------------------------------------------\r\n    :---- doc_func(func) ----\r\n    :Code for a function on line...{}...\r\n    :\r\n    {}\r\n    Comments preceeding function\r\n    {}\r\n    function?... {} ... or method? {}\r\n    Module name... {}\r\n    Full specs....\r\n    {}\r\n    ----------------------------------------------------------------------\r\n    """"""\r\n    out = (dedent(frmt)).format(*args)\r\n    if verbose:\r\n        print(out)\r\n    else:\r\n        return out\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# ---- (2) get_func .... code section ----\r\ndef get_func(func, line_nums=True, verbose=True):\r\n    """"""Get function information (ie. for a def)\r\n\r\n    Requires\r\n    --------\r\n    >>> from textwrap import dedent, indent, wrap\r\n    >>> import inspect\r\n\r\n    Returns\r\n    -------\r\n    The function information includes arguments and source code.\r\n    A string is returned for printing.\r\n\r\n    Notes\r\n    -----\r\n    Import the module containing the function and put the object name in\r\n    without quotes...\r\n\r\n    >>> from arraytools.utils import get_func\r\n    >>> get_func(get_func)  # returns this source code etc.\r\n    """"""\r\n    frmt = """"""\r\n    :-----------------------------------------------------------------\r\n    :Function: .... {} ....\r\n    :Line number... {}\r\n    :Docs:\r\n    {}\r\n    :Defaults: {}\r\n    :Keyword Defaults: {}\r\n    :Variable names:\r\n    {}\\n\r\n    :Source code:\r\n    {}\r\n    :\r\n    :-----------------------------------------------------------------\r\n    """"""\r\n    import inspect\r\n    from textwrap import dedent, wrap\r\n\r\n    if not inspect.isfunction(func):\r\n        out = ""\\nError... `{}` is not a function, but is of type... {}\\n""\r\n        print(out.format(func.__name__, type(func)))\r\n        return None\r\n\r\n    lines, ln_num = inspect.getsourcelines(func)\r\n    if line_nums:\r\n        code = """".join([""{:4d}  {}"".format(idx + ln_num, line)\r\n                        for idx, line in enumerate(lines)])\r\n    else:\r\n        code = """".join([""{}"".format(line) for line in lines])\r\n\r\n    vars_ = "", "".join([i for i in func.__code__.co_varnames])\r\n    vars_ = wrap(vars_, 50)\r\n    vars_ = ""\\n"".join([i for i in vars_])\r\n    args = [func.__name__, ln_num, dedent(func.__doc__), func.__defaults__,\r\n            func.__kwdefaults__, indent(vars_, ""    ""), code]\r\n    code_mem = dedent(frmt).format(*args)\r\n    if verbose:\r\n        print(code_mem)\r\n    else:\r\n        return code_mem\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# ---- (3) get_modu .... code section ----\r\ndef get_modu(obj, code=False, verbose=True):\r\n    """"""Get module (script) information, including source code for\r\n    documentation purposes.\r\n\r\n    Requires\r\n    --------\r\n    >>> from textwrap import dedent, indent\r\n    >>> import inspect\r\n\r\n    Returns\r\n    -------\r\n    A string is returned for printing.  It will be the whole module\r\n    so use with caution.\r\n\r\n    Notes\r\n    -----\r\n    Useage::\r\n\r\n    >>> from arraytools.utils import get_modu\r\n    >>> get_modu(tools, code=False, verbose=True)\r\n    >>> # No quotes around module name, code=True for module code\r\n\r\n   """"""\r\n    frmt = """"""\r\n    :-----------------------------------------------------------------\r\n    :Module: .... {} ....\r\n    :------\r\n    :File: ......\r\n    {}\\n\r\n    :Docs: ......\r\n    {}\\n\r\n    :Members: .....\r\n    {}\r\n    """"""\r\n    frmt0 = """"""\r\n    :{}\r\n    :-----------------------------------------------------------------\r\n    """"""\r\n    frmt1 = """"""\r\n    :Source code: .....\r\n    {}\r\n    :\r\n    :-----------------------------------------------------------------\r\n    """"""\r\n    import inspect\r\n    from textwrap import dedent\r\n\r\n    if not inspect.ismodule(obj):\r\n        out = ""\\nError... `{}` is not a module, but is of type... {}\\n""\r\n        print(out.format(obj.__name__, type(obj)))\r\n        return None\r\n    if code:\r\n        lines, _ = inspect.getsourcelines(obj)\r\n        frmt = frmt + frmt1\r\n        code = """".join([""{:4d}  {}"".format(idx, line)\r\n                        for idx, line in enumerate(lines)])\r\n    else:\r\n        lines = code = """"\r\n        frmt = frmt + frmt0\r\n    memb = [i[0] for i in inspect.getmembers(obj)]\r\n    args = [obj.__name__, obj.__file__, obj.__doc__, memb, code]\r\n    mod_mem = dedent(frmt).format(*args)\r\n    if verbose:\r\n        print(mod_mem)\r\n    else:\r\n        return mod_mem\r\n\r\n# ----------------------------------------------------------------------\r\n# ---- (4) dirr .... code section ----\r\ndef dirr(obj, colwise=False, cols=4, sub=None, prn=True):\r\n    """"""A formatted `dir` listing of an object, module, function... anything you\r\n    can get a listing for.\r\n\r\n    Source : arraytools.py_tools has a pure python equivalent\r\n\r\n    Other : arraytools `__init__._info()` has an abbreviated version\r\n\r\n    Parameters\r\n    ----------\r\n    colwise : boolean\r\n        `True` or `1`, otherwise, `False` or `0`\r\n    cols : number\r\n      pick a size to suit\r\n    sub : text\r\n      sub array with wildcards\r\n\r\n    - `arr*` : begin with `arr`\r\n    - `*arr` : endswith `arr` or\r\n    - `*arr*`: contains `arr`\r\n    prn : boolean\r\n      `True` for print or `False` to return output as string\r\n\r\n    Return:\r\n    -------\r\n    A directory listing of a module\'s namespace or a part of it if the\r\n    `sub` option is specified.\r\n\r\n    Notes\r\n    -----\r\n    See the `inspect` module for possible additions like `isfunction`,\r\n    `ismethod`, `ismodule`\r\n\r\n    **Examples**::\r\n\r\n        dirr(art, colwise=True, cols=3, sub=None, prn=True)  # all columnwise\r\n        dirr(art, colwise=True, cols=3, sub=\'arr\', prn=True) # just the `arr`\'s\r\n\r\n          (001)    _arr_common     arr2xyz         arr_json\r\n          (002)    arr_pnts        arr_polygon_fc  arr_polyline_fc\r\n          (003)    array2raster    array_fc\r\n          (004)    array_struct    arrays_cols\r\n    """"""\r\n    err = """"""\r\n    ...No matches found using substring .  `{0}`\r\n    ...check with wildcards, *, ... `*abc*`, `*abc`, `abc*`\r\n    """"""\r\n    d_arr = dir(obj)\r\n    a = np.array(d_arr)\r\n    dt = a.dtype.descr[0][1]\r\n    if sub not in (None, \'\', \' \'):\r\n        start = [0, 1][sub[0] == ""*""]\r\n        end = [0, -1][sub[-1] == ""*""]\r\n        if not start and abs(end):\r\n            a = [i for i in d_arr\r\n                 if i.startswith(sub[start:end], start, len(i))]\r\n        elif start and abs(end):\r\n            a = [i for i in d_arr\r\n                 if sub[1:4] in i[:len(i)]]\r\n        elif abs(end):\r\n            sub = sub.replace(""*"", """")\r\n            a = [i for i in d_arr\r\n                 if i.endswith(sub, start, len(i))]\r\n        else:\r\n            a = []\r\n        if len(a) == 0:\r\n            print(dedent(err).format(sub))\r\n            return None\r\n        num = max([len(i) for i in a])\r\n    else:\r\n        num = int("""".join([i for i in dt if i.isdigit()]))\r\n    frmt = (""{{!s:<{}}} "".format(num)) * cols\r\n    if colwise:\r\n        z = np.array_split(a, cols)\r\n        zl = [len(i) for i in z]\r\n        N = max(zl)\r\n        e = np.empty((N, cols), dtype=z[0].dtype)\r\n        for i in range(cols):\r\n            n = min(N, zl[i])\r\n            e[:n, i] = z[i]\r\n    else:\r\n        csze = len(a) / cols\r\n        rows = int(csze) + (csze % 1 > 0)\r\n        z = np.array_split(a, rows)\r\n        e = np.empty((len(z), cols), dtype=z[0].dtype)\r\n        N = len(z)\r\n        for i in range(N):\r\n            n = min(cols, len(z[i]))\r\n            e[i, :n] = z[i][:n]\r\n    if hasattr(obj, \'__name__\'):\r\n        args = [""-""*70, obj.__name__, obj]\r\n    else:\r\n        args = [""-""*70, type(obj), ""np version""]\r\n    txt_out = ""\\n{}\\n| dir({}) ...\\n|    {}\\n-------"".format(*args)\r\n    cnt = 1\r\n    for i in e:\r\n        txt_out += ""\\n  ({:>03.0f})    {}"".format(cnt, frmt.format(*i))\r\n        cnt += cols\r\n    if prn:\r\n        print(txt_out)\r\n    else:\r\n        return txt_out\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# ---- (5) wrapper .... code section ----\r\ndef wrapper(a, wdth=70):\r\n    """"""Wrap stuff using textwrap.wrap\r\n\r\n    Notes:\r\n    -----\r\n    TextWrapper class\r\n    __init__(self, width=70, initial_indent=\'\', subsequent_indent=\'\',\r\n             expand_tabs=True, replace_whitespace=True,\r\n             fix_sentence_endings=False, break_long_words=True,\r\n             drop_whitespace=True, break_on_hyphens=True, tabsize=8,\r\n             *, max_lines=None, placeholder=\' [...]\')\r\n    """"""\r\n    if isinstance(a, np.ndarray):\r\n        txt = [str(i) for i in a.tolist()]\r\n        txt = "", "".join(txt)\r\n    elif isinstance(a, (list, tuple)):\r\n        txt = "", "".join([str(i) for i in a])\r\n    txt = ""\\n"".join(wrap(txt, width=wdth))\r\n    return txt\r\n\r\n\r\ndef _utils_help_():\r\n    """"""arraytools.utils help...\r\n\r\n    Function list follows:\r\n    """"""\r\n    _hf = """"""\r\n    :-------------------------------------------------------------------:\r\n    : ---- arrtools functions  (loaded as \'art\') ----\r\n    : ---- from utils.py\r\n    (1)  doc_func(func=None)\r\n         documenting code using inspect\r\n    (2)  get_func(obj, line_nums=True, verbose=True)\r\n         pull in function code\r\n    (3)  get_modu(obj)\r\n         pull in module code\r\n    (4)  dirr(a)  object info\r\n    (5)  wrapper(a)  format objects as a string\r\n    :-------------------------------------------------------------------:\r\n    """"""\r\n    print(dedent(_hf))\r\n\r\n# ----------------------------------------------------------------------\r\n# .... final code section producing the featureclass and extendtable\r\n\r\n#\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n    testing = True\r\n    print(\'\\n{} in source script... {}\'.format(__name__, script))\r\n        # parameters here\r\nelse:\r\n    testing = False\r\n    # parameters here\r\n'"
all_scripts/vincenty.py,10,"b'# -*- coding: UTF-8 -*-\n""""""\n:Script:   vincenty.py\n:Author:   Dan.Patterson@carleton.ca\n:Created:  2014-10\n:Modified: 2017-01-03\n:Purpose:\n:  Calculates the Vincenty Inverse distance solution for 2 long/lat pairs\n:Source:\n:  http://www.movable-type.co.uk/scripts/latlong-vincenty.html  java code\n: From:\n:  T Vincenty, 1975 ""Direct and Inverse Solutions of Geodesics on the\n:  Ellipsoid with application of nested equations"", Survey Review,\n:  vol XXIII, no 176, 1975\n:  http://www.ngs.noaa.gov/PUBS_LIB/inverse.pdf\n: Other:\n:  https://github.com/geopy/geopy/blob/master/geopy/distance.py\n:  appears to be implemented in geopy as Vincenty\n:Notes:\n:  atan2(y,x) or atan2(sin, cos) not like Excel\n:  used fmod(x,y) to get the modulous as per python\n:\n: *** link to haversine... see if vincenty can be vectorized in the same way\n:  http://stackoverflow.com/questions/34552284/vectorize-haversine-distance-\n:        computation-along-path-given-by-list-of-coordinates\n:\n:Returns:\n:  distance in meters, initial and final bearings (as an azimuth from N)\n:\n:Examples:\n: long0  lat0  long1  lat1   dist       initial    final  head to\n: -75.0, 45.0, -75.0, 46.0   111141.548   0.000,   0.000   N\n: -75.0, 46.0, -75.0, 45.0   111141.548 180.000, 180.000   S\n: -76.0, 45.0, -75.0, 45.0    78846.334  89.646,  90.353   E\n: -75.0, 45.0, -76.0, 45.0    78846.334 270.353, 269.646   W\n: -76.0, 46.0, -75.0, 45.0   135869.091 144.526, 145.239   SE\n: -75.0, 46.0, -76.0, 45.0   135869.091 215.473, 214.760   SW\n: -76.0, 45.0, -75.0, 46.0   135869.091  34.760,  35.473   NE\n: -75.0, 45.0, -76.0, 46.0   135869.091 325.239, 324.526   NW\n: -90.0,  0.0    0.0   0.0 10018754.171  90.000   90.000   1/4 equator\n: -75.0   0.0  -75.0  90.0 10001965.729   0.000    0.000   to N pole\n:\n:---------------------------------------------------------------------:\n""""""\n# ---- imports, formats, constants ----\n\nimport sys\nimport numpy as np\nimport math\nfrom textwrap import dedent\n\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\n      \'float\': \'{: 0.3f}\'.format}\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2,\n                    suppress=True, threshold=100,\n                    formatter=ft)\nnp.ma.masked_print_option.set_display(\'-\')\n\nscript = sys.argv[0]\n\n# ---- functions ----\n\n\ndef vincenty(long0, lat0, long1, lat1, verbose=False):\n    """"""return the distance on the ellipsoid between two points using\n    Vincenty\'s Inverse method\n\n    `a`, `b` : numbers\n        semi-major and minor axes WGS84 model\n    `f` : number\n        inverse flattening\n    `L`, `dL` : number\n        delta longitude, initial and subsequent\n    `u0`, `u1` : number\n        reduced latitude\n    `s_sig` : number\n        sine sigma\n    `c_sig` : number\n        cosine sigma\n    """"""\n    a = 6378137.0\n    b = 6356752.314245\n    ab_b = (a**2 - b**2)/b**2\n    f = 1.0/298.257223563\n    twoPI = 2*math.pi\n    dL = L = math.radians(long1 - long0)\n    u0 = math.atan((1 - f) * math.tan(math.radians(lat0)))  # reduced latitudes\n    u1 = math.atan((1 - f) * math.tan(math.radians(lat1)))\n    s_u0 = math.sin(u0)\n    c_u0 = math.cos(u0)\n    s_u1 = math.sin(u1)\n    c_u1 = math.cos(u1)\n    # ---- combine repetitive terms ----\n    sc_01 = s_u0*c_u1\n    cs_01 = c_u0*s_u1\n    cc_01 = c_u0*c_u1\n    ss_01 = s_u0*s_u1\n    #\n    lambdaP = float()\n    max_iter = 20\n    # first approximation\n    cnt = 0\n    while (cnt < max_iter):\n        s_dL = math.sin(dL)\n        c_dL = math.cos(dL)\n        s_sig = math.sqrt((c_u1*s_dL)**2 + (cs_01 - sc_01*c_dL)**2)  # eq14\n        if (s_sig == 0):\n            return 0\n        c_sig = ss_01 + cc_01*c_dL                      # eq 15\n        sigma = math.atan2(s_sig, c_sig)                # eq 16\n        s_alpha = cc_01*s_dL/s_sig                      # eq 17\n        c_alpha2 = 1.0 - s_alpha**2\n        if c_alpha2 != 0.0:\n            c_sigM2 = c_sig - 2.0*s_u0*s_u1/c_alpha2    # eq 18\n        else:\n            c_sigM2 = c_sig\n        C = f/16.0 * c_alpha2*(4 + f*(4 - 3*c_alpha2))  # eq 10\n        lambdaP = dL\n        # dL => equation 11\n        dL = L + (1 - C)*f*s_alpha*(sigma +\n                                    C*s_sig*(c_sigM2 +\n                                             C*c_sig*(-1.0 + 2*c_sigM2**2)))\n        #\n        if (cnt == max_iter):          # is it time to bail?\n            return 0.0\n        elif((math.fabs(dL - lambdaP) > 1.0e-12) and (cnt < max_iter)):\n            cnt += 1\n        else:\n            break\n    # ---- end of while ----\n    uSq = c_alpha2 * ab_b\n    A = 1 + uSq/16384.0 * (4096 + uSq*(-768 + uSq*(320 - 175*uSq)))  # eq 3\n    B = uSq/1024.0 * (256 +  uSq*(-128 + uSq*(74 - 47*uSq)))         # eq 4\n    d_sigma = B*s_sig*(c_sigM2 +\n                      (B/4.0)*(c_sig*(-1 + 2*c_sigM2**2) -\n                      (B/6.0)*c_sigM2*(-3 + 4*s_sig**2)*(-3 +\n                      4*c_sigM2**2)))\n    # d_sigma => eq 6\n    dist = b*A*(sigma - d_sigma)                                     # eq 19\n    alpha1 = math.atan2(c_u1*s_dL,  cs_01 - sc_01*c_dL)\n    alpha2 = math.atan2(c_u0*s_dL, -sc_01 + cs_01*c_dL)\n    # normalize to 0...360  degrees\n    alpha1 = math.degrees(math.fmod((alpha1 + twoPI), twoPI))        # eq 20\n    alpha2 = math.degrees(math.fmod((alpha2 + twoPI), twoPI))        # eq 21\n    if verbose:\n        return dist, alpha1, alpha2, cnt\n    return dist, alpha1, alpha2, None\n\n\ndef demo():\n    """""" testing function edit as appropriate """"""\n    coord = [-76.0, 46.0, -75.0, 45.0]  # SE\n    #coord = [-90.0, 0.0, 0, 0.0]       # 1/4 equator\n    #coord = [-75.0, 0.0, -75.0, 90.0]  # to N pole\n    a0, a1, a2, a3 = coord\n    b0, b1, b2, cnt = vincenty(a0, a1, a2, a3, verbose=True)\n    frmt = """"""\n    :--------------------------------------------------------:\n    :Vincenty inverse...\n    :Longitude, Latitude\n    :From: ({:>12.8f}, {:>12.8f})\n    :To:   ({:>12.8f}, {:>12.8f})\n    :Distance: {:>10.3f} m\n    :Bearings...\n    :  Initial {:>8.2f} deg\n    :  Final   {:>8.2f} deg\n    :  Iterations taken.... {}\n    :--------------------------------------------------------:\n    """"""\n    print (dedent(frmt).format(a0, a1, a2, a3, b0, b1, b2, cnt))\n\n\ndef vin():\n    """""" """"""\n    #    long0, lat0, long1, lat1\n    vals = [[90.0,  0.0,  0.0,  0.0],\n            [0.0,  0.0, 0.0, 90.0],\n            [-75.0, 45.0, -75.0, 46.0]]\n    v = np.array(vals)\n    #\n    a = 6378137.0\n    b = 6356752.314245\n    f = 1.0/298.257223563\n    twoPI = 2*math.pi\n    #\n    long0 = v[:, 2]\n    long1 = v[:, 0]\n    lat0 = v[:, 1]\n    lat1 = v[:, 3]\n    dL = L = np.deg2rad(long1 - long0)\n    u0 = np.arctan2((1 - f) * np.tan(np.radians(lat0)))\n    u1 = np.arctan2((1 - f) * np.tan(np.radians(lat1)))\n    s_u0 = np.sin(u0)\n    c_u0 = np.cos(u0)\n    s_u1 = np.sin(u1)\n    c_u1 = np.cos(u1)\n    return long0\n# ---------------------------------------------------------------------\nif __name__ == ""__main__"":\n    """"""Main section...   """"""\n#    #print(""Script... {}"".format(script))\n#     ----- uncomment one of the  below  -------------------\n#    long0 = vin()  # not ready\n    demo()\n'"
arraytools_testing/array_tools_testing.py,5,"b'# -*- coding: utf-8 -*-\r\n""""""\r\narray_tools_testing\r\n===================\r\n\r\nScript :   array_tools_testing.py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified : 2018-09-19\r\n\r\nPurpose:  tools for working with numpy arrays\r\n\r\nUseage :\r\n\r\nReferences\r\n----------\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/numpyarraytotable.htm>`_.\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/tabletonumpyarray.htm>`_.\r\n---------------------------------------------------------------------\r\n""""""\r\n\r\nimport sys\r\nimport numpy as np\r\n#from arraytools.fc_tools._common import fc_info, fld_info\r\nimport arcpy\r\n\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=5, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\ndef id_geom(in_fc, as_pnts=True):\r\n    """"""The main code segment which gets the id and shape information and\r\n    explodes the geometry to individual points\r\n    """"""\r\n    with arcpy.da.SearchCursor(in_fc, [\'OBJECTID\', \'SHAPE@\']) as cursor:\r\n        a = [[row[0], row[1]] for row in cursor]\r\n    return a\r\n\r\n\r\ndef pip_arc(pnt_fc, poly_fc):\r\n    """"""Naive point-in-polygon using arcpy\r\n\r\n    pnt_fc : PointGeometry\r\n        PointGeometry only supports `within` geometry operations\r\n    poly_fc : Polygon\r\n        Polgon geometry\r\n\r\n    Notes:\r\n    ------\r\n    `within` options are `BOUNDARY`, `CLEMENTINI`, `PROPER`.\r\n    - `BOUNDARY` on boundary\r\n    - `CLEMENTINI` default, must be within, not on boundary\r\n    """"""\r\n    pnts = id_geom(pnt_fc)\r\n    polys = id_geom(poly_fc)\r\n    out = []\r\n    for pnt in pnts:\r\n        pid, p = pnt\r\n        for poly in polys:\r\n            plid, pol = poly\r\n            if p.within(pol, ""CLEMENTINI""):  # CLEMENTINI, PROPER\r\n                out.append([pid, plid])  #[pid, p, plid, pol])\r\n                break\r\n#            else:\r\n#                continue\r\n    return out  #pnts, polys, out\r\n\r\n\r\ndef extend_tbl(arr, in_fc=None, join_id=""PntID"", col_id=""PolyID""):\r\n    """"""ExtendTable example\r\n    """"""\r\n    dt = [(join_id, \'<i4\'), (col_id, \'<i4\')]\r\n    z = np.ndarray((len(arr), ), dtype=dt)\r\n    z[join_id] = arr[:, 0]\r\n    z[col_id] = arr[:, 1]\r\n    arcpy.da.ExtendTable(in_fc, ""OBJECTID"", z, join_id)\r\n\r\n\r\ndef pip_demo():\r\n    """"""Point in polygon demo\r\n    """"""\r\n    out = pip_arc(pnt_fc, poly_fc)\r\n    out2 = np.array(out)\r\n    flds = arcpy.ListFields(pnt_fc)\r\n    out_fld = ""PolyID""\r\n    fnames = [i.name for i in flds]\r\n    if out_fld in fnames:\r\n        out_fld += ""1""\r\n    extend_tbl(out2, pnt_fc, ""PntID"", out_fld)  # uncomment to test extend\r\n    print(""output array\\n{}"".format(out2))\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# .... final code section producing the featureclass and extendtable\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    pth = script.split(""/"")[:-1]\r\n    pnt_fc = r""\\array_tools.gdb\\pnts_2000""  # 1994 of 2000 within clementini\r\n    pnt_fc = ""\\\\"".join(pth) + pnt_fc\r\n    poly_fc = r""\\array_tools.gdb\\SamplingGrids""\r\n    poly_fc = ""\\\\"".join(pth) + poly_fc\r\nelse:\r\n    testing = False\r\n    # parameters here\r\n#\r\nif testing:\r\n    print(\'\\nScript source... {}\'.format(script))\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n'"
arraytools_testing/art_common.py,11,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nart_common\r\n===========\r\n\r\nScript :   art_common.py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified : 2018-09-24\r\n\r\nPurpose:  tools for working with numpy arrays\r\n\r\nart_common is a set of functions common to the implementation of array tools\r\nin testing model\r\n\r\nUseage :\r\n\r\nReferences\r\n----------\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/numpyarraytotable.htm>`_.\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/tabletonumpyarray.htm>`_.\r\n---------------------------------------------------------------------\r\n""""""\r\n\r\nimport sys\r\nfrom textwrap import dedent\r\nimport numpy as np\r\nfrom arcpy import AddMessage, ListFields, Raster\r\nfrom arcpy.da import Describe, TableToNumPyArray\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\n__all__ = [\'tweet\',\r\n           \'de_punc\',\r\n           \'_describe\',\r\n           \'fc_info\',\r\n           \'fld_info\',\r\n           \'null_dict\',\r\n           \'tbl_arr\'\r\n           ]\r\n\r\n\r\ndef tweet(msg):\r\n    """"""Print a message for both arcpy and python.\r\n\r\n    msg - a text message\r\n    """"""\r\n    m = ""\\n{}\\n"".format(msg)\r\n    AddMessage(m)\r\n    print(m)\r\n\r\n\r\ndef de_punc(s, punc=None, no_spaces=True, char=\'_\'):\r\n    """"""Remove punctuation and/or spaces in strings and replace with\r\n    underscores or nothing\r\n\r\n    Parameters\r\n    ----------\r\n    s : string\r\n        input string to parse\r\n    punc : string\r\n        A string of characters to replace ie. \'@ ""!\\\'\\\\[]\'\r\n    no_spaces : boolean\r\n        True, replaces spaces with underscore.  False, leaves spaces\r\n    char : string\r\n        Replacement character\r\n    """"""\r\n    if (punc is None) or not isinstance(punc, str):\r\n        punc = \'!""#$%&\\\'()*+,-./:;<=>?@[\\\\]^`{|}~\'  # _ removed\r\n    if no_spaces:\r\n        punc = "" "" + punc\r\n    s = """".join([[i, char][i in punc] for i in s])\r\n    return s\r\n\r\ndef _describe(in_fc):\r\n    """"""Simply return the arcpy.da.Describe object.\r\n\r\n    **desc.keys()** an abbreviated list::\r\n\r\n    \'OIDFieldName\'... \'areaFieldName\', \'baseName\'... \'catalogPath\',\r\n    \'dataType\'... \'extent\', \'featureType\', \'fields\', \'file\'... \'hasM\',\r\n    \'hasOID\', \'hasZ\', \'indexes\'... \'lengthFieldName\'... \'name\', \'path\',\r\n    \'rasterFieldName\', ..., \'shapeFieldName\', \'shapeType\',\r\n    \'spatialReference\'\r\n\r\n    """"""\r\n    return Describe(in_fc)\r\n\r\n\r\ndef fc_info(in_fc, prn=False):\r\n    """"""Return basic featureclass information, including the following...\r\n\r\n    Returns:\r\n    --------\r\n    shp_fld  :\r\n        field name which contains the geometry object\r\n    oid_fld  :\r\n        the object index/id field name\r\n    SR       :\r\n        spatial reference object (use SR.name to get the name)\r\n    shp_type :\r\n        shape type (Point, Polyline, Polygon, Multipoint, Multipatch)\r\n\r\n    Notes:\r\n    ------\r\n    Other useful parameters :\r\n        \'areaFieldName\', \'baseName\', \'catalogPath\',\'featureType\',\r\n        \'fields\', \'hasOID\', \'hasM\', \'hasZ\', \'path\'\r\n\r\n    Derive all field names :\r\n        all_flds = [i.name for i in desc[\'fields\']]\r\n    """"""\r\n    desc = _describe(in_fc)\r\n    args = [\'shapeFieldName\', \'OIDFieldName\', \'shapeType\', \'spatialReference\']\r\n    shp_fld, oid_fld, shp_type, SR = [desc[i] for i in args]\r\n    if prn:\r\n        frmt = ""FeatureClass:\\n   {}"".format(in_fc)\r\n        f = ""\\n{!s:<16}{!s:<14}{!s:<10}{!s:<10}""\r\n        frmt += f.format(*args)\r\n        frmt += f.format(shp_fld, oid_fld, shp_type, SR.name)\r\n        tweet(frmt)\r\n        return None\r\n    return shp_fld, oid_fld, shp_type, SR\r\n\r\n\r\ndef fld_info(in_fc, prn=False):\r\n    """"""Field information for a featureclass (in_fc).\r\n\r\n    Parameters:\r\n    -----------\r\n    prn : boolean\r\n        True - returns the values\r\n\r\n        False - simply prints the results\r\n\r\n    Field properties:\r\n    -----------------\r\n    \'aliasName\', \'baseName\', \'defaultValue\', \'domain\', \'editable\',\r\n    \'isNullable\', \'length\', \'name\', \'precision\', \'required\', \'scale\', \'type\'\r\n    """"""\r\n    flds = ListFields(in_fc)\r\n    f_info = [(i.name, i.type, i.length, i.isNullable, i.required)\r\n              for i in flds]\r\n    f = ""{!s:<14}{!s:<12}{!s:>7} {!s:<10}{!s:<10}""\r\n    if prn:\r\n        frmt = ""FeatureClass:\\n   {}\\n"".format(in_fc)\r\n        args = [""Name"", ""Type"", ""Length"", ""Nullable"", ""Required""]\r\n        frmt += f.format(*args) + ""\\n""\r\n        frmt += ""\\n"".join([f.format(*i) for i in f_info])\r\n        tweet(frmt)\r\n        return None\r\n    return f_info\r\n\r\n\r\ndef null_dict(flds):\r\n    """"""Produce a null dictionary from a list of fields\r\n    These must be field objects and not just their name.\r\n    """"""\r\n    dump_flds = [""OBJECTID"",""Shape_Length"", ""Shape_Area"", ""Shape""]\r\n    flds_oth = [f for f in flds\r\n                if f.name not in dump_flds]\r\n#    oid_geom = [\'OBJECTID\', \'SHAPE@X\', \'SHAPE@Y\']\r\n    nulls = {\'Double\':np.nan,\r\n             \'Single\':np.nan,\r\n             \'Short\':np.iinfo(np.int16).min,\r\n             \'SmallInteger\':np.iinfo(np.int16).min,\r\n             \'Long\':np.iinfo(np.int32).min,\r\n             \'Float\':np.nan,\r\n             \'Integer\':np.iinfo(np.int32).min,\r\n             \'String\':str(None),\r\n             \'Text\':str(None)}\r\n    fld_dict = {i.name: i.type for i in flds_oth}\r\n    nulls = {f.name:nulls[fld_dict[f.name]] for f in flds_oth}\r\n    return nulls\r\n\r\n\r\ndef tbl_arr(pth):\r\n    """"""Convert featureclass/table to a structured ndarray\r\n\r\n    Requires\r\n    --------\r\n    pth : string\r\n        path to input featureclass or table\r\n\r\n    """"""\r\n    flds = ListFields(pth)\r\n    nulls = null_dict(flds)\r\n    bad = [\'OID\', \'Geometry\', \'Shape_Length\', \'Shape_Area\']\r\n    f0 = [""OID@""]\r\n    f1 = [i.name for i in flds if i.type not in bad]\r\n    flds = f0 + f1\r\n    a = TableToNumPyArray(pth,\r\n                          field_names=flds,\r\n                          skip_nulls=False,\r\n                          null_value=nulls)\r\n    dt = np.array(a.dtype.descr)\r\n    nmes = dt[:, 0]\r\n    sze = dt[:, 1]\r\n    cleaned = []\r\n    for i in nmes:\r\n        i = de_punc(i)  # run de_punc to remove punctuation\r\n        cleaned.append(i)\r\n    a.dtype = list(zip(cleaned, sze))\r\n    return a\r\n\r\n# ---- raster section ----\r\n#\r\ndef rasterfile_info(fname, prn=False):\r\n    """"""Obtain raster stack information from the filename of an image\r\n    :\r\n    """"""\r\n    #\r\n    frmt = """"""\r\n    File path   - {}\r\n    Name        - {}\r\n    Spatial Ref - {}\r\n    Raster type - {}\r\n    Integer?    - {}\r\n    NoData      - {}\r\n    Min         - {}\r\n    Max         - {}\r\n    Mean        - {}\r\n    Std dev     - {}\r\n    Bands       - {}\r\n    Cell        - h {}   w {}\r\n    Lower Left  - X {}   Y {}\r\n    Upper Left  - X {}   Y {}\r\n    Extent      - h {}   w {}\r\n    """"""\r\n    desc = Describe(fname)\r\n    r_data_type = desc.datasetType  # \'RasterDataset\'\r\n    args = []\r\n    if r_data_type == \'RasterDataset\':\r\n        r = Raster(fname)\r\n        r.catalogPath            # full path name and file name\r\n        pth = r.path             # path only\r\n        name = r.name            # file name\r\n        SR = r.spatialReference\r\n        r_type = r.format        # \'TIFF\'\r\n        #\r\n        is_int = r.isInteger\r\n        nodata = r.noDataValue\r\n        r_max = r.maximum\r\n        r_min = r.minimum\r\n        r_mean = ""N/A""\r\n        r_std = ""N/A""\r\n        if not is_int:\r\n            r_mean = r.mean\r\n            r_std = r.standardDeviation\r\n        bands = r.bandCount\r\n        cell_hght = r.meanCellHeight\r\n        cell_wdth = r.meanCellWidth\r\n        extent = desc.Extent\r\n        LL = extent.lowerLeft  # Point (X, Y, #, #)\r\n        hght = r.height\r\n        wdth = r.width\r\n        UL = r.extent.upperLeft\r\n        args = [pth, name, SR.name, r_type, is_int, nodata, r_min, r_max,\r\n                r_mean, r_std, bands, cell_hght, cell_wdth, LL.X, LL.Y,\r\n                UL.X, UL.Y, hght, wdth]\r\n    if prn:\r\n        tweet(dedent(frmt).format(*args))\r\n    else:\r\n        return args\r\n\r\n# ----------------------------------------------------------------------\r\n# .... final code section producing the featureclass and extendtable\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    # parameters here\r\nelse:\r\n    testing = False\r\n    # parameters here\r\n#\r\nif testing:\r\n    print(\'\\nScript source... {}\'.format(script))\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n'"
arraytools_testing/excel2tbl.py,15,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nscript name\r\n===========\r\n\r\nScript :   ......py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified : 2018-06-04\r\n\r\nPurpose:  tools for working with numpy arrays\r\n\r\nUseage :\r\n\r\nReferences\r\n----------\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/numpyarraytotable.htm>`_.\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/tabletonumpyarray.htm>`_.\r\n---------------------------------------------------------------------\r\n""""""\r\n\r\nimport sys\r\nimport numpy as np\r\nimport xlrd\r\nimport arcpy.da\r\nfrom arcpy import env\r\n\r\nenv.overwriteOutput = True\r\n#from arcpytools import fc_info, tweet  #, frmt_rec, _col_format\r\n#import arcpy\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n# ----------------------------------------------------------------------\r\n# (10) excel_np\r\ndef excel_np(path, sheet_num=0, int_null=-999):\r\n    """"""Read excel files to numpy structured/record arrays.  Your spreadsheet\r\n    must adhere to simple rules::\r\n      - first row must contain the field names for the output array\r\n      - no blank rows or columns, basically, no fluff or formatting\r\n      - if you have nodata values, put them in, since blank cells will be\r\n        \'corrected\' as best as possible.\r\n      - text and numbers in a column, results in a text column\r\n\r\n    See arraytools.a_io for excel_np for complete description\r\n    """"""\r\n    def isfloat(a):\r\n        """"""float check""""""\r\n        try:\r\n            i = float(a)\r\n            return i\r\n        except ValueError:\r\n            return np.nan\r\n\r\n    def punc_space(name):\r\n        """"""delete punctuation and spaces and replace with \'_\'""""""\r\n        punc = list(\'!""#$%&\\\'()*+,-./:;<=>?@[\\\\]^`{|}~ \')\r\n        return """".join([[i, \'_\'][i in punc] for i in name])\r\n\r\n    # import xlrd\r\n    w = xlrd.open_workbook(path)        # xlrd.book.Book class\r\n    sheets = len(w.sheets())\r\n    if sheet_num > sheets:\r\n        return None\r\n    sheet = w.sheet_by_index(sheet_num) # sheet by number\r\n    # sheet = w.sheet_by_name(\'test\')   # case sensitive, not implemented\r\n    names = sheet.row_values(0)         # clean these up later\r\n    cols = sheet.ncols\r\n    rows = sheet.nrows\r\n    col_data = [sheet.col_values(i, 1, rows) for i in range(cols)]\r\n    row_guess = sheet.row_values(1)\r\n    row_dts = [np.asarray(i).dtype.kind for i in row_guess]\r\n    col_dts = [np.asarray(col_data[i]).dtype.kind\r\n               for i in range(cols)]\r\n    clean = []\r\n    for i in range(len(row_dts)):\r\n        c = col_data[i]\r\n        if row_dts[i] == col_dts[i]:    # same dtype... send to array\r\n            ar = np.asarray(c)\r\n        if row_dts[i] == \'f\':           # float? if so, substitute np.nan\r\n            ar = np.array([isfloat(i) for i in c])\r\n            is_nan = np.isnan(ar)       # find the nan values, then check\r\n            not_nan = ar[~is_nan]       # are the floats == ints?\r\n            if np.all(np.equal(not_nan, not_nan.astype(\'int\'))):  # integer?\r\n                ar[is_nan] = int_null   # assign the integer null\r\n                ar = ar.astype(\'int\')\r\n        elif row_dts[i] in (\'U\', \'S\'):  # unicode/string... send to array\r\n            ar = np.char.strip(ar)\r\n            ar = np.where(np.char.str_len(ar) == 0, \'None\', ar)\r\n        else:\r\n            ar = np.asarray(c)\r\n        clean.append(ar)\r\n    # ---- assemble the columns for the array ----\r\n    dt_str = [i.dtype.str for i in clean]\r\n    names = [i.strip() for i in names]      # clean up leading/trailing spaces\r\n    names = [punc_space(i) for i in names]  # replace punctuation and spaces\r\n    dts_name = list(zip(names, dt_str))\r\n    arr = np.empty((rows-1,), dtype= dts_name)\r\n    cnt = 0\r\n    for i in names:\r\n        arr[i] = clean[cnt]\r\n        cnt +=1\r\n    return arr\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# .... final code section producing the featureclass and extendtable\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    in_excel = script.rpartition(""/"")[0] + ""/Data/test.xlsx""\r\n    sheet_num = 0\r\n    int_null = -999\r\n    arr = excel_np(in_excel, sheet_num=sheet_num , int_null=int_null)\r\n    print(""Array returned...\\n{}"".format(arr))\r\n    # parameters here\r\nelse:\r\n    testing = False\r\n    in_excel = sys.argv[1]\r\n    sheet_num = int(sys.argv[2])\r\n    int_null = sys.argv[3]\r\n    if int_null in (\'-2147483648\', \'-32768\', \'-128\', \'-9\', \'-1\'):\r\n        int_null == int(int_null)\r\n    else:\r\n        int_null = \'-2147483648\'\r\n    out_tbl = sys.argv[4]\r\n    arr = excel_np(in_excel, sheet_num=sheet_num , int_null=int_null)\r\n    if arr is None:\r\n        print(""not a sheet number"")\r\n    else:\r\n        arcpy.da.NumPyArrayToTable(arr, out_tbl)\r\n\r\n    # parameters here\r\n#\r\nif testing:\r\n    print(\'\\nScript source... {}\'.format(script))\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n'"
arraytools_testing/np2tbl.py,5,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\nnp2tbl\r\n======\r\n\r\nScript  : np2tbl.py\r\n\r\nAuthor  :   Dan.Patterson@carleton.ca\r\n\r\nModified: 2018-09-23\r\n\r\nPurpose:  tools for working with numpy arrays\r\n\r\nReferences:\r\n----------\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/numpyarraytotable.htm>`_.\r\n\r\nDerived from python snippet output...\r\n\r\n>>> in_arr = \'C:/Temp/x.npy\'\r\n>>> out_gdb = \'C:/GIS/Tools_scripts/Table_tools/Table_tools.gdb\'\r\n>>> out_name = \'sample_1000_npy\'\r\n>>> arcpy.Tabletools.NumPyArrayToTable(in_arr, out_gdb, out_name)\r\n\r\n---------------------------------------------------------------------\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nfrom textwrap import dedent\r\nimport numpy as np\r\nfrom arcpy import (AddMessage, ListTables, ValidateTableName,\r\n                   MakeTableView_management)\r\nfrom arcpy.da import NumPyArrayToTable\r\nfrom arcpy.geoprocessing import env\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=5, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\ndef tweet(msg):\r\n    """"""Print a message for both arcpy and python.\r\n    : msg - a text message\r\n    """"""\r\n    m = ""\\n{}\\n"".format(msg)\r\n    AddMessage(m)\r\n    print(m)\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# .... final code section producing the featureclass and extendtable\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    pth = script.split(""/"")[:-1]\r\n    pth = ""/"".join(pth) + ""/Data/sample_20.npy""\r\n    a = np.load(pth)\r\n    frmt = ""Result...\\n{}""\r\n    print(frmt.format(a))\r\nelse:\r\n    testing = False\r\n    in_arr = sys.argv[1]\r\n    out_name = sys.argv[2]\r\n    out_gdb = sys.argv[3]\r\n    make_tbl_view = sys.argv[4]\r\n    env.workspace = out_gdb\r\n    tbls = ListTables()\r\n    out_name = ValidateTableName(out_name)\r\n    if tbls is not None:\r\n        if out_name in tbls:\r\n            out_name += \'_dup\'\r\n    out_tbl = out_gdb + ""/"" + out_name\r\n    # ---- call section for processing function\r\n    #\r\n    a = np.load(in_arr)\r\n    NumPyArrayToTable(a, out_tbl)  # create the table\r\n    if make_tbl_view in (True, \'True\', 1):\r\n        MakeTableView_management(out_tbl, out_name)\r\n    args = [in_arr, out_gdb, out_name]\r\n    msg = """"""\r\n    :------------------------------------------------------------\r\n\r\n    Input array... {}\r\n    Output gdb.... {}\r\n    Output name... {}\r\n\r\n    Conversion complete...\r\n    Add the table manually if you want to see it...\r\n\r\n    You need to refresh the geodatabase first since there is no\r\n    autorefresh\r\n\r\n    :------------------------------------------------------------\r\n    """"""\r\n    msg = dedent(msg).format(*args)\r\n    tweet(msg)\r\n\r\n#\r\nif not testing:\r\n    tweet(\'\\nConversion done...\')\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n'"
arraytools_testing/npy2ras.py,7,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nnpy2ras\r\n=======\r\n\r\nScript :   npy2ras.py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified : 2018-09-26\r\n\r\nPurpose :  tools for working with numpy arrays\r\n\r\nReferences\r\n----------\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/functions/numpyarraytoraster\r\n-function.htm>`_.\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/numpyarraytotable.htm>`_.\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/tabletonumpyarray.htm>`_.\r\n---------------------------------------------------------------------\r\n""""""\r\n\r\nimport sys\r\nimport numpy as np\r\nfrom arcpy import Point\r\nfrom arcgisscripting import NumPyArrayToRaster\r\nfrom arcpy import env\r\n\r\nenv.overwriteOutput = True\r\n\r\n#from arcpytools import fc_info, tweet  #, frmt_rec, _col_format\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n# ----------------------------------------------------------------------\r\n# .... final code section producing the featureclass and extendtable\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    pth = script.split(""/"")[:-1]\r\n    pth0 = ""/"".join(pth) + ""/Data/r00.npy""\r\n    LL_x = 300000.\r\n    LL_y = 5030000.\r\n    cell_sze =10.\r\n    no_data = 255\r\n    in_arr = np.load(pth0)\r\n    pth1 = ""/"".join(pth) + ""/Data/r01.tif""\r\n    # parameters here\r\nelse:\r\n    testing = False\r\n    pth0 = sys.argv[1]\r\n    LL_x = float(sys.argv[2])\r\n    LL_y = float(sys.argv[3])\r\n    cell_sze = float(sys.argv[4])\r\n    pth1 = sys.argv[5]\r\n    if pth1[-4:] != "".tif"":\r\n        pth1 += "".tif""\r\n    in_arr = np.load(pth0)\r\n    # parameters here\r\n#\r\nto_pro = True  # ---- change to True to produce tif for ArcGIS PRO\r\n\r\ndt_kind = in_arr.dtype.kind\r\nif dt_kind in (\'u\', \'i\'):\r\n    no_data = np.iinfo(in_arr.dtype.str).max\r\nelif dt_kind in (\'f\'):\r\n    no_data = np.iinfo(in_arr.dtype.str).max\r\nelse:\r\n    no_data = None\r\nif to_pro:\r\n    ras = NumPyArrayToRaster(in_arr,\r\n                             lower_left_corner=Point(LL_x, LL_y),\r\n                             x_cell_size=cell_sze,\r\n                             value_to_nodata=no_data\r\n                             )\r\n    ras.save(pth1)\r\n\r\nif testing:\r\n    print(\'\\nScript source... {}\'.format(script))\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n'"
arraytools_testing/ras2npy.py,4,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nras2npy.py\r\n==========\r\n\r\nScript :   ras2npy.py  # raster to numpy array as *.npy file\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified : 2018-09-25\r\n\r\nPurpose:  tools for working with numpy arrays\r\n\r\nUseage :\r\n\r\nReferences\r\n----------\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/functions/rastertonumpyarray\r\n-function.htm>`_.\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/numpyarraytotable.htm>`_.\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/tabletonumpyarray.htm>`_.\r\n---------------------------------------------------------------------\r\n""""""\r\n\r\nimport sys\r\nimport os\r\nfrom textwrap import dedent\r\nimport numpy as np\r\nfrom art_common import (tweet, rasterfile_info)\r\nfrom arcpy import Point, Raster, RasterToNumPyArray\r\nfrom arcpy import env\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=5, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\nenv.overwriteOutput = True\r\n\r\n# ----------------------------------------------------------------------\r\n# .... final code section producing the featureclass and extendtable\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    pth = script.split(""/"")[:-1]\r\n    pth0 = ""/"".join(pth) + ""/Data/r00.tif""\r\n    r = Raster(pth0)\r\n    out_arr = ""/"".join(pth) + ""/Data/r01.npy""\r\n    frmt = ""Result...\\n{}""\r\n#    print(frmt.format(a))\r\nelse:\r\n    testing = False\r\n    pth = sys.argv[1]\r\n    out_arr = sys.argv[2]\r\n    r = Raster(pth)\r\n# parameters here\r\nLL = r.extent.lowerLeft\r\ncols = int(r.extent.width/r.meanCellWidth)\r\nrows = int(r.extent.height/r.meanCellWidth)\r\na = RasterToNumPyArray(r,\r\n                       lower_left_corner=Point(LL.X, LL.Y),\r\n                       ncols=cols,\r\n                       nrows=rows,\r\n                       nodata_to_value=r.noDataValue\r\n                       )\r\n#\r\n# ---- overwrite existing outputs\r\nif os.path.isfile(out_arr):\r\n    tweet(""\\nRemoving ... {}\\nbefore saving"".format(out_arr))\r\n    os.remove(out_arr)\r\nnp.save(out_arr, a)\r\nif testing:\r\n    tweet(\'\\nScript source... {}\'.format(script))\r\nprint(\'\\nCleaning up\')\r\ndel r, tweet, rasterfile_info, Point, Raster, RasterToNumPyArray\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n'"
arraytools_testing/tbl2np.py,6,"b'# -*- coding: utf-8 -*-\r\n""""""\r\ntbl2np\r\n======\r\n\r\nScript :   tbl2np.py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified : 2018-09-24\r\n\r\nPurpose:  tools for working with numpy arrays\r\n\r\nUseage :\r\n\r\nReferences\r\n----------\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/numpyarraytotable.htm>`_.\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/tabletonumpyarray.htm>`_.\r\n---------------------------------------------------------------------\r\n""""""\r\n\r\nimport sys\r\nfrom textwrap import dedent\r\nimport numpy as np\r\nfrom art_common import (tweet, de_punc, _describe, fc_info, fld_info,\r\n                        null_dict, tbl_arr)\r\nfrom arcpy.da import Describe\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=5, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n# ----------------------------------------------------------------------\r\n# .... final code section producing the featureclass and extendtable\r\n\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    pth = script.split(""/"")[:-1]\r\n    pth = ""/"".join(pth) + ""/array_tools.gdb/pnts_2000""\r\n    a = tbl_arr(pth)\r\n    frmt = ""Result...\\n{}""\r\n    print(frmt.format(a))\r\nelse:\r\n    testing = False\r\n    in_tbl = sys.argv[1]\r\n    desc = Describe(in_tbl)\r\n    pth = desc[\'catalogPath\']\r\n#    out_folder = sys.argv[2]\r\n#    out_name = sys.argv[3]\r\n    out_arr = sys.argv[2]  # + ""/"" + out_name\r\n    # ---- call section for processing function\r\n    #\r\n    a = tbl_arr(pth)\r\n    np.save(out_arr, a)\r\n    args = [a, out_arr]\r\n    msg = """"""\r\n    :------------------------------------------------------------\r\n\r\n    Input table... {}\r\n    Output array.... {}\r\n\r\n    Conversion complete...\r\n    You can reload the array using np.load(drive:/path/name.py)\r\n\r\n    :------------------------------------------------------------\r\n    """"""\r\n    msg = dedent(msg).format(*args)\r\n    tweet(msg)\r\nif testing:\r\n    print(\'\\nScript source... {}\'.format(script))\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n'"
field_calculator/Field_Calculator_defs_2017_07_03.py,0,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   Field_Calculator_defs.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Created:  2013-??-??\r\n:Modified: 2016-09-17\r\n:Purpose: demonstration functions that can be used in the field calculator\r\n:Useage:\r\n:  copy the function into the code block and put in the suggested format\r\n:  into the expression box...modify that to reflect your field names and/or\r\n:  required input values\r\n:Tips:\r\n:  some people use the __esri_field_calculator_splitter__ within their scripts\r\n:  to separate the def from the expression box.\r\n:\r\n""""""\r\nimport sys\r\nimport arcpy\r\n\r\n# Simple no code block examples\r\n""""""\r\n- simple comparison and reclass\r\n  "" True condition "" if !some_field! == ""some_value"" else "" False condition ""\r\n  ""Agricultural 10 Acres"" if !Zone! == ""A-10"" else ""False""\r\n\r\n- compound comparisons and reclass\r\n  ""True"" if !some_field! < ""some_value""\r\n         else (""other value"" if !some_field! == ""final_value"" else ""False"")\r\n  ""True"" if !Random! < 5 else (""sort of"" if !Random! == 5 else ""False"")\r\n\r\n- geometry related\r\n  5000m along polygons and polylines\r\n  x = !Shape!.boundary().positionAlongLine(5000,False).centroid.X  # polygons\r\n  y = !Shape!.boundary().positionAlongLine(5000,False).centroid.Y  # polygons\r\n\r\n  x = !Shape!.positionAlongLine(5000,False).centroid.X  #for polylines\r\n  y = !Shape!.positionAlongLine(5000,False).centroid.Y  #for polylines\r\n\r\n  x = !Shape!.positionAlongLine(0.25,True).centroid.X  # polylines\r\n  y = !Shape!.positionAlongLine(0.25,True).centroid.Y  # polylines\r\n  pnt_num = !Shape!.pointCount - !Shape!.partCount\r\n          # number of vertices in a polygon\r\n\r\n""""""\r\n# Imports  normally they are handled within the def to remind people\r\n\r\nprint(""\\nScript location...since I always forget... {}"".format(sys.argv[0]))\r\n\r\n# ----- Code block section -----\r\n\r\n# (1) no_nulls_allowed([!a!, !b!, !c!...])\r\n\r\ndef no_nulls_allowed(fld_list):\r\n    """"""provide a list of fields""""""\r\n    good_stuff = []\r\n    for i in fld_list:\r\n        if i:\r\n            good_stuff.append(str(i))\r\n        out_str = "" "".join(good_stuff)\r\n    return out_str\r\n\r\n\r\ndef no_nulls_mini(fld_list):\r\n    ok_flds = [str(i) for i in fld_list if i]\r\n    return (""{} ""*len(ok_flds)).format(*ok_flds)\r\n\r\n\r\n# (2) Conversions\r\n\r\n""""""field name, extracts numbers from the beginning of a string until\r\na non-numeric entry is reached, otherwise None\r\nextract_int(field_name)   extract the integer from the front of a string\r\n""""""\r\n\r\nimport itertools\r\ndef extract_int(field_name):\r\n    try:\r\n        return int("""".join(itertools.takewhile(str.isdigit, str(field_name))))\r\n    except:\r\n        pass\r\n\r\n\r\n""""""-----------------------------------------\r\nfield name, extracts integers from a string\r\nextract_nums(field_name)   extracts all numbers from a string\r\n""""""\r\ndef extract_nums(field_name):\r\n    numbers = []\r\n    for i in field_name:\r\n        try:\r\n            val = int(i)\r\n            numbers.append(str(val))\r\n        except ValueError:\r\n            break\r\n    return int("""".join(numbers))\r\n\r\n\r\n""""""-----------------------------------------\r\nfield name, strips numbers from a string\r\nstrip_num(field_name)   strips all numbers from a string\r\n""""""\r\n####add string data digits, letters, printable punctuation etc\r\nimport string\r\ndef strip_num(field_name):\r\n    for i in string.digits:\r\n        field_name = field_name.replace(i,"""")\r\n    return field_name\r\n\r\n\r\n# (3) Comparisons\r\n\r\n""""""-----------------------------------------\r\nfield name, threshold value in the code block, modify expression to suite,\r\nif_elif_else(!test_fld!,5)    modify the code within the return section\r\n""""""\r\ndef if_elif_else(field_name, value):\r\n    if field_name <= value:\r\n        return value     # ie (1.0 + (field_name/2.0))\r\n    elif field_name <= 10:\r\n        return value     # ie (1.5 + 2*((field-1)/24.0))\r\n    else:\r\n        return value     # ie (max(5.0, 3.5+((field-25)/15.0)))\r\n\r\n\r\n""""""-----------------------------------------\r\nfield name, threshold value in the code block, modify expression to suite,\r\nreturns 0, 1\r\ngreaterThan( !Y_UTM!, 5000000)  copy to the expression box and modify to suit\r\n""""""\r\ndef greaterThan(field_name,value):\r\n    if field_name >= value:\r\n        return True\r\n    else:\r\n        return False\r\n\r\n\r\n""""""-----------------------------------------\r\nfield name, threshold value in the code block, modify expression to suite,\r\nreturns 0, 1\r\nlessThan( !Y_UTM!, 500000)  copy to the expression box and modify to suit\r\n""""""\r\ndef lessThan(field_name,value):\r\n    if field_name <= value:\r\n        return True\r\n    else:  return False\r\n\r\n#Others\r\n""""""return a text value for nulls in a text field\r\nreplaceNull(!some_field!,""\'TextValue\')   """"""\r\ndef replaceNull(field_name,value):\r\n    if field_name is None:\r\n        return value\r\n    else:\r\n        return field_name\r\n\r\n\r\n""""""-----------------------------------------\r\nreturn a text value with a word removed by index number\r\nremove_part(!textFld!,0,"" "")\r\n""""""\r\ndef remove_part(field_name,index=-1,sep="" ""):\r\n    a_list = field_name.split(sep)\r\n    out_str = """"\r\n    if abs(index) <= len(a_list):\r\n        a_list.pop(index)\r\n        for i in a_list:\r\n            out_str += (i + "" "")\r\n    return out_str.rstrip()\r\n\r\n\r\n""""""------------------------------------------\r\nLabel grid cells like excel\r\n""""""\r\nimport string\r\nc = -1\r\nr = 0\r\n\r\ndef code_grid(rows=1, cols=1):\r\n    global c, r\r\n    c += 1\r\n    UC = list(string.ascii_uppercase)\r\n    if c >= cols:\r\n        c = 0\r\n        r += 1\r\n    label = UC[c] + str(r)\r\n    return label\r\n\r\n\r\n# (4) Date-time\r\n\r\n# arcpy.time.ParseDateTimeString(!FIELD1!) + datetime.timedelta(hours=1)\r\n# arcpy.time.ParseDateTimeString(!FIELD1!) + datetime.timedelta(days=1)\r\n\r\nimport datetime\r\n\r\n#datetime.datetime.now() + datetime.timedelta(days=1)\r\n\r\n#Math related\r\n\r\n"""""" -----------------------------------------\r\nReturn a random number in the range 0-1\r\nrandomNum()  #enter into the expression box\r\n""""""\r\nimport numpy\r\ndef randomNum():\r\n    return numpy.random.random()\r\n\r\n\r\n""""""-----------------------------------------\r\nReturn a random number integer in the range start, end\r\nrandomInt(0,10)   #enter into the expression box\r\n""""""\r\nimport numpy\r\ndef randomInt(start, end):\r\n    return numpy.random.randint(start, end)\r\n\r\n\r\n""""""-----------------------------------------\r\nReturn the cumulative sum of a field\r\ncumulative(field_name)  # enter into the expression box\r\n""""""\r\nold = 0   # include this line\r\ndef cumulative(new):\r\n    \'\'\'accumulate values\'\'\'\r\n    global old\r\n    if old >= 0:\r\n        old = old + new\r\n    else:\r\n        old = new\r\n    return old\r\n\r\n#  or\r\n\r\ntotal = 0\r\ndef cumsum(in_field):\r\n    global total\r\n    total += in_field\r\n    return total\r\n\r\n""""""-----------------------------------------\r\ngeometric mean   not done yet\r\n""""""\r\nimport operator\r\ndef geometric_mean(iterable):\r\n    return (reduce(operator.mul, iterable)) ** (1.0/len(iterable))\r\n\r\n#Geometry related\r\n\r\n#--Counts\r\n""""""-----------------------------------------\r\nInput shape field, return number of parts\r\ncount_parts(!Shape!)    #enter into the expression box\r\n""""""\r\ndef count_parts(shape):\r\n    return shape.partCount\r\n\r\n\r\n""""""-----------------------------------------\r\nInput shape field, return number of points in a feature\r\ncount_pnts(!Shape!)     #enter into the expression box\r\n""""""\r\ndef count_pnts(shape):\r\n    counter = 0\r\n    num_parts = shape.partCount\r\n    num_pnts = 0\r\n    while counter < num_parts:\r\n        part = shape.getPart(counter)\r\n        pnt = part.next()\r\n        while pnt:\r\n            num_pnts += 1\r\n            pnt = part.next()\r\n            if not pnt:\r\n                pnt = part.next()\r\n        counter += 1\r\n    return num_pnts\r\n\r\n\r\n# -- Point features ----------------------------------------\r\n#  References\r\n#     https://geonet.esri.com/message/557195#557195\r\n#     https://geonet.esri.com/message/557196#557196\r\n\r\n""""""-----------------------------------------\r\ndist_to(shape, from_x, from_y)\r\ninput:      shape field, origin x,y\r\nreturns:    distance to the specified point\r\nexpression: dist_to(!Shape!, x, y)\r\n""""""\r\n\r\ndef dist_to(shape, from_x, from_y):\r\n    x = shape.centroid.X\r\n    y = shape.centroid.Y\r\n    distance = math.sqrt((x - from_x)**2 + (y - from_y)**2)\r\n    return distance\r\n\r\n\r\n"""""" -----------------------------------------\r\ndist_between(shape)\r\ninput:      shape field\r\nreturns:    distance between successive points\r\nexpression: dist_between(!Shape!)\r\n""""""\r\n\r\nx0 = 0.0\r\ny0 = 0.0\r\n\r\ndef dist_between(shape):\r\n    global x0\r\n    global y0\r\n    x = shape.centroid.X\r\n    y = shape.centroid.Y\r\n    if x0 == 0.0 and y0 == 0.0:\r\n        x0 = x\r\n        y0 = y\r\n    distance = math.sqrt((x - x0)**2 + (y - y0)**2)\r\n    x0 = x\r\n    y0 = y\r\n    return distance\r\n\r\n\r\n""""""-----------------------------------------\r\ndist_cumu(shape)\r\ninput:      shape field\r\nreturns:    cumulative distance between points\r\nexpression: dist_cumu(!Shape!)\r\n""""""\r\n\r\nx0 = 0.0\r\ny0 = 0.0\r\ndistance = 0.0\r\ndef dist_cumu(shape):\r\n    global x0\r\n    global y0\r\n    global distance\r\n    x = shape.firstpoint.X\r\n    y = shape.firstpoint.Y\r\n    if x0 == 0.0 and y0 == 0.0:\r\n        x0 = x\r\n        y0 = y\r\n    distance += math.sqrt((x - x0)**2 + (y - y0)**2)\r\n    x0 = x\r\n    y0 = y\r\n    return distance\r\n\r\n""""""-----------------------------------------\r\nazimuth_to(shape, from_x, from_y)\r\ninput:      shape field, from_x, from_y\r\nreturns:    angle between 0 and <360 between a specified point and others\r\nexpression: azimuth_to(!Shape!, from_x, from_y)\r\n""""""\r\ndef azimuth_to(shape, from_x, from_y):\r\n    x = shape.centroid.X\r\n    y = shape.centroid.Y\r\n    radian = math.atan((x - from_x)/(y - from_y))\r\n    degrees = math.degrees(radian)\r\n    if degrees < 0:\r\n        return degrees + 360.0\r\n    else:\r\n        return degrees\r\n\r\n""""""-----------------------------------------\r\nangle_between(shape)\r\ninput:      shape field\r\nreturns:    angle between successive points,\r\n            NE +ve 0 to 90, NW +ve 90 to 180,\r\n            SE -ve <0 to -90, SW -ve <-90 to -180\r\nexpression: angle_between(!Shape!)\r\n""""""\r\nx0 = 0.0;  y0 = 0.0;  angle = 0.0\r\ndef angle_between(shape):\r\n    global x0\r\n    global y0\r\n    x = shape.centroid.X\r\n    y = shape.centroid.Y\r\n    if x0 == 0.0 and y0 == 0.0:\r\n        x0 = x\r\n        y0 = y\r\n        return 0.0\r\n    radian = math.atan2((y - y0),(x - x0))\r\n    angle = math.degrees(radian)\r\n    x0 = x\r\n    y0 = y\r\n    return angle\r\n\r\n#--Polyline features\r\n\r\n""""""-----------------------------------------\r\nInput shape field: simple shape length\r\npoly_length(!Shape!)    #enter into the expression box\r\n""""""\r\ndef poly_length(shape):\r\n    return shape.length\r\n\r\n""""""-----------------------------------------\r\nInput shape field: returns cumulative length of polylines connected or not\r\npoly_cumu_len(!Shape!)    #enter into the expression box\r\n""""""\r\nlength = 0.0\r\ndef poly_cumu_len(shape):\r\n    global length\r\n    length += shape.length\r\n    return length\r\n\r\n""""""-----------------------------------------\r\nInput shape field: returns angle between 0 and <360 based upon the first and last point\r\nazimuth_to(!Shape!,from_x, from_y)  # ie azimuth_to(!Shape!,339200, 5025200)\r\n""""""\r\nimport math\r\ndef azimuth_to(shape, from_x, from_y):\r\n    x = shape.centroid.X\r\n    y = shape.centroid.Y\r\n    radian = math.atan2((y - from_y), (x - from_x))\r\n    degrees = math.degrees(radian)\r\n    if degrees < 0:\r\n        return  degrees + 360\r\n    return degrees\r\n\r\n\r\n""""""-----------------------------------------\r\nInput shape field, value (distance or decimal fraction, 0-1),\r\n    use_fraction (True/False), XorY (X or Y):\r\nReturns a point x meters or x  decimal fraction along a line,\r\n   user specifies whether X or Y coordinates\r\npnt_along(!Shape!, 100, False, \'X\')  # eg. X coordinate 100 m from start point\r\n""""""\r\ndef pnt_along(shape, value, use_fraction, XorY):\r\n    XorY = XorY.upper()\r\n    if use_fraction and (value > 1.0):\r\n        value = value/100.0\r\n    if shape.type == ""polygon"":\r\n        shape = shape.boundary()\r\n    pnt = shape.positionAlongLine(value, use_fraction)\r\n    if XorY == \'X\':\r\n        return pnt.centroid.X\r\n    else:\r\n        return pnt.centroid.Y\r\n\r\n# --Polygon\r\n\r\n\r\n# --Shapes in general\r\n##def shape_shift(shape,dX=0,dY=0):\r\n##    """""" shape_shift(!Shape!,0,0)\r\n##    __esri_field_calculator_splitter__\r\n##    shift/move/translate a shape by dX,dY\r\n##    """"""\r\n##    pnt = shape.firstPoint.X #.centroid\r\n##    #pnt.X += pnt.X + dX\r\n##    #pnt.Y = pnt.Y + dY\r\n##    return pnt\r\nimport arcpy\r\n\r\ndef shift_features(in_features, x_shift=None, y_shift=None):\r\n    """"""\r\n    Shifts features by an x and/or y value. The shift values are in\r\n    the units of the in_features coordinate system.\r\n\r\n    Parameters:\r\n    in_features: string\r\n        An existing feature class or feature layer.  If using a\r\n        feature layer with a selection, only the selected features\r\n        will be modified.\r\n\r\n    x_shift: float\r\n        The distance the x coordinates will be shifted.\r\n\r\n    y_shift: float\r\n        The distance the y coordinates will be shifted.\r\n    """"""\r\n\r\n    with arcpy.da.UpdateCursor(in_features, [\'SHAPE@XY\']) as cursor:\r\n        for row in cursor:\r\n            cursor.updateRow([[row[0][0] + (x_shift or 0),\r\n                               row[0][1] + (y_shift or 0)]])\r\n\r\n    return\r\n\r\n\r\n# shape_shift(!SHAPE@!,0,0)\r\n\r\n# formatting\r\n""""""partition a string or number into parts in a text field\r\n!field_name!   in the expression box""""""\r\ndef partition(field_name):\r\n    a = str(field_name)\r\n    return ""{0[0]}+{0[1]}"".format(a.partition(a[3:]))\r\n\r\n\r\n#field_name=602300\r\n#print frmt(field_name)\r\n'"
field_calculator/Field_Calculator_main.py,0,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nfield_calculator_tools\r\n======================\r\n\r\nScript :   field_calculator_tools.py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified : 2018-07-25\r\n\r\nPurpose :  tools for working ArcGIS Pro field calculator\r\n\r\nUseage:\r\n-------\r\n\r\n\r\n\r\n**Requires**\r\n------------\r\n  see import section and __init__.py in the `arraytools` folder\r\n\r\n**Notes**\r\n---------\r\n\r\n**Basic array information**\r\n\r\n""""""\r\n# ---- imports, formats, constants -------------------------------------------\r\nimport sys\r\nimport inspect\r\nfrom textwrap import dedent, indent\r\nimport numpy as np\r\nimport arcpy\r\n\r\narcpy.env.overwriteOutput = True\r\n\r\nscript = sys.argv[0]\r\n\r\n# ---- simple functions ------------------------------------------------------\r\n\r\n__all__ = []\r\n\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    create_output = False\r\n    in_tbl = ""/Carp_AOI""\r\n    gdb = ""/Field_calculator_tools.gdb""\r\n    flder = ""/"".join(script.split(""/"")[:-2])\r\n    wrkspace = flder + gdb\r\n    in_tbl = wrkspace + in_tbl\r\n    in_fld = None\r\nelse:\r\n    testing = False\r\n    create_output = True\r\n    in_tbl = sys.argv[1]\r\n    in_fld = sys.argv[2]\r\n    exp_key = sys.argv[3]\r\n\r\n# ---- Do the work -----------------------------------------------------------\r\n#\r\ndesc = arcpy.da.Describe(in_tbl)\r\nwrkspace = desc[\'path\']\r\n\r\n# ---- Expression functions\r\nfld_name = in_fld\r\nif exp_key == ""area sq km"":\r\n     args = [""Area_sqkm"", ""!Shape!.getArea(\'GEODESIC\',\'SQUAREKILOMETERS\')"",\r\n             ""#""]\r\nelif exp_key == ""leng km"":\r\n    args = [""Leng_km"", ""!Shape!.getLength(\'GEODESIC\',\'KILOMETERS\')"", ""#""]\r\nelif exp_key in (""sum angles"", ""min angle"", ""max angle""):\r\n    from angles_ import angles_poly\r\n    if inspect.isfunction(angles_poly):\r\n        lines, ln_num = inspect.getsourcelines(angles_poly)\r\n        code = """".join([""{}"".format(line) for line in lines])\r\n        if exp_key == ""sum angles"":\r\n            fld_expr = ""angles_poly(!Shape!, kind=\'sum\')""\r\n            fld_name = ""Angle_sum""\r\n        elif exp_key == ""min angle"":\r\n            fld_expr = ""angles_poly(!Shape!, kind=\'min\')""\r\n            fld_name = ""Angle_min""\r\n        elif exp_key == ""max angle"":\r\n            fld_expr = ""angles_poly(!Shape!, kind=\'max\')""\r\n            fld_name = ""Angle_max""\r\n    args = [fld_name, fld_expr, code]\r\nelif exp_key in (""cumu_dist""):\r\n    import cumu_dist\r\n    from cumu_dist import dist_cumu\r\n    if inspect.isfunction(dist_cumu):\r\n        lines, ln_num = inspect.getsourcelines(dist_cumu)\r\n        code = """".join([""{}"".format(line) for line in lines])\r\n        if exp_key == ""cumu_dist"":\r\n            fld_expr = ""dist_cumu(!Shape!, is_first=True)""\r\n            fld_name = ""Cumu_dist""\r\n    args = [fld_name, fld_expr, code]\r\n\r\nfld_name, fld_expr, code = args\r\n\r\narcpy.MakeTableView_management(\r\n        in_table=in_tbl,\r\n        out_view=""tbl_view"",\r\n        workspace=wrkspace)\r\n\r\nif in_fld in (None, """", "" ""):\r\n    fld_name = fld_name\r\nelse:\r\n    fld_name = in_fld\r\nfld_name = arcpy.ValidateFieldName(fld_name)\r\narcpy.AddField_management(\r\n        ""tbl_view"",\r\n        field_name=fld_name,\r\n        field_type=""DOUBLE"",\r\n        field_is_nullable=""NULLABLE"")\r\n\r\narcpy.CalculateField_management(\r\n        in_table=""tbl_view"",\r\n        field=fld_name,\r\n        expression=fld_expr,\r\n        code_block=code)\r\n\r\ndel in_fld, in_tbl, arcpy\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\n\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n#    pnts, mesh = _demo()'"
field_calculator/angle_between.py,0,"b'# -*- coding: UTF-8 -*-\r\n""""""-----------------------------------------\r\nInput shape field: returns angle between 0 and <360 based upon the first and last point\r\nazimuth_to(!Shape!,from_x, from_y, from_north=True)  \r\nie azimuth_to(!Shape!, 300050, 5000050, True)\r\n""""""\r\nimport math\r\ndef azimuth_to(shape, from_x, from_y, from_north):\r\n    x = shape.centroid.X\r\n    y = shape.centroid.Y\r\n    radian = math.atan2((y - from_y), (x - from_x))\r\n    angle = math.degrees(radian)\r\n    if from_north:\r\n        angle = (450 - angle) % 360\r\n    return angle\r\n__esri_field_calculator_splitter__\r\nazimuth_to(!Shape!, 300050, 5000050, True)'"
field_calculator/angles_.py,13,"b'# -*- coding: utf-8 -*-\r\nimport numpy as np\r\ndef angles_poly(a, inside=True, in_deg=True, kind=""sum""):\r\n    """"""Sequential angles from a poly* shape\r\n\tNOTE: this line.... angle = np.sum(angles)\r\n\t      can be changed to `np.min`, `np.max` or others\r\n\t\t  depending on what needs to be returned\r\n    """"""\r\n    import numpy as np\r\n    a = a.getPart()\r\n    a =np.asarray([[i.X, i.Y] for j in a for i in j])\r\n    if len(a) < 2:\r\n        return None\r\n    elif len(a) == 2:  # **** check\r\n        ba = a[1] - a[0]\r\n        return np.arctan2(*ba[::-1])\r\n    else:\r\n        angles = []\r\n        if np.allclose(a[0], a[-1]):  # closed loop\r\n            a = a[:-1]\r\n            r = (-1,) + tuple(range(len(a))) + (0,)\r\n        else:\r\n            r = tuple(range(len(a)))\r\n        for i in range(len(r)-2):\r\n            p0, p1, p2 = a[r[i]], a[r[i+1]], a[r[i+2]]\r\n            ba = p1 - p0\r\n            bc = p1 - p2\r\n            cr = np.cross(ba, bc)\r\n            dt = np.dot(ba, bc)\r\n            ang = np.arctan2(np.linalg.norm(cr), dt)\r\n            if not np.allclose(ang, np.pi):  # check for extra vertices\r\n                angles.append(ang)\r\n    if in_deg:\r\n        angles = np.degrees(angles)\r\n    if kind == ""sum"":\r\n        angle = np.sum(angles)\r\n    elif kind == ""min"":\r\n        angle = np.min(angles)\r\n    elif kind == ""max"":\r\n        angle = np.max(angles)\r\n    return angle\r\n#__esri_field_calculator_splitter__\r\n#angles_poly(!Shape!)'"
field_calculator/azimuth_sequential_pnts.py,0,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   azimuth_sequential_pnts.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2017-06-19\r\n:Purpose:  tools for working with numpy arrays\r\n:Useage:\r\n:\r\n:References:\r\n:\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nx0 = 0.0\r\ny0 = 0.0\r\nangle = 0.0\r\ndef angle_between(shape, from_north):\r\n    """"""Calculate the angle/azimuth between sequential points in a point file.\r\n    :Use:\r\n    : .... angle_between(!Shape!, True) ....\r\n    """"""\r\n    global x0\r\n    global y0\r\n    x = shape.centroid.X\r\n    y = shape.centroid.Y\r\n    if x0 == 0.0 and y0 == 0.0:\r\n        x0 = x\r\n        y0 = y\r\n        return 0.0\r\n    radian = math.atan2((y - y0), (x - x0))\r\n    angle = math.degrees(radian)\r\n    if from_north:\r\n        angle = (450 - angle) % 360\r\n    x0 = x\r\n    y0 = y\r\n    return angle\r\n# __esri_field_calculator_splitter__  # optionally\r\n# angle_between(!Shape!, True)\r\n#\r\n# Python command... expr is the code block above\r\n# arcpy.management.CalculateField(""poly_pnts"", ""Seq_angle"",\r\n#                                 ""angle_between(!Shape!, True)"", ""PYTHON_9.3"",\r\n#                                 expr)\r\n# Angle_between(!Shape!)\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n#    _demo()\r\n\r\nfrom random import uniform\r\ndef shift(val, start=-1, end=1):\r\n    """"""shift within the range - start and end""""""\r\n    jiggle = uniform(start, end)\r\n    return val + jiggle\r\n\r\na = 10\r\nprint(shift(10, -1, 1))'"
field_calculator/azimuth_to.py,0,"b'# -*- coding: UTF-8 -*-\r\n""""""-----------------------------------------\r\n:Input shape field:\r\n:returns angle between 0 and <360 from the first and last point\r\n:azimuth_to(!Shape!,from_x, from_y, from_north=True)\r\nie azimuth_to(!Shape!, 300050, 5000050, True)\r\n""""""\r\nimport math\r\n\r\ndef azimuth_to(shape, from_x, from_y, from_north):\r\n    x = shape.centroid.X\r\n    y = shape.centroid.Y\r\n    radian = math.atan2((y - from_y), (x - from_x))\r\n    angle = math.degrees(radian)\r\n    if from_north:\r\n        angle = (450 - angle) % 360\r\n    return angle\r\n#__esri_field_calculator_splitter__\r\n#azimuth_to(!Shape!, 300050, 5000050, True)'"
field_calculator/coordinate_defs.py,0,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   coordinate_defs.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2017-06-17\r\n:Purpose:  convert string formatted coordinates in some variant of degrees\r\n:\r\n:---------------------------------------------------------------------:\r\n""""""\r\ndef dd_mm_ss(dd, cal_long=True, use_sign=False, use_quad=False):\r\n    """"""decimal degrees to deg dec min""""""\r\n    deg_sign = u\'\\N{DEGREE SIGN}\'\r\n    deg = int(dd)\r\n    if deg < 0:\r\n        quad = [\'S\', \'W\'][cal_long]\r\n        deg = abs(deg)\r\n    else:\r\n        quad = [\'N\', \'E\'][cal_long]\r\n    if not use_quad:\r\n        quad = """"\r\n    if not use_sign:\r\n        deg_sign = """"\r\n    mins, secs = divmod(dd*3600, 60)\r\n    degs, mins = divmod(mins, 60)\r\n    frmt = ""{}{}-{:0.0f}-{:05.2f}{}"".format(deg, deg_sign, mins, secs, quad)\r\n    return frmt\r\n\r\n\r\ndef dd_dmm(dd, cal_long=True):\r\n    """"""decimal degrees to deg dec min""""""\r\n    deg_sign = u\'\\N{DEGREE SIGN}\'\r\n    deg = int(dd)\r\n    if deg < 0:\r\n        quad = [\'S\', \'W\'][cal_long]\r\n        deg = abs(deg)\r\n    else:\r\n        quad = [\'N\', \'E\'][cal_long]\r\n    minsec = divmod((deg - dd)*60, 60)[-1]\r\n    frmt = ""{}{} {:0.2f}\' {}"".format(deg, deg_sign, minsec, quad)\r\n    return frmt\r\n\r\n\r\ndef ddm_ddd(a, sep="" ""):\r\n    """""" convert degree, decimal minute string to decimal degrees\r\n    : a - degree, decimal minute string\r\n    : sep - usually a space, but check\r\n    : Useage - ddm_ddd(!SourceField!, sep="" "")\r\n    :    python parser, sourcefield is the input string field, destination\r\n    :    field is type double\r\n    """"""\r\n    d, m = [float(i) for i in a.split(sep)]\r\n    sign = [-1, 1][d > 0]\r\n    dd = sign*(abs(d) + m/60.)\r\n    return dd\r\n\r\n\r\ndef dms_ddd(a, sep="" ""):\r\n    """""" convert degree, minute, decimal second string to decimal degrees\r\n    : a - degree, minute, decimal second string\r\n    : sep - usually a space, but check\r\n    : Useage - dms_ddd(!SourceField!, sep="" "")\r\n    :    python parser, sourcefield is the input string field, destination\r\n    :    field is type double\r\n    """"""\r\n    d, m, s = [float(i) for i in a.split(sep)]\r\n    sign = [-1, 1][d > 0]\r\n    dd = sign*(abs(d) + (m + s/60.)/60.)\r\n    return dd\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Samples\r\n    """"""\r\n    a = \'45 30.30\'\r\n    b = \'-75 45.45\'\r\n    c = \'45 30 30.30\'\r\n    d = \'-75 45 45.45\'\r\n    print(\'Input... {:>12s} to... {:> 12.8f}\'.format(a, ddm_ddd(a, sep="" "")))\r\n    print(\'Input... {:>12s} to... {:> 12.8f}\'.format(b, ddm_ddd(b, sep="" "")))\r\n    print(\'Input... {:>12s} to... {:> 12.8f}\'.format(c, dms_ddd(c, sep="" "")))\r\n    print(\'Input... {:>12s} to... {:> 12.8f}\'.format(d, dms_ddd(d, sep="" "")))\r\n'"
field_calculator/cumu_dist.py,0,"b'# -*- coding: utf-8 -*-\r\ndef dist_cumu(shape, is_first=True):\r\n    import arcpy\r\n    global x0;  global y0;  global distance\r\n    #\r\n    arcpy.AddMessage(str(is_first))\r\n    if is_first:\r\n        import math\r\n        x0 = 0.0;  y0 = 0.0;  distance = 0.0\r\n    x = shape.centroid.X;  y = shape.centroid.Y\r\n#    arcpy.AddMessage((str(x)+ str(y)) )\r\n    if x0 == 0.0 and y0 == 0.0:\r\n        x0 = x; y0 = y\r\n    distance += math.sqrt((x - x0)**2 + (y - y0)**2)\r\n    x0 = x;  y0 = y\r\n    is_first = False\r\n    arcpy.AddMessage((""{}, {} {}"".format(x, y, distance) ))\r\n    return distance, is_first'"
field_calculator/date_time_defs.py,0,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   date_time_defs.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2017-06-17\r\n:Purpose:  date_time defs\r\n:  Can be used in the field calculator in ArcMap or ArcGIS Pro\r\n:---------------------------------------------------------------------:\r\n""""""\r\nfrom datetime import datetime\r\n\r\n# -*- coding: utf-8 -*-\r\n""""""\r\nScript:\xc2\xa0 DDddd_DMS_convert.py\r\nAuthor:\xc2\xa0 Dan.Patterson@carleton.ca\r\n\r\nPurpose: Formatting stuff\r\nNotes:\r\n- to use in the field calculator, set the parser to Python and\r\n\xc2\xa0 use \' !fieldname! \' in the expression box\r\n- degree sign, \xc2\xb0 ... ALT 248 on the numeric keypad\r\n- utf-8\xc2\xa0 u\'\\N{DEGREE SIGN}\' or u\'\\xb0\'\r\n""""""\r\n\r\ndef ddd_dms(a):\r\n    """"""Decimal degree to DMS format""""""\r\n    sign = [-1, 1][a > 0]\r\n    DD, dd = divmod(a, sign)\r\n    MM, ss = divmod(dd*60, sign)\r\n    SS, ssss = divmod(ss*60, 1)\r\n    frmt = ""{:0= 4}"" + u\'\\xb0\' + "" {:=2}\' {:0=7.4f}\\"" ""\r\n    DMS = frmt.format(int(sign*DD), int(MM), sign*ss*60)\r\n    return DMS\r\n\r\n\r\ndef get_date(fld):\r\n    """"""input a date field, strip off the time and format\r\n    :Useage  - get_date(!FieldName!)\r\n    :From    - 2017-06-17 20:35:58.777353 ... 2017-06-17\r\n    :Returns -2017-06-17\r\n    """"""\r\n    if fld is not None:\r\n        lst = [int(i) for i in (str(fld).split("" "")[0]).split(""-"")]\r\n        return ""{}-{:02.0f}-{:02.0f}"".format(*lst)\r\n    else:\r\n        return None\r\n\r\n\r\ndef get_time(fld):\r\n    """"""input a date field, strip off the date and format\r\n    :From    - 2017-06-17 20:35:58.777353 ... 2017-06-17\r\n    :Returns - 20 h 35 m  58.78 s\r\n    """"""\r\n    if fld is not None:\r\n        lst = [float(i) for i in (str(fld).split("" "")[1]).split("":"")]\r\n        return ""{:02.0f} h {:02.0f} m {: 5.2f} s"".format(*lst)\r\n    else:\r\n        return None\r\n\r\ndef _demo():\r\n    """"""\r\n\xc2\xa0\xc2\xa0\xc2\xa0 :other format options\r\n\xc2\xa0\xc2\xa0\xc2\xa0 :mess with the order of line 44 for different outputs\r\n\xc2\xa0\xc2\xa0\xc2\xa0 """"""\r\n    today = datetime.today()\r\n    print(\'\\n_demo def...\\nISO\xc2\xa0\xc2\xa0\xc2\xa0\xc2\xa0 :\', today)\r\n    print(\'format(): {:%a %b %d %H:%M:%S %Y}\'.format(today))\r\n    #return today\r\n\r\n#--------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """""" A simple test """"""\r\n    n = str(datetime.now())\r\n\r\n    print(""\\nget_date def...\\nDate from {} ... {}"".format(n, get_date(n)))\r\n    print(""\\nget_time def...\\nDate from {} ... {}"".format(n, get_time(n)))\r\n\r\n    _demo()\r\n\r\n    vals = [45.501234567890, -45.501234567890,\r\n            145.501234567890, -145.501234567890]\r\n    print(""\\nddd_dms def...\\nTest run with a in vals"")\r\n    for a in vals:\r\n        print(""{:> 22.12f} ... {!s:>20}"".format(a, ddd_dms(a)))\r\n'"
field_calculator/dist_to.py,0,"b'""\x00""\x00""\x00-\x00-\x00-\x00-\x00-\x00-\x00-\x00-\x00-\x00-\x00-\x00-\x00-\x00-\x00-\x00-\x00-\x00-\x00-\x00-\x00-\x00-\x00-\x00-\x00-\x00-\x00-\x00-\x00-\x00-\x00-\x00-\x00-\x00-\x00-\x00-\x00-\x00-\x00-\x00-\x00-\x00\r\x00\n\x00d\x00i\x00s\x00t\x00_\x00t\x00o\x00(\x00s\x00h\x00a\x00p\x00e\x00,\x00 \x00f\x00r\x00o\x00m\x00_\x00x\x00,\x00 \x00f\x00r\x00o\x00m\x00_\x00y\x00)\x00\r\x00\n\x00i\x00n\x00p\x00u\x00t\x00:\x00 \x00 \x00 \x00 \x00 \x00 \x00s\x00h\x00a\x00p\x00e\x00 \x00f\x00i\x00e\x00l\x00d\x00,\x00 \x00o\x00r\x00i\x00g\x00i\x00n\x00 \x00x\x00,\x00y\x00\r\x00\n\x00r\x00e\x00t\x00u\x00r\x00n\x00s\x00:\x00 \x00 \x00 \x00 \x00d\x00i\x00s\x00t\x00a\x00n\x00c\x00e\x00 \x00t\x00o\x00 \x00t\x00h\x00e\x00 \x00s\x00p\x00e\x00c\x00i\x00f\x00i\x00e\x00d\x00 \x00p\x00o\x00i\x00n\x00t\x00\r\x00\n\x00e\x00x\x00p\x00r\x00e\x00s\x00s\x00i\x00o\x00n\x00:\x00 \x00d\x00i\x00s\x00t\x00_\x00t\x00o\x00(\x00!\x00S\x00h\x00a\x00p\x00e\x00!\x00,\x00 \x00x\x00,\x00 \x00y\x00)\x00\r\x00\n\x00""\x00""\x00""\x00\r\x00\n\x00d\x00e\x00f\x00 \x00d\x00i\x00s\x00t\x00_\x00t\x00o\x00(\x00s\x00h\x00a\x00p\x00e\x00,\x00 \x00f\x00r\x00o\x00m\x00_\x00x\x00,\x00 \x00f\x00r\x00o\x00m\x00_\x00y\x00)\x00:\x00\r\x00\n\x00 \x00 \x00 \x00 \x00x\x00 \x00=\x00 \x00s\x00h\x00a\x00p\x00e\x00.\x00c\x00e\x00n\x00t\x00r\x00o\x00i\x00d\x00.\x00X\x00\r\x00\n\x00 \x00 \x00 \x00 \x00y\x00 \x00=\x00 \x00s\x00h\x00a\x00p\x00e\x00.\x00c\x00e\x00n\x00t\x00r\x00o\x00i\x00d\x00.\x00Y\x00\r\x00\n\x00 \x00 \x00 \x00 \x00d\x00i\x00s\x00t\x00a\x00n\x00c\x00e\x00 \x00=\x00 \x00m\x00a\x00t\x00h\x00.\x00s\x00q\x00r\x00t\x00(\x00(\x00x\x00 \x00-\x00 \x00f\x00r\x00o\x00m\x00_\x00x\x00)\x00*\x00*\x002\x00 \x00+\x00 \x00(\x00y\x00 \x00-\x00 \x00f\x00r\x00o\x00m\x00_\x00y\x00)\x00*\x00*\x002\x00)\x00\r\x00\n\x00 \x00 \x00 \x00 \x00r\x00e\x00t\x00u\x00r\x00n\x00 \x00d\x00i\x00s\x00t\x00a\x00n\x00c\x00e\x00\r\x00\n\x00_\x00_\x00e\x00s\x00r\x00i\x00_\x00f\x00i\x00e\x00l\x00d\x00_\x00c\x00a\x00l\x00c\x00u\x00l\x00a\x00t\x00o\x00r\x00_\x00s\x00p\x00l\x00i\x00t\x00t\x00e\x00r\x00_\x00_\x00\r\x00\n\x00d\x00i\x00s\x00t\x00_\x00t\x00o\x00(\x00!\x00S\x00h\x00a\x00p\x00e\x00!\x00,\x00 \x001\x001\x009\x004\x004\x005\x003\x00.\x001\x005\x008\x002\x008\x00,\x00 \x009\x008\x006\x004\x008\x005\x00.\x005\x007\x003\x008\x001\x009\x00)\x00'"
field_calculator/natural_pad.py,0,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   natural_pad.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2017-08-23\r\n:Purpose:  Returns a mixed text-number list accounting for numeric values\r\n:  [\'a1\', \'a20\', \'a2\', \'a10\'] should yield [\'a001\', \'a020\', \'a002\', \'a010\']\r\n""""""\r\n\r\nimport re\r\n\r\n\r\ndef nat_pad(val, pad=\'0000\'):\r\n    """"""natural sort... put the import re outside of the function\r\n    :if using the field calculator\r\n    : calculator expression- nat_pad(!data_field!, pad=\'a bunch of 0s\')\r\n    """"""\r\n    txt = re.split(\'([0-9]+)\', val)\r\n    l_val = len(str(val))\r\n    txt_out = ""{}{}{}"".format(txt[0], pad[:-l_val], txt[1])\r\n    return txt_out\r\n\r\n\r\n# --------------------------------------------------------------------------\r\nif __name__ == \'__main__\':\r\n    a = [\'a1\', \'a20\', \'a2\', \'a10\']\r\n    print(""input - \\n{}"".format(a))\r\n    vals = [nat_pad(i) for i in a]\r\n    print(""output - \\n{}"".format(vals))\r\n'"
field_calculator/natural_sort.cal.py,0,"b'import re\r\ndef natsort(lst):\r\n    """"""natural sort""""""\r\n    import re\r\n    convert = lambda text: int(text) if text.isdigit() else text\r\n    a_key = lambda key: [convert(c) for c in re.split(\'([0-9]+)\', key)]\r\n    return sorted(lst, key=a_key)\r\nif __name__ == \'__main__\':\r\n    a = [\'r1\', \'r1\', \'r1\', \'r4\', \'r4\', \'r7\', \'r7\', \'r7\', \'r10\', \'r10\']\r\n    b = sorted(a)\r\n    print(""input - \\n{}"".format(a))\r\n    print(""text sort - \\n{}"".format(b))\r\n    vals = natsort(a)\r\n    print(""natural sort - \\n{}"".format(vals))'"
field_calculator/natural_sort.py,0,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   natural_sort.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2017-08-23\r\n:Purpose:  Returns a mixed text-number list accounting for numeric values\r\n:  [\'a1\', \'a20\', \'a2\', \'a10\'] should yield [\'a1\', \'a2\', \'a10\', \'a20\']\r\n:Note:\r\n: C:\\Git_Dan\\JupyterNoteBooks\\Short_Samples\\Natural_sort.ipynb\r\n""""""\r\n\r\nimport re\r\n\r\n\r\ndef natsort(text_lst):\r\n    """"""natural sort returns text containing numbers sorted considering the\r\n    :  number in the sequence.\r\n    :originals used lambda expressions\r\n    :  convert = lambda text: int(text) if text.isdigit() else text\r\n    :  a_key = lambda key: [convert(c) for c in re.split(\'([0-9]+)\', key)]\r\n    """"""\r\n    def convert(text):\r\n        return int(text) if text.isdigit() else text\r\n\r\n    def a_key(key):\r\n        return [convert(c) for c in re.split(\'([0-9]+)\', key)]\r\n\r\n    return sorted(text_lst, key=a_key)\r\n\r\n\r\n# --------------------------------------------------------------------------\r\nif __name__ == \'__main__\':\r\n    """"""run with sample""""""\r\n    a = [\'a1\', \'a20\', \'a2\', \'a10\']\r\n    vals = natsort(a)\r\n#    print(""natural sort - \\n{}"".format(vals))\r\n'"
field_calculator/pad_date.py,0,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   .py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2017-xx-xx\r\n:Purpose:  tools for working with numpy arrays\r\n:Useage:\r\n:\r\n:References:\r\n:\r\n:---------------------------------------------------------------------:\r\n""""""\r\ndef pad_date(fld):\r\n    """"""input a date field, strip off the time and format""""""\r\n    if fld is not None:\r\n        lst = [int(i) for i in (str(fld).split("" "")[0]).split(""-"")]\r\n        return ""{}-{:02.0f}-{:02.0f}"".format(*lst)\r\n    else:\r\n        return None\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """""" A simple test """"""\r\n    from datetime import datetime\r\n    n = str(datetime.now())\r\n    print(pad_date(n))'"
field_calculator/pnt_along.py,0,"b'def pnt_along(shape, value=0.0, use_fraction=False, XorY=""X""):\r\n    """"""Position X or Y coordinate, x/y meters or decimal fraction along a line.\r\n\xc2\xa0\xc2\xa0\xc2\xa0 :Requires:\r\n\xc2\xa0\xc2\xa0\xc2\xa0 :--------\r\n\xc2\xa0\xc2\xa0\xc2\xa0 : shape field: python parser use !Shape!\r\n\xc2\xa0\xc2\xa0\xc2\xa0 : value: (distance or decimal fraction, 0-1)\r\n\xc2\xa0\xc2\xa0\xc2\xa0 : use_fraction: (True/False)\r\n\xc2\xa0\xc2\xa0\xc2\xa0 : XorY: specify X or Y coordinates\r\n\xc2\xa0\xc2\xa0\xc2\xa0 :\r\n\xc2\xa0\xc2\xa0\xc2\xa0 :Returns: the specified coordinate (X or Y) meters or % along line or boundary\r\n\xc2\xa0\xc2\xa0\xc2\xa0 :-------\r\n\xc2\xa0\xc2\xa0\xc2\xa0 :\r\n    :Useage: pnt_along(!Shape!, 100, False, ""X"") # X, 100 m from start point\r\n    :\r\n    """"""\r\n    XorY = XorY.upper()\r\n    if use_fraction and (value > 1.0):\r\n\xc2\xa0\xc2\xa0\xc2\xa0     value = value/100.0\r\n    if shape.type.lower() == ""polygon"":\r\n\xc2\xa0\xc2\xa0\xc2\xa0     shape = shape.boundary()\r\n    pnt = shape.positionAlongLine(value,use_fraction)\r\n    if XorY == \'X\':\r\n        return pnt.centroid.X\r\n    else:\r\n        return pnt.centroid.Y'"
field_calculator/rand_float.py,0,"b'i\x00m\x00p\x00o\x00r\x00t\x00 \x00n\x00u\x00m\x00p\x00y\x00 \x00a\x00s\x00 \x00n\x00p\x00\r\x00\n\x00d\x00e\x00f\x00 \x00r\x00a\x00n\x00d\x00_\x00f\x00l\x00o\x00a\x00t\x00(\x00N\x00=\x001\x00,\x00 \x00b\x00e\x00g\x00i\x00n\x00=\x000\x00,\x00 \x00e\x00n\x00d\x00=\x001\x000\x00,\x00 \x00d\x00e\x00c\x00i\x00=\x002\x00)\x00:\x00\r\x00\n\x00 \x00 \x00 \x00 \x00""\x00""\x00""\x00 \x00 \x00G\x00e\x00n\x00e\x00r\x00a\x00t\x00e\x00 \x00N\x00 \x00r\x00a\x00n\x00d\x00o\x00m\x00 \x00f\x00l\x00o\x00a\x00t\x00s\x00 \x00w\x00i\x00t\x00h\x00i\x00n\x00 \x00t\x00h\x00e\x00 \x00r\x00a\x00n\x00g\x00e\x00 \x00b\x00e\x00g\x00i\x00n\x00 \x00-\x00 \x00e\x00n\x00d\x00\r\x00\n\x00 \x00 \x00 \x00 \x00:\x00T\x00e\x00c\x00h\x00n\x00i\x00c\x00a\x00l\x00l\x00y\x00,\x00 \x00N\x00 \x00r\x00a\x00n\x00d\x00o\x00m\x00 \x00i\x00n\x00t\x00e\x00g\x00e\x00r\x00s\x00 \x00a\x00r\x00e\x00 \x00p\x00r\x00o\x00d\x00u\x00c\x00e\x00d\x00 \x00t\x00h\x00e\x00n\x00 \x00a\x00 \x00r\x00a\x00n\x00d\x00o\x00m\x00\r\x00\n\x00 \x00 \x00 \x00 \x00:\x00a\x00m\x00o\x00u\x00n\x00t\x00 \x00w\x00i\x00t\x00h\x00i\x00n\x00 \x000\x00-\x001\x00 \x00i\x00s\x00 \x00a\x00d\x00d\x00e\x00d\x00 \x00t\x00o\x00 \x00t\x00h\x00e\x00 \x00v\x00a\x00l\x00u\x00e\x00\r\x00\n\x00 \x00 \x00 \x00 \x00""\x00""\x00""\x00\r\x00\n\x00 \x00 \x00 \x00 \x00f\x00l\x00o\x00a\x00t\x00_\x00v\x00a\x00l\x00s\x00 \x00=\x00 \x00n\x00p\x00.\x00r\x00a\x00n\x00d\x00o\x00m\x00.\x00r\x00a\x00n\x00d\x00i\x00n\x00t\x00(\x00b\x00e\x00g\x00i\x00n\x00,\x00 \x00e\x00n\x00d\x00,\x00 \x00s\x00i\x00z\x00e\x00=\x00(\x00N\x00)\x00)\x00\r\x00\n\x00 \x00 \x00 \x00 \x00f\x00l\x00o\x00a\x00t\x00_\x00v\x00a\x00l\x00s\x00 \x00=\x00 \x00n\x00p\x00.\x00a\x00r\x00o\x00u\x00n\x00d\x00(\x00f\x00l\x00o\x00a\x00t\x00_\x00v\x00a\x00l\x00s\x00 \x00+\x00 \x00n\x00p\x00.\x00r\x00a\x00n\x00d\x00o\x00m\x00.\x00r\x00a\x00n\x00d\x00(\x00N\x00)\x00,\x00 \x00d\x00e\x00c\x00i\x00)\x00\r\x00\n\x00 \x00 \x00 \x00 \x00r\x00e\x00t\x00u\x00r\x00n\x00 \x00f\x00l\x00o\x00a\x00t\x00_\x00v\x00a\x00l\x00s\x00\r\x00\n\x00_\x00_\x00e\x00s\x00r\x00i\x00_\x00f\x00i\x00e\x00l\x00d\x00_\x00c\x00a\x00l\x00c\x00u\x00l\x00a\x00t\x00o\x00r\x00_\x00s\x00p\x00l\x00i\x00t\x00t\x00e\x00r\x00_\x00_\x00\r\x00\n\x00r\x00a\x00n\x00d\x00_\x00f\x00l\x00o\x00a\x00t\x00(\x00)\x00'"
field_calculator/rand_int.py,0,"b'i\x00m\x00p\x00o\x00r\x00t\x00 \x00n\x00u\x00m\x00p\x00y\x00 \x00a\x00s\x00 \x00n\x00p\x00\r\x00\n\x00d\x00e\x00f\x00 \x00r\x00a\x00n\x00d\x00_\x00i\x00n\x00t\x00(\x00l\x00o\x00w\x00=\x001\x00,\x00 \x00h\x00i\x00g\x00h\x00=\x001\x000\x00)\x00:\x00\r\x00\n\x00 \x00 \x00 \x00 \x00""\x00""\x00""\x00 \x00 \x00G\x00e\x00n\x00e\x00r\x00a\x00t\x00e\x00 \x00a\x00 \x00r\x00a\x00n\x00d\x00o\x00m\x00 \x00i\x00n\x00t\x00e\x00g\x00e\x00r\x00s\x00 \x00w\x00i\x00t\x00h\x00i\x00n\x00 \x00t\x00h\x00e\x00 \x00r\x00a\x00n\x00g\x00e\x00 \x00l\x00o\x00w\x00 \x00-\x00 \x00h\x00i\x00g\x00h\x00\r\x00\n\x00 \x00 \x00 \x00 \x00""\x00""\x00""\x00\r\x00\n\x00 \x00 \x00 \x00 \x00n\x00u\x00m\x00 \x00=\x00 \x00n\x00p\x00.\x00r\x00a\x00n\x00d\x00o\x00m\x00.\x00r\x00a\x00n\x00d\x00i\x00n\x00t\x00(\x00l\x00o\x00w\x00,\x00 \x00h\x00i\x00g\x00h\x00)\x00\r\x00\n\x00 \x00 \x00 \x00 \x00r\x00e\x00t\x00u\x00r\x00n\x00 \x00n\x00u\x00m\x00\r\x00\n\x00_\x00_\x00e\x00s\x00r\x00i\x00_\x00f\x00i\x00e\x00l\x00d\x00_\x00c\x00a\x00l\x00c\x00u\x00l\x00a\x00t\x00o\x00r\x00_\x00s\x00p\x00l\x00i\x00t\x00t\x00e\x00r\x00_\x00_\x00\r\x00\n\x00r\x00a\x00n\x00d\x00_\x00i\x00n\x00t\x00(\x00)\x00'"
field_calculator/rand_norm.py,0,"b'i\x00m\x00p\x00o\x00r\x00t\x00 \x00n\x00u\x00m\x00p\x00y\x00 \x00a\x00s\x00 \x00n\x00p\x00\r\x00\n\x00d\x00e\x00f\x00 \x00r\x00a\x00n\x00d\x00_\x00n\x00o\x00r\x00m\x00(\x00N\x00=\x001\x000\x00,\x00 \x00a\x00v\x00g\x00_\x00=\x001\x000\x00,\x00 \x00s\x00t\x00_\x00d\x00e\x00v\x00=\x001\x00,\x00 \x00d\x00e\x00c\x00i\x00=\x002\x00)\x00:\x00\r\x00\n\x00 \x00 \x00 \x00 \x00""\x00""\x00""\x00 \x00 \x00G\x00e\x00n\x00e\x00r\x00a\x00t\x00e\x00 \x00N\x00 \x00r\x00a\x00n\x00d\x00o\x00m\x00 \x00f\x00l\x00o\x00a\x00t\x00s\x00 \x00w\x00i\x00t\x00h\x00i\x00n\x00 \x00t\x00h\x00e\x00 \x00r\x00a\x00n\x00g\x00e\x00 \x00b\x00e\x00g\x00i\x00n\x00 \x00-\x00 \x00e\x00n\x00d\x00\r\x00\n\x00 \x00 \x00 \x00 \x00:\x00T\x00e\x00c\x00h\x00n\x00i\x00c\x00a\x00l\x00l\x00y\x00,\x00 \x00N\x00 \x00r\x00a\x00n\x00d\x00o\x00m\x00 \x00i\x00n\x00t\x00e\x00g\x00e\x00r\x00s\x00 \x00a\x00r\x00e\x00 \x00p\x00r\x00o\x00d\x00u\x00c\x00e\x00d\x00 \x00t\x00h\x00e\x00n\x00 \x00a\x00 \x00r\x00a\x00n\x00d\x00o\x00m\x00\r\x00\n\x00 \x00 \x00 \x00 \x00:\x00a\x00m\x00o\x00u\x00n\x00t\x00 \x00w\x00i\x00t\x00h\x00i\x00n\x00 \x000\x00-\x001\x00 \x00i\x00s\x00 \x00a\x00d\x00d\x00e\x00d\x00 \x00t\x00o\x00 \x00t\x00h\x00e\x00 \x00v\x00a\x00l\x00u\x00e\x00\r\x00\n\x00 \x00 \x00 \x00 \x00""\x00""\x00""\x00\r\x00\n\x00 \x00 \x00 \x00 \x00f\x00l\x00o\x00a\x00t\x00_\x00v\x00a\x00l\x00s\x00 \x00=\x00 \x00n\x00p\x00.\x00r\x00a\x00n\x00d\x00o\x00m\x00.\x00n\x00o\x00r\x00m\x00a\x00l\x00(\x00a\x00v\x00g\x00_\x00,\x00 \x00s\x00t\x00_\x00d\x00e\x00v\x00)\x00 \x00 \x00#\x00,\x00 \x00s\x00i\x00z\x00e\x00=\x00(\x00N\x00)\x00)\x00\r\x00\n\x00 \x00 \x00 \x00 \x00f\x00l\x00o\x00a\x00t\x00_\x00v\x00a\x00l\x00s\x00 \x00=\x00 \x00n\x00p\x00.\x00a\x00r\x00o\x00u\x00n\x00d\x00(\x00f\x00l\x00o\x00a\x00t\x00_\x00v\x00a\x00l\x00s\x00 \x00+\x00 \x00n\x00p\x00.\x00r\x00a\x00n\x00d\x00o\x00m\x00.\x00r\x00a\x00n\x00d\x00(\x00)\x00,\x00 \x00d\x00e\x00c\x00i\x00)\x00 \x00 \x00#\x00(\x00N\x00)\x00,\x00 \x00d\x00e\x00c\x00i\x00)\x00\r\x00\n\x00 \x00 \x00 \x00 \x00r\x00e\x00t\x00u\x00r\x00n\x00 \x00f\x00l\x00o\x00a\x00t\x00_\x00v\x00a\x00l\x00s\x00\r\x00\n\x00_\x00_\x00e\x00s\x00r\x00i\x00_\x00f\x00i\x00e\x00l\x00d\x00_\x00c\x00a\x00l\x00c\x00u\x00l\x00a\x00t\x00o\x00r\x00_\x00s\x00p\x00l\x00i\x00t\x00t\x00e\x00r\x00_\x00_\x00\r\x00\n\x00r\x00a\x00n\x00d\x00_\x00n\x00o\x00r\x00m\x00(\x00)\x00'"
field_calculator/seq_dup2.py,0,"b'# -*- coding: UTF-8 -*-\r\n# (1) ... field calculator code block\r\n#     Save as a *.cal file for loading directly into the code block.\r\n#     Uncomment the last 2 lines to have the seq_dup line split out\r\n#\r\nfld = """"\r\ndef seq_dup(val):\r\n    """"""sequential duplicate checks""""""\r\n    global fld\r\n    if val == fld:\r\n        ret = 1\r\n    else:\r\n        ret = 0\r\n    fld = val\r\n    return ret\r\n#__esri_field_calculator_splitter__  # used by *.cal files\r\n#seq_dup(!Test!)   # copy to expression section\r\n\r\n# (2) ... CalculateField format for use in scripts.\r\n#     Uncomment in your IDE, insert the following lines into your script----\r\n#\r\n#import arcpy\r\n#expr = \'\'\'\r\n#fld = """"\r\n#def seq_dup(val):\r\n#    """"""sequential duplicate checks""""""\r\n#    global fld\r\n#    if val == fld:\r\n#        ret = 1\r\n#    else:\r\n#        ret = 0\r\n#    fld = val\\\r\n#    return ret\r\n#\'\'\'\r\n#arcpy.management.CalculateField(""f1"", ""IndFld"", ""seq_dup(!Test!),\r\n#                                 ""PYTHON_9.3"",\r\n#                                 expression=expr)\r\n# ---- End of CalculateField section\r\n\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    """"""\r\n    import inspect\r\n    s = """".join([i for i in inspect.getsourcelines(seq_dup)[0]])\r\n    s = ""{}\\n{}"".format(\'fld = """"\', s)\r\n    print(s)\r\n'"
field_calculator/seq_group.py,0,"b'""""""Specify the number of records that you want to group in \'val\'\n\'id\' - a field containing sequential numbers or even random\nnumbers.\nThe number in \'id\' is divided by \'val\' and incremented if the\nmodulus == 0\n""""""\ncnt = 0\ndef seq_group(id, val):\n    global cnt\n    if id % val == 0 :\n        cnt += 1\n    return cnt\n'"
field_calculator/sequential_count.py,0,"b'""\x00""\x00""\x00\r\x00\n\x00C\x00o\x00u\x00n\x00t\x00 \x00t\x00h\x00e\x00 \x00n\x00u\x00m\x00b\x00e\x00r\x00 \x00o\x00f\x00 \x00\'\x00v\x00a\x00l\x00\'\x00s\x00 \x00t\x00h\x00a\x00t\x00 \x00e\x00x\x00i\x00s\x00t\x00 \x00i\x00n\x00 \x00a\x00 \x00f\x00i\x00e\x00l\x00d\x00 \x00a\x00n\x00d\x00 \x00n\x00u\x00m\x00b\x00e\x00r\x00 \x00a\x00n\x00d\x00 \x00f\x00o\x00r\x00m\x00a\x00t\x00\r\x00\n\x00t\x00h\x00e\x00m\x00,\x00 \x00i\x00n\x00c\x00r\x00e\x00m\x00e\x00n\x00t\x00i\x00n\x00g\x00 \x00t\x00h\x00e\x00 \x00n\x00u\x00m\x00b\x00e\x00r\x00 \x00e\x00a\x00c\x00h\x00 \x00t\x00i\x00m\x00e\x00 \x00o\x00n\x00e\x00 \x00i\x00s\x00 \x00f\x00o\x00u\x00n\x00d\x00\r\x00\n\x00""\x00""\x00""\x00\r\x00\n\x00o\x00l\x00d\x00 \x00=\x00 \x00""\x00""\x00\r\x00\n\x00c\x00n\x00t\x00 \x00=\x00 \x000\x00\r\x00\n\x00d\x00e\x00f\x00 \x00s\x00e\x00q\x00_\x00c\x00o\x00u\x00n\x00t\x00(\x00v\x00a\x00l\x00)\x00:\x00\r\x00\n\x00 \x00 \x00 \x00 \x00g\x00l\x00o\x00b\x00a\x00l\x00 \x00o\x00l\x00d\x00\r\x00\n\x00 \x00 \x00 \x00 \x00g\x00l\x00o\x00b\x00a\x00l\x00 \x00c\x00n\x00t\x00\r\x00\n\x00 \x00 \x00 \x00 \x00i\x00f\x00 \x00o\x00l\x00d\x00 \x00=\x00=\x00 \x00v\x00a\x00l\x00:\x00\r\x00\n\x00 \x00 \x00 \x00 \x00 \x00 \x00 \x00 \x00c\x00n\x00t\x00 \x00+\x00=\x00 \x001\x00\r\x00\n\x00 \x00 \x00 \x00 \x00 \x00 \x00 \x00 \x00r\x00e\x00t\x00 \x00=\x00 \x00""\x00{\x00}\x00 \x00{\x00:\x000\x004\x00.\x000\x00f\x00}\x00""\x00.\x00f\x00o\x00r\x00m\x00a\x00t\x00(\x00v\x00a\x00l\x00,\x00 \x00c\x00n\x00t\x00)\x00\r\x00\n\x00 \x00 \x00 \x00 \x00e\x00l\x00s\x00e\x00:\x00\r\x00\n\x00 \x00 \x00 \x00 \x00 \x00 \x00 \x00 \x00c\x00n\x00t\x00 \x00=\x00 \x000\x00\r\x00\n\x00 \x00 \x00 \x00 \x00 \x00 \x00 \x00 \x00r\x00e\x00t\x00 \x00=\x00 \x00""\x00{\x00}\x00 \x00{\x00:\x000\x004\x00.\x000\x00f\x00}\x00""\x00.\x00f\x00o\x00r\x00m\x00a\x00t\x00(\x00v\x00a\x00l\x00,\x00 \x00c\x00n\x00t\x00)\x00\r\x00\n\x00 \x00 \x00 \x00 \x00o\x00l\x00d\x00 \x00=\x00 \x00v\x00a\x00l\x00\r\x00\n\x00 \x00 \x00 \x00 \x00r\x00e\x00t\x00u\x00r\x00n\x00 \x00r\x00e\x00t\x00\r\x00\n\x00_\x00_\x00e\x00s\x00r\x00i\x00_\x00f\x00i\x00e\x00l\x00d\x00_\x00c\x00a\x00l\x00c\x00u\x00l\x00a\x00t\x00o\x00r\x00_\x00s\x00p\x00l\x00i\x00t\x00t\x00e\x00r\x00_\x00_\x00\r\x00\n\x00s\x00e\x00q\x00_\x00c\x00o\x00u\x00n\x00t\x00(\x00!\x00T\x00e\x00s\x00t\x00!\x00)\x00'"
field_calculator/sequential_dups.py,0,"b'# -*- coding: UTF-8 -*-\r\n# (1) ... field calculator code block\r\n#     Save as a *.cal file for loading directly into the code block.\r\n#     Uncomment the last 2 lines to have the seq_dup line split out\r\n#\r\nfld = """"\r\ndef seq_dup(val):\r\n    """"""sequential duplicate checks""""""\r\n    global fld\r\n    if val == fld:\r\n        ret = 1\r\n    else:\r\n        ret = 0\r\n    fld = val\r\n    return ret\r\n#__esri_field_calculator_splitter__  # used by *.cal files\r\n#seq_dup(!Test!)   # copy to expression section\r\n\r\n# (2) ... CalculateField format for use in scripts.\r\n#     Uncomment in your IDE, insert the following lines into your script----\r\n#\r\n#import arcpy\r\n#expr = \'\'\'\r\n#fld = """"\r\n#def seq_dup(val):\r\n#    """"""sequential duplicate checks""""""\r\n#    global fld\r\n#    if val == fld:\r\n#        ret = 1\r\n#    else:\r\n#        ret = 0\r\n#    fld = val\r\n#    return ret\r\n#\'\'\'\r\n#arcpy.management.CalculateField(""f1"", ""IndFld"", ""seq_dup(!Test!),\r\n#                                 ""PYTHON_9.3"",\r\n#                                 expression=expr)\r\n# ---- End of CalculateField section\r\n\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    """"""\r\n    import inspect\r\n    s = """".join([i for i in inspect.getsourcelines(seq_dup)[0]])\r\n    s = ""{}\\n{}"".format(\'fld = """"\', s)\r\n    print(s)\r\n'"
field_calculator/sequential_id.py,0,b'c\x00n\x00t\x00 \x00=\x00 \x000\x00\r\x00\n\x00d\x00e\x00f\x00 \x00s\x00e\x00q\x00_\x00c\x00o\x00u\x00n\x00t\x00(\x00v\x00a\x00l\x00)\x00:\x00\r\x00\n\x00 \x00 \x00 \x00 \x00g\x00l\x00o\x00b\x00a\x00l\x00 \x00c\x00n\x00t\x00\r\x00\n\x00 \x00 \x00 \x00 \x00i\x00f\x00 \x00c\x00n\x00t\x00 \x00>\x00=\x00 \x00v\x00a\x00l\x00 \x00:\x00\r\x00\n\x00 \x00 \x00 \x00 \x00 \x00 \x00 \x00 \x00c\x00n\x00t\x00 \x00+\x00=\x00 \x001\x00\r\x00\n\x00 \x00 \x00 \x00 \x00r\x00e\x00t\x00u\x00r\x00n\x00 \x00c\x00n\x00t\x00\r\x00\n\x00\r\x00\n\x00_\x00_\x00e\x00s\x00r\x00i\x00_\x00f\x00i\x00e\x00l\x00d\x00_\x00c\x00a\x00l\x00c\x00u\x00l\x00a\x00t\x00o\x00r\x00_\x00s\x00p\x00l\x00i\x00t\x00t\x00e\x00r\x00_\x00_\x00\r\x00\n\x00s\x00e\x00q\x00_\x00c\x00o\x00u\x00n\x00t\x00(\x000\x00)\x00'
field_calculator/shift_features.py,0,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nshift_features\r\n==============\r\n\r\nScript :   shift_features.py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified : 2018-07-25\r\n""""""\r\n\r\nimport arcpy\r\n\r\ndef shift_features(in_features, x_shift=None, y_shift=None):\r\n    """"""\r\n    Shifts features by an x and/or y value. The shift values are in\r\n    the units of the in_features coordinate system.\r\n\r\n    Parameters:\r\n    in_features: string\r\n        An existing feature class or feature layer.  If using a\r\n        feature layer with a selection, only the selected features\r\n        will be modified.\r\n\r\n    x_shift: float\r\n        The distance the x coordinates will be shifted.\r\n\r\n    y_shift: float\r\n        The distance the y coordinates will be shifted.\r\n    """"""\r\n\r\n    with arcpy.da.UpdateCursor(in_features, [\'SHAPE@XY\']) as cursor:\r\n        for row in cursor:\r\n            cursor.updateRow([[row[0][0] + (x_shift or 0),\r\n                               row[0][1] + (y_shift or 0)]])\r\n\r\n    return'"
field_calculator/string_defs.py,0,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   string_defs.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2017-06-17\r\n:Purpose:  tools for working strings\r\n:Useage:\r\n:  These are mini-onliners or so\r\n\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\n\r\na = \'A string with numbers 10   20 in it\'\r\n\r\nkeep_text = """".join([i for i in a if i.isalpha() or i == "" ""]).strip()\r\n\r\nstrip_spaces = "" "".join([i.strip() for i in a.split("" "") if i != """"])\r\n\r\nkeep_numb = """".join([i for i in a if i.isdigit() or i == "" ""]).strip()\r\n\r\nnum_csv = "", "".join([i for i in a.split() if i.isdigit() ]).strip()\r\n\r\nfrmt = """"""\r\nInput string......... {}\r\n\r\nJust text ........... {}\r\nStrip extra spaces .. {}\r\nJust numbers ........ {}\r\nNumbers to csv ...... {}\r\n\r\n""""""\r\nargs = [a, keep_text, strip_spaces, keep_numb, num_csv]\r\nprint(frmt.format(*args))\r\n'"
field_calculator/strip_concatenate.py,0,"b'd\x00e\x00f\x00 \x00s\x00t\x00r\x00i\x00p\x00_\x00c\x00o\x00n\x00c\x00a\x00t\x00e\x00n\x00a\x00t\x00e\x00(\x00i\x00n\x00_\x00f\x00l\x00d\x00s\x00,\x00 \x00s\x00t\x00r\x00i\x00p\x00_\x00l\x00i\x00s\x00t\x00=\x00[\x00""\x00 \x00""\x00,\x00 \x00""\x00,\x00""\x00,\x00 \x00N\x00o\x00n\x00e\x00]\x00)\x00:\x00\r\x00\n\x00 \x00 \x00 \x00 \x00""\x00""\x00""\x00P\x00r\x00o\x00v\x00i\x00d\x00e\x00 \x00t\x00h\x00e\x00 \x00f\x00i\x00e\x00l\x00d\x00s\x00 \x00a\x00s\x00 \x00a\x00 \x00l\x00i\x00s\x00t\x00 \x00i\x00e\x00 \x00[\x00a\x00,\x00 \x00b\x00,\x00 \x00c\x00]\x00 \x00t\x00o\x00 \x00s\x00t\x00r\x00i\x00p\x00 \x00s\x00p\x00a\x00c\x00e\x00s\x00\r\x00\n\x00 \x00 \x00 \x00 \x00:\x00 \x00a\x00n\x00d\x00 \x00r\x00e\x00m\x00o\x00v\x00e\x00 \x00n\x00u\x00l\x00l\x00s\x00\r\x00\n\x00 \x00 \x00 \x00 \x00:\x00 \x00u\x00s\x00e\x00:\x00 \x00p\x00y\x00t\x00h\x00o\x00n\x00 \x00p\x00a\x00r\x00s\x00e\x00r\x00\r\x00\n\x00 \x00 \x00 \x00 \x00:\x00 \x00s\x00y\x00n\x00t\x00a\x00x\x00:\x00 \x00s\x00t\x00r\x00i\x00p\x00_\x00s\x00t\x00u\x00f\x00f\x00(\x00\'\x00!\x00a\x00!\x00,\x00 \x00!\x00b\x00!\x00,\x00 \x00!\x00c\x00!\x00]\x00)\x00 \x00a\x00s\x00s\x00u\x00m\x00e\x00d\x00 \x00f\x00i\x00e\x00l\x00d\x00 \x00n\x00a\x00m\x00e\x00s\x00\r\x00\n\x00 \x00 \x00 \x00 \x00""\x00""\x00""\x00\r\x00\n\x00 \x00 \x00 \x00 \x00f\x00i\x00x\x00e\x00d\x00 \x00=\x00 \x00[\x00]\x00\r\x00\n\x00 \x00 \x00 \x00 \x00f\x00m\x00t\x00 \x00=\x00 \x00[\x00]\x00\r\x00\n\x00 \x00 \x00 \x00 \x00f\x00o\x00r\x00 \x00i\x00 \x00i\x00n\x00 \x00i\x00n\x00_\x00f\x00l\x00d\x00s\x00:\x00\r\x00\n\x00 \x00 \x00 \x00 \x00 \x00 \x00 \x00 \x00i\x00f\x00 \x00i\x00 \x00n\x00o\x00t\x00 \x00i\x00n\x00 \x00s\x00t\x00r\x00i\x00p\x00_\x00l\x00i\x00s\x00t\x00:\x00\r\x00\n\x00 \x00 \x00 \x00 \x00 \x00 \x00 \x00 \x00 \x00 \x00 \x00 \x00f\x00i\x00x\x00e\x00d\x00.\x00a\x00p\x00p\x00e\x00n\x00d\x00(\x00i\x00)\x00\r\x00\n\x00 \x00 \x00 \x00 \x00 \x00 \x00 \x00 \x00 \x00 \x00 \x00 \x00f\x00m\x00t\x00.\x00a\x00p\x00p\x00e\x00n\x00d\x00(\x00""\x00{\x00}\x00""\x00)\x00\r\x00\n\x00 \x00 \x00 \x00 \x00f\x00r\x00m\x00t\x00 \x00=\x00 \x00""\x00 \x00""\x00.\x00j\x00o\x00i\x00n\x00(\x00[\x00f\x00 \x00f\x00o\x00r\x00 \x00f\x00 \x00i\x00n\x00 \x00f\x00m\x00t\x00]\x00)\x00\r\x00\n\x00 \x00 \x00 \x00 \x00f\x00r\x00m\x00t\x00.\x00s\x00t\x00r\x00i\x00p\x00(\x00)\x00 \x00 \x00 \x00 \x00\r\x00\n\x00 \x00 \x00 \x00 \x00f\x00l\x00d\x00s\x00 \x00=\x00 \x00[\x00s\x00t\x00r\x00(\x00i\x00)\x00.\x00s\x00t\x00r\x00i\x00p\x00(\x00)\x00 \x00f\x00o\x00r\x00 \x00i\x00 \x00i\x00n\x00 \x00f\x00i\x00x\x00e\x00d\x00]\x00\r\x00\n\x00 \x00 \x00 \x00 \x00r\x00e\x00s\x00u\x00l\x00t\x00 \x00=\x00 \x00f\x00r\x00m\x00t\x00.\x00f\x00o\x00r\x00m\x00a\x00t\x00(\x00*\x00f\x00i\x00x\x00e\x00d\x00)\x00\r\x00\n\x00 \x00 \x00 \x00 \x00r\x00e\x00t\x00u\x00r\x00n\x00 \x00r\x00e\x00s\x00u\x00l\x00t\x00\r\x00\n\x00_\x00_\x00e\x00s\x00r\x00i\x00_\x00f\x00i\x00e\x00l\x00d\x00_\x00c\x00a\x00l\x00c\x00u\x00l\x00a\x00t\x00o\x00r\x00_\x00s\x00p\x00l\x00i\x00t\x00t\x00e\x00r\x00_\x00_\x00\r\x00\n\x00s\x00t\x00r\x00i\x00p\x00_\x00c\x00o\x00n\x00c\x00a\x00t\x00e\x00n\x00a\x00t\x00e\x00(\x00)\x00'"
field_calculator/strip_time.py,0,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   strip_time.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2017-09-14\r\n:Purpose:  strip time of of a date-time\r\n:Useage:\r\n:\r\n:References:\r\n:\r\n:---------------------------------------------------------------------:\r\n""""""\r\ndef strip_time(fld):\r\n    """"""input a date field, strip off the time and format""""""\r\n    if fld is not None:\r\n        lst = [int(i) for i in (str(fld).split("" "")[0]).split(""-"")]\r\n        return ""{}-{:02.0f}-{:02.0f}"".format(*lst)\r\n    else:\r\n        return None\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """""" A simple test """"""\r\n    from datetime import datetime\r\n    n = str(datetime.now())\r\n    print(strip_time(n))'"
frequency_statistics/frequency.py,13,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n=========\r\nfrequency\r\n=========\r\n\r\nScript :\r\n    frequency.py\r\n\r\nAuthor :\r\n    Dan_Patterson@carleton.ca\r\n\r\nModified : 2019-06-22\r\n    Original, 2018\r\n\r\nPurpose :\r\n    To supplant the Frequency tool for those that don\'t have an\r\n    advanced license.  Performs a frequency count of unique combinations of\r\n    fields and a set of statistical summary parameters.\r\n\r\nUseage :\r\n    Load the toolbox into Pro and run the tool script from there.\r\n\r\nReferences\r\n----------\r\n`Frequency\r\n<https://pro.arcgis.com/en/pro-app/tool-reference/analysis/frequency.htm>`_.\r\nLink to ArcGIS Pro help files current as of modified date.\r\n\r\nNotes\r\n-----\r\n\r\n>>> to_array = arcpy.da.TableToNumPyArray(r""C:\\folder\\sample.dbf"", ""*"")\r\n>>> arcpy.da.NumPyArrayToTable(from_array, r""C:\\folder_tbl\\test.gdb\\out"")\r\n\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nimport numpy.lib.recfunctions as rfn\r\nimport arcpy\r\narcpy.env.overwriteOutput = True\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\r\n      \'float\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\n# ============================================================================\r\n# ---- frequency section\r\n#\r\nmsg0 = """"""\r\nEither you failed to specify the geodatabase location and filename properly\r\nor you had flotsam, including spaces, in the path, like...\\n\r\n  {}\\n\r\nCreate a safe path and try again...\\n\r\n`Filenames and paths in Python`\r\nhttps://community.esri.com/blogs/dan_patterson/2016/08/14/filenames-and\r\n-file-paths-in-python.\r\n""""""\r\ndef check_path(out_fc):\r\n    """"""Check for a filegeodatabase and a filename""""""\r\n    _punc_ = \'!""#$%&\\\'()*+,-;<=>?@[]^`{|}~ \'\r\n    flotsam = "" "".join([i for i in _punc_]) + "" ... plus the `space`""\r\n    msg = msg0.format(flotsam)\r\n    if np.any([i in out_fc for i in _punc_]):\r\n        return (None, msg)\r\n    pth = out_fc.split(""\\\\"")\r\n    if len(pth) == 1:\r\n        return (None, msg)\r\n    name = pth[-1]\r\n    gdb = ""\\\\"".join(pth[:-1])\r\n    if gdb[-4:] != \'.gdb\':\r\n        return (None, msg)\r\n    return gdb, name\r\n\r\n\r\ndef freq(a, cls_flds=None, stat_fld=None):\r\n    """"""Frequency and crosstabulation\r\n\r\n    Parameters\r\n    ----------\r\n    a : array\r\n        A structured array\r\n    flds : field\r\n        Fields to use in the analysis\r\n\r\n    Notes\r\n    -----\r\n    1. slice the input array by the classification fields\r\n    2. sort the sliced array using the flds as sorting keys\r\n    3. use unique on the sorted array to return the results and the counts\r\n\r\n    >>> np.unique(ar, return_index=False, return_inverse=False,\r\n    ...           return_counts=True, axis=None)\r\n    """"""\r\n    if stat_fld is None:\r\n        a = a[cls_flds]  # (1) It is actually faster to slice the whole table\r\n    else:\r\n        all_flds = cls_flds + [stat_fld]\r\n        a = a[all_flds]\r\n    idx = np.argsort(a, axis=0, order=cls_flds)  # (2)\r\n    a_sort = a[idx]\r\n    uni, inv, cnts = np.unique(a_sort[cls_flds], False,\r\n                               True, return_counts=True)  # (3)\r\n    out_flds = ""Counts""\r\n    out_data = cnts\r\n    if stat_fld is not None:\r\n        splitter = np.where(np.diff(inv) == 1)[0] + 1\r\n        a0 = a_sort[stat_fld]\r\n        splits = np.split(a0, splitter)\r\n        sums = np.asarray([np.nansum(i.tolist()) for i in splits])\r\n        nans = np.asarray([np.sum(np.isnan(i.tolist())) for i in splits])\r\n        mins = np.asarray([np.nanmin(i.tolist()) for i in splits])\r\n        means = np.asarray([np.nanmean(i.tolist()) for i in splits])\r\n        maxs = np.asarray([np.nanmax(i.tolist()) for i in splits])\r\n        out_flds = [out_flds, stat_fld + ""_sums"", stat_fld + ""_NaN"",\r\n                    stat_fld + ""_min"", stat_fld + ""_mean"", stat_fld + ""_max""]\r\n        out_data = [out_data, sums, nans, mins, means, maxs]\r\n    out = rfn.append_fields(uni, names=out_flds, data=out_data, usemask=False)\r\n    return out\r\n\r\n# ---- testing and tool section ----------------------------------------------\r\n#\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    in_tbl = r""C:\\Arc_projects\\Free_Tools\\Free_tools.gdb\\sample_10k""\r\n    all_flds = ""*""\r\n    cls_flds = \'County;Town_class\'  # Facility, Time\r\n    stat_fld = \'Age\'\r\nelse:\r\n    testing = False\r\n    in_tbl = sys.argv[1]    # input table\r\n    cls_flds = sys.argv[2]  # classification field(s)\r\n    stat_fld = sys.argv[3]\r\n    out_tbl = sys.argv[4]   # results table\r\n\r\n# ---- common tasks\r\ncls_flds = cls_flds.split("";"")  # multiple to list, make a singleton a list\r\nif stat_fld in (None, \'NoneType\', """"):\r\n    stat_fld = None\r\na = arcpy.da.TableToNumPyArray(in_tbl, ""*"")  # use the whole array\r\nout = freq(a, cls_flds, stat_fld)  # do freq analysis\r\n\r\n# ---- create the output array and return the table\r\n# ----\r\nif not testing:\r\n    result = check_path(out_tbl)\r\n    if result[0] is None:\r\n        msg = ""...\\n{}\\n..."".format(result[1])\r\n        print(msg)\r\n        arcpy.AddMessage(msg)\r\n    else:\r\n        gdb, name = result\r\n    arcpy.da.NumPyArrayToTable(out, out_tbl)\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Do nothing for now.\r\n    """"""\r\n\r\n'"
geometry_tools/angles.py,13,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   .py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2017-xx-xx\r\n:Purpose:  tools for working with numpy arrays\r\n:Useage:\r\n:\r\n:References:\r\n: - https://en.wikipedia.org/wiki/Regular_polygon\r\n: sum of interior angles\r\n:   (n-2) * 180, where n is the number of sides\r\n:   n = 3  180 triangle\r\n:   n = 4  360 rectangle\r\n:   n = 5  540 pentagram\r\n:   n = 6  720 hexagram\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nfrom textwrap import dedent\r\nimport numpy as np\r\nimport arcpy\r\n\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\r\n      \'float\': \'{: 0.3f}\'.format}\r\n\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\n# ---- array functions -------------------------------------------------------\r\n#\r\ndef fc_info(in_fc, prn=False):\r\n    """"""Return basic featureclass information, including...\r\n    :\r\n    : shp_fld  - field name which contains the geometry object\r\n    : oid_fld  - the object index/id field name\r\n    : SR       - spatial reference object (use SR.name to get the name)\r\n    : shp_type - shape type (Point, Polyline, Polygon, Multipoint, Multipatch)\r\n    : - others: \'areaFieldName\', \'baseName\', \'catalogPath\',\'featureType\',\r\n    :   \'fields\', \'hasOID\', \'hasM\', \'hasZ\', \'path\'\r\n    : - all_flds =[i.name for i in desc[\'fields\']]\r\n    """"""\r\n    desc = arcpy.da.Describe(in_fc)\r\n    args = [\'shapeFieldName\', \'OIDFieldName\', \'shapeType\', \'spatialReference\']\r\n    shp_fld, oid_fld, shp_type, SR = [desc[i] for i in args]\r\n    if prn:\r\n        frmt = ""FeatureClass:\\n   {}"".format(in_fc)\r\n        f = ""\\n{!s:<16}{!s:<14}{!s:<10}{!s:<10}""\r\n        frmt += f.format(*args)\r\n        frmt += f.format(shp_fld, oid_fld, shp_type, SR.name)\r\n        tweet(frmt)\r\n    else:\r\n        return shp_fld, oid_fld, shp_type, SR\r\n\r\n\r\ndef _geo_array(polys):\r\n    """"""Convert polygon objects to arrays\r\n    """"""\r\n    arrays = [np.asarray(pt.__geo_interface__[\'coordinates\']).squeeze()\r\n              for pt in polys]  # for p in pt]\r\n    return arrays\r\n\r\n\r\ndef _get_shapes(in_fc):\r\n    """"""Get shapes from a featureclass, in_fc, using SHAPE@ returning\r\n    :  [<Polygon object at....>, ... (<Polygon object at....>]\r\n    """"""\r\n    with arcpy.da.SearchCursor(in_fc, \'SHAPE@\') as cursor:\r\n        a = [row[0] for row in cursor]\r\n    return a\r\n\r\n\r\ndef arcpnts_poly(in_, out_type=\'Polygon\', SR=None):\r\n    """"""Convert arcpy Point lists to poly* features\r\n    : out_type - either \'Polygon\' or \'Polyline\'\r\n    :\r\n    """"""\r\n    s = []\r\n    for i in in_:\r\n        arr = arcpy.Array(i)\r\n        if out_type == \'Polyline\':\r\n            g = arcpy.Polyline(arr, SR)\r\n        elif out_type == \'Polygon\':\r\n            g = arcpy.Polygon(arr, SR)\r\n        elif out_type == \'Points\':\r\n            g = arcpy.arcpy.Multipoint(arr[0], SR)\r\n        s.append(g)\r\n    return s\r\n\r\n\r\ndef angles_poly(a, inside=True, in_deg=True):\r\n    """"""Sequential angles from a poly* shape\r\n    : a - an array of points, derived from a polygon/polyline geometry\r\n    : inside - determine inside angles, outside if False\r\n    : in_deg - convert to degrees from radians\r\n    :\r\n    :Notes:\r\n    :-----\r\n    : 2 points - subtract 2nd and 1st points, effectively making the\r\n    :  calculation relative to the origin and x axis, aka... slope\r\n    : n points - sequential angle between 3 points\r\n    : - Check whether 1st and last points are duplicates.\r\n    :   \'True\' for polygons and closed loop polylines, it is checked using\r\n    :   np.allclose(a[0], a[-1])  # check first and last point\r\n    : - a rolling tuple is constructed to produce the point triplets\r\n    :   r = (-1,) + tuple(range(len(a))) + (0,)\r\n    :   for np.arctan2(np.linalg.norm(np.cross(ba, bc)), np.dot(ba, bc))\r\n    :\r\n    :Reference:\r\n    :---------\r\n    : https://stackoverflow.com/questions/21483999/\r\n    :         using-atan2-to-find-angle-between-two-vectors\r\n    :  *** keep to convert object to array\r\n    : a - a shape from the shape field\r\n    : a = p1.getPart()\r\n    : b =np.asarray([(i.X, i.Y) if i is not None else ()\r\n    :                for j in a for i in j])\r\n    """"""\r\n    #a = a.getPart()\r\n    #a = np.asarray([[i.X, i.Y] for j in a for i in j])\r\n    if len(a) < 2:\r\n        return None\r\n    elif len(a) == 2:  # **** check\r\n        ba = a[1] - a[0]\r\n        return np.arctan2(*ba[::-1])\r\n    else:\r\n        angles = []\r\n        if np.allclose(a[0], a[-1]):  # closed loop\r\n            a = a[:-1]\r\n            r = (-1,) + tuple(range(len(a))) + (0,)\r\n        else:\r\n            r = tuple(range(len(a)))\r\n        for i in range(len(r)-2):\r\n            p0, p1, p2 = a[r[i]], a[r[i+1]], a[r[i+2]]\r\n            ba = p1 - p0\r\n            bc = p1 - p2\r\n            cr = np.cross(ba, bc)\r\n            dt = np.dot(ba, bc)\r\n            ang = np.arctan2(np.linalg.norm(cr), dt)\r\n            angles.append(ang)\r\n    if in_deg:\r\n        angles = np.degrees(angles)\r\n    return angles\r\n\r\n\r\ndef call_angles(a):\r\n    """"""Call angles for each shape\r\n    """"""\r\n    out = []\r\n    for i in a:\r\n        out.append(angles_poly(i, inside=True, in_deg=True))\r\n    return out\r\n\r\n\r\ndef prn_report(arrs, out):\r\n    """"""Print a report summarizing the output\r\n    """"""\r\n    hdr = """"""\r\n    :----------------------------------------------------------------------\r\n    :Angle report....\r\n    """"""\r\n    frmt = """"""\r\n    ({}) number of angles... ({})\r\n    :array points...\r\n    {}\\n\r\n    :interior angles {}\r\n    :sum interior... {}\r\n    """"""\r\n    print(dedent(""\\n{}"").format(hdr))\r\n    cnt = 0\r\n    for i in arrs:\r\n        args = [cnt, len(i), i, out[cnt], sum(out[cnt])]\r\n        prn = [str(i) for i in args]\r\n        print(dedent(frmt).format(*prn))\r\n        cnt += 1\r\n\r\n# ------------------------------------------------------------------------\r\n# (1) ---- Checks to see if running in test mode or from a tool ----------\r\ndef _demo():\r\n    """"""run when script is in demo mode""""""\r\n    pth = script.replace(\'/angles.py\', \'\')\r\n    in_fc = \'C:/Git_Dan/a_Data/arcpytools_demo.gdb/three_shapes\'\r\n#    in_fc = pth + \'/geom_data.gdb/three_shapes\'\r\n#    in_fc = r""C:\\Git_Dan\\a_Data\\testdata.gdb\\Carp_5x5""   # full 25 polygons\r\n#    in_fc = r""C:\\Git_Dan\\a_Data\\arcpytools_demo.gdb\\xy1000_tree""\r\n    out_fc = pth + \'/geom_data.gdb/x\'\r\n    out_type = \'Polygon\'\r\n    testing = True\r\n    return in_fc, out_fc, out_type, testing\r\n\r\n\r\ndef _tool():\r\n    """"""run when script is from a tool""""""\r\n    in_fc = sys.argv[1]  #\r\n    out_fc = sys.argv[2]  #\r\n    out_type = sys.argv[3]  # Polygon, Polyline are options\r\n    testing = False\r\n    return in_fc, out_fc, out_type, testing\r\n\r\n# ---- main block ------------------------------------------------------------\r\n#\r\n# (1) check to see if in demo or tool mode\r\n# (2) obtain fc information\r\n# (3) split the fc into two arrays, one geometry, the 2nd attributes\r\n# (4) obtain the shapes and densify\r\n# (5) optionally produce the output fc\r\n\r\nif len(sys.argv) == 1:\r\n    in_fc, out_fc, out_type, testing = _demo()\r\nelse:\r\n    in_fc, out_fc, out_type, testing = _tool()\r\n\r\nshp_fld, oid_fld, shp_type, SR = fc_info(in_fc)\r\n\r\n\r\n# ---- produce output --------------------------------------------------------\r\n\r\npolys = _get_shapes(in_fc)\r\narrs =  _geo_array(polys)\r\nout = call_angles(arrs)\r\n#out = angles_poly(arrs, inside=True, in_deg=True)  # use for xy1000_tree only\r\n#p0, p1, p2 = polys\r\n#b = _get_attributes(in_fc)\r\n#out = densify([p0, p1], fact=2, sp_ref=SR)\r\n#arrs = _un(out, None)\r\n#out_shps = arcpnts_poly(out, out_type=out_type, SR=SR)\r\n#if not testing:\r\n#    if arcpy.Exists(out_fc):\r\n#        arcpy.Delete_management(out_fc)\r\n#    arcpy.CopyFeatures_management(out_shps, out_fc)\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n'"
geometry_tools/densify_geom.py,14,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   densify_geom.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2018-01-16\r\n:Purpose:  Densify geometry by a factor.\r\n:Notes:\r\n:  Uses functions from \'arraytools\'.  These have been consolidated here.\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\n# ---- array functions -------------------------------------------------------\r\n#\r\ndef _flat_(a_list, flat_list=None):\r\n    """"""Change the isinstance as appropriate\r\n    :  Flatten an object using recursion\r\n    :  see: itertools.chain() for an alternate method of flattening.\r\n    """"""\r\n    if flat_list is None:\r\n        flat_list = []\r\n    for item in a_list:\r\n        if isinstance(item, (list, tuple, np.ndarray, np.void)):\r\n            _flat_(item, flat_list)\r\n        else:\r\n            flat_list.append(item)\r\n    return flat_list\r\n\r\n\r\ndef _O_nd(obj, out=None):\r\n    """"""Flatten type \'O\' arrays to ndarray, using recursion\r\n    :Note: append retains internal shape, extend will flatten\r\n    :  nested lists into a list\r\n    """"""\r\n    if out is None:\r\n        out = []\r\n    sub_out = []\r\n    for el in obj:\r\n        el = np.asarray(el)\r\n        if el.dtype.kind in (\'O\', \'V\'):\r\n            sub_out.append(_O_nd(el, out))  # ---- recursion needed ---\r\n        else:\r\n            out.extend(el)  # was append\r\n    return out\r\n\r\ndef _densify_2D(a, fact=2):\r\n    """"""Densify a 2D array using np.interp.\r\n    :fact - the factor to density the line segments by\r\n    :Notes\r\n    :-----\r\n    :original construction of c rather than the zero\'s approach\r\n    :  c0 = c0.reshape(n, -1)\r\n    :  c1 = c1.reshape(n, -1)\r\n    :  c = np.concatenate((c0, c1), 1)\r\n    """"""\r\n    # Y = a changed all the y\'s to a\r\n    a = np.squeeze(a)\r\n    n_fact = len(a) * fact\r\n    b = np.arange(0, n_fact, fact)\r\n    b_new = np.arange(n_fact - 1)     # Where you want to interpolate\r\n    c0 = np.interp(b_new, b, a[:, 0])\r\n    c1 = np.interp(b_new, b, a[:, 1])\r\n    n = c0.shape[0]\r\n    c = np.zeros((n, 2))\r\n    c[:, 0] = c0\r\n    c[:, 1] = c1\r\n    return c\r\n\r\n\r\n# ---- featureclass functions ------------------------------------------------\r\n#\r\ndef fc_info(in_fc, prn=False):\r\n    """"""Return basic featureclass information, including...\r\n    :\r\n    : shp_fld  - field name which contains the geometry object\r\n    : oid_fld  - the object index/id field name\r\n    : SR       - spatial reference object (use SR.name to get the name)\r\n    : shp_type - shape type (Point, Polyline, Polygon, Multipoint, Multipatch)\r\n    : - others: \'areaFieldName\', \'baseName\', \'catalogPath\',\'featureType\',\r\n    :   \'fields\', \'hasOID\', \'hasM\', \'hasZ\', \'path\'\r\n    : - all_flds =[i.name for i in desc[\'fields\']]\r\n    """"""\r\n    desc = arcpy.da.Describe(in_fc)\r\n    args = [\'shapeFieldName\', \'OIDFieldName\', \'shapeType\', \'spatialReference\']\r\n    shp_fld, oid_fld, shp_type, SR = [desc[i] for i in args]\r\n    if prn:\r\n        frmt = ""FeatureClass:\\n   {}"".format(in_fc)\r\n        f = ""\\n{!s:<16}{!s:<14}{!s:<10}{!s:<10}""\r\n        frmt += f.format(*args)\r\n        frmt += f.format(shp_fld, oid_fld, shp_type, SR.name)\r\n        tweet(frmt)\r\n    else:\r\n        return shp_fld, oid_fld, shp_type, SR\r\n\r\n\r\ndef tweet(msg):\r\n    """"""Print a message for both arcpy and python.\r\n    : msg - a text message\r\n    """"""\r\n    m = ""\\n{}\\n"".format(msg)\r\n    arcpy.AddMessage(m)\r\n    print(m)\r\n    print(arcpy.GetMessages())\r\n\r\n\r\ndef _get_shapes(in_fc):\r\n    """"""Get shapes from a featureclass, in_fc, using SHAPE@ returning\r\n    :  [<Polygon object at....>, ... (<Polygon object at....>]\r\n    """"""\r\n    with arcpy.da.SearchCursor(in_fc, \'SHAPE@\') as cursor:\r\n        a = [row[0] for row in cursor]\r\n    return a\r\n\r\n\r\ndef _ndarray(in_fc, to_pnts=True, flds=None, SR=None):\r\n    """"""Convert featureclass geometry (in_fc) to a structured ndarray including\r\n    :  options to select fields and specify a spatial reference.\r\n    :\r\n    :Requires:\r\n    :--------\r\n    : in_fc - input featureclass\r\n    : to_pnts - True, convert the shape to points. False, centroid returned.\r\n    : flds - \'*\' for all, others: \'Shape\',  [\'SHAPE@X\', \'SHAPE@Y\'], or specify\r\n    """"""\r\n    if flds is None:\r\n        flds = ""*""\r\n    if SR is None:\r\n        desc = arcpy.da.Describe(in_fc)\r\n        SR = desc[\'spatialReference\']\r\n    args = [in_fc, flds, None, SR, to_pnts, (None, None)]\r\n    cur = arcpy.da.SearchCursor(*args)\r\n    a = cur._as_narray()\r\n    del cur\r\n    return a\r\n\r\n\r\ndef _get_attributes(in_fc):\r\n    """"""Get the attributes of features, returns the centroid coordinates\r\n    :  as fields in the table.\r\n    """"""\r\n    dt_b = [(\'IDs\', \'<i4\'), (\'Xc\', \'<f8\'), (\'Yc\', \'<f8\')]\r\n    b = _ndarray(in_fc, to_pnts=False)\r\n    dt = [[n, t] for n, t in b.dtype.descr[2:]]\r\n    for i in dt:\r\n        if i[0] in [""Shape_Length"", ""Shape_Area"", ""Shape""]:\r\n            i[0] = i[0] + ""_orig""\r\n    dt = [tuple(i) for i in dt]\r\n    dt_b.extend(dt)\r\n    b.dtype = dt_b\r\n    return b\r\n\r\n\r\ndef obj_shapes(in_, SR):\r\n    """"""object array of coordinates to shapes""""""\r\n    s = []\r\n    for shps in in_:\r\n        tmp = []\r\n        if isinstance(shps, (list, tuple)):\r\n            for shp in shps:\r\n                shp = np.asarray(shp)\r\n                shp = shp.squeeze()\r\n                pnts = [arcpy.Point(*p) for p in shp]\r\n                tmp.append(pnts)\r\n            arr = arcpy.Array(pnts)\r\n        else:\r\n            arr = arcpy.Array([arcpy.Point(*p) for p in shps])\r\n        #\r\n        if out_type == \'Polyline\':\r\n            g = arcpy.Polyline(arr, SR)\r\n        elif out_type == \'Polygon\':\r\n            g = arcpy.Polygon(arr, SR)\r\n        s.append(g)\r\n    return s\r\n\r\n\r\ndef arcpnts_poly(in_, out_type=\'Polygon\', SR=None):\r\n    """"""Convert arcpy Point lists to poly* features\r\n    : out_type - either \'Polygon\' or \'Polyline\'\r\n    :\r\n    """"""\r\n    s = []\r\n    for i in in_:\r\n        for j in i:\r\n            if out_type == \'Polygon\':\r\n                g = arcpy.Polygon(arcpy.Array(j), SR)\r\n            elif out_type == \'Polyline\':\r\n                g = arcpy.Polyline(arcpy.Array(j), SR)\r\n            elif out_type == \'Points\':\r\n                j = _flat_(j)\r\n                g = arcpy.Multipoint(arcpy.Array(j), SR)  # check\r\n            s.append(g)\r\n    return s\r\n\r\n\r\ndef _convert(a, fact=2):\r\n    """"""Do the shape conversion for the array parts.  Calls to _densify_2D\r\n    """"""\r\n    out = []\r\n    parts = len(a)\r\n    for i in range(parts):\r\n        sub_out = []\r\n        p = np.asarray(a[i]).squeeze()\r\n        if p.ndim == 2:\r\n            shp = _densify_2D(p, fact=fact)  # call _densify_2D\r\n            arc_pnts = [arcpy.Point(*p) for p in shp]\r\n            sub_out.append(arc_pnts)\r\n            out.extend(sub_out)\r\n        else:\r\n            for i in range(len(p)):\r\n                pp = p[i]\r\n                shp = _densify_2D(pp, fact=fact)\r\n                arc_pnts = [arcpy.Point(*p) for p in shp]\r\n                sub_out.append(arc_pnts)\r\n            out.append(sub_out)\r\n    return out\r\n\r\n\r\ndef densify(polys, fact=2, sp_ref=None):\r\n    """"""Convert polygon objects to arrays, densify.\r\n    :\r\n    :Requires:\r\n    :--------\r\n    : _densify_2D - the function that is called for each shape part\r\n    : _unpack - unpack objects\r\n    """"""\r\n    # ---- main section ----\r\n    out = []\r\n    for poly in polys:\r\n        p = poly.__geo_interface__[\'coordinates\']\r\n        back = _convert(p, fact)\r\n        out.append(back)\r\n    return out\r\n\r\n\r\n# ------------------------------------------------------------------------\r\n# (1) ---- Checks to see if running in test mode or from a tool ----------\r\n\r\ndef _demo():\r\n    """"""run when script is in demo mode""""""\r\n    pth = script.replace(\'densify_geom.py\', \'\')\r\n    in_fc = pth + \'/geom_data.gdb/polygon_demo\'\r\n#    in_fc = r""C:\\Git_Dan\\a_Data\\testdata.gdb\\Carp_5x5""   # full 25 polygons\r\n#    in_fc = r""C:\\Git_Dan\\a_Data\\arcpytools_demo.gdb\\xy1000_tree""\r\n    out_fc = pth + \'/geom_data.gdb/x1\'\r\n    fact = 2\r\n    out_type = \'Polygon\'  # \'Polyline\' or \'Points\'\r\n    testing = True\r\n    return in_fc, out_fc, fact, out_type, testing\r\n\r\n\r\ndef _tool():\r\n    """"""run when script is from a tool""""""\r\n    in_fc = sys.argv[1]  #\r\n    out_fc = sys.argv[2]  #\r\n    fact = int(sys.argv[3])  #\r\n    out_type = sys.argv[4]  # Polygon, Polyline are options\r\n    testing = False\r\n    return in_fc, out_fc, fact, out_type, testing\r\n\r\n\r\n# ---- main block ------------------------------------------------------------\r\n#\r\n# (1) check to see if in demo or tool mode\r\n# (2) obtain fc information\r\n# (3) convert multipart to singlepart\r\n# (4) split the fc into two arrays, one geometry, the 2nd attributes\r\n# (5) obtain the shapes and densify\r\n# (6) optionally produce the output fc\r\n# (7) join the attributes back\r\n\r\nif len(sys.argv) == 1:\r\n    in_fc, out_fc, fact, out_type, testing = _demo()\r\nelse:\r\n    in_fc, out_fc, fact, out_type, testing = _tool()\r\n\r\nshp_fld, oid_fld, shp_type, SR = fc_info(in_fc)\r\n\r\ntemp = out_fc + ""tmp""\r\nif arcpy.Exists(temp):\r\n    arcpy.Delete_management(temp)\r\narcpy.MultipartToSinglepart_management(in_fc, temp)\r\npolys = _get_shapes(temp)\r\na = densify(polys, fact=fact, sp_ref=SR)\r\nb = _get_attributes(temp)\r\nout_shps = arcpnts_poly(a, out_type=out_type, SR=SR)\r\nif not testing:\r\n    if arcpy.Exists(out_fc):\r\n        arcpy.Delete_management(out_fc)\r\n    arcpy.CopyFeatures_management(out_shps, out_fc)\r\n    arcpy.da.ExtendTable(out_fc, \'OBJECTID\', b, \'IDs\')\r\n# ---- cleanup\r\narcpy.Delete_management(temp)\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n'"
geometry_tools/geom_helper.py,6,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   .py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2017-xx-xx\r\n:Purpose:  tools for working with numpy arrays\r\n:Useage:\r\n:\r\n:References:\r\n:\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\n\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\r\n      \'float\': \'{: 0.3f}\'.format}\r\n\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\n# ---- array functions -------------------------------------------------------\r\n#\r\n\r\n__all__ = [\'tweet\', \'_describe\', \'fc_info\', \'_xyID\', \'_ndarray\', \'_two_arrays\']\r\n\r\ndef tweet(msg):\r\n    """"""Print a message for both arcpy and python.\r\n    : msg - a text message\r\n    """"""\r\n    m = ""\\n{}\\n"".format(msg)\r\n    arcpy.AddMessage(m)\r\n    print(m)\r\n    print(arcpy.GetMessages())\r\n\r\n\r\ndef _describe(in_fc):\r\n    """"""Simply return the arcpy.da.Describe object\r\n    : desc.keys() an abbreviated list...\r\n    : [... \'OIDFieldName\'... \'areaFieldName\', \'baseName\'... \'catalogPath\',\r\n    :  ... \'dataType\'... \'extent\', \'featureType\', \'fields\', \'file\'... \'hasM\',\r\n    :  \'hasOID\', \'hasZ\', \'indexes\'... \'lengthFieldName\'... \'name\', \'path\',\r\n    :  \'rasterFieldName\', ..., \'shapeFieldName\', \'shapeType\',\r\n    :  \'spatialReference\',  ...]\r\n    """"""\r\n    return arcpy.da.Describe(in_fc)\r\n\r\n\r\ndef fc_info(in_fc, prn=False):\r\n    """"""Return basic featureclass information, including...\r\n    :\r\n    : shp_fld  - field name which contains the geometry object\r\n    : oid_fld  - the object index/id field name\r\n    : SR       - spatial reference object (use SR.name to get the name)\r\n    : shp_type - shape type (Point, Polyline, Polygon, Multipoint, Multipatch)\r\n    : - others: \'areaFieldName\', \'baseName\', \'catalogPath\',\'featureType\',\r\n    :   \'fields\', \'hasOID\', \'hasM\', \'hasZ\', \'path\'\r\n    : - all_flds =[i.name for i in desc[\'fields\']]\r\n    """"""\r\n    desc = arcpy.da.Describe(in_fc)\r\n    args = [\'shapeFieldName\', \'OIDFieldName\', \'shapeType\', \'spatialReference\']\r\n    shp_fld, oid_fld, shp_type, SR = [desc[i] for i in args]\r\n    if prn:\r\n        frmt = ""FeatureClass:\\n   {}"".format(in_fc)\r\n        f = ""\\n{!s:<16}{!s:<14}{!s:<10}{!s:<10}""\r\n        frmt += f.format(*args)\r\n        frmt += f.format(shp_fld, oid_fld, shp_type, SR.name)\r\n        tweet(frmt)\r\n    else:\r\n        return shp_fld, oid_fld, shp_type, SR\r\n\r\n\r\ndef _xyID(in_fc, to_pnts=True):\r\n    """"""Convert featureclass geometry (in_fc) to a simple 2D structured array\r\n    :  with ID, X, Y values. Optionally convert to points, otherwise centroid.\r\n    """"""\r\n    flds = [\'OID@\', \'SHAPE@X\', \'SHAPE@Y\']\r\n    args = [in_fc, flds, None, None, to_pnts, (None, None)]\r\n    cur = arcpy.da.SearchCursor(*args)\r\n    a = cur._as_narray()\r\n    a.dtype = [(\'IDs\', \'<i4\'), (\'Xs\', \'<f8\'), (\'Ys\', \'<f8\')]\r\n    del cur\r\n    return a\r\n\r\n\r\ndef _ndarray(in_fc, to_pnts=True, flds=None, SR=None):\r\n    """"""Convert featureclass geometry (in_fc) to a structured ndarray including\r\n    :  options to select fields and specify a spatial reference.\r\n    :\r\n    :Requires:\r\n    :--------\r\n    : in_fc - input featureclass\r\n    : to_pnts - True, convert the shape to points. False, centroid returned.\r\n    : flds - \'*\' for all, others: \'Shape\',  [\'SHAPE@X\', \'SHAPE@Y\'], or specify\r\n    """"""\r\n    if flds is None:\r\n        flds = ""*""\r\n    if SR is None:\r\n        desc = arcpy.da.Describe(in_fc)\r\n        SR = desc[\'spatialReference\']\r\n    args = [in_fc, flds, None, SR, to_pnts, (None, None)]\r\n    cur = arcpy.da.SearchCursor(*args)\r\n    a = cur._as_narray()\r\n    del cur\r\n    return a\r\n\r\n\r\ndef _two_arrays(in_fc, both=True, split=True):\r\n    """"""Send to a numpy structured/array and split it into a geometry array\r\n    :  and an attribute array.  They can be joined back later if needed.\r\n    :\r\n    :Note:  The geometry array is returned as an object array.  See the\r\n    :----   main documentation\r\n    :\r\n    :Requires:\r\n    :--------\r\n    :functions:\r\n    :  _xyID - function to get geometry array\r\n    :  _ndarray - function to get the x, y, id array and attribute array\r\n    :   fc_info(in_fc) - function needed to return fc properties\r\n    :parameters:\r\n    :  both  - True, to return both arrays, False to return just geometry\r\n    :  split - True, split points by their geometry groups as an object array\r\n    :         - False, a sequential array with shape = (N,)\r\n    :variables:\r\n    :  dt_a = [(\'IDs\', \'<i4\'), (\'Xs\', \'<f8\'), (\'Ys\', \'<f8\')]\r\n    :  dt_b = [(\'IDs\', \'<i4\'), (\'Xc\', \'<f8\'), (\'Yc\', \'<f8\')]\r\n    :  dt_b.extend(b.dtype.descr[2:])\r\n    :       extend the dtype using the attribute dtype minus geometry and id\r\n    """"""\r\n    a = _xyID(in_fc, to_pnts=True)\r\n    shp_fld, oid_fld, SR, shp_type = fc_info(in_fc)\r\n    dt_a = [(\'IDs\', \'<i4\'), (\'Xs\', \'<f8\'), (\'Ys\', \'<f8\')]\r\n    dt_b = [(\'IDs\', \'<i4\'), (\'Xc\', \'<f8\'), (\'Yc\', \'<f8\')]\r\n    a.dtype = dt_a\r\n    b = None\r\n    if split:\r\n        ids = np.unique(a[\'IDs\'])\r\n        w = np.where(np.diff(a[\'IDs\']))[0] + 1\r\n        a = np.split(a, w)\r\n        a = np.array([[ids[i], a[i][[\'Xs\', \'Ys\']]] for i in range(len(ids))])\r\n    if both:\r\n        b = _ndarray(in_fc, to_pnts=False)\r\n        dt_b.extend(b.dtype.descr[2:])\r\n        b.dtype = dt_b\r\n    return a, b\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n#    _demo()\r\n'"
geometry_tools/sampling_grid.py,25,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   sampling_grid.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2018-01-28\r\n:Purpose:  tools for working with numpy arrays\r\n:Source:\r\n: http://www.arcgis.com/home/item.html?id=ddb2deec9b5e4a09affe60de68f5ff4e\r\n:\r\n:References:\r\n:----------\r\n:Phish_Nyet.py\r\n:  https://community.esri.com/blogs/dan_patterson/2016/09/09/\r\n:        numpy-snippets-3-phishnyet-creating-sampling-grids-using-numpy\r\n:n-gons....\r\n:  https://community.esri.com/blogs/dan_patterson/2016/09/09/\r\n:        n-gons-regular-polygonal-shape-generation\r\n:\r\n:Purpose:\r\n:-------\r\n: - Produce a sampling grid with user defined parameters.\r\n: - create hexagon shapes in two forms, flat-topped and pointy-topped\r\n:\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nfrom textwrap import dedent\r\nimport numpy as np\r\nimport arcpy\r\n\r\narcpy.overwriteOutputs = True\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# ---- main functions ----\r\ndef code_grid(cols=1, rows=1, zero_based=False, shaped=True, bottom_up=False):\r\n    """"""produce spreadsheet like labelling, either zero or 1 based\r\n    :  zero - A0,A1  or ones - A1, A2..\r\n    :  dig = list(\'0123456789\')  # string.digits\r\n    : import string .... string.ascii_uppercase\r\n    """"""\r\n    UC = list(""ABCDEFGHIJKLMNOPQRSTUVWXYZ"")\r\n    z = [1, 0][zero_based]\r\n    rc = [1, 0][zero_based]\r\n    c = [UC[c] + str(r)                # pull in the column heading\r\n         for r in range(z, rows + rc)  # label in the row letter\r\n         for c in range(cols)]         # label in the row number\r\n    c = np.asarray(c)\r\n    if shaped:\r\n        c = c.reshape(rows, cols)\r\n        if bottom_up:\r\n            c = np.flipud(c)\r\n    return c\r\n\r\n\r\ndef rotate(pnts, angle=0):\r\n    """"""rotate points about the origin in degrees, (+ve for clockwise) """"""\r\n    angle = np.deg2rad(angle)                 # convert to radians\r\n    s = np.sin(angle)\r\n    c = np.cos(angle)    # rotation terms\r\n    aff_matrix = np.array([[c, s], [-s, c]])  # rotation matrix\r\n    XY_r = np.dot(pnts, aff_matrix)           # numpy magic to rotate pnts\r\n    return XY_r\r\n\r\n\r\ndef rectangle(dx=1, dy=1, cols=1, rows=1):\r\n    """"""Create the array of pnts to pass on to arcpy using numpy magic\r\n    :  dx - increment in x direction, +ve moves west to east, left/right\r\n    :  dy - increment in y direction, -ve moves north to south, top/bottom\r\n    """"""\r\n    X = [0.0, 0.0, dx, dx, 0.0]       # X, Y values for a unit square\r\n    Y = [0.0, dy, dy, 0.0, 0.0]\r\n    seed = np.array(list(zip(X, Y)))  # [dx0, dy0] keep for insets\r\n    a = [seed + [j * dx, i * dy]       # make the shapes\r\n         for i in range(0, rows)   # cycle through the rows\r\n         for j in range(0, cols)]  # cycle through the columns\r\n    a = np.asarray(a)\r\n    return a\r\n\r\n\r\ndef hex_flat(dx=1, dy=1, cols=1, rows=1):\r\n    """"""generate the points for the flat-headed hexagon\r\n    :dy_dx - the radius width, remember this when setting hex spacing\r\n    :  dx - increment in x direction, +ve moves west to east, left/right\r\n    :  dy - increment in y direction, -ve moves north to south, top/bottom\r\n    """"""\r\n    f_rad = np.deg2rad([180., 120., 60., 0., -60., -120., -180.])\r\n    X = np.cos(f_rad) * dy\r\n    Y = np.sin(f_rad) * dy            # scaled hexagon about 0, 0\r\n    seed = np.array(list(zip(X, Y)))  # array of coordinates\r\n    dx = dx * 1.5\r\n    dy = dy * np.sqrt(3.)/2.0\r\n    hexs = [seed + [dx * i, dy * (i % 2)] for i in range(0, cols)]\r\n    m = len(hexs)\r\n    for j in range(1, rows):  # create the other rows\r\n        hexs += [hexs[h] + [0, dy * 2 * j] for h in range(m)]\r\n    return hexs\r\n\r\n\r\ndef hex_pointy(dx=1, dy=1, cols=1, rows=1):\r\n    """"""pointy hex angles, convert to sin, cos, zip and send\r\n    :dy_dx - the radius width, remember this when setting hex spacing\r\n    :  dx - increment in x direction, +ve moves west to east, left/right\r\n    :  dy - increment in y direction, -ve moves north to south, top/bottom\r\n    """"""\r\n    p_rad = np.deg2rad([150., 90, 30., -30., -90., -150., 150.])\r\n    X = np.cos(p_rad) * dx\r\n    Y = np.sin(p_rad) * dy      # scaled hexagon about 0, 0\r\n    seed = np.array(list(zip(X, Y)))\r\n    dx = dx * np.sqrt(3.)/2.0\r\n    dy = dy * 1.5\r\n    hexs = [seed + [dx * i * 2, 0] for i in range(0, cols)]\r\n    m = len(hexs)\r\n    for j in range(1, rows):  # create the other rows\r\n        hexs += [hexs[h] + [dx * (j % 2), dy * j] for h in range(m)]\r\n    return hexs\r\n\r\n\r\ndef repeat(seed=None, corner=[0, 0], cols=1, rows=1, angle=0):\r\n    """"""Create the array of pnts to pass on to arcpy using numpy magic to\r\n    :  produce a fishnet of the desired in_shp.\r\n    :seed - use grid_array, hex_flat or hex_pointy.  You specify the width\r\n    :       and height or its ratio when making the shapes\r\n    :corner - lower left corner of the shape pattern\r\n    :dx, dy - offset of the shapes... this is different\r\n    :rows, cols - the number of rows and columns to produce\r\n    :angle - rotation angle in degrees\r\n    """"""\r\n    if seed is None:\r\n        a = rectangle(dx=1, dy=1, cols=3, rows=3)\r\n    else:\r\n        a = np.asarray(seed)\r\n    if angle != 0:\r\n        a = [rotate(p, angle) for p in a]      # rotate the scaled points\r\n    pnts = [p + corner for p in a]            # translate them\r\n    return pnts\r\n\r\n\r\ndef output_polygons(output_shp, SR, pnts):\r\n    """"""produce the output polygon shapefile""""""\r\n    msg = \'\\nRead the script header... A projected coordinate system required\'\r\n    assert (SR is not None), msg  # and (SR.type == \'Projected\'), msg\r\n    polygons = []\r\n    for pnt in pnts:                 # create the polygon geometry\r\n        pl = arcpy.Polygon(arcpy.Array([arcpy.Point(*xy) for xy in pnt]), SR)\r\n        polygons.append(pl)\r\n    if arcpy.Exists(output_shp):     # overwrite any existing versions\r\n        arcpy.Delete_management(output_shp)\r\n    arcpy.CopyFeatures_management(polygons, output_shp)\r\n    return output_shp\r\n\r\n\r\ndef extend_tbl(output_shp, rows, cols):\r\n    shp = rows*cols\r\n    code_fld = np.empty((shp,), dtype=[(\'IDs\', \'<i4\'), (\'Grid_codes\', \'<U3\')])\r\n    codes = code_grid(cols=cols, rows=rows, zero_based=False,\r\n                      shaped=True, bottom_up=False).ravel()\r\n    code_fld[\'IDs\'] = np.arange(1, shp+1)\r\n    code_fld[\'Grid_codes\'] = codes\r\n    arcpy.da.ExtendTable(output_shp, \'OBJECTID\', code_fld, \'IDS\')\r\n\r\n\r\nmsg = """"""\r\n: --------------------------------------------------------------------\r\n: output {}\r\n: SR  .. {}\r\n: type . {}\r\n: corner .. {}\r\n: size..... {} (dx, dy)\r\n: cols/rows {}\r\n: sample seed\r\n{}\r\n: --------------------------------------------------------------------\r\n""""""\r\n\r\n\r\ndef _demo(seed=None, out_fc=False, SR=None, corner=[0, 0], angle=0):\r\n    """"""Generate the grid using the specified or default parameters\r\n    """"""\r\n    corner = corner  # [300000.0, 5000000.0]\r\n    dx, dy = [1, 1]\r\n    cols, rows = [3, 3]\r\n    if seed is None:\r\n#        seed = rectangle(dx=1, dy=1, cols=3, rows=3)\r\n        seed = hex_pointy(dx=10, dy=10, cols=3, rows=3)\r\n#        seed = hex_flat(dx=10, dy=10, cols=3, rows=3)\r\n        seed_t = \'rectangle\'\r\n    if SR is None:\r\n        SR = 3857  # -- WGS84 Web Mercator (Auxiliary Sphere)\r\n    pnts = repeat(seed=seed, corner=corner, cols=3, rows=3, angle=0)\r\n    args = ["""", SR, seed_t, corner, [dx, dy], [cols, rows], seed[0]]\r\n    print(dedent(msg).format(*args))\r\n    return pnts\r\n\r\n\r\ndef _tool():\r\n    """"""run when script is from a tool""""""\r\n    out_fc = sys.argv[1]  #\r\n    SR = sys.argv[2]\r\n    seed_t = sys.argv[3]\r\n    xtent = [float(i) for i in sys.argv[4].split("" "")]\r\n    L, B, R, T = xtent\r\n    corn_x = L  # float(sys.argv[4])\r\n    corn_y = T  # float(sys.argv[5])\r\n    dx = float(sys.argv[5])\r\n    dy = float(sys.argv[6]) * -1.0\r\n    cols = int(sys.argv[7])\r\n    rows = int(sys.argv[8])\r\n    #\r\n    angle = float(sys.argv[9])\r\n    corner = [corn_x, corn_y]\r\n    if seed_t == \'rectangle\':\r\n        seed = rectangle(dx, dy, cols, rows)\r\n    elif seed_t == \'hex_pointy\':\r\n        seed = hex_pointy(dx, dy, cols, rows)\r\n    elif seed_t == \'hex_flat\':\r\n        seed = hex_flat(dx, dy, cols, rows)\r\n    else:\r\n        seed = rectangle(dx, dy, cols, rows)\r\n    # ----\r\n    msg = """"""\r\n    : --------------------------------------------------------------------\r\n    : output {}\r\n    : SR  .. {}\r\n    : extent .. {}\r\n    : type . {}\r\n    : corner .. {}\r\n    : size..... {} (dx, dy)\r\n    : cols/rows {}\r\n    : sample seed\r\n    {}\r\n    """"""\r\n    args = [out_fc, SR, xtent, seed_t, corner, [dx, dy],\r\n            [cols, rows], seed[0]]\r\n    arcpy.AddMessage(dedent(msg).format(*args))\r\n    arcpy.GetMessages()\r\n    pnts = repeat(seed=seed, corner=corner, cols=cols, rows=rows, angle=angle)\r\n    return out_fc, SR, pnts, rows, cols\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# .... final code section producing the featureclass and extendtable\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    pnts = _demo()\r\nelse:\r\n    testing = False\r\n    out_fc, SR, pnts, rows, cols = _tool()\r\n#\r\nif not testing:\r\n    output_shp = output_polygons(out_fc, SR, pnts)\r\n    extend_tbl(output_shp, rows, cols)\r\n    print(\'\\nSampling grid was created... {}\'.format(out_fc))\r\n\r\n# ----------------------------------------------------------------------\r\n#\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n'"
sampling_grid/sampling_grid.py,29,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\nsampling_grid\r\n=============\r\n\r\nScript   :  sampling_grid.py\r\n\r\nAuthor   :  Dan.Patterson@carleton.ca\r\n\r\nModified :  2018-08-24\r\n\r\nPurpose  :  tools for working with numpy arrays\r\n\r\nSource :\r\n`<http://www.arcgis.com/home/item.html?id=ddb2deec9b5e4a09affe60de68f5ff4e>`_.\r\n\r\nReferences\r\n----------\r\n- Phish_Nyet.py\r\n`<https://community.esri.com/blogs/dan_patterson/2016/09/09/\r\nnumpy-snippets-3-phishnyet-creating-sampling-grids-using-numpy>`_.\r\n\r\n- n-gons\r\n`<https://community.esri.com/blogs/dan_patterson/2016/09/09/n-gons-\r\nregular-polygonal-shape-generation>`_.\r\n\r\nPurpose\r\n-------\r\n- Produce a sampling grid with user defined parameters.\r\n- create hexagon shapes in two forms, flat-topped and pointy-topped.\r\n\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nfrom textwrap import dedent\r\nimport numpy as np\r\nimport arcpy\r\n\r\narcpy.overwriteOutputs = True\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# ---- main functions ----\r\ndef code_grid(cols=1, rows=1, zero_based=False, shaped=True, bottom_up=False):\r\n    """"""produce spreadsheet like labelling, either zero or 1 based\r\n    see: code_grid.py for more details\r\n    """"""\r\n    alph = list("" ABCDEFGHIJKLMNOPQRSTUVWXYZ"")\r\n    UC = [(""{}{}"").format(alph[i], alph[j]).strip()\r\n          for i in range(27)\r\n          for j in range(1, 27)]\r\n    z = [1, 0][zero_based]\r\n    rc = [1, 0][zero_based]\r\n    c = [""{}{:02.0f}"".format(UC[c], r) # pull in the column heading\r\n         for r in range(z, rows + rc)  # label in the row letter\r\n         for c in range(cols)]         # label in the row number\r\n    c = np.asarray(c)\r\n    if shaped:\r\n        c = c.reshape(rows, cols)\r\n        if bottom_up:\r\n            c = np.flipud(c)\r\n    return c\r\n\r\n\r\ndef rotate(pnts, angle=0):\r\n    """"""rotate points about the origin in degrees, (+ve for clockwise) """"""\r\n    angle = np.deg2rad(angle)                 # convert to radians\r\n    s = np.sin(angle)\r\n    c = np.cos(angle)    # rotation terms\r\n    aff_matrix = np.array([[c, s], [-s, c]])  # rotation matrix\r\n    XY_r = np.dot(pnts, aff_matrix)           # numpy magic to rotate pnts\r\n    return XY_r\r\n\r\n\r\ndef triangle(dx=1, dy=1, cols=1, rows=1):\r\n    """"""Create a row of meshed triangles\r\n    """"""\r\n    grid_type = \'triangle\'\r\n    a, dx, b = dx/2.0, dx, dx*1.5\r\n    Xu = [0.0, a, dx, 0.0]   # X, Y values for a unit triangle, point up\r\n    Yu = [0.0, dy, 0.0, 0.0]\r\n    Xd = [a, b, dx, a]       # X, Y values for a unit triangle, point down\r\n    Yd = [dy, dy, 0.0, dy]   # shifted by dx\r\n    seedU = np.array(list(zip(Xu, Yu)))\r\n    seedD = np.array(list(zip(Xd, Yd)))\r\n    seed = np.array([seedU, seedD])\r\n    a = [seed + [j * dx, i * dy]       # make the shapes\r\n         for i in range(0, rows)       # cycle through the rows\r\n         for j in range(0, cols)]      # cycle through the columns\r\n    a = np.asarray(a)\r\n    s1, s2, s3, s4 = a.shape\r\n    a = a.reshape(s1*s2, s3, s4)\r\n    return a, grid_type\r\n\r\n\r\ndef rectangle(dx=1, dy=1, cols=1, rows=1):\r\n    """"""Create the array of pnts to pass on to arcpy using numpy magic\r\n    :  dx - increment in x direction, +ve moves west to east, left/right\r\n    :  dy - increment in y direction, -ve moves north to south, top/bottom\r\n    """"""\r\n    grid_type = \'rectangle\'\r\n    X = [0.0, 0.0, dx, dx, 0.0]       # X, Y values for a unit square\r\n    Y = [0.0, dy, dy, 0.0, 0.0]\r\n    seed = np.array(list(zip(X, Y)))  # [dx0, dy0] keep for insets\r\n    a = [seed + [j * dx, i * dy]      # make the shapes\r\n         for i in range(0, rows)      # cycle through the rows\r\n         for j in range(0, cols)]     # cycle through the columns\r\n    a = np.asarray(a)\r\n    return a, grid_type\r\n\r\n\r\ndef hex_flat(dx=1, dy=1, cols=1, rows=1):\r\n    """"""generate the points for the flat-headed hexagon\r\n    :dy_dx - the radius width, remember this when setting hex spacing\r\n    :  dx - increment in x direction, +ve moves west to east, left/right\r\n    :  dy - increment in y direction, -ve moves north to south, top/bottom\r\n    """"""\r\n    grid_type = \'hex_flat\'\r\n    f_rad = np.deg2rad([180., 120., 60., 0., -60., -120., -180.])\r\n    X = np.cos(f_rad) * dy\r\n    Y = np.sin(f_rad) * dy            # scaled hexagon about 0, 0\r\n    seed = np.array(list(zip(X, Y)))  # array of coordinates\r\n    dx = dx * 1.5\r\n    dy = dy * np.sqrt(3.)/2.0\r\n    hexs = [seed + [dx * i, dy * (i % 2)] for i in range(0, cols)]\r\n    m = len(hexs)\r\n    for j in range(1, rows):  # create the other rows\r\n        hexs += [hexs[h] + [0, dy * 2 * j] for h in range(m)]\r\n    return hexs, grid_type\r\n\r\n\r\ndef hex_pointy(dx=1, dy=1, cols=1, rows=1):\r\n    """"""pointy hex angles, convert to sin, cos, zip and send\r\n    :dy_dx - the radius width, remember this when setting hex spacing\r\n    :  dx - increment in x direction, +ve moves west to east, left/right\r\n    :  dy - increment in y direction, -ve moves north to south, top/bottom\r\n    """"""\r\n    grid_type = \'hex_pointy\'\r\n    p_rad = np.deg2rad([150., 90, 30., -30., -90., -150., 150.])\r\n    X = np.cos(p_rad) * dx\r\n    Y = np.sin(p_rad) * dy      # scaled hexagon about 0, 0\r\n    seed = np.array(list(zip(X, Y)))\r\n    dx = dx * np.sqrt(3.)/2.0\r\n    dy = dy * 1.5\r\n    hexs = [seed + [dx * i * 2, 0] for i in range(0, cols)]\r\n    m = len(hexs)\r\n    for j in range(1, rows):  # create the other rows\r\n        hexs += [hexs[h] + [dx * (j % 2), dy * j] for h in range(m)]\r\n    return hexs, grid_type\r\n\r\n\r\ndef repeat(seed=None, corner=[0, 0], cols=1, rows=1, angle=0):\r\n    """"""Create the array of pnts to pass on to arcpy using numpy magic to\r\n    :  produce a fishnet of the desired in_shp.\r\n    :seed - use grid_array, hex_flat or hex_pointy.  You specify the width\r\n    :       and height or its ratio when making the shapes\r\n    :corner - lower left corner of the shape pattern\r\n    :dx, dy - offset of the shapes... this is different\r\n    :rows, cols - the number of rows and columns to produce\r\n    :angle - rotation angle in degrees\r\n    """"""\r\n    if seed is None:\r\n        a = rectangle(dx=1, dy=1, cols=3, rows=3)\r\n    else:\r\n        a = np.asarray(seed)\r\n    if angle != 0:\r\n        a = [rotate(p, angle) for p in a]      # rotate the scaled points\r\n    pnts = [p + corner for p in a]            # translate them\r\n    return pnts\r\n\r\n\r\ndef output_polygons(output_shp, SR, pnts):\r\n    """"""produce the output polygon shapefile""""""\r\n    msg = \'\\nRead the script header... A projected coordinate system required\'\r\n    assert (SR is not None), msg  # and (SR.type == \'Projected\'), msg\r\n    polygons = []\r\n    for pnt in pnts:                 # create the polygon geometry\r\n        pl = arcpy.Polygon(arcpy.Array([arcpy.Point(*xy) for xy in pnt]), SR)\r\n        polygons.append(pl)\r\n    if arcpy.Exists(output_shp):     # overwrite any existing versions\r\n        arcpy.Delete_management(output_shp)\r\n    arcpy.CopyFeatures_management(polygons, output_shp)\r\n    return output_shp\r\n\r\n\r\ndef extend_tbl(output_shp, grid_type, rows, cols):\r\n    """"""Produce the column with the grid labels\r\n    """"""\r\n    if grid_type == \'triangle\':\r\n        cols = cols * 2\r\n    shp = rows*cols\r\n    code_fld = np.empty((shp,), dtype=[(\'IDs\', \'<i4\'), (\'Grid_codes\', \'<U3\')])\r\n    codes = code_grid(cols=cols, rows=rows, zero_based=False,\r\n                      shaped=True, bottom_up=False).ravel()\r\n    code_fld[\'IDs\'] = np.arange(1, shp+1)\r\n    code_fld[\'Grid_codes\'] = codes\r\n    arcpy.da.ExtendTable(output_shp, \'OBJECTID\', code_fld, \'IDS\')\r\n\r\n\r\nmsg = """"""\r\n: --------------------------------------------------------------------\r\n: output {}\r\n: SR  .. {}\r\n: type . {}\r\n: corner .. {}\r\n: size..... {} (dx, dy)\r\n: cols/rows {}\r\n: sample seed\r\n{}\r\n: --------------------------------------------------------------------\r\n""""""\r\n\r\n\r\ndef _demo(seed=None, out_fc=False, SR=None, corner=[0, 0], angle=0):\r\n    """"""Generate the grid using the specified or default parameters\r\n    """"""\r\n    corner = corner  # [300000.0, 5000000.0]\r\n    dx, dy = [1, 1]\r\n    cols, rows = [3, 3]\r\n    if seed is None:\r\n#        seed = rectangle(dx=1, dy=1, cols=3, rows=3)\r\n        seed, grid_type = hex_pointy(dx=10, dy=10, cols=3, rows=3)\r\n#        seed = hex_flat(dx=10, dy=10, cols=3, rows=3)\r\n        seed_t = \'rectangle\'\r\n    if SR is None:\r\n        SR = 3857  # -- WGS84 Web Mercator (Auxiliary Sphere)\r\n    pnts = repeat(seed=seed, corner=corner, cols=3, rows=3, angle=0)\r\n    args = ["""", SR, seed_t, corner, [dx, dy], [cols, rows], seed[0]]\r\n    print(dedent(msg).format(*args))\r\n    return pnts\r\n\r\n\r\ndef _tool():\r\n    """"""run when script is from a tool""""""\r\n    out_fc = sys.argv[1]  #\r\n    SR = sys.argv[2]\r\n    seed_t = sys.argv[3]\r\n    corn_x = float(sys.argv[4])\r\n    corn_y = float(sys.argv[5])\r\n    dx = float(sys.argv[6])\r\n    dy = float(sys.argv[7]) * -1.0\r\n    cols = int(sys.argv[8])\r\n    rows = int(sys.argv[9])\r\n    #\r\n    angle = float(sys.argv[10])\r\n    corner = [corn_x, corn_y]\r\n    if seed_t == \'rectangle\':\r\n        seed, grid_type = rectangle(dx, dy, cols, rows)\r\n    elif seed_t == \'hex_pointy\':\r\n        seed, grid_type = hex_pointy(dx, dy, cols, rows)\r\n    elif seed_t == \'hex_flat\':\r\n        seed, grid_type = hex_flat(dx, dy, cols, rows)\r\n    elif seed_t == \'triangles\':\r\n        seed, grid_type = triangle(dx, dy, cols, rows)\r\n    else:\r\n        seed, grid_type = rectangle(dx, dy, cols, rows)\r\n    # ----\r\n    msg = """"""\r\n    : --------------------------------------------------------------------\r\n    : output {}\r\n    : SR  .. {}\r\n    : Type . {}\r\n    : Top leftcorner .. {}\r\n    : Size..... {} (dx, dy)\r\n    : cols/rows {}\r\n    : grid type {}\r\n    : sample seed\r\n    {}\r\n    """"""\r\n    args = [out_fc, SR, seed_t, corner, [dx, dy],\r\n            [cols, rows], grid_type, seed[0]]\r\n    arcpy.AddMessage(dedent(msg).format(*args))\r\n    arcpy.GetMessages()\r\n    pnts = repeat(seed=seed, corner=corner, cols=cols, rows=rows, angle=angle)\r\n    return out_fc, SR, pnts, grid_type, rows, cols\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# .... final code section producing the featureclass and extendtable\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    pnts = _demo()\r\nelse:\r\n    testing = False\r\n    out_fc, SR, pnts, grid_type, rows, cols = _tool()\r\n#\r\nif not testing:\r\n    output_shp = output_polygons(out_fc, SR, pnts)\r\n    extend_tbl(output_shp, grid_type, rows, cols)\r\n    print(\'\\nSampling grid was created... {}\'.format(out_fc))\r\n\r\n# ----------------------------------------------------------------------\r\n#\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n'"
statistics/field_stats.py,23,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   field_stats.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2017-07-08\r\n:Purpose:  Descriptive statistics for tables using numpy.\r\n:\r\n:References:\r\n:  https://github.com/numpy/numpy/blob/master/numpy/lib/nanfunctions.py\r\n:  _replace_nan(a, val) -  mask = np.isnan(a) - to get the mask\r\n:\r\n:  a = [1, 2, np.nan, 3, np.nan, 4]\r\n:  _, mask = _replace_nan(a, 0)  # for mean\r\n:  mask = array([False, False,  True, False,  True, False], dtype=bool)\r\n:\r\n: ---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\n\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\r\n      \'float\': \'{: 0.3f}\'.format}\r\n\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\n# ---- skewness and kurtosis section -----------------------------------------\r\n\r\ndef skew_kurt(a, avg, var_x, std_x, col=True, mom=\'both\'):\r\n    """"""Momental and unbiased skewness\r\n    :Emulates the nan functions approach to calculating these parameters\r\n    :when data contains nan values.\r\n    :Requires:\r\n    :---------\r\n    :  a - an array of float/double values where there are at least 3 non-nan\r\n    :      numbers in each column.  This is not checked since this situation\r\n    :      should never arise in real world data sets that have been checked.\r\n    :  moment - both, skew or kurt  to return the moments\r\n    :Notes:\r\n    :------\r\n    : a= np.arange(16.).reshape(4,4)\r\n    : mask = [0, 5, 10, 15]\r\n    : masked_array = np.where(a == mask, np.nan, a)\r\n    """"""\r\n#    a, mask = _replace_nan(a, 0.)  # produce a masked of the nan values\r\n    if len(a.shape) == 1:\r\n        ax = 0\r\n    else:\r\n        ax = [1, 0][col]\r\n#    # ---- mean section ----\r\n    mask = np.isnan(a)\r\n    cnt = np.sum(~mask, axis=ax, dtype=np.intp, keepdims=False)\r\n    diff = a - avg\r\n    sqrd = diff * diff\r\n    cubed = sqrd * diff\r\n    fourP = sqrd * sqrd\r\n    x_3 = np.nansum(cubed, axis=ax)\r\n    x_4 = np.nansum(fourP, axis=ax)\r\n    skew_m = x_3 / (cnt * (std_x**3))\r\n    kurt_m = x_4 / (cnt * (var_x * var_x))\r\n    # skew_u = skew_m*((cnt**2)/((cnt-1)*(cnt-2)))  # could add if needed\r\n    if mom == \'skew\':\r\n        return skew_m\r\n    elif mom == \'kurt\':\r\n        return kurt_m\r\n    elif mom == \'both\':\r\n        return skew_m, kurt_m\r\n\r\n\r\n# -----------------------------------------------------------------------------\r\n# functions\r\n\r\ndef tweet(msg):\r\n    """"""Produce a message (msg)for both arcpy and python\r\n    """"""\r\n    m = ""{}"".format(msg)\r\n    arcpy.AddMessage(m)\r\n    print(m)\r\n    print(arcpy.GetMessages())\r\n\r\n\r\ndef cal_stats(in_fc, col_names):\r\n    """"""Calculate stats for an array of double types, with nodata (nan, None)\r\n    :  in the column.\r\n    :Requires:\r\n    :---------\r\n    : in_fc - input featureclass or table\r\n    : col_names - the columns... numeric (floating point, double)\r\n    :\r\n    :Notes:\r\n    :------  see the args tuple for examples of nan functions\r\n    :  np.nansum(b, axis=0)   # by column\r\n    :  np.nansum(b, axis=1)   # by row\r\n    :  c_nan = np.count_nonzero(~np.isnan(b), axis=0) count nan if needed\r\n    """"""\r\n    a = arcpy.da.FeatureClassToNumPyArray(in_fc, col_names)  # ""*"")\r\n    b = a.view(np.float).reshape(len(a), -1)\r\n    if len(a.shape) == 1:\r\n        ax = 0\r\n    else:\r\n        ax = [1, 0][True]  # ax = [1, 0][colwise]  colwise= True\r\n    mask = np.isnan(b)\r\n    cnt = np.sum(~mask, axis=ax, dtype=np.intp, keepdims=False)\r\n    n_sum = np.nansum(b, axis=0)\r\n    n_mean = np.nanmean(b, axis=0)\r\n    n_var = np.nanvar(b, axis=0)\r\n    n_std = np.nanstd(b, axis=0)\r\n    sk, kurt = skew_kurt(b, avg=n_mean, var_x=n_var, std_x=n_std, col=True, mom=\'both\')\r\n    args = (col_names, n_sum, np.nanmin(b, axis=0), np.nanmax(b, axis=0),\r\n            np.nanmedian(b, axis=0), n_mean, n_std, n_var, sk, kurt)\r\n    return col_names, args\r\n\r\n\r\ndef stats_tbl(col_names, args):\r\n    """"""Produce the output table\r\n    :   (\'N_\', \'<i4\'), (\'N_nan\', \'<i4\')\r\n    """"""\r\n    d = [(i, \'<f8\')\r\n         for i in [\'Sum\', \'Min\', \'Max\', \'Med\', \'Avg\',\r\n                   \'Std\', \'Var\', \'Skew\', \'Kurt\']]\r\n    dts = [(\'Fld\', \'<U15\')] + d\r\n    rows = len(col_names)\r\n    cols = len(dts)\r\n    z = np.empty(shape=(rows,), dtype=dts)\r\n    for i in range(cols):\r\n        z[z.dtype.names[i]] = args[i]\r\n    return z\r\n\r\n\r\nif len(sys.argv) == 1:\r\n    in_fc = r\'C:\\GIS\\Tools_scripts\\Statistics\\Stats_demo_01.gdb\\pnts_2000_norm\'\r\n    flds = arcpy.ListFields(in_fc)\r\n    col_names = [fld.name for fld in flds if fld.type == \'Double\']\r\n    out_tbl = None\r\nelse:\r\n    in_fc = sys.argv[1]\r\n    col_names = sys.argv[2]\r\n    col_names = col_names.split(\';\')\r\n    out_tbl = sys.argv[3]\r\n\r\ncol_names, args = cal_stats(in_fc, col_names)  # calculate statistics\r\nz = stats_tbl(col_names, args)                 # produce the table\r\n\r\ntweet(""\\n{}\\nSaving results to .... {}"".format(""-""*60, out_tbl))\r\ntweet(""Stats results...\\n{}\\n{}"".format(z.dtype.names, z))\r\n\r\nif out_tbl is not None:\r\n    arcpy.da.NumPyArrayToTable(z, out_tbl)\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n'"
statistics/rank_field.py,7,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   rank_field.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2017-07-12\r\n:Purpose:  tools for working with arcpy and numpy arrays\r\n:  - sort a table based on a field or fields\r\n:References:\r\n:(1) FeatureClassToNumPyArray (in_table, field_names, {where_clause},\r\n:                           {spatial_reference}, {explode_to_points},\r\n:                           {skip_nulls}, {null_value})\r\n: - http://pro.arcgis.com/en/pro-app/arcpy/data-access/\r\n:        featureclasstonumpyarray.htm\r\n:   -SHAPE@TRUECENTROID \xe2\x80\x94A tuple of the feature\'s true centroid coordinates\r\n:   -SHAPE@X \xe2\x80\x94 A double of the feature\'s x-coordinate.\r\n:   -SHAPE@Y \xe2\x80\x94 A double of the feature\'s y-coordinate.\r\n:\r\n:(2) TableToNumPyArray (in_table, field_names, {where_clause},\r\n:                   {skip_nulls}, {null_value})\r\n: - http://pro.arcgis.com/en/pro-app/arcpy/data-access/tabletonumpyarray.htm\r\n:\r\n:(3) ExtendTable(in_table, table_match_field, in_array,\r\n:              array_match_field, {append_only})\r\n: - http://pro.arcgis.com/en/pro-app/arcpy/data-access/extendtable.htm\r\n:Notes:\r\n:-----\r\n:\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\r\n      \'float\': \'{: 0.3f}\'.format}\r\n\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\n# -----------------------------------------------------------------------------\r\n# functions from arraytools\r\ndef tweet(msg):\r\n    """"""Produce a message (msg)for both arcpy and python\r\n    """"""\r\n    m = ""{}"".format(msg)\r\n    arcpy.AddMessage(m)\r\n    print(m)\r\n    print(arcpy.GetMessages())\r\n\r\n\r\ndef fc_info(in_fc):\r\n    """"""basic feature class information""""""\r\n    desc = arcpy.Describe(in_fc)\r\n    SR = desc.spatialReference      # spatial reference object\r\n    shp_fld = desc.shapeFieldName   # FID or OIDName, normally\r\n    oid_fld = desc.OIDFieldName     # Shapefield ...\r\n    return shp_fld, oid_fld, SR\r\n\r\n\r\n# -----------------------------------------------------------------------------\r\n# ---- Other defs ----\r\ndef rankmin(x):\r\n    """"""Returns a rank accounting for duplicates\r\n    :  The array must be sorted first\r\n    :  Warren W. solution at\r\n    : https://stackoverflow.com/questions/39059371/\r\n    :       can-numpys-argsort-give-equal-element-the-same-rank\r\n    """"""\r\n    u, inv, counts = np.unique(x, return_inverse=True, return_counts=True)\r\n    csum = np.zeros_like(counts)\r\n    csum[1:] = counts[:-1].cumsum()\r\n    return csum[inv]\r\n\r\n# ------------------------------------------------------------------------\r\n# ---- Checks to see if running in test mode or from a tool\r\nif len(sys.argv) == 1:\r\n    in_fc = r\'C:\\GIS\\Tools_scripts\\Statistics\\Stats_demo_01.gdb\\pnts_2K_normal\'\r\n    shp_fld, oid_fld, SR = fc_info(in_fc)\r\n    flds = arcpy.ListFields(in_fc)\r\n    #fld_names = [fld.name for fld in flds]\r\n    fld_names = [\'Rand_1_100\', oid_fld]\r\n    testing = True\r\n    rank_fld = \'Rand_1_100\'\r\n    rank_min = True\r\nelse:\r\n    in_fc = sys.argv[1]\r\n    fld_names = sys.argv[2]\r\n    rank_fld = sys.argv[3]\r\n    rank_min = sys.argv[4]\r\n    shp_fld, oid_fld, SR = fc_info(in_fc)\r\n    testing = False\r\n\r\n# ------------------------------------------------------------------------\r\n# ---- Create the array, sort extend/join to the input table ----\r\nif rank_fld == \'\':\r\n    rank_fld = \'Rank\'\r\nelse:\r\n    no_good = \' !""#$%&\\\'()*+,-./:;<=>?@[\\\\]^`{|}~\'\r\n    rank_fld = """".join([i for i in rank_fld if i not in no_good])\r\n\r\nif isinstance(fld_names, (list, tuple)):\r\n    order_by = fld_names\r\nelif isinstance(fld_names, (str)):\r\n    if str(fld_names).find("";"") == -1:\r\n        order_by = [fld_names, oid_fld]\r\n    else:\r\n        order_by = fld_names.split("";"") + [oid_fld]\r\n\r\na = arcpy.da.TableToNumPyArray(in_fc, field_names=order_by)\r\n\r\na_s = a[order_by]\r\nsrted = np.argsort(a_s, order=order_by)\r\n\r\ndt = [(oid_fld, \'<i4\'), (rank_fld, \'<i4\')]\r\nj_a = np.zeros(a.shape, dtype=dt)\r\nj_a[oid_fld] = a_s[srted][oid_fld]\r\n\r\nif rank_min in (\'true\', True):  # use regular or rankmin ranking method\r\n    r = a_s[srted][fld_names]\r\n    r = rankmin(r)\r\n    j_a[rank_fld] = r\r\nelse:\r\n    j_a[rank_fld] = np.arange(1, a.shape[0]+1)\r\n#\r\nif not testing:\r\n    arcpy.da.ExtendTable(in_table=in_fc,\r\n                         table_match_field=oid_fld,\r\n                         in_array=j_a,\r\n                         array_match_field=oid_fld)\r\n\r\nfrmt = """"""\r\n{}\r\n:Script....{}\r\n:Ranking... {}\r\n:Using fields...\r\n:   {}\r\n{}\r\n""""""\r\nargs = [""-""*70, script, in_fc, order_by, ""-""*70]\r\nmsg = frmt.format(*args)\r\ntweet(msg)\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n    pass\r\n'"
PointTools/Scripts/arcpytools_pnt.py,11,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\narcpytools_pnt\r\n==============\r\n\r\nScript:   arcpytools_pnt.py\r\n\r\nAuthor:   Dan.Patterson@carleton.ca\r\n\r\nModified: 2018-08-22\r\n\r\nPurpose:  tools for working with numpy arrays\r\n\r\nUseage:\r\n\r\nReferences:\r\n\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nfrom textwrap import dedent, indent\r\nimport numpy as np\r\nimport arcpy\r\n# from arcpytools import array_fc, array_struct, tweet\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\n\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\n__all__ = [\'_xyID\']\r\n\r\n\r\ndef tweet(msg):\r\n    """"""Produce a message for both arcpy and python\r\n    : msg - a text message\r\n    """"""\r\n    m = ""{}"".format(msg)\r\n    arcpy.AddMessage(m)\r\n    print(m)\r\n\r\n\r\n#def _describe(in_fc=None):\r\n#    """"""Simply return the arcpy.da.Describe object\r\n#    : desc.keys() an abbreviated list...\r\n#    : [... \'OIDFieldName\'... \'areaFieldName\', \'baseName\'... \'catalogPath\',\r\n#    :  ... \'dataType\'... \'extent\', \'featureType\', \'fields\', \'file\'... \'hasM\',\r\n#    :  \'hasOID\', \'hasZ\', \'indexes\'... \'lengthFieldName\'... \'name\', \'path\',\r\n#    :  \'rasterFieldName\', ..., \'shapeFieldName\', \'shapeType\',\r\n#    :  \'spatialReference\',  ...]\r\n#    """"""\r\n#    if in_fc is None:\r\n#        return None\r\n#    else:\r\n#        return arcpy.da.Describe(in_fc)\r\n\r\ndef fc_info(in_fc, prn=False):\r\n    """"""Return basic featureclass information, including...\r\n\r\n    Parameters:\r\n    -----------\r\n    shp_fld : field\r\n        field name which contains the geometry object\r\n    oid_fld : field\r\n        the object index/id field name\r\n    SR : spatial reference\r\n        spatial reference object (use SR.name to get the name)\r\n    shp_type : string\r\n        shape type (Point, Polyline, Polygon, Multipoint, Multipatch)\r\n    others : options\r\n        \'areaFieldName\', \'baseName\', \'catalogPath\',\'featureType\',\'fields\',\r\n        \'hasOID\', \'hasM\', \'hasZ\', \'path\'\r\n    all_flds : list\r\n         [i.name for i in desc[\'fields\']]\r\n    """"""\r\n    desc = arcpy.da.Describe(in_fc)\r\n    args = [\'shapeFieldName\', \'OIDFieldName\', \'shapeType\', \'spatialReference\']\r\n    shp_fld, oid_fld, shp_type, SR = [desc[i] for i in args]\r\n    if prn:\r\n        frmt = ""FeatureClass:\\n   {}"".format(in_fc)\r\n        f = ""\\n{!s:<16}{!s:<14}{!s:<10}{!s:<10}""\r\n        frmt += f.format(*args)\r\n        frmt += f.format(shp_fld, oid_fld, shp_type, SR.name)\r\n        tweet(frmt)\r\n    else:\r\n        return shp_fld, oid_fld, shp_type, SR\r\n\r\n\r\n# ---- geometry related -----------------------------------------------------\r\n#\r\ndef _xyID(in_fc, to_pnts=True):\r\n    """"""Convert featureclass geometry (in_fc) to a simple 2D structured array\r\n    :  with ID, X, Y values. Optionally convert to points, otherwise centroid.\r\n    """"""\r\n    flds = [\'OID@\', \'SHAPE@X\', \'SHAPE@Y\']\r\n    args = [in_fc, flds, None, None, to_pnts, (None, None)]\r\n    cur = arcpy.da.SearchCursor(*args)\r\n    a = cur._as_narray()\r\n    a.dtype = [(\'IDs\', \'<i4\'), (\'Xs\', \'<f8\'), (\'Ys\', \'<f8\')]\r\n    del cur\r\n    return a\r\n\r\n\r\ndef array_struct(a, fld_names=[\'X\', \'Y\'], dt=[\'<f8\', \'<f8\']):\r\n    """"""Convert an array to a structured array\r\n\r\n    Parameters:\r\n    -----------\r\n    a : array\r\n        an ndarray with shape at least (N, 2)\r\n    dt : string\r\n        dtype class\r\n    names : string or list of strings\r\n        names for the fields\r\n    """"""\r\n    dts = [(fld_names[i], dt[i]) for i in range(len(fld_names))]\r\n    z = np.zeros((a.shape[0],), dtype=dts)\r\n    names = z.dtype.names\r\n    for i in range(a.shape[1]):\r\n        z[names[i]] = a[:, i]\r\n    return z\r\n\r\n\r\ndef array_fc(a, out_fc, fld_names, SR):\r\n    """"""Array to featureclass/shapefile...optionally including all fields\r\n\r\n    Parameters:\r\n    -----------\r\n    out_fc : string\r\n        featureclass/shapefile... complete path\r\n    fld_names : string or list of strings\r\n        the Shapefield name ie [\'Shape\'] or [\'X\', \'Y\'s]\r\n    SR : spatial reference\r\n        spatial reference object (use SR.name to get the name)\r\n    See also :\r\n        NumpyArrayToFeatureClass, ListFields for information and options\r\n    """"""\r\n    if arcpy.Exists(out_fc):\r\n        arcpy.Delete_management(out_fc)\r\n    arcpy.da.NumPyArrayToFeatureClass(a, out_fc, fld_names, SR)\r\n    return out_fc\r\n\r\n\r\ndef fc_array(in_fc, flds, allpnts):\r\n    """"""Convert a featureclass to an ndarray...with optional fields besides the\r\n    FID/OIDName and Shape fields.\r\n\r\n    Parameters:\r\n    -----------\r\n    in_fc : text\r\n        Full path to the geodatabase and the featureclass name\r\n\r\n    flds : text or list\r\n        - ``\'\'   : just an object id and shape field``\r\n        - ``\'*\'  : all fields in the featureclass or``\r\n        - ``list : specific fields [\'OBJECTID\',\'Shape\',\'SomeClass\', etc]``\r\n\r\n    allpnts : boolean\r\n        - True `explodes` geometry to individual points.\r\n        - False returns the centroid\r\n\r\n    Requires:\r\n    ---------\r\n        fc_info(in_fc) function\r\n\r\n    See also:\r\n    ---------\r\n        FeatureClassToNumPyArray, ListFields for more information in current\r\n        arcpy documentation\r\n    """"""\r\n    out_flds = []\r\n    shp_fld, oid_fld, shp_type, SR = fc_info(in_fc)  # get the base information\r\n    fields = arcpy.ListFields(in_fc)      # all fields in the shapefile\r\n    if flds == """":                        # return just OID and Shape field\r\n        out_flds = [oid_fld, shp_fld]     # FID and Shape field required\r\n    elif flds == ""*"":                     # all fields\r\n        out_flds = [f.name for f in fields]\r\n    else:\r\n        out_flds = [oid_fld, shp_fld]\r\n        for f in fields:\r\n            if f.name in flds:\r\n                out_flds.append(f.name)\r\n    frmt = """"""\\nRunning \'fc_array\' with ....\r\n    \\nfeatureclass... {}\\nFields... {}\\nAll pnts... {}\\nSR... {}\r\n    """"""\r\n    args = [in_fc, out_flds, allpnts, SR.name]\r\n    msg = dedent(frmt).format(*args)\r\n    tweet(msg)\r\n    a = arcpy.da.FeatureClassToNumPyArray(in_fc, out_flds, """", SR, allpnts)\r\n    # out it goes in array format\r\n    return a, out_flds, SR\r\n\r\n\r\ndef arr2pnts(in_fc, as_struct=True, shp_fld=None, SR=None):\r\n    """"""Create points from an array.\r\n    :  in_fc - input featureclass\r\n    :  as_struct - if True, returns a structured array with X, Y fields,\r\n    :            - if False, returns an ndarray with dtype=\'<f8\'\r\n    :Notes: calls fc_info to return featureclass information\r\n    """"""\r\n    if shp_fld is None or SR is None:\r\n        shp_fld, oid_fld, SR = fc_info(in_fc)\r\n    a = arcpy.da.FeatureClassToNumPyArray(in_fc, ""*"", """", SR)\r\n    dt = [(\'X\', \'<f8\'), (\'Y\', \'<f8\')]\r\n    if as_struct:\r\n        shps = np.array([tuple(i) for i in a[shp_fld]], dtype=dt)\r\n    else:\r\n        shps = a[shp_fld]\r\n    return shps, shp_fld, SR\r\n\r\n\r\ndef arr2line(a, out_fc, SR=None):\r\n    """"""create lines from an array""""""\r\n    pass\r\n\r\n\r\ndef shapes2fc(shps, out_fc):\r\n    """"""Create a featureclass/shapefile from poly* shapes.\r\n    :  out_fc - full path and name to the output container (gdb or folder)\r\n    """"""\r\n    msg = ""\\nCan\'t overwrite the {}... rename"".format(out_fc)\r\n    try:\r\n        if arcpy.Exists(out_fc):\r\n            arcpy.Delete_management(out_fc)\r\n        arcpy.CopyFeatures_management(shps, out_fc)\r\n    except ValueError:\r\n        tweet(msg)\r\n\r\n\r\ndef arr2polys(a, out_fc, oid_fld, SR):\r\n    """"""Make poly* features from a structured array.\r\n    :  a - structured array\r\n    :  out_fc: a featureclass path and name, or None\r\n    :  oid_fld - object id field, used to partition the shapes into groups\r\n    :  SR - spatial reference object, or name\r\n    :Returns:\r\n    :-------\r\n    :  Produces the featureclass optionally, but returns the polygons anyway.\r\n    """"""\r\n    arcpy.overwriteOutput = True\r\n    pts = []\r\n    keys = np.unique(a[oid_fld])\r\n    for k in keys:\r\n        w = np.where(a[oid_fld] == k)[0]\r\n        v = a[\'Shape\'][w[0]:w[-1] + 1]\r\n        pts.append(v)\r\n    # Create a Polygon from an Array of Points, save to featueclass if needed\r\n    s = []\r\n    for pt in pts:\r\n        s.append(arcpy.Polygon(arcpy.Array([arcpy.Point(*p) for p in pt]), SR))\r\n    return s\r\n\r\n\r\ndef output_polylines(out_fc, SR, pnt_groups):\r\n    """"""Produce the output polygon featureclass.\r\n    :Requires:\r\n    :--------\r\n    : - A list of lists of points\r\n    :   aline = [[[0, 0], [1, 1]]]  # a list of points\r\n    :   aPolyline = [[aline]]       # a list of lists of points\r\n    """"""\r\n    msg = \'\\nRead the script header... A projected coordinate system required\'\r\n    assert (SR is not None), msg\r\n    polylines = []\r\n    for pnts in pnt_groups:\r\n        for pair in pnts:\r\n            arr = arcpy.Array([arcpy.Point(*xy) for xy in pair])\r\n            pl = arcpy.Polyline(arr, SR)\r\n            polylines.append(pl)\r\n    if arcpy.Exists(out_fc):     # overwrite any existing versions\r\n        arcpy.Delete_management(out_fc)\r\n    arcpy.CopyFeatures_management(polylines, out_fc)\r\n    return\r\n\r\n\r\ndef output_polygons(out_fc, SR, pnt_groups):\r\n    """"""Produce the output polygon featureclass.\r\n\r\n    Parameters:\r\n    -----------\r\n    out_fc : string\r\n        The path and name of the featureclass to be created.\r\n    SR : spatial reference of the output featureclass\r\n    pnts_groups :\r\n        The point groups, list of lists of points, to include parts rings.\r\n\r\n    Requires:\r\n    --------\r\n\r\n    - A list of lists of points.  Four points form a triangle is the minimum\r\n    -  aline = [[0, 0], [1, 1]]  # a list of points\r\n    -  aPolygon = [aline]        # a list of lists of points\r\n    """"""\r\n    msg = \'\\nRead the script header... A projected coordinate system required\'\r\n    assert (SR is not None), msg\r\n    polygons = []\r\n    for pnts in pnt_groups:\r\n        for pair in pnts:\r\n            arr = arcpy.Array([arcpy.Point(*xy) for xy in pair])\r\n            pl = arcpy.Polygon(arr, SR)\r\n            polygons.append(pl)\r\n    if arcpy.Exists(out_fc):     # overwrite any existing versions\r\n        arcpy.Delete_management(out_fc)\r\n    arcpy.CopyFeatures_management(polygons, out_fc)\r\n    return\r\n\r\n# ---- formatting, from arraytools ------------------------------------------\r\n#\r\n# ----------------------------------------------------------------------\r\n# (4) frmt_rec .... code section\r\n#  frmt_rec requires _col_format\r\ndef _col_format(a, c_name=""c00"", deci=0):\r\n    """"""Determine column format given a desired number of decimal places.\r\n    Used by frmt_struct.\r\n\r\n    `a` : column\r\n        A column in an array.\r\n    `c_name` : text\r\n        column name\r\n    `deci` : int\r\n        Desired number of decimal points if the data are numeric\r\n\r\n    Notes:\r\n    -----\r\n        The field is examined to determine whether it is a simple integer, a\r\n        float type or a list, array or string.  The maximum width is determined\r\n        based on this type.\r\n\r\n        Checks were also added for (N,) shaped structured arrays being\r\n        reformatted to (N, 1) shape which sometimes occurs to facilitate array\r\n        viewing.  A kludge at best, but it works for now.\r\n    """"""\r\n    a_kind = a.dtype.kind\r\n    if a_kind in (\'i\', \'u\'):  # ---- integer type\r\n        w_, m_ = [\':> {}.0f\', \'{:> 0.0f}\']\r\n        col_wdth = len(m_.format(a.max())) + 1\r\n        col_wdth = max(len(c_name), col_wdth) + 1  # + deci\r\n        c_fmt = w_.format(col_wdth, 0)\r\n    elif a_kind == \'f\' and np.isscalar(a[0]):  # ---- float type with rounding\r\n        w_, m_ = [\':> {}.{}f\', \'{:> 0.{}f}\']\r\n        a_max, a_min = np.round(np.sort(a[[0, -1]]), deci)\r\n        col_wdth = max(len(m_.format(a_max, deci)),\r\n                       len(m_.format(a_min, deci))) + 1\r\n        col_wdth = max(len(c_name), col_wdth) + 1\r\n        c_fmt = w_.format(col_wdth, deci)\r\n    # ---- lists, arrays, strings. Check for (N,) vs (N,1)\r\n    # I made some changes in how col_wdth is determined, old is commented\r\n    else:\r\n        if a.ndim == 1:  # ---- check for (N, 1) format of structured array\r\n            a = a[0]\r\n        dt = a.dtype.descr[0][1]\r\n        col_wdth = int("""".join([i for i in dt if i.isdigit()]))\r\n#       col_wdth = max([len(str(i)) for i in a])\r\n        col_wdth = max(len(c_name), col_wdth) + 1  # + deci\r\n        c_fmt = ""!s:>"" + ""{}"".format(col_wdth)\r\n    return c_fmt, col_wdth\r\n\r\n\r\ndef pd_(a, deci=2, use_names=True, prn=True):\r\n    """"""see help for `frmt_rec`...""""""\r\n    ret = frmt_rec(a, deci=deci, use_names=use_names, prn=prn)\r\n    return ret\r\n\r\n\r\ndef frmt_rec(a, deci=2, use_names=True, prn=True):\r\n    """"""Format a structured array with a mixed dtype.\r\n\r\n    NOTE : Can be called as `pd_(a, ... )` to emulate pandas dataframes\r\n        You should limit large arrays to a slice ie. a[:50]\r\n\r\n    Requires:\r\n    -------\r\n    `a` : array\r\n        A structured/recarray\r\n    `deci` : int\r\n        To facilitate printing, this value is the number of decimal\r\n        points to use for all floating point fields.\r\n    `use_names` : boolean\r\n        If no names are available, then create them\r\n    `prn` : boolean\r\n        True to print, False to return the string\r\n    Notes:\r\n    -----\r\n        `_col_format` : does the actual work of obtaining a representation of\r\n        the column format.\r\n\r\n        It is not really possible to deconstruct the exact number of decimals\r\n        to use for float values, so a decision had to be made to simplify.\r\n    """"""\r\n    dt_names = a.dtype.names\r\n    N = len(dt_names)\r\n    c_names = [[""C{:02.0f}"".format(i) for i in range(N)], dt_names][use_names]\r\n    # ---- get the column formats from ... _col_format ----\r\n    dts = []\r\n    wdths = []\r\n    pair = list(zip(dt_names, c_names))\r\n    for i in range(len(pair)):\r\n        fld, nme = pair[i]\r\n        c_fmt, col_wdth = _col_format(a[fld], c_name=nme, deci=deci)\r\n        dts.append(c_fmt)\r\n        wdths.append(col_wdth)\r\n    row_frmt = "" "".join([(\'{\' + i + \'}\') for i in dts])\r\n    hdr = [""!s:>"" + ""{}"".format(wdths[i]) for i in range(N)]\r\n    hdr2 = "" "".join([""{"" + hdr[i] + ""}"" for i in range(N)])\r\n    header = ""--n--"" + hdr2.format(*c_names)\r\n    header = ""\\n{}\\n{}"".format(header, ""-""*len(header))\r\n    txt = [header]\r\n    # ---- check for structured arrays reshaped to (N, 1) instead of (N,) ----\r\n    len_shp = len(a.shape)\r\n    idx = 0\r\n    for i in range(a.shape[0]):\r\n        if len_shp == 1:  # ---- conventional (N,) shaped array\r\n            row = "" {:03.0f} "".format(idx) + row_frmt.format(*a[i])\r\n        else:             # ---- reformatted to (N, 1)\r\n            row = "" {:03.0f} "".format(idx) + row_frmt.format(*a[i][0])\r\n        idx += 1\r\n        txt.append(row)\r\n    msg = ""\\n"".join([i for i in txt])\r\n    if prn:\r\n        print(msg)\r\n    else:\r\n        return msg\r\n\r\n# ----------------------------------------------------------------------\r\n# (5) form_ ... code section .....\r\n#  form_ requires make_row_format\r\n\r\ndef col_hdr(num=7):\r\n    """"""Print numbers from 1 to 10*num to show column positions""""""\r\n    args = [((\'{:<10}\')*num).format(*\'0123456789\'),\r\n            \'0123456789\'*num, \'-\'*10*num]\r\n    s = ""\\n{}\\n{}\\n{}"".format(args[0][1:], args[1][1:], args[2])  # *args)\r\n    print(s)\r\n\r\n\r\ndef make_row_format(dim=3, cols=5, a_kind=\'f\', deci=1,\r\n                    a_max=10, a_min=-10, wdth=100, prnt=False):\r\n    """"""Format the row based on input parameters\r\n\r\n    `dim` - int\r\n        Number of dimensions\r\n    `cols` : int\r\n        Columns per dimension\r\n\r\n    `a_kind`, `deci`, `a_max` and `a_min` allow you to specify a data type,\r\n    number of decimals and maximum and minimum values to test formatting.\r\n    """"""\r\n    if a_kind not in [\'f\', \'i\']:\r\n        a_kind = \'f\'\r\n    w_, m_ = [[\':{}.0f\', \'{:0.0f}\'], [\':{}.{}f\', \'{:0.{}f}\']][a_kind == \'f\']\r\n    m_fmt = max(len(m_.format(a_max, deci)), len(m_.format(a_min, deci))) + 1\r\n    w_fmt = w_.format(m_fmt, deci)\r\n    suffix = \'  \'\r\n    while m_fmt*cols*dim > wdth:\r\n        cols -= 1\r\n        suffix = \'.. \'\r\n    row_sub = ((\'{\' + w_fmt + \'}\')*cols + suffix)\r\n    row_frmt = (row_sub*dim).strip()\r\n    if prnt:\r\n        frmt = ""Row format: dim cols: ({}, {})  kind: {} decimals: {}\\n\\n{}""\r\n        print(dedent(frmt).format(dim, cols, a_kind, deci, row_frmt))\r\n        a = np.random.randint(a_min, a_max+1, dim*cols)\r\n        col_hdr(wdth//10)  # run col_hdr to produce the column headers\r\n        print(row_frmt.format(*a))\r\n    else:\r\n        return row_frmt\r\n\r\n\r\ndef form_(a, deci=2, wdth=100, title=""Array"", prefix="". . "", prn=True):\r\n    """"""Alternate format to frmt_ function.\r\n    Inputs are largely the same.\r\n\r\n    Requires:\r\n    ---------\r\n    make_row_format, _col_format - functions\r\n        used to format the rows and columns\r\n    """"""\r\n    def _piece(sub, i, frmt, linewidth):\r\n        """"""piece together 3D chunks by row""""""\r\n        s0 = sub.shape[0]\r\n        block = np.hstack([sub[j] for j in range(s0)])\r\n        txt = """"\r\n        if i is not None:\r\n            fr = ("":arr[{}"" + "", :{}""*len(a.shape[1:]) + ""]\\n"")\r\n            txt = fr.format(i, *sub.shape)\r\n        for line in block:\r\n            ln = frmt.format(*line)[:linewidth]\r\n            end = [""\\n"", ""...\\n""][len(ln) >= linewidth]\r\n            txt += indent(ln + end, "". . "")\r\n        return txt\r\n    # ---- main section ----\r\n    out = ""\\n{}... ndim: {}  shape: {}\\n"".format(title, a.ndim, a.shape)\r\n    linewidth = wdth\r\n    if a.ndim <= 1:\r\n        return a\r\n    elif a.ndim == 2:\r\n        a = a.reshape((1,) + a.shape)\r\n    # ---- pull the 1st and 3rd dimension for 3D and 4D arrays\r\n    frmt = make_row_format(dim=a.shape[-3],\r\n                           cols=a.shape[-1],\r\n                           a_kind=a.dtype.kind,\r\n                           deci=deci,\r\n                           a_max=a.max(),\r\n                           a_min=a.min(),\r\n                           wdth=wdth,\r\n                           prnt=False)\r\n    if a.ndim == 3:\r\n        s0, s1, s2 = a.shape\r\n        out += _piece(a, None, frmt, linewidth)  # ---- _piece ----\r\n    elif a.ndim == 4:\r\n        s0, s1, s2, _ = a.shape\r\n        for i in range(s0):\r\n            out = out + ""\\n"" + _piece(a[i], i, frmt, linewidth)  # ---- _piece\r\n    if prn:\r\n        print(out)\r\n    else:\r\n        return out\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n#    _demo()\r\n#    gdb_fc = [\'Data\', \'point_tools.gdb\', \'radial_pnts\']\r\n#    in_fc = ""/"".join(script.split(""/"")[:-2] + gdb_fc)\r\n#    result = fc_array(in_fc, flds="""", allpnts=True)  # a, out_flds, SR\r\n'"
PointTools/Scripts/circlepnts.py,10,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   createcompass.py\r\n:Author   Dan_Patterson@carleton.ca\r\n:Modified: 2017-04-02\r\n: if north angle is needed, you can use this to convert\r\n:if fromNorth:\r\n:    ang = np.mod((450.0 - ang), 360.)\r\n""""""\r\n\r\n# --------------------------------------------------------------------------\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\nfrom arcpytools_pnt import tweet\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2,\r\n                    suppress=True, threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\ndef _circle(radius=10, theta=22.5, xc=0.0, yc=0.0):\r\n    """"""Produce a circle/ellipse depending on parameters.\r\n    :  radius - distance from centre\r\n    :  theta - either a single value to form angles about a circle or\r\n    :        - a list or tuple of the desired angles\r\n    """"""\r\n    angles = np.deg2rad(np.arange(180.0, -180.0-theta, step=-theta))\r\n    x_s = radius*np.cos(angles) + xc    # X values\r\n    y_s = radius*np.sin(angles) + yc    # Y values\r\n    pnts = np.c_[x_s, y_s]\r\n    return pnts\r\n\r\n\r\n# --------------------------------------------------------------------------\r\ninFC = sys.argv[1]   # r""C:\\Data\\points\\points.gdb\\fishnet_label""\r\noutFC = sys.argv[2]  # r""C:\\Data\\points\\pnts2.shp""\r\nradius = float(sys.argv[3])  # radius = 2\r\ntheta = float(sys.argv[4])\r\na = arcpy.da.FeatureClassToNumPyArray(inFC, [""SHAPE@X"", ""SHAPE@Y""])\r\n\r\nfrmt = """"""... {} ...\r\n:Input featureclass : {}\r\n:Output featureclass: {}\r\n:Radius {}, angle step {}\r\n:Points:\r\n{}\r\n:\r\n""""""\r\nargs = [script, inFC, outFC, radius, theta, a]\r\nmsg = frmt.format(*args)\r\ntweet(msg)\r\n# ---- option (1) read X and Y separately, then reset the dtype names\r\n# ---- get the circle values, stack and set dtype\r\n# or a list like... theta = [0, 90, 180, 270]\r\n#    theta = [0, 90, 180, 270]\r\n\r\na.dtype = [(\'X\', \'<f8\'), (\'Y\', \'<f8\')]\r\nb = [_circle(radius, theta, a[\'X\'][i], a[\'Y\'][i])\r\n     for i in range(len(a))]\r\nc = np.vstack(b)\r\nc.dtype = a.dtype\r\nc = np.squeeze(c)\r\n\r\narcpy.da.NumPyArrayToFeatureClass(c, outFC, c.dtype.names)\r\narcpy.AddXY_management(outFC)\r\n\r\n# --------------------------------------------------------------------\r\nif __name__ == \'__main__\':\r\n    """"""produce some points around a centroid at angles and distances""""""\r\n# ---- option (2) read the centroid coordinates then reset the dtype name\r\n# ---- get the circle values, stack and set dtype\r\n#    a = arcpy.da.FeatureClassToNumPyArray(inFC, ""SHAPE@XY"")\r\n#    a.dtype = [(\'XY\', \'<f8\', (2,))]\r\n#    b = [_circle(radius, theta, a[\'XY\'][i,0], a[\'XY\'][i,1])\r\n#        for i in range(len(a))]\r\n'"
PointTools/Scripts/closest.py,23,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   closest.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2017-04-11\r\n:\r\n:Purpose:  Determine the nearest points based on euclidean distance within\r\n:  a point file and then connect them\r\n:References:\r\n:----------\r\n: - see near.py documentation for documentation\r\n:\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\n\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\nfrom arcpytools_pnt import fc_info, tweet\r\n\r\n# from textwrap import dedent\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.1f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=120, precision=2,\r\n                    suppress=True, threshold=140, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')\r\n\r\nscript = sys.argv[0]\r\n\r\narcpy.env.overwriteOutput = True\r\n# ---- functions ----\r\n\r\n\r\ndef n_near(a, N=3, ordered=True):\r\n    """"""Return the coordinates and distance to the nearest N points within\r\n    :  an 2D numpy array, \'a\', with optional ordering of the inputs.\r\n    :Requires:\r\n    :--------\r\n    : a - an ndarray of uniform int or float dtype.  Extract the fields\r\n    :     representing the x,y coordinates before proceeding.\r\n    : N - number of closest points to return\r\n    :Returns:\r\n    :-------\r\n    :  A structured array is returned containing an ID number.  The ID number\r\n    :  is the ID of the points as they were read.  The array will contain\r\n    :  (C)losest fields and distance fields\r\n    :  (C0_X, C0_Y, C1_X, C1_Y, Dist0, Dist1 etc) representing coordinates\r\n    :  and distance to the required \'closest\' points.\r\n    """"""\r\n    if not (isinstance(a, (np.ndarray)) and (N >= 1)):\r\n        print(""\\nInput error...read the docs\\n\\n{}"".format(n_near.__doc__))\r\n        return a\r\n    rows, cols = a.shape\r\n    dt_near = [(\'Xo\', \'<f8\'), (\'Yo\', \'<f8\')]\r\n    dt_new = [(\'C{}\'.format(i) + \'{}\'.format(j), \'<f8\')\r\n              for i in range(N)\r\n              for j in [\'_X\', \'_Y\']]\r\n    dt_near.extend(dt_new)\r\n    dt_dist = [(\'Dist{}\'.format(i), \'<f8\') for i in range(N)]\r\n    dt = [(\'ID\', \'<i4\'), *dt_near, *dt_dist]\r\n    n_array = np.zeros((rows,), dtype=dt)\r\n    n_array[\'ID\'] = np.arange(rows)\r\n    # ---- distance matrix calculation using einsum ----\r\n    if ordered:\r\n        a = a[np.argsort(a[:, 0])]\r\n    b = a.reshape(np.prod(a.shape[:-1]), 1, a.shape[-1])\r\n    diff = b - a\r\n    dist = np.einsum(\'ijk,ijk->ij\', diff, diff)\r\n    d = np.sqrt(dist).squeeze()\r\n    # ---- format for use in structured array output ----\r\n    # steps are outlined as follows....\r\n    #\r\n    kv = np.argsort(d, axis=1)       # sort \'d\' on last axis to get keys\r\n    coords = a[kv]                   # pull out coordinates using the keys\r\n    s0, s1, s2 = coords.shape\r\n    coords = coords.reshape((s0, s1*s2))\r\n    dist = np.sort(d)[:, 1:]         # slice sorted distances, skip 1st\r\n    # ---- construct the structured array ----\r\n    dt_names = n_array.dtype.names\r\n    s0, s1, s2 = (1, (N+1)*2 + 1, len(dt_names))\r\n    for i in range(0, s1):           # coordinate field names\r\n        nm = dt_names[i+1]\r\n        n_array[nm] = coords[:, i]\r\n    dist_names = dt_names[s1:s2]\r\n    for i in range(N):               # fill n_array with the results\r\n        nm = dist_names[i]\r\n        n_array[nm] = dist[:, i]\r\n    return coords, dist, n_array\r\n\r\n\r\ndef _uniq_by_row_col(a, axis=0):\r\n    """"""unique to emulate numpy 1.13 ...\r\n    :Requires:\r\n    :--------\r\n    : a - an array of uniform dtype with ndim > 1\r\n    : axis - if 0, then unique rows are returned, if 1, then unique columns\r\n    :\r\n    :References:\r\n    :----------\r\n    : - https://github.com/numpy/numpy/blob/master/numpy/lib/arraysetops.py\r\n    : - http://stackoverflow.com/questions/16970982/\r\n    :          find-unique-rows-in-numpy-array?noredirect=1&lq=1\r\n    :Notes:\r\n    :-----  Must reshape to a contiguous 2D array for this to work...\r\n    : a.dtype.char - [\'AllInteger\'] + [\'Datetime\'] + \'S\') = \'bBhHiIlLqQpPMmS\'\r\n    """"""\r\n    a = np.asanyarray(a)\r\n    a = np.swapaxes(a, axis, 0)\r\n    orig_shape, _ = a.shape, a.dtype  # orig_shape, orig_dtype\r\n    a = a.reshape(orig_shape[0], -1)\r\n    a = np.ascontiguousarray(a)\r\n    if a.dtype.char in (\'bBhHiIlLqQpPMmS\'):\r\n        dt = np.dtype((np.void, a.dtype.itemsize * a.shape[1]))\r\n    else:\r\n        dt = [(\'f{i}\'.format(i=i), a.dtype) for i in range(a.shape[1])]\r\n    b = a.view(dt)\r\n    _, idx = np.unique(b, return_index=True)\r\n    unique_a = a[idx]\r\n    return unique_a\r\n\r\n\r\ndef connect(in_fc, out_fc, N=1, testing=False):\r\n    """"""Run the analysis to form the closest point pairs.\r\n    :  Calls n_near to produce the nearest features.\r\n    """"""\r\n    shp_fld, oid_fld, shp_type, SR = fc_info(in_fc)\r\n    a = arcpy.da.FeatureClassToNumPyArray(in_fc, shp_fld, """", SR)\r\n    dt = \'<f8\'\r\n    b = np.array([tuple(i) for i in a[shp_fld]], dtype=dt)\r\n    coords, dist, n_array = n_near(b, N, ordered=True)  # ---- run n_near ----\r\n    fr_to = coords[:, :(N+1)*2]\r\n    frum = fr_to[:, :2]\r\n    twos = fr_to[:, 2:].reshape(-1, N, 2)\r\n    r = []\r\n    for i in range(len(frum)):\r\n        f = frum[i]\r\n        t = twos[i]\r\n        for j in range(len(t)):\r\n            r.append(np.array([f, t[j]]))\r\n    rr = np.array(r)\r\n    r0 = np.array([i[np.lexsort((i[:, 1], i[:, 0]))] for i in rr])  # slicesort\r\n    r1 = r0.reshape(-1, 4)\r\n    r2 = _uniq_by_row_col(r1, axis=0)  # use if np.version < 1.13\r\n    # r2 = unique_2d(r1)\r\n    r3 = r2[np.argsort(r2[..., 0])]\r\n    r3 = r3.reshape(-1, 2, 2)\r\n    if not testing:\r\n        s = []\r\n        for pt in r3:\r\n            arr = arcpy.Array([arcpy.Point(*p) for p in pt])\r\n            s.append(arcpy.Polyline(arr, SR))\r\n        if arcpy.Exists(out_fc):\r\n            arcpy.Delete_management(out_fc)\r\n        arcpy.CopyFeatures_management(s, out_fc)\r\n        return None\r\n    else:\r\n        return a, b, r0, r1, r2, r3\r\n\r\n\r\n# ---- Run the analysis ----\r\nfrmt = """"""\\n\r\n:Running ... {}\r\n:Using ..... {}\r\n:Finding ... {} closest points and forming connections\r\n:Producing.. {}\\n\r\n""""""\r\n\r\nin_fc = sys.argv[1]\r\nN = int(sys.argv[2])\r\nout_fc = sys.argv[3]\r\nargs = [script, in_fc, N, out_fc]\r\ntweet(frmt.format(*args))                    # call tweet\r\nret = connect(in_fc, out_fc, N=N, testing=False)   # call connect\r\n\r\n# ---------------------------------------------------------------------\r\nif __name__ == ""__main__"":\r\n    """"""Main section...   """"""\r\n#    print(""Script... {}"".format(script))\r\n""""""\r\n    in_fc = r""C:\\GIS\\array_projects\\data\\Pro_base.gdb\\small""\r\n    out_fc = r""C:\\GIS\\array_projects\\data\\Pro_base.gdb\\ft3""\r\n    N = 1\r\n    testing = True\r\n    a, b, r0, r1, r2, r3 = connect(in_fc, out_fc, N=N, testing=True)\r\n""""""\r\n'"
PointTools/Scripts/closest_od.py,33,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   closest_od.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2018-05-11\r\n:\r\n:Purpose:  Determine the nearest points based on euclidean distance between\r\n: point files and then connect them\r\n:\r\n:References:\r\n:----------\r\n: - see near.py documentation for documentation\r\n:\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\n\r\nimport sys\r\nimport warnings\r\nimport numpy as np\r\nimport arcpy\r\nfrom arcpytools_pnt import fc_info, tweet\r\n\r\nwarnings.simplefilter(\'ignore\', FutureWarning)\r\n\r\n# from textwrap import dedent\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.1f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=120, precision=2,\r\n                    suppress=True, threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')\r\n\r\nscript = sys.argv[0]\r\n\r\narcpy.env.overwriteOutput = True\r\n# ---- functions ----\r\n\r\ndef e_dist(a, b, metric=\'euclidean\'):\r\n    """"""Distance calculation for 1D, 2D and 3D points using einsum\r\n    : a, b   - list, tuple, array in 1,2 or 3D form\r\n    : metric - euclidean (\'e\',\'eu\'...), sqeuclidean (\'s\',\'sq\'...),\r\n    :-----------------------------------------------------------------------\r\n    """"""\r\n    a = np.asarray(a)\r\n    b = np.atleast_2d(b)\r\n    a_dim = a.ndim\r\n    b_dim = b.ndim\r\n    if a_dim == 1:\r\n        a = a.reshape(1, 1, a.shape[0])\r\n    if a_dim >= 2:\r\n        a = a.reshape(np.prod(a.shape[:-1]), 1, a.shape[-1])\r\n    if b_dim > 2:\r\n        b = b.reshape(np.prod(b.shape[:-1]), b.shape[-1])\r\n    diff = a - b\r\n    dist_arr = np.einsum(\'ijk,ijk->ij\', diff, diff)\r\n    if metric[:1] == \'e\':\r\n        dist_arr = np.sqrt(dist_arr)\r\n    dist_arr = np.squeeze(dist_arr)\r\n    return dist_arr\r\n\r\n\r\ndef line_dir(orig, dest, fromNorth=False):\r\n    """"""Direction of a line given 2 points\r\n    : orig, dest - two points representing the start and end of a line.\r\n    : fromNorth - True or False gives angle relative to x-axis)\r\n    :Notes:\r\n    :\r\n    """"""\r\n    orig = np.atleast_2d(orig)\r\n    dest = np.atleast_2d(dest)\r\n    dxy = dest - orig\r\n    ang = np.degrees(np.arctan2(dxy[:, 1], dxy[:, 0]))\r\n    if fromNorth:\r\n        ang = np.mod((450.0 - ang), 360.)\r\n    return ang\r\n\r\n\r\ndef to_array(in_fc):\r\n    """"""Extract the shapes and produce a coordinate array.\r\n    """"""\r\n    shp_fld, oid_fld, shp_type, SR = fc_info(in_fc)\r\n    in_flds = [oid_fld] + [\'SHAPE@X\', \'SHAPE@Y\']\r\n    a = arcpy.da.FeatureClassToNumPyArray(in_fc, in_flds)\r\n    a = a[[\'SHAPE@X\', \'SHAPE@Y\']]\r\n    a = a.view(np.float64).reshape(a.shape[0], 2).copy()\r\n    return a, SR\r\n\r\n\r\ndef n_near_od(orig, dest, N=3, ordered=True):\r\n    """"""Return the coordinates and distance to the nearest N points between\r\n    :  two 2D numpy arrays, \'a\' and \'b\', with optional ordering of the inputs.\r\n    :Requires:\r\n    :--------\r\n    : a, b - ndarrays of uniform int or float dtype.  Extract the fields\r\n    :     representing the x,y coordinates before proceeding.\r\n    : N - number of closest points to return\r\n    :Returns:\r\n    :-------\r\n    :  A structured array is returned containing an ID number.  The ID number\r\n    :  is the ID of the points as they were read.  The array will contain\r\n    :  (C)losest fields and distance fields\r\n    :  (Dest0_X, Dest0_Y, Dest1_X, Dest1_Y, Dist0, Dist1 etc)\r\n    :    representing coordinates\r\n    :  and distance to the required \'closest\' points.\r\n    """"""\r\n    rows, cols = orig.shape\r\n    dt_near = [(\'Orig_X\', \'<f8\'), (\'Orig_Y\', \'<f8\')]\r\n    dt_new = [(\'Dest{}\'.format(i) + \'{}\'.format(j), \'<f8\')\r\n              for i in range(N)\r\n              for j in [\'_X\', \'_Y\']]\r\n    dt_near.extend(dt_new)\r\n    dt_dist = [(\'Dist{}\'.format(i), \'<f8\') for i in range(N)]\r\n    dt_ang = [(\'Angle0\', \'<f8\')]\r\n    dt = [(\'ID\', \'<i4\'), *dt_near, *dt_dist]\r\n    dt.extend(dt_ang)\r\n    n_array = np.zeros((rows,), dtype=dt)\r\n    n_array[\'ID\'] = np.arange(1, rows+1)  # 1 to N+1 numbering like OBJECTID\r\n    n_array[\'Orig_X\'] = orig[:, 0]\r\n    n_array[\'Orig_Y\'] = orig[:, 1]\r\n    #\r\n    # ---- distance matrix calculation using einsum ----\r\n    d = e_dist(orig, dest, metric=\'euclidean\')\r\n    #\r\n    # ---- format for use in structured array output ----\r\n    # steps are outlined as follows....\r\n    #\r\n    kv = np.argsort(d, axis=1)   # sort \'d\' on last axis to get keys\r\n    toos = dest[kv]              # pull coordinates from destination\r\n    s0, s1, s2 = toos.shape\r\n    toos = toos.reshape((s0, s1*s2))\r\n    coords = np.c_[orig, toos]\r\n    dist = np.sort(d)  #[:, 1:]         # slice sorted distances, skip 1st\r\n    # ---- construct the structured array ----\r\n    dt_names = n_array.dtype.names\r\n    s0, s1, s2 = (1, (N+1)*2 + 1, len(dt_names))\r\n    for i in range(1, s1+1):           # coordinate field names\r\n        nm = dt_names[i]\r\n        n_array[nm] = coords[:, i-1]\r\n    dist_names = dt_names[s1:]\r\n    for i in range(N):               # fill n_array with the results\r\n        nm = dist_names[i]\r\n        n_array[nm] = dist[:, i]\r\n    return coords, dist, n_array\r\n\r\n\r\ndef _uniq_by_row_col(a, axis=0):\r\n    """"""unique to emulate numpy 1.13 ...\r\n    :Requires:\r\n    :--------\r\n    : a - an array of uniform dtype with ndim > 1\r\n    : axis - if 0, then unique rows are returned, if 1, then unique columns\r\n    :\r\n    :References:\r\n    :----------\r\n    : - https://github.com/numpy/numpy/blob/master/numpy/lib/arraysetops.py\r\n    : - http://stackoverflow.com/questions/16970982/\r\n    :          find-unique-rows-in-numpy-array?noredirect=1&lq=1\r\n    :Notes:\r\n    :-----  Must reshape to a contiguous 2D array for this to work...\r\n    : a.dtype.char - [\'AllInteger\'] + [\'Datetime\'] + \'S\') = \'bBhHiIlLqQpPMmS\'\r\n    """"""\r\n    a = np.asanyarray(a)\r\n    a = np.swapaxes(a, axis, 0)\r\n    orig_shape, _ = a.shape, a.dtype  # orig_shape, orig_dtype\r\n    a = a.reshape(orig_shape[0], -1)\r\n    a = np.ascontiguousarray(a)\r\n    if a.dtype.char in (\'bBhHiIlLqQpPMmS\'):\r\n        dt = np.dtype((np.void, a.dtype.itemsize * a.shape[1]))\r\n    else:\r\n        dt = [(\'f{i}\'.format(i=i), a.dtype) for i in range(a.shape[1])]\r\n    b = a.view(dt)\r\n    _, idx = np.unique(b, return_index=True)\r\n    unique_a = a[idx]\r\n    return unique_a\r\n\r\n\r\ndef make_polyline(pnts, out_=None, sr=None):\r\n    """"""make the polyline from the points\r\n    """"""\r\n    s = []\r\n    for pt in pnts:\r\n        arr = arcpy.Array([arcpy.Point(*p) for p in pt])\r\n        s.append(arcpy.Polyline(arr, sr))\r\n    if arcpy.Exists(out_):\r\n        arcpy.Delete_management(out_)\r\n    arcpy.CopyFeatures_management(s, out_)\r\n\r\n\r\ndef connect_od(orig_fc, dest_fc, out_fc, N=1, testing=False):\r\n    """"""Run the analysis to form the closest point pairs.\r\n    :  Calls n_near_od to produce the nearest features.\r\n    """"""\r\n    orig, SR0 = to_array(orig_fc)                 # call to_array\r\n    dest, SR1 = to_array(dest_fc)\r\n    # ---- run n_near ----\r\n    coords, dist, n_array = n_near_od(orig, dest, N, ordered=True)\r\n    # ----\r\n#    fr_to = coords[:, :(N+1)*2]\r\n#    frum = fr_to[:, :2]\r\n#    twos = fr_to[:, 2:].reshape(-1, N, 2)\r\n#    r = []\r\n#    for i in range(len(frum)):\r\n#        f = frum[i]\r\n#        t = twos[i]\r\n#        for j in range(len(t)):\r\n#            r.append(np.array([f, t[j]]))\r\n#    rr = np.array(r)\r\n#    r0 = np.array([i[np.lexsort((i[:, 1], i[:, 0]))] for i in rr])  # slicesort\r\n#    r1 = r0.reshape(-1, 4)\r\n#    r2 = _uniq_by_row_col(r1, axis=0)  # use if np.version < 1.13\r\n#    # r2 = unique_2d(r1)\r\n#    r3 = r2[np.argsort(r2[..., 0])]\r\n#    r3 = r3.reshape(-1, 2, 2)\r\n#    #\r\n#    # add angles\r\n    n = n_array.shape[0]\r\n    f = n_array[[\'Orig_X\', \'Orig_Y\']].view(np.float64).reshape(n, 2).copy()\r\n    t = n_array[[\'Dest0_X\', \'Dest0_Y\']].view(np.float64).reshape(n, 2).copy()\r\n    #\r\n    # calculate the angle\r\n    ang = line_dir(f, t, fromNorth=False)\r\n    n_array[\'Angle0\'] = ang\r\n#   #\r\n    # form the points\r\n    pnts = np.array(list(zip(f, t)))\r\n\r\n    if not testing:\r\n        make_polyline(pnts, out_=out_fc, sr=SR0)\r\n        arcpy.da.ExtendTable(out_fc, \'OBJECTID\', n_array, \'ID\')\r\n    return orig, dest,pnts, n_array\r\n\r\n\r\n# ---- Run the analysis ----\r\nfrmt = """"""\\n\r\n:Running ... {}\r\n:Testing ... {}\r\n:Using .....\r\n:  origins {}\r\n:  destins {}\r\n:Finding ... {} closest points and forming connections\r\n:Producing.. {}\\n\r\n""""""\r\n\r\n# ---- Run the analysis ----\r\n#\r\ndef _tool():\r\n    """"""Run the analysis from the tool\r\n    """"""\r\n    testing = False\r\n    orig_fc = sys.argv[1]\r\n    dest_fc = sys.argv[2]\r\n    N = int(sys.argv[3])\r\n    out_fc = sys.argv[4]\r\n#    out_fc = r""C:\\GIS\\A_Tools_scripts\\PointTools\\Data\\Near_testing.gdb\\a2b""\r\n    args = [\'closest_od.py\', testing, orig_fc, dest_fc, N, out_fc]\r\n    return args\r\n\r\n\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    pth = ""/"".join(script.split(""/"")[:-2]) + ""/Data/Near_testing.gdb""\r\n    orig_fc = pth + ""/orig_0""\r\n    dest_fc = pth + ""/dest_0""\r\n    N = 1\r\n#    out_fc = r""C:\\GIS\\A_Tools_scripts\\PointTools\\Data\\Near_testing.gdb\\a2b""\r\n    out_fc = None\r\n    args = [script, testing, orig_fc, dest_fc, N, out_fc]\r\nelse:\r\n    args = _tool()\r\n\r\n\r\ntweet(frmt.format(*args))                    # call tweet\r\n__, testing, orig_fc, dest_fc, N, out_fc = args\r\nreturned = connect_od(orig_fc, dest_fc, out_fc, N=N, testing=testing)   # call connect\r\norig, dest, pnts, n_array = returned\r\n\r\n# ---------------------------------------------------------------------\r\nif __name__ == ""__main__"":\r\n    """"""Main section...   """"""\r\n#    print(""Script... {}"".format(script))\r\n""""""\r\n    in_fc = r""C:\\GIS\\array_projects\\data\\Pro_base.gdb\\small""\r\n    out_fc = r""C:\\GIS\\array_projects\\data\\Pro_base.gdb\\ft3""\r\n    N = 1\r\n    testing = True\r\n    a, b, r0, r1, r2, r3 = connect(in_fc, out_fc, N=N, testing=True)\r\n""""""\r\n'"
PointTools/Scripts/closetbl.py,33,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   closetbl.py\r\n:Author:   Dan_Patterson@carleton.ca\r\n:Modified: 2018-03-20\r\n:\r\n:Purpose:  Determine the nearest points based on euclidean distance within\r\n:  a point file.  Emulates Generate Near Table in ArcMap\r\n:\r\n:References:\r\n:----------\r\n: - http://desktop.arcgis.com/en/arcmap/latest/tools/analysis-toolbox/\r\n:   generate-near-table.htm\r\n:\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\n\r\nimport sys\r\nimport numpy as np\r\nimport warnings\r\nimport arcpy\r\nfrom arcpytools_pnt import fc_info, tweet\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.1f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=120, precision=2,\r\n                    suppress=True, threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')\r\n\r\nwarnings.simplefilter(\'ignore\', FutureWarning)\r\n\r\nscript = sys.argv[0]\r\n\r\n\r\n# ---- functions ----\r\ndef e_dist(a, b, metric=\'euclidean\'):\r\n    """"""Distance calculation for 1D, 2D and 3D points using einsum\r\n    : a, b   - list, tuple, array in 1,2 or 3D form\r\n    : metric - euclidean (\'e\',\'eu\'...), sqeuclidean (\'s\',\'sq\'...),\r\n    :-----------------------------------------------------------------------\r\n    """"""\r\n    a = np.asarray(a)\r\n    b = np.atleast_2d(b)\r\n    a_dim = a.ndim\r\n    b_dim = b.ndim\r\n    if a_dim == 1:\r\n        a = a.reshape(1, 1, a.shape[0])\r\n    if a_dim >= 2:\r\n        a = a.reshape(np.prod(a.shape[:-1]), 1, a.shape[-1])\r\n    if b_dim > 2:\r\n        b = b.reshape(np.prod(b.shape[:-1]), b.shape[-1])\r\n    diff = a - b\r\n    dist_arr = np.einsum(\'ijk,ijk->ij\', diff, diff)\r\n    if metric[:1] == \'e\':\r\n        dist_arr = np.sqrt(dist_arr)\r\n    dist_arr = np.squeeze(dist_arr)\r\n    return dist_arr\r\n\r\n\r\ndef to_array(in_fc):\r\n    """"""Extract the shapes and produce a coordinate array.\r\n    """"""\r\n    shp_fld, oid_fld, shp_type, SR = fc_info(in_fc)\r\n    key_flds = [\'SHAPE@X\', \'SHAPE@Y\']\r\n    in_flds = [oid_fld] + key_flds\r\n    a = arcpy.da.FeatureClassToNumPyArray(in_fc, in_flds)\r\n    uni, idx = np.unique(a[key_flds], True)\r\n    uni_pnts = a[idx]\r\n    #a = a[[\'SHAPE@X\', \'SHAPE@Y\']]\r\n    a = uni.view(np.float64).reshape(uni.shape[0], 2)\r\n    return a, uni_pnts, idx\r\n\r\n\r\ndef line_dir(orig, dest, fromNorth=False):\r\n    """"""Direction of a line given 2 points\r\n    : orig, dest - two points representing the start and end of a line.\r\n    : fromNorth - True or False gives angle relative to x-axis)\r\n    :Notes:\r\n    :\r\n    """"""\r\n    orig = np.atleast_2d(orig)\r\n    dest = np.atleast_2d(dest)\r\n    dxy = dest - orig\r\n    ang = np.degrees(np.arctan2(dxy[:, 1], dxy[:, 0]))\r\n    if fromNorth:\r\n        ang = np.mod((450.0 - ang), 360.)\r\n    return ang\r\n\r\n\r\ndef near_tbl(a, b=None, N=1):\r\n    """"""Return the coordinates and distance to the nearest N points within\r\n    :  an 2D numpy array, \'a\', with optional ordering of the inputs.\r\n    :Requires:\r\n    :--------\r\n    : e_dist, fc_info, tweet from arcpytools\r\n    : a - shape coordinates extracted from a point array\r\n    : b - is b is None, then within file differences are used, otherwise\r\n    :     provide another set of coordinates to do between file distances\r\n    : N - the closest N distances and angles to calculate\r\n    :\r\n    :Returns:\r\n    :-------\r\n    :  A structured array containing Origin, Dest and Dist_FT\r\n    """"""\r\n    # ---- Calculate the distance array ----\r\n    offset = False\r\n    if b is None:\r\n        b = np.copy(a)\r\n        offset = True\r\n    dist = e_dist(a, b, metric=\'sqeuclidean\')  # use sqeuclidean for now\r\n    if dist.ndim == 1:\r\n        print(""do stuff"")\r\n        return dist\r\n    if offset:\r\n        np.fill_diagonal(dist, np.inf)\r\n    n, m = dist.shape\r\n    rows, cols = np.triu_indices(n, offset, m)  # shape and diag. offset\r\n    idx = dist[rows, cols].argsort()   # slicing with [:2] gives overall 2\r\n    r, c = rows[idx], cols[idx]\r\n    d = np.sqrt(dist[r, c])  # now take the sqrt to get the actual distance\r\n    az0 = line_dir(a[r], b[c], fromNorth=True)\r\n    az1 = line_dir(b[c], a[r], fromNorth=True)\r\n    z0 = list(zip(r, c, a[r, 0], a[r, 1], b[c, 0], b[c, 1], d, az0))\r\n    z1 = list(zip(c, r, b[c, 0], b[c, 1], a[r, 0], a[r, 1], d, az1))\r\n    dt = [(\'Orig\', \'<i4\'), (\'Dest\', \'<i4\'),\r\n          (\'X_orig\', \'<f8\'), (\'Y_orig\', \'<f8\'),\r\n          (\'X_dest\', \'<f8\'), (\'Y_dest\', \'<f8\'),\r\n          (\'OD_dist\', \'<f8\'), (\'Azim_N\', \'<f8\')]\r\n    ft = np.array(z0 + z1, dtype=dt)\r\n    ft_idx = np.argsort(ft, order=(\'Orig\', \'OD_dist\'))  # sort by Orig first\r\n    ft2 = ft[ft_idx]\r\n    num_pnts = len(a)\r\n    nt = np.asanyarray([ft2[ft2[\'Orig\'] == i][:N] for i in range(num_pnts)])\r\n    nt = np.asanyarray([i for i in nt if len(i) > 0])\r\n    nt = nt.reshape((np.product(nt.shape),))\r\n    return nt\r\n\r\n\r\nfrmt = """"""\\n\r\n:Running ... {}\r\n:Using ..... {}\r\n:optional .. {}\r\n:Finding ... {} closest points\r\n:Producing.. {}\\n\r\n""""""\r\n\r\ndef nn_kdtree(a, N=1, sorted_=True, to_tbl=True, as_cKD=True):\r\n    """"""Produce the N closest neighbours array with their distances using\r\n    scipy.spatial.KDTree as an alternative to einsum.\r\n\r\n    Parameters:\r\n    -----------\r\n    a : array\r\n        Assumed to be an array of point objects for which `nearest` is needed.\r\n    N : integer\r\n        Number of neighbors to return.  Note: the point counts as 1, so N=3\r\n        returns the closest 2 points, plus itself.\r\n        For table output, max N is limited to 5 so that the tabular output\r\n        isn\'t ridiculous.\r\n    sorted_ : boolean\r\n        A nice option to facilitate things.  See `xy_sort`.  Its mini-version\r\n        is included in this function.\r\n    to_tbl : boolean\r\n        Produce a structured array output of coordinate pairs and distances.\r\n    as_cKD : boolean\r\n        Whether to use the `c` compiled or pure python version\r\n\r\n    References:\r\n    -----------\r\n    `<https://stackoverflow.com/questions/52366421/how-to-do-n-d-distance-\r\n    and-nearest-neighbor-calculations-on-numpy-arrays/52366706#52366706>`_.\r\n\r\n    `<https://stackoverflow.com/questions/6931209/difference-between-scipy-\r\n    spatial-kdtree-and-scipy-spatial-ckdtree/6931317#6931317>`_.\r\n    """"""\r\n    def _xy_sort_(a):\r\n        """"""mini xy_sort""""""\r\n        a_view = a.view(a.dtype.descr * a.shape[1])\r\n        idx = np.argsort(a_view, axis=0, order=(a_view.dtype.names)).ravel()\r\n        a = np.ascontiguousarray(a[idx])\r\n        return a, idx\r\n    #\r\n    def xy_dist_headers(N):\r\n        """"""Construct headers for the optional table output""""""\r\n        vals = np.repeat(np.arange(N), 2)\r\n        names = [\'X_{}\', \'Y_{}\']*N + [\'d_{}\']*(N-1)\r\n        vals = (np.repeat(np.arange(N), 2)).tolist() + [i for i in range(1, N)]\r\n        n = [names[i].format(vals[i]) for i in range(len(vals))]\r\n        f = [\'<f8\']*N*2 + [\'<f8\']*(N-1)\r\n        return list(zip(n, f))\r\n    #\r\n    from scipy.spatial import cKDTree, KDTree\r\n    #\r\n    if sorted_:\r\n        a, idx_srt = _xy_sort_(a)\r\n    # ---- query the tree for the N nearest neighbors and their distance\r\n    if as_cKD:\r\n        t = cKDTree(a)\r\n    else:\r\n        t = KDTree(a)\r\n    dists, indices = t.query(a, N+1)  # so that point isn\'t duplicated\r\n    dists = dists[:,1:]               # and the array is 2D\r\n    frumXY = a[indices[:,0]]\r\n    indices = indices[:,1:]\r\n    if to_tbl and (N <= 5):\r\n        dt = xy_dist_headers(N+1)  # --- Format a structured array header\r\n        xys =  a[indices]\r\n        new_shp = (xys.shape[0], np.prod(xys.shape[1:]))\r\n        xys = xys.reshape(new_shp)\r\n        #ds = dists[:, 1]  # [d[1:] for d in dists]\r\n        arr = np.concatenate((frumXY, xys, dists), axis=1)\r\n        z = np.zeros((xys.shape[0],), dtype=dt)\r\n        names = z.dtype.names\r\n        for i, j in enumerate(names):\r\n            z[j] = arr[:, i]\r\n        return z\r\n    dists = dists.view(np.float64).reshape(dists.shape[0], -1)\r\n    return dists\r\n\r\n\r\ndef _tool():\r\n    """""" run the tool""""""\r\n    in_fc = sys.argv[1]\r\n    N = int(sys.argv[2])\r\n    out_tbl = sys.argv[3]\r\n    args = [script, in_fc, N, out_tbl]\r\n    tweet(frmt.format(*args))           # call tweet\r\n    a = to_array(in_fc)                 # call to_array\r\n#    nt = near_tbl(a, b=None, N=N)       # call near_tbl\r\n    nt = nn_kdtree(a, N=3, sorted_=True, to_tbl=True, as_cKD=True)\r\n    tweet(""\\nnear table\\n{}"".format(nt)) #.reshape(nt.shape[0], 1)))\r\n    arcpy.da.NumPyArrayToTable(nt, out_tbl)\r\n\r\nif len(sys.argv) == 1:\r\n    in_fc = r\'C:\\GIS\\A_Tools_scripts\\PointTools\\Point_tools.gdb\\pnts_01\'\r\n    in_fc = r\'C:\\GIS\\A_Tools_scripts\\Ice\\icebergs.gdb\\x_0\'\r\n    out_tbl = r\'C:\\GIS\\A_Tools_scripts\\Ice\\icebergs.gdb\\x_0_kd\'\r\n    a0 = arcpy.da.FeatureClassToNumPyArray(in_fc,\r\n                                          [\'OID@\', \'SHAPE@X\', \'SHAPE@Y\'])\r\n    a, uni_pnts, idx = to_array(in_fc)\r\n    ret = nn_kdtree(a, N=3, sorted_=True, to_tbl=True, as_cKD=True)\r\nelse:\r\n    _tool()\r\n\r\n# ---------------------------------------------------------------------\r\nif __name__ == ""__main__"":\r\n    """"""Main section...   """"""\r\n\r\n#    print(""Script... {}"".format(script))\r\n\r\n""""""\r\nfn = r\'C:\\GIS\\A_Tools_scripts\\PointTools\\Point_tools.gdb\\pnts_25\'\r\na = arcpy.da.FeatureClassToNumPyArray(fn, \'Shape\')\r\na = a[\'Shape\']\r\n""""""\r\n'"
PointTools/Scripts/dist_stats.py,20,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\nScript :   dist_stats.py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified : 2018-07-14\r\n\r\nPurpose:\r\n-------\r\n  Calculate standard distance and distance matrix for points grouped by an\r\n  attribute/key field.\r\n\r\n""""""\r\n# ---- imports, formats, constants ----\r\n#\r\n\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\nfrom arcpytools_pnt import fc_info, tweet, make_row_format, _col_format, form_\r\nfrom textwrap import dedent\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.1f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=100, precision=2,\r\n                    suppress=True, threshold=120,\r\n                    formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')\r\n\r\nscript = sys.argv[0]\r\n\r\n\r\ndef dist_arr(a):\r\n    """"""Minimum spanning tree prep... see main header\r\n    : paths from given data set...\r\n    """"""\r\n    # idx = np.lexsort(a.T)  # sort y, then x\r\n    idx = np.lexsort((a[:, 1], a[:, 0]))  # sort X, then Y\r\n    # idx= np.lexsort((a[:,0], a[:,1]))  # sort Y, then X\r\n    a_srt = a[idx, :]\r\n    d = _e_dist(a_srt)\r\n    frmt = """"""\\n    {}\\n    :Input array...\\n    {}\\n\\n    :Sorted array...\r\n    {}\\n\\n    :Distance...\\n    {}\r\n    """"""\r\n    args = [dist_arr.__doc__, a, a_srt, d]  # d.astype(\'int\')]\r\n    print(dedent(frmt).format(*args))\r\n    return idx, a_srt, d\r\n\r\n\r\ndef _e_dist(a):\r\n    """"""Return a 2D square-form euclidean distance matrix.\r\n    For other dimensions, use e_dist in arraytools, geom.py\r\n    """"""\r\n    b = a.reshape(np.prod(a.shape[:-1]), 1, a.shape[-1])\r\n    diff = a - b\r\n    d = np.sqrt(np.einsum(\'ijk,ijk->ij\', diff, diff)).squeeze()\r\n    return d\r\n\r\n\r\ndef process(in_fc, id_fld, prn=True):\r\n    """"""process the data for the tool or demo\r\n    """"""\r\n    frmt = """"""\r\n    Group .... {}\r\n    points in group {}\r\n    center ... x = {:<8.2f} y = {:<10.2f}\r\n    minimum .. x = {:<8.2f} y = {:<10.2f}\r\n    maximum .. x = {:<8.2f} y = {:<10.2f}\r\n    standard distance ... {:8.2f}\r\n    distance stats ....\r\n      mean:{:8.2f}  min:{:8.2f}  max:{:8.2f}  std:{:8.2f}\r\n    """"""\r\n    flds = [\'SHAPE@X\', \'SHAPE@Y\', id_fld]\r\n    a_in = arcpy.da.FeatureClassToNumPyArray(in_fc, flds)\r\n    a_sort = np.sort(a_in, order=id_fld)\r\n    a_split = np.split(a_sort, np.where(np.diff(a_sort[id_fld]))[0] + 1)\r\n    msg = """"\r\n    tbl = []\r\n    for i in range(len(a_split)):\r\n        a0 = a_split[i][[\'SHAPE@X\', \'SHAPE@Y\']]\r\n        a = a0.copy()\r\n        a = a.view((a.dtype[0], len(a.dtype.names)))  # art.geom._view_(a0)\r\n        cent = np.mean(a, axis=0)\r\n        min_ = np.min(a, axis=0)\r\n        max_ = np.max(a, axis=0)\r\n        var_x = np.var(a[:, 0])\r\n        var_y = np.var(a[:, 1])\r\n        stand_dist = np.sqrt(var_x + var_y)\r\n        dm = _e_dist(a)\r\n        dm_result = np.tril(dm, -1)\r\n        vals = dm_result[np.nonzero(dm_result)]\r\n        stats = [vals.mean(), vals.min(), vals.max(), vals.std()]\r\n        # hdr = ""Distance matrix...({}) "".format(i)\r\n        # m = form_(dm_result, deci=1, wdth=80, title=hdr, prn=False)\r\n        args = (i, len(a), *cent, *min_, *max_, stand_dist, *stats) #, m]\r\n        tbl.append(args)\r\n        msg += dedent(frmt).format(*args)\r\n    if prn:\r\n        tweet(msg)\r\n    flds = [\'ID\', \'N_pnts\', \'CentX\', \'CentY\', \'MinX\', \'MinY\', \'MaxX\', \'MaxY\',\r\n             \'Stand Dist\', \'Mean_dist\', \'Min_dist\', \'Max_dist\', \'Std_dist\']\r\n    dts = [\'<i4\', \'<i4\', \'<f8\', \'<f8\', \'<f8\', \'<f8\', \'<f8\', \'<f8\', \'<f8\',\r\n           \'<f8\', \'<f8\', \'<f8\', \'<f8\']\r\n    tbl = np.array(tbl)\r\n    tbl = np.core.records.fromarrays(tbl.transpose(),\r\n                                     names=flds,\r\n                                     formats=dts)\r\n    return a_in, a_split, msg, tbl\r\n\r\n\r\n# ---- main section ----\r\n#\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    fc = ""/Point_tools.gdb/pnts_in_mesh_Intersect""\r\n    flder = ""/"".join(script.split(""/"")[:-2])\r\n    in_fc = flder + fc\r\n    id_fld = \'ID_poly\'\r\n\r\nelse:\r\n    testing = False\r\n    in_fc = sys.argv[1]  # point layer\r\n    id_fld = sys.argv[2]\r\n    shp_fld, oid_fld, shp_type, SR = fc_info(in_fc)\r\n\r\na, a_split, msg, tbl = process(in_fc, id_fld)\r\n# ---------------------------------------------------------------------\r\nif __name__ == ""__main__"":\r\n    """"""Main section...   """"""\r\n#    print(""Script... {}"".format(script))\r\n#    _demo()\r\n'"
PointTools/Scripts/hulls.py,21,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\nhulls.py\r\n========\r\n\r\nScript:   hulls.py\r\n\r\nAuthor:   Dan.Patterson@carleton.ca\r\n\r\nModified: 2019-06-08\r\n\r\nPurpose:  working with numpy arrays to determine convex and concave hulls\r\n\r\nReferences:\r\n-----------\r\n`<https://community.esri.com/blogs/dan_patterson/2018/03/11/\r\nconcave-hulls-the-elusive-container>\'_.\r\n`<https://github.com/jsmolka/hull/blob/master/hull.py>\'_.\r\n`<https://stackoverflow.com/questions/563198/how-do-you-detect-where-two-\r\nline-segments-intersect#565282>\'_.\r\n`<http://www.codeproject.com/Tips/862988/Find-the-intersection-\r\npoint-of-two-line-segments>\'_.\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nfrom numpy.lib.recfunctions import structured_to_unstructured as stu\r\nfrom arcpytools_pnt import tweet, output_polylines, output_polygons\r\nimport arcpy\r\nimport warnings\r\nimport math\r\n\r\n\r\nwarnings.simplefilter(\'ignore\', FutureWarning)\r\n\r\narcpy.overwriteOutput = True\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\nPI = math.pi\r\n\r\n\r\ndef pnt_in_list(pnt, pnts_list):\r\n    """"""Check to see if a point is in a list of points\r\n    """"""\r\n    is_in = np.any([np.isclose(pnt, i) for i in pnts_list])\r\n    return is_in\r\n\r\n\r\ndef intersects(*args):\r\n    """"""Line intersection check.  Two lines or 4 points that form the lines.\r\n    :Requires:\r\n    :--------\r\n    :  intersects(line0, line1) or intersects(p0, p1, p2, p3)\r\n    :   p0, p1 -> line 1\r\n    :   p2, p3 -> line 2\r\n    :Returns: boolean, if the segments do intersect\r\n    :--------\r\n    :References:\r\n    :--------\r\n    : https://stackoverflow.com/questions/563198/how-do-you-detect-where-two-\r\n    :        line-segments-intersect#565282\r\n    """"""\r\n    if len(args) == 2:\r\n        p0, p1, p2, p3 = *args[0], *args[1]\r\n    elif len(args) == 4:\r\n        p0, p1, p2, p3 = args\r\n    else:\r\n        raise AttributeError(""Pass 2, 2-pnt lines or 4 points to the function"")\r\n    #\r\n    # ---- First check ----   np.cross(p1-p0, p3-p2 )\r\n    p0_x, p0_y, p1_x, p1_y, p2_x, p2_y, p3_x, p3_y = *p0, *p1, *p2, *p3\r\n    s10_x = p1_x - p0_x\r\n    s10_y = p1_y - p0_y\r\n    s32_x = p3_x - p2_x\r\n    s32_y = p3_y - p2_y\r\n    denom = s10_x * s32_y - s32_x * s10_y\r\n    if denom == 0.0:\r\n        return False\r\n    #\r\n    # ---- Second check ----  np.cross(p1-p0, p0-p2 )\r\n    den_gt0 = denom > 0\r\n    s02_x = p0_x - p2_x\r\n    s02_y = p0_y - p2_y\r\n    s_numer = s10_x * s02_y - s10_y * s02_x\r\n    if (s_numer < 0) == den_gt0:\r\n        return False\r\n    #\r\n    # ---- Third check ----  np.cross(p3-p2, p0-p2)\r\n    t_numer = s32_x * s02_y - s32_y * s02_x\r\n    if (t_numer < 0) == den_gt0:\r\n        return False\r\n    #\r\n    if ((s_numer > denom) == den_gt0) or ((t_numer > denom) == den_gt0):\r\n        return False\r\n    #\r\n    # ---- check to see if the intersection point is one of the input points\r\n    t = t_numer / denom\r\n    # substitute p0 in the equation\r\n    x = p0_x + (t * s10_x)\r\n    y = p0_y + (t * s10_y)\r\n    # be careful that you are comparing tuples to tuples, lists to lists\r\n    if sum([(x, y) == tuple(i) for i in [p0, p1, p2, p3]]) > 0:\r\n        return False\r\n    return True\r\n\r\n\r\ndef angle(p0, p1, prv_ang=0):\r\n    """"""Angle between two points and the previous angle, or zero.\r\n    """"""\r\n    ang = math.atan2(p0[1] - p1[1], p0[0] - p1[0])\r\n    a0 = (ang - prv_ang)\r\n    a0 = a0 % (PI * 2) - PI\r\n    return a0\r\n\r\n\r\ndef point_in_polygon(pnt, poly):  # pnt_in_poly(pnt, poly):  #\r\n    """"""Point is in polygon. ## fix this and use pip from arraytools\r\n    """"""\r\n    x, y = pnt\r\n    N = len(poly)\r\n    for i in range(N):\r\n        x0, y0, xy = [poly[i][0], poly[i][1], poly[(i + 1) % N]]\r\n        c_min = min([x0, xy[0]])\r\n        c_max = max([x0, xy[0]])\r\n        if c_min < x <= c_max:\r\n            p = y0 - xy[1]\r\n            q = x0 - xy[0]\r\n            y_cal = (x - x0) * p / q + y0\r\n            if y_cal < y:\r\n                return True\r\n    return False\r\n\r\n\r\ndef knn(pnts, p, k):\r\n    """"""\r\n    Calculates k nearest neighbours for a given point.\r\n\r\n    :param points: list of points\r\n    :param p: reference point\r\n    :param k: amount of neighbours\r\n    :return: list\r\n    """"""\r\n    s = sorted(pnts,\r\n               key=lambda x: math.sqrt((x[0]-p[0])**2 + (x[1]-p[1])**2))[0:k]\r\n    return s\r\n\r\n\r\ndef knn0(pnts, p, k):\r\n    """"""\r\n    Calculates k nearest neighbours for a given point.\r\n\r\n    points : array\r\n        list of points\r\n    p : two number array-like\r\n        reference point\r\n    k : integer\r\n        amount of neighbours\r\n    Returns:\r\n    --------\r\n    list of the k nearest neighbours, based on squared distance\r\n    """"""\r\n    p = np.asarray(p)\r\n    pnts = np.asarray(pnts)\r\n    diff = pnts - p[np.newaxis, :]\r\n    d = np.einsum(\'ij,ij->i\', diff, diff)\r\n    idx = np.argsort(d)[:k]\r\n#    s = [i.tolist() for i in pnts[idx]]\r\n    return pnts[idx].tolist()\r\n\r\n\r\ndef concave(points, k):\r\n    """"""Calculates the concave hull for given points\r\n    :Requires:\r\n    :--------\r\n    : points - initially the input set of points with duplicates removes and\r\n    :    sorted on the Y value first, lowest Y at the top (?)\r\n    : k - initially the number of points to start forming the concave hull,\r\n    :    k will be the initial set of neighbors\r\n    :Notes:  This recursively calls itself to check concave hull\r\n    : p_set - The working copy of the input points\r\n    :-----\r\n    """"""\r\n    k = max(k, 3)  # Make sure k >= 3\r\n    p_set = list(set(points[:]))  # Remove duplicates if not done already\r\n    if len(p_set) < 3:\r\n        raise Exception(""p_set length cannot be smaller than 3"")\r\n    elif len(p_set) == 3:\r\n        return p_set  # Points are a polygon already\r\n    k = min(k, len(p_set) - 1)  # Make sure k neighbours can be found\r\n\r\n    frst_p = cur_p = min(p_set, key=lambda x: x[1])\r\n    hull = [frst_p]  # Initialize hull with first point\r\n    p_set.remove(frst_p)  # Remove first point from p_set\r\n    prev_ang = 0\r\n\r\n    while (cur_p != frst_p or len(hull) == 1) and len(p_set) > 0:\r\n        if len(hull) == 3:\r\n            p_set.append(frst_p)  # Add first point again\r\n        knn_pnts = knn(p_set, cur_p, k)  # knn or knn0\r\n        cur_pnts = sorted(knn_pnts, key=lambda x: -angle(x, cur_p, prev_ang))\r\n\r\n        its = True\r\n        i = -1\r\n        while its and i < len(cur_pnts) - 1:\r\n            i += 1\r\n            last_point = 1 if cur_pnts[i] == frst_p else 0\r\n            j = 1\r\n            its = False\r\n            while not its and j < len(hull) - last_point:\r\n                its = intersects(hull[-1], cur_pnts[i], hull[-j - 1], hull[-j])\r\n                j += 1\r\n        if its:  # All points intersect, try a higher number of neighbours\r\n            return concave(points, k + 1)\r\n        prev_ang = angle(cur_pnts[i], cur_p)\r\n        cur_p = cur_pnts[i]\r\n        hull.append(cur_p)  # Valid candidate was found\r\n        p_set.remove(cur_p)\r\n\r\n    for point in p_set:\r\n        if not point_in_polygon(point, hull):\r\n            return concave(points, k + 1)\r\n    #\r\n    return hull\r\n\r\n\r\n# ---- convex hull ----------------------------------------------------------\r\n#\r\ndef cross(o, a, b):\r\n    """"""Cross-product for vectors o-a and o-b\r\n    """"""\r\n    xo, yo = o\r\n    xa, ya = a\r\n    xb, yb = b\r\n    return (xa - xo)*(yb - yo) - (ya - yo)*(xb - xo)\r\n#    return (a[0] - o[0]) * (b[1] - o[1]) - (a[1] - o[1]) * (b[0] - o[0])\r\n\r\n\r\ndef convex(points):\r\n    """"""Calculates the convex hull for given points\r\n    :Input is a list of 2D points [(x, y), ...]\r\n    """"""\r\n    points = sorted(set(points))  # Remove duplicates\r\n    if len(points) <= 1:\r\n        return points\r\n    # Build lower hull\r\n    lower = []\r\n    for p in points:\r\n        while len(lower) >= 2 and cross(lower[-2], lower[-1], p) <= 0:\r\n            lower.pop()\r\n        lower.append(p)\r\n    # Build upper hull\r\n    upper = []\r\n    for p in reversed(points):\r\n        while len(upper) >= 2 and cross(upper[-2], upper[-1], p) <= 0:\r\n            upper.pop()\r\n        upper.append(p)\r\n    print(""lower\\n{}\\nupper\\n{}"".format(lower, upper))\r\n    return np.array(lower[:-1] + upper)  # upper[:-1]) # for open loop\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# .... running script or testing code section\r\ndef _tool():\r\n    """"""run when script is from a tool\r\n    """"""\r\n    in_fc = sys.argv[1]\r\n    group_by = str(sys.argv[2])\r\n    k_factor = int(sys.argv[3])\r\n    hull_type = str(sys.argv[4])\r\n    out_type = str(sys.argv[5])\r\n    out_fc = sys.argv[6]\r\n    return in_fc, group_by, k_factor, hull_type, out_type, out_fc\r\n\r\n\r\ngdb_pth = ""/"".join(script.split(""/"")[:-2]) + ""/Data/Point_tools.gdb""\r\n\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    in_fc = gdb_pth + r""/r_sorted""\r\n    group_by = \'Group_\'\r\n    k_factor = 3\r\n    hull_type = \'concave\'  # \'convex\'\r\n    out_type = \'Polyline\'\r\n    out_fc = gdb_pth + r""/r_11""\r\nelse:\r\n    testing = False\r\n    in_fc, group_by, k_factor, hull_type, out_type, out_fc = _tool()\r\n\r\nmsg = """"""\\n\r\n-----------------------------------------------------------------------\r\n---- Concave/convex hull ----\r\nscript    {}\r\nTesting   {}\r\nin_fc     {}\r\ngroup_by  {}\r\nk_factor  {}\r\nhull_type {}\r\nout_type  {}\r\nout_fc    {}\r\n-----------------------------------------------------------------------\r\n\r\n""""""\r\nargs = [script, testing, in_fc, group_by, k_factor,\r\n        hull_type, out_type, out_fc]\r\ntweet(msg.format(*args))\r\n\r\ndesc = arcpy.da.Describe(in_fc)\r\nSR = desc[\'spatialReference\']\r\n#\r\n# (1) ---- get the points\r\nout_flds = [\'OID@\', \'SHAPE@X\', \'SHAPE@Y\'] + [group_by]\r\na = arcpy.da.FeatureClassToNumPyArray(in_fc, out_flds, """", SR, True)\r\n#\r\n# (2) ---- determine the unique groupings of the points\r\nuniq, idx, rev = np.unique(a[group_by], True, True)\r\ngroups = [a[np.where(a[group_by] == i)[0]] for i in uniq]\r\n#\r\n# (3) ---- for each group, perform the concave hull\r\nhulls = []\r\nfor i in range(0, len(groups)):\r\n    p = groups[i]\r\n    p = p[[\'SHAPE@X\', \'SHAPE@Y\']]\r\n    n = len(p)\r\n    p = stu(p)\r\n    #\r\n    # ---- point preparation section ------------------------------------\r\n    p = np.array(list(set([tuple(i) for i in p])))  # Remove duplicates\r\n    idx_cr = np.lexsort((p[:, 0], p[:, 1]))         # indices of sorted array\r\n    in_pnts = np.asarray([p[i] for i in idx_cr])    # p[idx_cr]  #\r\n    in_pnts = in_pnts.tolist()\r\n    in_pnts = [tuple(i) for i in in_pnts]\r\n    if hull_type == \'concave\':\r\n        cx = np.array(concave(in_pnts, k_factor))  # requires a list of tuples\r\n    else:\r\n        cx = np.array(convex(in_pnts))\r\n    hulls.append(cx.tolist())\r\n    # ----\r\n    #\r\nif out_type == \'Polyline\':\r\n    output_polylines(out_fc, SR, [hulls])\r\nelif out_type == \'Polygon\':\r\n    output_polygons(out_fc, SR, [hulls])\r\nelse:\r\n    for i in hulls:\r\n        print(""Hulls\\n{}"".format(np.array(i)))\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n'"
PointTools/Scripts/hulls_editing.py,54,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   .py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2018-xx-xx\r\n:Purpose:  tools for working with numpy arrays\r\n:Useage:\r\n:\r\n:References:\r\n: https://github.com/jsmolka/hull/blob/master/hull.py\r\n: https://stackoverflow.com/questions/563198/how-do-you-detect-where-two-\r\n:        line-segments-intersect#565282\r\n: http://www.codeproject.com/Tips/862988/Find-the-intersection-\r\n:       point-of-two-line-segments\r\n: considerCollinearOverlapAsIntersect => co_check\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nfrom arcpytools_pnt import output_polylines, output_polygons\r\nimport arcpy\r\nimport warnings\r\nimport math\r\n\r\n\r\nwarnings.simplefilter(\'ignore\', FutureWarning)\r\n\r\narcpy.overwriteOutput = True\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\nPI = math.pi\r\n\r\n\r\n## ---- Modified code from references\r\n##\r\ndef e_dist(a, b, metric=\'euclidean\'):\r\n    """"""Distance calculation for 1D, 2D and 3D points using einsum\r\n    : a, b   - list, tuple, array in 1,2 or 3D form\r\n    : metric - euclidean (\'e\',\'eu\'...), sqeuclidean (\'s\',\'sq\'...),\r\n    :-----------------------------------------------------------------------\r\n    """"""\r\n    a = np.asarray(a)\r\n    b = np.atleast_2d(b)\r\n    a_dim = a.ndim\r\n    b_dim = b.ndim\r\n    if a_dim == 1:\r\n        a = a.reshape(1, 1, a.shape[0])\r\n    if a_dim >= 2:\r\n        a = a.reshape(np.prod(a.shape[:-1]), 1, a.shape[-1])\r\n    if b_dim > 2:\r\n        b = b.reshape(np.prod(b.shape[:-1]), b.shape[-1])\r\n    diff = a - b\r\n    dist_arr = np.einsum(\'ijk,ijk->ij\', diff, diff)\r\n    if metric[:1] == \'e\':\r\n        dist_arr = np.sqrt(dist_arr)\r\n    dist_arr = np.squeeze(dist_arr)\r\n    return dist_arr\r\n\r\n\r\ndef nearest_n(pnts, pt, n):   # renamed tonearest_n(pnts, pt, n):\r\n    """"""n-nearest neighbours for a pnts list.\r\n    : pnts - points array (xs, ys)\r\n    : pt - reference point (px, py)\r\n    : n - nearest neighbours\r\n    : srted => sorted(sqrt((xs[0] - px[0])**2 + (ys[1] - py[1])**2))[0:k]\r\n    :Notes:\r\n    :------\r\n    :  for two 2d vectors U = (Ux, Uy) V = (Vx, Vy)\r\n    :  the crossproduct is    U x V = Ux*Vy - Uy*Vx\r\n    """"""\r\n    nn_idx = np.argsort(e_dist(pt, pnts))  # [:n]\r\n    p = pnts[nn_idx]\r\n    return p[:n]\r\n\r\n\r\ndef intersect_pnt(p0, p1, p2, p3):\r\n    """"""Returns the point of intersection of the segment passing through two\r\n    :  line segments (p0, p1) and (p2, p3)\r\n    :Notes:\r\n    :------\r\n    :         p0,            p1,             p2,            p3\r\n    : (array([0, 0]), array([10, 10]),array([0, 5]), array([5, 0]))\r\n    : s: array([[ 0,  0],    h: array([[  0.,   0.,   1.],\r\n    :           [10, 10],              [ 10.,  10.,   1.],\r\n    :           [ 0,  5],              [  0.,   5.,   1.],\r\n    :           [ 5,  0]])             [  5.,   0.,   1.]])\r\n    :Reference:\r\n    :---------\r\n    : https://stackoverflow.com/questions/3252194/numpy-and-line-intersections\r\n    """"""\r\n    s = np.vstack([p0, p1, p2, p3])      # s for stacked\r\n    h = np.hstack((s, np.ones((4, 1))))  # h for homogeneous\r\n    l1 = np.cross(h[0], h[1])            # get first line\r\n    l2 = np.cross(h[2], h[3])            # get second line\r\n    x, y, z = np.cross(l1, l2)           # point of intersection\r\n    if z == 0:                           # lines are parallel\r\n        return (float(\'inf\'), float(\'inf\'))\r\n    return (x/z, y/z)\r\n\r\n\r\ndef pnt_in_list(pnt, pnts_list):\r\n    """"""Check to see if a point is in a list of points\r\n    """"""\r\n    is_in = np.any([np.isclose(pnt, i) for i in pnts_list])\r\n    return is_in\r\n\r\n\r\ndef intersects(*args):\r\n    """"""Line intersection check.  Two lines or 4 points that form the lines.\r\n    :Requires:\r\n    :--------\r\n    :  intersects(line0, line1) or intersects(p0, p1, p2, p3)\r\n    :   p0, p1 -> line 1\r\n    :   p2, p3 -> line 2\r\n    :Returns: boolean, if the segments do intersect\r\n    :--------\r\n    :References:\r\n    :--------\r\n    : https://stackoverflow.com/questions/563198/how-do-you-detect-where-two-\r\n    :        line-segments-intersect#565282\r\n    """"""\r\n    if len(args) == 2:\r\n        p0, p1, p2, p3 = *args[0], *args[1]\r\n    elif len(args) == 4:\r\n        p0, p1, p2, p3 = args\r\n    else:\r\n        raise AttributeError(""Pass 2, 2-pnt lines or 4 points to the function"")\r\n    #\r\n    # ---- First check ----   np.cross(p1-p0, p3-p2 )\r\n    p0_x, p0_y, p1_x, p1_y, p2_x, p2_y, p3_x, p3_y = *p0, *p1, *p2, *p3\r\n    s10_x = p1_x - p0_x\r\n    s10_y = p1_y - p0_y\r\n    s32_x = p3_x - p2_x\r\n    s32_y = p3_y - p2_y\r\n    denom = s10_x * s32_y - s32_x * s10_y\r\n    if denom == 0.0:\r\n        return False\r\n    #\r\n    # ---- Second check ----  np.cross(p1-p0, p0-p2 )\r\n    den_gt0 = denom > 0\r\n    s02_x = p0_x - p2_x\r\n    s02_y = p0_y - p2_y\r\n    s_numer = s10_x * s02_y - s10_y * s02_x\r\n    if (s_numer < 0) == den_gt0:\r\n        return False\r\n    #\r\n    # ---- Third check ----  np.cross(p3-p2, p0-p2)\r\n    t_numer = s32_x * s02_y - s32_y * s02_x\r\n    if (t_numer < 0) == den_gt0:\r\n        return False\r\n    #\r\n    if ((s_numer > denom) == den_gt0) or ((t_numer > denom) == den_gt0):\r\n        return False\r\n    #\r\n    # ---- check to see if the intersection point is one of the input points\r\n    t = t_numer / denom\r\n    # substitute p0 in the equation\r\n    x = p0_x + (t * s10_x)\r\n    y = p0_y + (t * s10_y)\r\n    # be careful that you are comparing tuples to tuples, lists to lists\r\n    if sum([(x, y) == tuple(i) for i in [p0, p1, p2, p3]]) > 0:\r\n        return False\r\n    return True\r\n\r\n\r\ndef angle(p0, p1, prv_ang=0):\r\n    """"""Angle between two points and the previous angle, or zero.\r\n    """"""\r\n    ang = math.atan2(p0[1] - p1[1], p0[0] - p1[0])\r\n#    print(""ang {}  prev ang {}"".format(np.degrees(ang), np.degrees(prv_ang)))\r\n    a0 = (ang - prv_ang)\r\n    a0 = a0 % (PI * 2) - PI\r\n    return a0\r\n\r\n#def SortByAngle(kNearestPoints, currentPoint, prevPoint):\r\n#    \'\'\' Sorts the k nearest points given by angle \'\'\'\r\n#    angles = np.zeros(kNearestPoints.shape[0])\r\n#    i = 0\r\n#    for NearestPoint in kNearestPoints:\r\n#        # calculate the angle\r\n#        angle = np.arctan2(NearestPoint[1]-currentPoint[1],\r\n#                NearestPoint[0]-currentPoint[0]) - \\\r\n#                np.arctan2(prevPoint[1]-currentPoint[1],\r\n#                prevPoint[0]-currentPoint[0])\r\n#        angle = np.rad2deg(angle)\r\n#        # only positive angles\r\n#        angle = np.mod(angle+360,360)\r\n#        #print NearestPoint[0], NearestPoint[1], angle\r\n#        angles[i] = angle\r\n#        i=i+1\r\n#    return kNearestPoints[np.argsort(angles)]\r\n\r\ndef point_in_polygon(pnt, poly):  #pnt_in_poly(pnt, poly):  #\r\n    """"""Point is in polygon. ## fix this and use pip from arraytools\r\n    """"""\r\n    x, y = pnt\r\n    N = len(poly)\r\n    for i in range(N):\r\n        x0, y0, xy = [poly[i][0], poly[i][1], poly[(i + 1) % N]]\r\n        c_min = min([x0, xy[0]])\r\n        c_max = max([x0, xy[0]])\r\n        if c_min < x <= c_max:\r\n            p = y0 - xy[1]\r\n            q = x0 - xy[0]\r\n            y_cal = (x - x0) * p / q + y0\r\n            if y_cal < y:\r\n                return True\r\n    return False\r\n\r\n\r\ndef concave2(in_pnts, k):\r\n    """"""Calculates the concave hull for given points\r\n    :Requires:\r\n    :--------\r\n    : in_pnts - initially the input set of points with duplicates removes and\r\n    :    sorted on the Y value first, lowest Y at the top (?)\r\n    : k - initially the number of points to start forming the concave hull,\r\n    :    k will be the initial set of neighbors\r\n    :Notes:  This recursively calls itself to check concave hull\r\n    :-----\r\n    """"""\r\n    k = max(k, 3)  # Make sure k >= 3\r\n    pnts_ = in_pnts  # list(set(in_pnts[:]))  # Remove duplicates\r\n\r\n    if len(pnts_) < 3:\r\n        raise Exception(""Dataset length cannot be smaller than 3"")\r\n    elif len(pnts_) == 3:\r\n        return pnts_  # Points are a polygon already\r\n    k = min(k, len(pnts_) - 1)  # Make sure k neighbours can be found\r\n\r\n    # ---- get the point with the minimum y, throw it into the hull\r\n    # remove it from the input list\r\n    # first_pnt = cur_pnt = min(dataset, key=lambda x: x[1])\r\n    cur_pnt = pnts_[np.argmin(pnts_, axis=0)[1]]  #\r\n    first_pnt = cur_pnt\r\n    hull = [first_pnt]  # Initialize hull with first point\r\n    pnts_ = np.delete(pnts_, 0, axis=0)  # Remove first point from dataset\r\n    prv_ang = 0\r\n    # ----  need np.all since curr_pnt and first_pnt are arrays\r\n    while (np.all(cur_pnt != first_pnt) or len(hull) == 1) and len(pnts_) > 0:\r\n        if len(hull) == 3:  # Add first point again\r\n            pnts_ = np.append(pnts_, np.atleast_2d(first_pnt), axis=0)\r\n        nn_pnts = nearest_n(pnts_, cur_pnt, k)  # Find nearest neighbours\r\n#        print(""nn_pnts\\n{}"".format(nn_pnts))\r\n        #\r\n        c_points = sorted(nn_pnts, key=lambda x: -angle(x, cur_pnt, prv_ang))\r\n        #\r\n        is_True = True\r\n        i = -1\r\n        while is_True and i < len(c_points) - 1:\r\n            i += 1\r\n            last_point = 1 if np.all(c_points[i] == first_pnt) else 0\r\n            j = 1\r\n            is_True = False\r\n            while not is_True and j < len(hull) - last_point:\r\n                args = [hull[-1], c_points[i], hull[-j - 1], hull[-j]]\r\n                is_True = intersects(*args)\r\n                j += 1\r\n        if is_True:  # All intersect, try with higher number of neighbours\r\n            return concave(in_pnts, k + 1)\r\n        prv_ang = angle(c_points[i], cur_pnt)\r\n        cur_pnt = c_points[i]\r\n        hull.append(cur_pnt)  # Valid candidate was found\r\n#        pnts_.remove(cur_pnt)  # check\r\n        whr = np.where(pnts_ == cur_pnt)[0]\r\n        pnts_ = np.delete(pnts_, whr, axis=0)\r\n    # ---- final check again\r\n    for point in pnts_:  # final point in polygon check\r\n        if not pnt_in_poly(point, hull):\r\n            return concave(in_pnts, k + 1)\r\n    #\r\n    return hull\r\n#\r\n## --------------------------------------------------------------------------\r\ndef cross(o, a, b):\r\n    """"""\r\n    Calculates cross product.\r\n\r\n    :param o, a: vector\r\n    :param o, b: vector\r\n    :return: int\r\n    """"""\r\n    return (a[0] - o[0]) * (b[1] - o[1]) - (a[1] - o[1]) * (b[0] - o[0])\r\n\r\n\r\ndef convex(points):\r\n    """"""\r\n    Calculates the convex hull for given points\r\n    Input is a list of 2D points [(x, y), ...]\r\n\r\n    :param points: list of points\r\n    :return: list\r\n    """"""\r\n##    points = sorted(set(points))  # Remove duplicates\r\n    if len(points) <= 1:\r\n        return points\r\n\r\n    # Build lower hull\r\n    lower = []\r\n    for p in points:\r\n        while len(lower) >= 2 and cross(lower[-2], lower[-1], p) <= 0:\r\n            lower.pop()\r\n        lower.append(p)\r\n\r\n    # Build upper hull\r\n    upper = []\r\n    for p in reversed(points):\r\n        while len(upper) >= 2 and cross(upper[-2], upper[-1], p) <= 0:\r\n            upper.pop()\r\n        upper.append(p)\r\n\r\n    return np.array(lower[:-1] + upper[:-1])\r\n\r\n\r\n\r\n#import math\r\n#\r\n#\r\ndef knn(points, p, k):\r\n    """"""\r\n    Calculates k nearest neighbours for a given point.\r\n\r\n    :param points: list of points\r\n    :param p: reference point\r\n    :param k: amount of neighbours\r\n    :return: list\r\n    """"""\r\n    return sorted(points, key=lambda x: math.sqrt((x[0] - p[0]) ** 2 + (x[1] - p[1]) ** 2))[0:k]\r\n#\r\n#\r\n#def intersects(p1, p2, p3, p4):\r\n#    """"""\r\n#    Checks if lines p1, p2 and p3, p4 intersect.\r\n#\r\n#    :param p1, p2: line\r\n#    :param p3, p4: line\r\n#    :return: bool\r\n#    """"""\r\n#    p0_x, p0_y = p1\r\n#    p1_x, p1_y = p2\r\n#    p2_x, p2_y = p3\r\n#    p3_x, p3_y = p4\r\n#\r\n#    s10_x = p1_x - p0_x\r\n#    s10_y = p1_y - p0_y\r\n#    s32_x = p3_x - p2_x\r\n#    s32_y = p3_y - p2_y\r\n#\r\n#    denom = s10_x * s32_y - s32_x * s10_y\r\n#    if denom == 0:\r\n#        return False\r\n#\r\n#    denom_positive = denom > 0\r\n#    s02_x = p0_x - p2_x\r\n#    s02_y = p0_y - p2_y\r\n#    s_numer = s10_x * s02_y - s10_y * s02_x\r\n#    if (s_numer < 0) == denom_positive:\r\n#        return False\r\n#\r\n#    t_numer = s32_x * s02_y - s32_y * s02_x\r\n#    if (t_numer < 0) == denom_positive:\r\n#        return False\r\n#\r\n#    if ((s_numer > denom) == denom_positive) or ((t_numer > denom) == denom_positive):\r\n#        return False\r\n#\r\n#    t = t_numer / denom\r\n#    x = p0_x + (t * s10_x)\r\n#    y = p0_y + (t * s10_y)\r\n#\r\n#    if (x, y) in [p1, p2, p3, p4]:\r\n#        return False\r\n#\r\n#    return True\r\n#\r\n#\r\n#def angle(p1, p2, previous_angle=0):\r\n#    """"""\r\n#    Calculates angle between two points and previous angle.\r\n#\r\n#    :param p1: point\r\n#    :param p2: point\r\n#    :param previous_angle: previous angle\r\n#    :return: float\r\n#    """"""\r\n#    return (math.atan2(p1[1] - p2[1], p1[0] - p2[0]) - previous_angle) % (math.pi * 2) - math.pi\r\n#\r\n#\r\ndef point_in_polygon2(point, polygon):\r\n    """"""\r\n    Checks if point is in polygon.\r\n\r\n    :param point: point\r\n    :param polygon: polygon\r\n    :return: bool\r\n    """"""\r\n    size = len(polygon)\r\n    for i in range(size):\r\n        min_ = min([polygon[i][0], polygon[(i + 1) % size][0]])\r\n        max_ = max([polygon[i][0], polygon[(i + 1) % size][0]])\r\n        if min_ < point[0] <= max_:\r\n            p = polygon[i][1] - polygon[(i + 1) % size][1]\r\n            q = polygon[i][0] - polygon[(i + 1) % size][0]\r\n            point_y = (point[0] - polygon[i][0]) * p / q + polygon[i][1]\r\n            if point_y < point[1]:\r\n                return True\r\n    return False\r\n#\r\n#\r\ndef concave(points, k):\r\n    """"""\r\n    Calculates the concave hull for given points\r\n    Input is a list of 2D points [(x, y), ...]\r\n    k defines the number of of considered neighbours\r\n\r\n    :param points: list of points\r\n    :param k: considered neighbours\r\n    :return: list\r\n    """"""\r\n    k = max(k, 3)  # Make sure k >= 3\r\n    dataset = list(set(points[:]))  # Remove duplicates\r\n    if len(dataset) < 3:\r\n        raise Exception(""Dataset length cannot be smaller than 3"")\r\n    elif len(dataset) == 3:\r\n        return dataset  # Points are a polygon already\r\n    k = min(k, len(dataset) - 1)  # Make sure k neighbours can be found\r\n\r\n    first_point = current_point = min(dataset, key=lambda x: x[1])\r\n    hull = [first_point]  # Initialize hull with first point\r\n    dataset.remove(first_point)  # Remove first point from dataset\r\n    previous_angle = 0\r\n\r\n    while (current_point != first_point or len(hull) == 1) and len(dataset) > 0:\r\n        if len(hull) == 3:\r\n            dataset.append(first_point)  # Add first point again\r\n        kn_points = knn(dataset, current_point, k)  # Find nearest neighbours\r\n        c_points = sorted(kn_points, key=lambda x: -angle(x, current_point, previous_angle))\r\n\r\n        its = True\r\n        i = -1\r\n        while its and i < len(c_points) - 1:\r\n            i += 1\r\n            last_point = 1 if c_points[i] == first_point else 0\r\n            j = 1\r\n            its = False\r\n            while not its and j < len(hull) - last_point:\r\n                its = intersects(hull[-1], c_points[i], hull[-j - 1], hull[-j])\r\n                j += 1\r\n        if its:  # All points intersect, try again with higher number of neighbours\r\n            return concave(points, k + 1)\r\n        previous_angle = angle(c_points[i], current_point)\r\n        current_point = c_points[i]\r\n        hull.append(current_point)  # Valid candidate was found\r\n        dataset.remove(current_point)\r\n\r\n    for point in dataset:\r\n        if not point_in_polygon(point, hull):\r\n            return concave(points, k + 1)\r\n\r\n    return hull\r\n#\r\n\r\n#def cross(o, a, b):\r\n#    """"""\r\n#    Calculates cross product.\r\n#\r\n#    :param o, a: vector\r\n#    :param o, b: vector\r\n#    :return: int\r\n#    """"""\r\n#    return (a[0] - o[0]) * (b[1] - o[1]) - (a[1] - o[1]) * (b[0] - o[0])\r\n#\r\n#\r\n#def convex(points):\r\n#    """"""\r\n#    Calculates the convex hull for given points\r\n#    Input is a list of 2D points [(x, y), ...]\r\n#\r\n#    :param points: list of points\r\n#    :return: list\r\n#    """"""\r\n#    points = sorted(set(points))  # Remove duplicates\r\n#    if len(points) <= 1:\r\n#        return points\r\n#\r\n#    # Build lower hull\r\n#    lower = []\r\n#    for p in points:\r\n#        while len(lower) >= 2 and cross(lower[-2], lower[-1], p) <= 0:\r\n#            lower.pop()\r\n#        lower.append(p)\r\n#\r\n#    # Build upper hull\r\n#    upper = []\r\n#    for p in reversed(points):\r\n#        while len(upper) >= 2 and cross(upper[-2], upper[-1], p) <= 0:\r\n#            upper.pop()\r\n#        upper.append(p)\r\n#\r\n#    return lower[:-1] + upper[:-1]\r\n## ----------------------------------------------------------------------------\r\n##\r\ndef _tool():\r\n    """"""run when script is from a tool\r\n    """"""\r\n    in_fc = sys.argv[1]\r\n    group_by = str(sys.argv[2])\r\n    out_type = str(sys.argv[3])\r\n    out_fc = sys.argv[4]\r\n    return in_fc, from_north, cent, out_fc0, out_fc1\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# .... running script or testing code section\r\n\r\n\r\ngdb_pth = ""/"".join(script.split(""/"")[:-2]) + ""/Data/Point_tools.gdb""\r\n\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    in_fc = gdb_pth + r""/shapes""\r\n    group_by = \'ID\'\r\n    out_type = \'Polyline\'\r\n    out_fc = gdb_pth + r""/concave2""\r\n\r\nelse:\r\n    testing = False\r\n    in_fc, from_north, cent, out_fc0, out_fc1 = _tool()\r\n\r\n\r\ndesc = arcpy.da.Describe(in_fc)\r\nSR = desc[\'spatialReference\']\r\n#\r\n# (1) ---- get the points\r\nout_flds = [\'OID@\', \'SHAPE@X\', \'SHAPE@Y\'] + [group_by]\r\na = arcpy.da.FeatureClassToNumPyArray(in_fc, out_flds, """", SR, True)\r\n#\r\n# (2) ---- determine the unique groupings of the points\r\nuniq, idx, rev = np.unique(a[\'ID\'], True, True)\r\ngroups = [a[np.where(a[group_by] == i)[0]] for i in uniq]\r\n#\r\n# (3) ---- for each group, perform the concave hull\r\nhulls = []\r\nfor i in range(0, len(groups)):\r\n    p = groups[i]\r\n    n = len(p)\r\n    p = p[[\'SHAPE@X\', \'SHAPE@Y\']]\r\n    p = p.view(np.float64).reshape(n, 2)\r\n    idx_cr = np.lexsort((p[:, 0], p[:, 1]))       # indices of sorted array\r\n    in_pnts = np.asarray([p[i] for i in idx_cr])\r\n    in_pnts = in_pnts.tolist()\r\n    in_pnts = [tuple(i) for i in in_pnts]\r\n    cx = np.array(concave(in_pnts, 3))\r\n    hulls.append(cx.tolist())\r\nprint(""concave hull points...\\n{}"".format(hulls))\r\n\r\noutput_polylines(out_fc, SR, [hulls])\r\n# ----------------------------------------------------------------------------\r\ndef test():\r\n    """"""\r\n    cc = array([[442,  40],\r\n                [471, 187],\r\n                [433, 267],\r\n                [128, 261],\r\n                [ 33, 159],\r\n                [214,  49]])\r\n     """"""\r\n    p = [(207, 184), (393, 60), (197, 158), (197, 114), (128, 261),\r\n          (442, 40), (237, 159), (338, 75), (194, 93), (33, 159),\r\n          (393, 152), (433, 267), (324, 141), (384, 183), (273, 165),\r\n          (250, 257), (423, 198), (227, 68), (120, 184), (214, 49),\r\n          (256, 75), (379, 93), (312, 49), (471, 187), (366, 122)]\r\n    p = set(p)                                    # set removes duplicates\r\n    a = np.asarray(list(p))                       # convert to an array\r\n    idx_cr = np.lexsort((a[:, 0], a[:, 1]))       # indices of sorted array\r\n    in_pnts = np.asarray([a[i] for i in idx_cr])  # sorted array\r\n    cx = np.array(concave(in_pnts, 3))            # concave hull\r\n    cc = np.array(convex(in_pnts.tolist()))       # convex hull\r\n    return p, a, in_pnts, cx, cc\r\n\r\n\r\ndef test_main():\r\n    """""" """"""\r\n    in_fc = r""C:\\Git_Dan\\a_Data\\arcpytools_demo.gdb\\r_sorted""\r\n    out_fc = r""C:\\Git_Dan\\a_Data\\arcpytools_demo.gdb\\r_convex_hull""\r\n    desc = arcpy.da.Describe(in_fc)\r\n    SR = desc[\'spatialReference\']\r\n    flds = [\'SHAPE@X\', \'SHAPE@Y\']\r\n    args = [in_fc, flds, None, None, True, (None, None)]\r\n    cur = arcpy.da.SearchCursor(*args)\r\n    a = _xy(in_fc)                    # ---- get the points\r\n    a = uniq(a, axis=0)               # ---- get the unique points\r\n#    a_s = a[a[:, 1].argsort(axis=0)]  # sort to get lowest y-value\r\n    idx_cr = np.lexsort((a[:, 0], a[:, 1]))\r\n    in_pnts = np.asarray([a[i] for i in idx_cr])  # sort by column 1, then 0\r\n#    output_polylines(out_fc, SR, [pl])  # ***** it works\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n#    from tools import uniq\r\n#    in_fc = r""C:\\Git_Dan\\a_Data\\arcpytools_demo.gdb\\r_sorted""\r\n#    out_fc = r""C:\\Git_Dan\\a_Data\\arcpytools_demo.gdb\\r_convex_hull""\r\n#    desc = arcpy.da.Describe(in_fc)\r\n#    SR = desc[\'spatialReference\']\r\n#    flds = [\'SHAPE@X\', \'SHAPE@Y\']\r\n#    args = [in_fc, flds, None, None, True, (None, None)]\r\n#    cur = arcpy.da.SearchCursor(*args)\r\n#    a = _xy(in_fc)                    # ---- get the points\r\n#    a = uniq(a, axis=0)               # ---- get the unique points\r\n##    a_s = a[a[:, 1].argsort(axis=0)]  # sort to get lowest y-value\r\n#    idx_cr = np.lexsort((a[:, 0], a[:, 1]))\r\n#    in_pnts = np.asarray([a[i] for i in idx_cr])  # sort by column 1, then 0\r\n#    output_polylines(out_fc, SR, [pl])  # ***** it works\r\n\r\n#    in_pnts = [tuple(i) for i in a_s.tolist()]\r\n#    pnts = concave(in_pnts, 3)\r\n#    output_polylines(out_fc, SR, [pnts])\r\n#    ps = [(207, 184), (393, 60), (197, 158), (197, 114), (128, 261),\r\n#          (442, 40), (237, 159), (338, 75), (194, 93), (33, 159),\r\n#          (393, 152), (433, 267), (324, 141), (384, 183), (273, 165),\r\n#          (250, 257), (423, 198), (227, 68), (120, 184), (214, 49),\r\n#          (256, 75), (379, 93), (312, 49), (471, 187), (366, 122)]\r\n#    a = np.array(ps)\r\n#    a_s = a[a[:, 1].argsort(axis=0)]  # sort to get lowest y-value\r\n#    idx_cr = np.lexsort((a[:, 0], a[:, 1]))\r\n#    in_pnts = np.asarray([a[i] for i in idx_cr])  # sort by column 1, then 0\r\n##    cx = concave(in_pnts, 3)\r\n'"
PointTools/Scripts/mesh_pnts.py,12,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   mesh_pnts.py\r\n:Author:   Dan_Patterson@carleton.ca\r\n:Modified: 2017-04-11\r\n:Purpose:  Just makes points on a grid as well as the meshgrid\r\n:\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\nfrom textwrap import dedent\r\nfrom arcpytools_pnt import array_fc, fc_info, tweet\r\n\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=3, linewidth=80, precision=2, suppress=True,\r\n                    threshold=50, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\ndef mesh_xy(L=0, B=0, R=5, T=5, dx=1, dy=1, as_rec=True, top_down=True):\r\n    """"""Create a mesh of coordinates within the specified X, Y ranges\r\n    :Requires:\r\n    :--------\r\n    :  L(eft), R(igh), dx - coordinate min, max and delta x for X axis\r\n    :  B(ott), T(op), dy  - same,  Y axis\r\n    :  as_rec - produce a structured array (or convert to a record array)\r\n    :Returns:\r\n    :-------\r\n    :  A list of coordinates of X,Y pairs and an ID if as_rec is True.\r\n    :  A mesh grid X and Y coordinates is also produced.\r\n    :-------------\r\n    """"""\r\n    dt = [(\'Pnt_num\', \'<i4\'),\r\n          (\'X\', \'<f8\'), (\'Y\', \'<f8\'),\r\n          (\'Xs\', \'<f8\'), (\'Ys\', \'<f8\'), (\'RowCol\', \'U12\')]\r\n    x = np.arange(L, R + dx, dx, dtype=\'float64\')\r\n    if top_down:\r\n        y = np.arange(T, B-dy, -dy, dtype=\'float64\')\r\n    else:\r\n        y = np.arange(B, T+dy, dy, dtype=\'float64\')\r\n    mesh = np.meshgrid(x, y, sparse=False)\r\n    xs = mesh[0].ravel()\r\n    ys = mesh[1].ravel()\r\n    rc = np.indices(mesh[0].shape)\r\n    rows = rc[0].ravel()\r\n    cols = rc[1].ravel()\r\n    rcs = np.array(list(zip(rows, cols)))\r\n    rclbl = [""r{:03.0f} c{:03.0f}"".format(*i) for i in rcs]\r\n    if as_rec:\r\n        p = list(zip(np.arange(len(xs)), xs, ys, xs, ys, rclbl))\r\n        pnts = np.array(p, dtype=dt)\r\n    else:\r\n        p = list(zip(xs, ys, xs, ys))\r\n        pnts = np.array(p)\r\n    return pnts\r\n\r\n\r\n# ---- main section ----\r\n#\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    create_output = False\r\n    fc = ""/Point_tools.gdb/mesh_bounds""\r\n    flder = ""/"".join(script.split(""/"")[:-2])\r\n    extent_fc = flder + fc\r\n    angle = 30.0\r\n    out_fc = flder + ""/Point_tools.gdb/mesh_pnts""\r\n    dx = 250.0\r\n    dy = 250.0\r\n    top_down = True\r\nelse:\r\n    testing = False\r\n    create_output = True\r\n    extent_fc = sys.argv[1]  # must be a featureclass\r\n    dx = abs(float(sys.argv[2]))\r\n    dy = abs(float(sys.argv[3]))\r\n    top_down = sys.argv[4]\r\n    out_fc = sys.argv[5]\r\n\r\n\r\narcpy.env.overwriteOutput = True\r\nshp_fld, oid_fld, shp_type, SR = fc_info(extent_fc)\r\ndesc = arcpy.da.Describe(extent_fc)\r\nxtent = (desc[\'extent\'].__str__()).split("" "")[:4]\r\nL, B, R, T = [float(i) for i in xtent]\r\nfld_names = [\'X\', \'Y\']\r\npnts = mesh_xy(L, B, R, T, dx, dy, as_rec=True, top_down=top_down)\r\n# ---- create output\r\nif create_output:\r\n  array_fc(pnts, out_fc, fld_names, SR)\r\nln = ""-""*70\r\nfrmt = """"""\\n\r\n:{}:\r\n:Script... {}\r\n:Output to..... {}\r\n:using ........ {}\r\n:Processing extent specified...\r\n: L {}, B {}, R {}, T {}\r\n:X spacing...{}\r\n:Y spacing...{}\r\n:Points......\r\n{!r:}\r\n:\r\n:{}:""\r\n""""""\r\nargs = [ln, script, out_fc, SR.name, L, B, R, T, dx, dy, pnts, ln]\r\nmsg = frmt.format(*args)\r\ntweet(msg)\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\n\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n#    pnts, mesh = _demo()\r\n'"
PointTools/Scripts/movepnts.py,1,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:  movepnts.py\r\n:Author:  Dan.Patterson@carleton.ca\r\n:Modified: 2017-04-06\r\n:Notes:\r\n:- arcpy.da.FeatureClassToNumPyArray(in_table, field_names, {where_clause},\r\n:                                    {spatial_reference}, {explode_to_points},\r\n:                                    {skip_nulls}, {null_value})\r\n:- arcpy.da.NumPyArrayToFeatureClass(in_array, out_table, shape_fields,\r\n:                                    {spatial_reference})\r\n:- create multipart polygons: https://geonet.esri.com/message/461451\r\n: our house relative to 0,0 in MTM9\r\n: xy_shift = [341886,5023462]\r\n:\r\n:Spatial reference\r\n: NAD_1983_CSRS_MTM_9\r\n: WKID: 2951 Authority: EPSG\r\n: in_fc = r\'C:\\GIS\\Table_tools\\Table_tools.gdb\\polygon_demo\'\r\n: dx = 2\r\n: dy = 2\r\n: out_fc = r\'C:\\GIS\\Table_tools\\Table_tools.gdb\\bb\'\r\n""""""\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\nfrom arcpytools_pnt import fc_info, tweet\r\n\r\narcpy.env.overwriteOutput = True\r\n\r\n\r\n# ---- input parameters ----\r\nin_fc = sys.argv[1]\r\ndx = float(sys.argv[2])\r\ndy = float(sys.argv[3])\r\nout_fc = sys.argv[4]\r\nxy_shift = np.array([dx, dy], dtype=""<f8"")\r\nshp_field, OIDField, shp_type, SR = fc_info(in_fc)\r\n# ---- convert to array, shift and return ----\r\n# Apparently, there can be problems writing directly to a featureclass\r\n# so, write to in_memory changing the required field names, then copy out\r\narr = arcpy.da.FeatureClassToNumPyArray(in_fc, ""*"", """", SR, True)\r\narr[shp_field] = arr[shp_field] + xy_shift\r\nnms = [\'Feat_id\', \'XYs\'] + [i for i in arr.dtype.names[2:]]\r\narr.dtype.names = nms\r\ntemp_out = ""in_memory/temp2""\r\narcpy.da.NumPyArrayToFeatureClass(arr, temp_out, [\'XYs\'])\r\narcpy.CopyFeatures_management(temp_out, out_fc)\r\ndel temp_out\r\n# ---- the end ----\r\n'"
PointTools/Scripts/radial_sort.py,23,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   radial_sort.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2018-02-27\r\n:Purpose:  tools for working with numpy arrays\r\n:Useage:\r\n:\r\n:References:\r\n:\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport os\r\nfrom textwrap import dedent\r\nimport numpy as np\r\nimport arcpy\r\nfrom arcpytools_pnt import _describe, fc_info, tweet\r\nimport warnings\r\n\r\nwarnings.simplefilter(\'ignore\', FutureWarning)\r\n\r\narcpy.env.overwriteOutput = True\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n# -------------------------------------------------------------------------\r\n\r\nmsg0 = """"""\r\n: -----------------------------------------------------------\r\nScript....\r\n.... {}\r\nNo output produced because either...\r\n - The input and/or output path and/or filename has a space in it.\r\n - The output path was no good.\r\n - A projected coordinate system is required for the inputs\r\nCopy and run with locally stored data or fix one or more conditions...\r\n...\r\n: -----------------------------------------------------------\r\n""""""\r\n\r\nmsg1 = """"""\r\n: -----------------------------------------------------------\r\nScript....\r\n.... {}\r\nCompleted....\r\n...\r\n: -----------------------------------------------------------\r\n""""""\r\n\r\n\r\ndef check_files(file_path, ext=""""):\r\n    """"""Check expected file paths and extensions, to ensure compliance with\r\n    :  tool specifications\r\n    """"""\r\n    is_good = True\r\n    head, tail = os.path.split(file_path)\r\n    if not os.path.exists(head):\r\n        return False\r\n    if "" "" in tail:\r\n        return False\r\n    if os.path.splitext(tail)[1] != ext:\r\n        return False\r\n    if "" "" in file_path:\r\n        return False\r\n    return is_good\r\n    # ----\r\n\r\n\r\ndef extent_cent(in_fc):\r\n    """"""Some basic featureclass properties\r\n    """"""\r\n    ext = arcpy.Describe(in_fc).extent\r\n    ext_poly = ext.polygon\r\n    cent = ext_poly.centroid\r\n    return cent\r\n\r\n\r\ndef _xyID(in_fc, to_pnts=True):\r\n    """"""Convert featureclass geometry (in_fc) to a simple 2D structured array\r\n    :  with ID, X, Y values. Optionally convert to points, otherwise centroid.\r\n    """"""\r\n    flds = [\'OID@\', \'SHAPE@X\', \'SHAPE@Y\']\r\n    args = [in_fc, flds, None, None, to_pnts, (None, None)]\r\n    cur = arcpy.da.SearchCursor(*args)\r\n    a = cur._as_narray()\r\n    a.dtype = [(\'IDs\', \'<i4\'), (\'Xs\', \'<f8\'), (\'Ys\', \'<f8\')]\r\n    del cur\r\n    return a\r\n\r\n\r\ndef _center(a, remove_dup=True):\r\n    """"""Return the center of an array. If the array represents a polygon, then\r\n    :  a check is made for the duplicate first and last point to remove one.\r\n    """"""\r\n    if remove_dup:\r\n        if np.all(a[0] == a[-1]):\r\n            a = a[:-1]\r\n    return a.mean(axis=0)\r\n\r\n\r\ndef e_dist(a, b, metric=\'euclidean\'):\r\n    """"""Distance calculation for 1D, 2D and 3D points using einsum\r\n    : a, b   - list, tuple, array in 1,2 or 3D form\r\n    : metric - euclidean (\'e\',\'eu\'...), sqeuclidean (\'s\',\'sq\'...),\r\n    :-----------------------------------------------------------------------\r\n    """"""\r\n    a = np.asarray(a)\r\n    b = np.atleast_2d(b)\r\n    a_dim = a.ndim\r\n    b_dim = b.ndim\r\n    if a_dim == 1:\r\n        a = a.reshape(1, 1, a.shape[0])\r\n    if a_dim >= 2:\r\n        a = a.reshape(np.prod(a.shape[:-1]), 1, a.shape[-1])\r\n    if b_dim > 2:\r\n        b = b.reshape(np.prod(b.shape[:-1]), b.shape[-1])\r\n    diff = a - b\r\n    dist_arr = np.einsum(\'ijk,ijk->ij\', diff, diff)\r\n    if metric[:1] == \'e\':\r\n        dist_arr = np.sqrt(dist_arr)\r\n    dist_arr = np.squeeze(dist_arr)\r\n    return dist_arr\r\n\r\n\r\ndef radial_sort(pnts, cent=None):\r\n    """"""Sort about the point cloud center or from a given point\r\n    : pnts - an array of points (x,y) as array or list\r\n    : cent - list, tuple, array of the center\'s x,y coordinates\r\n    :      - cent = [0, 0] or np.array([0, 0])\r\n    :Returns: the angles in the range -180, 180 x-axis oriented\r\n    """"""\r\n    pnts = np.asarray(pnts, dtype=np.float64)\r\n    if cent is None:\r\n        cent = _center(pnts, remove_dup=False)\r\n    ba = pnts - cent\r\n    ang_ab = np.arctan2(ba[:, 1], ba[:, 0])\r\n    ang_ab = np.degrees(ang_ab)\r\n    sort_order = np.argsort(ang_ab)\r\n    return ang_ab, sort_order\r\n\r\n\r\ndef output_points(out_fc, pnts):\r\n    """"""Produce the output point featureclass""""""\r\n    msg = \'\\nRead the script header... A projected coordinate system required\'\r\n    assert (SR is not None), msg  # and (SR.type == \'Projected\'), msg\r\n    pnts_lst = []\r\n    for pnt in pnts:                 # create the point geometry\r\n        pnts_lst.append(arcpy.PointGeometry(arcpy.Point(*pnt), SR))\r\n    if arcpy.Exists(out_fc):     # overwrite any existing versions\r\n        arcpy.Delete_management(out_fc)\r\n    arcpy.CopyFeatures_management(pnts_lst, out_fc)\r\n    return out_fc\r\n\r\n\r\ndef output_polylines(out_fc, SR, pnts):\r\n    """"""Produce the output polyline featureclass""""""\r\n    msg = \'\\nRead the script header... A projected coordinate system required\'\r\n    assert (SR is not None), msg  # and (SR.type == \'Projected\'), msg\r\n    polylines = []\r\n    for pair in pnts:                 # create the polyline geometry\r\n        pl = arcpy.Polyline(arcpy.Array([arcpy.Point(*xy) for xy in pair]), SR)\r\n        polylines.append(pl)\r\n    if arcpy.Exists(out_fc):     # overwrite any existing versions\r\n        arcpy.Delete_management(out_fc)\r\n    arcpy.CopyFeatures_management(polylines, out_fc)\r\n    return out_fc\r\n\r\n\r\ndef test_envs(in_fc, cent, out_fc0, out_fc1):\r\n    """""" test the required parameters\r\n    """"""\r\n    # (1) ---- check input feature and for projected data\r\n    if not arcpy.Exists(in_fc):\r\n        tweet(""\\nThis file doesn\'t exist.\\n"")\r\n        return False, []\r\n    shp_fld, oid_fld, shp_type, SR = fc_info(in_fc)\r\n    #\r\n    if SR.type != \'Projected\':\r\n        tweet(""\\nRadial sorts only make sense for projected data.\\n"")\r\n        return False, []\r\n    if shp_type != \'Point\':\r\n        tweet(""\\nYou need a point file.\\n"")\r\n        return False, []\r\n    #\r\n    # (2) ---- check the output files\r\n    if out_fc0 not in (None, \'None\', "" "", """", ""#""):\r\n        is_good = check_files(out_fc0)\r\n        if not is_good:\r\n            tweet(""\\nWrong path or filename?....{}\\n"".format(out_fc0))\r\n            return False, []\r\n    if out_fc1 not in (None, \'None\', "" "", """", ""#""):\r\n        is_good = check_files(out_fc1)\r\n        if not is_good:\r\n            tweet(""\\nWrong path or filename?....{}\\n"".format(out_fc1))\r\n            return False, []\r\n    #\r\n    # (3) check the center ....\r\n    if cent in (None, \'None\', "" "", """", ""#""):\r\n        cent = None\r\n    elif isinstance(cent, str):\r\n        for i in ["", "", "","", "";""]:\r\n            cent = cent.replace(i, "" "")\r\n        try:\r\n            cent = [float(i.strip()) for i in cent.split("" "")]\r\n            if len(cent) != 2:\r\n                cent = [cent[0], cent[0]]\r\n                tweet(""\\nBad center so I used... {} instead \\n"".format(cent))\r\n        except ValueError:\r\n            cent = None\r\n            tweet(""\\nCenter used... {}\\n"".format(cent))\r\n    # (4) all should be good\r\n    return True, [out_fc0, out_fc1, cent, SR]\r\n\r\n\r\ndef _tool():\r\n    """"""run when script is from a tool\r\n    """"""\r\n    in_fc = sys.argv[1]\r\n    cent = str(sys.argv[2])\r\n    from_north = str(sys.argv[3])\r\n    out_fc0 = sys.argv[4]\r\n    out_fc1 = sys.argv[5]\r\n    return in_fc, from_north, cent, out_fc0, out_fc1\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# .... running script or testing code section\r\n\r\ngdb_pth = ""/"".join(script.split(""/"")[:-2]) + ""/Data/Point_tools.gdb""\r\n\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    in_fc = gdb_pth + r""/radial_pnts""\r\n    cent = None\r\n    out_fc0 = gdb_pth + r""/radial""\r\n    out_fc1 = gdb_pth + r""/OD_01""\r\nelse:\r\n    testing = False\r\n    in_fc, from_north, cent, out_fc0, out_fc1 = _tool()\r\n\r\n#\r\n# (1) Run the test to see whether to continue\r\n#\r\nresults = test_envs(in_fc, cent, out_fc0, out_fc1)\r\ncleared, vals = results\r\n#\r\nif not cleared:\r\n    tweet(dedent(msg0).format(script))\r\nelse:\r\n    tweet(""\\nPassed all checks-------------\\n"")\r\n    #\r\n    # ---- Process section ------------------------------\r\n    #\r\n    pnts_out, plys_out, cent, SR = vals\r\n    desc = _describe(in_fc)\r\n    arcpy.env.workspace = desc[\'path\']  # set the workspace to the gdb\r\n    arr = _xyID(in_fc, to_pnts=True)\r\n    indx = arr[\'IDs\']\r\n    pnts = arr[[\'Xs\', \'Ys\']]\r\n    pnts = pnts.view(np.float64).reshape(pnts.shape[0], 2)\r\n    if cent is None:\r\n        cent = np.mean(pnts, axis=0).tolist()\r\n    #\r\n    # (2) perform the radial sort ....\r\n    #\r\n    ang_ab, sort_order = radial_sort(pnts, cent=cent)  # angles and sort_order\r\n\r\n    # indx_sorted = indx[sort_order]\r\n    pnts_sorted = pnts[sort_order]\r\n    ang_sorted = ang_ab[sort_order]\r\n\r\n    dist_arr = e_dist(cent, pnts_sorted)\r\n    dist_indx = np.argsort(dist_arr)\r\n    dist_id = sort_order[dist_indx]  # indx[dist_indx]\r\n\r\n    pairs = [np.asarray([cent, pnt]) for pnt in pnts_sorted]\r\n\r\n    # ---- form the output results for use with extend table\r\n    #\r\n    dt = [(\'IDcent\', \'<i4\'), (\'Xp\', \'<f8\'), (\'Yp\', \'<f8\'), (\'Angle_\', \'<f8\'),\r\n          (\'Dist_\', \'<f8\'), (\'Orig_ID\', \'<i4\')]\r\n    Xsort = pnts_sorted[:, 0]\r\n    Ysort = pnts_sorted[:, 1]\r\n    ang_id = sort_order + 1  # the centroid ID for the sorted IDs\r\n    ext_tbl = np.empty(arr.shape, dtype=dt)\r\n    nms = ext_tbl.dtype.names\r\n    new_oid = np.arange(1, arr.shape[0]+1)  # to match OBJECTID values\r\n    vals = [new_oid, Xsort, Ysort, ang_sorted, dist_arr, ang_id]\r\n    for i in range(len(nms)):\r\n        ext_tbl[nms[i]] = vals[i]\r\n        ext_tbl2 = np.copy(ext_tbl)\r\n    # ---- create the output point file\r\n    tweet(""plys etc out {}, {}"".format(pnts_out, plys_out))\r\n    if pnts_out != ""#"":\r\n        output_points(out_fc0, pnts_sorted.tolist())\r\n        arcpy.da.ExtendTable(out_fc0, \'OBJECTID\', ext_tbl, \'IDcent\')\r\n    if plys_out != ""#"":\r\n        output_polylines(out_fc1, SR, pairs)\r\n        arcpy.da.ExtendTable(out_fc1, \'OBJECTID\', ext_tbl2, \'IDcent\')\r\nif not testing:\r\n    tweet(\'\\nDone....\')\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n#    _demo()\r\n'"
PointTools/Scripts/rotatepnts.py,11,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\nrotate_pnts\r\n===========\r\n\r\nScript:  rotate_pnts.py\r\n\r\nAuthor:  Dan.Patterson@carleton.ca\r\n\r\nModified: 2018-07-23\r\n\r\nNotes:\r\n-----\r\n>>> arcpy.da.FeatureClassToNumPyArray(in_table, field_names, {where_clause},\r\n                                      {spatial_reference}, {explode_to_points},\r\n                                      {skip_nulls}, {null_value})\r\n>>> arcpy.da.NumPyArrayToFeatureClass(in_array, out_table, shape_fields,\r\n                                      {spatial_reference})\r\n\r\nData references for standalone testing is from the Point_tools database\r\n""""""\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\nfrom arcpytools_pnt import fc_info, tweet\r\n\r\nscript = sys.argv[0]\r\n\r\narcpy.env.overwriteOutput = True\r\n\r\ndef extent_(a):\r\n    """"""Extent of an array.\r\n\r\n    Returns:\r\n    --------\r\n    L(eft), B(ottom), R(ight), T(op)\r\n\r\n    >>> a = np.array([[0, 0], [0, 1], [1, 1], [1, 0], [0, 0]])\r\n    >>> extent_(a)\r\n    [0, 0, 1, 1]\r\n    """"""\r\n    L, B = np.min(a, axis=0)\r\n    R, T = np.max(a, axis=0)\r\n    return [L, B, R, T]\r\n\r\n\r\ndef trans_rot(a, angle=0.0, unique=True):\r\n    """"""Translate and rotate and array of points about the point cloud origin.\r\n\r\n    Requires:\r\n    ---------\r\n    a : array\r\n        2d array of x,y coordinates.\r\n    angle : double\r\n        angle in degrees in the range -180. to 180\r\n    unique :\r\n        If True, then duplicate points are removed.  If False, then this would\r\n        be similar to doing a weighting on the points based on location.\r\n\r\n    Returns:\r\n    --------\r\n    Points rotated about the origin and translated back.\r\n\r\n    >>> a = np.array([[0, 0], [0, 1], [1, 1], [1, 0], [0, 0]])\r\n    >>> b = trans_rot(b, 45)\r\n    >>> b\r\n    array([[ 0.5,  1.5],\r\n           [ 1.5,  1.5],\r\n           [ 0.5, -0.5],\r\n           [ 1.5, -0.5]])\r\n\r\n    Notes:\r\n    ------\r\n    - if the points represent a polygon, make sure that the duplicate\r\n    - np.einsum(\'ij,kj->ik\', a - cent, R)  =  np.dot(a - cent, R.T).T\r\n    - ik does the rotation in einsum\r\n\r\n    >>> R = np.array(((c, s), (-s,  c)))  # clockwise about the origin\r\n    """"""\r\n    if unique:\r\n        a = np.unique(a, axis=0)\r\n    cent = a.mean(axis=0)\r\n    angle = np.radians(angle)\r\n    c, s = np.cos(angle), np.sin(angle)\r\n    R = np.array(((c, s), (-s,  c)))\r\n    return  np.einsum(\'ij,kj->ik\', a - cent, R) + cent\r\n\r\n\r\n# ---- main section ----\r\n#\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    fc = ""/Point_tools.gdb/std_dist_center""\r\n    flder = ""/"".join(script.split(""/"")[:-2])\r\n    in_fc = flder + fc\r\n    angle = 30.0\r\n    out_fc = flder + ""/Point_tools.gdb/rot_std_dist""\r\n\r\nelse:\r\n    testing = False\r\n    in_fc = sys.argv[1]\r\n    angle = float(sys.argv[2])\r\n    out_fc = sys.argv[3]\r\n\r\n\r\n# ---- convert to array, shift and return ----\r\n# Apparently, there can be problems writing directly to a featureclass\r\n# so, write to in_memory changing the required field names, then copy out\r\n#\r\nshp_fld, oid_fld, shp_type, SR = fc_info(in_fc)\r\narr = arcpy.da.FeatureClassToNumPyArray(in_fc, ""*"", """", SR, True)\r\na = arr[shp_fld]\r\nnew_pnts = trans_rot(a, angle)\r\nnms = [\'Feat_id\', \'XYs\'] + [i for i in arr.dtype.names[2:]]\r\narr.dtype.names = nms\r\narr[\'XYs\'] = new_pnts\r\nif not testing:\r\n    arcpy.da.NumPyArrayToFeatureClass(arr, out_fc, [\'XYs\'])\r\n#\r\nmsg = """"""\r\n-------------------------------------\r\nInput points..... {}\r\nRotation angle... {}\r\nOutput points.... {}\r\n-------------------------------------\r\n""""""\r\ntweet(msg.format(in_fc, angle, out_fc))\r\n# ---- the end ----\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n    fc = ""/Point_tools.gdb/std_dist_center""\r\n    flder = ""/"".join(script.split(""/"")[:-2])\r\n    in_fc = flder + fc'"
PointTools/Scripts/sort_pnts2line.py,24,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\nscript name\r\n===========\r\n\r\nScript :   sort_pnts2line.py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified : 2018-06-12\r\n\r\nPurpose:  tools for working with numpy arrays\r\n\r\nUseage :\r\n\r\nReferences\r\n----------\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/numpyarraytotable.htm>`_.\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/tabletonumpyarray.htm>`_.\r\n`<https://stackoverflow.com/questions/37742358/sorting-points-to-form-a-\r\ncontinuous-line/37744549#37744549`_.\r\n\r\n---------------------------------------------------------------------\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nfrom textwrap import dedent\r\nimport numpy as np\r\nfrom arcpytools import fc_info, tweet\r\nfrom arcpytools_pnt import frmt_rec, make_row_format, _col_format, form_\r\nimport arcpy\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.2f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\ndef e_dist(a, b, metric=\'euclidean\'):\r\n    """"""Distance calculation for 1D, 2D and 3D points using einsum\r\n    : a, b   - list, tuple, array in 1,2 or 3D form\r\n    : metric - euclidean (\'e\',\'eu\'...), sqeuclidean (\'s\',\'sq\'...),\r\n    :-----------------------------------------------------------------------\r\n    """"""\r\n    a = np.asarray(a)\r\n    b = np.atleast_2d(b)\r\n    a_dim = a.ndim\r\n    b_dim = b.ndim\r\n    if a_dim == 1:\r\n        a = a.reshape(1, 1, a.shape[0])\r\n    if a_dim >= 2:\r\n        a = a.reshape(np.prod(a.shape[:-1]), 1, a.shape[-1])\r\n    if b_dim > 2:\r\n        b = b.reshape(np.prod(b.shape[:-1]), b.shape[-1])\r\n    diff = a - b\r\n    dist_arr = np.einsum(\'ijk,ijk->ij\', diff, diff)\r\n    if metric[:1] == \'e\':\r\n        dist_arr = np.sqrt(dist_arr)\r\n    dist_arr = np.squeeze(dist_arr)\r\n    return dist_arr\r\n\r\n\r\ndef tbl_2_nparray(in_tbl, flds):\r\n    """"""Form the TableToNumPyArray to account for nulls for various dtypes.\r\n    This is essentially a shortcut to `arcpy.da.TableToNumPyArray`\r\n\r\n    Requires\r\n    --------\r\n    `in_tbl` :\r\n        table, or featureclass table name\r\n    `flds` :\r\n        list of field names\r\n    `skip_nulls` = False :\r\n        set within function\r\n    `null_value` :\r\n        determined from the dtype of the array...\r\n        otherwise you may as well do it manually\r\n\r\n    Source\r\n    ------\r\n    arraytools, apt.py module\r\n    """"""\r\n    int_min = np.iinfo(np.int32).min\r\n    float_min = np.finfo(np.float64).min\r\n    str_val = ""None""\r\n    nulls = {\'Double\':float_min, \'Integer\':int_min, \'Text\':str_val}\r\n    #\r\n    fld_dict = {i.name: i.type for i in arcpy.ListFields(in_tbl)}\r\n    null_dict = {f:nulls[fld_dict[f]] for f in flds}\r\n    a = arcpy.da.TableToNumPyArray(in_table=in_tbl, field_names=flds,\r\n                                   skip_nulls=False,\r\n                                   null_value=null_dict)\r\n    return a\r\n\r\n\r\ndef your_func_here():\r\n    """""" this is the place""""""\r\n    pass\r\n\r\n\r\n# ---- Run options: _demo or from _tool\r\n#\r\ndef _demo():\r\n    """"""Code to run if in demo mode\r\n    """"""\r\n#    a = np. array([\'1, 2, 3, 4, 5\', \'a, b, c\', \'6, 7, 8, 9\',\r\n#                   \'d, e, f, g, h\', \'10, 11, 12, 13\'])\r\n    # ---- make random points that should form a line ---- see link\r\n#    from scipy.spatial import kdtree as kd\r\n    x0 = np.linspace(0, 2 * np.pi, 10)\r\n    y0 = np.sin(x0)\r\n    a0 = np.c_[x0, y0]\r\n    d0 = e_dist(a0, a0)\r\n    idx0 = np.arange(len(x0))\r\n    np.fill_diagonal(d0, np.inf)\r\n    # ---- randomize\r\n    idx = np.random.permutation(idx0)\r\n    x = x0[idx]\r\n    y = y0[idx]\r\n    a = np.c_[x, y]  # points array\r\n    d = e_dist(a, a)\r\n    np.fill_diagonal(d, np.inf)\r\n    #\r\n    m = np.argmin(d0, axis=1)\r\n    ft = np.zeros((10,), dtype=[(\'ID\', \'<i4\'), (\'Closest\', \'<i4\')] )\r\n    ft[\'ID\'] = idx0\r\n    ft[\'Closest\'] = m\r\n    return idx, a0, a, d0, d, ft\r\n\r\n\r\ndef _tool():\r\n    """"""run when script is from a tool\r\n    """"""\r\n    in_tbl = sys.argv[1]\r\n    in_fld = sys.argv[2]\r\n    out_fld = sys.argv[3]  # output field name\r\n\r\n    # ---- main tool section\r\n    desc = arcpy.da.Describe(in_tbl)\r\n    # ---- main tool section\r\n    _, oid_fld, _, _ = fc_info(in_tbl, prn=False)  # run fc_info\r\n    #\r\n    flds = [oid_fld, in_fld]\r\n    tbl_path = desc[\'path\']\r\n    fnames = [i.name for i in arcpy.ListFields(in_tbl)]\r\n    if out_fld in fnames:\r\n        out_fld += \'dup\'\r\n    out_fld = arcpy.ValidateFieldName(out_fld, tbl_path)\r\n    args = [in_tbl, in_fld, out_fld, tbl_path]\r\n    msg = ""in_tbl {}\\nin_fld {}\\nout_fld  {}\\ntbl_path  {}"".format(*args)\r\n    tweet(msg)\r\n    #\r\n    # ---- call section for processing function\r\n    #\r\n    #in_arr = arcpy.da.TableToNumPyArray(in_tbl, vals)  # old\r\n    in_arr = tbl_2_nparray(in_tbl, flds)  # produce the table\r\n    #\r\n    tweet(""{!r:}"".format(in_arr))\r\n    #\r\n    a0 = in_arr[in_fld]\r\n    #\r\n    # do stuff here ********************************\r\n    #\r\n    sze = a0.dtype.str\r\n    # ---- reassemble the table for extending\r\n    dt = [(\'IDs\', \'<i8\'), (out_fld, sze)]\r\n    out_array = np.copy(in_arr.shape[0])\r\n    out_array[out_fld] = a0  # result goes here\r\n    out_array.dtype = dt\r\n    arcpy.da.ExtendTable(in_tbl, \'OBJECTID\', out_array, \'IDs\')\r\n\r\n# ----------------------------------------------------------------------\r\n# .... final code section producing the featureclass and extendtable\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    idx, a0, a, d0, d, ft = _demo()\r\n    frmt = """"""\r\n    Testing...\r\n    unsorted...\r\n    {}\r\n    sorted.....\r\n    {}\r\n    """"""\r\n    args = [a0, a]\r\n    tweet(dedent(frmt).format(a0, a))\r\n    tweet(form_(d0, prn=False))\r\n    tweet(form_(d, title=""Sorted..."", prn=False))\r\n    tweet(frmt_rec(ft, prn=False))\r\nelse:\r\n    testing = False\r\n    _tool()\r\n#\r\nif not testing:\r\n    tweet(\'Some message here...\')\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n#    a = _demo()\r\n'"
PointTools/Scripts/sortpnts.py,6,"b'# -*- coding: utf-8 -*-\r\n""""""\r\n:Script:   sortpnts.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2017-02-28\r\n:Purpose: Sort points by X or Y in ascending or descending order\r\n""""""\r\n\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\nfrom arcpytools_pnt import fc_info, tweet\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.1f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=100, precision=2,\r\n                    suppress=True, threshold=120, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')\r\n\r\nscript = sys.argv[0]\r\n\r\n# ---- Convert the featureclass to points returning base information ----\r\n# ---- The \'Shape\' field is changed to X and Y to facilitate sorting etc.\r\nin_fc = sys.argv[1]\r\nsrt_order = sys.argv[2]\r\nascend = sys.argv[3]\r\nout_fc = sys.argv[4]\r\n\r\nshp_fld, oid_fld, shp_type, SR = fc_info(in_fc)\r\n\r\na = arcpy.da.FeatureClassToNumPyArray(in_fc, ""*"", """", SR)\r\ndt = [(\'X\', \'<f8\'), (\'Y\', \'<f8\')]\r\nshps = np.array([tuple(i) for i in a[shp_fld]], dtype=dt)\r\n\r\nif srt_order == \'X\':\r\n    idx = np.argsort(shps, order=(\'X\', \'Y\'))\r\nelse:\r\n    idx = np.argsort(shps, order=(\'Y\', \'X\'))\r\n\r\nshps = a[idx]\r\nif not ascend:\r\n    shps = shps[::-1]\r\n\r\narcpy.da.NumPyArrayToFeatureClass(shps, out_fc, shp_fld, SR)\r\n#\r\nfrmt = """"""\\n\\nScript.... {}\\nUsing..... {}\\nSR...{}\\nSorting by... {},\r\nascending... {}\\nProducing ... {}\\n""""""\r\nargs = [script, in_fc, SR.name, srt_order, ascend, out_fc]\r\ntweet(frmt.format(*args))\r\n\r\n# -------------------------------------------------------------------------\r\nif __name__ == ""__main__"":\r\n    """""" No demo  """"""\r\n#    in_fc = r""C:\\GIS\\Geometry_projects\\Spiral_sort\\Polygons\\Parcels.shp""\r\n'"
PointTools/Scripts/spaced.py,15,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   spaced.py\r\n:Author:   Dan_Patterson@carleton.ca\r\n:Modified: 2017-04-11\r\n:Purpose:  tools for working with numpy arrays\r\n:\r\n:Original sources:\r\n:----------------\r\n:n_spaced :  ...\\arraytools\\geom\\n_spaced.py\r\n: - n_spaced(L=0, B=0, R=10, T=10, min_space=1, num=10, verbose=True)\r\n:   Produce num points within the bounds specified by the extent (L,B,R,T)\r\n:   L(eft), B, R, T(op) - extent coordinates\r\n:   min_space - minimum spacing between points.\r\n:   num - number of points... this value may not be reached if the extent\r\n:   is too small and the spacing is large relative to it.\r\n:\r\n:arr_struct :  ...\\arcpytools.py\r\n: - array_struct(a, fld_names=[\'X\', \'Y\'], dt=[\'<f8\', \'<f8\']):\r\n:   Convert an array to a structured array\r\n:   a - an ndarray with shape at least (N,2)\r\n:   dt = dtype class\r\n:   names - names for the fields\r\n:\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nfrom textwrap import dedent\r\nimport numpy as np\r\n# import arcpy\r\nfrom arcpytools_pnt import array_fc, array_struct, tweet\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\n# ---------------------------------------------------------------------------\r\n# ---- from arraytools.geom ----\r\ndef n_spaced(L=0, B=0, R=10, T=10, min_space=1, num=10, verbose=True):\r\n    """"""Produce num points within the bounds specified by the extent (L,B,R,T)\r\n    :Requires:\r\n    :--------\r\n    :  L(eft), B, R, T(op) - extent coordinates\r\n    :  min_space - minimum spacing between points.\r\n    :  num - number of points... this value may not be reached if the extent\r\n    :        is too small and the spacing is large relative to it.\r\n    """"""\r\n    #\r\n    def _pnts(L, B, R, T, num):\r\n        """"""Create the points""""""\r\n        xs = (R-L) * np.random.random_sample(size=num) + L\r\n        ys = (T-B) * np.random.random_sample(size=num) + B\r\n        return np.array(list(zip(xs, ys)))\r\n\r\n    def _not_closer(a, min_space=1):\r\n        """"""Find the points that are greater than min_space in the extent.""""""\r\n        b = a.reshape(np.prod(a.shape[:-1]), 1, a.shape[-1])\r\n        diff = b - a\r\n        dist = np.einsum(\'ijk,ijk->ij\', diff, diff)\r\n        dist_arr = np.sqrt(dist).squeeze()\r\n        case = ~(np.triu(dist_arr <= min_space, 1)).any(0)\r\n        return a[case]\r\n    #\r\n    cnt = 1\r\n    n = num * 2  # check double the number required as a check\r\n    result = 0\r\n    frmt = ""Examined: {}  Found: {}  Need: {}""\r\n    a0 = []\r\n    while (result < num) and (cnt < 6):  # keep using random points\r\n        a = _pnts(L, B, R, T, num)\r\n        if cnt > 1:\r\n            a = np.vstack((a0, a))\r\n        a0 = _not_closer(a, min_space)\r\n        result = len(a0)\r\n        if verbose:\r\n            print(dedent(frmt).format(n, result, num))\r\n        cnt += 1\r\n        n += n\r\n    # perform the final sample and calculation\r\n    use = min(num, result)\r\n    a0 = a0[:use]  # could use a0 = np.random.shuffle(a0)[:num]\r\n    a0 = a0[np.argsort(a0[:, 0])]\r\n    return a0\r\n\r\n\r\n# ---- main section ---------------------------------------------------------\r\n#\r\naoi = sys.argv[1]  # \'340000 5020000 344999.999999999 5025000 NaN NaN NaN NaN\'\r\nmin_space = int(sys.argv[2])\r\nnum = int(sys.argv[3])\r\nSR = sys.argv[4]\r\nout_fc = sys.argv[5]\r\n\r\nfrmt = """"""\\n\r\nAOI extent for points...\r\n{}\r\nMinimum spacing.... {}\r\nNumber of points... {}\r\nSpatial reference.. {}\r\nOutput featureclass.. {}\\n\r\n""""""\r\nargs = [aoi, min_space, num, SR, out_fc]\r\nmsg = frmt.format(*args)\r\ntweet(msg)\r\n# ---- perform the point creation ----\r\naoi = aoi.split("" "")[:4]             # extent is returned as a string\r\next = [round(float(i)) for i in aoi]\r\nL, B, R, T = ext\r\na = n_spaced(L, B, R, T, min_space, num, verbose=False)\r\nall_flds = [\'X\', \'Y\', \'x_coord\', \'y_coord\']\r\nxy_flds = all_flds[:2]\r\nxy_dt = [\'<f8\', \'<f8\', \'float\', \'float\']\r\na = np.c_[(a, a)]\r\nz = array_struct(a, fld_names=all_flds, dt=xy_dt)\r\n# z = np.zeros((len(a)), dtype=[(\'X\', \'<f8\'), (\'Y\', \'<f8\')])\r\n# fld_names = (\'X\', \'Y\')\r\n# z[\'X\'] = a[:, 0]\r\n# z[\'Y\'] = a[:, 1]\r\nout_fc = array_fc(z, out_fc, xy_flds, SR)\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n    pass\r\n'"
PointTools/Scripts/spanning_tree.py,36,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\nspanning_tree\r\n=============\r\n\r\nScript: spanning_tree.py\r\n\r\nAuthor: Dan_Patterson@carleton.ca\r\n\r\nModified: 2018-06-13\r\n\r\nOriginal: ... mst.py in my github\r\n    extensive documentation is there.\r\n\r\nPurpose:\r\n--------\r\n\r\nProduce a spanning tree from a point set.  I have yet to confirm\r\nwhether it constitutes a minimum spanning tree, since the implementation\r\ndoesn\'t specify whether Prim\'s algorithm is being used (see ref. 2)\r\n\r\nReferences:\r\n-----------\r\n`<http://stackoverflow.com/questions/41903502/sort-two-dimensional-\r\nlist-python>`_.\r\n\r\n`<http://peekaboo-vision.blogspot.ca/2012/02/simplistic-minimum-\r\nspanning-tree-in.html>`_.\r\n\r\nAlso referenced here...\r\n\r\n`<http://stackoverflow.com/questions/34374839/minimum-spanning-tree-\r\ndistance-and-graph>`_.\r\n\r\nNotes:\r\n------\r\n\r\n>>> array \'a\' array([[ 0,  0],  constructed for minimum spanning tree example\r\n                     [ 0,  8],\r\n                     [10,  8],\r\n                     [10,  0],\r\n                     [ 3,  4],\r\n                     [ 7,  4]])\r\n\r\n(1) sorting\r\n\r\n>>> np.lexsort((a[:,1], a[:,0])) sort by x, then y\r\n>>> np.lexsort(a.T) >= np.lexsort((a[:,0], a[:,1])) sort y, x\r\n\r\n(2) Distances\r\n\r\nunsorted....\r\n\r\n>>> np.linalg.norm(a[1:] - a[:-1], axis=1)\r\narray([ 8.0,  10.0,  8.0,  8.1,  4.0])\r\n>>> np.sum(np.linalg.norm(a[1:] - a[:-1], axis=1)) => 38.0622...\r\n\r\nsorted....\r\n\r\n>>> a_srt = a[np.lexsort(a.T),:]\r\n>>> np.linalg.norm(a_srt[1:] - a_srt[:-1], axis=1)\r\narray([ 8.0,  5.0,  4.0,  5.0,  8.0])\r\n>>> np.sum(np.linalg.norm(a_srt[1:] - a_srt[:-1], axis=1)) => 30.0...\r\n\r\n(3) Near results...\r\n\r\n>>> coords, dist, n_array = n_near(s, N=2)\r\n  ID     Xo    Yo  C0_x C0_y   C1_x C1_y   Dist0 Dist1\r\n([(0,  0.0, 0.0,  3.0, 4.0,   0.0, 8.0,  5.0,  8.0),\r\n  (1,  0.0, 8.0,  3.0, 4.0,   0.0, 0.0,  5.0,  8.0),\r\n  (2,  3.0, 4.0,  7.0, 4.0,   0.0, 0.0,  4.0,  5.0),\r\n  (3,  7.0, 4.0,  3.0, 4.0,  10.0, 8.0,  4.0,  5.0),\r\n  (4, 10.0, 8.0,  7.0, 4.0,  10.0, 0.0,  5.0,  8.0),\r\n  (5, 10.0, 0.0,  7.0, 4.0,  10.0, 8.0,  5.0,  8.0)],\r\ndtype=[(\'ID\', \'<i4\'),\r\n       (\'Xo\', \'<f8\'), (\'Yo\', \'<f8\'),\r\n       (\'C0_X\', \'<f8\'), (\'C0_Y\', \'<f8\'),\r\n       (\'C1_X\', \'<f8\'), (\'C1_Y\', \'<f8\'),\r\n       (\'Dist0\', \'<f8\'), (\'Dist1\', \'<f8\')])\r\n\r\nConnections:\r\n\r\n>>> o_d\r\narray([(0, 2, 5.0),\r\n       (2, 3, 4.0),\r\n       (2, 1, 5.0),\r\n       (3, 4, 5.0),\r\n       (3, 5, 5.0)],\r\n      dtype=[(\'Orig\', \'<i4\'), (\'Dest\', \'<i4\'), (\'Dist\', \'<f8\')])\r\n\r\n>>> a[o_d[\'Orig\']]     a[o_d[\'Dest\']]\r\narray([[ 0,  0],   array([[10,  8],\r\n       [10,  8],          [10,  0],\r\n       [10,  8],          [ 0,  8],\r\n       [10,  0],          [ 3,  4],\r\n       [10,  0]])         [ 7,  4]])\r\n\r\ndistance array:\r\n\r\n>>> array([[ 5.0,  8.0,  8.1,  10.0,  12.8],\r\n           [ 5.0,  8.0,  8.1,  10.0,  12.8],\r\n           [ 4.0,  5.0,  5.0,   8.1,   8.1],\r\n           [ 4.0,  5.0,  5.0,   8.1,   8.1],\r\n           [ 5.0,  8.0,  8.1,  10.0,  12.8],\r\n           [ 5.0,  8.0,  8.1,  10.0,  12.8]])\r\n\r\nBack to the original distance and sorted array, a_srt.\r\n  The distances are determined using the sorted points, the diagonal\r\n  distances are set to np.inf so that they have the maximal distance.\r\n  The distance values can be sorted to get their indices in the array\r\n  Then the array can be sliced to retrieve the points coordinates and the\r\n  distance array can be sliced to get the distances.\r\n\r\n>>> dix = np.arange(d.shape[0])\r\nd[dix, dix] = np.inf\r\n\r\ndistance array, `d`\r\n\r\n>>> d\r\narray([[ inf,  8.0,  5.0,  8.1,  10.0,  12.8],\r\n       [ 8.0,  inf,  5.0,  8.1,  12.8,  10.0],\r\n       [ 5.0,  5.0,  inf,  4.0,  8.1,  8.1],\r\n       [ 8.1,  8.1,  4.0,  inf,  5.0,  5.0],\r\n       [ 10.0,  12.8,  8.1,  5.0,  inf,  8.0],\r\n       [ 12.8,  10.0,  8.1,  5.0,  8.0,  inf]])\r\n\r\n>>> np.argsort(d[0])  # => array([2, 1, 3, 4, 5, 0])\r\n\r\n>>> a_srt[np.argsort(d[0])]\r\narray([[3, 4], [ 0, 8], [7, 4], [10, 0], [10, 8], [0, 0]])\r\n\r\n>>> d[0][np.argsort(d[0])]  # => array([ 5.0, 8.0, 8.1, 10.0, 12.8, inf])\r\n\r\n: ---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\n#\r\n\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\nfrom arcpytools_pnt import fc_info, tweet\r\nfrom textwrap import dedent\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.1f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=100, precision=2,\r\n                    suppress=True, threshold=120,\r\n                    formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')\r\n\r\nscript = sys.argv[0]\r\n\r\n# ---- process and functions ----\r\n# (1) run dist_arr which calls _e_dist\r\n# (2) perform the mst (minimum spanning tree, using Prims algorithm)\r\n# (3) connect the points and return the structured array and then the fc\r\n\r\n\r\ndef dist_arr(a, prn=False):\r\n    """"""Minimum spanning tree prep... see main header\r\n    : paths from given data set...\r\n    """"""\r\n    # idx = np.lexsort(a.T)  # sort y, then x\r\n    idx = np.lexsort((a[:, 1], a[:, 0]))  # sort X, then Y\r\n    # idx= np.lexsort((a[:,0], a[:,1]))  # sort Y, then X\r\n    a_srt = a[idx, :]\r\n    d = _e_dist(a_srt)\r\n    if prn:\r\n        frmt = """"""\\n    {}\\n    :Input array...\\n    {}\\n\\n    :Sorted array...\r\n        {}\\n\\n    :Distance...\\n    {}\r\n        """"""\r\n        args = [dist_arr.__doc__, a, a_srt, d]  # d.astype(\'int\')]\r\n        print(dedent(frmt).format(*args))\r\n    return idx, a_srt, d\r\n\r\n\r\ndef _e_dist(a):\r\n    """"""Return a 2D square-form euclidean distance matrix.  For other\r\n    dimensions, use e_dist in ein_geom.py\r\n    """"""\r\n    b = a.reshape(np.prod(a.shape[:-1]), 1, a.shape[-1])\r\n    diff = a - b\r\n    d = np.sqrt(np.einsum(\'ijk,ijk->ij\', diff, diff)).squeeze()\r\n    # d = np.triu(d)\r\n    return d\r\n\r\n\r\ndef mst(W, copy_W=True):\r\n    """"""Determine the minimum spanning tree for a set of points represented\r\n    by their inter-point distances... ie their \'W\'eights\r\n\r\n    Requires:\r\n    ---------\r\n    W: array\r\n      edge weights (distance, time) for a set of points. W needs to be\r\n      a square array or a np.triu perhaps\r\n\r\n    Returns:\r\n    --------\r\n\r\n    pairs: array\r\n        the pair of nodes that form the edges\r\n    """"""\r\n    if copy_W:\r\n        W = W.copy()\r\n    if W.shape[0] != W.shape[1]:\r\n        raise ValueError(""W needs to be square matrix of edge weights"")\r\n    Np = W.shape[0]\r\n    pairs = []\r\n    pnts_seen = [0]  # Add the first point\r\n    n_seen = 1\r\n    # exclude self connections by assigning inf to the diagonal\r\n    diag = np.arange(Np)\r\n    W[diag, diag] = np.inf\r\n    #\r\n    while n_seen != Np:\r\n        new_edge = np.argmin(W[pnts_seen], axis=None)\r\n        new_edge = divmod(new_edge, Np)\r\n        new_edge = [pnts_seen[new_edge[0]], new_edge[1]]\r\n        pairs.append(new_edge)\r\n        pnts_seen.append(new_edge[1])\r\n        W[pnts_seen, new_edge[1]] = np.inf\r\n        W[new_edge[1], pnts_seen] = np.inf\r\n        n_seen += 1\r\n    return np.vstack(pairs)\r\n\r\n\r\ndef connect(a, dists, edges):\r\n    """"""Return the full spanning tree, with points, connections and distance\r\n\r\n    a: point array\r\n        points to connect\r\n\r\n    dist: array\r\n        distance array, from _e_dist\r\n\r\n    edge: array\r\n        edges, from mst\r\n    """"""\r\n    p_f = edges[:, 0]\r\n    p_t = edges[:, 1]\r\n    d = dists[p_f, p_t]\r\n    n = p_f.shape[0]\r\n    dt = [(\'Orig\', \'<i4\'), (\'Dest\', \'i4\'), (\'Dist\', \'<f8\')]\r\n    out = np.zeros((n,), dtype=dt)\r\n    out[\'Orig\'] = p_f\r\n    out[\'Dest\'] = p_t\r\n    out[\'Dist\'] = d\r\n    return out\r\n\r\n\r\n# ---- main section ----\r\ndef _demo():\r\n    """"""A sample run demonstrating the principles and workflow""""""\r\n#    a = np.array([[0, 0], [0, 8], [10, 8], [10, 0], [3, 4], [7, 4]])\r\n    pth = script.split(""/"")[:-2] + [\'Point_tools.gdb\', \'unsorted_pnts\']\r\n    in_fc = ""/"".join(pth)\r\n    shp_fld, oid_fld, shp_type, SR = fc_info(in_fc)\r\n    a = arcpy.da.FeatureClassToNumPyArray(in_fc,\r\n                                          [\'SHAPE@X\', \'SHAPE@Y\'], """", SR)\r\n    a = a.view(np.dtype(\'float64\')).reshape(a.shape[0], 2)\r\n    idx, a_srt, d = dist_arr(a, prn=False)  # distance array and sorted pnts\r\n    pairs = mst(d)                  # the orig-dest pairs for the mst\r\n    o_d = connect(a_srt, d, pairs)  # produce an o-d structured array\r\n    os = a_srt[pairs[:, 0]]\r\n    ds = a_srt[pairs[:, 1]]\r\n    fr_to = np.array(list(zip(os, ds)))\r\n    return a, o_d, fr_to\r\n\r\n\r\ndef _tool():\r\n    in_fc = sys.argv[1]\r\n    out_fc = sys.argv[2]\r\n    shp_fld, oid_fld, shp_type, SR = fc_info(in_fc)\r\n    out_flds = [oid_fld, shp_fld]\r\n    frmt = """"""\\nScript.... {}\\nUsing..... {}\\nSR...{}\\n""""""\r\n    args = [script, in_fc, SR.name]\r\n    msg = frmt.format(*args)\r\n    tweet(msg)\r\n    a = arcpy.da.FeatureClassToNumPyArray(in_fc, shp_fld, """", SR)\r\n    if len(a) >= 2:\r\n        z = np.zeros((a.shape[0], 2))\r\n        z[:, 0] = a[\'Shape\'][:, 0]\r\n        z[:, 1] = a[\'Shape\'][:, 1]\r\n        idx, a_srt, d = dist_arr(z)\r\n        pairs = mst(d)\r\n        o_d = connect(a_srt, d, pairs)\r\n\r\n        os = a_srt[pairs[:, 0]]\r\n        ds = a_srt[pairs[:, 1]]\r\n\r\n        fr_to = np.array(list(zip(os, ds)))\r\n        s = []\r\n        for pt in fr_to:\r\n            s.append(arcpy.Polyline(arcpy.Array([arcpy.Point(*p) for p in pt]), SR))\r\n\r\n        if arcpy.Exists(out_fc):\r\n            arcpy.Delete_management(out_fc)\r\n        arcpy.CopyFeatures_management(s, out_fc)\r\n    else:\r\n        msg2 = """"""\r\n        |\r\n        ---- Potential User error......\r\n        Technically the script didn\'t fail.... but...\r\n        You need at least 2 different points... make sure you don\'t have an\r\n        incorrect selection\r\n        ---- Try again\r\n        |\r\n        """"""\r\n        tweet(dedent(msg2))\r\n\r\n\r\n# ---- demo section ----\r\n\r\n\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    a, o_d, fr_to = _demo()\r\nelse:\r\n    testing = False\r\n    _tool()\r\n""""""\r\nDissolve_management (in_features, out_feature_class, {dissolve_field},\r\n    {statistics_fields}, {multi_part}, {unsplit_lines})\r\nDissolve_management (in_features, out_feature_class, \'\', \'\',\r\n   \'SINGLE_PART\', \'UNSPLIT_LINES\')\r\n\r\ncollapse from-to data by reshaping it\r\nfr_to.shape\r\nOut[57]: (199, 2, 2)\r\n\r\nfr_to.reshape(199*2, 2)\r\n""""""\r\n# ---------------------------------------------------------------------\r\nif __name__ == ""__main__"":\r\n    """"""Main section...   """"""\r\n#    print(""Script... {}"".format(script))\r\n#    a = np.random.randint(1, 10, size=(10,2))\r\n#    a, d, pairs, o_d = _demo()\r\n'"
PointTools/Scripts/spiral.py,29,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   spiral.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2018-02-28\r\n:Purpose:  tools for working with numpy arrays\r\n:Useage:\r\n:\r\n:References:\r\n:\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\n\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=5, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\nroot5 = 2.23606797749978967\r\ngolden = (1.0 + np.sqrt(5.0))/2  # 1.6180339887498948482..\r\n\r\n\r\n# ---- spiral shape generation -----------------------------------------------\r\n#\r\n# ---- arcpy related ----\r\n#\r\ndef extent_scale(ext_fc, scale_by=1.0):\r\n    """"""Scale up/down the extent defined by an extent featureclass by a\r\n    : factor (1 = 100%.  The scaling is done about the center point.\r\n    """"""\r\n    fc_ext = arcpy.Describe(ext_fc).extent\r\n    ext_w = fc_ext.width\r\n    ext_h = fc_ext.height\r\n    buff_dist = min(ext_w, ext_h) * scale_by\r\n    ext_poly = fc_ext.polygon\r\n    ext_buff = ext_poly.buffer(buff_dist)\r\n    new_ext = ext_buff.extent\r\n    cent = ext_poly.centroid\r\n    return new_ext, cent\r\n\r\n\r\ndef output_polylines(output_shp, SR, pnts):\r\n    """"""Produce the output polygon shapefile""""""\r\n    msg = \'\\nRead the script header... A projected coordinate system required\'\r\n    assert (SR is not None), msg  # and (SR.type == \'Projected\'), msg\r\n    polylines = []\r\n    for pair in pnts:                 # create the polygon geometry\r\n        pl = arcpy.Polyline(arcpy.Array([arcpy.Point(*xy) for xy in pair]), SR)\r\n        polylines.append(pl)\r\n    if arcpy.Exists(output_shp):     # overwrite any existing versions\r\n        arcpy.Delete_management(output_shp)\r\n    arcpy.CopyFeatures_management(polylines, output_shp)\r\n    return output_shp\r\n\r\n\r\ndef shapes(sides=5, radius=1.0):\r\n    """"""coordinates of a pentagram with the y-axis as the bisector and the base\r\n    :  on the x-axis\r\n    :  clockwise - a, b, c, d, e\r\n    The vertices will have coordinates (x+rsin\xce\xb8,y+rcos\xce\xb8)(x+rsin\xe2\x81\xa1\xce\xb8,y+rcos\xe2\x81\xa1\xce\xb8)\r\n    , where \xce\xb8 is an integer multiple of 2\xcf\x80/n or 360/n if you prefer degrees\r\n    to radians.)\r\n    """"""\r\n    rad_360 = np.radians(360.).astype(float)\r\n    step = rad_360/sides\r\n    st_end = np.arange(0.0, rad_360+step, step)\r\n    x = np.sin(st_end) * radius\r\n    y = np.cos(st_end) * radius\r\n    pnts = np.c_[x, y]\r\n    return pnts\r\n\r\n\r\n# ---- spiral examples ------------------------------------------------------\r\n#\r\ndef spiral_archim(N, n, clockwise=True, reverse=False):\r\n    """"""Create an Archimedes spiral in the range 0 to N points with \'n\' steps\r\n    : between each incrementstep.  You could use np.linspace\r\n    :Notes: When n is small relative to N, then you begin to form rectangular\r\n    :  spirals, like rotated rectangles\r\n    :Tried:  N = 1000, n = 30\r\n    """"""\r\n    rnge = np.arange(0.0, N+1.0)\r\n    if clockwise:\r\n        rnge = rnge[::-1]\r\n    phi = rnge/n * np.pi\r\n    xs = phi * np.cos(phi)\r\n    ys = phi * np.sin(phi)\r\n    if reverse:\r\n        tmp = np.copy(xs)\r\n        xs = ys\r\n        ys = tmp\r\n    xy = np.c_[xs, ys]\r\n    wdth, hght = np.ptp(xy, axis=0)\r\n    return xs, ys, xy\r\n\r\n\r\ndef spiral_sqr(ULx=-10, n_max=100):\r\n    """"""Create a square spiral from the centre in a clockwise direction\r\n    : ULx = upper left x coordinate, relative to center (0, 0)\r\n    : n-max = maximum number of iterations should ULx not be reached\r\n    :- see spirangle, Ulam spiral\r\n    """"""\r\n    def W(x, y, c):\r\n        x -= c[0]\r\n        return x, y, c\r\n\r\n    def S(x, y, c):\r\n        y -= c[1]\r\n        return x, y, c\r\n\r\n    def E(x, y, c):\r\n        x += c[2]\r\n        return x, y, c\r\n\r\n    def N(x, y, c):\r\n        y += c[3]\r\n        return x, y, c\r\n\r\n    c = np.array([1, 1, 2, 2])\r\n    pos = [0, 0, c]\r\n    n = 0\r\n    v = [pos]\r\n    cont = True\r\n    while cont:\r\n        p0 = W(*v[-1])\r\n        p1 = S(*p0)\r\n        p2 = E(*p1)\r\n        p3 = N(*p2)\r\n        c = c + 2\r\n        p3 = [p3[0], p3[1], c]\r\n        for i in [p0, p1, p2, p3]:\r\n            v.append(i)\r\n        # --- print(p0, p0[0])  # for testing\r\n        if (p0[0] <= ULx):      # bail option 1\r\n            cont = False\r\n        if n > n_max:           # bail option 2\r\n            cont = False\r\n        n = n+1\r\n    coords = np.asarray([np.array([i[0], i[1]]) for i in v])[:-3]\r\n    return coords\r\n\r\n\r\n# -------Excellent one-------------------------------------------------------\r\n#  https://stackoverflow.com/questions/36834505/\r\n#        creating-a-spiral-array-in-python\r\ndef spiral_cw(A):\r\n    A = np.array(A)\r\n    out = []\r\n    while(A.size):\r\n        out.append(A[0])        # take first row\r\n        A = A[1:].T[::-1]       # cut off first row and rotate counterclockwise\r\n    return np.concatenate(out)\r\n\r\n\r\ndef spiral_ccw(A):\r\n    A = np.array(A)\r\n    out = []\r\n    while(A.size):\r\n        out.append(A[0][::-1])    # first row reversed\r\n        A = A[1:][::-1].T         # cut off first row and rotate clockwise\r\n    return np.concatenate(out)\r\n\r\n\r\ndef base_spiral(nrow, ncol):\r\n    return spiral_ccw(np.arange(nrow*ncol).reshape(nrow, ncol))[::-1]\r\n\r\n\r\ndef to_spiral(A):\r\n    A = np.array(A)\r\n    B = np.empty_like(A)\r\n    B.flat[base_spiral(*A.shape)] = A.flat\r\n    return B\r\n\r\n\r\ndef from_spiral(A):\r\n    A = np.array(A)\r\n    return A.flat[base_spiral(*A.shape)].reshape(A.shape)\r\n# ---- end code section--------------------------------------\r\n\r\n\r\ndef _demo():\r\n    """""" demo to create an archimedes spiral ----\r\n    """"""\r\n    # ---- (1) basic parameters\r\n    scale_by = 1.10  # scale output extent so it is slightly bigger than needed\r\n    pnts_cnt = 1000  # points for spiral\r\n    pnts_div = 40.0  # divisions between points\r\n    pth = r""C:\\GIS\\Geometry_projects\\Spiral_sort\\Polygons\\spiral_sort.gdb""\r\n    ext_poly = r""\\extent_line""\r\n    out_sp = r""\\spiral""\r\n    #\r\n    ext_fc = pth + ext_poly\r\n    out_fc = pth + out_sp\r\n    #\r\n    desc = arcpy.da.Describe(ext_fc)  # get infor from extent poly\r\n    SR = desc[\'spatialReference\']\r\n    #\r\n    # ---- (2) create the spiral\r\n    xs, ys, xy = spiral_archim(pnts_cnt, pnts_div)  # (1) make a spiral\r\n    w, h = np.ptp(xy, axis=0)\r\n    #\r\n    ext, cent = extent_scale(ext_fc, scale_by=scale_by)  # (2) get extent info\r\n    x_c = cent.X  # cent is an arcpy point object\r\n    y_c = cent.Y\r\n    x_fac = ext.width/float(w)\r\n    y_fac = ext.height/float(h)\r\n    #\r\n    # ---- (3) put it all together and create the featureclass\r\n    a = np.array(xy)\r\n    a[:, 0] = a[:, 0] * x_fac + x_c\r\n    a[:, 1] = a[:, 1] * y_fac + y_c\r\n    pnts = a.tolist()\r\n    output_polylines(out_fc, SR, [pnts])\r\n    #\r\n    return a\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n#    _demo()\r\n#    pth = r""C:\\Git_Dan\\a_Data\\arcpytools_demo.gdb""\r\n#    in_fc = pth + r""\\r_extent""'"
PointTools/Scripts/triangulate.py,19,"b'# -*- coding: utf-8 -*-\r\n""""""\r\ntriangulate\r\n===========\r\n\r\nScript :   triangulate.py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified : 2019-02-07\r\n\r\nPurpose:  triangulate poly* features using scipy/qhull functions.\r\n\r\nUseage :\r\n\r\n>>> tri = Voronoi(pnts)\r\n>>> dir(tri)\r\n[\'__class__\', \'__del__\', \'__delattr__\', \'__dict__\', \'__dir__\', \'__doc__\',\r\n \'__eq__\', \'__format__\', \'__ge__\', \'__getattribute__\', \'__gt__\', \'__hash__\',\r\n \'__init__\', \'__init_subclass__\', \'__le__\', \'__lt__\', \'__module__\', \'__ne__\',\r\n \'__new__\', \'__reduce__\', \'__reduce_ex__\', \'__repr__\', \'__setattr__\',\r\n \'__sizeof__\', \'__str__\', \'__subclasshook__\', \'__weakref__\', \'_add_points\',\r\n \'_points\', \'_qhull\', \'_ridge_dict\', \'_update\', \'add_points\', \'close\',\r\n \'max_bound\', \'min_bound\', \'ndim\', \'npoints\', \'point_region\', \'points\',\r\n \'regions\', \'ridge_dict\', \'ridge_points\', \'ridge_vertices\', \'vertices\']\r\n\r\n>>> tri.__class__\r\n<class \'scipy.spatial.qhull.Voronoi\'>\r\n\r\n>>> tri.__dict__.keys()\r\ndict_keys([\'_qhull\', \'vertices\', \'ridge_points\', \'ridge_vertices\', \'regions\',\r\n \'point_region\', \'_ridge_dict\', \'_points\', \'ndim\', \'npoints\', \'min_bound\',\r\n \'max_bound\'])\r\n\r\n>>> tri.min_bound, tri.max_bound\r\n(array([-4217.93, -3832.13]), array([5268.65, 5495.64]))\r\n\r\nNotes\r\n-----\r\nTo get the centroid geometry, you can do the following.\r\n\r\n>>> v = vor_pnts(aa, testing=False)\r\n>>> cents = [[p.centroid.X, p.centroid.Y] for p in polys]\r\n>>> polys = poly([v], SR)\r\n>>> pnts = [arcpy.PointGeometry(Point(i[0], i[1])) for i in cents]\r\n>>> out_fc = \'C:\\\\GIS\\\\A_Tools_scripts\\\\PointTools\\\\voronoi_delaunay.gdb\\\\p6\'\r\n>>> arcpy.FeatureToPoint_management (pnts, out_fc)\r\n\r\nReferences\r\n----------\r\nvoronoi/delaunay links:\r\n\r\n`<http://zderadicka.eu/voronoi-diagrams/>`_.\r\n`<http://scipy.github.io/devdocs/generated/scipy.spatial.Voronoi.html>`_.\r\n`<https://stackoverflow.com/questions/20515554/colorize-voronoi-diagram>`_.\r\n`<https://stackoverflow.com/questions/36063533/clipping-a-voronoi-diagram\r\n-python?noredirect=1&lq=1>`_.\r\n`<https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/\r\nscipy.spatial.Delaunay.html>`_.\r\n---------------------------------------------------------------------\r\n""""""\r\n\r\nimport sys\r\nimport numpy as np\r\nfrom scipy.spatial import Delaunay, Voronoi\r\n#\r\nfrom arcpy import Exists, AddMessage\r\nfrom arcpy.management import (Delete, CopyFeatures, MakeFeatureLayer)\r\nfrom arcpy.analysis import Clip\r\nfrom arcpy.da import Describe, FeatureClassToNumPyArray\r\nfrom arcpy.geoprocessing import env\r\nfrom arcpy.arcobjects import Array, Point\r\nfrom arcpy.arcobjects.geometries import Polygon\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=5, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\nenv.overwriteOutput = True\r\n\r\n# ----\r\n#\r\ndef tweet(msg):\r\n    """"""Print a message for both arcpy and python.\r\n\r\n    msg - a text message\r\n    """"""\r\n    m = ""\\n{}\\n"".format(msg)\r\n    AddMessage(m)\r\n    print(m)\r\n\r\n\r\ndef circle(radius=1.0, theta=10.0, xc=0.0, yc=0.0):\r\n    """"""Produce a circle/ellipse depending on parameters.\r\n\r\n    `radius` : number\r\n        Distance from centre\r\n    `theta` : number\r\n        Angle of densification of the shape around 360 degrees\r\n    """"""\r\n    angles = np.deg2rad(np.arange(180.0, -180.0-theta, step=-theta))\r\n    x_s = radius*np.cos(angles) + xc    # X values\r\n    y_s = radius*np.sin(angles) + yc    # Y values\r\n    pnts = np.c_[x_s, y_s]\r\n    return pnts\r\n\r\n\r\ndef infinity_circle(a, fac=10):\r\n    """"""Create an infinity circle to append to the original point list.\r\n\r\n    Parameters:\r\n    -----------\r\n    a : array\r\n        2D array of x, y point coordinates\r\n    fac : number\r\n        The factor to multiply the largest of the extent width or height for\r\n        the input point array\r\n    """"""\r\n    fac = max(fac, 1)\r\n    L, B = a.min(axis=0)\r\n    R, T = a.max(axis=0)\r\n    xc, yc = np.average(a, axis=0)\r\n    circle_radius = max([R-L, T-B]) * fac #increase radius by a factor of 10\r\n    circPnts = circle(radius=circle_radius, theta=10.0, xc=xc, yc=yc)\r\n    return circPnts\r\n\r\n\r\ndef poly(pnt_groups, SR):\r\n    """"""Short form polygon creation\r\n    """"""\r\n    polygons = []\r\n    for pnts in pnt_groups:\r\n        for pair in pnts:\r\n            arr = Array([Point(*xy) for xy in pair])\r\n            pl = Polygon(arr, SR)\r\n            polygons.append(pl)\r\n    return polygons\r\n\r\n\r\ndef vor_pnts(pnts, testing=False):\r\n    """"""Return the point indices""""""\r\n    out = []\r\n    avg = np.mean(pnts, axis=0)\r\n    p =  pnts - avg\r\n    tri = Voronoi(p)\r\n    for region in tri.regions:\r\n        r = [i for i in region if i != -1]\r\n        if (len(r) > 2):\r\n            poly = np.array([tri.vertices[i] + avg for i in r])\r\n            out.append(poly)\r\n            if testing:\r\n                print(""{}"".format(poly.T))\r\n    return out\r\n\r\n\r\ndef tri_pnts(pnts, testing=False):\r\n    """"""Triangulate the points and return the triangles\r\n\r\n    Parameters:\r\n    -----------\r\n    pnts : np.array\r\n        Points in array format.\r\n    out : array\r\n        an array of triangle points\r\n\r\n    Notes:\r\n    ------\r\n    >>> pnts = pnts.reshape((1,) + pnts.shape)  # a 3D set of points (ndim=3)\r\n    >>> [pnts]  # or pass in as a list\r\n    """"""\r\n    out = []\r\n    ps = np.unique(pnts, axis=0)  # get the unique points only\r\n    avg = np.mean(ps, axis=0)\r\n    p =  ps - avg\r\n    tri = Delaunay(p)\r\n    simps = tri.simplices\r\n    new_pnts = [p[s]+avg for s in simps]\r\n    if testing:\r\n        print(""{}"".format(new_pnts))\r\n    out.append(new_pnts)\r\n    out = np.array(out).squeeze()\r\n    return out\r\n\r\n\r\n# ---- Do the work\r\n#\r\ndef _tri_demo(tri_type=\'Delaunay\'):\r\n    """"""Triangulation demo.\r\n    """"""\r\n    from scipy.spatial import delaunay_plot_2d, voronoi_plot_2d\r\n    import matplotlib.pyplot as plt\r\n    xs =[ 48,   8, 623, 615, 196, 368, 112, 918, 318, 316, 427,\r\n         364, 849, 827, 438, 957, 495, 317, 985, 534]\r\n    ys = [674, 215, 834, 235, 748, 630, 876, 407,  33, 872, 893,\r\n          360, 397, 902, 420, 430, 678, 523, 604, 292]\r\n    aa = np.array(list(zip(xs, ys)))\r\n    c = infinity_circle(aa, fac=0)\r\n    a = np.vstack((aa, c))\r\n    d = v = None  # initialize the output to None\r\n    if tri_type == \'Delaunay\':\r\n        d = Delaunay(aa)\r\n        plot = delaunay_plot_2d(d)\r\n        x0, y0 = [0., 0.]\r\n        x1, y1 = [1000., 1000.]\r\n    else:\r\n        c = infinity_circle(a, fac=2)\r\n#        a = np.vstack((a, c))\r\n        x0, y0 = a.min(axis=0)\r\n        x1, y1 = a.max(axis=0)\r\n        v = Voronoi(a, qhull_options=\'Qbb Qc Qx\')\r\n        plot = voronoi_plot_2d(v, show_vertices=True, line_colors=\'y\',\r\n                               line_alpha=0.8, point_size=5)\r\n    # ----\r\n    plot.set_figheight(10)\r\n    plot.set_figwidth(10)\r\n    plt.axis([0, 1000, 0, 1000])\r\n#    plt.axis([x0, x1, y0, y1])\r\n    plt.show()\r\n    return aa, (d or v)\r\n\r\n# ----\r\n#\r\ndef _tri_tool():\r\n    """"""Triangulation for tool\r\n    """"""\r\n    in_fc = sys.argv[1]\r\n    tri_type = sys.argv[2]\r\n    out_fc = sys.argv[3]\r\n    xtent = sys.argv[4]\r\n    desc = Describe(in_fc)\r\n    SR = desc[\'spatialReference\']\r\n    flds = [\'SHAPE@X\', \'SHAPE@Y\']\r\n    allpnts = False\r\n    z = FeatureClassToNumPyArray(in_fc, flds, """", SR, allpnts)\r\n    a = np.zeros((z.shape[0], 2), dtype=\'<f8\')\r\n    a[:, 0] = z[\'SHAPE@X\']\r\n    a[:, 1] = z[\'SHAPE@Y\']\r\n    #\r\n    if tri_type == \'Delaunay\':\r\n        tweet(""Delaunay... clip extent {}"".format(xtent))\r\n        t = tri_pnts(a, True)  # must be a list of list of points\r\n        polys = poly(t, SR)\r\n        if Exists(out_fc):\r\n            Delete(out_fc)\r\n        CopyFeatures(polys, ""in_memory/temp"")\r\n        MakeFeatureLayer(""in_memory/temp"", ""temp"")\r\n        if xtent not in ("""", None):\r\n            Clip(""temp"", xtent, out_fc, None)\r\n        else:\r\n            CopyFeatures(""temp"", out_fc)\r\n    else:\r\n        tweet(""Voronoi... clip extent {}"".format(xtent))\r\n        c = infinity_circle(a, fac=10)\r\n        aa = np.vstack((a, c))\r\n        v = vor_pnts(aa, testing=False)\r\n        polys = poly([v], SR)\r\n        if Exists(out_fc):\r\n            Delete(out_fc)\r\n        CopyFeatures(polys, ""in_memory/temp"")\r\n        MakeFeatureLayer(""in_memory/temp"", ""temp"")\r\n        if xtent not in ("""", None, ):\r\n            Clip(""temp"", xtent, out_fc, None)\r\n        else:\r\n            CopyFeatures(""temp"", out_fc)\r\n\r\n# ---- main section .... calls demo or the tool\r\n#\r\n# uncomment the t = _tri... line below to graph\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    a, t = _tri_demo(\'Delaunay\')  # \'Delaunay\' \'Voronoi\'\r\nelse:\r\n    testing = False\r\n    _tri_tool()\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n'"
TableTools/Scripts/angles_.py,13,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nangles_\r\n=======\r\n\r\nRequires:\r\n---------\r\n\r\nThis function is called by poly_field_calcs.py from the Table Tools toolset\r\nIt is designed to be imported and used in the field calculator\r\n\r\nThere is a check for redundant vertices which affects the sum and max angles.\r\nSpecifically, 3 points on a 2 point line will have 180 degree max\r\n\r\nUseage:\r\n------\r\nIn the calling script use\r\n\r\n>>> from angles import angles_poly\r\n\r\nIn the field calculator calling function, feed it the shape field\r\n    angles_poly(!Shape!)\r\n""""""\r\n\r\ndef angles_poly(a, inside=True, in_deg=True, kind=""sum""):\r\n    """"""Sequential angles from a poly* shape\r\n\tNOTE: this line.... angle = np.sum(angles)\r\n\t      can be changed to `np.min`, `np.max` or others\r\n\t\t  depending on what needs to be returned\r\n    """"""\r\n    import numpy as np\r\n    a = a.getPart()\r\n    a =np.asarray([[i.X, i.Y] for j in a for i in j])\r\n    if len(a) < 2:\r\n        return None\r\n    elif len(a) == 2:  # **** check\r\n        ba = a[1] - a[0]\r\n        return np.arctan2(*ba[::-1])\r\n    else:\r\n        angles = []\r\n        if np.allclose(a[0], a[-1]):  # closed loop\r\n            a = a[:-1]\r\n            r = (-1,) + tuple(range(len(a))) + (0,)\r\n        else:\r\n            r = tuple(range(len(a)))\r\n        for i in range(len(r)-2):\r\n            p0, p1, p2 = a[r[i]], a[r[i+1]], a[r[i+2]]\r\n            ba = p1 - p0\r\n            bc = p1 - p2\r\n            cr = np.cross(ba, bc)\r\n            dt = np.dot(ba, bc)\r\n            ang = np.arctan2(np.linalg.norm(cr), dt)\r\n            if not np.allclose(ang, np.pi):  # check for extra vertices\r\n                angles.append(ang)\r\n    if in_deg:\r\n        angles = np.degrees(angles)\r\n    if kind == ""sum"":\r\n        angle = np.sum(angles)\r\n    elif kind == ""min"":\r\n        angle = np.min(angles)\r\n    elif kind == ""max"":\r\n        angle = np.max(angles)\r\n    return angle\r\n#__esri_field_calculator_splitter__\r\n#angles_poly(!Shape!)'"
TableTools/Scripts/arcpytools.py,9,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   arcpytools.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2018-06-21\r\n:Purpose:  tools for working with numpy arrays\r\n:Useage:\r\n:\r\n:References:\r\n:\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nfrom textwrap import dedent\r\nimport numpy as np\r\nimport arcpy\r\n# from arcpytools import array_fc, array_struct, tweet\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\n\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\n__all__ = [\'_col_format\', \'_describe\', \'_xyID\',\r\n           \'arr2line\', \'arr2pnts\', \'arr2polys\',\r\n           \'array_fc\', \'array_struct\', \'fc_array\',\r\n           \'fc_info\', \'frmt_rec\',\r\n           \'output_polygons\', \'output_polylines\',\r\n           \'pd_\', \'shapes2fc\', \'tweet\']\r\n\r\n\r\ndef tweet(msg):\r\n    """"""Produce a message for both arcpy and python\r\n    : msg - a text message\r\n    """"""\r\n    m = ""{}"".format(msg)\r\n    arcpy.AddMessage(m)\r\n    print(m)\r\n\r\n\r\ndef _describe(in_fc=None):\r\n    """"""Simply return the arcpy.da.Describe object\r\n    : desc.keys() an abbreviated list...\r\n    : [... \'OIDFieldName\'... \'areaFieldName\', \'baseName\'... \'catalogPath\',\r\n    :  ... \'dataType\'... \'extent\', \'featureType\', \'fields\', \'file\'... \'hasM\',\r\n    :  \'hasOID\', \'hasZ\', \'indexes\'... \'lengthFieldName\'... \'name\', \'path\',\r\n    :  \'rasterFieldName\', ..., \'shapeFieldName\', \'shapeType\',\r\n    :  \'spatialReference\',  ...]\r\n    """"""\r\n    if in_fc is not None:\r\n        return arcpy.da.Describe(in_fc)\r\n    else:\r\n        return None\r\n\r\ndef fc_info(in_fc, prn=False):\r\n    """"""Return basic featureclass information, including...\r\n\r\n    Parameters:\r\n    -----------\r\n    - shp_fld  :\r\n        field name which contains the geometry object\r\n    - oid_fld  :\r\n        the object index/id field name\r\n    - SR       :\r\n        spatial reference object (use SR.name to get the name)\r\n    - shp_type :\r\n        shape type (Point, Polyline, Polygon, Multipoint, Multipatch)\r\n    - others   :\r\n        \'areaFieldName\', \'baseName\', \'catalogPath\',\'featureType\',\'fields\',\r\n        \'hasOID\', \'hasM\', \'hasZ\', \'path\'\r\n\r\n\r\n     - all_flds :\r\n         [i.name for i in desc[\'fields\']]\r\n    """"""\r\n    desc = _describe(in_fc)\r\n    args = [\'shapeFieldName\', \'OIDFieldName\', \'shapeType\', \'spatialReference\']\r\n    shp_fld, oid_fld, shp_type, SR = [desc[i] for i in args]\r\n    if prn:\r\n        frmt = ""FeatureClass:\\n   {}"".format(in_fc)\r\n        f = ""\\n{!s:<16}{!s:<14}{!s:<10}{!s:<10}""\r\n        frmt += f.format(*args)\r\n        frmt += f.format(shp_fld, oid_fld, shp_type, SR.name)\r\n        tweet(frmt)\r\n    else:\r\n        return shp_fld, oid_fld, shp_type, SR\r\n\r\n\r\n# ---- geometry related -----------------------------------------------------\r\n#\r\ndef _xyID(in_fc, to_pnts=True):\r\n    """"""Convert featureclass geometry (in_fc) to a simple 2D structured array\r\n    :  with ID, X, Y values. Optionally convert to points, otherwise centroid.\r\n    """"""\r\n    flds = [\'OID@\', \'SHAPE@X\', \'SHAPE@Y\']\r\n    args = [in_fc, flds, None, None, to_pnts, (None, None)]\r\n    cur = arcpy.da.SearchCursor(*args)\r\n    a = cur._as_narray()\r\n    a.dtype = [(\'IDs\', \'<i4\'), (\'Xs\', \'<f8\'), (\'Ys\', \'<f8\')]\r\n    del cur\r\n    return a\r\n\r\n\r\ndef array_struct(a, fld_names=[\'X\', \'Y\'], dt=[\'<f8\', \'<f8\']):\r\n    """"""Convert an array to a structured array\r\n\r\n    Parameters:\r\n    -----------\r\n    - a : an ndarray with shape at least (N, 2)\r\n    -  dt : dtype class\r\n    -  names : names for the fields\r\n    """"""\r\n    dts = [(fld_names[i], dt[i]) for i in range(len(fld_names))]\r\n    z = np.zeros((a.shape[0],), dtype=dts)\r\n    names = z.dtype.names\r\n    for i in range(a.shape[1]):\r\n        z[names[i]] = a[:, i]\r\n    return z\r\n\r\n\r\ndef array_fc(a, out_fc, fld_names, SR):\r\n    """"""Array to featureclass/shapefile...optionally including all fields\r\n\r\n    Parameters:\r\n    -----------\r\n    - out_fc :  featureclass/shapefile... complete path\r\n    - fld_names : the Shapefield name ie [\'Shape\'] or [\'X\', \'Y\'s]\r\n    - SR : spatial reference of the output\r\n\r\n    See also :\r\n        NumpyArrayToFeatureClass, ListFields for information and options\r\n    """"""\r\n    if arcpy.Exists(out_fc):\r\n        arcpy.Delete_management(out_fc)\r\n    arcpy.da.NumPyArrayToFeatureClass(a, out_fc, fld_names, SR)\r\n    return out_fc\r\n\r\n\r\ndef fc_array(in_fc, flds, allpnts):\r\n    """"""Convert a featureclass to an ndarray...with optional fields besides the\r\n    FID/OIDName and Shape fields.\r\n\r\n    Parameters:\r\n    -----------\r\n    in_fc : text\r\n        Full path to the geodatabase and the featureclass name\r\n\r\n    flds : text or list\r\n        - ``\'\'   : just an object id and shape field``\r\n        - ``\'*\'  : all fields in the featureclass or``\r\n        - ``list : specific fields [\'OBJECTID\',\'Shape\',\'SomeClass\', etc]``\r\n\r\n    allpnts : boolean\r\n        - True `explodes` geometry to individual points.\r\n        - False returns the centroid\r\n\r\n    Requires:\r\n    ---------\r\n        fc_info(in_fc) function\r\n\r\n    See also:\r\n    ---------\r\n        FeatureClassToNumPyArray, ListFields for more information in current\r\n        arcpy documentation\r\n    """"""\r\n    out_flds = []\r\n    shp_fld, oid_fld, shp_type, SR = fc_info(in_fc)  # get the base information\r\n    fields = arcpy.ListFields(in_fc)      # all fields in the shapefile\r\n    if flds == """":                        # return just OID and Shape field\r\n        out_flds = [oid_fld, shp_fld]     # FID and Shape field required\r\n    elif flds == ""*"":                     # all fields\r\n        out_flds = [f.name for f in fields]\r\n    else:\r\n        out_flds = [oid_fld, shp_fld]\r\n        for f in fields:\r\n            if f.name in flds:\r\n                out_flds.append(f.name)\r\n    frmt = """"""\\nRunning \'fc_array\' with ....\r\n    \\nfeatureclass... {}\\nFields... {}\\nAll pnts... {}\\nSR... {}\r\n    """"""\r\n    args = [in_fc, out_flds, allpnts, SR.name]\r\n    msg = dedent(frmt).format(*args)\r\n    tweet(msg)\r\n    a = arcpy.da.FeatureClassToNumPyArray(in_fc, out_flds, """", SR, allpnts)\r\n    # out it goes in array format\r\n    return a, out_flds, SR\r\n\r\n\r\ndef arr2pnts(in_fc, as_struct=True, shp_fld=None, SR=None):\r\n    """"""Create points from an array.\r\n    :  in_fc - input featureclass\r\n    :  as_struct - if True, returns a structured array with X, Y fields,\r\n    :            - if False, returns an ndarray with dtype=\'<f8\'\r\n    :Notes: calls fc_info to return featureclass information\r\n    """"""\r\n    if shp_fld is None or SR is None:\r\n        shp_fld, oid_fld, SR = fc_info(in_fc)\r\n    a = arcpy.da.FeatureClassToNumPyArray(in_fc, ""*"", """", SR)\r\n    dt = [(\'X\', \'<f8\'), (\'Y\', \'<f8\')]\r\n    if as_struct:\r\n        shps = np.array([tuple(i) for i in a[shp_fld]], dtype=dt)\r\n    else:\r\n        shps = a[shp_fld]\r\n    return shps, shp_fld, SR\r\n\r\n\r\ndef arr2line(a, out_fc, SR=None):\r\n    """"""create lines from an array""""""\r\n    pass\r\n\r\n\r\ndef shapes2fc(shps, out_fc):\r\n    """"""Create a featureclass/shapefile from poly* shapes.\r\n    :  out_fc - full path and name to the output container (gdb or folder)\r\n    """"""\r\n    msg = ""\\nCan\'t overwrite the {}... rename"".format(out_fc)\r\n    try:\r\n        if arcpy.Exists(out_fc):\r\n            arcpy.Delete_management(out_fc)\r\n        arcpy.CopyFeatures_management(shps, out_fc)\r\n    except ValueError:\r\n        tweet(msg)\r\n\r\n\r\ndef arr2polys(a, out_fc, oid_fld, SR):\r\n    """"""Make poly* features from a structured array.\r\n    :  a - structured array\r\n    :  out_fc: a featureclass path and name, or None\r\n    :  oid_fld - object id field, used to partition the shapes into groups\r\n    :  SR - spatial reference object, or name\r\n    :Returns:\r\n    :-------\r\n    :  Produces the featureclass optionally, but returns the polygons anyway.\r\n    """"""\r\n    arcpy.overwriteOutput = True\r\n    pts = []\r\n    keys = np.unique(a[oid_fld])\r\n    for k in keys:\r\n        w = np.where(a[oid_fld] == k)[0]\r\n        v = a[\'Shape\'][w[0]:w[-1] + 1]\r\n        pts.append(v)\r\n    # Create a Polygon from an Array of Points, save to featueclass if needed\r\n    s = []\r\n    for pt in pts:\r\n        s.append(arcpy.Polygon(arcpy.Array([arcpy.Point(*p) for p in pt]), SR))\r\n    return s\r\n\r\n\r\ndef output_polylines(out_fc, SR, pnt_groups):\r\n    """"""Produce the output polygon featureclass.\r\n    :Requires:\r\n    :--------\r\n    : - A list of lists of points\r\n    :   aline = [[[0, 0], [1, 1]]]  # a list of points\r\n    :   aPolyline = [[aline]]       # a list of lists of points\r\n    """"""\r\n    msg = \'\\nRead the script header... A projected coordinate system required\'\r\n    assert (SR is not None), msg\r\n    polylines = []\r\n    for pnts in pnt_groups:\r\n        for pair in pnts:\r\n            arr = arcpy.Array([arcpy.Point(*xy) for xy in pair])\r\n            pl = arcpy.Polyline(arr, SR)\r\n            polylines.append(pl)\r\n    if arcpy.Exists(out_fc):     # overwrite any existing versions\r\n        arcpy.Delete_management(out_fc)\r\n    arcpy.CopyFeatures_management(polylines, out_fc)\r\n    return\r\n\r\n\r\ndef output_polygons(out_fc, SR, pnt_groups):\r\n    """"""Produce the output polygon featureclass.\r\n\r\n    Parameters:\r\n    -----------\r\n    out_fc : string\r\n        The path and name of the featureclass to be created.\r\n    SR : spatial reference of the output featureclass\r\n    pnts_groups :\r\n        The point groups, list of lists of points, to include parts rings.\r\n\r\n    Requires:\r\n    --------\r\n\r\n    - A list of lists of points.  Four points form a triangle is the minimum\r\n    -  aline = [[0, 0], [1, 1]]  # a list of points\r\n    -  aPolygon = [aline]        # a list of lists of points\r\n    """"""\r\n    msg = \'\\nRead the script header... A projected coordinate system required\'\r\n    assert (SR is not None), msg\r\n    polygons = []\r\n    for pnts in pnt_groups:\r\n        for pair in pnts:\r\n            arr = arcpy.Array([arcpy.Point(*xy) for xy in pair])\r\n            pl = arcpy.Polygon(arr, SR)\r\n            polygons.append(pl)\r\n    if arcpy.Exists(out_fc):     # overwrite any existing versions\r\n        arcpy.Delete_management(out_fc)\r\n    arcpy.CopyFeatures_management(polygons, out_fc)\r\n    return\r\n\r\n# ---- formatting, from arraytools ------------------------------------------\r\n#\r\n# ----------------------------------------------------------------------\r\n# (4) frmt_rec .... code section\r\n#  frmt_rec requires _col_format\r\ndef _col_format(a, c_name=""c00"", deci=0):\r\n    """"""Determine column format given a desired number of decimal places.\r\n    Used by frmt_struct.\r\n\r\n    `a` : column\r\n        A column in an array.\r\n    `c_name` : text\r\n        column name\r\n    `deci` : int\r\n        Desired number of decimal points if the data are numeric\r\n\r\n    Notes:\r\n    -----\r\n        The field is examined to determine whether it is a simple integer, a\r\n        float type or a list, array or string.  The maximum width is determined\r\n        based on this type.\r\n\r\n        Checks were also added for (N,) shaped structured arrays being\r\n        reformatted to (N, 1) shape which sometimes occurs to facilitate array\r\n        viewing.  A kludge at best, but it works for now.\r\n    """"""\r\n    a_kind = a.dtype.kind\r\n    if a_kind in (\'i\', \'u\'):  # ---- integer type\r\n        w_, m_ = [\':> {}.0f\', \'{:> 0.0f}\']\r\n        col_wdth = len(m_.format(a.max())) + 1\r\n        col_wdth = max(len(c_name), col_wdth) + 1  # + deci\r\n        c_fmt = w_.format(col_wdth, 0)\r\n    elif a_kind == \'f\' and np.isscalar(a[0]):  # ---- float type with rounding\r\n        w_, m_ = [\':> {}.{}f\', \'{:> 0.{}f}\']\r\n        a_max, a_min = np.round(np.sort(a[[0, -1]]), deci)\r\n        col_wdth = max(len(m_.format(a_max, deci)),\r\n                       len(m_.format(a_min, deci))) + 1\r\n        col_wdth = max(len(c_name), col_wdth) + 1\r\n        c_fmt = w_.format(col_wdth, deci)\r\n    # ---- lists, arrays, strings. Check for (N,) vs (N,1)\r\n    # I made some changes in how col_wdth is determined, old is commented\r\n    else:\r\n        if a.ndim == 1:  # ---- check for (N, 1) format of structured array\r\n            a = a[0]\r\n        dt = a.dtype.descr[0][1]\r\n        col_wdth = int("""".join([i for i in dt if i.isdigit()]))\r\n#       col_wdth = max([len(str(i)) for i in a])\r\n        col_wdth = max(len(c_name), col_wdth) + 1  # + deci\r\n        c_fmt = ""!s:>"" + ""{}"".format(col_wdth)\r\n    return c_fmt, col_wdth\r\n\r\n\r\ndef pd_(a, deci=2, use_names=True, prn=True):\r\n    """"""see help for `frmt_rec`...""""""\r\n    ret = frmt_rec(a, deci=deci, use_names=use_names, prn=prn)\r\n    return ret\r\n\r\n\r\ndef frmt_rec(a, deci=2, use_names=True, prn=True):\r\n    """"""Format a structured array with a mixed dtype.\r\n\r\n    NOTE : Can be called as `pd_(a, ... )` to emulate pandas dataframes\r\n        You should limit large arrays to a slice ie. a[:50]\r\n\r\n    Requires:\r\n    -------\r\n    `a` : array\r\n        A structured/recarray\r\n    `deci` : int\r\n        To facilitate printing, this value is the number of decimal\r\n        points to use for all floating point fields.\r\n    `use_names` : boolean\r\n        If no names are available, then create them\r\n    `prn` : boolean\r\n        True to print, False to return the string\r\n    Notes:\r\n    -----\r\n        `_col_format` : does the actual work of obtaining a representation of\r\n        the column format.\r\n\r\n        It is not really possible to deconstruct the exact number of decimals\r\n        to use for float values, so a decision had to be made to simplify.\r\n    """"""\r\n    dt_names = a.dtype.names\r\n    N = len(dt_names)\r\n    c_names = [[""C{:02.0f}"".format(i) for i in range(N)], dt_names][use_names]\r\n    # ---- get the column formats from ... _col_format ----\r\n    dts = []\r\n    wdths = []\r\n    pair = list(zip(dt_names, c_names))\r\n    for i in range(len(pair)):\r\n        fld, nme = pair[i]\r\n        c_fmt, col_wdth = _col_format(a[fld], c_name=nme, deci=deci)\r\n        dts.append(c_fmt)\r\n        wdths.append(col_wdth)\r\n    row_frmt = "" "".join([(\'{\' + i + \'}\') for i in dts])\r\n    hdr = [""!s:>"" + ""{}"".format(wdths[i]) for i in range(N)]\r\n    hdr2 = "" "".join([""{"" + hdr[i] + ""}"" for i in range(N)])\r\n    header = ""--n--"" + hdr2.format(*c_names)\r\n    header = ""\\n{}\\n{}"".format(header, ""-""*len(header))\r\n    txt = [header]\r\n    # ---- check for structured arrays reshaped to (N, 1) instead of (N,) ----\r\n    len_shp = len(a.shape)\r\n    idx = 0\r\n    for i in range(a.shape[0]):\r\n        if len_shp == 1:  # ---- conventional (N,) shaped array\r\n            row = "" {:03.0f} "".format(idx) + row_frmt.format(*a[i])\r\n        else:             # ---- reformatted to (N, 1)\r\n            row = "" {:03.0f} "".format(idx) + row_frmt.format(*a[i][0])\r\n        idx += 1\r\n        txt.append(row)\r\n    msg = ""\\n"".join([i for i in txt])\r\n    if prn:\r\n        print(msg)\r\n    else:\r\n        return msg\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n#    _demo()\r\n#    gdb_fc = [\'Data\', \'point_tools.gdb\', \'radial_pnts\']\r\n#    in_fc = ""/"".join(script.split(""/"")[:-2] + gdb_fc)\r\n#    result = fc_array(in_fc, flds="""", allpnts=True)  # a, out_flds, SR\r\n'"
TableTools/Scripts/concatenate_flds.py,18,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\nconcatenate_flds\r\n================\r\n\r\nScript :   concatenate_flds.py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified : 2018-05-26\r\n\r\nPurpose :  Concatenate fields from fields in a geodatabase table.\r\n\r\nReferences:\r\n\r\n---------------------------------------------------------------------\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=3, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\ndef tweet(msg):\r\n    """"""Print a message for both arcpy and python.\r\n    : msg - a text message\r\n    """"""\r\n    m = ""\\n{}\\n"".format(msg)\r\n    arcpy.AddMessage(m)\r\n    print(m)\r\n    print(arcpy.GetMessages())\r\n\r\n\r\ndef _cleanup(arrs, strip_list):\r\n    """"""clean the arrays if needed""""""\r\n    cleaned = []\r\n    for ar in arrs:\r\n        if ar.dtype.kind in (\'f\', \'i\'):\r\n            tmp = ar.astype(np.unicode_)\r\n        else:\r\n            tmp = ar\r\n        for i in strip_list:\r\n            tmp = np.char.strip(tmp)\r\n            tmp = np.char.replace(tmp, str(i), """")\r\n        cleaned.append(tmp)\r\n    return cleaned\r\n\r\n\r\ndef concat_flds(arrs, sep=\'space\', name=None, strip_list=None, with_ids=True):\r\n    """"""Concatenate a sequence of arrays to string format and return a\r\n    structured array or ndarray\r\n\r\n    - arrs : a list of single arrays of the same length\r\n    - sep  : the separator to separate the arrays\r\n    - name : used for structured array\r\n    """"""\r\n    def cleanup(arrs, strip_list):\r\n        """"""clean the arrays if needed""""""\r\n        cleaned = []\r\n        for ar in arrs:\r\n            if ar.dtype.kind in (\'f\', \'i\'):\r\n                tmp = ar.astype(np.unicode_)\r\n            else:\r\n                tmp = ar\r\n            for i in strip_list:\r\n                tmp = np.char.replace(tmp, str(i), """")\r\n                tmp = np.char.strip(tmp)\r\n            cleaned.append(tmp)\r\n        return cleaned\r\n    # ---- Main section\r\n    N = len(arrs)\r\n    if sep == \'space\':\r\n        sep = \' \'\r\n    elif sep == \'comma\':\r\n        sep = \', \'\r\n    elif sep == \'none\':\r\n        sep = \'\'\r\n    if N < 2:\r\n        return arrs\r\n    if strip_list is None:\r\n        cleaned = arrs\r\n    else:\r\n        cleaned = cleanup(arrs, strip_list)\r\n    a, b = cleaned[0], cleaned[1]\r\n    c = [""{}{}{}"".format(i, sep, j) for i, j in list(zip(a, b))]\r\n    if N > 2:\r\n        for i in range(2, len(cleaned)):\r\n            c = [""{}{}{}"".format(i, sep, j)\r\n                 for i, j in list(zip(c, cleaned[i]))]\r\n    c = np.asarray(c)\r\n    sze = c.dtype.str\r\n    if name is not None:\r\n        c.dtype = [(name, sze)]\r\n    else:\r\n        name = \'concat\'\r\n        c.dtype = [(name, sze)]\r\n    if with_ids:\r\n        tmp = np.copy(c)\r\n        dt = [(\'IDs\', \'<i8\'), (name, sze)]\r\n        c = np.empty((tmp.shape[0], ), dtype=dt)\r\n        c[\'IDs\'] = np.arange(1, tmp.shape[0] + 1)\r\n        c[name] = tmp\r\n    return c\r\n\r\n\r\n# ---- Run options: _demo or from _tool\r\n#\r\ndef _demo():\r\n    """"""Code to run if in demo mode\r\n    """"""\r\n    # in_tbl = r""C:\\Git_Dan\\arraytools\\Data\\numpy_demos.gdb\\sample_10k""\r\n    in_tbl = r""C:\\GIS\\Joe_address\\Joe_address\\Joe_address.gdb\\Addr_summary""\r\n    in_flds = [\'Street\', \'Len_range\', \'Test_txt\', \'Len_range2\']\r\n    nv = np.iinfo(np.int32).min  # use smallest int...it gets cast as needed\r\n    a = arcpy.da.TableToNumPyArray(in_tbl, in_flds,\r\n                                   skip_nulls=False,\r\n                                   null_value=nv)\r\n    a0 = a[\'Street\']\r\n    a1 = a[\'Len_range\']\r\n    a2 = a[\'Test_txt\']\r\n    a3 = a[\'Len_range2\']\r\n    arrs = [a0, a1, a2, a3]\r\n#    a = [np.arange(5, dtype=\'int\'),\r\n#            np.arange(10, 5, -1, dtype=\'float\'),\r\n#            np.array([\'a\', \'b\', \'c\', \'d\', \'e\'])]\r\n    strip_list = [nv, \'None\', None, """", "",""]\r\n    c = concat_flds(arrs, sep="" "", name=""Test"",\r\n                    strip_list=strip_list, with_ids=True)\r\n#    arcpy.da.ExtendTable(in_tbl, \'OBJECTID\', out_array, \'IDs\')\r\n    return arrs, c\r\n\r\n\r\ndef _tool():\r\n    """"""run when script is from a tool\r\n    """"""\r\n    in_tbl = sys.argv[1]\r\n    in_flds = sys.argv[2]\r\n    fld_sep = ""{}"".format(sys.argv[3])\r\n    strip_list = sys.argv[4]\r\n    out_fld = sys.argv[5]\r\n\r\n    if \';\' in in_flds:\r\n        in_flds = in_flds.split(\';\')\r\n    else:\r\n        in_flds = [in_flds]\r\n\r\n    desc = arcpy.da.Describe(in_tbl)\r\n    tbl_path = desc[\'path\']\r\n    fnames = [i.name for i in arcpy.ListFields(in_tbl)]\r\n    if out_fld in fnames:\r\n        out_fld += \'dup\'\r\n    out_fld = arcpy.ValidateFieldName(out_fld, tbl_path)\r\n    args = [in_tbl, in_flds, out_fld, tbl_path]\r\n    msg = ""in_tbl {}\\nin_fld {}\\nout_fld  {}\\ntbl_path  {}"".format(*args)\r\n    tweet(msg)\r\n    oid = \'OBJECTID\'\r\n    vals = [oid] + in_flds\r\n    nv = np.iinfo(np.int32).min  # use smallest int...it gets cast as needed\r\n    arr = arcpy.da.TableToNumPyArray(in_tbl, vals,\r\n                                     skip_nulls=False,\r\n                                     null_value=nv)\r\n    tweet(""{!r:}"".format(arr))\r\n    #\r\n    # ---- process arrays from the fields, concatenate, and ExtendTable ----\r\n    arrs = [arr[i] for i in in_flds]\r\n    out_array = concat_flds(arrs, sep=fld_sep, name=out_fld,\r\n                            strip_list=strip_list, with_ids=True)\r\n    arcpy.da.ExtendTable(in_tbl, \'OBJECTID\', out_array, \'IDs\')\r\n    del in_tbl, arr, out_array\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# .... final code section producing the featureclass and extendtable\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    arrs, c = _demo()\r\n    frmt = ""Result...\\n{}""\r\n    print(frmt.format(c))\r\nelse:\r\n    testing = False\r\n    _tool()\r\n#\r\nif not testing:\r\n    print(\'Concatenation done...\')\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n'"
TableTools/Scripts/cross_tab.py,18,"b' # -*- coding: UTF-8 -*-\r\n""""""\r\n=========\r\ncross_tab\r\n=========\r\n\r\nScript : cross_tab.py\r\n\r\nAuthor: Dan_Patterson@carleton.ca\r\n\r\nModified : 2019-02-23\r\n\r\nPurpose : Crosstabulate data\r\n\r\nReferences\r\n----------\r\n\r\n`<https://stackoverflow.com/questions/12983067/how-to-find-unique-vectors-of\r\n-a-2d-array-over-a-particular-axis-in-a-vectorized>`_.\r\n\r\n`<https://stackoverflow.com/questions/16970982/find-unique-rows-in-numpy\r\n-array>`_.\r\n\r\n`<http://stackoverflow.com/questions/38030054/create-adjacency-matrix-in-\r\npython-for-large-dataset>`_.\r\n""""""\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\nfrom textwrap import indent\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\r\n      \'float\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=50, linewidth=80, precision=2,\r\n                    suppress=True, threshold=100,\r\n                    formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')\r\n\r\nscript = sys.argv[0]\r\n\r\n\r\ndef tweet(msg):\r\n    """"""Print a message for both arcpy and python.\r\n    : msg - a text message\r\n    """"""\r\n    m = ""\\n{}\\n"".format(msg)\r\n    arcpy.AddMessage(m)\r\n    print(m)\r\n    print(arcpy.GetMessages())\r\n\r\n\r\ndef _prn(r, c, a):\r\n    """"""fancy print formatting.\r\n    """""" \r\n    r = r.tolist()\r\n    r.append(\'Total\')\r\n    c = c.tolist()\r\n    c.append(\'Total\')\r\n    r_sze = max(max([len(str(i)) for i in r]), 8)\r\n    c_sze = [max(len(str(i)), 5) for i in c]\r\n    f_0 = \'{{!s:<{}}} \'.format(r_sze)\r\n    f_1 = (\'{{!s:>{}}} \'*len(c)).format(*c_sze)\r\n    frmt = f_0 + f_1\r\n    hdr = \'Result\' + \'_\'*(r_sze-7)\r\n    txt = [frmt.format(hdr, *c)]\r\n    txt2 = txt + [frmt.format(r[i], *a[i]) for i in range(len(r))]\r\n    result = ""\\n"".join(txt2)\r\n    return result\r\n\r\n\r\ndef crosstab(row, col, verbose=False):\r\n    """"""Crosstabulate 2 data arrays, shape (N,), using np.unique.\r\n    scipy.sparse has similar functionality and is faster for large arrays.\r\n\r\n    Parameters\r\n    ----------\r\n    a : array\r\n        A 2D array of data with shape(N,) representing two variables\r\n    row : text\r\n        row variable\r\n    col : text\r\n        column variable\r\n\r\n    Returns\r\n    -------\r\n    ctab : the crosstabulation result as row, col, count array\r\n    a : the crosstabulation in a row, col, count, but filled out whether a\r\n        particular combination exists or not.\r\n    r, c : unique values/names for the row and column variables\r\n    """"""\r\n    dt = np.dtype([(\'row\', row.dtype), (\'col\', col.dtype)])\r\n    rc = np.asarray(list(zip(row, col)), dtype=dt)\r\n    r = np.unique(row)\r\n    c = np.unique(col)\r\n    u, idx, inv, cnt = np.unique(rc, return_index=True, return_inverse=True,\r\n                                 return_counts=True)\r\n    rcc_dt = u.dtype.descr\r\n    rcc_dt.append((\'Count\', \'<i4\'))\r\n    ctab = np.asarray(list(zip(u[\'row\'], u[\'col\'], cnt)), dtype=rcc_dt)\r\n    a = np.zeros((len(r)+1, len(c)+1), dtype=np.int_)\r\n    rc = [[(np.where(r == i[0])[0]).item(),\r\n           (np.where(c == i[1])[0]).item()] for i in ctab]\r\n    for i in range(len(ctab)):\r\n        rr, cc = rc[i]\r\n        a[rr, cc] = ctab[i][2]\r\n    a[-1,:] = np.sum(a, axis=0)\r\n    a[:, -1] = np.sum(a, axis=1)\r\n    result = _prn(r, c, a)\r\n    if verbose:\r\n        tweet(result)\r\n    return ctab, a, result, r, c\r\n\r\n\r\ndef tbl_2_np_array(in_tbl, flds, skip_nulls=False, null_value=None):\r\n    """"""Form the TableToNumPyArray to account for nulls for various dtypes\r\n\r\n    """"""\r\n    int_min = np.iinfo(np.int32).min\r\n    float_min = np.finfo(np.float64).min\r\n    str_val = ""None""\r\n    nulls = {\'Double\':float_min, \'Integer\':int_min, \'String\':str_val}\r\n    #\r\n    fld_dict = {i.name: i.type for i in arcpy.ListFields(in_tbl)}\r\n    null_dict = {f:nulls[fld_dict[f]] for f in flds}\r\n    t = arcpy.da.TableToNumPyArray(in_table=in_tbl, field_names=flds,\r\n                                   skip_nulls=False,\r\n                                   null_value=null_dict)\r\n    return t\r\n\r\n\r\ndef _demo():\r\n    """"""run a test using 2K of normally distributed points\r\n    : TableToNumPyArray(in_table, field_names, {where_clause},\r\n    :                   {skip_nulls}, {null_value})\r\n    """"""\r\n    # Load the table, with 2 fields, produce the table and the crosstabulation\r\n    f = ""/"".join(script.split(""/"")[:-2]) + \'/Table_tools.gdb/pnts_2K_normal\'\r\n    flds = [\'Text01\', \'Sequences\']\r\n    t = tbl_2_np_array(in_tbl=f, flds=flds)\r\n    #\r\n    row = t[flds[0]]\r\n    col = t[flds[1]]\r\n    ctab, a, result, r, c = crosstab(row, col, verbose=True)\r\n    return ctab, a, result, r, c\r\n\r\n\r\ndef _demo2():\r\n    """"""load a npy file\r\n\r\n    - /Data/sample_20.npy\r\n    - /Datasample_1000.npy\r\n    - /Data/sample_10K.npy\r\n    - /Data/sample_100K.npy\r\n\r\n    dtype=[(\'OBJECTID\', \'<i4\'), (\'f0\', \'<i4\'), (\'County\', \'<U2\'),\r\n           (\'Town\', \'<U6\'), (\'Facility\', \'<U8\'), (\'Time\', \'<i4\')])\r\n    """"""\r\n    f = ""/"".join(script.split(""/"")[:-3]) + ""/Data/sample_100K.npy""\r\n    t = np.load(f)\r\n    rows = t[\'County\']  #t[\'Text01\']\r\n    cols = t[\'Town\']\r\n    ctab, a, result, r, c = crosstab(rows, cols, verbose=False)\r\n    return ctab, a, result, r, c\r\n\r\n\r\nfrmt = """"""\r\nCrosstab results ....\r\n{}\\n\r\nThe array of counts/frequencies....\r\n{}\\n\r\nRow and column headers...\r\n{}\r\n{}\\n\r\nAnd as a fancy output which can be saved to a csv file using\r\n....np.savetxt(\'c:/path/file_name.csv\', array_name, fmt= \'%s\', delimiter=\', \')\r\n{}\r\n""""""\r\n\r\n\r\nif len(sys.argv) == 1:\r\n    ctab, a, result, r, c = _demo()\r\n    pre = \'    \'\r\n    msg = frmt.format(indent(str(ctab), pre), indent(str(a), pre),\r\n                      indent(str(r), pre), indent(str(c), pre),\r\n                      indent(result, pre))\r\n    print(""\\n{}{}"".format(\'---- cross_tab.py \', ""-""*60))\r\n    print(msg)\r\nelse:\r\n    in_tbl = sys.argv[1]\r\n    row_fld = sys.argv[2]\r\n    col_fld = sys.argv[3]\r\n    out_tbl = sys.argv[4]\r\n    txt_file = sys.argv[5]\r\n    flds = [row_fld, col_fld]\r\n    #\r\n    t = tbl_2_np_array(in_tbl=in_tbl, flds=flds)\r\n    #\r\n    rows = t[row_fld]\r\n    cols = t[col_fld]\r\n    ctab, a, result, r, c = crosstab(rows, cols, verbose=True)\r\n    args = [in_tbl, row_fld, col_fld]\r\n    pre = \'    \'\r\n    msg = frmt.format(indent(str(ctab), pre), indent(str(a), pre),\r\n                      indent(str(r), pre), indent(str(c), pre),\r\n                      indent(result, pre))\r\n    tweet(""{}{}"".format(\'---- cross_tab.py \', ""-""*60))\r\n    tweet(msg)\r\n    if not (out_tbl in [\'#\', \'\', None]):\r\n        arcpy.da.NumPyArrayToTable(ctab, out_tbl)\r\n    if not (txt_file in [\'#\', \'\', None]):\r\n        with open(txt_file, \'w\') as f:\r\n            f.write(""Crosstab for ... {}\\n"".format(in_tbl))\r\n            f.write(result)\r\n\r\nif __name__ == ""__main__"":\r\n    """"""run crosstabulation with data""""""\r\n#    ctab, a, result, r, c = _demo()\r\n'"
TableTools/Scripts/cumu_dist.py,0,"b'# -*- coding: utf-8 -*-\r\n"""""" input shape field: returns cumulative distance between points\r\ndist_cumu(!Shape!)    #enter into the expression box""""""\r\nimport math\r\nx0 = 0.0;  y0 = 0.0;  distance = 0.0\r\ndef dist_cumu(shape):\r\n    global x0;  global y0;  global distance\r\n    x = shape.firstpoint.X;  y = shape.firstpoint.Y\r\n    if x0 == 0.0 and y0 == 0.0:\r\n        x0 = x; y0 = y\r\n    distance += math.sqrt((x - x0)**2 + (y - y0)**2)\r\n    x0 = x;  y0 = y\r\n    return distance'"
TableTools/Scripts/excel2tbl.py,15,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nexcel2tbl\r\n===========\r\n\r\nScript :   excel2tbl.py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified : 2018-11-23\r\n\r\nPurpose:  tools for working with numpy arrays\r\n\r\nUseage :\r\n\r\nReferences\r\n----------\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/numpyarraytotable.htm>`_.\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/tabletonumpyarray.htm>`_.\r\n---------------------------------------------------------------------\r\n""""""\r\n# pylint: disable=C0103  # invalid-name\r\n# pylint: disable=R0914  # Too many local variables\r\n# pylint: disable=R1710  # inconsistent-return-statements\r\n# pylint: disable=W0105  # string statement has no effect\r\n\r\nimport sys\r\nimport numpy as np\r\nimport xlrd\r\nimport arcpy.da\r\nfrom arcpy import env\r\n\r\nenv.overwriteOutput = True\r\n#from arcpytools import fc_info, tweet  #, frmt_rec, _col_format\r\n#import arcpy\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n# ----------------------------------------------------------------------\r\n# ---- excel_np\r\ndef excel_np(path, sheet_num=0, int_null=-999):\r\n    """"""Read excel files to numpy structured/record arrays.  Your spreadsheet\r\n    must adhere to simple rules::\r\n      - first row must contain the field names for the output array\r\n      - no blank rows or columns, basically, no fluff or formatting\r\n      - if you have nodata values, put them in, since blank cells will be\r\n        \'corrected\' as best as possible.\r\n      - text and numbers in a column, results in a text column\r\n\r\n    See arraytools.a_io for excel_np for complete description\r\n    """"""\r\n    def isfloat(a):\r\n        """"""float check""""""\r\n        try:\r\n            i = float(a)\r\n            return i\r\n        except ValueError:\r\n            return np.nan\r\n\r\n    def punc_space(name):\r\n        """"""delete punctuation and spaces and replace with \'_\'""""""\r\n        punc = list(\'!""#$%&\\\'()*+,-./:;<=>?@[\\\\]^`{|}~ \')\r\n        return """".join([[i, \'_\'][i in punc] for i in name])\r\n\r\n    # import xlrd\r\n    w = xlrd.open_workbook(path)        # xlrd.book.Book class\r\n    sheets = len(w.sheets())\r\n    if sheet_num > sheets:\r\n        return None\r\n    sheet = w.sheet_by_index(sheet_num) # sheet by number\r\n    # sheet = w.sheet_by_name(\'test\')   # case sensitive, not implemented\r\n    names = sheet.row_values(0)         # clean these up later\r\n    cols = sheet.ncols\r\n    rows = sheet.nrows\r\n    col_data = [sheet.col_values(i, 1, rows) for i in range(cols)]\r\n    row_guess = sheet.row_values(1)\r\n    row_dts = [np.asarray(i).dtype.kind for i in row_guess]\r\n    col_dts = [np.asarray(col_data[i]).dtype.kind\r\n               for i in range(cols)]\r\n    clean = []\r\n    for i in range(len(row_dts)):\r\n        c = col_data[i]\r\n        if row_dts[i] == col_dts[i]:    # same dtype... send to array\r\n            ar = np.asarray(c)\r\n        if row_dts[i] == \'f\':           # float? if so, substitute np.nan\r\n            ar = np.array([isfloat(i) for i in c])\r\n            is_nan = np.isnan(ar)       # find the nan values, then check\r\n            not_nan = ar[~is_nan]       # are the floats == ints?\r\n            if np.all(np.equal(not_nan, not_nan.astype(\'int\'))):  # integer?\r\n                ar[is_nan] = int_null   # assign the integer null\r\n                ar = ar.astype(\'int\')\r\n        elif row_dts[i] in (\'U\', \'S\'):  # unicode/string... send to array\r\n            ar = np.char.strip(ar)\r\n            ar = np.where(np.char.str_len(ar) == 0, \'None\', ar)\r\n        else:\r\n            ar = np.asarray(c)\r\n        clean.append(ar)\r\n    # ---- assemble the columns for the array ----\r\n    dt_str = [i.dtype.str for i in clean]\r\n    names = [i.strip() for i in names]      # clean up leading/trailing spaces\r\n    names = [punc_space(i) for i in names]  # replace punctuation and spaces\r\n    dts_name = list(zip(names, dt_str))\r\n    arr = np.empty((rows-1,), dtype=dts_name)\r\n    cnt = 0\r\n    for i in names:\r\n        arr[i] = clean[cnt]\r\n        cnt += 1\r\n    return arr\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# .... final code section producing the featureclass and extendtable\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    in_excel = script.rpartition(""/"")[0] + ""/Data/test.xlsx""\r\n    sheet_num = 0\r\n    int_null = -999\r\n    arr = excel_np(in_excel, sheet_num=sheet_num, int_null=int_null)\r\n    print(""Array returned...\\n{}"".format(arr))\r\n    # parameters here\r\nelse:\r\n    testing = False\r\n    in_excel = sys.argv[1]\r\n    sheet_num = int(sys.argv[2])\r\n    int_null = sys.argv[3]\r\n    if int_null in (\'-2147483648\', \'-32768\', \'-128\', \'-9\', \'-999\', \'-1\'):\r\n        int_null == int(int_null)\r\n    else:\r\n        int_null = \'-2147483648\'\r\n    out_tbl = sys.argv[4]\r\n    arr = excel_np(in_excel, sheet_num=sheet_num, int_null=int_null)\r\n    if arr is None:\r\n        print(""not a sheet number"")\r\n    else:\r\n        arcpy.da.NumPyArrayToTable(arr, out_tbl)\r\n\r\n# parameters here\r\n#\r\nif testing:\r\n    print(\'\\nScript source... {}\'.format(script))\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n'"
TableTools/Scripts/extend_test.py,6,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   .py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2017-xx-xx\r\n:Purpose:  tools for working with numpy arrays\r\n:Useage:\r\n:\r\n:References:\r\n:\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\n\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\r\n      \'float\': \'{: 0.3f}\'.format}\r\n\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\ndef tweet(msg):\r\n    """"""Print a message for both arcpy and python.\r\n    : msg - a text message\r\n    """"""\r\n    m = ""\\n{}\\n"".format(msg)\r\n    arcpy.AddMessage(m)\r\n    print(m)\r\n    print(arcpy.GetMessages())\r\n\r\n\r\n#in_fc = r""C:\\Git_Dan\\a_Data\\testdata.gdb\\Polygon_pnts""\r\n# arcpy.env.workspace = ""C:/data/base.gdb""\r\nin_tbl = sys.argv[1]\r\nin_flds = sys.argv[2]\r\nout_fld = sys.argv[3]\r\n\r\nif \';\' in in_flds:\r\n    in_flds = in_flds.split(\';\')\r\nelse:\r\n    in_flds = [in_flds]\r\n\r\ndesc = arcpy.da.Describe(in_tbl)\r\ntbl_path = desc[\'path\']\r\nfnames = [i.name for i in arcpy.ListFields(in_tbl)]\r\nif out_fld in fnames:\r\n    out_fld += \'dup\'\r\nout_fld = arcpy.ValidateFieldName(out_fld, tbl_path)\r\nargs = [in_tbl, in_flds, out_fld, tbl_path]\r\nmsg = ""in_tbl {}\\nin_fld {}\\nout_fld  {}\\ntbl_path  {}"".format(*args)\r\ntweet(msg)\r\noid = \'OBJECTID\'\r\nvals = [oid] + in_flds\r\narr = arcpy.da.TableToNumPyArray(in_tbl, vals)\r\ntweet(""{!r:}"".format(arr))\r\nfor v in vals:\r\n    fix_null = np.where(arr[v] == \'None\', \'\', arr[v])\r\n    arr[v] = fix_null\r\n    tweet(""fixing vals...{}\\n{}"".format(v, arr[v]))\r\narr_sort = np.sort(arr, order=in_flds)\r\n# dt = [(oid, \'<i4\'), (vals + \'_rank2\', \'<i4\')]\r\ndt = [(oid, \'<i4\'), (out_fld, \'<i4\')]\r\nout_array = np.zeros((arr_sort.shape[0],), dtype=dt)\r\ndt_names = out_array.dtype.names\r\nout_array[dt_names[0]] = arr_sort[oid]\r\nout_array[dt_names[-1]] = np.arange(1, arr_sort.size + 1)  # gdb tables\r\narcpy.da.ExtendTable(in_tbl, \'OBJECTID\', out_array, \'OBJECTID\')\r\n\r\ndef _demo():\r\n    """"""\r\n    : -\r\n    """"""\r\n    pass\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n    _demo()\r\n\r\n'"
TableTools/Scripts/field_statistics.py,26,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   field_statistics.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2018-02-11\r\n:Purpose:  Descriptive statistics for tables using numpy.\r\n:\r\n:References:\r\n:  https://github.com/numpy/numpy/blob/master/numpy/lib/nanfunctions.py\r\n:  _replace_nan(a, val) -  mask = np.isnan(a) - to get the mask\r\n:\r\n:  a = [1, 2, np.nan, 3, np.nan, 4]\r\n:  _, mask = _replace_nan(a, 0)  # for mean\r\n:  mask = array([False, False,  True, False,  True, False], dtype=bool)\r\n:\r\n: ---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\n\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\n# ---- skewness and kurtosis section -----------------------------------------\r\n\r\ndef skew_kurt(a, avg, var_x, std_x, col=True, mom=\'both\'):\r\n    """"""Momental and unbiased skewness\r\n    :Emulates the nan functions approach to calculating these parameters\r\n    :when data contains nan values.\r\n    :Requires:\r\n    :---------\r\n    :  a - an array of float/double values where there are at least 3 non-nan\r\n    :      numbers in each column.  This is not checked since this situation\r\n    :      should never arise in real world data sets that have been checked.\r\n    :  moment - both, skew or kurt  to return the moments\r\n    :Notes:\r\n    :------\r\n    : a= np.arange(16.).reshape(4,4)\r\n    : mask = [0, 5, 10, 15]\r\n    : masked_array = np.where(a == mask, np.nan, a)\r\n    """"""\r\n#    a, mask = _replace_nan(a, 0.)  # produce a masked of the nan values\r\n    if len(a.shape) == 1:\r\n        ax = 0\r\n    else:\r\n        ax = [1, 0][col]\r\n#    # ---- mean section ----\r\n    mask = np.isnan(a)\r\n    cnt = np.sum(~mask, axis=ax, dtype=np.intp, keepdims=False)\r\n    diff = a - avg\r\n    sqrd = diff * diff\r\n    cubed = sqrd * diff\r\n    fourP = sqrd * sqrd\r\n    x_3 = np.nansum(cubed, axis=ax)\r\n    x_4 = np.nansum(fourP, axis=ax)\r\n    skew_m = x_3 / (cnt * (std_x**3))\r\n    kurt_m = x_4 / (cnt * (var_x * var_x))\r\n    # skew_u = skew_m*((cnt**2)/((cnt-1)*(cnt-2)))  # could add if needed\r\n    if mom == \'skew\':\r\n        return skew_m\r\n    elif mom == \'kurt\':\r\n        return kurt_m\r\n    elif mom == \'both\':\r\n        return skew_m, kurt_m\r\n\r\n\r\n# -----------------------------------------------------------------------------\r\n# functions\r\n\r\ndef tweet(msg):\r\n    """"""Produce a message (msg)for both arcpy and python\r\n    """"""\r\n    m = ""{}"".format(msg)\r\n    arcpy.AddMessage(m)\r\n    print(m)\r\n\r\n\r\n# (1) frmt_rec .... code section ... from frmts.py in arraytools\r\n#  frmt_rec requires _col_format\r\ndef _col_format(a, c_name=""c00"", deci=0):\r\n    """"""Determine column format given a desired number of decimal places.\r\n    :  Used by frmt_struct.\r\n    :  a - a column in an array\r\n    :  c_name - column name\r\n    :  deci - desired number of decimal points if the data are numeric\r\n    :Notes:\r\n    :-----\r\n    :  The field is examined to determine whether it is a simple integer, a\r\n    :  float type or a list, array or string.  The maximum width is determined\r\n    :  based on this type.\r\n    :  Checks were also added for (N,) shaped structured arrays being\r\n    :  reformatted to (N, 1) shape which sometimes occurs to facilitate array\r\n    :  viewing.  A kludge at best, but it works for now.\r\n    """"""\r\n    a_kind = a.dtype.kind\r\n    if a_kind in (\'i\', \'u\'):  # ---- integer type\r\n        w_, m_ = [\':> {}.0f\', \'{:> 0.0f}\']\r\n        col_wdth = len(m_.format(a.max())) + 1\r\n        col_wdth = max(len(c_name), col_wdth) + 1  # + deci\r\n        c_fmt = w_.format(col_wdth, 0)\r\n    elif a_kind == \'f\' and np.isscalar(a[0]):  # ---- float type with rounding\r\n        w_, m_ = [\':> {}.{}f\', \'{:> 0.{}f}\']\r\n        a_max, a_min = np.round(np.sort(a[[0, -1]]), deci)\r\n        col_wdth = max(len(m_.format(a_max, deci)),\r\n                       len(m_.format(a_min, deci))) + 1\r\n        col_wdth = max(len(c_name), col_wdth) + 1\r\n        c_fmt = w_.format(col_wdth, deci)\r\n    else:  # ---- lists, arrays, strings. Check for (N,) vs (N,1)\r\n        if a.ndim == 1:  # ---- check for (N, 1) format of structured array\r\n            a = a[0]\r\n        col_wdth = max([len(str(i)) for i in a])\r\n        col_wdth = max(len(c_name), col_wdth) + 1  # + deci\r\n        c_fmt = ""!s:>"" + ""{}"".format(col_wdth)\r\n    return c_fmt, col_wdth\r\n\r\n\r\ndef frmt_rec(a, deci=2, use_names=True, prn=True):\r\n    """"""Format a structured array with a mixed dtype.\r\n    :Requires\r\n    :-------\r\n    : a - a structured/recarray\r\n    : deci - to facilitate printing, this value is the number of decimal\r\n    :        points to use for all floating point fields.\r\n    : _col_format - does the actual work of obtaining a representation of\r\n    :  the column format.\r\n    :Notes\r\n    :-----\r\n    :  It is not really possible to deconstruct the exact number of decimals\r\n    :  to use for float values, so a decision had to be made to simplify.\r\n    """"""\r\n    dt_names = a.dtype.names\r\n    N = len(dt_names)\r\n    c_names = [[""C{:02.0f}"".format(i) for i in range(N)], dt_names][use_names]\r\n    # ---- get the column formats from ... _col_format ----\r\n    dts = []\r\n    wdths = []\r\n    pair = list(zip(dt_names, c_names))\r\n    for i in range(len(pair)):\r\n        fld, nme = pair[i]\r\n        c_fmt, col_wdth = _col_format(a[fld], c_name=nme, deci=deci)\r\n        dts.append(c_fmt)\r\n        wdths.append(col_wdth)\r\n    row_frmt = "" "".join([(\'{\' + i + \'}\') for i in dts])\r\n    hdr = [""!s:>"" + ""{}"".format(wdths[i]) for i in range(N)]\r\n    hdr2 = "" "".join([""{"" + hdr[i] + ""}"" for i in range(N)])\r\n    header = ""--n--"" + hdr2.format(*c_names)\r\n    header = ""\\n{}\\n{}"".format(header, ""-""*len(header))\r\n    txt = [header]\r\n    # ---- check for structured arrays reshaped to (N, 1) instead of (N,) ----\r\n    len_shp = len(a.shape)\r\n    idx = 0\r\n    for i in range(a.shape[0]):\r\n        if len_shp == 1:  # ---- conventional (N,) shaped array\r\n            row = "" {:03.0f} "".format(idx) + row_frmt.format(*a[i])\r\n        else:             # ---- reformatted to (N, 1)\r\n            row = "" {:03.0f} "".format(idx) + row_frmt.format(*a[i][0])\r\n        idx += 1\r\n        txt.append(row)\r\n    msg = ""\\n"".join([i for i in txt])\r\n    if prn:\r\n        print(msg)\r\n    else:\r\n        return msg\r\n\r\n\r\ndef cal_stats(in_fc, col_names):\r\n    """"""Calculate stats for an array of double types, with nodata (nan, None)\r\n    :  in the column.\r\n    :Requires:\r\n    :---------\r\n    : in_fc - input featureclass or table\r\n    : col_names - the columns... numeric (floating point, double)\r\n    :\r\n    :Notes:\r\n    :------  see the args tuple for examples of nan functions\r\n    :  np.nansum(b, axis=0)   # by column\r\n    :  np.nansum(b, axis=1)   # by row\r\n    :  c_nan = np.count_nonzero(~np.isnan(b), axis=0) count nan if needed\r\n    """"""\r\n    a = arcpy.da.FeatureClassToNumPyArray(in_fc, col_names)  # ""*"")\r\n    b = a.view(np.float).reshape(len(a), -1)\r\n    if len(a.shape) == 1:\r\n        ax = 0\r\n    else:\r\n        ax = [1, 0][True]  # ax = [1, 0][colwise]  colwise= True\r\n    mask = np.isnan(b)\r\n    cnt = np.sum(~mask, axis=ax, dtype=np.intp, keepdims=False)\r\n    n_sum = np.nansum(b, axis=0)\r\n    n_mean = np.nanmean(b, axis=0)\r\n    n_var = np.nanvar(b, axis=0)\r\n    n_std = np.nanstd(b, axis=0)\r\n    sk, kurt = skew_kurt(b, avg=n_mean, var_x=n_var, std_x=n_std,\r\n                         col=True, mom=\'both\')\r\n    args = (col_names, cnt, n_sum, np.nanmin(b, axis=0), np.nanmax(b, axis=0),\r\n            np.nanmedian(b, axis=0), n_mean, n_std, n_var, sk, kurt)\r\n    return col_names, args\r\n\r\n\r\ndef stats_tbl(col_names, args):\r\n    """"""Produce the output table\r\n    :   (\'N_\', \'<i4\'), (\'N_nan\', \'<i4\')\r\n    """"""\r\n    d = [(i, \'<f8\')\r\n         for i in [\'Sum\', \'Min\', \'Max\', \'Med\', \'Avg\',\r\n                   \'Std\', \'Var\', \'Skew\', \'Kurt\']]\r\n    dts = [(\'Field\', \'<U15\'), (\'N\', \'<i4\')] + d\r\n    rows = len(col_names)\r\n    cols = len(dts)\r\n    z = np.empty(shape=(rows,), dtype=dts)\r\n    for i in range(cols):\r\n        z[z.dtype.names[i]] = args[i]\r\n    return z\r\n\r\n\r\nif len(sys.argv) == 1:\r\n    in_fc = r\'C:\\GIS\\Tools_scripts\\Statistics\\Stats_demo_01.gdb\\pnts_2K_normal\'\r\n    flds = arcpy.ListFields(in_fc)\r\n    col_names = [fld.name for fld in flds if fld.type == \'Double\']\r\n    out_tbl = None\r\nelse:\r\n    in_fc = sys.argv[1]\r\n    col_names = sys.argv[2]\r\n    col_names = col_names.split(\';\')\r\n    out_tbl = sys.argv[3]\r\n\r\ncol_names, args = cal_stats(in_fc, col_names)  # calculate statistics\r\nz = stats_tbl(col_names, args)                 # produce the table\r\n\r\nmsg = frmt_rec(z, prn=False)  # fancy printout\r\ntweet(""\\n{}\\nSaving results to .... {}"".format(""-""*60, out_tbl))\r\ntweet(""Stats results...\\n{}"".format(msg))\r\n\r\nif not (out_tbl in (None, \'#\', \'\', \'None\')):\r\n    arcpy.da.NumPyArrayToTable(z, out_tbl)\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n'"
TableTools/Scripts/frequency.py,5,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n=========\r\nfrequency\r\n=========\r\n\r\nScript : frequency.py\r\n\r\nAuthor :  Dan_Patterson@carleton.ca\r\n\r\nModified : 2019-02-23\r\n\r\nPurpose :\r\n    To supplant the Frequency tool for those that don\'t have an\r\n    advanced license.\r\n\r\nUseage :  Load the toolbox into Pro and run the tool script from there.\r\n\r\nReferences\r\n----------\r\n`<http://desktop.arcgis.com/en/arcmap/latest/tools/analysis-toolbox/\r\nfrequency.htm>`_.\r\n\r\nNotes\r\n-----\r\n\r\n>>> to_array = arcpy.da.TableToNumPyArray(r""C:\\folder\\sample.dbf"", ""*"")\r\n>>> arcpy.da.NumPyArrayToTable(from_array, r""C:\\folder_tbl\\test.gdb\\out"")\r\n\r\nDev Info\r\n\r\n  tbx - C:\\GIS\\Tools_scripts\\Table_tools\\Table_tools.tbx\r\n  script - C:\\GIS\\Tools_scripts\\Table_tools\\Scripts\\frequency.py\r\n  arcpy.Tabletools.Frequency(""polygon_demo"",\r\n             r""C:\\GIS\\Tools_scripts\\Table_tools\\Table_tools.gdb\\f2"",\r\n             ""Test;main_part"", None)\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\n\r\nimport numpy.lib.recfunctions as rfn\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\r\n      \'float\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\ndef freq(a, flds=None):\r\n    """"""Frequency and crosstabulation\r\n\r\n    Parameters\r\n    ----------\r\n    a : array\r\n        A structured array\r\n    flds : field\r\n        Fields to use in the analysis\r\n\r\n    Notes\r\n    -----\r\n    : (1) slice the input array by the classification fields\r\n    : (2) sort the sliced array using the flds as sorting keys\r\n    : (3) use unique on the sorted array to return the results\r\n    : (4) a quick histogram to get the counts until numpy 1.12 can be used\r\n    : ... then ship the results back.  only uni and vals is needed. The\r\n    :     rest is for testing and future work.\r\n    >>> np.unique(ar, return_index=False, return_inverse=False,\r\n    ...           return_counts=False, axis=None)\r\n    """"""\r\n    a = a[flds]  # (1)\r\n    idx = np.argsort(a, axis=0, order=flds)  # (2)\r\n    a_sort = a[idx]\r\n    uni, cnts = np.unique(a_sort, return_counts=True)  # (3)\r\n    out = rfn.append_fields(uni, names=""Counts"", data=cnts, usemask=False)\r\n    return out\r\n\r\n\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    in_tbl = r""C:\\Arc_projects\\Table_tools\\Table_tools.gdb\\SamplingGrids""\r\n    all_flds = ""*""\r\n    cls_flds = \'Row_class;Col_class\'\r\nelse:\r\n    testing = False\r\n    in_tbl = sys.argv[1]           # input table\r\n    out_tbl = sys.argv[2]          # results table\r\n    cls_flds = sys.argv[3]         # classification field(s)\r\n\r\n# ---- common tasks\r\ncls_flds = cls_flds.split("";"")  # tidy up the two field inputs\r\na = arcpy.da.TableToNumPyArray(in_tbl, ""*"")  # use the full array\'s data\r\n\r\nout = freq(a, cls_flds)  # do freq analysis\r\n\r\n# create the output array and return the table\r\nif not testing:\r\n    arcpy.da.NumPyArrayToTable(out, out_tbl)\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Do nothing for now.\r\n    """"""\r\n\r\n'"
TableTools/Scripts/geometry.py,21,"b'# -*- coding: utf-8 -*-\r\n""""""\r\ngeometry\r\n========\r\n\r\nRequires:\r\n---------\r\n\r\nThis function is called by poly_field_calcs.py from the Table Tools toolset\r\nIt is designed to be imported and used in the field calculator\r\n\r\nThere is a check for redundant vertices which affects the sum and max angles.\r\nSpecifically, 3 points on a 2 point line will have 180 degree max\r\n\r\nUseage:\r\n------\r\nIn the calling script use\r\n\r\n>>> from angles import angles_poly\r\n\r\nIn the field calculator calling function, feed it the shape field\r\n    angles_poly(!Shape!)\r\n""""""\r\n\r\nimport numpy as np\r\nimport math\r\nfrom arcpytools import fc_info, tweet\r\nimport arcpy\r\n\r\ndef angles_(a, in_deg=True, kind=""sum""):\r\n    """"""Sequential angles from a poly* shape\r\n\r\n    `a` - shape\r\n        A shape from the shape field in a geodatabase\r\n\r\n    """"""\r\n    import numpy as np\r\n    a = a.getPart()\r\n    a = np.asarray([[i.X, i.Y] for j in a for i in j])\r\n    if len(a) < 2:\r\n        return None\r\n    elif len(a) == 2:\r\n        ba = a[1] - a[0]\r\n        return np.arctan2(*ba[::-1])\r\n    a0 = a[0:-2]\r\n    a1 = a[1:-1]\r\n    a2 = a[2:]\r\n    ba = a1 - a0\r\n    bc = a1 - a2\r\n    cr = np.cross(ba, bc)\r\n    dt = np.einsum(\'ij,ij->i\', ba, bc)\r\n#    ang = np.arctan2(np.linalg.norm(cr), dt)\r\n    ang = np.arctan2(cr, dt)\r\n    two_pi = np.pi*2.\r\n    angles = np.where(ang<0, ang + two_pi, ang)\r\n    if in_deg:\r\n        angles = np.degrees(angles)\r\n    if in_deg:\r\n        angles = np.degrees(angles)\r\n    if kind == ""sum"":\r\n        angle = np.sum(angles)\r\n    elif kind == ""min"":\r\n        angle = np.min(angles)\r\n    elif kind == ""max"":\r\n        angle = np.max(angles)\r\n    return angle\r\n\r\n\r\ndef lengths_(a, kind=""avg""):\r\n    """"""poly* side lengths.\r\n    Options include ""min length"", ""max length"", ""avg length""\r\n    """"""\r\n    import numpy as np\r\n    a = a.getPart()\r\n    a = np.asarray([[i.X, i.Y] for j in a for i in j])\r\n    if np.allclose(a[0], a[-1]):  # closed loop\r\n        a = a[:-1]\r\n    a0 = a[:-1]\r\n    b0 = a[1:]\r\n    diff = b0 - a0\r\n    s = np.power(diff, 2)\r\n    d = np.sqrt(s[:, 0] + s[:, 1])\r\n    if kind == ""avg"":\r\n        leng = np.mean(d)\r\n    elif kind == ""min"":\r\n        leng = np.min(d)\r\n    elif kind == ""max"":\r\n        leng = np.max(d)\r\n    return leng\r\n\r\n#__esri_field_calculator_splitter__\r\n#angles_poly(!Shape!)\r\n\r\n\r\ndef to_array(in_fc):\r\n    """"""Extract the shapes and produce a coordinate array.\r\n    """"""\r\n    shp_fld, oid_fld, shp_type, SR = fc_info(in_fc)\r\n    in_flds = [oid_fld] + [\'SHAPE@X\', \'SHAPE@Y\']\r\n    a = arcpy.da.FeatureClassToNumPyArray(in_fc, in_flds)\r\n    a = a[[\'SHAPE@X\', \'SHAPE@Y\']]\r\n    a = a.view(np.float64).reshape(a.shape[0], 2)\r\n    return a\r\n\r\n'"
TableTools/Scripts/nested_means.py,7,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nscript name here\r\n=======\r\n\r\nScript :   template.py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified : 2019-\r\n\r\nPurpose :  Tools for\r\n\r\nNotes:\r\n\r\nReferences:\r\n\r\n""""""\r\n# pylint: disable=C0103  # invalid-name\r\n# pylint: disable=R0914  # Too many local variables\r\n# pylint: disable=R1710  # inconsistent-return-statements\r\n# pylint: disable=W0105  # string statement has no effect\r\n\r\nimport sys\r\nimport os\r\nimport numpy as np\r\nimport arcpy\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=100, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\n#script = sys.argv[0]  # print this should you need to locate the script\r\n\r\n# ===========================================================================\r\n# ---- def section: def code blocks go here ---------------------------------\r\ndef tweet(msg):\r\n    """"""Print a message for both arcpy and python.\r\n    : msg - a text message\r\n    """"""\r\n    m = ""\\n{}\\n"".format(msg)\r\n    arcpy.AddMessage(m)\r\n    print(m)\r\n\r\n\r\ndef mean_split(a, minSize=3, cmax=9):\r\n    """"""split at means""""""\r\n    def slice_array(a):\r\n        m = np.mean(a)\r\n        yes = a <= m  # check for a less than the overal mean\r\n        a_left, a_rght = a[~yes], a[yes]  # slice the arrays\r\n        return m, a_left, a_rght\r\n    # ----\r\n    m, L, R = slice_array(a)\r\n    m0, L, _ = slice_array(L)\r\n    m1, _, R = slice_array(R)\r\n    means = [m0, m, m1]\r\n    while ((len(L) > minSize) and (len(R) > minSize) and (len(means) <= cmax)):\r\n        m0, L, _ = slice_array(L)\r\n        m1, _, R = slice_array(R)\r\n        means.extend([m0, m1])   \r\n    return sorted(means)\r\n\r\n# ===========================================================================\r\n# ---- main section: testing or tool run ------------------------------------\r\n#\r\ndef _common_():\r\n    """"""Stuff common to _demo_ and _tool()\r\n    """"""\r\n    script = sys.argv[0]\r\n    return script\r\n\r\ndef _demo_():\r\n    """"""Run in spyder\r\n    """"""\r\n    testing = True\r\n    script = _common_()\r\n    msg0 = ""\\nRunning... {} in Spyder\\n"".format(script)\r\n    tbl = r""C:\\Arc_projects\\Table_tools\\Table_tools.gdb\\pnts_2K_normal""\r\n    in_fld = ""Norm""\r\n    out_fld = ""New_class""\r\n    cut_off = 9\r\n    return msg0, testing, tbl, in_fld, out_fld, cut_off\r\n\r\ndef _tool_():\r\n    """"""run from a tool in arctoolbox in arcgis pro\r\n    """"""\r\n    testing = False\r\n    script = _common_()\r\n    tbl = sys.argv[1]\r\n    in_fld = sys.argv[2]\r\n    out_fld = sys.argv[3]\r\n    cut_off = int(sys.argv[4])\r\n    msg0 = ""\\nRunning... {} in in ArcGIS Pro\\n"".format(script)\r\n    return msg0, testing, tbl, in_fld, out_fld, cut_off\r\n# ===========================================================================\r\n# ---- main section: testing or tool run ------------------------------------\r\n#\r\nif len(sys.argv) == 1:\r\n    msg, testing, tbl, in_fld, out_fld, cut_off = _demo_()\r\nelse:\r\n    msg, testing, tbl, in_fld, out_fld, cut_off = _tool_()\r\n\r\nprint(msg)\r\n\r\n# ---- Do some work\r\n# (1)  Get the field from the table and make it a simple array\r\narr = arcpy.da.TableToNumPyArray(tbl, [\'OID@\', in_fld], """", True, None)\r\na = arr[in_fld]\r\n\r\n# (2)  Set up for the results\r\nout_arr = np.copy(arr)\r\nout_fld = arcpy.ValidateFieldName(out_fld, os.path.dirname(tbl))\r\nout_arr.dtype.names = [\'OID@\', out_fld]\r\n\r\n# (3)  Run the mean_split script... note minSize!!! set it smartly or...\r\nmeans = mean_split(a, minSize=3, cmax=cut_off)\r\nmeans = sorted(means)\r\nclassed = np.digitize(a, bins=means)\r\n\r\n# (4)  Send the results to the output array and add it back to arcgis pro\r\n#\r\nout_arr[out_fld] = classed\r\n#arcpy.da.ExtendTable(tbl, \'OID@\', out_arr, \'OID@\')\r\n\r\nfrmt = """"""\r\nmeans  : {}\r\ncounts : {}\r\n""""""\r\nc, m= np.histogram(a, means)\r\ntweet(frmt.format(m, c))\r\n# ==== Processing finished ====\r\n# ===========================================================================\r\n#\r\nif __name__ == ""__main__"":\r\n    """"""optional location for parameters""""""\r\n    msg = _demo_()\r\n\r\n'"
TableTools/Scripts/numpyarray2table.py,5,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   numpyarray2table.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2018-02-11\r\n:Purpose:  tools for working with numpy arrays\r\n:Useage:\r\n:\r\n:References:\r\n: - Derived from python snippet output...\r\n:  in_arr = \'C:/Temp/x.npy\'\r\n:  out_gdb = \'C:/GIS/Tools_scripts/Table_tools/Table_tools.gdb\'\r\n:  out_name = \'sample_1000_npy\'\r\n:  arcpy.Tabletools.NumPyArrayToTable(in_arr, out_gdb, out_name)\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nfrom textwrap import dedent\r\nimport numpy as np\r\nimport arcpy\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\ndef tweet(msg):\r\n    """"""Print a message for both arcpy and python.\r\n    : msg - a text message\r\n    """"""\r\n    m = ""\\n{}\\n"".format(msg)\r\n    arcpy.AddMessage(m)\r\n    print(m)\r\n\r\n\r\n# ---- Run options: _demo or from _tool\r\n#\r\ndef _demo():\r\n    """"""Code to run if in demo mode\r\n    """"""\r\n    a = np. array([\'1, 2, 3, 4, 5\', \'a, b, c\', \'6, 7, 8, 9\',\r\n                   \'d, e, f, g, h\', \'10, 11, 12, 13\'])\r\n    return a\r\n\r\n\r\ndef _tool():\r\n    """"""run when script is from a tool\r\n    """"""\r\n    in_arr = sys.argv[1]\r\n    out_gdb = sys.argv[2]\r\n    out_name = sys.argv[3]\r\n    #\r\n    arcpy.env.workspace = out_gdb\r\n    tbls = arcpy.ListTables()\r\n    out_name = arcpy.ValidateTableName(out_name)\r\n    if out_name in tbls:\r\n        out_name += \'_dup\'\r\n    #\r\n    # ---- call section for processing function\r\n    #\r\n    a = np.load(in_arr)\r\n    in_table = ""\\\\"".join([out_gdb, out_name])\r\n    # ---- where_clause= skip_nulls=  null_value=)\r\n    arcpy.da.NumPyArrayToTable(a, in_table)\r\n    arcpy.MakeTableView_management(in_table, out_name)\r\n    #\r\n    args = [in_arr, out_gdb, out_name]\r\n    msg = """"""\r\n    :------------------------------------------------------------\r\n    Input array... {}\r\n    Output gdb.... {}\r\n    Output name... {}\r\n\r\n    Conversion complete...\r\n    Add the table manually if you want to see it...\r\n    :------------------------------------------------------------\r\n    """"""\r\n    msg = dedent(msg).format(*args)\r\n    tweet(msg)\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# .... final code section producing the featureclass and extendtable\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    arrs = _demo()\r\n    frmt = ""Result...\\n{}""\r\n    print(frmt.format(arrs))\r\nelse:\r\n    testing = False\r\n    _tool()\r\n#\r\nif not testing:\r\n    tweet(\'\\nConversion done...\')\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n    a = _demo()\r\n'"
TableTools/Scripts/pnt_field_calcs.py,0,"b'# -*- coding: utf-8 -*-\r\n""""""\r\npnt_field_calcs\r\n================\r\n\r\nScript :   pnt_field_calcs.py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified : 2018-07-25\r\n\r\nPurpose :  tools for working ArcGIS Pro field calculator\r\n\r\nUseage:\r\n-------\r\n\r\n\r\n**Requires**\r\n------------\r\n\r\n\r\n**Notes**\r\n---------\r\n\r\n**Basic array information**\r\n\r\n""""""\r\n# ---- imports, formats, constants -------------------------------------------\r\nimport sys\r\nimport inspect\r\nfrom textwrap import dedent, indent\r\nimport numpy as np\r\nimport arcpy\r\n\r\narcpy.env.overwriteOutput = True\r\n\r\nscript = sys.argv[0]\r\n\r\n# ---- simple functions ------------------------------------------------------\r\n\r\n__all__ = []\r\n\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    create_output = False\r\n    in_tbl = ""/pnts_2k_normal""\r\n    gdb = ""/Table_tools.gdb""\r\n    flder = ""/"".join(script.split(""/"")[:-2])\r\n    wrkspace = flder + gdb\r\n    in_tbl = wrkspace + in_tbl\r\n    in_fld = None\r\nelse:\r\n    testing = False\r\n    create_output = True\r\n    in_tbl = sys.argv[1]\r\n    in_fld = sys.argv[2]\r\n    exp_key = sys.argv[3]\r\n\r\n# ---- Do the work -----------------------------------------------------------\r\n#\r\ndesc = arcpy.da.Describe(in_tbl)\r\nwrkspace = desc[\'path\']\r\n\r\n# ---- Expression functions\r\nfld_name = in_fld\r\n\r\nif exp_key == ""cumulative distance"":\r\n    from geometry import dist_cumu\r\n    if inspect.isfunction(dist_cumu):\r\n        lines, ln_num = inspect.getsourcelines(dist_cumu)\r\n        code = """".join([""{}"".format(line) for line in lines])\r\n        fld_expr = ""dist_cumu(!Shape!)""\r\n        fld_name = ""Cumu_dist""\r\n    args = [fld_name, fld_expr, code]\r\nelif exp_key == ""distance between"":\r\n    from geometry import dist_between\r\n    if inspect.isfunction(dist_between):\r\n        lines, ln_num = inspect.getsourcelines(dist_between)\r\n        code = """".join([""{}"".format(line) for line in lines])\r\n        fld_expr = ""dist_between(!Shape!)""\r\n        fld_name = ""Dist_btwn""\r\n    args = [fld_name, fld_expr, code]\r\n\r\nfld_name, fld_expr, code = args\r\n\r\narcpy.MakeTableView_management(\r\n        in_table=in_tbl,\r\n        out_view=""tbl_view"",\r\n        workspace=wrkspace)\r\n\r\nif in_fld in (None, """", "" ""):\r\n    fld_name = fld_name\r\nelse:\r\n    fld_name = in_fld\r\nfld_name = arcpy.ValidateFieldName(fld_name)\r\narcpy.AddField_management(\r\n        ""tbl_view"",\r\n        field_name=fld_name,\r\n        field_type=""DOUBLE"",\r\n        field_is_nullable=""NULLABLE"")\r\n\r\narcpy.CalculateField_management(\r\n        in_table=""tbl_view"",\r\n        field=fld_name,\r\n        expression=fld_expr,\r\n        code_block=code)\r\n\r\ndel in_fld, in_tbl, arcpy\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\n\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n#    pnts, mesh = _demo()'"
TableTools/Scripts/poly_field_calcs.py,0,"b'# -*- coding: utf-8 -*-\r\n""""""\r\npoly_field_calcs\r\n================\r\n\r\nScript :   poly_field_calcs.py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified : 2018-07-25\r\n\r\nPurpose :  tools for working ArcGIS Pro field calculator\r\n\r\nUseage:\r\n-------\r\n\r\n\r\n**Requires**\r\n------------\r\n  see import section and __init__.py in the `arraytools` folder\r\n\r\n**Notes**\r\n---------\r\n\r\n**Basic array information**\r\n\r\n""""""\r\n# ---- imports, formats, constants -------------------------------------------\r\nimport sys\r\nimport inspect\r\nfrom textwrap import dedent, indent\r\nimport numpy as np\r\nimport arcpy\r\n\r\narcpy.env.overwriteOutput = True\r\n\r\nscript = sys.argv[0]\r\n\r\n# ---- simple functions ------------------------------------------------------\r\n\r\n__all__ = []\r\n\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    create_output = False\r\n    in_tbl = ""/shapes_mtm9""\r\n    gdb = ""/Table_tools.gdb""\r\n    flder = ""/"".join(script.split(""/"")[:-2])\r\n    wrkspace = flder + gdb\r\n    in_tbl = wrkspace + in_tbl\r\n    in_fld = None\r\nelse:\r\n    testing = False\r\n    create_output = True\r\n    in_tbl = sys.argv[1]\r\n    in_fld = sys.argv[2]\r\n    exp_key = sys.argv[3]\r\n\r\n# ---- Do the work -----------------------------------------------------------\r\n#\r\ndesc = arcpy.da.Describe(in_tbl)\r\nwrkspace = desc[\'path\']\r\n\r\n# ---- Expression functions\r\nfld_name = in_fld\r\nif exp_key == ""area sq km"":\r\n     args = [""Area_sqkm"",\r\n             ""!Shape!.getArea(\'GEODESIC\',\'SQUAREKILOMETERS\')"", ""#""]\r\nelif exp_key == ""leng km"":\r\n    args = [""Leng_km"",\r\n            ""!Shape!.getLength(\'GEODESIC\',\'KILOMETERS\')"", ""#""]\r\nelif exp_key in (""sum angles"", ""min angle"", ""max angle""):\r\n    from geometry import angles_\r\n    if inspect.isfunction(angles_):\r\n        lines, ln_num = inspect.getsourcelines(angles_)\r\n        code = """".join([""{}"".format(line) for line in lines])\r\n        if exp_key == ""sum angles"":\r\n            fld_expr = ""angles_(!Shape!, kind=\'sum\')""\r\n            fld_name = ""Angle_sum""\r\n        elif exp_key == ""min angle"":\r\n            fld_expr = ""angles_(!Shape!, kind=\'min\')""\r\n            fld_name = ""Angle_min""\r\n        elif exp_key == ""max angle"":\r\n            fld_expr = ""angles_(!Shape!, kind=\'max\')""\r\n            fld_name = ""Angle_max""\r\n    args = [fld_name, fld_expr, code]\r\nelif exp_key in (""min length"", ""max length"", ""avg length""):\r\n    from geometry import lengths_\r\n    if inspect.isfunction(lengths_):\r\n        lines, ln_num = inspect.getsourcelines(lengths_)\r\n        code = """".join([""{}"".format(line) for line in lines])\r\n        if exp_key == ""avg length"":\r\n            fld_expr = ""lengths_(!Shape!, kind=\'avg\')""\r\n            fld_name = ""Length_avg""\r\n        elif exp_key == ""min length"":\r\n            fld_expr = ""lengths_(!Shape!, kind=\'min\')""\r\n            fld_name = ""Length_min""\r\n        elif exp_key == ""max length"":\r\n            fld_expr = ""lengths_(!Shape!, kind=\'max\')""\r\n            fld_name = ""Length_max""\r\n    args = [fld_name, fld_expr, code]\r\n\r\nfld_name, fld_expr, code = args\r\n\r\narcpy.MakeTableView_management(\r\n        in_table=in_tbl,\r\n        out_view=""tbl_view"",\r\n        workspace=wrkspace)\r\n\r\nif in_fld in (None, """", "" ""):\r\n    fld_name = fld_name\r\nelse:\r\n    fld_name = in_fld\r\nfld_name = arcpy.ValidateFieldName(fld_name)\r\narcpy.AddField_management(\r\n        ""tbl_view"",\r\n        field_name=fld_name,\r\n        field_type=""DOUBLE"",\r\n        field_is_nullable=""NULLABLE"")\r\n\r\narcpy.CalculateField_management(\r\n        in_table=""tbl_view"",\r\n        field=fld_name,\r\n        expression=fld_expr,\r\n        code_block=code)\r\n\r\ndel in_fld, in_tbl, arcpy\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\n\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n#    pnts, mesh = _demo()'"
TableTools/Scripts/query_reclass.py,7,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   query_reclass.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2017-11-16\r\n:Purpose:  tools for working with numpy arrays\r\n:Useage:  Used in arctoolbox for \'like\' sub-query style queries\r\n: - create a field in a geodatabase table and calculates values based on\r\n:   conditions being met.  Essentiall enables a reclassification into a new\r\n:   field.\r\n:\r\n:References:\r\n:----------\r\n: - SQL\r\n:   http://pro.arcgis.com/en/pro-app/help/mapping/navigation/\r\n:        sql-reference-for-elements-used-in-query-expressions.\r\n:        htm#GUID-68D21843-5274-4AF4-B7F3-165892232A43\r\n: - TableSelect\r\n:   http://pro.arcgis.com/en/pro-app/tool-reference/analysis/table-select.htm\r\n:   arcpy.analysis.TableSelect(""xy1000"", path_to_table,\r\n:                              ""Address_ LIKE \'%Street%\'"")\r\n: - CalculateField\r\n:   http://pro.arcgis.com/en/pro-app/tool-reference/data-management/\r\n:        calculate-field.htm\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nfrom textwrap import dedent\r\nimport numpy as np\r\nimport arcpy\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\r\n      \'float\': \'{: 0.3f}\'.format}\r\n\r\nnp.set_printoptions(edgeitems=5, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\n# ------------------------------------------------------------------------\r\n# ---- functions ----\r\ndef tweet(msg):\r\n    """"""Print a message for both arcpy and python.\r\n    : msg - a text message\r\n    """"""\r\n    m = ""{}"".format(msg)\r\n    arcpy.AddMessage(m)\r\n    print(m)\r\n    print(arcpy.GetMessages())\r\n\r\n\r\ndef _demo():\r\n    """"""run when script is standalone""""""\r\n    in_fc = r\'C:\\Git_Dan\\a_Data\\testdata.gdb\\xy1000\'\r\n    flds = [\'OID@\', \'Address_\']\r\n    in_fld = \'Address_\'\r\n    out_fld = \'Str_class\'\r\n    from_s = [\'Street\', \'Lane\', \'Court\']  # partial set\r\n    to_s = [10, 20, 30]\r\n    dt = [(\'IDs\', \'<i4\'), (\'Str_class\', \'<i4\')]\r\n    testing = True\r\n    return in_fc, flds, in_fld, out_fld, from_s, to_s, dt, testing\r\n\r\n\r\ndef _tool():\r\n    """"""run when script is from a tool""""""\r\n    in_fc = sys.argv[1]\r\n    in_fld = sys.argv[2]\r\n    out_fld = sys.argv[3]\r\n    from_s = sys.argv[4].split("","")\r\n    to_s = sys.argv[5].split("","")\r\n    testing = False\r\n    return in_fc, in_fld, out_fld, from_s, to_s, testing\r\n\r\n\r\n# ------------------------------------------------------------------------\r\n# (1) ---- Checks to see if running in test mode or from a tool ----------\r\nif len(sys.argv) == 1:\r\n    in_fc, flds, in_fld, out_fld, from_s, to_s, dt, testing = _demo()\r\nelse:\r\n    in_fc, in_fld, out_fld, from_s, to_s, testing = _tool()\r\n\r\n#\r\n# ------------------------------------------------------------------------\r\n# (2) ---- Create the array from the cursor, print inputs\r\n#\r\ndesc = arcpy.da.Describe(in_fc)\r\ntbl_path = desc[\'path\']\r\nfnames = [i.name for i in arcpy.ListFields(in_fc)]\r\nif out_fld in fnames:\r\n    out_fld += \'_dupl\'\r\nout_fld = arcpy.ValidateFieldName(out_fld)\r\nflds = [\'OBJECTID\', in_fld]\r\nargs = [in_fc, flds, None, None, False, (None, None)]\r\ncur = arcpy.da.SearchCursor(*args)\r\na = cur._as_narray()\r\n# ----\r\nargs = [""-""*60, in_fc, in_fld, out_fld, from_s, to_s, a]\r\nfrmt = """"""\r\n{}\\nInput table:  {}\\nin_fld:   {}\\nout_fld:  {}\\n\r\nFrom values  {}\\nTo values    {}\r\nInput array...\r\n{!r:}""""""\r\nmsg = frmt.format(*args)\r\ntweet(msg)\r\n#\r\n# ------------------------------------------------------------------------\r\n# (3) ----  Check the output dtype and produce the empty array -----------\r\n#\r\nto_s = np.asarray(to_s)\r\ndt_2s = to_s.dtype.str\r\ndt_k = to_s.dtype.kind\r\ndt = [(\'IDs\', \'<i4\'), (out_fld, dt_2s)]\r\nout = np.zeros((len(a),), dtype=dt)\r\nif dt_k in (\'i\', \'I\', \'l\', \'L\'):\r\n    fill_v = -9\r\nelif dt_k in (\'U\', \'S\'):\r\n    fill_v = None\r\nelse:\r\n    fill_v = np.nan\r\nout[out_fld].fill(fill_v)\r\nout[\'IDs\'] = np.arange(1, len(a) + 1, dtype=\'int32\')\r\ncnt = 0\r\nfor f in from_s:\r\n    idx = np.array([i for i, item in enumerate(a[in_fld]) if f in item])\r\n    out[out_fld][idx] = to_s[cnt]\r\n    cnt += 1\r\n#\r\n# ------------------------------------------------------------------------\r\n# (4) ---- Do the table joining ------------------------------------------\r\nif testing:\r\n    tweet(""Output array...\\n{!r:}"".format(out.reshape(out.shape[0], -1)))\r\nelse:\r\n    arcpy.da.ExtendTable(in_fc, \'OBJECTID\', out, \'OBJECTID\')\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n#    in_fc, flds, in_fld, out_fld, from_s, to_s, dt, testing = _demo()\r\n'"
TableTools/Scripts/rank_field.py,7,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   rank_field.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2017-07-12\r\n:Purpose:  tools for working with arcpy and numpy arrays\r\n:  - sort a table based on a field or fields\r\n:References:\r\n:(1) FeatureClassToNumPyArray (in_table, field_names, {where_clause},\r\n:                           {spatial_reference}, {explode_to_points},\r\n:                           {skip_nulls}, {null_value})\r\n: - http://pro.arcgis.com/en/pro-app/arcpy/data-access/\r\n:        featureclasstonumpyarray.htm\r\n:   -SHAPE@TRUECENTROID \xe2\x80\x94A tuple of the feature\'s true centroid coordinates\r\n:   -SHAPE@X \xe2\x80\x94 A double of the feature\'s x-coordinate.\r\n:   -SHAPE@Y \xe2\x80\x94 A double of the feature\'s y-coordinate.\r\n:\r\n:(2) TableToNumPyArray (in_table, field_names, {where_clause},\r\n:                   {skip_nulls}, {null_value})\r\n: - http://pro.arcgis.com/en/pro-app/arcpy/data-access/tabletonumpyarray.htm\r\n:\r\n:(3) ExtendTable(in_table, table_match_field, in_array,\r\n:              array_match_field, {append_only})\r\n: - http://pro.arcgis.com/en/pro-app/arcpy/data-access/extendtable.htm\r\n:Notes:\r\n:-----\r\n:\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\r\n      \'float\': \'{: 0.3f}\'.format}\r\n\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\n# -----------------------------------------------------------------------------\r\n# functions from arraytools\r\ndef tweet(msg):\r\n    """"""Produce a message (msg)for both arcpy and python\r\n    """"""\r\n    m = ""{}"".format(msg)\r\n    arcpy.AddMessage(m)\r\n    print(m)\r\n    print(arcpy.GetMessages())\r\n\r\n\r\ndef fc_info(in_fc):\r\n    """"""basic feature class information""""""\r\n    desc = arcpy.Describe(in_fc)    # fix to use da.Describe\r\n    SR = desc.spatialReference      # spatial reference object\r\n    shp_fld = desc.shapeFieldName   # FID or OIDName, normally\r\n    oid_fld = desc.OIDFieldName     # Shapefield ...\r\n    return shp_fld, oid_fld, SR\r\n\r\n\r\n# -----------------------------------------------------------------------------\r\n# ---- Other defs ----\r\ndef rankmin(x):\r\n    """"""Returns a rank accounting for duplicates\r\n    :  The array must be sorted first\r\n    :  Warren W. solution at\r\n    : https://stackoverflow.com/questions/39059371/\r\n    :       can-numpys-argsort-give-equal-element-the-same-rank\r\n    """"""\r\n    u, inv, counts = np.unique(x, return_inverse=True, return_counts=True)\r\n    csum = np.zeros_like(counts)\r\n    csum[1:] = counts[:-1].cumsum()\r\n    return csum[inv]\r\n\r\n\r\n# ------------------------------------------------------------------------\r\n# ---- Checks to see if running in test mode or from a tool\r\nif len(sys.argv) == 1:\r\n    in_tbl = r\'C:\\GIS\\Tools_scripts\\Statistics\\Stats_demo_01.gdb\\pnts_2K_normal\'\r\n    desc = arcpy.da.Describe(in_tbl)\r\n    oid_fld = desc[\'OIDFieldName\']\r\n    flds = arcpy.ListFields(in_tbl)\r\n    # fld_names = [fld.name for fld in flds]\r\n    fld_names = [\'Rand_1_100\', oid_fld]\r\n    testing = True\r\n    rank_fld = \'Rand_1_100\'\r\n    rank_min = True\r\nelse:\r\n    in_tbl = sys.argv[1]\r\n    fld_names = sys.argv[2]\r\n    rank_fld = sys.argv[3]\r\n    rank_min = sys.argv[4]\r\n    #\r\n    desc = arcpy.da.Describe(in_tbl)\r\n    oid_fld = desc[\'OIDFieldName\']\r\n    testing = False\r\n\r\n# ------------------------------------------------------------------------\r\n# ---- Create the array, sort extend/join to the input table ----\r\nif rank_fld == \'\':\r\n    rank_fld = \'Rank\'\r\nelse:\r\n    no_good = \' !""#$%&\\\'()*+,-./:;<=>?@[\\\\]^`{|}~\'\r\n    rank_fld = """".join([i for i in rank_fld if i not in no_good])\r\n\r\nif isinstance(fld_names, (list, tuple)):\r\n    order_by = fld_names\r\nelif isinstance(fld_names, (str)):\r\n    if str(fld_names).find("";"") == -1:\r\n        order_by = [fld_names, oid_fld]\r\n    else:\r\n        order_by = fld_names.split("";"") + [oid_fld]\r\n\r\na = arcpy.da.TableToNumPyArray(in_tbl, field_names=order_by)\r\n\r\na_s = a[order_by]\r\nsrted = np.argsort(a_s, order=order_by)\r\n\r\ndt = [(oid_fld, \'<i4\'), (rank_fld, \'<i4\')]\r\nj_a = np.zeros(a.shape, dtype=dt)\r\nj_a[oid_fld] = a_s[srted][oid_fld]\r\n\r\nif rank_min in (\'true\', True):  # use regular or rankmin ranking method\r\n    r = a_s[srted][fld_names]\r\n    r = rankmin(r)\r\n    j_a[rank_fld] = r\r\nelse:\r\n    j_a[rank_fld] = np.arange(1, a.shape[0]+1)\r\n#\r\nif not testing:\r\n    arcpy.da.ExtendTable(in_table=in_tbl,\r\n                         table_match_field=oid_fld,\r\n                         in_array=j_a,\r\n                         array_match_field=oid_fld)\r\n\r\nfrmt = """"""\r\n{}\r\n:Script....{}\r\n:Ranking... {}\r\n:Using fields...\r\n:   {}\r\n{}\r\n""""""\r\nargs = [""-""*70, script, in_tbl, order_by, ""-""*70]\r\nmsg = frmt.format(*args)\r\ntweet(msg)\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n    pass\r\n'"
TableTools/Scripts/rank_flds.py,6,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   rank_flds.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2017-11-01\r\n:Purpose:  tools for working with numpy arrays\r\n:Useage:  Runk the toolbox, select the parameters\r\n:\r\n:References: sure are\r\n:\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\n\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\r\n      \'float\': \'{: 0.3f}\'.format}\r\n\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\ndef tweet(msg):\r\n    """"""Print a message for both arcpy and python.\r\n    : msg - a text message\r\n    """"""\r\n    m = ""\\n{}\\n"".format(msg)\r\n    arcpy.AddMessage(m)\r\n    print(m)\r\n    print(arcpy.GetMessages())\r\n\r\n\r\nin_tbl = sys.argv[1]\r\nin_flds = sys.argv[2]\r\nout_fld = sys.argv[3]\r\n\r\nif \';\' in in_flds:\r\n    in_flds = in_flds.split(\';\')\r\nelse:\r\n    in_flds = [in_flds]\r\n\r\ndesc = arcpy.da.Describe(in_tbl)\r\ntbl_path = desc[\'path\']\r\nfnames = [i.name for i in arcpy.ListFields(in_tbl)]\r\nif out_fld in fnames:\r\n    out_fld += \'dup\'\r\nout_fld = arcpy.ValidateFieldName(out_fld, tbl_path)\r\nargs = [in_tbl, in_flds, out_fld, tbl_path]\r\nmsg = ""in_tbl {}\\nin_fld {}\\nout_fld  {}\\ntbl_path  {}"".format(*args)\r\ntweet(msg)\r\noid = \'OBJECTID\'\r\nvals = [oid] + in_flds\r\narr = arcpy.da.TableToNumPyArray(in_tbl, vals)\r\ntweet(""{!r:}"".format(arr))\r\nfor v in vals:\r\n    fix_null = np.where(arr[v] == \'None\', \'\', arr[v])\r\n    arr[v] = fix_null\r\narr_sort = np.sort(arr, order=in_flds)\r\ndt = [(oid, \'<i4\'), (out_fld, \'<i4\')]\r\nout_array = np.zeros((arr_sort.shape[0],), dtype=dt)\r\ndt_names = out_array.dtype.names\r\nout_array[dt_names[0]] = arr_sort[oid]\r\nout_array[dt_names[-1]] = np.arange(1, arr_sort.size + 1)  # gdb tables\r\narcpy.da.ExtendTable(in_tbl, \'OBJECTID\', out_array, \'OBJECTID\')\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n'"
TableTools/Scripts/sequences.py,12,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\nsequences\r\n================\r\n\r\nScript:   sequential_funcs.py\r\nAuthor:   Dan.Patterson@carleton.ca\r\nModified: 2018-06-02\r\nPurpose :\r\n    Calculating sequential patterns for fields in geodatabase tables\r\nUseage :\r\n\r\nReferences:\r\n-----------\r\n  http://pro.arcgis.com/en/pro-app/arcpy/functions/\r\n       numpyarraytoraster-function.htm\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\nfrom arcpytools import fc_info, tweet, frmt_rec, _col_format\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\ndef sequences(data, stepsize=0):\r\n    """"""Return a list of arrays of sequences values denoted by stepsize\r\n\r\n    data :\r\n        List/array of values in 1D\r\n    stepsize :\r\n        Separation between the values.  If stepsize=0, sequences of equal\r\n        values will be searched.  If stepsize is 1, then sequences incrementing\r\n        by 1... etcetera.  Stepsize can be both positive or negative\r\n\r\n    >>> # check for incrementing sequence by 1\'s\r\n    d = [1, 2, 3, 4, 4, 5]\r\n    s, o = sequences(d, 1, True)\r\n    # s = [array([1, 2, 3, 4]), array([4, 5])]\r\n    # o = array([[1, 4, 4],\r\n    #            [4, 2, 6]])\r\n\r\n    Notes:\r\n    ------\r\n    For strings, use\r\n\r\n    >>> partitions = np.where(a[1:] != a[:-1])[0] + 1\r\n\r\n    Variants:\r\n    ---------\r\n    Change `N` in the expression to find other splits in the data\r\n\r\n    >>> np.split(data, np.where(np.abs(np.diff(data)) >= N)[0]+1)\r\n\r\n    References:\r\n    -----------\r\n\r\n    `<https://stackoverflow.com/questions/7352684/how-to-find-the-groups-of-\r\n    sequences-elements-from-an-array-in-numpy>`_.\r\n\r\n    `<https://stackoverflow.com/questions/50551776/python-chunk-array-on-\r\n    condition#50551924>`_.\r\n    """"""\r\n    #\r\n    a = np.array(data)\r\n    a_dt = a.dtype.kind\r\n    if a_dt in (\'U\', \'S\'):\r\n        seqs = np.split(a, np.where(a[1:] != a[:-1])[0] + 1)\r\n    elif a_dt in (\'i\', \'f\'):\r\n        seqs = np.split(a, np.where(np.diff(a) != stepsize)[0] + 1)\r\n    vals = [i[0] for i in seqs]\r\n    cnts = [len(i) for i in seqs]\r\n    seq_num = np.arange(len(cnts))\r\n    too = np.cumsum(cnts)\r\n    frum = np.zeros_like(too)\r\n    frum[1:] = too[:-1]\r\n    dt = [(\'ID\', \'<i4\'), (\'Value\', a.dtype.str), (\'Count\', \'<i4\'),\r\n          (\'From_\', \'<i4\'), (\'To_\', \'<i4\')]\r\n    out = np.array(list(zip(seq_num, vals, cnts, frum, too)), dtype=dt)\r\n    return out\r\n\r\n\r\n# ---- Run options: _demo or from _tool\r\n#\r\ndef _demo():\r\n    """"""Code to run if in demo mode\r\n    Requires:\r\n        arcpytools fc_info, tweet\r\n    """"""\r\n    tbl = ""Table_tools.gdb/pnts_2k_normal""\r\n    in_tbl = ""/"".join(script.split(""/"")[:-2] + [tbl])\r\n    #\r\n    _, oid_fld, _, _ = fc_info(in_tbl, prn=False)  # run fc_info\r\n    #\r\n    in_fld = \'SequenceTxt\'  #\'Sequences2\'  # \'SequenceTxt\'\r\n    stepsize = 0\r\n    in_flds = [oid_fld, in_fld]   # OBJECTID, plus another field\r\n    a = arcpy.da.TableToNumPyArray(in_tbl, in_flds, skip_nulls=False,\r\n                                   null_value=-1)\r\n#    a = [1, 1, 2, 2, 3, 3, 3, 4, 4, 5, 5, 5, 5, 4, 4, 3, 3, 3, 2, 1]\r\n    a = a[in_fld]\r\n    out_tbl = None\r\n    return in_tbl, a, in_fld, stepsize, out_tbl\r\n\r\n\r\ndef _tool():\r\n    """"""run when script is from a tool\r\n    """"""\r\n    in_tbl = sys.argv[1]\r\n    in_fld = sys.argv[2]\r\n    stepsize = int(sys.argv[3])\r\n    out_tbl = sys.argv[4]  # output field name\r\n    #\r\n    # ---- main tool section\r\n    _, oid_fld, _, _ = fc_info(in_tbl, prn=False)  # run fc_info\r\n    #\r\n    flds = [oid_fld, in_fld]\r\n    in_arr = arcpy.da.TableToNumPyArray(in_tbl, flds, skip_nulls=False,\r\n                                   null_value=-1)\r\n    a = in_arr[in_fld]  # do stuff with array\r\n    tweet(""{!r:}"".format(a))\r\n    return in_tbl, a, in_fld, stepsize, out_tbl\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# .... final code section producing the featureclass and extendtable\r\n#\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    in_tbl, a, in_fld, stepsize, out_tbl = _demo()\r\nelse:\r\n    testing = False\r\n    in_tbl, a, in_fld, stepsize, out_tbl = _tool()\r\n\r\n\r\nmsg = """"""\r\n---- sequences ------------------------------------------------------\r\nProcessing ... {}\r\ninput field .. {}\r\nstep size  ... {} (difference between adjacent values)\r\noutput table . {}\r\n\r\n----\r\nValue : value in the field\r\nCount : number of observations in that sequence\r\nFrom_ : start location of the sequence (includes this index)\r\nTo_   : end location of the sequence (up to but not including)\r\nNoData: -1\r\n""""""\r\n\r\ntweet(msg.format(in_tbl, in_fld, stepsize, out_tbl))\r\n\r\nout = sequences(a, stepsize=0)\r\nif out_tbl not in (""#"", """", "" "", None, \'None\'):\r\n    arcpy.da.NumPyArrayToTable(out, out_tbl)\r\nprn = frmt_rec(out[:50], 0, True, False)\r\ntweet(prn)\r\n#\r\n## ---- reassemble the table for extending ----\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n'"
TableTools/Scripts/sequential_funcs.py,30,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\nsequential_funcs\r\n================\r\n\r\nScript:   sequential_funcs.py\r\n\r\nAuthor:   Dan.Patterson@carleton.ca\r\n\r\nModified: 2018-12-28\r\n\r\nPurpose :\r\n    Calculating sequential values for fields in geodatabase tables\r\n\r\nUseage :\r\n\r\nReferences\r\n----------\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/numpyarraytotable.htm>`_.\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/tabletonumpyarray.htm>`_.\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\nfrom arcpytools import fc_info, tweet\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=5, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\ndef has_nulls(a):\r\n    """"""Check to see if nulls are in the array passed from the featureclass\r\n    """"""\r\n    #\r\n    a_kind = a.dtype.kind\r\n    if a_kind == \'i\':\r\n        m = a == np.iinfo(np.int32).min\r\n    elif a_kind == \'f\':\r\n        m = np.isnan(a)\r\n    else:\r\n        m = a == None\r\n    return m\r\n\r\n\r\ndef tbl_2_nparray(in_tbl, flds):\r\n    """"""Form the TableToNumPyArray to account for nulls for various dtypes.\r\n    This is essentially a shortcut to `arcpy.da.TableToNumPyArray`\r\n\r\n    Requires\r\n    --------\r\n    `in_tbl` :\r\n        table, or featureclass table name\r\n    `flds` :\r\n        list of field names\r\n    `skip_nulls` = False :\r\n        set within function\r\n    `null_value` :\r\n        determined from the dtype of the array...\r\n        otherwise you may as well do it manually\r\n\r\n    Source\r\n    ------\r\n    arraytools, apt.py module\r\n    """"""\r\n    nulls = {\'Double\':np.nan,\r\n             \'Single\':np.nan,\r\n             \'Integer\':np.iinfo(np.int32).min,\r\n             \'OID\':np.iinfo(np.int32).min,\r\n             \'String\':""None""}\r\n    #\r\n    fld_dict = {i.name: i.type for i in arcpy.ListFields(in_tbl)}\r\n    null_dict = {f:nulls[fld_dict[f]] for f in flds}\r\n    a = arcpy.da.TableToNumPyArray(in_table=in_tbl,\r\n                                   field_names=flds,\r\n                                   skip_nulls=False,\r\n                                   null_value=null_dict)\r\n    return a\r\n\r\n\r\ndef cum_sum(a):\r\n    """"""Cumulative sum""""""\r\n    return np.nancumsum(a)\r\n\r\ndef max_diff(a):\r\n    """"""diff from max""""""\r\n    return a - np.nanmax(a)\r\n\r\n\r\ndef mean_diff(a):\r\n    """"""diff from mean""""""\r\n    return a - np.nanmean(a)\r\n\r\n\r\ndef median_diff(a):\r\n    """"""diff from median""""""\r\n    return a - np.nanmedian(a)\r\n\r\n\r\ndef min_diff(a):\r\n    """"""diff from min""""""\r\n    return a - np.nanmin(a)\r\n\r\n\r\ndef percent(a):\r\n    """"""value percentage""""""\r\n    m = has_nulls(a)\r\n    if not np.alltrue(m):\r\n        a = np.ma.MaskedArray(a, mask=m)\r\n        return (a/(np.ma.sum(a) * 1.0)) * 100.\r\n    else:\r\n        return (a/(np.sum(a) * 1.)) * 100.\r\n\r\n\r\ndef seq_diff(a):\r\n    """"""Sequential diffs""""""\r\n    return a[1:] - a[:-1]\r\n\r\n\r\ndef seq_number(a):\r\n    """"""Sequentially number the class values in a field\r\n    """"""\r\n    uni = np.unique(a)\r\n    max_sze = [len(i) for i in uni]\r\n    out = np.chararray(len(a), max_sze + 5, True)\r\n    for u in uni:\r\n        idx = np.where(a == u)[0]\r\n        cnt = 0\r\n        for i in idx:\r\n            out[i] = ""{}{:02.0f}"".format(u, cnt)\r\n            cnt += 1\r\n    return out\r\n\r\n\r\ndef val_diff(a, val):\r\n    """"""diff from a value""""""\r\n    return a - val\r\n\r\n\r\ndef z_score(a):\r\n    """"""Z-scores""""""\r\n    return mean_diff(a)/np.nanstd(a)\r\n\r\n\r\ndef seq_count(a):\r\n    """"""See `running_count` in arraytools.tools for a fuller version\r\n    """"""\r\n    idx = a.argsort(kind=\'mergesort\')\r\n    s_a = a[idx]\r\n    neq = np.where(s_a[1:] != s_a[:-1])[0] + 1\r\n    run = np.ones(a.shape, int)\r\n    run[neq[0]] -= neq[0]\r\n    run[neq[1:]] -= np.diff(neq)\r\n    out = np.empty_like(run)\r\n    out[idx] = run.cumsum()\r\n    z = np.array([""{}_{:0>3}"".format(*i) for i in list(zip(a, out))])\r\n    return z\r\n\r\n\r\ndef form_output(in_tbl, in_arr, out_fld=""Result_"", del_fld=True,\r\n                vals=None, idx=0, xtend=False):\r\n    """"""Form the output table given a field name and join field\r\n\r\n    Requires:\r\n    ---------\r\n\r\n    tbl :\r\n        input table\r\n    fld_name :\r\n        output field names, should contain OBJECTID and desired output field\r\n    vals :\r\n        values for output field\r\n    sze :\r\n        string representation of output field\r\n    idx :\r\n        index to start values from... usually 0 or 1 (ie for sequential)\r\n\r\n    """"""\r\n    desc = arcpy.da.Describe(in_tbl)\r\n    tbl_path = desc[\'path\']\r\n    oid_fld = desc[\'OIDFieldName\']   # \'OBJECTID\'\r\n    fnames = [i.name for i in arcpy.ListFields(in_tbl)]\r\n    if del_fld in (\'True\', \'true\', True, 1):\r\n        del_fld = True\r\n    else:\r\n        del_fld = False\r\n    if out_fld not in fnames:\r\n        out_fld = out_fld\r\n    elif out_fld in fnames and del_fld:\r\n        arcpy.DeleteField_management(in_tbl, out_fld)\r\n        tweet(""\\nDeleting field {}"".format(out_fld))\r\n    else:\r\n        out_fld += \'dup\'\r\n    out_fld = arcpy.ValidateFieldName(out_fld, tbl_path)\r\n    #\r\n    sze = vals.dtype.str\r\n    dt = [(\'IDs\', \'<i4\'), (out_fld, sze)]  # ie \'<f8\'\r\n    out_array = np.zeros((in_arr.shape[0],), dtype=dt)\r\n    out_array[\'IDs\'] = in_arr[oid_fld]\r\n    out_array[out_fld][idx:] = vals\r\n    if xtend:\r\n        arcpy.da.ExtendTable(in_tbl, oid_fld, out_array, \'IDs\')\r\n    return out_array\r\n\r\n\r\n# ---- Run options: _demo or from _tool\r\n#\r\ndef _demo():\r\n    """"""Code to run if in demo mode\r\n    Requires:\r\n        arcpytools fc_info, tweet\r\n    """"""\r\n    tbl = ""Table_tools.gdb/pnts_2k_normal""\r\n    in_tbl = ""/"".join(script.split(""/"")[:-2] + [tbl])\r\n    #\r\n    _, oid_fld, _, _ = fc_info(in_tbl, prn=False)  # run fc_info\r\n    #\r\n    in_fld = \'Unif\'  # \'Sequences2\'  #\'Ys\'\r\n    del_fld = True\r\n    out_fld = \'Result_fld\'\r\n    in_flds = [oid_fld, in_fld]   # OBJECTID, plus another field\r\n    in_arr = tbl_2_nparray(in_tbl, in_flds)\r\n    c = np.array([\'cumulative sum\', \'diff from max\',\r\n                  \'diff from mean\', \'diff from median\',\r\n                  \'diff from min\', \'diff from value\',\r\n                  \'percent\', \'sequential diff\',\r\n                  \'sequential number\',\r\n                  \'z_score\'])\r\n    func = \'percent\'  #np.random.choice(c)\r\n    xtend = False\r\n    val = None\r\n    return in_tbl, in_arr, in_fld, out_fld, del_fld, func, xtend, val\r\n\r\ndef _tool():\r\n    """"""run when script is from a tool\r\n    """"""\r\n    in_tbl = sys.argv[1]\r\n    in_fld = sys.argv[2]\r\n    func = sys.argv[3]\r\n    out_fld = sys.argv[4]  # output field name\r\n    del_fld = sys.argv[5]\r\n    val = sys.argv[6]\r\n    #\r\n    # ---- main tool section\r\n    _, oid_fld, _, _ = fc_info(in_tbl, prn=False)  # run fc_info\r\n    #\r\n    flds = [oid_fld, in_fld]\r\n    in_arr = tbl_2_nparray(in_tbl, flds)\r\n    tweet(""{!r:}"".format(in_arr))\r\n    xtend = True\r\n    return in_tbl, in_arr, in_fld, out_fld, del_fld, func, xtend, val\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# .... final code section producing the featureclass and extendtable\r\n#\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    in_tbl, in_arr, in_fld, out_fld, del_fld, func, xtend, val = _demo()\r\nelse:\r\n    testing = False\r\n    in_tbl, in_arr, in_fld, out_fld, del_fld, func, xtend, val = _tool()\r\n\r\na = in_arr[in_fld]  # do stuff with array\r\n\r\nif func == \'cumulative sum\':\r\n    result = cum_sum(a)  # sequential diff call\r\n    idx = 0\r\nelif func == \'diff from max\':\r\n    result = max_diff(a)\r\n    idx = 0\r\nelif func == \'diff from mean\':\r\n    result = mean_diff(a)\r\n    idx = 0\r\nelif func == \'diff from median\':\r\n    result = median_diff(a)\r\n    idx = 0\r\nelif func == \'diff from min\':\r\n    result = min_diff(a)\r\n    idx = 0\r\nelif func == \'diff from value\':\r\n    idx = 0\r\n    val_orig = val\r\n    try:    val = int(val)\r\n    except:    val = 0\r\n    try:    val = float(val)\r\n    except:    val = 0\r\n    finally:\r\n        frmt = ""Difference value entered... {!r:}... Value used... {!r:}""\r\n        tweet(frmt.format(val_orig, val))\r\n        pass\r\n    result = val_diff(a, val)\r\nelif func == \'percent\':\r\n    result = percent(a)\r\n    idx = 0\r\nelif func == \'sequential diff\':\r\n    result = seq_diff(a)  # sequential diff call\r\n    idx = 1\r\nelif func == \'sequential number\':\r\n    result = seq_number(a)\r\n    idx = 0\r\nelif func == \'z_score\':\r\n    result = z_score(a)\r\n    idx = 0\r\nelse:\r\n    result = seq_diff(a)\r\n    idx = 1\r\n#\r\n# ---- reassemble the table for extending ----\r\nout_array = form_output(in_tbl,\r\n                        in_arr,\r\n                        out_fld=out_fld,\r\n                        del_fld=del_fld,\r\n                        vals=result,\r\n                        idx=idx,\r\n                        xtend=xtend)\r\nmsg = """"""\r\nProcessing... {}\r\nfunction..... {}\r\ninput field.. {}\r\noutput field. {}\r\n""""""\r\n\r\ntweet(msg.format(in_tbl, func, in_fld, out_fld))\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n#    tbl = r""C:\\Git_Dan\\arraytools\\array_tools_testing\\Data\\pnts_2000.npy""\r\n'"
TableTools/Scripts/sequential_funcs_txt.py,13,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\nsequential_funcs_txt\r\n====================\r\n\r\nScript:   sequential_funcs_txt.py\r\n\r\nAuthor:   Dan.Patterson@carleton.ca\r\n\r\nModified: 2018-06-04\r\n\r\nPurpose :\r\n    Calculating sequential values for fields in geodatabase tables\r\n\r\nUseage :\r\n\r\nReferences\r\n----------\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/numpyarraytotable.htm>`_.\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/tabletonumpyarray.htm>`_.\r\n---------------------------------------------------------------------\r\n\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\nfrom arcpytools import fc_info, tweet\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\ndef has_nulls(a):\r\n    """"""Check to see if nulls are in the array passed from the featureclass\r\n    """"""\r\n    #\r\n    a_kind = a.dtype.kind\r\n    if a_kind == \'i\':\r\n        m = a == np.iinfo(np.int32).min\r\n    elif a_kind == \'f\':\r\n        m = np.isnan(a)\r\n    else:\r\n        m = a == None\r\n    return m\r\n\r\n\r\ndef tbl_2_nparray(in_tbl, flds):\r\n    """"""Form the TableToNumPyArray to account for nulls for various dtypes.\r\n    This is essentially a shortcut to `arcpy.da.TableToNumPyArray`\r\n\r\n    Requires\r\n    --------\r\n    `in_tbl` :\r\n        table, or featureclass table name\r\n    `flds` :\r\n        list of field names\r\n    `skip_nulls` = False :\r\n        set within function\r\n    `null_value` :\r\n        determined from the dtype of the array...\r\n        otherwise you may as well do it manually\r\n\r\n    Source\r\n    ------\r\n    arraytools, apt.py module\r\n    """"""\r\n    nulls = {\'Double\':np.nan,\r\n             \'Integer\':np.iinfo(np.int32).min,\r\n             \'OID\':np.iinfo(np.int32).min,\r\n             \'String\':""None""}\r\n    #\r\n    fld_dict = {i.name: i.type for i in arcpy.ListFields(in_tbl)}\r\n    null_dict = {f:nulls[fld_dict[f]] for f in flds}\r\n    a = arcpy.da.TableToNumPyArray(in_table=in_tbl,\r\n                                   field_names=flds,\r\n                                   skip_nulls=False,\r\n                                   null_value=null_dict)\r\n    return a\r\n\r\n\r\ndef seq_text(a):\r\n    """"""Sequentially number the text class values in a field\r\n    """"""\r\n    uni, counts = np.unique(a, False, False, True)\r\n    max_sze = max([len(i) for i in uni])\r\n    max_cnts = max([len(str(i)) for i in counts])\r\n    frmt = ""{}_{:0{}.0f}""\r\n    out = np.chararray(len(a), max_sze + 5, True)\r\n    for u in uni:\r\n        idx = np.where(a == u)[0]\r\n        cnt = 0\r\n        for i in idx:\r\n            out[i] = frmt.format(u, cnt, max_cnts)\r\n            cnt += 1\r\n    return out\r\n\r\n\r\ndef form_output(in_tbl, in_arr, out_fld=""Result_"", del_fld=True,\r\n                vals=None, idx=0, xtend=False):\r\n    """"""Form the output table given a field name and join field\r\n\r\n    Requires:\r\n    ---------\r\n\r\n    tbl :\r\n        input table\r\n    fld_name :\r\n        output field names, should contain OBJECTID and desired output field\r\n    vals :\r\n        values for output field\r\n    sze :\r\n        string representation of output field\r\n    idx :\r\n        index to start values from... usually 0 or 1 (ie for sequential)\r\n\r\n    """"""\r\n    desc = arcpy.da.Describe(in_tbl)\r\n    tbl_path = desc[\'path\']\r\n    oid_fld = desc[\'OIDFieldName\']   # \'OBJECTID\'\r\n    fnames = [i.name for i in arcpy.ListFields(in_tbl)]\r\n    if del_fld in (\'True\', \'true\', True, 1):\r\n        del_fld = True\r\n    else:\r\n        del_fld = False\r\n    if out_fld not in fnames:\r\n        out_fld = out_fld\r\n    elif out_fld in fnames and del_fld:\r\n        arcpy.DeleteField_management(in_tbl, out_fld)\r\n        tweet(""\\nDeleting field {}"".format(out_fld))\r\n    else:\r\n        out_fld += \'dup\'\r\n    out_fld = arcpy.ValidateFieldName(out_fld, tbl_path)\r\n    #\r\n    sze = vals.dtype.str\r\n    dt = [(\'IDs\', \'<i4\'), (out_fld, sze)]  # ie \'<f8\'\r\n    out_array = np.zeros((in_arr.shape[0],), dtype=dt)\r\n    out_array[\'IDs\'] = in_arr[oid_fld]\r\n    out_array[out_fld][idx:] = vals\r\n    if xtend:\r\n        arcpy.da.ExtendTable(in_tbl, oid_fld, out_array, \'IDs\')\r\n    return out_array\r\n\r\n\r\n# ---- Run options: _demo or from _tool\r\n#\r\ndef _demo():\r\n    """"""Code to run if in demo mode\r\n    Requires:\r\n        arcpytools fc_info, tweet\r\n    """"""\r\n    tbl = ""Table_tools.gdb/pnts_2k_normal""\r\n    in_tbl = ""/"".join(script.split(""/"")[:-2] + [tbl])\r\n    #\r\n    _, oid_fld, _, _ = fc_info(in_tbl, prn=False)  # run fc_info\r\n    #\r\n    in_fld = \'Text01\'  # \'Sequences2\'  #\'Ys\'\r\n    del_fld = True\r\n    out_fld = \'Result_fld\'\r\n    in_flds = [oid_fld, in_fld]   # OBJECTID, plus another field\r\n    in_arr = tbl_2_nparray(in_tbl, in_flds)\r\n    # c = np.array([\'sequential text\'])\r\n    func = \'sequential text\'\r\n    xtend = False\r\n    return in_tbl, in_arr, in_fld, out_fld, del_fld, func, xtend\r\n\r\ndef _tool():\r\n    """"""run when script is from a tool\r\n    """"""\r\n    in_tbl = sys.argv[1]\r\n    in_fld = sys.argv[2]\r\n    func = sys.argv[3]\r\n    out_fld = sys.argv[4]  # output field name\r\n    del_fld = sys.argv[5]\r\n    #\r\n    # ---- main tool section\r\n    _, oid_fld, _, _ = fc_info(in_tbl, prn=False)  # run fc_info\r\n    #\r\n    flds = [oid_fld, in_fld]\r\n    in_arr = tbl_2_nparray(in_tbl, flds)\r\n    tweet(""{!r:}"".format(in_arr))\r\n    xtend = True\r\n    return in_tbl, in_arr, in_fld, out_fld, del_fld, func, xtend\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# .... final code section producing the featureclass and extendtable\r\n#\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    in_tbl, in_arr, in_fld, out_fld, del_fld, func, xtend = _demo()\r\nelse:\r\n    testing = False\r\n    in_tbl, in_arr, in_fld, out_fld, del_fld, func, xtend = _tool()\r\n\r\na = in_arr[in_fld]  # do stuff with array\r\n\r\nif func == \'sequential text\':\r\n    result = seq_text(a)\r\n    idx = 0\r\nelse:\r\n    result = seq_text(a)\r\n    idx = 1\r\n#\r\n# ---- reassemble the table for extending ----\r\nout_array = form_output(in_tbl,\r\n                        in_arr,\r\n                        out_fld=out_fld,\r\n                        del_fld=del_fld,\r\n                        vals=result,\r\n                        idx=idx,\r\n                        xtend=xtend)\r\nmsg = """"""\r\nProcessing... {}\r\nfunction..... {}\r\ninput field.. {}\r\noutput field. {}\r\n""""""\r\n\r\ntweet(msg.format(in_tbl, func, in_fld, out_fld))\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n'"
TableTools/Scripts/split_field.py,10,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   .py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2017-xx-xx\r\n:Purpose:  tools for working with numpy arrays\r\n:Useage:\r\n:\r\n:References:\r\n:\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\n\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\ndef tweet(msg):\r\n    """"""Print a message for both arcpy and python.\r\n    : msg - a text message\r\n    """"""\r\n    m = ""\\n{}\\n"".format(msg)\r\n    arcpy.AddMessage(m)\r\n    print(m)\r\n    print(arcpy.GetMessages())\r\n\r\n\r\ndef sp(a, sep="",""):\r\n    """""" split stuff""""""\r\n    name = a.dtype.names\r\n    if name is not None:\r\n        a = a[name[0]]\r\n    shp = a.shape[0]\r\n    a0 = np.char.partition(a, sep="", "")\r\n    n_max = np.max([len(i) for i in a0])\r\n    out = [a0[:, 0]]\r\n    a0 = a0[:, -1]\r\n    for i in range(n_max+1):\r\n        a0 = np.char.partition(a0, \', \')\r\n        out.append(a0[:, 0])\r\n        a0 = a0[:, -1]\r\n    b = np.array(list(zip(out)))\r\n    b = b.squeeze().T\r\n    cnts = np.char.count(a, \', \')\r\n    f = np.empty((shp, max(cnts+1)), dtype=np.unicode)\r\n    for i in range(shp):\r\n        c = cnts[i]\r\n        f[i, :c+1] = [j.strip() for j in a[i].split(\',\')]\r\n    return f\r\n\r\n\r\n# ---- Run options: _demo or from _tool\r\n#\r\ndef _demo():\r\n    """"""Code to run if in demo mode\r\n    """"""\r\n    in_tbl = r""C:\\Git_Dan\\arraytools\\Data\\numpy_demos.gdb\\sample_10k""\r\n    in_fld = \'Test\'\r\n    a = arcpy.da.TableToNumPyArray(in_tbl, in_fld)\r\n#    a = np. array([\'1, 2, 3, 4, 5\', \'a, b, c\', \'6, 7, 8, 9\',\r\n#                   \'d, e, f, g, h\', \'10, 11, 12, 13\'])\r\n    return a\r\n\r\n\r\ndef _tool():\r\n    """"""run when script is from a tool\r\n    """"""\r\n    in_tbl = sys.argv[1]\r\n    in_flds = sys.argv[2]\r\n    out_fld = sys.argv[3]\r\n\r\n    if \';\' in in_flds:\r\n        in_flds = in_flds.split(\';\')\r\n    else:\r\n        in_flds = [in_flds]\r\n\r\n    desc = arcpy.da.Describe(in_tbl)\r\n    tbl_path = desc[\'path\']\r\n    fnames = [i.name for i in arcpy.ListFields(in_tbl)]\r\n    if out_fld in fnames:\r\n        out_fld += \'dup\'\r\n    out_fld = arcpy.ValidateFieldName(out_fld, tbl_path)\r\n    args = [in_tbl, in_flds, out_fld, tbl_path]\r\n    msg = ""in_tbl {}\\nin_fld {}\\nout_fld  {}\\ntbl_path  {}"".format(*args)\r\n    tweet(msg)\r\n    oid = \'OBJECTID\'\r\n    vals = [oid] + in_flds\r\n    arr = arcpy.da.TableToNumPyArray(in_tbl, vals)\r\n    tweet(""{!r:}"".format(arr))\r\n    arcpy.da.ExtendTable(in_tbl, \'OBJECTID\', arr, \'OBJECTID\')\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n    a = _demo()\r\n'"
TableTools/Scripts/strided_funcs.py,28,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\nstrided_funcs\r\n=============\r\n\r\nScript :   strided_funcs.py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified : 2018-07-28\r\n\r\nPurpose:  tools for working with numpy arrays using srided functions\r\n\r\nUseage :\r\n\r\nGenerate sequence\r\n\r\ndef seq(N=100, diff_val=1):\r\n    \'\'\'generate a sequence in the range 0, N\r\n    +ve or -ve values in the range of 1 are added\r\n    to the values\r\n    \'\'\'\r\n    neg = np.random.random()*-diff_val\r\n    pos = np.random.random()*diff_val\r\n    a = np.arange(0, N, 1) + \\\r\n       [[np.random.random(), -np.random.random()][np.random.random <= 0.5]\r\n       for i in range(0, N)]\r\n    return a\r\n\r\nReferences\r\n----------\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/numpyarraytotable.htm>`_.\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/tabletonumpyarray.htm>`_.\r\n---------------------------------------------------------------------\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nfrom numpy.lib.stride_tricks import as_strided\r\nfrom arcpytools import fc_info, tweet  #, frmt_rec, _col_format\r\nimport arcpy\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\ndef has_nulls(a):\r\n    """"""Check to see if nulls are in the array passed from the featureclass\r\n    """"""\r\n    #\r\n    a_kind = a.dtype.kind\r\n    if a_kind == \'i\':\r\n        m = a == np.iinfo(np.int32).min\r\n    elif a_kind == \'f\':\r\n        m = np.isnan(a)\r\n    else:\r\n        m = a == None\r\n    return m\r\n\r\n\r\ndef stride(a, win=(3, 3), stepby=(1,1)):\r\n    """"""Provide a 2D sliding/moving array view.  There is no edge\r\n    correction for outputs.  From arraytools.tools\r\n    """"""\r\n\r\n    err = """"""Array shape, window and/or step size error.\r\n    Use win=(3,) with stepby=(1,) for 1D array\r\n    or win=(3,3) with stepby=(1,1) for 2D array\r\n    or win=(1,3,3) with stepby=(1,1,1) for 3D\r\n    ----    a.ndim != len(win) != len(stepby) ----\r\n    """"""\r\n    assert (a.ndim == len(win)) and (len(win) == len(stepby)), err\r\n    shape = np.array(a.shape)  # array shape (r, c) or (d, r, c)\r\n    win_shp = np.array(win)    # window      (3, 3) or (1, 3, 3)\r\n    ss = np.array(stepby)      # step by     (1, 1) or (1, 1, 1)\r\n    newshape = tuple(((shape - win_shp) // ss) + 1) + tuple(win_shp)\r\n    newstrides = tuple(np.array(a.strides) * ss) + a.strides\r\n    a_s = as_strided(a, shape=newshape, strides=newstrides).squeeze()\r\n    return a_s\r\n\r\n\r\ndef tbl_2_nparray(in_tbl, flds):\r\n    """"""Form the TableToNumPyArray to account for nulls for various dtypes.\r\n    This is essentially a shortcut to `arcpy.da.TableToNumPyArray`\r\n\r\n    Requires\r\n    --------\r\n    `in_tbl` :\r\n        table, or featureclass table name\r\n    `flds` :\r\n        list of field names\r\n    `skip_nulls` = False :\r\n        set within function\r\n    `null_value` :\r\n        determined from the dtype of the array...\r\n        otherwise you may as well do it manually\r\n\r\n    Source\r\n    ------\r\n    arraytools, apt.py module\r\n    """"""\r\n    nulls = {\'Double\':np.nan,\r\n             \'Integer\':np.iinfo(np.int32).min,\r\n             \'OID\':np.iinfo(np.int32).min,\r\n             \'String\':None}\r\n    #\r\n    fld_dict = {i.name: i.type for i in arcpy.ListFields(in_tbl)}\r\n    null_dict = {f:nulls[fld_dict[f]] for f in flds}\r\n    a = arcpy.da.TableToNumPyArray(in_table=in_tbl,\r\n                                   field_names=flds,\r\n                                   skip_nulls=False,\r\n                                   null_value=null_dict)\r\n    return a\r\n\r\n\r\ndef strided_func(a, step=3, func=\'mean\'):\r\n    """""" strides a numeric array in preparation for numeric calculations.\r\n\r\n    `a` : array of floats form most functions\r\n        numeric array with shape (N,)\r\n    `step` : integer\r\n        the step window size such as a 3, 5 or 7 moving window\r\n    """"""\r\n    def mode(b, axis=None):\r\n        """"""Calculate the modal value and optional count\r\n        """"""\r\n        modes, cnts = np.unique(b, return_counts=True, axis=axis)\r\n        idx = np.argmax(cnts)\r\n        return modes[idx]  #  counts[index]\r\n\r\n    start = step // 2  # integer division to determine the start for filling\r\n    b = stride(a, win=(step,), stepby=(1,))\r\n    out_array = np.zeros((a.shape[0],), dtype=a.dtype)\r\n    out_array.fill(np.nan)\r\n    if func == \'mean\':\r\n        out_array[start: -start] = np.nanmean(b, axis=1)\r\n    elif func == \'median\':\r\n        out_array[start: -start] = np.nanmedian(b, axis=1)\r\n    elif func == \'min\':\r\n        out_array[start: -start] = np.nanmin(b, axis=1)\r\n    elif func == \'max\':\r\n        out_array[start: -start] = np.nanmax(b, axis=1)\r\n    elif func == \'sum\':\r\n        out_array[start: -start] = np.nansum(b, axis=1)\r\n    elif func == \'mode\':\r\n        out_array[start: -start] = [mode(i) for i in b]\r\n    elif func == \'trend\':  # returns -1, 0, 1 for down, flat, up\r\n        out_array[start: -start] = [mode(np.sign(np.diff(i))) for i in b]\r\n    return out_array\r\n\r\n\r\ndef form_output(in_tbl, in_arr, out_fld=""Result_"",\r\n                vals=None, idx=0, xtend=False):\r\n    """"""Form the output table given a field name and join field\r\n\r\n    Requires:\r\n    ---------\r\n\r\n    tbl :\r\n        input table\r\n    fld_name :\r\n        output field names, should contain OBJECTID and desired output field\r\n    vals :\r\n        values for output field\r\n    sze :\r\n        string representation of output field\r\n    idx :\r\n        index to start values from... usually 0 or 1 (ie for sequential)\r\n\r\n    """"""\r\n    desc = arcpy.da.Describe(in_tbl)\r\n    tbl_path = desc[\'path\']\r\n    oid_fld = desc[\'OIDFieldName\']   # \'OBJECTID\'\r\n    del_fld = True\r\n    fnames = [i.name for i in arcpy.ListFields(in_tbl)]\r\n    if del_fld in (\'True\', \'true\', True, 1):\r\n        del_fld = True\r\n    else:\r\n        del_fld = False\r\n    if out_fld not in fnames:\r\n        out_fld = out_fld\r\n    elif out_fld in fnames and del_fld:\r\n        arcpy.DeleteField_management(in_tbl, out_fld)\r\n        tweet(""\\nDeleting field {}"".format(out_fld))\r\n    else:\r\n        out_fld += \'dup\'\r\n    out_fld = arcpy.ValidateFieldName(out_fld, tbl_path)\r\n    #\r\n    sze = vals.dtype.str\r\n    dt = [(\'IDs\', \'<i4\'), (out_fld, sze)]  # ie \'<f8\'\r\n    out_array = np.zeros((in_arr.shape[0],), dtype=dt)\r\n    out_array[\'IDs\'] = in_arr[oid_fld]\r\n    out_array[out_fld][idx:] = vals\r\n    if xtend:\r\n        arcpy.da.ExtendTable(in_tbl, oid_fld, out_array, \'IDs\')\r\n    return out_array\r\n\r\n\r\n# ---- Run options: _demo or from _tool\r\n#\r\ndef _demo():\r\n    """"""Code to run if in demo mode\r\n    """"""\r\n    tbl = ""Table_tools.gdb/pnts_2k_normal""\r\n    in_tbl = ""/"".join(script.split(""/"")[:-2] + [tbl])\r\n    #\r\n    _, oid_fld, _, _ = fc_info(in_tbl, prn=False)  # run fc_info\r\n    #\r\n    in_fld = \'Sequences\'  # \'Norm\'  # \'Ys\'\r\n    out_fld = \'Result_fld\'\r\n    in_flds = [oid_fld, in_fld]   # OBJECTID, plus another field\r\n    in_arr = tbl_2_nparray(in_tbl, in_flds)\r\n    func = \'mean\'  #np.random.choice(c)\r\n    win_size = 5\r\n    xtend = False\r\n    return in_tbl, in_arr, in_fld, out_fld, func, win_size, xtend\r\n\r\n\r\ndef _tool():\r\n    """"""run when script is from a tool\r\n    """"""\r\n    in_tbl = sys.argv[1]\r\n    in_fld = sys.argv[2]\r\n    out_fld = sys.argv[3]  # output field name\r\n    func = sys.argv[4]\r\n    win_size = int(sys.argv[5])\r\n\r\n    # ---- main tool section\r\n    desc = arcpy.da.Describe(in_tbl)\r\n    # ---- main tool section\r\n    _, oid_fld, _, _ = fc_info(in_tbl, prn=False)  # run fc_info\r\n    #\r\n    flds = [oid_fld, in_fld]\r\n    tbl_path = desc[\'path\']\r\n    fnames = [i.name for i in arcpy.ListFields(in_tbl)]\r\n    if out_fld in fnames:\r\n        out_fld += \'dup\'\r\n    out_fld = arcpy.ValidateFieldName(out_fld, tbl_path)\r\n    args = [in_tbl, in_fld, out_fld, tbl_path]\r\n    msg = ""in_tbl {}\\nin_fld {}\\nout_fld  {}\\ntbl_path  {}"".format(*args)\r\n    tweet(msg)\r\n    #\r\n    # ---- call section for processing function\r\n    #\r\n    _, oid_fld, _, _ = fc_info(in_tbl, prn=False)  # run fc_info\r\n    #\r\n    # ---- remove the selection by calling the table\r\n    in_tbl  = desc[\'catalogPath\']\r\n    #\r\n    flds = [oid_fld, in_fld]\r\n    in_arr = tbl_2_nparray(in_tbl, flds)\r\n    tweet(""{!r:}"".format(in_arr))\r\n    xtend = True\r\n    return in_tbl, in_arr, in_fld, out_fld, func, win_size, xtend\r\n\r\n# ----------------------------------------------------------------------\r\n# .... final code section producing the featureclass and extendtable\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    in_tbl, in_arr, in_fld, out_fld, func, win_size, xtend = _demo()\r\nelse:\r\n    testing = False\r\n    in_tbl, in_arr, in_fld, out_fld, func, win_size, xtend = _tool()\r\n#\r\nif not testing:\r\n    tweet(\'Some message here...\')\r\n\r\n# ---- reassemble the table for extending ----\r\n\r\na = in_arr[in_fld]\r\nresult = strided_func(a, step=win_size, func=func)\r\n\r\nout_array = form_output(in_tbl,\r\n                        in_arr,\r\n                        out_fld=out_fld,\r\n                        vals=result,\r\n                        idx=0,\r\n                        xtend=xtend)\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n#    a = _demo()\r\n'"
TableTools/Scripts/strip_stuff.py,9,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   strip_stuff.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2018-02-10\r\n:Purpose:  tools for working with numpy arrays\r\n:Useage:\r\n:\r\n:References:\r\n:\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\npunc = [\'!\', \'""\', \'#\', \'$\', \'%\', \'&\', ""\'"", \'(\', \')\', \'*\', \'+\', \',\', \'-\', \'.\',\r\n        \'/\', \':\', \';\', \'<\', \'=\', \'>\', \'?\', \'@\', \'[\', \'\\\\\', \']\', \'^\', \'_\', \'`\',\r\n        \'{\', \'|\', \'}\', \'~\']\r\nwhitesp = [\' \', \'\\t\', \'\\n\', \'\\r\', \'\\x0b\', \'\\x0c\']\r\n\r\n\r\ndef tweet(msg):\r\n    """"""Print a message for both arcpy and python.\r\n    : msg - a text message\r\n    """"""\r\n    m = ""\\n{}\\n"".format(msg)\r\n    arcpy.AddMessage(m)\r\n    print(m)\r\n\r\n\r\ndef clean_fld(a, strip_list, new_value=""""):\r\n    """"""clean the arrays if needed""""""\r\n    tmp = np.copy(a)\r\n    for i in strip_list:\r\n        tmp = np.char.replace(tmp, str(i), new_value)\r\n    cleaned = tmp\r\n    return cleaned\r\n\r\n\r\n# ---- Run options: _demo or from _tool\r\n#\r\ndef _demo():\r\n    """"""Code to run if in demo mode\r\n    """"""\r\n    a = np. array([\'1, 2, 3, 4, 5\', \'a, b, c\', \'6, 7, 8, 9\',\r\n                   \'d, e, f, g, h\', \'10, 11, 12, 13\'])\r\n    cleaned = clean_fld(a, punc)\r\n    return a, cleaned\r\n\r\n\r\ndef _tool():\r\n    """"""run when script is from a tool\r\n    """"""\r\n    in_tbl = sys.argv[1]\r\n    in_fld = sys.argv[2]\r\n    out_fld = sys.argv[3]\r\n    all_punc = sys.argv[4]\r\n    all_white = sys.argv[5]\r\n    all_extra = sys.argv[6]\r\n    all_others = sys.argv[7]\r\n\r\n    a0 = [[], punc][all_punc in (True, \'True\', \'true\')]\r\n    a1 = [[], whitesp][all_white in (True, \'True\', \'true\')]\r\n    if len(all_others) == 1:\r\n        a2 = list(all_others)\r\n    elif len(all_others) > 1:\r\n        if "";"" in all_others:\r\n            a2 = all_others.replace("";"", ""xx"")\r\n            a2 = a2.split(\'xx\')[:-1]\r\n    else:\r\n        a2 = []\r\n    #\r\n    strip_list = a0 + a1 + a2\r\n    desc = arcpy.da.Describe(in_tbl)\r\n    tbl_path = desc[\'path\']\r\n    is_gdb_tbl = tbl_path[-4:] == \'.gdb\'\r\n    fnames = [i.name for i in arcpy.ListFields(in_tbl)]\r\n    if out_fld in fnames:\r\n        out_fld += \'dup\'\r\n    out_fld = arcpy.ValidateFieldName(out_fld, tbl_path)\r\n    args = [in_tbl, in_fld, out_fld, tbl_path]\r\n    msg = ""in_tbl {}\\nin_fld {}\\nout_fld  {}\\ntbl_path  {}"".format(*args)\r\n    tweet(msg)\r\n    tweet(""Removing .... {}"".format(strip_list))\r\n    oid = \'OBJECTID\'\r\n    vals = [oid, in_fld]\r\n    #\r\n    # ---- do the work\r\n    #\r\n    arr = arcpy.da.TableToNumPyArray(in_tbl, vals)\r\n    tweet(""{!r:}"".format(arr))\r\n    a0 = arr[in_fld]\r\n    #\r\n    cleaned = clean_fld(a0, strip_list)  # punc\r\n    #\r\n    if all_extra in (True, \'True\', \'true\'):\r\n        sps = [\'    \',  \'   \', \'  \']\r\n        for i in sps:\r\n            cleaned = np.char.replace(cleaned, i, "" "")\r\n    sze = cleaned.dtype.str\r\n    dt = [(\'IDs\', \'<i8\'), (out_fld, sze)]\r\n    out_array = np.empty((arr.shape[0], ), dtype=dt)\r\n    out_array[\'IDs\'] = np.arange(1, arr.shape[0] + 1)\r\n    out_array[out_fld] = cleaned\r\n    #\r\n    #\r\n    arcpy.da.ExtendTable(in_tbl, \'OBJECTID\', out_array, \'IDs\')\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# .... final code section producing the featureclass and extendtable\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    arrs, c = _demo()\r\n    frmt = ""Result...\\n{}""\r\n    print(frmt.format(c))\r\nelse:\r\n    testing = False\r\n    _tool()\r\n#\r\nif not testing:\r\n    print(\'Concatenation done...\')\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n    a, c = _demo()\r\n'"
TableTools/Scripts/table2numpy.py,19,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\ntable2numpyarray\r\n================\r\n\r\nScript :   table2numpyarray.py\r\n\r\nAuthor:   Dan.Patterson@carleton.ca\r\n\r\nModified: 2018-03-18\r\n\r\nPurpose:  tools for working with numpy arrays\r\n\r\nUseage:\r\n\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\ndef tweet(msg):\r\n    """"""Print a message for both arcpy and python.\r\n    : msg - a text message\r\n    """"""\r\n    m = ""\\n{}\\n"".format(msg)\r\n    arcpy.AddMessage(m)\r\n    print(m)\r\n\r\n\r\n# ---- Run options: _demo or from _tool\r\n#\r\ndef _demo():\r\n    """"""Code to run if in demo mode\r\n    """"""\r\n    a = np.array([\'1, 2, 3, 4, 5\', \'a, b, c\', \'6, 7, 8, 9\',\r\n                  \'d, e, f, g, h\', \'10, 11, 12, 13\'])\r\n    return a\r\n\r\n\r\ndef _tool():\r\n    """"""run when script is from a tool\r\n    """"""\r\n    in_tbl = sys.argv[1]\r\n    in_flds = sys.argv[2]\r\n    out_folder = sys.argv[3]  # output folder name\r\n    out_filename = sys.argv[4]\r\n    out_name = ""\\\\"".join([out_folder, out_filename])\r\n    # ---- main tool section\r\n    desc = arcpy.da.Describe(in_tbl)\r\n    args = [in_tbl, in_flds, out_name]\r\n    msg = ""Input table.. {}\\nfields...\\n{}\\nOutput arr  {}"".format(*args)\r\n    tweet(msg)\r\n    #\r\n    # ---- call section for processing function\r\n    #\r\n    oid = \'OBJECTID\'\r\n    in_flds = in_flds.split("";"")\r\n    if oid in in_flds:\r\n        vals = in_flds\r\n    else:\r\n        vals = [oid] + in_flds\r\n    #\r\n    # ---- create the field dictionary\r\n    f_info = np.array([[i.name, i.type] for i in arcpy.ListFields(in_tbl)])\r\n    f_dict = {\'OBJECTID\': -1}\r\n    for f in in_flds:\r\n        if f in f_info[:, 0]:\r\n            n, t = f_info[f_info[:, 0] == f][0]\r\n            if t in (\'Integer\', \'Short\', \'Long\'):\r\n                t = np.iinfo(np.int32).min\r\n            elif t in (\'Double\', \'Float\'):\r\n                t = np.nan\r\n            elif t in (\'String\', \'Text\'):\r\n                t = np.unicode_(None)\r\n            else:\r\n                t = np.iinfo(np.int32).min\r\n            f_dict[n] = t\r\n    # ---- where_clause= skip_nulls=  null_value=)\r\n    arr = arcpy.da.TableToNumPyArray(in_tbl, vals, ""#"", False, f_dict)\r\n    #\r\n    np.save(out_name, arr)\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# .... final code section producing the featureclass and extendtable\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    arrs = _demo()\r\n    frmt = ""Result...\\n{}""\r\n    print(frmt.format(arrs))\r\nelse:\r\n    testing = False\r\n    _tool()\r\n#\r\nif not testing:\r\n    print(\'Demo done...\')\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n    a = _demo()\r\n\r\n#in_tbl = \'C:\\\\GIS\\\\A_Tools_scripts\\\\Numpy_arc\\\\Numpy_arc.gdb\\\\sample_1000\'\r\n#arr = arcpy.da.TableToNumPyArray(in_tbl, \'*""\'\r\n#uni = [i[0] for i in np.unique(arr)]\r\n#uni = [\'A\', \'B\', \'C\', \'D\']\r\n#\r\n#np.array(uni)  # array([\'A\', \'B\', \'C\', \'D\'],  dtype=\'<U1\')\r\n#\r\n##uni_u = [a.encode(\'utf8\') for a in uni]\r\n#\r\n##np.array(uni_u) #  array([b\'A\', b\'B\', b\'C\', b\'D\'],  dtype=\'|S1\')\r\n#\r\n## ---- compressed file demo ----\r\n#in_npy = r\'C:\\GIS\\A_Tools_scripts\\Numpy_arc\\Data\\sample_1000b.npy\'\r\n#arr = np.load(in_npy)\r\n#\r\n#out_npy = r\'C:\\GIS\\A_Tools_scripts\\Numpy_arc\\Data\\sample_1000b_compressed\'\r\n#np.savez_compressed(out_npy, arr)\r\n#\r\n#arr_z = np.load(out_npy + r\'.npz\')  # ---- note added the *.npz extension\r\n#arr_z.keys()  # => [\'arr_0\']\r\n#arr_2 = arr_z[\'arr_0\']\r\n#np.all(arr == arr_2)  # ---- both arrays equal? ==> True\r\n#\r\n## another\r\n#a = np.random.randint(0, 100, size=(2000, 1500)) * 1.0\r\n#\r\n#a.shape  # => (2000, 1500) a.dtype => dtype(\'float64\')\r\n#out_npy = r\'C:\\GIS\\A_Tools_scripts\\Numpy_arc\\Data\\float_2000x1500_comp.npz\'\r\n#\r\n#np.savez_compressed(out_npy, a)\r\n#r = arcpy.NumPyArrayToRaster(a)\r\n#\r\n#r.save(r\'C:\\GIS\\A_Tools_scripts\\Numpy_arc\\Data\\float_1500x2000.tif\')\r\n'"
TableTools/Scripts/table_rotate.py,12,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\ntable_rotate\r\n============\r\n\r\nScript :   table_rotate.py\r\n\r\nAuthor :   Dan.Patterson@carleton.ca\r\n\r\nModified : 2018-12-30\r\n\r\nPurpose:  tools for working with numpy arrays\r\nUseage:\r\n\r\nReferences:\r\n\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nimport numpy.lib.recfunctions as rfn\r\nimport arcpy\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\r\n      \'float\': \'{: 0.3f}\'.format}\r\n\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\ndef tweet(msg):\r\n    """"""Produce a message for both arcpy and python\r\n    : msg - a text message\r\n    """"""\r\n    m = ""\\n{}\\n"".format(msg)\r\n    arcpy.AddMessage(m)\r\n    print(m)\r\n    print(arcpy.GetMessages())\r\n\r\n\r\ndef null_dict(flds):\r\n    """"""Produce a null dictionary from a list of fields\r\n    These must be field objects and not just their name.\r\n    """"""\r\n    dump_flds = [""OBJECTID"",""Shape_Length"", ""Shape_Area"", ""Shape""]\r\n    flds_oth = [f for f in flds\r\n                if f.name not in dump_flds]\r\n#    oid_geom = [\'OBJECTID\', \'SHAPE@X\', \'SHAPE@Y\']\r\n    nulls = {\'Double\':np.nan,\r\n             \'Single\':np.nan,\r\n             \'Short\':np.iinfo(np.int16).min,\r\n             \'SmallInteger\':np.iinfo(np.int16).min,\r\n             \'Long\':np.iinfo(np.int32).min,\r\n             \'Float\':np.nan,\r\n             \'Integer\':np.iinfo(np.int32).min,\r\n             \'String\':str(None),\r\n             \'Text\':str(None)}\r\n    fld_dict = {i.name: i.type for i in flds_oth}\r\n    nulls = {f.name:nulls[fld_dict[f.name]] for f in flds_oth}\r\n    return nulls\r\n\r\n\r\ndef tbl_arr(in_tbl, in_flds=None):\r\n    """"""Convert a table to an array\r\n\r\n    Requires:\r\n    --------\r\n    in_tbl : table\r\n        a table from within arcmap\r\n    in_flds : either None, a list/tuple of field names.\r\n        If None or an empty list or tuple, then all fields are returned.\r\n    """"""\r\n    flds = arcpy.ListFields(in_tbl)\r\n    dump_flds = [""Shape_Length"", ""Shape_Area"", ""Shape""]\r\n    flds = [f for f in flds if f.name not in dump_flds]\r\n    in_flds = [f.name for f in flds]\r\n    nulls = null_dict(flds)\r\n    if not isinstance(flds, (list, tuple, type(None), """")):\r\n        return ""Input is not correct""\r\n    if flds is None:\r\n        in_flds = ""*""\r\n    elif isinstance(in_flds, (list, tuple)):\r\n        if len(in_flds) == 0:\r\n            in_flds = ""*""\r\n        else:\r\n            in_flds = [f.name for f in flds]\r\n    a = arcpy.da.TableToNumPyArray(in_tbl,\r\n                                   in_flds,\r\n                                   skip_nulls=False,\r\n                                   null_value=nulls)\r\n    return a\r\n\r\n\r\ndef rotate_tbl(a, max_cols=20, line_wdth=79):\r\n    """"""Rotate a structured array to offer another view of the data.\r\n\r\n    max_cols : integer\r\n        maximum number of columns to print\r\n    line_wdth : integer\r\n        slice the line after this width\r\n\r\n    Notes:\r\n    ------\r\n    Be reasonable... the 1000 record table just isn\'t going to work. The\r\n    maximum number of rows can be specified and the column widths are\r\n    determined from the data therein.  Hopefully everything will fit within\r\n    the margin width... If not, reduce the number of rows, or roll your own.\r\n\r\n    dt = "", "".join([""(\'C{}\', \'<U{}\')"".format(i, j) for i, j in enumerate(w)])\r\n    w is the widths below and e is the empty object array\r\n    arcpy.Tabletools.RotateTable(""polygon_demo"",\r\n                              ""OBJECTID;file_part;main_part;Test;Pnts;Shape"")\r\n    """"""\r\n    cut = min(a.shape[0], max_cols)\r\n    rc = (len(a[0]), cut + 1)\r\n    a = a[:cut]\r\n    e = np.empty(rc, dtype=np.object)\r\n    e[:, 0] = a.dtype.names\r\n    types = (list, tuple, np.ndarray)\r\n    u0 = [[[j, \'seq\'][isinstance(j, types)] for j in i] for i in a]\r\n    u = np.array(u0, dtype=np.unicode_)\r\n    e[:, 1:] = u[:].T\r\n    widths = [max([len(i) for i in e[:, j]]) for j in range(e.shape[1])]\r\n    f = [""{{!s: <{}}} "".format(width + 1) for width in widths]\r\n    txt = """".join(i for i in f)\r\n    txt = ""\\n"".join([txt.format(*e[i, :])[:line_wdth]\r\n                     for i in range(e.shape[0])])\r\n    hdr_txt = ""Attribute | Records....\\n{}"".format(txt)\r\n    tweet(hdr_txt)\r\n    return txt  #, e, widths\r\n\r\n# ---- main section ----\r\n\r\nscript = sys.argv[0]\r\nif len(sys.argv) > 1:\r\n    in_tbl = sys.argv[1]\r\n    in_flds = sys.argv[2]\r\n    out_txt = sys.argv[3]\r\n    a = tbl_arr(in_tbl, in_flds=None)[:5]  # in_flds)\r\n    txt = rotate_tbl(a)  # rotate table demo\r\n    f = open(out_txt, \'w\')\r\n    print(txt, file=f)\r\n    f.close()    \r\nelse:\r\n    in_tbl = r""C:\\Git_Dan\\arraytools\\array_tools_testing\\array_tools.gdb\\pnts_2000""\r\n    a = tbl_arr(in_tbl, in_flds=None)[:5]  # in_flds)\r\n    out_txt = ""C:/Temp/rot_.txt""\r\n    txt = rotate_tbl(a)  # rotate table demo\r\n    f = open(out_txt, \'w\')\r\n    print(txt, file=f)\r\n    f.close()\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n\r\n'"
TableTools/Scripts/table_shell_script.py,10,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\nscript name\r\n===========\r\n\r\nScript :   ......py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified : 2018-06-04\r\n\r\nPurpose:  tools for working with numpy arrays\r\n\r\nUseage :\r\n\r\nReferences\r\n----------\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/numpyarraytotable.htm>`_.\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/tabletonumpyarray.htm>`_.\r\n---------------------------------------------------------------------\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nfrom arcpytools import fc_info, tweet  #, frmt_rec, _col_format\r\nimport arcpy\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\ndef has_nulls(a):\r\n    """"""Check to see if nulls are in the array passed from the featureclass\r\n    """"""\r\n    #\r\n    a_kind = a.dtype.kind\r\n    if a_kind == \'i\':\r\n        m = a == np.iinfo(np.int32).min\r\n    elif a_kind == \'f\':\r\n        m = np.isnan(a)\r\n    else:\r\n        m = a == None\r\n    return m\r\n\r\n\r\ndef tbl_2_nparray(in_tbl, flds):\r\n    """"""Form the TableToNumPyArray to account for nulls for various dtypes.\r\n    This is essentially a shortcut to `arcpy.da.TableToNumPyArray`\r\n\r\n    Requires\r\n    --------\r\n    `in_tbl` :\r\n        table, or featureclass table name\r\n    `flds` :\r\n        list of field names\r\n    `skip_nulls` = False :\r\n        set within function\r\n    `null_value` :\r\n        determined from the dtype of the array...\r\n        otherwise you may as well do it manually\r\n\r\n    Source\r\n    ------\r\n    arraytools, apt.py module\r\n    """"""\r\n    nulls = {\'Double\':np.nan,\r\n             \'Integer\':np.iinfo(np.int32).min,\r\n             \'OID\':np.iinfo(np.int32).min,\r\n             \'String\':""None""}\r\n    #\r\n    fld_dict = {i.name: i.type for i in arcpy.ListFields(in_tbl)}\r\n    null_dict = {f:nulls[fld_dict[f]] for f in flds}\r\n    a = arcpy.da.TableToNumPyArray(in_table=in_tbl,\r\n                                   field_names=flds,\r\n                                   skip_nulls=False,\r\n                                   null_value=null_dict)\r\n    return a\r\n\r\n\r\ndef your_func_here():\r\n    """""" this is the place""""""\r\n    pass\r\n\r\n\r\n# ---- Run options: _demo or from _tool\r\n#\r\ndef _demo():\r\n    """"""Code to run if in demo mode\r\n    """"""\r\n    a = np. array([\'1, 2, 3, 4, 5\', \'a, b, c\', \'6, 7, 8, 9\',\r\n                   \'d, e, f, g, h\', \'10, 11, 12, 13\'])\r\n    return a\r\n\r\n\r\ndef _tool():\r\n    """"""run when script is from a tool\r\n    """"""\r\n    in_tbl = sys.argv[1]\r\n    in_fld = sys.argv[2]\r\n    out_fld = sys.argv[3]  # output field name\r\n\r\n    # ---- main tool section\r\n    desc = arcpy.da.Describe(in_tbl)\r\n    # ---- main tool section\r\n    _, oid_fld, _, _ = fc_info(in_tbl, prn=False)  # run fc_info\r\n    #\r\n    flds = [oid_fld, in_fld]\r\n    tbl_path = desc[\'path\']\r\n    fnames = [i.name for i in arcpy.ListFields(in_tbl)]\r\n    if out_fld in fnames:\r\n        out_fld += \'dup\'\r\n    out_fld = arcpy.ValidateFieldName(out_fld, tbl_path)\r\n    args = [in_tbl, in_fld, out_fld, tbl_path]\r\n    msg = ""in_tbl {}\\nin_fld {}\\nout_fld  {}\\ntbl_path  {}"".format(*args)\r\n    tweet(msg)\r\n    #\r\n    # ---- call section for processing function\r\n    #\r\n    #in_arr = arcpy.da.TableToNumPyArray(in_tbl, vals)  # old\r\n    in_arr = tbl_2_nparray(in_tbl, flds)  # produce the table\r\n    #\r\n    tweet(""{!r:}"".format(in_arr))\r\n    #\r\n    a0 = in_arr[in_fld]\r\n    #\r\n    # do stuff here ********************************\r\n    #\r\n    sze = a0.dtype.str\r\n    # ---- reassemble the table for extending\r\n    dt = [(\'IDs\', \'<i8\'), (out_fld, sze)]\r\n    out_array = np.copy(in_arr.shape[0])\r\n    out_array[out_fld] = a0  # result goes here\r\n    out_array.dtype = dt\r\n    arcpy.da.ExtendTable(in_tbl, \'OBJECTID\', out_array, \'IDs\')\r\n\r\n# ----------------------------------------------------------------------\r\n# .... final code section producing the featureclass and extendtable\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    a = _demo()\r\n    frmt = ""Testing...\\n{}""\r\n    print(frmt.format(a))\r\nelse:\r\n    testing = False\r\n    _tool()\r\n#\r\nif not testing:\r\n    tweet(\'Some message here...\')\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n#    a = _demo()\r\n'"
TableTools/Scripts/table_to_csv.py,12,"b'# -*- coding: utf-8 -*-\r\n""""""\r\ntable_to_csv\r\n============\r\n\r\nScript :   table_to_csv.py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified : 2018-12-29\r\n\r\nPurpose :  tools for working with numpy arrays and geometry\r\n\r\nNotes:\r\n\r\nReferences:\r\n\r\n""""""\r\nimport sys\r\nfrom textwrap import dedent\r\nimport numpy as np\r\nimport arcpy\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\ndef tweet(msg):\r\n    """"""Produce a message for both arcpy and python\r\n    : msg - a text message\r\n    """"""\r\n    m = ""\\n{}\\n"".format(msg)\r\n    arcpy.AddMessage(m)\r\n    print(m)\r\n    print(arcpy.GetMessages())\r\n\r\n\r\ndef null_dict(flds):\r\n    """"""Produce a null dictionary from a list of fields\r\n    These must be field objects and not just their name.\r\n    """"""\r\n    dump_flds = [""OBJECTID"",""Shape_Length"", ""Shape_Area"", ""Shape""]\r\n    flds_oth = [f for f in flds\r\n                if f.name not in dump_flds]\r\n#    oid_geom = [\'OBJECTID\', \'SHAPE@X\', \'SHAPE@Y\']\r\n    nulls = {\'Double\':np.nan,\r\n             \'Single\':np.nan,\r\n             \'Short\':np.iinfo(np.int16).min,\r\n             \'SmallInteger\':np.iinfo(np.int16).min,\r\n             \'Long\':np.iinfo(np.int32).min,\r\n             \'Float\':np.nan,\r\n             \'Integer\':np.iinfo(np.int32).min,\r\n             \'String\':str(None),\r\n             \'Text\':str(None)}\r\n    fld_dict = {i.name: i.type for i in flds_oth}\r\n    nulls = {f.name:nulls[fld_dict[f.name]] for f in flds_oth}\r\n    return nulls\r\n\r\n\r\ndef tbl_arr(in_tbl, in_flds=None):\r\n    """"""Convert a table to text\r\n\r\n    Requires:\r\n    --------\r\n    in_tbl : table\r\n        a table from within arcmap\r\n    in_flds : either None, a list/tuple of field names.\r\n        If None or an empty list or tuple, then all fields are returned.\r\n    """"""\r\n    flds = arcpy.ListFields(in_tbl)\r\n    nulls = null_dict(flds)\r\n    if not isinstance(in_flds, (list, tuple, type(None), """")):\r\n        return ""Input is not correct""\r\n    if in_flds is None:\r\n        in_flds = ""*""\r\n    elif isinstance(in_flds, (list, tuple)):\r\n        if len(in_flds) == 0:\r\n            in_flds = ""*""\r\n    a = arcpy.da.TableToNumPyArray(in_tbl,\r\n                                   in_flds,\r\n                                   skip_nulls=False,\r\n                                   null_value=nulls)\r\n    return a\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# (3) save_txt .... code section ---\r\ndef save_txt(a, name=""arr.txt"", sep="", "", dt_hdr=True):\r\n    """"""Save a NumPy structured, recarray to text.\r\n\r\n    Requires:\r\n    --------\r\n    a     : array\r\n        input array\r\n    fname : filename\r\n        output filename and path otherwise save to script folder\r\n    sep   : separator\r\n        column separater, include a space if needed\r\n    dt_hdr: boolean\r\n        if True, add dtype names to the header of the file\r\n    """"""\r\n    a_names = "", "".join(i for i in a.dtype.names)\r\n    hdr = ["""", a_names][dt_hdr]  # use """" or names from input array\r\n    s = np.array(a.tolist(), dtype=np.unicode_)\r\n    widths = [max([len(i) for i in s[:, j]])\r\n              for j in range(s.shape[1])]\r\n#    frmt = sep.join([""%{}s"".format(i) for i in widths])\r\n    frmt = sep.join([""%s"" for i in widths])  # stripped out space padding\r\n    # vals = "", "".join([i[1] for i in a.dtype.descr])\r\n    np.savetxt(name, a, fmt=frmt, header=hdr, comments="""")\r\n    print(""\\nFile saved..."")\r\n\r\n\r\n# ---- main section ----\r\nif len(sys.argv) > 1:\r\n    script = sys.argv[0]\r\n    in_tbl = sys.argv[1]\r\n    out_csv = str(sys.argv[2]).replace(""\\\\"", ""/"")\r\n    \r\n    frmt = """"""\\n\r\n    :---------------------------------------------------------------------:\r\n    Running.... {}\r\n    Input table ....... {}\r\n    Output csv file... {}\\n\r\n    :---------------------------------------------------------------------:\r\n    """"""\r\n    args = [script, in_tbl, out_csv]\r\n    msg = dedent(frmt).format(*args)\r\n    tweet(""Input parameters {}"".format(msg))\r\n    #\r\n    flds = arcpy.ListFields(in_tbl)\r\n    fnames = [i.name for i in flds if i.type not in (\'Geometry\', \'Raster\')]\r\n#    fnames = "";"".join([i for i in fnames])\r\n    a = tbl_arr(in_tbl, in_flds=fnames)  # call tble_arr to get array\r\n    #\r\n    save_txt(a, name=out_csv)\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n\r\n'"
TableTools/Scripts/table_to_text.py,10,"b'# -*- coding: UTF-8 -*-\n""""""\ntable_to_text\n=============\n\nScript :   table_to_text.py\nAuthor :   Dan.Patterson@carleton.ca\nModified : 2018-12-29\n\nPurpose:\n--------\nTo produce a formatted list/array format for ndarray, structured arrays,\n recarrays\n\nReferences:\n-----------\n`<http://pyopengl.sourceforge.net/pydoc/numpy.lib.recfunctions.html>`_.\n\n`<http://desktop.arcgis.com/en/arcmap/latest/analyze/arcpy-data-access/\nfeatureclasstonumpyarray.htm>`_.\n\n""""""\nimport sys\nimport numpy as np\nimport arcpy\nfrom textwrap import dedent\n\nformatter = {\'float\': \'{:0.3f}\'.format,\n             \'float64\': \'{:0.3f}\'.format}\nnp.set_printoptions(edgeitems=3, linewidth=80, precision=2, suppress=True,\n                    threshold=200, formatter=formatter)\nscript = sys.argv[0]\n\n\ndef null_dict(flds):\n    """"""Produce a null dictionary from a list of fields\n    These must be field objects and not just their name.\n    """"""\n    dump_flds = [""OBJECTID"",""Shape_Length"", ""Shape_Area"", ""Shape""]\n    flds_oth = [f for f in flds\n                if f.name not in dump_flds]\n#    oid_geom = [\'OBJECTID\', \'SHAPE@X\', \'SHAPE@Y\']\n    nulls = {\'Double\':np.nan,\n             \'Single\':np.nan,\n             \'Short\':np.iinfo(np.int16).min,\n             \'SmallInteger\':np.iinfo(np.int16).min,\n             \'Long\':np.iinfo(np.int32).min,\n             \'Float\':np.nan,\n             \'Integer\':np.iinfo(np.int32).min,\n             \'String\':str(None),\n             \'Text\':str(None)}\n    fld_dict = {i.name: i.type for i in flds_oth}\n    nulls = {f.name:nulls[fld_dict[f.name]] for f in flds_oth}\n    return nulls\n\n\ndef tbl_arr(in_tbl, in_flds=None):\n    """"""Convert a table to an array\n\n    Requires:\n    --------\n    in_tbl : table\n        a table from within arcmap\n    in_flds : either None, a list/tuple of field names.\n        If None or an empty list or tuple, then all fields are returned.\n    """"""\n    flds = arcpy.ListFields(in_tbl)\n    dump_flds = [""Shape_Length"", ""Shape_Area"", ""Shape""]\n    flds = [f for f in flds if f.name not in dump_flds]\n    in_flds = [f.name for f in flds]\n    nulls = null_dict(flds)\n    if not isinstance(flds, (list, tuple, type(None), """")):\n        return ""Input is not correct""\n    if flds is None:\n        in_flds = ""*""\n    elif isinstance(in_flds, (list, tuple)):\n        if len(in_flds) == 0:\n            in_flds = ""*""\n        else:\n            in_flds = [f.name for f in flds]\n    a = arcpy.da.TableToNumPyArray(in_tbl,\n                                   in_flds,\n                                   skip_nulls=False,\n                                   null_value=nulls)\n    return a\n\n\ndef tweet(msg):\n    """"""Produce a message for both arcpy and python\n    : msg - a text message\n    """"""\n    m = ""\\n{}\\n"".format(msg)\n    arcpy.AddMessage(m)\n    print(m)\n    print(arcpy.GetMessages())\n\n\n# ----------------------------------------------------------------------\n# (6) frmt_struct .... code section --- from frmts.py in arraytools\n#\ndef _col_format(a, nme=""fld"", deci=0):\n    """"""Determine column format given a desired number of decimal places.\n    Used by frmt_struct.\n\n    Parameters:\n    -----------\n    a : column in an array\n    nme : column name\n    deci : integer\n        desired number of decimal points if the data are numeric\n\n    Notes:\n    -----\n    The field is examined to determine whether it is a simple integer, a\n    float type or a list, array or string.  The maximum width is determined\n    based on this type.\n    """"""\n    a_kind = a.dtype.kind\n    a_nm = nme\n    if a_kind in (\'i\', \'u\'):                 # integer type\n        w_, m_ = [\':> {}.0f\', \'{:> 0.0f}\']\n        col_wdth = len(m_.format(a.max())) + 1\n        col_wdth = max(len(a_nm), col_wdth) + 1  # + deci\n        c_fmt = w_.format(col_wdth, 0)\n        # print(""name {} c_fmt {}, wdth {}"".format(a_nm, c_fmt, col_wdth))\n    elif a_kind == \'f\' and np.isscalar(a[0]):   # float type with rounding\n        w_, m_ = [\':> {}.{}f\', \'{:> 0.{}f}\']\n        a_max, a_min = np.round(np.sort(a[[0, -1]]), deci)\n        col_wdth = max(len(m_.format(a_max, deci)),\n                       len(m_.format(a_min, deci))) + 1\n        col_wdth = max(len(a_nm), col_wdth) + 1\n        c_fmt = w_.format(col_wdth, deci)\n    else:                                   # lists, arrays, strings\n        col_wdth = max([len(str(i)) for i in a])\n        col_wdth = max(len(a_nm), col_wdth) + 1  # + deci\n        c_fmt = ""!s:>"" + ""{}"".format(col_wdth)\n    return c_fmt, col_wdth\n\n\n#def frmt_csv(a):\n#    """"""Format a structured/recarray to csv format\n#    """"""\ndef frmt_struct(a, deci=2, f_names=True, prn=False):\n    """"""Format a structured array with a mixed dtype.\n\n    Requires:\n    ---------\n    a : structured/recarray\n    deci : integer\n        To facilitate printing, this value is the number of decimal points to\n        use for all floating point fields.\n    _col_format : function\n        Does the work to obtain a representation of the column format.\n\n    Notes\n    -----\n        It is not really possible to deconstruct the exact number of decimals\n    to use for float values, so a decision had to be made to simplify.\n    """"""\n    nms = a.dtype.names\n    N = len(nms)\n    title = [""ABCDEFGHIJKLMNOPQRSTUVWXYZ""[:N], nms][f_names]\n    # ---- get the column formats from ... _col_format ----\n    dts = []\n    wdths = []\n    for i in nms:\n        c_fmt, col_wdth = _col_format(a[i], nme=i, deci=deci)\n        dts.append(c_fmt)\n        wdths.append(max(col_wdth, len(i)))\n        # print(""name {} c_frmt {}, wdth {}"".format(i, c_fmt, col_wdth))\n    rf = "" "".join([(\'{\' + i + \'}\') for i in dts])\n    hdr = [""!s:>"" + ""{}"".format(wdths[i]) for i in range(N)]\n    hdr2 = "" "".join([""{"" + hdr[i] + ""}"" for i in range(N)])\n    header = hdr2.format(*title)\n    txt = [header]\n    for i in range(a.shape[0]):\n        row = rf.format(*a[i])\n        txt.append(row)\n    if prn:\n        for i in txt:\n            print(i)\n    msg = ""\\n"".join([i for i in txt])\n    return msg\n\n\n# ---- main section ----\nif len(sys.argv) > 1:\n    script = sys.argv[0]\n    in_tbl = sys.argv[1]\n    in_flds = sys.argv[2]\n    out_txt = str(sys.argv[3]).replace(""\\\\"", ""/"")\n    if in_flds in (""#"", None, """") or isinstance(in_flds, (list, tuple)):\n        in_flds = None\n    else:\n        in_flds = in_flds.split("";"")\n    \n    frmt = """"""\\n\n    :---------------------------------------------------------------------:\n    Running.... {}\n    Input table ....... {}\n    With fields... {}\n    Output text file... {}\\n\n    :---------------------------------------------------------------------:\n    """"""\n    args = [script, in_tbl, in_flds, out_txt]\n    msg = dedent(frmt).format(*args)\n    tweet(""Input parameters {}"".format(msg))\n    a = tbl_arr(in_tbl, in_flds)  # call tble_arr to get array\n    #\n    msg = frmt_struct(a, deci=2, f_names=True, prn=False)   \n    f = open(out_txt, \'w\')\n    print(msg, file=f)\n    f.close()\nelse:\n    in_tbl = r""C:\\Git_Dan\\arraytools\\array_tools_testing\\array_tools.gdb\\pnts_2000""\n\n\nif __name__ == ""__main__"":\n    """"""run sample""""""\n#    in_tbl = r""C:\\GIS\\Tools_scripts\\Table_tools\\Table_tools.gdb\\poly_pnts""\n#    in_flds = [\'OBJECTID\', \'Shape\', \'Id\', \'Area\', \'file_part\', \'X_c\']\n#    in_flds = None\n#    out_txt = r""c:\\temp\\x.txt""\n'"
concavehull/Scripts/arcpytools_ch.py,7,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   arcpytools.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2018-03-31\r\n:Purpose:  tools for working with numpy arrays\r\n:Useage:\r\n:\r\n:References:\r\n:\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nfrom textwrap import dedent\r\nimport numpy as np\r\nimport arcpy\r\n# from arcpytools import array_fc, array_struct, tweet\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\n\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\n__all__ = [\'_xyID\']\r\n\r\n\r\ndef tweet(msg):\r\n    """"""Produce a message for both arcpy and python\r\n    : msg - a text message\r\n    """"""\r\n    m = ""\\n{}\\n"".format(msg)\r\n    arcpy.AddMessage(m)\r\n    print(m)\r\n#    return None\r\n\r\n\r\ndef _describe(in_fc):\r\n    """"""Simply return the arcpy.da.Describe object\r\n    : desc.keys() an abbreviated list...\r\n    : [... \'OIDFieldName\'... \'areaFieldName\', \'baseName\'... \'catalogPath\',\r\n    :  ... \'dataType\'... \'extent\', \'featureType\', \'fields\', \'file\'... \'hasM\',\r\n    :  \'hasOID\', \'hasZ\', \'indexes\'... \'lengthFieldName\'... \'name\', \'path\',\r\n    :  \'rasterFieldName\', ..., \'shapeFieldName\', \'shapeType\',\r\n    :  \'spatialReference\',  ...]\r\n    """"""\r\n    return arcpy.da.Describe(in_fc)\r\n\r\n\r\ndef fc_info(in_fc, prn=False):\r\n    """"""Return basic featureclass information, including...\r\n    :\r\n    : shp_fld  - field name which contains the geometry object\r\n    : oid_fld  - the object index/id field name\r\n    : SR       - spatial reference object (use SR.name to get the name)\r\n    : shp_type - shape type (Point, Polyline, Polygon, Multipoint, Multipatch)\r\n    : - others: \'areaFieldName\', \'baseName\', \'catalogPath\',\'featureType\',\r\n    :           \'fields\', \'hasOID\', \'hasM\', \'hasZ\', \'path\'\r\n    : - all_flds =[i.name for i in desc[\'fields\']]\r\n    """"""\r\n    desc = _describe(in_fc)\r\n    args = [\'shapeFieldName\', \'OIDFieldName\', \'shapeType\', \'spatialReference\']\r\n    shp_fld, oid_fld, shp_type, SR = [desc[i] for i in args]\r\n    if prn:\r\n        frmt = ""FeatureClass:\\n   {}"".format(in_fc)\r\n        f = ""\\n{!s:<16}{!s:<14}{!s:<10}{!s:<10}""\r\n        frmt += f.format(*args)\r\n        frmt += f.format(shp_fld, oid_fld, shp_type, SR.name)\r\n        tweet(frmt)\r\n    else:\r\n        return shp_fld, oid_fld, shp_type, SR\r\n\r\n\r\ndef _xyID(in_fc, to_pnts=True):\r\n    """"""Convert featureclass geometry (in_fc) to a simple 2D structured array\r\n    :  with ID, X, Y values. Optionally convert to points, otherwise centroid.\r\n    """"""\r\n    flds = [\'OID@\', \'SHAPE@X\', \'SHAPE@Y\']\r\n    args = [in_fc, flds, None, None, to_pnts, (None, None)]\r\n    cur = arcpy.da.SearchCursor(*args)\r\n    a = cur._as_narray()\r\n    a.dtype = [(\'IDs\', \'<i4\'), (\'Xs\', \'<f8\'), (\'Ys\', \'<f8\')]\r\n    del cur\r\n    return a\r\n\r\n\r\ndef array_struct(a, fld_names=[\'X\', \'Y\'], dt=[\'<f8\', \'<f8\']):\r\n    """"""Convert an array to a structured array\r\n    :Requires:\r\n    :--------\r\n    :  a - an ndarray with shape at least (N, 2)\r\n    :  dt = dtype class\r\n    :  names - names for the fields\r\n    """"""\r\n    dts = [(fld_names[i], dt[i]) for i in range(len(fld_names))]\r\n    z = np.zeros((a.shape[0],), dtype=dts)\r\n    names = z.dtype.names\r\n    for i in range(a.shape[1]):\r\n        z[names[i]] = a[:, i]\r\n    return z\r\n\r\n\r\ndef array_fc(a, out_fc, fld_names, SR):\r\n    """"""array to featureclass/shapefile...optionally including all fields\r\n    :  syntax: array_fc(a, out_fc, fld_names, SR)\r\n    :  see: NumpyArrayToFeatureClass, ListFields for information and options\r\n    :  out_fc:   featureclass/shapefile... complete path\r\n    :  fld_names:  is the Shapefield name ie [\'Shape\'] or [\'X\', \'Y\'s]\r\n    """"""\r\n    if arcpy.Exists(out_fc):\r\n        arcpy.Delete_management(out_fc)\r\n    arcpy.da.NumPyArrayToFeatureClass(a, out_fc, fld_names, SR)\r\n    return out_fc\r\n\r\n\r\ndef fc_array(in_fc, flds, allpnts):\r\n    """"""Convert featureclass to an ndarray...with optional fields besides the\r\n    :  FID/OIDName and Shape fields.\r\n    :Syntax: read_shp(input_FC,other_flds, explode_to_points)\r\n    :   input_FC    shapefile\r\n    :   other_flds   ""*"", or specific fields [\'FID\',\'Shape\',\'SomeClass\', etc]\r\n    :   see:  FeatureClassToNumPyArray, ListFields for more information\r\n    """"""\r\n    out_flds = []\r\n    shp_fld, oid_fld, shp_type, SR = fc_info(in_fc)  # get the base information\r\n    fields = arcpy.ListFields(in_fc)      # all fields in the shapefile\r\n    if flds == """":                        # return just OID and Shape field\r\n        out_flds = [oid_fld, shp_fld]     # FID and Shape field required\r\n    elif flds == ""*"":                     # all fields\r\n        out_flds = [f.name for f in fields]\r\n    else:\r\n        out_flds = [oid_fld, shp_fld]\r\n        for f in fields:\r\n            if f.name in flds:\r\n                out_flds.append(f.name)\r\n    frmt = """"""\r\n    Inside....\r\n    Running \'fc_array\' with...\\n{}\\nFields...{}\\nAll pnts...{}\\nSR...{}\r\n    """"""\r\n    args = [in_fc, out_flds, allpnts, SR.name]\r\n    msg = dedent(frmt).format(*args)\r\n    tweet(msg)\r\n    a = arcpy.da.FeatureClassToNumPyArray(in_fc, out_flds, """", SR, allpnts)\r\n    # out it goes in array format\r\n    return a, out_flds, SR\r\n\r\n\r\ndef arr2pnts(in_fc, as_struct=True, shp_fld=None, SR=None):\r\n    """"""Create points from an array.\r\n    :  in_fc - input featureclass\r\n    :  as_struct - if True, returns a structured array with X, Y fields,\r\n    :            - if False, returns an ndarray with dtype=\'<f8\'\r\n    :Notes: calls fc_info to return featureclass information\r\n    """"""\r\n    if shp_fld is None or SR is None:\r\n        shp_fld, oid_fld, SR = fc_info(in_fc)\r\n    a = arcpy.da.FeatureClassToNumPyArray(in_fc, ""*"", """", SR)\r\n    dt = [(\'X\', \'<f8\'), (\'Y\', \'<f8\')]\r\n    if as_struct:\r\n        shps = np.array([tuple(i) for i in a[shp_fld]], dtype=dt)\r\n    else:\r\n        shps = a[shp_fld]\r\n    return shps, shp_fld, SR\r\n\r\n\r\ndef arr2line(a, out_fc, SR=None):\r\n    """"""create lines from an array""""""\r\n    pass\r\n\r\n\r\ndef shapes2fc(shps, out_fc):\r\n    """"""Create a featureclass/shapefile from poly* shapes.\r\n    :  out_fc - full path and name to the output container (gdb or folder)\r\n    """"""\r\n    msg = ""\\nCan\'t overwrite the {}... rename"".format(out_fc)\r\n    try:\r\n        if arcpy.Exists(out_fc):\r\n            arcpy.Delete_management(out_fc)\r\n        arcpy.CopyFeatures_management(shps, out_fc)\r\n    except ValueError:\r\n        tweet(msg)\r\n\r\n\r\ndef arr2polys(a, out_fc, oid_fld, SR):\r\n    """"""Make poly* features from a structured array.\r\n    :  a - structured array\r\n    :  out_fc: a featureclass path and name, or None\r\n    :  oid_fld - object id field, used to partition the shapes into groups\r\n    :  SR - spatial reference object, or name\r\n    :Returns:\r\n    :-------\r\n    :  Produces the featureclass optionally, but returns the polygons anyway.\r\n    """"""\r\n    arcpy.overwriteOutput = True\r\n    pts = []\r\n    keys = np.unique(a[oid_fld])\r\n    for k in keys:\r\n        w = np.where(a[oid_fld] == k)[0]\r\n        v = a[\'Shape\'][w[0]:w[-1] + 1]\r\n        pts.append(v)\r\n    # Create a Polygon from an Array of Points, save to featueclass if needed\r\n    s = []\r\n    for pt in pts:\r\n        s.append(arcpy.Polygon(arcpy.Array([arcpy.Point(*p) for p in pt]), SR))\r\n    return s\r\n\r\n\r\ndef output_polylines(out_fc, SR, pnt_groups):\r\n    """"""Produce the output polygon featureclass.\r\n    :Requires:\r\n    :--------\r\n    : - A list of lists of points\r\n    :   aline = [[[0, 0], [1, 1]]]  # a list of points\r\n    :   aPolyline = [[aline]]       # a list of lists of points\r\n    """"""\r\n    msg = \'\\nRead the script header... A projected coordinate system required\'\r\n    assert (SR is not None), msg\r\n    polylines = []\r\n    for pnts in pnt_groups:\r\n        for pair in pnts:\r\n            arr = arcpy.Array([arcpy.Point(*xy) for xy in pair])\r\n            pl = arcpy.Polyline(arr, SR)\r\n            polylines.append(pl)\r\n    if arcpy.Exists(out_fc):     # overwrite any existing versions\r\n        arcpy.Delete_management(out_fc)\r\n    arcpy.CopyFeatures_management(polylines, out_fc)\r\n    return\r\n\r\n\r\ndef output_polygons(out_fc, SR, pnt_groups):\r\n    """"""Produce the output polygon featureclass.\r\n    :Requires:\r\n    :--------\r\n    : - A list of lists of points\r\n    :   aline = [[0, 0], [1, 1]]  # a list of points\r\n    :   aPolygon = [aline]       # a list of lists of points\r\n    """"""\r\n    msg = \'\\nRead the script header... A projected coordinate system required\'\r\n    assert (SR is not None), msg\r\n    polygons = []\r\n    for pnts in pnt_groups:\r\n        for pair in pnts:\r\n            arr = arcpy.Array([arcpy.Point(*xy) for xy in pair])\r\n            pl = arcpy.Polygon(arr, SR)\r\n            polygons.append(pl)\r\n    if arcpy.Exists(out_fc):     # overwrite any existing versions\r\n        arcpy.Delete_management(out_fc)\r\n    arcpy.CopyFeatures_management(polygons, out_fc)\r\n    return\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n#    _demo()\r\n    gdb_fc = [\'Data\', \'point_tools.gdb\', \'radial_pnts\']\r\n    in_fc = ""/"".join(script.split(""/"")[:-2] + gdb_fc)\r\n    result = fc_array(in_fc, flds="""", allpnts=True)  # a, out_flds, SR\r\n'"
concavehull/Scripts/hulls.py,21,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\nhulls.py\r\n========\r\n\r\nScript:   hulls.py\r\n\r\nAuthor:   Dan.Patterson@carleton.ca\r\n\r\nModified: 2019-06-08\r\n\r\nPurpose:  working with numpy arrays to determine convex and concave hulls\r\n\r\nReferences:\r\n-----------\r\n`<https://community.esri.com/blogs/dan_patterson/2018/03/11/\r\nconcave-hulls-the-elusive-container>\'_.\r\n`<https://github.com/jsmolka/hull/blob/master/hull.py>\'_.\r\n`<https://stackoverflow.com/questions/563198/how-do-you-detect-where-two-\r\nline-segments-intersect#565282>\'_.\r\n`<http://www.codeproject.com/Tips/862988/Find-the-intersection-\r\npoint-of-two-line-segments>\'_.\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nfrom numpy.lib.recfunctions import structured_to_unstructured as stu\r\nfrom arcpytools_pnt import tweet, output_polylines, output_polygons\r\nimport arcpy\r\nimport warnings\r\nimport math\r\n\r\n\r\nwarnings.simplefilter(\'ignore\', FutureWarning)\r\n\r\narcpy.overwriteOutput = True\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\nPI = math.pi\r\n\r\n\r\ndef pnt_in_list(pnt, pnts_list):\r\n    """"""Check to see if a point is in a list of points\r\n    """"""\r\n    is_in = np.any([np.isclose(pnt, i) for i in pnts_list])\r\n    return is_in\r\n\r\n\r\ndef intersects(*args):\r\n    """"""Line intersection check.  Two lines or 4 points that form the lines.\r\n    :Requires:\r\n    :--------\r\n    :  intersects(line0, line1) or intersects(p0, p1, p2, p3)\r\n    :   p0, p1 -> line 1\r\n    :   p2, p3 -> line 2\r\n    :Returns: boolean, if the segments do intersect\r\n    :--------\r\n    :References:\r\n    :--------\r\n    : https://stackoverflow.com/questions/563198/how-do-you-detect-where-two-\r\n    :        line-segments-intersect#565282\r\n    """"""\r\n    if len(args) == 2:\r\n        p0, p1, p2, p3 = *args[0], *args[1]\r\n    elif len(args) == 4:\r\n        p0, p1, p2, p3 = args\r\n    else:\r\n        raise AttributeError(""Pass 2, 2-pnt lines or 4 points to the function"")\r\n    #\r\n    # ---- First check ----   np.cross(p1-p0, p3-p2 )\r\n    p0_x, p0_y, p1_x, p1_y, p2_x, p2_y, p3_x, p3_y = *p0, *p1, *p2, *p3\r\n    s10_x = p1_x - p0_x\r\n    s10_y = p1_y - p0_y\r\n    s32_x = p3_x - p2_x\r\n    s32_y = p3_y - p2_y\r\n    denom = s10_x * s32_y - s32_x * s10_y\r\n    if denom == 0.0:\r\n        return False\r\n    #\r\n    # ---- Second check ----  np.cross(p1-p0, p0-p2 )\r\n    den_gt0 = denom > 0\r\n    s02_x = p0_x - p2_x\r\n    s02_y = p0_y - p2_y\r\n    s_numer = s10_x * s02_y - s10_y * s02_x\r\n    if (s_numer < 0) == den_gt0:\r\n        return False\r\n    #\r\n    # ---- Third check ----  np.cross(p3-p2, p0-p2)\r\n    t_numer = s32_x * s02_y - s32_y * s02_x\r\n    if (t_numer < 0) == den_gt0:\r\n        return False\r\n    #\r\n    if ((s_numer > denom) == den_gt0) or ((t_numer > denom) == den_gt0):\r\n        return False\r\n    #\r\n    # ---- check to see if the intersection point is one of the input points\r\n    t = t_numer / denom\r\n    # substitute p0 in the equation\r\n    x = p0_x + (t * s10_x)\r\n    y = p0_y + (t * s10_y)\r\n    # be careful that you are comparing tuples to tuples, lists to lists\r\n    if sum([(x, y) == tuple(i) for i in [p0, p1, p2, p3]]) > 0:\r\n        return False\r\n    return True\r\n\r\n\r\ndef angle(p0, p1, prv_ang=0):\r\n    """"""Angle between two points and the previous angle, or zero.\r\n    """"""\r\n    ang = math.atan2(p0[1] - p1[1], p0[0] - p1[0])\r\n    a0 = (ang - prv_ang)\r\n    a0 = a0 % (PI * 2) - PI\r\n    return a0\r\n\r\n\r\ndef point_in_polygon(pnt, poly):  # pnt_in_poly(pnt, poly):  #\r\n    """"""Point is in polygon. ## fix this and use pip from arraytools\r\n    """"""\r\n    x, y = pnt\r\n    N = len(poly)\r\n    for i in range(N):\r\n        x0, y0, xy = [poly[i][0], poly[i][1], poly[(i + 1) % N]]\r\n        c_min = min([x0, xy[0]])\r\n        c_max = max([x0, xy[0]])\r\n        if c_min < x <= c_max:\r\n            p = y0 - xy[1]\r\n            q = x0 - xy[0]\r\n            y_cal = (x - x0) * p / q + y0\r\n            if y_cal < y:\r\n                return True\r\n    return False\r\n\r\n\r\ndef knn(pnts, p, k):\r\n    """"""\r\n    Calculates k nearest neighbours for a given point.\r\n\r\n    :param points: list of points\r\n    :param p: reference point\r\n    :param k: amount of neighbours\r\n    :return: list\r\n    """"""\r\n    s = sorted(pnts,\r\n               key=lambda x: math.sqrt((x[0]-p[0])**2 + (x[1]-p[1])**2))[0:k]\r\n    return s\r\n\r\n\r\ndef knn0(pnts, p, k):\r\n    """"""\r\n    Calculates k nearest neighbours for a given point.\r\n\r\n    points : array\r\n        list of points\r\n    p : two number array-like\r\n        reference point\r\n    k : integer\r\n        amount of neighbours\r\n    Returns:\r\n    --------\r\n    list of the k nearest neighbours, based on squared distance\r\n    """"""\r\n    p = np.asarray(p)\r\n    pnts = np.asarray(pnts)\r\n    diff = pnts - p[np.newaxis, :]\r\n    d = np.einsum(\'ij,ij->i\', diff, diff)\r\n    idx = np.argsort(d)[:k]\r\n#    s = [i.tolist() for i in pnts[idx]]\r\n    return pnts[idx].tolist()\r\n\r\n\r\ndef concave(points, k):\r\n    """"""Calculates the concave hull for given points\r\n    :Requires:\r\n    :--------\r\n    : points - initially the input set of points with duplicates removes and\r\n    :    sorted on the Y value first, lowest Y at the top (?)\r\n    : k - initially the number of points to start forming the concave hull,\r\n    :    k will be the initial set of neighbors\r\n    :Notes:  This recursively calls itself to check concave hull\r\n    : p_set - The working copy of the input points\r\n    :-----\r\n    """"""\r\n    k = max(k, 3)  # Make sure k >= 3\r\n    p_set = list(set(points[:]))  # Remove duplicates if not done already\r\n    if len(p_set) < 3:\r\n        raise Exception(""p_set length cannot be smaller than 3"")\r\n    elif len(p_set) == 3:\r\n        return p_set  # Points are a polygon already\r\n    k = min(k, len(p_set) - 1)  # Make sure k neighbours can be found\r\n\r\n    frst_p = cur_p = min(p_set, key=lambda x: x[1])\r\n    hull = [frst_p]  # Initialize hull with first point\r\n    p_set.remove(frst_p)  # Remove first point from p_set\r\n    prev_ang = 0\r\n\r\n    while (cur_p != frst_p or len(hull) == 1) and len(p_set) > 0:\r\n        if len(hull) == 3:\r\n            p_set.append(frst_p)  # Add first point again\r\n        knn_pnts = knn(p_set, cur_p, k)  # knn or knn0\r\n        cur_pnts = sorted(knn_pnts, key=lambda x: -angle(x, cur_p, prev_ang))\r\n\r\n        its = True\r\n        i = -1\r\n        while its and i < len(cur_pnts) - 1:\r\n            i += 1\r\n            last_point = 1 if cur_pnts[i] == frst_p else 0\r\n            j = 1\r\n            its = False\r\n            while not its and j < len(hull) - last_point:\r\n                its = intersects(hull[-1], cur_pnts[i], hull[-j - 1], hull[-j])\r\n                j += 1\r\n        if its:  # All points intersect, try a higher number of neighbours\r\n            return concave(points, k + 1)\r\n        prev_ang = angle(cur_pnts[i], cur_p)\r\n        cur_p = cur_pnts[i]\r\n        hull.append(cur_p)  # Valid candidate was found\r\n        p_set.remove(cur_p)\r\n\r\n    for point in p_set:\r\n        if not point_in_polygon(point, hull):\r\n            return concave(points, k + 1)\r\n    #\r\n    return hull\r\n\r\n\r\n# ---- convex hull ----------------------------------------------------------\r\n#\r\ndef cross(o, a, b):\r\n    """"""Cross-product for vectors o-a and o-b\r\n    """"""\r\n    xo, yo = o\r\n    xa, ya = a\r\n    xb, yb = b\r\n    return (xa - xo)*(yb - yo) - (ya - yo)*(xb - xo)\r\n#    return (a[0] - o[0]) * (b[1] - o[1]) - (a[1] - o[1]) * (b[0] - o[0])\r\n\r\n\r\ndef convex(points):\r\n    """"""Calculates the convex hull for given points\r\n    :Input is a list of 2D points [(x, y), ...]\r\n    """"""\r\n    points = sorted(set(points))  # Remove duplicates\r\n    if len(points) <= 1:\r\n        return points\r\n    # Build lower hull\r\n    lower = []\r\n    for p in points:\r\n        while len(lower) >= 2 and cross(lower[-2], lower[-1], p) <= 0:\r\n            lower.pop()\r\n        lower.append(p)\r\n    # Build upper hull\r\n    upper = []\r\n    for p in reversed(points):\r\n        while len(upper) >= 2 and cross(upper[-2], upper[-1], p) <= 0:\r\n            upper.pop()\r\n        upper.append(p)\r\n    print(""lower\\n{}\\nupper\\n{}"".format(lower, upper))\r\n    return np.array(lower[:-1] + upper)  # upper[:-1]) # for open loop\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# .... running script or testing code section\r\ndef _tool():\r\n    """"""run when script is from a tool\r\n    """"""\r\n    in_fc = sys.argv[1]\r\n    group_by = str(sys.argv[2])\r\n    k_factor = int(sys.argv[3])\r\n    hull_type = str(sys.argv[4])\r\n    out_type = str(sys.argv[5])\r\n    out_fc = sys.argv[6]\r\n    return in_fc, group_by, k_factor, hull_type, out_type, out_fc\r\n\r\n\r\ngdb_pth = ""/"".join(script.split(""/"")[:-2]) + ""/Data/Point_tools.gdb""\r\n\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    in_fc = gdb_pth + r""/r_sorted""\r\n    group_by = \'Group_\'\r\n    k_factor = 3\r\n    hull_type = \'concave\'  # \'convex\'\r\n    out_type = \'Polyline\'\r\n    out_fc = gdb_pth + r""/r_11""\r\nelse:\r\n    testing = False\r\n    in_fc, group_by, k_factor, hull_type, out_type, out_fc = _tool()\r\n\r\nmsg = """"""\\n\r\n-----------------------------------------------------------------------\r\n---- Concave/convex hull ----\r\nscript    {}\r\nTesting   {}\r\nin_fc     {}\r\ngroup_by  {}\r\nk_factor  {}\r\nhull_type {}\r\nout_type  {}\r\nout_fc    {}\r\n-----------------------------------------------------------------------\r\n\r\n""""""\r\nargs = [script, testing, in_fc, group_by, k_factor,\r\n        hull_type, out_type, out_fc]\r\ntweet(msg.format(*args))\r\n\r\ndesc = arcpy.da.Describe(in_fc)\r\nSR = desc[\'spatialReference\']\r\n#\r\n# (1) ---- get the points\r\nout_flds = [\'OID@\', \'SHAPE@X\', \'SHAPE@Y\'] + [group_by]\r\na = arcpy.da.FeatureClassToNumPyArray(in_fc, out_flds, """", SR, True)\r\n#\r\n# (2) ---- determine the unique groupings of the points\r\nuniq, idx, rev = np.unique(a[group_by], True, True)\r\ngroups = [a[np.where(a[group_by] == i)[0]] for i in uniq]\r\n#\r\n# (3) ---- for each group, perform the concave hull\r\nhulls = []\r\nfor i in range(0, len(groups)):\r\n    p = groups[i]\r\n    p = p[[\'SHAPE@X\', \'SHAPE@Y\']]\r\n    n = len(p)\r\n    p = stu(p)\r\n    #\r\n    # ---- point preparation section ------------------------------------\r\n    p = np.array(list(set([tuple(i) for i in p])))  # Remove duplicates\r\n    idx_cr = np.lexsort((p[:, 0], p[:, 1]))         # indices of sorted array\r\n    in_pnts = np.asarray([p[i] for i in idx_cr])    # p[idx_cr]  #\r\n    in_pnts = in_pnts.tolist()\r\n    in_pnts = [tuple(i) for i in in_pnts]\r\n    if hull_type == \'concave\':\r\n        cx = np.array(concave(in_pnts, k_factor))  # requires a list of tuples\r\n    else:\r\n        cx = np.array(convex(in_pnts))\r\n    hulls.append(cx.tolist())\r\n    # ----\r\n    #\r\nif out_type == \'Polyline\':\r\n    output_polylines(out_fc, SR, [hulls])\r\nelif out_type == \'Polygon\':\r\n    output_polygons(out_fc, SR, [hulls])\r\nelse:\r\n    for i in hulls:\r\n        print(""Hulls\\n{}"".format(np.array(i)))\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n'"
polygon_line_tools/Scripts/arcpytools_plt.py,23,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\n:Script:   arcpytools_plt.py\r\n:Author:   Dan.Patterson@carleton.ca\r\n:Modified: 2018-09-12\r\n:Purpose:  tools for working with numpy arrays\r\n:Useage:\r\n:\r\n:References:\r\n:\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport os\r\nfrom textwrap import dedent\r\nimport warnings\r\n\r\nimport numpy as np\r\nimport arcpy\r\n# from arcpytools import array_fc, array_struct, tweet\r\n\r\nwarnings.simplefilter(\'ignore\', FutureWarning)\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\n\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\n__all__ = [\'_col_format\', \'_describe\', \'_xyID\',\r\n           \'arr2line\', \'arr2pnts\', \'arr2polys\',\r\n           \'array_fc\', \'array_struct\', \'fc_array\',\r\n           \'fc_info\', \'frmt_rec\',\r\n           \'output_polygons\', \'output_polylines\',\r\n           \'pd_\', \'shapes2fc\', \'tweet\']\r\n\r\n\r\ndef tweet(msg):\r\n    """"""Produce a message for both arcpy and python\r\n    : msg - a text message\r\n    """"""\r\n    m = ""{}"".format(msg)\r\n    arcpy.AddMessage(m)\r\n    print(m)\r\n\r\n\r\ndef _describe(in_fc=None):\r\n    """"""Simply return the arcpy.da.Describe object\r\n    : desc.keys() an abbreviated list...\r\n    : [... \'OIDFieldName\'... \'areaFieldName\', \'baseName\'... \'catalogPath\',\r\n    :  ... \'dataType\'... \'extent\', \'featureType\', \'fields\', \'file\'... \'hasM\',\r\n    :  \'hasOID\', \'hasZ\', \'indexes\'... \'lengthFieldName\'... \'name\', \'path\',\r\n    :  \'rasterFieldName\', ..., \'shapeFieldName\', \'shapeType\',\r\n    :  \'spatialReference\',  ...]\r\n    """"""\r\n    if in_fc is not None:\r\n        return arcpy.da.Describe(in_fc)\r\n    else:\r\n        return None\r\n\r\ndef fc_info(in_fc, prn=False):\r\n    """"""Return basic featureclass information, including...\r\n\r\n    Parameters:\r\n    -----------\r\n    - shp_fld  :\r\n        field name which contains the geometry object\r\n    - oid_fld  :\r\n        the object index/id field name\r\n    - SR       :\r\n        spatial reference object (use SR.name to get the name)\r\n    - shp_type :\r\n        shape type (Point, Polyline, Polygon, Multipoint, Multipatch)\r\n    - others   :\r\n        \'areaFieldName\', \'baseName\', \'catalogPath\',\'featureType\',\'fields\',\r\n        \'hasOID\', \'hasM\', \'hasZ\', \'path\'\r\n\r\n     - all_flds :\r\n         [i.name for i in desc[\'fields\']]\r\n    """"""\r\n    desc = _describe(in_fc)\r\n    args = [\'shapeFieldName\', \'OIDFieldName\', \'shapeType\', \'spatialReference\']\r\n    shp_fld, oid_fld, shp_type, SR = [desc[i] for i in args]\r\n    if prn:\r\n        frmt = ""FeatureClass:\\n   {}"".format(in_fc)\r\n        f = ""\\n{!s:<16}{!s:<14}{!s:<10}{!s:<10}""\r\n        frmt += f.format(*args)\r\n        frmt += f.format(shp_fld, oid_fld, shp_type, SR.name)\r\n        tweet(frmt)\r\n    else:\r\n        return shp_fld, oid_fld, shp_type, SR\r\n\r\n\r\n# ---- geometry related -----------------------------------------------------\r\n#\r\ndef get_polys(in_fc):\r\n    """"""Return polygons from a polygon featureclass\r\n    """"""\r\n    out_polys = []\r\n    out_ids = []\r\n    with arcpy.da.SearchCursor(in_fc,  [""SHAPE@"", ""OID@""]) as cursor:\r\n        for row in cursor:\r\n            out_polys.append(row[0])\r\n            out_ids.append(row[1])\r\n    return out_polys, out_ids\r\n\r\n\r\ndef _poly_ext(p):\r\n    """"""poly* extent\r\n    """"""\r\n    L, B = p.extent.lowerLeft.X, p.extent.lowerLeft.Y\r\n    R, T = p.extent.upperRight.X, p.extent.upperRight.Y\r\n    return L, B, R, T\r\n\r\n\r\ndef trans_rot(a, angle):\r\n    """"""simplified translate and rotate\r\n    """"""\r\n    cent = a.mean(axis=0)\r\n    angle = np.radians(angle)\r\n    c, s = np.cos(angle), np.sin(angle)\r\n    R = np.array(((c, s), (-s,  c)))\r\n    return  np.einsum(\'ij,kj->ik\', a - cent, R) + cent\r\n\r\n\r\ndef cal_area(poly, cuts, cutters, factor):\r\n    """"""Calculate the areas\r\n    """"""\r\n    tot_area = poly.area\r\n    fract_areas = np.array([c.area/tot_area for c in cuts])\r\n    c_sum = np.cumsum(fract_areas)\r\n    f_list = np.linspace(0.0, 1.0, factor, endpoint=False)\r\n    idxs = [np.argwhere(c_sum <= i)[-1][0] for i in f_list[1:]]\r\n    out = []\r\n    for idx in idxs:\r\n        c_line = cutters[idx]\r\n        left, right = poly.cut(c_line)\r\n        out.append([left.area, c_line])\r\n    return out\r\n\r\n\r\ndef _xyID(in_fc, to_pnts=True):\r\n    """"""Convert featureclass geometry (in_fc) to a simple 2D structured array\r\n    :  with ID, X, Y values. Optionally convert to points, otherwise centroid.\r\n    """"""\r\n    flds = [\'OID@\', \'SHAPE@X\', \'SHAPE@Y\']\r\n    args = [in_fc, flds, None, None, to_pnts, (None, None)]\r\n    cur = arcpy.da.SearchCursor(*args)\r\n    a = cur._as_narray()\r\n    a.dtype = [(\'IDs\', \'<i4\'), (\'Xs\', \'<f8\'), (\'Ys\', \'<f8\')]\r\n    del cur\r\n    return a\r\n\r\n\r\ndef array_struct(a, fld_names=[\'X\', \'Y\'], dt=[\'<f8\', \'<f8\']):\r\n    """"""Convert an array to a structured array\r\n\r\n    Parameters:\r\n    -----------\r\n    - a : an ndarray with shape at least (N, 2)\r\n    -  dt : dtype class\r\n    -  names : names for the fields\r\n    """"""\r\n    dts = [(fld_names[i], dt[i]) for i in range(len(fld_names))]\r\n    z = np.zeros((a.shape[0],), dtype=dts)\r\n    names = z.dtype.names\r\n    for i in range(a.shape[1]):\r\n        z[names[i]] = a[:, i]\r\n    return z\r\n\r\n\r\ndef array_fc(a, out_fc, fld_names, SR):\r\n    """"""Array to featureclass/shapefile...optionally including all fields\r\n\r\n    Parameters:\r\n    -----------\r\n    - out_fc :  featureclass/shapefile... complete path\r\n    - fld_names : the Shapefield name ie [\'Shape\'] or [\'X\', \'Y\'s]\r\n    - SR : spatial reference of the output\r\n\r\n    See also :\r\n        NumpyArrayToFeatureClass, ListFields for information and options\r\n    """"""\r\n    if arcpy.Exists(out_fc):\r\n        arcpy.Delete_management(out_fc)\r\n    arcpy.da.NumPyArrayToFeatureClass(a, out_fc, fld_names, SR)\r\n    return out_fc\r\n\r\n\r\ndef fc_array(in_fc, flds=""*"", allpnts=True):\r\n    """"""Convert a featureclass to an ndarray...with optional fields besides the\r\n    FID/OIDName and Shape fields.\r\n\r\n    Parameters:\r\n    -----------\r\n    in_fc : text\r\n        Full path to the geodatabase and the featureclass name\r\n\r\n    flds : text or list\r\n        - ``\'\'   : just an object id and shape field``\r\n        - ``\'*\'  : all fields in the featureclass or``\r\n        - ``list : specific fields [\'OBJECTID\',\'Shape\',\'SomeClass\', etc]``\r\n\r\n    allpnts : boolean\r\n        - True `explodes` geometry to individual points.\r\n        - False returns the centroid\r\n\r\n    Requires:\r\n    ---------\r\n        fc_info(in_fc) function\r\n\r\n    See also:\r\n    ---------\r\n        FeatureClassToNumPyArray, ListFields for more information in current\r\n        arcpy documentation\r\n    """"""\r\n    out_flds = []\r\n    shp_fld, oid_fld, shp_type, SR = fc_info(in_fc)  # get the base information\r\n    flds_all = arcpy.ListFields(in_fc)\r\n    flds_oth = [f for f in flds_all if f.type not in (\'OID\', \'Geometry\')]\r\n    fld_names = [f.name for f in flds_oth\r\n                 if f.name not in [""Shape_Length"", ""Shape_Area"", ""Shape""]]\r\n    oid_geom = [oid_fld, \'SHAPE@X\', \'SHAPE@Y\']\r\n    nulls = {\'Double\':np.nan,\r\n             \'Single\':np.nan,\r\n             \'Short\':np.iinfo(np.int32).min,\r\n             \'Long\':np.iinfo(np.int32).min,\r\n             \'Float\':np.nan,\r\n             \'Integer\':np.iinfo(np.int32).min,\r\n             \'String\':str(None),\r\n             \'Text\':str(None)}\r\n    fld_dict = {i.name: i.type for i in flds_oth}\r\n    null_dict = {f:nulls[fld_dict[f]] for f in fld_names}\r\n    if flds == """":                        # return just OID and Shape values\r\n        out_flds = oid_geom  # FID and Shape X, Y\r\n    elif flds == ""*"":                     # all fields\r\n        out_flds = oid_geom + fld_names\r\n    else:\r\n        out_flds = [oid_fld, \'SHAPE@X\', \'SHAPE@Y\']\r\n        for f in flds_oth:\r\n            if f.name in flds:\r\n                out_flds.append(f.name)\r\n#    pth = os.path.dirname(in_fc)\r\n#    out_flds = [arcpy.ValidateFieldName(f, pth) for f in out_flds]\r\n    frmt = """"""\\nRunning \'fc_array\' with ....\r\n    \\nfeatureclass... {}\\nFields... {}\\nAll pnts... {}\\nSR... {}\r\n    """"""\r\n    args = [in_fc, out_flds, allpnts, SR.name]\r\n    msg = dedent(frmt).format(*args)\r\n    tweet(msg)\r\n    a = arcpy.da.FeatureClassToNumPyArray(in_fc,\r\n                                          field_names=out_flds,\r\n                                          where_clause="""",\r\n                                          spatial_reference=SR,\r\n                                          explode_to_points=allpnts,\r\n                                          skip_nulls=False,\r\n                                          null_value=null_dict)\r\n    # out it goes in array format\r\n    return a, out_flds, SR\r\n\r\n\r\ndef arr2pnts(in_fc, as_struct=True, shp_fld=None, SR=None):\r\n    """"""Create points from an array.\r\n    :  in_fc - input featureclass\r\n    :  as_struct - if True, returns a structured array with X, Y fields,\r\n    :            - if False, returns an ndarray with dtype=\'<f8\'\r\n    :Notes: calls fc_info to return featureclass information\r\n    """"""\r\n    if shp_fld is None or SR is None:\r\n        shp_fld, oid_fld, SR = fc_info(in_fc)\r\n    a = arcpy.da.FeatureClassToNumPyArray(in_fc, ""*"", """", SR)\r\n    dt = [(\'X\', \'<f8\'), (\'Y\', \'<f8\')]\r\n    if as_struct:\r\n        shps = np.array([tuple(i) for i in a[shp_fld]], dtype=dt)\r\n    else:\r\n        shps = a[shp_fld]\r\n    return shps, shp_fld, SR\r\n\r\n\r\ndef arr2line(a, out_fc, SR=None):\r\n    """"""create lines from an array""""""\r\n    pass\r\n\r\n\r\ndef shapes2fc(shps, out_fc):\r\n    """"""Create a featureclass/shapefile from poly* shapes.\r\n    :  out_fc - full path and name to the output container (gdb or folder)\r\n    """"""\r\n    msg = ""\\nCan\'t overwrite the {}... rename"".format(out_fc)\r\n    try:\r\n        if arcpy.Exists(out_fc):\r\n            arcpy.Delete_management(out_fc)\r\n        arcpy.CopyFeatures_management(shps, out_fc)\r\n    except ValueError:\r\n        tweet(msg)\r\n\r\n\r\ndef arr2polys(a, out_fc, oid_fld, SR):\r\n    """"""Make poly* features from a structured array.\r\n    :  a - structured array\r\n    :  out_fc: a featureclass path and name, or None\r\n    :  oid_fld - object id field, used to partition the shapes into groups\r\n    :  SR - spatial reference object, or name\r\n    :Returns:\r\n    :-------\r\n    :  Produces the featureclass optionally, but returns the polygons anyway.\r\n    """"""\r\n    arcpy.overwriteOutput = True\r\n    pts = []\r\n    keys = np.unique(a[oid_fld])\r\n    for k in keys:\r\n        w = np.where(a[oid_fld] == k)[0]\r\n        v = a[\'Shape\'][w[0]:w[-1] + 1]\r\n        pts.append(v)\r\n    # Create a Polygon from an Array of Points, save to featueclass if needed\r\n    s = []\r\n    for pt in pts:\r\n        s.append(arcpy.Polygon(arcpy.Array([arcpy.Point(*p) for p in pt]), SR))\r\n    return s\r\n\r\n\r\ndef output_polylines(out_fc, SR, pnt_groups):\r\n    """"""Produce the output polygon featureclass.\r\n    :Requires:\r\n    :--------\r\n    : - A list of lists of points\r\n    :   aline = [[[0, 0], [1, 1]]]  # a list of points\r\n    :   aPolyline = [[aline]]       # a list of lists of points\r\n    """"""\r\n    msg = \'\\nRead the script header... A projected coordinate system required\'\r\n    assert (SR is not None), msg\r\n    polylines = []\r\n    for pnts in pnt_groups:\r\n        for pair in pnts:\r\n            arr = arcpy.Array([arcpy.Point(*xy) for xy in pair])\r\n            pl = arcpy.Polyline(arr, SR)\r\n            polylines.append(pl)\r\n    if arcpy.Exists(out_fc):     # overwrite any existing versions\r\n        arcpy.Delete_management(out_fc)\r\n    arcpy.CopyFeatures_management(polylines, out_fc)\r\n    return\r\n\r\n\r\ndef output_polygons(out_fc, SR, pnt_groups):\r\n    """"""Produce the output polygon featureclass.\r\n\r\n    Parameters:\r\n    -----------\r\n    out_fc : string\r\n        The path and name of the featureclass to be created.\r\n    SR : spatial reference of the output featureclass\r\n    pnts_groups :\r\n        The point groups, list of lists of points, to include parts rings.\r\n\r\n    Requires:\r\n    --------\r\n\r\n    - A list of lists of points.  Four points form a triangle is the minimum\r\n    -  aline = [[0, 0], [1, 1]]  # a list of points\r\n    -  aPolygon = [aline]        # a list of lists of points\r\n    """"""\r\n    msg = \'\\nRead the script header... A projected coordinate system required\'\r\n    assert (SR is not None), msg\r\n    polygons = []\r\n    for pnts in pnt_groups:\r\n        for pair in pnts:\r\n            arr = arcpy.Array([arcpy.Point(*xy) for xy in pair])\r\n            pl = arcpy.Polygon(arr, SR)\r\n            polygons.append(pl)\r\n    if arcpy.Exists(out_fc):     # overwrite any existing versions\r\n        arcpy.Delete_management(out_fc)\r\n    arcpy.CopyFeatures_management(polygons, out_fc)\r\n    return\r\n\r\n# ---- formatting, from arraytools ------------------------------------------\r\n#\r\n# ----------------------------------------------------------------------\r\n# (4) frmt_rec .... code section\r\n#  frmt_rec requires _col_format\r\ndef _col_format(a, c_name=""c00"", deci=0):\r\n    """"""Determine column format given a desired number of decimal places.\r\n    Used by frmt_struct.\r\n\r\n    `a` : column\r\n        A column in an array.\r\n    `c_name` : text\r\n        column name\r\n    `deci` : int\r\n        Desired number of decimal points if the data are numeric\r\n\r\n    Notes:\r\n    -----\r\n        The field is examined to determine whether it is a simple integer, a\r\n        float type or a list, array or string.  The maximum width is determined\r\n        based on this type.\r\n\r\n        Checks were also added for (N,) shaped structured arrays being\r\n        reformatted to (N, 1) shape which sometimes occurs to facilitate array\r\n        viewing.  A kludge at best, but it works for now.\r\n    """"""\r\n    a_kind = a.dtype.kind\r\n    if a_kind in (\'i\', \'u\'):  # ---- integer type\r\n        w_, m_ = [\':> {}.0f\', \'{:> 0.0f}\']\r\n        col_wdth = len(m_.format(a.max())) + 1\r\n        col_wdth = max(len(c_name), col_wdth) + 1  # + deci\r\n        c_fmt = w_.format(col_wdth, 0)\r\n    elif a_kind == \'f\' and np.isscalar(a[0]):  # ---- float type with rounding\r\n        w_, m_ = [\':> {}.{}f\', \'{:> 0.{}f}\']\r\n        a_max, a_min = np.round(np.sort(a[[0, -1]]), deci)\r\n        col_wdth = max(len(m_.format(a_max, deci)),\r\n                       len(m_.format(a_min, deci))) + 1\r\n        col_wdth = max(len(c_name), col_wdth) + 1\r\n        c_fmt = w_.format(col_wdth, deci)\r\n    # ---- lists, arrays, strings. Check for (N,) vs (N,1)\r\n    # I made some changes in how col_wdth is determined, old is commented\r\n    else:\r\n        if a.ndim == 1:  # ---- check for (N, 1) format of structured array\r\n            a = a[0]\r\n        dt = a.dtype.descr[0][1]\r\n        col_wdth = int("""".join([i for i in dt if i.isdigit()]))\r\n#       col_wdth = max([len(str(i)) for i in a])\r\n        col_wdth = max(len(c_name), col_wdth) + 1  # + deci\r\n        c_fmt = ""!s:>"" + ""{}"".format(col_wdth)\r\n    return c_fmt, col_wdth\r\n\r\n\r\ndef pd_(a, deci=2, use_names=True, prn=True):\r\n    """"""see help for `frmt_rec`...""""""\r\n    ret = frmt_rec(a, deci=deci, use_names=use_names, prn=prn)\r\n    return ret\r\n\r\n\r\ndef frmt_rec(a, deci=2, use_names=True, prn=True):\r\n    """"""Format a structured array with a mixed dtype.\r\n\r\n    NOTE : Can be called as `pd_(a, ... )` to emulate pandas dataframes\r\n        You should limit large arrays to a slice ie. a[:50]\r\n\r\n    Requires:\r\n    -------\r\n    `a` : array\r\n        A structured/recarray\r\n    `deci` : int\r\n        To facilitate printing, this value is the number of decimal\r\n        points to use for all floating point fields.\r\n    `use_names` : boolean\r\n        If no names are available, then create them\r\n    `prn` : boolean\r\n        True to print, False to return the string\r\n    Notes:\r\n    -----\r\n        `_col_format` : does the actual work of obtaining a representation of\r\n        the column format.\r\n\r\n        It is not really possible to deconstruct the exact number of decimals\r\n        to use for float values, so a decision had to be made to simplify.\r\n    """"""\r\n    dt_names = a.dtype.names\r\n    N = len(dt_names)\r\n    c_names = [[""C{:02.0f}"".format(i) for i in range(N)], dt_names][use_names]\r\n    # ---- get the column formats from ... _col_format ----\r\n    dts = []\r\n    wdths = []\r\n    pair = list(zip(dt_names, c_names))\r\n    for i in range(len(pair)):\r\n        fld, nme = pair[i]\r\n        c_fmt, col_wdth = _col_format(a[fld], c_name=nme, deci=deci)\r\n        dts.append(c_fmt)\r\n        wdths.append(col_wdth)\r\n    row_frmt = "" "".join([(\'{\' + i + \'}\') for i in dts])\r\n    hdr = [""!s:>"" + ""{}"".format(wdths[i]) for i in range(N)]\r\n    hdr2 = "" "".join([""{"" + hdr[i] + ""}"" for i in range(N)])\r\n    header = ""--n--"" + hdr2.format(*c_names)\r\n    header = ""\\n{}\\n{}"".format(header, ""-""*len(header))\r\n    txt = [header]\r\n    # ---- check for structured arrays reshaped to (N, 1) instead of (N,) ----\r\n    len_shp = len(a.shape)\r\n    idx = 0\r\n    for i in range(a.shape[0]):\r\n        if len_shp == 1:  # ---- conventional (N,) shaped array\r\n            row = "" {:03.0f} "".format(idx) + row_frmt.format(*a[i])\r\n        else:             # ---- reformatted to (N, 1)\r\n            row = "" {:03.0f} "".format(idx) + row_frmt.format(*a[i][0])\r\n        idx += 1\r\n        txt.append(row)\r\n    msg = ""\\n"".join([i for i in txt])\r\n    if prn:\r\n        print(msg)\r\n    else:\r\n        return msg\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n#    _demo()\r\n#    gdb_fc = [\'Data\', \'point_tools.gdb\', \'radial_pnts\']\r\n#    in_fc = ""/"".join(script.split(""/"")[:-2] + gdb_fc)\r\n#    result = fc_array(in_fc, flds="""", allpnts=True)  # a, out_flds, SR\r\n'"
polygon_line_tools/Scripts/code_grid.py,5,"b'# -*- coding: utf-8 -*-\r\n""""""\r\ncode_grid\r\n=========\r\n\r\nScript :   code_grid.py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified : 2018-08-19\r\n\r\nPurpose:  produce a spreadsheet-like numbering system for \'grid\' cells\r\n\r\nThis use padding A01 to facilitate sorting.\r\nIf you want a different system change\r\n>>> >>> ""{}{}"".format(UC[c], r)    # A1 to whatever, no padding\r\n>>> ""{}{:02.0f}"".format(UC[c], r)  # A01 to ..99\r\n>>> ""{}{:03.0f}"".format(UC[c], r)  # A001 to A999\r\n>>> # etc\r\n>>> c0 = code_grid(cols=5, rows=3, zero_based=False, shaped=True, bottom_up=False)\r\n[[\'A01\' \'B01\' \'C01\' \'D01\' \'E01\']\r\n [\'A02\' \'B02\' \'C02\' \'D02\' \'E02\']\r\n [\'A03\' \'B03\' \'C03\' \'D03\' \'E03\']]\r\n\r\n---------------------------------------------------------------------\r\n""""""\r\n\r\nimport sys\r\nimport numpy as np\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\ndef code_grid(cols=1, rows=1, zero_based=False, shaped=True, bottom_up=False):\r\n    """"""produce spreadsheet like labelling, either zero or 1 based\r\n    :  zero - A0,A1  or ones - A1, A2..\r\n    :  dig = list(\'0123456789\')  # string.digits\r\n    : import string .... string.ascii_uppercase\r\n    """"""\r\n    alph = list("" ABCDEFGHIJKLMNOPQRSTUVWXYZ"")\r\n    UC = [(""{}{}"").format(alph[i], alph[j]).strip()\r\n          for i in range(27)\r\n          for j in range(1,27)]\r\n    z = [1, 0][zero_based]\r\n    rc = [1, 0][zero_based]\r\n    c = [""{}{:02.0f}"".format(UC[c], r) # pull in the column heading\r\n         for r in range(z, rows + rc)  # label in the row letter\r\n         for c in range(cols)]         # label in the row number\r\n    c = np.asarray(c)\r\n    if shaped:\r\n        c = c.reshape(rows, cols)\r\n        if bottom_up:\r\n            c = np.flipud(c)\r\n    return c\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n    c0 = code_grid(cols=3, rows=100, zero_based=False, shaped=True, bottom_up=False)\r\n    print(c0)\r\n'"
polygon_line_tools/Scripts/densify_geom.py,15,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\ndensify_geom\r\n============\r\n\r\nScript  :   densify_geom.py\r\n\r\nAuthor  :   Dan.Patterson@carleton.ca\r\n\r\nModified :  2018-09-12\r\n\r\nPurpose :   Densify geometry by a factor.\r\n\r\nNotes :\r\n  Uses functions from \'arraytools\'.  These have been consolidated here.\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nfrom arcpytools_plt import tweet, fc_info, fc_array\r\nimport arcpy\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(\'int32\')),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\n# ---- array functions -------------------------------------------------------\r\n#\r\ndef _flat_(a_list, flat_list=None):\r\n    """"""Change the isinstance as appropriate\r\n    :  Flatten an object using recursion\r\n    :  see: itertools.chain() for an alternate method of flattening.\r\n    """"""\r\n    if flat_list is None:\r\n        flat_list = []\r\n    for item in a_list:\r\n        if isinstance(item, (list, tuple, np.ndarray, np.void)):\r\n            _flat_(item, flat_list)\r\n        else:\r\n            flat_list.append(item)\r\n    return flat_list\r\n\r\n\r\ndef _O_nd(obj, out=None):\r\n    """"""Flatten type \'O\' arrays to ndarray, using recursion\r\n    :Note: append retains internal shape, extend will flatten\r\n    :  nested lists into a list\r\n    """"""\r\n    if out is None:\r\n        out = []\r\n    sub_out = []\r\n    for el in obj:\r\n        el = np.asarray(el)\r\n        if el.dtype.kind in (\'O\', \'V\'):\r\n            sub_out.append(_O_nd(el, out))  # ---- recursion needed ---\r\n        else:\r\n            out.extend(el)  # was append\r\n    return out\r\n\r\ndef _densify_2D(a, fact=2):\r\n    """"""Densify a 2D array using np.interp.\r\n    :fact - the factor to density the line segments by\r\n    :Notes\r\n    :-----\r\n    :original construction of c rather than the zero\'s approach\r\n    :  c0 = c0.reshape(n, -1)\r\n    :  c1 = c1.reshape(n, -1)\r\n    :  c = np.concatenate((c0, c1), 1)\r\n    """"""\r\n    # Y = a changed all the y\'s to a\r\n    a = np.squeeze(a)\r\n    n_fact = len(a) * fact\r\n    b = np.arange(0, n_fact, fact)\r\n    b_new = np.arange(n_fact - 1)     # Where you want to interpolate\r\n    c0 = np.interp(b_new, b, a[:, 0])\r\n    c1 = np.interp(b_new, b, a[:, 1])\r\n    n = c0.shape[0]\r\n    c = np.zeros((n, 2))\r\n    c[:, 0] = c0\r\n    c[:, 1] = c1\r\n    return c\r\n\r\n\r\n# ---- featureclass functions ------------------------------------------------\r\n#\r\ndef _get_shapes(in_fc):\r\n    """"""Get shapes from a featureclass, in_fc, using SHAPE@ returning\r\n    :  [<Polygon object at....>, ... (<Polygon object at....>]\r\n    """"""\r\n    with arcpy.da.SearchCursor(in_fc, \'SHAPE@\') as cursor:\r\n        a = [row[0] for row in cursor]\r\n    return a\r\n\r\n\r\ndef obj_shapes(in_, SR):\r\n    """"""object array of coordinates to shapes""""""\r\n    s = []\r\n    for shps in in_:\r\n        tmp = []\r\n        if isinstance(shps, (list, tuple)):\r\n            for shp in shps:\r\n                shp = np.asarray(shp)\r\n                shp = shp.squeeze()\r\n                pnts = [arcpy.Point(*p) for p in shp]\r\n                tmp.append(pnts)\r\n            arr = arcpy.Array(pnts)\r\n        else:\r\n            arr = arcpy.Array([arcpy.Point(*p) for p in shps])\r\n        #\r\n        if out_type == \'Polyline\':\r\n            g = arcpy.Polyline(arr, SR)\r\n        elif out_type == \'Polygon\':\r\n            g = arcpy.Polygon(arr, SR)\r\n        s.append(g)\r\n    return s\r\n\r\n\r\ndef arcpnts_poly(in_, out_type=\'Polygon\', SR=None):\r\n    """"""Convert arcpy Point lists to poly* features\r\n    : out_type - either \'Polygon\' or \'Polyline\'\r\n    :\r\n    """"""\r\n    s = []\r\n    for i in in_:\r\n        for j in i:\r\n            if out_type == \'Polygon\':\r\n                g = arcpy.Polygon(arcpy.Array(j), SR)\r\n            elif out_type == \'Polyline\':\r\n                g = arcpy.Polyline(arcpy.Array(j), SR)\r\n            elif out_type == \'Points\':\r\n                j = _flat_(j)\r\n                g = arcpy.Multipoint(arcpy.Array(j), SR)  # check\r\n            s.append(g)\r\n    return s\r\n\r\n\r\ndef _convert(a, fact=2):\r\n    """"""Do the shape conversion for the array parts.  Calls to _densify_2D\r\n    """"""\r\n    out = []\r\n    parts = len(a)\r\n    for i in range(parts):\r\n        sub_out = []\r\n        p = np.asarray(a[i]).squeeze()\r\n        if p.ndim == 2:\r\n            shp = _densify_2D(p, fact=fact)  # call _densify_2D\r\n            arc_pnts = [arcpy.Point(*p) for p in shp]\r\n            sub_out.append(arc_pnts)\r\n            out.extend(sub_out)\r\n        else:\r\n            for i in range(len(p)):\r\n                pp = p[i]\r\n                shp = _densify_2D(pp, fact=fact)\r\n                arc_pnts = [arcpy.Point(*p) for p in shp]\r\n                sub_out.append(arc_pnts)\r\n            out.append(sub_out)\r\n    return out\r\n\r\n\r\ndef densify(polys, fact=2, sp_ref=None):\r\n    """"""Convert polygon objects to arrays, densify.\r\n    :\r\n    :Requires:\r\n    :--------\r\n    : _densify_2D - the function that is called for each shape part\r\n    : _unpack - unpack objects\r\n    """"""\r\n    # ---- main section ----\r\n    out = []\r\n    for poly in polys:\r\n        p = poly.__geo_interface__[\'coordinates\']\r\n        back = _convert(p, fact)\r\n        out.append(back)\r\n    return out\r\n\r\n\r\n# ---- main block ------------------------------------------------------------\r\n#\r\n# (1) obtain fc information\r\n# (2) convert multipart to singlepart\r\n# (3) split the fc into two arrays, one geometry, the 2nd attributes\r\n# (4) obtain the shapes and densify\r\n# (5) optionally produce the output fc\r\n# (6) join the attributes back\r\n\r\nif len(sys.argv) == 1:\r\n    in_pth = script.split(""/"")[:-2] + [""Polygon_lineTools.gdb""]\r\n    in_fc = ""/"".join(in_pth) + ""/shapes_mtm9""#    in_fc = r""C:\\Git_Dan\\a_Data\\arcpytools_demo.gdb\\xy1000_tree""\r\n    out_fc = ""/"".join(in_pth) + \'/x2\'\r\n    fact = 2\r\n    out_type = \'Polygon\'  # \'Polyline\' or \'Points\'\r\n    testing = False\r\nelse:\r\n    in_fc = sys.argv[1]  #\r\n    out_fc = sys.argv[2]  #\r\n    fact = int(sys.argv[3])  #\r\n    out_type = sys.argv[4]  # Polygon, Polyline are options\r\n    testing = False\r\n\r\n\r\nshp_fld, oid_fld, shp_type, SR = fc_info(in_fc)\r\ntemp = out_fc + ""tmp""\r\nif arcpy.Exists(temp):\r\n    arcpy.Delete_management(temp)\r\narcpy.MultipartToSinglepart_management(in_fc, temp)\r\npolys = _get_shapes(temp)\r\na = densify(polys, fact=fact, sp_ref=SR)\r\nb, _, _ = fc_array(in_fc, flds=""*"", allpnts=False) #_get_attributes(temp)\r\ndt = b.dtype.descr\r\ndtn = [(i[0].replace(""@"", ""_""), i[1]) for i in dt]\r\nb.dtype = np.dtype(dtn)\r\nout_shps = arcpnts_poly(a, out_type=out_type, SR=SR)\r\n#\r\n# ---- if not testing, save the geometry and extend (join) the attributes\r\nif not testing:\r\n    if arcpy.Exists(out_fc):\r\n        arcpy.Delete_management(out_fc)\r\n    arcpy.CopyFeatures_management(out_shps, out_fc)\r\n    arcpy.da.ExtendTable(out_fc, \'OBJECTID\', b, \'OBJECTID\', append_only=False)\r\n# ---- cleanup\r\narcpy.Delete_management(temp)\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n'"
polygon_line_tools/Scripts/poly_to_pnts.py,9,"b'# -*- coding: utf-8 -*-\r\n""""""\r\npoly_to_pnts.py\r\n===============\r\n\r\nScript :   poly_to_pnts.py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified : 2018-09-12\r\n\r\nPurpose:\r\n--------\r\n    Tools for working with numpy arrays.  This converts poly* features\r\n    to points\r\n\r\nReferences\r\n----------\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/numpyarraytotable.htm>`_.\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/tabletonumpyarray.htm>`_.\r\n---------------------------------------------------------------------\r\n""""""\r\n\r\nimport sys\r\nimport numpy as np\r\nfrom arcpytools_plt import fc_array, fc_info, tweet  #, frmt_rec, _col_format\r\nimport arcpy\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\ndef append_fld(arr, name=\'Pnt_id\', data=None):\r\n    """"""Append an id field to a subarray of points.\r\n    Emulated after recfunctions append_fields\r\n    """"""\r\n    dt = arr.dtype.descr + [(name, \'<i4\')]\r\n    if data is None:\r\n        data = np.arange(arr.shape[0])\r\n    e = np.empty(arr.shape[0], dtype=dt)\r\n    dt_new = np.dtype(dt)\r\n    for i in dt_new.names[:-1]:\r\n        e[i] = arr[i]\r\n    e[dt_new.names[-1]] = data\r\n    return e\r\n\r\n\r\ndef to_pnts(in_fc, out_fc, keep_flds=None, to_file=False):\r\n    """"""Convert a featureclass to a point file with unique point id values\r\n    added to indicate the points within the poly* features.  The output field\r\n    names (keep_flds) can be specified or all will be used if \'*\' is used.\r\n\r\n    Requires:\r\n    ---------\r\n        fc_array - from arcpytools_plt.py\r\n    Notes:\r\n    ------\r\n    - a Pnt_id field is added to indicate the order of the points making the\r\n      poly* feature\r\n    - the duplicate last point is removed for polygon features\r\n    - potentially troublesome field names are changed.\r\n    """"""\r\n    shp_fld, oid_fld, shp_type, SR = fc_info(in_fc)\r\n    if keep_flds is None:\r\n        keep_flds = ""*""\r\n    a, out_flds, SR = fc_array(in_fc, flds=keep_flds, allpnts=True)\r\n    a_s = np.split(a, np.where(np.diff(a[oid_fld]))[0] + 1)\r\n    out = []\r\n    for i in a_s:\r\n        ids = np.arange(i.shape[0])\r\n        out.append(append_fld(i, name=\'Pnt_id\', data=ids ))\r\n    # ---- remove duplicate last point and stack the points\r\n    if shp_type == ""Polygon"":\r\n        for i in range(len(a_s)):\r\n            out[i] = out[i][:-1]\r\n    out = np.hstack(out)\r\n    # ---- replace dodgy field names\r\n    kv = {\'OBJECTID\': \'Old_ID\', \'SHAPE@X\': \'X_\', \'SHAPE@Y\': \'Y_\',\r\n          \'Shape_Length\': \'Leng_orig\', \'Shape_Area\': \'Area_orig\'}\r\n    change = [kv.get(i, i) for i in out.dtype.names]\r\n    out.dtype.names = change\r\n    if to_file:\r\n        if arcpy.Exists(out_fc):\r\n            arcpy.Delete_management(out_fc)\r\n        arcpy.da.NumPyArrayToFeatureClass(out, out_fc,\r\n                                          [\'X_\', \'Y_\'], SR)\r\n    return out\r\n\r\n\r\n# ---- Do the work\r\n#\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    in_pth = script.split(""/"")[:-2] + [""Polygon_lineTools.gdb""]\r\n    in_fc = ""/"".join(in_pth) + ""/Polygons""\r\n    out_fc = ""/"".join(in_pth) + ""/poly_pnts""\r\n    keep_flds = ""*""\r\n    out = to_pnts(in_fc, out_fc, keep_flds, to_file=True)\r\nelse:\r\n    in_fc = sys.argv[1]\r\n    out_fc = sys.argv[2]\r\n    keep_flds = sys.argv[3]\r\n    if keep_flds not in (""#"", """", "" "", None):\r\n        keep_flds = keep_flds.split("";"")\r\n    tweet(keep_flds)\r\n    out = to_pnts(in_fc, out_fc, keep_flds, to_file=True)\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n'"
polygon_line_tools/Scripts/sampling_grid.py,29,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\nsampling_grid\r\n=============\r\n\r\nScript   :  sampling_grid.py\r\n\r\nAuthor   :  Dan.Patterson@carleton.ca\r\n\r\nModified :  2018-08-24\r\n\r\nPurpose  :  tools for working with numpy arrays\r\n\r\nSource :\r\n`<http://www.arcgis.com/home/item.html?id=ddb2deec9b5e4a09affe60de68f5ff4e>`_.\r\n\r\nReferences\r\n----------\r\n- Phish_Nyet.py\r\n`<https://community.esri.com/blogs/dan_patterson/2016/09/09/\r\nnumpy-snippets-3-phishnyet-creating-sampling-grids-using-numpy>`_.\r\n\r\n- n-gons\r\n`<https://community.esri.com/blogs/dan_patterson/2016/09/09/n-gons-\r\nregular-polygonal-shape-generation>`_.\r\n\r\nPurpose\r\n-------\r\n- Produce a sampling grid with user defined parameters.\r\n- create hexagon shapes in two forms, flat-topped and pointy-topped.\r\n\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nfrom textwrap import dedent\r\nimport numpy as np\r\nimport arcpy\r\n\r\narcpy.overwriteOutputs = True\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# ---- main functions ----\r\ndef code_grid(cols=1, rows=1, zero_based=False, shaped=True, bottom_up=False):\r\n    """"""produce spreadsheet like labelling, either zero or 1 based\r\n    see: code_grid.py for more details\r\n    """"""\r\n    alph = list("" ABCDEFGHIJKLMNOPQRSTUVWXYZ"")\r\n    UC = [(""{}{}"").format(alph[i], alph[j]).strip()\r\n          for i in range(27)\r\n          for j in range(1, 27)]\r\n    z = [1, 0][zero_based]\r\n    rc = [1, 0][zero_based]\r\n    c = [""{}{:02.0f}"".format(UC[c], r) # pull in the column heading\r\n         for r in range(z, rows + rc)  # label in the row letter\r\n         for c in range(cols)]         # label in the row number\r\n    c = np.asarray(c)\r\n    if shaped:\r\n        c = c.reshape(rows, cols)\r\n        if bottom_up:\r\n            c = np.flipud(c)\r\n    return c\r\n\r\n\r\ndef rotate(pnts, angle=0):\r\n    """"""rotate points about the origin in degrees, (+ve for clockwise) """"""\r\n    angle = np.deg2rad(angle)                 # convert to radians\r\n    s = np.sin(angle)\r\n    c = np.cos(angle)    # rotation terms\r\n    aff_matrix = np.array([[c, s], [-s, c]])  # rotation matrix\r\n    XY_r = np.dot(pnts, aff_matrix)           # numpy magic to rotate pnts\r\n    return XY_r\r\n\r\n\r\ndef triangle(dx=1, dy=1, cols=1, rows=1):\r\n    """"""Create a row of meshed triangles\r\n    """"""\r\n    grid_type = \'triangle\'\r\n    a, dx, b = dx/2.0, dx, dx*1.5\r\n    Xu = [0.0, a, dx, 0.0]   # X, Y values for a unit triangle, point up\r\n    Yu = [0.0, dy, 0.0, 0.0]\r\n    Xd = [a, b, dx, a]       # X, Y values for a unit triangle, point down\r\n    Yd = [dy, dy, 0.0, dy]   # shifted by dx\r\n    seedU = np.array(list(zip(Xu, Yu)))\r\n    seedD = np.array(list(zip(Xd, Yd)))\r\n    seed = np.array([seedU, seedD])\r\n    a = [seed + [j * dx, i * dy]       # make the shapes\r\n         for i in range(0, rows)       # cycle through the rows\r\n         for j in range(0, cols)]      # cycle through the columns\r\n    a = np.asarray(a)\r\n    s1, s2, s3, s4 = a.shape\r\n    a = a.reshape(s1*s2, s3, s4)\r\n    return a, grid_type\r\n\r\n\r\ndef rectangle(dx=1, dy=1, cols=1, rows=1):\r\n    """"""Create the array of pnts to pass on to arcpy using numpy magic\r\n    :  dx - increment in x direction, +ve moves west to east, left/right\r\n    :  dy - increment in y direction, -ve moves north to south, top/bottom\r\n    """"""\r\n    grid_type = \'rectangle\'\r\n    X = [0.0, 0.0, dx, dx, 0.0]       # X, Y values for a unit square\r\n    Y = [0.0, dy, dy, 0.0, 0.0]\r\n    seed = np.array(list(zip(X, Y)))  # [dx0, dy0] keep for insets\r\n    a = [seed + [j * dx, i * dy]      # make the shapes\r\n         for i in range(0, rows)      # cycle through the rows\r\n         for j in range(0, cols)]     # cycle through the columns\r\n    a = np.asarray(a)\r\n    return a, grid_type\r\n\r\n\r\ndef hex_flat(dx=1, dy=1, cols=1, rows=1):\r\n    """"""generate the points for the flat-headed hexagon\r\n    :dy_dx - the radius width, remember this when setting hex spacing\r\n    :  dx - increment in x direction, +ve moves west to east, left/right\r\n    :  dy - increment in y direction, -ve moves north to south, top/bottom\r\n    """"""\r\n    grid_type = \'hex_flat\'\r\n    f_rad = np.deg2rad([180., 120., 60., 0., -60., -120., -180.])\r\n    X = np.cos(f_rad) * dy\r\n    Y = np.sin(f_rad) * dy            # scaled hexagon about 0, 0\r\n    seed = np.array(list(zip(X, Y)))  # array of coordinates\r\n    dx = dx * 1.5\r\n    dy = dy * np.sqrt(3.)/2.0\r\n    hexs = [seed + [dx * i, dy * (i % 2)] for i in range(0, cols)]\r\n    m = len(hexs)\r\n    for j in range(1, rows):  # create the other rows\r\n        hexs += [hexs[h] + [0, dy * 2 * j] for h in range(m)]\r\n    return hexs, grid_type\r\n\r\n\r\ndef hex_pointy(dx=1, dy=1, cols=1, rows=1):\r\n    """"""pointy hex angles, convert to sin, cos, zip and send\r\n    :dy_dx - the radius width, remember this when setting hex spacing\r\n    :  dx - increment in x direction, +ve moves west to east, left/right\r\n    :  dy - increment in y direction, -ve moves north to south, top/bottom\r\n    """"""\r\n    grid_type = \'hex_pointy\'\r\n    p_rad = np.deg2rad([150., 90, 30., -30., -90., -150., 150.])\r\n    X = np.cos(p_rad) * dx\r\n    Y = np.sin(p_rad) * dy      # scaled hexagon about 0, 0\r\n    seed = np.array(list(zip(X, Y)))\r\n    dx = dx * np.sqrt(3.)/2.0\r\n    dy = dy * 1.5\r\n    hexs = [seed + [dx * i * 2, 0] for i in range(0, cols)]\r\n    m = len(hexs)\r\n    for j in range(1, rows):  # create the other rows\r\n        hexs += [hexs[h] + [dx * (j % 2), dy * j] for h in range(m)]\r\n    return hexs, grid_type\r\n\r\n\r\ndef repeat(seed=None, corner=[0, 0], cols=1, rows=1, angle=0):\r\n    """"""Create the array of pnts to pass on to arcpy using numpy magic to\r\n    :  produce a fishnet of the desired in_shp.\r\n    :seed - use grid_array, hex_flat or hex_pointy.  You specify the width\r\n    :       and height or its ratio when making the shapes\r\n    :corner - lower left corner of the shape pattern\r\n    :dx, dy - offset of the shapes... this is different\r\n    :rows, cols - the number of rows and columns to produce\r\n    :angle - rotation angle in degrees\r\n    """"""\r\n    if seed is None:\r\n        a = rectangle(dx=1, dy=1, cols=3, rows=3)\r\n    else:\r\n        a = np.asarray(seed)\r\n    if angle != 0:\r\n        a = [rotate(p, angle) for p in a]      # rotate the scaled points\r\n    pnts = [p + corner for p in a]            # translate them\r\n    return pnts\r\n\r\n\r\ndef output_polygons(output_shp, SR, pnts):\r\n    """"""produce the output polygon shapefile""""""\r\n    msg = \'\\nRead the script header... A projected coordinate system required\'\r\n    assert (SR is not None), msg  # and (SR.type == \'Projected\'), msg\r\n    polygons = []\r\n    for pnt in pnts:                 # create the polygon geometry\r\n        pl = arcpy.Polygon(arcpy.Array([arcpy.Point(*xy) for xy in pnt]), SR)\r\n        polygons.append(pl)\r\n    if arcpy.Exists(output_shp):     # overwrite any existing versions\r\n        arcpy.Delete_management(output_shp)\r\n    arcpy.CopyFeatures_management(polygons, output_shp)\r\n    return output_shp\r\n\r\n\r\ndef extend_tbl(output_shp, grid_type, rows, cols):\r\n    """"""Produce the column with the grid labels\r\n    """"""\r\n    if grid_type == \'triangle\':\r\n        cols = cols * 2\r\n    shp = rows*cols\r\n    code_fld = np.empty((shp,), dtype=[(\'IDs\', \'<i4\'), (\'Grid_codes\', \'<U3\')])\r\n    codes = code_grid(cols=cols, rows=rows, zero_based=False,\r\n                      shaped=True, bottom_up=False).ravel()\r\n    code_fld[\'IDs\'] = np.arange(1, shp+1)\r\n    code_fld[\'Grid_codes\'] = codes\r\n    arcpy.da.ExtendTable(output_shp, \'OBJECTID\', code_fld, \'IDS\')\r\n\r\n\r\nmsg = """"""\r\n: --------------------------------------------------------------------\r\n: output {}\r\n: SR  .. {}\r\n: type . {}\r\n: corner .. {}\r\n: size..... {} (dx, dy)\r\n: cols/rows {}\r\n: sample seed\r\n{}\r\n: --------------------------------------------------------------------\r\n""""""\r\n\r\n\r\ndef _demo(seed=None, out_fc=False, SR=None, corner=[0, 0], angle=0):\r\n    """"""Generate the grid using the specified or default parameters\r\n    """"""\r\n    corner = corner  # [300000.0, 5000000.0]\r\n    dx, dy = [1, 1]\r\n    cols, rows = [3, 3]\r\n    if seed is None:\r\n#        seed = rectangle(dx=1, dy=1, cols=3, rows=3)\r\n        seed, grid_type = hex_pointy(dx=10, dy=10, cols=3, rows=3)\r\n#        seed = hex_flat(dx=10, dy=10, cols=3, rows=3)\r\n        seed_t = \'rectangle\'\r\n    if SR is None:\r\n        SR = 3857  # -- WGS84 Web Mercator (Auxiliary Sphere)\r\n    pnts = repeat(seed=seed, corner=corner, cols=3, rows=3, angle=0)\r\n    args = ["""", SR, seed_t, corner, [dx, dy], [cols, rows], seed[0]]\r\n    print(dedent(msg).format(*args))\r\n    return pnts\r\n\r\n\r\ndef _tool():\r\n    """"""run when script is from a tool""""""\r\n    out_fc = sys.argv[1]  #\r\n    SR = sys.argv[2]\r\n    seed_t = sys.argv[3]\r\n    corn_x = float(sys.argv[4])\r\n    corn_y = float(sys.argv[5])\r\n    dx = float(sys.argv[6])\r\n    dy = float(sys.argv[7]) * -1.0\r\n    cols = int(sys.argv[8])\r\n    rows = int(sys.argv[9])\r\n    #\r\n    angle = float(sys.argv[10])\r\n    corner = [corn_x, corn_y]\r\n    if seed_t == \'rectangle\':\r\n        seed, grid_type = rectangle(dx, dy, cols, rows)\r\n    elif seed_t == \'hex_pointy\':\r\n        seed, grid_type = hex_pointy(dx, dy, cols, rows)\r\n    elif seed_t == \'hex_flat\':\r\n        seed, grid_type = hex_flat(dx, dy, cols, rows)\r\n    elif seed_t == \'triangles\':\r\n        seed, grid_type = triangle(dx, dy, cols, rows)\r\n    else:\r\n        seed, grid_type = rectangle(dx, dy, cols, rows)\r\n    # ----\r\n    msg = """"""\r\n    : --------------------------------------------------------------------\r\n    : output {}\r\n    : SR  .. {}\r\n    : Type . {}\r\n    : Top leftcorner .. {}\r\n    : Size..... {} (dx, dy)\r\n    : cols/rows {}\r\n    : grid type {}\r\n    : sample seed\r\n    {}\r\n    """"""\r\n    args = [out_fc, SR, seed_t, corner, [dx, dy],\r\n            [cols, rows], grid_type, seed[0]]\r\n    arcpy.AddMessage(dedent(msg).format(*args))\r\n    arcpy.GetMessages()\r\n    pnts = repeat(seed=seed, corner=corner, cols=cols, rows=rows, angle=angle)\r\n    return out_fc, SR, pnts, grid_type, rows, cols\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# .... final code section producing the featureclass and extendtable\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    pnts = _demo()\r\nelse:\r\n    testing = False\r\n    out_fc, SR, pnts, grid_type, rows, cols = _tool()\r\n#\r\nif not testing:\r\n    output_shp = output_polygons(out_fc, SR, pnts)\r\n    extend_tbl(output_shp, grid_type, rows, cols)\r\n    print(\'\\nSampling grid was created... {}\'.format(out_fc))\r\n\r\n# ----------------------------------------------------------------------\r\n#\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n'"
polygon_line_tools/Scripts/split_by_area.py,15,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\nsplit_by_area\r\n===========\r\n\r\nScript :   split_by_area.py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified:  2018-08-27\r\n\r\nPurpose :  tools for working with numpy arrays\r\n\r\nNotes:\r\n-----\r\nThe xs and ys form pairs with the first and last points being identical\r\nThe pairs are constructed using n-1 to ensure that you don\'t form a\r\nline from identical points.\r\n\r\nFirst split polygon is a sample of a multipart.  Added 0, 0 and 0, 80\r\nback in\r\n\r\n>>> xs = [0., 0., 80., 0, 0., 100., 100., 0.]\r\n>>> ys = [0., 30., 30., 80., 100., 100., 0., 0.]\r\n>>> a = np.array(list(zip(xs, ys))) * 1.0  # --- must be floats\r\n>>> v = np.array([[50., 0], [50, 100.]])\r\n>>> ext = np.array([[0., 0], [0, 100.],[100, 100.], [100., 0.], [0., 0.]])\r\nreturn a, v\r\n\r\nReferences:\r\n----------\r\n\r\n`<https://stackoverflow.com/questions/3252194/numpy-and-line-intersections>`_.\r\n`<https://community.esri.com/message/627051?commentID=627051#comment-627051>`\r\n`<https://community.esri.com/message/779043-re-how-to-divide-irregular-\r\npolygon-into-equal-areas-using-arcgis-105?commentID=779043#comment-779043>`\r\n\r\nThis is a good one\r\n`<https://tereshenkov.wordpress.com/2017/09/10/dividing-a-polygon-into-a-given\r\n-number-of-equal-areas-with-arcpy/>`\r\n\r\n---------------------------------------------------------------------\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport math\r\nfrom textwrap import dedent\r\nimport numpy as np\r\nimport warnings\r\nfrom arcpytools_plt import (tweet, fc_info, _poly_ext,\r\n                            trans_rot, cal_area, get_polys)\r\nimport arcpy\r\n\r\nwarnings.simplefilter(\'ignore\', FutureWarning)\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=5, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\n# ---- Do the work or run the demo ------------------------------------------\r\n#\r\n\r\nfrmt = """"""\r\nInput features.... {}\r\nOutput features... {}\r\nNumber of splits . {}\r\nSplit types ...... {}\r\n""""""\r\n\r\ndef _cut_poly(poly, p_id, step=1.0, split_axis=""X"", split_fac=4, SR=None):\r\n    """"""Perform the poly* cutting and return the result.\r\n\r\n    step : number\r\n        fractional step for division, 1.0 equates to 1%\r\n    split_face : number\r\n        number of areas to produce, 4, means split into 4 equal areas\r\n    """"""\r\n    L, B, R, T = _poly_ext(poly)\r\n#    s_fac = math.ceil((R - L)/step)\r\n#    lefts = np.linspace(L+dx, R, num=s_fac, endpoint=True)\r\n    dx = step\r\n    dy = step\r\n    if split_axis == ""X"":\r\n        lefts = np.arange(L+dx, R+dx, dx, dtype=\'float\')\r\n        splitters = np.array([[[l, B-1.0], [l, T+1.0]] for l in lefts])\r\n    elif s_axis == \'Y\':\r\n        tops = np.arange(B+dy, T+dy, dy, dtype=\'float\')\r\n        splitters = np.array([[[R+1.0, t], [L-1.0, t]] for t in tops])\r\n    cutters = []\r\n    for s in splitters:\r\n        s = s.tolist()\r\n        c = arcpy.Polyline(arcpy.Array([arcpy.Point(*xy) for xy in s]), SR)\r\n        cutters.append(c)\r\n    # ----\r\n    cuts = []\r\n    for i in cutters:\r\n        rght = poly\r\n        if i.crosses(poly):\r\n            try:\r\n                left, rght = poly.cut(i)\r\n                if rght is None:\r\n                    cuts.append(left)\r\n                cuts.append(left)\r\n                poly = rght\r\n                rght = left\r\n            except RuntimeError:\r\n                tweet(""Issues with poly...{}"".format(p_id))\r\n                continue\r\n        else:\r\n            cuts.append(rght)\r\n    return cuts, cutters\r\n\r\n\r\ndef final_cut(cutters, poly):\r\n    """""" final cut\r\n    """"""\r\n    cuts = []\r\n    for i in cutters:\r\n        rght = poly\r\n        if i.crosses(poly):\r\n            try:\r\n                left, rght = poly.cut(i)\r\n                if rght is None:\r\n                    cuts.append(left)\r\n                cuts.append(left)\r\n                poly = rght\r\n                rght = left\r\n            except RuntimeError:\r\n                tweet(""Issues with poly...{}"".format(p_id))\r\n                continue\r\n        else:\r\n            cuts.append(rght)\r\n    return cuts  # , cutters\r\n\r\n# ---- demo and tool section -------------------------------------------------\r\n#\r\n\r\nif len(sys.argv) == 1:\r\n    testing = False\r\n    in_pth = script.split(""/"")[:-2] + [""Polygon_lineTools.gdb""]\r\n    in_fc = ""/"".join(in_pth) + ""/shapes_mtm9""\r\n    out_fc = ""/"".join(in_pth) + ""/c0""\r\n    s_axis = ""Y""\r\n    s_fac = 4\r\nelse:\r\n    testing = False\r\n    in_fc = sys.argv[1]\r\n    out_fc = sys.argv[2]\r\n    s_fac = int(sys.argv[3])\r\n    s_axis = sys.argv[4]\r\n\r\n# ---- for both\r\n#\r\nshp_fld, oid_fld, shp_type, SR = fc_info(in_fc)\r\nout_polys, out_ids = get_polys(in_fc)\r\n#old_ids = np.repeat(out_ids, s_fac)  # produce data for the output id field\r\n\r\n# ---- instant bail\r\nif SR.type == \'Projected\':\r\n    result_ = []\r\n    for i in range(len(out_polys)):\r\n        poly = out_polys[i]\r\n        p_id = out_ids[i]\r\n        cuts, cutters = _cut_poly(poly, p_id, step=1,\r\n                                  split_axis = s_axis,\r\n                                  split_fac=4, SR=SR)\r\n        idxs = cal_area(poly, cuts, cutters, s_fac)\r\n        f_cutters = [cutters[i] for i in idxs]\r\n        r = final_cut(f_cutters, poly)\r\n        result_.extend(r)\r\n    if not testing:\r\n        if arcpy.Exists(out_fc):\r\n            arcpy.Delete_management(out_fc)\r\n        arcpy.CopyFeatures_management(result_, out_fc)\r\n        out_ids = np.repeat(out_ids, s_fac)\r\n        id_fld = np.zeros((len(result_),),\r\n                          dtype=[(""key"", ""<i4""), (""Old_ID"", ""<i4"")])\r\n        id_fld[""key""] = np.arange(1, len(result_) + 1)\r\n        id_fld[""Old_ID""] = out_ids\r\n        arcpy.da.ExtendTable(out_fc, oid_fld, id_fld, ""key"")\r\nelse:\r\n    msg = """"""\r\n    -----------------------------------------------------------------\r\n    Input data is not in a projected coordinate system....\r\n    bailing...\r\n    -----------------------------------------------------------------\r\n    """"""\r\n    tweet(msg)\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n'"
polygon_line_tools/Scripts/split_by_sector.py,20,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\nsplit_by_sector\r\n===============\r\n\r\nScript   :  split_by_sector.py\r\n\r\nAuthor   :  Dan.Patterson@carleton.ca\r\n\r\nModified :  2018-08-30\r\n\r\nPurpose  :  tools for working with numpy arrays\r\n\r\nSource :\r\n\r\nReferences:\r\n----------\r\n\r\n`<https://stackoverflow.com/questions/3252194/numpy-and-line-intersections>`_.\r\n`<https://community.esri.com/message/627051?commentID=627051#comment-627051>`\r\n`<https://community.esri.com/message/779043-re-how-to-divide-irregular-\r\npolygon-into-equal-areas-using-arcgis-105?commentID=779043#comment-779043>`\r\n\r\nThis is a good one\r\n`<https://tereshenkov.wordpress.com/2017/09/10/dividing-a-polygon-into-a-given\r\n-number-of-equal-areas-with-arcpy/>`\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport numpy as np\r\nfrom arcpytools_plt import(tweet, fc_info, get_polys)\r\nimport arcpy\r\n\r\n\r\nft={\'bool\': lambda x: repr(x.astype(\'int32\')),\r\n    \'float\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2,\r\n                    suppress=True, threshold=100,\r\n                    formatter=ft)\r\n\r\nscript = sys.argv[0]\r\n\r\n__all__ = [""plot_"",\r\n           ""to_polygon"",\r\n           ""cal_sect"",\r\n           ""sectors"",\r\n           ""process""\r\n           ]\r\n#---- functions ----\r\n\r\ndef plot_(pnts):\r\n    """"""plot a circle, arc sector etc\r\n    """"""\r\n    import matplotlib.pyplot as plt\r\n    import matplotlib\r\n    from matplotlib.patches import Polygon\r\n    from matplotlib.collections import PatchCollection\r\n    #x_min = pnts[:,0].min()\r\n    #x_max = pnts[:,0].max()\r\n    #y_min = pnts[:,1].min()\r\n    #y_max = pnts[:,1].max()\r\n    fig, ax = plt.subplots()\r\n    patches = []\r\n    # Points need to form a closed loopset closed to True if your 1st and\r\n    # last pnt aren\'t equal.\r\n    for i in pnts:\r\n        polygon = Polygon(i, closed=False)\r\n        patches.append(polygon)\r\n    p = PatchCollection(patches, cmap=matplotlib.cm.jet, alpha=1.0)\r\n    colors = 100*np.random.rand(len(patches))\r\n    p.set_array(np.array(colors))\r\n    #ax.set_xlim(x_min-0.5, x_max+0.5)  # (x_min, x_max)\r\n    #ax.set_ylim(y_min-0.5, y_max+0.5)  # y_min, y_max)\r\n    ax.add_collection(p)\r\n    plt.axis(\'equal\')\r\n    plt.show()\r\n#    plt.close()\r\n    #return fig, ax\r\n\r\n\r\ndef to_polygon(pnts):\r\n    """"""create polygons from list or array pairs.  pass [pnts] if you are only\r\n    creating one polygon.  multiple polygons will already be represented as a\r\n    list of lists of points.\r\n    In short, it expects a 3d array or its list equivalent\r\n        """"""\r\n    polygons = []\r\n    for pair in pnts:\r\n        pl = arcpy.Polygon(arcpy.Array([arcpy.Point(*xy) for xy in pair]))\r\n        polygons.append(pl)\r\n    return polygons\r\n\r\n\r\ndef cal_sect(poly, cutters, pnts, factor):\r\n    """"""Calculate the areas\r\n    """"""\r\n    # ---- have to intersect here\r\n    cuts = [cutters[i].intersect(poly, 4) for i in range(len(cutters))]\r\n    tot_area = poly.area\r\n    fract_areas = np.array([c.area/tot_area for c in cuts])\r\n    c_sum = np.cumsum(fract_areas)\r\n    f_list = np.linspace(0.0, 1.0, factor, endpoint=False)\r\n    idxs = [np.argwhere(c_sum <= i)[-1][0] for i in f_list[1:]]\r\n    splits = np.split(cuts, idxs, axis=0)\r\n    return idxs, splits\r\n\r\n\r\ndef sectors(radius=100, theta=0.5, xc=0.0, yc=0.0):\r\n    """"""Create sectors radiating out from the geometry center.  A circle is\r\n    created and the geometry is parsed adding the center point to each set\r\n    of points to form a polygon.  The first and last point can be duplicated\r\n    if needed.\r\n    """"""\r\n    def xy(x_s, y_s):\r\n        z = np.zeros((len(x_s), 2), \'float\')\r\n        z[:, 0] = x_s\r\n        z[:, 1] = y_s\r\n        return z\r\n\r\n    # ---- Make a circle first ----\r\n    angles = np.deg2rad(np.arange(180.0, -180.0-theta, step=-theta))\r\n    x_s = radius*np.cos(angles)     # X values\r\n    y_s = radius*np.sin(angles)     # Y values\r\n    pnts = xy(x_s, y_s)\r\n    # ----\r\n    fr = pnts[:-1]\r\n    too = pnts[1:]\r\n    cent = np.array([[xc, yc]])\r\n    z = np.array([[0., 0.]])\r\n    zs = z.repeat(len(fr), axis=0)\r\n    sect = np.array(list(zip(zs, fr, too))) + cent\r\n    pnts = pnts + cent\r\n    return sect, pnts\r\n\r\n\r\ndef process(in_polys):\r\n    """"""Process the splits\r\n\r\n    Parameters:\r\n    -----------\r\n    in_fc: text\r\n        input featureclass\r\n    out_fc: text\r\n        output featureclass\r\n    s_fac: integer\r\n        split factor\r\n\r\n    Requires:\r\n    ---------\r\n        sectors, to_polygon, cal_sect\r\n\r\n    Notes:\r\n    ------\r\n    You can fine-tune the analysis by changing the theta value from 1.0 to a\r\n    smaller value 360 circle sectors result when theta = 1, 720 when it equal\r\n    0.5.  Processing time changes minimally on a per polygon basis.\r\n    """"""\r\n    result_ = []\r\n    for i in range(len(in_polys)):\r\n        poly = in_polys[i]\r\n        ext = max(poly.extent.width, poly.extent.height)\r\n        xc, yc = cent = [poly.centroid.X, poly.centroid.Y]\r\n        sect, pnts = sectors(radius=ext, theta=0.5, xc=xc, yc=yc)  # theta=1\r\n        cutters = to_polygon(sect)\r\n        idxs, splits = cal_sect(poly, cutters, pnts, s_fac)\r\n        ps = np.split(pnts, np.array(idxs)+1)\r\n        new_polys = [np.vstack((cent, ps[i-1], ps[i][0], cent))\r\n                     for i in range(0, len(ps))]\r\n        r = to_polygon(new_polys)\r\n        rs = [i.intersect(poly, 4) for i in r]\r\n        #p = arcpy.Polygon(r[0])\r\n        result_.extend(rs)\r\n    return result_\r\n\r\n\r\n# ---- demo and tool section -------------------------------------------------\r\n# large Canada  Can_0_sp_lcc\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    in_pth = script.split(""/"")[:-2] + [""Polygon_lineTools.gdb""]\r\n    in_fc = ""/"".join(in_pth) + ""/shapes_mtm9""  # ""/Big""  #\r\n    out_fc = ""/"".join(in_pth) + ""/s1""\r\n    s_fac = 4\r\nelse:\r\n    testing = False\r\n    in_fc = sys.argv[1]\r\n    out_fc = sys.argv[2]\r\n    s_fac = int(sys.argv[3])\r\n\r\n# ---- for both\r\n#\r\nshp_fld, oid_fld, shp_type, SR = fc_info(in_fc)\r\n\r\n# ---- instant bail if not projected\r\nif SR.type == \'Projected\':\r\n    in_polys, out_ids = get_polys(in_fc)\r\n    out_polys = process(in_polys)\r\n    if not testing:\r\n        if arcpy.Exists(out_fc):\r\n            arcpy.Delete_management(out_fc)\r\n        arcpy.CopyFeatures_management(out_polys, out_fc)\r\n        out_ids = np.repeat(out_ids, s_fac)\r\n        id_fld = np.zeros((len(out_polys),),\r\n                          dtype=[(""key"", ""<i4""), (""Old_ID"", ""<i4"")])\r\n        id_fld[""key""] = np.arange(1, len(out_polys) + 1)\r\n        id_fld[""Old_ID""] = out_ids\r\n        arcpy.da.ExtendTable(out_fc, oid_fld, id_fld, ""key"")\r\nelse:\r\n    msg = """"""\r\n    -----------------------------------------------------------------\r\n    Input data is not in a projected coordinate system....\r\n    bailing...\r\n    -----------------------------------------------------------------\r\n    """"""\r\n    tweet(msg)\r\n\r\n#----------------------\r\nif __name__==""__main__"":\r\n    """"""Uncomment what you want to see""""""\r\n    #print(""Script... {}"".format(script))\r\n    #circ_pnts = _circle(radius=1, theta=30, xc=5, yc=5)\r\n    #print(""\\ncircle points...\\n{}"".format(circ_pnts))\r\n    #arc_pnts = _arc(radius=10, start=0, stop=90.5, step=5, xc=0.0, yc=0.0)\r\n    #print(""\\narc points...\\n{}"".format(arc_pnts))\r\n    #pnts = arc_sector()\r\n    #pnts = buffer_ring()\r\n    #multi_sector_demo()\r\n    #multiring_buffer_demo()\r\n\r\n'"
polygon_line_tools/Scripts/split_polys.py,18,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\nsplit_polys\r\n===========\r\n\r\nScript :   split_polys.py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified:  2018-09-07\r\n\r\nPurpose :  tools for working with numpy arrays\r\n\r\nNotes:\r\n-----\r\nThe xs and ys form pairs with the first and last points being identical\r\nThe pairs are constructed using n-1 to ensure that you don\'t form a\r\nline from identical points.\r\n\r\nFirst split polygon is a sample of a multipart.  Added 0, 0 and 0, 80\r\nback in\r\n\r\n>>> xs = [0., 0., 80., 0, 0., 100., 100., 0.]\r\n>>> ys = [0., 30., 30., 80., 100., 100., 0., 0.]\r\n>>> a = np.array(list(zip(xs, ys))) * 1.0  # --- must be floats\r\n>>> v = np.array([[50., 0], [50, 100.]])\r\n>>> ext = np.array([[0., 0], [0, 100.],[100, 100.], [100., 0.], [0., 0.]])\r\nreturn a, v\r\n\r\nReferences:\r\n----------\r\n\r\n`<https://stackoverflow.com/questions/3252194/numpy-and-line-intersections>`_.\r\n`<https://community.esri.com/message/627051?commentID=627051#comment-627051>`\r\n`<https://community.esri.com/message/779043-re-how-to-divide-irregular-\r\npolygon-into-equal-areas-using-arcgis-105?commentID=779043#comment-779043>`\r\n\r\nThis is a good one\r\n`<https://tereshenkov.wordpress.com/2017/09/10/dividing-a-polygon-into-a-given\r\n-number-of-equal-areas-with-arcpy/>`\r\n\r\n---------------------------------------------------------------------\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nimport math\r\nfrom textwrap import dedent\r\nimport numpy as np\r\nimport warnings\r\nfrom arcpytools_plt import tweet, fc_info, _poly_ext, trans_rot, cal_area\r\nimport arcpy\r\n\r\nwarnings.simplefilter(\'ignore\', FutureWarning)\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=5, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\n# ---- Do the work or run the demo ------------------------------------------\r\n#\r\n\r\nfrmt = """"""\r\nInput features.... {}\r\nOutput features... {}\r\nNumber of splits . {}\r\nSplit types ...... {}\r\n""""""\r\n\r\n\r\ndef _cut_poly(poly, p_id, step=1.0, split_fac=4, SR=None):\r\n    """"""Perform the poly* cutting and return the result.\r\n\r\n    step : number\r\n        fractional step for division, 1.0 equates to 1%\r\n    split_face : number\r\n        number of areas to produce, 4, means split into 4 equal areas\r\n    """"""\r\n    L, B, R, T = _poly_ext(poly)\r\n    dx = 100.0/step\r\n    s_fac = math.ceil((R - L)/dx)\r\n    lefts = np.linspace(L+dx, R, num=s_fac, endpoint=True)\r\n    splitters = np.array([[[l, B-1.0], [l, T+1.0]] for l in lefts])\r\n#    n = []\r\n#    for i in splitters:\r\n#        n.append(trans_rot(i, 45))\r\n#    splitters = np.array(n)\r\n    cutters = []\r\n    for s in splitters:\r\n        s = s.tolist()\r\n        c = arcpy.Polyline(arcpy.Array([arcpy.Point(*xy) for xy in s]), SR)\r\n        cutters.append(c)\r\n    # ----\r\n    cuts = []\r\n    for i in cutters:\r\n        rght = poly\r\n        if i.crosses(poly):\r\n            try:\r\n                left, rght = poly.cut(i)\r\n                if rght is None:\r\n                    cuts.append(left)\r\n                cuts.append(left)\r\n                poly = rght\r\n                rght = left\r\n            except RuntimeError:\r\n                tweet(""Issues with poly...{}"".format(p_id))\r\n                continue\r\n        else:\r\n            cuts.append(rght)\r\n    return cuts, cutters\r\n\r\n\r\ndef get_polys(in_fc):\r\n    """"""Return polygons from a polygon featureclass\r\n    """"""\r\n    out_polys = []\r\n    out_ids = []\r\n    with arcpy.da.SearchCursor(in_fc,  [""SHAPE@"", ""OID@""]) as cursor:\r\n        for row in cursor:\r\n            out_polys.append(row[0])\r\n            out_ids.append(row[1])\r\n    return out_polys, out_ids\r\n\r\n\r\ndef do_work(in_fc, out_fc, s_type, s_fac, s_axis):\r\n    """"""Do the actual work for either the demo or the tool\r\n\r\n    Requires:\r\n    --------\r\n    in_fc : feautureclass\r\n        polygon or polyline featureclass\r\n    out_fc : featureclass\r\n        same as input\r\n    s_type : choice of `extent` or `areas`\r\n        `extent` uses the bounds with `s_fac` to subdivide the range into\r\n        sub-polygons\r\n\r\n    **extent option**\r\n\r\n    x_tent : extent parameter\r\n        LBRT L(eft), B(ottom), R(ight), T(op) in the featureclass units\r\n    s_fac : number\r\n        used to represent the divisions, for example, a factor of 2 is the\r\n        same as half (1/2) or 50%\r\n\r\n    **width option**\r\n\r\n    s_fac : number\r\n        represents the distance for the cut spacing for the x or y directions\r\n\r\n    **area option**\r\n\r\n    s_perc : double\r\n        Percentage of the total area representing sub-area size. 25% would\r\n        result in 4 sub-areas with approximately 25% of the total area.\r\n    """"""\r\n#    def __poly_ext__(p):\r\n#        """"""poly* extent\r\n#        """"""\r\n#        L, B = p.extent.lowerLeft.X, p.extent.lowerLeft.Y\r\n#        R, T = p.extent.upperRight.X, p.extent.upperRight.Y\r\n#        return L, B, R, T\r\n\r\n    def __cutter__(p, s_type, s_fac, s_axis):\r\n        """"""Produce the cutters for the shape\r\n        fac = np.linspace(L, R, num=divisor+1, endpoint=True)\r\n        steps, incr = np.linspace(ax_min, ax_max, num=pieces+1,\r\n                              endpoint=True, retstep=True)\r\n        """"""\r\n        L, B, R, T = _poly_ext(p)\r\n        if s_type == \'extent\':\r\n            dx = (R - L)/s_fac\r\n            dy = (T - B)/s_fac\r\n        elif s_type == \'distance\':  # just use the s_fact\r\n            dx = s_fac\r\n            dy = s_fac\r\n            if s_axis == \'X\':\r\n                s_fac = math.ceil((R - L)/s_fac)\r\n            else:\r\n                s_fac = math.ceil((T - B)/s_fac)\r\n        elif s_type == \'area\':\r\n            dx = s_fac\r\n            dy = s_fac\r\n        else:\r\n            dx = 2\r\n            dy = 2\r\n        if s_axis == \'X\':\r\n            lefts = np.linspace(L+dx, R, num=s_fac, endpoint=True)\r\n            splitters = np.array([[[l, B-1.0], [l, T+1.0]] for l in lefts])\r\n        elif s_axis == \'Y\':\r\n            tops = np.linspace(B+dy, T, num=s_fac, endpoint=True)\r\n            splitters = np.array([[[R+1.0, t], [L-1.0, t]] for t in tops])\r\n        cutters = []\r\n        for s in splitters:\r\n            s = s.tolist()\r\n            c = arcpy.Polyline(arcpy.Array([arcpy.Point(*xy) for xy in s]), SR)\r\n            cutters.append(c)\r\n        return cutters\r\n    #\r\n    def __get_cutters__(poly, s_fac, s_axis):\r\n        """"""Get the poly* cutters\r\n        """"""\r\n        cutters = __cutter__(poly, s_fac, s_axis)\r\n        return cutters\r\n    #\r\n    def __cut__(poly, p_id, cutters):\r\n        """"""Perform the poly* cutting and return the result.\r\n        """"""\r\n        cuts = []\r\n        for i in cutters:\r\n            rght = poly\r\n            if i.crosses(poly):\r\n                try:\r\n                    left, rght = poly.cut(i)\r\n                    if rght is None:\r\n                        cuts.append(left)\r\n                    cuts.append(left)\r\n                    poly = rght\r\n                    rght = left\r\n                except RuntimeError:\r\n                    tweet(""Issues with poly...{}"".format(p_id))\r\n                    continue\r\n            else:\r\n                cuts.append(rght)\r\n        return cuts\r\n    # ---- Do the work -------------------------------------------------------\r\n    #\r\n    s_fac = int(s_fac)\r\n    #\r\n    # (1) get the polys and the cutters\r\n    out_polys = []\r\n    out_ids = []\r\n    with arcpy.da.SearchCursor(in_fc,  [""SHAPE@"", ""OID@""]) as cursor:\r\n        for row in cursor:\r\n            poly = row[0]\r\n            p_id = row[1]\r\n            cutters = __cutter__(poly, s_type, s_fac, s_axis)\r\n            cuts = __cut__(poly, p_id, cutters)\r\n            out_polys.extend(cuts)\r\n            out_ids.append(p_id)\r\n    #\r\n    for p in out_polys:\r\n        if not (p.area > 0.):\r\n            out_polys.remove(p)\r\n    return out_polys, out_ids\r\n\r\n\r\n# ---- demo and tool section -------------------------------------------------\r\n#\r\n""""""\r\n\r\n""""""\r\n\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    in_pth = script.split(""/"")[:-2] + [""Polygon_lineTools.gdb""]\r\n    in_fc = ""/"".join(in_pth) + ""/shapes_mtm9""\r\n    out_fc = ""/"".join(in_pth) + ""/c0""\r\n    s_type = \'extent\'\r\n    s_fac = 4\r\n    s_axis = \'X\'\r\n    shp_fld, oid_fld, shp_type, SR = fc_info(in_fc)\r\n    out_polys, out_ids = get_polys(in_fc)\r\n\r\nelse:\r\n    in_fc = sys.argv[1]\r\n    out_fc = sys.argv[2]\r\n    s_type = sys.argv[3]\r\n    # --- extent parameters\r\n    s_fac = int(sys.argv[4])\r\n    s_axis = sys.argv[5]\r\n    shp_fld, oid_fld, shp_type, SR = fc_info(in_fc)\r\n    out_polys, out_ids = do_work(in_fc, out_fc, s_type, s_fac, s_axis)\r\n    for p in out_polys:\r\n        if not (p.area > 0.):\r\n            out_polys.remove(p)\r\n    if arcpy.Exists(out_fc):\r\n        arcpy.Delete_management(out_fc)\r\n    arcpy.CopyFeatures_management(out_polys, out_fc)\r\n    out_ids = np.repeat(out_ids, s_fac)\r\n    id_fld = np.zeros((len(out_polys),),\r\n                      dtype=[(""key"", ""<i4""), (""Old_ID"", ""<i4"")])\r\n    id_fld[""key""] = np.arange(1, len(out_polys) + 1)\r\n    id_fld[""Old_ID""] = out_ids\r\n    arcpy.da.ExtendTable(out_fc, oid_fld, id_fld, ""key"")\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n'"
polygon_line_tools/Scripts/transect_lines.py,14,"b'# -*- coding: utf-8 -*-\r\n""""""\r\nscript name here\r\n=======\r\n\r\nScript :   template.py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified : 2019-\r\n\r\nPurpose :  Tools for\r\n\r\nNotes:\r\n\r\nReferences:\r\n\r\n""""""\r\n# pylint: disable=C0103  # invalid-name\r\n# pylint: disable=R0914  # Too many local variables\r\n# pylint: disable=R1710  # inconsistent-return-statements\r\n# pylint: disable=W0105  # string statement has no effect\r\n\r\nimport sys\r\nimport numpy as np\r\nimport arcpy\r\n\r\narcpy.overwriteOutputs = True\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n# ===========================================================================\r\n# ---- def section: def code blocks go here ---------------------------------\r\n#\r\ndef transect_lines(N=5, orig=None, dist=1, x_offset=0, y_offset=0,\r\n                   bearing=0, as_ndarray=True):\r\n    """"""Construct transect lines from origin-destination points given a\r\n    distance and bearing from the origin point\r\n\r\n    Parameters\r\n    ----------\r\n    N : number\r\n        The number of transect lines\r\n    orig : array-like\r\n         A single origin.  If None, the cartesian origin (0, 0) is used\r\n    dist : number or array-like\r\n        The distance(s) from the origin\r\n    x_offset, y_offset : number\r\n        If the `orig` is a single location, you can construct offset lines\r\n        using these values\r\n    bearing : number or array-like\r\n        If a single number, parallel lines are produced. An array of values\r\n        equal to the `orig` can be used.\r\n\r\n    Returns\r\n    -------\r\n    Two outputs are returned, the first depends on the `as_ndarray` setting.\r\n\r\n    1. True, a structured array. False - a recarray\r\n    2. An ndarray with the field names in case the raw data are required.\r\n\r\n    Notes\r\n    -----\r\n    It is easiest of you pick a `corner`, then use x_offset, y_offset to\r\n    control whether you are moving horizontally and vertically from the origin.\r\n    The bottom left is easiest, and positive offsets move east and north from.\r\n\r\n    Use XY to Line tool in ArcGIS Pro to convert the from/to pairs to a line.\r\n    See references\r\n\r\n    Examples\r\n    --------\r\n    >>> out, data = transect_lines(N=5, orig=None,\r\n                                   dist=100, x_offset=10,\r\n                                   y_offset=0, bearing=45, as_ndarray=True)\r\n    >>> data\r\n    array([[  0.  ,   0.  ,  70.71,  70.71],\r\n           [ 10.  ,   0.  ,  80.71,  70.71],\r\n           [ 20.  ,   0.  ,  90.71,  70.71],\r\n           [ 30.  ,   0.  , 100.71,  70.71],\r\n           [ 40.  ,   0.  , 110.71,  70.71]])\r\n    >>> out\r\n    array([( 0., 0.,  70.71, 70.71), (10., 0.,  80.71, 70.71),\r\n    ...    (20., 0.,  90.71, 70.71), (30., 0., 100.71, 70.71),\r\n    ...    (40., 0., 110.71, 70.71)],\r\n    ...   dtype=[(\'X_from\', \'<f8\'), (\'Y_from\', \'<f8\'),\r\n    ...          (\'X_to\', \'<f8\'), (\'Y_to\', \'<f8\')])\r\n    ...\r\n    ... Create the table and the lines\r\n    >>> tbl = \'c:/folder/your.gdb/table_name\'\r\n    >>> # arcpy.da.NumPyArrayToTable(a, tbl)\r\n    >>> # arcpy.XYToLine_management(\r\n    ... #       in_table, out_featureclass,\r\n    ... #       startx_field, starty_field, endx_field, endy_field,\r\n    ... #       {line_type}, {id_field}, {spatial_reference}\r\n    ... This is general syntax, the first two are paths of source and output\r\n    ... files, followed by coordinates and options parameters.\r\n    ...\r\n    ... To create compass lines\r\n    >>> b = np.arange(0, 361, 22.5)\r\n    >>> a, data = transect_lines(N=10, orig=[299000, 4999000],\r\n                                 dist=100, x_offset=0, y_offset=0,\r\n                                 bearing=b, as_ndarray=True)\r\n\r\n    References\r\n    ----------\r\n    `<https://community.esri.com/blogs/dan_patterson/2019/01/17/transect-\r\n    lines-parallel-lines-offset-lines>`_.\r\n\r\n    `<http://pro.arcgis.com/en/pro-app/tool-reference/data-management\r\n    /xy-to-line.htm>`_.\r\n    """"""\r\n    def _array_struct_(a, fld_names=[\'X\', \'Y\'], kinds=[\'<f8\', \'<f8\']):\r\n        """"""Convert an array to a structured array""""""\r\n        dts = list(zip(fld_names, kinds))\r\n        z = np.zeros((a.shape[0],), dtype=dts)\r\n        for i in range(a.shape[1]):\r\n            z[fld_names[i]] = a[:, i]\r\n        return z\r\n    #\r\n    if orig is None:\r\n        orig = np.array([0., 0.])\r\n    args = [orig, dist, bearing]\r\n    arrs = [np.atleast_1d(i) for i in args]\r\n    orig, dist, bearing = arrs\r\n    # o_shp, d_shp, b_shp = [i.shape for i in arrs]\r\n    #\r\n    rads = np.deg2rad(bearing)\r\n    dx = np.sin(rads) * dist\r\n    dy = np.cos(rads) * dist\r\n    #\r\n    n = len(bearing)\r\n    N = [N, n][n > 1]  # either the number of lines or bearings\r\n    x_orig = np.arange(N) * x_offset + orig[0]\r\n    y_orig = np.arange(N) * y_offset + orig[1]\r\n    x_dest = x_orig + dx\r\n    y_dest = y_orig + dy\r\n    # ---- create the output array\r\n    names = [\'X_from\', \'Y_from\', \'X_to\', \'Y_to\']\r\n    cols = len(names)\r\n    kind = [\'<f8\']*cols\r\n    data = np.vstack([x_orig, y_orig, x_dest, y_dest]).T\r\n    if as_ndarray:  # **** add this as a flag\r\n        out = _array_struct_(data, fld_names=names, kinds=kind)\r\n    else:\r\n        out = data.transpose()\r\n        out = np.core.records.fromarrays(out, names=names, formats=kind)\r\n    return out  #, data\r\n\r\n\r\n# ===========================================================================\r\n# ---- main section: testing or tool run ------------------------------------\r\n# transect_lines(N=5, orig=None, dist=1, x_offset=0, y_offset=0,\r\n#                   bearing=0, as_ndarray=True)\r\nif len(sys.argv) == 1:\r\n    create_output = True\r\n    in_pth = script.split(""/"")[:-2] + [""Polygon_lineTools.gdb""]\r\n    origin = ""Bottom left""  # sys.argv[1]\r\n    origin_x = 300100.      # sys.argv[2] orig = [origin_x, origin_y]\r\n    origin_y = 5000000.     # sys.argv[3]\r\n    N = 10                  # sys.argv[4] transects\r\n    dist = 1000.            # sys.argv[5] line length\r\n    x_offset = 50          # sys.argv[6] \r\n    y_offset = 0          # sys.argv[7]\r\n    dens_dist = 100\r\n    bearing = 0.0           # sys.argv[8]\r\n    out_fc = ""/"".join(in_pth) + ""/a0""  # sys.argv[9]\r\n    SR = 2951\r\n    #\r\n    if origin == ""Top left"":\r\n        y_offset = -y_offset\r\n    orig = [origin_x, origin_y]\r\n    arr = transect_lines(N, orig, dist, x_offset, y_offset, bearing, True)\r\nelse:\r\n    create_output = True\r\n    origin = sys.argv[1]\r\n    origin_x = float(sys.argv[2])\r\n    origin_y = float(sys.argv[3])\r\n    N = int(sys.argv[4])\r\n    dist = float(sys.argv[5])\r\n    x_offset = float(sys.argv[6])\r\n    y_offset = float(sys.argv[7])\r\n    bearing = float(sys.argv[8])\r\n    out_fc = sys.argv[9]\r\n    SR = sys.argv[10]\r\n    #\r\n    if origin == ""Top left"":\r\n        y_offset = -y_offset\r\n    orig = [origin_x, origin_y]\r\n    arr = transect_lines(N, orig, dist, x_offset, y_offset, bearing, True)\r\n\r\n# ---- Create the table and the lines\r\nif create_output:\r\n    tbl = out_fc\r\n    arcpy.da.NumPyArrayToTable(arr, tbl)\r\n    out_fc = tbl + ""_t""\r\n    arcpy.XYToLine_management(tbl, out_fc, \'X_from\', \'Y_from\', \'X_to\', \'Y_to\',\r\n                              spatial_reference=SR)\r\n    ... #       startx_field, starty_field, endx_field, endy_field,\r\n    ... #       {line_type}, {id_field}, {spatial_reference}\r\n\r\n# ==== Processing finished ====\r\n# ===========================================================================\r\n#\r\nif __name__ == ""__main__"":\r\n    """"""optional location for parameters""""""\r\n    \r\n\r\n'"
polygon_line_tools/Scripts/triangulate.py,11,"b'# -*- coding: utf-8 -*-\r\n""""""\r\ntriangulate\r\n===========\r\n\r\nScript :   triangulate.py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified : 2018-08-24\r\n\r\nPurpose:  triangulate poly* features\r\n\r\nUseage :\r\n\r\nReferences\r\n----------\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/numpyarraytotable.htm>`_.\r\n`<http://pro.arcgis.com/en/pro-app/arcpy/data-access/tabletonumpyarray.htm>`_.\r\n`<https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/\r\nscipy.spatial.Delaunay.html>`_.\r\n---------------------------------------------------------------------\r\n""""""\r\n\r\nimport sys\r\nimport numpy as np\r\nfrom scipy.spatial import Delaunay, Voronoi\r\nfrom arcpytools_plt import fc_info #, tweet  #, frmt_rec, _col_format\r\nimport arcpy\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\narcpy.env.overwriteOutput = True\r\n\r\n# ----\r\n#\r\ndef _xyID(in_fc, to_pnts=True):\r\n    """"""Convert featureclass geometry (in_fc) to a simple 2D structured array\r\n    :  with ID, X, Y values. Optionally convert to points, otherwise centroid.\r\n    """"""\r\n    flds = [\'OID@\', \'SHAPE@X\', \'SHAPE@Y\']\r\n    args = [in_fc, flds, None, None, to_pnts, (None, None)]\r\n    cur = arcpy.da.SearchCursor(*args)\r\n    a = cur._as_narray()\r\n    a.dtype = [(\'IDs\', \'<i4\'), (\'Xs\', \'<f8\'), (\'Ys\', \'<f8\')]\r\n    del cur\r\n    return a\r\n\r\n\r\ndef poly(pnt_groups, SR):\r\n    """"""Short form polygon creation\r\n    """"""\r\n    polygons = []\r\n    for pnts in pnt_groups:\r\n        for pair in pnts:\r\n            arr = arcpy.Array([arcpy.Point(*xy) for xy in pair])\r\n            pl = arcpy.Polygon(arr, SR)\r\n            polygons.append(pl)\r\n    return polygons\r\n\r\n\r\ndef vor_pnts(pnts, ri_type=""Voronoi"", testing=False):\r\n    """"""not used with polygons""""""\r\n    out = []\r\n    for ps in pnts:\r\n        avg = np.mean(ps, axis=0)\r\n        p =  ps - avg\r\n        tri = Voronoi(p)\r\n        for region in tri.regions:\r\n            if not -1 in region:\r\n                polygon = np.array([tri.vertices[i] + avg for i in region])\r\n                out.append(polygon)\r\n                if testing:\r\n                    print(""{}"".format(polygon.T))\r\n    #ts = [i for i in t if i.ndim == 2]\r\n    return out\r\n\r\n\r\ndef tri_pnts(pnts, testing=False):\r\n    """"""Triangulate the points and return the triangles\r\n\r\n    Parameters:\r\n    -----------\r\n    pnts : np.array\r\n        Points in array format.\r\n    out : array\r\n        an array of triangle points\r\n\r\n    Notes:\r\n    ------\r\n    >>> pnts = pnts.reshape((1,) + pnts.shape)  # a 3D set of points (ndim=3)\r\n    >>> [pnts]  # or pass in as a list\r\n    """"""\r\n    out = []\r\n    for ps in pnts:\r\n        ps = np.unique(ps, axis=0)  # get the unique points only\r\n        avg = np.mean(ps, axis=0)\r\n        p =  ps - avg\r\n        tri = Delaunay(p)\r\n        simps = tri.simplices\r\n        new_pnts = [p[s]+avg for s in simps]\r\n        if testing:\r\n            print(""{}"".format(new_pnts))\r\n        out.append(new_pnts)\r\n    return out\r\n\r\n\r\ndef pnt_groups(in_fc):\r\n    """"""Simple def to convert shapes to points from a featureclass\r\n    """"""\r\n    shp_fld, oid_fld, shp_type, SR = fc_info(in_fc)\r\n    flds = [\'OID@\', \'SHAPE@X\', \'SHAPE@Y\']\r\n    args = [in_fc, flds, None, None, True, (None, None)]\r\n    cur = arcpy.da.SearchCursor(*args)\r\n    a = cur._as_narray()\r\n    a.dtype = [(\'IDs\', \'<i4\'), (\'Xs\', \'<f8\'), (\'Ys\', \'<f8\')]\r\n    del cur\r\n    pts = []\r\n    keys = np.unique(a[\'IDs\'])\r\n    for k in keys:\r\n        w = np.where(a[\'IDs\'] == k)[0]\r\n        z = a[[\'Xs\', \'Ys\']][w[0]:w[-1] + 1]\r\n        z = np.copy(z.view(np.float64).reshape(z.shape[0], 2))\r\n        pts.append(z)\r\n    return pts, a, SR\r\n\r\n# ---- Do the work\r\n#\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    in_pth = script.split(""/"")[:-2] + [""Polygon_lineTools.gdb""]\r\n    in_fc = ""/"".join(in_pth) + ""/shapes_mtm9"" #/Densified_4x""\r\n    out_fc = ""/"".join(in_pth) + ""/v3""\r\n#    keep_flds = ""*""\r\n#    shp_fld, oid_fld, shp_type, SR = fc_info(in_fc)\r\n#    pts, a, SR = pnt_groups(in_fc)\r\n#    t = tri_pnts(pts, True)\r\n#    polys = poly(t, SR)\r\n#    arcpy.CopyFeatures_management(polys, ""in_memory/temp"")\r\n#    arcpy.analysis.Clip(""in_memory/temp"", in_fc, out_fc, None)\r\nelse:\r\n    in_fc = sys.argv[1]\r\n    out_fc = sys.argv[2]\r\n\r\n# finish up\r\n#\r\n#keep_flds = ""*""\r\n#shp_fld, oid_fld, shp_type, SR = fc_info(in_fc)\r\nif not testing:\r\n    pts, a, SR = pnt_groups(in_fc)\r\n    t = tri_pnts(pts, True)\r\n    polys = poly(t, SR)\r\n    if arcpy.Exists(out_fc):\r\n        arcpy.Delete_management(out_fc)\r\n    arcpy.CopyFeatures_management(polys, ""in_memory/temp"")\r\n    arcpy.MakeFeatureLayer_management(""in_memory/temp"", ""temp"")\r\n    arcpy.management.SelectLayerByLocation(""temp"",\r\n                                           ""WITHIN_CLEMENTINI"",\r\n                                           in_fc, None,\r\n                                           ""NEW_SELECTION"", ""NOT_INVERT"")\r\n    arcpy.CopyFeatures_management(""temp"", out_fc)\r\n\r\n    #arcpy.analysis.Clip(""in_memory/temp"", in_fc, out_fc, None)\r\n    #\r\n    #arcpy.analysis.Intersect([""in_memory/temp"",in_fc], out_fc,\r\n    #                         ""ONLY_FID"", None, ""INPUT"")\r\n\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n'"
triangulation_tools/Scripts/arcpytools_pnt.py,11,"b'# -*- coding: UTF-8 -*-\r\n""""""\r\narcpytools_pnt\r\n==============\r\n\r\nScript:   arcpytools_pnt.py\r\n\r\nAuthor:   Dan.Patterson@carleton.ca\r\n\r\nModified: 2018-08-22\r\n\r\nPurpose:  tools for working with numpy arrays\r\n\r\nUseage:\r\n\r\nReferences:\r\n\r\n:---------------------------------------------------------------------:\r\n""""""\r\n# ---- imports, formats, constants ----\r\nimport sys\r\nfrom textwrap import dedent, indent\r\nimport numpy as np\r\nimport arcpy\r\n# from arcpytools import array_fc, array_struct, tweet\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\n\r\nnp.set_printoptions(edgeitems=10, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\n\r\n__all__ = [\'_xyID\']\r\n\r\n\r\ndef tweet(msg):\r\n    """"""Produce a message for both arcpy and python\r\n    : msg - a text message\r\n    """"""\r\n    m = ""{}"".format(msg)\r\n    arcpy.AddMessage(m)\r\n    print(m)\r\n\r\n\r\n#def _describe(in_fc=None):\r\n#    """"""Simply return the arcpy.da.Describe object\r\n#    : desc.keys() an abbreviated list...\r\n#    : [... \'OIDFieldName\'... \'areaFieldName\', \'baseName\'... \'catalogPath\',\r\n#    :  ... \'dataType\'... \'extent\', \'featureType\', \'fields\', \'file\'... \'hasM\',\r\n#    :  \'hasOID\', \'hasZ\', \'indexes\'... \'lengthFieldName\'... \'name\', \'path\',\r\n#    :  \'rasterFieldName\', ..., \'shapeFieldName\', \'shapeType\',\r\n#    :  \'spatialReference\',  ...]\r\n#    """"""\r\n#    if in_fc is None:\r\n#        return None\r\n#    else:\r\n#        return arcpy.da.Describe(in_fc)\r\n\r\ndef fc_info(in_fc, prn=False):\r\n    """"""Return basic featureclass information, including...\r\n\r\n    Parameters:\r\n    -----------\r\n    shp_fld : field\r\n        field name which contains the geometry object\r\n    oid_fld : field\r\n        the object index/id field name\r\n    SR : spatial reference\r\n        spatial reference object (use SR.name to get the name)\r\n    shp_type : string\r\n        shape type (Point, Polyline, Polygon, Multipoint, Multipatch)\r\n    others : options\r\n        \'areaFieldName\', \'baseName\', \'catalogPath\',\'featureType\',\'fields\',\r\n        \'hasOID\', \'hasM\', \'hasZ\', \'path\'\r\n    all_flds : list\r\n         [i.name for i in desc[\'fields\']]\r\n    """"""\r\n    desc = arcpy.da.Describe(in_fc)\r\n    args = [\'shapeFieldName\', \'OIDFieldName\', \'shapeType\', \'spatialReference\']\r\n    shp_fld, oid_fld, shp_type, SR = [desc[i] for i in args]\r\n    if prn:\r\n        frmt = ""FeatureClass:\\n   {}"".format(in_fc)\r\n        f = ""\\n{!s:<16}{!s:<14}{!s:<10}{!s:<10}""\r\n        frmt += f.format(*args)\r\n        frmt += f.format(shp_fld, oid_fld, shp_type, SR.name)\r\n        tweet(frmt)\r\n    else:\r\n        return shp_fld, oid_fld, shp_type, SR\r\n\r\n\r\n# ---- geometry related -----------------------------------------------------\r\n#\r\ndef _xyID(in_fc, to_pnts=True):\r\n    """"""Convert featureclass geometry (in_fc) to a simple 2D structured array\r\n    :  with ID, X, Y values. Optionally convert to points, otherwise centroid.\r\n    """"""\r\n    flds = [\'OID@\', \'SHAPE@X\', \'SHAPE@Y\']\r\n    args = [in_fc, flds, None, None, to_pnts, (None, None)]\r\n    cur = arcpy.da.SearchCursor(*args)\r\n    a = cur._as_narray()\r\n    a.dtype = [(\'IDs\', \'<i4\'), (\'Xs\', \'<f8\'), (\'Ys\', \'<f8\')]\r\n    del cur\r\n    return a\r\n\r\n\r\ndef array_struct(a, fld_names=[\'X\', \'Y\'], dt=[\'<f8\', \'<f8\']):\r\n    """"""Convert an array to a structured array\r\n\r\n    Parameters:\r\n    -----------\r\n    a : array\r\n        an ndarray with shape at least (N, 2)\r\n    dt : string\r\n        dtype class\r\n    names : string or list of strings\r\n        names for the fields\r\n    """"""\r\n    dts = [(fld_names[i], dt[i]) for i in range(len(fld_names))]\r\n    z = np.zeros((a.shape[0],), dtype=dts)\r\n    names = z.dtype.names\r\n    for i in range(a.shape[1]):\r\n        z[names[i]] = a[:, i]\r\n    return z\r\n\r\n\r\ndef array_fc(a, out_fc, fld_names, SR):\r\n    """"""Array to featureclass/shapefile...optionally including all fields\r\n\r\n    Parameters:\r\n    -----------\r\n    out_fc : string\r\n        featureclass/shapefile... complete path\r\n    fld_names : string or list of strings\r\n        the Shapefield name ie [\'Shape\'] or [\'X\', \'Y\'s]\r\n    SR : spatial reference\r\n        spatial reference object (use SR.name to get the name)\r\n    See also :\r\n        NumpyArrayToFeatureClass, ListFields for information and options\r\n    """"""\r\n    if arcpy.Exists(out_fc):\r\n        arcpy.Delete_management(out_fc)\r\n    arcpy.da.NumPyArrayToFeatureClass(a, out_fc, fld_names, SR)\r\n    return out_fc\r\n\r\n\r\ndef fc_array(in_fc, flds, allpnts):\r\n    """"""Convert a featureclass to an ndarray...with optional fields besides the\r\n    FID/OIDName and Shape fields.\r\n\r\n    Parameters:\r\n    -----------\r\n    in_fc : text\r\n        Full path to the geodatabase and the featureclass name\r\n\r\n    flds : text or list\r\n        - ``\'\'   : just an object id and shape field``\r\n        - ``\'*\'  : all fields in the featureclass or``\r\n        - ``list : specific fields [\'OBJECTID\',\'Shape\',\'SomeClass\', etc]``\r\n\r\n    allpnts : boolean\r\n        - True `explodes` geometry to individual points.\r\n        - False returns the centroid\r\n\r\n    Requires:\r\n    ---------\r\n        fc_info(in_fc) function\r\n\r\n    See also:\r\n    ---------\r\n        FeatureClassToNumPyArray, ListFields for more information in current\r\n        arcpy documentation\r\n    """"""\r\n    out_flds = []\r\n    shp_fld, oid_fld, shp_type, SR = fc_info(in_fc)  # get the base information\r\n    fields = arcpy.ListFields(in_fc)      # all fields in the shapefile\r\n    if flds == """":                        # return just OID and Shape field\r\n        out_flds = [oid_fld, shp_fld]     # FID and Shape field required\r\n    elif flds == ""*"":                     # all fields\r\n        out_flds = [f.name for f in fields]\r\n    else:\r\n        out_flds = [oid_fld, shp_fld]\r\n        for f in fields:\r\n            if f.name in flds:\r\n                out_flds.append(f.name)\r\n    frmt = """"""\\nRunning \'fc_array\' with ....\r\n    \\nfeatureclass... {}\\nFields... {}\\nAll pnts... {}\\nSR... {}\r\n    """"""\r\n    args = [in_fc, out_flds, allpnts, SR.name]\r\n    msg = dedent(frmt).format(*args)\r\n    tweet(msg)\r\n    a = arcpy.da.FeatureClassToNumPyArray(in_fc, out_flds, """", SR, allpnts)\r\n    # out it goes in array format\r\n    return a, out_flds, SR\r\n\r\n\r\ndef arr2pnts(in_fc, as_struct=True, shp_fld=None, SR=None):\r\n    """"""Create points from an array.\r\n    :  in_fc - input featureclass\r\n    :  as_struct - if True, returns a structured array with X, Y fields,\r\n    :            - if False, returns an ndarray with dtype=\'<f8\'\r\n    :Notes: calls fc_info to return featureclass information\r\n    """"""\r\n    if shp_fld is None or SR is None:\r\n        shp_fld, oid_fld, SR = fc_info(in_fc)\r\n    a = arcpy.da.FeatureClassToNumPyArray(in_fc, ""*"", """", SR)\r\n    dt = [(\'X\', \'<f8\'), (\'Y\', \'<f8\')]\r\n    if as_struct:\r\n        shps = np.array([tuple(i) for i in a[shp_fld]], dtype=dt)\r\n    else:\r\n        shps = a[shp_fld]\r\n    return shps, shp_fld, SR\r\n\r\n\r\ndef arr2line(a, out_fc, SR=None):\r\n    """"""create lines from an array""""""\r\n    pass\r\n\r\n\r\ndef shapes2fc(shps, out_fc):\r\n    """"""Create a featureclass/shapefile from poly* shapes.\r\n    :  out_fc - full path and name to the output container (gdb or folder)\r\n    """"""\r\n    msg = ""\\nCan\'t overwrite the {}... rename"".format(out_fc)\r\n    try:\r\n        if arcpy.Exists(out_fc):\r\n            arcpy.Delete_management(out_fc)\r\n        arcpy.CopyFeatures_management(shps, out_fc)\r\n    except ValueError:\r\n        tweet(msg)\r\n\r\n\r\ndef arr2polys(a, out_fc, oid_fld, SR):\r\n    """"""Make poly* features from a structured array.\r\n    :  a - structured array\r\n    :  out_fc: a featureclass path and name, or None\r\n    :  oid_fld - object id field, used to partition the shapes into groups\r\n    :  SR - spatial reference object, or name\r\n    :Returns:\r\n    :-------\r\n    :  Produces the featureclass optionally, but returns the polygons anyway.\r\n    """"""\r\n    arcpy.overwriteOutput = True\r\n    pts = []\r\n    keys = np.unique(a[oid_fld])\r\n    for k in keys:\r\n        w = np.where(a[oid_fld] == k)[0]\r\n        v = a[\'Shape\'][w[0]:w[-1] + 1]\r\n        pts.append(v)\r\n    # Create a Polygon from an Array of Points, save to featueclass if needed\r\n    s = []\r\n    for pt in pts:\r\n        s.append(arcpy.Polygon(arcpy.Array([arcpy.Point(*p) for p in pt]), SR))\r\n    return s\r\n\r\n\r\ndef output_polylines(out_fc, SR, pnt_groups):\r\n    """"""Produce the output polygon featureclass.\r\n    :Requires:\r\n    :--------\r\n    : - A list of lists of points\r\n    :   aline = [[[0, 0], [1, 1]]]  # a list of points\r\n    :   aPolyline = [[aline]]       # a list of lists of points\r\n    """"""\r\n    msg = \'\\nRead the script header... A projected coordinate system required\'\r\n    assert (SR is not None), msg\r\n    polylines = []\r\n    for pnts in pnt_groups:\r\n        for pair in pnts:\r\n            arr = arcpy.Array([arcpy.Point(*xy) for xy in pair])\r\n            pl = arcpy.Polyline(arr, SR)\r\n            polylines.append(pl)\r\n    if arcpy.Exists(out_fc):     # overwrite any existing versions\r\n        arcpy.Delete_management(out_fc)\r\n    arcpy.CopyFeatures_management(polylines, out_fc)\r\n    return\r\n\r\n\r\ndef output_polygons(out_fc, SR, pnt_groups):\r\n    """"""Produce the output polygon featureclass.\r\n\r\n    Parameters:\r\n    -----------\r\n    out_fc : string\r\n        The path and name of the featureclass to be created.\r\n    SR : spatial reference of the output featureclass\r\n    pnts_groups :\r\n        The point groups, list of lists of points, to include parts rings.\r\n\r\n    Requires:\r\n    --------\r\n\r\n    - A list of lists of points.  Four points form a triangle is the minimum\r\n    -  aline = [[0, 0], [1, 1]]  # a list of points\r\n    -  aPolygon = [aline]        # a list of lists of points\r\n    """"""\r\n    msg = \'\\nRead the script header... A projected coordinate system required\'\r\n    assert (SR is not None), msg\r\n    polygons = []\r\n    for pnts in pnt_groups:\r\n        for pair in pnts:\r\n            arr = arcpy.Array([arcpy.Point(*xy) for xy in pair])\r\n            pl = arcpy.Polygon(arr, SR)\r\n            polygons.append(pl)\r\n    if arcpy.Exists(out_fc):     # overwrite any existing versions\r\n        arcpy.Delete_management(out_fc)\r\n    arcpy.CopyFeatures_management(polygons, out_fc)\r\n    return\r\n\r\n# ---- formatting, from arraytools ------------------------------------------\r\n#\r\n# ----------------------------------------------------------------------\r\n# (4) frmt_rec .... code section\r\n#  frmt_rec requires _col_format\r\ndef _col_format(a, c_name=""c00"", deci=0):\r\n    """"""Determine column format given a desired number of decimal places.\r\n    Used by frmt_struct.\r\n\r\n    `a` : column\r\n        A column in an array.\r\n    `c_name` : text\r\n        column name\r\n    `deci` : int\r\n        Desired number of decimal points if the data are numeric\r\n\r\n    Notes:\r\n    -----\r\n        The field is examined to determine whether it is a simple integer, a\r\n        float type or a list, array or string.  The maximum width is determined\r\n        based on this type.\r\n\r\n        Checks were also added for (N,) shaped structured arrays being\r\n        reformatted to (N, 1) shape which sometimes occurs to facilitate array\r\n        viewing.  A kludge at best, but it works for now.\r\n    """"""\r\n    a_kind = a.dtype.kind\r\n    if a_kind in (\'i\', \'u\'):  # ---- integer type\r\n        w_, m_ = [\':> {}.0f\', \'{:> 0.0f}\']\r\n        col_wdth = len(m_.format(a.max())) + 1\r\n        col_wdth = max(len(c_name), col_wdth) + 1  # + deci\r\n        c_fmt = w_.format(col_wdth, 0)\r\n    elif a_kind == \'f\' and np.isscalar(a[0]):  # ---- float type with rounding\r\n        w_, m_ = [\':> {}.{}f\', \'{:> 0.{}f}\']\r\n        a_max, a_min = np.round(np.sort(a[[0, -1]]), deci)\r\n        col_wdth = max(len(m_.format(a_max, deci)),\r\n                       len(m_.format(a_min, deci))) + 1\r\n        col_wdth = max(len(c_name), col_wdth) + 1\r\n        c_fmt = w_.format(col_wdth, deci)\r\n    # ---- lists, arrays, strings. Check for (N,) vs (N,1)\r\n    # I made some changes in how col_wdth is determined, old is commented\r\n    else:\r\n        if a.ndim == 1:  # ---- check for (N, 1) format of structured array\r\n            a = a[0]\r\n        dt = a.dtype.descr[0][1]\r\n        col_wdth = int("""".join([i for i in dt if i.isdigit()]))\r\n#       col_wdth = max([len(str(i)) for i in a])\r\n        col_wdth = max(len(c_name), col_wdth) + 1  # + deci\r\n        c_fmt = ""!s:>"" + ""{}"".format(col_wdth)\r\n    return c_fmt, col_wdth\r\n\r\n\r\ndef pd_(a, deci=2, use_names=True, prn=True):\r\n    """"""see help for `frmt_rec`...""""""\r\n    ret = frmt_rec(a, deci=deci, use_names=use_names, prn=prn)\r\n    return ret\r\n\r\n\r\ndef frmt_rec(a, deci=2, use_names=True, prn=True):\r\n    """"""Format a structured array with a mixed dtype.\r\n\r\n    NOTE : Can be called as `pd_(a, ... )` to emulate pandas dataframes\r\n        You should limit large arrays to a slice ie. a[:50]\r\n\r\n    Requires:\r\n    -------\r\n    `a` : array\r\n        A structured/recarray\r\n    `deci` : int\r\n        To facilitate printing, this value is the number of decimal\r\n        points to use for all floating point fields.\r\n    `use_names` : boolean\r\n        If no names are available, then create them\r\n    `prn` : boolean\r\n        True to print, False to return the string\r\n    Notes:\r\n    -----\r\n        `_col_format` : does the actual work of obtaining a representation of\r\n        the column format.\r\n\r\n        It is not really possible to deconstruct the exact number of decimals\r\n        to use for float values, so a decision had to be made to simplify.\r\n    """"""\r\n    dt_names = a.dtype.names\r\n    N = len(dt_names)\r\n    c_names = [[""C{:02.0f}"".format(i) for i in range(N)], dt_names][use_names]\r\n    # ---- get the column formats from ... _col_format ----\r\n    dts = []\r\n    wdths = []\r\n    pair = list(zip(dt_names, c_names))\r\n    for i in range(len(pair)):\r\n        fld, nme = pair[i]\r\n        c_fmt, col_wdth = _col_format(a[fld], c_name=nme, deci=deci)\r\n        dts.append(c_fmt)\r\n        wdths.append(col_wdth)\r\n    row_frmt = "" "".join([(\'{\' + i + \'}\') for i in dts])\r\n    hdr = [""!s:>"" + ""{}"".format(wdths[i]) for i in range(N)]\r\n    hdr2 = "" "".join([""{"" + hdr[i] + ""}"" for i in range(N)])\r\n    header = ""--n--"" + hdr2.format(*c_names)\r\n    header = ""\\n{}\\n{}"".format(header, ""-""*len(header))\r\n    txt = [header]\r\n    # ---- check for structured arrays reshaped to (N, 1) instead of (N,) ----\r\n    len_shp = len(a.shape)\r\n    idx = 0\r\n    for i in range(a.shape[0]):\r\n        if len_shp == 1:  # ---- conventional (N,) shaped array\r\n            row = "" {:03.0f} "".format(idx) + row_frmt.format(*a[i])\r\n        else:             # ---- reformatted to (N, 1)\r\n            row = "" {:03.0f} "".format(idx) + row_frmt.format(*a[i][0])\r\n        idx += 1\r\n        txt.append(row)\r\n    msg = ""\\n"".join([i for i in txt])\r\n    if prn:\r\n        print(msg)\r\n    else:\r\n        return msg\r\n\r\n# ----------------------------------------------------------------------\r\n# (5) form_ ... code section .....\r\n#  form_ requires make_row_format\r\n\r\ndef col_hdr(num=7):\r\n    """"""Print numbers from 1 to 10*num to show column positions""""""\r\n    args = [((\'{:<10}\')*num).format(*\'0123456789\'),\r\n            \'0123456789\'*num, \'-\'*10*num]\r\n    s = ""\\n{}\\n{}\\n{}"".format(args[0][1:], args[1][1:], args[2])  # *args)\r\n    print(s)\r\n\r\n\r\ndef make_row_format(dim=3, cols=5, a_kind=\'f\', deci=1,\r\n                    a_max=10, a_min=-10, wdth=100, prnt=False):\r\n    """"""Format the row based on input parameters\r\n\r\n    `dim` - int\r\n        Number of dimensions\r\n    `cols` : int\r\n        Columns per dimension\r\n\r\n    `a_kind`, `deci`, `a_max` and `a_min` allow you to specify a data type,\r\n    number of decimals and maximum and minimum values to test formatting.\r\n    """"""\r\n    if a_kind not in [\'f\', \'i\']:\r\n        a_kind = \'f\'\r\n    w_, m_ = [[\':{}.0f\', \'{:0.0f}\'], [\':{}.{}f\', \'{:0.{}f}\']][a_kind == \'f\']\r\n    m_fmt = max(len(m_.format(a_max, deci)), len(m_.format(a_min, deci))) + 1\r\n    w_fmt = w_.format(m_fmt, deci)\r\n    suffix = \'  \'\r\n    while m_fmt*cols*dim > wdth:\r\n        cols -= 1\r\n        suffix = \'.. \'\r\n    row_sub = ((\'{\' + w_fmt + \'}\')*cols + suffix)\r\n    row_frmt = (row_sub*dim).strip()\r\n    if prnt:\r\n        frmt = ""Row format: dim cols: ({}, {})  kind: {} decimals: {}\\n\\n{}""\r\n        print(dedent(frmt).format(dim, cols, a_kind, deci, row_frmt))\r\n        a = np.random.randint(a_min, a_max+1, dim*cols)\r\n        col_hdr(wdth//10)  # run col_hdr to produce the column headers\r\n        print(row_frmt.format(*a))\r\n    else:\r\n        return row_frmt\r\n\r\n\r\ndef form_(a, deci=2, wdth=100, title=""Array"", prefix="". . "", prn=True):\r\n    """"""Alternate format to frmt_ function.\r\n    Inputs are largely the same.\r\n\r\n    Requires:\r\n    ---------\r\n    make_row_format, _col_format - functions\r\n        used to format the rows and columns\r\n    """"""\r\n    def _piece(sub, i, frmt, linewidth):\r\n        """"""piece together 3D chunks by row""""""\r\n        s0 = sub.shape[0]\r\n        block = np.hstack([sub[j] for j in range(s0)])\r\n        txt = """"\r\n        if i is not None:\r\n            fr = ("":arr[{}"" + "", :{}""*len(a.shape[1:]) + ""]\\n"")\r\n            txt = fr.format(i, *sub.shape)\r\n        for line in block:\r\n            ln = frmt.format(*line)[:linewidth]\r\n            end = [""\\n"", ""...\\n""][len(ln) >= linewidth]\r\n            txt += indent(ln + end, "". . "")\r\n        return txt\r\n    # ---- main section ----\r\n    out = ""\\n{}... ndim: {}  shape: {}\\n"".format(title, a.ndim, a.shape)\r\n    linewidth = wdth\r\n    if a.ndim <= 1:\r\n        return a\r\n    elif a.ndim == 2:\r\n        a = a.reshape((1,) + a.shape)\r\n    # ---- pull the 1st and 3rd dimension for 3D and 4D arrays\r\n    frmt = make_row_format(dim=a.shape[-3],\r\n                           cols=a.shape[-1],\r\n                           a_kind=a.dtype.kind,\r\n                           deci=deci,\r\n                           a_max=a.max(),\r\n                           a_min=a.min(),\r\n                           wdth=wdth,\r\n                           prnt=False)\r\n    if a.ndim == 3:\r\n        s0, s1, s2 = a.shape\r\n        out += _piece(a, None, frmt, linewidth)  # ---- _piece ----\r\n    elif a.ndim == 4:\r\n        s0, s1, s2, _ = a.shape\r\n        for i in range(s0):\r\n            out = out + ""\\n"" + _piece(a[i], i, frmt, linewidth)  # ---- _piece\r\n    if prn:\r\n        print(out)\r\n    else:\r\n        return out\r\n\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n#    print(""Script... {}"".format(script))\r\n#    _demo()\r\n#    gdb_fc = [\'Data\', \'point_tools.gdb\', \'radial_pnts\']\r\n#    in_fc = ""/"".join(script.split(""/"")[:-2] + gdb_fc)\r\n#    result = fc_array(in_fc, flds="""", allpnts=True)  # a, out_flds, SR\r\n'"
triangulation_tools/Scripts/triangulate.py,19,"b'# -*- coding: utf-8 -*-\r\n""""""\r\ntriangulate\r\n===========\r\n\r\nScript :   triangulate.py\r\n\r\nAuthor :   Dan_Patterson@carleton.ca\r\n\r\nModified : 2019-03-03\r\n\r\nPurpose:  triangulate poly* features using scipy/qhull functions.\r\n\r\nUseage :\r\n\r\n>>> tri = Voronoi(pnts)\r\n>>> dir(tri)\r\n[\'__class__\', \'__del__\', \'__delattr__\', \'__dict__\', \'__dir__\', \'__doc__\',\r\n \'__eq__\', \'__format__\', \'__ge__\', \'__getattribute__\', \'__gt__\', \'__hash__\',\r\n \'__init__\', \'__init_subclass__\', \'__le__\', \'__lt__\', \'__module__\', \'__ne__\',\r\n \'__new__\', \'__reduce__\', \'__reduce_ex__\', \'__repr__\', \'__setattr__\',\r\n \'__sizeof__\', \'__str__\', \'__subclasshook__\', \'__weakref__\', \'_add_points\',\r\n \'_points\', \'_qhull\', \'_ridge_dict\', \'_update\', \'add_points\', \'close\',\r\n \'max_bound\', \'min_bound\', \'ndim\', \'npoints\', \'point_region\', \'points\',\r\n \'regions\', \'ridge_dict\', \'ridge_points\', \'ridge_vertices\', \'vertices\']\r\n\r\n>>> tri.__class__\r\n<class \'scipy.spatial.qhull.Voronoi\'>\r\n\r\n>>> tri.__dict__.keys()\r\ndict_keys([\'_qhull\', \'vertices\', \'ridge_points\', \'ridge_vertices\', \'regions\',\r\n \'point_region\', \'_ridge_dict\', \'_points\', \'ndim\', \'npoints\', \'min_bound\',\r\n \'max_bound\'])\r\n\r\n>>> tri.min_bound, tri.max_bound\r\n(array([-4217.93, -3832.13]), array([5268.65, 5495.64]))\r\n\r\nNotes\r\n-----\r\nTo get the centroid geometry, you can do the following.\r\n\r\n>>> v = vor_pnts(aa, testing=False)\r\n>>> cents = [[p.centroid.X, p.centroid.Y] for p in polys]\r\n>>> polys = poly([v], SR)\r\n>>> pnts = [arcpy.PointGeometry(Point(i[0], i[1])) for i in cents]\r\n>>> out_fc = \'C:\\\\GIS\\\\A_Tools_scripts\\\\PointTools\\\\voronoi_delaunay.gdb\\\\p6\'\r\n>>> arcpy.FeatureToPoint_management (pnts, out_fc)\r\n\r\nReferences\r\n----------\r\nvoronoi/delaunay links:\r\n\r\n`<http://zderadicka.eu/voronoi-diagrams/>`_.\r\n`<http://scipy.github.io/devdocs/generated/scipy.spatial.Voronoi.html>`_.\r\n`<https://stackoverflow.com/questions/20515554/colorize-voronoi-diagram>`_.\r\n`<https://stackoverflow.com/questions/36063533/clipping-a-voronoi-diagram\r\n-python?noredirect=1&lq=1>`_.\r\n`<https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/\r\nscipy.spatial.Delaunay.html>`_.\r\n---------------------------------------------------------------------\r\n""""""\r\n\r\nimport sys\r\nimport numpy as np\r\nfrom scipy.spatial import Delaunay, Voronoi\r\n#\r\nfrom arcpy import Exists\r\nfrom arcpy.management import (Delete, CopyFeatures, MakeFeatureLayer)\r\nfrom arcpy.analysis import Clip\r\nfrom arcpy.da import Describe, FeatureClassToNumPyArray\r\nfrom arcpy.geoprocessing import env\r\nfrom arcpy.arcobjects import Array, Point\r\nfrom arcpy.arcobjects.geometries import Polygon\r\n\r\nft = {\'bool\': lambda x: repr(x.astype(np.int32)),\r\n      \'float_kind\': \'{: 0.3f}\'.format}\r\nnp.set_printoptions(edgeitems=5, linewidth=80, precision=2, suppress=True,\r\n                    threshold=100, formatter=ft)\r\nnp.ma.masked_print_option.set_display(\'-\')  # change to a single -\r\n\r\nscript = sys.argv[0]  # print this should you need to locate the script\r\n\r\nenv.overwriteOutput = True\r\n\r\n# ----\r\n#\r\ndef tweet(msg):\r\n    """"""Print a message for both arcpy and python.\r\n\r\n    msg - a text message\r\n    """"""\r\n    m = ""\\n{}\\n"".format(msg)\r\n    AddMessage(m)\r\n    print(m)\r\n\r\n\r\ndef circle(radius=1.0, theta=10.0, xc=0.0, yc=0.0):\r\n    """"""Produce a circle/ellipse depending on parameters.\r\n\r\n    `radius` : number\r\n        Distance from centre\r\n    `theta` : number\r\n        Angle of densification of the shape around 360 degrees\r\n    """"""\r\n    angles = np.deg2rad(np.arange(180.0, -180.0-theta, step=-theta))\r\n    x_s = radius*np.cos(angles) + xc    # X values\r\n    y_s = radius*np.sin(angles) + yc    # Y values\r\n    pnts = np.c_[x_s, y_s]\r\n    return pnts\r\n\r\n\r\ndef infinity_circle(a, fac=10):\r\n    """"""Create an infinity circle to append to the original point list.\r\n\r\n    Parameters:\r\n    -----------\r\n    a : array\r\n        2D array of x, y point coordinates\r\n    fac : number\r\n        The factor to multiply the largest of the extent width or height for\r\n        the input point array\r\n    """"""\r\n    fac = max(fac, 1)\r\n    L, B = a.min(axis=0)\r\n    R, T = a.max(axis=0)\r\n    xc, yc = np.average(a, axis=0)\r\n    circle_radius = max([R-L, T-B]) * fac #increase radius by a factor of 10\r\n    circPnts = circle(radius=circle_radius, theta=10.0, xc=xc, yc=yc)\r\n    return circPnts\r\n\r\n\r\ndef poly(pnt_groups, SR):\r\n    """"""Short form polygon creation\r\n    """"""\r\n    polygons = []\r\n    for pnts in pnt_groups:\r\n        for pair in pnts:\r\n            arr = Array([Point(*xy) for xy in pair])\r\n            pl = Polygon(arr, SR)\r\n            polygons.append(pl)\r\n    return polygons\r\n\r\n\r\ndef vor_pnts(pnts, testing=False):\r\n    """"""Return the point indices""""""\r\n    out = []\r\n    avg = np.mean(pnts, axis=0)\r\n    p =  pnts - avg\r\n    tri = Voronoi(p)\r\n    for region in tri.regions:\r\n        r = [i for i in region if i != -1]\r\n        if (len(r) > 2):\r\n            poly = np.array([tri.vertices[i] + avg for i in r])\r\n            out.append(poly)\r\n            if testing:\r\n                print(""{}"".format(poly.T))\r\n    return out\r\n\r\n\r\ndef tri_pnts(pnts, testing=False):\r\n    """"""Triangulate the points and return the triangles\r\n\r\n    Parameters:\r\n    -----------\r\n    pnts : np.array\r\n        Points in array format.\r\n    out : array\r\n        an array of triangle points\r\n\r\n    Notes:\r\n    ------\r\n    >>> pnts = pnts.reshape((1,) + pnts.shape)  # a 3D set of points (ndim=3)\r\n    >>> [pnts]  # or pass in as a list\r\n    """"""\r\n    out = []\r\n    ps = np.unique(pnts, axis=0)  # get the unique points only\r\n    avg = np.mean(ps, axis=0)\r\n    p =  ps - avg\r\n    tri = Delaunay(p)\r\n    simps = tri.simplices\r\n    new_pnts = [p[s]+avg for s in simps]\r\n    if testing:\r\n        print(""{}"".format(new_pnts))\r\n    out.append(new_pnts)\r\n    out = np.array(out).squeeze()\r\n    return out\r\n\r\n\r\n# ---- Do the work\r\n#\r\n#\r\ndef _tri_demo(tri_type=\'Delaunay\'):\r\n    """"""Triangulation demo.\r\n    """"""\r\n    from scipy.spatial import delaunay_plot_2d, voronoi_plot_2d\r\n    import matplotlib.pyplot as plt\r\n    xs =[ 48,   8, 623, 615, 196, 368, 112, 918, 318, 316, 427,\r\n         364, 849, 827, 438, 957, 495, 317, 985, 534]\r\n    ys = [674, 215, 834, 235, 748, 630, 876, 407,  33, 872, 893,\r\n          360, 397, 902, 420, 430, 678, 523, 604, 292]\r\n    aa = np.array(list(zip(xs, ys)))\r\n    c = infinity_circle(aa, fac=0)\r\n    a = np.vstack((aa, c))\r\n    d = v = None  # initialize the output to None\r\n    if tri_type == \'Delaunay\':\r\n        d = Delaunay(aa)\r\n        plot = delaunay_plot_2d(d)\r\n        x0, y0 = [0., 0.]\r\n        x1, y1 = [1000., 1000.]\r\n    else:\r\n        c = infinity_circle(a, fac=2)\r\n#        a = np.vstack((a, c))\r\n        x0, y0 = a.min(axis=0)\r\n        x1, y1 = a.max(axis=0)\r\n        v = Voronoi(a, qhull_options=\'Qbb Qc Qx\')\r\n        plot = voronoi_plot_2d(v, show_vertices=True, line_colors=\'y\',\r\n                               line_alpha=0.8, point_size=5)\r\n    # ----\r\n    plot.set_figheight(10)\r\n    plot.set_figwidth(10)\r\n    plt.axis([0, 1000, 0, 1000])\r\n#    plt.axis([x0, x1, y0, y1])\r\n    plt.show()\r\n    return aa, (d or v)\r\n\r\n# ----\r\n#\r\ndef _tri_tool():\r\n    """"""Triangulation for tool\r\n    """"""\r\n    in_fc = sys.argv[1]\r\n    tri_type = sys.argv[2]\r\n    out_fc = sys.argv[3]\r\n    xtent = sys.argv[4]\r\n    desc = Describe(in_fc)\r\n    SR = desc[\'spatialReference\']\r\n    flds = [\'SHAPE@X\', \'SHAPE@Y\']\r\n    allpnts = False\r\n    z = FeatureClassToNumPyArray(in_fc, flds, """", SR, allpnts)\r\n    a = np.zeros((z.shape[0], 2), dtype=\'<f8\')\r\n    a[:, 0] = z[\'SHAPE@X\']\r\n    a[:, 1] = z[\'SHAPE@Y\']\r\n    #\r\n    if tri_type == \'Delaunay\':\r\n        tweet(""Delaunay... clip extent {}"".format(xtent))\r\n        t = tri_pnts(a, True)  # must be a list of list of points\r\n        polys = poly(t, SR)\r\n        if Exists(out_fc):\r\n            Delete(out_fc)\r\n        CopyFeatures(polys, ""in_memory/temp"")\r\n        MakeFeatureLayer(""in_memory/temp"", ""temp"")\r\n        if xtent not in ("""", None):\r\n            Clip(""temp"", xtent, out_fc, None)\r\n        else:\r\n            CopyFeatures(""temp"", out_fc)\r\n    else:\r\n        tweet(""Voronoi... clip extent {}"".format(xtent))\r\n        c = infinity_circle(a, fac=10)\r\n        aa = np.vstack((a, c))\r\n        v = vor_pnts(aa, testing=False)\r\n        polys = poly([v], SR)\r\n        if Exists(out_fc):\r\n            Delete(out_fc)\r\n        CopyFeatures(polys, ""in_memory/temp"")\r\n        MakeFeatureLayer(""in_memory/temp"", ""temp"")\r\n        if xtent not in ("""", None, ):\r\n            Clip(""temp"", xtent, out_fc, None)\r\n        else:\r\n            CopyFeatures(""temp"", out_fc)\r\n\r\n# ---- main section .... calls demo or the tool\r\n#\r\n# uncomment the t = _tri... line below to graph\r\nif len(sys.argv) == 1:\r\n    testing = True\r\n    a, t = _tri_demo(\'Delaunay\')  # \'Delaunay\' \'Voronoi\'\r\nelse:\r\n    testing = False\r\n    _tri_tool()\r\n# ----------------------------------------------------------------------\r\n# __main__ .... code section\r\nif __name__ == ""__main__"":\r\n    """"""Optionally...\r\n    : - print the script source name.\r\n    : - run the _demo\r\n    """"""\r\n'"
