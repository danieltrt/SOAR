file_path,api_count,code
pointnet/train.py,12,"b'import argparse\nimport math\nimport h5py\nimport numpy as np\nimport tensorflow as tf\nimport socket\nimport importlib\nimport os\nimport sys\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(BASE_DIR)\nsys.path.append(os.path.join(BASE_DIR, \'models\'))\nsys.path.append(os.path.join(BASE_DIR, \'utils\'))\nimport provider\nimport tf_util\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--gpu\', type=int, default=0, help=\'GPU to use [default: GPU 0]\')\nparser.add_argument(\'--model\', default=\'pointnet_cls\', help=\'Model name: pointnet_cls or pointnet_cls_basic [default: pointnet_cls]\')\nparser.add_argument(\'--log_dir\', default=\'log\', help=\'Log dir [default: log]\')\nparser.add_argument(\'--num_point\', type=int, default=1024, help=\'Point Number [256/512/1024/2048] [default: 1024]\')\nparser.add_argument(\'--max_epoch\', type=int, default=250, help=\'Epoch to run [default: 250]\')\nparser.add_argument(\'--batch_size\', type=int, default=32, help=\'Batch Size during training [default: 32]\')\nparser.add_argument(\'--learning_rate\', type=float, default=0.001, help=\'Initial learning rate [default: 0.001]\')\nparser.add_argument(\'--momentum\', type=float, default=0.9, help=\'Initial learning rate [default: 0.9]\')\nparser.add_argument(\'--optimizer\', default=\'adam\', help=\'adam or momentum [default: adam]\')\nparser.add_argument(\'--decay_step\', type=int, default=200000, help=\'Decay step for lr decay [default: 200000]\')\nparser.add_argument(\'--decay_rate\', type=float, default=0.7, help=\'Decay rate for lr decay [default: 0.8]\')\nparser.add_argument(""--adv"", action = ""store_true"", help = ""use adversarial training [default: false]"")\nparser.add_argument(""--classes"", type = int, default = 40, help = ""number of classes [default: 40]"")\nFLAGS = parser.parse_args()\n\n\nBATCH_SIZE = FLAGS.batch_size\nNUM_POINT = FLAGS.num_point\nMAX_EPOCH = FLAGS.max_epoch\nBASE_LEARNING_RATE = FLAGS.learning_rate\nGPU_INDEX = FLAGS.gpu\nMOMENTUM = FLAGS.momentum\nOPTIMIZER = FLAGS.optimizer\nDECAY_STEP = FLAGS.decay_step\nDECAY_RATE = FLAGS.decay_rate\nADV = FLAGS.adv\nNUM_CLASSES = FLAGS.classes\n\nMODEL = importlib.import_module(FLAGS.model) # import network module\nMODEL_FILE = os.path.join(BASE_DIR, \'models\', FLAGS.model+\'.py\')\nLOG_DIR = FLAGS.log_dir\nif not os.path.exists(LOG_DIR): os.mkdir(LOG_DIR)\nos.system(\'cp %s %s\' % (MODEL_FILE, LOG_DIR)) # bkp of model def\nos.system(\'cp train.py %s\' % (LOG_DIR)) # bkp of train procedure\nLOG_FOUT = open(os.path.join(LOG_DIR, \'log_train.txt\'), \'w\')\nLOG_FOUT.write(str(FLAGS)+\'\\n\')\n\nMAX_NUM_POINT = 2048\n\nBN_INIT_DECAY = 0.5\nBN_DECAY_DECAY_RATE = 0.5\nBN_DECAY_DECAY_STEP = float(DECAY_STEP)\nBN_DECAY_CLIP = 0.99\n\nHOSTNAME = socket.gethostname()\n\n# ModelNet40 official train/test split\nTRAIN_FILES = [""""] if NUM_CLASSES != 40 else provider.getDataFiles( \\\n    os.path.join(BASE_DIR, \'data/modelnet40_ply_hdf5_2048/train_files.txt\'))\nTEST_FILES = [""""] if NUM_CLASSES != 40 else provider.getDataFiles(\\\n    os.path.join(BASE_DIR, \'data/modelnet40_ply_hdf5_2048/test_files.txt\'))\n\ndef log_string(out_str):\n    LOG_FOUT.write(out_str+\'\\n\')\n    LOG_FOUT.flush()\n    print(out_str)\n\n\ndef get_learning_rate(batch):\n    learning_rate = tf.train.exponential_decay(\n                        BASE_LEARNING_RATE,  # Base learning rate.\n                        batch * BATCH_SIZE,  # Current index into the dataset.\n                        DECAY_STEP,          # Decay step.\n                        DECAY_RATE,          # Decay rate.\n                        staircase=True)\n    learning_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!\n    return learning_rate        \n\ndef get_bn_decay(batch):\n    bn_momentum = tf.train.exponential_decay(\n                      BN_INIT_DECAY,\n                      batch*BATCH_SIZE,\n                      BN_DECAY_DECAY_STEP,\n                      BN_DECAY_DECAY_RATE,\n                      staircase=True)\n    bn_decay = tf.minimum(BN_DECAY_CLIP, 1 - bn_momentum)\n    return bn_decay\n\ndef train():\n    with tf.Graph().as_default():\n        with tf.device(\'/gpu:\'+str(GPU_INDEX)):\n            pointclouds_pl, labels_pl = MODEL.placeholder_inputs(BATCH_SIZE, NUM_POINT)\n            is_training_pl = tf.placeholder(tf.bool, shape=())\n            print(is_training_pl)\n            \n            # Note the global_step=batch parameter to minimize. \n            # That tells the optimizer to helpfully increment the \'batch\' parameter for you every time it trains.\n            batch = tf.Variable(0)\n            bn_decay = get_bn_decay(batch)\n            tf.summary.scalar(\'bn_decay\', bn_decay)\n\n            # Get model and loss\n            pred, end_points = MODEL.get_model(pointclouds_pl, is_training_pl, bn_decay=bn_decay, num_classes = NUM_CLASSES)\n            loss = MODEL.get_loss(pred, labels_pl, end_points)\n            tf.summary.scalar(\'loss\', loss)\n            \n            if ADV:\n                import adversarial_attacks\n                def model_loss_fn(x, t):\n                    with tf.variable_scope(tf.get_variable_scope(), reuse = tf.AUTO_REUSE):\n                        y, end_points = MODEL.get_model(x, is_training_pl, bn_decay = bn_decay, num_classes = NUM_CLASSES)\n                    if t is None:\n                        loss = None\n                    else:\n                        loss = MODEL.get_loss(y, t, end_points)\n                    return y, loss\n                x_adv = adversarial_attacks.iter_grad_op(pointclouds_pl, model_loss_fn, one_hot = False, iter = 1, eps = 1, ord = ""2"")\n                _, adv_loss = model_loss_fn(x_adv, labels_pl)\n\n            correct = tf.equal(tf.argmax(pred, 1), tf.to_int64(labels_pl))\n            accuracy = tf.reduce_sum(tf.cast(correct, tf.float32)) / float(BATCH_SIZE)\n            tf.summary.scalar(\'accuracy\', accuracy)\n\n            # Get training operator\n            learning_rate = get_learning_rate(batch)\n            tf.summary.scalar(\'learning_rate\', learning_rate)\n            if OPTIMIZER == \'momentum\':\n                optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=MOMENTUM)\n            elif OPTIMIZER == \'adam\':\n                optimizer = tf.train.AdamOptimizer(learning_rate)\n            if ADV:\n                train_op = optimizer.minimize((loss + adv_loss) / 2.0, global_step=batch)\n            else:\n                train_op = optimizer.minimize(loss, global_step=batch)\n            \n            # Add ops to save and restore all the variables.\n            saver = tf.train.Saver()\n            \n        # Create a session\n        config = tf.ConfigProto()\n        config.gpu_options.allow_growth = True\n        config.allow_soft_placement = True\n        config.log_device_placement = False\n        sess = tf.Session(config=config)\n\n        # Add summary writers\n        #merged = tf.merge_all_summaries()\n        merged = tf.summary.merge_all()\n        train_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, \'train\'),\n                                  sess.graph)\n        test_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, \'test\'))\n\n        # Init variables\n        init = tf.global_variables_initializer()\n        # To fix the bug introduced in TF 0.12.1 as in\n        # http://stackoverflow.com/questions/41543774/invalidargumenterror-for-tensor-bool-tensorflow-0-12-1\n        #sess.run(init)\n        sess.run(init, {is_training_pl: True})\n\n        ops = {\'pointclouds_pl\': pointclouds_pl,\n               \'labels_pl\': labels_pl,\n               \'is_training_pl\': is_training_pl,\n               \'pred\': pred,\n               \'loss\': loss,\n               \'train_op\': train_op,\n               \'merged\': merged,\n               \'step\': batch}\n\n        for epoch in range(MAX_EPOCH):\n            log_string(\'**** EPOCH %03d ****\' % (epoch))\n            sys.stdout.flush()\n             \n            train_one_epoch(sess, ops, train_writer)\n            eval_one_epoch(sess, ops, test_writer)\n            \n            # Save the variables to disk.\n            if epoch % 10 == 0:\n                save_path = saver.save(sess, os.path.join(LOG_DIR, ""model.ckpt""))\n                log_string(""Model saved in file: %s"" % save_path)\n\n\n\ndef train_one_epoch(sess, ops, train_writer):\n    """""" ops: dict mapping from string to tf ops """"""\n    is_training = True\n    \n    # Shuffle train files\n    train_file_idxs = np.arange(0, len(TRAIN_FILES))\n    np.random.shuffle(train_file_idxs)\n    \n    for fn in range(len(TRAIN_FILES)):\n        log_string(\'----\' + str(fn) + \'-----\')\n        if NUM_CLASSES == 40:\n            current_data, current_label = provider.loadDataFile(TRAIN_FILES[train_file_idxs[fn]])\n        else:\n            with np.load(""point_clouds_unique_train.npz"") as file:\n                current_data, current_label = file[""points""], file[""labels""]\n        current_data = current_data[:,0:NUM_POINT,:]\n        current_data, current_label, _ = provider.shuffle_data(current_data, np.squeeze(current_label))            \n        current_label = np.squeeze(current_label)\n        \n        file_size = current_data.shape[0]\n        num_batches = file_size // BATCH_SIZE\n        \n        total_correct = 0\n        total_seen = 0\n        loss_sum = 0\n       \n        for batch_idx in range(num_batches):\n            start_idx = batch_idx * BATCH_SIZE\n            end_idx = (batch_idx+1) * BATCH_SIZE\n            \n            # Augment batched point clouds by rotation and jittering\n            rotated_data = provider.rotate_point_cloud(current_data[start_idx:end_idx, :, :])\n            jittered_data = provider.jitter_point_cloud(rotated_data)\n            feed_dict = {ops[\'pointclouds_pl\']: jittered_data,\n                         ops[\'labels_pl\']: current_label[start_idx:end_idx],\n                         ops[\'is_training_pl\']: is_training,}\n            summary, step, _, loss_val, pred_val = sess.run([ops[\'merged\'], ops[\'step\'],\n                ops[\'train_op\'], ops[\'loss\'], ops[\'pred\']], feed_dict=feed_dict)\n            train_writer.add_summary(summary, step)\n            pred_val = np.argmax(pred_val, 1)\n            correct = np.sum(pred_val == current_label[start_idx:end_idx])\n            total_correct += correct\n            total_seen += BATCH_SIZE\n            loss_sum += loss_val\n        \n        log_string(\'mean loss: %f\' % (loss_sum / float(num_batches)))\n        log_string(\'accuracy: %f\' % (total_correct / float(total_seen)))\n\n        \ndef eval_one_epoch(sess, ops, test_writer):\n    """""" ops: dict mapping from string to tf ops """"""\n    is_training = False\n    total_correct = 0\n    total_seen = 0\n    loss_sum = 0\n    total_seen_class = [0 for _ in range(NUM_CLASSES)]\n    total_correct_class = [0 for _ in range(NUM_CLASSES)]\n    \n    for fn in range(len(TEST_FILES)):\n        log_string(\'----\' + str(fn) + \'-----\')\n        if NUM_CLASSES == 40:\n            current_data, current_label = provider.loadDataFile(TEST_FILES[fn])\n        else:\n            with np.load(""point_clouds_unique_test.npz"") as file:\n                current_data, current_label = file[""points""], file[""labels""]\n        current_data = current_data[:,0:NUM_POINT,:]\n        current_label = np.squeeze(current_label)\n        \n        file_size = current_data.shape[0]\n        num_batches = file_size // BATCH_SIZE\n        \n        for batch_idx in range(num_batches):\n            start_idx = batch_idx * BATCH_SIZE\n            end_idx = (batch_idx+1) * BATCH_SIZE\n\n            feed_dict = {ops[\'pointclouds_pl\']: current_data[start_idx:end_idx, :, :],\n                         ops[\'labels_pl\']: current_label[start_idx:end_idx],\n                         ops[\'is_training_pl\']: is_training}\n            summary, step, loss_val, pred_val = sess.run([ops[\'merged\'], ops[\'step\'],\n                ops[\'loss\'], ops[\'pred\']], feed_dict=feed_dict)\n            pred_val = np.argmax(pred_val, 1)\n            correct = np.sum(pred_val == current_label[start_idx:end_idx])\n            total_correct += correct\n            total_seen += BATCH_SIZE\n            loss_sum += (loss_val*BATCH_SIZE)\n            for i in range(start_idx, end_idx):\n                l = current_label[i]\n                total_seen_class[l] += 1\n                total_correct_class[l] += (pred_val[i-start_idx] == l)\n            \n    log_string(\'eval mean loss: %f\' % (loss_sum / float(total_seen)))\n    log_string(\'eval accuracy: %f\'% (total_correct / float(total_seen)))\n    log_string(\'eval avg class acc: %f\' % (np.mean(np.array(total_correct_class)/np.array(total_seen_class,dtype=np.float))))\n         \n\n\nif __name__ == ""__main__"":\n    train()\n    LOG_FOUT.close()\n'"
pointnet2/modelnet_h5_dataset.py,7,"b'\'\'\'\n    ModelNet dataset. Support ModelNet40, XYZ channels. Up to 2048 points.\n    Faster IO than ModelNetDataset in the first epoch.\n\'\'\'\n\nimport os\nimport sys\nimport numpy as np\nimport h5py\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(BASE_DIR)\nROOT_DIR = BASE_DIR\nsys.path.append(os.path.join(ROOT_DIR, \'utils\'))\nimport provider\n\n\n# Download dataset for point cloud classification\nDATA_DIR = os.path.join(ROOT_DIR, \'data\')\nif not os.path.exists(DATA_DIR):\n    os.mkdir(DATA_DIR)\nif not os.path.exists(os.path.join(DATA_DIR, \'modelnet40_ply_hdf5_2048\')):\n    www = \'https://shapenet.cs.stanford.edu/media/modelnet40_ply_hdf5_2048.zip\'\n    zipfile = os.path.basename(www)\n    os.system(\'wget %s; unzip %s\' % (www, zipfile))\n    os.system(\'mv %s %s\' % (zipfile[:-4], DATA_DIR))\n    os.system(\'rm %s\' % (zipfile))\n\n\ndef shuffle_data(data, labels):\n    """""" Shuffle data and labels.\n        Input:\n          data: B,N,... numpy array\n          label: B,... numpy array\n        Return:\n          shuffled data, label and shuffle indices\n    """"""\n    idx = np.arange(len(labels))\n    np.random.shuffle(idx)\n    return data[idx, ...], labels[idx], idx\n\ndef getDataFiles(list_filename):\n    return [line.rstrip() for line in open(list_filename)]\n\ndef load_h5(h5_filename):\n    f = h5py.File(h5_filename)\n    data = f[\'data\'][:]\n    label = f[\'label\'][:]\n    return (data, label)\n\ndef loadDataFile(filename):\n    return load_h5(filename)\n\n\nclass ModelNetH5Dataset(object):\n    def __init__(self, list_filename, batch_size = 32, npoints = 1024, shuffle=True, np_file = False):\n        self.list_filename = list_filename\n        self.batch_size = batch_size\n        self.npoints = npoints\n        self.shuffle = shuffle\n        self.np_file = np_file\n        self.h5_files = [""""] if self.np_file else getDataFiles(self.list_filename)\n        self.reset()\n\n    def reset(self):\n        \'\'\' reset order of h5 files \'\'\'\n        self.file_idxs = np.arange(0, len(self.h5_files))\n        if self.shuffle: np.random.shuffle(self.file_idxs)\n        self.current_data = None\n        self.current_label = None\n        self.current_file_idx = 0\n        self.batch_idx = 0\n   \n    def _augment_batch_data(self, batch_data):\n        rotated_data = provider.rotate_point_cloud(batch_data)\n        rotated_data = provider.rotate_perturbation_point_cloud(rotated_data)\n        jittered_data = provider.random_scale_point_cloud(rotated_data[:,:,0:3])\n        jittered_data = provider.shift_point_cloud(jittered_data)\n        jittered_data = provider.jitter_point_cloud(jittered_data)\n        rotated_data[:,:,0:3] = jittered_data\n        return provider.shuffle_points(rotated_data)\n\n\n    def _get_data_filename(self):\n        return self.h5_files[self.file_idxs[self.current_file_idx]]\n\n    def _load_data_file(self, filename):\n        if self.np_file:\n                with np.load(""point_clouds_unique_train.npz"" if self.shuffle else ""point_clouds_unique_test.npz"") as file:\n                        self.current_data, self.current_label = file[""points""], file[""labels""]\n        else:\n                self.current_data,self.current_label = load_h5(filename)\n        self.current_label = np.squeeze(self.current_label)\n        self.batch_idx = 0\n        if self.shuffle:\n            self.current_data, self.current_label, _ = shuffle_data(self.current_data,self.current_label)\n    \n    def _has_next_batch_in_file(self):\n        return self.batch_idx*self.batch_size < self.current_data.shape[0]\n\n    def num_channel(self):\n        return 3\n\n    def has_next_batch(self):\n        # TODO: add backend thread to load data\n        if (self.current_data is None) or (not self._has_next_batch_in_file()):\n            if self.current_file_idx >= len(self.h5_files):\n                return False\n            self._load_data_file(self._get_data_filename())\n            self.batch_idx = 0\n            self.current_file_idx += 1\n        return self._has_next_batch_in_file()\n\n    def next_batch(self, augment=False):\n        \'\'\' returned dimension may be smaller than self.batch_size \'\'\'\n        start_idx = self.batch_idx * self.batch_size\n        end_idx = min((self.batch_idx+1) * self.batch_size, self.current_data.shape[0])\n        bsize = end_idx - start_idx\n        batch_label = np.zeros((bsize), dtype=np.int32)\n        data_batch = self.current_data[start_idx:end_idx, 0:self.npoints, :].copy()\n        label_batch = self.current_label[start_idx:end_idx].copy()\n        self.batch_idx += 1\n        if augment: data_batch = self._augment_batch_data(data_batch)\n        return data_batch, label_batch \n\nif __name__==\'__main__\':\n    d = ModelNetH5Dataset(\'data/modelnet40_ply_hdf5_2048/train_files.txt\')\n    print(d.shuffle)\n    print(d.has_next_batch())\n    ps_batch, cls_batch = d.next_batch(True)\n    print(ps_batch.shape)\n    print(cls_batch.shape)\n'"
pointnet2/train.py,9,"b'\'\'\'\n    Single-GPU training.\n    Will use H5 dataset in default. If using normal, will shift to the normal dataset.\n\'\'\'\nimport argparse\nimport math\nfrom datetime import datetime\nimport h5py\nimport numpy as np\nimport tensorflow as tf\nimport socket\nimport importlib\nimport os\nimport sys\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nROOT_DIR = BASE_DIR\nsys.path.append(BASE_DIR)\nsys.path.append(os.path.join(ROOT_DIR, \'models\'))\nsys.path.append(os.path.join(ROOT_DIR, \'utils\'))\nimport provider\nimport tf_util\nimport modelnet_dataset\nimport modelnet_h5_dataset\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--gpu\', type=int, default=0, help=\'GPU to use [default: GPU 0]\')\nparser.add_argument(\'--model\', default=\'pointnet2_cls_ssg\', help=\'Model name [default: pointnet2_cls_ssg]\')\nparser.add_argument(\'--log_dir\', default=\'log\', help=\'Log dir [default: log]\')\nparser.add_argument(\'--num_point\', type=int, default=1024, help=\'Point Number [default: 1024]\')\nparser.add_argument(\'--max_epoch\', type=int, default=251, help=\'Epoch to run [default: 251]\')\nparser.add_argument(\'--batch_size\', type=int, default=16, help=\'Batch Size during training [default: 16]\')\nparser.add_argument(\'--learning_rate\', type=float, default=0.001, help=\'Initial learning rate [default: 0.001]\')\nparser.add_argument(\'--momentum\', type=float, default=0.9, help=\'Initial learning rate [default: 0.9]\')\nparser.add_argument(\'--optimizer\', default=\'adam\', help=\'adam or momentum [default: adam]\')\nparser.add_argument(\'--decay_step\', type=int, default=200000, help=\'Decay step for lr decay [default: 200000]\')\nparser.add_argument(\'--decay_rate\', type=float, default=0.7, help=\'Decay rate for lr decay [default: 0.7]\')\nparser.add_argument(\'--normal\', action=\'store_true\', help=\'Whether to use normal information\')\nparser.add_argument(""--adv"", action = ""store_true"", help = ""use adversarial training [default: false]"")\nparser.add_argument(""--classes"", type = int, default = 40, help = ""number of classes [default: 40]"")\nFLAGS = parser.parse_args()\n\nEPOCH_CNT = 0\n\nBATCH_SIZE = FLAGS.batch_size\nNUM_POINT = FLAGS.num_point\nMAX_EPOCH = FLAGS.max_epoch\nBASE_LEARNING_RATE = FLAGS.learning_rate\nGPU_INDEX = FLAGS.gpu\nMOMENTUM = FLAGS.momentum\nOPTIMIZER = FLAGS.optimizer\nDECAY_STEP = FLAGS.decay_step\nDECAY_RATE = FLAGS.decay_rate\nADV = FLAGS.adv\nNUM_CLASSES = FLAGS.classes\n\nMODEL = importlib.import_module(FLAGS.model) # import network module\nMODEL_FILE = os.path.join(ROOT_DIR, \'models\', FLAGS.model+\'.py\')\nLOG_DIR = FLAGS.log_dir\nif not os.path.exists(LOG_DIR): os.mkdir(LOG_DIR)\nos.system(\'cp %s %s\' % (MODEL_FILE, LOG_DIR)) # bkp of model def\nos.system(\'cp train.py %s\' % (LOG_DIR)) # bkp of train procedure\nLOG_FOUT = open(os.path.join(LOG_DIR, \'log_train.txt\'), \'w\')\nLOG_FOUT.write(str(FLAGS)+\'\\n\')\n\nBN_INIT_DECAY = 0.5\nBN_DECAY_DECAY_RATE = 0.5\nBN_DECAY_DECAY_STEP = float(DECAY_STEP)\nBN_DECAY_CLIP = 0.99\n\nHOSTNAME = socket.gethostname()\n\n# Shapenet official train/test split\nif FLAGS.normal:\n    assert(NUM_POINT<=10000)\n    DATA_PATH = os.path.join(ROOT_DIR, \'data/modelnet40_normal_resampled\')\n    TRAIN_DATASET = modelnet_dataset.ModelNetDataset(root=DATA_PATH, npoints=NUM_POINT, split=\'train\', normal_channel=FLAGS.normal, batch_size=BATCH_SIZE)\n    TEST_DATASET = modelnet_dataset.ModelNetDataset(root=DATA_PATH, npoints=NUM_POINT, split=\'test\', normal_channel=FLAGS.normal, batch_size=BATCH_SIZE)\nelse:\n    assert(NUM_POINT<=2048)\n    TRAIN_DATASET = modelnet_h5_dataset.ModelNetH5Dataset(os.path.join(BASE_DIR, \'data/modelnet40_ply_hdf5_2048/train_files.txt\'), batch_size=BATCH_SIZE, npoints=NUM_POINT, shuffle=True, np_file = NUM_CLASSES != 40)\n    TEST_DATASET = modelnet_h5_dataset.ModelNetH5Dataset(os.path.join(BASE_DIR, \'data/modelnet40_ply_hdf5_2048/test_files.txt\'), batch_size=BATCH_SIZE, npoints=NUM_POINT, shuffle=False, np_file = NUM_CLASSES != 40)\n\ndef log_string(out_str):\n    LOG_FOUT.write(out_str+\'\\n\')\n    LOG_FOUT.flush()\n    print(out_str)\n\ndef get_learning_rate(batch):\n    learning_rate = tf.train.exponential_decay(\n                        BASE_LEARNING_RATE,  # Base learning rate.\n                        batch * BATCH_SIZE,  # Current index into the dataset.\n                        DECAY_STEP,          # Decay step.\n                        DECAY_RATE,          # Decay rate.\n                        staircase=True)\n    learning_rate = tf.maximum(learning_rate, 0.00001) # CLIP THE LEARNING RATE!\n    return learning_rate        \n\ndef get_bn_decay(batch):\n    bn_momentum = tf.train.exponential_decay(\n                      BN_INIT_DECAY,\n                      batch*BATCH_SIZE,\n                      BN_DECAY_DECAY_STEP,\n                      BN_DECAY_DECAY_RATE,\n                      staircase=True)\n    bn_decay = tf.minimum(BN_DECAY_CLIP, 1 - bn_momentum)\n    return bn_decay\n\ndef train():\n    with tf.Graph().as_default():\n        with tf.device(\'/gpu:\'+str(GPU_INDEX)):\n            pointclouds_pl, labels_pl = MODEL.placeholder_inputs(BATCH_SIZE, NUM_POINT)\n            is_training_pl = tf.placeholder(tf.bool, shape=())\n            \n            # Note the global_step=batch parameter to minimize. \n            # That tells the optimizer to helpfully increment the \'batch\' parameter\n            # for you every time it trains.\n            batch = tf.get_variable(\'batch\', [],\n                initializer=tf.constant_initializer(0), trainable=False)\n            bn_decay = get_bn_decay(batch)\n            tf.summary.scalar(\'bn_decay\', bn_decay)\n\n            # Get model and loss \n            pred, end_points = MODEL.get_model(pointclouds_pl, is_training_pl, bn_decay=bn_decay, num_classes = NUM_CLASSES)\n            total_loss = MODEL.get_loss(pred, labels_pl, end_points)\n            #losses = tf.get_collection(\'losses\')\n            #total_loss = tf.add_n(losses, name=\'total_loss\')\n            #tf.summary.scalar(\'total_loss\', total_loss)\n            #for l in losses + [total_loss]:\n            #    tf.summary.scalar(l.op.name, l)\n\n            if ADV:\n                import adversarial_attacks\n                def model_loss_fn(x, t):\n                    with tf.variable_scope(tf.get_variable_scope(), reuse = tf.AUTO_REUSE):\n                        y, end_points = MODEL.get_model(x, is_training_pl, bn_decay = bn_decay, num_classes = NUM_CLASSES)\n                    if t is None:\n                        loss = None\n                    else:\n                        loss = MODEL.get_loss(y, t, end_points)\n                    return y, loss\n                x_adv = adversarial_attacks.iter_grad_op(pointclouds_pl, model_loss_fn, one_hot = False, iter = 1, eps = 1, ord = ""2"")\n                _, adv_loss = model_loss_fn(x_adv, labels_pl)\n\n            correct = tf.equal(tf.argmax(pred, 1), tf.to_int64(labels_pl))\n            accuracy = tf.reduce_sum(tf.cast(correct, tf.float32)) / float(BATCH_SIZE)\n            tf.summary.scalar(\'accuracy\', accuracy)\n\n            print ""--- Get training operator""\n            # Get training operator\n            learning_rate = get_learning_rate(batch)\n            tf.summary.scalar(\'learning_rate\', learning_rate)\n            if OPTIMIZER == \'momentum\':\n                optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=MOMENTUM)\n            elif OPTIMIZER == \'adam\':\n                optimizer = tf.train.AdamOptimizer(learning_rate)\n            if ADV:\n                train_op = optimizer.minimize((total_loss + adv_loss) / 2.0, global_step=batch)\n            else:\n                train_op = optimizer.minimize(total_loss, global_step=batch)\n            \n            # Add ops to save and restore all the variables.\n            saver = tf.train.Saver()\n        \n        # Create a session\n        config = tf.ConfigProto()\n        config.gpu_options.allow_growth = True\n        config.allow_soft_placement = True\n        config.log_device_placement = False\n        sess = tf.Session(config=config)\n\n        # Add summary writers\n        merged = tf.summary.merge_all()\n        train_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, \'train\'), sess.graph)\n        test_writer = tf.summary.FileWriter(os.path.join(LOG_DIR, \'test\'), sess.graph)\n\n        # Init variables\n        init = tf.global_variables_initializer()\n        sess.run(init)\n\n        ops = {\'pointclouds_pl\': pointclouds_pl,\n               \'labels_pl\': labels_pl,\n               \'is_training_pl\': is_training_pl,\n               \'pred\': pred,\n               \'loss\': total_loss,\n               \'train_op\': train_op,\n               \'merged\': merged,\n               \'step\': batch,\n               \'end_points\': end_points}\n\n        best_acc = -1\n        for epoch in range(MAX_EPOCH):\n            log_string(\'**** EPOCH %03d ****\' % (epoch))\n            sys.stdout.flush()\n             \n            train_one_epoch(sess, ops, train_writer)\n            eval_one_epoch(sess, ops, test_writer)\n\n            # Save the variables to disk.\n            if epoch % 10 == 0:\n                save_path = saver.save(sess, os.path.join(LOG_DIR, ""model.ckpt""))\n                log_string(""Model saved in file: %s"" % save_path)\n\n\ndef train_one_epoch(sess, ops, train_writer):\n    """""" ops: dict mapping from string to tf ops """"""\n    is_training = True\n    \n    log_string(str(datetime.now()))\n\n    # Make sure batch data is of same size\n    cur_batch_data = np.zeros((BATCH_SIZE,NUM_POINT,TRAIN_DATASET.num_channel()))\n    cur_batch_label = np.zeros((BATCH_SIZE), dtype=np.int32)\n\n    total_correct = 0\n    total_seen = 0\n    loss_sum = 0\n    batch_idx = 0\n    while TRAIN_DATASET.has_next_batch():\n        batch_data, batch_label = TRAIN_DATASET.next_batch(augment=True)\n        #batch_data = provider.random_point_dropout(batch_data)\n        bsize = batch_data.shape[0]\n        cur_batch_data[0:bsize,...] = batch_data\n        cur_batch_label[0:bsize] = batch_label\n\n        feed_dict = {ops[\'pointclouds_pl\']: cur_batch_data,\n                     ops[\'labels_pl\']: cur_batch_label,\n                     ops[\'is_training_pl\']: is_training,}\n        summary, step, _, loss_val, pred_val = sess.run([ops[\'merged\'], ops[\'step\'],\n            ops[\'train_op\'], ops[\'loss\'], ops[\'pred\']], feed_dict=feed_dict)\n        train_writer.add_summary(summary, step)\n        pred_val = np.argmax(pred_val, 1)\n        correct = np.sum(pred_val[0:bsize] == batch_label[0:bsize])\n        total_correct += correct\n        total_seen += bsize\n        loss_sum += loss_val\n        if (batch_idx+1)%50 == 0:\n            log_string(\' ---- batch: %03d ----\' % (batch_idx+1))\n            log_string(\'mean loss: %f\' % (loss_sum / 50))\n            log_string(\'accuracy: %f\' % (total_correct / float(total_seen)))\n            total_correct = 0\n            total_seen = 0\n            loss_sum = 0\n        batch_idx += 1\n\n    TRAIN_DATASET.reset()\n        \ndef eval_one_epoch(sess, ops, test_writer):\n    """""" ops: dict mapping from string to tf ops """"""\n    global EPOCH_CNT\n    is_training = False\n\n    # Make sure batch data is of same size\n    cur_batch_data = np.zeros((BATCH_SIZE,NUM_POINT,TEST_DATASET.num_channel()))\n    cur_batch_label = np.zeros((BATCH_SIZE), dtype=np.int32)\n\n    total_correct = 0\n    total_seen = 0\n    loss_sum = 0\n    batch_idx = 0\n    shape_ious = []\n    total_seen_class = [0 for _ in range(NUM_CLASSES)]\n    total_correct_class = [0 for _ in range(NUM_CLASSES)]\n    \n    log_string(str(datetime.now()))\n    log_string(\'---- EPOCH %03d EVALUATION ----\'%(EPOCH_CNT))\n    \n    while TEST_DATASET.has_next_batch():\n        batch_data, batch_label = TEST_DATASET.next_batch(augment=False)\n        bsize = batch_data.shape[0]\n        # for the last batch in the epoch, the bsize:end are from last batch\n        cur_batch_data[0:bsize,...] = batch_data\n        cur_batch_label[0:bsize] = batch_label\n\n        feed_dict = {ops[\'pointclouds_pl\']: cur_batch_data,\n                     ops[\'labels_pl\']: cur_batch_label,\n                     ops[\'is_training_pl\']: is_training}\n        summary, step, loss_val, pred_val = sess.run([ops[\'merged\'], ops[\'step\'],\n            ops[\'loss\'], ops[\'pred\']], feed_dict=feed_dict)\n        test_writer.add_summary(summary, step)\n        pred_val = np.argmax(pred_val, 1)\n        correct = np.sum(pred_val[0:bsize] == batch_label[0:bsize])\n        total_correct += correct\n        total_seen += bsize\n        loss_sum += loss_val\n        batch_idx += 1\n        for i in range(0, bsize):\n            l = batch_label[i]\n            total_seen_class[l] += 1\n            total_correct_class[l] += (pred_val[i] == l)\n    \n    log_string(\'eval mean loss: %f\' % (loss_sum / float(batch_idx)))\n    log_string(\'eval accuracy: %f\'% (total_correct / float(total_seen)))\n    log_string(\'eval avg class acc: %f\' % (np.mean(np.array(total_correct_class)/np.array(total_seen_class,dtype=np.float))))\n    EPOCH_CNT += 1\n\n    TEST_DATASET.reset()\n    return total_correct/float(total_seen)\n\n\nif __name__ == ""__main__"":\n    log_string(\'pid: %s\'%(str(os.getpid())))\n    train()\n    LOG_FOUT.close()\n'"
src/adversarial_attacks.py,48,"b'import numpy as np\nfrom perturb_proj_tree import PerturbProjTree\nfrom alpha_shape import alpha_shape_border\nfrom sampling import farthest_point_sampling, radial_basis_sampling, sample_on_line_segments\n\ndef iter_l2_attack_n_proj(model, x, y, params):\n    epsilon = float(params[""epsilon""])\n    n = int(params[""n""])\n    tau = float(params[""tau""])\n\n    epsilon = epsilon / float(n)\n    tree = PerturbProjTree(x, thickness = tau)\n    x_perturb = x\n\n    for i in range(n):\n        grad = model.grad_fn(x_perturb, y)\n        perturb = epsilon * grad / np.sqrt(np.sum(grad ** 2))\n        x_perturb = x_perturb + perturb\n        x_perturb = tree.project(x_perturb, perturb)\n\n    return x_perturb\n\ndef mom_l2_attack_n_proj(model, x, y, params):\n    epsilon = params[""epsilon""]\n    mu = params[""mu""]\n    n = params[""n""]\n    tau = params[""tau""]\n\n    epsilon = epsilon / float(n)\n    tree = PerturbProjTree(x, thickness = tau)\n    x_perturb = x\n    grad = np.zeros(x.shape)\n\n    for i in range(n):\n        curr_grad = model.grad_fn(x_perturb, y)\n        grad = mu * grad + curr_grad / np.mean(np.abs(curr_grad))\n        perturb = epsilon * grad / np.sqrt(np.sum(grad ** 2))\n        x_perturb = x_perturb + perturb\n        x_perturb = tree.project(x_perturb, perturb)\n\n    return x_perturb\n\ndef mom_l2_attack(model, x, y, params):\n    epsilon = params[""epsilon""]\n    mu = params[""mu""]\n    n = params[""n""]\n\n    epsilon = epsilon / float(n)\n    x_perturb = x\n    grad = np.zeros(x.shape)\n\n    for i in range(n):\n        curr_grad = model.grad_fn(x_perturb, y)\n        grad = mu * grad + curr_grad / np.mean(np.abs(curr_grad))\n        perturb = epsilon * grad / np.sqrt(np.sum(grad ** 2))\n        x_perturb = x_perturb + perturb\n\n    return x_perturb\n\ndef iter_l2_attack_1_proj(model, x, y, params):\n    epsilon = params[""epsilon""]\n    n = params[""n""]\n    tau = params[""tau""]\n\n    epsilon = epsilon / float(n)\n    tree = PerturbProjTree(x, thickness = tau)\n    x_perturb = x\n\n    for i in range(n):\n        grad = model.grad_fn(x_perturb, y)\n        perturb = epsilon * grad / np.sqrt(np.sum(grad ** 2))\n        x_perturb = x_perturb + perturb\n\n    x_perturb = tree.project(x_perturb, x_perturb - x)\n\n    return x_perturb\n\ndef iter_l2_attack(model, x, y, params):\n    epsilon = float(params[""epsilon""])\n    n = int(params[""n""])\n\n    epsilon = epsilon / float(n)\n    x_perturb = x\n\n    for i in range(n):\n        grad = model.grad_fn(x_perturb, y)\n        perturb = epsilon * grad / np.sqrt(np.sum(grad ** 2))\n        x_perturb = x_perturb + perturb\n\n    return x_perturb\n\ndef normal_jitter(model, x, y, params):\n    epsilon = float(params[""epsilon""])\n    tau = float(params[""tau""])\n\n    tree = PerturbProjTree(x, thickness = tau)\n    perturb = np.random.normal(size = x.shape)\n    perturb = epsilon * perturb / np.sqrt(np.sum(perturb ** 2))\n    x_perturb = x + perturb\n    x_perturb = tree.project(x_perturb, perturb)\n\n    return x_perturb\n\ndef iter_l2_attack_1_sampling(model, x, y, params):\n    epsilon = params[""epsilon""]\n    n = params[""n""]\n    k = params[""k""]\n    kappa = params[""kappa""]\n    tri_all_points = params[""tri_all_points""]\n\n    epsilon = epsilon / float(n)\n    x_perturb = x\n\n    for i in range(n):\n        grad = model.grad_fn(x_perturb, y)\n        perturb = epsilon * grad / np.sqrt(np.sum(grad ** 2))\n        x_perturb = x_perturb + perturb\n\n    sort_idx = np.argsort(np.linalg.norm(x_perturb - x, axis = 1))\n    perturbed = x_perturb[sort_idx[k:]]\n\n    border_points, border_triangles = alpha_shape_border(x_perturb if tri_all_points else perturbed)\n\n    triangles = []\n\n    for tri in border_triangles:\n        triangles.append(border_points[tri])\n\n    sampled = farthest_point_sampling(np.array(triangles), perturbed, k, kappa)\n\n    x_sample = np.empty((len(x_perturb), 3))\n\n    for i in range(len(sort_idx)):\n        if i < k:\n            x_sample[sort_idx[i]] = sampled[i]\n        else:\n            x_sample[sort_idx[i]] = x_perturb[sort_idx[i]]\n\n    return x_sample\n\ndef iter_l2_attack_n_sampling(model, x, y, params):\n    epsilon = float(params[""epsilon""])\n    n = int(params[""n""])\n    k = int(params[""k""])\n    kappa = int(params[""kappa""])\n    tri_all_points = str(params[""tri_all_points""]) == ""True""\n\n    epsilon = epsilon / float(n)\n    x_perturb = x\n\n    for i in range(n):\n        grad = model.grad_fn(x_perturb, y)\n        perturb = epsilon * grad / np.sqrt(np.sum(grad ** 2))\n        x_perturb = x_perturb + perturb\n\n        sort_idx = np.argsort(np.linalg.norm(x_perturb - x, axis = 1))\n        perturbed = x_perturb[sort_idx[k:]]\n\n        border_points, border_triangles = alpha_shape_border(x_perturb if tri_all_points else perturbed)\n\n        triangles = []\n\n        for tri in border_triangles:\n            triangles.append(border_points[tri])\n\n        sampled = farthest_point_sampling(np.array(triangles), perturbed, k, kappa)\n\n        x_sample = np.empty((len(x_perturb), 3))\n\n        for i in range(len(sort_idx)):\n            if i < k:\n                x_sample[sort_idx[i]] = sampled[i]\n            else:\n                x_sample[sort_idx[i]] = x_perturb[sort_idx[i]]\n\n        x_perturb = x_sample\n\n    return x_perturb\n\ndef iter_l2_attack_1_sampling_all(model, x, y, params):\n    epsilon = params[""epsilon""]\n    n = params[""n""]\n    k = params[""k""]\n    kappa = params[""kappa""]\n    tri_all_points = params[""tri_all_points""]\n\n    epsilon = epsilon / float(n)\n    x_perturb = x\n\n    for i in range(n):\n        grad = model.grad_fn(x_perturb, y)\n        perturb = epsilon * grad / np.sqrt(np.sum(grad ** 2))\n        x_perturb = x_perturb + perturb\n\n    sort_idx = np.argsort(np.linalg.norm(x_perturb - x, axis = 1))\n    perturbed = x_perturb[sort_idx[k:]]\n\n    border_points, border_triangles = alpha_shape_border(x_perturb if tri_all_points else perturbed)\n\n    triangles = []\n\n    for tri in border_triangles:\n        triangles.append(border_points[tri])\n\n    sampled = farthest_point_sampling(np.array(triangles), None, len(x_perturb), kappa)\n\n    return sampled\n\ndef iter_l2_attack_1_sampling_rbf(model, x, y, params):\n    epsilon = params[""epsilon""]\n    n = params[""n""]\n    k = params[""k""]\n    kappa = params[""kappa""]\n    num_farthest = params[""num_farthest""]\n    shape = params[""shape""]\n\n    epsilon = epsilon / float(n)\n    x_perturb = x\n\n    for i in range(n):\n        grad = model.grad_fn(x_perturb, y)\n        perturb = epsilon * grad / np.sqrt(np.sum(grad ** 2))\n        x_perturb = x_perturb + perturb\n\n    sort_idx = np.argsort(np.linalg.norm(x_perturb - x, axis = 1))\n    perturbed = x_perturb[sort_idx[k:]]\n\n    border_points, border_triangles = alpha_shape_border(x_perturb)\n\n    triangles = []\n\n    for tri in border_triangles:\n        triangles.append(border_points[tri])\n\n    sampled = radial_basis_sampling(np.array(triangles), perturbed, k, kappa, num_farthest, shape)\n\n    x_sample = np.empty((len(x_perturb), 3))\n\n    for i in range(len(sort_idx)):\n        if i < k:\n            x_sample[sort_idx[i]] = sampled[i]\n        else:\n            x_sample[sort_idx[i]] = x_perturb[sort_idx[i]]\n\n    return x_sample\n\ndef iter_l2_attack_n_sampling_rbf(model, x, y, params):\n    epsilon = params[""epsilon""]\n    n = params[""n""]\n    k = params[""k""]\n    kappa = params[""kappa""]\n    num_farthest = params[""num_farthest""]\n    shape = params[""shape""]\n\n    epsilon = epsilon / float(n)\n    x_perturb = x\n\n    for i in range(n):\n        grad = model.grad_fn(x_perturb, y)\n        perturb = epsilon * grad / np.sqrt(np.sum(grad ** 2))\n        x_perturb = x_perturb + perturb\n\n        sort_idx = np.argsort(np.linalg.norm(x_perturb - x, axis = 1))\n        perturbed = x_perturb[sort_idx[k:]]\n\n        border_points, border_triangles = alpha_shape_border(x_perturb)\n\n        triangles = []\n\n        for tri in border_triangles:\n            triangles.append(border_points[tri])\n\n        sampled = radial_basis_sampling(np.array(triangles), perturbed, k, kappa, num_farthest, shape)\n\n        x_sample = np.empty((len(x_perturb), 3))\n\n        for i in range(len(sort_idx)):\n            if i < k:\n                x_sample[sort_idx[i]] = sampled[i]\n            else:\n                x_sample[sort_idx[i]] = x_perturb[sort_idx[i]]\n\n        x_perturb = x_sample\n\n    return x_perturb\n\ndef iter_l2_attack_top_k(model, x, y, params):\n    epsilon = params[""epsilon""]\n    n = params[""n""]\n    top_k = params[""top_k""]\n\n    epsilon = epsilon / float(n)\n    x_perturb = x\n\n    for i in range(n):\n        grad = model.grad_fn(x_perturb, y)\n        perturb = epsilon * grad / np.sqrt(np.sum(grad ** 2))\n        x_perturb = x_perturb + perturb\n\n    sort_idx = np.argsort(np.linalg.norm(x_perturb - x, axis = 1))\n    x_max = np.empty((len(x_perturb), 3))\n\n    for i in range(len(sort_idx)):\n        if i < len(sort_idx) - top_k:\n            x_max[sort_idx[i]] = x[sort_idx[i]]\n        else:\n            x_max[sort_idx[i]] = x_perturb[sort_idx[i]]\n\n    return x_max\n\ndef iter_l2_adversarial_sticks(model, x, y, params):\n    epsilon = float(params[""epsilon""])\n    n = int(params[""n""])\n    top_k = int(params[""top_k""])\n    sigma = int(params[""sigma""])\n\n    epsilon = epsilon / float(n)\n    tree = PerturbProjTree(x)\n    x_perturb = x\n\n    for i in range(n):\n        grad = model.grad_fn(x_perturb, y)\n        perturb = epsilon * grad / np.sqrt(np.sum(grad ** 2))\n        x_perturb = x_perturb + perturb\n\n    sort_idx = np.argsort(np.linalg.norm(x_perturb - x, axis = 1))\n    perturbed_idx = sort_idx[-top_k:]\n    perturbed = x_perturb[perturbed_idx]\n    x_proj = tree.project(perturbed, perturbed - x[perturbed_idx])\n    x_sample = sample_on_line_segments(x_proj, perturbed, sigma)\n    x_max = np.empty((len(x_perturb), 3))\n\n    for i in range(len(sort_idx)):\n        if i < sigma:\n            x_max[sort_idx[i]] = x_sample[i]\n        elif i < len(sort_idx) - top_k:\n            x_max[sort_idx[i]] = x[sort_idx[i]]\n        else:\n            x_max[sort_idx[i]] = x_perturb[sort_idx[i]]\n\n    return x_max\n\ndef iter_l2_attack_fft(model, x, y, params):\n    epsilon = params[""epsilon""]\n    n = params[""n""]\n\n    epsilon = epsilon / float(n)\n    x_perturb = np.fft.fft2(x)\n\n    for i in range(n):\n        grad = model.grad_freq_fn(x_perturb, y)\n        perturb = epsilon * grad / np.sqrt(np.sum(grad * np.conj(grad)))\n        x_perturb = x_perturb + perturb\n\n    return np.real(np.fft.ifft2(x_perturb))\n\ndef iter_l2_attack_sinks(model, x, y, params):\n    eta = float(params[""eta""])\n    mu = float(params[""mu""])\n    lambda_ = float(params[""lambda_""])\n    n = int(params[""n""])\n    num_sinks = int(params[""num_sinks""])\n\n    dists = np.linalg.norm(x[:, np.newaxis, :] - x[np.newaxis, :, :], axis = 2)\n    dists = np.where(np.eye(len(x)) > 0.0, np.full(dists.shape, np.inf), dists)\n    avg_min_dist = np.mean(np.min(dists, axis = 1))\n    mu = mu * avg_min_dist\n\n    perturbed_idx = np.argsort(np.linalg.norm(model.grad_fn(x, y), axis = 1))\n    sink_source = x[perturbed_idx[-num_sinks:]]\n    \n    pred_idx = np.argmax(model.pred_fn(x))\n    \n    lo = 0.0\n    hi = float(lambda_)\n    res = x\n    \n    # binary search for lambda\n    for _ in range(20):\n        mid = (lo + hi) / 2.0\n        \n        model.reset_sink_fn(sink_source)\n\n        for i in range(n):\n            model.train_sink_fn(x, y, sink_source, mu, mid, eta)\n        \n        x_perturb = model.x_perturb_sink_fn(x, sink_source, mu, mid)\n        \n        if pred_idx == np.argmax(model.pred_fn(x_perturb)):\n            hi = mid\n        else:\n            lo = mid\n            res = x_perturb\n\n    return res\n\ndef chamfer_attack(model, x, y, params):\n    eta = float(params[""eta""])\n    alpha = float(params[""alpha""])\n    lambda_ = float(params[""lambda_""])\n    n = int(params[""n""])\n    \n    pred_idx = np.argmax(model.pred_fn(x))\n    \n    lo = 0.0\n    hi = float(alpha)\n    res = x\n    \n    # binary search for lambda\n    for _ in range(20):\n        mid = (lo + hi) / 2.0\n        \n        model.reset_chamfer_fn(x + 0.0001 * np.random.normal(size = x.shape))\n\n        for i in range(n):\n            model.train_chamfer_fn(x, y, mid, lambda_, eta)\n        \n        x_perturb = model.x_perturb_chamfer_fn()\n        \n        if pred_idx == np.argmax(model.pred_fn(x_perturb)):\n            hi = mid\n        else:\n            lo = mid\n            res = x_perturb\n\n    return res\n'"
src/adversarial_defenses.py,17,"b'import numpy as np\n\ndef remove_outliers_defense(model, x, params):\n    top_k = int(params[""top_k""])\n    num_std = float(params[""num_std""])\n\n    dists = x[np.newaxis, :, :] - x[:, np.newaxis, :]\n    dists = np.linalg.norm(dists, axis = 2)\n\n    dists = np.where(np.eye(len(x)) > 0.0, np.full(dists.shape, np.inf), dists)\n    dists = np.sort(dists, axis = 1)[:, :top_k]\n    dists = np.mean(dists, axis = 1)\n\n    avg = np.mean(dists)\n    std = num_std * np.std(dists)\n\n    remove = dists > avg + std\n    idx = np.argmin(remove)\n    x[remove] = x[idx]\n\n    return x\n\ndef remove_salient_defense(model, x, params):\n    top_k = int(params[""top_k""])\n\n    grads = model.output_grad_fn(x)\n    norms = np.linalg.norm(grads, axis = 2)\n    norms = np.max(norms, axis = 0)\n    remove = np.argsort(norms)[-top_k:]\n\n    mask = np.zeros(len(x))\n    mask[remove] = 1.0\n    idx = np.argmin(mask)\n    x[remove] = x[idx]\n\n    return x\n\ndef random_perturb_defense(model, x, params):\n    std = float(params[""std""])\n\n    x = x + std * np.random.randn(*x.shape)\n\n    return x\n\ndef random_remove_defense(model, x, params):\n    num_points = int(params[""num_points""])\n\n    remove = np.random.choice(len(x), size = num_points, replace = False)\n    mask = np.zeros(len(x))\n    mask[remove] = 1.0\n    idx = np.argmin(mask)\n    x[remove] = x[idx]\n\n    return x'"
src/alpha_shape.py,12,"b""import numpy as np\nfrom scipy.spatial import Delaunay\nfrom numba import jit\nfrom projection import cross\n\ndef alpha_shape_border(x, alpha_std = 0.0, epsilon = 0.001):\n    if epsilon is not None:\n        # jiggle the points a little, so less holes form, as the 3D Delaunay\n        # triangulation seeks to create tetrahedrons, while we want surface triangles\n        x_perturbed = x + epsilon * np.random.random(x.shape) * np.random.choice(np.array([-1, 1]), size = x.shape)\n        x = np.concatenate((x, x_perturbed))\n\n    triangles = set() # set of sets of 3D points (tuples)\n    DT = Delaunay(x) # 3D Delaunay triangulation\n\n    idx_pointer, indexes = DT.vertex_neighbor_vertices\n    min_dists = []\n\n    # compute the alpha value by averaging the distance to nearby points\n    # based on the Delaunay triangulation, for simplicity and speed\n    # each point should only be connected to a few other points in the triangulation\n    for i in range(DT.points.shape[0]):\n        for j in range(idx_pointer[i], idx_pointer[i + 1]):\n            min_dists.append(np.linalg.norm(DT.points[i] - DT.points[indexes[j]]))\n\n    min_dists = np.array(min_dists)\n    avg = np.mean(min_dists)\n    std = np.std(min_dists)\n    alpha = avg + alpha_std * std\n\n    for simplex_idx in DT.simplices:\n        r = circumscribed_radius(DT.points[simplex_idx])\n\n        if r < alpha:\n            # add faces of the tetrahedron to the boundary set, and remove inner faces that are repeated\n            tri_a = frozenset({simplex_idx[0], simplex_idx[1], simplex_idx[2]})\n            tri_b = frozenset({simplex_idx[0], simplex_idx[1], simplex_idx[3]})\n            tri_c = frozenset({simplex_idx[0], simplex_idx[2], simplex_idx[3]})\n            tri_d = frozenset({simplex_idx[1], simplex_idx[2], simplex_idx[3]})\n\n            for tri in (tri_a, tri_b, tri_c, tri_d):\n                if tri in triangles:\n                    triangles.remove(tri)\n                else:\n                    triangles.add(tri)\n\n    res = []\n\n    for tri in triangles:\n        res.append(list(tri))\n\n    return DT.points, np.array(res)\n\n# helper to compute the circumscribed circle's radius of a tetrahedron\n@jit(nopython = True)\ndef circumscribed_radius(simplex):\n    a = simplex[0]\n    b = simplex[1]\n    c = simplex[2]\n    d = simplex[3]\n    V = np.abs(np.dot(b - a, cross(c - a, d - a))) / 6.0 # volume\n    dist_a = np.linalg.norm(a - b) * np.linalg.norm(c - d)\n    dist_b = np.linalg.norm(a - c) * np.linalg.norm(b - d)\n    dist_c = np.linalg.norm(a - d) * np.linalg.norm(b - c)\n    return np.sqrt((dist_a + dist_b + dist_c) * (-dist_a + dist_b + dist_c) * (dist_a - dist_b + dist_c) * (dist_a + dist_b - dist_c)) / (V * 24.0)\n"""
src/example_alpha_shape.py,3,"b'import numpy as np\nimport h5py\nfrom matplotlib import pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom alpha_shape import alpha_shape_border\n\nnp.random.seed(1234)\n\nview_label = ""chair""\noffset_idx = 0\nnum_points = 1024\nf = h5py.File(""../data/point_clouds.hdf5"", ""r"")\nshape_names = [line.rstrip() for line in open(""../data/shape_names.txt"")]\n\npc = f[""points""][:][:, :num_points]\nlabels = f[""labels""][:]\n\nf.close()\n\nprint(""Shape:"", pc.shape)\nprint(""Number of points:"", num_points)\nprint(""Labels:"", [shape_names[idx] for idx in np.unique(labels)])\nprint(""Selected label:"", view_label)\n\nmatch_idx = np.where(labels == shape_names.index(view_label))[0]\nview_pc = pc[match_idx[offset_idx]]\n\nprint(""Shape index:"", match_idx[offset_idx])\n\nplt.figure(figsize = (30, 15))\n\ndef scale_plot():\n    plt.gca().auto_scale_xyz((-1, 1), (-1, 1), (-1, 1))\n    plt.gca().view_init(0, 0)\n    plt.axis(""off"")\n\nplt.subplot(121, projection = ""3d"")\n\nplt.gca().scatter(*view_pc.T, zdir = ""y"", s = 20, c = view_pc.T[1], cmap = ""winter"")\n\nscale_plot()\n\nplt.subplot(122, projection = ""3d"")\n\nalpha_points, alpha_triangles = alpha_shape_border(view_pc, alpha_std = 0.0, epsilon = 0.001)\nalpha_points = alpha_points[:, (0, 2, 1)]\n\nprint(""Number of points in alpha shape:"", alpha_points.shape[0])\nprint(""Number of triangles in alpha shape:"", alpha_triangles.shape[0])\n\nplt.gca().plot_trisurf(*alpha_points.T, triangles = alpha_triangles, cmap = ""winter"")\n\nscale_plot()\n\nplt.subplots_adjust(left = 0, bottom = 0, right = 1, top = 1, wspace = 0, hspace = 0)\nplt.show()\n# plt.savefig("""", bbox_inches = 0)\n'"
src/example_perturb_proj_tree.py,5,"b'import numpy as np\nimport h5py\nimport time\nfrom matplotlib import pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom alpha_shape import alpha_shape_border\nfrom perturb_proj_tree import PerturbProjTree\n\nnp.random.seed(1234)\n\nview_label = ""chair""\noffset_idx = 0\nnum_points = 1024\nf = h5py.File(""../data/point_clouds.hdf5"", ""r"")\nshape_names = [line.rstrip() for line in open(""../data/shape_names.txt"")]\n\npc = f[""points""][:][:, :num_points]\nlabels = f[""labels""][:]\n\nf.close()\n\nprint(""Shape:"", pc.shape)\nprint(""Number of points:"", num_points)\nprint(""Labels:"", [shape_names[idx] for idx in np.unique(labels)])\nprint(""Selected label:"", view_label)\n\nmatch_idx = np.where(labels == shape_names.index(view_label))[0]\nview_pc = pc[match_idx[offset_idx]]\n\nprint(""Shape index:"", match_idx[offset_idx])\n\nplt.figure(figsize = (30, 15))\n\ndef scale_plot():\n    plt.gca().auto_scale_xyz((-1, 1), (-1, 1), (-1, 1))\n    plt.gca().view_init(0, 0)\n    plt.axis(""off"")\n\nplt.subplot(121, projection = ""3d"")\n\nalpha_points, alpha_triangles = alpha_shape_border(view_pc)\nalpha_points = alpha_points[:, (0, 2, 1)]\n\nprint(""Number of points in alpha shape:"", alpha_points.shape[0])\nprint(""Number of triangles in alpha shape:"", alpha_triangles.shape[0])\n\nplt.gca().plot_trisurf(*alpha_points.T, triangles = alpha_triangles)\n\nrand_perturb = (np.random.random(view_pc.shape) * 0.03 + 0.03) * np.random.choice(np.array([-1, 1]), size = view_pc.shape)\nperturbed = view_pc + rand_perturb\n\ncolors = np.random.random(num_points)\nplt.gca().scatter(*perturbed.T, zdir = ""y"", s = 300, c = colors, cmap = ""rainbow"")\n\nscale_plot()\n\nplt.subplot(122, projection = ""3d"")\n\nplt.gca().plot_trisurf(*alpha_points.T, triangles = alpha_triangles)\n\nprint(""Projection started."")\nstart_time = time.time()\ntree = PerturbProjTree(view_pc, thickness = 0.01)\nprojected = tree.project(perturbed, rand_perturb)\nend_time = time.time()\nprint(""Projection ended in %f seconds."" % (end_time - start_time))\n\nplt.gca().scatter(*projected.T, zdir = ""y"", s = 300, c = colors, cmap = ""rainbow"")\n\nscale_plot()\n\nplt.subplots_adjust(left = 0, bottom = 0, right = 1, top = 1, wspace = 0, hspace = 0)\nplt.show()\n'"
src/example_projection.py,3,"b'import numpy as np\nfrom matplotlib import pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom projection import project_point_to_triangle\n\ntri = np.array([[0.0, 0.0, 0.0], [0.0, 1.0, 0.0], [1.0, 1.0, 0.0]])\np_perturb = np.array([1.5, 1.5, 0.5])\n\np_proj = project_point_to_triangle(p_perturb, tri, thickness = 0.1)\n\nplt.figure(figsize = (15, 15))\nplt.subplot(111, projection = ""3d"")\nplt.gca().scatter(*np.vstack((p_perturb, p_proj)).T, s = 500, depthshade = False, c = [""r"", ""g""])\nplt.gca().plot_trisurf(*tri.T, triangles = ((0, 1, 2)))\nplt.show()\n'"
src/perturb_proj_tree.py,18,"b'import numpy as np\nfrom numba import jit\nfrom projection import project_point_to_triangle, bounding_sphere, corner_point\nfrom alpha_shape import alpha_shape_border\n\n@jit(nopython = True)\ndef _query(query_point, query_radius, curr_node, thickness, center, radius_lo, radius_hi, inside_node, outside_node, triangle, is_leaf):\n    # project a point onto its nearest triangles and find the nearest projection location\n    nearest = (None, np.inf)\n\n    if is_leaf[curr_node]:\n        if np.linalg.norm(query_point - center[curr_node]) <= query_radius:\n            # project the point at the leaf node\n            proj_point = project_point_to_triangle(query_point, triangle[curr_node], thickness = thickness)\n            proj_dist = np.linalg.norm(query_point - proj_point)\n            nearest = (proj_point, proj_dist)\n    else:\n        dist = np.linalg.norm(query_point - center[curr_node])\n\n        if dist > radius_lo[curr_node] + query_radius: # query and partition spheres are completely not overlapping\n            nearest = _query(query_point, query_radius, outside_node[curr_node], thickness, center, radius_lo, radius_hi, inside_node, outside_node, triangle, is_leaf)\n        elif dist < radius_hi[curr_node] - query_radius: # query and partition spheres are completely overlapping\n            nearest = _query(query_point, query_radius, inside_node[curr_node], thickness, center, radius_lo, radius_hi, inside_node, outside_node, triangle, is_leaf)\n        else:\n            # must examine both subtrees as the border of the query sphere overlaps the border of the partition sphere\n            nearest_inside = _query(query_point, query_radius, inside_node[curr_node], thickness, center, radius_lo, radius_hi, inside_node, outside_node, triangle, is_leaf)\n            nearest_outside = _query(query_point, query_radius, outside_node[curr_node], thickness, center, radius_lo, radius_hi, inside_node, outside_node, triangle, is_leaf)\n\n            if nearest_inside[1] < nearest_outside[1]:\n                nearest = nearest_inside\n            else:\n                nearest = nearest_outside\n\n    return nearest\n\n@jit(nopython = True)\ndef _project(x_perturb, perturb, max_radius, thickness, root, center, radius_lo, radius_hi, inside_node, outside_node, triangle, is_leaf):\n    epsilon = 1e-8\n    distances = np.sqrt(np.sum(perturb ** 2, axis = 1))\n    x_proj = []\n\n    for i in range(len(x_perturb)):\n        if np.abs(distances[i]) < epsilon: # points that are not perturbed are also not projected\n            x_proj.append(x_perturb[i])\n        else:\n            # query radius = the perturbation distance\n            # + maximum radius of all triangle circumcircles\n            # + thickness of each triangle\n            nearest_point, nearest_dist = _query(x_perturb[i], distances[i] + max_radius + thickness, root, thickness, center, radius_lo, radius_hi, inside_node, outside_node, triangle, is_leaf)\n\n            if nearest_point is None:\n                x_proj.append(x_perturb[i] - perturb[i])\n            else:\n                x_proj.append(nearest_point)\n\n    return x_proj\n\n@jit(nopython = True)\ndef _calc_tri_center(border_points, border_tri):\n    triangles = []\n    tri_center = []\n    max_radius = 0.0\n\n    for i in range(len(border_tri)):\n        # get the minimum bounding sphere of each triangle\n        tri = border_points[border_tri[i]]\n        center, radius = bounding_sphere(tri)\n        triangles.append(tri)\n        tri_center.append(center)\n        max_radius = max(max_radius, radius)\n\n    return triangles, tri_center, max_radius\n\n# each triangle is represented as a point in the tree\nclass PerturbProjTree:\n    def __init__(self, x, alpha_std = 0.0, thickness = 0.0):\n        self.thickness = thickness\n\n        # construct the bounding triangles of the points\n        border_points, border_tri = alpha_shape_border(x, alpha_std = alpha_std)\n        triangles, tri_center, self.max_radius = _calc_tri_center(border_points, border_tri)\n        triangles = np.array(triangles)\n        tri_center = np.vstack(tri_center)\n\n        self.center = np.empty((len(triangles) * 2, 3))\n        self.radius_lo = np.empty(len(triangles) * 2)\n        self.radius_hi = np.empty(len(triangles) * 2)\n        self.inside_node = np.empty(len(triangles) * 2, dtype = int)\n        self.outside_node = np.empty(len(triangles) * 2, dtype = int)\n        self.triangle = np.empty((len(triangles) * 2, 3, 3))\n        self.is_leaf = np.empty(len(triangles) * 2, dtype = bool)\n        self.curr_idx = 0\n\n        self.root = self.build(triangles, tri_center)\n\n    def build(self, curr_triangles, curr_tri_center):\n        if len(curr_triangles) == 0:\n            print(""Bad stuff happened when partitioning!!!"")\n            return None\n\n        if len(curr_triangles) == 1:\n            self.center[self.curr_idx] = curr_tri_center[0]\n            self.triangle[self.curr_idx] = curr_triangles[0]\n            self.is_leaf[self.curr_idx] = True\n            self.curr_idx += 1\n            return self.curr_idx - 1\n\n        # pick corner point to partition with\n        partition_center = corner_point(curr_tri_center)\n\n        # get distances from each triangle\'s point to the partition point\n        distances = np.linalg.norm(curr_tri_center - partition_center[np.newaxis, :], axis = 1)\n\n        # pick the middle point to for the partition radius\n        lo = len(distances) // 2\n        hi = lo - 1\n        # sort by negative distances so all triangle points with the same distance\n        # as the picked mid distance will be to the right in the partition array\n        partition = np.argpartition(-distances, (hi, lo))\n        partition_radius_lo = distances[partition[lo]]\n        partition_radius_hi = distances[partition[hi]]\n\n        inside_idx = partition[lo:]\n        outside_idx = partition[:lo]\n\n        inside_node = self.build(curr_triangles[inside_idx], curr_tri_center[inside_idx])\n        outside_node = self.build(curr_triangles[outside_idx], curr_tri_center[outside_idx])\n\n        self.center[self.curr_idx] = partition_center\n        self.radius_lo[self.curr_idx] = partition_radius_lo\n        self.radius_hi[self.curr_idx] = partition_radius_hi\n        self.inside_node[self.curr_idx] = inside_node\n        self.outside_node[self.curr_idx] = outside_node\n        self.is_leaf[self.curr_idx] = False\n        self.curr_idx += 1\n\n        return self.curr_idx - 1\n\n    def project(self, x_perturb, perturb):\n        res = _project(x_perturb, perturb, self.max_radius, self.thickness, self.root, self.center, self.radius_lo, self.radius_hi, self.inside_node, self.outside_node, self.triangle, self.is_leaf)\n        return np.vstack(res)\n'"
src/perturb_proj_tree_old.py,10,"b'import numpy as np\nfrom projection import project_point_to_triangle, bounding_sphere, corner_point\nfrom alpha_shape import alpha_shape_border\nfrom collections import namedtuple\n\nNode = namedtuple(""Node"", (""center"", ""radius_lo"", ""radius_hi"", ""inside_node"", ""outside_node""))\nLeaf = namedtuple(""Leaf"", (""center"", ""triangle""))\n\n# each triangle is represented as a point in the tree\nclass PerturbProjTree:\n    def __init__(self, x, alpha_std = 0.0, thickness = 0.0):\n        self.thickness = thickness\n\n        # construct the bounding triangles of the points\n        border_points, border_tri = alpha_shape_border(x, alpha_std = alpha_std)\n        triangles = []\n        tri_center = []\n        self.max_radius = 0.0\n\n        for tri in border_tri:\n            # get the minimum bounding sphere of each triangle\n            tri = border_points[tri]\n            center, radius = bounding_sphere(tri)\n            triangles.append(tri)\n            tri_center.append(center)\n            self.max_radius = max(self.max_radius, radius)\n\n        triangles = np.array(triangles)\n        tri_center = np.vstack(tri_center)\n\n        self.root = self.build(triangles, tri_center)\n\n    def build(self, curr_triangles, curr_tri_center):\n        if len(curr_triangles) == 0:\n            return None\n\n        if len(curr_triangles) == 1:\n            return Leaf(curr_tri_center[0], curr_triangles[0])\n\n        # pick corner point to partition with\n        partition_center = corner_point(curr_tri_center)\n\n        # get distances from each triangle\'s point to the partition point\n        distances = np.linalg.norm(curr_tri_center - partition_center[np.newaxis, :], axis = 1)\n\n        # pick the middle point to for the partition radius\n        lo = len(distances) // 2\n        hi = lo - 1\n        # sort by negative distances so all triangle points with the same distance\n        # as the picked mid distance will be to the right in the partition array\n        partition = np.argpartition(-distances, (hi, lo))\n        partition_radius_lo = distances[partition[lo]]\n        partition_radius_hi = distances[partition[hi]]\n\n        inside_idx = partition[lo:]\n        outside_idx = partition[:lo]\n\n        inside_node = self.build(curr_triangles[inside_idx], curr_tri_center[inside_idx])\n        outside_node = self.build(curr_triangles[outside_idx], curr_tri_center[outside_idx])\n\n        return Node(partition_center, partition_radius_lo, partition_radius_hi, inside_node, outside_node)\n\n    def project(self, x_perturb, perturb):\n        distances = np.linalg.norm(perturb, axis = 1)\n        x_proj = []\n        avg_proj_count = 0.0\n\n        for point, dist in zip(x_perturb, distances):\n            if np.isclose(dist, 0.0): # points that are not perturbed are also not projected\n                x_proj.append(point)\n            else:\n                self.projection_count = 0\n                # query radius = the perturbation distance\n                # + maximum radius of all triangle circumcircles\n                # + thickness of each triangle\n                nearest_point, nearest_dist = self.query(point, dist + self.max_radius + self.thickness, self.root)\n                x_proj.append(nearest_point)\n                avg_proj_count += self.projection_count / float(len(x_perturb))\n\n        print(""Average points projected:"", avg_proj_count)\n\n        return np.vstack(x_proj)\n\n    def query(self, query_point, query_radius, curr_node):\n        # project a point onto its nearest triangles and find the nearest projection location\n        nearest = (None, float(""inf""))\n\n        if type(curr_node) == Leaf:\n            if np.linalg.norm(query_point - curr_node.center) <= query_radius:\n                # project the point at the leaf node\n                proj_point = project_point_to_triangle(query_point, curr_node.triangle, thickness = self.thickness)\n                proj_dist = np.linalg.norm(query_point - proj_point)\n                nearest = (proj_point, proj_dist)\n                self.projection_count += 1\n        elif type(curr_node) == Node:\n            dist = np.linalg.norm(query_point - curr_node.center)\n\n            if dist > curr_node.radius_lo + query_radius: # query and partition spheres are completely not overlapping\n                nearest = self.query(query_point, query_radius, curr_node.outside_node)\n            elif dist < curr_node.radius_hi - query_radius: # query and partition spheres are completely overlapping\n                nearest = self.query(query_point, query_radius, curr_node.inside_node)\n            else:\n                # must examine both subtrees as the border of the query sphere overlaps the border of the partition sphere\n                nearest_inside = self.query(query_point, query_radius, curr_node.inside_node)\n                nearest_outside = self.query(query_point, query_radius, curr_node.outside_node)\n\n                if nearest_inside[1] < nearest_outside[1]:\n                    nearest = nearest_inside\n                else:\n                    nearest = nearest_outside\n\n        return nearest\n'"
src/perturb_proj_tree_spheres.py,12,"b'import numpy as np\nfrom projection import project_point_to_triangle, bounding_sphere\nfrom alpha_shape import alpha_shape_border\nfrom collections import namedtuple\n\nNode = namedtuple(""Node"", (""center"", ""radius"", ""inside_node"", ""outside_node""))\nLeaf = namedtuple(""Leaf"", (""bucket""))\n\n# each triangle is represented as a sphere in the tree\nclass PerturbProjTree:\n    def __init__(self, x, alpha_std = 0.0, thickness = 0.0):\n        self.thickness = thickness\n\n        # construct the bounding triangles of the points\n        border_points, border_tri = alpha_shape_border(x, alpha_std = alpha_std)\n        triangles = []\n        tri_center = []\n        tri_radius = []\n\n        for tri in border_tri:\n            # get the minimum bounding sphere of each triangle\n            tri = border_points[tri]\n            center, radius = bounding_sphere(tri)\n            triangles.append(tri)\n            tri_center.append(center)\n            tri_radius.append(radius + self.thickness)\n\n        triangles = np.array(triangles)\n        tri_center = np.vstack(tri_center)\n        tri_radius = np.array(tri_radius)\n\n        self.root = self.build(triangles, tri_center, tri_radius)\n\n    def build(self, curr_triangles, curr_tri_center, curr_tri_radius):\n        if len(curr_triangles) == 0:\n            return None\n\n        if len(curr_triangles) == 1:\n            return Leaf(curr_triangles)\n\n        # pick random point to partition with\n        partition_center = curr_tri_center[np.random.randint(len(curr_tri_center))]\n\n        # get distances from each triangle\'s bounding circle to the partition point\n        distances_center = np.linalg.norm(curr_tri_center - partition_center[np.newaxis, :], axis = 1)\n        # get the distance from partition point to farthest point of each bounding circle\n        distances = distances_center + curr_tri_radius\n\n        # pick the middle point to for the partition radius\n        mid = len(distances) * 3 // 5\n        # sort by negative distances so all bounding spheres with the same distance\n        # as the picked mid distance will be to the right in the partition array\n        partition = np.argpartition(-distances, mid)\n        partition_radius = distances[partition[mid]]\n\n        # bounding spheres that are completely within the partition sphere are counted as inside\n        # bounding spheres that are completely outside the partition sphere are counted as outside\n        # bounding spheres that straddle the partiton sphere are counted as both inside and outside\n        # if a bounding sphere is tangent to the partition sphere, then it is counted as either\n        # inside, or both inside and outside, depending on the position of the bounding sphere\n\n        # if the distance from the partition point to the farthest point in a boundary sphere is less\n        # than or equal to the radius of the partition sphere, then the bounding sphere is definitely inside\n        # if the aforementioned distance is greater than the partition radius, then the bounding sphere may be\n        # either inside, or both inside and outside, and distances must be checked to find the truth\n        inside_idx = partition[mid:]\n        outside_idx = partition[:mid]\n        both_inside_outside = np.nonzero(distances_center[outside_idx] - curr_tri_radius[outside_idx] <= partition_radius)[0]\n        inside_idx = np.concatenate((inside_idx, both_inside_outside))\n\n        if len(inside_idx) == len(curr_triangles):\n            # if the attempt to partition bounding spheres fails, then a bucket that stores a list of triangles is built\n            return Leaf(curr_triangles)\n\n        inside_node = self.build(curr_triangles[inside_idx], curr_tri_center[inside_idx], curr_tri_radius[inside_idx])\n        outside_node = self.build(curr_triangles[outside_idx], curr_tri_center[outside_idx], curr_tri_radius[outside_idx])\n\n        return Node(partition_center, partition_radius, inside_node, outside_node)\n\n    def project(self, x_perturb, perturb):\n        distances = np.linalg.norm(perturb, axis = 1)\n        x_proj = []\n        avg_proj_count = 0.0\n\n        for point, dist in zip(x_perturb, distances):\n            self.projection_count = 0\n            nearest_point, nearest_dist = self.query(point, dist, self.root)\n            x_proj.append(nearest_point)\n            print(""hi"")\n            avg_proj_count += self.projection_count / float(len(x_perturb))\n\n        print(""Average points projected:"", avg_proj_count)\n\n        return np.vstack(x_proj)\n\n    def query(self, query_point, query_radius, curr_node):\n        # project a point onto its nearest triangles and find the nearest projection location\n        nearest = (None, float(""inf""))\n\n        if type(curr_node) == Leaf:\n            for tri in curr_node.bucket:\n                # go through each point in the bucket and project it\n                proj_point = project_point_to_triangle(query_point, tri, thickness = self.thickness)\n                proj_dist = np.linalg.norm(query_point - proj_point)\n\n                if proj_dist < nearest[1]:\n                    nearest = (proj_point, proj_dist)\n\n            self.projection_count += len(curr_node.bucket)\n        elif type(curr_node) == Node:\n            dist = np.linalg.norm(query_point - curr_node.center)\n\n            if dist > curr_node.radius + query_radius: # query and partition spheres are completely not overlapping\n                nearest = self.query(query_point, query_radius, curr_node.outside_node)\n            elif dist <= curr_node.radius - query_radius: # query and partition spheres are completely overlapping\n                nearest = self.query(query_point, query_radius, curr_node.inside_node)\n            else:\n                # must examine both subtrees as the border of the query sphere overlaps the border of the partition sphere\n                nearest_inside = self.query(query_point, query_radius, curr_node.inside_node)\n                nearest_outside = self.query(query_point, query_radius, curr_node.outside_node)\n\n                if nearest_inside[1] < nearest_outside[1]:\n                    nearest = nearest_inside\n                else:\n                    nearest = nearest_outside\n\n        return nearest\n'"
src/pointnet2_interface.py,1,"b'import tensorflow as tf\nimport numpy as np\nimport importlib\nimport sys\n\nclass PointNet2Interface:\n    def __init__(self, max_points, fft = False, sink = None, chamfer = False):\n        tf.reset_default_graph()\n\n        checkpoint_path = ""pointnet2/log/model.ckpt""\n\n        sys.path.append(""pointnet2/models"")\n        model = importlib.import_module(""pointnet2_cls_ssg"")\n\n        self.x_pl, self.y_pl = model.placeholder_inputs(1, max_points)\n        self.is_training = tf.placeholder(tf.bool, shape = ())\n\n        with tf.variable_scope(tf.get_variable_scope(), reuse = tf.AUTO_REUSE):\n            logits, end_points = model.get_model(self.x_pl, self.is_training)\n\n        self.y_pred = tf.nn.softmax(logits)\n        loss = model.get_loss(logits, self.y_pl, end_points)\n        self.grad_loss_wrt_x = tf.gradients(loss, self.x_pl)[0]\n\n        self.grad_out_wrt_x = []\n\n        for i in range(40):\n            self.grad_out_wrt_x.append(tf.gradients(logits[:, i], self.x_pl)[0])\n\n        # load saved parameters\n        saver = tf.train.Saver()\n        config = tf.ConfigProto()\n        config.gpu_options.allow_growth = True\n        self.sess = tf.Session(config = config)\n        saver.restore(self.sess, checkpoint_path)\n        print(""Model restored!"")\n\n        if sink is not None:\n            self.x_clean = tf.placeholder(tf.float32, shape = self.x_pl.shape.as_list())\n            self.init_sink_pl = tf.placeholder(tf.float32, shape = (1, sink, 3))\n            self.sink_source = tf.placeholder(tf.float32, shape = (1, sink, 3))\n            self.epsilon = tf.placeholder(tf.float32, shape = ())\n            self.lambda_ = tf.placeholder(tf.float32, shape = ())\n            self.eta = tf.placeholder(tf.float32, shape = ())\n\n            sinks = tf.get_variable(""sinks"", dtype = tf.float32, shape = (1, sink, 3))\n            self.init_sinks = tf.assign(sinks, self.init_sink_pl)\n\n            dist = tf.linalg.norm(self.sink_source[:, :, tf.newaxis, :] - self.x_clean[:, tf.newaxis, :, :], axis = 3)\n            rbf = tf.exp(-((dist / self.epsilon) ** 2))[:, :, :, tf.newaxis]\n            perturb = rbf * (sinks[:, :, tf.newaxis, :] - self.x_clean[:, tf.newaxis, :, :])\n            self.x_perturb = self.x_clean + tf.tanh(tf.reduce_sum(perturb, axis = 1))\n\n            with tf.variable_scope(tf.get_variable_scope(), reuse = tf.AUTO_REUSE):\n                logits, end_points = model.get_model(self.x_perturb, self.is_training)\n\n            loss = model.get_loss(logits, self.y_pl, end_points)\n            loss_dist = tf.sqrt(tf.reduce_sum((self.x_perturb - self.x_clean) ** 2, axis = (1, 2), keep_dims = True))\n            optimizer = tf.train.AdamOptimizer(learning_rate = self.eta)\n            self.train = optimizer.minimize(-loss + self.lambda_ * loss_dist, var_list = [sinks])\n            self.init_optimizer = tf.variables_initializer([optimizer.get_slot(sinks, name) for name in optimizer.get_slot_names()] + list(optimizer._get_beta_accumulators()))\n\n        if chamfer:\n            self.x_clean_chamfer = tf.placeholder(tf.float32, shape = self.x_pl.shape.as_list())\n            self.x_init_chamfer = tf.placeholder(tf.float32, shape = self.x_pl.shape.as_list())\n            self.lambda_chamfer = tf.placeholder(tf.float32, shape = ())\n            self.alpha_chamfer = tf.placeholder(tf.float32, shape = ())\n            self.eta_chamfer = tf.placeholder(tf.float32, shape = ())\n            self.x_chamfer = tf.get_variable(""x_chamfer"", dtype = tf.float32, shape = self.x_pl.shape.as_list())\n\n            self.init_x_chamfer = tf.assign(self.x_chamfer, self.x_init_chamfer)\n            \n            dist = tf.linalg.norm(self.x_chamfer[:, :, tf.newaxis, :] - self.x_clean_chamfer[:, tf.newaxis, :, :], axis = 3)\n            dist = tf.reduce_min(dist, axis = 2, keep_dims = True)\n            loss_chamfer = tf.reduce_mean(dist, axis = 1, keep_dims = True)\n            \n            with tf.variable_scope(tf.get_variable_scope(), reuse = tf.AUTO_REUSE):\n                logits, end_points = model.get_model(self.x_chamfer, self.is_training)\n\n            loss = model.get_loss(logits, self.y_pl, end_points)\n            \n            loss_l2 = tf.sqrt(tf.reduce_sum((self.x_chamfer - self.x_clean_chamfer) ** 2, axis = (1, 2), keep_dims = True))\n            optimizer_chamfer = tf.train.AdamOptimizer(learning_rate = self.eta_chamfer)\n            self.train_chamfer = optimizer_chamfer.minimize(-loss + self.alpha_chamfer * (loss_chamfer + self.lambda_chamfer * loss_l2), var_list = [self.x_chamfer])\n            self.init_optimizer_chamfer = tf.variables_initializer([optimizer_chamfer.get_slot(self.x_chamfer, name) for name in optimizer_chamfer.get_slot_names()] + list(optimizer_chamfer._get_beta_accumulators()))\n\n    def clean_up(self):\n        self.sess.close()\n\n    def pred_fn(self, x):\n        return self.sess.run(self.y_pred, feed_dict = {self.x_pl: [x], self.is_training: False})[0].astype(float)\n\n    def reset_sink_fn(self, sinks):\n        self.sess.run(self.init_optimizer)\n        self.sess.run(self.init_sinks, feed_dict = {self.init_sink_pl: [sinks]})\n\n    def reset_chamfer_fn(self, x):\n        self.sess.run(self.init_optimizer_chamfer)\n        self.sess.run(self.init_x_chamfer, feed_dict = {self.x_init_chamfer: [x]})\n    \n    def x_perturb_sink_fn(self, x, sink_source, epsilon, lambda_):\n        return self.sess.run(self.x_perturb, feed_dict = {self.x_clean: [x], self.sink_source: [sink_source], self.epsilon: epsilon, self.lambda_: lambda_, self.is_training: False})[0].astype(float)\n\n    def x_perturb_chamfer_fn(self):\n        return self.sess.run(self.x_chamfer)[0].astype(float)\n    \n    def grad_fn(self, x, y):\n        return self.sess.run(self.grad_loss_wrt_x, feed_dict = {self.x_pl: [x], self.y_pl: [y], self.is_training: False})[0].astype(float)\n\n    def train_sink_fn(self, x, y, sink_source, epsilon, lambda_, eta):\n        self.sess.run(self.train, feed_dict = {self.x_clean: [x], self.y_pl: [y], self.sink_source: [sink_source], self.epsilon: epsilon, self.lambda_: lambda_, self.eta: eta, self.is_training: False})\n\n    def train_chamfer_fn(self, x, y, alpha_chamfer, lambda_chamfer, eta_chamfer):\n        self.sess.run(self.train_chamfer, feed_dict = {self.x_clean_chamfer: [x], self.y_pl: [y], self.alpha_chamfer: alpha_chamfer, self.lambda_chamfer: lambda_chamfer, self.eta_chamfer: eta_chamfer, self.is_training: False})\n    \n    def output_grad_fn(self, x):\n        res = []\n\n        for i in range(len(self.grad_out_wrt_x)):\n            res.append(self.sess.run(self.grad_out_wrt_x[i], feed_dict = {self.x_pl: [x], self.is_training: False})[0].astype(float))\n\n        return np.array(res)\n'"
src/pointnet_interface.py,1,"b'import tensorflow as tf\nimport numpy as np\nimport importlib\nimport sys\n\nclass PointNetInterface:\n    def __init__(self, max_points, fft = False, sink = None, chamfer = False):\n        tf.reset_default_graph()\n\n        checkpoint_path = ""pointnet/log/model.ckpt""\n\n        sys.path.append(""pointnet/models"")\n        model = importlib.import_module(""pointnet_cls"")\n\n        self.x_pl, self.y_pl = model.placeholder_inputs(1, max_points)\n        self.is_training = tf.placeholder(tf.bool, shape = ())\n\n        with tf.variable_scope(tf.get_variable_scope(), reuse = tf.AUTO_REUSE):\n            logits, end_points = model.get_model(self.x_pl, self.is_training)\n\n        self.y_pred = tf.nn.softmax(logits)\n        loss = model.get_loss(logits, self.y_pl, end_points)\n        self.grad_loss_wrt_x = tf.gradients(loss, self.x_pl)[0]\n\n        self.grad_out_wrt_x = []\n\n        for i in range(40):\n            self.grad_out_wrt_x.append(tf.gradients(logits[:, i], self.x_pl)[0])\n\n        # load saved parameters\n        saver = tf.train.Saver()\n        config = tf.ConfigProto()\n        config.gpu_options.allow_growth = True\n        self.sess = tf.Session(config = config)\n        saver.restore(self.sess, checkpoint_path)\n        print(""Model restored!"")\n\n        if fft:\n            self.x_freq = tf.placeholder(tf.complex64, shape = self.x_pl.shape.as_list())\n            self.x_time = tf.real(tf.ifft2d(self.x_freq))\n\n            with tf.variable_scope(tf.get_variable_scope(), reuse = tf.AUTO_REUSE):\n                logits, end_points = model.get_model(self.x_time, self.is_training)\n\n            loss = model.get_loss(logits, self.y_pl, end_points)\n            self.grad_loss_wrt_x_freq = tf.gradients(loss, self.x_freq)[0]\n\n        if sink is not None:\n            self.x_clean = tf.placeholder(tf.float32, shape = self.x_pl.shape.as_list())\n            self.init_sink_pl = tf.placeholder(tf.float32, shape = (1, sink, 3))\n            self.sink_source = tf.placeholder(tf.float32, shape = (1, sink, 3))\n            self.epsilon = tf.placeholder(tf.float32, shape = ())\n            self.lambda_ = tf.placeholder(tf.float32, shape = ())\n            self.eta = tf.placeholder(tf.float32, shape = ())\n\n            sinks = tf.get_variable(""sinks"", dtype = tf.float32, shape = (1, sink, 3))\n            self.init_sinks = tf.assign(sinks, self.init_sink_pl)\n\n            dist = tf.linalg.norm(self.sink_source[:, :, tf.newaxis, :] - self.x_clean[:, tf.newaxis, :, :], axis = 3)\n            rbf = tf.exp(-((dist / self.epsilon) ** 2))[:, :, :, tf.newaxis]\n            perturb = rbf * (sinks[:, :, tf.newaxis, :] - self.x_clean[:, tf.newaxis, :, :])\n            self.x_perturb = self.x_clean + tf.tanh(tf.reduce_sum(perturb, axis = 1))\n\n            with tf.variable_scope(tf.get_variable_scope(), reuse = tf.AUTO_REUSE):\n                logits, end_points = model.get_model(self.x_perturb, self.is_training)\n\n            loss = model.get_loss(logits, self.y_pl, end_points)\n            loss_dist = tf.sqrt(tf.reduce_sum((self.x_perturb - self.x_clean) ** 2, axis = (1, 2), keep_dims = True))\n            optimizer = tf.train.AdamOptimizer(learning_rate = self.eta)\n            self.train = optimizer.minimize(-loss + self.lambda_ * loss_dist, var_list = [sinks])\n            self.init_optimizer = tf.variables_initializer([optimizer.get_slot(sinks, name) for name in optimizer.get_slot_names()] + list(optimizer._get_beta_accumulators()))\n        \n        if chamfer:\n            self.x_clean_chamfer = tf.placeholder(tf.float32, shape = self.x_pl.shape.as_list())\n            self.x_init_chamfer = tf.placeholder(tf.float32, shape = self.x_pl.shape.as_list())\n            self.lambda_chamfer = tf.placeholder(tf.float32, shape = ())\n            self.alpha_chamfer = tf.placeholder(tf.float32, shape = ())\n            self.eta_chamfer = tf.placeholder(tf.float32, shape = ())\n            self.x_chamfer = tf.get_variable(""x_chamfer"", dtype = tf.float32, shape = self.x_pl.shape.as_list())\n\n            self.init_x_chamfer = tf.assign(self.x_chamfer, self.x_init_chamfer)\n            \n            dist = tf.linalg.norm(self.x_chamfer[:, :, tf.newaxis, :] - self.x_clean_chamfer[:, tf.newaxis, :, :], axis = 3)\n            dist = tf.reduce_min(dist, axis = 2, keep_dims = True)\n            loss_chamfer = tf.reduce_mean(dist, axis = 1, keep_dims = True)\n            \n            with tf.variable_scope(tf.get_variable_scope(), reuse = tf.AUTO_REUSE):\n                logits, end_points = model.get_model(self.x_chamfer, self.is_training)\n\n            loss = model.get_loss(logits, self.y_pl, end_points)\n            \n            loss_l2 = tf.sqrt(tf.reduce_sum((self.x_chamfer - self.x_clean_chamfer) ** 2, axis = (1, 2), keep_dims = True))\n            optimizer_chamfer = tf.train.AdamOptimizer(learning_rate = self.eta_chamfer)\n            self.train_chamfer = optimizer_chamfer.minimize(-loss + self.alpha_chamfer * (loss_chamfer + self.lambda_chamfer * loss_l2), var_list = [self.x_chamfer])\n            self.init_optimizer_chamfer = tf.variables_initializer([optimizer_chamfer.get_slot(self.x_chamfer, name) for name in optimizer_chamfer.get_slot_names()] + list(optimizer_chamfer._get_beta_accumulators()))\n\n    def clean_up(self):\n        self.sess.close()\n\n    def pred_fn(self, x):\n        return self.sess.run(self.y_pred, feed_dict = {self.x_pl: [x], self.is_training: False})[0].astype(float)\n\n    def reset_sink_fn(self, sinks):\n        self.sess.run(self.init_optimizer)\n        self.sess.run(self.init_sinks, feed_dict = {self.init_sink_pl: [sinks]})\n    \n    def reset_chamfer_fn(self, x):\n        self.sess.run(self.init_optimizer_chamfer)\n        self.sess.run(self.init_x_chamfer, feed_dict = {self.x_init_chamfer: [x]})\n\n    def x_perturb_sink_fn(self, x, sink_source, epsilon, lambda_):\n        return self.sess.run(self.x_perturb, feed_dict = {self.x_clean: [x], self.sink_source: [sink_source], self.epsilon: epsilon, self.lambda_: lambda_, self.is_training: False})[0].astype(float)\n\n    def x_perturb_chamfer_fn(self):\n        return self.sess.run(self.x_chamfer)[0].astype(float)\n    \n    def grad_fn(self, x, y):\n        return self.sess.run(self.grad_loss_wrt_x, feed_dict = {self.x_pl: [x], self.y_pl: [y], self.is_training: False})[0].astype(float)\n\n    def grad_freq_fn(self, x, y):\n        return self.sess.run(self.grad_loss_wrt_x_freq, feed_dict = {self.x_freq: [x], self.y_pl: [y], self.is_training: False})[0].astype(float)\n\n    def train_sink_fn(self, x, y, sink_source, epsilon, lambda_, eta):\n        self.sess.run(self.train, feed_dict = {self.x_clean: [x], self.y_pl: [y], self.sink_source: [sink_source], self.epsilon: epsilon, self.lambda_: lambda_, self.eta: eta, self.is_training: False})\n\n    def train_chamfer_fn(self, x, y, alpha_chamfer, lambda_chamfer, eta_chamfer):\n        self.sess.run(self.train_chamfer, feed_dict = {self.x_clean_chamfer: [x], self.y_pl: [y], self.alpha_chamfer: alpha_chamfer, self.lambda_chamfer: lambda_chamfer, self.eta_chamfer: eta_chamfer, self.is_training: False})\n    \n    def output_grad_fn(self, x):\n        res = []\n\n        for i in range(len(self.grad_out_wrt_x)):\n            res.append(self.sess.run(self.grad_out_wrt_x[i], feed_dict = {self.x_pl: [x], self.is_training: False})[0].astype(float))\n\n        return np.array(res)\n'"
src/projection.py,26,"b'import numpy as np\nfrom numba import jit\n\n@jit(nopython = True)\ndef cross(a, b):\n    return np.array((a[1] * b[2] - a[2] * b[1], a[2] * b[0] - a[0] * b[2], a[0] * b[1] - a[1] * b[0]))\n\n@jit(nopython = True)\ndef norm(a):\n    return np.sqrt(np.sum(a ** 2, axis = 1))\n\n@jit(nopython = True)\ndef project_point_to_triangle(p_perturb, tri, thickness = 0.0):\n    epsilon = 1e-8\n    A = tri[0]\n    B = tri[1]\n    C = tri[2]\n\n    n = cross(B - A, C - A) # normal of triangle\n    n = n / np.linalg.norm(n)\n\n    proj_perpendicular = n * np.dot(p_perturb - A, n) # vector from triangle to p_perturb\n    proj_perpendicular_norm = np.linalg.norm(proj_perpendicular)\n\n    if np.abs(proj_perpendicular_norm) < epsilon:\n        tri_width = np.zeros(3) # perturbation is on the triangle\n    else:\n        tri_width = thickness * proj_perpendicular / proj_perpendicular_norm # vector describing triangle thickness\n\n    if proj_perpendicular_norm > np.linalg.norm(tri_width):\n        p_proj = p_perturb - proj_perpendicular + tri_width # project and offset due to the thickness\n    else:\n        p_proj = p_perturb # keep perturbation since it is in the thick triangle\n\n    p_proj_tri = p_perturb - proj_perpendicular # projection onto triangle, ignoring thickness\n    A_n = cross(B - A, p_proj_tri - A)\n    B_n = cross(C - B, p_proj_tri - B)\n    C_n = cross(A - C, p_proj_tri - C)\n\n    if np.dot(n, A_n) < 0.0 or np.dot(n, B_n) < 0.0 or np.dot(n, C_n) < 0.0: # projection not in triangle\n        border_planes = ((A, B, A + n), (A, C, A + n), (B, C, B + n))\n        border_planes_n = np.vstack((cross(n, A + n - B), cross(n, A + n - C), cross(n, B + n - C)))\n        border_planes_n = border_planes_n / norm(border_planes_n).reshape((3, 1))\n\n        border_points = np.empty((3, 3))\n\n        for i in range(3):\n            plane = border_planes[i]\n            normal = border_planes_n[i]\n            center = (plane[0] + plane[1]) / 2.0 # center and radius (half of the length) of an edge\n            radius = np.linalg.norm(center - plane[0])\n            p_plane = p_proj_tri - normal * np.dot(p_proj_tri - plane[0], normal) # project p_proj_tri onto plane\n\n            if np.linalg.norm(p_plane - center) > radius:\n                points = np.vstack((plane[0], plane[1]))\n                idx = np.argmin(norm(points - p_plane)) # get closest vertex of triangle\n                border_points[i] = points[idx]\n            else:\n                border_points[i] = p_plane\n\n        # get closest intersection point\n        border_dists = norm(p_proj_tri - border_points)\n        closest_border_point = border_points[np.argmin(border_dists)]\n        closest_to_proj = p_perturb - closest_border_point\n        closest_to_proj_norm = np.linalg.norm(closest_to_proj)\n\n        # clip point to sphere with radius thickness, centered at the closest border point\n        if closest_to_proj_norm > thickness and closest_to_proj_norm >= epsilon:\n            p_proj = closest_border_point + thickness * closest_to_proj / closest_to_proj_norm\n        else:\n            p_proj = p_perturb\n\n    return p_proj\n\n@jit(nopython = True)\ndef bounding_sphere(tri):\n    # minimum bounding sphere of 3D triangle\n    A = tri[0]\n    B = tri[1]\n    C = tri[2]\n    A_to_B = B - A\n    A_to_C = C - A\n    B_to_C = C - B\n\n    if np.dot(B - A, C - A) <= 0.0 or np.dot(A - B, C - B) <= 0.0 or np.dot(A - C, B - C) <= 0.0:\n        # right or obtuse triangle\n        edges = np.array((np.linalg.norm(A_to_B), np.linalg.norm(A_to_C), np.linalg.norm(B_to_C)))\n        idx = np.argmax(edges)\n        radius = edges[idx] / 2.0\n        a = np.vstack((A, B, A, C, B, C))\n        b = a[idx * 2:idx * 2 + 1]\n        center = np.sum(b, axis = 0) / 2.0\n    else:\n        # acute triangle\n        normal = cross(A_to_B, A_to_C)\n        # get the center of the bounding sphere\n        center = A + (np.sum(A_to_B ** 2) * cross(A_to_C, normal) + np.sum(A_to_C ** 2) * cross(normal, A_to_B)) / (np.sum(normal ** 2) * 2.0)\n        # get the radius of the bounding sphere\n        radius = np.max(norm(tri - center))\n\n    return center, radius\n\n@jit(nopython = True)\ndef corner_point(points):\n    res = np.full(3, -np.inf)\n\n    for i in range(len(points)):\n        for j in range(3):\n            skip = points[i][j] != res[j]\n\n            if points[i][j] > res[j]:\n                res = points[i]\n\n            if skip:\n                break\n\n    return res\n'"
src/run_adversarial_attacks.py,7,"b'import numpy as np\nimport h5py\nimport adversarial_attacks\nimport adversarial_defenses\nfrom true_proj import project_points_to_triangles\nfrom pointnet_interface import PointNetInterface\nfrom pointnet2_interface import PointNet2Interface\nimport time\nimport os\nimport argparse\n\nstart_time = time.time()\n\nnp.random.seed(1234)\n\nmodels = {\n        ""pointnet"": PointNetInterface,\n        ""pointnet2"": PointNet2Interface\n}\n\nattacks = {\n        ""none"": lambda _a, x, _b, _c: x,\n        ""iter_l2_attack"": adversarial_attacks.iter_l2_attack,\n        ""normal_jitter"": adversarial_attacks.normal_jitter,\n        ""iter_l2_attack_n_proj"": adversarial_attacks.iter_l2_attack_n_proj,\n        ""iter_l2_attack_n_sampling"": adversarial_attacks.iter_l2_attack_n_sampling,\n        ""iter_l2_adversarial_sticks"": adversarial_attacks.iter_l2_adversarial_sticks,\n        ""iter_l2_attack_sinks"": adversarial_attacks.iter_l2_attack_sinks,\n        ""chamfer_attack"": adversarial_attacks.chamfer_attack\n}\n\ndefenses = {\n        ""none"": lambda _a, x, _b: x,\n        ""remove_outliers_defense"": adversarial_defenses.remove_outliers_defense,\n        ""remove_salient_defense"": adversarial_defenses.remove_salient_defense,\n        ""random_perturb_defense"": adversarial_defenses.random_perturb_defense,\n        ""random_remove_defense"": adversarial_defenses.random_remove_defense\n}\n\nparser = argparse.ArgumentParser(description = ""Adversarial attacks and defenses on PointNet and PointNet++."")\nparser.add_argument(""--model"", required = True, choices = models)\nparser.add_argument(""--attack"", required = True, choices = attacks)\nparser.add_argument(""--defense"", required = True, choices = defenses)\nparser.add_argument(""--attack-args"", required = True, nargs = ""*"", type = lambda key_value: key_value.split(""="", 1))\nparser.add_argument(""--defense-args"", required = True, nargs = ""*"", type = lambda key_value: key_value.split(""="", 1))\nargs = parser.parse_args()\nprint(args)\n\ntest_model = args.model\ntest_attack = args.attack\ntest_defense = args.defense\nattack_args = dict(args.attack_args)\ndefense_args = dict(args.defense_args)\n\nfft = test_attack == ""iter_l2_attack_fft""\nsink = int(attack_args[""num_sinks""]) if test_attack == ""iter_l2_attack_sinks"" else None\nchamfer = test_attack == ""chamfer_attack""\n\nclass_names_path = ""Adversarial-point-perturbations-on-3D-objects/data/shape_names.txt""\ninput_data_path = ""Adversarial-point-perturbations-on-3D-objects/data/point_clouds.hdf5""\noutput_dir = ""Adversarial-point-perturbations-on-3D-objects/output_save""\nnum_point_clouds = 10000\nmax_points = 1024\n\ntry:\n    os.makedirs(output_dir)\nexcept OSError:\n    pass\n\nclass_names = [line.rstrip() for line in open(class_names_path)]\n\nwith h5py.File(input_data_path, ""r"") as file:\n    X = file[""points""][:][:num_point_clouds, :max_points, :]\n    Y = file[""labels""][:][:num_point_clouds]\n    T = file[""faces""][:][:num_point_clouds, :, :3, :]\n\nmodel_name = test_model\nmodel_type = models[test_model]\nmodel = model_type(max_points, fft = fft, sink = sink, chamfer = chamfer)\n\nattack_name = test_attack\nattack_fn = attacks[test_attack]\nattack_dict = attack_args\n\ndefense_name = test_defense\ndefense_fn = defenses[test_defense]\ndefense_dict = defense_args\n\nprint(""Model name\\t%s"" % model_name)\nprint(""Attack name\\t%s"" % attack_name)\nprint(""Attack parameters\\t%s"" % attack_dict)\nprint(""Defense name\\t%s"" % defense_name)\nprint(""Defense parameters\\t%s"" % defense_dict)\nattack_start_time = time.time()\n\nsuccessfully_attacked = 0\ntotal_attacked = 0\nall_attacked = []\n#avg_dist = 0.0\n\nfor idx in range(len(X)):\n    x = X[idx]\n    t = T[idx]\n    y_idx = Y[idx] # index of correct output\n    y_pred = model.pred_fn(x)\n    y_pred_idx = np.argmax(y_pred)\n\n    if y_pred_idx == y_idx: # model makes correct prediction\n        x_adv = defense_fn(model, attack_fn(model, np.copy(x), y_idx, attack_dict), defense_dict)\n        y_adv_pred = model.pred_fn(x_adv)\n        grad_adv = model.grad_fn(x_adv, y_idx)\n        y_adv_pred_idx = np.argmax(y_adv_pred)\n\n        #if defense_name == ""none"":\n        #    x_adv_proj = project_points_to_triangles(x_adv, t)\n        #    dist = np.max(np.linalg.norm(x_adv_proj - x_adv, axis = 1))\n        #    avg_dist += dist\n\n        if y_adv_pred_idx != y_idx:\n            successfully_attacked += 1\n\n        total_attacked += 1\n\n        all_attacked.append((x, y_pred, x_adv, y_adv_pred, grad_adv))\n\nall_attacked = list(zip(*all_attacked))\nall_attacked = [np.array(a) for a in all_attacked]\ntimestamp = int(time.time())\nsave_file = ""%s/%d_%s_%s_%s.npz"" % (output_dir, timestamp, model_name, attack_name, defense_name)\nnp.savez_compressed(save_file, x = all_attacked[0], y_pred = all_attacked[1], x_adv = all_attacked[2], y_adv_pred = all_attacked[3], grad_adv = all_attacked[4])\n\n#avg_dist = avg_dist / float(len(X))\n\nprint(""Current time\\t%d"" % timestamp)\nprint(""Elapsed time\\t%f"" % (timestamp - attack_start_time))\nprint(""Number of attempted attacks\\t%d"" % total_attacked)\nprint(""Number of successful attacks\\t%d"" % successfully_attacked)\n\n#if defense_name == ""none"":\n#    print(""Average Haussdorf distance\\t%f"" % avg_dist)\n\nprint(""Data saved in\\t%s"" % save_file)\nprint()\n\nmodel.clean_up()\n\nprint(""Total elapsed time\\t%f"" % (time.time() - start_time))\n'"
src/run_old_adversarial_attacks.py,8,"b'import numpy as np\nimport h5py\nimport adversarial_attacks\nimport adversarial_defenses\nfrom true_proj import project_points_to_triangles\nfrom pointnet_interface import PointNetInterface\nfrom pointnet2_interface import PointNet2Interface\nimport time\nimport os\n\nstart_time = time.time()\n\nnp.random.seed(1234)\n\nmodels = (\n        (""pointnet"", PointNetInterface),\n        (""pointnet2"", PointNet2Interface)\n)\n\ntest_models = (0,)\n\nattacks = (\n        (""none"", lambda _a, x, _b, _c: x, {}),\n        (""iter_l2_attack_1_proj"", adversarial_attacks.iter_l2_attack_1_proj, {""epsilon"": 1.0, ""n"": 10, ""tau"": 0.05}),\n        (""iter_l2_attack"", adversarial_attacks.iter_l2_attack, {""epsilon"": 2.0, ""n"": 20}),\n        (""mom_l2_attack"", adversarial_attacks.mom_l2_attack, {""epsilon"": 1.0, ""mu"": 1.0, ""n"": 10}),\n        (""normal_jitter"", adversarial_attacks.normal_jitter, {""epsilon"": 1.0, ""tau"": 0.05}),\n        (""iter_l2_attack_n_proj"", adversarial_attacks.iter_l2_attack_n_proj, {""epsilon"": 1.0, ""n"": 20, ""tau"": 0.05}),\n        (""mom_l2_attack_n_proj"", adversarial_attacks.mom_l2_attack_n_proj, {""epsilon"": 1.0, ""mu"": 1.0, ""n"": 10, ""tau"": 0.05}),\n        (""iter_l2_attack_1_sampling"", adversarial_attacks.iter_l2_attack_1_sampling, {""epsilon"": 3.0, ""n"": 10, ""k"": 500, ""kappa"": 10, ""tri_all_points"": True}),\n        (""iter_l2_attack_1_sampling_all"", adversarial_attacks.iter_l2_attack_1_sampling_all, {""epsilon"": 3.0, ""n"": 10, ""k"": 500, ""kappa"": 10, ""tri_all_points"": True}),\n        (""iter_l2_attack_n_sampling"", adversarial_attacks.iter_l2_attack_n_sampling, {""epsilon"": 2.0, ""n"": 20, ""k"": 500, ""kappa"": 10, ""tri_all_points"": True}),\n        (""iter_l2_attack_1_sampling_rbf"", adversarial_attacks.iter_l2_attack_1_sampling_rbf, {""epsilon"": 3.0, ""n"": 10, ""k"": 500, ""kappa"": 10, ""num_farthest"": None, ""shape"": 5.0}),\n        (""iter_l2_attack_n_sampling_rbf"", adversarial_attacks.iter_l2_attack_n_sampling_rbf, {""epsilon"": 3.0, ""n"": 10, ""k"": 500, ""kappa"": 10, ""num_farthest"": None, ""shape"": 5.0}),\n        (""iter_l2_attack_top_k"", adversarial_attacks.iter_l2_attack_top_k, {""epsilon"": 3.0, ""n"": 10, ""top_k"": 10}),\n        (""iter_l2_adversarial_sticks"", adversarial_attacks.iter_l2_adversarial_sticks, {""epsilon"": 2.0, ""n"": 20, ""top_k"": 30, ""sigma"": 200}),\n        (""iter_l2_attack_fft"", adversarial_attacks.iter_l2_attack_fft, {""epsilon"": 20.0, ""n"": 10}),\n        (""iter_l2_attack_sinks"", adversarial_attacks.iter_l2_attack_sinks, {""eta"": 0.1, ""mu"": 5.0, ""lambda_"": 10000.0, ""n"": 20, ""num_sinks"": 30}),\n        (""chamfer_attack"", adversarial_attacks.chamfer_attack, {""eta"": 0.1, ""alpha"": 10000.0, ""lambda_"": 0.002, ""n"": 20})\n)\n\nfft = False\nsink = 30\nchamfer = True\n\ntest_attacks = (0, 2, 5, 9, 13, 15, 16)\n\ndefenses = (\n        (""none"", lambda _a, x, _b: x, {}),\n        (""remove_outliers_defense"", adversarial_defenses.remove_outliers_defense, {""top_k"": 10, ""num_std"": 1.0}),\n        (""remove_salient_defense"", adversarial_defenses.remove_salient_defense, {""top_k"": 200}),\n        (""random_perturb_defense"", adversarial_defenses.random_perturb_defense, {""std"": 0.05}),\n        (""random_remove_defense"", adversarial_defenses.random_remove_defense, {""num_points"": 200})\n)\n\ntest_defenses = (0, 1, 2, 3, 4)\n\nclass_names_path = ""Adversarial-point-perturbations-on-3D-objects/data/shape_names.txt""\ninput_data_path = ""Adversarial-point-perturbations-on-3D-objects/data/point_clouds.hdf5""\noutput_dir = ""Adversarial-point-perturbations-on-3D-objects/output_save""\nnum_point_clouds = 10000\nmax_points = 1024\n\ntry:\n    os.makedirs(output_dir)\nexcept OSError:\n    pass\n\nclass_names = [line.rstrip() for line in open(class_names_path)]\n\nwith h5py.File(input_data_path, ""r"") as file:\n    X = file[""points""][:][:num_point_clouds, :max_points, :]\n    Y = file[""labels""][:][:num_point_clouds]\n    T = file[""faces""][:][:num_point_clouds, :, :3, :]\n\nfor model_idx in test_models:\n    model_name = models[model_idx][0]\n    model_type = models[model_idx][1]\n    model = model_type(max_points, fft = fft, sink = sink, chamfer = chamfer)\n\n    for attack_idx in test_attacks:\n        attack_name = attacks[attack_idx][0]\n        attack_fn = attacks[attack_idx][1]\n        attack_dict = attacks[attack_idx][2]\n\n        attack_cache = {}\n\n        for defense_idx in test_defenses:\n            defense_name = defenses[defense_idx][0]\n            defense_fn = defenses[defense_idx][1]\n            defense_dict = defenses[defense_idx][2]\n\n            print(""Model name\\t%s"" % model_name)\n            print(""Attack name\\t%s"" % attack_name)\n            print(""Attack parameters\\t%s"" % attack_dict)\n            print(""Defense name\\t%s"" % defense_name)\n            print(""Defense parameters\\t%s"" % defense_dict)\n            attack_start_time = time.time()\n\n            successfully_attacked = 0\n            total_attacked = 0\n            all_attacked = []\n            #avg_dist = 0.0\n\n            for idx in range(len(X)):\n                x = X[idx]\n                t = T[idx]\n                y_idx = Y[idx] # index of correct output\n                y_pred = model.pred_fn(x)\n                y_pred_idx = np.argmax(y_pred)\n\n                if y_pred_idx == y_idx: # model makes correct prediction\n                    if idx in attack_cache:\n                        curr_x_adv = attack_cache[idx]\n                    else:\n                        curr_x_adv = attack_fn(model, np.copy(x), y_idx, attack_dict)\n                        attack_cache[idx] = curr_x_adv\n\n                    x_adv = defense_fn(model, np.copy(curr_x_adv), defense_dict)\n                    y_adv_pred = model.pred_fn(x_adv)\n                    grad_adv = model.grad_fn(x_adv, y_idx)\n                    y_adv_pred_idx = np.argmax(y_adv_pred)\n\n                    #if defense_name == ""none"":\n                    #    x_adv_proj = project_points_to_triangles(x_adv, t)\n                    #    dist = np.max(np.linalg.norm(x_adv_proj - x_adv, axis = 1))\n                    #    avg_dist += dist\n\n                    if y_adv_pred_idx != y_idx:\n                        successfully_attacked += 1\n\n                    total_attacked += 1\n\n                    all_attacked.append((x, y_pred, x_adv, y_adv_pred, grad_adv))\n\n            all_attacked = list(zip(*all_attacked))\n            all_attacked = [np.array(a) for a in all_attacked]\n            timestamp = int(time.time())\n            save_file = ""%s/%d_%s_%s_%s.npz"" % (output_dir, timestamp, model_name, attack_name, defense_name)\n            np.savez_compressed(save_file, x = all_attacked[0], y_pred = all_attacked[1], x_adv = all_attacked[2], y_adv_pred = all_attacked[3], grad_adv = all_attacked[4])\n\n            #avg_dist = avg_dist / float(len(X))\n\n            print(""Current time\\t%d"" % timestamp)\n            print(""Elapsed time\\t%f"" % (timestamp - attack_start_time))\n            print(""Number of attempted attacks\\t%d"" % total_attacked)\n            print(""Number of successful attacks\\t%d"" % successfully_attacked)\n\n            #if defense_name == ""none"":\n            #    print(""Average Haussdorf distance\\t%f"" % avg_dist)\n\n            print(""Data saved in\\t%s"" % save_file)\n            print()\n\n    model.clean_up()\n\nprint(""Total elapsed time\\t%f"" % (time.time() - start_time))\n'"
src/sampling.py,24,"b'import numpy as np\nfrom numba import jit\nfrom projection import cross, norm\n\n@jit(nopython = True)\ndef _init_seed():\n    np.random.seed(1234)\n\n_init_seed()\n\n@jit(nopython = True)\ndef binary_search(a, b):\n    lo = 0\n    hi = len(a) - 1\n\n    while lo < hi:\n        m = (lo + hi - 1) // 2\n\n        if a[m] < b:\n            lo = m + 1\n        else:\n            hi = m\n\n    if a[hi] == b:\n        return min(hi + 1, len(a) - 1)\n    else:\n        return hi\n\n@jit(nopython = True)\ndef sample_points(triangles, num_points):\n    prefix_areas = []\n\n    for i in range(len(triangles)):\n        area = np.linalg.norm(cross(triangles[i][2] - triangles[i][0], triangles[i][1] - triangles[i][0])) / 2.0\n\n        if i == 0:\n            prefix_areas.append(area)\n        else:\n            prefix_areas.append(prefix_areas[i - 1] + area)\n\n    prefix_areas = np.array(prefix_areas)\n    total_area = prefix_areas[-1]\n    points = np.empty((num_points, 3))\n\n    for i in range(num_points):\n        rand = np.random.uniform(0.0, total_area)\n        idx = binary_search(prefix_areas, rand)\n\n        a = triangles[idx][0]\n        b = triangles[idx][1]\n        c = triangles[idx][2]\n\n        r1 = np.random.random()\n        r2 = np.random.random()\n\n        if r1 + r2 >= 1.0:\n            r1 = 1.0 - r1\n            r2 = 1.0 - r2\n\n        point = a + r1 * (c - a) + r2 * (b - a)\n        points[i] = point\n\n    return points\n\n@jit(nopython = True)\ndef farthest_point(sampled_points, initial_points, num_points):\n    curr_points = np.empty((num_points, 3))\n    dists = np.full(len(sampled_points), np.inf)\n\n    if initial_points is None:\n        dists = np.minimum(dists, norm(sampled_points - sampled_points[0].reshape((1, -1))))\n        curr_points[0] = sampled_points[0]\n        start_idx = 1\n    else:\n        for i in range(len(initial_points)):\n            dists = np.minimum(dists, norm(sampled_points - initial_points[i].reshape((1, -1))))\n\n        start_idx = 0\n\n    for i in range(start_idx, num_points):\n        curr_points[i] = sampled_points[np.argmax(dists)]\n        dists = np.minimum(dists, norm(sampled_points - curr_points[i].reshape((1, -1))))\n\n    return curr_points\n\n@jit(nopython = True)\ndef farthest_point_sampling(triangles, initial_points, num_points, kappa):\n    sampled_points = sample_points(triangles, kappa * num_points)\n    return farthest_point(sampled_points, initial_points, num_points)\n\n@jit(nopython = True)\ndef gaussian_rbf(norm, shape):\n    return np.exp(-((shape * norm) ** 2))\n\n@jit(nopython = True)\ndef radial_basis(sampled_points, initial_points, num_points, shape):\n    probs = []\n    total_prob = 0.0\n    curr_points = np.empty((num_points, 3))\n\n    for i in range(len(sampled_points)):\n        prob = -np.inf\n\n        for j in range(len(initial_points)):\n            prob = max(prob, gaussian_rbf(np.linalg.norm(sampled_points[i] - initial_points[j]), shape))\n\n        probs.append(prob)\n        total_prob += prob\n\n    for i in range(num_points):\n        rand = np.random.uniform(0.0, total_prob)\n        sum_prob = 0.0\n\n        for j in range(len(sampled_points)):\n            sum_prob += probs[j]\n\n            if rand < sum_prob or j == len(sampled_points) - 1:\n                curr_points[i] = sampled_points[j]\n                total_prob -= probs[j]\n                probs[j] = 0.0\n                break\n\n    return curr_points\n\n@jit(nopython = True)\ndef radial_basis_sampling(triangles, initial_points, num_points, kappa, num_farthest, shape):\n    if num_farthest is None:\n        sampled_points = sample_points(triangles, kappa * num_points)\n        radial_basis_points = radial_basis(sampled_points, initial_points, kappa // 2 * num_points, shape)\n        return farthest_point(radial_basis_points, None, num_points)\n    else:\n        sampled_points = sample_points(triangles, kappa * num_points)\n        radial_basis_points = radial_basis(sampled_points, initial_points, num_points - num_farthest, shape)\n        initial_points = np.concatenate((initial_points, radial_basis_points))\n        return np.concatenate((farthest_point(sampled_points, initial_points, num_farthest), radial_basis_points))\n\n@jit(nopython = True)\ndef sample_on_line_segments(x, x_perturb, sigma):\n    small_perturb = 0.01\n    norms = norm(x_perturb - x)\n    prefix = []\n\n    for i in range(len(norms)):\n        if i == 0:\n            prefix.append(norms[i])\n        else:\n            prefix.append(prefix[i - 1] + norms[i])\n\n    total_prob = prefix[-1]\n    count = np.zeros(len(norms))\n\n    for i in range(sigma):\n        rand = np.random.uniform(0.0, total_prob)\n        idx = binary_search(prefix, rand)\n        count[idx] += 1.0\n\n    x_sample = np.empty((sigma, 3))\n    idx = 0\n\n    for i in range(len(norms)):\n        for j in range(count[i]):\n            x_sample[idx] = x[i] + (x_perturb[i] - x[i]) * j / count[i] + small_perturb * np.random.randn(3)\n            idx += 1\n\n    return x_sample\n'"
src/true_proj.py,4,"b'import numpy as np\nfrom numba import jit\nfrom projection import project_point_to_triangle\n\ndef project_points_to_triangles(x, t):\n    triangles = set()\n\n    for tri in t:\n        triangles.add(tuple(sorted([tuple(a) for a in tri])))\n\n    triangles = np.array(list(triangles))\n\n    return _project_points_to_triangles(x, triangles)\n\n@jit(nopython = True)\ndef _project_points_to_triangles(x, t):\n    x_proj = np.empty((len(x), 3))\n\n    for i in range(len(x)):\n        min_dist = np.inf\n\n        for j in range(len(t)):\n            p = project_point_to_triangle(x[i], t[j])\n            dist = np.linalg.norm(x[i] - p)\n\n            if dist < min_dist:\n                x_proj[i] = p\n                min_dist = dist\n\n    return x_proj\n'"
src/visualize_array_attack_defense.py,2,"b'from matplotlib import pyplot as plt\nimport numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D\n\nplt.rcParams[""text.usetex""] = True\nplt.rcParams[""text.latex.unicode""] = True\n\n# defenses x attacks\npaths = [\n    [\n        ""../output_save/1582094622_pointnet_none_none.npz"",\n        ""../output_save/1582096639_pointnet_iter_l2_attack_none.npz"",\n        ""../output_save/1582508068_pointnet_chamfer_attack_none.npz"",\n        ""../output_save/1582152554_pointnet_iter_l2_attack_n_proj_none.npz"",\n        ""../output_save/1582182744_pointnet_iter_l2_attack_n_sampling_none.npz"",\n        ""../output_save/1582186116_pointnet_iter_l2_adversarial_sticks_none.npz"",\n        ""../output_save/1582453960_pointnet_iter_l2_attack_sinks_none.npz""\n    ],\n    [\n        ""../output_save/1582094981_pointnet_none_remove_outliers_defense.npz"",\n        ""../output_save/1582097003_pointnet_iter_l2_attack_remove_outliers_defense.npz"",\n        ""../output_save/1582508626_pointnet_chamfer_attack_remove_outliers_defense.npz"",\n        ""../output_save/1582152895_pointnet_iter_l2_attack_n_proj_remove_outliers_defense.npz"",\n        ""../output_save/1582183086_pointnet_iter_l2_attack_n_sampling_remove_outliers_defense.npz"",\n        ""../output_save/1582186458_pointnet_iter_l2_adversarial_sticks_remove_outliers_defense.npz"",\n        ""../output_save/1582454309_pointnet_iter_l2_attack_sinks_remove_outliers_defense.npz""\n    ],\n    [\n        ""../output_save/1582095976_pointnet_none_remove_salient_defense.npz"",\n        ""../output_save/1582097991_pointnet_iter_l2_attack_remove_salient_defense.npz"",\n        ""../output_save/1582510577_pointnet_chamfer_attack_remove_salient_defense.npz"",\n        ""../output_save/1582153775_pointnet_iter_l2_attack_n_proj_remove_salient_defense.npz"",\n        ""../output_save/1582183969_pointnet_iter_l2_attack_n_sampling_remove_salient_defense.npz"",\n        ""../output_save/1582187336_pointnet_iter_l2_adversarial_sticks_remove_salient_defense.npz"",\n        ""../output_save/1582455365_pointnet_iter_l2_attack_sinks_remove_salient_defense.npz""\n    ]\n]\n\nxlabels = [""None"", ""Iter. gradient $L_2$"", ""Chamfer"", ""Distributional"", ""Perturb. resample"", ""Adv. sticks"", ""Adv. sinks""]\nylabels = [""None"", ""Remove outliers"", ""Remove salient""]\n\nmodel = ""stool""\noffset_idx = 0\nshape_names = [line.rstrip() for line in open(""../data/shape_names.txt"")]\n\nfiles = []\n\nfor attack_paths in paths:\n    f = []\n\n    for path in attack_paths:\n        f.append(np.load(path))\n\n    files.append(f)\n\nmodel_idx = shape_names.index(model)\nmatch_idx = np.where(np.argmax(files[0][0][""y_pred""], axis = 1) == model_idx)[0]\nmatch_idx = match_idx[offset_idx]\n\nplt.figure(figsize = (30, 15))\n\ndef scale_plot():\n    scale = 0.85\n    plt.gca().auto_scale_xyz((-scale, scale), (-scale, scale), (-scale, scale))\n    plt.gca().view_init(30, 60)\n    plt.axis(""off"")\n\nfor i, attack_files in enumerate(files):\n    for j, f in enumerate(attack_files):\n        plt.subplot(len(files), len(files[0]), i * len(files[0]) + j + 1, projection = ""3d"")\n        plt.gca().scatter(*f[""x_adv""][match_idx].T, zdir = ""y"", s = 5, c = f[""x_adv""][match_idx].T[2], cmap = ""winter"")\n        scale_plot()\n\nfor i in range(len(xlabels)):\n    plt.gcf().text(i / (float(len(xlabels)) + 0.35) + 0.5 / len(xlabels) + 0.05, 0.9, xlabels[i], fontsize = 30, horizontalalignment = ""center"")\n\nfor i in range(len(ylabels)):\n    plt.gcf().text(0.05, i / (float(len(ylabels)) + 0.1) + 0.5 / len(ylabels), ylabels[-i - 1], fontsize = 30, rotation = ""vertical"", verticalalignment = ""center"")\n\nplt.gcf().text(0.5, 0.96, ""Attacks"", fontsize = 40, horizontalalignment = ""center"")\nplt.gcf().text(0.01, 0.5, ""Defenses"", fontsize = 40, rotation = ""vertical"", verticalalignment = ""center"")\n\nplt.subplots_adjust(left = 0.05, bottom = 0, right = 1, top = 0.95, wspace = 0, hspace = 0)\nplt.savefig(""../figures/attack_defense.pdf"", bbox_inches = ""tight"")\nplt.show()\n'"
src/visualize_array_object_attack.py,2,"b'from matplotlib import pyplot as plt\nimport numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D\n\nplt.rcParams[""text.usetex""] = True\nplt.rcParams[""text.latex.unicode""] = True\n\n# attacks\npaths = [\n    ""../output_save/1582094622_pointnet_none_none.npz"",\n    ""../output_save/1582096639_pointnet_iter_l2_attack_none.npz"",\n    ""../output_save/1582508068_pointnet_chamfer_attack_none.npz"",\n    ""../output_save/1582152554_pointnet_iter_l2_attack_n_proj_none.npz"",\n    ""../output_save/1582182744_pointnet_iter_l2_attack_n_sampling_none.npz"",\n    ""../output_save/1582186116_pointnet_iter_l2_adversarial_sticks_none.npz"",\n    ""../output_save/1582453960_pointnet_iter_l2_attack_sinks_none.npz""\n]\n\nxlabels = [""None"", ""Iter. gradient $L_2$"", ""Chamfer"", ""Distributional"", ""Perturb. resample"", ""Adv. sticks"", ""Adv. sinks""]\n\nmodels = [""car"", ""person"", ""lamp"", ""chair"", ""vase""]\noffset_idx = [1, 0, 0, 1, 0]\nshape_names = [line.rstrip() for line in open(""../data/shape_names.txt"")]\n\nfiles = []\n\nfor path in paths:\n    files.append(np.load(path))\n\nmatch_idx = []\n\nfor i, model in enumerate(models):\n    model_idx = shape_names.index(model)\n    idx = np.where(np.argmax(files[0][""y_pred""], axis = 1) == model_idx)[0]\n    idx = idx[offset_idx[i]]\n    match_idx.append(idx)\n\nplt.figure(figsize = (24, 20))\n\ndef scale_plot():\n    scale = 0.7\n    plt.gca().auto_scale_xyz((-scale, scale), (-scale, scale), (-scale, scale))\n    plt.gca().view_init(30, 120)\n    plt.axis(""off"")\n\nfor i, idx in enumerate(match_idx):\n    for j, f in enumerate(files):\n        plt.subplot(len(match_idx), len(files), i * len(files) + j + 1, projection = ""3d"")\n        plt.gca().scatter(*f[""x_adv""][idx].T, zdir = ""y"", s = 5, c = f[""x_adv""][idx].T[2], cmap = ""winter"")\n        scale_plot()\n\nfor i in range(len(xlabels)):\n    plt.gcf().text(i / (float(len(xlabels)) + 0.35) + 0.5 / len(xlabels) + 0.05, 0.93, xlabels[i], fontsize = 30, horizontalalignment = ""center"")\n\nfor i in range(len(models)):\n    plt.gcf().text(0.05, i / (float(len(models)) + 0.1) + 0.5 / len(models), models[-i - 1].capitalize(), fontsize = 30, rotation = ""vertical"", verticalalignment = ""center"")\n\nplt.gcf().text(0.5, 0.96, ""Attacks"", fontsize = 40, horizontalalignment = ""center"")\nplt.gcf().text(0.01, 0.5, ""Objects"", fontsize = 40, rotation = ""vertical"", verticalalignment = ""center"")\n\nplt.subplots_adjust(left = 0.05, bottom = 0, right = 1, top = 0.95, wspace = 0, hspace = 0)\nplt.savefig(""../figures/object_attack.pdf"", bbox_inches = ""tight"")\nplt.show()\n'"
src/visualize_perturbed_point_cloud.py,11,"b'import numpy as np\nfrom matplotlib import pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nnp.random.seed(1234)\n\nview_label = ""stool""\noffset_idx = 0\nf = np.load(""../output_save/final/1564208936_pointnet_iter_l2_adversarial_sticks_none.npz"")\nshape_names = [line.rstrip() for line in open(""../data/shape_names.txt"")]\nshow_grads = True\nshow_true = False\n\nx = f[""x""]\ny_pred = f[""y_pred""]\ny_pred_idx = np.argmax(y_pred, axis = 1)\nx_adv = f[""x_adv""]\ny_adv_pred = f[""y_adv_pred""]\ny_adv_pred_idx = np.argmax(y_adv_pred, axis = 1)\ngrad_adv = f[""grad_adv""]\n\nprint(""Shape:"", x.shape)\nprint(""Labels:"", [shape_names[idx] for idx in np.unique(y_pred_idx)])\nprint(""Successful attacks:"", np.count_nonzero(y_pred_idx != y_adv_pred_idx))\n\navg_zero_grad = np.sum(np.all(np.isclose(grad_adv, 0.0), axis = 2)) / float(len(x))\nprint(""Average number of points with zero gradients:"", avg_zero_grad)\n\nprint(""Selected label:"", view_label)\n\nmatch_idx = np.where(np.logical_and(y_pred_idx != y_adv_pred_idx, y_pred_idx == shape_names.index(view_label)))[0]\nx_view = x[match_idx[offset_idx]]\ny_pred_idx_view = y_pred_idx[match_idx[offset_idx]]\nx_adv_view = x_adv[match_idx[offset_idx]]\ny_adv_pred_idx_view = y_adv_pred_idx[match_idx[offset_idx]]\ngrad_adv_view = grad_adv[match_idx[offset_idx]]\n\nprint(""Attack result label:"", shape_names[y_adv_pred_idx_view])\nprint(""Clean prediction confidence:"", y_pred[match_idx[offset_idx]][y_pred_idx_view])\nprint(""Adversarial prediction confidence:"", y_adv_pred[match_idx[offset_idx]][y_adv_pred_idx_view])\nprint(""Shape index:"", match_idx[offset_idx])\n\ndef scale_plot():\n    scale = 0.7\n    plt.gca().auto_scale_xyz((-scale, scale), (-scale, scale), (-scale, scale))\n    plt.gca().view_init(-30, 200)\n    plt.axis(""off"")\n\nif show_true:\n    plt.figure(figsize = (30, 15))\n\n    plt.subplot(121, projection = ""3d"")\n\n    plt.gca().scatter(*x_view.T, zdir = ""y"", s = 50, c = x_view.T[1], cmap = ""winter"")\n\n    scale_plot()\n\n    plt.subplot(122, projection = ""3d"")\nelse:\n    plt.figure(figsize = (15, 15))\n    plt.subplot(111, projection = ""3d"")\n\nif show_grads:\n    grad_adv_view = np.linalg.norm(grad_adv_view, axis = 1)\n    close_to_zero = np.isclose(grad_adv_view, 0.0)\n    point_color = np.logical_not(close_to_zero).astype(float)\n\n    plt.gca().scatter(*x_adv_view.T, zdir = ""y"", s = 50, c = -point_color, cmap = ""winter"")\nelse:\n    plt.gca().scatter(*x_adv_view.T, zdir = ""y"", s = 50, c = x_adv_view.T[1], cmap = ""winter"")\n\nscale_plot()\n\nplt.subplots_adjust(left = 0, bottom = 0, right = 1, top = 1, wspace = 0, hspace = 0)\nplt.tight_layout()\nplt.savefig(""../figures/adv_sticks_grads.pdf"", bbox_inches = ""tight"")\nplt.show()\n'"
pointnet/models/pointnet_cls.py,1,"b'import tensorflow as tf\nimport numpy as np\nimport math\nimport sys\nimport os\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(BASE_DIR)\nsys.path.append(os.path.join(BASE_DIR, \'../utils\'))\nimport tf_util\nfrom transform_nets import input_transform_net, feature_transform_net\n\ndef placeholder_inputs(batch_size, num_point):\n    pointclouds_pl = tf.placeholder(tf.float32, shape=(batch_size, num_point, 3))\n    labels_pl = tf.placeholder(tf.int32, shape=(batch_size))\n    return pointclouds_pl, labels_pl\n\n\ndef get_model(point_cloud, is_training, bn_decay=None, num_classes = 40):\n    """""" Classification PointNet, input is BxNx3, output Bx40 """"""\n    batch_size = point_cloud.get_shape()[0].value\n    num_point = point_cloud.get_shape()[1].value\n    end_points = {}\n\n    with tf.variable_scope(\'transform_net1\') as sc:\n        transform = input_transform_net(point_cloud, is_training, bn_decay, K=3)\n    point_cloud_transformed = tf.matmul(point_cloud, transform)\n    input_image = tf.expand_dims(point_cloud_transformed, -1)\n\n    net = tf_util.conv2d(input_image, 64, [1,3],\n                         padding=\'VALID\', stride=[1,1],\n                         bn=True, is_training=is_training,\n                         scope=\'conv1\', bn_decay=bn_decay)\n    net = tf_util.conv2d(net, 64, [1,1],\n                         padding=\'VALID\', stride=[1,1],\n                         bn=True, is_training=is_training,\n                         scope=\'conv2\', bn_decay=bn_decay)\n\n    with tf.variable_scope(\'transform_net2\') as sc:\n        transform = feature_transform_net(net, is_training, bn_decay, K=64)\n    end_points[\'transform\'] = transform\n    net_transformed = tf.matmul(tf.squeeze(net, axis=[2]), transform)\n    net_transformed = tf.expand_dims(net_transformed, [2])\n\n    net = tf_util.conv2d(net_transformed, 64, [1,1],\n                         padding=\'VALID\', stride=[1,1],\n                         bn=True, is_training=is_training,\n                         scope=\'conv3\', bn_decay=bn_decay)\n    net = tf_util.conv2d(net, 128, [1,1],\n                         padding=\'VALID\', stride=[1,1],\n                         bn=True, is_training=is_training,\n                         scope=\'conv4\', bn_decay=bn_decay)\n    net = tf_util.conv2d(net, 1024, [1,1],\n                         padding=\'VALID\', stride=[1,1],\n                         bn=True, is_training=is_training,\n                         scope=\'conv5\', bn_decay=bn_decay)\n\n    # Symmetric function: max pooling\n    net = tf_util.max_pool2d(net, [num_point,1],\n                             padding=\'VALID\', scope=\'maxpool\')\n\n    net = tf.reshape(net, [batch_size, -1])\n    features = tf.identity(net, name = ""feature_vector"")\n    net = tf_util.fully_connected(net, 512, bn=True, is_training=is_training,\n                                  scope=\'fc1\', bn_decay=bn_decay)\n    net = tf_util.dropout(net, keep_prob=0.7, is_training=is_training,\n                          scope=\'dp1\')\n    net = tf_util.fully_connected(net, 256, bn=True, is_training=is_training,\n                                  scope=\'fc2\', bn_decay=bn_decay)\n    net = tf_util.dropout(net, keep_prob=0.7, is_training=is_training,\n                          scope=\'dp2\')\n    net = tf_util.fully_connected(net, num_classes, activation_fn=None, scope=\'fc3\')\n\n    return net, end_points\n\n\ndef get_loss(pred, label, end_points, reg_weight=0.001):\n    """""" pred: B*NUM_CLASSES,\n        label: B, """"""\n    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=pred, labels=label)\n    classify_loss = tf.reduce_mean(loss)\n    tf.summary.scalar(\'classify loss\', classify_loss)\n\n    # Enforce the transformation as orthogonal matrix\n    transform = end_points[\'transform\'] # BxKxK\n    K = transform.get_shape()[1].value\n    mat_diff = tf.matmul(transform, tf.transpose(transform, perm=[0,2,1]))\n    mat_diff -= tf.constant(np.eye(K), dtype=tf.float32)\n    mat_diff_loss = tf.nn.l2_loss(mat_diff) \n    tf.summary.scalar(\'mat loss\', mat_diff_loss)\n\n    return classify_loss + mat_diff_loss * reg_weight\n\n\nif __name__==\'__main__\':\n    with tf.Graph().as_default():\n        inputs = tf.zeros((32,1024,3))\n        outputs = get_model(inputs, tf.constant(True))\n        print(outputs)\n'"
pointnet/utils/pc_util.py,30,"b'"""""" Utility functions for processing point clouds.\n\nAuthor: Charles R. Qi, Hao Su\nDate: November 2016\n""""""\n\nimport os\nimport sys\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(BASE_DIR)\n\n# Draw point cloud\nfrom eulerangles import euler2mat\n\n# Point cloud IO\nimport numpy as np\nfrom plyfile import PlyData, PlyElement\n\n \n# ----------------------------------------\n# Point Cloud/Volume Conversions\n# ----------------------------------------\n\ndef point_cloud_to_volume_batch(point_clouds, vsize=12, radius=1.0, flatten=True):\n    """""" Input is BxNx3 batch of point cloud\n        Output is Bx(vsize^3)\n    """"""\n    vol_list = []\n    for b in range(point_clouds.shape[0]):\n        vol = point_cloud_to_volume(np.squeeze(point_clouds[b,:,:]), vsize, radius)\n        if flatten:\n            vol_list.append(vol.flatten())\n        else:\n            vol_list.append(np.expand_dims(np.expand_dims(vol, -1), 0))\n    if flatten:\n        return np.vstack(vol_list)\n    else:\n        return np.concatenate(vol_list, 0)\n\n\ndef point_cloud_to_volume(points, vsize, radius=1.0):\n    """""" input is Nx3 points.\n        output is vsize*vsize*vsize\n        assumes points are in range [-radius, radius]\n    """"""\n    vol = np.zeros((vsize,vsize,vsize))\n    voxel = 2*radius/float(vsize)\n    locations = (points + radius)/voxel\n    locations = locations.astype(int)\n    vol[locations[:,0],locations[:,1],locations[:,2]] = 1.0\n    return vol\n\n#a = np.zeros((16,1024,3))\n#print point_cloud_to_volume_batch(a, 12, 1.0, False).shape\n\ndef volume_to_point_cloud(vol):\n    """""" vol is occupancy grid (value = 0 or 1) of size vsize*vsize*vsize\n        return Nx3 numpy array.\n    """"""\n    vsize = vol.shape[0]\n    assert(vol.shape[1] == vsize and vol.shape[1] == vsize)\n    points = []\n    for a in range(vsize):\n        for b in range(vsize):\n            for c in range(vsize):\n                if vol[a,b,c] == 1:\n                    points.append(np.array([a,b,c]))\n    if len(points) == 0:\n        return np.zeros((0,3))\n    points = np.vstack(points)\n    return points\n\n# ----------------------------------------\n# Point cloud IO\n# ----------------------------------------\n\ndef read_ply(filename):\n    """""" read XYZ point cloud from filename PLY file """"""\n    plydata = PlyData.read(filename)\n    pc = plydata[\'vertex\'].data\n    pc_array = np.array([[x, y, z] for x,y,z in pc])\n    return pc_array\n\n\ndef write_ply(points, filename, text=True):\n    """""" input: Nx3, write points to filename as PLY format. """"""\n    points = [(points[i,0], points[i,1], points[i,2]) for i in range(points.shape[0])]\n    vertex = np.array(points, dtype=[(\'x\', \'f4\'), (\'y\', \'f4\'),(\'z\', \'f4\')])\n    el = PlyElement.describe(vertex, \'vertex\', comments=[\'vertices\'])\n    PlyData([el], text=text).write(filename)\n\n\n# ----------------------------------------\n# Simple Point cloud and Volume Renderers\n# ----------------------------------------\n\ndef draw_point_cloud(input_points, canvasSize=500, space=200, diameter=25,\n                     xrot=0, yrot=0, zrot=0, switch_xyz=[0,1,2], normalize=False):\n    """""" Render point cloud to image with alpha channel.\n        Input:\n            points: Nx3 numpy array (+y is up direction)\n        Output:\n            gray image as numpy array of size canvasSizexcanvasSize\n    """"""\n    image = np.zeros((canvasSize, canvasSize))\n    if input_points is None or input_points.shape[0] == 0:\n        return image\n\n    points = input_points[:, switch_xyz]\n    M = euler2mat(zrot, yrot, xrot)\n    points = (np.dot(M, points.transpose())).transpose()\n\n    # Normalize the point cloud\n    # We normalize scale to fit points in a unit sphere\n    if normalize:\n        centroid = np.mean(points, axis=0)\n        points -= centroid\n        furthest_distance = np.max(np.sqrt(np.sum(abs(points)**2,axis=-1)))\n        points /= furthest_distance\n    \n    dist = np.linalg.norm(points, axis = -1)\n    points = points[dist <= 1]\n    if len(points) == 0:\n        return image\n\n    # Pre-compute the Gaussian disk\n    radius = (diameter-1)/2.0\n    disk = np.zeros((diameter, diameter))\n    for i in range(diameter):\n        for j in range(diameter):\n            if (i - radius) * (i-radius) + (j-radius) * (j-radius) <= radius * radius:\n                disk[i, j] = np.exp((-(i-radius)**2 - (j-radius)**2)/(radius**2))\n    mask = np.argwhere(disk > 0)\n    dx = mask[:, 0]\n    dy = mask[:, 1]\n    dv = disk[disk > 0]\n    \n    # Order points by z-buffer\n    zorder = np.argsort(points[:, 2])\n    points = points[zorder, :]\n    points[:, 2] = (points[:, 2] - np.min(points[:, 2])) / (np.max(points[:, 2] - np.min(points[:, 2])))\n    max_depth = np.max(points[:, 2])\n       \n    for i in range(points.shape[0]):\n        j = points.shape[0] - i - 1\n        x = points[j, 0]\n        y = points[j, 1]\n        xc = canvasSize/2 + (x*space)\n        yc = canvasSize/2 + (y*space)\n        xc = int(np.round(xc))\n        yc = int(np.round(yc))\n        \n        px = dx + xc\n        py = dy + yc\n        \n        image[px, py] = image[px, py] * 0.7 + dv * (max_depth - points[j, 2]) * 0.3\n    \n    image = image / np.max(image)\n    return image\n\ndef point_cloud_three_views(points):\n    """""" input points Nx3 numpy array (+y is up direction).\n        return an numpy array gray image of size 500x1500. """""" \n    # +y is up direction\n    # xrot is azimuth\n    # yrot is in-plane\n    # zrot is elevation\n    img1 = draw_point_cloud(points, zrot=110/180.0*np.pi, xrot=45/180.0*np.pi, yrot=0/180.0*np.pi)\n    img2 = draw_point_cloud(points, zrot=70/180.0*np.pi, xrot=135/180.0*np.pi, yrot=0/180.0*np.pi)\n    img3 = draw_point_cloud(points, zrot=180.0/180.0*np.pi, xrot=90/180.0*np.pi, yrot=0/180.0*np.pi)\n    image_large = np.concatenate([img1, img2, img3], 1)\n    return image_large\n\n\nfrom PIL import Image\ndef point_cloud_three_views_demo():\n    """""" Demo for draw_point_cloud function """"""\n    points = read_ply(\'../third_party/mesh_sampling/piano.ply\')\n    im_array = point_cloud_three_views(points)\n    img = Image.fromarray(np.uint8(im_array*255.0))\n    img.save(\'piano.jpg\')\n\nif __name__==""__main__"":\n    point_cloud_three_views_demo()\n\n\nimport matplotlib.pyplot as plt\ndef pyplot_draw_point_cloud(points, output_filename):\n    """""" points is a Nx3 numpy array """"""\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection=\'3d\')\n    ax.scatter(points[:,0], points[:,1], points[:,2])\n    ax.set_xlabel(\'x\')\n    ax.set_ylabel(\'y\')\n    ax.set_zlabel(\'z\')\n    #savefig(output_filename)\n\ndef pyplot_draw_volume(vol, output_filename):\n    """""" vol is of size vsize*vsize*vsize\n        output an image to output_filename\n    """"""\n    points = volume_to_point_cloud(vol)\n    pyplot_draw_point_cloud(points, output_filename)\n'"
pointnet/utils/tf_util.py,0,"b'"""""" Wrapper functions for TensorFlow layers.\n\nAuthor: Charles R. Qi\nDate: November 2016\n""""""\n\nimport numpy as np\nimport tensorflow as tf\n\ndef _variable_on_cpu(name, shape, initializer, use_fp16=False):\n  """"""Helper to create a Variable stored on CPU memory.\n  Args:\n    name: name of the variable\n    shape: list of ints\n    initializer: initializer for Variable\n  Returns:\n    Variable Tensor\n  """"""\n  with tf.device(\'/cpu:0\'):\n    dtype = tf.float16 if use_fp16 else tf.float32\n    var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype)\n  return var\n\ndef _variable_with_weight_decay(name, shape, stddev, wd, use_xavier=True):\n  """"""Helper to create an initialized Variable with weight decay.\n\n  Note that the Variable is initialized with a truncated normal distribution.\n  A weight decay is added only if one is specified.\n\n  Args:\n    name: name of the variable\n    shape: list of ints\n    stddev: standard deviation of a truncated Gaussian\n    wd: add L2Loss weight decay multiplied by this float. If None, weight\n        decay is not added for this Variable.\n    use_xavier: bool, whether to use xavier initializer\n\n  Returns:\n    Variable Tensor\n  """"""\n  if use_xavier:\n    initializer = tf.contrib.layers.xavier_initializer()\n  else:\n    initializer = tf.truncated_normal_initializer(stddev=stddev)\n  var = _variable_on_cpu(name, shape, initializer)\n  if wd is not None:\n    weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name=\'weight_loss\')\n    tf.add_to_collection(\'losses\', weight_decay)\n  return var\n\n\ndef conv1d(inputs,\n           num_output_channels,\n           kernel_size,\n           scope,\n           stride=1,\n           padding=\'SAME\',\n           use_xavier=True,\n           stddev=1e-3,\n           weight_decay=0.0,\n           activation_fn=tf.nn.relu,\n           bn=False,\n           bn_decay=None,\n           is_training=None):\n  """""" 1D convolution with non-linear operation.\n\n  Args:\n    inputs: 3-D tensor variable BxLxC\n    num_output_channels: int\n    kernel_size: int\n    scope: string\n    stride: int\n    padding: \'SAME\' or \'VALID\'\n    use_xavier: bool, use xavier_initializer if true\n    stddev: float, stddev for truncated_normal init\n    weight_decay: float\n    activation_fn: function\n    bn: bool, whether to use batch norm\n    bn_decay: float or float tensor variable in [0,1]\n    is_training: bool Tensor variable\n\n  Returns:\n    Variable tensor\n  """"""\n  with tf.variable_scope(scope) as sc:\n    num_in_channels = inputs.get_shape()[-1].value\n    kernel_shape = [kernel_size,\n                    num_in_channels, num_output_channels]\n    kernel = _variable_with_weight_decay(\'weights\',\n                                         shape=kernel_shape,\n                                         use_xavier=use_xavier,\n                                         stddev=stddev,\n                                         wd=weight_decay)\n    outputs = tf.nn.conv1d(inputs, kernel,\n                           stride=stride,\n                           padding=padding)\n    biases = _variable_on_cpu(\'biases\', [num_output_channels],\n                              tf.constant_initializer(0.0))\n    outputs = tf.nn.bias_add(outputs, biases)\n\n    if bn:\n      outputs = batch_norm_for_conv1d(outputs, is_training,\n                                      bn_decay=bn_decay, scope=\'bn\')\n\n    if activation_fn is not None:\n      outputs = activation_fn(outputs)\n    return outputs\n\n\n\n\ndef conv2d(inputs,\n           num_output_channels,\n           kernel_size,\n           scope,\n           stride=[1, 1],\n           padding=\'SAME\',\n           use_xavier=True,\n           stddev=1e-3,\n           weight_decay=0.0,\n           activation_fn=tf.nn.relu,\n           bn=False,\n           bn_decay=None,\n           is_training=None):\n  """""" 2D convolution with non-linear operation.\n\n  Args:\n    inputs: 4-D tensor variable BxHxWxC\n    num_output_channels: int\n    kernel_size: a list of 2 ints\n    scope: string\n    stride: a list of 2 ints\n    padding: \'SAME\' or \'VALID\'\n    use_xavier: bool, use xavier_initializer if true\n    stddev: float, stddev for truncated_normal init\n    weight_decay: float\n    activation_fn: function\n    bn: bool, whether to use batch norm\n    bn_decay: float or float tensor variable in [0,1]\n    is_training: bool Tensor variable\n\n  Returns:\n    Variable tensor\n  """"""\n  with tf.variable_scope(scope) as sc:\n      kernel_h, kernel_w = kernel_size\n      num_in_channels = inputs.get_shape()[-1].value\n      kernel_shape = [kernel_h, kernel_w,\n                      num_in_channels, num_output_channels]\n      kernel = _variable_with_weight_decay(\'weights\',\n                                           shape=kernel_shape,\n                                           use_xavier=use_xavier,\n                                           stddev=stddev,\n                                           wd=weight_decay)\n      stride_h, stride_w = stride\n      outputs = tf.nn.conv2d(inputs, kernel,\n                             [1, stride_h, stride_w, 1],\n                             padding=padding)\n      biases = _variable_on_cpu(\'biases\', [num_output_channels],\n                                tf.constant_initializer(0.0))\n      outputs = tf.nn.bias_add(outputs, biases)\n\n      if bn:\n        outputs = batch_norm_for_conv2d(outputs, is_training,\n                                        bn_decay=bn_decay, scope=\'bn\')\n        #outputs = tf.contrib.layers.batch_norm(outputs, is_training = is_training, decay = (bn_decay if bn_decay is not None else 0.9), scope = ""bn"")\n\n      if activation_fn is not None:\n        outputs = activation_fn(outputs)\n      return outputs\n\n\ndef conv2d_transpose(inputs,\n                     num_output_channels,\n                     kernel_size,\n                     scope,\n                     stride=[1, 1],\n                     padding=\'SAME\',\n                     use_xavier=True,\n                     stddev=1e-3,\n                     weight_decay=0.0,\n                     activation_fn=tf.nn.relu,\n                     bn=False,\n                     bn_decay=None,\n                     is_training=None):\n  """""" 2D convolution transpose with non-linear operation.\n\n  Args:\n    inputs: 4-D tensor variable BxHxWxC\n    num_output_channels: int\n    kernel_size: a list of 2 ints\n    scope: string\n    stride: a list of 2 ints\n    padding: \'SAME\' or \'VALID\'\n    use_xavier: bool, use xavier_initializer if true\n    stddev: float, stddev for truncated_normal init\n    weight_decay: float\n    activation_fn: function\n    bn: bool, whether to use batch norm\n    bn_decay: float or float tensor variable in [0,1]\n    is_training: bool Tensor variable\n\n  Returns:\n    Variable tensor\n\n  Note: conv2d(conv2d_transpose(a, num_out, ksize, stride), a.shape[-1], ksize, stride) == a\n  """"""\n  with tf.variable_scope(scope) as sc:\n      kernel_h, kernel_w = kernel_size\n      num_in_channels = inputs.get_shape()[-1].value\n      kernel_shape = [kernel_h, kernel_w,\n                      num_output_channels, num_in_channels] # reversed to conv2d\n      kernel = _variable_with_weight_decay(\'weights\',\n                                           shape=kernel_shape,\n                                           use_xavier=use_xavier,\n                                           stddev=stddev,\n                                           wd=weight_decay)\n      stride_h, stride_w = stride\n      \n      # from slim.convolution2d_transpose\n      def get_deconv_dim(dim_size, stride_size, kernel_size, padding):\n          dim_size *= stride_size\n\n          if padding == \'VALID\' and dim_size is not None:\n            dim_size += max(kernel_size - stride_size, 0)\n          return dim_size\n\n      # caculate output shape\n      batch_size = inputs.get_shape()[0].value\n      height = inputs.get_shape()[1].value\n      width = inputs.get_shape()[2].value\n      out_height = get_deconv_dim(height, stride_h, kernel_h, padding)\n      out_width = get_deconv_dim(width, stride_w, kernel_w, padding)\n      output_shape = [batch_size, out_height, out_width, num_output_channels]\n\n      outputs = tf.nn.conv2d_transpose(inputs, kernel, output_shape,\n                             [1, stride_h, stride_w, 1],\n                             padding=padding)\n      biases = _variable_on_cpu(\'biases\', [num_output_channels],\n                                tf.constant_initializer(0.0))\n      outputs = tf.nn.bias_add(outputs, biases)\n\n      if bn:\n        outputs = batch_norm_for_conv2d(outputs, is_training,\n                                        bn_decay=bn_decay, scope=\'bn\')\n\n      if activation_fn is not None:\n        outputs = activation_fn(outputs)\n      return outputs\n\n   \n\ndef conv3d(inputs,\n           num_output_channels,\n           kernel_size,\n           scope,\n           stride=[1, 1, 1],\n           padding=\'SAME\',\n           use_xavier=True,\n           stddev=1e-3,\n           weight_decay=0.0,\n           activation_fn=tf.nn.relu,\n           bn=False,\n           bn_decay=None,\n           is_training=None):\n  """""" 3D convolution with non-linear operation.\n\n  Args:\n    inputs: 5-D tensor variable BxDxHxWxC\n    num_output_channels: int\n    kernel_size: a list of 3 ints\n    scope: string\n    stride: a list of 3 ints\n    padding: \'SAME\' or \'VALID\'\n    use_xavier: bool, use xavier_initializer if true\n    stddev: float, stddev for truncated_normal init\n    weight_decay: float\n    activation_fn: function\n    bn: bool, whether to use batch norm\n    bn_decay: float or float tensor variable in [0,1]\n    is_training: bool Tensor variable\n\n  Returns:\n    Variable tensor\n  """"""\n  with tf.variable_scope(scope) as sc:\n    kernel_d, kernel_h, kernel_w = kernel_size\n    num_in_channels = inputs.get_shape()[-1].value\n    kernel_shape = [kernel_d, kernel_h, kernel_w,\n                    num_in_channels, num_output_channels]\n    kernel = _variable_with_weight_decay(\'weights\',\n                                         shape=kernel_shape,\n                                         use_xavier=use_xavier,\n                                         stddev=stddev,\n                                         wd=weight_decay)\n    stride_d, stride_h, stride_w = stride\n    outputs = tf.nn.conv3d(inputs, kernel,\n                           [1, stride_d, stride_h, stride_w, 1],\n                           padding=padding)\n    biases = _variable_on_cpu(\'biases\', [num_output_channels],\n                              tf.constant_initializer(0.0))\n    outputs = tf.nn.bias_add(outputs, biases)\n    \n    if bn:\n      outputs = batch_norm_for_conv3d(outputs, is_training,\n                                      bn_decay=bn_decay, scope=\'bn\')\n\n    if activation_fn is not None:\n      outputs = activation_fn(outputs)\n    return outputs\n\ndef fully_connected(inputs,\n                    num_outputs,\n                    scope,\n                    use_xavier=True,\n                    stddev=1e-3,\n                    weight_decay=0.0,\n                    activation_fn=tf.nn.relu,\n                    bn=False,\n                    bn_decay=None,\n                    is_training=None):\n  """""" Fully connected layer with non-linear operation.\n  \n  Args:\n    inputs: 2-D tensor BxN\n    num_outputs: int\n  \n  Returns:\n    Variable tensor of size B x num_outputs.\n  """"""\n  with tf.variable_scope(scope) as sc:\n    num_input_units = inputs.get_shape()[-1].value\n    weights = _variable_with_weight_decay(\'weights\',\n                                          shape=[num_input_units, num_outputs],\n                                          use_xavier=use_xavier,\n                                          stddev=stddev,\n                                          wd=weight_decay)\n    outputs = tf.matmul(inputs, weights)\n    biases = _variable_on_cpu(\'biases\', [num_outputs],\n                             tf.constant_initializer(0.0))\n    outputs = tf.nn.bias_add(outputs, biases)\n     \n    if bn:\n      outputs = batch_norm_for_fc(outputs, is_training, bn_decay, \'bn\')\n      #outputs = tf.contrib.layers.batch_norm(outputs, is_training = is_training, decay = (bn_decay if bn_decay is not None else 0.9), scope = ""bn"")\n\n    if activation_fn is not None:\n      outputs = activation_fn(outputs)\n    return outputs\n\n\ndef max_pool2d(inputs,\n               kernel_size,\n               scope,\n               stride=[2, 2],\n               padding=\'VALID\'):\n  """""" 2D max pooling.\n\n  Args:\n    inputs: 4-D tensor BxHxWxC\n    kernel_size: a list of 2 ints\n    stride: a list of 2 ints\n  \n  Returns:\n    Variable tensor\n  """"""\n  with tf.variable_scope(scope) as sc:\n    kernel_h, kernel_w = kernel_size\n    stride_h, stride_w = stride\n    outputs = tf.nn.max_pool(inputs,\n                             ksize=[1, kernel_h, kernel_w, 1],\n                             strides=[1, stride_h, stride_w, 1],\n                             padding=padding,\n                             name=sc.name)\n    return outputs\n\ndef avg_pool2d(inputs,\n               kernel_size,\n               scope,\n               stride=[2, 2],\n               padding=\'VALID\'):\n  """""" 2D avg pooling.\n\n  Args:\n    inputs: 4-D tensor BxHxWxC\n    kernel_size: a list of 2 ints\n    stride: a list of 2 ints\n  \n  Returns:\n    Variable tensor\n  """"""\n  with tf.variable_scope(scope) as sc:\n    kernel_h, kernel_w = kernel_size\n    stride_h, stride_w = stride\n    outputs = tf.nn.avg_pool(inputs,\n                             ksize=[1, kernel_h, kernel_w, 1],\n                             strides=[1, stride_h, stride_w, 1],\n                             padding=padding,\n                             name=sc.name)\n    return outputs\n\n\ndef max_pool3d(inputs,\n               kernel_size,\n               scope,\n               stride=[2, 2, 2],\n               padding=\'VALID\'):\n  """""" 3D max pooling.\n\n  Args:\n    inputs: 5-D tensor BxDxHxWxC\n    kernel_size: a list of 3 ints\n    stride: a list of 3 ints\n  \n  Returns:\n    Variable tensor\n  """"""\n  with tf.variable_scope(scope) as sc:\n    kernel_d, kernel_h, kernel_w = kernel_size\n    stride_d, stride_h, stride_w = stride\n    outputs = tf.nn.max_pool3d(inputs,\n                               ksize=[1, kernel_d, kernel_h, kernel_w, 1],\n                               strides=[1, stride_d, stride_h, stride_w, 1],\n                               padding=padding,\n                               name=sc.name)\n    return outputs\n\ndef avg_pool3d(inputs,\n               kernel_size,\n               scope,\n               stride=[2, 2, 2],\n               padding=\'VALID\'):\n  """""" 3D avg pooling.\n\n  Args:\n    inputs: 5-D tensor BxDxHxWxC\n    kernel_size: a list of 3 ints\n    stride: a list of 3 ints\n  \n  Returns:\n    Variable tensor\n  """"""\n  with tf.variable_scope(scope) as sc:\n    kernel_d, kernel_h, kernel_w = kernel_size\n    stride_d, stride_h, stride_w = stride\n    outputs = tf.nn.avg_pool3d(inputs,\n                               ksize=[1, kernel_d, kernel_h, kernel_w, 1],\n                               strides=[1, stride_d, stride_h, stride_w, 1],\n                               padding=padding,\n                               name=sc.name)\n    return outputs\n\n\n\n\n\ndef batch_norm_template(inputs, is_training, scope, moments_dims, bn_decay):\n  """""" Batch normalization on convolutional maps and beyond...\n  Ref.: http://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow\n  \n  Args:\n      inputs:        Tensor, k-D input ... x C could be BC or BHWC or BDHWC\n      is_training:   boolean tf.Varialbe, true indicates training phase\n      scope:         string, variable scope\n      moments_dims:  a list of ints, indicating dimensions for moments calculation\n      bn_decay:      float or float tensor variable, controling moving average weight\n  Return:\n      normed:        batch-normalized maps\n  """"""\n  decay = bn_decay if bn_decay is not None else 0.9\n  return tf.contrib.layers.batch_norm(inputs, center = True, scale = True, decay = decay, is_training = is_training, scope = scope, updates_collections = None)\n  # with tf.variable_scope(scope) as sc:\n  #   num_channels = inputs.get_shape()[-1].value\n  #   # beta = tf.Variable(tf.constant(0.0, shape=[num_channels]),\n  #   #                    name=\'beta\', trainable=True)\n  #   # gamma = tf.Variable(tf.constant(1.0, shape=[num_channels]),\n  #   #                     name=\'gamma\', trainable=True)\n  #   beta = tf.get_variable(""beta"", shape = [num_channels], trainable = True, initializer = tf.zeros_initializer)\n  #   gamma = tf.get_variable(""gamma"", shape = [num_channels], trainable = True, initializer = tf.ones_initializer)\n  #   batch_mean, batch_var = tf.nn.moments(inputs, moments_dims, name=\'moments\')\n  #   decay = bn_decay if bn_decay is not None else 0.9\n  #   ema = tf.train.ExponentialMovingAverage(decay=decay)\n  #   # Operator that maintains moving averages of variables.\n  #   def apply_fn():\n  #     return ema.apply([batch_mean, batch_var])\n  #   ema_apply_op = tf.cond(is_training,\n  #                         apply_fn,\n  #                         lambda: tf.no_op())\n  \n  #   # Update moving average and return current batch\'s avg and var.\n  #   def mean_var_with_update():\n  #     with tf.control_dependencies([ema_apply_op]):\n  #       return tf.identity(batch_mean), tf.identity(batch_var)\n    \n  #   def not_training_update():\n  #     return ema.average(batch_mean), ema.average(batch_var)\n\n  #   # ema.average returns the Variable holding the average of var.\n  #   mean, var = tf.cond(is_training,\n  #                       mean_var_with_update,\n  #                       not_training_update)\n  #   normed = tf.nn.batch_normalization(inputs, mean, var, beta, gamma, 1e-3)\n  # return normed\n\n\ndef batch_norm_for_fc(inputs, is_training, bn_decay, scope):\n  """""" Batch normalization on FC data.\n  \n  Args:\n      inputs:      Tensor, 2D BxC input\n      is_training: boolean tf.Varialbe, true indicates training phase\n      bn_decay:    float or float tensor variable, controling moving average weight\n      scope:       string, variable scope\n  Return:\n      normed:      batch-normalized maps\n  """"""\n  return batch_norm_template(inputs, is_training, scope, [0,], bn_decay)\n\n\ndef batch_norm_for_conv1d(inputs, is_training, bn_decay, scope):\n  """""" Batch normalization on 1D convolutional maps.\n  \n  Args:\n      inputs:      Tensor, 3D BLC input maps\n      is_training: boolean tf.Varialbe, true indicates training phase\n      bn_decay:    float or float tensor variable, controling moving average weight\n      scope:       string, variable scope\n  Return:\n      normed:      batch-normalized maps\n  """"""\n  return batch_norm_template(inputs, is_training, scope, [0,1], bn_decay)\n\n\n\n  \ndef batch_norm_for_conv2d(inputs, is_training, bn_decay, scope):\n  """""" Batch normalization on 2D convolutional maps.\n  \n  Args:\n      inputs:      Tensor, 4D BHWC input maps\n      is_training: boolean tf.Varialbe, true indicates training phase\n      bn_decay:    float or float tensor variable, controling moving average weight\n      scope:       string, variable scope\n  Return:\n      normed:      batch-normalized maps\n  """"""\n  return batch_norm_template(inputs, is_training, scope, [0,1,2], bn_decay)\n\n\n\ndef batch_norm_for_conv3d(inputs, is_training, bn_decay, scope):\n  """""" Batch normalization on 3D convolutional maps.\n  \n  Args:\n      inputs:      Tensor, 5D BDHWC input maps\n      is_training: boolean tf.Varialbe, true indicates training phase\n      bn_decay:    float or float tensor variable, controling moving average weight\n      scope:       string, variable scope\n  Return:\n      normed:      batch-normalized maps\n  """"""\n  return batch_norm_template(inputs, is_training, scope, [0,1,2,3], bn_decay)\n\n\ndef dropout(inputs,\n            is_training,\n            scope,\n            keep_prob=0.5,\n            noise_shape=None):\n  """""" Dropout layer.\n\n  Args:\n    inputs: tensor\n    is_training: boolean tf.Variable\n    scope: string\n    keep_prob: float in [0,1]\n    noise_shape: list of ints\n\n  Returns:\n    tensor variable\n  """"""\n  with tf.variable_scope(scope) as sc:\n    outputs = tf.cond(is_training,\n                      lambda: tf.nn.dropout(inputs, keep_prob, noise_shape),\n                      lambda: inputs)\n    return outputs\n'"
pointnet2/models/pointnet2_cls_ssg.py,0,"b'""""""\n    PointNet++ Model for point clouds classification\n""""""\n\nimport os\nimport sys\nBASE_DIR = os.path.dirname(__file__)\nsys.path.append(BASE_DIR)\nsys.path.append(os.path.join(BASE_DIR, \'../utils\'))\nimport tensorflow as tf\nimport numpy as np\nimport tf_util\nfrom pointnet_util import pointnet_sa_module\n\ndef placeholder_inputs(batch_size, num_point):\n    pointclouds_pl = tf.placeholder(tf.float32, shape=(batch_size, num_point, 3))\n    labels_pl = tf.placeholder(tf.int32, shape=(batch_size))\n    return pointclouds_pl, labels_pl\n\ndef get_model(point_cloud, is_training, bn_decay=None, num_classes = 40):\n    """""" Classification PointNet, input is BxNx3, output Bx40 """"""\n    batch_size = point_cloud.get_shape()[0].value\n    num_point = point_cloud.get_shape()[1].value\n    end_points = {}\n    l0_xyz = point_cloud\n    l0_points = None\n    end_points[\'l0_xyz\'] = l0_xyz\n\n    # Set abstraction layers\n    # Note: When using NCHW for layer 2, we see increased GPU memory usage (in TF1.4).\n    # So we only use NCHW for layer 1 until this issue can be resolved.\n    l1_xyz, l1_points, l1_indices = pointnet_sa_module(l0_xyz, l0_points, npoint=512, radius=0.2, nsample=32, mlp=[64,64,128], mlp2=None, group_all=False, is_training=is_training, bn_decay=bn_decay, scope=\'layer1\', use_nchw=True)\n    l2_xyz, l2_points, l2_indices = pointnet_sa_module(l1_xyz, l1_points, npoint=128, radius=0.4, nsample=64, mlp=[128,128,256], mlp2=None, group_all=False, is_training=is_training, bn_decay=bn_decay, scope=\'layer2\')\n    l3_xyz, l3_points, l3_indices = pointnet_sa_module(l2_xyz, l2_points, npoint=None, radius=None, nsample=None, mlp=[256,512,1024], mlp2=None, group_all=True, is_training=is_training, bn_decay=bn_decay, scope=\'layer3\')\n\n    # Fully connected layers\n    net = tf.reshape(l3_points, [batch_size, -1])\n    features = tf.identity(net, name = ""feature_vector"")\n    net = tf_util.fully_connected(net, 512, bn=True, is_training=is_training, scope=\'fc1\', bn_decay=bn_decay)\n    net = tf_util.dropout(net, keep_prob=0.5, is_training=is_training, scope=\'dp1\')\n    net = tf_util.fully_connected(net, 256, bn=True, is_training=is_training, scope=\'fc2\', bn_decay=bn_decay)\n    net = tf_util.dropout(net, keep_prob=0.5, is_training=is_training, scope=\'dp2\')\n    net = tf_util.fully_connected(net, num_classes, activation_fn=None, scope=\'fc3\')\n\n    return net, end_points\n\n\ndef get_loss(pred, label, end_points):\n    """""" pred: B*NUM_CLASSES,\n        label: B, """"""\n    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=pred, labels=label)\n    classify_loss = tf.reduce_mean(loss)\n    tf.summary.scalar(\'classify loss\', classify_loss)\n    tf.add_to_collection(\'losses\', classify_loss)\n    return classify_loss\n\n\nif __name__==\'__main__\':\n    with tf.Graph().as_default():\n        inputs = tf.zeros((32,1024,3))\n        output, _ = get_model(inputs, tf.constant(True))\n        print(output)\n'"
pointnet2/utils/eulerangles.py,26,"b'# emacs: -*- mode: python-mode; py-indent-offset: 4; indent-tabs-mode: nil -*-\n# vi: set ft=python sts=4 ts=4 sw=4 et:\n### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##\n#\n#   See COPYING file distributed along with the NiBabel package for the\n#   copyright and license terms.\n#\n### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ### ##\n\'\'\' Module implementing Euler angle rotations and their conversions\n\nSee:\n\n* http://en.wikipedia.org/wiki/Rotation_matrix\n* http://en.wikipedia.org/wiki/Euler_angles\n* http://mathworld.wolfram.com/EulerAngles.html\n\nSee also: *Representing Attitude with Euler Angles and Quaternions: A\nReference* (2006) by James Diebel. A cached PDF link last found here:\n\nhttp://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.110.5134\n\nEuler\'s rotation theorem tells us that any rotation in 3D can be\ndescribed by 3 angles.  Let\'s call the 3 angles the *Euler angle vector*\nand call the angles in the vector :math:`alpha`, :math:`beta` and\n:math:`gamma`.  The vector is [ :math:`alpha`,\n:math:`beta`. :math:`gamma` ] and, in this description, the order of the\nparameters specifies the order in which the rotations occur (so the\nrotation corresponding to :math:`alpha` is applied first).\n\nIn order to specify the meaning of an *Euler angle vector* we need to\nspecify the axes around which each of the rotations corresponding to\n:math:`alpha`, :math:`beta` and :math:`gamma` will occur.\n\nThere are therefore three axes for the rotations :math:`alpha`,\n:math:`beta` and :math:`gamma`; let\'s call them :math:`i` :math:`j`,\n:math:`k`.\n\nLet us express the rotation :math:`alpha` around axis `i` as a 3 by 3\nrotation matrix `A`.  Similarly :math:`beta` around `j` becomes 3 x 3\nmatrix `B` and :math:`gamma` around `k` becomes matrix `G`.  Then the\nwhole rotation expressed by the Euler angle vector [ :math:`alpha`,\n:math:`beta`. :math:`gamma` ], `R` is given by::\n\n   R = np.dot(G, np.dot(B, A))\n\nSee http://mathworld.wolfram.com/EulerAngles.html\n\nThe order :math:`G B A` expresses the fact that the rotations are\nperformed in the order of the vector (:math:`alpha` around axis `i` =\n`A` first).\n\nTo convert a given Euler angle vector to a meaningful rotation, and a\nrotation matrix, we need to define:\n\n* the axes `i`, `j`, `k`\n* whether a rotation matrix should be applied on the left of a vector to\n  be transformed (vectors are column vectors) or on the right (vectors\n  are row vectors).\n* whether the rotations move the axes as they are applied (intrinsic\n  rotations) - compared the situation where the axes stay fixed and the\n  vectors move within the axis frame (extrinsic)\n* the handedness of the coordinate system\n\nSee: http://en.wikipedia.org/wiki/Rotation_matrix#Ambiguities\n\nWe are using the following conventions:\n\n* axes `i`, `j`, `k` are the `z`, `y`, and `x` axes respectively.  Thus\n  an Euler angle vector [ :math:`alpha`, :math:`beta`. :math:`gamma` ]\n  in our convention implies a :math:`alpha` radian rotation around the\n  `z` axis, followed by a :math:`beta` rotation around the `y` axis,\n  followed by a :math:`gamma` rotation around the `x` axis.\n* the rotation matrix applies on the left, to column vectors on the\n  right, so if `R` is the rotation matrix, and `v` is a 3 x N matrix\n  with N column vectors, the transformed vector set `vdash` is given by\n  ``vdash = np.dot(R, v)``.\n* extrinsic rotations - the axes are fixed, and do not move with the\n  rotations.\n* a right-handed coordinate system\n\nThe convention of rotation around ``z``, followed by rotation around\n``y``, followed by rotation around ``x``, is known (confusingly) as\n""xyz"", pitch-roll-yaw, Cardan angles, or Tait-Bryan angles.\n\'\'\'\n\nimport math\n\nimport sys\nif sys.version_info >= (3,0):\n    from functools import reduce\n\nimport numpy as np\n\n\n_FLOAT_EPS_4 = np.finfo(float).eps * 4.0\n\n\ndef euler2mat(z=0, y=0, x=0):\n    \'\'\' Return matrix for rotations around z, y and x axes\n\n    Uses the z, then y, then x convention above\n\n    Parameters\n    ----------\n    z : scalar\n       Rotation angle in radians around z-axis (performed first)\n    y : scalar\n       Rotation angle in radians around y-axis\n    x : scalar\n       Rotation angle in radians around x-axis (performed last)\n\n    Returns\n    -------\n    M : array shape (3,3)\n       Rotation matrix giving same rotation as for given angles\n\n    Examples\n    --------\n    >>> zrot = 1.3 # radians\n    >>> yrot = -0.1\n    >>> xrot = 0.2\n    >>> M = euler2mat(zrot, yrot, xrot)\n    >>> M.shape == (3, 3)\n    True\n\n    The output rotation matrix is equal to the composition of the\n    individual rotations\n\n    >>> M1 = euler2mat(zrot)\n    >>> M2 = euler2mat(0, yrot)\n    >>> M3 = euler2mat(0, 0, xrot)\n    >>> composed_M = np.dot(M3, np.dot(M2, M1))\n    >>> np.allclose(M, composed_M)\n    True\n\n    You can specify rotations by named arguments\n\n    >>> np.all(M3 == euler2mat(x=xrot))\n    True\n\n    When applying M to a vector, the vector should column vector to the\n    right of M.  If the right hand side is a 2D array rather than a\n    vector, then each column of the 2D array represents a vector.\n\n    >>> vec = np.array([1, 0, 0]).reshape((3,1))\n    >>> v2 = np.dot(M, vec)\n    >>> vecs = np.array([[1, 0, 0],[0, 1, 0]]).T # giving 3x2 array\n    >>> vecs2 = np.dot(M, vecs)\n\n    Rotations are counter-clockwise.\n\n    >>> zred = np.dot(euler2mat(z=np.pi/2), np.eye(3))\n    >>> np.allclose(zred, [[0, -1, 0],[1, 0, 0], [0, 0, 1]])\n    True\n    >>> yred = np.dot(euler2mat(y=np.pi/2), np.eye(3))\n    >>> np.allclose(yred, [[0, 0, 1],[0, 1, 0], [-1, 0, 0]])\n    True\n    >>> xred = np.dot(euler2mat(x=np.pi/2), np.eye(3))\n    >>> np.allclose(xred, [[1, 0, 0],[0, 0, -1], [0, 1, 0]])\n    True\n\n    Notes\n    -----\n    The direction of rotation is given by the right-hand rule (orient\n    the thumb of the right hand along the axis around which the rotation\n    occurs, with the end of the thumb at the positive end of the axis;\n    curl your fingers; the direction your fingers curl is the direction\n    of rotation).  Therefore, the rotations are counterclockwise if\n    looking along the axis of rotation from positive to negative.\n    \'\'\'\n    Ms = []\n    if z:\n        cosz = math.cos(z)\n        sinz = math.sin(z)\n        Ms.append(np.array(\n                [[cosz, -sinz, 0],\n                 [sinz, cosz, 0],\n                 [0, 0, 1]]))\n    if y:\n        cosy = math.cos(y)\n        siny = math.sin(y)\n        Ms.append(np.array(\n                [[cosy, 0, siny],\n                 [0, 1, 0],\n                 [-siny, 0, cosy]]))\n    if x:\n        cosx = math.cos(x)\n        sinx = math.sin(x)\n        Ms.append(np.array(\n                [[1, 0, 0],\n                 [0, cosx, -sinx],\n                 [0, sinx, cosx]]))\n    if Ms:\n        return reduce(np.dot, Ms[::-1])\n    return np.eye(3)\n\n\ndef mat2euler(M, cy_thresh=None):\n    \'\'\' Discover Euler angle vector from 3x3 matrix\n\n    Uses the conventions above.\n\n    Parameters\n    ----------\n    M : array-like, shape (3,3)\n    cy_thresh : None or scalar, optional\n       threshold below which to give up on straightforward arctan for\n       estimating x rotation.  If None (default), estimate from\n       precision of input.\n\n    Returns\n    -------\n    z : scalar\n    y : scalar\n    x : scalar\n       Rotations in radians around z, y, x axes, respectively\n\n    Notes\n    -----\n    If there was no numerical error, the routine could be derived using\n    Sympy expression for z then y then x rotation matrix, which is::\n\n      [                       cos(y)*cos(z),                       -cos(y)*sin(z),         sin(y)],\n      [cos(x)*sin(z) + cos(z)*sin(x)*sin(y), cos(x)*cos(z) - sin(x)*sin(y)*sin(z), -cos(y)*sin(x)],\n      [sin(x)*sin(z) - cos(x)*cos(z)*sin(y), cos(z)*sin(x) + cos(x)*sin(y)*sin(z),  cos(x)*cos(y)]\n\n    with the obvious derivations for z, y, and x\n\n       z = atan2(-r12, r11)\n       y = asin(r13)\n       x = atan2(-r23, r33)\n\n    Problems arise when cos(y) is close to zero, because both of::\n\n       z = atan2(cos(y)*sin(z), cos(y)*cos(z))\n       x = atan2(cos(y)*sin(x), cos(x)*cos(y))\n\n    will be close to atan2(0, 0), and highly unstable.\n\n    The ``cy`` fix for numerical instability below is from: *Graphics\n    Gems IV*, Paul Heckbert (editor), Academic Press, 1994, ISBN:\n    0123361559.  Specifically it comes from EulerAngles.c by Ken\n    Shoemake, and deals with the case where cos(y) is close to zero:\n\n    See: http://www.graphicsgems.org/\n\n    The code appears to be licensed (from the website) as ""can be used\n    without restrictions"".\n    \'\'\'\n    M = np.asarray(M)\n    if cy_thresh is None:\n        try:\n            cy_thresh = np.finfo(M.dtype).eps * 4\n        except ValueError:\n            cy_thresh = _FLOAT_EPS_4\n    r11, r12, r13, r21, r22, r23, r31, r32, r33 = M.flat\n    # cy: sqrt((cos(y)*cos(z))**2 + (cos(x)*cos(y))**2)\n    cy = math.sqrt(r33*r33 + r23*r23)\n    if cy > cy_thresh: # cos(y) not close to zero, standard form\n        z = math.atan2(-r12,  r11) # atan2(cos(y)*sin(z), cos(y)*cos(z))\n        y = math.atan2(r13,  cy) # atan2(sin(y), cy)\n        x = math.atan2(-r23, r33) # atan2(cos(y)*sin(x), cos(x)*cos(y))\n    else: # cos(y) (close to) zero, so x -> 0.0 (see above)\n        # so r21 -> sin(z), r22 -> cos(z) and\n        z = math.atan2(r21,  r22)\n        y = math.atan2(r13,  cy) # atan2(sin(y), cy)\n        x = 0.0\n    return z, y, x\n\n\ndef euler2quat(z=0, y=0, x=0):\n    \'\'\' Return quaternion corresponding to these Euler angles\n\n    Uses the z, then y, then x convention above\n\n    Parameters\n    ----------\n    z : scalar\n       Rotation angle in radians around z-axis (performed first)\n    y : scalar\n       Rotation angle in radians around y-axis\n    x : scalar\n       Rotation angle in radians around x-axis (performed last)\n\n    Returns\n    -------\n    quat : array shape (4,)\n       Quaternion in w, x, y z (real, then vector) format\n\n    Notes\n    -----\n    We can derive this formula in Sympy using:\n\n    1. Formula giving quaternion corresponding to rotation of theta radians\n       about arbitrary axis:\n       http://mathworld.wolfram.com/EulerParameters.html\n    2. Generated formulae from 1.) for quaternions corresponding to\n       theta radians rotations about ``x, y, z`` axes\n    3. Apply quaternion multiplication formula -\n       http://en.wikipedia.org/wiki/Quaternions#Hamilton_product - to\n       formulae from 2.) to give formula for combined rotations.\n    \'\'\'\n    z = z/2.0\n    y = y/2.0\n    x = x/2.0\n    cz = math.cos(z)\n    sz = math.sin(z)\n    cy = math.cos(y)\n    sy = math.sin(y)\n    cx = math.cos(x)\n    sx = math.sin(x)\n    return np.array([\n             cx*cy*cz - sx*sy*sz,\n             cx*sy*sz + cy*cz*sx,\n             cx*cz*sy - sx*cy*sz,\n             cx*cy*sz + sx*cz*sy])\n\n\ndef quat2euler(q):\n    \'\'\' Return Euler angles corresponding to quaternion `q`\n\n    Parameters\n    ----------\n    q : 4 element sequence\n       w, x, y, z of quaternion\n\n    Returns\n    -------\n    z : scalar\n       Rotation angle in radians around z-axis (performed first)\n    y : scalar\n       Rotation angle in radians around y-axis\n    x : scalar\n       Rotation angle in radians around x-axis (performed last)\n\n    Notes\n    -----\n    It\'s possible to reduce the amount of calculation a little, by\n    combining parts of the ``quat2mat`` and ``mat2euler`` functions, but\n    the reduction in computation is small, and the code repetition is\n    large.\n    \'\'\'\n    # delayed import to avoid cyclic dependencies\n    import nibabel.quaternions as nq\n    return mat2euler(nq.quat2mat(q))\n\n\ndef euler2angle_axis(z=0, y=0, x=0):\n    \'\'\' Return angle, axis corresponding to these Euler angles\n\n    Uses the z, then y, then x convention above\n\n    Parameters\n    ----------\n    z : scalar\n       Rotation angle in radians around z-axis (performed first)\n    y : scalar\n       Rotation angle in radians around y-axis\n    x : scalar\n       Rotation angle in radians around x-axis (performed last)\n\n    Returns\n    -------\n    theta : scalar\n       angle of rotation\n    vector : array shape (3,)\n       axis around which rotation occurs\n\n    Examples\n    --------\n    >>> theta, vec = euler2angle_axis(0, 1.5, 0)\n    >>> print(theta)\n    1.5\n    >>> np.allclose(vec, [0, 1, 0])\n    True\n    \'\'\'\n    # delayed import to avoid cyclic dependencies\n    import nibabel.quaternions as nq\n    return nq.quat2angle_axis(euler2quat(z, y, x))\n\n\ndef angle_axis2euler(theta, vector, is_normalized=False):\n    \'\'\' Convert angle, axis pair to Euler angles\n\n    Parameters\n    ----------\n    theta : scalar\n       angle of rotation\n    vector : 3 element sequence\n       vector specifying axis for rotation.\n    is_normalized : bool, optional\n       True if vector is already normalized (has norm of 1).  Default\n       False\n\n    Returns\n    -------\n    z : scalar\n    y : scalar\n    x : scalar\n       Rotations in radians around z, y, x axes, respectively\n\n    Examples\n    --------\n    >>> z, y, x = angle_axis2euler(0, [1, 0, 0])\n    >>> np.allclose((z, y, x), 0)\n    True\n\n    Notes\n    -----\n    It\'s possible to reduce the amount of calculation a little, by\n    combining parts of the ``angle_axis2mat`` and ``mat2euler``\n    functions, but the reduction in computation is small, and the code\n    repetition is large.\n    \'\'\'\n    # delayed import to avoid cyclic dependencies\n    import nibabel.quaternions as nq\n    M = nq.angle_axis2mat(theta, vector, is_normalized)\n    return mat2euler(M)\n'"
pointnet2/utils/pc_util.py,30,"b'"""""" Utility functions for processing point clouds.\n\nAuthor: Charles R. Qi, Hao Su\nDate: November 2016\n""""""\n\nimport os\nimport sys\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nsys.path.append(BASE_DIR)\n\n# Draw point cloud\nfrom eulerangles import euler2mat\n\n# Point cloud IO\nimport numpy as np\nfrom plyfile import PlyData, PlyElement\n\n \n# ----------------------------------------\n# Point Cloud/Volume Conversions\n# ----------------------------------------\n\ndef point_cloud_to_volume_batch(point_clouds, vsize=12, radius=1.0, flatten=True):\n    """""" Input is BxNx3 batch of point cloud\n        Output is Bx(vsize^3)\n    """"""\n    vol_list = []\n    for b in range(point_clouds.shape[0]):\n        vol = point_cloud_to_volume(np.squeeze(point_clouds[b,:,:]), vsize, radius)\n        if flatten:\n            vol_list.append(vol.flatten())\n        else:\n            vol_list.append(np.expand_dims(np.expand_dims(vol, -1), 0))\n    if flatten:\n        return np.vstack(vol_list)\n    else:\n        return np.concatenate(vol_list, 0)\n\n\ndef point_cloud_to_volume(points, vsize, radius=1.0):\n    """""" input is Nx3 points.\n        output is vsize*vsize*vsize\n        assumes points are in range [-radius, radius]\n    """"""\n    vol = np.zeros((vsize,vsize,vsize))\n    voxel = 2*radius/float(vsize)\n    locations = (points + radius)/voxel\n    locations = locations.astype(int)\n    vol[locations[:,0],locations[:,1],locations[:,2]] = 1.0\n    return vol\n\n#a = np.zeros((16,1024,3))\n#print point_cloud_to_volume_batch(a, 12, 1.0, False).shape\n\ndef volume_to_point_cloud(vol):\n    """""" vol is occupancy grid (value = 0 or 1) of size vsize*vsize*vsize\n        return Nx3 numpy array.\n    """"""\n    vsize = vol.shape[0]\n    assert(vol.shape[1] == vsize and vol.shape[1] == vsize)\n    points = []\n    for a in range(vsize):\n        for b in range(vsize):\n            for c in range(vsize):\n                if vol[a,b,c] == 1:\n                    points.append(np.array([a,b,c]))\n    if len(points) == 0:\n        return np.zeros((0,3))\n    points = np.vstack(points)\n    return points\n\n# ----------------------------------------\n# Point cloud IO\n# ----------------------------------------\n\ndef read_ply(filename):\n    """""" read XYZ point cloud from filename PLY file """"""\n    plydata = PlyData.read(filename)\n    pc = plydata[\'vertex\'].data\n    pc_array = np.array([[x, y, z] for x,y,z in pc])\n    return pc_array\n\n\ndef write_ply(points, filename, text=True):\n    """""" input: Nx3, write points to filename as PLY format. """"""\n    points = [(points[i,0], points[i,1], points[i,2]) for i in range(points.shape[0])]\n    vertex = np.array(points, dtype=[(\'x\', \'f4\'), (\'y\', \'f4\'),(\'z\', \'f4\')])\n    el = PlyElement.describe(vertex, \'vertex\', comments=[\'vertices\'])\n    PlyData([el], text=text).write(filename)\n\n\n# ----------------------------------------\n# Simple Point cloud and Volume Renderers\n# ----------------------------------------\n\ndef draw_point_cloud(input_points, canvasSize=500, space=200, diameter=25,\n                     xrot=0, yrot=0, zrot=0, switch_xyz=[0,1,2], normalize=False):\n    """""" Render point cloud to image with alpha channel.\n        Input:\n            points: Nx3 numpy array (+y is up direction)\n        Output:\n            gray image as numpy array of size canvasSizexcanvasSize\n    """"""\n    image = np.zeros((canvasSize, canvasSize))\n    if input_points is None or input_points.shape[0] == 0:\n        return image\n\n    points = input_points[:, switch_xyz]\n    M = euler2mat(zrot, yrot, xrot)\n    points = (np.dot(M, points.transpose())).transpose()\n\n    # Normalize the point cloud\n    # We normalize scale to fit points in a unit sphere\n    if normalize:\n        centroid = np.mean(points, axis=0)\n        points -= centroid\n        furthest_distance = np.max(np.sqrt(np.sum(abs(points)**2,axis=-1)))\n        points /= furthest_distance\n    \n    dist = np.linalg.norm(points, axis = -1)\n    points = points[dist <= 1]\n\n    # Pre-compute the Gaussian disk\n    radius = (diameter-1)/2.0\n    disk = np.zeros((diameter, diameter))\n    for i in range(diameter):\n        for j in range(diameter):\n            if (i - radius) * (i-radius) + (j-radius) * (j-radius) <= radius * radius:\n                disk[i, j] = np.exp((-(i-radius)**2 - (j-radius)**2)/(radius**2))\n    mask = np.argwhere(disk > 0)\n    dx = mask[:, 0]\n    dy = mask[:, 1]\n    dv = disk[disk > 0]\n    \n    # Order points by z-buffer\n    zorder = np.argsort(points[:, 2])\n    points = points[zorder, :]\n    points[:, 2] = (points[:, 2] - np.min(points[:, 2])) / (np.max(points[:, 2] - np.min(points[:, 2])))\n    max_depth = np.max(points[:, 2])\n       \n    for i in range(points.shape[0]):\n        j = points.shape[0] - i - 1\n        x = points[j, 0]\n        y = points[j, 1]\n        xc = canvasSize/2 + (x*space)\n        yc = canvasSize/2 + (y*space)\n        xc = int(np.round(xc))\n        yc = int(np.round(yc))\n        \n        px = dx + xc\n        py = dy + yc\n        \n        image[px, py] = image[px, py] * 0.7 + dv * (max_depth - points[j, 2]) * 0.3\n    \n    image = image / np.max(image)\n    return image\n\ndef point_cloud_three_views(points):\n    """""" input points Nx3 numpy array (+y is up direction).\n        return an numpy array gray image of size 500x1500. """""" \n    # +y is up direction\n    # xrot is azimuth\n    # yrot is in-plane\n    # zrot is elevation\n    img1 = draw_point_cloud(points, zrot=110/180.0*np.pi, xrot=45/180.0*np.pi, yrot=0/180.0*np.pi)\n    img2 = draw_point_cloud(points, zrot=70/180.0*np.pi, xrot=135/180.0*np.pi, yrot=0/180.0*np.pi)\n    img3 = draw_point_cloud(points, zrot=180.0/180.0*np.pi, xrot=90/180.0*np.pi, yrot=0/180.0*np.pi)\n    image_large = np.concatenate([img1, img2, img3], 1)\n    return image_large\n\n\nfrom PIL import Image\ndef point_cloud_three_views_demo():\n    """""" Demo for draw_point_cloud function """"""\n    points = read_ply(\'../third_party/mesh_sampling/piano.ply\')\n    im_array = point_cloud_three_views(points)\n    img = Image.fromarray(np.uint8(im_array*255.0))\n    img.save(\'piano.jpg\')\n\nif __name__==""__main__"":\n    point_cloud_three_views_demo()\n\n\nimport matplotlib.pyplot as plt\ndef pyplot_draw_point_cloud(points, output_filename):\n    """""" points is a Nx3 numpy array """"""\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection=\'3d\')\n    ax.scatter(points[:,0], points[:,1], points[:,2])\n    ax.set_xlabel(\'x\')\n    ax.set_ylabel(\'y\')\n    ax.set_zlabel(\'z\')\n    #savefig(output_filename)\n\ndef pyplot_draw_volume(vol, output_filename):\n    """""" vol is of size vsize*vsize*vsize\n        output an image to output_filename\n    """"""\n    points = volume_to_point_cloud(vol)\n    pyplot_draw_point_cloud(points, output_filename)\n'"
pointnet2/utils/plyfile.py,18,"b'#   Copyright 2014 Darsh Ranjan\n#\n#   This file is part of python-plyfile.\n#\n#   python-plyfile is free software: you can redistribute it and/or\n#   modify it under the terms of the GNU General Public License as\n#   published by the Free Software Foundation, either version 3 of the\n#   License, or (at your option) any later version.\n#\n#   python-plyfile is distributed in the hope that it will be useful,\n#   but WITHOUT ANY WARRANTY; without even the implied warranty of\n#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n#   General Public License for more details.\n#\n#   You should have received a copy of the GNU General Public License\n#   along with python-plyfile.  If not, see\n#       <http://www.gnu.org/licenses/>.\n\nfrom itertools import islice as _islice\n\nimport numpy as _np\nfrom sys import byteorder as _byteorder\n\n\ntry:\n    _range = xrange\nexcept NameError:\n    _range = range\n\n\n# Many-many relation\n_data_type_relation = [\n    (\'int8\', \'i1\'),\n    (\'char\', \'i1\'),\n    (\'uint8\', \'u1\'),\n    (\'uchar\', \'b1\'),\n    (\'uchar\', \'u1\'),\n    (\'int16\', \'i2\'),\n    (\'short\', \'i2\'),\n    (\'uint16\', \'u2\'),\n    (\'ushort\', \'u2\'),\n    (\'int32\', \'i4\'),\n    (\'int\', \'i4\'),\n    (\'uint32\', \'u4\'),\n    (\'uint\', \'u4\'),\n    (\'float32\', \'f4\'),\n    (\'float\', \'f4\'),\n    (\'float64\', \'f8\'),\n    (\'double\', \'f8\')\n]\n\n_data_types = dict(_data_type_relation)\n_data_type_reverse = dict((b, a) for (a, b) in _data_type_relation)\n\n_types_list = []\n_types_set = set()\nfor (_a, _b) in _data_type_relation:\n    if _a not in _types_set:\n        _types_list.append(_a)\n        _types_set.add(_a)\n    if _b not in _types_set:\n        _types_list.append(_b)\n        _types_set.add(_b)\n\n\n_byte_order_map = {\n    \'ascii\': \'=\',\n    \'binary_little_endian\': \'<\',\n    \'binary_big_endian\': \'>\'\n}\n\n_byte_order_reverse = {\n    \'<\': \'binary_little_endian\',\n    \'>\': \'binary_big_endian\'\n}\n\n_native_byte_order = {\'little\': \'<\', \'big\': \'>\'}[_byteorder]\n\n\ndef _lookup_type(type_str):\n    if type_str not in _data_type_reverse:\n        try:\n            type_str = _data_types[type_str]\n        except KeyError:\n            raise ValueError(""field type %r not in %r"" %\n                             (type_str, _types_list))\n\n    return _data_type_reverse[type_str]\n\n\ndef _split_line(line, n):\n    fields = line.split(None, n)\n    if len(fields) == n:\n        fields.append(\'\')\n\n    assert len(fields) == n + 1\n\n    return fields\n\n\ndef make2d(array, cols=None, dtype=None):\n    \'\'\'\n    Make a 2D array from an array of arrays.  The `cols\' and `dtype\'\n    arguments can be omitted if the array is not empty.\n\n    \'\'\'\n    if (cols is None or dtype is None) and not len(array):\n        raise RuntimeError(""cols and dtype must be specified for empty ""\n                           ""array"")\n\n    if cols is None:\n        cols = len(array[0])\n\n    if dtype is None:\n        dtype = array[0].dtype\n\n    return _np.fromiter(array, [(\'_\', dtype, (cols,))],\n                        count=len(array))[\'_\']\n\n\nclass PlyParseError(Exception):\n\n    \'\'\'\n    Raised when a PLY file cannot be parsed.\n\n    The attributes `element\', `row\', `property\', and `message\' give\n    additional information.\n\n    \'\'\'\n\n    def __init__(self, message, element=None, row=None, prop=None):\n        self.message = message\n        self.element = element\n        self.row = row\n        self.prop = prop\n\n        s = \'\'\n        if self.element:\n            s += \'element %r: \' % self.element.name\n        if self.row is not None:\n            s += \'row %d: \' % self.row\n        if self.prop:\n            s += \'property %r: \' % self.prop.name\n        s += self.message\n\n        Exception.__init__(self, s)\n\n    def __repr__(self):\n        return (\'PlyParseError(%r, element=%r, row=%r, prop=%r)\' %\n                self.message, self.element, self.row, self.prop)\n\n\nclass PlyData(object):\n\n    \'\'\'\n    PLY file header and data.\n\n    A PlyData instance is created in one of two ways: by the static\n    method PlyData.read (to read a PLY file), or directly from __init__\n    given a sequence of elements (which can then be written to a PLY\n    file).\n\n    \'\'\'\n\n    def __init__(self, elements=[], text=False, byte_order=\'=\',\n                 comments=[], obj_info=[]):\n        \'\'\'\n        elements: sequence of PlyElement instances.\n\n        text: whether the resulting PLY file will be text (True) or\n            binary (False).\n\n        byte_order: \'<\' for little-endian, \'>\' for big-endian, or \'=\'\n            for native.  This is only relevant if `text\' is False.\n\n        comments: sequence of strings that will be placed in the header\n            between the \'ply\' and \'format ...\' lines.\n\n        obj_info: like comments, but will be placed in the header with\n            ""obj_info ..."" instead of ""comment ..."".\n\n        \'\'\'\n        if byte_order == \'=\' and not text:\n            byte_order = _native_byte_order\n\n        self.byte_order = byte_order\n        self.text = text\n\n        self.comments = list(comments)\n        self.obj_info = list(obj_info)\n        self.elements = elements\n\n    def _get_elements(self):\n        return self._elements\n\n    def _set_elements(self, elements):\n        self._elements = tuple(elements)\n        self._index()\n\n    elements = property(_get_elements, _set_elements)\n\n    def _get_byte_order(self):\n        return self._byte_order\n\n    def _set_byte_order(self, byte_order):\n        if byte_order not in [\'<\', \'>\', \'=\']:\n            raise ValueError(""byte order must be \'<\', \'>\', or \'=\'"")\n\n        self._byte_order = byte_order\n\n    byte_order = property(_get_byte_order, _set_byte_order)\n\n    def _index(self):\n        self._element_lookup = dict((elt.name, elt) for elt in\n                                    self._elements)\n        if len(self._element_lookup) != len(self._elements):\n            raise ValueError(""two elements with same name"")\n\n    @staticmethod\n    def _parse_header(stream):\n        \'\'\'\n        Parse a PLY header from a readable file-like stream.\n\n        \'\'\'\n        lines = []\n        comments = {\'comment\': [], \'obj_info\': []}\n        while True:\n            line = stream.readline().decode(\'ascii\').strip()\n            fields = _split_line(line, 1)\n\n            if fields[0] == \'end_header\':\n                break\n\n            elif fields[0] in comments.keys():\n                lines.append(fields)\n            else:\n                lines.append(line.split())\n\n        a = 0\n        if lines[a] != [\'ply\']:\n            raise PlyParseError(""expected \'ply\'"")\n\n        a += 1\n        while lines[a][0] in comments.keys():\n            comments[lines[a][0]].append(lines[a][1])\n            a += 1\n\n        if lines[a][0] != \'format\':\n            raise PlyParseError(""expected \'format\'"")\n\n        if lines[a][2] != \'1.0\':\n            raise PlyParseError(""expected version \'1.0\'"")\n\n        if len(lines[a]) != 3:\n            raise PlyParseError(""too many fields after \'format\'"")\n\n        fmt = lines[a][1]\n\n        if fmt not in _byte_order_map:\n            raise PlyParseError(""don\'t understand format %r"" % fmt)\n\n        byte_order = _byte_order_map[fmt]\n        text = fmt == \'ascii\'\n\n        a += 1\n        while a < len(lines) and lines[a][0] in comments.keys():\n            comments[lines[a][0]].append(lines[a][1])\n            a += 1\n\n        return PlyData(PlyElement._parse_multi(lines[a:]),\n                       text, byte_order,\n                       comments[\'comment\'], comments[\'obj_info\'])\n\n    @staticmethod\n    def read(stream):\n        \'\'\'\n        Read PLY data from a readable file-like object or filename.\n\n        \'\'\'\n        (must_close, stream) = _open_stream(stream, \'read\')\n        try:\n            data = PlyData._parse_header(stream)\n            for elt in data:\n                elt._read(stream, data.text, data.byte_order)\n        finally:\n            if must_close:\n                stream.close()\n\n        return data\n\n    def write(self, stream):\n        \'\'\'\n        Write PLY data to a writeable file-like object or filename.\n\n        \'\'\'\n        (must_close, stream) = _open_stream(stream, \'write\')\n        try:\n            stream.write(self.header.encode(\'ascii\'))\n            stream.write(b\'\\r\\n\')\n            for elt in self:\n                elt._write(stream, self.text, self.byte_order)\n        finally:\n            if must_close:\n                stream.close()\n\n    @property\n    def header(self):\n        \'\'\'\n        Provide PLY-formatted metadata for the instance.\n\n        \'\'\'\n        lines = [\'ply\']\n\n        if self.text:\n            lines.append(\'format ascii 1.0\')\n        else:\n            lines.append(\'format \' +\n                         _byte_order_reverse[self.byte_order] +\n                         \' 1.0\')\n\n        # Some information is lost here, since all comments are placed\n        # between the \'format\' line and the first element.\n        for c in self.comments:\n            lines.append(\'comment \' + c)\n\n        for c in self.obj_info:\n            lines.append(\'obj_info \' + c)\n\n        lines.extend(elt.header for elt in self.elements)\n        lines.append(\'end_header\')\n        return \'\\r\\n\'.join(lines)\n\n    def __iter__(self):\n        return iter(self.elements)\n\n    def __len__(self):\n        return len(self.elements)\n\n    def __contains__(self, name):\n        return name in self._element_lookup\n\n    def __getitem__(self, name):\n        return self._element_lookup[name]\n\n    def __str__(self):\n        return self.header\n\n    def __repr__(self):\n        return (\'PlyData(%r, text=%r, byte_order=%r, \'\n                \'comments=%r, obj_info=%r)\' %\n                (self.elements, self.text, self.byte_order,\n                 self.comments, self.obj_info))\n\n\ndef _open_stream(stream, read_or_write):\n    if hasattr(stream, read_or_write):\n        return (False, stream)\n    try:\n        return (True, open(stream, read_or_write[0] + \'b\'))\n    except TypeError:\n        raise RuntimeError(""expected open file or filename"")\n\n\nclass PlyElement(object):\n\n    \'\'\'\n    PLY file element.\n\n    A client of this library doesn\'t normally need to instantiate this\n    directly, so the following is only for the sake of documenting the\n    internals.\n\n    Creating a PlyElement instance is generally done in one of two ways:\n    as a byproduct of PlyData.read (when reading a PLY file) and by\n    PlyElement.describe (before writing a PLY file).\n\n    \'\'\'\n\n    def __init__(self, name, properties, count, comments=[]):\n        \'\'\'\n        This is not part of the public interface.  The preferred methods\n        of obtaining PlyElement instances are PlyData.read (to read from\n        a file) and PlyElement.describe (to construct from a numpy\n        array).\n\n        \'\'\'\n        self._name = str(name)\n        self._check_name()\n        self._count = count\n\n        self._properties = tuple(properties)\n        self._index()\n\n        self.comments = list(comments)\n\n        self._have_list = any(isinstance(p, PlyListProperty)\n                              for p in self.properties)\n\n    @property\n    def count(self):\n        return self._count\n\n    def _get_data(self):\n        return self._data\n\n    def _set_data(self, data):\n        self._data = data\n        self._count = len(data)\n        self._check_sanity()\n\n    data = property(_get_data, _set_data)\n\n    def _check_sanity(self):\n        for prop in self.properties:\n            if prop.name not in self._data.dtype.fields:\n                raise ValueError(""dangling property %r"" % prop.name)\n\n    def _get_properties(self):\n        return self._properties\n\n    def _set_properties(self, properties):\n        self._properties = tuple(properties)\n        self._check_sanity()\n        self._index()\n\n    properties = property(_get_properties, _set_properties)\n\n    def _index(self):\n        self._property_lookup = dict((prop.name, prop)\n                                     for prop in self._properties)\n        if len(self._property_lookup) != len(self._properties):\n            raise ValueError(""two properties with same name"")\n\n    def ply_property(self, name):\n        return self._property_lookup[name]\n\n    @property\n    def name(self):\n        return self._name\n\n    def _check_name(self):\n        if any(c.isspace() for c in self._name):\n            msg = ""element name %r contains spaces"" % self._name\n            raise ValueError(msg)\n\n    def dtype(self, byte_order=\'=\'):\n        \'\'\'\n        Return the numpy dtype of the in-memory representation of the\n        data.  (If there are no list properties, and the PLY format is\n        binary, then this also accurately describes the on-disk\n        representation of the element.)\n\n        \'\'\'\n        return [(prop.name, prop.dtype(byte_order))\n                for prop in self.properties]\n\n    @staticmethod\n    def _parse_multi(header_lines):\n        \'\'\'\n        Parse a list of PLY element definitions.\n\n        \'\'\'\n        elements = []\n        while header_lines:\n            (elt, header_lines) = PlyElement._parse_one(header_lines)\n            elements.append(elt)\n\n        return elements\n\n    @staticmethod\n    def _parse_one(lines):\n        \'\'\'\n        Consume one element definition.  The unconsumed input is\n        returned along with a PlyElement instance.\n\n        \'\'\'\n        a = 0\n        line = lines[a]\n\n        if line[0] != \'element\':\n            raise PlyParseError(""expected \'element\'"")\n        if len(line) > 3:\n            raise PlyParseError(""too many fields after \'element\'"")\n        if len(line) < 3:\n            raise PlyParseError(""too few fields after \'element\'"")\n\n        (name, count) = (line[1], int(line[2]))\n\n        comments = []\n        properties = []\n        while True:\n            a += 1\n            if a >= len(lines):\n                break\n\n            if lines[a][0] == \'comment\':\n                comments.append(lines[a][1])\n            elif lines[a][0] == \'property\':\n                properties.append(PlyProperty._parse_one(lines[a]))\n            else:\n                break\n\n        return (PlyElement(name, properties, count, comments),\n                lines[a:])\n\n    @staticmethod\n    def describe(data, name, len_types={}, val_types={},\n                 comments=[]):\n        \'\'\'\n        Construct a PlyElement from an array\'s metadata.\n\n        len_types and val_types can be given as mappings from list\n        property names to type strings (like \'u1\', \'f4\', etc., or\n        \'int8\', \'float32\', etc.). These can be used to define the length\n        and value types of list properties.  List property lengths\n        always default to type \'u1\' (8-bit unsigned integer), and value\n        types default to \'i4\' (32-bit integer).\n\n        \'\'\'\n        if not isinstance(data, _np.ndarray):\n            raise TypeError(""only numpy arrays are supported"")\n\n        if len(data.shape) != 1:\n            raise ValueError(""only one-dimensional arrays are ""\n                             ""supported"")\n\n        count = len(data)\n\n        properties = []\n        descr = data.dtype.descr\n\n        for t in descr:\n            if not isinstance(t[1], str):\n                raise ValueError(""nested records not supported"")\n\n            if not t[0]:\n                raise ValueError(""field with empty name"")\n\n            if len(t) != 2 or t[1][1] == \'O\':\n                # non-scalar field, which corresponds to a list\n                # property in PLY.\n\n                if t[1][1] == \'O\':\n                    if len(t) != 2:\n                        raise ValueError(""non-scalar object fields not ""\n                                         ""supported"")\n\n                len_str = _data_type_reverse[len_types.get(t[0], \'u1\')]\n                if t[1][1] == \'O\':\n                    val_type = val_types.get(t[0], \'i4\')\n                    val_str = _lookup_type(val_type)\n                else:\n                    val_str = _lookup_type(t[1][1:])\n\n                prop = PlyListProperty(t[0], len_str, val_str)\n            else:\n                val_str = _lookup_type(t[1][1:])\n                prop = PlyProperty(t[0], val_str)\n\n            properties.append(prop)\n\n        elt = PlyElement(name, properties, count, comments)\n        elt.data = data\n\n        return elt\n\n    def _read(self, stream, text, byte_order):\n        \'\'\'\n        Read the actual data from a PLY file.\n\n        \'\'\'\n        if text:\n            self._read_txt(stream)\n        else:\n            if self._have_list:\n                # There are list properties, so a simple load is\n                # impossible.\n                self._read_bin(stream, byte_order)\n            else:\n                # There are no list properties, so loading the data is\n                # much more straightforward.\n                self._data = _np.fromfile(stream,\n                                          self.dtype(byte_order),\n                                          self.count)\n\n        if len(self._data) < self.count:\n            k = len(self._data)\n            del self._data\n            raise PlyParseError(""early end-of-file"", self, k)\n\n        self._check_sanity()\n\n    def _write(self, stream, text, byte_order):\n        \'\'\'\n        Write the data to a PLY file.\n\n        \'\'\'\n        if text:\n            self._write_txt(stream)\n        else:\n            if self._have_list:\n                # There are list properties, so serialization is\n                # slightly complicated.\n                self._write_bin(stream, byte_order)\n            else:\n                # no list properties, so serialization is\n                # straightforward.\n                self.data.astype(self.dtype(byte_order),\n                                 copy=False).tofile(stream)\n\n    def _read_txt(self, stream):\n        \'\'\'\n        Load a PLY element from an ASCII-format PLY file.  The element\n        may contain list properties.\n\n        \'\'\'\n        self._data = _np.empty(self.count, dtype=self.dtype())\n\n        k = 0\n        for line in _islice(iter(stream.readline, b\'\'), self.count):\n            fields = iter(line.strip().split())\n            for prop in self.properties:\n                try:\n                    self._data[prop.name][k] = prop._from_fields(fields)\n                except StopIteration:\n                    raise PlyParseError(""early end-of-line"",\n                                        self, k, prop)\n                except ValueError:\n                    raise PlyParseError(""malformed input"",\n                                        self, k, prop)\n            try:\n                next(fields)\n            except StopIteration:\n                pass\n            else:\n                raise PlyParseError(""expected end-of-line"", self, k)\n            k += 1\n\n        if k < self.count:\n            del self._data\n            raise PlyParseError(""early end-of-file"", self, k)\n\n    def _write_txt(self, stream):\n        \'\'\'\n        Save a PLY element to an ASCII-format PLY file.  The element may\n        contain list properties.\n\n        \'\'\'\n        for rec in self.data:\n            fields = []\n            for prop in self.properties:\n                fields.extend(prop._to_fields(rec[prop.name]))\n\n            _np.savetxt(stream, [fields], \'%.18g\', newline=\'\\r\\n\')\n\n    def _read_bin(self, stream, byte_order):\n        \'\'\'\n        Load a PLY element from a binary PLY file.  The element may\n        contain list properties.\n\n        \'\'\'\n        self._data = _np.empty(self.count, dtype=self.dtype(byte_order))\n\n        for k in _range(self.count):\n            for prop in self.properties:\n                try:\n                    self._data[prop.name][k] = \\\n                        prop._read_bin(stream, byte_order)\n                except StopIteration:\n                    raise PlyParseError(""early end-of-file"",\n                                        self, k, prop)\n\n    def _write_bin(self, stream, byte_order):\n        \'\'\'\n        Save a PLY element to a binary PLY file.  The element may\n        contain list properties.\n\n        \'\'\'\n        for rec in self.data:\n            for prop in self.properties:\n                prop._write_bin(rec[prop.name], stream, byte_order)\n\n    @property\n    def header(self):\n        \'\'\'\n        Format this element\'s metadata as it would appear in a PLY\n        header.\n\n        \'\'\'\n        lines = [\'element %s %d\' % (self.name, self.count)]\n\n        # Some information is lost here, since all comments are placed\n        # between the \'element\' line and the first property definition.\n        for c in self.comments:\n            lines.append(\'comment \' + c)\n\n        lines.extend(list(map(str, self.properties)))\n\n        return \'\\r\\n\'.join(lines)\n\n    def __getitem__(self, key):\n        return self.data[key]\n\n    def __setitem__(self, key, value):\n        self.data[key] = value\n\n    def __str__(self):\n        return self.header\n\n    def __repr__(self):\n        return (\'PlyElement(%r, %r, count=%d, comments=%r)\' %\n                (self.name, self.properties, self.count,\n                 self.comments))\n\n\nclass PlyProperty(object):\n\n    \'\'\'\n    PLY property description.  This class is pure metadata; the data\n    itself is contained in PlyElement instances.\n\n    \'\'\'\n\n    def __init__(self, name, val_dtype):\n        self._name = str(name)\n        self._check_name()\n        self.val_dtype = val_dtype\n\n    def _get_val_dtype(self):\n        return self._val_dtype\n\n    def _set_val_dtype(self, val_dtype):\n        self._val_dtype = _data_types[_lookup_type(val_dtype)]\n\n    val_dtype = property(_get_val_dtype, _set_val_dtype)\n\n    @property\n    def name(self):\n        return self._name\n\n    def _check_name(self):\n        if any(c.isspace() for c in self._name):\n            msg = ""Error: property name %r contains spaces"" % self._name\n            raise RuntimeError(msg)\n\n    @staticmethod\n    def _parse_one(line):\n        assert line[0] == \'property\'\n\n        if line[1] == \'list\':\n            if len(line) > 5:\n                raise PlyParseError(""too many fields after ""\n                                    ""\'property list\'"")\n            if len(line) < 5:\n                raise PlyParseError(""too few fields after ""\n                                    ""\'property list\'"")\n\n            return PlyListProperty(line[4], line[2], line[3])\n\n        else:\n            if len(line) > 3:\n                raise PlyParseError(""too many fields after ""\n                                    ""\'property\'"")\n            if len(line) < 3:\n                raise PlyParseError(""too few fields after ""\n                                    ""\'property\'"")\n\n            return PlyProperty(line[2], line[1])\n\n    def dtype(self, byte_order=\'=\'):\n        \'\'\'\n        Return the numpy dtype description for this property (as a tuple\n        of strings).\n\n        \'\'\'\n        return byte_order + self.val_dtype\n\n    def _from_fields(self, fields):\n        \'\'\'\n        Parse from generator.  Raise StopIteration if the property could\n        not be read.\n\n        \'\'\'\n        return _np.dtype(self.dtype()).type(next(fields))\n\n    def _to_fields(self, data):\n        \'\'\'\n        Return generator over one item.\n\n        \'\'\'\n        yield _np.dtype(self.dtype()).type(data)\n\n    def _read_bin(self, stream, byte_order):\n        \'\'\'\n        Read data from a binary stream.  Raise StopIteration if the\n        property could not be read.\n\n        \'\'\'\n        try:\n            return _np.fromfile(stream, self.dtype(byte_order), 1)[0]\n        except IndexError:\n            raise StopIteration\n\n    def _write_bin(self, data, stream, byte_order):\n        \'\'\'\n        Write data to a binary stream.\n\n        \'\'\'\n        _np.dtype(self.dtype(byte_order)).type(data).tofile(stream)\n\n    def __str__(self):\n        val_str = _data_type_reverse[self.val_dtype]\n        return \'property %s %s\' % (val_str, self.name)\n\n    def __repr__(self):\n        return \'PlyProperty(%r, %r)\' % (self.name,\n                                        _lookup_type(self.val_dtype))\n\n\nclass PlyListProperty(PlyProperty):\n\n    \'\'\'\n    PLY list property description.\n\n    \'\'\'\n\n    def __init__(self, name, len_dtype, val_dtype):\n        PlyProperty.__init__(self, name, val_dtype)\n\n        self.len_dtype = len_dtype\n\n    def _get_len_dtype(self):\n        return self._len_dtype\n\n    def _set_len_dtype(self, len_dtype):\n        self._len_dtype = _data_types[_lookup_type(len_dtype)]\n\n    len_dtype = property(_get_len_dtype, _set_len_dtype)\n\n    def dtype(self, byte_order=\'=\'):\n        \'\'\'\n        List properties always have a numpy dtype of ""object"".\n\n        \'\'\'\n        return \'|O\'\n\n    def list_dtype(self, byte_order=\'=\'):\n        \'\'\'\n        Return the pair (len_dtype, val_dtype) (both numpy-friendly\n        strings).\n\n        \'\'\'\n        return (byte_order + self.len_dtype,\n                byte_order + self.val_dtype)\n\n    def _from_fields(self, fields):\n        (len_t, val_t) = self.list_dtype()\n\n        n = int(_np.dtype(len_t).type(next(fields)))\n\n        data = _np.loadtxt(list(_islice(fields, n)), val_t, ndmin=1)\n        if len(data) < n:\n            raise StopIteration\n\n        return data\n\n    def _to_fields(self, data):\n        \'\'\'\n        Return generator over the (numerical) PLY representation of the\n        list data (length followed by actual data).\n\n        \'\'\'\n        (len_t, val_t) = self.list_dtype()\n\n        data = _np.asarray(data, dtype=val_t).ravel()\n\n        yield _np.dtype(len_t).type(data.size)\n        for x in data:\n            yield x\n\n    def _read_bin(self, stream, byte_order):\n        (len_t, val_t) = self.list_dtype(byte_order)\n\n        try:\n            n = _np.fromfile(stream, len_t, 1)[0]\n        except IndexError:\n            raise StopIteration\n\n        data = _np.fromfile(stream, val_t, n)\n        if len(data) < n:\n            raise StopIteration\n\n        return data\n\n    def _write_bin(self, data, stream, byte_order):\n        \'\'\'\n        Write data to a binary stream.\n\n        \'\'\'\n        (len_t, val_t) = self.list_dtype(byte_order)\n\n        data = _np.asarray(data, dtype=val_t).ravel()\n\n        _np.array(data.size, dtype=len_t).tofile(stream)\n        data.tofile(stream)\n\n    def __str__(self):\n        len_str = _data_type_reverse[self.len_dtype]\n        val_str = _data_type_reverse[self.val_dtype]\n        return \'property list %s %s %s\' % (len_str, val_str, self.name)\n\n    def __repr__(self):\n        return (\'PlyListProperty(%r, %r, %r)\' %\n                (self.name,\n                 _lookup_type(self.len_dtype),\n                 _lookup_type(self.val_dtype)))\n'"
