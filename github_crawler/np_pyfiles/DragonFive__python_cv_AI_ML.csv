file_path,api_count,code
myTools.py,0,"b'# -*- coding: gbk -*-\n#__author__ = \'ASUS\'\n\nimport time,sys, urllib, urllib2, json,socket,re\n\n# \xe6\x96\xb0\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaapython\xe6\x96\x87\xe4\xbb\xb6\ndef newPy(fname):\n    fileName=r""E:\\\xe8\xb5\x84\xe6\x96\x99\\onedrive\\code\\python\\opencv""+""\\\\""+fname;\n    f = open(fileName, ""a"");\n    f.write(""# -*- coding: gbk -*-\\n#__author__ = \'dragonfive\'\\n"");\n    # \xe8\x8e\xb7\xe5\x8f\x96\xe5\xbd\x93\xe5\x89\x8d\xe6\x97\xb6\xe9\x97\xb4\n    now_clock = getSysTime()\n    f.write(""# ""+now_clock+""\\n"")\n    # \xe8\x8e\xb7\xe5\x8f\x96\xe5\xbd\x93\xe5\x89\x8d\xe5\x9c\xb0\xe7\x82\xb9\n    myip = Getmyip()\n    address = myip.getAddByIp(myip.get_ex_ip()).encode(\'gbk\');\n    f.write(""# ""+address+"" dragonfive \\n"");\n    # \xe8\x81\x94\xe7\xb3\xbb\xe6\x96\xb9\xe5\xbc\x8f\n    f.write(""# any question mail\xef\xbc\x9adragonfive1992@gmail.com \\n"");\n    # \xe7\x89\x88\xe6\x9d\x83\xe4\xbf\xa1\xe6\x81\xaf\n    f.write(""# copyright 1992-""+time.strftime(r""%Y"")+ "" dragonfive \\n"");\n    f.close();\n\n# \xe8\x8e\xb7\xe5\x8f\x96\xe7\xb3\xbb\xe7\xbb\x9f\xe6\x97\xb6\xe9\x97\xb4\ndef getSysTime():\n    now_clock = time.strftime(r""%d/%m/%Y %H:%M:%S"")\n    return now_clock\n\n# \xe8\x8e\xb7\xe5\x8f\x96\xe7\x99\xbe\xe5\xba\xa6ipstore\xe7\x9a\x84apikey\ndef getMyApikey():\n    return ""9b447786edf207a8813e115d44d85187""\n\n\nclass Getmyip:\n    # \xe8\x8e\xb7\xe5\xbe\x97\xe6\x9c\xac\xe6\x9c\xba\xe5\x86\x85\xe7\xbd\x91IP\n    def get_local_ip(self):\n        localIP=socket.gethostbyname(socket.gethostname())\n        return localIP;\n\n    # \xe8\x8e\xb7\xe5\x8f\x96\xe5\xa4\x96\xe7\xbd\x91IP\n    def get_ex_ip(self):\n        try:\n            myip = self.visit(""http://www.whereismyip.com/"")\n        except:\n            try:\n                myip = self.visit(""http://www.ip138.com/ip2city.asp"")\n            except:\n                myip = ""So sorry!!!""\n        return myip\n\n    def visit(self,url):\n        opener = urllib2.urlopen(url)\n        ourl=opener.geturl()\n        if url == ourl:\n            str = opener.read()\n            asd=re.search(\'\\d+\\.\\d+\\.\\d+\\.\\d+\',str).group(0)\n        return asd\n\n    # \xe8\x8e\xb7\xe5\x8f\x96\xe4\xb8\x80\xe4\xb8\xaaip\xe7\x9a\x84\xe5\x8c\xba\xe5\x9f\x9f\xe5\x9c\xb0\xe5\x9d\x80\xef\xbc\x8c\xe7\x94\xa8\xe7\x99\xbe\xe5\xba\xa6ipstore\xe7\x9a\x84\xe5\xb7\xa5\xe5\x85\xb7;\n    def getAddByIp(self,ipaddr):#\xe8\xbf\x99\xe9\x87\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x94\xa8\xe5\xa4\x9a\xe5\x8f\x82\xe6\x95\xb0\xe8\xbf\x9b\xe8\xa1\x8c\xe4\xbc\x98\xe5\x8c\x96\xef\xbc\x9b\n        url = \'http://apis.baidu.com/chazhao/ipsearch/ipsearch?ip=\'+ipaddr\n        req = urllib2.Request(url)\n\n        req.add_header(""apikey"", getMyApikey());\n\n        resp = urllib2.urlopen(req)\n        content = json.loads(resp.read())\n        if(content):\n            address=content[""data""][""country""]+"":""+content[""data""][""city""]+"":""+content[""data""][""operator""]\n            return address;\n\ndef is_huiwenshu(n):\n    return n==int(\'\'.join(list(reversed(str(n)))))\n\n\ndef get_huiwenshu():\n    it=range(100,1000)\n    it = filter(is_huiwenshu,it)\n    print([i for i in it if i<1000])\n\n'"
ML_in_action/04cumsum.py,0,"b""# -*- coding: utf-8 -*-\n#__author__ = 'ASUS'\nfrom numpy import *\nnum=range(1,50)\n\nsums=cumsum(num)\nprint sums"""
ML_in_action/05parzenWindow.py,0,"b'#-*- coding: utf-8 -*-\n#__author__ = \'ASUS\'\n\ndef parzen(x,h,N):\n    """"""\n    \xe5\x88\x9b\xe5\xbb\xba\xe4\xb8\x80\xe4\xb8\xaa\xe6\x96\xb9\xe5\xbd\xa2\xe7\xaa\x97\n    :param x: \xe6\xa0\xb7\xe6\x9c\xac\xe5\x88\x97\xe8\xa1\xa8\xef\xbc\x8c\xe8\xbf\x99\xe9\x87\x8c\xe6\xaf\x8f\xe4\xb8\xaa\xe6\xa0\xb7\xe6\x9c\xac\xe9\x83\xbd\xe6\x98\xaf\xe7\x94\xa8\xe4\xb8\x80\xe7\xbb\xb4\xe7\x89\xb9\xe5\xbe\x81\n    :param h: \xe7\xab\x8b\xe6\x96\xb9\xe4\xbd\x93\xe8\xbe\xb9\xe9\x95\xbf \xe8\xbf\x99\xe9\x87\x8c\xe5\x8f\xaa\xe6\x9c\x89\xe4\xb8\x80\xe4\xb8\xaa\xe7\x89\xb9\xe5\xbe\x81\xef\xbc\x9f\n    :param N: \xe6\xa0\xb7\xe6\x9c\xac\xe6\x80\xbb\xe6\x95\xb0\n    :return: \xe8\xbf\x94\xe5\x9b\x9e\xe6\xaf\x8f\xe4\xb8\xaa\xe6\xa0\xb7\xe6\x9c\xac\xe8\x90\xbd\xe5\x85\xa5\xe7\xaa\x97\xe5\x86\x85\xe7\x9a\x84\xe6\xa6\x82\xe7\x8e\x87\n    """"""\n    f=x[1:N] # \xe5\x88\x97\xe8\xa1\xa8\xe5\x88\x87\xe7\x89\x87\n    b=0;     # \xe6\x9a\x82\xe5\xad\x98\xe6\xaf\x8f\xe6\xac\xa1\xe5\x86\x85\xe5\xbe\xaa\xe7\x8e\xaf\xe7\x9a\x84\xe5\x92\x8c\n    p= One(1000);\n    for fi in f:\n        for xj in x:\n            if abs((fi-xj)/h)<=1/2: # \xe5\x88\xa4\xe6\x96\xad\xe6\x98\xaf\xe5\x90\xa6\xe5\x9c\xa8\xe6\x96\xb9\xe7\xaa\x97\xe5\x86\x85\xe9\x83\xa8;\n                b=b+1;\n\n'"
ML_in_action/p06liziqun.py,4,"b""# -*- coding: gbk -*-\n#__author__ = 'dragonfive'\n# 22/12/2015 15:56:32\n# China:\xe5\x8c\x97\xe4\xba\xac\xe5\xb8\x82:\xe4\xb8\xad\xe5\x9b\xbd\xe7\xa7\x91\xe5\xad\xa6\xe9\x99\xa2\xe5\xa4\xa7\xe5\xad\xa6 dragonfive \n# copyright 1992-2015 dragonfive\n\n\nimport random,numpy as np\n# f(x)=x^3-5x^2-2x+3\ndef shiyingdu(x):\n    return x**3-5*x**2-2*x+3\n\n\n# \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe7\xb2\x92\xe5\xad\x90\xe7\x9a\x84\xe9\x80\x9f\xe5\xba\xa6\xe5\x92\x8c\xe4\xbd\x8d\xe7\xbd\xae \xe6\xaf\x8f\xe4\xb8\xaa\xe7\x82\xb9\xe7\x9a\x84\xe5\xb1\x80\xe9\x83\xa8\xe6\x9c\x80\xe4\xbc\x98\xe5\x92\x8c\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe5\x85\xa8\xe5\xb1\x80\xe6\x9c\x80\xe4\xbc\x98\n# num\xe4\xb8\xba\xe7\xb2\x92\xe5\xad\x90\xe4\xb8\xaa\xe6\x95\xb0 [-2,5]\ndef initial_lizi(num,lower_bound,upper_bound):\n    #\xe4\xb8\x8b\xe9\x9d\xa2\xe7\x94\x9f\xe6\x88\x90num\xe4\xb8\xaa\xe9\x9a\x8f\xe6\x9c\xba\xe6\x95\xb0;\xe5\x81\x9a\xe4\xbd\x8d\xe7\xbd\xae\xe4\xbf\xa1\xe6\x81\xaf\n    location=np.array(random.sample(range(lower_bound,upper_bound+1),num))*1.0;# \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x90\x84\xe4\xb8\xaa\xe7\xb2\x92\xe5\xad\x90\xe7\x9a\x84\xe4\xbd\x8d\xe7\xbd\xae;\n    # return location\n    # \xe4\xb8\x8b\xe9\x9d\xa2\xe7\x94\x9f\xe6\x88\x90\xe9\x80\x9f\xe5\xba\xa6\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe4\xbf\xa1\xe6\x81\xaf,\xe4\xbd\x8d\xe7\xbd\xae\xe6\x9b\xb4\xe6\x96\xb0\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe5\xa6\x82\xe6\x9e\x9c\xe8\xb6\x8a\xe7\x95\x8c\xe4\xba\x86\xef\xbc\x8c\xe6\xb3\xa8\xe6\x84\x8f\xe5\xbe\x80\xe5\x9b\x9e\xe6\x8b\x89\n    speed = np.array([0.1]*num);\n    # \xe4\xb8\x8b\xe9\x9d\xa2\xe8\xae\xa1\xe7\xae\x97\xe5\x88\x9d\xe5\xa7\x8b\xe7\x9a\x84\xe5\xb1\x80\xe9\x83\xa8\xe6\x9c\x80\xe4\xbc\x98\xe5\x92\x8c\xe5\x85\xa8\xe5\xb1\x80\xe6\x9c\x80\xe4\xbc\x98;\n    localBestY =  [shiyingdu(x) for x in location];\n    localBestX = location;\n    wholeBestY = max(localBestY);\n    wholeBestX = location[localBestY.index(wholeBestY)];\n    return location,speed,localBestX,localBestY,wholeBestX,wholeBestY\n\n# \xe4\xb8\x8b\xe9\x9d\xa2\xe6\x9b\xb4\xe6\x96\xb0\xe9\x80\x9f\xe5\xba\xa6 , \xe4\xbd\x8d\xe7\xbd\xae\xef\xbc\x8c\xe6\x9c\x80\xe4\xbc\x98\xe5\x80\xbc\n\ndef update(location,speed,localBestX,localBestY,wholeBestX,wholeBestY,opr,lower_bound,upper_bound):\n    speed=np.array([ speed[i]+0.2*(localBestX[i]-location[i])+0.2*(wholeBestX-location[i])  for i in range(0,len(speed)) ])\n    location=np.array([ location[i]+speed[i]  for i in range(0,len(speed))  ])\n    for i in range(0,len(location)):\n        if location[i]<lower_bound:\n            location[i]=lower_bound\n        elif location[i] > upper_bound:\n            location[i]=upper_bound\n\n\n    for i in range(0,len(location)):\n        if opr*shiyingdu(location[i]) > opr*localBestY[i]:\n            localBestY[i] = shiyingdu(location[i])\n        else:\n            localBestX[i] = location[i];\n    if opr == 1 and opr*wholeBestY < max(localBestY):\n        wholeBestY = max(localBestY)\n        wholeBestX = location[localBestY.index(wholeBestY)]\n    elif opr == -1 and opr*wholeBestY > min(localBestY):\n        wholeBestY = min(localBestY)\n        wholeBestX = location[localBestY.index(wholeBestY)]\n\n    return location,speed,localBestX,localBestY,wholeBestX,wholeBestY;\n\n\n\n# \xe4\xb8\x8b\xe9\x9d\xa2\xe6\x89\xa7\xe8\xa1\x8c\xe7\xa8\x8b\xe5\xba\x8f\n\nlocation,speed,localBestX,localBestY,wholeBestX,wholeBestY=initial_lizi(5,-2,5)\nfor i in range(0,100):\n    location,speed,localBestX,localBestY,wholeBestX,wholeBestY=update(location,speed,localBestX,localBestY,wholeBestX,wholeBestY,1,-2,5);\n\nprint(wholeBestX,wholeBestY);\n\nlocation,speed,localBestX,localBestY,wholeBestX,wholeBestY=initial_lizi(5,-2,5)\nfor i in range(0,100):\n    location,speed,localBestX,localBestY,wholeBestX,wholeBestY=update(location,speed,localBestX,localBestY,wholeBestX,wholeBestY,-1,-2,5);\nprint(wholeBestX,wholeBestY);\n# print(x,y,z,m,n)\n\n# print(initial_lizi(5,-2,5))\n\n\n\n"""
ML_in_action/p07gauss2dim.py,19,"b'# -*- coding: gbk -*-\n#__author__ = \'dragonfive\'\n# 22/12/2015 18:36:28\n# China:\xe5\x8c\x97\xe4\xba\xac\xe5\xb8\x82:\xe4\xb8\xad\xe5\x9b\xbd\xe7\xa7\x91\xe5\xad\xa6\xe9\x99\xa2\xe5\xa4\xa7\xe5\xad\xa6 dragonfive\n# copyright 1992-2015 dragonfive\n\n\n\nimport matplotlib.pyplot as plt,numpy as np,random\n\n# \xe7\x94\xa8\xe6\x9d\xa5\xe7\x94\x9f\xe6\x88\x90\xe9\xab\x98\xe6\x96\xaf\xe5\x88\x86\xe5\xb8\x83\xe7\x9a\x84\xe4\xba\x8c\xe7\xbb\xb4\xe7\xa9\xba\xe9\x97\xb4\xe6\x95\xb0\xe6\x8d\xae\xe7\x82\xb9,\xe5\xb9\xb6\xe4\xbf\x9d\xe5\xad\x98\xe5\x88\xb0\xe6\x96\x87\xe4\xbb\xb6gaussData.npy\xe4\xb8\xad\ndef generage_randomdata():\n    Sigma = [[1, 0], [0, 1]]\n    mu1 = [1, -1];\n    # x1, y1 = np.random.multivariate_normal(mu1, Sigma, 200).T\n    z1 = np.random.multivariate_normal(mu1, Sigma, 200)\n\n    mu2=[5.5,-4.5];\n    z2 = np.random.multivariate_normal(mu2, Sigma, 200)\n\n    mu3=[1,4];\n    z3 = np.random.multivariate_normal(mu3, Sigma, 200)\n\n    mu4=[6,4.5];\n    z4 = np.random.multivariate_normal(mu4, Sigma, 200)\n\n    mu5=[9,0.0];\n    z5 = np.random.multivariate_normal(mu5, Sigma, 200)\n\n    z=np.concatenate((z1,z2,z3,z4,z5),axis=0) # \xe5\x90\x88\xe5\xb9\xb6\xe4\xb8\x80\xe4\xba\x9bnp\xe7\x9a\x84\xe6\x95\xb0\xe7\xbb\x84\n\n    # plt.plot(z.T[0],z.T[1],""*"")\n    # plt.axis(\'equal\');\n    # plt.show()\n\n    # \xe4\xb8\x8b\xe9\x9d\xa2\xe6\x98\xaf\xe4\xbf\x9d\xe5\xad\x98\xe6\x96\xb9\xe5\xbc\x8f\n    np.savetxt(\'gaussData.txt\',z,fmt=[\'%s\']*z.shape[1],newline=\'\\n\');\n    # #\xe4\xb8\x8b\xe9\x9d\xa2\xe6\x98\xaf\xe8\xaf\xbb\xe5\x8f\x96\xe6\x96\xb9\xe5\xbc\x8f\n    # p=np.loadtxt(\'gaussData.txt\');\n\n\n# \xe8\xae\xa1\xe7\xae\x97\xe4\xb8\xa4\xe4\xb8\xaa\xe5\x90\x91\xe9\x87\x8f\xe6\xac\xa7\xe5\xbc\x8f\xe8\xb7\x9d\xe7\xa6\xbb\xe7\x9a\x84\xe6\x96\xb9\xe6\xb3\x95,\xe5\x90\x91\xe9\x87\x8f\xe6\x98\xaf\xe5\x8f\xaf\xe4\xbb\xa5\xe5\xa4\x9a\xe7\xbb\xb4\xe7\x9a\x84\ndef dist_eclud(pointA,pointB):\n    return np.sqrt(np.sum(np.power(pointA-pointB,2)))\n\n# \xe4\xb8\x8b\xe9\x9d\xa2\xe4\xb8\xba\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe4\xb8\xad\xe5\xbf\x83\ndef initial_center(DataSet,num_of_center):\n    dims = np.shape(DataSet)[1] # \xe8\x8e\xb7\xe5\xbe\x97\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe7\xbb\xb4\xe6\x95\xb0\n    centers=np.mat(np.zeros((num_of_center,dims))) # \xe5\x8f\x91\xe7\x8e\xb0python\xe4\xb8\x8ematlab\xe7\x9c\x9f\xe7\x9a\x84\xe5\xa5\xbd\xe5\x83\x8f\xe5\x95\x8a;\n\n    # \xe5\xaf\xb9\xe4\xba\x8e\xe6\xaf\x8f\xe4\xb8\xaa\xe7\xbb\xb4\xe5\xba\xa6\xe6\xb1\x82\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe7\x9a\x84\xe6\x9c\x80\xe5\xa4\xa7\xe5\x80\xbc\xe5\x92\x8c\xe6\x9c\x80\xe5\xb0\x8f\xe5\x80\xbc,\xe7\x84\xb6\xe5\x90\x8e\xe9\x9a\x8f\xe6\x9c\xba\xe8\xae\xa1\xe7\xae\x97\xe5\x90\x84\xe4\xb8\xaa\xe4\xb8\xad\xe5\xbf\x83\xe7\x82\xb9\xe7\x9a\x84\xe8\xaf\xa5\xe7\xbb\xb4\xe5\xba\xa6\xe7\x9a\x84\xe5\x80\xbc;\n    for i in range(0,dims):\n        minJ = min(DataSet[:,i])      # \xe5\x9c\xa8\xe4\xba\x8c\xe7\xbb\xb4\xe6\x95\xb0\xe7\xbb\x84\xe4\xb8\xad\xe8\x8e\xb7\xe5\xbe\x97\xe6\x9f\x90\xe4\xb8\x80\xe5\x88\x97\xe7\x9a\x84\xe6\x96\xb9\xe6\xb3\x95\n        rangeJ = float(max(DataSet[:,i])-minJ) # \xe6\xb1\x82\xe5\x8f\x98\xe5\x8c\x96\xe8\x8c\x83\xe5\x9b\xb4,\xe8\xbf\x99\xe6\xa0\xb7\xe7\x94\x9f\xe6\x88\x90\xe9\x9a\x8f\xe6\x9c\xba\xe6\x95\xb0\xe8\xbf\x98\xe5\x8f\xaf\xe4\xbb\xa5\xe9\x80\x89\xe6\x88\x91\xe4\xbb\xac\xe4\xbd\xbf\xe7\x94\xa8\xe7\x9a\x84\xe6\x95\xb4\xe6\x95\xb0;\n        centers[:,i]=minJ+rangeJ*np.random.rand(num_of_center,1) # \xe8\xbf\x99\xe9\x87\x8c\xe4\xbd\xbf\xe7\x94\xa8\xe7\x9a\x84\xe6\x98\xafnp\xe5\xba\x93\xe9\x87\x8c\xe9\x9d\xa2\xe7\x9a\x84random\xe6\x96\xb9\xe6\xb3\x95\xef\xbc\x8c\xe7\x9c\x8b\xe8\xb5\xb7\xe6\x9d\xa5\xe6\x9b\xb4\xe5\xbf\xab\xe4\xb8\x80\xe7\x82\xb9\n\n\n    return centers;\n\n# \xe4\xb8\x8b\xe9\x9d\xa2\xe6\x98\xafk\xe5\x9d\x87\xe5\x80\xbc\xe8\x81\x9a\xe7\xb1\xbb\xe7\x9a\x84\xe8\xbf\x87\xe7\xa8\x8b\ndef kMeans(dataSet,num_of_centers):\n    # \xe5\x85\x88\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe4\xb8\xad\xe5\xbf\x83\xe7\x82\xb9\xe5\x92\x8c\xe6\xaf\x8f\xe4\xb8\xaa\xe7\x82\xb9\xe7\x9a\x84\xe5\xbd\x92\xe5\xb1\x9e\xe7\xb0\x87\n    num_of_point = np.shape(dataSet)[0]\n    cluster_of_point=np.mat(np.zeros((num_of_point,2)))  #\xe5\xbf\xab\xe9\x80\x9f\xe5\x9c\xb0\xe7\x94\x9f\xe6\x88\x90\xe4\xba\x8c\xe7\xbb\xb4\xe6\x95\xb0\xe7\xbb\x84\xe7\x9a\x84\xe6\x96\xb9\xe5\xbc\x8f;\n    cluster_centers = initial_center(dataSet,num_of_centers)\n    clusterChanged = True;\n    while clusterChanged: # \xe5\xbe\xaa\xe7\x8e\xaf\xe7\x9b\xb4\xe5\x88\xb0\xe5\xbd\x92\xe5\xb1\x9e\xe4\xb8\x8d\xe5\x8f\x98\xe5\x8c\x96\n        clusterChanged = False;\n        # \xe4\xb8\x8b\xe9\x9d\xa2\xe9\x87\x8d\xe6\x96\xb0\xe8\xae\xa1\xe7\xae\x97\xe8\xb7\x9d\xe7\xa6\xbb dist_eclud\n        for i in range(num_of_point):\n            min_dist = np.inf;min_center = -1;\n            for j in range(num_of_centers):\n                dist_j = dist_eclud(dataSet[i,:],cluster_centers[j,:])\n                if dist_j<min_dist:\n                    min_dist=dist_j;min_center=j;\n            # \xe6\x9b\xb4\xe6\x96\xb0\xe5\xbd\x92\xe5\xb1\x9e\n            if min_center!=cluster_of_point[i,0]:\n                clusterChanged = True;\n            cluster_of_point[i,:]=min_center,min_dist**2\n\n\n        # \xe6\x9b\xb4\xe6\x96\xb0\xe4\xb8\xad\xe5\xbf\x83\xe7\x82\xb9\xe5\x9d\x90\xe6\xa0\x87\n        for i in range(num_of_centers):\n            point_in_clusteri = dataSet[ np.nonzero(cluster_of_point[:,0].A==i)[0] ] # \xe7\xbb\x9f\xe8\xae\xa1\xe5\xbd\x92\xe5\xb1\x9e\xe4\xb8\xbai\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89\xe6\xb4\x97\xe6\xa0\x87\xe7\xbb\x84\xe6\x88\x90\xe7\x9a\x84\n            cluster_centers[i,:] = np.mean(point_in_clusteri,axis=0);\n\n    return cluster_centers,cluster_of_point\n\n\n\nif __name__==""__main__"":\n\n    # generage_randomdata()\n    # print()\n    dataset = np.loadtxt(\'gaussData.txt\')\n    centers , clusters=kMeans(dataset,5)\n    # print(centers)\n    # print(clusters)\n    plt.plot(centers[:,0],centers[:,1],\'x\')\n\n    biaozhi={0.0:\'b*\',1.0:\'m+\',2.0:\'rx\',3.0:\'go\',4.0:\'y<\'}\n    for i in range(dataset.shape[0]):\n        plt.plot(dataset[i,0],dataset[i,1],biaozhi.get(clusters[i,0]))\n\n    plt.show()'"
ML_in_action/p08spectualRefl.py,8,"b'# -*- coding: gbk -*-\n#__author__ = \'dragonfive\'\n# 24/12/2015 11:05:12\n# China:\xe5\x8c\x97\xe4\xba\xac\xe5\xb8\x82:\xe4\xb8\xad\xe5\x9b\xbd\xe7\xa7\x91\xe5\xad\xa6\xe9\x99\xa2\xe5\xa4\xa7\xe5\xad\xa6 dragonfive \n# copyright 1992-2015 dragonfive\n\n#coding=utf-8\n#MSC means Multiple Spectral Clustering\nimport numpy as np\nimport scipy as sp\nimport scipy.linalg as linalg\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n\ndef getNormLaplacian(W):\n\t""""""input matrix W=(w_ij)\n\t""compute D=diag(d1,...dn)\n\t""and L=D-W\n\t""and Lbar=D^(-1/2)LD^(-1/2)\n\t""return Lbar\n\t""""""\n\td=[np.sum(row) for row in W]\n\tD=np.diag(d)\n\tL=D-W\n\t#Dn=D^(-1/2)\n\tDn=np.power(np.linalg.matrix_power(D,-1),0.5)\n\tLbar=np.dot(np.dot(Dn,L),Dn)\n\treturn Lbar\n\ndef getKSmallestEigVec(Lbar,k):\n\t""""""input\n\t""matrix Lbar and k\n\t""return\n\t""k smallest eigen values and their corresponding eigen vectors\n\t""""""\n\teigval,eigvec=linalg.eig(Lbar)\n\tdim=len(eigval)\n\n\t#\xe6\x9f\xa5\xe6\x89\xbe\xe5\x89\x8dk\xe5\xb0\x8f\xe7\x9a\x84eigval\n\tdictEigval=dict(zip(eigval,range(0,dim)))\n\tkEig=np.sort(eigval)[0:k]\n\tix=[dictEigval[k] for k in kEig]\n\treturn eigval[ix],eigvec[:,ix]\n\ndef checkResult(Lbar,eigvec,eigval,k):\n\t""""""\n\t""input\n\t""matrix Lbar and k eig values and k eig vectors\n\t""print norm(Lbar*eigvec[:,i]-lamda[i]*eigvec[:,i])\n\t""""""\n\tcheck=[np.dot(Lbar,eigvec[:,i])-eigval[i]*eigvec[:,i] for i in range(0,k)]\n\tlength=[np.linalg.norm(e) for e in check]/np.spacing(1)\n\tprint(""Lbar*v-lamda*v are %s*%s"" % (length,np.spacing(1)))\n\ng=nx.karate_club_graph()\nnodeNum=len(g.nodes())\nm=nx.to_numpy_matrix(g)\nLbar=getNormLaplacian(m)\nk=2\nkEigVal,kEigVec=getKSmallestEigVec(Lbar,k)\nprint(""k eig val are %s"" % kEigVal)\nprint(""k eig vec are %s"" % kEigVec)\ncheckResult(Lbar,kEigVec,kEigVal,k)\n\n#\xe8\xb7\xb3\xe8\xbf\x87k means\xef\xbc\x8c\xe7\x94\xa8\xe6\x9c\x80\xe7\xae\x80\xe5\x8d\x95\xe7\x9a\x84\xe7\xac\xa6\xe5\x8f\xb7\xe5\x88\xa4\xe5\x88\xab\xe7\x9a\x84\xe6\x96\xb9\xe6\xb3\x95\xe6\x9d\xa5\xe6\xb1\x82\xe7\x82\xb9\xe7\x9a\x84\xe5\xbd\x92\xe5\xb1\x9e\n\nclusterA=[i for i in range(0,nodeNum) if kEigVec[i,1]>0]\nclusterB=[i for i in range(0,nodeNum) if kEigVec[i,1]<0]\n\n#draw graph\ncolList=dict.fromkeys(g.nodes())\nfor node,score in colList.items():\n\tif node in clusterA:\n\t\tcolList[node]=0\n\telse:\n\t\tcolList[node]=0.6\nplt.figure(figsize=(8,8))\npos=nx.spring_layout(g)\nnx.draw_networkx_edges(g,pos,alpha=0.4)\nnx.draw_networkx_nodes(g,pos,nodelist=colList.keys(),\n\t\tnode_color=colList.values(),\n\t\tcmap=plt.cm.Reds_r)\nnx.draw_networkx_labels(g,pos,font_size=10,font_family=\'sans-serif\')\nplt.axis(\'off\')\nplt.title(""karate_club spectral clustering"")\nplt.savefig(""spectral_clustering_result.png"")\nplt.show()\n\n\n\n'"
ML_in_action/p09mySpectral.py,18,"b'# -*- coding: gbk -*-\n#__author__ = \'dragonfive\'\n# 25/12/2015 10:57:07\n# China:\xe5\x8c\x97\xe4\xba\xac\xe5\xb8\x82:\xe7\xa7\x91\xe6\x8a\x80\xe7\xbd\x91 dragonfive\n# any question mail\xef\xbc\x9adragonfive1992@gmail.com\n# copyright 1992-2015 dragonfive \n\nfrom p07gauss2dim import initial_center,kMeans\nimport random,numpy as np,networkx as nx,matplotlib.pyplot as plt\n\n# \xe5\x85\x88\xe8\xae\xa1\xe7\xae\x97\xe7\xa9\xba\xe9\x97\xb4\xe7\x82\xb9\xe7\x9a\x84\xe6\xac\xa7\xe5\xbc\x8f\xe8\xb7\x9d\xe7\xa6\xbb\ndef dist_eclud(pointA,pointB):\n    return np.sqrt(sum(np.power(pointA-pointB,2)))\n\n# \xe6\xa0\xb9\xe6\x8d\xae\xe9\x98\x88\xe5\x80\xbc\xe8\xae\xbe\xe7\xbd\xae\xe8\xbf\x94\xe5\x9b\x9e\xe5\x80\xbc\xef\xbc\x8c\xe5\xa4\xa7\xe4\xba\x8e\xe9\x98\x88\xe5\x80\xbc\xe4\xb8\xba0\xef\xbc\x8c\xe5\xb0\x8f\xe4\xba\x8e\xe9\x98\x88\xe5\x80\xbc\xe8\xae\xa1\xe7\xae\x97\xe4\xba\xb2\xe5\x92\x8c\xe5\xba\xa6\ndef compute_qinhedu(dis,threshold,sigma):\n    if dis>threshold or dis==0:\n        return 0\n    else:\n        return np.exp(-1*np.power(dis,2)/(2*np.power(sigma,2)))\n\n#\xe6\x89\xbe\xe5\x88\xb0\xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaa\xe8\xa1\x8c\xe7\x9a\x84\xe5\x89\x8dk\xe4\xb8\xaa\xe8\xae\xa1\xe7\xae\x97\xe4\xba\xb2\xe5\x92\x8c\xe5\xba\xa6\xef\xbc\x8c\xe5\x85\xb6\xe5\xae\x83\xe7\x9a\x84\xe8\xb5\x8b\xe5\x80\xbc\xe4\xb8\xba0,\xe8\xbf\x94\xe5\x9b\x9e\xe4\xba\xb2\xe5\x92\x8c\xe5\xba\xa6\xe7\x9f\xa9\xe9\x98\xb5\ndef get_qinhemat(dist_mat,k,sigma):\n    num_point = np.shape(dist_mat)[0]\n    qinhedu_mat = np.mat(np.zeros((num_point,num_point)))\n    for index_dist in range(num_point):\n        # \xe5\x85\x88\xe6\x89\xbe\xe5\x88\xb0\xe7\xac\xack\xe5\xb0\x8f\xe7\x9a\x84\xe8\xb7\x9d\xe7\xa6\xbb;\xe5\x85\x88\xe4\xbb\x8e\xe5\xb0\x8f\xe5\x88\xb0\xe5\xa4\xa7\xe6\x8e\x92\xe5\xba\x8f\xe6\x89\xbe\xe5\x88\xb0\xe7\xac\xack\xe4\xb8\xaa\xe5\x80\xbc\n        temp_vector=dist_mat[index_dist,:]\n        threshold_value = np.sort(temp_vector)[0,k]\n        # \xe6\xaf\x94\xe9\x98\x88\xe5\x80\xbc\xe5\xa4\xa7\xe7\x9a\x84\xe9\x83\xbd\xe8\xae\xbe\xe7\xbd\xae\xe4\xb8\xba0;\xe4\xb8\x8e\xe8\x87\xaa\xe5\xb7\xb1\xe7\x9a\x84\xe4\xba\xb2\xe5\x92\x8c\xe5\xba\xa6\xe8\xae\xbe\xe4\xb8\xba0;\xe8\xae\xa1\xe7\xae\x97\xe5\x89\x8dk\xe4\xb8\xaa\xe7\x9a\x84\xe4\xba\xb2\xe5\x92\x8c\xe5\xba\xa6\n        qinhedu_mat[index_dist]=np.mat([ compute_qinhedu(dist_mat[index_dist,dist_pointi],threshold_value,sigma) for dist_pointi in range(num_point)])\n\n    return qinhedu_mat\n\n# \xe4\xb8\x8b\xe9\x9d\xa2\xe8\xae\xa1\xe7\xae\x97\xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\xe7\x9a\x84\xe6\x8b\x89\xe6\x99\xae\xe6\x8b\x89\xe6\x96\xaf\xe7\x9f\xa9\xe9\x98\xb5\ndef get_normal_lapalase(qinhedu_mat):\n    # \xe9\xa6\x96\xe5\x85\x88\xe8\x8e\xb7\xe5\xbe\x97\xe5\xba\xa6\xe5\x90\x91\xe9\x87\x8f\n    num_point = np.shape(qinhedu_mat)[0]\n    du_vec = [np.sum(dist_vec) for dist_vec in qinhedu_mat]\n    du_mat = np.diag(du_vec)\n    laplase_mat = dist_mat - qinhedu_mat\n    #dn=du_mat^(-1/2)\n    dn = np.power(np.linalg.matrix_power(du_mat,-1),0.5) # \xe9\x87\x8c\xe9\x9d\xa2\xe7\x9a\x84\xe6\xb1\x82\xe9\x80\x86\xef\xbc\x9fdist_mat\n    normal_lap = np.dot(np.dot(dn,laplase_mat),dn)\n    return normal_lap\n\n\n# \xe8\xae\xa1\xe7\xae\x97\xe7\x9f\xa9\xe9\x98\xb5\xe9\x87\x8ck\xe4\xb8\xaa\xe6\x9c\x80\xe5\xb0\x8f\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe5\x80\xbc\xe5\x92\x8c\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe5\x90\x91\xe9\x87\x8f\ndef getKSmallestEigVec(normal_lap,k):\n\teigval,eigvec=np.linalg.eig(normal_lap)   #\xe6\xb1\x82\xe4\xb8\x80\xe4\xb8\xaa\xe7\x9f\xa9\xe9\x98\xb5\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe5\x80\xbc\xe5\x92\x8c\xe7\x89\xb9\xe5\xbe\x81\xe5\x90\x91\xe9\x87\x8f\n\tdim=len(eigval)\n\n\t#\xe6\x9f\xa5\xe6\x89\xbe\xe5\x89\x8dk\xe5\xb0\x8f\xe7\x9a\x84eigval \xe8\xbf\x99\xe9\x87\x8c\xe7\x9a\x84\xe6\x96\xb9\xe6\xb3\x95\xe6\x98\xaf\xe5\x85\x88\xe7\x94\x9f\xe6\x88\x90\xe4\xb8\x80\xe4\xb8\xaa\xe5\x85\xb7\xe6\x9c\x89\xe6\xaf\x94\xe8\xbe\x83\xe5\xaf\xb9\xe8\xb1\xa1\xe5\x92\x8c\xe5\xbe\x85\xe6\x89\xbe\xe5\x85\x83\xe7\xb4\xa0\xe7\x9a\x84\xe5\xad\x97\xe5\x85\xb8\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe5\xaf\xb9\xe5\xaf\xb9\xe6\xaf\x94\xe8\xbe\x83\xe5\x85\x83\xe7\xb4\xa0\xe6\x8e\x92\xe5\xba\x8f\xef\xbc\x8c\xe4\xbb\x8e\xe5\xad\x97\xe5\x85\xb8\xe4\xb8\xad\xe6\x89\xbe\xe5\x88\xb0\xe5\x89\x8dk\xe4\xb8\xaa\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe5\xba\x8f\xe5\x8f\xb7\xe3\x80\x82\n    #np.sort()\xe8\xbf\x99\xe4\xb8\xaa\xe6\x96\xb9\xe6\xb3\x95\xe4\xb8\x8d\xe6\x98\xaf\xe5\x9c\xa8\xe6\x9c\xac\xe5\x9c\xb0\xe6\x8e\x92\xe5\xba\x8f\xe7\x9a\x84,\xe8\x80\x8c\xe6\x98\xaf\xe6\x8a\x8a\xe6\x8e\x92\xe5\xba\x8f\xe7\xbb\x93\xe6\x9e\x9c\xe8\xbf\x94\xe5\x9b\x9e\n    # zip()\xe5\x87\xbd\xe6\x95\xb0 \xe8\xbf\x94\xe5\x9b\x9e\xe4\xb8\x80\xe4\xb8\xaa\xe5\x85\x83\xe7\xbb\x84\xe7\x9a\x84\xe9\x9b\x86\xe5\x90\x88 \xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaa\xe5\x85\x83\xe7\xbb\x84\xe4\xbb\x8e\xe5\x8f\x82\xe6\x95\xb0\xe4\xb8\xad\xe7\x9a\x84\xe5\x90\x84\xe4\xb8\xaa\xe9\x9b\x86\xe5\x90\x88\xe6\x8c\x89\xe4\xb8\x8b\xe6\xa0\x87\xe5\xba\x8f\xe5\x8f\xb7\xe6\x8a\xbd\xe5\x8f\x96;\n    # \xe8\xbf\x99\xe4\xb8\xaa\xe5\xad\x97\xe5\x85\xb8\xe5\x88\x9b\xe5\xbb\xba\xe7\x9a\x84\xe6\x96\xb9\xe5\xbc\x8f\xe5\xbe\x88\xe7\xbb\x8f\xe5\x85\xb8\n\tdictEigval=dict(zip(eigval,range(0,dim)))\n\tkEig=np.sort(eigval)[0:k]\n\tix=[dictEigval[k] for k in kEig]\n\treturn eigval[ix],eigvec[:,ix]   # \xe7\x89\xb9\xe5\xbe\x81\xe5\x90\x91\xe9\x87\x8f\xe6\x98\xaf\xe6\x8c\x89\xe5\x88\x97\xe5\xad\x98\xe6\x94\xbe\xe7\x9a\x84\n\nif __name__==""__main__"":\n\n    # \xe9\xa6\x96\xe5\x85\x88\xe8\xaf\xbb\xe5\x8f\x96\xe5\x8e\x9f\xe5\xa7\x8b\xe6\x95\xb0\xe6\x8d\xae\n    samples = np.loadtxt(\'sample.txt\')\n    # print(samples)\n    # print(np.shape(samples))\n\n    # \xe5\xbb\xba\xe7\xab\x8b\xe9\x82\xbb\xe6\x8e\xa5\xe7\x9f\xa9\xe9\x98\xb5\n    num_point = np.shape(samples)[0]\n    dist_mat = np.mat(np.zeros((num_point,num_point)))\n    # \xe8\xae\xa1\xe7\xae\x97\xe6\xaf\x8f\xe4\xb8\xaa\xe7\x82\xb9\xe4\xb8\x8e\xe5\x85\xb6\xe4\xbb\x96\xe6\x89\x80\xe6\x9c\x89\xe7\x82\xb9\xe7\x9a\x84\xe8\xb7\x9d\xe7\xa6\xbb\n    for index_pointA in range(num_point):\n        dist_mat[index_pointA,:]=[dist_eclud(samples[index_pointA],pointB) for pointB in samples]\n    # print(dist_mat[1:6,1:6])\n    # dist_mat = (dist_mat+dist_mat.T)/2\n    #\n    # print(dist_mat[1:6,1:6])\n    qinhedu_mat = get_qinhemat(dist_mat,70,35)\n\n    qinhedu_mat = (qinhedu_mat+qinhedu_mat.T)/2\n\n    norm_lap = get_normal_lapalase(qinhedu_mat)\n    keigval,keigvec=getKSmallestEigVec(norm_lap,4)\n\n    centers , clusters=kMeans(keigvec,2)\n\n\n    biaozhi={0.0:\'cx--\',1.0:\'mo:\'}\n    for i in range(samples.shape[0]):\n        plt.plot(samples[i,0],samples[i,1],biaozhi.get(clusters[i,0]))\n\n    plt.show()\n\n\n    # clusterA=[i for i in range(0,num_point) if keigvec[i,0]>0] #\xe5\xb1\x9e\xe4\xba\x8e\xe7\xac\xac\xe4\xb8\x80\xe7\xb1\xbb\xe7\x9a\x84\xe4\xb8\x8b\xe6\xa0\x87\xe9\x9b\x86\xe5\x90\x88\n    # clusterB=[i for i in range(0,num_point) if keigvec[i,0]<0]\n    # print(keigvec[:,0])\n    # print(keigvec[:,1])\n    #\n    # plt.plot(samples[clusterA,0],samples[clusterA,1],\'*\')\n    # plt.plot(samples[clusterB,0],samples[clusterB,1],\'x\')\n    # plt.axis(\'equal\')\n    # plt.show()\n'"
ML_in_action/p10testForsklearn.py,4,"b""# -*- coding: gbk -*-\n#__author__ = 'dragonfive'\n# 27/12/2015 22:37:38\n# China:\xe5\x8c\x97\xe4\xba\xac\xe5\xb8\x82:\xe4\xb8\xad\xe5\x9b\xbd\xe7\xa7\x91\xe5\xad\xa6\xe9\x99\xa2\xe5\xa4\xa7\xe5\xad\xa6 dragonfive \n# any question mail\xef\xbc\x9adragonfive1992@gmail.com \n# copyright 1992-2015 dragonfive\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.feature_extraction import image\nfrom sklearn.cluster import spectral_clustering, KMeans\n\n\n\n\n# \xe4\xb8\x8b\xe9\x9d\xa2\xe7\x94\xa8sklearn\xe5\xba\x93\xe5\x81\x9ak\xe5\x9d\x87\xe5\x80\xbc\nfrom p09mySpectral import dist_eclud, get_qinhemat\n\ndef sklearn_kmeans():\n    dataset = np.loadtxt('gaussData.txt')\n    lables = KMeans(5).fit_predict(dataset);\n\n    plt.scatter(dataset[:,0],dataset[:,1],c=lables)\n    plt.axis('equal')\n    plt.show()\n\n# \xe4\xb8\x8b\xe9\x9d\xa2\xe7\x94\xa8sklearn\xe5\xba\x93\xe5\x81\x9a\xe8\xb0\xb1\xe8\x81\x9a\xe7\xb1\xbb\ndef sklearn_spectual():\n    samples = np.loadtxt('sample.txt')\n    # \xe5\xbb\xba\xe7\xab\x8b\xe9\x82\xbb\xe6\x8e\xa5\xe7\x9f\xa9\xe9\x98\xb5\n    num_point = np.shape(samples)[0]\n    dist_mat = np.mat(np.zeros((num_point,num_point)))\n    # \xe8\xae\xa1\xe7\xae\x97\xe6\xaf\x8f\xe4\xb8\xaa\xe7\x82\xb9\xe4\xb8\x8e\xe5\x85\xb6\xe4\xbb\x96\xe6\x89\x80\xe6\x9c\x89\xe7\x82\xb9\xe7\x9a\x84\xe8\xb7\x9d\xe7\xa6\xbb\n    for index_pointA in range(num_point):\n        dist_mat[index_pointA,:]=[dist_eclud(samples[index_pointA],pointB) for pointB in samples]\n\n    qinhedu_mat = get_qinhemat(dist_mat,8,2)\n\n    qinhedu_mat = (qinhedu_mat+qinhedu_mat.T)/2\n    lables=spectral_clustering(qinhedu_mat,2)\n\n    clusterA=[i for i in range(0,num_point) if lables[i]==1] #\xe5\xb1\x9e\xe4\xba\x8e\xe7\xac\xac\xe4\xb8\x80\xe7\xb1\xbb\xe7\x9a\x84\xe4\xb8\x8b\xe6\xa0\x87\xe9\x9b\x86\xe5\x90\x88\n    clusterB=[i for i in range(0,num_point) if lables[i]==0]\n    plt.plot(samples[clusterA,0],samples[clusterA,1],'cx')\n    plt.plot(samples[clusterB,0],samples[clusterB,1],'mo')\n    plt.axis('equal')\n    plt.show()\n\n\nif __name__ == '__main__':\n    sklearn_spectual();\n\n\n\n"""
ML_in_action/p13dezhouPuke.py,12,"b""# -*- coding: gbk -*-\n#__author__ = 'dragonfive'\n# 06/01/2016 13:12:25\n# China:\xe5\x8c\x97\xe4\xba\xac\xe5\xb8\x82:\xe4\xb8\xad\xe5\x9b\xbd\xe7\xa7\x91\xe5\xad\xa6\xe9\x99\xa2\xe5\xa4\xa7\xe5\xad\xa6 dragonfive \n# any question mail\xef\xbc\x9adragonfive2013@gmail.com\n# copyright 1992-2016 dragonfive \n\n# \xe4\xb8\x8b\xe9\x9d\xa2\xe7\x9a\x84\xe5\x87\xbd\xe6\x95\xb0\xe4\xba\xa7\xe7\x94\x9f5\xe5\xbc\xa0\xe7\x89\x8c\xe5\x8f\x8a\xe5\x85\xb6\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x844\xe4\xb8\xaa\xe8\x8a\xb1\xe8\x89\xb2\nimport random as rd\nimport numpy as np\ndef getPointAndColor(num):\n    cards = rd.sample(range(1,53),num);\n    points = np.sort([i%13+1 for i in cards])[::-1] # \xe6\xaf\x8f\xe5\xbc\xa0\xe7\x89\x8c\xe5\x8f\xaa\xe5\x87\xba\xe7\x8e\xb0\xe4\xb8\x80\xe6\xac\xa1\n    colors = ['brmf'[(i-1)/13] for i in cards]  # \xe6\xaf\x8f\xe4\xb8\xaa\xe7\x89\x8c\xe6\x9c\x89\xe4\xb8\x80\xe4\xb8\xaa\xe8\x8a\xb1\xe8\x89\xb2\n    myDtype = [('point',int),('color',str,1)]\n    pAndC = [tuple((points[i],colors[i])) for i in range(num)]\n    return np.array(pAndC,myDtype)\n\n\n# \xe5\xbe\x97\xe5\x88\x86\xe8\xa7\x84\xe5\x88\x99\xe6\x98\xafhttp://news.tongbu.com/44701.html\n# 1.\xe7\x9a\x87\xe5\xae\xb6\xe5\x90\x8c\xe8\x8a\xb1\xe9\xa1\xba\n# 2.\xe5\x90\x8c\xe8\x8a\xb1\xe9\xa1\xba\n# 3.\xe5\x9b\x9b\xe6\x9d\xa1\n# 4.\xe8\x91\xab\xe8\x8a\xa6\n# 5.\xe5\x90\x8c\xe8\x8a\xb1\n# 6.\xe9\xa1\xba\xe5\xad\x90\n# 7.\xe4\xb8\x89\xe6\x9d\xa1\n# 8.\xe4\xb8\xa4\xe5\xaf\xb9\n# 9.\xe5\xaf\xb9\xe5\xad\x90\n# 10.\xe6\x9d\x82\xe7\x89\x8c\ndef tiaozi(player):\n    points = [player[i][0] for i in range(player.size)]\n    dui = np.unique(points,return_counts=True)\n    # \xe6\x8c\x89\xe7\x85\xa7\xe5\x87\xba\xe7\x8e\xb0\xe6\xac\xa1\xe6\x95\xb0\xe5\xaf\xb9\xe4\xb8\x8b\xe6\xa0\x87\xe6\x8e\x92\xe5\xba\x8f;\xe5\x8f\x96\xe5\x87\xba\xe6\x8e\x92\xe5\x90\x8d\xe5\x9c\xa8\xe5\x89\x8d\xe4\xb8\xa4\xe4\xb8\xaa\xe7\x9a\x84\xe4\xb8\x8b\xe6\xa0\x87;\n    CountAndIndex=np.array([tuple((dui[0][i],dui[1][i])) for i in range(np.size(dui[1]))],dtype=[('points',int),('count',int)])\n    CountAndIndex.sort(order='count')\n    # print(CountAndIndex)\n    maxDui = CountAndIndex[np.size(dui[1])-1][1] #\xe6\x9c\x80\xe5\xa4\xa7\xe7\x9a\x84\xe4\xb8\xaa\xe6\x95\xb0\n    secondDui = CountAndIndex[np.size(dui[1])-2][1] #\xe6\xac\xa1\xe5\xa4\xa7\xe7\x9a\x84\xe4\xb8\xaa\xe6\x95\xb0\n    if maxDui == 1: # \xe4\xb8\x8d\xe6\x98\xaf\xe4\xb8\x80\xe5\xaf\xb9\n        return 10\n    elif maxDui == 2 and secondDui == 2:#\xe8\xa1\xa8\xe7\xa4\xba\xe6\x98\xaf\xe4\xb8\xa4\xe5\xaf\xb9\n        return 8\n    elif maxDui == 2: #\xe8\xa1\xa8\xe7\xa4\xba\xe6\x98\xaf\xe4\xb8\x80\xe5\xaf\xb9\n        return 9\n    elif maxDui == 3 and secondDui == 2 : # \xe8\xa1\xa8\xe7\xa4\xba\xe8\x91\xab\xe8\x8a\xa6\n        return 7\n    elif maxDui == 3: #\xe8\xa1\xa8\xe7\xa4\xba3\xe6\x9d\xa1\n        return 3\n    elif maxDui == 4: # \xe8\xa1\xa8\xe7\xa4\xba4\xe6\x9d\xa1\n        return 3\n\n# \xe5\x88\xa4\xe6\x96\xad\xe6\x98\xaf\xe4\xb8\x8d\xe6\x98\xaf\xe9\xa1\xba\xe5\xad\x90\ndef shunzi(player):\n    points = np.sort([player[i][0] for i in range(player.size)])[::-1]\n    maxp = points[0]\n    minp = points[-1]\n    if minp==1 and points[-2]==10 and tonghua(player): # \xe9\xab\x98\xe7\xba\xa7\xe5\x90\x8c\xe8\x8a\xb1\xe9\xa1\xba\n        return 1\n    elif ((maxp-minp==4) or minp==1 and points[-2]==10)  and tonghua(player): # \xe5\x90\x8c\xe8\x8a\xb1\xe9\xa1\xba\n        return 2\n    elif(maxp-minp==4) or minp==1 and points[-2]==10:\n        return 6\n    else:\n        return 10\n\ndef tonghua(player):\n    colors = [player[i][1] for i in range(player.size)]\n    if np.unique(colors).size == 1:\n        return 5\n    else:\n        return 10\n\n# \xe4\xbb\xa5\xe9\xab\x98\xe7\x89\x8c\xe7\x9a\x84\xe6\x96\xb9\xe5\xbc\x8f\xe8\x8e\xb7\xe5\xbe\x97\xe6\x89\x80\xe6\x9c\x89\xe7\x89\x8c\xe7\x9a\x84\xe7\x82\xb9\xe6\x95\xb0\xe5\x92\x8c\ndef getAllpoints(player):\n    # \xe5\x85\x88\xe8\x8e\xb7\xe5\xbe\x97\xe7\x82\xb9\xe6\x95\xb0\xe4\xbb\x8e\xe5\xa4\xa7\xe5\x88\xb0\xe5\xb0\x8f\xe7\x9a\x84\xe6\x8e\x92\xe5\x88\x97,\xe7\x84\xb6\xe5\x90\x8e\xe8\xbf\x94\xe5\x9b\x9e\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe6\x95\xb4\xe6\x95\xb0\n    return int(''.join([str(player[i][0]) for i in range(player.size)]))\n\n# \xe8\x8e\xb7\xe5\xbe\x97\xe7\x89\x8c\xe5\x9e\x8b\ndef getPukeType(player):\n    typeShun = shunzi(player) # \xe5\x85\x88\xe8\xae\xa4\xe4\xb8\xba\xe6\x98\xaf\xe6\x9d\x82\xe7\x89\x8c\n    typeTiao = tiaozi(player)\n    typeTong = tonghua(player)\n\n    if typeTiao<=2:\n        return typeTiao\n    elif typeTiao<=4:\n        return typeTiao\n    elif typeTong==5:\n        return typeTong\n    elif typeShun == 6:\n        return typeShun\n    else:\n        return typeTiao\n\n# \xe8\x8e\xb7\xe5\xbe\x97\xe5\xaf\xb9\xe5\xad\x90\xe7\x9a\x84\xe6\x8e\x92\xe5\xba\x8f\ndef getDuiziSeq(player):\n    points = [player[i][0] for i in range(player.size)]\n    dui = np.unique(points,return_counts=True)\n    # \xe6\x8c\x89\xe7\x85\xa7\xe5\x87\xba\xe7\x8e\xb0\xe6\xac\xa1\xe6\x95\xb0\xe5\xaf\xb9\xe4\xb8\x8b\xe6\xa0\x87\xe6\x8e\x92\xe5\xba\x8f;\xe5\x8f\x96\xe5\x87\xba\xe6\x8e\x92\xe5\x90\x8d\xe5\x9c\xa8\xe5\x89\x8d\xe4\xb8\xa4\xe4\xb8\xaa\xe7\x9a\x84\xe4\xb8\x8b\xe6\xa0\x87;\n    CountAndIndex=np.array([tuple((dui[0][i],dui[1][i])) for i in range(np.size(dui[1]))],dtype=[('points',int),('count',int)])\n    CountAndIndex.sort(order=['count','points'])\n\n    return  int(''.join(map(str,CountAndIndex[:][0])))\n#\xe6\xaf\x94\xe7\x82\xb9\xe6\x95\xb0\ndef comparePoints(player1,player2,type):\n    points1 = np.sort([player1[i][0] for i in range(player1.size)])[::-1]\n    points2 = np.sort([player2[i][0] for i in range(player2.size)])[::-1]\n    if type==2 or type==6: #\xe4\xb8\xa4\xe4\xb8\xaa\xe9\xa1\xba\xe5\xad\x90\xe6\xaf\x94\xe8\xbe\x83\xe6\xac\xa1\xe5\xa4\xa7\xe5\x80\xbc\n        return points2[1]-points1[1] # \xe5\xb0\x8f\xe4\xba\x8e0\xe5\x88\x991\xe8\x83\x9c\xe5\x87\xba\n    if type == 5 or type==10:#\xe5\x90\x8c\xe8\x8a\xb1\xe5\x92\x8c\xe6\x9d\x82\xe7\x89\x8c\xe6\xaf\x94\n        return getAllpoints(player2)-getAllpoints(player1)\n    else: # \xe5\x89\xa9\xe4\xb8\x8b\xe7\x9a\x84\xe6\x98\xaf\xe4\xb8\x80\xe5\xaf\xb9 \xe4\xba\x8c\xe5\xaf\xb9 \xe4\xb8\x89\xe6\x9d\xa1 \xe8\x91\xab\xe8\x8a\xa6\xe5\x92\x8c\xe5\x9b\x9b\xe6\x9d\xa1 \xe5\x81\x9a\xe6\xb3\x95\xe6\x8c\x89\xe5\xaf\xb9\xe6\x95\xb0\xe5\x92\x8c\xe7\x82\xb9\xe6\x95\xb0\xe6\x8e\x92\xe5\xba\x8f\n        seq1 = getDuiziSeq(player1)\n        seq2 = getDuiziSeq(player2)\n        return seq2-seq1\n\n    # return 0;\n\n\n# \xe4\xb8\x8b\xe9\x9d\xa2\xe6\xaf\x94\xe8\xbe\x83\xe4\xb8\xa4\xe4\xba\xba\xe7\x9a\x84\xe7\x89\x8c\ndef comparePlayer(player1,plaer2):\n    # \xe5\x85\x88\xe6\xaf\x94\xe7\x89\x8c\xe5\x9e\x8b\n    type1 = getPukeType(player1)\n    type2 = getPukeType(plaer2)\n    if type1 != type2:\n        return type1-type2 # \xe5\xb0\x8f\xe4\xba\x8e0\xe5\x88\x991\xe8\x83\x9c\xe5\x87\xba\n    else:\n        return comparePoints(player1,plaer2,type2)\n\nif __name__ == '__main__':\n    # \xe8\x8e\xb7\xe5\xbe\x97\xe4\xb8\xa4\xe4\xb8\xaa\xe7\x8e\xa9\xe5\xae\xb6\xe7\x9a\x84\xe7\x89\x8c;\n    playerA = getPointAndColor(5)\n    playerB = getPointAndColor(5)\n    print(playerA,playerB,comparePlayer(playerA,playerB))\n\n\n"""
ML_in_action/p14kNN.py,8,"b""# -*- coding: gbk -*-\n#__author__ = 'dragonfive'\n# 24/01/2016 21:49:28\n# China:\xe5\x8c\x97\xe4\xba\xac\xe5\xb8\x82:\xe7\xa7\x91\xe6\x8a\x80\xe7\xbd\x91 dragonfive \n# any question mail\xef\xbc\x9adragonfive1992@gmail.com \n# copyright 1992-2016 dragonfive\n\n# \xe5\xb0\x86\xe6\x96\x87\xe4\xbb\xb6\xe8\xae\xb0\xe5\xbd\x95\xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xbanumpy\xe7\x9a\x84\xe8\xa7\xa3\xe6\x9e\x90\xe7\xa8\x8b\xe5\xba\x8f\n\n\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport operator as op\n\n\n# \xe5\xaf\xb9\xe6\xb5\xb7\xe4\xbc\xa6\xe7\x9a\x84\xe5\x85\xb4\xe8\xb6\xa3\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\x86\xe7\xb1\xbb\ndef classify0(inX,dataSet,labels,k):\n    dataSetSize = dataSet.shape[0]\n    diffMat = dataSet-np.tile(inX,(dataSetSize,1))\n    sqDiffMat=diffMat**2\n    distances = sqDiffMat.sum(axis=1)**0.5\n    sortedDistIndex = distances.argsort()\n    classCount={}\n    for i in range(k):\n        voteIlabel = labels[sortedDistIndex[i]]\n        classCount[voteIlabel]=classCount.get(voteIlabel,0)+1\n    sortedClassCount = sorted(classCount.iteritems(),key=op.itemgetter(1),reverse=True)\n    return sortedClassCount[0][0]\n\n\n# \xe4\xbb\x8e\xe6\x96\x87\xe4\xbb\xb6\xe4\xb8\xad\xe8\xaf\xbb\xe5\x87\xba\xe6\x95\xb0\xe6\x8d\xae\xe6\x9d\xa5\ndef file2matrix(filename):\n    returnMat = np.array(np.loadtxt(filename,usecols={0,1,2}));\n    classLabelVector = np.array(np.loadtxt(filename,usecols={3}));\n    return returnMat,classLabelVector\n\n# datingDataMat,datingLabels = file2matrix('datingTestSet2.txt')\n# fig = plt.figure()\n# ax=fig.add_subplot(111)\n# ax.scatter(datingDataMat[:,0],datingDataMat[:,1],15.0*np.array(datingLabels),15.0*np.array(datingLabels))\n# plt.show()\n\n\n\n# \xe5\xaf\xb9\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe8\xbf\x9b\xe8\xa1\x8c\xe5\xbd\x92\xe4\xb8\x80\xe5\x8c\x96\ndef autoNorm(dataSet):\n    # dataSet = np.zeros((1,1))\n    minVals = dataSet.min(0)\n    maxVals = dataSet.max(0)\n    ranges = maxVals-minVals\n    normDataSet = np.zeros(dataSet.shape)\n    normDataSet = dataSet - np.tile(minVals,(dataSet.shape[0],1))\n    normDataSet = normDataSet/np.tile(ranges,(dataSet.shape[0],1))\n    return normDataSet,ranges,minVals\n\n\n\n# normMat,ranges,minVals = autoNorm(datingDataMat)\n# print(normMat)\n\n\n# \xe7\x94\xa8\xe6\xb5\x8b\xe8\xaf\x95\xe9\x9b\x86\xe8\xbf\x9b\xe8\xa1\x8c\xe6\xb5\x8b\xe8\xaf\x95\ndef datingClassTest():\n    hoRatio = 0.10\n    datingDataMat,datingLabels = file2matrix('datingTestSet2.txt')\n    normMat,ranges,minVals = autoNorm(datingDataMat)\n    m = normMat.shape[0]\n    numTestVecs = int(m*hoRatio)\n    errorCount = 0.0\n    for i in range(numTestVecs):\n        classifierResult = classify0(normMat[i,:],normMat[numTestVecs:m,:],\\\n                                     datingLabels[numTestVecs:m],3)\n        print('\xe5\x88\x86\xe7\xb1\xbb\xe5\x99\xa8\xe5\x88\x86\xe7\x9a\x84\xe7\xb1\xbb\xe6\x98\xaf%d,\xe7\x9c\x9f\xe6\xad\xa3\xe7\x9a\x84\xe7\xb1\xbb\xe5\x88\xab\xe6\x98\xaf%d'%(classifierResult,datingLabels[i]))\n        if classifierResult != datingLabels[i]:\n            errorCount+=1.0\n\n    print('\xe9\x94\x99\xe8\xaf\xaf\xe7\x8e\x87\xe6\x98\xaf%f'%( errorCount/numTestVecs ))\n\ndatingClassTest()\n\n\n\n\n\n\n\n"""
ML_in_action/p15_decision_Tree.py,0,"b""# -*- coding: gbk -*-\n#__author__ = 'dragonfive'\n# 22/02/2016 14:28:17\n# China:\xe5\x8c\x97\xe4\xba\xac\xe5\xb8\x82:\xe7\xa7\x91\xe6\x8a\x80\xe7\xbd\x91 dragonfive \n# any question mail\xef\xbc\x9adragonfive1992@gmail.com \n# copyright 1992-2016 dragonfive \n\n\n\n# \xe8\xae\xa1\xe7\xae\x97\xe7\xbb\x99\xe5\xae\x9a\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe7\x9a\x84\xe9\xa6\x99\xe5\x86\x9c\xe7\x86\xb5\nfrom math import log\n\ndef calc_ShannonEnt(dataSet):\n    numEntries = len(dataSet)\n    labelCounts = {}\n\n    # \xe5\xbb\xba\xe7\xab\x8b\xe7\xbb\x93\xe6\x9e\x9c\xe7\x9a\x84\xe5\xad\x97\xe5\x85\xb8\n    for featVec in dataSet:\n        currentLabel = featVec[-1]\n        if currentLabel not in labelCounts.keys():\n            labelCounts[currentLabel] = 0\n        labelCounts[currentLabel] += 1\n\n    # \xe4\xb8\x8b\xe9\x9d\xa2\xe8\xae\xa1\xe7\xae\x97\xe9\xa6\x99\xe5\x86\x9c\xe7\x86\xb5\n    shannonEnt = 0.0\n    for key in labelCounts:\n        pKey = float(labelCounts[key])/numEntries\n        shannonEnt += -1*pKey*log(pKey,2)\n\n    return shannonEnt\n\n\n\n\n\n\n\n"""
cv_python/01usePIL.py,0,"b'# -*- coding: gbk -*-\n#__author__ = \'ASUS\'\nfrom PIL import Image\nfrom pylab import *\nimport os\nimport re\nimport chardet\nzhPattern = re.compile(u\'[\\u4e00-\\u9fa5]+\')\n# pil_mg=Image.open(r\'E:\\\xe8\xb5\x84\xe6\x96\x99\\onedrive\\code\\test\\image\\lena.png\'.decode(\'utf-8\').encode(\'gbk\')).convert(\'L\')\n# #im=array(pil_mg.resize((128,128)))   # \xe8\xb0\x83\xe6\x95\xb4\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe5\x88\x86\xe8\xbe\xa8\xe7\x8e\x87\n# im=array(pil_mg)\n# #imshow(im)\n# figure()\n# #\xe4\xb8\x8d\xe4\xbd\xbf\xe7\x94\xa8\xe9\xa2\x9c\xe8\x89\xb2\xe4\xbf\xa1\xe6\x81\xaf\n# gray()\n# # \xe5\x9c\xa8\xe5\x8e\x9f\xe7\x82\xb9\xe7\x9a\x84\xe5\xb7\xa6\xe4\xb8\x8a\xe8\xa7\x92\xe6\x98\xbe\xe7\xa4\xba\xe8\xbd\xae\xe5\xbb\x93\xe5\x9b\xbe\xe5\x83\x8f\n# contour(im,origin=\'image\')\n# axis(\'equal\')\n# axis(\'off\')\n# figure()\n# hist(im.flatten(),2)\n# # \xe4\xb8\x80\xe4\xba\x9b\xe7\x82\xb9\n# x=[100,100,400,400]\n# y=[200,500,200,500]\n\n# # \xe4\xbd\xbf\xe7\x94\xa8\xe7\xba\xa2\xe8\x89\xb2\xe5\xbd\xa2\xe7\x8a\xb6\xe6\xa0\x87\xe8\xae\xb0\xe6\x8f\x8f\xe7\xbb\x98\xe7\x82\xb9\n# #plot(x,y,\'r*\')\n#\n# # \xe6\x8f\x8f\xe7\xbb\x98\xe9\x93\xbe\xe6\x8e\xa5\xe4\xb8\xa4\xe4\xb8\xaa\xe7\x82\xb9\xe7\x9a\x84\xe7\xba\xbf\n# # plot(x[:2],y[:2])\n#\n# #\xe6\xb7\xbb\xe5\x8a\xa0\xe6\xa0\x87\xe9\xa2\x98\xef\xbc\x8c\xe6\x98\xbe\xe7\xa4\xba\xe6\x8f\x8f\xe7\xbb\x98\xe7\x9a\x84\xe5\x9b\xbe\xe5\x83\x8f\n# title(\'plotting:""file""\')\n# show()\npath=r\'E:\\\xe8\xb5\x84\xe6\x96\x99\\onedrive\\code\\test\\image\'\nfiles=os.listdir(path)\nprint [ f for f in files if not zhPattern.search(str(f))]\nif zhPattern.search(\'\xe5\x91\xb5\xe5\x91\xb5\'.decode(\'gbk\')):\n    print (\'\xe5\x91\xb5\xe5\x91\xb5\')\n\nprint [str(f) for f in files if zhPattern.search(str(f).decode(\'gbk\'))]\nfor r in [f for f in files if zhPattern.search(str(f).decode(\'gbk\'))]:# \xe7\x94\xa8\xe4\xba\x8e\xe6\xa3\x80\xe6\xb5\x8b\xe6\x98\xaf\xe5\x90\xa6\xe6\x9c\x89\xe4\xb8\xad\xe6\x96\x87\xe5\xad\x97\xe7\xac\xa6\n    print chardet.detect(str(r))    # \xe7\x94\xa8\xe6\x9d\xa5\xe6\xa3\x80\xe6\xb5\x8b\xe5\xad\x97\xe7\xac\xa6\xe4\xb8\xb2\xe7\x9a\x84\xe7\xbc\x96\xe7\xa0\x81\xe7\xb1\xbb\xe5\x9e\x8b\n\n\n'"
cv_python/02circleMap.py,0,"b""# -*- coding: gbk*-\n#__author__ = 'ASUS'\n\nfrom PIL import Image\n#from pylab import *\nfrom pylab import array\nimport matplotlib.pyplot as plt\nimport os\n\n# im=array(Image.open(r'E:\\\xc3\x97\xc3\x8a\xc3\x81\xc3\x8f\\onedrive\\code\\test\\image\\lena.png'))\n# plt.imshow(im)\n# print 'Please click 3 point'\n# x=plt.ginput(4)\n# print x\n# plt.show()\n\npath=r'E:\\\xc3\x97\xc3\x8a\xc3\x81\xc3\x8f\\onedrive\\code\\test\\image'\nimgelist=os.listdir(path)\nfor image in imgelist:\n    im=array(Image.open(path+'\\\\'+image))\n    if len(im.shape)==2:\n        plt.gray()\n    plt.imshow(im)\n    x=plt.ginput(2)\n    y=[image]+x\n    print y\n\nplt.show()\n"""
cv_python/03useNumpy.py,0,"b""# -*- coding: utf-8  -*-\n#__author__ = 'ASUS'\n\nfrom PIL import Image\nfrom numpy import *\nimport pylab as plt\nimport imtools\n# im =array(Image.open(r'E:\\\xe8\xb5\x84\xe6\x96\x99\\onedrive\\code\\test\\image\\lena.png').convert('L'))\n# img2=255-im;\n# plt.gray()\n# plt.imshow(img2)\n# plt.figure()\n# plt.gray()\n# plt.imshow(im)\n# x=range(2,10)\n#\n# y=[ i**2 for i in x]\n# plt.plot(x,y)\n# plt.show()\n# im =array(Image.open(r'E:\\\xe8\xb5\x84\xe6\x96\x99\\onedrive\\code\\test\\image\\lena.png'.decode('utf-8').encode('gbk')).convert('L'))\n# im2,cdf=imtools.histeq(im)\n# plt.gray()\n# plt.subplot(221);plt.imshow(im);plt.title('orgin image')\n# plt.subplot(222);plt.hist(im.flatten(),128);plt.title('orgin image hist')\n# plt.subplot(223);plt.imshow(im2);plt.title('junheng image')\n# plt.subplot(224);plt.hist(im2.flatten(),128);plt.title('junheng image hist')\n# plt.show()\n\n#im2=interp(im.flatten(),bins[:-1],cdf)\nf=[2,4,6,2,6]\na=[2,4,6]\nb=[4,8,16]\n\nd=interp(f,a,b)  #\xe8\xbf\x99\xe4\xb8\xaa\xe6\x8f\x92\xe5\x80\xbc\xe5\xb0\xb1\xe6\x98\xaf\xe5\xaf\xb9f\xe4\xb8\xad\xe7\x9a\x84\xe5\x80\xbc\xe5\xa6\x82\xe6\x9e\x9c\xe5\x9c\xa8a\xe4\xb8\xad\xef\xbc\x8c\xe5\xb0\xb1\xe6\x89\xbe\xe5\x88\xb0\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe5\x9c\xa8b\xe4\xb8\xad\xe7\x9a\x84\xe4\xb9\x8b\xe4\xbd\x9c\xe4\xb8\xba\xe6\x96\xb0\xe5\x80\xbc;\nprint d\n"""
cv_python/GuiTest.py,2,"b""# -*- coding: utf-8 -*-\n# __author__ = 'dragon'\n\n\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\n#x = np.arange(0, 5, 0.1);\n#y = np.sin(x)\n#plt.plot(x, y)\n\n# 'E:\\\\test\\\\image\\\\lena.png'\n# .decode('utf8').encode('gbk')\n# unicode(r'E:\\\xe8\xb5\x84\xe6\x96\x99\\onedrive\\code\\test\\image\\lena.png','unicode')\nimg=cv2.imread(r'E:\\\xe8\xb5\x84\xe6\x96\x99\\onedrive\\code\\test\\image\\lena.png'.decode('utf8').encode('gbk'),1)\ncv2.imshow('image',img)\n# cv2.namedWindow('image',cv2.WINDOW_NORMAL)\n# cv2.imshow('image',img)\n# cv2.imwrite(r'E:\\\xe8\xb5\x84\xe6\x96\x99\\onedrive\\code\\test\\image\\lena2.png'.decode('utf8').encode('gbk'),img);\n\nkey = cv2.waitKey(0)\nif key == 27:\n    print '\xe9\x80\x80\xe5\x87\xba'.decode('utf8').encode('gbk')\nelse:\n    print '\xe4\xbf\x9d\xe5\xad\x98'.decode('utf8').encode('gbk')\n\n\n#cv2.destroyWindow('image')#\xe5\x8f\xaf\xe4\xbb\xa5\xe5\x88\xa0\xe9\x99\xa4\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe7\xaa\x97\xe5\x8f\xa3;\n#cv2.destroyAllWindows()#\xe5\x88\xa0\xe9\x99\xa4\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe7\xaa\x97\xe5\x8f\xa3\n"""
cv_python/imtools.py,0,"b'# -*- coding: utf-8 -*-\n#__author__ = \'ASUS\'\nimport os\nfrom PIL import Image\nfrom numpy import *\ndef get_imlist(path):\n    """"""\n    :param path: \xe8\xb7\xaf\xe5\xbe\x84\n    :return:\xe8\xbf\x94\xe5\x9b\x9e\xe4\xbb\xa5jpg\xe7\xbb\x93\xe5\xb0\xbe\xe7\x9a\x84\xe6\x96\x87\xe4\xbb\xb6\xe5\x90\x8d\xe5\x88\x97\xe8\xa1\xa8;\n    """"""\n    return [os.path.join(path,f) for f in os.listdir(path) if f.endswith(\'.jpg\')]\n\ndef imresize(im,sz):\n    """"""\n    \xe4\xbd\xbf\xe7\x94\xa8PIL\xe5\xaf\xb9\xe8\xb1\xa1\xe9\x87\x8d\xe6\x96\xb0\xe5\xae\x9a\xe4\xb9\x89\xe5\x9b\xbe\xe5\x83\x8f\xe6\x95\xb0\xe7\xbb\x84\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\n    :param im: \xe5\x9b\xbe\xe5\x83\x8f\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84PIL\xe6\x95\xb0\xe7\xbb\x84\n    :param sz: \xe6\x94\xb9\xe5\x8f\x98\xe5\x90\x8e\xe7\x9a\x84size\n    :return:\xe8\xbf\x94\xe5\x9b\x9e\xe6\x94\xb9\xe5\x8f\x98\xe5\x90\x8e\xe7\x9a\x84\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84PIL\xe6\x95\xb0\xe7\xbb\x84\n    """"""\n    image=Image.fromarray(uint8(im))\n    return array(image.resize(sz))\n\ndef histeq(im,nbr_bins=256):\n    """"""\n    \xe5\xaf\xb9\xe4\xb8\x80\xe5\x89\xaf\xe7\x81\xb0\xe5\xba\xa6\xe5\x9b\xbe\xe5\x83\x8f\xe8\xbf\x9b\xe8\xa1\x8c\xe7\x9b\xb4\xe6\x96\xb9\xe5\x9b\xbe\xe5\x9d\x87\xe8\xa1\xa1\xe5\x8c\x96\n    :param im:\n    :param nbr_bins:\n    :return:\n    """"""\n    #\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe7\x9b\xb4\xe6\x96\xb9\xe5\x9b\xbe bins\xe6\x98\xafimhist\xe7\x9a\x84\xe9\x95\xbf\xe5\xba\xa6+1\n    imhist,bins=histogram(im.flatten(),nbr_bins,normed=True)\n    cdf=imhist.cumsum()#\xe7\xb4\xaf\xe7\xa7\xaf\xe5\x88\x86\xe5\xb8\x83\xe5\x87\xbd\xe6\x95\xb0 \xe5\x8f\xaf\xe4\xbb\xa5\xe8\xaf\x95\xe4\xb8\x80\xe8\xaf\x95\xe6\xb1\x82\xe7\xb4\xaf\xe5\x8a\xa0\xe5\x92\x8c;\n    cdf=255*cdf/cdf[-1];\n    #\xe4\xbd\xbf\xe7\x94\xa8\xe7\xb4\xaf\xe7\xa7\xaf\xe5\x88\x86\xe5\xb8\x83\xe5\x87\xbd\xe6\x95\xb0\xe7\x9a\x84\xe7\xba\xbf\xe6\x80\xa7\xe6\x8f\x92\xe5\x80\xbc\xef\xbc\x8c\xe8\xae\xa1\xe7\xae\x97\xe6\x96\xb0\xe7\x9a\x84\xe5\x83\x8f\xe7\xb4\xa0\xe5\x80\xbc;\n    im2=interp(im.flatten(),bins[:-1],cdf)\n\n    return im2.reshape(im.shape),cdf\n\n\ndef compute_average(imlist):\n    """"""\n    \xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xe5\x83\x8f\xe5\x88\x97\xe8\xa1\xa8\xe7\x9a\x84\xe5\xb9\xb3\xe5\x9d\x87\xe5\x9b\xbe\xe5\x83\x8f\n    :param imlist: \xe5\x9b\xbe\xe5\x83\x8f\xe8\xb7\xaf\xe5\xbe\x84\xe5\x88\x97\xe8\xa1\xa8\n    :return:\xe8\xbf\x94\xe5\x9b\x9e\xe7\x9a\x84\xe6\x98\xaf\xe5\xb9\xb3\xe5\x9d\x87\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe6\x95\xb0\xe7\xbb\x84\n    """"""\n    #\xe6\x89\x93\xe5\xbc\x80\xe7\xac\xac\xe4\xb8\x80\xe5\x89\xaf\xe5\x9b\xbe\xe5\x83\x8f\xe6\x8a\x8a\xe4\xbb\x96\xe4\xbf\x9d\xe5\xad\x98\xe5\x9c\xa8\xe6\xb5\xae\xe7\x82\xb9\xe5\x9e\x8b\xe6\x95\xb0\xe7\xbb\x84\xe4\xb8\xad\n    averageim=array(Image.open(imlist[0]),\'f\')\n    for imname in imlist[1:]:#\xe4\xbb\x8e\xe4\xb8\x8b\xe6\xa0\x87\xe4\xb8\xba1\xe7\x9a\x84\xe4\xbd\x8d\xe7\xbd\xae\xe5\xbc\x80\xe5\xa7\x8b \xe5\x9b\xa0\xe4\xb8\xba\xe4\xb8\x8b\xe6\xa0\x87\xe4\xb8\xba0\xe7\x9a\x84\xe5\xb7\xb2\xe7\xbb\x8f\xe5\x8f\x96\xe8\xbf\x87\xe4\xba\x86\n        try:\n            averageim+=array(Image.open(imname))\n        except:\n            print imname+\'...skipped\'\n    averageim/=len(imlist)\n\n    #\xe8\xbf\x94\xe5\x9b\x9euint8\xe5\x9e\x8b\xe7\x9a\x84\xe5\xb9\xb3\xe5\x9d\x87\xe5\x9b\xbe\xe5\x83\x8f\n    return array(averageim,\'uint8\')\n'"
cv_python/matplotTest.py,0,"b'# -*- coding: utf-8 -*-\n#__author__ = \'ASUS\'\n\n\nimport numpy\nimport cv2\nimport matplotlib.pyplot as plt\n\n\n# img = cv2.imread(r\'E:\\\xe8\xb5\x84\xe6\x96\x99\\onedrive\\code\\test\\image\\result.png\'.decode(\'utf8\').encode(\'gbk\'),0)\n# plt.imshow(img, cmap = \'gray\', interpolation = \'bicubic\')\n# plt.xticks([]), plt.yticks([]) # to hide tick values on X and Y axis\n# plt.show()\n\nimg = cv2.imread(r\'E:\\\xe8\xb5\x84\xe6\x96\x99\\onedrive\\code\\test\\image\\lena.png\'.decode(\'utf8\').encode(\'gbk\'),1)\n#I = cv2.cvtColor(I, cv2.COLOR_RGB2GRAY)\nregion = img[200:370,200:370]\nb,g,r = cv2.split(img)\n\nimg2 = cv2.merge([r,g,b])\na,b,c = cv2.split(region)\nimg3 = cv2.merge([c,b,a])\nplt.subplot(121);plt.imshow(img2, cmap = \'gray\', interpolation = \'bicubic\')\nplt.subplot(122);plt.imshow(img3)\n#plt.xticks([]), plt.yticks([]) # to hide tick values on X and Y axis\nplt.show()\n# cv2.imshow(""\xe5\x85\xa8\xe9\x83\xa8"".decode(\'utf8\').encode(\'gbk\'),I)\n# cv2.imshow(""\xe9\x83\xa8\xe5\x88\x86"".decode(\'utf8\').encode(\'gbk\'),region)\n#cv2.waitKey(0)\n\n# cv2.destroyAllWindows()'"
cv_python/opencvTest (2).py,0,"b'# -*- coding: utf-8 -*-\n#__author__ = \'ASUS\'\nimport cv2\n\nimg = cv2.imread(r\'E:\\\xe8\xb5\x84\xe6\x96\x99\\onedrive\\code\\test\\image\\lena.png\'.decode(\'utf-8\').encode(\'gbk\'),1)\nimg2=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) #\xe5\xb0\x86\xe7\x9c\x9f\xe5\xbd\xa9\xe5\x9b\xbe\xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xba\xe7\x81\xb0\xe5\xba\xa6\xe5\x9b\xbe\ncv2.namedWindow(""Image"")\ncv2.imshow(""Image"", img2)\ncv2.waitKey (0)\ncv2.destroyAllWindows()'"
cv_python/opencvTest.py,0,"b'import cv2\n\nimg = cv2.imread(\'lena.jpg\')\ncv2.namedWindow(""Image"")\ncv2.imshow(""Image"", img)\ncv2.waitKey (0)\ncv2.destroyAllWindows()\n'"
cv_python/p11imgSegBySpectual.py,1,"b""# -*- coding: gbk -*-\n#__author__ = 'dragonfive'\n# 28/12/2015 17:27:41\n# United States:New York:DigitalOcean dragonfive \n# any question mail\xc2\xa3\xc2\xbadragonfive1992@gmail.com \n# copyright 1992-2015 dragonfive \n\nimport time\n\nimport numpy as np\nimport scipy as sp\nimport matplotlib.pyplot as plt\n\nfrom sklearn.feature_extraction import image\nfrom sklearn.cluster import spectral_clustering\n\nlena = sp.misc.lena()\n# Downsample the image by a factor of 4\nlena = lena[::2, ::2] + lena[1::2, ::2] + lena[::2, 1::2] + lena[1::2, 1::2]\nlena = lena[::2, ::2] + lena[1::2, ::2] + lena[::2, 1::2] + lena[1::2, 1::2]\n\n# Convert the image into a graph with the value of the gradient on the\n# edges.\ngraph = image.img_to_graph(lena)\n\n# Take a decreasing function of the gradient: an exponential\n# The smaller beta is, the more independent the segmentation is of the\n# actual image. For beta=1, the segmentation is close to a voronoi\nbeta = 5\neps = 1e-6\ngraph.data = np.exp(-beta * graph.data / lena.std()) + eps\n\n# Apply spectral clustering (this step goes much faster if you have pyamg\n# installed)\nN_REGIONS = 11\n\n###############################################################################\n# Visualize the resulting regions\n\nfor assign_labels in ('kmeans', 'discretize'):\n    t0 = time.time()\n    labels = spectral_clustering(graph, n_clusters=N_REGIONS,\n                                 assign_labels=assign_labels,\n                                 random_state=1)\n    t1 = time.time()\n    labels = labels.reshape(lena.shape)\n\n    plt.figure(figsize=(5, 5))\n    plt.imshow(lena,   cmap=plt.cm.gray)\n    for l in range(N_REGIONS):\n        plt.contour(labels == l, contours=1,\n                    colors=[plt.cm.spectral(l / float(N_REGIONS)), ])\n    plt.xticks(())\n    plt.yticks(())\n    plt.title('Spectral clustering: %s, %.2fs' % (assign_labels, (t1 - t0)))\n\nplt.show()"""
cv_python/p12KLtrans.py,6,"b""# -*- coding: gbk -*-\n#__author__ = 'dragonfive'\n# 03/01/2016 21:58:25\n# China:\xe5\x8c\x97\xe4\xba\xac\xe5\xb8\x82:\xe7\xa7\x91\xe6\x8a\x80\xe7\xbd\x91 dragonfive\n# any question mail\xef\xbc\x9adragonfive1992@gmail.com\n# copyright 1992-2015 dragonfive\n\n# \xe6\x9c\xac\xe6\x96\x87\xe4\xbb\xb6\xe8\xbf\x9b\xe8\xa1\x8cK\xe2\x80\x94\xe2\x80\x94L\xe5\x8f\x98\xe6\x8d\xa2\nimport numpy as np\n\n# \xe4\xb8\x8b\xe9\x9d\xa2\xe7\x9a\x84\xe5\x87\xbd\xe6\x95\xb0\xe4\xba\xa7\xe7\x94\x9fK\xe2\x80\x94\xe2\x80\x94L\xe5\x8f\x98\xe6\x8d\xa2\xe7\x9a\x84\xe5\x8f\x98\xe6\x8d\xa2\xe7\x9f\xa9\xe9\x98\xb5\ndef K_Ltrans(x):\n    # x=np.array([[2,4,5,5,3,2],[2,3,4,5,4,3]])\n    y=np.cov(x) # \xe8\xbf\x99\xe9\x87\x8cnp.cov\xe4\xbb\xa5\xe8\xa1\x8c\xe4\xb8\xba\xe5\x90\x91\xe9\x87\x8f\n\n    # \xe4\xb8\x8b\xe9\x9d\xa2\xe5\xaf\xb9\xe5\x8d\x8f\xe6\x96\xb9\xe5\xb7\xae\xe5\x81\x9a\xe7\x89\xb9\xe5\xbe\x81\xe5\x80\xbc\xe5\x88\x86\xe8\xa7\xa3\n    # \xe4\xb8\x8b\xe9\x9d\xa2\xe6\xb1\x82\xe7\x89\xb9\xe5\xbe\x81\xe5\x80\xbc\xe5\x92\x8c\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe5\x90\x91\xe9\x87\x8f\n    eigval,eigvec=np.linalg.eig(y) # \xe5\x81\x9a\xe7\x89\xb9\xe5\xbe\x81\xe5\x80\xbc\xe5\x88\x86\xe8\xa7\xa3\n    newEig = np.vstack([eigval,eigvec.T]) # \xe5\x90\x88\xe5\xb9\xb6\xe4\xb8\xa4\xe4\xb8\xaalist\n\n    # \xe4\xb8\x8b\xe9\x9d\xa2\xe6\x98\xaf\xe7\xbb\x93\xe6\x9e\x84\xe5\x8c\x96\xe6\x8e\x92\xe5\xba\x8f\xe7\x9a\x84\xe6\x96\xb9\xe5\xbc\x8f\xef\xbc\x8c\xe6\x95\xb0\xe7\xbb\x84\xe4\xb8\xad\xe6\xaf\x8f\xe4\xb8\x80\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaatuple \xe5\x8f\xaf\xe4\xbb\xa5\xe6\xa8\xa1\xe6\x8b\x9f\xe7\xbb\x93\xe6\x9e\x84\xe4\xbd\x93\n    dtypes=[('val',float),('vec1',float),('vec2',float)]\n    newEig2 = np.array(map(tuple,newEig.T), dtype=dtypes)\n    sortedlist = np.sort(newEig2,order='val')\n    return [ list(tup)[1:3]  for tup in sortedlist ]\n\n\n\n\n\n"""
cv_python/图像ROI.py,0,"b'# -*- coding: utf-8 -*-\n#__author__ = \'ASUS\'\n\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as mlp\n\n\n\n\n# img = cv2.imread(r\'E:\\\xe8\xb5\x84\xe6\x96\x99\\onedrive\\code\\test\\image\\ship2.jpg\'.decode(\'utf-8\').encode(\'gbk\'),1)\n# b,g,r=cv2.split(img);\n# img=cv2.merge([r,g,b])\n# # \xe4\xb8\x8b\xe9\x9d\xa2\xe7\x9a\x84\xe4\xbb\xa3\xe7\xa0\x81\xe6\x8a\x8a\xe5\xb0\x8f\xe8\x88\xb9\xe5\x8f\xaa\xe5\xa4\x8d\xe5\x88\xb6\xe5\x87\xa0\xe4\xbb\xbd\n# height,width = 265-241,776-743\n# smallBoat = img[241:265,743:776]\n# img[383:383+height,94:94+width]=smallBoat\n# img[383:383+height,130:130+width]=smallBoat\n# img[410:410+height,94:94+width]=smallBoat\n# img[410:410+height,130:130+width]=smallBoat\n#\n# img[:,:,0]=255\n# mlp.imshow(img)\n# mlp.show()\n\n# #\xe4\xb8\x8b\xe9\x9d\xa2\xe7\x9a\x84\xe7\xa8\x8b\xe5\xba\x8f\xe6\x8a\x8a\xe5\x9b\xbe\xe7\x89\x87\xe8\xbe\x93\xe5\x87\xba\xe8\xb4\x9f\xe7\x89\x87\n# img = cv2.imread(r\'E:\\\xe8\xb5\x84\xe6\x96\x99\\onedrive\\code\\test\\image\\result.png\'.decode(\'utf-8\').encode(\'gbk\'),0)\n# width,height=img.shape\n#\n# # img2=255-img;#\xe8\xbf\x99\xe6\x9d\xa1\xe8\xaf\xad\xe5\x8f\xa5\xe9\x80\x9a\xe8\xbf\x87\xe7\x9f\xa9\xe9\x98\xb5\xe8\xbf\x90\xe7\xae\x97\xef\xbc\x8c\xe7\x94\x9f\xe6\x88\x90\xe8\xb4\x9f\xe7\x89\x87\n# img2 = cv2.bitwise_not(img) #\xe8\xbf\x99\xe6\xa0\xb7\xe4\xb9\x9f\xe8\x83\xbd\xe7\x94\x9f\xe6\x88\x90\xe8\xb4\x9f\xe7\x89\x87\xef\xbc\x8c\xe6\x8c\x89\xe4\xbd\x8d\xe5\x8f\x96\xe5\x8f\x8d\n# # print(img.shape)\n# # for hehe in img[:,:]==0:\n# #     hehe=255\n# # for hehe in img[:,:]==255:\n# #     hehe=0\n#\n# cv2.imshow(""hehe"",img)\n# cv2.waitKey(0)\n# cv2.destroyAllWindows()\n\n\n\n# # \xe4\xb8\x8b\xe9\x9d\xa2\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x9b\xbe\xe7\x89\x87\xe6\xb7\xb7\xe5\x90\x88\n# # E:\\\xe8\xb5\x84\xe6\x96\x99\\onedrive\\code\\test\\image#\n# img=cv2.imread(r\'E:\\\xe8\xb5\x84\xe6\x96\x99\\onedrive\\code\\test\\image\\dezert.jpg\'.decode(\'utf-8\').encode(\'gbk\'),1)\n# b,g,r=cv2.split(img)\n# img=cv2.merge([r,g,b])\n#\n# person =img[213:328,410:487]\n# zeroImg = 0*img+person[0,0];\n# zeroImg[213:328,330:407]=person\n# twoImg = cv2.addWeighted(img,1,zeroImg,0.2,0)\n#\n# oneImg = 0*img+person[0,0];\n# oneImg[213:328,250:327]=person\n# threeImg = cv2.addWeighted(twoImg,1,oneImg,0.2,0)\n#\n# mlp.imshow(threeImg)\n# mlp.show()\n\n'"
cv_python/图像的基本操作.py,0,"b'# -*- coding: utf-8 -*-\n#__author__ = \'ASUS\'\n\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimg1=cv2.imread(r\'E:\\\xe8\xb5\x84\xe6\x96\x99\\onedrive\\code\\test\\image\\lena.png\'.decode(\'utf-8\').encode(\'gbk\'),1)\nb,g,r = cv2.split(img1)\nimg= cv2.merge([r,g,b])\n\n# for prow in img:\n#     for point in prow:\n#         if point==0:\n#             point=255\n#         else:\n#             point=0\n\n\n# cv2.imshow(""grayphto"",img)\n# cv2.waitKey(0)\n#\n# cv2.destroyAllWindows()\n\nprint(img.shape) #\xe8\xbe\x93\xe5\x87\xba\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe9\x95\xbf\xe5\xae\xbd\xe5\x92\x8c\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe6\x98\xaf\xe7\x81\xb0\xe5\xba\xa6\xe5\x9b\xbe\xef\xbc\x8c\xe5\x88\x99\xe4\xb8\x8d\xe8\xbe\x93\xe5\x87\xba\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0;\nprint(img.size)  #\xe8\xbe\x93\xe5\x87\xba\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe5\x83\x8f\xe7\xb4\xa0\xe6\x95\xb0\xe7\x9b\xae\nprint(img.dtype) #\xe8\xbe\x93\xe5\x87\xba\xe5\x9b\xbe\xe5\x83\x8f\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe7\xb1\xbb\xe5\x9e\x8b\npartail = img[100:140,100:200]\n# print px\nplt.subplot(121)\nplt.imshow(img)\nplt.subplot(122)\nplt.imshow(partail)\n\n#px=img[100,100:150]\n#print px\n\n\nplt.show()\n\n\n'"
cv_python/物体跟踪.py,2,"b'# -*- coding: utf-8 -*-\n""""""\n__author__ = \'ASUS\'\ncreate on 2015-10-25 20:46:29\n""""""\n\nimport cv2\nimport numpy as np\n\ncap=cv2.VideoCapture(0)\nwhile(1):\n    # \xe8\x8e\xb7\xe5\xbe\x97\xe6\xaf\x8f\xe4\xb8\x80\xe5\xb8\xa7\n    ret,frame=cap.read()\n\n    # \xe8\xbd\xac\xe6\x8d\xa2\xe5\x88\xb0HSV\xe7\xa9\xba\xe9\x97\xb4\n    hsv=cv2.cvtColor(frame,cv2.COLOR_BGR2HSV)\n\n    # \xe8\xae\xbe\xe5\xae\x9a\xe8\x93\x9d\xe8\x89\xb2\xe7\x9a\x84\xe9\x98\x88\xe5\x80\xbc\n    lower_blue=np.array([100,50,50])\n    upper_blue=np.array([130,255,255])\n\n    # \xe6\xa0\xb9\xe6\x8d\xae\xe9\x98\x88\xe5\x80\xbc\xe6\x9e\x84\xe5\xbb\xba\xe6\x8e\xa9\xe8\x86\x9c\n    mask=cv2.inRange(hsv,lower_blue,upper_blue);\n\n    #\xe5\xaf\xb9\xe5\x8e\x9f\xe5\x9b\xbe\xe5\x83\x8f\xe5\x92\x8c\xe6\x8e\xa9\xe8\x86\x9c\xe8\xbf\x9b\xe8\xa1\x8c\xe4\xbd\x8d\xe8\xbf\x90\xe7\xae\x97\n    res=cv2.bitwise_and(frame,frame,mask=mask)\n\n    #\xe6\x98\xbe\xe7\xa4\xba\xe5\x9b\xbe\xe5\x83\x8f\n    cv2.imshow(\'frame\',frame)\n    cv2.imshow(\'mask\',mask)\n    cv2.imshow(\'res\',res)\n    k=cv2.waitKey(300)\n    if k==27:\n        break\n    else:\n        cv2.destroyWindow(\'frame\')\n        cv2.destroyWindow(\'mask\')\n        cv2.destroyWindow(\'res\')\n\ncv2.destroyAllWindows()\n\n\n\n\n\n\n'"
my_deep_learning_lib/backward.py,9,"b'class Network(object):\n\n    def update_mini_batch(self, mini_batch, eta):\n        """"""Update the network\'s weights and biases by applying\n        gradient descent using backpropagation to a single mini batch.\n        The ""mini_batch"" is a list of tuples ""(x, y)"", and ""eta""\n        is the learning rate.""""""\n        nabla_b = [np.zeros(b.shape) for b in self.biases]\n        nabla_w = [np.zeros(w.shape) for w in self.weights]\n        for x, y in mini_batch:\n            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n        self.weights = [w-(eta/len(mini_batch))*nw \n                        for w, nw in zip(self.weights, nabla_w)]\n        self.biases = [b-(eta/len(mini_batch))*nb \n                       for b, nb in zip(self.biases, nabla_b)]\n\n   def backprop(self, x, y):\n        """"""Return a tuple ""(nabla_b, nabla_w)"" representing the\n        gradient for the cost function C_x.  ""nabla_b"" and\n        ""nabla_w"" are layer-by-layer lists of numpy arrays, similar\n        to ""self.biases"" and ""self.weights"".""""""\n        nabla_b = [np.zeros(b.shape) for b in self.biases]\n        nabla_w = [np.zeros(w.shape) for w in self.weights]\n        # feedforward\n        activation = x\n        activations = [x] # list to store all the activations, layer by layer\n        zs = [] # list to store all the z vectors, layer by layer\n        for b, w in zip(self.biases, self.weights):\n            z = np.dot(w, activation)+b\n            zs.append(z)\n            activation = sigmoid(z)\n            activations.append(activation)\n        # backward pass\n        delta = self.cost_derivative(activations[-1], y) * \\\n            sigmoid_prime(zs[-1])\n        nabla_b[-1] = delta\n        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n        # Note that the variable l in the loop below is used a little\n        # differently to the notation in Chapter 2 of the book.  Here,\n        # l = 1 means the last layer of neurons, l = 2 is the\n        # second-last layer, and so on.  It\'s a renumbering of the\n        # scheme in the book, used here to take advantage of the fact\n        # that Python can use negative indices in lists.\n        for l in xrange(2, self.num_layers):\n            z = zs[-l]\n            sp = sigmoid_prime(z)\n            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n            nabla_b[-l] = delta\n            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n        return (nabla_b, nabla_w)\n    def cost_derivative(self, output_activations, y):\n        """"""Return the vector of partial derivatives \\partial C_x /\n        \\partial a for the output activations.""""""\n        return (output_activations-y) \n\ndef sigmoid(z):\n    """"""The sigmoid function.""""""\n    return 1.0/(1.0+np.exp(-z))\n\ndef sigmoid_prime(z):\n    """"""Derivative of the sigmoid function.""""""\n    return sigmoid(z)*(1-sigmoid(z))\n'"
my_deep_learning_lib/network2.py,25,"b'""""""network2.py\n~~~~~~~~~~~~~~\nAn improved version of network.py, implementing the stochastic\ngradient descent learning algorithm for a feedforward neural network.\nImprovements include the addition of the cross-entropy cost function,\nregularization, and better initialization of network weights.  Note\nthat I have focused on making the code simple, easily readable, and\neasily modifiable.  It is not optimized, and omits many desirable\nfeatures.\n""""""\n\n#### Libraries\n# Standard library\nimport json\nimport random\nimport sys\n\n# Third-party libraries\nimport numpy as np\n\n\n#### Define the quadratic and cross-entropy cost functions\n\nclass QuadraticCost(object):\n\n    @staticmethod\n    def fn(a, y):\n        """"""Return the cost associated with an output ``a`` and desired output\n        ``y``.\n        """"""\n        return 0.5*np.linalg.norm(a-y)**2\n\n    @staticmethod\n    def delta(z, a, y):\n        """"""Return the error delta from the output layer.""""""\n        return (a-y) * sigmoid_prime(z)\n\n\nclass CrossEntropyCost(object):\n\n    @staticmethod\n    def fn(a, y):\n        """"""Return the cost associated with an output ``a`` and desired output\n        ``y``.  Note that np.nan_to_num is used to ensure numerical\n        stability.  In particular, if both ``a`` and ``y`` have a 1.0\n        in the same slot, then the expression (1-y)*np.log(1-a)\n        returns nan.  The np.nan_to_num ensures that that is converted\n        to the correct value (0.0).\n        """"""\n        return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n\n    @staticmethod\n    def delta(z, a, y):\n        """"""Return the error delta from the output layer.  Note that the\n        parameter ``z`` is not used by the method.  It is included in\n        the method\'s parameters in order to make the interface\n        consistent with the delta method for other cost classes.\n        """"""\n        return (a-y)\n\n\n#### Main Network class\nclass Network(object):\n\n    def __init__(self, sizes, cost=CrossEntropyCost):\n        """"""The list ``sizes`` contains the number of neurons in the respective\n        layers of the network.  For example, if the list was [2, 3, 1]\n        then it would be a three-layer network, with the first layer\n        containing 2 neurons, the second layer 3 neurons, and the\n        third layer 1 neuron.  The biases and weights for the network\n        are initialized randomly, using\n        ``self.default_weight_initializer`` (see docstring for that\n        method).\n        """"""\n        self.num_layers = len(sizes)\n        self.sizes = sizes\n        self.default_weight_initializer()\n        self.cost=cost\n\n    def default_weight_initializer(self):\n        """"""Initialize each weight using a Gaussian distribution with mean 0\n        and standard deviation 1 over the square root of the number of\n        weights connecting to the same neuron.  Initialize the biases\n        using a Gaussian distribution with mean 0 and standard\n        deviation 1.\n        Note that the first layer is assumed to be an input layer, and\n        by convention we won\'t set any biases for those neurons, since\n        biases are only ever used in computing the outputs from later\n        layers.\n        """"""\n        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n        self.weights = [np.random.randn(y, x)/np.sqrt(x)\n                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n\n    def large_weight_initializer(self):\n        """"""Initialize the weights using a Gaussian distribution with mean 0\n        and standard deviation 1.  Initialize the biases using a\n        Gaussian distribution with mean 0 and standard deviation 1.\n        Note that the first layer is assumed to be an input layer, and\n        by convention we won\'t set any biases for those neurons, since\n        biases are only ever used in computing the outputs from later\n        layers.\n        This weight and bias initializer uses the same approach as in\n        Chapter 1, and is included for purposes of comparison.  It\n        will usually be better to use the default weight initializer\n        instead.\n        """"""\n        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n        self.weights = [np.random.randn(y, x)\n                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n\n    def feedforward(self, a):\n        """"""Return the output of the network if ``a`` is input.""""""\n        for b, w in zip(self.biases, self.weights):\n            a = sigmoid(np.dot(w, a)+b)\n        return a\n\n    def SGD(self, training_data, epochs, mini_batch_size, eta,\n            lmbda = 0.0,\n            evaluation_data=None,\n            monitor_evaluation_cost=False,\n            monitor_evaluation_accuracy=False,\n            monitor_training_cost=False,\n            monitor_training_accuracy=False):\n        """"""Train the neural network using mini-batch stochastic gradient\n        descent.  The ``training_data`` is a list of tuples ``(x, y)``\n        representing the training inputs and the desired outputs.  The\n        other non-optional parameters are self-explanatory, as is the\n        regularization parameter ``lmbda``.  The method also accepts\n        ``evaluation_data``, usually either the validation or test\n        data.  We can monitor the cost and accuracy on either the\n        evaluation data or the training data, by setting the\n        appropriate flags.  The method returns a tuple containing four\n        lists: the (per-epoch) costs on the evaluation data, the\n        accuracies on the evaluation data, the costs on the training\n        data, and the accuracies on the training data.  All values are\n        evaluated at the end of each training epoch.  So, for example,\n        if we train for 30 epochs, then the first element of the tuple\n        will be a 30-element list containing the cost on the\n        evaluation data at the end of each epoch. Note that the lists\n        are empty if the corresponding flag is not set.\n        """"""\n        if evaluation_data: n_data = len(evaluation_data)\n        n = len(training_data)\n        evaluation_cost, evaluation_accuracy = [], []\n        training_cost, training_accuracy = [], []\n        for j in xrange(epochs):\n            random.shuffle(training_data)\n            mini_batches = [\n                training_data[k:k+mini_batch_size]\n                for k in xrange(0, n, mini_batch_size)]\n            for mini_batch in mini_batches:\n                self.update_mini_batch(\n                    mini_batch, eta, lmbda, len(training_data))\n            print ""Epoch %s training complete"" % j\n            if monitor_training_cost:\n                cost = self.total_cost(training_data, lmbda)\n                training_cost.append(cost)\n                print ""Cost on training data: {}"".format(cost)\n            if monitor_training_accuracy:\n                accuracy = self.accuracy(training_data, convert=True)\n                training_accuracy.append(accuracy)\n                print ""Accuracy on training data: {} / {}"".format(\n                    accuracy, n)\n            if monitor_evaluation_cost:\n                cost = self.total_cost(evaluation_data, lmbda, convert=True)\n                evaluation_cost.append(cost)\n                print ""Cost on evaluation data: {}"".format(cost)\n            if monitor_evaluation_accuracy:\n                accuracy = self.accuracy(evaluation_data)\n                evaluation_accuracy.append(accuracy)\n                print ""Accuracy on evaluation data: {} / {}"".format(\n                    self.accuracy(evaluation_data), n_data)\n            print\n        return evaluation_cost, evaluation_accuracy, \\\n            training_cost, training_accuracy\n\n    def update_mini_batch(self, mini_batch, eta, lmbda, n):\n        """"""Update the network\'s weights and biases by applying gradient\n        descent using backpropagation to a single mini batch.  The\n        ``mini_batch`` is a list of tuples ``(x, y)``, ``eta`` is the\n        learning rate, ``lmbda`` is the regularization parameter, and\n        ``n`` is the total size of the training data set.\n        """"""\n        nabla_b = [np.zeros(b.shape) for b in self.biases]\n        nabla_w = [np.zeros(w.shape) for w in self.weights]\n        for x, y in mini_batch:\n            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n        self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw\n                        for w, nw in zip(self.weights, nabla_w)]\n        self.biases = [b-(eta/len(mini_batch))*nb\n                       for b, nb in zip(self.biases, nabla_b)]\n\n    def backprop(self, x, y):\n        """"""Return a tuple ``(nabla_b, nabla_w)`` representing the\n        gradient for the cost function C_x.  ``nabla_b`` and\n        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n        to ``self.biases`` and ``self.weights``.""""""\n        nabla_b = [np.zeros(b.shape) for b in self.biases]\n        nabla_w = [np.zeros(w.shape) for w in self.weights]\n        # feedforward\n        activation = x\n        activations = [x] # list to store all the activations, layer by layer\n        zs = [] # list to store all the z vectors, layer by layer\n        for b, w in zip(self.biases, self.weights):\n            z = np.dot(w, activation)+b\n            zs.append(z)\n            activation = sigmoid(z)\n            activations.append(activation)\n        # backward pass\n        delta = (self.cost).delta(zs[-1], activations[-1], y)\n        nabla_b[-1] = delta\n        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n        # Note that the variable l in the loop below is used a little\n        # differently to the notation in Chapter 2 of the book.  Here,\n        # l = 1 means the last layer of neurons, l = 2 is the\n        # second-last layer, and so on.  It\'s a renumbering of the\n        # scheme in the book, used here to take advantage of the fact\n        # that Python can use negative indices in lists.\n        for l in xrange(2, self.num_layers):\n            z = zs[-l]\n            sp = sigmoid_prime(z)\n            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n            nabla_b[-l] = delta\n            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n        return (nabla_b, nabla_w)\n\n    def accuracy(self, data, convert=False):\n        """"""Return the number of inputs in ``data`` for which the neural\n        network outputs the correct result. The neural network\'s\n        output is assumed to be the index of whichever neuron in the\n        final layer has the highest activation.\n        The flag ``convert`` should be set to False if the data set is\n        validation or test data (the usual case), and to True if the\n        data set is the training data. The need for this flag arises\n        due to differences in the way the results ``y`` are\n        represented in the different data sets.  In particular, it\n        flags whether we need to convert between the different\n        representations.  It may seem strange to use different\n        representations for the different data sets.  Why not use the\n        same representation for all three data sets?  It\'s done for\n        efficiency reasons -- the program usually evaluates the cost\n        on the training data and the accuracy on other data sets.\n        These are different types of computations, and using different\n        representations speeds things up.  More details on the\n        representations can be found in\n        mnist_loader.load_data_wrapper.\n        """"""\n        if convert:\n            results = [(np.argmax(self.feedforward(x)), np.argmax(y))\n                       for (x, y) in data]\n        else:\n            results = [(np.argmax(self.feedforward(x)), y)\n                        for (x, y) in data]\n        return sum(int(x == y) for (x, y) in results)\n\n    def total_cost(self, data, lmbda, convert=False):\n        """"""Return the total cost for the data set ``data``.  The flag\n        ``convert`` should be set to False if the data set is the\n        training data (the usual case), and to True if the data set is\n        the validation or test data.  See comments on the similar (but\n        reversed) convention for the ``accuracy`` method, above.\n        """"""\n        cost = 0.0\n        for x, y in data:\n            a = self.feedforward(x)\n            if convert: y = vectorized_result(y)\n            cost += self.cost.fn(a, y)/len(data)\n        cost += 0.5*(lmbda/len(data))*sum(\n            np.linalg.norm(w)**2 for w in self.weights)\n        return cost\n\n    def save(self, filename):\n        """"""Save the neural network to the file ``filename``.""""""\n        data = {""sizes"": self.sizes,\n                ""weights"": [w.tolist() for w in self.weights],\n                ""biases"": [b.tolist() for b in self.biases],\n                ""cost"": str(self.cost.__name__)}\n        f = open(filename, ""w"")\n        json.dump(data, f)\n        f.close()\n\n#### Loading a Network\ndef load(filename):\n    """"""Load a neural network from the file ``filename``.  Returns an\n    instance of Network.\n    """"""\n    f = open(filename, ""r"")\n    data = json.load(f)\n    f.close()\n    cost = getattr(sys.modules[__name__], data[""cost""])\n    net = Network(data[""sizes""], cost=cost)\n    net.weights = [np.array(w) for w in data[""weights""]]\n    net.biases = [np.array(b) for b in data[""biases""]]\n    return net\n\n#### Miscellaneous functions\ndef vectorized_result(j):\n    """"""Return a 10-dimensional unit vector with a 1.0 in the j\'th position\n    and zeroes elsewhere.  This is used to convert a digit (0...9)\n    into a corresponding desired output from the neural network.\n    """"""\n    e = np.zeros((10, 1))\n    e[j] = 1.0\n    return e\n\ndef sigmoid(z):\n    """"""The sigmoid function.""""""\n    return 1.0/(1.0+np.exp(-z))\n\ndef sigmoid_prime(z):\n    """"""Derivative of the sigmoid function.""""""\n    return sigmoid(z)*(1-sigmoid(z))\n'"
cv_python/imageSearch/colordescriptor.py,2,"b'# -*- coding: utf-8 -*-\n#__author__ = \'DragonFive\'\n# import the necessary packages\nimport numpy as np\nimport cv2\n\nclass ColorDescriptor:\n\tdef __init__(self, bins):\n\t\t# store the number of bins for the 3D histogram\n\t\tself.bins = bins\n\n\tdef describe(self, image):\n\t\t# convert the image to the HSV color space and initialize\n\t\t# the features used to quantify the image\n\t\timage = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\t\tfeatures = []\n\n\t\t# grab the dimensions and compute the center of the image\n\t\t(h, w) = image.shape[:2]\n\t\t(cX, cY) = (int(w * 0.5), int(h * 0.5))\n\n\t\t# divide the image into four rectangles/segments (top-left,\n\t\t# top-right, bottom-right, bottom-left)\n\t\tsegments = [(0, cX, 0, cY), (cX, w, 0, cY), (cX, w, cY, h),\n\t\t\t(0, cX, cY, h)]\n\n\t\t# construct an elliptical mask representing the center of the\n\t\t# image\n\t\t(axesX, axesY) = (int(w * 0.75) / 2, int(h * 0.75) / 2)\n\t\tellipMask = np.zeros(image.shape[:2], dtype = ""uint8"")\n\t\tcv2.ellipse(ellipMask, (cX, cY), (axesX, axesY), 0, 0, 360, 255, -1)\n\n\t\t# loop over the segments\n\t\tfor (startX, endX, startY, endY) in segments:\n\t\t\t# construct a mask for each corner of the image, subtracting\n\t\t\t# the elliptical center from it\n\t\t\tcornerMask = np.zeros(image.shape[:2], dtype = ""uint8"")\n\t\t\tcv2.rectangle(cornerMask, (startX, startY), (endX, endY), 255, -1)\n\t\t\tcornerMask = cv2.subtract(cornerMask, ellipMask)\n\n\t\t\t# extract a color histogram from the image, then update the\n\t\t\t# feature vector\n\t\t\thist = self.histogram(image, cornerMask)\n\t\t\tfeatures.extend(hist)\n\n\t\t# extract a color histogram from the elliptical region and\n\t\t# update the feature vector\n\t\thist = self.histogram(image, ellipMask, True)\n\t\tfeatures.extend(hist)\n\n\t\t# return the feature vector\n\t\treturn features\n\n\tdef histogram(self, image, mask, isCenter = False):\n\t\t# extract a 3D color histogram from the masked region of the\n\t\t# image, using the supplied number of bins per channel; then\n\t\t# normalize the histogram\n\t\t# last param is hsv\'s three channel\'s range\n\t\thist = cv2.calcHist([image], [0, 1, 2], mask, self.bins,\n\t\t    [0, 180, 0, 256, 0, 256])\n\t\thist = cv2.normalize(hist).flatten()\n\t \t# \xe5\xa4\x84\xe7\x90\x86\xe4\xb8\xad\xe5\xbf\x83\xe5\xa4\x84\xef\xbc\x8c\xe8\xae\xa9\xe5\xae\x83\xe6\x9d\x83\xe9\x87\x8d\xe5\xa4\xa7\xe4\xb8\x80\xe7\x82\xb9\n\t\tif isCenter:\n\t\t\tweight = 5.0\n\t\t\t\n\t\t# return the histogram\n\t\treturn hist\n\n'"
cv_python/imageSearch/index.py,0,"b'import colordescriptor\nimport structuredescriptor\nimport glob\nimport argparse\nimport cv2\n\nsearchArgParser = argparse.ArgumentParser()\nsearchArgParser.add_argument(""-d"", ""--dataset"", required = True, help = ""Path to the directory that contains the images to be indexed"")\nsearchArgParser.add_argument(""-c"", ""--colorindex"", required = True, help = ""Path to where the computed color index will be stored"")\nsearchArgParser.add_argument(""-s"", ""--structureindex"", required = True, help = ""Path to where the computed structure index will be stored"")\narguments = vars(searchArgParser.parse_args())\n\nidealBins = (8, 12, 3)\ncolorDesriptor = colordescriptor.ColorDescriptor(idealBins)\n\noutput = open(arguments[""colorindex""], ""w"")\n\nfor imagePath in glob.glob(arguments[""dataset""] + ""/*.png""):\n    imageName = imagePath[imagePath.rfind(""/"") + 1 : ]\n    image = cv2.imread(imagePath)\n    features = colorDesriptor.describe(image)\n    # write features to file\n    features = [str(feature).replace(""\\n"", """") for feature in features]\n    output.write(""%s,%s\\n"" % (imageName, "","".join(features)))\n# close index file\noutput.close()\n\nidealDimension = (16, 16)\nstructureDescriptor = structuredescriptor.StructureDescriptor(idealDimension)\n\noutput = open(arguments[""structureindex""], ""w"")\n\nfor imagePath in glob.glob(""dataset"" + ""/*.png""):\n    imageName = imagePath[imagePath.rfind(""/"") + 1 : ]\n    image = cv2.imread(imagePath)\n    structures = structureDescriptor.describe(image)\n    # write structures to file\n    structures = [str(structure).replace(""\\n"", """") for structure in structures]\n    output.write(""%s,%s\\n"" % (imageName, "","".join(structures)))\n# close index file\noutput.close()\n'"
cv_python/imageSearch/search.py,0,"b'import colordescriptor \nimport structuredescriptor\nimport searcher\nimport argparse\nimport cv2\n\nsearchArgParser = argparse.ArgumentParser()\nsearchArgParser.add_argument(""-c"", ""--colorindex"", required = True, help = ""Path to where the computed color index will be stored"")\nsearchArgParser.add_argument(""-s"", ""--structureindex"", required = True, help = ""Path to where the computed structure index will be stored"")\nsearchArgParser.add_argument(""-q"", ""--query"", required = True, help = ""Path to the query image"")\nsearchArgParser.add_argument(""-r"", ""--resultpath"", required = True, help = ""Path to the result path"")\nsearchArguments = vars(searchArgParser.parse_args())\n\nidealBins = (8, 12, 3)\nidealDimension = (16, 16)\n\ncolorDescriptor = colordescriptor.ColorDescriptor(idealBins)\nstructureDescriptor = structuredescriptor.StructureDescriptor(idealDimension)\nqueryImage = cv2.imread(searchArguments[""query""])\ncolorIndexPath = searchArguments[""colorindex""]\nstructureIndexPath = searchArguments[""structureindex""]\nresultPath = searchArguments[""resultpath""]\n\nqueryFeatures = colorDescriptor.describe(queryImage)\nqueryStructures = structureDescriptor.describe(queryImage)\n\nimageSearcher = searcher.Searcher(colorIndexPath, structureIndexPath)\nsearchResults = imageSearcher.search(queryFeatures, queryStructures,5)\n\nfor imageName, score in searchResults:\n    queryResult = cv2.imread(resultPath + ""/"" + imageName)\n    cv2.imshow(""Result Score: "" + str(int(score)) + "" (lower is better)"", queryResult)\n    cv2.waitKey(0)\n\ncv2.imshow(""Query"", queryImage)\ncv2.waitKey(0)\n\n\n'"
cv_python/imageSearch/searcher.py,1,"b'# -*- coding: utf-8 -*-\n# import the necessary packages\nimport numpy as np\nimport csv\nimport re\n \nclass Searcher:\n\tdef __init__(self, colorIndexPath, structureIndexPath):\n\t\t# store our index path\n\t\tself.indexPath, self.structureIndexPath = colorIndexPath, structureIndexPath\n\n\tdef chi2_distance(self, histA, histB, eps = 1e-10):\n\t\t# compute the chi-squared distance\n\t\td = 0.5 * np.sum([((a - b) ** 2) / (a + b + eps)\n\t\t    for (a, b) in zip(histA, histB)])\n\n\t\t# return the chi-squared distance\n\t\treturn d\n\t\n\t# \xe8\xbf\x99\xe9\x87\x8c\xe7\x9a\x84\xe8\xbf\x94\xe5\x9b\x9e\xe5\x80\xbc\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa\xe5\xad\x97\xe5\x85\xb8\xef\xbc\x8c\xe5\x86\x85\xe5\xae\xb9\xe6\x98\xaf\xe5\x9b\xbe\xe5\x83\x8f\xe5\x90\x8d\xe5\x92\x8c\xe7\x89\xb9\xe5\xbe\x81\xe7\x9a\x84\xe5\xaf\xb9\xe5\xba\x94;\n\tdef searchByColor(self, queryFeatures):\n\t\t# initialize our dictionary of results\n\t\tresults = {}\n\n\t\t# open the index file for reading\n\t\twith open(self.indexPath) as f:\n\t\t\t# initialize the CSV reader\n\t\t\treader = csv.reader(f)\n \n\t\t\t# loop over the rows in the index\n\t\t\tfor row in reader:\n\t\t\t\t# parse out the image ID and features, then compute the\n\t\t\t\t# chi-squared distance between the features in our index\n\t\t\t\t# and our query features\n\t\t\t\tfeatures = [float(x) for x in row[1:]]\n\t\t\t\td = self.chi2_distance(features, queryFeatures)\n \n\t\t\t\t# now that we have the distance between the two feature\n\t\t\t\t# vectors, we can udpate the results dictionary -- the\n\t\t\t\t# key is the current image ID in the index and the\n\t\t\t\t# value is the distance we just computed, representing\n\t\t\t\t# how \'similar\' the image in the index is to our query\n\t\t\t\tresults[row[0]] = d\n \n\t\t\t# close the reader\n\t\t\tf.close()\n \t\treturn results\n\t\t# \xe8\xbf\x99\xe9\x87\x8c\xe8\xbf\x98\xe4\xb8\x8d\xe9\x9c\x80\xe8\xa6\x81\xe6\x8e\x92\xe5\xba\x8f\n\t\t# sort our results, so that the smaller distances (i.e. the\n\t\t# more relevant images are at the front of the list)\n\t\t#results = sorted([(v, k) for (k, v) in results.items()])\n \n\t\t# return our (limited) results\n\t\t#return results[:limit]\n\n\tdef structureDistance(self, structures, queryStructures, eps = 1e-5):\n\t\tdistance = 0\n\t\tnormalizeRatio = 5e3\n\t\tfor index in xrange(len(queryStructures)):\n\t\t\tfor subIndex in xrange(len(queryStructures[index])):\n\t\t\t\ta = structures[index][subIndex]\n\t\t\t\tb = queryStructures[index][subIndex]\n\t\t\t\tdistance += (a - b) ** 2 / (a + b + eps)\n\t\treturn distance / normalizeRatio\n    \n    # \xe6\x8a\x8a\xe7\x9f\xa9\xe9\x98\xb5\xe8\xbd\xac\xe5\x8c\x96\xe4\xb8\xba\xe5\x90\x91\xe9\x87\x8f;\n\tdef transformRawQuery(self, rawQueryStructures):\n\t\tqueryStructures = []\n\t\tfor substructure in rawQueryStructures:\n\t\t\tstructure = []\n\t\t\tfor line in substructure:\n\t\t\t\tfor tripleColor in line:\n\t\t\t\t\tstructure.append(float(tripleColor))\n\t\t\tqueryStructures.append(structure)\n\t\treturn queryStructures\n\t    \n\tdef searchByStructure(self, rawQueryStructures):\n\t\tsearchResults = {}\n\t\tqueryStructures = self.transformRawQuery(rawQueryStructures)\n\t\twith open(self.structureIndexPath) as indexFile:\n\t\t\treader = csv.reader(indexFile)\n\t\t\tfor line in reader:\n\t\t\t\tstructures = []\n\t\t\t\tfor structure in line[1:]:\n\t\t\t\t\tstructure = structure.replace(""["", """").replace(""]"", """")\n\t\t\t\t\tstructure = re.split(""\\s+"", structure)\n\t\t\t\t\tif structure[0] == """":\n\t\t\t\t\t\tstructure = structure[1:]\n\t\t\t\t\tstructure = [float(eachValue) for eachValue in structure]\n\t\t\t\t\tstructures.append(structure)\n\t\t\t\tdistance = self.structureDistance(structures, queryStructures)\n\t\t\t\tsearchResults[line[0]] = distance\n\t\t\tindexFile.close()\n\t\t# print ""structure"", sorted(searchResults.iteritems(), key = lambda item: item[1], reverse = False)\n\t\treturn searchResults\n\n\n\t\n\tdef search(self, queryFeatures, rawQueryStructures, limit = 3):\n\t\tfeatureResults = self.searchByColor(queryFeatures)\n\t\tstructureResults = self.searchByStructure(rawQueryStructures)\n\t\tresults = {}\n\t\tfor key, value in featureResults.iteritems():\n\t\t\tresults[key] = value + structureResults[key]\n\t\tresults = sorted(results.iteritems(), key = lambda item: item[1], reverse = False)\n\t\treturn results[ : limit]\n\n'"
cv_python/imageSearch/structuredescriptor.py,0,"b'# -*- coding: utf-8 -*-\n#__author__ = \'DragonFive\'\n\nimport numpy as np\nimport cv2\n\nclass StructureDescriptor:\n    __slot__ = [""dimension""]\n    def __init__(self, dimension):\n        self.dimension = dimension\n    def describe(self, image):\n        image = cv2.resize(image, self.dimension, interpolation=cv2.INTER_CUBIC)\n        # image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n        return image\n\n\n\n\n'"
