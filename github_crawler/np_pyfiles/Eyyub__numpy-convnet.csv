file_path,api_count,code
mnist_loader.py,4,"b'# Credits to: http://neuralnetworksanddeeplearning.com/\n""""""\nmnist_loader\n~~~~~~~~~~~~\n\nA library to load the MNIST image data.  For details of the data\nstructures that are returned, see the doc strings for ``load_data``\nand ``load_data_wrapper``.  In practice, ``load_data_wrapper`` is the\nfunction usually called by our neural network code.\n""""""\n\n#### Libraries\n# Standard library\nimport cPickle\nimport gzip\n\n# Third-party libraries\nimport numpy as np\n\ndef load_data(path):\n    """"""Return the MNIST data as a tuple containing the training data,\n    the validation data, and the test data.\n\n    The ``training_data`` is returned as a tuple with two entries.\n    The first entry contains the actual training images.  This is a\n    numpy ndarray with 50,000 entries.  Each entry is, in turn, a\n    numpy ndarray with 784 values, representing the 28 * 28 = 784\n    pixels in a single MNIST image.\n\n    The second entry in the ``training_data`` tuple is a numpy ndarray\n    containing 50,000 entries.  Those entries are just the digit\n    values (0...9) for the corresponding images contained in the first\n    entry of the tuple.\n\n    The ``validation_data`` and ``test_data`` are similar, except\n    each contains only 10,000 images.\n\n    This is a nice data format, but for use in neural networks it\'s\n    helpful to modify the format of the ``training_data`` a little.\n    That\'s done in the wrapper function ``load_data_wrapper()``, see\n    below.\n    """"""\n    f = gzip.open(path, \'rb\')\n    training_data, validation_data, test_data = cPickle.load(f)\n    f.close()\n    return (training_data, validation_data, test_data)\n\ndef load_data_wrapper():\n    """"""Return a tuple containing ``(training_data, validation_data,\n    test_data)``. Based on ``load_data``, but the format is more\n    convenient for use in our implementation of neural networks.\n\n    In particular, ``training_data`` is a list containing 50,000\n    2-tuples ``(x, y)``.  ``x`` is a 784-dimensional numpy.ndarray\n    containing the input image.  ``y`` is a 10-dimensional\n    numpy.ndarray representing the unit vector corresponding to the\n    correct digit for ``x``.\n\n    ``validation_data`` and ``test_data`` are lists containing 10,000\n    2-tuples ``(x, y)``.  In each case, ``x`` is a 784-dimensional\n    numpy.ndarry containing the input image, and ``y`` is the\n    corresponding classification, i.e., the digit values (integers)\n    corresponding to ``x``.\n\n    Obviously, this means we\'re using slightly different formats for\n    the training data and the validation / test data.  These formats\n    turn out to be the most convenient for use in our neural network\n    code.""""""\n    tr_d, va_d, te_d = load_data()\n    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n    training_results = [vectorized_result(y) for y in tr_d[1]]\n    training_data = zip(training_inputs, training_results)\n    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n    validation_data = zip(validation_inputs, va_d[1])\n    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n    test_data = zip(test_inputs, te_d[1])\n    return (training_data, validation_data, test_data)\n\ndef vectorized_result(j):\n    """"""Return a 10-dimensional unit vector with a 1.0 in the jth\n    position and zeroes elsewhere.  This is used to convert a digit\n    (0...9) into a corresponding desired output from the neural\n    network.""""""\n    e = np.zeros((10, 1))\n    e[j] = 1.0\n    return e\n'"
mnist_test.py,12,"b""import numpy as np\nfrom nn.network import Network\nfrom nn.layer.fullyconnected import FullyConnected\nfrom nn.layer.flatten import Flatten\nfrom nn.layer.conv import Conv\nfrom nn.activation import sigmoid, relu , lkrelu , mse, linear, cross_entropy\nimport mnist_loader\n\ndef accuracy(net, X, Y):\n    a = (np.argmax(cross_entropy._softmax(net.forward(X)), axis=1) == np.argmax(Y, axis=1))\n    return np.sum(a) / float(X.shape[0]) * 100.\n\ndef one_hot(x, size):\n    a = np.zeros((x.shape[0], size))\n    a[np.arange(x.shape[0]), x] = 1.\n    return a\n\nif __name__ == '__main__':\n    batch_size = 20\n\n    # A simple strided convnet\n    layers = [\n        Conv((4, 4, 1, 20), strides=2, activation=lkrelu, filter_init=lambda shp: np.random.normal(size=shp) * np.sqrt(1.0 / (28*28 + 13*13*20)) ),\n        Conv((5, 5, 20, 40), strides=2, activation=lkrelu, filter_init=lambda shp:  np.random.normal(size=shp) *  np.sqrt(1.0 / (13*13*20 + 5*5*40)) ),\n        Flatten((5, 5, 40)),\n        FullyConnected((5*5*40, 100), activation=sigmoid, weight_init=lambda shp: np.random.normal(size=shp) * np.sqrt(1.0 / (5*5*40 + 100.))),\n        FullyConnected((100, 10), activation=linear, weight_init=lambda shp: np.random.normal(size=shp) * np.sqrt(1.0 / (110.)))\n    ]\n    lr = 0.001\n    k = 2000\n    net = Network(layers, lr=lr, loss=cross_entropy)\n\n    (train_data_X, train_data_Y), v, (tx, ty) = mnist_loader.load_data('./data/mnist.pkl.gz')\n    train_data_Y = one_hot(train_data_Y, size=10)\n    ty = one_hot(ty, size=10)\n    train_data_X = np.reshape(train_data_X, [-1, 28, 28, 1])\n    tx = np.reshape(tx, [-1, 28, 28, 1])\n    for epoch in xrange(100000):\n        shuffled_index = np.random.permutation(train_data_X.shape[0])\n\n        batch_train_X = train_data_X[shuffled_index[:batch_size]]\n        batch_train_Y = train_data_Y[shuffled_index[:batch_size]]\n        net.train_step((batch_train_X, batch_train_Y))\n\n        loss = np.sum(cross_entropy.compute((net.forward(batch_train_X), batch_train_Y)))\n        print 'Epoch: %d loss : %f' % (epoch, loss)\n        if epoch % 1000 == 1:\n            print 'Accuracy on first 500 test set\\'s batch : %f' % accuracy(net, tx[:500], ty[:500])\n        if epoch % 5000 == 5000 - 1:\n            print 'Accuracy over all test set %f' % accuracy(net, tx, ty)\n"""
nn/__init__.py,0,b''
nn/activation.py,6,"b'from abc import ABCMeta, abstractmethod\nimport numpy as np\n\nclass AbstractActivation(object):\n    __metaclass__ = ABCMeta\n\n    @abstractmethod\n    def compute(self, x):\n        raise NotImplementedError()\n\n    @abstractmethod\n    def deriv(self, x):\n        raise NotImplementedError()\n\nclass Relu(AbstractActivation):\n    def compute(self, x):\n        return np.maximum(0, x)\n\n    def deriv(self, x):\n        return 1. * (x > 0)\n\nclass LeakyRelu(AbstractActivation):\n    def compute(self, x):\n        return np.maximum(0.01, x)\n\n    def deriv(self, x):\n        g = 1. * (x > 0)\n        g[g == 0.] = 0.01\n        return g\n\nclass Sigmoid(AbstractActivation):\n    def compute(self, x):\n        return 1. / (1. + np.exp(-x))\n\n    def deriv(self, x):\n        y = self.compute(x)\n        return y * (1. - y)\n\nclass Linear(AbstractActivation):\n    def compute(self, x):\n        return x\n\n    def deriv(self, x):\n        return 1.\n\nclass Loss(AbstractActivation):\n    pass\n\nclass MeanSquaredError(Loss):\n    def compute(self, (X, Y)):\n        return (1. / 2. * X.shape[0]) * ((X - Y) ** 2.)\n\n    def deriv(self, (X, Y)):\n        return (X - Y) / X.shape[0]\n\nclass CrossEntropy(Loss):\n    def _softmax(self, X):\n        expvx = np.exp(X - np.max(X, axis=1)[..., np.newaxis])\n        return expvx/np.sum(expvx, axis=1, keepdims=True)\n\n    def compute(self, (X, Y)):\n        sf = self._softmax(X)\n        return -np.log(sf[np.arange(X.shape[0]), np.argmax(Y, axis=1)]) / X.shape[0]\n\n    def deriv(self, (X, Y)):\n        err = self._softmax(X)\n        return (err - Y) / X.shape[0]\n\nrelu = Relu()\nlkrelu = LeakyRelu()\nsigmoid = Sigmoid()\nlinear = Linear()\n\nmse = MeanSquaredError()\ncross_entropy = CrossEntropy()\n'"
nn/network.py,0,"b'import numpy as np\nfrom collections import deque\n\nclass Network(object):\n    def __init__(self, layers, lr, loss):\n        self.layers = layers\n        self.loss = loss\n        self._lr = lr\n\n    @property\n    def lr(self):\n        return self._lr\n\n    @lr.setter\n    def lr(self, v):\n        self._lr = v\n\n    def forward(self, inputs):\n        activation = inputs\n        for l in self.layers:\n            activation = l.forward(activation)\n        return activation\n\n    def train_step(self, mini_batch):\n        mini_batch_inputs, mini_batch_outputs = mini_batch\n        zs = deque([mini_batch_inputs])\n        activation = mini_batch_inputs\n        for l in self.layers:\n            z, activation = l.train_forward(activation)\n            zs.appendleft(z)\n\n        loss_err = self.loss.deriv((activation, mini_batch_outputs))\n        lz = zs.popleft()\n        backwarded_err = loss_err\n        grads = deque()\n        for l in reversed(self.layers):\n            layer_err = l.get_layer_error(lz, backwarded_err) #local\n            lz = zs.popleft()\n            grads.appendleft(l.get_grad(lz, layer_err))\n            backwarded_err = l.backward(layer_err) # backwarded error\n\n        # update step\n        for l in self.layers:\n            l.update(self.lr * grads.popleft())\n\n        assert len(grads) == 0\n'"
nn/layer/__init__.py,0,b''
nn/layer/conv.py,10,"b'import numpy as np\nfrom layer import AbstractLayer\n\nclass Conv(AbstractLayer):\n    def __init__(self, fshape, activation, filter_init, strides=1):\n        self.fshape = fshape\n        self.strides = strides\n        self.filters = filter_init(self.fshape)\n        self.activation = activation\n\n    def forward(self, inputs):\n        s = (inputs.shape[1] - self.fshape[0]) / self.strides + 1\n        fmap = np.zeros((inputs.shape[0], s, s, self.fshape[-1]))\n        for j in xrange(s):\n            for i in xrange(s):\n                fmap[:, j, i, :] = np.sum(inputs[:, j * self.strides:j * self.strides + self.fshape[0], i * self.strides:i * self.strides + self.fshape[1], :, np.newaxis] * self.filters, axis=(1, 2, 3))\n        return self.activation.compute(fmap)\n\n    def train_forward(self, inputs):\n        s = (inputs.shape[1] - self.fshape[0]) / self.strides + 1\n        fmap = np.zeros((inputs.shape[0], s, s, self.fshape[-1]))\n        for j in xrange(s):\n            for i in xrange(s):\n                fmap[:, j, i, :] = np.sum(inputs[:, j * self.strides:j * self.strides + self.fshape[0], i * self.strides:i * self.strides + self.fshape[1], :, np.newaxis] * self.filters, axis=(1, 2, 3))\n        return (fmap, self.activation.compute(fmap))\n\n    def get_layer_error(self, z, backwarded_err):\n        return backwarded_err * self.activation.deriv(z)\n\n    def backward(self, layer_err):\n        bfmap_shape = (layer_err.shape[1] - 1) * self.strides + self.fshape[0]\n        backwarded_fmap = np.zeros((layer_err.shape[0], bfmap_shape, bfmap_shape, self.fshape[-2]))\n        s = (backwarded_fmap.shape[1] - self.fshape[0]) / self.strides + 1\n        for j in xrange(s):\n            for i in xrange(s):\n                backwarded_fmap[:, j * self.strides:j  * self.strides + self.fshape[0], i * self.strides:i * self.strides + self.fshape[1]] += np.sum(self.filters[np.newaxis, ...] * layer_err[:, j:j+1, i:i+1, np.newaxis, :], axis=4)\n        return backwarded_fmap\n\n    def get_grad(self, inputs, layer_err):\n        total_layer_err = np.sum(layer_err, axis=(0, 1, 2))\n        filters_err = np.zeros(self.fshape)\n        s = (inputs.shape[1] - self.fshape[0]) / self.strides + 1\n        summed_inputs = np.sum(inputs, axis=0)\n        for j in xrange(s):\n            for i in xrange(s):\n                filters_err += summed_inputs[j  * self.strides:j * self.strides + self.fshape[0], i * self.strides:i * self.strides + self.fshape[1], :, np.newaxis]\n        return filters_err * total_layer_err\n\n    def update(self, grad):\n        self.filters -= grad\n'"
nn/layer/flatten.py,3,"b'import numpy as np\nfrom layer import AbstractLayer\n\nclass Flatten(AbstractLayer):\n    def __init__(self, shape):\n        self.shape = shape\n\n    def forward(self, inputs):\n        return np.reshape(inputs, (inputs.shape[0], -1))\n\n    def train_forward(self, inputs):\n        z = np.reshape(inputs, (inputs.shape[0], -1))\n        return (z, z)\n\n    def get_layer_error(self, z, next_layer_err):\n        return next_layer_err\n\n    def backward(self, layer_err):\n        return np.reshape(layer_err, (layer_err.shape[0],) + self.shape)\n\n    def get_grad(self, inputs, layer_err):\n        return 0.\n\n    def update(self, grad):\n        pass\n'"
nn/layer/fullyconnected.py,0,"b'from layer import AbstractLayer\n\nclass FullyConnected(AbstractLayer):\n    def __init__(self, wshape, activation, weight_init):\n        self.wshape = wshape\n        self.W = weight_init(self.wshape)\n        self.activation = activation\n\n    def forward(self, inputs):\n        return self.activation.compute(inputs.dot(self.W))\n\n    def train_forward(self, inputs):\n        z = inputs.dot(self.W)\n        return (z, self.activation.compute(z))\n\n    def get_layer_error(self, z, backwarded_err):\n        return backwarded_err * self.activation.deriv(z)\n\n    def backward(self, layer_err):\n        return layer_err.dot(self.W.T)\n\n    def get_grad(self, inputs, layer_err):\n        return inputs.T.dot(layer_err)\n\n    def update(self, grad):\n        self.W -= grad\n'"
nn/layer/layer.py,0,"b'from abc import ABCMeta, abstractmethod\n\nclass AbstractLayer(object):\n    __metaclass__ = ABCMeta\n\n    @abstractmethod\n    def forward(self, inputs):\n        raise NotImplementedError()\n\n    @abstractmethod\n    def train_forward(self, inputs):\n        raise NotImplementedError()\n\n    @abstractmethod\n    def get_layer_error(self, z, backwarded_err):\n        raise NotImplementedError()\n\n    @abstractmethod\n    def backward(self, layer_err):\n        raise NotImplementedError()\n\n    @abstractmethod\n    def get_grad(self, inputs, layer_err):\n        raise NotImplementedError()\n\n    @abstractmethod\n    def update(self, grad):\n        raise NotImplementedError()\n'"
