file_path,api_count,code
setup.py,0,"b'from setuptools import setup\nfrom setuptools import find_packages\n\nwith open(""README.md"", ""r"") as fh:\n    long_description = fh.read()\n\nsetup(\n    name=\'drlkit\',\n    version=\'0.1.7\',\n    description=\'A High Level Python Deep Reinforcement Learning library. Great for beginners, for prototyping and quickly comparing algorithms\',\n    install_requires=[\n            \'torch\',\n            \'numpy\',\n            \'tqdm\',\n            \'matplotlib\',\n            \'box2d-py\',\n            \'gym\',\n            \'keras\',\n            \'scikit-image\',\n        ],\n    license=\'MIT\',\n    py_modules=[\'drlkit\'],\n    classifiers=[\n        ""Programming Language :: Python :: 3"",\n        ""Programming Language :: Python :: 3.6"",\n        ""Programming Language :: Python :: 3.7"",\n        ""License :: OSI Approved :: GNU General Public License v2 or later (GPLv2+)"",\n        ""Operating System :: OS Independent"",\n        ],\n    long_description=long_description,\n    long_description_content_type=""text/markdown"",\n    url=""https://github.com/FranckNdame/drlkit"",\n    author=""Franck Ndame"",\n    author_email=""franck.mpouli@gmail.com"",\n    packages=find_packages(),\n    python_requires=\'>=3.6\',\n)\n'"
drlkit/__init__.py,0,b''
drlkit/main.py,1,"b'import numpy as np\nfrom agents.DQNAgent import DQNAgent\nfrom utils.plot import Plot\nfrom environments.wrapper import EnvironmentWrapper\nimport os\n\n#import models\nENV_NAME = ""LunarLander-v2""\nenv = EnvironmentWrapper(ENV_NAME, max_ts=1000)\nagent = DQNAgent(state_size=8, action_size=env.env.action_space.n, seed=0)\n\n### Train the agent\n#env.fit(agent, n_episodes=2000)\n\n# See the results\n#Plot.basic_plot(np.arange(len(env.scores)), env.scores, xlabel=\'Episode #\', ylabel=\'Score\')\n\nenv.load_model(agent, ""./models/LunarLander-v2-4477.pth"")\nenv.play(num_episodes=15, trained=False)\n\n'"
drlkit/agents/DQNAgent.py,2,"b'from __future__ import absolute_import\nimport torch\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport numpy as np\n\n\nimport sys\nimport os\nimport random\nsys.path.append("".."")\n\nfrom drlkit.utils.memory import Memory\nfrom drlkit.utils import preprocess as pr\nfrom drlkit.networks.pytorch.DQN import DQN\n\n# DQN, Dueling QN, Policy gradient, Actor critic\n\n# Check if GPU is available\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\nclass DQNAgent(object):\n\tdef __init__(\n\t\tself, state_size, action_size, seed=0,\n\t\tbuffer_size = 1_000_000, batch_size=64, gamma=0.99,\n\t\ttau=1e-3, lr=5e-4, update_every=4, network=""dqn""\n\t):\n\t\t""""""\n\t\tInitialize a Torch Agent object.\n\t\t\t\t\n\t\tParams\n\t\t======\n\t\t\tstate_size (int): dimension of each state\n\t\t\taction_size (int): dimension of each action\n\t\t\tseed (int): random seed\n\t\t\t\n\t\tHyper Params\n\t\t======\n\t\t\tbuffer_size (int): replay buffer size\n\t\t\tbatch_size (int): batch size\n\t\t\tgamma (float): discount rate\n\t\t\ttau float: soft update of target parameters\n\t\t\tlr (float): learning rate\n\t\t\tupdate_every (int): network update frequency\n\t\t""""""\n\t\t\n\t\t# Params\n\t\tself.state_size = state_size\n\t\tself.action_size = action_size\n\t\tself.seed = random.seed(seed)\n\t\t\n\t\t# Hyperparameters\n\t\tself.gamma = gamma\n\t\tself.tau = tau\n\t\tself.update_every = update_every\n\t\t\n\t\t# Policy Network\n\t\tself.policy_network = DQN(state_size, action_size, seed).to(device)\n\t\n\t\t# Target Network\n\t\tself.target_network =  DQN(state_size, action_size, seed).to(device)\n\t\t# Optimizer\n\t\tself.optimizer = optim.Adam(self.policy_network.parameters(), lr=lr)\n\t\t\n\t\t# Experience replay\n\t\tself.memory = Memory(action_size, seed, buffer_size, batch_size)\n\t\t\n\t\tself.time_step = 0\n\t\tself.loss = 0\n\t\t\n\t\t\n\tdef step(self, state, action, reward, next_state, done):\n\t\t# Save experience to memory\n\t\tself.memory.save(state, action, reward, next_state, done)\n\t\t\n\t\t# learn i.e update weights at self.update_every\n\t\tself.time_step = (self.time_step + 1) % self.update_every\n\t\tif not self.time_step:\n\t\t\t# If the memory can provide sample\n\t\t\tif self.memory.can_provide:\n\t\t\t\texperience = self.memory.sample()\n\t\t\t\tself.learn(experience, self.gamma)\n\t\t\t\t\n\t\t\n\tdef act(self, state, eps=0.):\n\t\t""""""Returns an action for the given state following the current policy\n\t\t\n\t\tParams\n\t\t======\n\t\t\tstate (list): current state\n\t\t\teps (float): exploration rate\n\t\t\n\t\t""""""\n\t\t# Unsqueeze: TODO\n\t\t\n\t\tstate = torch.from_numpy(state).float().unsqueeze(0).to(device)\n\t\tself.policy_network.eval()\n\t\twith torch.no_grad():\n\t\t\taction_values = self.policy_network(state)\n\t\tself.policy_network.train()\n\t\t\n\t\tif random.random() > eps:\n\t\t\t# Select greedy action\n\t\t\treturn np.argmax(action_values.cpu().data.numpy())\n\t\telse:\n\t\t\t# select stochastic action\n\t\t\treturn random.choice(np.arange(self.action_size))\n\t\t\t\n\t\t\n\tdef learn(self, experiences, gamma):\n\t\t""""""Update weights and bias using batch of experience tuples.\n\t\t\n\t\tParams\n\t\t======\n\t\t\texperiences (Tuple[torch.Tensor]): tuple of state, action, reward, next_state, done tuples\n\t\t\tgamma (float): Discount rate\n\t\t""""""\n\t\t\n\t\tstates, actions, rewards, next_states, dones = experiences\n\t\t\n\t\t# Get the predicted action for the next state from the target network\n\t\tQ_targets_next = self.target_network(next_states).detach().max(1)[0].unsqueeze(1)\n\t\t\n\t\t# Get Q target for current state\n\t\tQ_target = rewards + (gamma * Q_targets_next * (1- dones))\n\t\t\n\t\t# Get expected Q from policy model\n\t\tQ_expected = self.policy_network(states).gather(1, actions)\n\t\t\n\t\t# Compute loss\n\t\t""""""\n\t\tloss = (sqrt[target_network^2 - policy_network^2])\n\t\t""""""\n\t\tloss = F.mse_loss(Q_expected, Q_target)\n\t\tself.loss = loss\n\t\t\n\t\t\n\t\t# Minimize loss\n\t\tself.optimizer.zero_grad() # Clear gradients\n\t\tloss.backward()\n\t\tself.optimizer.step() # Perform a single optimization step\n\t\t# Update target network\n\t\tself.soft_update(self.policy_network, self.target_network, self.tau)\n\t\t\n\tdef soft_update(self, policy_model, target_model, tau):\n\t\t""""""Soft update model parameters.\n\t\t\xce\xb8_target = \xcf\x84*\xce\xb8_policy+ (1 - \xcf\x84)*\xce\xb8_target\n\n\t\tParams\n\t\t======\n\t\t\tpolicy_model: weights will be copied from\n\t\t\ttarget_model: weights will be copied to\n\t\t\ttau (float): interpolation parameter \n\t\t""""""\n\t\tfor target_param, policy_param in zip(target_model.parameters(), policy_model.parameters()):\n\t\t\ttarget_param.data.copy_(tau*policy_param.data + (1.0-tau)*target_param.data)\n\t\t\t\t\n\t\t\t\t'"
drlkit/agents/TD3Agent.py,0,"b'from __future__ import absolute_import\nimport torch\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport numpy as np\n\n\nimport sys\nimport os\nimport random\nsys.path.append("".."")\n\nfrom drlkit.utils.memory import Memory\nfrom drlkit.utils import preprocess as pr\nfrom drlkit.networks.pytorch.DQN import DQN\n\n# Check if GPU is available\n# Selecting the device (CPU or GPU)\ndevice = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\n# Building the whole Training Process into a class\n\nclass TD3(object):\n\t\n\tdef __init__(self, state_space, action_space, max_action. seed=0):\n\t\tself.actor = Actor(state_space, action_space, max_action).to(device)\n\t\tself.actor_target = Actor(state_space, action_space, max_action).to(device)\n\t\tself.actor_target.load_state_dict(self.actor.state_dict())\n\t\tself.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n\t\tself.critic = Critic(state_space, action_space).to(device)\n\t\tself.critic_target = Critic(state_space, action_space).to(device)\n\t\tself.critic_target.load_state_dict(self.critic.state_dict())\n\t\tself.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n\t\tself.max_action = max_action\n\t\t\n\t\tself.memory = Memory(action_size, seed, buffer_size, batch_size)\n\n\tdef select_action(self, state):\n\t\tstate = torch.Tensor(state.reshape(1, -1)).to(device)\n\t\treturn self.actor(state).cpu().data.numpy().flatten()\n\n\tdef train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n\t\t\n\t\tfor it in range(iterations):\n\t\t\t\n\t\t\t# Step 4: We sample a batch of transitions (s, s\xe2\x80\x99, a, r) from the memory\n\t\t\tbatch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = self.memory.sample(batch_size)\n\t\t\tstate = torch.Tensor(batch_states).to(device)\n\t\t\tnext_state = torch.Tensor(batch_next_states).to(device)\n\t\t\taction = torch.Tensor(batch_actions).to(device)\n\t\t\treward = torch.Tensor(batch_rewards).to(device)\n\t\t\tdone = torch.Tensor(batch_dones).to(device)\n\t\t\t\n\t\t\t# Step 5: From the next state s\xe2\x80\x99, the Actor target plays the next action a\xe2\x80\x99\n\t\t\tnext_action = self.actor_target(next_state)\n\t\t\t\n\t\t\t# Step 6: We add Gaussian noise to this next action a\xe2\x80\x99 and we clamp it in a range of values supported by the environment\n\t\t\tnoise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n\t\t\tnoise = noise.clamp(-noise_clip, noise_clip)\n\t\t\tnext_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n\t\t\t\n\t\t\t# Step 7: The two Critic targets take each the couple (s\xe2\x80\x99, a\xe2\x80\x99) as input and return two Q-values Qt1(s\xe2\x80\x99,a\xe2\x80\x99) and Qt2(s\xe2\x80\x99,a\xe2\x80\x99) as outputs\n\t\t\ttarget_Q1, target_Q2 = self.critic_target(next_state, next_action)\n\t\t\t\n\t\t\t# Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n\t\t\ttarget_Q = torch.min(target_Q1, target_Q2)\n\t\t\t\n\t\t\t# Step 9: We get the final target of the two Critic models, which is: Qt = r + \xce\xb3 * min(Qt1, Qt2), where \xce\xb3 is the discount factor\n\t\t\ttarget_Q = reward + ((1 - done) * discount * target_Q).detach()\n\t\t\t\n\t\t\t# Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n\t\t\tcurrent_Q1, current_Q2 = self.critic(state, action)\n\t\t\t\n\t\t\t# Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n\t\t\tcritic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n\t\t\t\n\t\t\t# Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n\t\t\tself.critic_optimizer.zero_grad()\n\t\t\tcritic_loss.backward()\n\t\t\tself.critic_optimizer.step()\n\t\t\t\n\t\t\t# Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n\t\t\tif it % policy_freq == 0:\n\t\t\t\tactor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n\t\t\t\tself.actor_optimizer.zero_grad()\n\t\t\t\tactor_loss.backward()\n\t\t\t\tself.actor_optimizer.step()\n\t\t\t\t\n\t\t\t\t# Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n\t\t\t\tfor param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n\t\t\t\t\ttarget_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n\t\t\t\t\n\t\t\t\t# Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n\t\t\t\tfor param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n\t\t\t\t\ttarget_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n\t\n\t# Making a save method to save a trained model\n\tdef save(self, filename, directory):\n\t\ttorch.save(self.actor.state_dict(), \'%s/%s_actor.pth\' % (directory, filename))\n\t\ttorch.save(self.critic.state_dict(), \'%s/%s_critic.pth\' % (directory, filename))\n\t\n\t# Making a load method to load a pre-trained model\n\tdef load(self, filename, directory):\n\t\tself.actor.load_state_dict(torch.load(\'%s/%s_actor.pth\' % (directory, filename)))\n\t\tself.critic.load_state_dict(torch.load(\'%s/%s_critic.pth\' % (directory, filename)))'"
drlkit/agents/__init__.py,0,b''
drlkit/environments/__init__.py,0,b''
drlkit/environments/wrapper.py,5,"b'import gym\nimport os\nfrom collections import deque\nimport sys\nimport torch\nimport numpy as np\nsys.path.append("".."")\nfrom drlkit import models\nfrom drlkit.utils.exceptions import AgentMissing\n\nclass EnvironmentWrapper(object):\n\tdef __init__(\n\t\tself, name, max_ts=2_500, \n\t\teps_start=1.0, eps_min =0.01, eps_decay=0.995,\n\t\tseed=0, print_info=False\n\t):\n\t\t# initialise environment\n\t\tself.env_name = name\n\t\tenv = gym.make(name)\n\t\tenv.seed(seed)\n\t\tself.env = env\n\t\t\n\t\t# track scores\n\t\tself.scores = []\n\t\tself.scores_window = deque(maxlen=100)\n\t\t\n\t\t# exploration - exploitation\n\t\tself.eps = eps_start\n\t\tself.max_ts = max_ts\n\t\tself.eps_min = eps_min\n\t\tself.eps_decay = eps_decay\n\t\t\n\t\t# debug\n\t\tself.print_info = print_info\n\t\t\n\t\tself.done = False\n\t\tself.best_score = 0\n\t\t\n\t\t\n\tdef fit(self, agent, n_episodes, save_every=None):\n\t\tself.agent = agent\n\t\tself.n_episodes = n_episodes\n\t\tif not save_every:\n\t\t\tsave_every = n_episodes//3\n\t\tself.save_every = save_every\n\t\tfor i_episode in range(1, n_episodes+1):\n\t\t\tstate = self.env.reset()\n\t\t\tscore = 0\n\t\t\tfor t in range(self.max_ts):\n\t\t\t\taction = agent.act(state, self.eps)\n\t\t\t\tnext_state, reward, self.done, info = self.env.step(action)\n\t\t\t\tif self.print_info: print(info)\n\t\t\t\tagent.step(state, action, reward, next_state, self.done)\n\t\t\t\tstate = next_state\n\t\t\t\tscore += reward\n\t\t\t\tif self.done:\n\t\t\t\t\tbreak\n\t\t\t# After the episode\n\t\t\tself.scores_window.append(score) # push recent score\n\t\t\tself.scores.append(score) # save recent score\n\t\t\tself.eps = max(self.eps_min, self.eps*self.eps_decay) # decrease exploration rate\n\t\t\tself.monitor_progress(i_episode)\n\t\tprint(""==== Training complete! ====="")\n\t\tprint(f""# Episodes: {n_episodes} || score: {np.mean(self.scores_window)}"")\n\t\t\n\t\t\t\n\tdef monitor_progress(self, episode, training=True):\n\t\tprint(\'\\rEpisode {}\\tAverage Score: {:.2f}\'.format(episode, np.mean(self.scores_window)), end="""")\n\t\tif not episode % 100:\n\t\t\tprint(\'\\rEpisode {}\\tAverage Score: {:.2f}\'.format(episode, np.mean(self.scores_window)))\n\t\t\tprint(f""Loss: {self.agent.loss}\\n==================================\\n"")\n\t\tif training:\n\t\t\tif (not episode % self.save_every) or episode == self.n_episodes:\n\t\t\t\tprint(\'\\nSaving agent @ {:d} episodes!\\tAverage Score: {:.2f}\'.format(episode, np.mean(self.scores_window)))\n\t\t\t\tfilename = f\'{self.env_name}-{episode}.pth\'\n\t\t\t\tself.best_score = np.mean(self.scores_window)\n\t\t\t\tself.save_model(filename, self.agent.target_network.state_dict())\n\t\t\t\n\t\t\n\t\t\t\t\n\t\t\n\tdef save_model(self, filename, file):\n\t\t""""""Save model\n\t\t\t\t\n\t\tParams\n\t\t======\n\t\t\tdir (string): directory\n\t\t\tfilename (string): filename\n\t\t\tfile (dictionary): model parameters\t\t\n\t\t""""""\n\n\t\tpath = ""./models/""\n\t\tif not os.path.exists(path):\n\t\t\tos.makedirs(path)\n\t\t\tprint(""Directory created"")\n\t\ttorch.save(file, path+filename)\n\t\tprint(f""Model saved! @ {path+filename}"")\n\t\t\t\n\tdef list_models(self):\n\t\tpath = path = ""./models/""\n\t\tif not os.path.exists(path):\n\t\t\tprint(f""No model found!"")\n\t\telse:\n\t\t\tlst = os.listdir(path)\n\t\t\tprint(f""Models Available"")\n\t\t\tprint(""==========================="")\n\t\t\ti = 1\n\t\t\tfor item in lst:\n\t\t\t\tif item != ""__init__.py"":\n\t\t\t\t\tprint(f""{i} - {item}"") \n\t\t\t\t\ti += 1\n\t\t\n\t\t\n\t\t\n\tdef load_model(self, agent, path):\n\t\t\tself.agent = agent\n\t\t\tif not os.path.exists(path):\n\t\t\t\tprint(f""No such model saved!  @ {path}"")\n\t\t\t\treturn\n\t\t\tif torch.cuda.is_available():\t\n\t\t\t\tself.agent.policy_network.load_state_dict(torch.load(path))\n\t\t\telse:\n\t\t\t\tself.agent.policy_network.load_state_dict(torch.load(path, map_location=torch.device(\'cpu\')))\n\t\t\t\t\n\t\t\t\t\n\t\t\tprint(""Model Loaded!"")\n\t\t\n\t\t\n\tdef play(self, num_episodes=10, max_ts=200, trained=True, plot=True):\n\t\tif not self.agent:\n\t\t\traise AgentMissing()\n\t\tfor i in range(1,num_episodes+1):\n\t\t\tstate = self.env.reset()\n\t\t\tscore = 0\n\t\t\tfor ts in range(max_ts):\n\t\t\t\taction = self.agent.act(state) if trained else self.env.action_space.sample()\n\t\t\t\tself.env.render()\n\t\t\t\tstate, reward, done, _ = self.env.step(action)\n\t\t\t\tscore += reward\n\t\t\t\t\n\t\t\t\tif done:\n\t\t\t\t\tbreak \n\t\t\tself.scores_window.append(score) # push recent score\n\t\t\tself.scores.append(score) # save recent score\n\t\t\tself.monitor_progress(i, False)\n\t\t\t\t\t\n\t\tself.env.close()'"
drlkit/models/__init__.py,0,b''
drlkit/networks/__init__.py,0,b''
drlkit/utils/__init__.py,0,b''
drlkit/utils/exceptions.py,0,"b'class AgentMissing(Exception):\n\t"""""" Raised when play() is called before training or loading the agent""""""\n\tmessage = ""No agent was initialized""\n\t\n\tdef __str__(self):\n\t\treturn AgentMissing.message'"
drlkit/utils/memory.py,5,"b'from collections import deque, namedtuple\nimport numpy as np\nimport random\nimport torch\n\n# Check if GPU is available\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\nclass Memory(object):\n    """"""Replay memory buffer to store experience tuples.""""""\n    def __init__(self, action_size, seed, buffer_size=1000000, batch_size=64):\n        """"""Initialize a ReplayBuffer object.\n        \n        Params\n        =====1``````````=\n            action_size (int): size of each action\n            buffer_size (int): size of memory buffer\n            batch_size (int): size of each training minibatch\n            seed (int): random seed\n        """"""\n        self.action_size = action_size\n        self.buffer = deque(maxlen=buffer_size)\n        self.batch_size = batch_size\n        self.seed = seed\n        \n        self.experience = namedtuple(""Experience"", field_names=[""state"", ""action"", ""reward"", ""next_state"", ""done""])\n\n    def save(self, state, action, reward, next_state, done):\n        """"""Save new experience to buffer""""""\n        experience = self.experience(state, action, reward, next_state, done)\n        self.buffer.append(experience)\n    \n    def sample(self):\n        """"""Sample a batch of random experiences from memory""""""\n        experiences = random.sample(self.buffer, k=self.batch_size)\n\n        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n        \n        return (states, actions, rewards, next_states, dones)\n        \n        \n    @property\n    def can_provide(self):\n        """"""Check if the buffer can provide samples""""""\n        return len(self.buffer) >= self.batch_size\n'"
drlkit/utils/plot.py,0,"b'import matplotlib.pyplot as plt\nimport numpy as np\n\nclass Plot(object):\n\t\t\n\t@staticmethod\n\tdef basic_plot(x, y, xlabel=""x-axis"", ylabel=""y-axis""):\n\t\t""""""Generate a basic plot\n\t\t\n\t\tParams\n\t\t======\n\t\tx (list): x-axis\n\t\ty (list): y-axis\n\t\txlabel (string): x-axis label\n\t\tylabel (string): y-axis label\n\t\t""""""\n\t\t# plot the scores\n\t\tfig = plt.figure()\n\t\tax = fig.add_subplot(111)\n\t\tplt.plot(x, y)\n\t\tplt.ylabel(ylabel)\n\t\tplt.xlabel(xlabel)\n\t\tplt.show()'"
drlkit/utils/preprocess.py,3,"b'from skimage import transform # Preprocess the frames || scikit-image\nfrom skimage.color import rgb2gray # Grayscale the frames\nfrom collections import deque\n\n\ndef preprocess(frame, stacked_frames, crop=(5,-10,5,-10), new_size=[100,80], stack_size=4, is_new_episode=True, state_w=110, state_h=84):\n    preprocessed_frame = preprocess_frame(frame, crop, new_size)\n    return stack_frames(stacked_frames, preprocessed_frame, is_new_episode, stack_size, state_w, state_h)\n\n\ndef preprocess_frame(frame, crop, new_size):\n    # Grayscale frame\n    gs_frame = rgb2gray(frame)\n    # Crop unnecessary pixels\n    cropped_frame = gs_frame[crop[0]:crop[1], crop[2]:crop[3]]\n    # Normalize pixel values 0 - 255 to 0 - 1\n    normalized_frame = cropped_frame/255.0\n    # Resize\n    resized_frame = transform.resize(normalized_frame, new_size)\n    return resized_frame\n\ndef initialize_stack_frame(width=110, height=84, max_len=4):\n    return  deque([np.zeros((width, height), dtype=np.int) for _ in range(stack_size)], maxlen=max_len)\n\ndef stack_frames(stacked_frames, state, is_new_episode, stack_size=4, state_w=110, state_h=84):\n    # Preprocess frame\n    # frame = preprocess_frame(state)\n\n    if is_new_episode:\n        # Clear stacked frames\n        stacked_frames = initialize_stack_frame(state_w, state_h)\n        # since this is a new episode, we only copy the same frame 4x\n        for _ in range(stack_size):\n            stacked_frames.append(frame)\n        # Stack the frames\n        stacked_state = np.stack(stacked_frames, axis=2) # ??\n    else:\n        # Push new frame to queue and pop oldest frame\n        stacked_frames.append(frame)\n        # Build the stacked state (first dimensuin specifies different frames)\n        stacked_state = np.stack(stacked_frames, axis=2)\n\n    return stacked_state, stacked_frames\n'"
drlkit/networks/pytorch/ConvDQN.py,0,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ConvDQN(nn.Module):\n\t""""""Agent Model.""""""\n\t\n\tdef __init__(self, state_size, action_size, seed, layer1_units=64, layer2_units=64):\n\t\t""""""Initialize parameters and build model.\n\t\t\t\tParams\n\t\t\t\t======\n\t\t\t\t\tstate_size (int): Dimension of each state\n\t\t\t\t\taction_size (int): Dimension of each action\n\t\t\t\t\tseed (int): Random seed\n\t\t\t\t\tlayer1_units (int): Number of nodes in first hidden layer\n\t\t\t\t\tlayer2_units (int): Number of nodes in second hidden layer\n\t\t""""""\n\t\t\n\t\tsuper(DQN, self).__init__()\n\t\tself.seed = torch.manual_seed(seed)\n\t\tself.conv1 = nn.Conv1d(state_size, layer1_units, (4,4))\n\t\tself.conv2 = nn.Conv1d(layer1_units, layer2_units, (2,2))\n\t\tself.fc3 = nn.Linear(layer2_units, action_size)\n\t\t\n\tdef forward(self, x):\n\t\tx = F.relu(self.conv1(x))\n\t\tx = F.relu(self.conv2(x))\n\t\treturn self.fc3(x)\n'"
drlkit/networks/pytorch/DQN.py,0,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass DQN(nn.Module):\n\t""""""Agent Model.""""""\n\t\n\tdef __init__(self, state_size, action_size, seed, layer1_units=64, layer2_units=64):\n\t\t""""""Initialize parameters and build model.\n\t\t\t\tParams\n\t\t\t\t======\n\t\t\t\t\tstate_size (int): Dimension of each state\n\t\t\t\t\taction_size (int): Dimension of each action\n\t\t\t\t\tseed (int): Random seed\n\t\t\t\t\tlayer1_units (int): Number of nodes in first hidden layer\n\t\t\t\t\tlayer2_units (int): Number of nodes in second hidden layer\n\t\t""""""\n\t\t\n\t\tsuper(DQN, self).__init__()\n\t\tself.seed = torch.manual_seed(seed)\n\t\tself.fc1 = nn.Linear(state_size, layer1_units)\n\t\tself.fc2 = nn.Linear(layer1_units, layer2_units)\n\t\tself.fc3 = nn.Linear(layer2_units, action_size)\n\t\t\n\tdef forward(self, x):\n\t\tx = F.relu(self.fc1(x))\n\t\tx = F.relu(self.fc2(x))\n\t\treturn self.fc3(x)\n'"
drlkit/networks/pytorch/TD3.py,0,"b'import torch\nimport torch.nn as nn\nimport torch.nn.Functional as F\n\nclass Actor(nn.Module):\n\t"""""" Neural Network for the Actor Model """"""\n\t\n\tdef __init__(self, state_size, action_size, max_action, seed=0, layer1_units= 400, layer2_units=300):\n\t\t""""""Initialize parameters and build model.\n\t\t\t\tParams\n\t\t\t\t======\n\t\t\t\t\tstate_size (int): Dimension of each state\n\t\t\t\t\taction_size (int): Dimension of each action\n\t\t\t\t\tseed (int): Random seed\n\t\t\t\t\tlayer1_units (int): Number of nodes in first hidden layer\n\t\t\t\t\tlayer2_units (int): Number of nodes in second hidden layer\n\t\t""""""\n\t\t\n\t\tsuper(Actor, self).__init__()\n\t\tself.layer_1 = nn.Linear(state_size, layer1_units)\n\t\tself.layer_2 = nn.Linear(layer1_units, layer2_units)\n\t\tself.layer_3 = nn. Linear(layer2_units, action_size)\n\t\tself.max_action = max_action\n\t\t\n\tdef forward(self, x):\n\t\tx = F.relu(self.layer_1(x))\n\t\tx = F.relu(self.layer_2(x))\n\t\tx = self.max_action * torch.tanh(self.layer_3(x))\n\t\treturn x\n\t\t\n\nclass Critic(nn.Module):\n\t"""""" Neural Network for the Critic Model """"""\n\t\n\tdef __init__(self, state_size, action_size, seed=0, first_layer_units=400, second_layer_units=300):\n\t\t""""""Initialize parameters and build model.\n\t\t\t\tParams\n\t\t\t\t======\n\t\t\t\t\tstate_size (int): Dimension of each state\n\t\t\t\t\taction_size (int): Dimension of each action\n\t\t\t\t\tseed (int): Random seed\n\t\t\t\t\tlayer1_units (int): Number of nodes in first hidden layer\n\t\t\t\t\tlayer2_units (int): Number of nodes in second hidden layer\n\t\t""""""\n\t\t\n\t\tsuper(Critic, self).__init__()\n\t\t# First Critic Network\n\t\tself.layer_1 = nn.Linear(state_size + action_size, first_layer_units)\n\t\tself.layer_2 = nn.Linear(first_layer_units, second_layer_units)\n\t\tself.layer_3 = nn.Linear(second_layer_units, 1)\n\t\t\n\t\t# Second Critic Network\n\t\tself.layer_4 = nn.Linear(state_size + action_size, first_layer_units)\n\t\tself.layer_5 = nn.Linear(first_layer_units, second_layer_units)\n\t\tself.layer_6 = nn.Linear(second_layer_units, 1)\n\t\t\n\t\t\n\tdef forward(self, x, u):\n\t\txu = torch.cat([x, u], 1)\n\t\t\n\t\t# Forward Propagation on the first Critic neural Network\n\t\tx1 = F.relu(self.layer_1(xu))\n\t\tx1 = F.relu(self.layer_2(x1))\n\t\tx1 = self.layer_3(x1)\n\t\t\n\t\tx2 = F.relu(self.layer_4(xu))\n\t\tx2 = F.relu(self.layer_5(x2))\n\t\tx2 = self.layer_6(x2)\n\t\t\n\t\treturn x1, x2\n\t\t\n\tdef Q1(self, x, u):\n\t\t# Concatebate x and u\n\t\txu = torch.cat([x,u], 1)\n\t\tx1 = F.relu(self.layer_1(xu))\n\t\tx1 = F.relu(self.layer_2(x1))\n\t\tx1 = self.layer_3(x1)\n\t\treturn x1\n\t\t\n\tdef Q2(self, x, u):\n\t\txu = torch.cat([x, u], 1)\n\t\tx2 = F.relu(self.layer_4(xu))\n\t\tx2 = F.relu(self.layer_5(xu))\n\t\tx2 = self.layer_6(xu)\n\t\treturn x2\n\t\t\t\n\t\n\t\t\n\t\t'"
drlkit/networks/pytorch/__init__.py,0,b''
