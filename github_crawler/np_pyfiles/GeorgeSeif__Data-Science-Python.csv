file_path,api_count,code
covariance_boston.py,6,"b'import pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport seaborn.apionly as sns\nfrom tabulate import tabulate\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_boston\n\nimport ml_helpers as helpers\n\n# NOTE that this loads as a dictionairy\nboston_data = load_boston()\n\ntrain_data = np.array(boston_data.data)\ntrain_labels = np.array(boston_data.target)\n\nnum_features = boston_data.data.shape[1]\nunique_labels = np.unique(train_labels)\nnum_classes = len(unique_labels)\n\n\nprint(""The boston dataset has "" + str(num_features) + "" features"")\nprint(boston_data.feature_names)\n\n\n\n# Put everything into a Pandas DataFrame\ndata = pd.DataFrame(data=np.c_[train_data], columns=boston_data.feature_names)\n# print(tabulate(data, headers=\'keys\', tablefmt=\'psql\'))\n\n\n\n# Compute the covariance matrix\ncov_mat_boston = np.cov(train_data.T)\nprint(""Covariance matrix"")\nprint(cov_mat_boston)\n\n\n\n# Normalize the data and then recompute the covariance matrix\nnormalized_train_data = helpers.normalize_data(train_data)\nnormalized_cov_mat_boston = np.cov(normalized_train_data.T)\nprint(""Normalized data covariance matrix"")\nprint(normalized_cov_mat_boston)\n\n\n\n# create scatterplot matrix\nfig = sns.pairplot(data=data, hue=\'CRIM\')\n\nplt.show()'"
explore_wine_data.py,13,"b'import pandas as pd\nimport numpy as np\nfrom tabulate import tabulate\nimport seaborn.apionly as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_wine\n\n\n# ------------------------------------------------------------------------------------------------\n\n# Read in the data\n# NOTE that this loads as a dictionairy\nwine_data = load_wine()\n\ntrain_data = np.array(wine_data.data)\ntrain_labels = np.array(wine_data.target)\n\nnum_features = wine_data.data.shape[1]\nunique_labels = np.unique(train_labels)\nnum_classes = len(unique_labels)\n\n\nprint(""The wine dataset has "" + str(num_features) + "" features"")\nprint(wine_data.feature_names)\nprint(""The wine dataset has "" + str(num_classes) + "" categoryes"")\nprint(wine_data.target_names)\n\n\n# Put everything into a Pandas DataFrame\ndata = pd.DataFrame(data=np.c_[train_data, train_labels], columns=wine_data.feature_names + [\'category\'])\n# print(tabulate(data, headers=\'keys\', tablefmt=\'psql\'))\n\n# ------------------------------------------------------------------------------------------------\n\n\n\n\n\n# ------------------------------------------------------------------------------------------------\n\n# Create histogram\nhist_feature_name=\'color_intensity\'\nbin_edges = np.arange(0, data[hist_feature_name].max() + 1, 1)\nfig = plt.hist(data[hist_feature_name], bins=bin_edges)\n\nplt.ylabel(\'count\')\nplt.xlabel(hist_feature_name)\nplt.show()\n\n# ------------------------------------------------------------------------------------------------\n\n\n\n\n\n\n\n# ------------------------------------------------------------------------------------------------\n\n# Create grouped bar plot\n\n\nvar_name_1 = \'alcohol\'\nvar_name_2 = \'color_intensity\'\n\n\n# Setting the positions and width for the bars\npos = list(range(num_classes))\nwidth = 0.1\n\n# Plotting the bars\nfig, ax = plt.subplots(figsize=(10,5))\n\n# Set the position of the x ticks\nax.set_xticks([p + 1.5 * width for p in pos])\nax.set_xticklabels(list(range(num_classes)))\n\nclass_0_data = data[data.category==0]\nalcohol_values_0 = class_0_data[var_name_1].values \nmean_alcohol_0 = np.mean(alcohol_values_0)\ncolor_values_0 = class_0_data[var_name_2].values \nmean_color_0 = np.mean(color_values_0)\n\nclass_1_data = data[data.category==1]\nalcohol_values_1 = class_1_data[var_name_1].values \nmean_alcohol_1 = np.mean(alcohol_values_1)\ncolor_values_1 = class_1_data[var_name_2].values \nmean_color_1 = np.mean(color_values_1)\n\nclass_2_data = data[data.category==2]\nalcohol_values_2 = class_2_data[var_name_1].values \nmean_alcohol_2 = np.mean(alcohol_values_2)\ncolor_values_2 = class_2_data[var_name_2].values \nmean_color_2 = np.mean(color_values_2)\n\nplt.bar(pos, [mean_alcohol_0, mean_alcohol_1, mean_alcohol_2], width, alpha=1.0, color=\'#EE3224\', label=\'alcohol\')\nplt.bar([p + width for p in pos], [mean_color_0, mean_color_1, mean_color_2], width, alpha=1.0, color=\'#F78F1E\', label=\'color_intensity\')\n\n\nplt.legend([var_name_1, \'color_intensity\'], loc=\'upper left\')\n\nplt.show()\n\n\n# ------------------------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n# ------------------------------------------------------------------------------------------------\n\n# Create scatterplot\nscatter_feature_name_1=\'color_intensity\'\nscatter_feature_name_2=\'alcohol\'\nfig = plt.scatter(data[scatter_feature_name_1], data[scatter_feature_name_2])\n\nplt.xlabel(scatter_feature_name_1)\nplt.ylabel(scatter_feature_name_2)\nplt.show()\n\n\n\n# Create scatterplot matrix\nfig = sns.pairplot(data=data[[\'alcohol\', \'color_intensity\', \'malic_acid\', \'magnesium\', \'category\']], hue=\'category\')\n\nplt.show()\n\n# ------------------------------------------------------------------------------------------------\n\n\n\n# ------------------------------------------------------------------------------------------------\n\n# Create bee swarm plot\nsns.swarmplot(x=\'category\', y=\'total_phenols\', data=data)\nplt.show()\n\n# ------------------------------------------------------------------------------------------------\n\n\n\n\n\n# ------------------------------------------------------------------------------------------------\n\n# Cumulative Distribution Function Plots\n\n\n# Sort and normalize data\nx = np.sort(data[\'hue\'])\ny = np.arange(1, x.shape[0] + 1, dtype=\'float32\') / x.shape[0]\n\nplt.plot(x, y, marker=\'o\', linestyle=\'\')\n\nplt.ylabel(\'ECDF\')\nplt.xlabel(\'hue\')\n\neightieth_percentile = x[y <= 0.75].max()\n\nplt.axhline(0.75, color=\'black\', linestyle=\'--\')\nplt.axvline(eightieth_percentile, color=\'black\', label=\'75th percentile\')\nplt.legend()\nplt.show()'"
ml_helpers.py,24,"b'import numpy as np\nimport random\n\n# Split the data into train and test sets\ndef train_test_split(X, y, test_size=0.2):\n\t# First, shuffle the data\n    train_data, train_labels = shuffle_data(X, y)\n\n    # Split the training data from test data in the ratio specified in test_size\n    split_i = len(y) - int(len(y) // (1 / test_size))\n    x_train, x_test = train_data[:split_i], train_data[split_i:]\n    y_train, y_test = train_labels[:split_i], train_labels[split_i:]\n\n    return x_train, x_test, y_train, y_test\n\n# Randomly shuffle the data\ndef shuffle_data(data, labels):\n\tif(len(data) != len(labels)):\n\t\traise Exception(""The given data and labels do NOT have the same length"")\n\n\tcombined = list(zip(data, labels))\n\trandom.shuffle(combined)\n\tdata[:], labels[:] = zip(*combined)\n\treturn data, labels\n\n# Calculate the distance between two vectors\ndef euclidean_distance(vec_1, vec_2):\n\tif(len(vec_1) != len(vec_2)):\n\t\traise Exception(""The two vectors do NOT have equal length"")\n\n\tdistance = 0\n\tfor i in range(len(vec_1)):\n\t\tdistance += pow((vec_1[i] - vec_2[i]), 2)\n\n\treturn np.sqrt(distance)\n\n# Compute the mean and variance of each feature of a data set\ndef compute_mean_and_var(data):\n\tnum_elements = len(data)\n\ttotal = [0] * data.shape[1]\n\tfor sample in data:\n\t\ttotal = total + sample\n\tmean_features = np.divide(total, num_elements)\n\n\ttotal = [0] * data.shape[1]\n\tfor sample in data:\n\t\ttotal = total + np.square(sample - mean_features)\n\n\tstd_features = np.divide(total, num_elements)\n\n\tvar_features = std_features ** 2\n\n\treturn mean_features, var_features\n\n# Normalize data by subtracting mean and dividing by standard deviation\ndef normalize_data(data):\n\tmean_features, var_features = compute_mean_and_var(data)\n\tstd_features = np.sqrt(var_features)\n\n\tfor index, sample in enumerate(data):\n\t\tdata[index] = np.divide((sample - mean_features), std_features) \n\n\treturn data\n\n# Divide dataset based on if sample value on feature index is larger than\n# the given threshold\ndef divide_on_feature(X, feature_i, threshold):\n    split_func = None\n    if isinstance(threshold, int) or isinstance(threshold, float):\n        split_func = lambda sample: sample[feature_i] >= threshold\n    else:\n        split_func = lambda sample: sample[feature_i] == threshold\n\n    X_1 = np.array([sample for sample in X if split_func(sample)])\n    X_2 = np.array([sample for sample in X if not split_func(sample)])\n\n    return np.array([X_1, X_2])\n\n# Return random subsets (with replacements) of the data\ndef get_random_subsets(X, y, n_subsets, replacements=True):\n    n_samples = np.shape(X)[0]\n    # Concatenate x and y and do a random shuffle\n    X_y = np.concatenate((X, y.reshape((1, len(y))).T), axis=1)\n    np.random.shuffle(X_y)\n    subsets = []\n\n    # Uses 50% of training samples without replacements\n    subsample_size = n_samples // 2\n    if replacements:\n        subsample_size = n_samples      # 100% with replacements\n\n    for _ in range(n_subsets):\n        idx = np.random.choice(range(n_samples), size=np.shape(range(subsample_size)), replace=replacements)\n        X = X_y[idx][:, :-1]\n        y = X_y[idx][:, -1]\n        subsets.append([X, y])\n    return subsets\n\n# Calculate the entropy of label array y\ndef calculate_entropy(y):\n    log2 = lambda x: np.log(x) / np.log(2)\n    unique_labels = np.unique(y)\n    entropy = 0\n    for label in unique_labels:\n        count = len(y[y == label])\n        p = count / len(y)\n        entropy += -p * log2(p)\n    return entropy\n\n# Returns the mean squared error between y_true and y_pred\ndef mean_squared_error(y_true, y_pred):\n    mse = np.mean(np.power(y_true - y_pred, 2))\n    return mse\n\n# The sigmoid function\ndef sigmoid(val):\n\treturn np.divide(1, (1 + np.exp(-1*val)))\n\n# The derivative of the sigmoid function\ndef sigmoid_gradient(val):\n    return sigmoid(val) * (1 - sigmoid(val))\n\n# Compute the covariance matrix of an array\ndef compute_cov_mat(data):\n\t# Compute the mean of the data\n\tmean_vec = np.mean(data, axis=0)\n\n\t# Compute the covariance matrix\n\tcov_mat = (data - mean_vec).T.dot((data - mean_vec)) / (data.shape[0]-1)\n\n\treturn cov_mat\n\n\n# Perform PCA dimensionality reduction\ndef pca(data, exp_var_percentage=95):\n\n\t# Compute the covariance matrix\n\tcov_mat = compute_cov_mat(data)\n\n\t# Compute the eigen values and vectors of the covariance matrix\n\teig_vals, eig_vecs = np.linalg.eig(cov_mat)\n\n\t# Make a list of (eigenvalue, eigenvector) tuples\n\teig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n\n\t# Sort the (eigenvalue, eigenvector) tuples from high to low\n\teig_pairs.sort(key=lambda x: x[0], reverse=True)\n\n\t# Only keep a certain number of eigen vectors based on the ""explained variance percentage""\n\t# which tells us how much information (variance) can be attributed to each of the principal components\n\ttot = sum(eig_vals)\n\tvar_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]\n\tcum_var_exp = np.cumsum(var_exp)\n\n\tnum_vec_to_keep = 0\n\n\tfor index, percentage in enumerate(cum_var_exp):\n\t\tif percentage > exp_var_percentage:\n\t\t\tnum_vec_to_keep = index + 1\n\t\t\tbreak\n\n\t# Compute the projection matrix based on the top eigen vectors\n\tproj_mat = eig_pairs[0][1].reshape(4,1)\n\tfor eig_vec_idx in range(1, num_vec_to_keep):\n\t\tproj_mat = np.hstack((proj_mat, eig_pairs[eig_vec_idx][1].reshape(4,1)))\n\n\t# Project the data \n\tpca_data = data.dot(proj_mat)\n\n\treturn pca_data\n\n# 1D Gaussian Function\ndef gaussian_1d(val, mean, standard_dev):\n\tcoeff = 1 / (standard_dev * np.sqrt(2 * np.pi))\n\texponent = (-1 * (val - mean) ** 2) / (2 * (standard_dev ** 2))\n\tgauss = coeff * np.exp(exponent)\n\treturn gauss\n\n# 2D Gaussian Function\ndef gaussian_2d(x_val, y_val, x_mean, y_mean, x_standard_dev, y_standard_dev):\n\tx_gauss = gaussian_1d(x_val, x_mean, x_standard_dev)\n\ty_gauss = gaussian_1d(y_val, y_mean, y_standard_dev)\n\tgauss = x_gauss * y_gauss\n\treturn gauss\n'"
plt_helpers.py,2,"b'import matplotlib\n \ndef scatterplot(x_data, y_data, x_label="""", y_label="""", title=""""):\n\n    # Create the plot object\n    _, ax = plt.subplots()\n\n    # Plot the data, set the size (s), color and transparency (alpha)\n    # of the points\n    ax.scatter(x_data, y_data, s = 30, color = \'#539caf\', alpha = 0.75)\n\n    # Label the axes and provide a title\n    ax.set_title(title)\n    ax.set_xlabel(x_label)\n    ax.set_ylabel(y_label)\n\n\ndef lineplot(x_data, y_data, x_label="""", y_label="""", title=""""):\n    # Create the plot object\n    _, ax = plt.subplots()\n\n    # Plot the best fit line, set the linewidth (lw), color and\n    # transparency (alpha) of the line\n    ax.plot(x_data, y_data, lw = 2, color = \'#539caf\', alpha = 1)\n\n    # Label the axes and provide a title\n    ax.set_title(title)\n    ax.set_xlabel(x_label)\n    ax.set_ylabel(y_label)\n\n\n# Line plot with 2 different y values\ndef lineplot2y(x_data, y1_data, y2_data, x_label="""", y1_color=""#539caf"", y1_label="""", y2_color=""#7663b0"", y2_label="""", title=""""):\n    # Each variable will actually have its own plot object but they\n    # will be displayed in just one plot\n    # Create the first plot object and draw the line\n    _, ax1 = plt.subplots()\n    ax1.plot(x_data, y1_data, color = y1_color)\n    # Label axes\n    ax1.set_ylabel(y1_label, color = y1_color)\n    ax1.set_xlabel(x_label)\n    ax1.set_title(title)\n\n    # Create the second plot object, telling matplotlib that the two\n    # objects have the same x-axis\n    ax2 = ax1.twinx()\n    ax2.plot(x_data, y2_data, color = y2_color)\n    ax2.set_ylabel(y2_label, color = y2_color)\n    # Show right frame line\n    ax2.spines[\'right\'].set_visible(True)\n\n\ndef histogram(data, n_bins, cumulative=False, x_label = """", y_label = """", title = """"):\n    _, ax = plt.subplots()\n    ax.hist(data, n_bins = n_bins, cumulative = cumulative, color = \'#539caf\')\n    ax.set_ylabel(y_label)\n    ax.set_xlabel(x_label)\n    ax.set_title(title)\n\n\n\n# Overlay 2 histograms to compare them\ndef overlaid_histogram(data1, data2, n_bins = 0, data1_name="""", data1_color=""#539caf"", data2_name="""", data2_color=""#7663b0"", x_label="""", y_label="""", title=""""):\n    # Set the bounds for the bins so that the two distributions are fairly compared\n    max_nbins = 10\n    data_range = [min(min(data1), min(data2)), max(max(data1), max(data2))]\n    binwidth = (data_range[1] - data_range[0]) / max_nbins\n\n\n    if n_bins == 0\n    \tbins = np.arange(data_range[0], data_range[1] + binwidth, binwidth)\n    else: \n    \tbins = n_bins\n\n    # Create the plot\n    _, ax = plt.subplots()\n    ax.hist(data1, bins = bins, color = data1_color, alpha = 1, label = data1_name)\n    ax.hist(data2, bins = bins, color = data2_color, alpha = 0.75, label = data2_name)\n    ax.set_ylabel(y_label)\n    ax.set_xlabel(x_label)\n    ax.set_title(title)\n    ax.legend(loc = \'best\')\n\n\n# Probability Density Function\ndef densityplot(x_data, density_est, x_label="""", y_label="""", title=""""):\n    _, ax = plt.subplots()\n    ax.plot(x_data, density_est(x_data), color = \'#539caf\', lw = 2)\n    ax.set_ylabel(y_label)\n    ax.set_xlabel(x_label)\n    ax.set_title(title)\n\n\n\ndef barplot(x_data, y_data, error_data, x_label="""", y_label="""", title=""""):\n    _, ax = plt.subplots()\n    # Draw bars, position them in the center of the tick mark on the x-axis\n    ax.bar(x_data, y_data, color = \'#539caf\', align = \'center\')\n    # Draw error bars to show standard deviation, set ls to \'none\'\n    # to remove line between points\n    ax.errorbar(x_data, y_data, yerr = error_data, color = \'#297083\', ls = \'none\', lw = 2, capthick = 2)\n    ax.set_ylabel(y_label)\n    ax.set_xlabel(x_label)\n    ax.set_title(title)\n\n\n\ndef stackedbarplot(x_data, y_data_list, colors, y_data_names="""", x_label="""", y_label="""", title=""""):\n    _, ax = plt.subplots()\n    # Draw bars, one category at a time\n    for i in range(0, len(y_data_list)):\n        if i == 0:\n            ax.bar(x_data, y_data_list[i], color = colors[i], align = \'center\', label = y_data_names[i])\n        else:\n            # For each category after the first, the bottom of the\n            # bar will be the top of the last category\n            ax.bar(x_data, y_data_list[i], color = colors[i], bottom = y_data_list[i - 1], align = \'center\', label = y_data_names[i])\n    ax.set_ylabel(y_label)\n    ax.set_xlabel(x_label)\n    ax.set_title(title)\n    ax.legend(loc = \'upper right\')\n\n\n\ndef groupedbarplot(x_data, y_data_list, colors, y_data_names="""", x_label="""", y_label="""", title=""""):\n    _, ax = plt.subplots()\n    # Total width for all bars at one x location\n    total_width = 0.8\n    # Width of each individual bar\n    ind_width = total_width / len(y_data_list)\n    # This centers each cluster of bars about the x tick mark\n    alteration = np.arange(-(total_width/2), total_width/2, ind_width)\n\n    # Draw bars, one category at a time\n    for i in range(0, len(y_data_list)):\n        # Move the bar to the right on the x-axis so it doesn\'t\n        # overlap with previously drawn ones\n        ax.bar(x_data + alteration[i], y_data_list[i], color = colors[i], label = y_data_names[i], width = ind_width)\n    ax.set_ylabel(y_label)\n    ax.set_xlabel(x_label)\n    ax.set_title(title)\n    ax.legend(loc = \'upper right\')\n\n\n\n\ndef boxplot(x_data, y_data, base_color=""#539caf"", median_color=""#297083"", x_label="""", y_label="""", title=""""):\n    _, ax = plt.subplots()\n\n    # Draw boxplots, specifying desired style\n    ax.boxplot(y_data\n               # patch_artist must be True to control box fill\n               , patch_artist = True\n               # Properties of median line\n               , medianprops = {\'color\': median_color}\n               # Properties of box\n               , boxprops = {\'color\': base_color, \'facecolor\': base_color}\n               # Properties of whiskers\n               , whiskerprops = {\'color\': base_color}\n               # Properties of whisker caps\n               , capprops = {\'color\': base_color})\n\n    # By default, the tick label starts at 1 and increments by 1 for\n    # each box drawn. This sets the labels to the ones we want\n    ax.set_xticklabels(x_data)\n    ax.set_ylabel(y_label)\n    ax.set_xlabel(x_label)\n    ax.set_title(title)'"
statistics_helpers.py,0,"b'import math\n\ndef mean(x): \n    return sum(x) / len(x)\n\ndef median(v):\n    """"""finds the \'middle-most\' value of v""""""\n    n = len(v)\n    sorted_v = sorted(v)\n    midpoint = n // 2\n    \n    if n % 2 == 1:\n        # if odd, return the middle value\n        return sorted_v[midpoint]\n    else:\n        # if even, return the average of the middle values\n        lo = midpoint - 1\n        hi = midpoint\n        return (sorted_v[lo] + sorted_v[hi]) / 2\n        \ndef quantile(x, p):\n    """"""returns the pth-percentile value in x""""""\n    p_index = int(p * len(x))\n    return sorted(x)[p_index]\n\ndef mode(x):\n    """"""returns a list, might be more than one mode""""""\n    counts = Counter(x)\n    max_count = max(counts.values())\n    return [x_i for x_i, count in counts.iteritems()\n            if count == max_count]\n\n\ndef data_range(x):\n    return max(x) - min(x)\n\ndef variance(x):\n    """"""assumes x has at least two elements""""""\n    n = len(x)\n    deviations = de_mean(x)\n    return sum_of_squares(deviations) / (n - 1)\n    \ndef standard_deviation(x):\n    return math.sqrt(variance(x))\n\ndef interquartile_range(x):\n    return quantile(x, 0.75) - quantile(x, 0.25)\n\n\ndef covariance(x, y):\n    n = len(x)\n    return dot(de_mean(x), de_mean(y)) / (n - 1)\n\ndef correlation(x, y):\n    stdev_x = standard_deviation(x)\n    stdev_y = standard_deviation(y)\n    if stdev_x > 0 and stdev_y > 0:\n        return covariance(x, y) / stdev_x / stdev_y\n    else:\n        return 0 # if no variation, correlation is zero\n'"
statistics_iris.py,12,"b'import pandas as pd\nimport numpy as np\nfrom scipy import stats\nfrom tabulate import tabulate\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\n\ndef compute_list_median(x):\n\tx = np.sort(x)\n\n\ttmp = round(0.5 * x.shape[0])\n\n\tif x.shape[0] % 2:\n\t    median = x[tmp - 1]\n\telse:\n\t    median = x[tmp - 1] + (x[tmp] - x[tmp - 1]) / 2.\n\t    \n\treturn median\n\n# NOTE that this loads as a dictionairy\niris_data = load_iris()\n\ntrain_data = np.array(iris_data.data)\ntrain_labels = np.array(iris_data.target)\n\nnum_features = iris_data.data.shape[1]\nunique_labels = np.unique(train_labels)\nnum_classes = len(unique_labels)\n\n\nprint(""The iris dataset has "" + str(num_features) + "" features"")\nprint(iris_data.feature_names)\nprint(""The iris dataset has "" + str(num_classes) + "" classes"")\nprint(iris_data.target_names)\n\n\n# Strip for easier indexing\nfor i in range(len(iris_data.feature_names)):\n\tiris_data.feature_names[i] = iris_data.feature_names[i].replace(\' (cm)\',\'\')\n\n# Put everything into a Pandas DataFrame\ndata = pd.DataFrame(data=np.c_[train_data, train_labels], columns=iris_data.feature_names + [\'class\'])\n# print(tabulate(data, headers=\'keys\', tablefmt=\'psql\'))\n\n\n\n# Create histogram\nhist_feature_name=\'sepal length\'\nbin_edges = np.arange(0, data[hist_feature_name].max() + 1, 1)\nfig = plt.hist(data[hist_feature_name], bins=bin_edges)\n\nplt.ylabel(\'count\')\nplt.xlabel(hist_feature_name)\n# plt.show()\n\n\n\n# Compute the mean sepal length and draw it on the same histogram\nsepal_length_values = data[\'sepal length\'].values \nmean_sepal_length = sum(i for i in sepal_length_values) / len(sepal_length_values)\nmean_sepal_length = np.mean(sepal_length_values)\nprint(""Mean sepal length (cm) = "" + str(mean_sepal_length))\n\nplt.axvline(mean_sepal_length, color=\'green\', linewidth=2)\n\n\n\n# Compute the variance of the sepal length feature and draw it on the same histogram\nvariance_sepal_length = sum([(i - mean_sepal_length)**2 for i in sepal_length_values]) / (len(sepal_length_values) - 1)\nvariance_sepal_length = np.var(sepal_length_values, ddof=1)\n\nprint(""Variance of sepal length (cm) = "" + str(variance_sepal_length))\n\nplt.axvline(mean_sepal_length + variance_sepal_length, color=\'red\', linewidth=2)\nplt.axvline(mean_sepal_length - variance_sepal_length, color=\'red\', linewidth=2)\n\n\n# Other values\nmin_sepal_length = np.min(sepal_length_values)\nprint(""Minimum sepal length (cm) = "" + str(min_sepal_length))\n\nmax_sepal_length = np.max(sepal_length_values)\nprint(""Maximum sepal length (cm) = "" + str(max_sepal_length))\n\nsorted_sepal_length_values = np.sort(sepal_length_values)\npercentile_20th = sorted_sepal_length_values[round(0.25 * sorted_sepal_length_values.shape[0]) + 1]\npercentile_80th = sorted_sepal_length_values[round(0.75 * sorted_sepal_length_values.shape[0]) + 1]\nprint(""20th Percentile sepal length (cm) = "" + str(percentile_20th))\nprint(""80th Percentile sepal length (cm) = "" + str(percentile_80th))\n\nmedian_sepal_length = compute_list_median(sepal_length_values)\nmedian_sepal_length = np.median(sepal_length_values)\nprint(""Median sepal length (cm) = "" + str(median_sepal_length))\n\n\nplt.show()'"
