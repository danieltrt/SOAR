file_path,api_count,code
1_Datatype.py,0,"b""# -*- coding: utf8 -*-\nimport tensorflow as tf\n\n# Constant\nc1=tf.constant(1,dtype=tf.float32,name='const_1')\nc2=tf.constant(2,dtype=tf.float32,name='const_2')\nc3=c1+c2\nprint(c1,c2,c3)\n\nwith tf.Session() as sess:\n    print(sess.run(c3))\n\n# Variable\nv1=tf.Variable(0,dtype=tf.float32,name='var_1')\nv2=tf.Variable(0,dtype=tf.float32,name='var_2')\nv1.assign(1)\nv2.assign(2)\nv3=v1+v2\nprint(v1,v2,v3)\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    print(sess.run(v3))\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    sess.run(v1.assign(1))\n    sess.run(v2.assign(2))\n    print(sess.run(v3))\n\n# placeholder\np1=tf.placeholder(shape=[2,2],dtype=tf.float32,name='place_1')\np2=tf.placeholder(shape=[2,2],dtype=tf.float32,name='place_2')\np3=p1+p2\nprint(p1,p2,p3)\n\nfeed_dict={p1:[[1,2],[3,4]], p2:[[5,6],[7,8]]}\nwith tf.Session() as sess:\n   print(sess.run(p3,feed_dict=feed_dict))\n"""
2_Linear_Classifier.py,6,"b""import tensorflow as tf\nimport numpy as np\n\ninput_data=np.array([[0,0,1],[0,1,1],[1,0,1],[1,1,1]],dtype=np.float32)\noutput_data=np.array([[0,0,1,1]],dtype=np.float32).T\n\nx =tf.placeholder(dtype=np.float32,shape=[4,3])\ny_=tf.placeholder(dtype=np.float32,shape=[4,1])\n\nweight=tf.Variable(np.random.random_sample([3,1]),dtype=tf.float32,name='weight')\nbias=tf.Variable(np.random.random_sample([1]),dtype=tf.float32,name='bias')\n\ny=tf.matmul(x,weight)+bias\n\nLoss=tf.reduce_mean(tf.square(y-y_))\ntrain=tf.train.GradientDescentOptimizer(learning_rate=1e-2).minimize(Loss)\n\nfeed_dict={x:input_data, y_:output_data}\n\ninit=tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n\tsess.run(init)\n\tfor i in range(10000):\n\t\t_,l=sess.run([train,Loss],feed_dict=feed_dict)\n\t\tif i%100==0:\n\t\t\tprint(l)\n\tprint(Loss)\n\tprint(y)\n\tprint(sess.run(Loss,feed_dict=feed_dict))\n\tprint(sess.run(y,feed_dict=feed_dict))"""
3-1_Numpy_vs_Tensorflow.py,27,"b""import numpy as np\nimport matplotlib.pyplot as plt\n\nN = 100 \nD = 2 \nK = 3 \nX = np.zeros((N*K,D)) \ny = np.zeros(N*K, dtype='uint8') \n\n# data generation\nfor j in range(K):\n  ix = range(N*j,N*(j+1))\n  r = np.linspace(0.0,1,N)  \n  t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2\n  X[ix] = np.c_[r*np.sin(t), r*np.cos(t)] \n  y[ix] = j\n\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=plt.cm.Spectral)\nplt.show()\n\n# hpyerparameter setting\n\nH=100 # Hidden layer node \nw1=0.01*np.random.rand(D,H)/np.sqrt(D/2)\nb1=np.zeros([1,H])\nw2=0.01*np.random.rand(H,3)/np.sqrt(H/2)\nb2=np.zeros([1,K])\n\nlearning_rate=1e-2\nreg_lambda=1e-2\niteration=10000\nnum_examples=X.shape[0]\n\nfor i in range(10000):\n    \n    # forward propagation\n    hidden_layer=np.maximum(0,np.dot(X,w1)+b1)\n    output_layer= np.dot(hidden_layer,w2)+b2\n\n    # softmax classifier\n    exp_scores=np.exp(output_layer)\n    probs=exp_scores/np.sum(exp_scores,axis=1,keepdims=True)\n    \n    # loss calculation\n    cross_entropy_loss= -np.sum(1.0*np.log(probs[range(num_examples),y]))\n    data_loss=cross_entropy_loss/num_examples\n    reg_loss=1/2*reg_lambda*(np.sum(w1*w1)+np.sum(w2*w2))\n    \n    loss=data_loss+reg_loss\n    \n    # error calculation\n    errors=probs\n    errors[range(num_examples),y]-=1\n    \n    # backpropagation\n    # output layer -> hidden layer\n    dw2=np.dot(hidden_layer.T,errors)\n    db2=np.sum(errors,axis=0,keepdims=True)\n\n    #ReLU backprop\n    dhidden_error=np.dot(errors,w2.T)\n    dhidden_error[hidden_layer<=0]=0\n\n    # hidden layer -> input layer\n    dw1=np.dot(X.T,dhidden_error)\n    db1=np.sum(dhidden_error,axis=0,keepdims=True)\n\n    # derivative (1/2*w^2) = w\n    dw2+=reg_lambda*w2\n    dw1+=reg_lambda*w1\n\n    # model update\n    w1+= -learning_rate*dw1\n    b1+= -learning_rate*db1\n    w2+= -learning_rate*dw2\n    b2+= -learning_rate*db2\n\nhidden_layer = np.maximum(0, np.dot(X, w1) + b1)\nscores = np.dot(hidden_layer, w2) + b2\npredicted_class = np.argmax(scores, axis=1)\nprint ('training accuracy: %.2f' % (np.mean(predicted_class == y)))\n\nh = 0.02\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))\n\nZ = np.dot(np.maximum(0, np.dot(np.c_[xx.ravel(), yy.ravel()], w1) + b1), w2) + b2\nZ = np.argmax(Z, axis=1)\nZ = Z.reshape(xx.shape)\n\nfig = plt.figure()\nplt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\nplt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.show()"""
3-2_Numpy_vs_Tensorflow.py,13,"b""import numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nN = 100 # number of points per class\nD = 2 # dimensionality\nK = 3 # number of classes\nH=100 \nX = np.zeros((N*K,D)) # data matrix (each row = single example)\ny = np.zeros(N*K, dtype='uint8') # class labels\n\nfor j in range(K):\n  ix = range(N*j,N*(j+1))\n  r = np.linspace(0.0,1,N) # radius\n  t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta\n  X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n  y[ix]= j\n\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=plt.cm.Spectral)\nplt.show()\n\n# reshape y\nnew_y=np.zeros([N*K,K])\nfor i in range(N*K):\n\tnew_y[i,y[i]]=1\n\n\nlearning_rate=1e-2\nreg_lambda=1e-2\niteration=10000\nnum_examples=X.shape[0]\n\n\nx =tf.placeholder(shape=[300,2],dtype=tf.float32,name='x') # feed X\ny_=tf.placeholder(shape=[300,3],dtype=tf.float32,name='y_') # feed new_y\n\nw1=tf.Variable(0.01*np.random.rand(D,H)/np.sqrt(D/2),dtype=tf.float32,name='w1')\nb1=tf.Variable(np.zeros([1,H]),dtype=tf.float32,name='b1' )\nw2=tf.Variable(0.01*np.random.rand(H,3)/np.sqrt(H/2),dtype=tf.float32,name='w2')\nb2=tf.Variable(np.zeros([1,K]),dtype=tf.float32,name='b2' )\n\nhidden_layer= tf.nn.relu(tf.matmul(x,w1)+b1)\noutput_layer=tf.matmul(hidden_layer,w2)+b2\ncross_entropy_loss=tf.nn.softmax_cross_entropy_with_logits(output_layer,y_)\ntrain_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy_loss)\naccuracy=tf.reduce_mean(tf.cast(tf.equal(tf.argmax(output_layer,1),tf.argmax(y_,1)),dtype=tf.float32))\n\nfeed_dict={x:X,y_:new_y}\ninit= tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n\tsess.run(init)\n\tfor x in range(iteration):\n\t\t_,acc=sess.run([train_step,accuracy],feed_dict=feed_dict)\n\t\tif x%1000==0:\n\t\t\tprint(acc)\n\n\tw1=w1.eval()\n\tb1=b1.eval()\n\tw2=w2.eval()\n\tb2=b2.eval()\n\nh = 0.02\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))\n\nZ = np.dot(np.maximum(0, np.dot(np.c_[xx.ravel(), yy.ravel()], w1) + b1), w2) + b2\nZ = np.argmax(Z, axis=1)\nZ = Z.reshape(xx.shape)\n\nfig = plt.figure()\nplt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\nplt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.show()"""
4_basic_cnn.py,0,"b'from tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\'MNIST_data\', one_hot=True)\n\nimport tensorflow as tf\nsess = tf.InteractiveSession()\n\nx = tf.placeholder(tf.float32, shape=[None, 784])\ny_ = tf.placeholder(tf.float32, shape=[None, 10])\nx_image = tf.reshape(x, [-1,28,28,1])\n\n\ndef weight_variable(shape):\n  initial = tf.truncated_normal(shape, stddev=0.1)\n  return tf.Variable(initial)\n\ndef bias_variable(shape):\n  initial = tf.constant(0.1, shape=shape)\n  return tf.Variable(initial)\n\ndef conv2d(x, W):\n  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=\'SAME\')\n\ndef max_pool_2x2(x):\n  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\'SAME\')\n\nW_conv1 = weight_variable([5, 5, 1, 32])\nb_conv1 = bias_variable([32])\n\nh_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\nh_pool1 = max_pool_2x2(h_conv1)\n\nW_conv2 = weight_variable([5, 5, 32, 64])\nb_conv2 = bias_variable([64])\n\nh_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\nh_pool2 = max_pool_2x2(h_conv2)\n\nW_fc1 = weight_variable([7 * 7 * 64, 1024])\nb_fc1 = bias_variable([1024])\n\nh_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\nh_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n\nkeep_prob = tf.placeholder(tf.float32)\nh_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n\nW_fc2 = weight_variable([1024, 10])\nb_fc2 = bias_variable([10])\n\ny_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n\ncross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y_))\ntrain_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\ncorrect_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\nsess.run(tf.global_variables_initializer())\n\nfor i in range(20000):\n\tbatch = mnist.train.next_batch(50)\n\tif i%100 == 0:\n\t\ttrain_accuracy = accuracy.eval(feed_dict={x:batch[0], y_: batch[1], keep_prob: 1.0})\n\t\tprint(""step %d, training accuracy %g""%(i, train_accuracy))\n\ttrain_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n\nbatch_size=200\nacc_total=0\n\nj=0\nwhile j<len(mnist.test.images):\n    test_input = mnist.test.images[j:j+batch_size]\n    test_output= mnist.test.labels[j:j+batch_size]\n    batch_acc=accuracy.eval(feed_dict={x: test_input, y_: test_output, keep_prob: 1.0})\n    acc_total+=batch_acc\n    j+=batch_size  \n    \nnum_ops=len(mnist.test.images)/batch_size\nprint(""total accuracy: %f""% (acc_total/num_ops))'"
5_basic_rnn.py,2,"b'import numpy as np\nimport tensorflow as tf\n\ninput_string=""barackobama""\n\nstring_to_char=list(input_string)\nset_of_char=set(string_to_char)\nlook_up_table=list(set_of_char)\n\ndict_char={ c:idx for idx,c in enumerate(set_of_char)}\n\nx1=[dict_char[i] for i in input_string[:-1]]\ny1=[dict_char[i] for i in input_string[1:]]\n\nlength=len(x1)\n\ndef index_to_onehot(x,length):\n\tarr=[]\n\tfor i in x:\n\t\tnew_arr=[int(i==j) for j in range(length)]\n\t\tarr.append(new_arr)\n\treturn arr\t\n\ninput_x  =np.array(index_to_onehot(x1,length),dtype=np.float32)\n#output_y_=np.array(index_to_onehot(y1,length),dtype=np.float32)\noutput_y_=y1\n\nnum_units=length\nbatch_size=1\nlearning_rate=1e-2\n\nrnn_cell=tf.nn.rnn_cell.BasicRNNCell(num_units=num_units)\nhidden_state_initial=rnn_cell.zero_state(batch_size,dtype=tf.float32)\ninput_x_split=tf.split(0,length,input_x)\n\n# https://github.com/ahangchen/TensorFlowDoc/blob/master/api_docs/python/functions_and_classes/shard0/tf.nn.rnn.md\noutputs, state = tf.nn.rnn(rnn_cell, input_x_split, hidden_state_initial)\n\nlogits = tf.reshape(tf.concat(1, outputs), [-1, num_units])\ntargets = tf.reshape(output_y_, [-1])\nweights = tf.ones([length* batch_size])\n\nloss = tf.nn.seq2seq.sequence_loss_by_example([logits], [targets], [weights])\ncost = tf.reduce_sum(loss) / batch_size\ntrain_op = tf.train.AdamOptimizer(1e-2).minimize(cost)\n\n# https://github.com/tensorflow/tensorflow/blob/287db3a9b0701021f302e7bb58af5cf89fdcd424/tensorflow/g3doc/api_docs/python/functions_and_classes/shard5/tf.contrib.legacy_seq2seq.sequence_loss_by_example.md\nwith tf.Session() as sess:\n\tsess.run(tf.global_variables_initializer())\n\tprint(dict_char)\n\tprint(len(sess.run(outputs)))\n\tprint(sess.run([logits]))\n\tprint(sess.run([targets]))\n\n\tfor i in range(100):\n\t\tsess.run(train_op)\n\t\tresult = sess.run(tf.argmax(logits, 1))\n\t\tprint(result)\n\tprint(result, [look_up_table[t] for t in result])\t'"
image_reader.py,0,"b'import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nbatch_size =16\nepoch = 10 \n\n# Read image files by name\n\nshoes_filename_queue = tf.train.string_input_producer(tf.train.match_filenames_once(""./shoes/*.jpg""),num_epochs=epoch)\nbags_filename_queue = tf.train.string_input_producer(tf.train.match_filenames_once(""./bags/*.jpg""),num_epochs=epoch)\n\n# Define Reader\n\nimage_reader = tf.WholeFileReader()\n\n# reader returns filename & image data\n\n_, shoes_file = image_reader.read(shoes_filename_queue)\n_, bags_file = image_reader.read(bags_filename_queue)\n\n# decode data with decode_jpg function\n\nshoes_image = tf.image.decode_jpeg(shoes_file)\nbags_image = tf.image.decode_jpeg(bags_file)\n\n# change shape and data type according to usage\n\nshoes_image = tf.cast(tf.reshape(shoes_image,shape=[64,64,3]),dtype=tf.float32)\nbags_image = tf.cast(tf.reshape(bags_image,shape=[64,64,3]),dtype=tf.float32)\n\n# make shuffled batch with tf.train.shuffle_batch\n\nnum_preprocess_threads = 1\nmin_queue_examples = 256\nbatch_shoes = tf.train.shuffle_batch([shoes_image],\n\t\t\t\t    batch_size=batch_size,\n\t\t\t\t    num_threads=num_preprocess_threads,\n\t\t\t\t    capacity=min_queue_examples + 3 * batch_size,\n\t\t\t\t    min_after_dequeue=min_queue_examples)\n\nbatch_bags = tf.train.shuffle_batch([bags_image],\n\t\t\t\t    batch_size=batch_size,\n\t\t\t\t    num_threads=num_preprocess_threads,\n\t\t\t\t    capacity=min_queue_examples + 3 * batch_size,\n\t\t\t\t    min_after_dequeue=min_queue_examples)\n\nwith tf.Session() as sess:\n\tfor i in range(epoch):\n\t\tprint(batch_shoes,batch_bags)\n'"
