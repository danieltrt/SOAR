file_path,api_count,code
setup.py,0,"b'import setuptools\nimport pyik\n\nwith open(""README.md"", ""r"") as fh:\n    long_description = fh.read()\n\nsetuptools.setup(\n    name=""pyik"",\n    version=pyik.__version__,\n    author=""Hans Dembinski"",\n    author_email=""hans.dembinski@gmail.com"",\n    description=""PyIK - The Python Instrument Kit"",\n    long_description=long_description,\n    long_description_content_type=""text/markdown"",\n    license=""BSD"",\n    url=""https://github.com/hdembinski/pyik"",\n    packages=setuptools.find_packages(),\n    tests_require=[""numpy"", ""scipy"", ""matplotlib""],\n    install_requires=[""numpy""],\n    classifiers=[\n        ""Programming Language :: Python :: 2"",\n        ""Programming Language :: Python :: 3"",\n        ""Development Status :: 4 - Beta"",\n        ""Intended Audience :: Science/Research"",\n        ""Topic :: Scientific/Engineering :: Physics"",\n        ""License :: OSI Approved :: BSD License"",\n        ""Operating System :: OS Independent"",\n    ],\n)\n'"
examples/chi2_fit.py,9,"b'# -*- coding: utf-8 -*-\n\n""""""\nThis example demonstrates fitting a model to a (simulated) dataset using\nnumpyext.chi2_fit, which wraps Minuit.\n""""""\n\nimport numpy as np\nfrom matplotlib import pyplot\nfrom pyik.fit import ChiSquareFunction\nfrom pyik.mplext import cornertext\n\nnp.random.seed(1)\n\n\ndef model(x, pars):\n  """"""A slightly complex function. Needs to be vectorized.""""""\n  a0, a1, x_break = pars  # unpack parameter vector\n  x = np.atleast_1d(x)  # x needs to be a numpy array\n  y = np.empty_like(x)\n  mask = x <= x_break\n  y[mask] = a0 * x[mask]\n  y[~mask] = a0 * x[~mask] + a1 * (x[~mask] - x_break)**2\n  return y\n\n# Simulate a dataset of n measurements\nn = 20\nparsTrue = (2.0, 0.5, 13.0)\n\nxs = np.linspace(0, 20, n)\nys = model(xs, parsTrue)\n\n# Add some noise to the points\neys = 1.5 * np.ones(n)\nys += np.random.randn(n) * eys\n\n# Perform a fit to the data points; reuse the errors used to generate the noise\n# Note: fits to data without xerrors are much faster\nstarts = (1.0, 1.0, 10.0)  # starting values\n# define bounds for parameter ""x_break""\nlower_bounds = (-np.inf, -np.inf, 0.0)\nupper_bounds = (np.inf, np.inf, 20.0)\npars, cov, chi2, ndof = \\\n  ChiSquareFunction(model, xs, ys, eys) \\\n  .Minimize(starts,\n            lower_bounds=lower_bounds,\n            upper_bounds=upper_bounds)\n\n# Generate a plot of the fit\nnew_xs = np.linspace(0., 20, 1000)\n\nfigure = pyplot.figure()\n\npyplot.plot(new_xs, model(new_xs, pars), \'b\')\npyplot.errorbar(xs, ys, eys, fmt=\'ok\')\n\npyplot.xlim(-1, 21)\npyplot.xlabel(""x"")\npyplot.ylabel(""y"")\n\ns = ""Fit Example:\\n""\nfor i, label in enumerate((""a_0"", ""a_1"", ""x_{brk}"")):\n  s += ""$%s = %.2f \\pm %.2f$ (True: $%.1f$)\\n"" % (label, pars[i], cov[i, i]**0.5, parsTrue[i])\ns += ""$\\\\chi^2 / n_{dof} = %.3f$"" % (chi2 / ndof)\n\ncornertext(s)\n\npyplot.show()\n'"
pyik/__init__.py,0,"b'__version__ = ""0.9.2""\n'"
pyik/corsika.py,17,"b'# -*- coding: utf-8 -*-\nfrom __future__ import print_function\nfrom six.moves import range\n\n""""""Utility functions to read CORSIKA longitudinal files and headers of ground particle files.""""""\n\n# transcribed from particle identifiers in CORSIKA manual, only some\n# to note Offline first converts to PDG and only has a few names hard-coded\n# the packages responsible for that in Offline are not included in these CORSIKA files\n# CAVEAT| to view labels correctly need useTex enabled in matplotlibrc\nprimary_dic = {1: r\'$\\gamma$\', 2: r\'$e^{+}$\', 3: r\'$e^{-}$\',\n               5: r""$\\mu^{+}$"", 6: r""$\\mu^{-}$"",\n               7: r""$\\pi^{o}$"", 8: r""$\\pi^{+}$"",\n               9: r""$\\pi^{-}$"", 13: ""n"", 14: ""p"",\n               402: ""He"", 703: ""Li"", 904: ""Be"",\n               1105: ""B"", 1206: ""C"", 1407: ""N"",\n               1608: ""O"", 2814: ""Si"", 5626: ""Fe""}\n\n\nclass LongitudinalDataProvider:\n    """"""\n    Reads a longitudinal file and provides the data content as numpy arrays.\n\n    The class accepts a filename and automatically reads the file. The arrays are\n    accessible as data members of the class. Handles files both written with\n    and without the SLANT option.\n    """"""\n\n    def __init__(self, fn):\n        import numpy as np\n        import math\n\n        self.fn = fn\n\n        lines = file(fn).readlines()\n\n        words = [x for x in lines[0].split() if x]\n\n        n = int(words[3]) - 1\n        self.size = n\n\n        if words[4] == ""SLANT"":\n            self.depthType = ""slant""\n        elif words[4] == ""VERTICAL"":\n            self.depthType = ""vertical""\n        else:\n            raise Exception(\n                ""Could not determine depth type from first line of longitudinal file"")\n\n        self.depth = np.empty(n)\n\n        self.nPhoton = np.empty(n)\n        self.nElectronP = np.empty(n)\n        self.nElectronM = np.empty(n)\n        self.nMuonP = np.empty(n)\n        self.nMuonM = np.empty(n)\n        self.nHadron = np.empty(n)\n        self.nNuclei = np.empty(n)\n        self.nCherenkov = np.empty(n)\n\n        columns = (self.depth, self.nPhoton,\n                   self.nElectronP, self.nElectronM,\n                   self.nMuonP, self.nMuonM,\n                   self.nHadron, None, self.nNuclei,\n                   self.nCherenkov)\n\n        i = 0\n        for line in lines[2:2 + n]:\n            for j, x in enumerate(columns):\n                if j == 0:\n                    if line[:6] == "" *****"":\n                        # correct for overflow assuming equal steps in slant\n                        # depth\n                        dx = x[1] - x[0]\n                        x[i] = x[i - 1] + dx\n                    else:\n                        x[i] = float(line[:6])\n                else:\n                    if x is None:\n                        continue\n                    x[i] = float(line[6 + (j - 1) * 12:6 + j * 12])\n            i += 1\n\n        # second to last empty bogus for muons and hadrons (Corsika 6.735 with SLANT option) !\n        # extrapolate with power law if value is too low\n        def extrapol(entry, x, xp, xpp, n, np, npp):\n            log = math.log\n            logn = (log(np) - log(npp)) / (xp - xpp) * (x - xp) + log(np)\n            if n == 0 or (logn - log(n)) > 0.7:  # = factor of two\n                return math.exp(logn)\n            else:\n                return n\n\n        self.nMuonP[n - 1] = extrapol(""MU+"", self.depth[n - 1], self.depth[n - 2], self.depth[n - 3],\n                                      self.nMuonP[n - 1], self.nMuonP[n - 2], self.nMuonP[n - 3])\n        self.nMuonM[n - 1] = extrapol(""MU-"", self.depth[n - 1], self.depth[n - 2], self.depth[n - 3],\n                                      self.nMuonM[n - 1], self.nMuonM[n - 2], self.nMuonM[n - 3])\n        self.nHadron[n - 1] = extrapol(\n            ""HADRONS"", self.depth[n - 1], self.depth[n - 2], self.depth[n - 3],\n            self.nHadron[n - 1], self.nHadron[n - 2], self.nHadron[n - 3])\n\n        self.eLossPhoton = np.empty(n)\n        self.eIonLossEm = np.empty(n)\n        self.eCutLossEm = np.empty(n)\n        self.eIonLossMuon = np.empty(n)\n        self.eCutLossMuon = np.empty(n)\n        self.eIonLossHadron = np.empty(n)\n        self.eCutLossHadron = np.empty(n)\n        self.eLossNeutrino = np.empty(n)\n\n        columns = (self.eLossPhoton,\n                   self.eIonLossEm, self.eCutLossEm,\n                   self.eIonLossMuon, self.eCutLossMuon,\n                   self.eIonLossHadron, self.eCutLossHadron,\n                   self.eLossNeutrino, None)\n\n        i = 0\n        for line in lines[5 + n:5 + 2 * n]:\n            for j, x in enumerate(columns):\n                if x is None:\n                    continue\n                x[i] = float(line[7 + j * 12:7 + (j + 1) * 12])\n            i += 1\n\n\nclass SteeringDataProvider:\n    """"""\n    Reads a steering card and provides the data content.\n\n    The class accepts a filename and automatically reads the file.\n    The data in the steering card is provided in form of attributes of the class.\n\n    Limitations\n    -----------\n    This class is far from complete. I added only the most interesting things.\n    """"""\n\n    def __init__(self, fn, fixed=False, old=False, lyon=False):\n        lines = file(fn).readlines()\n\n        iterator = 0\n        for line in lines:\n            words = [x for x in line.split() if x]\n            if not words:\n                continue\n            key = words[0]\n            if key == ""RUNNR"":\n                self.runnr = int(words[1])\n            elif key == ""PRMPAR"":\n                self.primary = int(words[1])\n            elif key == ""ERANGE"":\n                self.energyRange = float(\n                    words[1]) * 1e9, float(words[2]) * 1e9  # in eV\n            elif key == ""ESLOPE"":\n                self.energySlope = float(words[1])\n            elif key == ""THETAP"":\n                self.thetaRange = float(words[1]), float(words[2])  # in Deg\n            elif key == ""PHIP"":\n                self.phiRange = float(words[1]), float(words[2])  # in Deg\n\n            elif key == ""THIN"":\n                self.thinning = [float(words[1]), float(\n                    words[2]), float(words[3])]\n            elif key == ""ATMOD"":\n                if old:\n                    self.atmod = words[1]\n                else:\n                    self.atmod = words[2]\n\n            elif (key == ""PRIMARY"" and words[1] == \'PARTICLE\' and words[2] == \'IDENTIFICATION\'):\n                self.particle = int(words[4])\n            elif (key == ""PRIMARY"" and words[1] == \'ENERGY\'):\n                self.energy = float(words[5])\n            if not fixed:\n                if (key == ""PRIMARY"" and words[1] == \'ANGLES\'):\n                    if lyon:\n                        self.zenith = float(words[6])\n                        self.azimuth = float(words[11])\n                    else:\n                        self.zenith = float(words[5])\n                        self.azimuth = float(words[9])\n            if fixed:\n                if (key == ""THETA""):\n                    self.zenith = float(words[6])\n                if (key == ""PHI""):\n                    self.azimuth = float(words[6])\n\n            # atmospheric profile, parameters for the top of the atmosphere\n            # are added (see CORSIKA manual)\n            if old == False:\n                if key == ""ATMA"":\n                    self.atma = [float(words[1]), float(\n                        words[2]), float(words[3]), float(words[4])]\n                    self.atma.append(0.01128292)\n                elif key == ""ATMB"":\n                    self.atmb = [float(words[1]), float(\n                        words[2]), float(words[3]), float(words[4])]\n                    self.atmb.append(1.)\n                elif key == ""ATMC"":\n                    self.atmc = [float(words[1]), float(\n                        words[2]), float(words[3]), float(words[4])]\n                    self.atmc.append(1.e9)\n                elif key == ""ATMLAY"":\n                    self.atmlay = [float(words[1]), float(\n                        words[2]), float(words[3]), float(words[4])]\n                    self.atmlay.insert(0, 0.)\n\n            if old:\n                if key == ""H"" and iterator == 0:\n                    lay1 = words\n                    iterator += 1\n                elif key == ""H"" and iterator == 1:\n                    lay2 = words\n                    iterator += 1\n                elif key == ""H"" and iterator == 2:\n                    lay3 = words\n                    iterator += 1\n                elif key == ""H"" and iterator == 3:\n                    lay4 = words\n                    iterator += 1\n                elif key == ""H"" and iterator == 4:\n                    lay5 = words\n                    iterator += 1\n\n            # extend here\n\n        if old:\n            try:\n                self.atma = [float(lay1[8]), float(lay2[8]),\n                             float(lay3[8]), float(lay4[7])]\n                self.atma.append(0.01128292)\n\n                self.atmb = [float(lay1[10]), float(lay2[10]),\n                             float(lay3[10]), float(lay4[9])]\n                self.atmb.append(1.)\n\n                self.atmc = [float(lay1[15][:-1]), float(lay2[15][:-1]),\n                             float(lay3[15][:-1]), float(lay4[14][:-1])]\n                self.atmc.append(1.e9)\n            except:\n                self.atma = 0\n\n\ndef IsDataFileValid(filename):\n    """""" Test whether a CORSIKA particle file is comlete (has a RUN end tag).""""""\n\n    import os\n    min_size = 26215\n    if os.path.exists(filename) and os.stat(filename).st_size > min_size:\n        f = file(filename, ""rb"")\n        f.seek(-min_size, 2)\n        if \'RUNE\' in f.read(min_size):\n            return True\n    return False\n\n\ndef createShowerInfoLibrary(corsikafilenames, libraryfilename, fixed=False, old=False, lyon=False):\n    """"""\n    Read many CORSIKA steering files (*.lst) to create a library that contains the properties of the shower.\n    The library is saved to a HDF file.\n\n    CAVEATS\n    -------\n    relies on parsing of file. if not working for your file, please compare parsing.\n    CORSIKA gives energy in GeV, for your analysis may need to correct\n\n\n    Parameters\n    ----------\n    corsikafilenames: list of CORSIKA ground particle file names or lst files (in event particle files do not exist)\n    libraryfilename: name of shelve file\n    fixed: boolean to indicate whether the library had fixed zenith angles or not (changes parsing)\n\n    old: boolean to indicate whether the library used an older CORSIKA version (changes parsing)\n    lyon: boolean to indicate whether the library is the Lyon one or not (changes parsing)\n\n    Examples\n    --------\n    >> from pyik import corsika\n    >> from glob import glob\n    >> corsika.createShowerInfoLibrary( glob(""/data/pQGSJet/DAT00084*lst""), ""mylibrary"")\n    """"""\n\n    from sys import stdout\n    from os import path\n    import fnmatch\n    from collections import defaultdict\n    import pandas as pd\n\n    # ensures that users will submit a list of files\n    if not isinstance(corsikafilenames, list):\n        print(""Program created for multiple inputs."")\n        return\n\n    atmos_values = defaultdict(list)\n    bad_files = []\n    for ifn, fn in enumerate(sorted(corsikafilenames)):\n        # ensure only CORSIKA *.lst files checked\n        if (not fnmatch.fnmatchcase(fn, ""*DAT*"") or not fnmatch.fnmatchcase(fn, ""*.lst"")):\n            continue\n\n        if not path.exists(fn):\n            print(""### warning: file not found"", fn)\n            continue\n        if (not IsDataFileValid(fn) and not fnmatch.fnmatchcase(fn, ""*.lst"")):\n            print(fn, ""has no valid run end tag, not reading it."")\n            continue\n\n        stdout.write(""Scanning file %i/%i: %s    \\r"" %\n                     (ifn + 1, len(corsikafilenames), fn))\n        stdout.flush()\n\n        if path.exists(fn):\n            lst = SteeringDataProvider(fn, fixed, old, lyon)\n            if lst.atma == 0:\n                bad_files.append(fn)\n            else:\n                try:\n                    atmos_values[""id""].append(lst.runnr)\n                    # CORSIKA gives energy in GeV, for your analysis may need to correct\n                    atmos_values[""energy_mc""].append(lst.energy)\n                    atmos_values[""primary_id""].append(lst.particle)\n\n                    if lst.particle in primary_dic:\n                        atmos_values[""primary""].append(\n                            primary_dic[lst.particle])\n                    else:\n                        atmos_values[""primary""].append(""undefined"")\n\n                    atmos_values[""zenith_mc""].append(lst.zenith)\n                    atmos_values[""azimuth_mc""].append(lst.azimuth)\n\n                    month = int(lst.atmod[-2:])\n                    atmos_values[""atm_key""].append(\n                        month)  # could be month or season\n\n                    atm_type = ""monthly""\n                    if month > 12:\n                        atm_type = ""seasonal""\n                    atmos_values[""atm_type""].append(atm_type)\n\n                    atmos_values[""atma_0""].append(lst.atma[0])\n                    atmos_values[""atma_1""].append(lst.atma[1])\n                    atmos_values[""atma_2""].append(lst.atma[2])\n                    atmos_values[""atma_3""].append(lst.atma[3])\n                    atmos_values[""atma_4""].append(lst.atma[4])\n\n                    atmos_values[""atmb_0""].append(lst.atmb[0])\n                    atmos_values[""atmb_1""].append(lst.atmb[1])\n                    atmos_values[""atmb_2""].append(lst.atmb[2])\n                    atmos_values[""atmb_3""].append(lst.atmb[3])\n                    atmos_values[""atmb_4""].append(lst.atmb[4])\n\n                    atmos_values[""atmc_0""].append(lst.atmc[0])\n                    atmos_values[""atmc_1""].append(lst.atmc[1])\n                    atmos_values[""atmc_2""].append(lst.atmc[2])\n                    atmos_values[""atmc_3""].append(lst.atmc[3])\n                    atmos_values[""atmc_4""].append(lst.atmc[4])\n\n                except:\n                    print(""\\nAtmosphere information not present in steering file of {}!"".format(fn))\n        else:\n            print(""\\nCannot read steering file, information will not be available!"")\n\n    print(""\\nBad files:"")\n    for bad in bad_files:\n        print(bad)\n\n    atm_arr = pd.DataFrame(atmos_values)\n    atm_arr.set_index([\'id\'], drop=False, inplace=True)\n    atm_arr.to_hdf(libraryfilename + "".hdf"", ""atmosphere"")\n'"
pyik/ellipse.py,12,"b'# -*- coding: utf-8 -*-\nfrom six.moves import range\nimport numpy as np\n\n\ndef fit_ellipse(x, y, centered=False):\n    """"""\n    Non-iterative least-squares fit of an ellispe to data points.\n\n    Parameters\n    ----------\n    x : array of x coordinates\n    y : array of y coordinates\n    centered : optional, set True if ellipse has known center at (0, 0)\n\n    Returns\n    -------\n    Ellipse in matrix form xT M x = 1.\n    If centered == False:\n        2x2 matrix M, center point\n    If centered == True:\n        2x2 matrix M\n\n    Notes\n    -----\n    Algorithm is fast and robust, but biased towards smaller ellipses.\n    This is because it minimizes algebraic distances instead of\n    geometric distances. For high-precision results, an iterative method\n    can be used with this result as starting point.\n\n    Original algorithm presented in:\n    Halir, Flusser,\n    Proc. 6th Int. Conf. in Central Europe on\n    Computer Graphics and Visualization,\n    WSCG, vol. 98 (1998)\n\n    Added in this version:\n    - treatment of known centre\n    - fix for very low number of points\n    - conversion to matrix representation\n    """"""\n\n    dot = np.dot\n    inv = np.linalg.inv\n    eig = np.linalg.eig\n    cols = lambda *args: np.column_stack(args)\n    rows = lambda *args: np.row_stack(args)\n\n    x = np.atleast_1d(x)\n    y = np.atleast_1d(y)\n\n    n = x.shape[0]\n\n    d1 = cols(x ** 2, x * y, y ** 2)\n    if centered:\n        d2 = cols(np.ones(n),)\n    else:\n        d2 = cols(x, y, np.ones(n))\n\n    s1 = dot(d1.T, d1)\n    s2 = dot(d1.T, d2)\n    s3 = dot(d2.T, d2)\n\n    if centered:\n        t = -s2.T / s3\n    else:\n        t = -dot(inv(s3), s2.T)\n\n    m = s1 + dot(s2, t)\n    m = rows(m[2, :] / 2.0, -m[1, :], m[0, :] / 2.0)\n\n    w, v = eig(m)\n    v = np.real(v)\n    cond = 4.0 * v[0] * v[2] - v[1] ** 2 > 0\n    for ind in range(len(cond)):\n        if cond[ind] > 0:\n            break\n    a1 = np.real(np.squeeze(v[:, ind]))\n    a2 = dot(t, a1)\n\n    m = np.empty((2, 2))\n    m[0, 0] = a1[0]\n    m[0, 1] = m[1, 0] = 0.5 * a1[1]\n    m[1, 1] = a1[2]\n\n    if centered:\n        r2 = -a2[-1]\n        return m / r2\n    else:\n        c = -0.5 * dot(inv(m), a2[:2])\n        r2 = dot(dot(c, m), c) - a2[-1]\n        return m / r2, c\n'"
pyik/fit.py,47,"b'# -*- coding: utf-8 -*-\n""""""Functions related to fitting and covariance calculation. Uses nlopt library.""""""\nfrom __future__ import print_function\nfrom six.moves import range\nimport numpy as np\n\n\ndef covariance(function, vmin, up, fast=False, bounds=None):\n    """"""\n    Numerically compute the covariance matrix from a chi^2 or -logLikelihood function.\n\n    Parameters\n    ----------\n    function: function-like\n      The function may accept only a vector argument and has to return a scalar.\n    vmin: array of floats\n      Position of the minimum.\n    up: float\n      Threshold value to pass when climbing uphill.\n      up = 1   for a chi^2 function\n      up = 0.5 for a -logLikelihood function\n    fast: boolean\n      If true invert hesse matrix at the minimum, use this if computing function is expensive.\n\n    Examples\n    --------\n    >>> cov = ((2.0,0.2),(0.2,2.0))\n    >>> invcov = np.linalg.inv(cov)\n    >>> xs = np.array((1.0,-1.0))\n    >>> def ChiSquare(pars, grad = None): return np.dot(xs-pars,np.dot(invcov,xs-pars))\n    >>> def NegLogLike(pars, grad = None): return 0.5*ChiSquare(pars)\n    >>> covariance(ChiSquare, xs, 1.0)\n    array([[2. , 0.2],\n           [0.2, 2. ]])\n    >>> covariance(ChiSquare, xs, 1.0, fast=True)\n    array([[2. , 0.2],\n           [0.2, 2. ]])\n    >>> covariance(NegLogLike, xs, 0.5)\n    array([[2. , 0.2],\n           [0.2, 2. ]])\n    >>> covariance(NegLogLike, xs, 0.5, fast=True)\n    array([[2. , 0.2],\n           [0.2, 2. ]])\n\n    Notes\n    -----\n    The algorithm is slow (it needs many function evaluations), but robust.\n    The covariance matrix is derived by explicitly following the chi^2\n    or -logLikelihood function uphill until it crosses the 1-sigma contour.\n\n    The fast alternative is to invert the hessian matrix at the minimum.\n    """"""\n\n    from scipy.optimize import brentq\n\n    class Func:\n\n        def __init__(self, function, vmin, up):\n            self.dir = np.zeros_like(vmin)\n            self.up = up\n            self.vmin = vmin\n            self.fmin = function(vmin)\n            self.func = function\n\n        def __call__(self, x):\n            return self.func(self.vmin + x * self.dir) - self.fmin - self.up\n\n        def SetDirection(self, i, j):\n            self.dir *= 0\n            self.dir[abs(i)] = 1 if i >= 0 else -1\n            self.dir[abs(j)] = 1 if j >= 0 else -1\n\n        def GetBoundary(self, sign):\n            eps = np.sqrt(np.finfo(np.double).eps)\n            h = eps\n            x0 = abs(np.dot(self.vmin, self.dir))\n\n            def IsNonsense(x):\n                return np.isnan(x) or np.isinf(x)\n\n            def x(h):\n                return sign * (h * x0 if x0 != 0 else h)\n\n            while True:\n                # (1) do smallest possible step first,\n                #     then grow exponentially until zero+ is crossed,\n\n                if IsNonsense(x(h)):\n                    raise Exception(""profile does not cross fmin + up"")\n\n                t = self(x(h))\n\n                if IsNonsense(t):\n                    # (2) if stepped into nonsense region (infinite, nan, ...),\n                    #     do bisection search towards last valid step\n                    a = h / 8.0\n                    b = h\n                    while True:\n                        if 2 * (b - a) < eps * (b + a):\n                            raise Exception(\n                                ""profile does not cross fmin + up"")\n                        h = (a + b) / 2.0\n                        t = self(x(h))\n\n                        if IsNonsense(t):\n                            b = h\n                            continue\n\n                        if t < 0:\n                            a = h\n                            continue\n\n                        return x(h)\n\n                if t > 0:\n                    return x(h)\n\n                h *= 16\n\n    n = len(vmin)\n\n    if fast:\n        from pyik.numpyext import hessian\n        releps = 1e-3\n        dvmin = np.abs(vmin) * releps\n        dvmin = vmin * releps\n        dvmin[dvmin == 0] = releps\n        if bounds is not None:\n            m = (vmin - dvmin) < bounds[:, 0]\n            dvmin[m] = vmin[m] - bounds[m, 0]\n            m = (vmin + dvmin) > bounds[:, 1]\n            dvmin[m] = bounds[m, 1] - vmin[m]\n        a = hessian(function, vmin, dvmin) / up\n    else:\n        # Ansatz: (f(r) - fmin)/up = 1/2 r^T C r == 1\n        # Diagonal elements:\n        # 1 != 1/2 sum_{ij} delta_ik x delta_jk x C_ij\n        #    = x^2/2 C_kk\n        # => C_kk = 2/x^2\n        # Off-diagonal elements:\n        # 1 != 1/2 x (delta_ik + delta_il) C_ij x (delta_jk + delta_jl)\n        #    = x^2/2 (C_kk + C_kl + C_lk + C_ll) = x^2/2 (2 C_kl + C_kk + C_ll)\n        # => C_kl = 0.5 * (2/x^2 - C_kk - C_ll)\n\n        func = Func(function, vmin, up)\n        d = np.empty((n, n))\n        for i in range(n):\n            func.SetDirection(i, i)\n\n            if bounds is None:\n                xu = func.GetBoundary(+1)\n                t = func(-xu)\n                xd = -xu if (t > 0.0 and not np.isinf(t)) \\\n                    else func.GetBoundary(-1)\n            else:\n                xd = bounds[i][0]\n                xu = bounds[i][1]\n                if xd == xu:\n                    d[i, i] = 0.0\n                    continue\n\n            x1 = +brentq(func, 0, xu)\n            x2 = -brentq(func, xd, 0)\n            x = 0.5 * (x1 + x2)\n\n            if x < 0:\n                raise Exception(""x may not be negative"")\n\n            d[i, i] = x\n\n        for i in range(n - 1):\n            for j in range(i + 1, n):\n                func.SetDirection(i, j)\n\n                if (bounds is not None and\n                    (bounds[i][0] == bounds[i][1] or\n                     bounds[j][0] == bounds[j][1])):\n                    d[i, j] = d[j, i] = 0.0\n                    continue\n\n                xu = func.GetBoundary(+1)\n                t = func(-xu)\n                xd = -xu if (t > 0.0 and not np.isinf(t)) \\\n                    else func.GetBoundary(-1)\n\n                x1 = +brentq(func, 0, xu)\n                x2 = -brentq(func, xd, 0)\n                x = 0.5 * (x1 + x2)\n\n                if x < 0:\n                    raise Exception(""x may not be negative"")\n\n                # check whether x is in possible range\n                a = d[i, i]\n                b = d[j, j]\n                xmax = np.inf if a <= b else 1.0 / (1.0 / b - 1.0 / a)\n                xmin = 1.0 / (1.0 / b + 1.0 / a)\n\n                if x <= xmin:\n                    print(""covariance(...):"", xmin, ""<"", x, ""<"", xmax, ""violated"")\n                    x = xmin * 1.01\n                if x >= xmax:\n                    print(""covariance(...):"", xmin, ""<"", x, ""<"", xmax, ""violated"")\n                    x = xmax * 0.99\n\n                d[i, j] = d[j, i] = x\n\n        a = 2.0 / d ** 2\n\n        for i in range(n - 1):\n            for j in range(i + 1, n):\n                a[i, j] = a[j, i] = 0.5 * (a[i, j] - a[i, i] - a[j, j])\n\n    # Beware: in case of a chi^2 we calculated\n    # t^2 = (d^2 chi^2 / d par^2)^{-1},\n    # while s^2 = (1/2 d^2 chi^2 / d par^2)^{-1} is correct,\n    # thus s^2 = 2 t^2\n\n    m = np.diag(a) > 0.0\n    if np.all(m):\n        cov = 2.0 * np.linalg.inv(a)\n    else:\n        k = a.shape[0]\n        n = np.sum(m)\n        mm = np.outer(m, m).flatten()\n        a1 = a.flatten()[mm]\n        cov1 = 2.0 * np.linalg.inv(a1.reshape(n, n))\n        cov = np.nan * np.empty(k * k)\n        cov[mm] = cov1\n        cov = cov.reshape(k, k)\n\n    # first aid, if 1-sigma contour does not look like hyper-ellipsoid\n    for i in range(n):\n        if cov[i, i] < 0:\n            print(""covariance(...): error, cov[%i,%i] < 0, returning zero"" % (i, i))\n            for j in range(n):\n                cov[i, j] = 0\n\n    return cov\n\n\nclass Minimizer(object):\n    """"""\n    Convenience wrapper for nlopt.\n\n    Notes\n    -----\n    Implemented methods:\n    BOBYQA, SBPLX, PRAXIS, COBYLA, MLSL, DIRECT, DIRECT-L\n    """"""\n\n    @staticmethod\n    def GetMethods():\n        # first entry is default\n        return (""BOBYQA"", ""SBPLX"", ""PRAXIS"", ""COBYLA"", ""MLSL"", ""DIRECT"", ""DIRECT-L"")\n\n    def __init__(self):\n        self.upper_bounds = None\n        self.lower_bounds = None\n        self.maxeval = None\n        self.ftolabs = 1e-6\n        self.ftolrel = 0.0\n        self.method = None\n        self.stochastic_population = 0\n        self.max_time = 0\n        self.neval = 0\n\n    def SetMethod(self, method):\n        if method not in self.GetMethods():\n            raise ValueError(""method %s is not recognized"" % method)\n        self.method = method\n\n    def SetUpperBounds(self, bounds):\n        self.upper_bounds = bounds\n\n    def SetLowerBounds(self, bounds):\n        self.lower_bounds = bounds\n\n    def SetMaximumEvaluations(self, maxeval):\n        self.maxeval = maxeval\n\n    def SetAbsoluteTolerance(self, ftolabs):\n        self.ftolabs = ftolabs\n\n    def SetRelativeTolerance(self, ftolrel):\n        self.ftolrel = ftolrel\n\n    def SetStochasticPopulation(self, pop):\n        self.stochastic_population = pop\n\n    def SetMaximumTime(self, max_time):\n        self.max_time = max_time\n\n    def GetNumberOfEvaluations(self):\n        return self.neval\n\n    def GetNumberOfFittedParameters(self):\n        k = 0\n        for i in range(len(self.lower_bounds)):\n            if self.lower_bounds[i] < self.upper_bounds[i]:\n                k += 1\n        return k\n\n    def __call__(self, function, starts):\n        import nlopt\n\n        nExt = len(starts)\n\n        fix = []\n        iStarts = []\n        iLower = []\n        iUpper = []\n        for ipar in range(nExt):\n            s = starts[ipar]\n            l = - \\\n                np.inf if self.lower_bounds is None else self.lower_bounds[ipar]\n            u = np.inf if self.upper_bounds is None else self.upper_bounds[ipar]\n            if s < l or u < s:\n                raise ValueError(\n                    ""Par %i: starting value %g not in range [%g,%g]"" % (ipar, s, l, u))\n            if l == u:\n                fix.append(ipar)\n            else:\n                iStarts.append(s)\n                iLower.append(l)\n                iUpper.append(u)\n\n        iStarts = np.asarray(iStarts)\n        iLower = np.array(iLower)\n        iUpper = np.array(iUpper)\n\n        def WrappedFunction(iPars, grad=None):\n            self.neval += 1\n            ePars = np.empty(nExt)\n            ipar = 0\n            for k in range(nExt):\n                if k in fix:\n                    ePars[k] = starts[k]\n                else:\n                    ePars[k] = iPars[ipar]\n                    ipar += 1\n            return function(ePars)\n\n        def MinimizeWithMethod(method):\n            if method == ""PRAXIS"":\n                method = nlopt.LN_PRAXIS\n            elif method == ""BOBYQA"":\n                method = nlopt.LN_BOBYQA\n            elif method == ""COBYLA"":\n                method = nlopt.LN_COBYLA\n            elif method == ""SBPLX"":\n                method = nlopt.LN_SBPLX\n            elif method == ""MLSL"":\n                method = nlopt.G_MLSL_LDS, nlopt.LN_BOBYQA\n            elif method == ""DIRECT"":\n                method = nlopt.GN_DIRECT\n            elif method == ""DIRECT-L"":\n                method = nlopt.GN_DIRECT_L\n            npar = len(iStarts)\n            if type(method) == tuple:\n                opt = nlopt.opt(method[0], npar)\n                local_opt = nlopt.opt(method[1], npar)\n                local_opt.set_lower_bounds(iLower)\n                local_opt.set_upper_bounds(iUpper)\n                opt.set_local_optimizer(local_opt)\n                opt.set_population(self.stochastic_population)\n            else:\n                opt = nlopt.opt(method, npar)\n            opt.set_min_objective(WrappedFunction)\n            opt.set_ftol_abs(self.ftolabs)\n            opt.set_ftol_rel(self.ftolrel)\n            opt.set_maxtime(self.max_time)\n            if self.maxeval is None:\n                opt.set_maxeval(1000 + 100 * npar ** 2)\n            else:\n                opt.set_maxeval(self.maxeval)\n            opt.set_lower_bounds(iLower)\n            opt.set_upper_bounds(iUpper)\n            sqrtdbleps = np.sqrt(np.finfo(np.double).eps)\n            opt.set_xtol_rel(sqrtdbleps)\n            iresult = opt.optimize(iStarts)\n            if np.isinf(opt.last_optimum_value()):\n                raise ValueError(""got inf"")\n            if np.isnan(opt.last_optimum_value()):\n                raise ValueError(""got nan"")\n            return iresult\n\n        iResult = None\n        if self.method is None:\n            # try in order...\n            for method in (""BOBYQA"", ""SBPLX"", ""PRAXIS""):\n                try:\n                    iResult = MinimizeWithMethod(method)\n                    break\n                except:\n                    import sys\n                    etype, e = sys.exc_info()[:2]\n                    if etype is KeyboardInterrupt:\n                        raise SystemExit(""KeyboardInterrupt"")\n                    else:\n                        print(""Caught exception %s during method %s: %s, trying next"" % (etype, method, e))\n            if iResult is None:\n                raise\n        else:\n            iResult = MinimizeWithMethod(self.method)\n\n        result = np.empty(nExt)\n        ipar = 0\n        for k in range(nExt):\n            if k in fix:\n                result[k] = starts[k]\n            else:\n                result[k] = iResult[ipar]\n                ipar += 1\n        return result\n\n\nclass ChiSquareFunction(object):\n    """"""\n    Chi^2 function for fitting a model to data with the least-squares method.\n\n    Parameters\n    ----------\n    model: function-like\n      Model function. Has to be callable with two arguments:\n        x, the abscissa value (may be a vector)\n        par, the parameter vector\n      The function has to be vectorized, see numpy.frompyfunc and numpy.vectorize.\n    xs: array of floats\n      Array of abscissa values (may be an array of vectors).\n    ys: array of floats\n      Data array.\n    yerrs: array of floats (optional)\n      Array with the expected uncertainties of the data values.\n      If this is not set, the computed covariance of your fit may be very wrong!\n    xerrs: array of floats (optional)\n      Array with the expected uncertainties of the abscissa values.\n      Be careful when using this. The computation becomes very slow if these\n      are defined. If your abscissa values have uncertainties,\n      it is better to define a -logLikelihood function for your problem.\n\n    Examples\n    --------\n    >>> def model(x, par): return par[0]*x\n    >>> xs = np.linspace(0,1,5)\n    >>> ys = model(xs,[2]) + np.array([ 1.1446054 , -1.84184869,  0.43702669,  0.0513386 , -0.79315476])\n    >>> yerrs = np.ones(5)\n    >>> par, cov, chi2, ndof = ChiSquareFunction(model, xs, ys, yerrs).Minimize(1)\n    >>> print(""%.3f +/- %.3f"" % (par,np.sqrt(cov)))\n    1.468 +/- 0.730\n    >>> print(""chi2/ndof = %.1f/%i = %.1f"" % (chi2, ndof, chi2/ndof))\n    chi2/ndof = 5.0/4 = 1.2\n    """"""\n\n    def __init__(self, model, xs, ys, yerrs=None, xerrs=None):\n        self.model = model\n\n        xs = np.atleast_1d(xs)\n        ys = np.atleast_1d(ys)\n\n        if yerrs is None:\n            yerrs = np.ones_like(ys)\n            self.guessCovariance = True\n        else:\n            self.guessCovariance = False\n            yerrs = np.atleast_1d(yerrs)\n            if yerrs.shape != ys.shape:\n                raise ValueError(""shapes of ys and yerrs differ"")\n\n        # check for invalid yerrs\n        mask = yerrs > 0\n        nBad = len(mask) - np.sum(mask)\n        if nBad > 0:\n            print(""Warning: %i zeros found in yerrs, corresponding data points will be ignored"" % nBad)\n\n        self.ys = ys[mask].flatten()\n        self.yerrs2 = yerrs[mask].flatten() ** 2\n\n        if ys.ndim != 1:\n            self.xs = np.array([t[mask].flatten() for t in xs])\n            self.xerrs = None if xerrs is None else np.array(\n                [t[mask].flatten() for t in xerrs])\n        else:\n            self.xs = xs[mask]\n            self.xerrs = None if xerrs is None else xerrs[mask]\n\n    def __call__(self, par, grad=None):\n\n        from pyik.numpyext import derivative\n\n        def parmodel(x):\n            return self.model(x, par)\n\n        yerrs2 = self.yerrs2\n        if self.xerrs is not None:\n            eys = np.frompyfunc(lambda x, xerr: derivative(\n                parmodel, x) * xerr, 2, 1)(self.xs, self.xerrs)\n            # yerrs2 += eys * eys\n            yerrs2 = yerrs2 + eys * eys\n\n        try:\n            yms = parmodel(self.xs)\n        except TypeError:\n            raise TypeError(\n                ""model is not vectorized, see numpy.frompyfunc and numpy.vectorize"")\n\n        result = np.sum((self.ys - yms) ** 2 / yerrs2)\n\n        # workaround for strange bug in sum\n        if result.shape:\n            return result[0]\n        else:\n            return result\n\n    def Minimize(self, starts, lower_bounds=None, upper_bounds=None, method=None,\n                 absolute_tolerance=1e-6, relative_tolerance=0.0,\n                 stochastic_population=0, max_evaluations=None,\n                 max_time=0, covarianceMethod=""fast""):\n        """"""\n        Minimize the chi^2 function.\n\n        Parameters\n        ----------\n        starts: array of floats or single float\n          Array with starting values for the numerical minimizer.\n        lower_bounds: array of floats or single float (optional)\n          Array with lower bounds on the parameters.\n        upper_bounds: array of floats or single float (optional)\n          Array with upper bounds on the parameters.\n        method: string (optional)\n          Minimization method to use, see Minimizer.GetMethods()\n          for the available selection.\n        absolute_tolerance : float (optional)\n          Absolute tolerance used as stopping criterion. If set\n          to zero, this criterion will be disabled.\n        relative_tolerance : float (optional)\n          Relative tolerance used as stopping criterion. If set\n          to zero, this criterion will be disabled.\n        stochastic_population : int (optional)\n          Size of the stochastic population for stochastic search\n          algorithms. If set to zero, the internal heuristic\n          will be used.\n        max_evaluations : int (optional)\n          Maximum number of function evaluations used as stopping\n          criterion. If set to None, this criterion will be\n          disabled.\n        covarianceMethod: string\n          Either ""fast"" or ""slow"". Fast algorithm computes inverse\n          of Hesse matrix from numerical derivatives, slow follows\n          the chi2 function until it crosses +1.\n\n        Returns\n        -------\n        par: array of floats\n          Best fit of the parameter vector.\n        cov: matrix of floats\n          Covariance matrix of the solution.\n        chi2: float\n          chi^2 value at the minimum.\n        ndof: float\n          Statistical degrees of freedom of the fit.\n        """"""\n\n        # to decide on format of return values in case of 1d fits\n        do_squeeze = hasattr(starts, ""__len__"")\n\n        starts = np.atleast_1d(starts)\n\n        m = Minimizer()\n\n        if method is not None:\n            m.SetMethod(method)\n\n        if lower_bounds is not None:\n            if np.any(starts < lower_bounds):\n                raise ValueError(\n                    ""A start value is smaller than its lower bound."")\n            m.SetLowerBounds(lower_bounds)\n\n        if upper_bounds is not None:\n            if np.any(starts > upper_bounds):\n                raise ValueError(\n                    ""A start value is larger than its upper bound."")\n            m.SetUpperBounds(upper_bounds)\n\n        m.SetAbsoluteTolerance(absolute_tolerance)\n        m.SetRelativeTolerance(relative_tolerance)\n        m.SetStochasticPopulation(stochastic_population)\n        m.SetMaximumEvaluations(max_evaluations)\n        m.SetMaximumTime(max_time)\n\n        pars = m(self, starts)\n        chi2 = self(pars)\n        ndof = len(self.ys) - len(starts)\n\n        if covarianceMethod is None:\n\n            if do_squeeze:\n                pars = np.squeeze(pars)\n            return pars, chi2, ndof\n\n        else:\n            useFastMethod = covarianceMethod == ""fast""\n\n            up = 1.0\n            if self.guessCovariance:\n                # this assumes equal errors on all data points and chi2 = ndof\n                up = chi2 / ndof\n                chi2 = ndof\n            cov = covariance(self, pars, up, fast=useFastMethod)\n            if do_squeeze:\n                pars, cov = np.squeeze(pars), np.squeeze(cov)\n            return pars, cov, chi2, ndof\n'"
pyik/locked_shelve.py,0,"b'# from http://code.activestate.com/recipes/576591-simple-shelve-with-linux-file-locking/\n#\n# Copyright (c) 21 Dec 2008 Michael Ihde\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the ""Software""), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\nimport shelve\nimport fcntl\nimport types\nfrom six.moves import builtins\nfrom fcntl import LOCK_SH, LOCK_EX, LOCK_UN, LOCK_NB\n\n\ndef _close(self):\n    shelve.Shelf.close(self)\n    fcntl.flock(self.lckfile.fileno(), LOCK_UN)\n    self.lckfile.close()\n\n\ndef open(filename, flag=\'c\', protocol=None, writeback=False, block=True, lckfilename=None):\n    """"""\n    Open the sheve file, createing a lockfile at filename.lck.\n\n    If block is False then a IOError will be raised if the lock cannot be acquired.\n    """"""\n    if lckfilename is None:\n        lckfilename = filename + "".lck""\n    lckfile = builtins.open(lckfilename, \'w\')\n\n    # Accquire the lock\n    if flag == \'r\':\n        lockflags = LOCK_SH\n    else:\n        lockflags = LOCK_EX\n    if not block:\n        lockflags = LOCK_NB\n    fcntl.flock(lckfile.fileno(), lockflags)\n\n    shelf = shelve.open(filename, flag, protocol, writeback)\n    shelf.close = types.MethodType(_close, shelf)\n    shelf.lckfile = lckfile\n    return shelf\n'"
pyik/misc.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nContains miscellanious helper functions/classes.\n\nPlease try to add functions to the other files first if they can be topically grouped.\n\n""""""\n\n\ndef cprint(pstr, cstr=""white""):\n    """"""\n    Print with color specified in cstr and fall back to normal print if fabric\n    not installed.\n    """"""\n\n    try:\n        import fabric.api as fab\n        from fabric.colors import red, green, blue, yellow, cyan, magenta, white\n        cmap = {""white"": white, ""yellow"": yellow, ""cyan"": cyan, ""green"": green, ""blue"": blue,\n                ""red"": red, ""magenta"": magenta, ""g"": green, ""b"": blue, ""w"": white, ""r"": red,\n                ""m"": magenta, ""c"": cyan, ""y"": yellow}\n        fab.puts(cmap[cstr.lower()](pstr))\n    except:\n        print(pstr)\n\n'"
pyik/mplext.py,37,"b'# -*- coding: utf-8 -*-\n""""""\nContains extensions to matplotlib.\n\nSpecial functions to extend matplotlib in areas where it lacks certain functionality.\n\n""""""\nfrom __future__ import print_function\nfrom six.moves import range\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport matplotlib.colors as mplcolors\nimport colorsys\n\n\ndef lighten_color(color, amount):\n    """"""\n    Lightens the given color by multiplying (1-luminosity) by the given amount.\n    Input can be matplotlib color string, hex string, or RGB tuple. Can also darken.\n\n    Examples:\n    >> lighten_color(\'g\', 0.3)\n    >> lighten_color(\'#F034A3\', 0.6)\n    >> lighten_color((.3,.55,.1), 0.5)\n    """"""\n    c = mplcolors.cnames.get(color, color)\n    c = colorsys.rgb_to_hls(*mplcolors.to_rgb(c))\n    return colorsys.hls_to_rgb(c[0], min(max(c[1] + amount, 0.0), 1.0), c[2])\n\n\ndef plot_stacked(\n    xe, w, labels, colors=None, threshold=0.0, sub_threshold_color=""0.8"", reverse=True\n):\n    """"""\n    Plot stacked histograms.\n\n    A legend is not automatically drawn by this command. Call pyplot.legend() or figure.legend() to generate it.\n\n    Parameters\n    ----------\n    xe: array(N+1)\n        Bin edges.\n    w: array((M, N))\n        Counts. M is the number of stacked histograms.\n    labels: sequences of strings\n        Labels for the legend.\n    colors: sequence over colors OR None (default: None)\n        Colors to be used for each entry.\n    threshold: float (default: 0.0)\n        Fraction below which the stacked histograms are grouped together.\n    sub_threshold_color: matplotlib color (default: ""0.8"")\n        Color for the sub-threshold stacks.\n    reverse: boolean (default: True)\n        If true, plot the largest component at the bottom of the stack and work towards smaller and smaller components. If false, start with the smallest component.\n    """"""\n    n = w.shape[0]\n    assert len(labels) == n\n    wsum = np.sum(w, axis=1)\n\n    def transposed(items):\n        return map(list, zip(*items))\n\n    wsum, indices = transposed(sorted(zip(wsum, range(n)), reverse=reverse))\n    fractions = wsum / np.sum(wsum)\n    indices = np.array(indices)\n    wsum = np.sum(w[indices[fractions < threshold]], axis=0)\n    indices = indices[fractions >= threshold]\n    n = len(indices)\n    plot_hist(xe, wsum, facecolor=sub_threshold_color, zorder=n)\n    for i, j in enumerate(indices):\n        wsum += w[j]\n        plot_hist(\n            xe,\n            wsum,\n            color=f""C{n-1-i}"" if colors is None else colors[j],\n            facecolor=f""C{n-1-i}"" if colors is None else colors[j],\n            zorder=n - 1 - i,\n            label=labels[j],\n        )\n\n\ndef plot_bracket(x, y, yerr, xerr=None, capsize=3, axes=None, **kwargs):\n    """"""\n    Plot brackets to indicate errors.\n\n    Parameters\n    ----------\n    x,y,yerr,xerr: central value and errors\n    markersize: size of the bracket\n    capsize: length of the tips of the bracket\n    """"""\n\n    if axes is None:\n        axes = plt.gca()\n\n    for k in (""mec"", ""markeredgecolor"", ""marker"", ""m""):\n        if k in kwargs:\n            raise ValueError(""keyword %s not allowed"" % k)\n\n    col = ""k""\n    for k in (""c"", ""color""):\n        if k in kwargs:\n            col = kwargs[k]\n            del kwargs[k]\n    kwargs[""ls""] = ""None""\n\n    x = np.atleast_1d(x)\n    y = np.atleast_1d(y)\n\n    if yerr is not None:\n        yerr = np.atleast_1d(yerr)\n        if len(yerr.shape) == 1:\n            yd = y - yerr\n            yu = y + yerr\n        elif len(yerr.shape) == 2 and yerr.shape[0] == 2:\n            yd = y - yerr[0]\n            yu = y + yerr[1]\n        else:\n            raise ValueError(""yerr has unexpected shape"")\n\n        dx = 0.01\n        dy = 0.05\n        t = 2 * dy * capsize\n        w = 0.5\n        m1 = (\n            (-w - dx, t + dy),\n            (-w - dx, -dy),\n            (w + dx, -dy),\n            (w + dx, t + dy),\n            (w - dx, t + dy),\n            (w - dx, dy),\n            (-w + dx, dy),\n            (-w + dx, t + dy),\n        )\n        m2 = (\n            (-w - dx, -t - dy),\n            (-w - dx, dy),\n            (w + dx, dy),\n            (w + dx, -t - dy),\n            (w - dx, -t - dy),\n            (w - dx, -dy),\n            (-w + dx, -dy),\n            (-w + dx, -t - dy),\n        )\n        axes.plot(x, yd, marker=m1, color=col, mec=col, **kwargs)\n        axes.plot(x, yu, marker=m2, color=col, mec=col, **kwargs)\n\n    if xerr is not None:\n        xerr = np.atleast_1d(xerr)\n        if len(xerr.shape) == 1:\n            xd = x - xerr\n            xu = x + xerr\n        elif len(xerr.shape) == 2 and xerr.shape[0] == 2:\n            xd = x - xerr[0]\n            xu = x + xerr[1]\n        else:\n            raise ValueError(""xerr has unexpected shape"")\n\n        dx = 0.05\n        dy = 0.01\n        t = 2 * dx * capsize\n        h = 0.5\n        m1 = (\n            (t + dx, -h - dy),\n            (-dx, -h - dy),\n            (-dx, h + dy),\n            (t + dx, h + dy),\n            (t + dx, h - dy),\n            (dx, h - dy),\n            (dx, -h + dy),\n            (t + dx, -h + dy),\n        )\n        m2 = (\n            (-t - dx, -h - dy),\n            (dx, -h - dy),\n            (dx, h + dy),\n            (-t - dx, h + dy),\n            (-t - dx, h - dy),\n            (-dx, h - dy),\n            (-dx, -h + dy),\n            (-t - dx, -h + dy),\n        )\n        axes.plot(xd, y, marker=m1, color=col, mec=col, **kwargs)\n        axes.plot(xu, y, marker=m2, color=col, mec=col, **kwargs)\n\n\ndef plot_hist(xedges, ws, axes=None, **kwargs):\n    """"""\n    Plot histogram data in ROOT style.\n\n    Parameters\n    ----------\n    xedge: lower bin boundaries + upper boundary of last bin\n    w: content of the bins\n    facecolor: a matplotlib color definition to fill the histogram\n    axes: the axes to draw on (defaults to the current axes)\n    """"""\n    if axes is None:\n        axes = plt.gca()\n\n    m = len(ws)\n    n = 2 * m + 2\n\n    xy = np.zeros((2, n))\n\n    xy[0][0] = xedges[0]\n    xy[0][-1] = xedges[-1]\n\n    for i in range(m):\n        xy[0][1 + 2 * i] = xedges[i]\n        xy[1][1 + 2 * i] = ws[i]\n        xy[0][1 + 2 * i + 1] = xedges[i + 1]\n        xy[1][1 + 2 * i + 1] = ws[i]\n\n    if ""fc"" in kwargs:\n        kwargs[""facecolor""] = kwargs[""fc""]\n        del kwargs[""fc""]\n    if ""c"" in kwargs:\n        kwargs[""color""] = kwargs[""c""]\n        del kwargs[""c""]\n\n    if ""facecolor"" in kwargs:\n        if ""color"" in kwargs:\n            kwargs[""edgecolor""] = kwargs[""color""]\n            del kwargs[""color""]\n        if ""label"" in kwargs:\n            # label hack\n            from matplotlib.patches import Rectangle\n\n            r = Rectangle((0, 0), 0, 0, **kwargs)\n            axes.add_patch(r)\n            del kwargs[""label""]\n        return axes.fill_between(xy[0], 0, xy[1], **kwargs)\n    else:\n        return axes.plot(xy[0], xy[1], **kwargs)\n\n\ndef plot_boxerrors(xedges, ys, yes, axes=None, **kwargs):\n    """"""\n    Plot error boxes for a histogram.\n\n    (Recommended way to show systematic uncertainties).\n\n    Parameters\n    ----------\n    xedge: array of floats\n      Lower bin boundaries + upper boundary of last bin as returned\n      by numpy.histogram.\n    ys: array of floats\n      Center of the box.\n    yes: array of floats\n      Distance of the edge of the box from the center. Maybe one-dimensional\n      for symmetric boxes or two-dimensional for asymmetric boxes.\n    axes: Axes (optional, default: None)\n      The axes to draw on (defaults to the current axes).\n\n    Optional keyword arguments are forwarded to the matplotlib.patch.Rectangle\n    objects. Useful keywords are: facecolor, edgecolor, alpha, zorder.\n    """"""\n\n    from matplotlib.patches import Rectangle\n\n    if axes is None:\n        axes = plt.gca()\n\n    xedges = np.atleast_1d(xedges)\n    ys = np.atleast_1d(ys)\n    yes = np.atleast_1d(yes)\n\n    n = len(ys)\n    isAsymmetric = len(yes.shape) == 2\n    rs = []\n    for i in range(n):\n        x0 = xedges[i]\n        y0 = ys[i] - yes[i][0] if isAsymmetric else ys[i] - yes[i]\n        xw = xedges[i + 1] - xedges[i]\n        yw = yes[i][0] + yes[i][1] if isAsymmetric else 2 * yes[i]\n        if yw > 0:\n            r = Rectangle((x0, y0), xw, yw, **kwargs)\n            rs.append(r)\n            axes.add_artist(r)\n    return rs\n\n\ndef cornertext(text, loc=2, color=None, frameon=False, axes=None, **kwargs):\n    """"""\n    Conveniently places text in a corner of a plot.\n\n    Parameters\n    ----------\n    text: string or sequence of strings\n      Text to be placed in the plot. May be a sequence of strings to get\n      several lines of text.\n    loc: integer or string\n      Location of text, same as in legend(...).\n    color: color or sequence of colors\n      For making colored text. May be a sequence of colors to color\n      each text line differently.\n    frameon: boolean (optional)\n      Whether to draw a border around the text. Default is False.\n    axes: Axes (optional, default: None)\n      Axes object which houses the text (defaults to the current axes).\n    fontproperties: matplotlib.font_manager.FontProperties object\n      Change the font style.\n\n    Other keyword arguments are forwarded to the text instance.\n    """"""\n\n    from matplotlib.offsetbox import AnchoredOffsetbox, VPacker, TextArea\n    from matplotlib import rcParams\n    from matplotlib.font_manager import FontProperties\n    import warnings\n\n    if axes is None:\n        axes = plt.gca()\n\n    locTranslate = {\n        ""upper right"": 1,\n        ""upper left"": 2,\n        ""lower left"": 3,\n        ""lower right"": 4,\n        ""right"": 5,\n        ""center left"": 6,\n        ""center right"": 7,\n        ""lower center"": 8,\n        ""upper center"": 9,\n        ""center"": 10,\n    }\n\n    if isinstance(loc, str):\n        if loc in locTranslate:\n            loc = locTranslate[loc]\n        else:\n            message = (\n                \'Unrecognized location ""%s"". Falling back on ""upper left""; valid \'\n                ""locations are\\n\\t%s""\n            ) % (loc, ""\\n\\t"".join(locTranslate.keys()))\n            warnings.warn(message)\n            loc = 2\n\n    if ""borderpad"" in kwargs:\n        borderpad = kwargs[""borderpad""]\n        del kwargs[""borderpad""]\n    else:\n        borderpad = rcParams[""legend.borderpad""]\n\n    if ""borderaxespad"" in kwargs:\n        borderaxespad = kwargs[""borderaxespad""]\n        del kwargs[""borderaxespad""]\n    else:\n        borderaxespad = rcParams[""legend.borderaxespad""]\n\n    if ""handletextpad"" in kwargs:\n        handletextpad = kwargs[""handletextpad""]\n        del kwargs[""handletextpad""]\n    else:\n        handletextpad = rcParams[""legend.handletextpad""]\n\n    if ""fontproperties"" in kwargs:\n        fontproperties = kwargs[""fontproperties""]\n        del kwargs[""fontproperties""]\n    else:\n        if ""size"" in kwargs:\n            size = kwargs[""size""]\n            del kwargs[""size""]\n        elif ""fontsize"" in kwargs:\n            size = kwargs[""fontsize""]\n            del kwargs[""fontsize""]\n        else:\n            size = rcParams[""legend.fontsize""]\n        fontproperties = FontProperties(size=size)\n\n    texts = [text] if isinstance(text, str) else text\n\n    colors = (\n        [color for t in texts] if (isinstance(color, str) or color is None) else color\n    )\n\n    tas = []\n    for t, c in zip(texts, colors):\n        ta = TextArea(\n            t,\n            textprops={""color"": c, ""fontproperties"": fontproperties},\n            multilinebaseline=True,\n            minimumdescent=True,\n            **kwargs,\n        )\n        tas.append(ta)\n\n    vpack = VPacker(children=tas, pad=0, sep=handletextpad)\n\n    aob = AnchoredOffsetbox(\n        loc, child=vpack, pad=borderpad, borderpad=borderaxespad, frameon=frameon\n    )\n\n    axes.add_artist(aob)\n    return aob\n\n\ndef uncertainty_ellipse(par, cov, cfactor=1.51, axes=None, **kwargs):\n    """"""\n    Draw a 2D uncertainty ellipse.\n\n    Parameters\n    ----------\n    par: array-like\n      The parameter vector.\n    cov: array-like\n      The covariance matrix.\n    cfactor: float (optional, default: 1.51 for 68 % coverage)\n      Scaling factor to give the ellipse a desired coverage.\n    axes: Axes (optional, default: None)\n      The axes to draw on (defaults to the current axes).\n    Other keyword-based arguments may be given, which are forwarded to\n    the ellipse object.\n\n    Returns\n    -------\n    An ellipse patch.\n\n    Notes\n    -----\n    To compute the coverage factor with scipy, do\n    >>> from scipy.stats import chi2\n    >>> p_coverage = 0.68 # desired coverage\n    >>> round(chi2(2).ppf(p_coverage) ** 0.5, 4)\n    1.5096\n    """"""\n\n    from math import atan2, pi, sqrt\n    from matplotlib.patches import Ellipse\n\n    if axes is None:\n        axes = plt.gca()\n\n    u, s, v = np.linalg.svd(cov)\n\n    angle = atan2(u[1, 0], u[0, 0]) * 180.0 / pi\n    s0 = cfactor * sqrt(s[0])\n    s1 = cfactor * sqrt(s[1])\n\n    ellipse = Ellipse(xy=par, width=2.0 * s0, height=2.0 * s1, angle=angle, **kwargs)\n    axes.add_patch(ellipse)\n\n    return ellipse\n\n\ndef ViolinPlot(\n    x,\n    y,\n    bins=10,\n    range=None,\n    offsetX=0,\n    offsetY=0,\n    color=""k"",\n    marker=""o"",\n    draw=""amv"",\n    xmean=False,\n    extend=3,\n    outliers=True,\n    textpos=None,\n    axes=None,\n    **kwargs,\n):\n    """"""\n    Draw a violin (kernel density estimate) plot with mean and median profiles.\n\n    Adapted from http://pyinsci.blogspot.de/2009/09/violin-plot-with-matplotlib.html.\n    Updated and simplified version to maximize data/ink ratio according to Tufte.\n\n    Parameters\n    ----------\n    x: array of type float\n      Data dimension to bin in\n    y: array of type float\n      Data to create profile from\n    bins: int or array of type float or None\n      Number of x bins or array of bin edges\n      If None: Take x as bin centers and y as already binned values\n    range (optional):\n      The range in x used for binning\n\n    color: Matplotlib-compatible color value\n      Color that is used to draw markers and violins\n\n    marker: Matplotlib-compatible marker\n      Marker that is used for mean profile\n\n    draw: string (default: ""amv"")\n      What to draw: a (mean), m (median), v (violins), s (only 1 sigma violins), c (number of entries within bins)\n\n    extend: float in units of standard deviation (default: 3)\n      If a float x is given, the violins are drawn from -l to +u\n      The values l, u are possibly asymmetric quantiles with respect to x sigma (normal distribution)\n      If None is given, the violins are drawn in between the most extreme points\n\n    outliers: bool (default: True)\n      If true, will draw outlier points outside of extend\n\n    axes: Axes (optional, default: None)\n      The axes to draw on (defaults to the current axes)\n\n    Other keyword-based arguments may be given, which are forwarded to\n    the individual plot/errorbar/fill_between calls.\n\n    Returns\n    -------\n    Calculated profile values\n    """"""\n\n    if axes is None:\n        axes = plt.gca()\n    else:\n        plt.sca(axes)\n\n    from scipy.stats import gaussian_kde, norm\n    from pyik.numpyext import bin, centers, mad\n    from scipy.stats.mstats import mquantiles\n\n    s1 = norm.cdf(-1)\n    sx = norm.cdf(-3 if extend is None else -extend)\n\n    if bins is not None:\n        ybins, xedgs = bin(x, np.column_stack((x, y)), bins=bins, range=range)\n        xcens, xhws = centers(xedgs)\n    else:\n        xcens = x\n        # xhws = (x[1:] - x[:-1]) / 2.\n        # xhws = np.append(xhws, xhws[-1])\n        xhws = np.ones(len(x)) * min((x[1:] - x[:-1]) / 2.0)\n        ybins = y\n\n    l = len(ybins)\n    means, stds, meds, mads, ns = (\n        np.zeros(l),\n        np.zeros(l),\n        np.zeros(l),\n        np.zeros(l),\n        np.zeros(l),\n    )\n\n    for i, ybin in enumerate(ybins):\n\n        ybind = np.asfarray(ybin)\n\n        if bins is not None:\n\n            if len(ybind) < 1:\n                continue\n\n            if len(ybind) == 1:\n\n                xbinh = np.atleast_1d(ybind[0][0])\n                ybinh = np.atleast_1d(ybind[0][1])\n\n            else:\n                m = np.isfinite(ybind.T[1])\n                xbinh = ybind.T[0][m]\n                ybinh = ybind.T[1][m]\n\n            if xmean:\n                xcens[i] = np.mean(xbinh)\n                xhws[i] = np.std(xbinh)\n\n        else:\n\n            ybinh = ybind[np.isfinite(ybind)]\n\n        means[i] = np.mean(ybinh)\n        stds[i] = np.std(ybinh, ddof=1)\n        meds[i] = np.median(ybinh)\n        mads[i] = mad(ybinh)\n        ns[i] = len(ybinh)\n        qs = mquantiles(ybinh, prob=[sx, s1, 1 - s1, 1 - sx])\n\n        if len(ybinh) > 1:\n            # calculates the kernel density\n            try:\n                k = gaussian_kde(ybinh)\n            except:\n                print(\n                    ""Warning! Error in estimating kernel density for data in bin %s! Skipping bin...""\n                    % i\n                )\n                continue\n\n            # support of violins\n            if extend is None:\n                m = k.dataset.min()\n                M = k.dataset.max()\n                y = np.arange(m, M, (M - m) / 200.0)\n            else:\n                y = np.linspace(qs[0], qs[-1], extend * 100)\n\n            # scaling violins\n            v = k.evaluate(y)\n            vmax = v.max()\n            v = v / vmax * xhws[i] * 0.8\n\n            # violins\n            if ""v"" in draw and ""s"" not in draw:\n                plt.fill_betweenx(\n                    y,\n                    xcens[i] - v + offsetX,\n                    xcens[i] + offsetX,\n                    facecolor=color,\n                    edgecolor=""none"",\n                    lw=0.5,\n                    zorder=0,\n                    alpha=0.1,\n                )\n\n                # # hack to remove (overdraw) the inner white line that looks ugly\n                # plt.fill_betweenx(y, xcens[i], xcens[i] + v, facecolor=""None"",\n                #                   edgecolor=""white"", lw=2, zorder=0)\n\n            # median\n            if ""m"" in draw:\n\n                # mean uncertainty violin part\n                mask = (y > meds[i] - (meds[i] - qs[1]) / np.sqrt(ns[i])) & (\n                    y < meds[i] + (qs[2] - meds[i]) / np.sqrt(ns[i])\n                )\n                plt.fill_betweenx(\n                    y[mask],\n                    xcens[i] - v[mask] + offsetX,\n                    xcens[i] + offsetX,\n                    facecolor=color,\n                    alpha=0.5,\n                    edgecolor=""None"",\n                    zorder=3,\n                )\n\n                if ""v"" in draw:  # 1 sigma violin part\n                    a = 0.25\n                    if ""s"" in draw:\n                        a = 0.15\n                    mask = (y > qs[1]) & (y < qs[2])\n                    plt.fill_betweenx(\n                        y[mask],\n                        xcens[i] - v[mask] + offsetX,\n                        xcens[i] + offsetX,\n                        facecolor=color,\n                        edgecolor=""none"",\n                        lw=0.5,\n                        zorder=1,\n                        alpha=a,\n                    )\n\n                # # and to remove inner line again\n                # plt.fill_betweenx(y[mask], xcens[i], xcens[i] + v[mask], facecolor=""none"",\n                #                   edgecolor=""white"", lw=4, zorder=1)\n\n                wm = xhws[i] * 0.8 * k.evaluate(meds[i]) / vmax\n                plt.plot(\n                    (xcens[i] - wm + offsetX, xcens[i] + offsetX),\n                    (meds[i], meds[i]),\n                    ls=""-"",\n                    lw=1,\n                    color=color,\n                    zorder=3,\n                )\n\n            if outliers:\n\n                youts = ybinh[(ybinh < qs[0]) | (ybinh > qs[-1])]\n                xouts = np.ones(len(youts)) * xcens[i]\n\n                plt.plot(\n                    xouts + offsetX,\n                    youts,\n                    marker=""."",\n                    ls=""None"",\n                    ms=2,\n                    color=color,\n                    zorder=1,\n                )\n\n    # Mean profile\n    if ""a"" in draw:\n        zero_mask = ns > 0\n        merrbar = plt.errorbar(\n            xcens[zero_mask] + offsetX,\n            means[zero_mask],\n            stds[zero_mask] / np.sqrt(ns[zero_mask]),\n            marker=marker,\n            ls=""None"",\n            elinewidth=1,\n            mew=2,\n            mfc=""white"",\n            mec=color,\n            color=color,\n            capsize=0,\n            zorder=4,\n            **kwargs,\n        )\n        # matplotlib is fucking up the zorder for me if not explicitly told what to do\n        for el in merrbar[2]:\n            el.set_zorder(1)\n\n    if textpos is None:\n        textpos = y.min()\n\n    if ""c"" in draw:\n        for n, x in zip(ns, xcens + offsetX):\n            plt.annotate(\n                str(n.astype(int)),\n                xy=(x, textpos + offsetY),\n                xycoords=(""data"", ""data""),\n                rotation=90,\n                xytext=(0, 0),\n                textcoords=""offset points"",\n                va=""top"",\n                ha=""center"",\n                color=color,\n                size=9,\n            )\n\n    # to bring all the violins into visible x-range\n    plt.xlim(min(xcens - 2 * xhws), max(xcens + 2 * xhws))\n\n    return xcens, xhws, means, stds, meds, mads, ns\n\n\ndef plot_labeled_vspan(\n    x0, x1, label, y=0.5, axes=None, color=""k"", facecolor=None, fontsize=None, zorder=0\n):\n    """"""\n    Draw a vspan with a label in the center.\n\n    Parameters\n    ----------\n    x0: float\n        data coordinate where span starts.\n    x1: float\n        data coordinate where span ends.\n    label: str\n        Text label.\n    y (optional): float\n        Vertical axes coordinate around which to center label. Default: 0.5.\n    axes (optional): Axes instance\n        Axes instance to draw onto. Default: matplotlib.pyplot.gca().\n    color (optional): str or sequence\n        Color for the text and the span (if facecolor is not set).\n        Default: black.\n    facecolor (optional): str or sequence\n        Color for the span. Default: color lightened by 0.75.\n    fontsize (optional): str or float\n        Fontsize for the text.\n    zorder (optional): int\n        z-placement of span. Default: zorder=0.\n    """"""\n    if axes is None:\n        axes = plt.gca()\n    facecolor = lighten_color(color, 0.75) if facecolor is None else facecolor\n    span = axes.axvspan(x0, x1, facecolor=facecolor, zorder=zorder)\n    text = axes.text(\n        0.5 * (x0 + x1),\n        y,\n        label,\n        transform=axes.get_xaxis_transform(),\n        ha=""center"",\n        va=""center"",\n        fontsize=fontsize,\n        rotation=90,\n        zorder=zorder + 1,\n    )\n    return span, text\n'"
pyik/numpyext.py,164,"b'# -*- coding: utf-8 -*-\n""""""Contains extensions to numpy.""""""\nfrom six.moves import range\nimport numpy as np\n\n\ndef linear_least_squares_fit(model, npar, x, y, yerr=None):\n    """"""\n    Fits a model that is linear in the parameters.\n\n    Parameters\n    ----------\n    model: vectorized function, args=(x, par)\n    npar: number of parameters for model (length of par vector)\n    x, y, yerr: coordinates and errors of data points\n\n    Returns\n    -------\n    x: best-fit vector of parameters\n    cov: covariance matrix of parameters\n    chi2: chi2 at minimum\n    ndof: statistical degrees of freedom\n    """"""\n\n    if yerr is None:\n        b = np.atleast_1d(y)\n        X = np.transpose([model(x, u) for u in np.identity(npar)])\n    else:\n        ye = np.atleast_1d(yerr)\n        b = np.atleast_1d(y) / ye\n        X = np.transpose([model(x, u) / ye for u in np.identity(npar)])\n\n    XTX_inv = np.linalg.inv(np.dot(X.T, X))\n    x = np.dot(np.dot(XTX_inv, X.T), b)\n    chi2 = np.sum((b - np.dot(X, x))**2)\n    ndof = len(y) - npar\n    return x, XTX_inv, chi2, ndof\n\n\ndef rebin(factor, w, edges=None, axis=0):\n    """"""\n    Re-bins a N-dimensional histogram along a chosen axis.\n\n    Parameters\n    ----------\n    factor: integer\n      Number of neighboring bins to merge. Number of original\n      bins must be divisible by factor.\n    w: array-like\n      Number field that represents the histogram content.\n    edges: array-like (optional)\n      Bin edges of the axis to re-bin.\n    axis: integer (optional)\n      Axis to re-bin, defaults to first axis.\n\n    Returns\n    -------\n    w: array\n      Number field that represents the re-binned histogram\n      content.\n    edges: array (only if edges were supplied)\n      Bin edges after re-binning.\n    """"""\n\n    w = np.atleast_1d(w)\n    nbin = w.shape[axis]\n    if nbin % factor != 0:\n        raise ValueError(""factor %i is not a divider of %i bins"" % (factor, nbin))\n    n = nbin / factor\n    shape = np.array(w.shape)\n    shape[axis] = n\n    w2 = np.zeros(shape, dtype=w.dtype)\n    for i in range(factor):\n        mask = [slice(x) for x in shape]\n        mask[axis] = slice(i, nbin, factor)\n        w2 += w[mask]\n\n    if edges is not None:\n        edges2 = [edges[factor * i] for i in range(n)] + [edges[-1]]\n        return w2, edges2\n    else:\n        return w2\n\n\ndef bin(x, y, bins=10, range=None):\n    """"""\n    Bin x and returns lists of the y-values inside each bin.\n\n    Parameters\n    ----------\n    x: array-like\n      Variable that is binned.\n    y: array-like\n      Variable that is sorted according to the binning of x.\n    bins: integer or array-like\n      Number of bins or array of lower bin edges + last high bin edge.\n    range: tuple, lenght of 2 (optional)\n      If range is set, only (x,y) pairs are used where x is inside the range.\n      Ignored, if bins is an array.\n\n    Returns\n    -------\n    yBins: list of lists\n      List of y-values which correspond to the x-bins.\n    xegdes: array of floats\n      Lower bin edges. Has length len(yBins)+1.\n    """"""\n\n    ys = np.atleast_1d(y)\n    xs = np.atleast_1d(x)\n\n    if type(bins) is int:\n        if range is None:\n            range = (min(x), max(x) + np.finfo(float).eps)\n        else:\n            mask = (range[0] <= xs) & (xs < range[1])\n            xs = xs[mask]\n            ys = ys[mask]\n        xedges = np.linspace(range[0], range[1], bins + 1)\n    else:\n        xedges = bins\n        bins = len(xedges) - 1\n\n    binnedys = []\n    for i in range(bins):\n\n        if i == bins - 1:\n            binnedys.append(ys[(xedges[i] <= xs) & (xs <= xedges[i + 1])])\n        else:\n            binnedys.append(ys[(xedges[i] <= xs) & (xs < xedges[i + 1])])\n\n    return binnedys, xedges\n\n\ndef profile(x, y, bins=10, range=None, sigma_cut=None):\n    """"""\n    Compute the (robust) profile of a set of data points.\n\n    Parameters\n    ----------\n    x,y : array-like\n      Input data. The (x,y) pairs are binned according to the x-array,\n      while the averages are computed from the y-values inside a x-bin.\n    bins : int or array-like, optional\n      Defines the number of equal width bins in the given range (10,\n      by default). If bins is an array, it is used for the bin edges\n      of the profile.\n    range : (float,float), optional\n      The lower and upper range of the bins. If not provided, range\n      is simply ``(a.min(), a.max())``. Values outside the range are\n      ignored.\n    sigma_cut : float, optional\n      If sigma_cut is set, outliers in the data are rejected before\n      computing the profile. Outliers are detected based on the scaled\n      MAD and the median of the distribution of the y\'s in each bin.\n      All data points with |y - median| > sigma_cut x MAD are ignored\n      in the computation.\n\n    Returns\n    -------\n    yavg : array of dtype float\n      Returns the averages of the y-values in each bin.\n    ystd : array of dtype float\n      Returns the standard deviation in each bin. If you want the\n      uncertainty of ymean, calculate: yunc = ystd/numpy.sqrt(n-1).\n    n : array of dtype int\n      Returns the number of events in each bin.\n    xedge : array of dtype float\n      Returns the bin edges. Beware: it has length(yavg)+1.\n\n    Examples\n    --------\n    >>> yavg, ystd, n, xedge = profile(np.array([0.,1.,2.,3.]), np.array([0.,1.,2.,3.]), 2)\n    >>> yavg\n    array([0.5, 2.5])\n    >>> ystd\n    array([0.5, 0.5])\n    >>> n\n    array([2, 2])\n    >>> xedge\n    array([0. , 1.5, 3. ])\n    """"""\n\n    y = np.asfarray(np.atleast_1d(y))\n\n    n, xedge = np.histogram(x, bins=bins, range=range)\n\n    if sigma_cut is None:\n        ysum = np.histogram(x, bins=bins, range=range, weights=y)[0]\n        yysum = np.histogram(x, bins=bins, range=range, weights=y * y)[0]\n    else:\n        if sigma_cut <= 0:\n            raise ValueError(""sigma_cut <= 0 detected, has to be positive"")\n        # sort y into bins\n        ybin = bin(x, y, bins, range)[0]\n\n        if type(bins) is int:\n            nbins = bins\n        else:\n            nbins = len(bins) - 1\n\n        # reject outliers in calculation of avg, std\n        ysum = np.zeros(nbins)\n        yysum = np.zeros(nbins)\n        for i in range(nbins):\n            ymed = np.median(ybin[i])\n            ymad = mad(ybin[i])\n            for y in ybin[i]:\n                if ymad == 0 or abs(y - ymed) < sigma_cut * ymad:\n                    ysum[i] += y\n                    yysum[i] += y * y\n                else:\n                    n[i] -= 1\n\n    mask = n == 0\n    n[mask] = 1\n    yavg = ysum / n\n    ystd = np.sqrt(yysum / n - yavg * yavg)\n    yavg[mask] = np.nan\n    ystd[mask] = np.nan\n\n    return yavg, ystd, n, xedge\n\n\ndef profile2d(x, y, z, bins=(10, 10), range=None):\n    """"""\n    Compute the profile of a set of data points in 2d.\n    """"""\n\n    if not isinstance(z, np.ndarray):\n        z = np.array(z)\n\n    ws, xedges, yedges = np.histogram2d(x, y, bins, range)\n\n    zsums = np.histogram2d(x, y, bins, range, weights=z)[0]\n    zzsums = np.histogram2d(x, y, bins, range, weights=z * z)[0]\n\n    zavgs = zsums / ws\n    zstds = np.sqrt(zzsums / ws - zavgs * zavgs)\n\n    return zavgs, zstds, ws, xedges, yedges\n\n\ndef centers(x):\n    """"""\n    Compute the centers of an array of bin edges.\n\n    Parameters\n    ----------\n    x: array-like\n      A 1-d array containing lower bin edges.\n\n    Returns\n    -------\n    c: array of dtype float\n      Returns the centers of the bins.\n    hw: array of dtype float\n      Returns the half-width of the bins.\n\n    Examples\n    --------\n    >>> centers([0.0, 1.0, 2.0])\n    (array([0.5, 1.5]), array([0.5, 0.5]))\n    """"""\n\n    x = np.atleast_1d(x)\n    assert len(x) > 1, ""Array should have size > 1 to make call to centers() reasonable!""\n    hw = 0.5 * (x[1:] - x[:-1])\n    return x[:-1] + hw, hw\n\n\ndef derivative(f, x, step=None, order=1):\n    """"""\n    Numerically calculate the first or second derivative of a function.\n\n    Parameters\n    ----------\n    f: function-like\n      Function of which to calculate the derivative.\n      It has to accept a single float argument and may return a vector or a float.\n    x: float\n      Where to evaluate the derivative.\n    step: float (optional)\n      By default, the step size for the numerical derivative is calculated\n      automatically. This may take many more evaluations of f(x) than necessary.\n      The calculation can be speed up by setting the step size.\n    order: integer (optional)\n      Order of the derivative. May be 1 or 2 for the first or second derivative.\n\n    Returns\n    -------\n    The first or second derivative of f(x).\n\n    Notes\n    -----\n    Numerically calculated derivatives are not exact and we do not give an error\n    estimate.\n\n    Examples\n    --------\n    >>> def f(x) : return 2 + x + 2*x*x + x*x*x\n    >>> round(derivative(f, 1.0), 3)\n    8\n    >>> round(derivative(f, 1.0, step=1e-3), 3)\n    8\n    >>> round(derivative(f, 1.0, order=2), 3)\n    10\n    >>> np.round(derivative(f, np.ones(2)), 3)\n    array([8., 8.])\n    >>> np.round(derivative(f, np.ones(2), order=2), 3)\n    array([10., 10.])\n\n    Notes\n    -----\n    The first derivative is calculated with the five point stencil,\n    see e.g. Wikipedia. The code to determine the step size was taken\n    from the GNU scientific library.\n    """"""\n\n    eps = np.finfo(float).eps\n    # the correct power is 1/order of h in the\n    # error term of the numerical formula\n    h0 = h = eps ** 0.33 if order == 1 else eps ** 0.25\n\n    userStep = step is not None\n    for i in range(10):\n        dx = step if userStep else (h * x if np.all(x) else h)\n        tmp = x + dx\n        dx = tmp - x\n\n        fpp = f(x + 2.0 * dx)\n        fp = f(x + dx)\n        fm = f(x - dx)\n        fmm = f(x - 2.0 * dx)\n\n        if userStep:\n            break\n\n        if order == 1:\n            a = np.abs(fpp - fp)\n            b = np.abs(fpp + fp)\n            if np.all(a > 0.5 * b * h0):\n                break\n        else:\n            a = np.abs(fpp + fmm - fp - fm)\n            b = np.abs(fpp + fmm + fp + fm)\n            if np.all(a > 0.5 * b * h0):\n                break\n\n        h *= 10\n\n    if order == 1:\n        return (fmm - fpp + 8.0 * (fp - fm)) / (12.0 * dx)\n    else:\n        return (fpp + fmm - fp - fm) / (3.0 * dx * dx)\n\n\ndef derivativeND(f, xs, step=1e-8):\n    """"""\n    Numerically calculates the first derivatives of an R^n -> R function.\n\n    The derivatives can be calculated at several points at once.\n\n    Parameters\n    ----------\n    f : callable\n      An R^n -> R function to differentiate. Has to be callable with\n      f(xs), where xs is a 2-d array of shape n_points x n_variables.\n    xs : array-like\n      A 2-d array of function values of shape n_points x n_variables.\n    step : float\n      Step size for the differentiation.\n\n    Notes\n    -----\n    The derivatives are calculated using the central finite difference\n    method with 2nd order accuracy (i.e., a two point stencil) for each\n    dimension.\n\n    Returns\n    -------\n    A 2-d array of the derivatives for each point and dimension. The\n    shape is n_points x n_variables.\n\n    Examples\n    --------\n    >>> def f(xy):\n    ...     x, y = xy.T\n    ...     return x ** 2 + y ** 2\n    ...\n    >>> derivativeND(f, ([0., 0.], [1., 0.], [0., 1.]))\n    array([[0., 0.],\n           [2., 0.],\n           [0., 2.]])\n    """"""\n    xs = np.atleast_2d(xs)\n\n    n_rows, n_vars = xs.shape\n\n    bloated_xs = np.repeat(xs, n_vars, 0)\n    epsilons = np.tile(np.eye(n_vars) * step, [n_rows, 1])\n\n    return (f(bloated_xs + epsilons) -\n            f(bloated_xs - epsilons)).reshape(-1, n_vars) / (2 * step)\n\n\ndef jacobian(f, x, steps=None):\n    """"""\n    Numerically calculate the matrix of first derivatives.\n\n    Parameters\n    ----------\n    f: function-like\n      Has to be callable as f(x).\n    x: array-like\n      Vector of parameters.\n    steps: array-like (optional)\n      Vector of deltas to use in the numerical approximation,\n      see derivative(...). Has to have the same length as x.\n\n    Returns\n    -------\n    The Jacobi matrix of the first derivatives.\n\n    Examples\n    --------\n    >>> def f(v): return 0.5*np.dot(v,v)\n    >>> jacobian(f,np.ones(2))\n    array([[1., 1.]])\n    >>> def f(v): return np.dot(v,v)*v\n    >>> jacobian(f,np.ones(2))\n    array([[4., 2.],\n           [2., 4.]])\n    """"""\n\n    nx = len(x)\n\n    # cheap way to determine dimension of f\'s output\n    y = f(x)\n    ny = len(y) if hasattr(y, ""__len__"") else 1\n\n    jacobi = np.zeros((ny, nx))\n\n    e = np.zeros(nx)\n\n    for ix in range(nx):\n        e *= 0\n        e[ix] = 1\n\n        der = derivative(lambda z: f(x + z * e), 0,\n                         step=None if steps is None else steps[ix])\n\n        for iy in range(ny):\n            jacobi[iy, ix] = der[iy] if ny > 1 else der\n\n    return jacobi\n\n\ndef hessian(f, x, steps):\n    """"""\n    Numerically calculate the matrix of second derivatives.\n\n    Parameters\n    ----------\n    f: function-like\n      Has to be callable as f(x).\n    x: array-like\n      Vector of parameters.\n    steps: array-like\n      Vector of deltas to use in the numerical approximation.\n      Has to have the same length as x.\n\n    Returns\n    -------\n    The symmetric Hesse matrix of the second derivatives.\n    """"""\n\n    xx = np.array(x, dtype=np.float)\n\n    n = len(x)\n    hesse = np.empty((n, n))\n\n    for i in range(n):\n        for j in range(i, n):\n            xpp = xx.copy()\n            xpp[i] += steps[i]\n            xpp[j] += steps[j]\n\n            xmm = xx.copy()\n            xmm[i] -= steps[i]\n            xmm[j] -= steps[j]\n\n            if i == j:\n                xm = xx.copy()\n                xm[i] -= steps[i]\n\n                xp = xx.copy()\n                xp[i] += steps[i]\n\n                hesse[i, i] = ((f(xmm) + f(xpp) - f(xp) - f(xm))\n                               / (3.0 * steps[i] * steps[i]))\n\n            else:\n                xpm = xx.copy()\n                xpm[i] += steps[i]\n                xpm[j] -= steps[j]\n\n                xmp = xx.copy()\n                xmp[i] -= steps[i]\n                xmp[j] += steps[j]\n\n                hesse[i, j] = hesse[j, i] = (\n                    f(xpp) + f(xmm) - f(xpm) - f(xmp)) / (4.0 * steps[i] * steps[j])\n\n    return hesse\n\n\ndef propagate_covariance(f, x, cov):\n    """"""\n    Compute the covariance matrix of y for the transformation y = f(x), given x with covariance matrix cov.\n\n    Parameters\n    ----------\n    f: function-like\n      Has to be callable as f(x).\n    x: array-like\n      Vector of parameters.\n    cov: 2-d array of floats\n      Covariance matrix of x.\n\n    Returns\n    -------\n    fcov: matrix of floats\n      The covariance matrix of the output of f.\n\n    Examples\n    --------\n    >>> v = np.ones(2)\n    >>> cov = np.ones((2,2))\n    >>> def f(r):return np.dot(r,r)\n    >>> ""%.3g"" % propagate_covariance(f,v,cov)\n    \'16\'\n    >>> def f(r):return 2*r\n    >>> propagate_covariance(f,v,cov)\n    array([[4., 4.],\n           [4., 4.]])\n    """"""\n\n    ncol = len(x)\n    dx = np.empty(ncol)\n    for icol in range(ncol):\n        dx[icol] = (np.sqrt(cov[icol][icol]) if cov[icol][icol] > 0.0 else 1.0) * 1e-3\n\n    jacobi = jacobian(f, x, dx)\n\n    return np.dot(jacobi, np.dot(cov, jacobi.T))\n\n\ndef uncertainty(f, x, cov):\n    """"""\n    Compute the standard deviation of f(v), given v with covariance matrix cov.\n\n    This is a convenience function that wraps propagate_covariance(...).\n\n    Parameters\n    ----------\n    f: function-like\n      Has to be callable as f(x).\n    x: array-like or single float\n      Vector of parameters.\n    cov: 2-d array of floats or single float\n      Covariance matrix of x.\n\n    Returns\n    -------\n    The standard deviation of f(x).\n\n    Examples\n    --------\n    >>> def f(r):return np.dot(r,r)\n    >>> v = np.ones(2)\n    >>> cov = np.ones((2,2))\n    >>> ""%.3g"" % uncertainty(f,v,cov)\n    \'4\'\n    """"""\n\n    prop_cov = propagate_covariance(f, np.atleast_1d(x), np.atleast_2d(cov))\n    return np.sqrt(prop_cov[0, 0])\n\n\ndef quantiles(ds, qs, weights=None):\n    """"""\n    Compute the quantiles qs of 1-d ds with possible weights.\n\n    Parameters\n    ----------\n    ds : ds to calculate quantiles from\n      1-d array of numbers\n\n    qs : 1-d array of quantiles\n\n    weights : 1-d array of weights, optional (default: None)\n      Is expected to correspond point-to-point to values in ds\n\n    Returns\n    -------\n    quantiles of ds corresponding to qs\n      1-d array of equal length to qs\n    """"""\n\n    if weights is None:\n        from scipy.stats.mstats import mquantiles\n        return mquantiles(ds, qs)\n    else:\n        ds = np.atleast_1d(ds)\n        qs = np.atleast_1d(qs)\n        weights = np.atleast_1d(weights)\n\n        assert len(ds) == len(\n            weights), ""Data and weights arrays need to have equal length!""\n        assert np.all((qs >= 0) & (qs <= 1)\n                      ), ""Quantiles need to be within 0 and 1!""\n        assert np.all(weights > 0), ""Each weight must be > 0!""\n\n        m_sort = np.argsort(ds)\n        ds_sort = ds[m_sort]\n        ws_sort = weights[m_sort]\n\n        ps = (np.cumsum(ws_sort) - 0.5 * ws_sort) / np.sum(ws_sort)\n        return np.interp(qs, ps, ds_sort)\n\n\ndef median(a, weights=None, axis=0):\n    """"""\n    Compute the median of data in a with optional weights.\n\n    Parameters\n    ----------\n    a : data to calculate median from\n      n-d array of numbers\n\n    weights : weights of equal shape to a\n      n-d array of numbers\n\n    axis : axis to calculate median over (optional, default: 0)\n      To note, weighted calculation does currently only support 1-d arrays\n\n    Returns\n    -------\n    Median: float or 1-d array of floats\n    """"""\n\n    a = np.atleast_1d(a)\n    if weights is None:\n        return np.median(a, axis=axis)\n    else:\n        assert a.ndim == 1, ""Only 1-d calculation of weighted median is currently supported!""\n        return quantiles(a, 0.5, weights)[0]\n\n\ndef mad(a, weights=None, axis=0):\n    """"""\n    Calculate the scaled median absolute deviation of a random distribution.\n\n    Parameters\n    ----------\n    a : array-like\n      1-d or 2-d array of random numbers.\n\n    weights : array-like\n      Weights corresponding to data in a.\n      Calculation with weights is currently only supported for 1-d data.\n\n    Returns\n    -------\n    mad : float or 1-d array of floats\n      Scaled median absolute deviation of input sample. The scaling factor\n      is chosen such that the MAD estimates the standard deviation of a\n      normal distribution.\n\n    Notes\n    -----\n    The MAD is a robust estimate of the true standard deviation of a random\n    sample. It is robust in the sense that its output is not sensitive to\n    outliers.\n\n    The standard deviation is usually estimated by the square root of\n    the sample variance. Note, that just one value in the sample has to be\n    infinite for the sample variance to be also infinite. The MAD still\n    provides the desired answer in such a case.\n\n    In general, the sample variance is very sensitive to the tails of the\n    distribution and will give undesired results if the sample distribution\n    deviates even slightly from a true normal distribution. Many real world\n    distributions are not exactly normal, so this is a serious issue.\n    Fortunately, this is not the case for the MAD.\n\n    Of course there is a price to pay for these nice features. If the sample is\n    drawn from a normal distribution, the sample variance is the more\n    efficient estimate of the true width of the Gaussian, i.e. its\n    statistical uncertainty is smaller than that of the MAD.\n\n    Examples\n    --------\n    >>> a = [1.,0.,5.,4.,2.,3.,1e99]\n    >>> round(mad(a), 3)\n    2.965\n    """"""\n\n    const = 1.482602218505602  # 1.0/inverse_cdf(3/4) of normal distribution\n    med = median(a, weights=weights, axis=axis)\n\n    if axis == 0:\n        absdevs = np.absolute(a - med)\n    elif axis == 1:\n        absdevs = np.absolute(a.T - med).T\n\n    return const * median(absdevs, weights=weights, axis=axis)\n\n\nclass ConvexHull:\n    """"""\n    Calculate the (fractional) convex hull of a point cloud in 2-d.\n\n    Parameters\n    ----------\n    x: 1-d array\n      vector of parameters\n\n    y: 1-d array\n      vector of parameters\n\n    frac: int\n      fraction of points contained in convex hull, default is 1.0\n\n    byprob: boolean\n      if false and frac < 1.0, will remove points contained in hull shape\n      if true and frac < 1.0, will remove least probable point based on kde estimate\n\n    Returns\n    -------\n    points: 2-d array of floats\n      remaining points to analyses hull object\n\n    hull: object generated by scipy.spatial.qhull.ConvexHull\n      contains information of ConvexHull\n\n\n    Notes\n    -----\n    A convex hull can be thought of as a rubber band put around the point cloud.\n    To plot a closed object, use the simplices contained in ""hull"".\n\n    Examples\n    --------\n    >>> m1 = [-0.9, -0.1, -0.0, 0.7, 1.3, 0.4, 0.6, -1.9, 0.2, -1.1]\n    >>> m2 = [ 0.1, 0.7, -0.9, -0.1, -0.5, -0.7, -0.9, -0.2, -0.2, -0.5]\n    >>> hull = ConvexHull(m1, m2)\n    >>> points, hull = hull()\n\n    Plot the hull:\n    for simplex in hull.simplices:\n       plt.plot(points[simplex, 0], points[simplex, 1], \'k--\')\n    """"""\n\n    def __init__(self, x, y, frac=1.0, byprob=True):\n        from scipy.stats import gaussian_kde\n\n        self.x = np.atleast_1d(x)\n        self.y = np.atleast_1d(y)\n        self.frac = frac\n        self.remove = byprob\n\n        data = np.vstack([self.x, self.y])\n        self.kernel = gaussian_kde(data)\n\n    def __call__(self):\n        return self.fractionalHull()\n\n    def convexHull(self, pos):\n        from scipy.spatial import ConvexHull\n        return ConvexHull(pos)\n\n    def removal(self, pos, bound):\n        x = np.array([p[0] for p in pos])\n        y = np.array([p[1] for p in pos])\n        for b in range(len(bound)):\n            px = np.where(x == bound[b][0])\n            py = np.where(y == bound[b][1])\n            if px == py:\n                x = np.delete(x, px)\n                y = np.delete(y, px)\n        return x, y\n\n    def removeByProb(self, pos, bound):\n        boundary = np.vstack([bound[:, 0], bound[:, 1]])\n        prob = self.kernel(boundary)\n\n        index = prob.argsort()\n        prob = prob[index]\n        boundary = bound[index]\n\n        return self.removal(pos, [boundary[0]])\n\n    def removePoints(self, pos):\n        hull = self.convexHull(pos)\n        boundary = np.dstack((pos[hull.vertices, 0], pos[hull.vertices, 1]))[0]\n\n        if not self.remove:\n            x, y = self.removal(pos, boundary)\n        if self.remove:\n            x, y = self.removeByProb(pos, boundary)\n\n        points = np.dstack((x, y))[0]\n        hull = self.convexHull(points)\n        return points, hull\n\n    def fractionalHull(self):\n        points = np.dstack((self.x.copy(), self.y.copy()))[0]\n        n = self.frac * len(points)\n        if self.frac == 1:\n            hull = self.convexHull(points)\n        else:\n            while len(points) > n:\n                points, hull = self.removePoints(points)\n        # boundary = np.dstack((points[hull.vertices,0], points[hull.vertices,1]))[0]\n        return points, hull\n\n\ndef bootstrap(function, x, r=1000):\n    """"""\n    Generate r balanced bootstrap replicas of x and returns the results of a statistical function on them.\n\n    Notes\n    -----\n    The bootstrap is a non-parametric method to obtain the statistical bias\n    and variance of a statistical estimate. In general, the result is\n    approximate. You should only use this if you have no idea of the\n    theoretical form of the underlying p.d.f. from which the data are drawn.\n    Otherwise you should draw samples from that p.d.f., which may be fitted to\n    the data.\n\n    To obtain good results, r has to be in the range of 200 to 1000. As with\n    every simulation technique, the precision of the result is proportional to\n    r^(-1/2).\n\n    Parameters\n    ----------\n    function: callable\n      The statistical function. It has to accept an array of the type of x and may\n      return a float or another array.\n    x: array-like\n      The original input data for the statistical function.\n    r: int\n      Number of bootstrap replicas.\n\n    Returns\n    -------\n    Array of results of statFunc.\n    """"""\n\n    n = np.alen(x)\n    xx = np.array(x)\n    iB = np.array(np.random.permutation(n * r) % n)\n    xbGen = (xx[iB[ir * n:(ir + 1) * n]] for ir in range(r))\n    ybs = map(function, xbGen)\n    return np.array(ybs)\n\n\ndef bootstrap_confidence_interval(statfunc, x, coverage=0.68, replicas=1000):\n    """"""\n    Calculate the bootstrap confidence interval of the result of a statistical function.\n\n    Notes\n    -----\n    See remarks of BootstrapReplicas.\n\n    Parameters\n    ----------\n    statfunc: callable\n      The statistical function. It has to accept an array of the type of x and may\n      return a float or another array.\n    x: array-like\n      The original input data for the statistical function.\n    coverage: float\n      Fraction of bootstrap replicas inside the interval.\n    replicas: integer\n      Number of bootstrap replicas (defines accuracy of interval)\n\n    Returns\n    -------\n    v,dv-,dv+ : floats or arrays of floats\n      statfunc(x), downward uncertainty interval, upward uncertainty interval\n    """"""\n\n    if len(x) == 0:\n        return 0, 0, 0\n\n    r = int(round(replicas / 200.0)) * 200  # has to be multiple of 200\n    q = int(round(r * coverage))\n    qA = (r - q) / 2\n    qB = r - qA\n\n    t = statfunc(x)\n    tB = np.sort(bootstrap(statfunc, x, r), axis=0)\n\n    return t, t - tB[qA], tB[qB] - t\n\n\ndef bootstrap_covariance(statfunc, x, r=1000):\n    """"""\n    Calculate the uncertainty of statfunc over data set x with a balanced bootstrap.\n\n    Notes\n    -----\n    See remarks of BootstrapReplicas.\n\n    Parameters\n    ----------\n    statfunc: callable\n      The statistical function. It has to be callable as statfunc(x)\n      and may return a float or another array.\n    x: array-like\n      The original input data for the statistical function.\n\n    Returns\n    -------\n    The covariance matrix of the result of statfunc.\n    """"""\n\n    return np.cov(bootstrap(statfunc, x, r))\n\n\ndef binomial_proportion(nsel, ntot, coverage=0.68):\n    """"""\n    Calculate a binomial proportion (e.g. efficiency of a selection) and its confidence interval.\n\n    Parameters\n    ----------\n    nsel: array-like\n      Number of selected events.\n    ntot: array-like\n      Total number of events.\n    coverage: float (optional)\n      Requested fractional coverage of interval (default: 0.68).\n\n    Returns\n    -------\n    p: array of dtype float\n      Binomial fraction.\n    dpl: array of dtype float\n      Lower uncertainty delta (p - pLow).\n    dpu: array of dtype float\n      Upper uncertainty delta (pUp - p).\n\n    Examples\n    --------\n    >>> p, dpl, dpu = binomial_proportion(50,100,0.68)\n    >>> round(p, 3)\n    0.5\n    >>> round(dpl, 3)\n    0.049\n    >>> round(dpu, 3)\n    0.049\n    >>> abs(np.sqrt(0.5*(1.0-0.5)/100.0)-0.5*(dpl+dpu)) < 1e-3\n    True\n\n    Notes\n    -----\n    The confidence interval is approximate and uses the score method\n    of Wilson. It is based on the log-likelihood profile and can\n    undercover the true interval, but the coverage is on average\n    closer to the nominal coverage than the exact Clopper-Pearson\n    interval. It is impossible to achieve perfect nominal coverage\n    as a consequence of the discreteness of the data.\n    """"""\n\n    from scipy.stats import norm\n\n    z = norm().ppf(0.5 + 0.5 * coverage)\n    z2 = z * z\n    p = np.asarray(nsel, dtype=np.float) / ntot\n    div = 1.0 + z2 / ntot\n    pm = (p + z2 / (2 * ntot))\n    dp = z * np.sqrt(p * (1.0 - p) / ntot + z2 / (4 * ntot * ntot))\n    pl = (pm - dp) / div\n    pu = (pm + dp) / div\n\n    return p, p - pl, pu - p\n\n\ndef poisson_uncertainty(x):\n    """"""\n    Return ""exact"" confidence intervals, assuming a Poisson distribution for k.\n\n    Notes\n    -----\n    Exact confidence intervals from the Neyman construction tend to overcover\n    discrete distributions like the Poisson and Binomial distributions. This\n    is due to the discreteness of the observable and cannot be avoided.\n\n    Parameters\n    ----------\n    x: array-like or single integer\n      Observed number of events.\n\n    Returns\n    -------\n    A tuple containing the uncertainty deltas or an array of such tuples.\n    Order: (low, up).\n    """"""\n\n    from scipy.stats import chi2\n\n    x = np.atleast_1d(x)\n    r = np.empty((2, len(x)))\n    r[0] = x - chi2.ppf(0.16, 2 * x) / 2\n    r[1] = chi2.ppf(0.84, 2 * (x + 1)) / 2 - x\n    return r\n\n\ndef azip(*args):\n    """"""Convenience wrapper for numpy.column_stack.""""""\n    return np.column_stack(args)\n\n\ndef IsInUncertaintyEllipse(point, center, covariance, alpha=0.68):\n    """"""Test whether a point is inside the hypervolume defined by a covariance matrix.\n\n    Parameters\n    ----------\n    point: array of floats\n      Point to test.\n    center: array of floats\n      Center of the hypervolume.\n    covariance: 2d array of floats\n      Covariance matrix that defines the confidence interval.\n    alpha: float (optional)\n      Requested coverage of the hypervolume.\n\n    Returns\n    -------\n    True if point is covered and False otherwise.\n    """"""\n\n    from scipy.stats import chi2\n    w, u = np.linalg.eig(covariance)\n    x = np.dot(u.T, point - center) / np.sqrt(w)\n    return np.sum(x * x) <= chi2(len(center)).ppf(alpha)\n\n\ndef LOOCV(function, xs, ys, estimates, xerrs=None, yerrs=None):\n    """"""\n    Performs a Leave-one-out cross-validation of the prediction power of function assuming normally distributed values.\n\n    Parameters\n    ----------\n    function: callable function f(xs,pars)\n        Function which is evaluated at xs to predict ys.\n        Fit parameters are expected as a second argument.\n    xs: array-like\n        Function variable\n    ys: array-like\n        ys = function(xs,pars)\n    estimates: array-like\n        Estimates of the optimized parameters of function(xs,pars).\n        At least provide np.ones(n) or np.zeros(n) according to the number of parameters.\n\n    Returns\n    -------\n    LOOCV: The LOOCV value being proportional to bias^2 + variance.\n        The prediction power of function is proportional to -LOOCV.\n    """"""\n\n    xs = np.atleast_1d(xs)\n    ys = np.atleast_1d(ys)\n\n    from pyik.fit import ChiSquareFunction\n\n    # wrapper to np.delete if arr might be None\n    def DelIfNotNone(arr, i):\n        return None if arr is None else np.delete(arr, i)\n\n    loocv = 0.\n    for i in range(len(xs)):\n\n        # fitting function to all points except for the (i+1)th\n        pars_i = ChiSquareFunction(function, np.delete(xs, i), np.delete(ys, i),\n                                   xerrs=DelIfNotNone(xerrs, i),\n                                   yerrs=DelIfNotNone(yerrs, i)).Minimize(starts=estimates)[0]\n\n        # estimating residual at left-out point\n        loocv += (ys[i] - function(xs[i], pars_i)) ** 2\n\n    return loocv\n\n\nclass FeldmanCousins(object):\n    """"""\n    A convenience class to calculate confidence intervals using a unified frequentistic approach developed by Feldman & Cousins.\n\n    In particular, the method yields valid results when there are (physical) constraints on the parameters of the assumed pdf.\n    Application example: Estimation of upper limits for empty bins of an energy spectrum measurement.\n\n    Notes\n    -----\n    The method is described in detail in arXiv:physics/9711021v2.\n    The confidence intervals are created with the Neyman construction using a ranking according to likelihood-ratios.\n    Undercoverage resulting from decision-biased choices of the confidence interval after looking at data,\n    known as flip-flopping, or empty confidence intervals in forbidden parameter regions are avoided.\n\n    The standard constructor declares a Poisson distribution. To manually change the distribution, use SetCustomPdf().\n    SetNormalPdf() and SetLogNormalPdf() can be used to use the respective pdfs.\n    Note that the parameter and variable ranges need to be carefully adjusted in these cases.\n\n    In the case of discrete distributions e.g. Poisson, the confidence intervals will\n    overcover by construction due to the discreteness of the random variable.\n\n    Parameters\n    ----------\n    cl: float between 0 and 1\n        Desired coverage of the constructed confidence intervals.\n\n    Optional parameters\n    -------------------\n    nbg: float\n        Mean expectation of background events for the Poisson distribution.\n    murange: array-like\n        Lower and upper limits of the parameter range.\n        In any case, the upper parameter limit\n        needs to be well above the observation x for which the interval is evaluated.\n    xvrange:\n        Lower und upper limits of the variable range.\n    mustep:\n        Step size in the constructed (true) parameter space.\n        Smaller values will result in more accurate results but also in a significant increase in computing time.\n\n    Example\n    -------\n    >>> fc=FeldmanCousins(0.95)\n    >>> np.round(fc.FCLimits(0.), 3)\n    array([0.   , 3.095])\n    >>> np.round(fc.FCLimits(1.), 3)\n    array([0.055, 5.145])\n    """"""\n\n    def __init__(self, cl, nbg=0., murange=None, xvrange=None, mustep=0.005):\n        from scipy.stats import poisson\n        from pyik.fit import Minimizer\n\n        self._m = Minimizer()\n        self._cl = cl\n        # strictly, the poisson pmf doesn\'t exist for mu+bg=0, but the confidence\n        # interval still exists in the limit bg -> 0.\n        self._bg = max(nbg, 1e-10)\n        self._pdf = lambda k, mu: poisson.pmf(k, mu + self._bg)\n        self._murange = [0., 100.]\n        self._xrange = self._murange\n        self._mustep = mustep\n        self._discrete_pdf = True\n        self._pdftypes = [""poisson"", ""normal"", ""lognormal"", ""custom""]\n        self._pdftype = self._pdftypes[0]\n        if murange is not None:\n            self.SetParameterBounds(murange)\n        if xvrange is not None:\n            self.SetVariableBounds(xvrange)\n\n    def SetParameterBounds(self, bounds):\n        """"""Define the parameter limits.""""""\n        self._murange = bounds\n\n    def SetVariableBounds(self, bounds):\n        """"""Define the variable limits.""""""\n        self._xrange = bounds\n\n    def SetCustomPDF(self, pdf, murange, xvrange, discrete=False):\n        """"""\n        Declare a custom probability distribution function.\n\n        Parameters\n        ----------\n        pdf: function-like\n            The custom pdf. Is supposed to accept variable (observable) as first argument\n            and parameter as second argument.\n        murange: array-like\n            The parameter range.\n        xvrange: array-like\n            The observable range.\n        discrete (optional): boolean\n            Declare whether discrete or continuous variables are expected.\n        """"""\n        self._pdf = pdf\n        self.SetParameterBounds(murange)\n        self.SetVariableBounds(xvrange)\n        self._discrete_pdf = discrete\n        self._pdftype = self._pdftypes[-1]\n\n    def SetNormalPdf(self, sigma, murange=[0, 100]):\n        """"""Prepare a normal pdf with s.d. sigma and (constrained) parameter range murange.""""""\n        from scipy.stats import norm\n\n        self._pdf = lambda x, mu: norm.pdf(x, loc=mu, scale=sigma)\n\n        self.SetParameterBounds(murange)\n        self.SetVariableBounds([-murange[-1], murange[-1]])\n        self._discrete_pdf = False\n        self._pdftype = self._pdftypes[1]\n\n    def SetLogNormalPdf(self, sigma, murange=[0, 100]):\n        """"""Prepare a log-normal pdf with parameter (not s.d.) sigma and (constrained) parameter range murange.""""""\n        # the scipy.stats implementation of the log-normal pdf differs from the\n        # common mathematical def., therefore the pdf will be defined by hand\n        # here.\n        from scipy.stats import norm\n\n        self._pdf = lambda x, mu: norm.pdf(np.log(x), loc=mu, scale=sigma) / x\n\n        self.SetParameterBounds(murange)\n        self.SetVariableBounds(murange)\n        self._discrete_pdf = False\n        self._pdftype = self._pdftypes[2]\n\n    def FCLimits(self, x):\n        """"""\n        The actual function to calculate the confidence intervals.\n\n        Parameters\n        ----------\n        x: scalar or array-like\n            An observed value or an array of observed values.\n\n        Returns\n        -------\n        The lower and upper confidence limits as tuple or array with shape (len(x),2)\n        depending on the input shape.\n        """"""\n        x = np.atleast_1d(x)\n\n        if len(x) > 1:\n            return np.asfarray(np.vectorize(self.FCLimitsScalarX)(x)).T\n        else:\n            return self.FCLimitsScalarX(x[0])\n\n    def FCLimitsScalarX(self, x):\n        mucont = np.linspace(\n            self._murange[0], self._murange[-1], (self._murange[-1] - self._murange[0]) / self._mustep)\n        mulow, muup = self._murange\n\n        found = 0\n        for mu in mucont:\n            xlow, xup = self.GetVariableLimits(mu)\n            if found == 0 and (xlow <= x <= xup):\n                mulow = mu\n                found |= 1\n                continue\n            if found == 1 and not (xlow <= x <= xup):\n                muup = mu\n                break\n\n        return mulow, muup\n\n    def EstimateOptimumParameter(self, x):\n        """"""\n        Maximum-likelihood estimation of the optimum parameter for fixed observation x.\n\n        Used in the limit calculation when the analytic form is unknown.\n        Internal function.\n        """"""\n\n        def minfct(pars):\n            return -self._pdf(x, pars[0])\n\n        self._m.SetLowerBounds([self._mumin - self._bg])\n        self._m.SetUpperBounds([self._mumax])\n        self._m.SetMethod(""BOBYQA"")\n        return self._m(lambda p, g: minfct(p), np.asfarray([x]))[0]\n\n    def GetVariableLimits(self, mu):\n        """"""\n        Calculate the confidence intervals on the variable x assuming a fixed parameter mu.\n\n        Internal function.\n        """"""\n        if self._discrete_pdf:\n            xcont = np.linspace(int(self._xrange[0]), int(\n                self._xrange[-1]), int(self._xrange[-1] - self._xrange[0]) + 1)\n        else:\n            xcont = np.linspace(\n                self._xrange[0], self._xrange[-1], (self._xrange[-1] - self._xrange[0]) / self._mustep)\n\n        dx = xcont[1] - xcont[0]\n\n        if self._pdftype == ""poisson"":\n            mubest = self.Boundarize(xcont - self._bg)\n        elif self._pdftype == ""normal"":\n            mubest = self.Boundarize(xcont)\n        elif self._pdftype == ""lognormal"":\n            mubest = self.Boundarize(np.log(xcont))\n        else:\n            mubest = self.Boundarize(np.vectorize(\n                self.EstimateOptimumParameter)(xcont))\n\n        ps = self.Finitize(self._pdf(xcont, mu))\n        psbest = self.Finitize(self._pdf(xcont, mubest))\n\n        LR = self.Finitize(ps / psbest)\n\n        # sorting in order of decreasing probability\n        LRorder = np.argsort(LR)[::-1]\n\n        xsort, psort = xcont[LRorder], ps[LRorder]\n\n        psum = np.cumsum(psort * dx)\n\n        cli = np.where(psum >= self._cl)[0]\n        cli = cli[0] + 1 if len(cli) != 0. else len(psum)\n\n        cxsort = np.sort(xsort[:cli])\n\n        return cxsort[0], cxsort[-1]\n\n    def Finitize(self, arr):\n        arr[np.isfinite(arr) == False] = 0.0\n        return arr\n\n    def Boundarize(self, arr):\n        arr[arr < self._murange[0]] = self._murange[0]\n        arr[arr > self._murange[-1]] = self._murange[-1]\n        return arr\n\n\ndef qprint(x, s, latex=False):\n    """"""Pretty print numbers with uncertainties.\n\n    Examples\n    --------\n\n    >>> qprint(12.3333,2.3333)\n    \'12.3 +/- 2.3\'\n    >>> qprint(12.3333,0.2333)\n    \'12.33 +/- 0.23\'\n    >>> qprint(12.3333,0.02333)\n    \'12.333 +/- 0.023\'\n    >>> qprint(123.3333,23.333)\n    \'123 +/- 23\'\n    >>> qprint(1233.3333,23.333)\n    \'(1.233 +/- 0.023) x 10^3\'\n    """"""\n\n    x, s = float(x), float(s)\n\n    nx = int(np.floor(np.log10(np.abs(x))))\n    ns = int(np.floor(np.log10(s)))\n\n    sExp = None\n    if np.abs(nx) >= 3:\n        x /= 10 ** nx\n        s /= 10 ** nx\n        sExp = ""(%i)"" % nx if nx < 0 else ""%i"" % nx\n        ns -= nx\n    n = max(0, -ns + 1)\n    if latex:\n        pm = r""\\pm""\n    else:\n        pm = ""+/-""\n    if sExp:\n        return (""(%%.%if %%s %%.%if) x 10^%%s"" % (n, n)) % (x, pm, s, sExp)\n    else:\n        return (""%%.%if %%s %%.%if"" % (n, n)) % (x, pm, s)\n\n\ndef multivariate_gaussian(x, mu, cov):\n    """"""\n    Multivariate gaussian pdf with expectation vector mu and covariance matrix sigma.\n\n    Parameters\n    ----------\n    x: array of n-floats\n      Point where to estimate the pdf\n    mu: array of n-floats\n      Expectation vector of the pdf\n    cov: 2d array of n x n -floats\n      Covariance matrix\n\n    Returns\n    -------\n    Float value corresponding to the probability density at x\n\n    Example\n    --------\n    >>> mu,cov = np.asfarray([1.,1.]),np.asfarray([[0.5, 0.],[0., 0.3]])\n    >>> x = np.asfarray([2.,2.])\n    >>> from scipy.stats import norm\n    >>> ""%.6f"" % (norm.pdf(x[0],mu[0],cov[0][0]**0.5)*norm.pdf(x[1],mu[1],cov[1][1]**0.5))\n    \'0.028553\'\n    >>> ""%.6f"" % multivariate_gaussian(x,mu,cov)\n    \'0.028553\'\n    """"""\n\n    n = len(x)\n    if len(mu) != n or cov.shape != (n, n):\n        raise AssertionError(""Error! Input dimensions are not matching!"")\n\n    det = np.linalg.det(cov)\n    if det == 0:\n        raise ValueError(""Error! Covariance matrix is singular!"")\n\n    norm = ((2 * np.pi) ** n * np.absolute(det)) ** 0.5\n    d = x - mu\n    return np.exp(-0.5 * np.dot(np.dot(d.T, np.linalg.inv(cov)), d)) / norm\n\n\nclass multivariate_gaussian_evaluator(object):\n    """"""\n    Convenience class to utilize the multivariate_gaussian function.\n\n    It will return the probabilities of select sample in relation to a greater distribution.\n\n    If coverage is specified, it will return the mean, length of the axes of the hyperellipsoid, directional\n    vector for orientation of the hyperellipsoid, and boolean array saying if points in (True) or\n    outside of hyperellipsoid.\n\n    Parameters\n    ----------\n    data: m-parameter by n-d array\n      vector of parameters from which the multivariate gaussian will be made\n\n    points: a-parameter by b-d array\n      vector of parameters from which the probability of the multivariate gaussian will be calculated, default is data\n\n    coverage: float, default None\n      requested coverage of the hypervolume, associated with m-d multivariate gaussian\n\n    quantiles: boolean\n      if True, uses medians for mean vector; this should be more stable wrt outliers.\n      if False, uses means for mean vector\n\n    Returns\n    -------\n    default:\n      array of probabilities for specified points\n\n    if coverage specified:\n      mean vector, length vector, directional vector, isin\n\n    Notes\n    -----\n    Equation for coverage and explanation of MVN can be found at:\n    http://jonathantemplin.com/files/multivariate/mv11icpsr/mv11icpsr_lecture04.pdf\n\n    Examples\n    --------\n    Using coverage\n    >>> m1 = [-0.9, -0.1, -0.0, 0.7, 1.3, 0.4, 0.6, -1.9, 0.2, -1.1]\n    >>> m2 = [ 0.1, 0.7, -0.9, -0.1, -0.5, -0.7, -0.9, -0.2, -0.2, -0.5]\n    >>> mvn = multivariate_gaussian_evaluator([m1, m2], coverage=[0.682])\n    >>> mean, length, direct, isin = mvn()\n\n    Draw the ellipse\n    >>> from matplotlib.patches import Ellipse\n    >>> ell2 = Ellipse(xy=(mean[0], mean[1]), width=length[0]*2, height=length[1]*2, angle=np.degrees(np.arctan2(*direct[:,0][::-1])))\n    \n    You need to manually add the Ellipse to axes \'ax\': ax.add_artist(ell2)\n\n    Without coverage\n    >>> m1 = [-0.9, -0.1, -0.0, 0.7, 1.3, 0.4, 0.6, -1.9, 0.2, -1.1]\n    >>> m2 = [ 0.1, 0.7, -0.9, -0.1, -0.5, -0.7, -0.9, -0.2, -0.2, -0.5]\n    >>> xmin = np.min(m1)\n    >>> xmax = np.max(m1)\n    >>> ymin = np.min(m2)\n    >>> ymax = np.max(m2)\n    >>> X, Y = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n    >>> positions = np.vstack([X.ravel(), Y.ravel()])\n\n    Draw as color mesh\n    >> mvn = multivariate_gaussian_evaluator([m1, m2], points = positions)\n    >> val = mvn()\n    >> Z = np.reshape(val.T, X.shape)\n    >> plt.imshow(np.rot(Z,2), cmap=plt.cm.gist_earth_r, extent=[xmin, xmax, ymin, ymax])\n    >> plt.plot(self.x, self.y, \'k.\', markersize=2)\n    >> plt.xlim([xmin, xmax])\n    >> plt.ylim([ymin, ymax])\n    """"""\n\n    def __init__(self, data, points=None, wts=None, coverage=None, quantiles=True):\n        self.data = np.array(data)\n\n        if quantiles:\n            if wts is None:\n                self.mean = np.median(self.data, axis=1)\n            else:\n                assert self.data.ndim == 2, ""Only 2-d calculation of weighted median is currently supported!""\n                m1 = median(self.data[0], weights=wts)\n                m2 = median(self.data[1], weights=wts)\n                self.mean = np.array([m1, m2])\n\n        else:\n            self.mean = np.average(self.data, axis=1, weights=wts)\n        self.cov = np.cov(self.data, aweights=wts)\n\n        if points is None:\n            self.points = np.array(data)\n        else:\n            self.points = np.array(points)\n\n        self.coverage = coverage\n\n    def __call__(self):\n        if self.coverage is not None:\n            length, direct = self.coverage_axes()\n            isin = self.is_in_coverage()\n            return self.mean, length, direct, isin\n        else:\n            return self.multivariate_gauss_prob()\n\n    def multivariate_gauss_prob(self):\n        points = self.points.copy()\n        prob = [0] * len(points[0])\n        pos = points.T\n        for i in range(len(pos)):\n            prob[i] = multivariate_gaussian(pos[i], self.mean, self.cov)\n        return np.array(prob)\n\n    def coverage_axes(self):\n        from scipy.stats import chi2\n        w, u = np.linalg.eig(self.cov.copy())\n        mult = chi2(len(self.data)).ppf(self.coverage)\n        return np.sqrt(mult * w), u\n\n    # identical to IsInUncertaintyEllipse, defined here to reduce computational time\n    def is_in_coverage(self):\n        from scipy.stats import chi2\n        points = self.points.copy()\n        points = points.T\n        w, u = np.linalg.eig(self.cov.copy())\n\n        isin = [1] * len(points)\n        for p in range(len(points)):\n            x = np.dot(u.T, points[p] - self.mean.copy()) / np.sqrt(w)\n            isin[p] = int(np.sum(x * x) <=\n                          chi2(len(self.mean.copy())).ppf(self.coverage))\n        return np.array(isin)\n\n\ndef LikelihoodRatioSignificance(LLnull, LLalt, ndof=1):\n    """"""\n    Test the significance given two fit results of different models to the SAME data assuming an approximate chi2 statistic.\n\n    Parameters\n    ----------\n    LLnull: int\n      value of log likelihood of null hypothesis data\n    LLalt: int\n      value of log likelihood of alt hypothesis data\n\n    Returns\n    -------\n    int, p-value of probability of null hypothesis being true compared to alt hypothesis\n      calculating 1 - cdf(d, 1), this is the p-value connected with correctness of the null hypothesis\n      a small value of sf means that the probability of the null hypothesis being true compared to the alternative is small\n      however, the p-value is not the probability itself!\n      neither the one of null being false, nor the one of alt being true!\n\n    Example\n    --------\n    >>> from scipy.stats import gaussian_kde\n    >>> import numpy as np\n    >>> from pyik.numpyext import multivariate_gaussian, LikelihoodRatioSignificance\n    >>> m1 = [-0.9, -0.1, -0.0, 0.7, 1.3, 0.4, 0.6, -1.9, 0.2, -1.1]\n    >>> m2 = [ 0.1, 0.7, -0.9, -0.1, -0.5, -0.7, -0.9, -0.2, -0.2, -0.5]\n    >>> data = np.array([m1, m2])\n    >>> kernel = gaussian_kde(data)\n    >>> kde_values = kernel(data)\n    >>> LLalt = np.sum(np.log(kde_values))\n    >>>\n    >>> cov = np.cov(data)\n    >>> mu = np.mean(data, axis=1)\n    >>> gauss = [0]*len(data[0])\n    >>> data = data.T\n    >>> for row in range(len(data)): gauss[row] = multivariate_gaussian(data[row], mu, cov)\n    >>> LLnull = np.sum(np.log(gauss))\n    >>> round(LikelihoodRatioSignificance(LLnull,LLalt), 4)\n    0.15\n\n    Authors\n    -------\n    Alexander Schulz\n    """"""\n    from scipy.stats import chi2\n\n    d = 2 * LLalt - 2 * LLnull  # the log-likelihood ratio\n    # this is approximately distributed according to Chi2 dist with degrees of freedom 1\n    if d < 0:\n        raise AssertionError(\n            ""The log-likelihood ratio is negative, you are probably doing it wrong :D!"")\n\n    sf = chi2.sf(d, ndof)\n    return sf\n'"
pyik/performance.py,4,"b'# -*- coding: utf-8 -*-\nimport numpy as np\nimport warnings\nimport multiprocessing as mp\n\n\ndef pmap(function, *arguments, **kwargs):\n    """"""\n    Parallelized version of map. It calls function on arguments using numprocesses threads.\n\n    Also try the numexpr package or numba, which are easy to use and may give you better\n    performance.\n\n    Parameters\n    ----------\n    function : function\n      The function to call.\n    arguments : sequence(s)\n      One or more sequences, which are passed to the function. If N sequences are\n      provided, function is called with N arguments.\n    numprocesses : int, optional (default = None)\n      The number of processes to keep busy. If None, the number of CPUs is used.\n    nchunks: int, optional (default = None)\n      The number of chunks in which the arguments are split.\n      By default this is set to the number of processes, but sometimes you want\n      fewer chunks, so that they do not become too short.\n\n    Returns\n    -------\n    results : list or ndarray\n      The values returned by the function calls, as would be done by\n      map(function, *arguments). If async is True, the order of the results is\n      arbitrary. If the first of arguments is an ndarray, the result is converted to an\n      ndarray.\n\n    Notes\n    -----\n    Uses multiprocessing queues to create parallel version of map.\n    Note that, for pmap to be useful, the function itself (applied on one input) should be time consuming and hard to optimize.\n    Pmap is not beneficial in case of a cheap function and huge number of inputs.\n    In this case it is much better to run the function in a lower level, e.g. via numpy vectorization.\n\n    Limitations\n    -----------\n    The function needs to be a pure function, which means that\n    it may not manipulate a shared global state during its calls.\n    For example, you cannot use a function that computes a sum over the arguments.\n\n    Examples\n    --------\n    >>> def f(x): return x*x\n    >>> pmap(f, (1, 2, 3))\n    [1, 4, 9]\n\n    See Also\n    --------\n    multiprocessing\n    """"""\n\n    assert len(arguments) > 0, ""at least one iterable argument is required""\n    assert all(np.iterable(args) for args in arguments), ""arguments must be iterable""\n    length = len(arguments[0])\n    assert all(length == len(args) for args in arguments[1:]), ""all arguments must have same length""\n\n    numprocesses = kwargs.get(""numprocesses"", mp.cpu_count())\n    nchunks = kwargs.get(""nchunks"", numprocesses)\n\n    chunksize = kwargs.get(""chunksize"", None)\n    if chunksize is not None:\n        warnings.warn(""chunksize keyword is deprecated, please use nchunks keyword"",\n                      DeprecationWarning)\n        nchunks = chunksize\n\n    if not kwargs.get(""async"", True):\n        warnings.warn(""async == False is deprecated, pmap results are always in ""\n                      ""correct order now"",\n                      DeprecationWarning)\n\n    nchunks = min(numprocesses, length, nchunks)\n    argchunks = [np.array_split(args, nchunks) for args in arguments]\n\n    def worker(f, conn):\n        args = conn.recv()\n        result = list(map(f, *args))\n        conn.send(result)\n\n    pipes = [mp.Pipe() for _ in range(nchunks)]\n    procs = [mp.Process(target=worker, args=(function, p[1])) for p in pipes]\n\n    for p in procs:\n        p.daemon = True\n        p.start()\n\n    for i, p in enumerate(pipes):\n        args = [args[i] for args in argchunks]\n        p[0].send(args)\n\n    results = [p[0].recv() for p in pipes]\n\n    for p in procs:\n        p.join()\n\n    first_arg = arguments[0]\n    if isinstance(first_arg, np.ndarray):\n        return np.concatenate([x for x in results], axis=0)\n    res = []\n    for r in results:\n        res += r\n    return res\n\n\ndef cached(keepOpen=False, lockCacheFile=False, trackCode=True):\n    """"""\n    Decorator which caches the result of a slow function in an automatically\n    generated file in the current working directory.\n\n\n    Parameters\n    ----------\n    keepOpen : bool, optional (default = False)\n      Determines whether to keep the cache file open during the whole session.\n    lockCacheFile : bool, optional (default = False)\n      Locks the cache file during access (necessary when used with pmap)\n      don\'t use this option together with keepOpen, otherwise you will get a dead lock\n    trackCode: bool, optional (default = True)\n      If true, also changes in the code will be tracked.\n\n    Returns\n    -------\n    f : decorated function\n\n    Notes\n    -----\n    This function decorators enhances a function by wrapping code around it\n    that implements the caching of the result. See the examples how to use it.\n    The decorator is able to cache the results of several individual functions\n    in a single script, each called with several individual arguments.\n\n    The cache is useful, for example, if you are a slow routine that generates\n    some data which you want to plot. Typically, the plot part needs several\n    development cycles to look good. While you develop the display, the slow\n    parts of your analysis are quickly supplied by the cache for you.\n\n    The cache file is always created in the current working directory. The\n    filename is cache-<script name>-<function name>.pkl. If the decorator\n    is used within an interactive session, the filename will be python.cache.\n    If you want to place and name the file yourself, use the ""cached_at"" decorator.\n\n    The decorator detects edits in the cached function via a hash of its byte\n    code. If you want to make sure that the cache is updated, please delete\n    the cache file manually.\n\n    Limitations\n    -----------\n    The function is only are allowed to accept and return objects\n    that can be handled by the pickle module. Fortunately, that is practically\n    everything.\n\n    Examples\n    --------\n    In order to make a cached version of a slow function, do\n\n    >>> @cached\n    ... def slow_function1(a):\n    ...     return a\n    >>> @cached\n    ... def slow_function2(b):\n    ...     return b*b\n    >>> slow_function1(2)\n    2\n    >>> slow_function2(2)\n    4\n    >>> slow_function1(3)\n    3\n\n    Any calls to slow_function1 and slow_function2 are transparently cached\n    from now on.\n\n    See also\n    --------\n    shelve\n    pickle\n    """"""\n\n    import os\n    from types import FunctionType\n    from functools import wraps\n    import inspect\n\n    # To preserve backwards compatibility, calling without any arguments is\n    # still supported. In this case, keepOpen is the function to be cached.\n    if isinstance(keepOpen, FunctionType):\n        function = keepOpen\n        # check for name of script and assume interactive session otherwise\n        prog = inspect.getfile(function)\n        prog = os.path.splitext(os.path.basename(prog))[\n            0] if os.path.exists(prog) else ""python""\n        cacheFileName = ""cache-"" + prog + ""-"" + function.__name__ + "".pkl""\n        return cached_at(cacheFileName)(function)\n\n    def decorator(function):\n        # check for name of script and assume interactive session otherwise\n        prog = inspect.getfile(function)\n        prog = os.path.splitext(os.path.basename(prog))[\n            0] if os.path.exists(prog) else ""python""\n        cacheFileName = ""cache-"" + prog + ""-"" + function.__name__ + "".pkl""\n\n        @wraps(function)\n        @cached_at(cacheFileName, keepOpen, lockCacheFile, trackCode)\n        def decorated_function(*args, **kwargs):\n\n            return function(*args, **kwargs)\n\n        return decorated_function\n\n    return decorator\n\n\ndef cached_at(cacheFileName, keepOpen=False, lockCacheFile=False, trackCode=True):\n    """"""\n    Decorator which caches the result of a slow function in a file.\n\n    Parameters\n    ----------\n    cacheFileName : string\n      Path and filename of the cache file.\n    keepOpen : bool, optional (default = False)\n      Determines whether to keep the cache file open during the whole session.\n    lockCacheFile : bool, optional (default = False)\n      locks the cache file during access (necessary when used with pmap)\n      don\'t use this option together with keepOpen, otherwise you will get a dead lock\n    trackCode: bool, optional (default = True)\n      If true, also changes in the code will be tracked.\n\n    Returns\n    -------\n    f : decorated function\n\n    Notes\n    -----\n    See the decorator ""cached"" in this module.\n\n    Limitations\n    -----------\n    See the decorator ""cached"" in this module.\n\n    Examples\n    --------\n    In order to make a cached version of a slow function, do\n\n    >>> @cached_at(""mycache1.tmp"")\n    ... def slow_function1(a):\n    ...   return a\n    >>> @cached_at(""mycache2.tmp"")\n    ... def slow_function2(b):\n    ...   return b*b\n    >>> slow_function1(2)\n    2\n    >>> slow_function2(2)\n    4\n    >>> slow_function1(3)\n    3\n\n    See the decorator ""cached"" in this module for more information.\n\n    See also\n    --------\n    shelve\n    pickle\n    """"""\n\n    from functools import wraps\n\n    if keepOpen and lockCacheFile:\n        raise ValueError(""keepOpen cannot be used together with lockCacheFile"")\n\n    if lockCacheFile:\n        from . import locked_shelve as shelve\n    else:\n        import shelve\n\n    if keepOpen:\n        # Open the shelve on function decoration\n        _d = shelve.open(cacheFileName, protocol=-1, writeback=False)\n\n    def decorator(function):\n\n        @wraps(function)\n        def decorated_function(*args, **kwargs):\n\n            from six.moves import cPickle as pickle\n            import six\n            import inspect\n            import hashlib\n\n            def encode(x):\n                if six.PY2:\n                    return x\n                else:\n                    return x.encode(""utf-8"")\n\n            # Pickle the function arguments to use them as key\n            # it is preferable not to include the function name in the pickle\n            # when the function name changes, the cache file name changes anyway\n            # if the user decides to recall the function and manually recall the\n            # cache, it will still work\n            key = str(pickle.dumps((args, kwargs), protocol=-1))\n            code_hash = hashlib.md5(encode(inspect.getsource(function))).digest()\n\n            if keepOpen:\n                d = _d  # Use open shelve\n            else:\n                d = shelve.open(cacheFileName, protocol=-1, writeback=False)\n\n            create = True  # this variable is necessary, cached output could be anything, also None\n            if key in d:\n                assert ""cache"" in d[\n                    key], ""Your cache might be outdated. Try to delete and create it again!""\n                if trackCode:\n                    if code_hash in d[key][""code_hash""]:\n                        output = d[key][""cache""]\n                        create = False\n                else:\n                    output = d[key][""cache""]\n                    create = False\n\n            if create:\n                if not keepOpen:\n                    d.close()\n                output = function(*args, **kwargs)\n                if not keepOpen:\n                    d = shelve.open(cacheFileName, protocol=-\n                                    1, writeback=False)\n\n                if key in d:\n                    if pickle.dumps([d[key][""cache""]], protocol=-1) == pickle.dumps([output], protocol=-1):\n                        dk = d[key]\n                        dk[""code_hash""] += [code_hash]\n                        d[key] = dk\n                    else:\n                        d[key] = {""code_hash"": [code_hash], ""cache"": output}\n                else:\n                    d[key] = {""code_hash"": [code_hash], ""cache"": output}\n\n            if not keepOpen:\n                d.close()\n\n            return output\n\n        return decorated_function\n\n    return decorator\n\n\ndef memoized(function):\n    """"""\n    Caches the output of a function in memory to increase performance.\n\n    Returns\n    -------\n    f : decorated function\n\n    Notes\n    -----\n    This decorator speeds up slow calculations that you need over and over\n    in a script. If you want to keep the results of a slow function for\n    several script executions, use the ""cached"" decorator instead\n    (which also allows mutable arguments).\n\n    Limitations\n    -----------\n    Use this decorator only for functions with immutable arguments, like\n    numbers, tuples, and strings. The decorator is intended for simple\n    mathematical functions and optimized for performance.\n    """"""\n\n    from functools import wraps\n\n    cache = {}\n\n    @wraps(function)\n    def decorated_function(*args):\n\n        if args in cache:\n            output = cache[args]\n        else:\n            output = function(*args)\n            cache[args] = output\n        return output\n\n    return decorated_function\n'"
pyik/rootext.py,39,"b'# -*- coding: utf-8 -*-\nfrom six.moves import range\nimport numpy as np\nimport tempfile\nimport ROOT\n\n__all__ = [""ROOT"", ""ToVector"", ""ToNumpy"", ""PyTFile""]\n\n\nclass PyTFile(ROOT.TFile):\n\n    def __init__(self, *args):\n        self.save = ROOT.gDirectory\n        ROOT.TFile.__init__(self, *args)\n        ROOT.gDirectory = self.save\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, type, value, traceback):\n        self.Close()\n\n\nwith tempfile.NamedTemporaryFile() as f:\n    f.write(""""""#include <vector>\n\n#define GetVectorBuffer(T,x) \\\n  T* GetVectorBuffer##x(std::vector<T>& v) { return &v[0]; }\n\nGetVectorBuffer(double, D)\nGetVectorBuffer(float, F)\nGetVectorBuffer(long, L)\nGetVectorBuffer(unsigned long, UL)\nGetVectorBuffer(int, I)\nGetVectorBuffer(unsigned int, UI)\nGetVectorBuffer(short, S)\nGetVectorBuffer(unsigned short, US)\n"""""")\n    f.flush()\n    ROOT.gROOT.LoadMacro(f.name)\n    f.close()\n    del f\n\n\nclass NDArray(np.ndarray):\n\n    def __new__(subtype, shape, dtype, buffer, parent, offset=0,\n                strides=None, order=None, info=None):\n        obj = np.ndarray.__new__(\n            subtype, shape, dtype, buffer, offset, strides, order)\n        obj.parent = parent\n        return obj\n\n    def __array_finalize__(self, obj):\n        if obj is None:\n            return\n        self.parent = getattr(obj, ""parent"", None)\n\n\ndef ToVector(array):\n    """"""\n    Turns a Python list or Numpy array into a ROOT std::vector instance by copying the data.\n\n    Examples\n    --------\n    >>> a = (1, 2, 3)\n    >>> ra = ToVector(a)\n    >>> print type(ra), ra[0], ra[1], ra[2]\n    <class \'ROOT.vector<int,allocator<int> >\'> 1 2 3\n    """"""\n\n    n = len(array)\n    if isinstance(array[0], str):\n        res = ROOT.std.vector(""string"")(n)\n    elif isinstance(array[0], float) or (type(array) == np.ndarray and np.issubdtype(array[0].dtype, np.float)):\n        res = ROOT.std.vector(""double"")(n)\n    elif isinstance(array[0], int) or (type(array) == np.ndarray and np.issubdtype(array[0].dtype, np.float)):\n        res = ROOT.std.vector(""int"")(n)\n    for i in range(n):\n        res[i] = array[i]\n    return res\n\n\ndef ToNumpy(x):\n    """"""\n    Turns ROOT data structures into numpy structures.\n\n    Fast treatment (no copy):\n    std::vector<T>\n\n    All other objects are copied.\n\n    Examples\n    --------\n    >>> a = ROOT.std.vector(""double"")(3)\n    >>> a[0] = 1; a[1] = 2; a[2] = 3\n    >>> na = ToNumpy(a)\n    >>> print type(na), na\n    <class \'pyik.rootext.NDArray\'> [ 1.  2.  3.]\n    """"""\n\n    if isinstance(x, str):\n        f = ROOT.TFile.Open(x)\n        d = {}\n        for key in f.GetListOfKeys():\n            d[key.GetName()] = ToNumpy(key.ReadObj())\n        f.Close()\n        return d\n    elif isinstance(x, ROOT.TVectorD):\n        return NDArray((x.GetNoElements(),), np.float64, x.GetMatrixArray(), x)\n    elif isinstance(x, ROOT.TMatrixD) or isinstance(x, ROOT.TMatrixDSym):\n        # return NDArray((x.GetNrows(),x.GetNcols()), np.float64, x.GetMatrixArray(), x)\n        a = np.empty((x.GetNrows(), x.GetNcols()), np.float64)\n        for ix in range(x.GetNrows()):\n            for iy in range(x.GetNcols()):\n                a[ix, iy] = x(ix, iy)\n        return a\n    # here a reference to the original object is stored so that\n    # the original object is not deleted by gargabe collection\n    elif isinstance(x, ROOT.std.vector(""double"")):\n        return NDArray((x.size(),), np.float64, ROOT.GetVectorBufferD(x), x)\n    elif isinstance(x, ROOT.std.vector(""float"")):\n        return NDArray((x.size(),), np.float32, ROOT.GetVectorBufferF(x), x)\n    elif isinstance(x, ROOT.std.vector(""long"")):\n        return NDArray((x.size(),), np.int64, ROOT.GetVectorBufferL(x), x)\n    elif isinstance(x, ROOT.std.vector(""unsigned long"")):\n        return NDArray((x.size(),), np.uint64, ROOT.GetVectorBufferUL(x), x)\n    elif isinstance(x, ROOT.std.vector(""int"")):\n        return NDArray((x.size(),), np.int32, ROOT.GetVectorBufferI(x), x)\n    elif isinstance(x, ROOT.std.vector(""unsigned int"")):\n        return NDArray((x.size(),), np.uint32, ROOT.GetVectorBufferUI(x), x)\n    elif isinstance(x, ROOT.std.vector(""short"")):\n        return NDArray((x.size(),), np.int16, ROOT.GetVectorBufferS(x), x)\n    elif isinstance(x, ROOT.std.vector(""unsigned short"")):\n        return NDArray((x.size(),), np.uint16, ROOT.GetVectorBufferUS(x), x)\n    elif isinstance(x, ROOT.TH3):\n        nx = x.GetXaxis().GetNbins()\n        ny = x.GetYaxis().GetNbins()\n        nz = x.GetZaxis().GetNbins()\n        xedges = np.empty(nx + 1)\n        yedges = np.empty(ny + 1)\n        zedges = np.empty(nz + 1)\n        h = np.empty((nx, ny, nz))\n        for ix in range(nx):\n            xedges[ix] = x.GetXaxis().GetBinLowEdge(ix + 1)\n        xedges[nx] = x.GetXaxis().GetBinLowEdge(nx + 1)\n        for iy in range(ny):\n            yedges[iy] = x.GetYaxis().GetBinLowEdge(iy + 1)\n        yedges[ny] = x.GetYaxis().GetBinLowEdge(ny + 1)\n        for iz in range(nz):\n            zedges[iz] = x.GetZaxis().GetBinLowEdge(iz + 1)\n        zedges[nz] = x.GetZaxis().GetBinLowEdge(nz + 1)\n        for ix in range(nx):\n            for iy in range(ny):\n                for iz in range(nz):\n                    h[ix, iy, iz] = x.GetBinContent(ix + 1, iy + 1, iz + 1)\n        return h, xedges, yedges, zedges\n    elif isinstance(x, ROOT.TH2):\n        nx = x.GetXaxis().GetNbins()\n        ny = x.GetYaxis().GetNbins()\n        xedges = np.empty(nx + 1)\n        yedges = np.empty(ny + 1)\n        h = np.empty((nx, ny))\n        for ix in range(nx):\n            xedges[ix] = x.GetXaxis().GetBinLowEdge(ix + 1)\n        xedges[nx] = x.GetXaxis().GetBinLowEdge(nx + 1)\n        for iy in range(ny):\n            yedges[iy] = x.GetYaxis().GetBinLowEdge(iy + 1)\n        yedges[ny] = x.GetYaxis().GetBinLowEdge(ny + 1)\n        for ix in range(nx):\n            for iy in range(ny):\n                h[ix, iy] = x.GetBinContent(ix + 1, iy + 1)\n        return h, xedges, yedges\n    elif isinstance(x, ROOT.TProfile):\n        n = x.GetXaxis().GetNbins()\n        xedges = np.empty(n + 1)\n        hs = np.empty(n)\n        hes = np.empty(n)\n        for i in range(n):\n            xedges[i] = x.GetXaxis().GetBinLowEdge(i + 1)\n            hs[i] = x.GetBinContent(i + 1)\n            hes[i] = x.GetBinError(i + 1)\n        xedges[n] = x.GetXaxis().GetBinLowEdge(n + 1)\n        return hs, hes, xedges\n    elif isinstance(x, ROOT.TH1):\n        n = x.GetXaxis().GetNbins()\n        xedges = np.empty(n + 1)\n        h = np.empty(n)\n        for i in range(n):\n            xedges[i] = x.GetXaxis().GetBinLowEdge(i + 1)\n            h[i] = x.GetBinContent(i + 1)\n        xedges[n] = x.GetXaxis().GetBinLowEdge(n + 1)\n        return h, xedges\n    elif isinstance(x, ROOT.TGraphErrors):\n        n = x.GetN()\n        xs = np.ndarray((n,), dtype=np.float64, buffer=x.GetX())\n        ys = np.ndarray((n,), dtype=np.float64, buffer=x.GetY())\n        xes = np.ndarray((n,), dtype=np.float64, buffer=x.GetEX())\n        yes = np.ndarray((n,), dtype=np.float64, buffer=x.GetEY())\n        return xs, ys, xes, yes\n    elif isinstance(x, ROOT.TGraphAsymmErrors):\n        n = x.GetN()\n        xs = np.ndarray((n,), dtype=np.float64, buffer=x.GetX())\n        ys = np.ndarray((n,), dtype=np.float64, buffer=x.GetY())\n        xeds = np.ndarray((n,), dtype=np.float64, buffer=x.GetEXlow())\n        xeus = np.ndarray((n,), dtype=np.float64, buffer=x.GetEXhigh())\n        yeds = np.ndarray((n,), dtype=np.float64, buffer=x.GetEYlow())\n        yeus = np.ndarray((n,), dtype=np.float64, buffer=x.GetEYhigh())\n        return xs, ys, (xeds, xeus), (yeds, yeus)\n    elif isinstance(x, ROOT.TVector2):\n        return np.asfarray([x.X(), x.Y()])\n    elif isinstance(x, ROOT.TVector3):\n        return np.asfarray([x.X(), x.Y(), x.Z()])\n    else:\n        raise ValueError(\n            ""Cannot handle type %s yet, please have a look at pyik.rootext and implement it"" % (type(x)))\n'"
pyik/time_conversion.py,0,"b'# -*- coding: utf-8 -*-\n""""""Time conversion utilities.""""""\nfrom __future__ import print_function\n\ndef utc_to_gps(utcsec):\n    """"""\n    Convert UTC seconds to GPS seconds.\n\n    Parameters\n    ----------\n    utcsec: int\n      Time in utc seconds.\n\n    Returns\n    -------\n    Time in GPS seconds.\n\n    Notes\n    -----\n    The code is ported from Offline.\n\n    Examples\n    --------\n    >>> utc_to_gps(315964800) # Jan 6th, 1980\n    0\n    """"""\n\n    import time\n\n    def seconds(year, month, day):\n        return time.mktime((year, month, day, 0, 0, 0, 0, 0, 0)) - time.timezone\n\n    kUTCGPSOffset0 = (10 * 365 + 7) * 24 * 3600\n    kLeapSecondList = (\n        (seconds(1981, 7, 1), 1),  # 1 JUL 1981\n        (seconds(1982, 7, 1), 2),  # 1 JUL 1982\n        (seconds(1983, 7, 1), 3),  # 1 JUL 1983\n        (seconds(1985, 7, 1), 4),  # 1 JUL 1985\n        (seconds(1988, 1, 1), 5),  # 1 JAN 1988\n        (seconds(1990, 1, 1), 6),  # 1 JAN 1990\n        (seconds(1991, 1, 1), 7),  # 1 JAN 1991\n        (seconds(1992, 7, 1), 8),  # 1 JUL 1992\n        (seconds(1993, 7, 1), 9),  # 1 JUL 1993\n        (seconds(1994, 7, 1), 10),  # 1 JUL 1994\n        (seconds(1996, 1, 1), 11),  # 1 JAN 1996\n        (seconds(1997, 7, 1), 12),  # 1 JUL 1997\n        (seconds(1999, 1, 1), 13),  # 1 JAN 1999\n        # DV: 2000 IS a leap year since it is divisible by 400,\n        #     ie leap years here are 2000 and 2004 -> leap days = 6\n        (seconds(2006, 1, 1), 14),  # 1 JAN 2006\n        (seconds(2009, 1, 1), 15),  # 1 JAN 2009, from IERS Bulletin C No. 36\n        (seconds(2012, 7, 1), 16),  # 1 JUL 2012, from IERS Bulletin C No. 43\n        # 1 JUL 2015, from IERS Bulletin C No. 49\n        # (https://hpiers.obspm.fr/iers/bul/bulc/bulletinc.49)\n        (seconds(2015, 7, 1), 17),\n        (seconds(2017, 1, 1), 18)   # 1 JAN 2017, from IERS Bulletin C No. 52\n    )\n\n    leapSeconds = 0\n    for x in reversed(kLeapSecondList):\n        if utcsec >= x[0]:\n            leapSeconds = x[1]\n            break\n\n    return utcsec - kUTCGPSOffset0 + leapSeconds\n\n\ndef gps_to_utc(gpssec):\n    """"""\n    Convert GPS seconds to UTC seconds.\n\n    Parameters\n    ----------\n    gpssec: int\n      Time in GPS seconds.\n\n    Returns\n    -------\n    Time in UTC seconds.\n\n    Notes\n    -----\n    The code is ported from Offline.\n\n    Examples\n    --------\n    >>> gps_to_utc(0) # Jan 6th, 1980\n    315964800\n    """"""\n\n    kSecPerDay = 24 * 3600\n    kUTCGPSOffset0 = (10 * 365 + 7) * kSecPerDay\n\n    kLeapSecondList = (\n        ((361 + 0 * 365 + 0 + 181) * kSecPerDay + 0, 1),  # 1 JUL 1981\n        ((361 + 1 * 365 + 0 + 181) * kSecPerDay + 1, 2),  # 1 JUL 1982\n        ((361 + 2 * 365 + 0 + 181) * kSecPerDay + 2, 3),  # 1 JUL 1983\n        ((361 + 4 * 365 + 1 + 181) * kSecPerDay + 3, 4),  # 1 JUL 1985\n        ((361 + 7 * 365 + 1) * kSecPerDay + 4, 5),  # 1 JAN 1988\n        ((361 + 9 * 365 + 2) * kSecPerDay + 5, 6),  # 1 JAN 1990\n        ((361 + 10 * 365 + 2) * kSecPerDay + 6, 7),  # 1 JAN 1991\n        ((361 + 11 * 365 + 3 + 181) * kSecPerDay + 7, 8),  # 1 JUL 1992\n        ((361 + 12 * 365 + 3 + 181) * kSecPerDay + 8, 9),  # 1 JUL 1993\n        ((361 + 13 * 365 + 3 + 181) * kSecPerDay + 9, 10),  # 1 JUL 1994\n        ((361 + 15 * 365 + 3) * kSecPerDay + 10, 11),  # 1 JAN 1996\n        ((361 + 16 * 365 + 4 + 181) * kSecPerDay + 11, 12),  # 1 JUL 1997\n        ((361 + 18 * 365 + 4) * kSecPerDay + 12, 13),  # 1 JAN 1999\n        # DV: 2000 IS a leap year since it is divisible by 400,\n        #     ie leap years here are 2000 and 2004 -> leap days = 6\n        ((361 + 25 * 365 + 6) * kSecPerDay + 13, 14),  # 1 JAN 2006\n        ((361 + 28 * 365 + 7) * kSecPerDay + 14, 15),  # 1 JAN 2009\n        ((361 + 31 * 365 + 8 + 181) * kSecPerDay + 15, 16),  # 1 JUL 2012\n        ((361 + 34 * 365 + 8 + 181) * kSecPerDay + 16, 17),  # 1 JUL 2015\n        ((361 + 36 * 365 + 9) * kSecPerDay + 17, 18)  # 1 JAN 2017\n    )\n\n    leapSeconds = 0\n    for x in reversed(kLeapSecondList):\n        if gpssec >= x[0]:\n            leapSeconds = x[1]\n            break\n\n    return gpssec + kUTCGPSOffset0 - leapSeconds\n\n\ndef mjd(year, month, day, hour=0, minute=0, second=0):\n    """"""\n    Calculate the modified Julian date from an ordinary date and time.\n\n    Notes\n    -----\n    The formulas are taken from Wikipedia.\n\n    Example\n    -------\n    >>> mjd(2000, 1, 1)\n    51544.0\n    """"""\n    a = (14 - month) // 12\n    y = year + 4800 - a\n    m = month + 12 * a - 3\n    jdn = day + (153 * m + 2) // 5 + 365 * y + \\\n        y // 4 - y // 100 + y // 400 - 32045\n    jd = jdn + (hour - 12) / 24. + minute / 1400. + second / 86400.\n    mjd = jd - 2400000.5\n    return mjd\n\n\ndef UTCSecondToLocalDatetime(utcsec, timezone=""Europe/Berlin""):\n    """"""\n    Convert utc second to local datetime (specify correct local timezone with string).\n\n    Parameters\n    ----------\n    utcsec: int\n      Time in utc seconds (unix time).\n\n    timezone: string\n      Time zone string compatible with pytz format\n\n    Returns\n    -------\n      dt: Datetime object corresponding to local time.\n\n    Notes\n    -----\n    Adapted from a stackoverflow answer.\n\n    To list all available time zones:\n    >> import pytz\n    >> pytz.all_timezones\n\n    To print the returned datetime object in a certain format:\n    >> from pyik.time_conversion import UTCSecondToLocalDatetime\n    >> dt = UTCSecondToLocalDatetime(1484314559)\n    >> dt.strftime(""%d/%m/%Y, %H:%M:%S"")\n    """"""\n\n    import pytz\n    from datetime import datetime\n\n    local_tz = pytz.timezone(timezone)\n    utc_dt = datetime.utcfromtimestamp(utcsec)\n    local_dt = utc_dt.replace(tzinfo=pytz.utc).astimezone(local_tz)\n\n    return local_tz.normalize(local_dt)\n\n\ndef DatetimeToGPSSeconds(dt, timezone=None):\n    """"""\n    Convert a datetime expressed as python datetime object to (int) gps seconds.\n\n    Parameters\n    ----------\n    dt: python datetime object\n\n    timezone: string, timezone object or None (default)\n      This is the time zone the datetime dt is specified in\n      If None, assumes that dt is naive, i.e. represents UTC time\n\n    Returns\n    --------\n      gps: time in gps seconds\n\n    """"""\n    import calendar\n    import pytz\n    from datetime import datetime\n\n    if dt.tzinfo is not None:\n        print(""Warning! Your datetime is timezone-aware. This function assumes the timezone that is passed to the function and won\'t do a conversion between timezones!"")\n\n    if timezone is None:\n\n        ts = calendar.timegm(dt.utctimetuple())\n\n    else:\n\n        if isinstance(timezone, str):\n            tz = pytz.timezone(timezone)\n        else:\n            tz = timezone\n\n        dt = dt.replace(tzinfo=tz)\n        utc_naive = dt.replace(tzinfo=None) - dt.utcoffset()\n        ts = (utc_naive - datetime(1970, 1, 1)).total_seconds()\n\n    return int(utc_to_gps(ts))\n'"
tests/test_mplext.py,0,b''
tests/test_numpyext.py,1,"b'import numpy as np\nfrom numpy.testing import assert_allclose, assert_equal\n\n\ndef test_profile():\n    from pyik.numpyext import profile\n    x = [0.0, 1.0, 2.0, 3.0]\n    assert_equal(x, (0, 1.0, 2.0, 3.0))\n    yavg, ystd, n, xe = profile(x, x, bins=2)\n    assert_allclose(yavg, (0.5, 2.5), 1e-3)\n    assert_allclose(ystd, (0.5, 0.5), 1e-3)\n    assert_equal(n, (2, 2))\n    assert_allclose(xe, (0., 1.5, 3.))\n\n\ndef test_centers():\n    from pyik.numpyext import centers\n    c, w = centers([0, 1, 3])\n    assert_equal(c, (0.5, 2.0))\n    assert_equal(w, (0.5, 1.0))\n\n\ndef test_derivative():\n    from pyik.numpyext import derivative\n    def f(x):\n        return 2 + x + 2 * x ** 2 + x ** 3\n    def fp(x):\n        return 1 + 4 * x + 3 * x ** 2\n    def fpp(x):\n        return 4 + 6 * x\n\n    assert_allclose(derivative(f, 1.0), fp(1.0))\n    assert_allclose(derivative(f, 1.0, step=1e-3), fp(1.0))\n    assert_allclose(derivative(f, 1.0, order=2), fpp(1.0))\n    ones = np.ones(2)\n    assert_allclose(derivative(f, ones), fp(ones))\n    assert_allclose(derivative(f, ones, order=2), fpp(ones))\n\n\ndef test_derivativeND():\n    from pyik.numpyext import derivativeND\n    def f(xy):\n        x, y = xy.T\n        return x ** 2 + y ** 2\n    \n    xy = ((0., 0.), (1., 0.), (0., 1.))\n    result = derivativeND(f, xy)\n    assert_allclose(result, ((0., 0.), (2., 0.), (0., 2.)))\n\n'"
tests/test_performance.py,2,"b'import os\nimport contextlib\nimport numpy as np\nfrom numpy.testing import assert_allclose, assert_equal\n\n\n@contextlib.contextmanager\ndef pushd(new):\n    old = os.getcwd()\n    os.chdir(str(new))\n    yield\n    os.chdir(old)\n\n\ndef log(x, mode=""r""):\n    return open(str(x), mode)\n\ndef f1(x):\n    return 2 * x\n\ndef f2(x, y):\n    return x * y\n    \ndef test_pmap():\n    from pyik.performance import pmap\n    assert pmap(f1, [1, 2, 3]) == [2, 4, 6]\n    assert pmap(f2, (1, 2, 3), [3, 4, 5]) == [3, 8, 15]\n    out = pmap(f1, np.ones(3))\n    assert isinstance(out, np.ndarray)\n    assert_equal(out, (2, 2, 2))\n\n\ndef test_cached_at(tmpdir):\n    from pyik.performance import cached_at\n    dbfilename = str(tmpdir.join(""foo.db""))\n    @cached_at(dbfilename)\n    def func(x):\n        with log(tmpdir.join(""log""), ""a"") as l:\n            l.write(""x"") # leave trace of call\n        return 2 * x\n    assert func(2) == 4\n    assert func(2) == 4\n    assert log(tmpdir.join(""log"")).read() == ""x"" # only one call\n\n\ndef test_cached(tmpdir):\n    from pyik.performance import cached\n    @cached\n    def func(x):\n        with log(tmpdir.join(""log""), ""a"") as l:\n            l.write(""x"") # leave trace of call\n        return 2 * x\n    with pushd(tmpdir):\n        assert func(2) == 4\n        assert func(2) == 4\n    assert log(tmpdir.join(""log"")).read() == ""x"" # only one call\n'"
