file_path,api_count,code
train.py,19,"b'import numpy as np\r\nimport pandas as pd\r\nimport matplotlib.pyplot as plt\r\nimport time\r\n\r\n\r\ndef plot_confusion_matrix(df_confusion, title=\'Confusion matrix\', cmap=plt.cm.gray_r):\r\n    """"""\r\n    Used to plot confusion matrix.\r\n    """"""\r\n    \r\n    plt.matshow(df_confusion, cmap=cmap)\r\n    plt.title(title)\r\n    plt.colorbar()\r\n    tick_marks = np.arange(len(df_confusion.columns))\r\n    plt.xticks(tick_marks, df_confusion.columns, rotation=45)\r\n    plt.yticks(tick_marks, df_confusion.index)\r\n    plt.tight_layout()\r\n    plt.ylabel(df_confusion.index.name)\r\n    plt.xlabel(df_confusion.columns.name)\r\n    plt.show()\r\n\r\n\r\nclass NN(object):\r\n    """"""\r\n    A network that uses Sigmoid activation function.\r\n    """"""\r\n\r\n    def __init__(self):\r\n        \r\n        self.nodes = []\r\n        self.layers = {}\r\n        self.weights = {}\r\n        self.n_classes = 0\r\n        \r\n        # For Debugging\r\n        self.grads = {}\r\n        self.regs = {}\r\n        self.dels = {}\r\n        \r\n        # For analysis\r\n        self.history = {}\r\n\r\n    def add_layer(self, n_nodes, output_layer=False):\r\n        """"""\r\n        Adds a layer of specified no. of output nodes.  \r\n        For the output layer, the flag  output_layer must be True.\r\n        A network must have an output layer.\r\n        """"""\r\n\r\n        if not output_layer:\r\n            self.nodes.append(n_nodes)\r\n        else:\r\n            self.n_classes = n_nodes\r\n\r\n    def sigmoid(self, z):\r\n        """"""\r\n        Calculates the sigmoid activation function.\r\n        """"""\r\n\r\n        return 1 / (1 + np.exp(-z))\r\n\r\n    def predict(self, x, predict=True, argmax=True, rand_weights=False):\r\n        """"""\r\n        Performs a pass of forward propagation.\r\n        If predict is set to True, trained weights are used\r\n        and predictions are returned in a single vector(if argmax is set to True) \r\n        with labels from 0 to (n_classes - 1).\r\n        """"""\r\n        \r\n        np.random.seed(777)\r\n        nodes = self.nodes\r\n        layers = {}\r\n        weights = {}\r\n\r\n        # -------------- for input layer\r\n        m = x.shape[0]\r\n        x = np.append(np.ones(m).reshape(m, 1), x, axis=1)\r\n        layers[\'a%d\' % 1] = x\r\n        \r\n        # --------------- for dense layers\r\n        if rand_weights:\r\n            for i in range(len(nodes)):\r\n                m, n = x.shape\r\n                # x = np.append(np.ones(m).reshape(m, 1), x, axis=1)\r\n                w = np.random.randn(nodes[i], n)\r\n                z = x.dot(w.T)\r\n                a = np.append(np.ones(m).reshape(m, 1), self.sigmoid(z), axis=1)\r\n\r\n                if not predict:\r\n                    layers[\'a%d\' % (i + 2)] = a\r\n                    weights[\'w%d\' % (i + 1)] = w\r\n                x = a\r\n\r\n            # --------------- for output layer\r\n            m, n = x.shape\r\n            w = np.random.randn(self.n_classes, n)\r\n            z = x.dot(w.T)\r\n            a = self.sigmoid(z)\r\n            x = a\r\n\r\n            if not predict:\r\n                layers[\'a%d\' % (len(layers) + 1)] = a\r\n                weights[\'w%d\' % (len(weights) + 1)] = w\r\n\r\n        else:\r\n            \r\n            # --------------- for dense layers\r\n            for i in range(len(nodes)):\r\n                m, n = x.shape\r\n                # x = np.append(np.ones(m).reshape(m, 1), x, axis=1)\r\n                w = self.weights[\'w%d\' % (i + 1)]\r\n                z = x.dot(w.T)\r\n                a = np.append(np.ones(m).reshape(m, 1), self.sigmoid(z), axis=1)\r\n                if not predict:\r\n                    layers[\'a%d\' % (i + 2)] = a\r\n                    weights[\'w%d\' % (i + 1)] = w\r\n\r\n                x = a\r\n\r\n            # --------------- for output layer\r\n            w = self.weights[\'w%d\' % (len(nodes) + 1)]\r\n            z = x.dot(w.T)\r\n            a = self.sigmoid(z)\r\n            x = a\r\n\r\n            if not predict:\r\n                layers[\'a%d\' % (len(layers) + 1)] = a\r\n                weights[\'w%d\' % (len(weights) + 1)] = w\r\n\r\n        output = x\r\n        if predict:\r\n            return np.argmax(output, axis=1) if argmax else output\r\n        elif rand_weights:\r\n            self.layers = layers\r\n            self.weights = weights\r\n        else:\r\n            return layers, weights\r\n\r\n    def cost(self, x, y, lamda=0):\r\n        """"""\r\n        Calculates the cost for given data and labels, with trained weights.\r\n        """"""\r\n        \r\n        weights = self.weights\r\n        layers, _ = self.predict(x, predict=False)\r\n\r\n        m, n = x.shape\r\n        reg2 = 0\r\n        for i in range(len(weights)):\r\n            reg2 += np.sum(weights[\'w%d\' % (i + 1)][:, 1:] ** 2)\r\n\r\n        j = (-1 / m) * np.sum(y.T.dot(np.log(layers[\'a%d\' % (len(layers))])) +\r\n                              (1 - y).T.dot(np.log(1 - layers[\'a%d\' % (len(layers))]))) + (lamda / (2 * m)) * reg2\r\n\r\n        return j\r\n\r\n    def fit(self, data, labels, test=[], test_labels=[], alpha=0.01, lamda=0, epochs=50):\r\n        """"""\r\n        Performs specified no. of epoches. \r\n        One epoch = one pass of forward propagation + one pass of backpropagation. \r\n        """"""\r\n\r\n        # ----------- Checking data format\r\n        assert self.n_classes, \'You must have the output layer. Set output_layer=True while adding the output layer.\'\r\n        assert (labels.shape[0] == data.shape[0]), \'The labels must have same no. of rows as training data.\'\r\n        assert (labels.shape[1] == self.n_classes), \'The labels should have same no. of columns as there are \' \\\r\n                                                    \'classes. Convert labels into categorical form before training.\'\r\n        assert (epochs > 0), \'No. of epochs must be greater than or equal to 1.\'\r\n        assert (len(test) == len(test_labels)), ""Invalid combination of test set and test_labels provided."" \\\r\n                                                    ""Please check the provided test and test_labels.""\r\n\r\n        # ------------- Training\r\n        start = time.time()\r\n        self.predict(data, predict=False, rand_weights=True)\r\n        j_history = []\r\n        j_test_history = []\r\n\r\n        for epoch in range(epochs):\r\n            layers, weights = self.predict(data, predict=False, rand_weights=False)\r\n            m, n = data.shape\r\n\r\n            # ----------------- Calculating del terms\r\n            delta1 = layers[\'a%d\' % (len(layers))] - labels\r\n            delta = delta1.dot(weights[\'w%d\' % (len(weights))]) * layers[\'a%d\' % (len(layers) - 1)] * \\\r\n                    (1 - layers[\'a%d\' % (len(layers) - 1)])\r\n\r\n            dels = {\r\n                \'del%d\' % (len(layers)): delta1,\r\n                \'del%d\' % (len(layers) - 1): delta\r\n            }\r\n\r\n            for i in range(len(weights) - 2):\r\n                delta = delta[:, 1:].dot(weights[\'w%d\' % (len(layers) - 2 - i)]) * \\\r\n                        layers[\'a%d\' % (len(layers) - 2 - i)] * (1 - layers[\'a%d\' % (len(layers) - 2 - i)])\r\n                dels[\'del%d\' % (len(layers) - 2 - i)] = delta\r\n\r\n            # ------------------ Calculating grad and regularization terms\r\n            grads = {\r\n                \'grad%d\' % (len(weights)): (1 / m) * (\r\n                    dels[\'del%d\' % (len(layers))].T.dot(layers[\'a%d\' % (len(weights))]))\r\n            }\r\n\r\n            regs = {\r\n                \'reg%d\' % (len(weights)): (lamda / m) * weights[\'w%d\' % (len(weights))]\r\n            }\r\n\r\n            for i in range(len(weights) - 1):\r\n                grad = (1 / m) * \\\r\n                       (dels[\'del%d\' % (len(layers) - 1 - i)][:, 1:].T.dot(layers[\'a%d\' % (len(weights) - 1 - i)]))\r\n\r\n                grads[\'grad%d\' % (len(weights) - 1 - i)] = grad\r\n\r\n                reg = (lamda / m) * weights[\'w%d\' % (len(weights) - 1 - i)]\r\n                reg[:, 0] = 0\r\n                regs[\'reg%d\' % (len(weights) - 1 - i)] = reg\r\n\r\n            # -------------- for debugging later on\r\n            self.grads = grads\r\n            self.regs = regs\r\n            self.dels = dels\r\n\r\n            # ----------------- Updating Parameters\r\n            for i in range(1, len(weights) + 1):\r\n                weights[\'w%d\' % i] = weights[\'w%d\' % i] - alpha * grads[\'grad%d\' % i] - regs[\'reg%d\' % i]\r\n\r\n            self.layers = layers\r\n            self.weights = weights\r\n\r\n            # ----------------- Analysis steps\r\n\r\n            j = float(self.cost(data, labels, lamda=lamda))\r\n            j_history.append(j)\r\n\r\n            print(""\\r"" + ""{}% |"".format(int(100 * i / epochs) + 1) + \'#\' * int((int(100 * i / epochs) + 1) / 5) +\r\n                  \' \' * (20 - int((int(100 * i / epochs) + 1) / 5)) + \'|\',\r\n                  end="""") if not i % (epochs / 100) else print("""", end="""")\r\n\r\n            acc = 100 * np.sum(np.argmax(layers[\'a%d\' % (len(layers))], axis=1) == np.argmax(labels, axis=1)) / m\r\n            \r\n            if len(test):\r\n                m1, n1 = test.shape\r\n                j_test = float(self.cost(test, test_labels, lamda=lamda))\r\n                j_test_history.append(j_test)\r\n                test_prediction = self.predict(test)\r\n                acc_test = 100 * np.sum(test_prediction == np.argmax(test_labels, axis=1)) / m1\r\n                print(acc_test, type(acc_test))\r\n                print(\'Train cost: %0.2f\\tTrain acc.: %0.2f%%\\tTest cost: %0.2f\\tTest acc.: %0.2f%%\' % (j, acc, j_test, acc_test))\r\n                \r\n            else:\r\n                print(\'cost: %0.2f\\tacc.: %0.2f%%\' % (j, acc))\r\n\r\n        print(""Displaying Cost vs Iterations graph..."")\r\n        \r\n        if len(test):\r\n            plot_test, =plt.plot(range(epochs), j_test_history, \'r\')\r\n        \r\n        plot_train, =plt.plot(range(epochs), j_history, \'b\')        \r\n        plt.xlabel(\'Iterations\')\r\n        plt.ylabel(\'Cost\')\r\n        \r\n        if len(test):\r\n            plot_test, =plt.plot(range(epochs), j_test_history, \'r\')\r\n            plt.legend([plot_test, plot_train], [\'Test\', ""Train""])\r\n        else:\r\n            plt.legend([plot_train], [""Train""])\r\n            \r\n        plt.show()\r\n\r\n        cm = pd.crosstab(np.argmax(self.layers[\'a%d\' % (len(self.layers))], axis=1), np.argmax(labels, axis=1),\r\n                         rownames=[\'Actual\'], colnames=[\'Predicted\'])\r\n        print(""Plotting Confusion Matrix"")\r\n        plot_confusion_matrix(cm)\r\n\r\n        end = time.time()\r\n        \r\n        if len(test):\r\n            print(\'Final train cost: %0.2f\\tFinal train acc.: %0.2f%%\\tFinal test cost: %0.2f\\tFinal test acc.: %0.2f%%\' % (j, acc, j_test, acc_test))\r\n        else:\r\n            print(\'Final cost: %0.2f\\tFinal acc.: %0.2f%%\' % (j, acc))\r\n        print(\'Execution time: %0.2fs\' % (end - start))\r\n        \r\n        self.history[\'run%d\' % (len(self.train_history) + 1)] = {\r\n            \'layers\': self.nodes,\r\n            \'alpha\': alpha,\r\n            \'lambda\': lamda,\r\n            \'epochs\': epochs,\r\n            \'Final train acc.\': acc,\r\n            \'Final train cost\': j,\r\n            \'Final test acc.\': acc_test if len(test) else \'N/A\',\r\n            \'Final test cost\': j_test if len(test) else \'N/A\',\r\n            \'Execution time\': (end - start)\r\n        }\r\n        \r\n# --------------- Sample run\r\n\r\nif __name__ == ""__main__"":\r\n    x = np.array([[1, 2, 3], [2, 3, 4], [3, 4, 5], [4, 5, 6], [5, 6, 7], [6, 7, 8], [7, 8, 9]])  # Test data\r\n    y = np.array([[0, 1], [1, 0], [1, 0], [0, 1], [0, 1], [1, 0], [0, 1]])  # Test labels\r\n\r\n    model = NN()\r\n    model.add_layer(70)\r\n    model.add_layer(70)\r\n    model.add_layer(2, output_layer=True)\r\n\r\n    model.fit(x, y, epochs=1000)\r\n'"
