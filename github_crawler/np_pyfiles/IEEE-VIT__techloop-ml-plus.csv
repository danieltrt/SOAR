file_path,api_count,code
Sessions/ML02/Scatter Plot.py,2,"b'import numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.array([10, 1, 5, 6, 2, 4, 6, 3, 7, 6, 4, 2, 3, 6, 4, 5, 9, 1, 2, 8, 7])\ny = np.array([45, 20, 36, 34, 40, 31, 26, 24, 42, 49, 37, 10, 15, 35, 27, 20, 46, 6, 40, 38, 30])\n\nplt.xlabel(\'x\')\nplt.ylabel(\'y\')\n\nplt.scatter(x, y, color = ""m"", marker = ""o"", s = 30)\n\nplt.show()\n'"
users/keshmalp/logistic regression.py,3,"b""import numpy as np\r\n\r\n#Credit score range 300-850 on the basis of which you will get a loan \r\ng = np.array([500,720,690,300,370,632,820,425,740,430,421,323,620,519,620,530,422,616])\r\n #random initialization whether loan will be sanctioned\r\nh = np.array([0,1,1,0,0,1,1,0,1,0,1,0,1,1,1,0,0,1])\r\ndef sigmoid(g):\r\n    return 1/(1+np.exp(-g))\r\nalpha=0.00001\r\n\r\n\r\ndef grad_decent(g,h,inpu):\r\n    d1,d2=0,0\r\n    for i in range(1000):\r\n        grad1 = 0\r\n        grad2 = 0\r\n        for j in range(len(g)):\r\n            grad1 += sigmoid(d1+d2*g[j]) - h[j]\r\n            grad2 += (sigmoid(d1+d2*g[j]) - h[j])*g[j]\r\n        d1 -= alpha*grad1\r\n        d2 -= alpha*grad2\r\n    return sigmoid(d1+d2*inpu)\r\n\r\ninpu = int(input('Enter the credit score you have'))\r\nprint('probability of you getting getting the loan=  ' + str(grad_decent(g,h,inpu)))\r\n"""
users/mayankshah1607/logistic_regression.py,3,"b""import numpy as np \n#Import data\nx = np.array([17,23,31,21,20,21,24,22,31,41,26,21,33,38,16,27,28,17,17]) #Temperature in Celcius [INPUT]\ny = np.array([1,1,0,1,1,1,0,0,0,0,0,1,0,0,1,1,0,1,1]) # 1 = It rained ; 0 = it didn't rain [ OUTPUT]\n\n#Sigmoid Function\ndef sig(z):\n    return 1 / (1 + np.exp(-(z)))\n\nlearning_rate = 0.0001\n\ndef gradientDes(x,y,val):  #Val is the value of Temperature in celcius for which we have to determine if it rained or no\n    t1 = 0\n    t2 = 0\n    for i in range(10000):\n        gradient1 = 0\n        gradient2 = 0\n        for j in range(len(x)):\n            gradient1 = gradient1 + ((sig(t1+(t2*x[j])) - y[j]))\n            gradient2 = gradient2 + ((sig(t1+(t2*x[j])) - y[j]) * x[j] )\n        t1 = t1 - (learning_rate * gradient1)\n        t2 = t2 - (learning_rate * gradient2)\n    return sig(t1 + (t2 * val))\n\na = int(input('Enter the temperature in celcius: '))\nprint('There is a '+str(round(gradientDes(x,y,a)*100))+ '% chance that it will rain')"""
users/mythilirajendra/logistic_task1.py,3,"b'import numpy as np\nimport matplotlib.pyplot as plt\n\nx= np.array([1,2,3,5,8,9,11,12])    \ny= np.array([0,0,0,1,1,0,1,1])\n\ndef sigmoid(z):                        \n    return 1/(1+np.exp(-z))\n\na[i]= sigmoid(W*x[i]+B)\n\ndef loss(a,y):\n    error=0\n    for i in range(len(x)):\n        temp= -(y[i]*log(a[i])+(1-y[i])*log(1-a[i]))\n        error+=temp\n    return error/float(len(x))\n\n\ndef gradient(x,y,iterations,alpha):\n    W=0\n    B=0\n    for i in range(iterations):\n        B_gradient=0\n        W_gradient=0\n        for j in range(len(x)):\n            B_gradient += (a[j]-y[j])/len(x)\n            W_gradient += ((a[j]-y[j])*x[j])/len(x)\n        W= W- alpha*W_gradient\n        B= B- alpha*B_gradient\n    return W,B\n\nalpha= 0.001\niterations= 10000\nW2,B2= gradient(x,y,10000,0.001)\n\ndef probability(val):\n    return sigmoid(W2*val+B2)\n    \nvalue=int(input(\'Enter the value:\'))\nprint( "" the probability is"" + probability(value))\n        \n        \n        \n'"
Sessions/ML_03/Classification/logistic_regression.py,0,"b""# Logistic Regression\n\n# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Importing the dataset\ndataset = pd.read_csv('Social_Network_Ads.csv')\nX = pd.DataFrame(dataset.iloc[:, 1:4].values)\ny = pd.DataFrame(dataset.iloc[:, 4].values)\n\nX = X.values\ny = y.values\n\n#Handling Categorical values\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlabelencoder_X = LabelEncoder()\nX[:, 0] = labelencoder_X.fit_transform(X[:, 0])\nonehotencoder = OneHotEncoder(categorical_features = [0])\nX = onehotencoder.fit_transform(X).toarray()\n\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n\n# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n# Fitting Logistic Regression to the Training set\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0)\nclassifier.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test) \n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)"""
Sessions/ML_03/Pre-Processing/pre_processing.py,0,"b""import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Importing the dataset\ndataset = pd.read_csv('Data.csv', index=False)\nX = dataset.iloc[:, 0:3].values\ny = dataset.iloc[:, 3].values\n\n\n\n#Handling Missing Values\nfrom sklearn.preprocessing import Imputer\nage_and_salary = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)\nX[:, 1:3] = age_and_salary.fit_transform(X[:, 1:3])\n\n\n\n#Handling Categorical values\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlabelencoder_X = LabelEncoder()\nX[:, 0] = labelencoder_X.fit_transform(X[:, 0])\nonehotencoder = OneHotEncoder(categorical_features = [0])\nX = onehotencoder.fit_transform(X).toarray()\n\nlabelencoder_y = LabelEncoder()\ny[:,0] = labelencoder_y.fit_transform(y[:,0])\n\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n\n# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)"""
users/Sourav/linear regression/multivariate linear regression.py,4,"b""import numpy as np\n##This linear approach is done in order to determine the marks scored in an exam on the factors such as hours studied, hours slept and hours played on a day.\nx1= np.array([6,3,4,7,8,10,6,8,3,9])## hours studied\nx2= np.array([6,7,4,6,9,5,5,6,8,6])## hours slept\nx3= np.array([6,5,5,3,2,2,4,5,3,1])## hours played\ny = np.array([90,91,70,75,80,89,93,77,79,86])## marks scored\nn=len(x1)\nm1= 12\nm2= 12\nm3= 12\nc = 12\n\nlearning_rate= 0.001\n\np=int(input('Enter the number of hours studied in a day :- '))\nq=int(input('Enter the number of hours slept in a day :-  '))\nr=int(input('Enter the number of hours played in a day :- '))\nif (p+q+r<24) : ##Since it Impossible to things more than 24 hours in a day\n    def gradient_descent(m1,m2,m3,c):\n        for i in range(10000):\n            c_gradient = 0\n            m1_gradient = 0\n            m2_gradient = 0\n            m3_gradient = 0\n            for j in range(n):\n                m1_gradient += 2*(m1*x1[j] +m2*x2[j] +m3*x3[j] + c - y[j])*x1[j]/n\n                m2_gradient += 2*(m1*x1[j] +m2*x2[j] +m3*x3[j] + c - y[j])*x2[j]/n\n                m3_gradient += 2*(m1*x1[j] +m2*x2[j] +m3*x3[j] + c - y[j])*x3[j]/n\n                c_gradient += 2*(m1*x1[j] +m2*x2[j] +m3*x3[j] + c - y[j])/n\n            m1 = m1 - (learning_rate * m1_gradient)\n            m2 = m2 - (learning_rate * m2_gradient)\n            m3 = m3 - (learning_rate * m3_gradient)\n            c = c - (learning_rate * c_gradient)\n        return m1,m2,m3,c\n\n\n    m1,m2,m3,c = gradient_descent(m1,m2,m2,c)\n\n    def expect_value (p,q,r):\n        y1=m1*p+m2*q+m3*r+c\n        if (y1>100): ## since marks scored cannot be greater than 100\n            return ('Marks scored :- 100')\n        else:\n            return print('Marks scored :- ',y1)\n\n\n    expect_value(p,q,r)\n\nelse:\n    print('Impossible time distribution')\n"""
users/Sourav/logistic regression/multivariate logistic regression.py,5,"b""import numpy as np\n## This logistic approach is used to determine the percentage possibility of rain on the factors such as temperature,wind speed and relative humidity\n\nx1 = np.array([30,33,29,23,24,26,30,35,31,29,31,28,30,34]) ##temperature(in Celcius)\nx2 = np.array([11,12,10,14,8,9,11,9,10,11,12,14,13,10]) ##wind speed(in km/h)\nx3 = np.array([63,60,67,75,72,80,89,79,60,65,70,85,88,75])## relative humidity (in percentage)\nn=len(x1)\ny = np.array([0,0,1,1,1,1,1,0,0,1,0,1,0,0]) ## 1 depicts it rained,0 depicts it didn't\n\ndef sigmoid(x):\n    return 1/(1+np.exp(-x))\n\nm1=12\nm2=12\nm3=12\nc=12\nlearning_rate = 0.001\np=int(input('Enter temperature in Celcius :-  '))\nq=int(input('Enter wind speed in km/h :-  '))\nr=int(input('relative humidity in percentage :-  '))\n\n\n\ndef gradient_descent(m1,m2,m3,c):\n    for i in range(10000):\n        c_gradient=0\n        m1_gradient=0\n        m2_gradient=0\n        m3_gradient=0\n        for j in range(n):\n            c_gradient += (sigmoid(m1*x1[j]+m2*x2[j]+m3*x3[j]) - y[j])\n            m1_gradient += (sigmoid(m1*x1[j]+m2*x2[j]+m3*x3[j]) - y[j])*x1[j]\n            m2_gradient += (sigmoid(m1*x1[j]+m2*x2[j]+m3*x3[j]) - y[j])*x2[j]\n            m3_gradient += (sigmoid(m1*x1[j]+m2*x2[j]+m3*x3[j]) - y[j])*x3[j]\n        c -= learning_rate*c_gradient\n        m1 -= learning_rate*m1_gradient\n        m2 -= learning_rate*m2_gradient\n        m3 -= learning_rate*m3_gradient\n    return m1,m2,m3,c\n\nm1,m2,m3,c=gradient_descent(m1,m2,m3,c)\n\ndef expect_value(p,q,r):\n    y1=sigmoid(m1*p+m2*q+m3*r+c)\n    return print('The percentage probabilty that it will rain is :-',y1*100)\n\nexpect_value(p,q,r)\n"""
users/dhruvds9/Task 1/Logistic_Regression.py,3,"b'import numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.array([17,23,31,21,20,21,24,22,31,41,26,21,33,38,16,27,28,17,17,21,23,32,26,16])\ny = np.array([1,1,0,1,1,1,0,0,0,0,0,1,0,0,1,1,0,1,1,1,1,0,1,1])\n\nw = 0\nb = 0\n\ndef func(x,w,b):\n    return (w*x+b)\n\ndef sigmoid (x,w,b):\n    return(1/(1+ np.exp(-func(x,w,b))))\n\ndef gradient_descent (x,y,w,b,iterations,alpha,temp):\n\tfor i in range (iterations):\n\t\tw_gradient = 0\n\t\tb_gradient = 0 \n\t\tfor j in range (len(x)):\n\t\t\tb_gradient = b_gradient + alpha*((2*(sigmoid(x[j],w,b) - y[j]))/len(x))\n\t\t\tw_gradient = w_gradient + alpha*((2*((sigmoid(x[j],w,b) - y[j])*x[j]))/len(x))\n\t\tw = w - w_gradient\n\t\tb = b - b_gradient\n\treturn sigmoid(temp,w,b)\n   \ntemp = int(input(""Enter the temp : ""))\n\nprint(""There is a "",str(round(gradient_descent(x,y,w,b,1000,0.001,temp)*100)),""% chance that it will rain"")\n'"
users/reallyinvincible/Task_1/LogisticRegression.py,1,"b""import numpy as np\nimport matplotlib.pyplot as plt\n\nb0, b1 = 0, 0\n\ndef sigmoid(x):\n    return round(1/(1 + np.exp(-x)), 5)\n\ndef train(age, status, alpha, iterations):\n    global b0, b1\n    for i in range(iterations):\n        correction1 = 0\n        correction2 = 0\n        for j in range(len(age)):\n            correction1 += (sigmoid(round(b0+(b1*age[j]), 5)) - status[j])\n            correction2 += (sigmoid(round(b0+(b1*age[j]), 5)) - status[j]) * age[j]\n        b0 = b0 - (alpha * correction1)\n        b1 = b1 - (alpha * correction2)\n        #Uncomment the next line to see how the machine is learning\n        #print(b0, b1)\n\ndef data_acquire():\n    file = open('titanic.csv')\n    status, age = [], []\n    status = []\n    age = []\n    for i in file:\n        data = i.split(',')\n        if(data[6] != '' and 'Pass' not in data[0]):\n            status.append(int(data[1]))\n            age.append(round(float(data[6])))\n    return age, status\n\ndef predict():\n    user = float(input('\\nEnter your age to check your probability of survival on board Titanic\\n'))\n    print('There is ' + str(sigmoid(b0 + user * b1)*100) + '% chance of your survival on board Titanic')\n    choice = input('\\nWanna Try another prediction(Y/N)\\n')\n    if choice == 'Y':\n        predict()\n    else:\n        quit()\n\nif  __name__ == '__main__':\n    print('Acquiring data')\n    age, status = data_acquire()\n    print('Data Acquisition Complete ')\n    alpha = 0.00001\n    #Uncomment to try your own values\n    #alpha = float(input('\\nEnter the learning rate\\n'))\n    iterations = 10000\n    #iterations = int(input('\\nEnter the number of time the model should be trained\\n'))\n    print('Training started')\n    train(age, status, alpha, iterations)\n    print('Training Complete')\n    print('Get ready to predict')\n    plt.plot([i for i in range(0, 50)], [sigmoid(b0 + b1 * i) for i in range(50)], color = 'r')\n\n    plt.show()\n    predict()\n\n"""
users/ritvikkolhe/Task 1/Logistic_Regression.py,3,"b'import numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\nx=np.array([10,1,5,6,2,4,6,3,7,6,0,4,2,3,6,4,5,9,1,2,8,7]) #Hours studied\ny=np.array([1,0,1,1,0,1,1,0,1,1,0,1,0,1,1,0,0,1,0,1,1,1]) #Pass=1 and Fail=0\n\nplt.scatter(x,y,color=""r"",marker=""*"",s=100)\n\nW=0\nB=0\nalpha = 0.01\n\ndef Sigmoid(z):\n    G_Z = float(1.0 / float((1.0 + math.exp(-1.0*z))))\n    return G_Z\n    \ndef Gradient_Descent(a,b,x,y,hours):\n    for i in range(10000):\n        gradient1=0\n        gradient2=0\n        for j in range(len(x)):\n            gradient1+=((Sigmoid(a+(b*x[j]))-y[j]))\n            gradient2+=((Sigmoid(a+(b*x[j]))-y[j])*x[j])\n        a-=(alpha*gradient1)\n        b-=(alpha*gradient2)\n    return a,b,Sigmoid((b*hours)+a)\n\ndef graph(a,b):\n    y=[]\n    x=np.array(range(0,12))\n    for i in x:\n        y.append(Sigmoid(b*i+a))\n    plt.plot(x,y)\n    plt.show()\n\nhr=int(input(\'Enter no. of hours studied: \'))\nW,B,percentage=Gradient_Descent(W,B,x,y,hr)\nprint(\'\\nThere is \'+str(round(percentage*100,2))+\'% chance that you will pass in the exam.\')\n\ngraph(W,B)\n'"
users/srivatsanrr/ml_02/logistcregressor.py,0,"b'# -*- coding: utf-8 -*-\n""""""\nCreated on Sun Feb  4 11:08:26 2018\n\n@author: Srivatsan\nLogReg using numpy\n""""""\nfrom numpy import *\nimport matplotlib.pyplot as plt\nalpha=0.001\nl=w=c=0\nitern=20000   \ndef hyplin(x,w,c):\n    return (float(w*x+c))\n\ndef sigmoid(var,w,c):\n    return float(1/(1+exp(-hyplin(var,w,c))))\n    \ndef graddec(x, y, w, c,m, itern, alpha):\n    for j in range(itern):\n        for i in range(m):\n            w=w- (alpha*(2/m)*(sigmoid(x[i],w,c)-y[i])*x[i])\n            c=c- (alpha*(2/m)*(sigmoid(x[i],w,c)-y[i]))\n    return [w, c]\n\ndef predict(w, c): #predicts output for given input after classification\n    hours= float(input())\n    t= sigmoid(hours,w,c)\n    \n    if t>=0.5:\n        print (""Pass"")\n    else:\n        print (""Fail"")\n    return t\n\npoints = genfromtxt(""exam.csv"", delimiter="","")\nx= points[...,1]\ny= points[...,2]\nm=len(x)\n[w, c]= graddec(x, y,w,c,m, itern, alpha)\nt=predict(w,c)\nprint(w,c, t) #Prints values of theta0, theta1 and preticted probability\nt=[t for i in range(m)]\nx1=arange(len(t))\nplt.plot(x1,t)\nplt.scatter(x, y, color = ""m"", marker = ""o"", s = 30)\nplt.xlabel(""Number of hours of study"")\nplt.ylabel(""Probability of pass"")\nplt.show()\n'"
