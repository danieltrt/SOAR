file_path,api_count,code
create_face_model.py,2,"b'#!/usr/bin/python\nimport cv2\nimport os\nimport sys\nimport numpy as np\nfrom PIL import Image\nimport imutils\nimport argparse\n\ncascadePath = ""face_cascades/haarcascade_profileface.xml""\nfaceCascade = cv2.CascadeClassifier(cascadePath)\nrecognizer = cv2.face.LBPHFaceRecognizer_create()\n\n\ndef get_images_and_labels(path):\n    """"""\n    convert images to matrices\n    assign label to every image according to person\n    Using test data to make the machine learn this data\n    Args:\n            path: path to images directory\n\n    Returns:\n    matrix of images, labels\n    """"""\n    i = 0\n    image_paths = [os.path.join(path, f)\n                   for f in os.listdir(path) if not f.endswith(\'.sad\')]\n    images = []\n    labels = []\n    for image_path in image_paths:\n        image_pil = Image.open(image_path).convert(\'L\')\n        image = np.array(image_pil, \'uint8\')\n        image = imutils.resize(image, width=min(500, image.shape[1]))\n        nbr = int(os.path.split(image_path)[1].split(\n            ""."")[0].replace(""subject"", """"))\n        faces = faceCascade.detectMultiScale(image)\n        for (x, y, w, h) in faces:\n            images.append(image[y: y + h, x: x + w])\n            # cv2.imwrite(""subject02.""+str(i)+"".jpg"",image[y: y + h, x: x + w])\n            # i=i+1\n            labels.append(nbr)\n            cv2.imshow(""Adding faces to traning set"",\n                       image[y: y + h, x: x + w])\n            cv2.imshow(\'win\', image[y: y + h, x: x + w])\n            cv2.waitKey(50)\n    return images, labels\n\n\nap = argparse.ArgumentParser()\nap.add_argument(""-i"", ""--images"", required=True,\n                help=""path to images directory"")\nargs = vars(ap.parse_args())\npath = args[""images""]\nimages, labels = get_images_and_labels(path)\ncv2.destroyAllWindows()\n\nrecognizer.train(images, np.array(labels))\n""""""\nsave the trained data to cont.yaml file\n""""""\nrecognizer.save(""model.yaml"")\n'"
main.py,0,"b'import argparse\nimport glob\nimport os\nimport time\n\nimport cv2\nimport imutils\nfrom imutils.object_detection import non_max_suppression\n\nsubject_label = 1\nfont = cv2.FONT_HERSHEY_SIMPLEX\nlist_of_videos = []\ncascade_path = ""face_cascades/haarcascade_profileface.xml""\nface_cascade = cv2.CascadeClassifier(cascade_path)\nhog = cv2.HOGDescriptor()\nhog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\nrecognizer = cv2.face.LBPHFaceRecognizer_create()\ncount = 0\n\n\ndef detect_people(frame):\n    """"""\n    detect humans using HOG descriptor\n    Args:\n        frame:\n    Returns:\n        processed frame\n    """"""\n    (rects, weights) = hog.detectMultiScale(frame, winStride=(8, 8), padding=(16, 16), scale=1.06)\n    rects = non_max_suppression(rects, probs=None, overlapThresh=0.65)\n    for (x, y, w, h) in rects:\n        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n    return frame\n\n\ndef detect_face(frame):\n    """"""\n    detect human faces in image using haar-cascade\n    Args:\n        frame:\n    Returns:\n    coordinates of detected faces\n    """"""\n    faces = face_cascade.detectMultiScale(frame, 1.1, 2, 0, (20, 20))\n    return faces\n\n\ndef recognize_face(frame_orginal, faces):\n    """"""\n    recognize human faces using LBPH features\n    Args:\n        frame_orginal:\n        faces:\n    Returns:\n        label of predicted person\n    """"""\n    predict_label = []\n    predict_conf = []\n    for x, y, w, h in faces:\n        frame_orginal_grayscale = cv2.cvtColor(frame_orginal[y: y + h, x: x + w], cv2.COLOR_BGR2GRAY)\n        cv2.imshow(""cropped"", frame_orginal_grayscale)\n        predict_tuple = recognizer.predict(frame_orginal_grayscale)\n        a, b = predict_tuple\n        predict_label.append(a)\n        predict_conf.append(b)\n        print(""Predition label, confidence: "" + str(predict_tuple))\n    return predict_label\n\n\ndef draw_faces(frame, faces):\n    """"""\n    draw rectangle around detected faces\n    Args:\n        frame:\n        faces:\n    Returns:\n    face drawn processed frame\n    """"""\n    for (x, y, w, h) in faces:\n        xA = x\n        yA = y\n        xB = x + w\n        yB = y + h\n        cv2.rectangle(frame, (xA, yA), (xB, yB), (0, 255, 0), 2)\n    return frame\n\n\ndef put_label_on_face(frame, faces, labels):\n    """"""\n    draw label on faces\n    Args:\n        frame:\n        faces:\n        labels:\n    Returns:\n        processed frame\n    """"""\n    i = 0\n    for x, y, w, h in faces:\n        cv2.putText(frame, str(labels[i]), (x, y), font, 1, (255, 255, 255), 2)\n        i += 1\n    return frame\n\n\ndef background_subtraction(previous_frame, frame_resized_grayscale, min_area):\n    """"""\n    This function returns 1 for the frames in which the area\n    after subtraction with previous frame is greater than minimum area\n    defined.\n    Thus expensive computation of human detection face detection\n    and face recognition is not done on all the frames.\n    Only the frames undergoing significant amount of change (which is controlled min_area)\n    are processed for detection and recognition.\n    """"""\n    frameDelta = cv2.absdiff(previous_frame, frame_resized_grayscale)\n    thresh = cv2.threshold(frameDelta, 25, 255, cv2.THRESH_BINARY)[1]\n    thresh = cv2.dilate(thresh, None, iterations=2)\n    im2, cnts, hierarchy = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    temp = 0\n    for c in cnts:\n        # if the contour is too small, ignore it\n        if cv2.contourArea(c) > min_area:\n            temp = 1\n    return temp\n\n\nif __name__ == \'__main__\':\n    """"""\n    main function\n    """"""\n    ap = argparse.ArgumentParser()\n    ap.add_argument(""-v"", ""--videos"", required=True, help=""path to videos directory"")\n    args = vars(ap.parse_args())\n    path = args[""videos""]\n    for f in os.listdir(path):\n        list_of_videos = glob.glob(os.path.join(os.path.abspath(path), f))\n        print(os.path.join(os.path.abspath(path), f) + ""*.mp4"")\n        print(list_of_videos)\n        if os.path.exists(""model.yaml""):\n            recognizer.read(""model.yaml"")\n            for video in list_of_videos:\n                camera = cv2.VideoCapture(os.path.join(path, video))\n                grabbed, frame = camera.read()\n                print(frame.shape)\n                frame_resized = imutils.resize(frame, width=min(800, frame.shape[1]))\n                frame_resized_grayscale = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)\n                print(frame_resized.shape)\n\n                # defining min cuoff area\n                min_area = (3000 / 800) * frame_resized.shape[1]\n\n                while True:\n                    starttime = time.time()\n                    previous_frame = frame_resized_grayscale\n                    grabbed, frame = camera.read()\n                    if not grabbed:\n                        break\n                    frame_resized = imutils.resize(frame, width=min(800, frame.shape[1]))\n                    frame_resized_grayscale = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)\n                    temp = background_subtraction(previous_frame, frame_resized_grayscale, min_area)\n                    if temp == 1:\n                        frame_processed = detect_people(frame_resized)\n                        faces = detect_face(frame_resized_grayscale)\n                        if len(faces) > 0:\n                            frame_processed = draw_faces(frame_processed, faces)\n                            label = recognize_face(frame_resized, faces)\n                            frame_processed = put_label_on_face(frame_processed, faces, label)\n\n                        cv2.imshow(""Detected Human and face"", frame_processed)\n                        key = cv2.waitKey(1) & 0xFF\n                        if key == ord(""q""):\n                            break\n                        endtime = time.time()\n                        print(""Time to process a frame: "" + str(starttime - endtime))\n                    else:\n                        count = count + 1\n                        print(""Number of frame skipped in the video= "" + str(count))\n\n                camera.release()\n                cv2.destroyAllWindows()\n\n\n        else:\n            print(""model file not found"")\n        list_of_videos = []\n'"
scripts/LOG_Filter_video.py,2,"b'import numpy as np\nimport cv2\nimport imutils\nimport sys\nfrom collections import deque\nfrom scipy.misc import imread\nimport argparse\nfrom scipy import signal\n# use argument parser to parse arguments such as video\nap = argparse.ArgumentParser()\nap.add_argument(""-v"", ""--video"",\n                help=""path to the (optional) video file"")\nap.add_argument(""-b"", ""--buffer"", type=int, default=64,\n                help=""max buffer size"")\nargs = vars(ap.parse_args())\n# max buffer size for video\npts = deque(maxlen=args[""buffer""])\n# if no arguments were passed then use a webcam\nif not args.get(""video"", False):\n    camera = cv2.VideoCapture(0)\nelse:\n    camera = cv2.VideoCapture(args[""video""])\nwhile True:\n    # grab the current frame\n    (grabbed, image2) = camera.read()\n\n    # if we are viewing a video and we did not grab a frame,\n    # then we have reached the end of the video\n    if args.get(""video"") and not grabbed:\n        break\n    # resize the image to precess faster\n    image2 = imutils.resize(image2, height=500)\n    # convert to grayscale\n    image1 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n    image = imutils.resize(image1, height=500)\n    # declaring a gaussian filter of 5*5\n    gaussian = np.ones((5, 5), np.float32) / 25\n    # declaring a laplacian filter\n    laplacian = np.array([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]])\n    # applying laplacian and gaussian filters\n    dst = cv2.filter2D(image, -1, gaussian)\n    dst1 = cv2.filter2D(dst, -1, laplacian)\n    # invert the image\n    dst1 = (255 - dst1)\n    # blurring the image to get only prominent edges\n    th2 = cv2.filter2D(dst1, -1, gaussian)\n    # applying adaptive threshold to increase intensity of prominent edges and\n    # making the image as binary image\n    th3 = cv2.adaptiveThreshold(\n        th2, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n    # applying the edges as mask on original frame\n    res1 = cv2.bitwise_and(image2, image2, mask=th3)\n    cv2.imshow(\'wine\', res1)\n    key = cv2.waitKey(1) & 0xFF\n    if key == ord(""q""):\n        break\ncamera.release()\ncv2.destroyAllWindows()\n'"
scripts/LOG_edge_detection.py,0,"b""import cv2\nimport numpy as np\nimport imutils\n# from matplotlib import pyplot as plt\nimport sys\n# loading image\n#img0 = cv2.imread('SanFrancisco.jpg',)\nimg2 = cv2.imread(sys.argv[1],)\nimg0 = imutils.resize(img2, height=500)\n# converting to gray scale\ngray = cv2.cvtColor(img0, cv2.COLOR_BGR2GRAY)\n\n# remove noise\nimg = cv2.bilateralFilter(gray, 9, 75, 75)\n# img = cv2.GaussianBlur(gray,(3,3),0)\n# convolute with proper kernels\nlaplacian = cv2.Laplacian(img, cv2.CV_8U)\nsobelx = cv2.Sobel(img, cv2.CV_8U, 1, 0, ksize=5)  # x\nsobely = cv2.Sobel(img, cv2.CV_8U, 0, 1, ksize=5)  # y\nedged = cv2.Canny(img, 30, 200)\n# cv2.imshow('new',edged)\n# cv2.waitKey(0)\n# plt.subplot(2,2,1),plt.imshow(img,cmap = 'gray')\n# plt.title('Original'), plt.xticks([]), plt.yticks([])\n# plt.subplot(2,2,2),plt.imshow(laplacian,cmap = 'gray')\n# plt.title('Laplacian'), plt.xticks([]), plt.yticks([])\n# plt.subplot(2,2,3),plt.imshow(sobelx,cmap = 'gray')\n# plt.title('Sobel X'), plt.xticks([]), plt.yticks([])\n# plt.subplot(2,2,4),plt.imshow(sobely,cmap = 'gray')\n\n# plt.title('Sobel Y'), plt.xticks([]), plt.yticks([])\n(_, cnts, _) = cv2.findContours(\n    edged.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\nprint(cnts)\n# plt.show()\n# On this output, draw all of the contours that we have detected\n# in white, and set the thickness to be 3 pixels\ncv2.drawContours(img0, cnts, -1, 255, 2)\n\n# Spawn new windows that shows us the donut\n# (in grayscale) and the detected contour\ncv2.imshow('Output Contour', img0)\n\n# Wait indefinitely until you push a key.  Once you do, close the windows\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n"""
scripts/LOG_image.py,2,"b""import numpy as np\nimport cv2\nimport imutils\nimport sys\nfrom scipy.misc import imread\nfrom scipy import signal\nimage2 = cv2.imread(sys.argv[1],)\nimage2 = imutils.resize(image2, height=500)\ncv2.imshow('image', image2)\ncv2.waitKey(0)\nimage1 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\nimage = imutils.resize(image1, height=500)\ncv2.imshow('gdh', image)\ncv2.waitKey(0)\ngaussian = np.ones((5, 5), np.float32) / 25\nlaplacian = np.array([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]])\ndst = cv2.filter2D(image, -1, gaussian)\ncv2.imshow('dsd', dst)\ncv2.waitKey(0)\ndst1 = cv2.filter2D(dst, -1, laplacian)\ncv2.imshow('jh', dst1)\ncv2.waitKey(0)\n# (_,cnts, _) = cv2.findContours(dst1.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n# cv2.drawContours(imag,e cnts, -1, 255, 2)\n# cv2.imshow('hello',image)\n# cv2.waitKey(0)\n# print(dst1)\ndst1 = (255 - dst1)\ncv2.imshow('dhgf', dst1)\ncv2.waitKey(0)\nres = cv2.bitwise_and(image2, image2, mask=dst1)\ncv2.imshow('win', res)\ncv2.waitKey(0)\nprint(dst1.shape)\nth2 = cv2.filter2D(dst1, -1, gaussian)\nth3 = cv2.adaptiveThreshold(\n    th2, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\ncv2.imshow('wind', th3)\ncv2.waitKey(0)\n# kernal = cv2.getStructuringElement(cv2.MORPH_RECT,(7,7))\n# closed = cv2.morphologyEx(th3,cv2.MORPH_CLOSE,kernal)\n# cv2.imshow('win3',closed)\n# cv2.waitKey(0)\nres1 = cv2.bitwise_and(image2, image2, mask=th3)\ncv2.imshow('final', res1)\ncv2.waitKey(0)\n# a = 0\n# b = 0\n# count = 0\n# for i in th3:\n# \tfor j in i:\n# \t\tif j == 0:\n# \t\t\tcount = count+1\n# \t\t\timage2[i][j] = [0,255,0]\n# \ta = a+1\n# print(count)\n# cv2.imshow('final',image2)\n# cv2.waitKey(0)\n"""
scripts/LOG_image1.py,2,"b""import numpy as np\nimport cv2\n\n\nimport imutils\nimport sys\nfrom scipy.misc import imread\nfrom scipy import signal\nimage2 = cv2.imread(sys.argv[1],)\nimage2 = imutils.resize(image2, height=500)\ncv2.imshow('image', image2)\ncv2.waitKey(0)\nimage1 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\nimage = imutils.resize(image1, height=500)\ncv2.imshow('gdh', image)\ncv2.waitKey(0)\ngaussian = np.ones((5, 5), np.float32) / 25\nlaplacian = np.array([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]])\ndst = cv2.filter2D(image, -1, gaussian)\ncv2.imshow('dsd', dst)\ncv2.waitKey(0)\ndst1 = cv2.filter2D(dst, -1, laplacian)\ncv2.imshow('jh', dst1)\ncv2.waitKey(0)\n# invert image\ndst1 = (255 - dst1)\ncv2.imshow('dhgf', dst1)\ncv2.waitKey(0)\nth2 = cv2.filter2D(dst1, -1, gaussian)\nth3 = cv2.adaptiveThreshold(\n    th2, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 15, 2)\ncv2.imshow('wind', th3)\ncv2.waitKey(0)\nres1 = cv2.bitwise_and(image2, image2, mask=th3)\ncv2.imwrite('processed_img.jpeg', res1)\ncv2.imshow('wine', res1)\ncv2.waitKey(0)\n_, th4, _ = cv2.findContours(th3, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\ncv2.drawContours(image2, th4, -1, 255, 2)\ncv2.imshow('window', image2)\ncv2.waitKey(0)\noutput = cv2.connectedComponentsWithStats(th3, 4, cv2.CV_32S)\n# Get the results\n# The first cell is the number of labels\nnum_labels = output[0]\n# The second cell is the label matrix\nlabels = output[1]\n# The third cell is the stat matrix\nstats = output[2]\n# The fourth cell is the centroid matrix\ncentroids = output[3]\nprint(num_labels)\nprint(labels)\nprint(stats)\nprint(centroids)\n# (_,cnts, _) = cv2.findContours(th3.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n# cv2.drawContours(image2, cnts, -1, 255, 2)\n# cv2.imshow('win4',image2)\n# cv2.waitKey(0)\n"""
scripts/LOG_image2.py,2,"b'import numpy as np\nimport cv2\nimport imutils\nimport sys\nfrom collections import deque\nfrom scipy.misc import imread\nimport argparse\nfrom scipy import signal\nap = argparse.ArgumentParser()\nap.add_argument(""-v"", ""--video"",\n                help=""path to the (optional) video file"")\nap.add_argument(""-b"", ""--buffer"", type=int, default=64,\n                help=""max buffer size"")\nargs = vars(ap.parse_args())\npts = deque(maxlen=args[""buffer""])\nif not args.get(""video"", False):\n    camera = cv2.VideoCapture(0)\nelse:\n    camera = cv2.VideoCapture(args[""video""])\nwhile True:\n    # grab the current frame\n    (grabbed, image2) = camera.read()\n\n    # if we are viewing a video and we did not grab a frame,\n    # then we have reached the end of the video\n    if args.get(""video"") and not grabbed:\n        break\n    image2 = imutils.resize(image2, height=500)\n    image1 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n    image = imutils.resize(image1, height=500)\n    gaussian = np.ones((5, 5), np.float32) / 25\n    laplacian = np.array([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]])\n    dst = cv2.filter2D(image, -1, gaussian)\n    dst1 = cv2.filter2D(dst, -1, laplacian)\n    # invert image\n    dst1 = (255 - dst1)\n    th2 = cv2.filter2D(dst1, -1, gaussian)\n    th3 = cv2.adaptiveThreshold(\n        th2, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n    res1 = cv2.bitwise_and(image2, image2, mask=th3)\n    cv2.imshow(\'wine\', res1)\n    key = cv2.waitKey(1) & 0xFF\n    if key == ord(""q""):\n        break\ncamera.release()\ncv2.destroyAllWindows()\n'"
scripts/SURF.py,0,"b""import cv2\nimport imutils\nimport sys\nimg = cv2.imread(sys.argv[1])\nimg = imutils.resize(img, width=500, height=500)\nsurf = cv2.xfeatures2d.SURF_create(500)\nkp, des = surf.detectAndCompute(img, None)\nf = open('a.txt', 'w')\nprint(kp)\nprint(len(kp))\nimg2 = cv2.drawKeypoints(img, kp, None, (255, 0, 0), 4)\ncv2.imshow('window', img2)\ncv2.waitKey(0)\n"""
scripts/car_detect.py,0,"b'import cv2\nimport sys\nimport os\nimport imutils\nfrom imutils.object_detection import non_max_suppression\n\n# The locatio of the image and the cascade file path\n# imagePath =\'/home/unnirajendran/Desktop/face_detect/image1.jpg\'\n# cascPath = \'/home/unnirajendran/Desktop/face_detect/haarcascade_frontalface_default.xml\'\nimagePath = sys.argv[1]\ncascPath = sys.argv[2]\n# Create the haar cascade\ncar_cascade = cv2.CascadeClassifier(cascPath)\n\n# Read the image\nimage = cv2.imread(imagePath)\n\n# Resize the image so it fits in the screen\nimage1 = imutils.resize(image, height=500)\ngray = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n\n# Detect faces in the image\nfaces = car_cascade.detectMultiScale(\n    gray,\n    scaleFactor=1.1,\n    minNeighbors=5,\n    minSize=(30, 30),\n    # flags = cv2.cv.CV_HAAR_SCALE_IMAGE\n    flags=0\n)\nface = non_max_suppression(faces, probs=None, overlapThresh=0.3)\nif format(len(faces)) == 1:\n    print(""Found {0} face!"".format(len(faces)))\nelse:\n    print(""Found {0} faces!"".format(len(faces)))\n\n# Draw a rectangle around the faces\nfor (x, y, w, h) in face:\n    cv2.rectangle(image1, (x, y), (x + w, y + h), (0, 255, 0), 2)\n\ncv2.imshow(""Faces found"", image1)\ncv2.waitKey(0)\n'"
scripts/compare_histograms.py,0,"b""import cv2\nfrom matplotlib import pyplot as plt\n\nimg = cv2.imread('data/images/image1.jpg')\ncolor = ('b', 'g', 'r')\nfor i, col in enumerate(color):\n    histr = cv2.calcHist([img], [i], None, [256], [0, 256])\n    plt.plot(histr, color=col)\n    plt.xlim([0, 256])\nplt.show()\n"""
scripts/detect_face.py,0,"b'import cv2\nimport sys\nimport os\nimport imutils\n\n# The locatio of the image and the cascade file path\n# imagePath =\'/home/unnirajendran/Desktop/face_detect/image1.jpg\'\n# cascPath = \'/home/unnirajendran/Desktop/face_detect/haarcascade_frontalface_default.xml\'\nimagePath = sys.argv[1]\ncascPath = sys.argv[2]\n# Create the haar cascade\nfaceCascade = cv2.CascadeClassifier(cascPath)\n\n# Read the image\nimage = cv2.imread(imagePath)\n\n# Resize the image so it fits in the screen\nimage1 = imutils.resize(image, height=500)\ngray = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n\n# Detect faces in the image\nfaces = faceCascade.detectMultiScale(\n    gray,\n    scaleFactor=1.1,\n    minNeighbors=5,\n    minSize=(30, 30),\n    #flags = cv2.cv.CV_HAAR_SCALE_IMAGE\n    flags=0\n)\n\nif format(len(faces)) == 1:\n    print(""Found {0} face!"".format(len(faces)))\nelse:\n    print(""Found {0} faces!"".format(len(faces)))\n\n# Draw a rectangle around the faces\nfor (x, y, w, h) in faces:\n    cv2.rectangle(image1, (x, y), (x + w, y + h), (0, 255, 0), 2)\n\ncv2.imshow(""Faces found"", image1)\ncv2.waitKey(0)\n'"
scripts/donut.py,1,"b""import cv2  # Import OpenCV\nimport numpy as np  # Import NumPy\nimport sys\n\n# Read in the image as grayscale - Note the 0 flag\nim = cv2.imread(sys.argv[1], 0)\n\n# Run findContours - Note the RETR_EXTERNAL flag\n# Also, we want to find the best contour possible with CHAIN_APPROX_NONE\n_, contours, hierarchy = cv2.findContours(\n    im, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n# Create an output of all zeroes that has the same shape as the input\n# image\nout = np.zeros_like(im)\n\n# On this output, draw all of the contours that we have detected\n# in white, and set the thickness to be 3 pixels\ncv2.drawContours(out, contours, -1, 255, 2)\n\n# Spawn new windows that shows us the donut\n# (in grayscale) and the detected contour\ncv2.imshow('Donut', im)\ncv2.imshow('Output Contour', out)\n\n# Wait indefinitely until you push a key.  Once you do, close the windows\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n"""
scripts/extract_image.py,0,"b'import os\nimport sys\n\nimport Surv_final\nimport cv2\n\npath = ""/home/arpit/Projects/Survelliance_System/data/extra2/""\ncamera = cv2.VideoCapture(sys.argv[1])\ni = 12000\nwhile True:\n    grabbed, frame = camera.read()\n    if not grabbed:\n        break\n    rects = Surv_final.detect_face(frame)\n    if len(rects) > 0:\n        for x, y, w, h in rects:\n            cropped = frame[y:y + h, x:x + w]\n            cv2.imwrite(os.path.join(path, str(i) + "".jpg""), cropped)\n        # for i in range(1,5):\n        # \tframe = camera.read()\n    i += 1\n'"
scripts/face_detect.py,0,"b'import sys\n\nimport cv2\nimport imutils\n\n# The locatio of the image and the cascade file path\nimagePath = sys.argv[1]\ncascPath = \'cascade.xml\'\n\n# Create the haar cascade\nfaceCascade = cv2.CascadeClassifier(cascPath)\n\n# Read the image\nimage = cv2.imread(imagePath)\n\n# Resize the image so it fits in the screen\nimage1 = imutils.resize(image, height=500)\ngray = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n\n# Detect faces in the image\nfaces = faceCascade.detectMultiScale(\n    gray,\n    scaleFactor=1.1,\n    minNeighbors=5,\n    minSize=(30, 30),\n    # flags = cv2.cv.CV_HAAR_SCALE_IMAGE\n    flags=0\n)\n\nif format(len(faces)) == 1:\n    print(""Found {0} face!"".format(len(faces)))\nelse:\n    print(""Found {0} faces!"".format(len(faces)))\n\n# Draw a rectangle around the faces\nfor (x, y, w, h) in faces:\n    cv2.rectangle(image1, (x, y), (x + w, y + h), (0, 255, 0), 2)\n\ncv2.imshow(""Faces found"", image1)\ncv2.waitKey(0)\n'"
scripts/face_detection_video.py,0,"b'import cv2\nimport sys\n\ncascPath = ""haarcascade_frontalface_default.xml""\nfaceCascade = cv2.CascadeClassifier(cascPath)\n\n# give the name of the input video file\n\ncap = cv2.VideoCapture(sys.argv[1])\n\nwhile cap.isOpened():\n    # Capture frame-by-frame\n    ret, frame = cap.read()\n\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\n    faces = faceCascade.detectMultiScale(\n        gray,\n        scaleFactor=1.1,\n        minNeighbors=5,\n        minSize=(30, 30),\n        # flags=cv2.cv.CV_HAAR_SCALE_IMAGE\n        flags=0\n    )\n\n    # Draw a rectangle around the faces\n    for (x, y, w, h) in faces:\n        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n\n    # Display the resulting frame\n    cv2.imshow(\'Video\', frame)\n\n    if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n        break\n\n# When everything is done, release the capture\ncap.release()\ncv2.destroyAllWindows()\n'"
scripts/face_rec1.py,0,"b'from time import time\nimport numpy\nimport os\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.decomposition import RandomizedPCA\nfrom sklearn.svm import SVC\nfrom PIL import Image\n\n# Path to the root image directory containing sub-directories of images\npath = ""/home/arpit/ATnT_data/""\ntestImage = ""/home/arpit/new/10.pgm""\n\n# Flat image Feature Vector\nX = []\n# Int array of Label Vector\nY = []\n\nn_sample = 0  # Total number of Images\nh = 112  # Height of image in float\nw = 92  # Width of image in float\nn_features = 187500  # Length of feature vector\ntarget_names = []  # Array to store the names of the persons\nlabel_count = 0\nn_classes = 0\n\nfor directory in os.listdir(path):\n    for file in os.listdir(path + directory):\n        print(path + directory + ""/"" + file)\n        img = Image.open(path + directory + ""/"" + file)\n        featurevector = numpy.array(img).flatten()\n        # print len(featurevector)\n        X.append(featurevector)\n        Y.append(label_count)\n        n_sample += 1\n    target_names.append(directory)\n    label_count += 1\n\n# print Y\n# print target_names\nn_classes = len(target_names)\n\n###############################################################################\n# Split into a training set and a test set using a stratified k fold\n\n# split into a training and teststing set\nX_train, X_test, y_train, y_test = train_test_split(\n    X, Y, test_size=0.25, random_state=42)\n\n###############################################################################\n# Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled\n# dataset): unsupervised feature extraction / dimensionality reduction\nn_components = 10\n\nprint(""Extracting the top %d eigenfaces from %d faces""\n      % (n_components, len(X_test)))\nt0 = time()\npca = RandomizedPCA(n_components=n_components, whiten=True).fit(X_train)\nprint(""done in %0.3fs"" % (time() - t0))\n\neigenfaces = pca.components_.reshape((n_components, h, w))\n\nprint(""Projecting the input data on the eigenfaces orthonormal basis"")\nt0 = time()\nX_train_pca = pca.transform(X_train)\nX_test_pca = pca.transform(X_test)\nprint(""done in %0.3fs"" % (time() - t0))\n\n###############################################################################\n# Train a SVM classification model\nprint(""Fitting the classifier to the training set"")\nt0 = time()\nparam_grid = {\'C\': [1e3, 5e3, 1e4, 5e4, 1e5],\n              \'gamma\': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\nclf = GridSearchCV(SVC(kernel=\'rbf\', class_weight=\'balanced\'), param_grid)\nclf = clf.fit(X_train_pca, y_train)\nprint(""done in %0.3fs"" % (time() - t0))\nprint(""Best estimator found by grid search:"")\nprint(clf.best_estimator_)\n\n###############################################################################\n# Quantitative evaluation of the model quality on the test set\n\nprint(""Predicting people\'s names on the test set"")\nt0 = time()\ny_pred = clf.predict(X_test_pca)\n# print clf.score(X_test_pca,y_test)\nprint(""done in %0.3fs"" % (time() - t0))\nprint(classification_report(y_test, y_pred, target_names=target_names))\nprint(confusion_matrix(y_test, y_pred, labels=range(n_classes)))\n\n###############################################################################\n\nA = []\nimage = Image.open(testImage)\nfeature_vec = numpy.array(image).flatten()\nA.append(feature_vec)\nA_pca = pca.transform(A)\nA_predict = clf.predict(A_pca)\nB = numpy.array(A)\nprint(B)\nprint(A_predict)\nprint(classification_report(B, A_predict))\n# Prediction of user based on the model\n# test = []\n# testImage=Image.open(testImage)\n# testImageFeatureVector=numpy.array(testImage).flatten()\n# test.append(testImageFeatureVector)\n# testImagePCA = pca.transform(test)\n# testImagePredict=clf.predict(testImagePCA)\n# print(clf.score(testImagePCA))\n# print(clf.score(X_train_pca,testImagePCA))\n# print(clf.best_params_)\n# print(clf.best_score_)\n# print(testImagePredict)\n# print(target_names[testImagePredict[0]])\n'"
scripts/face_rec_demo.py,11,"b'#!/usr/bin/env python\n# Software License Agreement (BSD License)\n#\n# Copyright (c) 2012, Philipp Wagner <bytefish[at]gmx[dot]de>.\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n#  * Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n#  * Redistributions in binary form must reproduce the above\n#    copyright notice, this list of conditions and the following\n#    disclaimer in the documentation and/or other materials provided\n#    with the distribution.\n#  * Neither the name of the author nor the names of its\n#    contributors may be used to endorse or promote products derived\n#    from this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n# ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS\n# FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE\n# COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,\n# INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,\n# BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN\n# ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n# POSSIBILITY OF SUCH DAMAGE.\n\n\nimport os\nimport sys\nimport cv2\nimport numpy as np\n\n\ndef normalize(X, low, high, dtype=None):\n    """"""Normalizes a given array in X to a value between low and high.""""""\n    X = np.asarray(X)\n    minX, maxX = np.min(X), np.max(X)\n    # normalize to [0...1].\n    X -= float(minX)\n    X /= float((maxX - minX))\n    # scale to [low...high].\n    X = X * (high - low)\n    X = X + low\n    if dtype is None:\n        return np.asarray(X)\n    return np.asarray(X, dtype=dtype)\n\n\ndef read_images(path, sz=None):\n    """"""Reads the images in a given folder, resizes images on the fly if size is given.\n\n    Args:\n        path: Path to a folder with subfolders representing the subjects (persons).\n        sz: A tuple with the size Resizes \n\n    Returns:\n        A list [X,y]\n\n            X: The images, which is a Python list of numpy arrays.\n            y: The corresponding labels (the unique number of the subject, person) in a Python list.\n    """"""\n    c = 0\n    X, y = [], []\n    for dirname, dirnames, filenames in os.walk(path):\n        for subdirname in dirnames:\n            subject_path = os.path.join(dirname, subdirname)\n            for filename in os.listdir(subject_path):\n                try:\n                    im = cv2.imread(os.path.join(\n                        subject_path, filename), cv2.IMREAD_GRAYSCALE)\n                    # resize to given size (if given)\n                    if sz is not None:\n                        im = cv2.resize(im, sz)\n                    X.append(np.asarray(im, dtype=np.uint8))\n                    y.append(c)\n                except IOError, (errno, strerror):\n                    print ""I/O error({0}): {1}"".format(errno, strerror)\n                except:\n                    print ""Unexpected error:"", sys.exc_info()[0]\n                    raise\n            c = c + 1\n    return [X, y]\n\nif __name__ == ""__main__"":\n    # This is where we write the images, if an output_dir is given\n    # in command line:\n    out_dir = None\n    # You\'ll need at least a path to your image data, please see\n    # the tutorial coming with this source code on how to prepare\n    # your image data:\n    if len(sys.argv) < 2:\n        print ""USAGE: facerec_demo.py </path/to/images> [</path/to/store/images/at>]""\n        sys.exit()\n    # Now read in the image data. This must be a valid path!\n    [X, y] = read_images(sys.argv[1])\n    # Convert labels to 32bit integers. This is a workaround for 64bit machines,\n    # because the labels will truncated else. This will be fixed in code as\n    # soon as possible, so Python users don\'t need to know about this.\n    # Thanks to Leo Dirac for reporting:\n    y = np.asarray(y, dtype=np.int32)\n    # If a out_dir is given, set it:\n    if len(sys.argv) == 3:\n        out_dir = sys.argv[2]\n    # Create the Eigenfaces model. We are going to use the default\n    # parameters for this simple example, please read the documentation\n    # for thresholding:\n    model = cv2.createEigenFaceRecognizer()\n    # Read\n    # Learn the model. Remember our function returns Python lists,\n    # so we use np.asarray to turn them into NumPy lists to make\n    # the OpenCV wrapper happy:\n    model.train(np.asarray(X), np.asarray(y))\n    # We now get a prediction from the model! In reality you\n    # should always use unseen images for testing your model.\n    # But so many people were confused, when I sliced an image\n    # off in the C++ version, so I am just using an image we\n    # have trained with.\n    #\n    # model.predict is going to return the predicted label and\n    # the associated confidence:\n    [p_label, p_confidence] = model.predict(np.asarray(X[0]))\n    # Print it:\n    print ""Predicted label = %d (confidence=%.2f)"" % (p_label, p_confidence)\n    # Cool! Finally we\'ll plot the Eigenfaces, because that\'s\n    # what most people read in the papers are keen to see.\n    #\n    # Just like in C++ you have access to all model internal\n    # data, because the cv::FaceRecognizer is a cv::Algorithm.\n    #\n    # You can see the available parameters with getParams():\n    print model.getParams()\n    # Now let\'s get some data:\n    mean = model.getMat(""mean"")\n    eigenvectors = model.getMat(""eigenvectors"")\n    # We\'ll save the mean, by first normalizing it:\n    mean_norm = normalize(mean, 0, 255, dtype=np.uint8)\n    mean_resized = mean_norm.reshape(X[0].shape)\n    if out_dir is None:\n        cv2.imshow(""mean"", mean_resized)\n    else:\n        cv2.imwrite(""%s/mean.png"" % (out_dir), mean_resized)\n    # Turn the first (at most) 16 eigenvectors into grayscale\n    # images. You could also use cv::normalize here, but sticking\n    # to NumPy is much easier for now.\n    # Note: eigenvectors are stored by column:\n    for i in xrange(min(len(X), 16)):\n        eigenvector_i = eigenvectors[:, i].reshape(X[0].shape)\n        eigenvector_i_norm = normalize(eigenvector_i, 0, 255, dtype=np.uint8)\n        # Show or save the images:\n        if out_dir is None:\n            cv2.imshow(""%s/eigenface_%d"" % (out_dir, i), eigenvector_i_norm)\n        else:\n            cv2.imwrite(""%s/eigenface_%d.png"" %\n                        (out_dir, i), eigenvector_i_norm)\n    # Show the images:\n    if out_dir is None:\n        cv2.waitKey(0)\n'"
scripts/face_recognition.py,3,"b'#!/usr/bin/python\n\n# Import the required modules\nimport cv2\nimport os\nimport numpy as np\nfrom PIL import Image\nimport imutils\n\nresolution = (100, 100)\nscaler = \'ANTIALIAS\'\nresample = {\n    \'ANTIALIAS\': Image.ANTIALIAS,\n    \'BILINEAR\': Image.BILINEAR,\n    \'BICUBIC\': Image.BICUBIC\n}\n# For face detection we will use the Haar Cascade provided by OpenCV.\ncascadePath = ""haarcascade_frontalface_default.xml""\nfaceCascade = cv2.CascadeClassifier(cascadePath)\n\n# For face recognition we will the the LBPH Face Recognizer\nrecognizer = cv2.face.createEigenFaceRecognizer(100)\n\n\ndef get_images_and_labels(path):\n    # Append all the absolute image paths in a list image_paths\n    # We will not read the image with the .sad extension in the training set\n    # Rather, we will use them to test our accuracy of the training\n    image_paths = [os.path.join(path, f)\n                   for f in os.listdir(path) if not f.endswith(\'.sad\')]\n    # images will contains face images\n    images = []\n    # labels will contains the label that is assigned to the image\n    labels = []\n    for image_path in image_paths:\n        print(image_path)\n        # Read the image and convert to grayscale\n        image_pil = Image.open(image_path).convert(\'L\')\n        image_pil.resize(resolution, resample[scaler])\n        print(image_pil)\n        # Convert the image format into numpy array\n        image = np.array(image_pil, \'uint8\')\n        # Get the label of the image\n        nbr = int(os.path.split(image_path)[1].split(\n            ""."")[0].replace(""subject"", """"))\n        # Detect the face in the image\n        faces = faceCascade.detectMultiScale(image)\n        # If face is detected, append the face to images and the label to\n        # labels\n        for (x, y, w, h) in faces:\n            images.append(image[y: y + h, x: x + w])\n            labels.append(nbr)\n            cv2.imshow(""Adding faces to traning set..."",\n                       image[y: y + h, x: x + w])\n            cv2.waitKey(50)\n    # return the images list and labels list\n    return images, labels\n\n\n# Path to the Yale Dataset\npath = \'./yalefaces\'\n# Call the get_images_and_labels function and get the face images and the\n# corresponding labels\nimages, labels = get_images_and_labels(path)\ncv2.destroyAllWindows()\n\n# Perform the tranining\nrecognizer.train(images, np.array(labels))\n\n# Append the images with the extension .sad into image_paths\nimage_paths = [os.path.join(path, f)\n               for f in os.listdir(path) if f.endswith(\'.sad\')]\nfor image_path in image_paths:\n    predict_image_pil = Image.open(image_path).convert(\'L\')\n    predict_image = np.array(predict_image_pil, \'uint8\')\n    faces = faceCascade.detectMultiScale(predict_image)\n    for (x, y, w, h) in faces:\n        nbr_predicted = recognizer.predict(predict_image[y: y + h, x: x + w])\n        print(nbr_predicted)\n        nbr_actual = int(os.path.split(image_path)[\n                         1].split(""."")[0].replace(""subject"", """"))\n        if nbr_actual == nbr_predicted:\n            print(""{} is Correctly Recognized with confidence {}"")\n        else:\n            print(""{} is Incorrect Recognized as {}"")\n        cv2.imshow(""Recognizing Face"", predict_image[y: y + h, x: x + w])\n        cv2.waitKey(0)\n'"
scripts/face_recognition1.py,0,"b'""""""\n===================================================\nFaces recognition example using eigenfaces and SVMs\n===================================================\n\nThe dataset used in this example is a preprocessed excerpt of the\n""Labeled Faces in the Wild"", aka LFW_:\n\n  http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz (233MB)\n\n.. _LFW: http://vis-www.cs.umass.edu/lfw/\n\nExpected results for the top 5 most represented people in the dataset:\n\n================== ============ ======= ========== =======\n                   precision    recall  f1-score   support\n================== ============ ======= ========== =======\n     Ariel Sharon       0.67      0.92      0.77        13\n     Colin Powell       0.75      0.78      0.76        60\n  Donald Rumsfeld       0.78      0.67      0.72        27\n    George W Bush       0.86      0.86      0.86       146\nGerhard Schroeder       0.76      0.76      0.76        25\n      Hugo Chavez       0.67      0.67      0.67        15\n       Tony Blair       0.81      0.69      0.75        36\n\n      avg / total       0.80      0.80      0.80       322\n================== ============ ======= ========== =======\n\n""""""\nfrom __future__ import print_function\n\nfrom time import time\nimport logging\nimport matplotlib.pyplot as plt\n\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.datasets import fetch_lfw_people\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.decomposition import RandomizedPCA\nfrom sklearn.svm import SVC\n\n\nprint(__doc__)\n\n# Display progress logs on stdout\nlogging.basicConfig(level=logging.INFO, format=\'%(asctime)s %(message)s\')\n\n\n###############################################################################\n# Download the data, if not already on disk and load it as numpy arrays\n\nlfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n\n# introspect the images arrays to find the shapes (for plotting)\nn_samples, h, w = lfw_people.images.shape\n\n# for machine learning we use the 2 data directly (as relative pixel\n# positions info is ignored by this model)\nX = lfw_people.data\nn_features = X.shape[1]\n\n# the label to predict is the id of the person\ny = lfw_people.target\ntarget_names = lfw_people.target_names\nn_classes = target_names.shape[0]\n\nprint(""Total dataset size:"")\nprint(""n_samples: %d"" % n_samples)\nprint(""n_features: %d"" % n_features)\nprint(""n_classes: %d"" % n_classes)\n\n\n###############################################################################\n# Split into a training set and a test set using a stratified k fold\n\n# split into a training and testing set\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42)\n\n\n###############################################################################\n# Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled\n# dataset): unsupervised feature extraction / dimensionality reduction\nn_components = 150\n\nprint(""Extracting the top %d eigenfaces from %d faces""\n      % (n_components, X_train.shape[0]))\nt0 = time()\npca = RandomizedPCA(n_components=n_components, whiten=True).fit(X_train)\nprint(""done in %0.3fs"" % (time() - t0))\n\neigenfaces = pca.components_.reshape((n_components, h, w))\n\nprint(""Projecting the input data on the eigenfaces orthonormal basis"")\nt0 = time()\nX_train_pca = pca.transform(X_train)\nX_test_pca = pca.transform(X_test)\nprint(""done in %0.3fs"" % (time() - t0))\n\n\n###############################################################################\n# Train a SVM classification model\n\nprint(""Fitting the classifier to the training set"")\nt0 = time()\nparam_grid = {\'C\': [1e3, 5e3, 1e4, 5e4, 1e5],\n              \'gamma\': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\nclf = GridSearchCV(SVC(kernel=\'rbf\', class_weight=\'balanced\'), param_grid)\nclf = clf.fit(X_train_pca, y_train)\nprint(""done in %0.3fs"" % (time() - t0))\nprint(""Best estimator found by grid search:"")\nprint(clf.best_estimator_)\n\n\n###############################################################################\n# Quantitative evaluation of the model quality on the test set\n\nprint(""Predicting people\'s names on the test set"")\nt0 = time()\ny_pred = clf.predict(X_test_pca)\nprint(""done in %0.3fs"" % (time() - t0))\n\nprint(classification_report(y_test, y_pred, target_names=target_names))\nprint(confusion_matrix(y_test, y_pred, labels=range(n_classes)))\n\n\n###############################################################################\n# Qualitative evaluation of the predictions using matplotlib\n\ndef plot_gallery(images, titles, h, w, n_row=3, n_col=4):\n    """"""Helper function to plot a gallery of portraits""""""\n    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n    for i in range(n_row * n_col):\n        plt.subplot(n_row, n_col, i + 1)\n        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n        plt.title(titles[i], size=12)\n        plt.xticks(())\n        plt.yticks(())\n\n\n# plot the result of the prediction on a portion of the test set\n\ndef title(y_pred, y_test, target_names, i):\n    pred_name = target_names[y_pred[i]].rsplit(\' \', 1)[-1]\n    true_name = target_names[y_test[i]].rsplit(\' \', 1)[-1]\n    return \'predicted: %s\\ntrue:      %s\' % (pred_name, true_name)\n\nprediction_titles = [title(y_pred, y_test, target_names, i)\n                     for i in range(y_pred.shape[0])]\n\nplot_gallery(X_test, prediction_titles, h, w)\n\n# plot the gallery of the most significative eigenfaces\n\neigenface_titles = [""eigenface %d"" % i for i in range(eigenfaces.shape[0])]\nplot_gallery(eigenfaces, eigenface_titles, h, w)\n\nplt.show()\n'"
scripts/face_recognition_eigen_PCA.py,0,"b'""""""\n===================================================\nFaces recognition example using eigenfaces and SVMs\n===================================================\n\nThe dataset used in this example is a preprocessed excerpt of the\n""Labeled Faces in the Wild"", aka LFW_:\n\n  http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz (233MB)\n\n.. _LFW: http://vis-www.cs.umass.edu/lfw/\n\nExpected results for the top 5 most represented people in the dataset:\n\n================== ============ ======= ========== =======\n                   precision    recall  f1-score   support\n================== ============ ======= ========== =======\n     Ariel Sharon       0.67      0.92      0.77        13\n     Colin Powell       0.75      0.78      0.76        60\n  Donald Rumsfeld       0.78      0.67      0.72        27\n    George W Bush       0.86      0.86      0.86       146\nGerhard Schroeder       0.76      0.76      0.76        25\n      Hugo Chavez       0.67      0.67      0.67        15\n       Tony Blair       0.81      0.69      0.75        36\n\n      avg / total       0.80      0.80      0.80       322\n================== ============ ======= ========== =======\n\n""""""\nfrom __future__ import print_function\n\nfrom time import time\nimport logging\nimport matplotlib.pyplot as plt\n\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.datasets import fetch_lfw_people\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.decomposition import RandomizedPCA\nfrom sklearn.svm import SVC\n\n\nprint(__doc__)\n\n# Display progress logs on stdout\nlogging.basicConfig(level=logging.INFO, format=\'%(asctime)s %(message)s\')\n\n\n###############################################################################\n# Download the data, if not already on disk and load it as numpy arrays\n\nlfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n\n# introspect the images arrays to find the shapes (for plotting)\nn_samples, h, w = lfw_people.images.shape\n\n# for machine learning we use the 2 data directly (as relative pixel\n# positions info is ignored by this model)\nX = lfw_people.data\nn_features = X.shape[1]\n\n# the label to predict is the id of the person\ny = lfw_people.target\nprint(y)\ntarget_names = lfw_people.target_names\nn_classes = target_names.shape[0]\n\nprint(""Total dataset size:"")\nprint(""n_samples: %d"" % n_samples)\nprint(""n_features: %d"" % n_features)\nprint(""n_classes: %d"" % n_classes)\n\n\n###############################################################################\n# Split into a training set and a test set using a stratified k fold\n\n# split into a training and testing set\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42)\n\nprint(X_train)\n###############################################################################\n# Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled\n# dataset): unsupervised feature extraction / dimensionality reduction\nn_components = 150\n\nprint(""Extracting the top %d eigenfaces from %d faces""\n      % (n_components, X_train.shape[0]))\nt0 = time()\npca = RandomizedPCA(n_components=n_components, whiten=True).fit(X_train)\nprint(""done in %0.3fs"" % (time() - t0))\n\neigenfaces = pca.components_.reshape((n_components, h, w))\n\nprint(""Projecting the input data on the eigenfaces orthonormal basis"")\nt0 = time()\nX_train_pca = pca.transform(X_train)\nX_test_pca = pca.transform(X_test)\nprint(""done in %0.3fs"" % (time() - t0))\n\n\n###############################################################################\n# Train a SVM classification model\n\nprint(""Fitting the classifier to the training set"")\nt0 = time()\nparam_grid = {\'C\': [1e3, 5e3, 1e4, 5e4, 1e5],\n              \'gamma\': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1], }\nclf = GridSearchCV(SVC(kernel=\'rbf\', class_weight=\'balanced\'), param_grid)\nclf = clf.fit(X_train_pca, y_train)\nprint(""done in %0.3fs"" % (time() - t0))\nprint(""Best estimator found by grid search:"")\nprint(clf.best_estimator_)\n\n\n###############################################################################\n# Quantitative evaluation of the model quality on the test set\n\nprint(""Predicting people\'s names on the test set"")\nt0 = time()\ny_pred = clf.predict(X_test_pca)\nprint(""done in %0.3fs"" % (time() - t0))\n\nprint(classification_report(y_test, y_pred, target_names=target_names))\nprint(confusion_matrix(y_test, y_pred, labels=range(n_classes)))\n\n\n###############################################################################\n# Qualitative evaluation of the predictions using matplotlib\n\ndef plot_gallery(images, titles, h, w, n_row=3, n_col=4):\n    """"""Helper function to plot a gallery of portraits""""""\n    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n    for i in range(n_row * n_col):\n        plt.subplot(n_row, n_col, i + 1)\n        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n        plt.title(titles[i], size=12)\n        plt.xticks(())\n        plt.yticks(())\n\n\n# plot the result of the prediction on a portion of the test set\n\ndef title(y_pred, y_test, target_names, i):\n    pred_name = target_names[y_pred[i]].rsplit(\' \', 1)[-1]\n    true_name = target_names[y_test[i]].rsplit(\' \', 1)[-1]\n    return \'predicted: %s\\ntrue:      %s\' % (pred_name, true_name)\n\nprediction_titles = [title(y_pred, y_test, target_names, i)\n                     for i in range(y_pred.shape[0])]\n\nplot_gallery(X_test, prediction_titles, h, w)\n\n# plot the gallery of the most significative eigenfaces\n\neigenface_titles = [""eigenface %d"" % i for i in range(eigenfaces.shape[0])]\nplot_gallery(eigenfaces, eigenface_titles, h, w)\n\nplt.show()\n'"
scripts/face_recognizer.py,2,"b'#!/usr/bin/python\n\n# Import the required modules\nimport cv2\nimport os\nimport sys\nimport numpy as np\nfrom PIL import Image\n# For face detection we will use the Haar Cascade provided by OpenCV.\ncascadePath = ""haarcascade_frontalface_default.xml""\nfaceCascade = cv2.CascadeClassifier(cascadePath)\nrecognizer = cv2.face.createLBPHFaceRecognizer()\n\n\ndef get_images_and_labels(path):\n    image_paths = [os.path.join(path, f)\n                   for f in os.listdir(path) if not f.endswith(\'.sad\')]\n# images will contains face images\n    images = []\n# labels will contains the label that is assigned to the image\n    labels = []\n    for image_path in image_paths:\n        image_pil = Image.open(image_path).convert(\'L\')\n        image = np.array(image_pil, \'uint8\')\n        nbr = int(os.path.split(image_path)[1].split(\n            ""."")[0].replace(""subject"", """"))\n        faces = faceCascade.detectMultiScale(image)\n        for (x, y, w, h) in faces:\n            images.append(image[y: y + h, x: x + w])\n            labels.append(nbr)\n            cv2.imshow(""Adding faces to traning set..."",\n                       image[y: y + h, x: x + w])\n            cv2.waitKey(50)\n    return images, labels\n\n# Path to the Yale Dataset\npath = sys.argv[1]\nimages, labels = get_images_and_labels(path)\ncv2.destroyAllWindows()\n\nrecognizer.train(images, np.array(labels))\nrecognizer.save(""cont.yaml"")\n# recognizer.load(""/home/arpit/Projects/Survelliance_System/cont.yaml"")\n'"
scripts/face_recognizer1.py,3,"b'#!/usr/bin/python\n\n# Import the required modules\nimport cv2\nimport os\nimport numpy as np\nfrom PIL import Image\n# For face detection we will use the Haar Cascade provided by OpenCV.\ncascadePath = ""haarcascade_frontalface_default.xml""\nfaceCascade = cv2.CascadeClassifier(cascadePath)\n# For face recognition we will the the LBPH Face Recognizer\nrecognizer = cv2.face.createLBPHFaceRecognizer()\n\n\ndef get_images_and_labels(path):\n    # Append all the absolute image paths in a list image_paths\n    # We will not read the image with the .sad extension in the training set\n    # Rather, we will use them to test our accuracy of the training\n    image_paths = [os.path.join(path, f)\n                   for f in os.listdir(path) if not f.endswith(\'.sad\')]\n    # images will contains face images\n    images = []\n    # labels will contains the label that is assigned to the image\n    labels = []\n    for image_path in image_paths:\n        # Read the image and convert to grayscale\n        image_pil = Image.open(image_path).convert(\'L\')\n    # Convert the image format into numpy array\n        image = np.array(image_pil, \'uint8\')\n    # Get the label of the image\n        nbr = int(os.path.split(image_path)[1].split(\n            ""."")[0].replace(""subject"", """"))\n    # Detect the face in the image\n        faces = faceCascade.detectMultiScale(image)\n    # If face is detected, append the face to images and the label to labels\n        for (x, y, w, h) in faces:\n            images.append(image[y: y + h, x: x + w])\n            labels.append(nbr)\n            cv2.imshow(""Adding faces to traning set..."",\n                       image[y: y + h, x: x + w])\n            cv2.waitKey(50)\n    # return the images list and labels list\n    return images, labels\n\n# Path to the Yale Dataset\npath = \'./yalefaces\'\n# Call the get_images_and_labels function and get the face images and the\n# corresponding labels\nimages, labels = get_images_and_labels(path)\ncv2.destroyAllWindows()\n\n# Perform the tranining\nrecognizer.train(images, np.array(labels))\n\n# Append the images with the extension .sad into image_paths\nimage_paths = [os.path.join(path, f)\n               for f in os.listdir(path) if f.endswith(\'.sad\')]\nfor image_path in image_paths:\n    predict_image_pil = Image.open(image_path).convert(\'L\')\n    predict_image = np.array(predict_image_pil, \'uint8\')\n    faces = faceCascade.detectMultiScale(predict_image)\n    for (x, y, w, h) in faces:\n        nbr_predicted = recognizer.predict(predict_image[y: y + h, x: x + w])\n        nbr_actual = int(os.path.split(image_path)[\n                         1].split(""."")[0].replace(""subject"", """"))\n        # print(nbr_predicted)\n        label_predicted, confidence_pred = nbr_predicted\n        # print(confidence_pred)\n        if nbr_actual == label_predicted:\n            print(""{} is Correctly Recognized with confidence {}"".format(\n                nbr_actual, confidence_pred))\n        else:\n            print(""{} is Incorrect Recognized as {} with confidence {}"".format(\n                nbr_actual, label_predicted, confidence_pred))\n        cv2.imshow(""Recognizing Face"", predict_image[y: y + h, x: x + w])\n        cv2.waitKey(1000)\n'"
scripts/face_reocog_yaml.py,0,b''
scripts/feature_matching.py,0,"b""import numpy as np\nimport cv2\nimport sys\nimport matplotlib.pyplot as plt\nimg1 = cv2.imread(sys.argv[1], 0)\nimg2 = cv2.imread(sys.argv[2], 0)\norb = cv2.ORB_create()\nkp1, des1 = orb.detectAndCompute(img1, None)\nkp2, des2 = orb.detectAndCompute(img2, None)\nbf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\nmatches = bf.match(des1, des2)\nmatches = sorted(matches, key=lambda x: x.distance)\nimg3 = cv2.drawMatches(img1, kp1, img2, kp2, matches[:10], None, flags=2)\ncv2.imshow('window', img3)\ncv2.waitKey(0)\n"""
scripts/gameboy.py,0,"b'# import the necessary packages\nimport imutils\n# from skimage import exposure\nimport numpy as np\nimport argparse\nimport cv2\n\n# construct the argument parser and parse the arguments\nap = argparse.ArgumentParser()\nap.add_argument(""-q"", ""--query"", required=True,\n                help=""Path to the query image"")\nargs = vars(ap.parse_args())\n# load the query image, compute the ratio of the old height\n# to the new height, clone it, and resize it\nimage = cv2.imread(args[""query""])\nratio = image.shape[0] / 300.0\norig = image.copy()\nimage = imutils.resize(image, height=300)\n\n# convert the image to grayscale, blur it, and find edges\n# in the image\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\ngray = cv2.bilateralFilter(gray, 11, 17, 17)\nedged = cv2.Canny(gray, 30, 200)\ncv2.imshow(\'edged\', edged)\n# find contours in the edged image, keep only the largest\n# ones, and initialize our screen contour\n(_, cnts, _) = cv2.findContours(\n    edged.copy(), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\ncnts = sorted(cnts, key=cv2.contourArea, reverse=True)[:10]\nscreenCnt = None\n# loop over our contours\nfor c in cnts:\n    # approximate the contour\n    peri = cv2.arcLength(c, True)\n    approx = cv2.approxPolyDP(c, 0.02 * peri, True)\n\n    # if our approximated contour has four points, then\n    # we can assume that we have found our screen\n    if len(approx) == 4:\n        screenCnt = approx\n        break\ncv2.drawContours(image, [screenCnt], -1, (0, 255, 0), 3)\ncv2.imshow(""Game Boy Screen"", image)\ncv2.waitKey(0)\n'"
scripts/goood_features_to_track.py,1,"b""import numpy as np\nimport cv2\nfrom matplotlib import pyplot as plt\nimport sys\nimg = cv2.imread(sys.argv[1])\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\ncorners = cv2.goodFeaturesToTrack(gray, 25, 0.01, 10)\ncorners = np.int0(corners)\n\nfor i in corners:\n    x, y = i.ravel()\n    cv2.circle(img, (x, y), 3, 255, -1)\ncv2.imshow('window', img)\ncv2.waitKey(0)\n"""
scripts/hand_detect.py,0,"b'import cv2\nimport sys\nimport os\nimport imutils\n\n# The locatio of the image and the cascade file path\n# imagePath =\'/home/unnirajendran/Desktop/face_detect/image1.jpg\'\n# cascPath = \'/home/unnirajendran/Desktop/face_detect/haarcascade_frontalface_default.xml\'\nimagePath = sys.argv[1]\ncascPath = sys.argv[2]\n# Create the haar cascade\nfaceCascade = cv2.CascadeClassifier(cascPath)\n\n# Read the image\nimage = cv2.imread(imagePath)\n\n# Resize the image so it fits in the screen\nimage1 = imutils.resize(image, height=500)\ngray = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n\n# Detect faces in the image\nfaces = faceCascade.detectMultiScale(\n    gray,\n    scaleFactor=1.1,\n    minNeighbors=5,\n    minSize=(30, 30),\n    #flags = cv2.cv.CV_HAAR_SCALE_IMAGE\n    flags=0\n)\n\nif format(len(faces)) == 1:\n    print(""Found {0} face!"".format(len(faces)))\nelse:\n    print(""Found {0} faces!"".format(len(faces)))\n\n# Draw a rectangle around the faces\nfor (x, y, w, h) in faces:\n    cv2.rectangle(image1, (x, y), (x + w, y + h), (0, 255, 0), 2)\n\ncv2.imshow(""Faces found"", image1)\ncv2.waitKey(0)\n'"
scripts/head_shoulders_detect.py,0,"b'from __future__ import print_function\nfrom imutils.object_detection import non_max_suppression\nfrom imutils import paths\nimport numpy as np\nimport argparse\nimport imutils\nimport cv2\nimport time\nimport sys\ncascade_path = ""HS.xml""\nimage_path = sys.argv[1]\n# image = cv2.imread(image_path)\ncascade = cv2.CascadeClassifier(cascade_path)\ncamera = cv2.VideoCapture(sys.argv[1])\n# start = time.time()\nwhile True:\n    grabbed, frame = camera.read()\n    if not grabbed:\n        break\n    frame = imutils.resize(frame, height=300)\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    start = time.time()\n    faces = cascade.detectMultiScale(\n        gray,\n        scaleFactor=1.1,\n        minNeighbors=5,\n        minSize=(10, 10),\n        flags=0\n    )\n    end = time.time()\n    total_time_taken = end - start\n    print(total_time_taken)\n    for (x, y, w, h) in faces:\n        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n    cv2.imshow(""face and Shoulders"", frame)\n    key = cv2.waitKey(1) & 0xFF\n    if key == ord(""q""):\n        break\ncamera.release()\ncv2.destroyAllWindows()\n'"
scripts/human_detect_HOG_SVM.py,0,"b'# import the necessary packages\nfrom __future__ import print_function\nimport argparse\nimport datetime\nimport imutils\nimport cv2\nfrom imutils.object_detection import non_max_suppression\ncascade = cv2.CascadeClassifier(\'lbpcascade_profileface.xml\')\n# construct the argument parse and parse the arguments\nap = argparse.ArgumentParser()\nap.add_argument(""-i"", ""--image"", required=True, help=""path to the input image"")\nap.add_argument(""-w"", ""--win-stride"", type=str,\n                default=""(8, 8)"", help=""window stride"")\nap.add_argument(""-p"", ""--padding"", type=str,\n                default=""(16, 16)"", help=""object padding"")\nap.add_argument(""-s"", ""--scale"", type=float,\n                default=1.05, help=""image pyramid scale"")\nap.add_argument(""-m"", ""--mean-shift"", type=int,\n                default=-1, help=""whether or not mean"")\nargs = vars(ap.parse_args())\n# evaluate the command line arguments (using the eval function like\n# this is not good form, but let\'s tolerate it for the example)\nwinStride = eval(args[""win_stride""])\npadding = eval(args[""padding""])\nmeanShift = True if args[""mean_shift""] > 0 else False\n\n# initialize the HOG descriptor/person detector\nhog = cv2.HOGDescriptor()\nhog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n\n# load the image and resize it\nimage = cv2.imread(args[""image""])\nimage = imutils.resize(image, width=min(400, image.shape[1]))\n# detect people in the image\nstart = datetime.datetime.now()\n(rects, weights) = hog.detectMultiScale(image, hitThreshold=0, winStride=winStride,\n                                        padding=padding, scale=args[""scale""], useMeanshiftGrouping=meanShift)\nprint(len(rects))\nface = cascade.detectMultiScale(image)\nprint(len(face))\nprint(""[INFO] detection took: {}s"".format(\n    (datetime.datetime.now() - start).total_seconds()))\npick = non_max_suppression(rects, probs=None, overlapThresh=10)\n# draw the original bounding boxes\nfor (x, y, w, h) in pick:\n    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n\n# show the output image\ncv2.imshow(""Detections"", image)\ncv2.waitKey(0)\n'"
scripts/image_outline_fincountors.py,3,"b""import numpy as np\nimport cv2\nimport imutils\nimport sys\nfrom scipy.misc import imread\nfrom scipy import signal\n\n\ndef auto_canny(image, sigma=0.25):\n    # compute the median of the single channel pixel intensities\n    v = np.median(image)\n\n    # apply automatic Canny edge detection using the computed median\n    lower = int(max(0, (1.0 - sigma) * v))\n    upper = int(min(255, (1.0 + sigma) * v))\n    edged = cv2.Canny(image, lower, upper)\n    # return the edged image\n    return edged\n\nimage2 = cv2.imread(sys.argv[1],)\nimage2 = imutils.resize(image2, height=500)\ngaussian = np.ones((5, 5), np.float32) / 25\nlaplacian = np.array([[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]])\ndst = cv2.filter2D(image2, -1, gaussian)\ncv2.imshow('dsd', dst)\ncv2.waitKey(0)\ndst = cv2.Canny(dst, 99, 100)\n_, th4, _ = cv2.findContours(dst, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\ncv2.drawContours(image2, th4, -1, 255, 2)\ncv2.imshow('window', image2)\ncv2.waitKey(0)\nauto = auto_canny(image2)\n_, th5, _ = cv2.findContours(auto, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\ncv2.drawContours(image2, th5, -1, 255, 2)\ncv2.imshow('windas', image2)\ncv2.waitKey(0)\n"""
scripts/object_recognition_SURF.py,4,"b'import numpy as np\nimport cv2\nfrom matplotlib import pyplot as plt\nimport sys\nimport imutils\nMIN_MATCH_COUNT = 10\nprint(\'hello\')\nimg1 = cv2.imread(sys.argv[1], 0)          # queryImage\nimg2 = cv2.imread(sys.argv[2], 0)  # trainImage\nimg1 = imutils.resize(img1, width=500, height=500)\nimg2 = imutils.resize(img2, width=500, height=500)\nprint(\'hel\')\n# Initiate SIFT detector\nsift = cv2.xfeatures2d.SIFT_create(300)\n\n# find the keypoints and descriptors with SIFT\nkp1, des1 = sift.detectAndCompute(img1, None)\nkp2, des2 = sift.detectAndCompute(img2, None)\nprint(len(kp1))\nprint(len(kp2))\nFLANN_INDEX_KDTREE = 0\nindex_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=10)\nsearch_params = dict(checks=5000)\n\nflann = cv2.FlannBasedMatcher(index_params, search_params)\n\nmatches = flann.knnMatch(des1, des2, k=2)\n\n# store all the good matches as per Lowe\'s ratio test.\ngood = []\nfor m, n in matches:\n    if m.distance < 0.7 * n.distance:\n        good.append(m)\nif len(good) > MIN_MATCH_COUNT:\n    src_pts = np.float32([kp1[m.queryIdx].pt for m in good]).reshape(-1, 1, 2)\n    dst_pts = np.float32([kp2[m.trainIdx].pt for m in good]).reshape(-1, 1, 2)\n\n    M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n    matchesMask = mask.ravel().tolist()\n\n    h, w = img1.shape\n    pts = np.float32([[0, 0], [0, h - 1], [w - 1, h - 1],\n                      [w - 1, 0]]).reshape(-1, 1, 2)\n    dst = cv2.perspectiveTransform(pts, M)\n\n    img2 = cv2.polylines(img2, [np.int32(dst)], True, 255, 3, cv2.LINE_AA)\n\nelse:\n    print(""Not enough matches are found - %d/%d"" %\n          (len(good), MIN_MATCH_COUNT))\n    matchesMask = None\ndraw_params = dict(matchColor=(0, 255, 0),  # draw matches in green color\n                   singlePointColor=None,\n                   matchesMask=matchesMask,  # draw only inliers\n                   flags=2)\n\nimg3 = cv2.drawMatches(img1, kp1, img2, kp2, good, None, **draw_params)\n# plt.imshow(img3, \'gray\'),plt.show()\ncv2.imshow(\'window\', img3)\ncv2.waitKey(0)\n'"
scripts/object_tracking.py,2,"b""import cv2\nimport numpy as np\n\ncap = cv2.VideoCapture('video.mp4')\n\nwhile(1):\n\n    # Take each frame\n    frame = cap.read()\n    print(frame)\n    # Convert BGR to HSV\n    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n    print(hsv)\n    # define range of blue color in HSV\n    lower_blue = np.array([110, 50, 50])\n    upper_blue = np.array([130, 255, 255])\n\n    # Threshold the HSV image to get only blue colors\n    mask = cv2.inRange(hsv, lower_blue, upper_blue)\n\n    # Bitwise-AND mask and original image\n    res = cv2.bitwise_and(frame, frame, mask=mask)\n\n    cv2.imshow('frame', frame)\n    cv2.imshow('mask', mask)\n    cv2.imshow('res', res)\n    k = cv2.waitKey(5) & 0xFF\n    if k == 27:\n        break\n\ncv2.destroyAllWindows()\n"""
scripts/object_tracking_2.py,1,"b'# import the necessary packages\nfrom collections import deque\n\nimport numpy as np\nimport argparse\nimport imutils\nimport cv2\n\n# construct the argument parse and parse the arguments\nap = argparse.ArgumentParser()\nap.add_argument(""-v"", ""--video"",\n                help=""path to the (optional) video file"")\nap.add_argument(""-b"", ""--buffer"", type=int, default=64,\n                help=""max buffer size"")\nargs = vars(ap.parse_args())\n# define the lower and upper boundaries of the ""green""\n# ball in the HSV color space, then initialize the\n# list of tracked points\ngreenLower = (29, 86, 6)\ngreenUpper = (64, 255, 255)\npts = deque(maxlen=args[""buffer""])\n\n# if a video path was not supplied, grab the reference\n# to the webcam\nif not args.get(""video"", False):\n    camera = cv2.VideoCapture(0)\n\n# otherwise, grab a reference to the video file\nelse:\n    camera = cv2.VideoCapture(args[""video""])\n# keep looping\nwhile True:\n    # grab the current frame\n    (grabbed, frame) = camera.read()\n\n    # if we are viewing a video and we did not grab a frame,\n    # then we have reached the end of the video\n    if args.get(""video"") and not grabbed:\n        break\n\n    # resize the frame, blur it, and convert it to the HSV\n    # color space\n    frame = imutils.resize(frame, width=600)\n    # blurred = cv2.GaussianBlur(frame, (11, 11), 0)\n    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n\n    # construct a mask for the color ""green"", then perform\n    # a series of dilations and erosions to remove any small\n    # blobs left in the mask\n    mask = cv2.inRange(hsv, greenLower, greenUpper)\n    mask = cv2.erode(mask, None, iterations=2)\n    mask = cv2.dilate(mask, None, iterations=2)\n    # find contours in the mask and initialize the current\n# (x, y) center of the ball\n    cnts = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL,\n                            cv2.CHAIN_APPROX_SIMPLE)[-2]\n    center = None\n\n    # only proceed if at least one contour was found\n    if len(cnts) > 0:\n        # find the largest contour in the mask, then use\n        # it to compute the minimum enclosing circle and\n        # centroid\n        c = max(cnts, key=cv2.contourArea)\n        ((x, y), radius) = cv2.minEnclosingCircle(c)\n        M = cv2.moments(c)\n        center = (int(M[""m10""] / M[""m00""]), int(M[""m01""] / M[""m00""]))\n\n        # only proceed if the radius meets a minimum size\n        if radius > 10:\n            # draw the circle and centroid on the frame,\n            # then update the list of tracked points\n            cv2.circle(frame, (int(x), int(y)), int(radius), (0, 255, 255), 2)\n            cv2.circle(frame, center, 5, (0, 0, 255), -1)\n\n# update the points queue\n    pts.appendleft(center)\n    # loop over the set of tracked points\n    for i in range(1, len(pts)):\n        # if either of the tracked points are None, ignore\n        # them\n        if pts[i - 1] is None or pts[i] is None:\n            continue\n\n        # otherwise, compute the thickness of the line and\n        # draw the connecting lines\n        thickness = int(np.sqrt(args[""buffer""] / float(i + 1)) * 2.5)\n        cv2.line(frame, pts[i - 1], pts[i], (0, 0, 255), thickness)\n\n        # show the frame to our screen\n    cv2.imshow(""Frame"", frame)\n    key = cv2.waitKey(1) & 0xFF\n\n    # if the \'q\' key is pressed, stop the loop\n    if key == ord(""q""):\n        break\n\n# cleanup the camera and close any open windows\ncamera.release()\ncv2.destroyAllWindows()\n'"
scripts/outline.py,0,"b'import numpy as np\nimport cv2\nimport imutils\nimport sys\nfrom collections import deque\nfrom scipy.misc import imread\nimport argparse\nfrom scipy import signal\nbgs = cv2.createBackgroundSubtractorMOG2()\ncamera = cv2.VideoCapture(sys.argv[1])\ngrabbed, frame1 = camera.read()\nwhile True:\n    grabbed, frame = camera.read()\n    if not grabbed:\n        break\n    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    f = bgs.apply(frame)\n    cv2.imshow(\'window\', f)\n    key = cv2.waitKey(1) & 0xFF\n    if key == ord(""q""):\n        break\ncamera.release()\ncv2.destryallwindows()\n'"
scripts/pedestrian.py,1,"b'# import the necessary packages\nfrom imutils.object_detection import non_max_suppression\nfrom imutils import paths\nimport numpy as np\nimport argparse\nimport imutils\nimport cv2\n\n# construct the argument parse and parse the arguments\nap = argparse.ArgumentParser()\nap.add_argument(""-i"", ""--images"", required=True,\n                help=""path to images directory"")\nargs = vars(ap.parse_args())\n\n# initialize the HOG descriptor/person detector\nhog = cv2.HOGDescriptor()\nhog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n# loop over the image paths\nfor imagePath in paths.list_images(args[""images""]):\n    # load the image and resize it to (1) reduce detection time\n    # and (2) improve detection accuracy\n    image = cv2.imread(imagePath)\n    image = imutils.resize(image, width=min(400, image.shape[1]))\n    orig = image.copy()\n    # detect people in the image\n    (rects, weights) = hog.detectMultiScale(\n        image, winStride=(4, 4), padding=(8, 8), scale=1.05)\n    print(rects)\n    # draw the original bounding boxes\n    for (x, y, w, h) in rects:\n        cv2.rectangle(orig, (x, y), (x + w, y + h), (0, 0, 255), 2)\n\n    # apply non-maxima suppression to the bounding boxes using a\n    # fairly large overlap threshold to try to maintain overlapping\n    # boxes that are still people\n    rects = np.array([[x, y, x + w, y + h] for (x, y, w, h) in rects])\n    pick = non_max_suppression(rects, probs=None, overlapThresh=0.65)\n\n    # draw the final bounding boxes\n    for (xA, yA, xB, yB) in pick:\n        cv2.rectangle(image, (xA, yA), (xB, yB), (0, 255, 0), 2)\n\n    # show some information on the number of bounding boxes\n    filename = imagePath[imagePath.rfind(""/"") + 1:]\n    print(""[INFO] {}: {} original boxes, {} after suppression"".format(\n        filename, len(rects), len(pick)))\n\n    # show the output images\n    cv2.imshow(""Before NMS"", orig)\n    cv2.imshow(""After NMS"", image)\n    cv2.waitKey(0)\n'"
scripts/rename.py,0,"b'import os\nimport sys\npath = ""/home/arpit/Projects/Survelliance_System/data/extra2/""\ni = 460\nfor file1 in os.listdir(path):\n    os.rename(os.path.join(path, file1), os.path.join(\n        path, ""subject03"" + ""."" + str(i) + "".jpg""))\n    i = i + 1\n'"
scripts/script.py,0,"b""import glob\nfilelist = glob.glob('/home/arpit/caffe/caffe-master/data/img_rating/*.JPEG')\nfilelist_file = open(\n    '/home/arpit/caffe/caffe-master/data/img_rating/list.txt', 'w')\nfor filename in filelist:\n    filelist_file.write(filename)\n    filelist_file.write('\\n')\nfilelist_file.close()\n"""
scripts/track_human_withHOG_SVM.py,1,"b'# import the necessary packages\nfrom __future__ import print_function\nfrom imutils.object_detection import non_max_suppression\nfrom imutils import paths\nimport numpy as np\nimport argparse\nimport imutils\nimport cv2\nimport time\nimport sys\n# funciton to draw head and shoulders of human\n\n\ndef draw_Head_shoulders(frame):\n    # cascade_path = ""HS.xml""\n    cascade_path = ""haarcascade_profileface.xml""\n    cascade = cv2.CascadeClassifier(cascade_path)\n    frame = imutils.resize(frame, height=300)\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    start = time.time()\n    faces = cascade.detectMultiScale(\n        gray,\n        scaleFactor=1.1,\n        minNeighbors=5,\n        minSize=(10, 10),\n        flags=0\n    )\n    end = time.time()\n    total_time_taken = end - start\n    print(total_time_taken)\n    for (x, y, w, h) in faces:\n        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n    cv2.imshow(""face and Shoulders"", frame)\n    key = cv2.waitKey(1)\n# construct the argument parse and parse the arguments\n# ap = argparse.ArgumentParser()\n# ap.add_argument(""-i"", ""--images"", required=True, help=""path to images directory"")\n# args = vars(ap.parse_args())\n\n# initialize the HOG descriptor/person detector\nhog = cv2.HOGDescriptor()\nhog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n# for imagePath in paths.list_images(args[""images""]):\n# load the image and resize it to (1) reduce detection time\n# and (2) improve detection accuracy\nfgbg = cv2.createBackgroundSubtractorMOG2()\ncamera = cv2.VideoCapture(sys.argv[1])\nwhile True:\n    grabbed, frame = camera.read()\n    if not grabbed:\n        break\n    image = imutils.resize(frame, width=min(400, frame.shape[1]))\n    orig = image.copy()\n    start = time.time()\n    fgbg.apply(image)\n    # detect people in the image\n    (rects, weights) = hog.detectMultiScale(image, winStride=(4, 4),\n                                            padding=(16, 16), scale=1.06)\n    if type(rects) is not tuple:\n        cropped = image[rects[0][1]:rects[0][1] + rects[0]\n                        [3], rects[0][0]:rects[0][0] + rects[0][2]]\n        draw_Head_shoulders(cropped)\n        # print(cropped)\n    # draw the original bounding boxes\n    # print(rects)\n    for (x, y, w, h) in rects:\n        cv2.rectangle(orig, (x, y), (x + w, y + h), (0, 0, 255), 2)\n    end = time.time()\n    totalTimeTaken = end - start\n    # print(totalTimeTaken)\n    # apply non-maxima suppression to the bounding boxes using a\n    # fairly large overlap threshold to try to maintain overlapping\n    # boxes that are still people\n    rects = np.array([[x, y, x + w, y + h] for (x, y, w, h) in rects])\n    pick = non_max_suppression(rects, probs=None, overlapThresh=0.65)\n\n    # draw the final bounding boxes\n    for (xA, yA, xB, yB) in pick:\n        cv2.rectangle(image, (xA, yA), (xB, yB), (0, 255, 0), 2)\n\n    # show some information on the number of bounding boxes\n    # print(""[INFO] {}: {} original boxes, {} after suppression"".format(filename, len(rects), len(pick)))\n\n    # show the output images\n    # cv2.imshow(""Before NMS"", orig)\n    cv2.imshow(""window"", image)\n    key = cv2.waitKey(1) & 0xFF\n    if key == ord(""q""):\n        break\ncamera.release()\ncv2.destroyAllWindows()\n'"
scripts/webcam.py,0,"b""import cv2\nimport sys\n\ncascPath = '/home/unnirajendran/Desktop/face_detect/haarcascade_frontalface_default.xml'\nfaceCascade = cv2.CascadeClassifier(cascPath)\n\nvideo_capture = cv2.VideoCapture(0)\n\nwhile True:\n    # Capture frame-by-frame\n    ret, frame = video_capture.read()\n\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\n    faces = faceCascade.detectMultiScale(\n        gray,\n        scaleFactor=1.1,\n        minNeighbors=5,\n        minSize=(30, 30),\n        # flags=cv2.cv.CV_HAAR_SCALE_IMAGE\n        flags=0\n    )\n\n    # Draw a rectangle around the faces\n    for (x, y, w, h) in faces:\n        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n\n    # Display the resulting frame\n    cv2.imshow('Video', frame)\n\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\n# When everything is done, release the capture\nvideo_capture.release()\ncv2.destroyAllWindows()\n"""
