file_path,api_count,code
compact.py,0,"b'from utils.processor import Processor\nimport glob\n\n#for file_path in glob.glob(""train/five/open3/npys/""):\n#    pcr = Processor(file_path,crop=False, overwrite=True)\n#    pcr.compact()\n#    print(""finished"", file_path)\nfor file_path in glob.glob(""train/*/*/npys/""):\n    pcr = Processor(file_path,crop=False, overwrite=True)\n    pcr.fuse()\n    print(""finished"", file_path)\n\n\nfor file_path in glob.glob(""test/*/*/npys/""):\n   pcr = Processor(file_path,crop=False, overwrite=True)\n   pcr.fuse()\n   print(""finished"", file_path)\nprint(""finished all"")\n'"
train_arg.py,0,"b'from utils.pointnet import PointNet\nfrom utils.datasets import MonoDataset\nimport argparse\nimport numpy as np\n## load arguments\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--batchsize\', type=int, default=16, help=\'input batch size\')\nparser.add_argument(\'--num_points\', type=int, default=320, help=\'input batch size\')\nparser.add_argument(\'--workers\', type=int, help=\'number of data loading workers\', default=1)\nparser.add_argument(\'--num_epoch\', type=int, default=25, help=\'number of epochs to train for\')\nparser.add_argument(\'--outf\', type=str, default=\'cls\',  help=\'output folder\')\nparser.add_argument(\'--model\', type=str, default = \'\',  help=\'model path\')\nparser.add_argument(\'--num_classes\', type=int, default = 6,  help=\'num classes\')\n\nopt = parser.parse_args()\nprint (opt)\n\ntrain_paths = [""train/zero/"", ""train/one/"", ""train/two/"", ""train/three/"", ""train/four/"", ""train/five/"", ""train/thumbup/"", ""train/thumbdown/"", ""train/frame/"", ""train/bird/""]\ntest_paths = [""test/zero/"", ""test/one/"", ""test/two/"", ""test/three/"", ""test/four/"", ""test/five/"", ""test/thumbup/"", ""test/thumbdown/"", ""test/frame/"", ""test/bird/""]\n\ndataset = MonoDataset(left=True, right=True, num_points=opt.num_points, file_paths=train_paths)\ntest_dataset = MonoDataset(left=True, right=True, num_points=opt.num_points, file_paths=test_paths)\n\npnt = PointNet(batchsize=opt.batchsize, num_points=opt.num_points, num_epoch=opt.num_epoch, outf=opt.outf, model=opt.model, num_classes=opt.num_classes, ptype=\'small\')\nprint(pnt.train(dataset, test_dataset)[2].shape)\n'"
train_arg_dual.py,0,"b'from utils.dualnet import DualNet\nfrom utils.datasets import DuoDataset\nimport argparse\nimport numpy as np\n## load arguments\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--batchsize\', type=int, default=32, help=\'input batch size\')\nparser.add_argument(\'--num_points\', type=int, default=320, help=\'input batch size\')\nparser.add_argument(\'--workers\', type=int, help=\'number of data loading workers\', default=1)\nparser.add_argument(\'--num_epoch\', type=int, default=1, help=\'number of epochs to train for\')\nparser.add_argument(\'--outf\', type=str, default=\'cls\',  help=\'output folder\')\nparser.add_argument(\'--model\', type=str, default = \'\',  help=\'model path\')\nparser.add_argument(\'--num_classes\', type=int, default = 10,  help=\'num classes\')\n\nopt = parser.parse_args()\nprint (opt)\n\ntrain_paths = [""train/zero/"", ""train/one/"", ""train/two/"", ""train/three/"", ""train/four/"", ""train/five/"", ""train/thumbup/"", ""train/thumbdown/"", ""train/frame/"", ""train/bird/""]\ntest_paths = [""test/zero/"", ""test/one/"", ""test/two/"", ""test/three/"", ""test/four/"", ""test/five/"", ""test/thumbup/"", ""test/thumbdown/"", ""test/frame/"", ""test/bird/""]\n\ndataset = DuoDataset(num_points=opt.num_points, file_paths=train_paths)\ntest_dataset = DuoDataset(num_points=opt.num_points, file_paths=test_paths)\n\npnt = DualNet(batchsize=opt.batchsize, num_points=opt.num_points, num_epoch=opt.num_epoch, outf=opt.outf, model=opt.model, num_classes=opt.num_classes, ptype=\'modified\')\nprint(pnt.train(dataset, test_dataset))\n'"
train_augmentation.py,5,"b'from utils.pointnet import PointNet\nfrom utils.datasets import MonoDataset\nimport argparse\nimport numpy as np\nimport datetime\nimport os\nimport shutil\n\ncurrent_time = datetime.datetime.now()\nprefix = current_time.strftime(""%m-%d:%H:%M"") + ""aug/""\npath = ""test_results/"" + prefix\n\nif not os.path.exists(path):\n    os.makedirs(path)\nelse:\n    shutil.rmtree(path)\n    os.makedirs(path)\n\ntrain_paths = [""train/zero/"", ""train/one/"", ""train/two/"", ""train/three/"", ""train/four/"", ""train/five/"", ""train/thumbup/"", ""train/thumbdown/"", ""train/frame/"", ""train/bird/""]\ntest_paths = [""test/zero/"", ""test/one/"", ""test/two/"", ""test/three/"", ""test/four/"", ""test/five/"", ""test/thumbup/"", ""test/thumbdown/"", ""test/frame/"", ""test/bird/""]\n\ndataset = MonoDataset(left=True, right=True, num_points=320, file_paths=train_paths)\ntest_dataset = MonoDataset(left=True, right=True, num_points=320, file_paths=test_paths)\n\nnum_trials = 25\ntrial = []\ntest = []\n\nfor j in range(num_trials):\n    print(""small test number 1: "", j)\n    pnt = PointNet(num_points=320, num_classes=10, num_epoch=64, batchsize=32, ptype=\'small\', alpha=0.001, beta=0.01)\n    trial.append(pnt.train(dataset, test_dataset)[0])\ntest.append(trial)\ntrial = []\nnp.save(path + ""acc"", np.array(test))\n\nfor j in range(num_trials):\n    print(""small test number 2: "", j)\n    pnt = PointNet(num_points=320, num_classes=10, num_epoch=64, batchsize=32, ptype=\'small\', alpha=0.002, beta=0.01)\n    trial.append(pnt.train(dataset, test_dataset)[0])\ntest.append(trial)\ntrial = []\nnp.save(path + ""acc"", np.array(test))\n\nfor j in range(num_trials):\n    print(""small test number 3: "", j)\n    pnt = PointNet(num_points=320, num_classes=10, num_epoch=64, batchsize=32, ptype=\'small\', alpha=0.005, beta=0.01)\n    trial.append(pnt.train(dataset, test_dataset)[0])\ntest.append(trial)\ntrial = []\nnp.save(path + ""acc"", np.array(test))\n\nfor j in range(num_trials):\n    print(""small test number 4: "", j)\n    pnt = PointNet(num_points=320, num_classes=10, num_epoch=64, batchsize=32, ptype=\'small\', alpha=0.01, beta=0.01)\n    trial.append(pnt.train(dataset, test_dataset)[0])\ntest.append(trial)\ntrial = []\nnp.save(path + ""acc"", np.array(test))\n\nfor j in range(num_trials):\n    print(""small test number 5: "", j)\n    pnt = PointNet(num_points=320, num_classes=10, num_epoch=64, batchsize=32, ptype=\'small\', alpha=0.02, beta=0.01)\n    trial.append(pnt.train(dataset, test_dataset)[0])\ntest.append(trial)\ntrial = []\nnp.save(path + ""acc"", np.array(test))\n'"
train_confusion.py,10,"b'\nfrom utils.pointnet import PointNet\nfrom utils.datasets import MonoDataset\nimport argparse\nimport numpy as np\nimport datetime\n\ncurrent_time = datetime.datetime.now()\nprefix = current_time.strftime(""%Y-%m-%d:%H:%M"")\n\ntest_acc = []\ntest_mat = []\ntrial_acc = []\ntrial_mat = np.zeros((6,6))\nldr_left = MonoDataset(left=True, right=False, num_points=320)\nldr_right = MonoDataset(left=False, right=True, num_points=320)\nldr_fused = MonoDataset(left=True, right=True, num_points=320)\n\ntrain_paths = [""train/open/"", ""train/thumbup/"", ""train/thumbdown/"", ""train/twofinger/"", ""train/bird/"", ""train/frame/""]\ntest_paths =  [""test/open/"", ""test/thumbup/"", ""test/thumbdown/"",""test/twofinger/"",""test/bird/"",""test/frame/""]\n\n\n# single test LL:\n\ndataset = ldr_left.loadFolders(train_paths)\ntest_dataset = ldr_left.loadFolders(test_paths)\n\ntrial_num = 100\n\nfor j in range(trial_num):\n    print(""FF: "", j)\n    pnt = PointNet(num_points=320, num_classes=6, num_epoch=32, batchsize=20, ptype=\'small\', alpha=0, beta=0)\n    res = pnt.train(dataset, test_dataset)\n    trial_acc.append(res[0])\n    trial_mat += res[1]\n    print(trial_mat)\n\ntest_acc.append(trial_acc)\ntest_mat.append(trial_mat)\ntrial_acc = []\ntrial_mat = np.zeros((6,6))\n\n\nnp.save(""test_results/acc"" + prefix, np.array(test_acc))\nnp.save(""test_results/mat"" + prefix, np.array(test_mat))\n\n# single test RR:\n\ndataset = ldr_right.loadFolders(train_paths)\ntest_dataset = ldr_right.loadFolders(test_paths)\n\ntrial_num = 100\n\nfor j in range(trial_num):\n    print(""FF: "", j)\n    pnt = PointNet(num_points=320, num_classes=6, num_epoch=32, batchsize=20, ptype=\'small\', alpha=0, beta=0)\n    res = pnt.train(dataset, test_dataset)\n    trial_acc.append(res[0])\n    trial_mat += res[1]\n    print(trial_mat)\n\ntest_acc.append(trial_acc)\ntest_mat.append(trial_mat)\ntrial_acc = []\ntrial_mat = np.zeros((6,6))\n\n\n\nnp.save(""test_results/acc"" + prefix, np.array(test_acc))\nnp.save(""test_results/mat"" + prefix, np.array(test_mat))\n\n\n# single test FF:\n\ndataset = ldr_fused.loadFolders(train_paths)\ntest_dataset = ldr_fused.loadFolders(test_paths)\n\ntrial_num = 100\n\nfor j in range(trial_num):\n    print(""FF: "", j)\n    pnt = PointNet(num_points=320, num_classes=6, num_epoch=32, batchsize=20, ptype=\'small\', alpha=0, beta=0)\n    res = pnt.train(dataset, test_dataset)\n    trial_acc.append(res[0])\n    trial_mat += res[1]\n    print(trial_mat)\n\ntest_acc.append(trial_acc)\ntest_mat.append(trial_mat)\ntrial_acc = []\ntrial_mat = np.zeros((6,6))\n\nnp.save(""test_results/acc"" + prefix, np.array(test_acc))\nnp.save(""test_results/mat"" + prefix, np.array(test_mat))\n\n'"
train_fusion.py,27,"b'\nfrom utils.pointnet import PointNet\nfrom utils.dualnet import DualNet\nfrom utils.datasets import MonoDataset, DuoDataset\nimport argparse\nimport numpy as np\nimport datetime\nimport os\nimport shutil\n\ncurrent_time = datetime.datetime.now()\nprefix = current_time.strftime(""%m-%d:%H:%M"") + ""fusion/""\npath = ""test_results/"" + prefix\n\nif not os.path.exists(path):\n    os.makedirs(path)\nelse:\n    shutil.rmtree(path)\n    os.makedirs(path)\n\n# initialize final variables for return\ntest_acc = []\ntest_conf = []\ntest_energy = []\n\ntrain_paths = [""train/zero/"", ""train/one/"", ""train/two/"", ""train/three/"", ""train/four/"", ""train/five/"", ""train/thumbup/"", ""train/ell/"", ""train/frame/"", ""train/bird/""]\ntest_paths = [""test/zero/"", ""test/one/"", ""test/two/"", ""test/three/"", ""test/four/"", ""test/five/"", ""test/thumbup/"", ""test/ell/"", ""test/frame/"", ""test/bird/""]\n\n# initialize running variables for collection\ntrial_acc = []\ntrial_conf = np.zeros((len(test_paths),len(test_paths)))\ntrial_energy = np.zeros(len(test_paths)*201)\n\n# IMPORTANT\nnum_trials = 25\nepochs = 100\nbs = 67\nnum_points = 320\n\n# Single test LL\n\ndataset = MonoDataset(left=True, right=False, num_points=num_points, file_paths=train_paths)\ntest_dataset = MonoDataset(left=True, right=False, num_points=num_points, file_paths=test_paths)\n\nfor j in range(num_trials):\n    print(""LL: "", j)\n    pnt = PointNet(num_points=320, num_classes=len(test_paths), num_epoch=epochs, batchsize=bs, ptype=\'small\', alpha=0.002, beta=0.01)\n    res = pnt.train(dataset, test_dataset)\n    trial_acc.append(res[0])\n    trial_conf += res[1]\n    trial_energy += res[2]\n\ntest_acc.append(trial_acc)\ntest_conf.append(trial_conf)\ntest_energy.append(trial_energy)\n\n# reinitialize running variables for collection\ntrial_acc = []\ntrial_conf = np.zeros((len(test_paths),len(test_paths)))\ntrial_energy = np.zeros(len(test_paths)*201)\n\nnp.save(path + ""acc"", np.array(test_acc))\nnp.save(path + ""conf"", np.array(test_conf))\nnp.save(path + ""energy"", np.array(test_energy))\n\n# Single test RR\n\ndataset = MonoDataset(left=False, right=True, num_points=num_points, file_paths=train_paths)\ntest_dataset = MonoDataset(left=False, right=True, num_points=num_points, file_paths=test_paths)\n\nfor j in range(num_trials):\n    print(""RR: "", j)\n    pnt = PointNet(num_points=320, num_classes=len(test_paths), num_epoch=epochs, batchsize=bs, ptype=\'small\', alpha=0.002, beta=0.01)\n    res = pnt.train(dataset, test_dataset)\n    trial_acc.append(res[0])\n    trial_conf += res[1]\n    trial_energy += res[2]\n\ntest_acc.append(trial_acc)\ntest_conf.append(trial_conf)\ntest_energy.append(trial_energy)\n\n# reinitialize running variables for collection\ntrial_acc = []\ntrial_conf = np.zeros((len(test_paths),len(test_paths)))\ntrial_energy = np.zeros(len(test_paths)*201)\n\nnp.save(path + ""acc"", np.array(test_acc))\nnp.save(path + ""conf"", np.array(test_conf))\nnp.save(path + ""energy"", np.array(test_energy))\n\n# Single test F\n\ndataset = MonoDataset(left=True, right=True, num_points=num_points, file_paths=train_paths)\ntest_dataset = MonoDataset(left=True, right=True, num_points=num_points, file_paths=test_paths)\n\nfor j in range(num_trials):\n    print(""F: "", j)\n    pnt = PointNet(num_points=320, num_classes=len(test_paths), num_epoch=epochs, batchsize=bs, ptype=\'small\', alpha=0.002, beta=0.01)\n    res = pnt.train(dataset, test_dataset)\n    trial_acc.append(res[0])\n    trial_conf += res[1]\n    trial_energy += res[2]\n\ntest_acc.append(trial_acc)\ntest_conf.append(trial_conf)\ntest_energy.append(trial_energy)\n\n# reinitialize running variables for collection\ntrial_acc = []\ntrial_conf = np.zeros((len(test_paths),len(test_paths)))\ntrial_energy = np.zeros(len(test_paths)*201)\n\nnp.save(path + ""acc"", np.array(test_acc))\nnp.save(path + ""conf"", np.array(test_conf))\nnp.save(path + ""energy"", np.array(test_energy))\n\n# Dual test\n\ndataset = DuoDataset(num_points=320, file_paths=train_paths)\ntest_dataset = DuoDataset(num_points=320, file_paths=test_paths)\n\nfor j in range(num_trials):\n    print(""Dual: "", j)\n    pnt = DualNet(num_points=320, num_classes=len(test_paths), num_epoch=epochs, batchsize=bs, ptype=\'\', alpha=0.002, beta=0.01)\n    res = pnt.train(dataset, test_dataset)\n    trial_acc.append(res[0])\n    trial_conf += res[1]\n    trial_energy += res[2]\n\ntest_acc.append(trial_acc)\ntest_conf.append(trial_conf)\ntest_energy.append(trial_energy)\n\n# reinitialize running variables for collection\ntrial_acc = []\ntrial_conf = np.zeros((len(test_paths),len(test_paths)))\ntrial_energy = np.zeros(len(test_paths)*201)\n\nnp.save(path + ""acc"", np.array(test_acc))\nnp.save(path + ""conf"", np.array(test_conf))\nnp.save(path + ""energy"", np.array(test_energy))\n\n# Dual test modified\n\ndataset = DuoDataset(num_points=320, file_paths=train_paths)\ntest_dataset = DuoDataset(num_points=320, file_paths=test_paths)\n\nfor j in range(num_trials):\n    print(""Dual Mod: "", j)\n    pnt = DualNet(num_points=320, num_classes=len(test_paths), num_epoch=epochs, batchsize=bs, ptype=\'modified\', alpha=0.002, beta=0.01)\n    res = pnt.train(dataset, test_dataset)\n    trial_acc.append(res[0])\n    trial_conf += res[1]\n    trial_energy += res[2]\n\ntest_acc.append(trial_acc)\ntest_conf.append(trial_conf)\ntest_energy.append(trial_energy)\n\n# reinitialize running variables for collection\ntrial_acc = []\ntrial_conf = np.zeros((len(test_paths),len(test_paths)))\ntrial_energy = np.zeros(len(test_paths)*201)\n\nnp.save(path + ""acc"", np.array(test_acc))\nnp.save(path + ""conf"", np.array(test_conf))\nnp.save(path + ""energy"", np.array(test_energy))\n'"
train_network_ablation.py,19,"b'\nfrom utils.pointnet import PointNet\nfrom utils.dualnet import DualNet\nfrom utils.datasets import DuoDataset, MonoDataset\nimport argparse\nimport numpy as np\nimport datetime\n\ncurrent_time = datetime.datetime.now()\nprefix = current_time.strftime(""%Y-%m-%d:%H:%M"")\n\ntest_acc = []\ntest_mat = []\ntrial_acc = []\ntrial_mat = np.zeros((10,10))\n\ntrain_paths = [""train/zero/"", ""train/one/"", ""train/two/"", ""train/three/"", ""train/four/"", ""train/five/"", ""train/thumbdown/"", ""train/thumbup/"", ""train/frame/"", ""train/bird/""]\ntest_paths = [""test/zero/"", ""test/one/"", ""test/two/"", ""test/three/"", ""test/four/"", ""test/five/"", ""test/thumbdown/"", ""test/thumbup/"", ""test/frame/"", ""test/bird/""]\n\ndataset = MonoDataset(left=True, right=False, num_points=320, file_paths=train_paths)\ntest_dataset = MonoDataset(left=True, right=False, num_points=320, file_paths=test_paths)\n\n# single test nominal:\nnum_trials = 10\n\nfor j in range(trial_num):\n    print(""nominal: "", j)\n    pnt = PointNet(num_points=320, num_classes=10, num_epoch=30, batchsize=32, ptype=\'\', alpha=0, beta=0)\n    res = pnt.train(dataset, test_dataset)\n    trial_acc.append(res[0])\n    trial_mat += res[1]\n    print(trial_mat)\n\ntest_acc.append(trial_acc)\ntest_mat.append(trial_mat)\ntrial_acc = []\ntrial_mat = np.zeros((10,10))\n\nnp.save(""test_results/acc"" + prefix, np.array(test_acc))\nnp.save(""test_results/mat"" + prefix, np.array(test_mat))\n\n# single test nominal+dropout:\n\nfor j in range(trial_num):\n    print(""dropout: "", j)\n    pnt = PointNet(num_points=320, num_classes=10, num_epoch=30, batchsize=32, ptype=\'dropout\', alpha=0, beta=0)\n    res = pnt.train(dataset, test_dataset)\n    trial_acc.append(res[0])\n    trial_mat += res[1]\n    print(trial_mat)\n\ntest_acc.append(trial_acc)\ntest_mat.append(trial_mat)\ntrial_acc = []\ntrial_mat = np.zeros((10,10))\n\nnp.save(""test_results/acc"" + prefix, np.array(test_acc))\nnp.save(""test_results/mat"" + prefix, np.array(test_mat))\n\n# single test small:\n\nfor j in range(trial_num):\n    print(""smallnet: "", j)\n    pnt = PointNet(num_points=320, num_classes=10, num_epoch=30, batchsize=32, ptype=\'small\', alpha=0, beta=0)\n    res = pnt.train(dataset, test_dataset)\n    trial_acc.append(res[0])\n    trial_mat += res[1]\n    print(trial_mat)\n\ntest_acc.append(trial_acc)\ntest_mat.append(trial_mat)\ntrial_acc = []\ntrial_mat = np.zeros((10,10))\n\n# single test small+dropout:\n\nnp.save(""test_results/acc"" + prefix, np.array(test_acc))\nnp.save(""test_results/mat"" + prefix, np.array(test_mat))\n\nfor j in range(trial_num):\n    print(""smallnet+dropout: "", j)\n    pnt = PointNet(num_points=320, num_classes=10, num_epoch=30, batchsize=32, ptype=\'small+dropout\', alpha=0, beta=0)\n    res = pnt.train(dataset, test_dataset)\n    trial_acc.append(res[0])\n    trial_mat += res[1]\n    print(trial_mat)\n\ntest_acc.append(trial_acc)\ntest_mat.append(trial_mat)\ntrial_acc = []\ntrial_mat = np.zeros((10,10))\n\n\nnp.save(""test_results/acc"" + prefix, np.array(test_acc))\nnp.save(""test_results/mat"" + prefix, np.array(test_mat))\n\ndataset = DuoDataset(num_points=320, file_paths=train_paths)\ntest_dataset = DuoDataset(num_points=320, file_paths=test_paths)\n\nfor j in range(trial_num):\n    print(""dualnet: "", j)\n    pnt = DualNet(num_points=320, num_classes=10, num_epoch=30, batchsize=32, ptype=\'\', alpha=0, beta=0)\n    res = pnt.train(dataset, test_dataset)\n    trial_acc.append(res[0])\n    trial_mat += res[1]\n    print(trial_mat)\n\ntest_acc.append(trial_acc)\ntest_mat.append(trial_mat)\ntrial_acc = []\ntrial_mat = np.zeros((10,10))\n\n\nnp.save(""test_results/acc"" + prefix, np.array(test_acc))\nnp.save(""test_results/mat"" + prefix, np.array(test_mat))\n\nfor j in range(trial_num):\n    print(""dualnet modified: "", j)\n    pnt = DualNet(num_points=320, num_classes=10, num_epoch=30, batchsize=32, ptype=\'modified\', alpha=0, beta=0)\n    res = pnt.train(dataset, test_dataset)\n    trial_acc.append(res[0])\n    trial_mat += res[1]\n    print(trial_mat)\n\ntest_acc.append(trial_acc)\ntest_mat.append(trial_mat)\ntrial_acc = []\ntrial_mat = np.zeros((10,10))\n\n\nnp.save(""test_results/acc"" + prefix, np.array(test_acc))\nnp.save(""test_results/mat"" + prefix, np.array(test_mat))\n\n\n'"
two_camera_player.py,11,"b'import pyrealsense2 as rs\nimport numpy as np\nimport cv2\nimport open3d as od\nfrom utils.player import Player\nfrom utils.processor import Processor\nimport datetime\nimport os\nimport time\n\ndef main():\n    record = True\n    plr = Player(colour=True, record=False)\n\n    # initialize placeholder point clouds\n    pc_left = rs.pointcloud()\n    pc_right = rs.pointcloud()\n\n    # create prefix and directories for naming files\n    current_time = datetime.datetime.now()\n    prefix = current_time.strftime(""%Y-%m-%d:%H:%M"")\n\n    npy_dir = ""data/"" + prefix + ""/npys/""\n    ply_dir = ""data/"" + prefix + ""/plys/""\n\n    os.makedirs(npy_dir)\n    os.makedirs(ply_dir)\n\n\n    # initialise counters for naming files\n    ply_counter = npy_counter = 0\n\n    try:\n        while True:\n            [frame_left, frame_right] = plr.getFrames()\n            if not frame_left or not frame_right:\n                continue\n            depth_frame_left = frame_left.get_depth_frame()\n            depth_frame_right = frame_right.get_depth_frame()\n            \n            depth_color_frame_left = rs.colorizer().colorize(depth_frame_left)\n            depth_color_frame_right = rs.colorizer().colorize(depth_frame_right)\n\n            # Convert depth_frame to numpy array to render image\n            depth_color_image_left = np.asanyarray(depth_color_frame_left.get_data())\n            depth_color_image_right = np.asanyarray(depth_color_frame_right.get_data())\n\n            depth_color_image = np.hstack((depth_color_image_left,depth_color_image_right))\n\n            # resize to fit screen, change as desired\n            image = cv2.resize(depth_color_image, (1440, 720))\n\n            # Render image in opencv window\n            cv2.imshow(""Depth Stream"", image)\n            key = cv2.waitKey(1)\n\n            # if s is pressed save .npy\n            if key == ord(\'s\') and not record:\n\n                points_left = pc_left.calculate(depth_frame_left)\n                points_right = pc_right.calculate(depth_frame_right)\n                \n                np.save(npy_dir + str(npy_counter) + ""left"", np.array(points_left.get_vertices()))\n                print(""File saved to "" + npy_dir + str(npy_counter) + ""left.npy"")\n\n                np.save(npy_dir + str(npy_counter) + ""right"", np.array(points_right.get_vertices()))\n                print(""File saved to "" + npy_dir + str(npy_counter) + ""right.npy"")\n\n                npy_counter += 1\n            if record:\n                # convert and save left file\n                points_left = pc_left.calculate(depth_frame_left)\n                points_left = np.array(points_left.get_vertices())\n                # print(points_left.shape)\n                points_left = points_left[np.nonzero(points_left)]\n                np.save(npy_dir + str(npy_counter) + ""left"", points_left)\n                print(""File saved to "" + npy_dir + str(npy_counter) + ""left.npy"")\n\n                # convert and save right file\n                points_right = pc_right.calculate(depth_frame_right)\n                points_right = np.array(points_right.get_vertices())\n                points_right = points_right[np.nonzero(points_right)]\n                np.save(npy_dir + str(npy_counter) + ""right"", points_right)\n                print(""File saved to "" + npy_dir + str(npy_counter) + ""right.npy"")\n\n                npy_counter += 1\n                if npy_counter > 200:\n                    raise Exception(""finished recording"")\n                # time.sleep(0.1)\n\n            \n            # if a is pressed save .ply\n            if key == ord(\'a\'):\n\n                color_frame_left = frame_left.get_color_frame()\n                color_frame_right = frame_right.get_color_frame()\n\n                # ply\'s require a colour mapping\n                pc_left.map_to(color_frame_left)\n                pc_right.map_to(color_frame_right)\n\n                points_left = pc_left.calculate(depth_frame_left)\n                points_right = pc_right.calculate(depth_frame_right)\n\n                points_left.export_to_ply(ply_dir + str(ply_counter) + ""left.ply"", color_frame_left)\n                print(""File saved to "" + ply_dir + str(ply_counter) + ""left.ply"")\n\n                points_right.export_to_ply(ply_dir + str(ply_counter) + ""right.ply"", color_frame_right)\n                print(""File saved to "" + ply_dir + str(ply_counter) + ""right.ply"")\n\n                ply_counter += 1\n\n            # if pressed escape exit program \n            if key == 27:\n                cv2.destroyAllWindows()\n                break\n    finally:\n        print(""Stopping pipelines."")\n        plr.stop()\n        print(""Compacting files."")\n        # pcr = Processor(npy_dir,crop=False, overwrite=True)\n        # pcr.compact()\n        \n\nif __name__ == ""__main__"":\n    main()\n'"
two_camera_reader.py,6,"b'import pyrealsense2 as rs\nimport numpy as np\nimport cv2\nimport open3d as od\nfrom utils.reader import Player\nfrom utils.processor import Processor\nimport datetime\nimport os\n\ndef main():\n    plr = Player(colour=True, record=False)\n\n    # initialize placeholder point clouds\n    pc_left = rs.pointcloud()\n    pc_right = rs.pointcloud()\n\n    # create prefix and directories for naming files\n    current_time = datetime.datetime.now()\n    prefix = current_time.strftime(""%Y-%m-%d:%H:%M"")\n    \n    npy_dir = ""data/"" + prefix + ""/npys/""\n    ply_dir = ""data/"" + prefix + ""/plys/""\n\n    # initialise counters for naming files\n    ply_counter = npy_counter = 0\n    image = np.zeros(1)\n\n    try:\n        while True:\n            key = cv2.waitKey(1)\n            if key == ord(\'l\'):\n                plr.playback_left.resume()\n                plr.playback_right.resume()\n                [frame_left, frame_right] = plr.getFrames()\n                if not frame_left or not frame_right:\n                    continue\n                depth_frame_left = frame_left.get_depth_frame()\n                depth_frame_right = frame_right.get_depth_frame()\n                \n                depth_color_frame_left = rs.colorizer().colorize(depth_frame_left)\n                depth_color_frame_right = rs.colorizer().colorize(depth_frame_right)\n\n                # Convert depth_frame to numpy array to render image\n                depth_color_image_left = np.asanyarray(depth_color_frame_left.get_data())\n                depth_color_image_right = np.asanyarray(depth_color_frame_right.get_data())\n\n                depth_color_image = np.hstack((depth_color_image_left,depth_color_image_right))\n\n                # resize to fit screen, change as desired\n                image = cv2.resize(depth_color_image, (1440, 720))\n            else:\n                plr.playback_left.pause()\n                plr.playback_right.pause()\n\n            # Render image in opencv window\n            cv2.imshow(""Depth Stream"", image)\n\n            # if s is pressed save .npy\n            if key == ord(\'s\'):\n\n                points_left = pc_left.calculate(depth_frame_left)\n                points_right = pc_right.calculate(depth_frame_right)\n                \n                np.save(npy_dir + str(npy_counter) + ""left"", np.array(points_left.get_vertices()))\n                print(""File saved to "" + npy_dir + str(npy_counter) + ""left.npy"")\n\n                np.save(npy_dir + str(npy_counter) + ""right"", np.array(points_right.get_vertices()))\n                print(""File saved to "" + npy_dir + str(npy_counter) + ""right.npy"")\n\n                npy_counter += 1\n\n            \n            # if a is pressed save .ply\n            if key == ord(\'a\'):\n\n                color_frame_left = frame_left.get_color_frame()\n                color_frame_right = frame_right.get_color_frame()\n\n                # ply\'s require a colour mapping\n                pc_left.map_to(color_frame_left)\n                pc_right.map_to(color_frame_right)\n\n                points_left = pc_left.calculate(depth_frame_left)\n                points_right = pc_right.calculate(depth_frame_right)\n\n                points_left.export_to_ply(ply_dir + str(ply_counter) + ""left.ply"", color_frame_left)\n                print(""File saved to "" + ply_dir + str(ply_counter) + ""left.ply"")\n\n                points_right.export_to_ply(ply_dir + str(ply_counter) + ""right.ply"", color_frame_right)\n                print(""File saved to "" + ply_dir + str(ply_counter) + ""right.ply"")\n\n                ply_counter += 1\n\n            # if pressed escape exit program \n            if key == 27:\n                cv2.destroyAllWindows()\n                break\n    finally:\n        print(""Stopping pipelines."")\n        plr.stop()\n        print(""Compacting files."")\n        pcr = Processor(npy_dir,crop=False, overwrite=False)\n        pcr.compact()\n        \n\nif __name__ == ""__main__"":\n    main()'"
archive/estimate_transform.py,4,"b'import numpy as np\nimport open3d as od\nimport copy\nimport argparse\nimport numpy as np\n\ndef draw_registration_result_original_color(source, target, transformation):\n    source_temp = copy.deepcopy(source)\n    source_temp.transform(transformation)\n    od.draw_geometries([source_temp, target])\n\nif __name__ == ""__main__"":\n    # Create object for parsing command-line options\n    parser = argparse.ArgumentParser(description=""Stitch two ply files into one."")\n\n    parser.add_argument(""-i1"", ""--input1"", type=str, help=""Path to the left ply file"")\n    parser.add_argument(""-i2"", ""--input2"", type=str, help=""Path to the right ply file"")\n    # Parse the command line arguments to an object\n    args = parser.parse_args()\n    # Safety if no parameter have been given\n    if not args.input1 or not args.input2:\n        print(""No input paramater have been given."")\n        print(""For help type --help"")\n        exit()\n    \n    source = od.read_point_cloud(args.input1)\n    target = od.read_point_cloud(args.input2)\n    # draw initial alignment\n    current_transformation = np.load(""base_transform.npy"")\n    # current_transformation = np.array([[-0.035209, 0.720650, 0.692404, -0.435650], [-0.689849, 0.483771, -0.538585, 0.248754], [-0.723097, -0.496617, 0.480106, 0.236933], [0.000000, 0.000000, 0.000000, 1.000000]])\n\n    draw_registration_result_original_color(\n            source, target, current_transformation)\n\n\n    # colored pointcloud registration\n    # This is implementation of following paper\n    # J. Park, Q.-Y. Zhou, V. Koltun,\n    # Colored Point Cloud Registration Revisited, ICCV 2017\n    # colored pointcloud registration\n\n\n\n    voxel_radius = [0.02, 0.02, 0.02, 0.01, 0.005];\n    max_iter = [ 50, 50, 50, 500, 500];\n\n    # current_transformation = np.identity(4)\n\n    for scale in range(len(voxel_radius)):\n        iter = max_iter[scale]\n        radius = voxel_radius[scale]\n        print([iter,radius,scale])\n\n        print(""3-1. Downsample with a voxel size %.2f"" % radius)\n        # source_down = source\n        # target_down = target\n        source_down = od.voxel_down_sample(source, radius)\n        target_down = od.voxel_down_sample(target, radius)\n\n        print(""3-2. Estimate normal."")\n        od.estimate_normals(source_down, od.KDTreeSearchParamHybrid(\n                radius = 2*radius, max_nn = 5))\n        od.estimate_normals(target_down, od.KDTreeSearchParamHybrid(\n                radius = 2*radius, max_nn = 5))\n\n        print(""3-3. Applying colored point cloud registration"")\n        result_icp = od.registration_colored_icp(source_down, target_down,\n                radius, current_transformation,\n                od.ICPConvergenceCriteria(relative_fitness = 1e-6,\n                relative_rmse = 1e-6, max_iteration = iter))\n        current_transformation = result_icp.transformation\n        print(""Current Transform: "", current_transformation)\n        draw_registration_result_original_color(\n            source_down, target_down, current_transformation)\n\n    draw_registration_result_original_color(    \n            source, target, current_transformation)\n    np.save(""transform"", current_transformation)\n    '"
archive/fuse.py,0,"b'from utils.fuser import Fuser\n\nfsr = Fuser(""/home/ilya/Documents/HandOfDepth/data/strip3/plys/"")\nfsr.estimateAverageTransform()'"
archive/stitch_two_npy.py,13,"b'import open3d as od\nimport numpy as np\nimport argparse\n\nparser = argparse.ArgumentParser(description=""Stitch two npy files into one."")\n\nparser.add_argument(""-i1"", ""--input1"", type=str, help=""Path to the left npy file"")\nparser.add_argument(""-i2"", ""--input2"", type=str, help=""Path to the right npy file"")\n# Parse the command line arguments to an object\nargs = parser.parse_args()\n# Safety if no parameter have been given\nif not args.input1 or not args.input2:\n    print(""No input paramater have been given."")\n    print(""For help type --help"")\n    exit()\n\nsource = np.load(args.input1)\ntarget = np.load(args.input2)\n\n\ntransform = np.load(""test2.npy"")\n\ndef applyTransform(points, transform):\n    for i in range(len(points[0])):\n        pt = np.array([[points[0][i]], [points[1][i]], [points[2][i]], [1]])\n        # print(pt)\n        pt_new = np.dot(transform, pt) \n        points[0][i] = pt_new[0,0]\n        points[1][i] = pt_new[1,0]\n        points[2][i] = pt_new[2,0]\n\ndef croppedValues(arr, xlim, ylim, zlim):\n    crop = []\n    for i, entry in enumerate(arr):\n        if abs(entry[0]) <= xlim and abs(entry[1]) <= ylim and abs(entry[2]) <= zlim:\n            crop.append(i)\n    return crop\n\ndef subsampleColumns(arr, num_points):\n    subsampling = np.random.choice([i for i in range(len(arr[0]))], num_points)\n    subsampling = np.linspace(0,len(arr[0]) - 1,num_points)\n    subsampling = np.asarray(subsampling, dtype=np.int)\n    return arr[:,subsampling]\n\n# combined = np.concatenate((source,target), axis=1)\n# print(combined.shape)\n# np.save(""result"", combined)\n\napplyTransform(target, transform)\n\nsource_sub = subsampleColumns(source, 1500)\n\n\nsource_sub = np.transpose(source_sub)\nsource = np.transpose(source)\ntarget = np.transpose(target)\n\nsource = source[croppedValues(source, 10, 10, 0.63)]\nsource_sub = source_sub[croppedValues(source_sub, 10, 10, 0.63)]\ntarget = target[croppedValues(target, 10, 10, 0.63)]\n\nsource_sub_pcd = od.PointCloud()\nsource_sub_pcd.points = od.Vector3dVector(source_sub)\n\n\nsource_pcd = od.PointCloud()\nsource_pcd.points = od.Vector3dVector(source)\n\ntarget_pcd = od.PointCloud()\ntarget_pcd.points = od.Vector3dVector(target)\n\n# od.draw_geometries([target_pcd])\nod.draw_geometries([source_pcd])\n# od.draw_geometries([source_pcd, target_pcd])\n\ndownpcd = od.voxel_down_sample(source_pcd, voxel_size = 0.0035)\nod.draw_geometries([downpcd])\nprint(len(downpcd.points))\n\n# od.draw_geometries([target_pcd])\n# od.draw_geometries([source_pcd])\n# od.draw_geometries([source_pcd, target_pcd])'"
archive/stitch_two_ply.py,1,"b'import open3d as od\nimport numpy as np\nimport argparse\n\nparser = argparse.ArgumentParser(description=""Stitch two ply files into one."")\n\nparser.add_argument(""-i1"", ""--input1"", type=str, help=""Path to the left ply file"")\nparser.add_argument(""-i2"", ""--input2"", type=str, help=""Path to the right ply file"")\n# Parse the command line arguments to an object\nargs = parser.parse_args()\n# Safety if no parameter have been given\nif not args.input1 or not args.input2:\n    print(""No input paramater have been given."")\n    print(""For help type --help"")\n    exit()\n\nsource = od.read_point_cloud(args.input1)\ntarget = od.read_point_cloud(args.input2)\ntransform = np.load(""test.npy"")\n\ntarget.transform(transform)\nod.draw_geometries([source, target])\n'"
utils/datasets.py,9,"b'import numpy as np\nimport glob\nimport torch\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader, Dataset\n\nclass MonoDataset(Dataset):\n    def __init__(self, transform=None, left=True, right=True, num_points=320, file_paths=\'\'):\n        self.identifiers = []\n        self.targets = []\n        self.data = []\n\n        def subsampleColumns(arr, num_points):\n            # no camera data\n            if arr.shape[1] == 0:\n                return np.zeros((3,num_points))\n\n            subsampling = np.linspace(0,len(arr[0]) - 1,num_points)\n            subsampling = np.asarray(subsampling, dtype=np.int)\n            return arr[:,subsampling]\n        identity = 0\n        for target, path in enumerate(file_paths):\n            # define what files to load\n            if left and right:\n                file_type=""*/npys/*fused.npy""\n            elif left:\n                file_type=""*/npys/*leftreduced.npy""\n            else:\n                file_type=""*/npys/*rightreduced.npy""\n\n            filenames = glob.glob(path + file_type)\n            print(path + file_type, len(filenames))\n            for name in filenames:\n                arr = np.load(name)\n                arr = subsampleColumns(arr, num_points)\n                # attach point cloud tensor\n                self.data.append(Variable(torch.from_numpy(arr)))\n                # attach label to each sample for error energy calculation\n                self.identifiers.append(identity)\n                identity += 1\n                # attach class target\n                self.targets.append(target)\n\n            print(""loaded: "", path)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        #get images and labels here \n        #returned images must be tensor\n        #labels should be int\n        return self.data[idx], self.targets[idx], self.identifiers[idx]\n\nclass DuoDataset(Dataset):\n    def __init__(self, transform=None, num_points=320, file_paths=\'\'):\n        self.identifiers = []\n        self.targets = []\n        self.data_left = []\n        self.data_right = []\n\n        def subsampleColumns(arr, num_points):\n            # no camera data\n            if arr.shape[1] == 0:\n                return np.zeros((3,num_points))\n            subsampling = np.linspace(0,len(arr[0]) - 1,num_points)\n            subsampling = np.asarray(subsampling, dtype=np.int)\n            return arr[:,subsampling]\n        identity = 0\n        for target, path in enumerate(file_paths):\n            # define what files to load\n            filenames_left = glob.glob(path + ""*/npys/*leftreduced.npy"")\n            filenames_right = glob.glob(path + ""*/npys/*rightreduced.npy"")\n\n            for i in range(len(filenames_left)):\n                arr_left = np.load(filenames_left[i])\n                arr_right = np.load(filenames_right[i])\n\n                arr_left = subsampleColumns(arr_left, num_points)\n                arr_right = subsampleColumns(arr_right, num_points)\n\n                # attach point cloud tensor\n                self.data_left.append(Variable(torch.from_numpy(arr_left)))\n                self.data_right.append(Variable(torch.from_numpy(arr_right)))\n\n                # attach label to each sample for error energy calculation\n                self.identifiers.append(identity)\n                identity += 1\n                # attach class target\n                self.targets.append(target)\n\n            print(""loaded: "", path)\n\n    def __len__(self):\n        return len(self.data_left)\n\n    def __getitem__(self, idx):\n        #get images and labels here \n        #returned images must be tensor\n        #labels should be int\n        return self.data_left[idx], self.data_right[idx], self.targets[idx], self.identifiers[idx]'"
utils/dualnet.py,6,"b'from __future__ import print_function\nimport argparse\nimport os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torch.utils.data\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport glob\nimport time\n\nclass DualNet(object):\n    def __init__(self, batchsize=32, num_points=2500, num_epoch=25, outf=\'cls\', model=\'\', num_classes=2, alpha=0.01, beta=0, ptype=\'\'):\n        self.batchsize=batchsize\n        self.num_points=num_points\t\n        self.num_epoch=num_epoch\n        self.outf=outf\n        self.model=model\n        self.num_classes=num_classes\n        self.alpha=alpha\n        self.beta=beta\n\n        global DualNetCls, PointNetDenseCls\n        if ptype == \'\':\n            from utils.torchnet.dualnet import DualNetCls\n        elif ptype == \'modified\':\n            from utils.torchnet.dualnet_modified import DualNetCls\n\n    def train(self, dataset, test_dataset):\n\n        def randomAugment(points_left, points_right, alpha, beta):\n            disp = np.random.rand(3,1)*beta\n            disp = np.tile(disp,self.num_points)\n            noise_left = np.random.normal(0,alpha,(3,self.num_points))\n            noise_right = np.random.normal(0,alpha,(3,self.num_points))\n            \n            for i in range(len(points_left)):        \n                points_left[i] = points_left[i].add(Variable(torch.from_numpy(disp + noise_left)).type(torch.FloatTensor))\n                points_right[i] = points_right[i].add(Variable(torch.from_numpy(disp + noise_right)).type(torch.FloatTensor))\n\n        blue = lambda x:\'\\033[94m\' + x + \'\\033[0m\'\n\n        # initialise dataloader as single thread else socket errors\n        dataloader = torch.utils.data.DataLoader(dataset, batch_size=self.batchsize,\n                                                shuffle=True)#, num_workers=int(self.workers))\n\n        testdataloader = torch.utils.data.DataLoader(test_dataset, batch_size=self.batchsize,\n                                                shuffle=True)#, num_workers=int(self.workers))\n\n        print(""size of train: "", len(dataset))\n        print(""size of test: "", len(test_dataset))\n\n        print(\'classes: \', self.num_classes)\n\n        try:\n            os.makedirs(self.outf)\n        except OSError:\n            pass\n\n\n        classifier = DualNetCls(k = self.num_classes)\n\n\n        if self.model != \'\':\n            classifier.load_state_dict(torch.load(self.model))\n\n\n        optimizer = optim.SGD(classifier.parameters(), lr=0.001, momentum=0.99)\n        classifier.cuda()\n\n        num_batch = len(dataset)/self.batchsize\n        test_acc = []\n        for epoch in range(self.num_epoch):\n            for i, data in enumerate(dataloader, 0):\n                points_left, points_right, target = data[0].type(torch.FloatTensor), data[1].type(torch.FloatTensor),  data[2].type(torch.LongTensor)\n\n                # add translation/jitter augmentation\n                randomAugment(points_left, points_right, self.alpha, self.beta)\n\n                points_left, points_right, target = Variable(points_left), Variable(points_right), Variable(target)\n                points_left, points_right, target = points_left.cuda(), points_right.cuda(), target.cuda()\n                optimizer.zero_grad()\n                classifier = classifier.train()\n                pred, _ = classifier(points_left, points_right)\n                # print(pred)\n                loss = F.nll_loss(pred, target)\n                loss.backward()\n                optimizer.step()\n                pred_choice = pred.data.max(1)[1]\n                correct = pred_choice.eq(target.data).cpu().sum()\n                # print(\'[%d: %d/%d] train loss: %f accuracy: %f\' %(epoch, i, num_batch, loss.item(),correct.item() / float(self.batchsize)))\n\n                if i % 50 == 0:\n                    j, data = next(enumerate(testdataloader, 0))\n                    points_left, points_right, target = data[0].type(torch.FloatTensor), data[1].type(torch.FloatTensor),  data[2].type(torch.LongTensor)\n                    points_left, points_right, target = Variable(points_left), Variable(points_right), Variable(target)\n                    points_left, points_right, target = points_left.cuda(), points_right.cuda(), target.cuda()\n                    classifier = classifier.eval()\n                    pred, _ = classifier(points_left, points_right)\n                    # print(pred)\n                    loss = F.nll_loss(pred, target)\n                    pred_choice = pred.data.max(1)[1]\n                    correct = pred_choice.eq(target.data).cpu().sum()\n                    print(\'[%d: %d/%d] %s loss: %f accuracy: %f\' %(epoch, i, num_batch, blue(\'test\'), loss.item(), correct.item()/float(self.batchsize)))\n\n            # out, _ = classifier.sim_data\n            # print(out[0:10])\n            torch.save(classifier.state_dict(),""model"")# \'%s/cls_model_%d.pth\' % (self.outf, epoch))\n\n        acc = 0\n        confusion_matrix = np.zeros((self.num_classes, self.num_classes))\n        # empty matrix for recording instance accuracies\n        accuracy_matrix = np.zeros(len(test_dataset))\n\n        classifier.eval()\n        with torch.no_grad():\n            # self.batchsize = 1\n            testdataloader = torch.utils.data.DataLoader(test_dataset, self.batchsize, shuffle=True)\n            for i, data in enumerate(testdataloader, 0):\n                points_left, points_right, target, identifier = data[0].type(torch.FloatTensor), data[1].type(torch.FloatTensor),  data[2].type(torch.LongTensor), data[3]\n                points_left, points_right, target = Variable(points_left), Variable(points_right), Variable(target)\n                points_left, points_right, target = points_left.cuda(), points_right.cuda(), target.cuda()\n                # start = time.time()\n                pred, _ = classifier(points_left, points_right)\n                # print(""time: "", time.time()-start)\n                # print(target, pred)\n                loss = F.nll_loss(pred, target)\n                pred_choice = pred.data.max(1)[1]\n                correct = pred_choice.eq(target.data).cpu().sum()\n                # print(correct.item())\n                acc += correct.item()\n\n                # update confusion matrix and accuracy\n                for i, t in enumerate(target):\n                    confusion_matrix[int(t), int(pred_choice[i])] += 1\n                    if int(t) == int(pred_choice[i]): # if correct set to 1\n                        accuracy_matrix[int(identifier[i])] = 1 \n            \n        print(""final acc: "", acc/len(test_dataset))\n        return acc/len(test_dataset), confusion_matrix, accuracy_matrix\n'"
utils/fuser.py,4,"b'import numpy as np\nimport open3d as od\nimport copy\nimport argparse\nimport numpy as np\nimport glob\n\nclass Fuser(object):\n    def __init__(self, file_path):\n        self.file_path = file_path\n        self.base_transform = np.linalg.inv(np.load(""base_transform_new.npy""))\n        self.voxel_radius = [0.01, 0.01, 0.01]\n        self.max_iter = [50, 50, 50]\n        self.max_nn = 3\n        self.relative_fitness = 1e-6\n        self.relative_rmse = 1e-6\n\n    def draw_registration_result_original_color(self, source, target, transformation):\n        source_temp = copy.deepcopy(source)\n        source_temp.transform(transformation)\n        od.draw_geometries([source_temp, target])\n\n    def estimateAverageTransform(self):\n        targets = glob.glob(self.file_path+""*left.ply"")\n        sources = glob.glob(self.file_path+""*right.ply"")\n        assert len(sources) == len(targets)\n        transform_list = []\n        for i in range(len(sources)):\n            tran = (self.estimateTransform(sources[i], targets[i]))\n            transform_list.append(tran)\n            if np.linalg.norm(self.base_transform - tran) < 0.1:\n                self.base_transform =  tran\n        \n        # estimate good average transform\n        # avg_transformation = self.base_transform\n\n        avg_transformation =  np.mean(transform_list, axis=0)\n        print(avg_transformation)\n        # print(transform_list)\n\n        # display final result\n        for i in range(len(transform_list) - 1):\n            source = od.read_point_cloud(sources[i])\n            target = od.read_point_cloud(targets[i])\n            self.draw_registration_result_original_color(    \n        source, target, avg_transformation)\n        np.save(self.file_path + ""transform"", avg_transformation)\n        print(avg_transformation)\n\n    # use color icp to estimate transform between two point clouds\n    def estimateTransform(self, source, target):\n        source = od.read_point_cloud(source)\n        target = od.read_point_cloud(target)\n        current_transformation = self.base_transform\n\n        for scale in range(len(self.voxel_radius)):\n            iterations = self.max_iter[scale]\n            radius = self.voxel_radius[scale]\n\n            source_down = od.voxel_down_sample(source, radius)\n            target_down = od.voxel_down_sample(target, radius)\n\n            od.estimate_normals(source_down, od.KDTreeSearchParamHybrid(\n                    radius = 2*radius, max_nn = self.max_nn))\n            od.estimate_normals(target_down, od.KDTreeSearchParamHybrid(\n                    radius = 2*radius, max_nn = self.max_nn))\n\n            result_icp = od.registration_colored_icp(source_down, target_down,\n                    radius, current_transformation,\n                    od.ICPConvergenceCriteria(relative_fitness = self.relative_fitness,\n                    relative_rmse = self.relative_rmse, max_iteration = iterations))\n\n            current_transformation = result_icp.transformation\n            # self.draw_registration_result_original_color(\n            # source_down, target_down, current_transformation)\n        \n        return current_transformation'"
utils/player.py,0,"b'import pyrealsense2 as rs\nimport numpy as np\nimport cv2\nimport datetime\nimport time\nimport json\nimport os\nimport shutil\n\n\nclass Player(object):\n    def __init__(self, colour=False, infrared=False, record=True, hw_sync=True):\n        self.colour = colour\n        self.ctx = rs.context()\n        self.devices = self.ctx.query_devices()\n        print(""Detected device serial numbers: "")\n        try:\n            self.device_left = self.devices[0].get_info(rs.camera_info.serial_number)\n            self.device_right = self.devices[1].get_info(rs.camera_info.serial_number)\n        except Exception:\n            raise(Exception(""Two devices not found.""))\n\n        print(self.device_left)\n        print(self.device_right)\n\n        # load settings from json\n\n        json_data = open(""hand.json"").read()\n        adv_string = json.loads(json_data)\n        json_string = str(adv_string).replace(""\'"", \'\\""\')\n        advnc_mode_left = rs.rs400_advanced_mode(self.devices[0])\n        advnc_mode_right = rs.rs400_advanced_mode(self.devices[1])\n        advnc_mode_left.load_json(json_string)\n        advnc_mode_right.load_json(json_string)\n\n        # create save folder\n        current_time = datetime.datetime.now()\n        prefix = current_time.strftime(""%Y-%m-%d:%H:%M"")\n\n        path = ""data/"" + prefix + ""/""\n        if not os.path.exists(path):\n            os.makedirs(path)\n        else:\n            shutil.rmtree(path)\n            os.makedirs(path)\n\n\n        self.pipeline_left = rs.pipeline()\n        self.config_left = rs.config()\n        self.config_left.enable_device(self.device_left)\n        self.config_left.enable_stream(rs.stream.depth, 1280, 720, rs.format.z16, 30) \n\n\n        if infrared:    \n            self.config_left.enable_stream(rs.stream.infrared, 1, 1280, 720, rs.format.y8, 30)\n        if colour:\n            self.config_left.enable_stream(rs.stream.color, 1280, 720, rs.format.rgb8, 30)\n        if record:\n            self.config_left.enable_record_to_file(""data/"" + prefix + ""/left.bag"")\n\n\n        \n\n\n        self.pipeline_right = rs.pipeline()\n        self.config_right = rs.config()\n        self.config_right.enable_device(self.device_right)\n        self.config_right.enable_stream(rs.stream.depth, 1280, 720, rs.format.z16, 30)\n        if infrared:\n            self.config_right.enable_stream(rs.stream.infrared, 1, 1280, 720, rs.format.y8, 30)\n        if colour:\n            self.config_right.enable_stream(rs.stream.color, 1280, 720, rs.format.rgb8, 30)\n        if record:\n            self.config_right.enable_record_to_file(""data/"" + prefix + ""/right.bag"")\n\n\n        #print(""Starting Recording in:"")\n        # print(""3"")\n        # time.sleep(1)\n        # print(""2"")\n        # time.sleep(1)\n        # print(""1"")\n        # time.sleep(1)\n        # print(""now"")\n\n        # Start streaming\n        \n        self.playback_left = self.pipeline_left.start(self.config_left)\n        start = time.time()\n        self.playback_right = self.pipeline_right.start(self.config_right)\n        print(""Stream desync: "", time.time() - start)\n\n        self.profile_left = self.playback_left.get_stream(rs.stream.depth)\n        self.intrinsics_left = self.profile_left.as_video_stream_profile().get_intrinsics()\n        print(""left intrinsics: "", self.intrinsics_left)\n\n        self.profile_right = self.playback_right.get_stream(rs.stream.depth)\n        self.intrinsics_right = self.profile_right.as_video_stream_profile().get_intrinsics() \n        print(""right intrinsics: "", self.intrinsics_right)\n        if hw_sync:\n            self.ds_left = self.devices[0].query_sensors()[0]\n            self.ds_left.set_option(rs.option.inter_cam_sync_mode, 1)\n            print(""left option: "", [""null"", ""master"", ""slave""][int(self.ds_left.get_option(rs.option.inter_cam_sync_mode))])\n\n            self.ds_right = self.devices[1].query_sensors()[0]\n            self.ds_right.set_option(rs.option.inter_cam_sync_mode, 2)\n            print(""right option: "", [""null"", ""master"", ""slave""][int(self.ds_right.get_option(rs.option.inter_cam_sync_mode))])\n        \n\n        \n    # return current camera frames  (depth and colour)\n    def getFrames(self):\n        #frameset_left = rs.composite_frame(rs.frame())\n        #frameset_right = rs.composite_frame(rs.frame())\n        #frame_right = self.pipeline_right.wait_for_frames()\n        #frame_left = self.pipeline_left.poll_for_frames(frameset_left)\n\n        #self.playback_left.resume()    \n        frame_left = self.pipeline_left.wait_for_frames()\n        #self.playback_left.pause()\n\n        #self.playback_right.resume()\n        frame_right = self.pipeline_right.wait_for_frames()\n        #self.playback_right.pause()\n\n        ret_frames = [None, None]\n\n        if frame_left:\n            ret_frames[0] = frame_left\n        # else:\n        #     time.sleep(0.015)\n\n        if frame_right:\n            ret_frames[1] = frame_right\n        # else:\n        #     time.sleep(0.015) \n        #print(ret_frames[0].timestamp - ret_frames[1].timestamp)\n        return ret_frames\n        \n            \n            \n    def stop(self):\n        self.pipeline_right.stop()\n        self.pipeline_left.stop()\n\n\n\n'"
utils/pointnet.py,5,"b'from __future__ import print_function\nimport argparse\nimport os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torch.utils.data\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport glob\nimport time\n\nclass PointNet(object):\n    def __init__(self, batchsize=32, num_points=2500, num_epoch=25, outf=\'cls\', model=\'\', num_classes=2, alpha=0.01, beta=0.02, ptype=\'\'):\n        self.batchsize=batchsize\n        self.num_points=num_points\t\n        self.num_epoch=num_epoch\n        self.outf=outf\n        self.model=model\n        self.num_classes=num_classes\n        self.alpha=alpha\n        self.beta=beta\n\n        global PointNetCls, PointNetDenseCls\n        if ptype == \'\':\n            from utils.torchnet.pointnet import PointNetCls, PointNetDenseCls\n        elif ptype == \'small\':\n            from utils.torchnet.pointnet_small import PointNetCls, PointNetDenseCls\n        elif ptype == \'dropout\':\n            from utils.torchnet.pointnet_dropout import PointNetCls, PointNetDenseCls\n        elif ptype == \'normless\':\n            from utils.torchnet.pointnet_normless import PointNetCls, PointNetDenseCls\n        elif ptype == \'small+dropout\':\n            from utils.torchnet.pointnet_small_dropout import PointNetCls, PointNetDenseCls\n\n\n\n    def train(self, dataset, test_dataset):\n\n        def randomAugment(points, alpha, beta):\n            disp = np.random.rand(3,1)*beta\n            disp = np.tile(disp,self.num_points)\n            noise = np.random.normal(0,alpha,(3,self.num_points))\n            for i in range(len(points)):        \n                points[i] = points[i].add(Variable(torch.from_numpy(disp + noise)).type(torch.FloatTensor))\n\n        blue = lambda x:\'\\033[94m\' + x + \'\\033[0m\'\n\n        # initialise dataloader as single thread else socket errors\n        dataloader = torch.utils.data.DataLoader(dataset, batch_size=self.batchsize,\n                                                shuffle=True)#, num_workers=int(self.workers))\n\n        testdataloader = torch.utils.data.DataLoader(test_dataset, batch_size=self.batchsize,\n                                                shuffle=True)#, num_workers=int(self.workers))\n\n        print(""size of train: "", len(dataset))\n        print(""size of test: "", len(test_dataset))\n\n        print(\'classes: \', self.num_classes)\n\n        try:\n            os.makedirs(self.outf)\n        except OSError:\n            pass\n\n\n        classifier = PointNetCls(k = self.num_classes)\n\n\n        if self.model != \'\':\n            classifier.load_state_dict(torch.load(self.model))\n\n\n        optimizer = optim.SGD(classifier.parameters(), lr=0.001,momentum=0.99)\n        classifier.cuda()\n\n        num_batch = len(dataset)/self.batchsize\n        test_acc = []\n        for epoch in range(self.num_epoch):\n            for i, data in enumerate(dataloader, 0):\n                points, target = data[0].type(torch.FloatTensor), data[1].type(torch.LongTensor)\n\n                # add translation/jitter augmentation\n                randomAugment(points, self.alpha, self.beta)\n\n                points, target = Variable(points), Variable(target)\n                points, target = points.cuda(), target.cuda()\n                optimizer.zero_grad()\n                classifier = classifier.train()\n                pred, _ = classifier(points)\n                # print(pred)\n                loss = F.nll_loss(pred, target)\n                loss.backward()\n                optimizer.step()\n                pred_choice = pred.data.max(1)[1]\n                correct = pred_choice.eq(target.data).cpu().sum()\n                # print(\'[%d: %d/%d] train loss: %f accuracy: %f\' %(epoch, i, num_batch, loss.item(),correct.item() / float(self.batchsize)))\n\n                if i % 50 == 0:\n                    j, data = next(enumerate(testdataloader, 0))\n                    points, target = data[0].type(torch.FloatTensor), data[1].type(torch.LongTensor)\n                    points, target = Variable(points), Variable(target)\n                    # points = points.transpose(2,1)\n                    points, target = points.cuda(), target.cuda()\n                    classifier = classifier.eval()\n                    pred, _ = classifier(points)\n                    # print(pred)\n                    loss = F.nll_loss(pred, target)\n                    pred_choice = pred.data.max(1)[1]\n                    correct = pred_choice.eq(target.data).cpu().sum()\n                    print(\'[%d: %d/%d] %s loss: %f accuracy: %f\' %(epoch, i, num_batch, blue(\'test\'), loss.item(), correct.item()/float(self.batchsize)))\n\n            # out, _ = classifier.sim_data\n            # print(out[0:10])\n            torch.save(classifier.state_dict(),""model"")# \'%s/cls_model_%d.pth\' % (self.outf, epoch))\n\n        acc = 0\n        confusion_matrix = np.zeros((self.num_classes, self.num_classes))\n        # empty matrix for recording instance accuracies\n        accuracy_matrix = np.zeros(len(test_dataset))\n\n        classifier.eval()\n        with torch.no_grad():\n            # self.batchsize = 1\n            testdataloader = torch.utils.data.DataLoader(test_dataset, self.batchsize, shuffle=True)\n            for i, data in enumerate(testdataloader, 0):\n                points, target, identifier = data[0].type(torch.FloatTensor), data[1].type(torch.LongTensor), data[2]\n                points, target = Variable(points), Variable(target)\n                points, target = points.cuda(), target.cuda()\n                start = time.time()\n                pred, _ = classifier(points)\n                # print(""time: "", time.time()-start)\n                # print(target, pred)\n                loss = F.nll_loss(pred, target)\n                pred_choice = pred.data.max(1)[1]\n                correct = pred_choice.eq(target.data).cpu().sum()\n                # print(correct.item())\n                acc += correct.item()\n\n                # update confusion matrix and accuracy\n                for i, t in enumerate(target):\n                    confusion_matrix[int(t), int(pred_choice[i])] += 1\n                    if int(t) == int(pred_choice[i]): # if correct set to 1\n                        accuracy_matrix[int(identifier[i])] = 1 \n            \n        print(""final acc: "", acc/len(test_dataset))\n        return acc/len(test_dataset), confusion_matrix, accuracy_matrix\n'"
utils/processor.py,52,"b'import numpy as np\nimport glob\nimport open3d as od\nimport re\nimport os\nimport threading\nimport scipy.signal\n\nclass Processor(object):\n    # initialise with limits for point cloud cropping, as well as file path\n    def __init__(self, file_path, xlim=100, ylim=100, zlim=0.1, crop=True, overwrite=False):\n        self.xlim = xlim\n        self.ylim = ylim\n        self.zlim = zlim\n        self.file_path = file_path\n        self.crop = crop\n        self.overwrite = overwrite\n\n    def compact(self):\n        def atoi(text):\n            return int(text) if text.isdigit() else text.lower()\n            \n        def natural_keys(text):\n            return [atoi(c) for c in re.split(\'(\\d+)\', text)]\n\n        def croppedValues(arr):\n            crop = []\n            for i, entry in enumerate(arr):\n                if abs(entry[0]) <= self.xlim and abs(entry[1]) <= self.ylim and abs(entry[2]) <= self.zlim:\n                    crop.append(i)\n            return crop\n\n        npy_list = glob.glob(self.file_path + ""*.npy"")\n        npy_list.sort(key=natural_keys)\n        npy_lists = np.array_split(npy_list, 6)\n        threads = []\n        for i in range(len(npy_lists)):\n            # print(npy_list)\n            t = threading.Thread(target=self.compact_worker, args=(npy_lists[i],))\n            threads.append(t)\n            t.start()\n       \n    def compact_worker(self, npy_list):\n            for npy in npy_list:\n                points = np.load(npy)\n                print(""started: "", npy)\n                # if this is an unprocessed file, process\n                if type(points[0]) is type(np.void(0)):\n                    points = points[np.nonzero(points)]\n                    points_new = np.empty((3,len(points)),dtype=np.ndarray)\n\n                    # convert np.void (what realsense returns) back into float32 array\n                    for i in range(len(points)):\n                        points_new[0][i] = points[i][0]\n                        points_new[1][i] = points[i][1]\n                        points_new[2][i] = points[i][2]\n                    points_new = np.asarray(points_new, dtype=np.float32)\n                    # if crop, crop\n                    if self.crop:\n                        points_new = points_new[croppedValues(points_new)]\n                    \n                    print(""finished: "", npy)\n                    if self.overwrite:\n                        np.save(npy, points_new)\n                    else:\n                        new_name = self.file_path + ""processed"" + os.path.basename(npy)\n                        np.save(new_name, points_new)\n\n                else:\n                    print(""finished: "", npy)\n                    print(""skipped"")\n\n    def compactSingleThread(self):\n        def atoi(text):\n            return int(text) if text.isdigit() else text.lower()\n            \n        def natural_keys(text):\n            return [atoi(c) for c in re.split(\'(\\d+)\', text)]\n\n        def croppedValues(arr):\n            crop = []\n            for i, entry in enumerate(arr):\n                if abs(entry[0]) <= self.xlim and abs(entry[1]) <= self.ylim and abs(entry[2]) <= self.zlim:\n                    crop.append(i)\n            return crop\n\n        npy_list = glob.glob(self.file_path + ""npys/*.npy"")\n        npy_list.sort(key=natural_keys)\n        for npy in npy_list:\n                points = np.load(npy)\n                print(""started: "", npy)\n                # if this is an unprocessed file, process\n                if type(points[0]) is type(np.void(0)):\n                    points = points[np.nonzero(points)]\n                    points_new = np.array([[points[0][0]],[points[0][1]],[points[0][2]]])\n\n                    # convert np.void (what realsense returns) back into float32 array\n                    for i in range(1, len(points)):\n                        arr = np.array([[points[i][0]],[points[i][1]],[points[i][2]]])\n                        points_new = np.hstack((points_new, arr))\n                    \n                    # if crop, crop\n                    if self.crop:\n                        points_new = points_new[croppedValues(points_new)]\n\n                    self.plot(points_new)\n                    \n                    print(""finished: "", npy)\n                    if self.overwrite:\n                        np.save(npy, points_new)\n                    else:\n                        new_name = self.file_path + ""npys/processed"" + os.path.basename(npy)\n                        np.save(new_name, points_new)\n\n                else:\n                    print(""finished: "", npy)\n                    print(""skipped"")\n\n    def plot(self, arr): \n        arr = np.transpose(arr)\n        for i in range(len(arr)):\n            arr[i] = (arr[i][0],arr[i][1],arr[i][2])\n        pcd = od.PointCloud()\n        pcd.points = od.Vector3dVector(arr)\n        od.draw_geometries([pcd])\n\n    def fuse(self):\n        def atoi(text):\n            return int(text) if text.isdigit() else text.lower()\n            \n        def natural_keys(text):\n            return [atoi(c) for c in re.split(\'(\\d+)\', text)]\n\n        transform = np.load(""base_transform_new.npy"")\n\n        npy_list_left = glob.glob(self.file_path + ""*left.npy"")\n        npy_list_left.sort(key=natural_keys)\n\n        npy_list_right = glob.glob(self.file_path + ""*right.npy"")\n        npy_list_right.sort(key=natural_keys)\n        # print(npy_list_right)\n        if len(npy_list_left) != len(npy_list_right):\n            raise Exception(""Mismatch in file numbers."")\n\n        npy_lists_left = np.array_split(npy_list_left, 6)\n        npy_lists_right = np.array_split(npy_list_right, 6)\n        \n        threads = []\n        for i in range(len(npy_lists_left)):\n            t = threading.Thread(target=self.fuse_worker, args=(npy_lists_left[i],npy_lists_right[i],transform,))\n            threads.append(t)\n            t.start()\n\n    def fuse_worker(self, npy_list_left, npy_list_right, transform):\n        def croppedValues(arr):\n            crop = []\n            hist = np.histogram(arr[:,2], 30)\n            print(hist)\n            peaks = scipy.signal.find_peaks(hist[0], height=len(arr[:,2])/40,distance=15)\n            # print(peaks)\n\n            if len(peaks[0]) >= 2: # two peaks, cut face/body, isolate hand\n                peak1 = peaks[0][0]\n                peak2 = peaks[0][1]\n                min_z = max(hist[1][peak1+5], hist[1][peak2-5])\n\n            elif len(peaks[0]) == 1: # one peak, isolate hand\n                peak = peaks[0][0]\n                min_z = hist[1][peak+5]\n\n            else: # no peaks, um, try something else...\n                min_z = hist[1][2] + 0.2\n\n            for i, entry in enumerate(arr):\n                if entry[2] <= (min_z):\n                    crop.append(i)\n\n            if len(crop) < 320: # we accidentally cropped the whole thing\n                min_z = min(arr[:,2]) + 0.15\n\n                for i, entry in enumerate(arr):\n                    if entry[2] <= (min_z):\n                        crop.append(i)\n                if len(crop) < 320: \n                    return list(range(0,len(arr.T[0])))\n            return crop\n\n\n        for i in range(len(npy_list_left)):\n            source_name = npy_list_right[i]\n            target_name = npy_list_left[i]\n            source = np.load(source_name)\n            target = np.load(target_name)\n            print(""loaded: "", source_name,"" and "", target_name)\n\n            source = np.transpose(source)\n            target = np.transpose(target)\n\n            source_pcd = od.PointCloud()\n            source_pcd.points = od.Vector3dVector(source)\n\n            target_pcd = od.PointCloud()\n            target_pcd.points = od.Vector3dVector(target)\n\n            source = source[croppedValues(source)]\n            target = target[croppedValues(target)]\n            \n            # voxel downsampling\n            source_pcd = od.voxel_down_sample(source_pcd, voxel_size = 0.008)\n            target_pcd = od.voxel_down_sample(target_pcd, voxel_size = 0.008)\n\n            np.save(source_name[:-4] + ""reduced"",  np.asarray(source_pcd.points).T)\n            source_pcd.transform(transform)\n            np.save(source_name[:-4] + ""reducedtrans"",  np.asarray(source_pcd.points).T)\n\n\n            # od.draw_geometries([source_pcd])\n            # od.draw_geometries([target_pcd])\n\n            source = np.asarray(source_pcd.points).T\n            target = np.asarray(target_pcd.points).T    \n            \n            np.save(target_name[:-4] + ""reduced"", target)\n            np.save(target_name[:-8] + ""fused"", np.concatenate((source, target), axis=1))\n            print(""reduced: "", source_name,"" and "", target_name)\n    \n    # def fuseSingleThread(self):\n    #     def atoi(text):\n    #         return int(text) if text.isdigit() else text.lower()\n            \n    #     def natural_keys(text):\n    #         return [atoi(c) for c in re.split(\'(\\d+)\', text)]\n\n    #     transform = np.load(""test2.npy"")\n\n    #     npy_list_left = glob.glob(self.file_path + ""*left.npy"")\n    #     npy_list_left.sort(key=natural_keys)\n\n    #     npy_list_right = glob.glob(self.file_path + ""*right.npy"")\n    #     npy_list_right.sort(key=natural_keys)\n\n    #     if len(npy_list_left) != len(npy_list_right):\n    #         raise Exception(""Mismatch in file numbers."")\n\n    #     def croppedValues(arr):\n    #         crop = []\n    #         for i, entry in enumerate(arr):\n    #             if abs(entry[0]) <= self.xlim and abs(entry[1]) <= self.ylim and abs(entry[2]) <= self.zlim:\n    #                 crop.append(i)\n    #         return crop\n\n    #     for i in range(len(npy_list_left)):\n    #         source_name = npy_list_right[i]\n    #         target_name = npy_list_left[i]\n    #         source = np.load(source_name)\n    #         target = np.load(target_name)\n    #         # print(""loaded: "", source_name,"" and "", target_name)\n\n    #         source = np.transpose(source)\n    #         target = np.transpose(target)\n\n    #         source = source[croppedValues(source)]\n    #         target = target[croppedValues(target)]\n\n    #         source_pcd = od.PointCloud()\n    #         source_pcd.points = od.Vector3dVector(source)\n\n    #         target_pcd = od.PointCloud()\n    #         target_pcd.points = od.Vector3dVector(target)\n            \n    #         # voxel downsampling\n    #         source_pcd = od.voxel_down_sample(source_pcd, voxel_size = 0.0035)\n    #         target_pcd = od.voxel_down_sample(target_pcd, voxel_size = 0.0035)\n\n    #         source_pcd.transform(transform)\n\n    #         # od.draw_geometries([source_pcd])\n    #         # od.draw_geometries([target_pcd])\n\n    #         source = np.asarray(source_pcd.points)\n    #         target = np.asarray(target_pcd.points)\n\n    #         source = np.transpose(source)\n    #         target = np.transpose(target)\n    #         np.save(source_name[:-4] + ""reduced"", source)\n    #         np.save(target_name[:-4] + ""reduced"", target)\n    #         np.save(target_name[:-8] + ""fused"", np.concatenate((source, target), axis=1))\n    #         # print(""reduced: "", source_name,"" and "", target_name)\n\n    ## convert old npys to new npys\n    # def convert(self):\n    #     def atoi(text):\n    #         return int(text) if text.isdigit() else text.lower()\n            \n    #     def natural_keys(text):\n    #         return [atoi(c) for c in re.split(\'(\\d+)\', text)]\n\n    #     npy_list = glob.glob(self.file_path + ""npys/*.npy"")\n    #     npy_list.sort(key=natural_keys)\n    #     for npy in npy_list:\n    #         points = np.load(npy)\n    #         print(""started: "", npy)\n    #         points_new = np.array([[points[0][0]],[points[0][1]],[points[0][2]]])\n\n    #         # convert np.void (what realsense returns) back into float32 array\n    #         for i in range(1, len(points)):\n    #             arr = np.array([[points[i][0]],[points[i][1]],[points[i][2]]])\n    #             points_new = np.hstack((points_new, arr))\n    #         # self.plot(points_new)\n    #         print(""finished: "", npy)\n    #         if self.overwrite:\n    #             np.save(npy, points_new)\n    #         else:\n    #             new_name = self.file_path + ""npys/processed"" + os.path.basename(npy)\n    #             np.save(new_name, points_new)\n\n    # register npys based on single transform, and also downsample\n'"
utils/reader.py,0,"b'import pyrealsense2 as rs\nimport numpy as np\nimport cv2\nimport argparse\nimport os.path\nimport time\n\nclass Player(object):\n    def __init__(self, colour=False, infrared=False, record=False, hw_sync=False):\n        # Create object for parsing command-line options\n        parser = argparse.ArgumentParser(description=""Read recorded bag file and display depth stream in jet colormap.\\\n                                        Remember to change the stream resolution, fps and format to match the recorded."")\n        # Add argument which takes path to a bag file as an input\n        parser.add_argument(""-i1"", ""--input1"", type=str, help=""Path to the left bag file"")\n        parser.add_argument(""-i2"", ""--input2"", type=str, help=""Path to the right bag file"")\n        # Parse the command line arguments to an object\n        args = parser.parse_args()\n        # Safety if no parameter have been given\n        if not args.input1 or not args.input2:\n            print(""No input paramater have been given."")\n            print(""For help type --help"")\n            exit()\n        # Check if the given file have bag extension\n        if os.path.splitext(args.input1)[1] != "".bag"" or os.path.splitext(args.input2)[1] != "".bag"":\n            print(""The given file is not of correct file format."")\n            print(""Only .bag files are accepted"")\n            exit()\n\n\n        self.pipeline_left = rs.pipeline()\n        self.config_left = rs.config()\n        rs.config.enable_device_from_file(self.config_left, args.input1)\n        self.config_left.enable_stream(rs.stream.depth, 1280, 720, rs.format.z16, 30) \n\n        if infrared:    \n            self.config_left.enable_stream(rs.stream.infrared, 1, 1280, 720, rs.format.y8, 30)\n        if colour:\n            self.config_left.enable_stream(rs.stream.color, 1280, 720, rs.format.rgb8, 30)\n\n        self.pipeline_right = rs.pipeline()\n        self.config_right = rs.config()\n        rs.config.enable_device_from_file(self.config_right, args.input2)\n        self.config_right.enable_stream(rs.stream.depth, 1280, 720, rs.format.z16, 30)\n        if infrared:\n            self.config_right.enable_stream(rs.stream.infrared, 1, 1280, 720, rs.format.y8, 30)\n        if colour:\n            self.config_right.enable_stream(rs.stream.color, 1280, 720, rs.format.rgb8, 30)\n\n        # Start streaming from file and pause the streams\n        self.start_left = self.pipeline_left.start(self.config_left)\n        self.playback_left = self.start_left.get_device().as_playback()\n        time.sleep(0.2)\n        # time.sleep(0.219)\n        self.start_right = self.pipeline_right.start(self.config_right)\n        self.playback_right =  self.start_right.get_device().as_playback()\n\n        # Create opencv window to render image in\n        cv2.namedWindow(""Depth Stream"", cv2.WINDOW_AUTOSIZE)\n\n        self.profile_left = self.start_left.get_stream(rs.stream.depth)\n        self.intrinsics_left = self.profile_left.as_video_stream_profile().get_intrinsics()\n        print(""left intrinsics: "", self.intrinsics_left)\n\n        self.profile_right = self.start_right.get_stream(rs.stream.depth)\n        self.intrinsics_right = self.profile_right.as_video_stream_profile().get_intrinsics() \n        print(""right intrinsics: "", self.intrinsics_right)\n\n\n        \n    # return current camera frames  (depth and colour)\n    def getFrames(self): \n        # frame_right = self.pipeline_right.wait_for_frames() \n        # frame_left = self.pipeline_left.wait_for_frames()\n        time.sleep(0.04)\n        frame_left = rs.composite_frame(rs.frame())\n        frame_right = rs.composite_frame(rs.frame())\n\n        self.playback_right.resume()\n        s_left = self.pipeline_right.poll_for_frames(frame_left)\n        self.playback_right.pause()\n\n        self.playback_left.resume()\n        s_right = self.pipeline_left.poll_for_frames(frame_right)\n        self.playback_left.pause()\n        if not s_right or not s_left:\n            [frame_left, frame_right] = self.getFrames()\n\n        return [frame_left, frame_right]\n        \n            \n            \n    def stop(self):\n        self.pipeline_right.stop()\n        self.pipeline_left.stop()\n\n\n\n'"
utils/torchnet/dualnet.py,1,"b'from __future__ import print_function\nimport argparse\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torch.utils.data\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nfrom torch.autograd import Variable\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pdb\nimport torch.nn.functional as F\n\n\nclass STN3d(nn.Module):\n    def __init__(self):\n        super(STN3d, self).__init__()\n        self.conv1 = torch.nn.Conv1d(3, 64, 1)\n        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n        self.conv3 = torch.nn.Conv1d(128, 256, 1)\n        self.fc1 = nn.Linear(256, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, 9)\n        self.relu = nn.ReLU()\n\n        self.bn1 = nn.BatchNorm1d(64)\n        self.bn2 = nn.BatchNorm1d(128)\n        self.bn3 = nn.BatchNorm1d(256)\n        self.bn4 = nn.BatchNorm1d(128)\n        self.bn5 = nn.BatchNorm1d(64)\n\n\n    def forward(self, x):\n        batchsize = x.size()[0]\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.relu(self.bn3(self.conv3(x)))\n        x = torch.max(x, 2, keepdim=True)[0]\n        x = x.view(-1, 256)\n\n        x = F.relu(self.bn4(self.fc1(x)))\n        x = F.relu(self.bn5(self.fc2(x)))\n        x = self.fc3(x)\n\n        iden = Variable(torch.from_numpy(np.array([1,0,0,0,1,0,0,0,1]).astype(np.float32))).view(1,9).repeat(batchsize,1)\n        if x.is_cuda:\n            iden = iden.cuda()\n        x = x + iden\n        x = x.view(-1, 3, 3)\n        return x\n\n\nclass PointNetfeat(nn.Module):\n    def __init__(self, global_feat = True):\n        super(PointNetfeat, self).__init__()\n        self.stn = STN3d()\n        self.conv1 = torch.nn.Conv1d(3, 64, 1)\n        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n        self.conv3 = torch.nn.Conv1d(128, 256, 1)\n        self.bn1 = nn.BatchNorm1d(64)\n        self.bn2 = nn.BatchNorm1d(128)\n        self.bn3 = nn.BatchNorm1d(256)\n        self.global_feat = global_feat\n    def forward(self, x):\n        batchsize = x.size()[0]\n        n_pts = x.size()[2]\n        trans = self.stn(x)\n        x = x.transpose(2,1)\n        x = torch.bmm(x, trans)\n        x = x.transpose(2,1)\n        x = F.relu(self.bn1(self.conv1(x)))\n        pointfeat = x\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = self.bn3(self.conv3(x))\n        x = torch.max(x, 2, keepdim=True)[0]\n        x = x.view(-1, 256)\n        if self.global_feat:\n            return x, trans\n        else:\n            x = x.view(-1, 256, 1).repeat(1, 1, n_pts)\n            return torch.cat([x, pointfeat], 1), trans\n\nclass DualNetCls(nn.Module):\n    def __init__(self, k = 2):\n        super(DualNetCls, self).__init__()\n        self.feat_left = PointNetfeat(global_feat=True)\n        self.feat_right = PointNetfeat(global_feat=True)\n\n        self.fc1 = nn.Linear(512, 256)\n        self.fc2 = nn.Linear(256, 64)\n        self.fc3 = nn.Linear(64, k)\n        self.fc_drop = nn.Dropout2d()\n        self.bn1 = nn.BatchNorm1d(256)\n        self.bn2 = nn.BatchNorm1d(64)\n        self.relu = nn.ReLU()\n    def forward(self, x_left, x_right):\n        x_left, trans = self.feat_left(x_left)\n        x_right, trans = self.feat_right(x_right)\n        x = torch.cat((x_left, x_right), 1)\n        x = F.relu(self.bn1(self.fc1(x)))\n        x = F.relu(self.bn2(self.fc2(x)))\n        x = self.fc3(x)\n        return F.log_softmax(x, dim=0), trans\n'"
utils/torchnet/dualnet_modified.py,1,"b'from __future__ import print_function\nimport argparse\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torch.utils.data\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nfrom torch.autograd import Variable\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pdb\nimport torch.nn.functional as F\n\n\nclass STN3d(nn.Module):\n    def __init__(self):\n        super(STN3d, self).__init__()\n        self.conv1 = torch.nn.Conv1d(3, 64, 1)\n        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n        self.conv3 = torch.nn.Conv1d(128, 256, 1)\n        self.fc1 = nn.Linear(256, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, 9)\n        self.relu = nn.ReLU()\n\n        self.bn1 = nn.BatchNorm1d(64)\n        self.bn2 = nn.BatchNorm1d(128)\n        self.bn3 = nn.BatchNorm1d(256)\n        self.bn4 = nn.BatchNorm1d(128)\n        self.bn5 = nn.BatchNorm1d(64)\n\n\n    def forward(self, x):\n        batchsize = x.size()[0]\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.relu(self.bn3(self.conv3(x)))\n        x = torch.max(x, 2, keepdim=True)[0]\n        x = x.view(-1, 256)\n\n        x = F.relu(self.bn4(self.fc1(x)))\n        x = F.relu(self.bn5(self.fc2(x)))\n        x = self.fc3(x)\n\n        iden = Variable(torch.from_numpy(np.array([1,0,0,0,1,0,0,0,1]).astype(np.float32))).view(1,9).repeat(batchsize,1)\n        if x.is_cuda:\n            iden = iden.cuda()\n        x = x + iden\n        x = x.view(-1, 3, 3)\n        return x\n\n\nclass PointNetfeat(nn.Module):\n    def __init__(self, global_feat = True):\n        super(PointNetfeat, self).__init__()\n        self.stn = STN3d()\n        self.conv1 = torch.nn.Conv1d(3, 128, 1)\n        self.conv2 = torch.nn.Conv1d(128, 256, 1)\n        self.conv3 = torch.nn.Conv1d(256, 512, 1)\n        self.bn1 = nn.BatchNorm1d(128)\n        self.bn2 = nn.BatchNorm1d(256)\n        self.bn3 = nn.BatchNorm1d(512)\n        self.global_feat = global_feat\n    def forward(self, x_left, x_right):\n        batchsize = x_left.size()[0]\n        n_pts = x_left.size()[2]\n\n        trans_left = self.stn(x_left)\n        trans_right = self.stn(x_right)\n\n        x_right = x_right.transpose(2,1)\n        x_right = torch.bmm(x_right, trans_right)\n        x_right = x_right.transpose(2,1)\n\n        x_left = x_left.transpose(2,1)\n        x_left = torch.bmm(x_left, trans_left)\n        x_left = x_left.transpose(2,1)\n\n        x = torch.cat((x_left, x_right), 2)\n        \n        x = F.relu(self.bn1(self.conv1(x)))\n        pointfeat = x\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = self.bn3(self.conv3(x))\n        x = torch.max(x, 2, keepdim=True)[0]\n        x = x.view(-1, 512)\n        if self.global_feat:\n            return x, trans_left, trans_right\n        else:\n            x = x.view(-1, 512, 1).repeat(1, 1, n_pts)\n            return torch.cat([x, pointfeat], 1), trans_left, trans_right\n\nclass DualNetCls(nn.Module):\n    def __init__(self, k = 2):\n        super(DualNetCls, self).__init__()\n        self.feat = PointNetfeat(global_feat=True)\n        self.fc1 = nn.Linear(512, 256)\n        self.fc2 = nn.Linear(256, 64)\n        self.fc3 = nn.Linear(64, k)\n        self.fc_drop = nn.Dropout2d()\n        self.bn1 = nn.BatchNorm1d(256)\n        self.bn2 = nn.BatchNorm1d(64)\n        self.relu = nn.ReLU()\n\n    def forward(self, x_left, x_right):\n        x, trans_left, trans_right = self.feat(x_left, x_right)\n        x = F.relu(self.bn1(self.fc1(x)))\n        x = F.relu(self.bn2(self.fc2(x)))\n        x = self.fc3(x)\n        return F.log_softmax(x, dim=0), (trans_left, trans_right)\n'"
utils/torchnet/pointnet.py,1,"b""from __future__ import print_function\nimport argparse\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torch.utils.data\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nfrom torch.autograd import Variable\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pdb\nimport torch.nn.functional as F\n\n\nclass STN3d(nn.Module):\n    def __init__(self):\n        super(STN3d, self).__init__()\n        self.conv1 = torch.nn.Conv1d(3, 64, 1)\n        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n        self.fc1 = nn.Linear(1024, 512)\n        self.fc2 = nn.Linear(512, 256)\n        self.fc3 = nn.Linear(256, 9)\n        self.relu = nn.ReLU()\n\n        self.bn1 = nn.BatchNorm1d(64)\n        self.bn2 = nn.BatchNorm1d(128)\n        self.bn3 = nn.BatchNorm1d(1024)\n        self.bn4 = nn.BatchNorm1d(512)\n        self.bn5 = nn.BatchNorm1d(256)\n\n\n    def forward(self, x):\n        batchsize = x.size()[0]\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.relu(self.bn3(self.conv3(x)))\n        x = torch.max(x, 2, keepdim=True)[0]\n        x = x.view(-1, 1024)\n\n        x = F.relu(self.bn4(self.fc1(x)))\n        x = F.relu(self.bn5(self.fc2(x)))\n        x = self.fc3(x)\n\n        iden = Variable(torch.from_numpy(np.array([1,0,0,0,1,0,0,0,1]).astype(np.float32))).view(1,9).repeat(batchsize,1)\n        if x.is_cuda:\n            iden = iden.cuda()\n        x = x + iden\n        x = x.view(-1, 3, 3)\n        return x\n\n\nclass PointNetfeat(nn.Module):\n    def __init__(self, global_feat = True):\n        super(PointNetfeat, self).__init__()\n        self.stn = STN3d()\n        self.conv1 = torch.nn.Conv1d(3, 64, 1)\n        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n        self.bn1 = nn.BatchNorm1d(64)\n        self.bn2 = nn.BatchNorm1d(128)\n        self.bn3 = nn.BatchNorm1d(1024)\n        self.global_feat = global_feat\n    def forward(self, x):\n        batchsize = x.size()[0]\n        n_pts = x.size()[2]\n        trans = self.stn(x)\n        x = x.transpose(2,1)\n        x = torch.bmm(x, trans)\n        x = x.transpose(2,1)\n        x = F.relu(self.bn1(self.conv1(x)))\n        pointfeat = x\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = self.bn3(self.conv3(x))\n        x = torch.max(x, 2, keepdim=True)[0]\n        x = x.view(-1, 1024)\n        if self.global_feat:\n            return x, trans\n        else:\n            x = x.view(-1, 1024, 1).repeat(1, 1, n_pts)\n            return torch.cat([x, pointfeat], 1), trans\n\nclass PointNetCls(nn.Module):\n    def __init__(self, k = 2):\n        super(PointNetCls, self).__init__()\n        self.feat = PointNetfeat(global_feat=True)\n        self.fc1 = nn.Linear(1024, 512)\n        self.fc2 = nn.Linear(512, 256)\n        self.fc3 = nn.Linear(256, k)\n        self.fc_drop = nn.Dropout2d()\n        self.bn1 = nn.BatchNorm1d(512)\n        self.bn2 = nn.BatchNorm1d(256)\n        self.relu = nn.ReLU()\n    def forward(self, x):\n        x, trans = self.feat(x)\n        x = F.relu(self.bn1(self.fc1(x)))\n        x = F.relu(self.bn2(self.fc2(x)))\n\n        # x = F.relu(self.bn2(self.fc2(self.fc_drop(x)))) # added dropout\n        # x = F.dropout(x, training=self.training)\n\n        x = self.fc3(x)\n        return F.log_softmax(x, dim=0), trans\n\nclass PointNetDenseCls(nn.Module):\n    def __init__(self, k = 2):\n        super(PointNetDenseCls, self).__init__()\n        self.k = k\n        self.feat = PointNetfeat(global_feat=False)\n        self.conv1 = torch.nn.Conv1d(1088, 512, 1)\n        self.conv2 = torch.nn.Conv1d(512, 256, 1)\n        self.conv3 = torch.nn.Conv1d(256, 128, 1)\n        self.conv4 = torch.nn.Conv1d(128, self.k, 1)\n        self.bn1 = nn.BatchNorm1d(512)\n        self.bn2 = nn.BatchNorm1d(256)\n        self.bn3 = nn.BatchNorm1d(128)\n\n    def forward(self, x):\n        batchsize = x.size()[0]\n        n_pts = x.size()[2]\n        x, trans = self.feat(x)\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.relu(self.bn3(self.conv3(x)))\n        x = self.conv4(x)\n        x = x.transpose(2,1).contiguous()\n        x = F.log_softmax(x.view(-1,self.k), dim=-1)\n        x = x.view(batchsize, n_pts, self.k)\n        return x, trans\n\n\nif __name__ == '__main__':\n    sim_data = Variable(torch.rand(32,3,2500))\n    trans = STN3d()\n    out = trans(sim_data)\n    print('stn', out.size())\n\n    pointfeat = PointNetfeat(global_feat=True)\n    out, _ = pointfeat(sim_data)\n    print('global feat', out.size())\n\n    pointfeat = PointNetfeat(global_feat=False)\n    out, _ = pointfeat(sim_data)\n    print('point feat', out.size())\n\n    cls = PointNetCls(k = 5)\n    out, _ = cls(sim_data)\n    print('class', out.size())\n\n    seg = PointNetDenseCls(k = 3)\n    out, _ = seg(sim_data)\n    print('seg', out.size())\n"""
utils/torchnet/pointnet_dropout.py,1,"b""from __future__ import print_function\nimport argparse\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torch.utils.data\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nfrom torch.autograd import Variable\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pdb\nimport torch.nn.functional as F\n\n\nclass STN3d(nn.Module):\n    def __init__(self):\n        super(STN3d, self).__init__()\n        self.conv1 = torch.nn.Conv1d(3, 64, 1)\n        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n        self.fc1 = nn.Linear(1024, 512)\n        self.fc2 = nn.Linear(512, 256)\n        self.fc3 = nn.Linear(256, 9)\n        self.relu = nn.ReLU()\n\n        self.bn1 = nn.BatchNorm1d(64)\n        self.bn2 = nn.BatchNorm1d(128)\n        self.bn3 = nn.BatchNorm1d(1024)\n        self.bn4 = nn.BatchNorm1d(512)\n        self.bn5 = nn.BatchNorm1d(256)\n\n\n    def forward(self, x):\n        batchsize = x.size()[0]\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.relu(self.bn3(self.conv3(x)))\n        x = torch.max(x, 2, keepdim=True)[0]\n        x = x.view(-1, 1024)\n\n        x = F.relu(self.bn4(self.fc1(x)))\n        x = F.relu(self.bn5(self.fc2(x)))\n        x = self.fc3(x)\n\n        iden = Variable(torch.from_numpy(np.array([1,0,0,0,1,0,0,0,1]).astype(np.float32))).view(1,9).repeat(batchsize,1)\n        if x.is_cuda:\n            iden = iden.cuda()\n        x = x + iden\n        x = x.view(-1, 3, 3)\n        return x\n\n\nclass PointNetfeat(nn.Module):\n    def __init__(self, global_feat = True):\n        super(PointNetfeat, self).__init__()\n        self.stn = STN3d()\n        self.conv1 = torch.nn.Conv1d(3, 64, 1)\n        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n        self.bn1 = nn.BatchNorm1d(64)\n        self.bn2 = nn.BatchNorm1d(128)\n        self.bn3 = nn.BatchNorm1d(1024)\n        self.global_feat = global_feat\n    def forward(self, x):\n        batchsize = x.size()[0]\n        n_pts = x.size()[2]\n        trans = self.stn(x)\n        x = x.transpose(2,1)\n        x = torch.bmm(x, trans)\n        x = x.transpose(2,1)\n        x = F.relu(self.bn1(self.conv1(x)))\n        pointfeat = x\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = self.bn3(self.conv3(x))\n        x = torch.max(x, 2, keepdim=True)[0]\n        x = x.view(-1, 1024)\n        if self.global_feat:\n            return x, trans\n        else:\n            x = x.view(-1, 1024, 1).repeat(1, 1, n_pts)\n            return torch.cat([x, pointfeat], 1), trans\n\nclass PointNetCls(nn.Module):\n    def __init__(self, k = 2):\n        super(PointNetCls, self).__init__()\n        self.feat = PointNetfeat(global_feat=True)\n        self.fc1 = nn.Linear(1024, 512)\n        self.fc2 = nn.Linear(512, 256)\n        self.fc3 = nn.Linear(256, k)\n        self.fc_drop = nn.Dropout2d()\n        self.bn1 = nn.BatchNorm1d(512)\n        self.bn2 = nn.BatchNorm1d(256)\n        self.relu = nn.ReLU()\n    def forward(self, x):\n        x, trans = self.feat(x)\n        x = F.relu(self.bn1(self.fc1(x)))\n        x = F.relu(self.bn2(self.fc2(self.fc_drop(x)))) # added dropout\n        x = F.dropout(x, training=self.training)\n\n        x = self.fc3(x)\n        return F.log_softmax(x, dim=0), trans\n\nclass PointNetDenseCls(nn.Module):\n    def __init__(self, k = 2):\n        super(PointNetDenseCls, self).__init__()\n        self.k = k\n        self.feat = PointNetfeat(global_feat=False)\n        self.conv1 = torch.nn.Conv1d(1088, 512, 1)\n        self.conv2 = torch.nn.Conv1d(512, 256, 1)\n        self.conv3 = torch.nn.Conv1d(256, 128, 1)\n        self.conv4 = torch.nn.Conv1d(128, self.k, 1)\n        self.bn1 = nn.BatchNorm1d(512)\n        self.bn2 = nn.BatchNorm1d(256)\n        self.bn3 = nn.BatchNorm1d(128)\n\n    def forward(self, x):\n        batchsize = x.size()[0]\n        n_pts = x.size()[2]\n        x, trans = self.feat(x)\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.relu(self.bn3(self.conv3(x)))\n        x = self.conv4(x)\n        x = x.transpose(2,1).contiguous()\n        x = F.log_softmax(x.view(-1,self.k), dim=-1)\n        x = x.view(batchsize, n_pts, self.k)\n        return x, trans\n\n\nif __name__ == '__main__':\n    sim_data = Variable(torch.rand(32,3,2500))\n    trans = STN3d()\n    out = trans(sim_data)\n    print('stn', out.size())\n\n    pointfeat = PointNetfeat(global_feat=True)\n    out, _ = pointfeat(sim_data)\n    print('global feat', out.size())\n\n    pointfeat = PointNetfeat(global_feat=False)\n    out, _ = pointfeat(sim_data)\n    print('point feat', out.size())\n\n    cls = PointNetCls(k = 5)\n    out, _ = cls(sim_data)\n    print('class', out.size())\n\n    seg = PointNetDenseCls(k = 3)\n    out, _ = seg(sim_data)\n    print('seg', out.size())\n"""
utils/torchnet/pointnet_normless.py,1,"b""from __future__ import print_function\nimport argparse\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torch.utils.data\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nfrom torch.autograd import Variable\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pdb\nimport torch.nn.functional as F\n\n\nclass STN3d(nn.Module):\n    def __init__(self):\n        super(STN3d, self).__init__()\n        self.conv1 = torch.nn.Conv1d(3, 64, 1)\n        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n        self.fc1 = nn.Linear(1024, 512)\n        self.fc2 = nn.Linear(512, 256)\n        self.fc3 = nn.Linear(256, 9)\n        self.relu = nn.ReLU()\n\n        # self.bn1 = nn.BatchNorm1d(64)\n        # self.bn2 = nn.BatchNorm1d(128)\n        # self.bn3 = nn.BatchNorm1d(1024)\n        # self.bn4 = nn.BatchNorm1d(512)\n        # self.bn5 = nn.BatchNorm1d(256)\n\n\n    def forward(self, x):\n        batchsize = x.size()[0]\n        x = F.selu(self.conv1(x))\n        x = F.selu(self.conv2(x))\n        x = F.selu(self.conv3(x))\n        x = torch.max(x, 2, keepdim=True)[0]\n        x = x.view(-1, 1024)\n\n        x = F.selu(self.fc1(x))\n        x = F.selu(self.fc2(x))\n        x = self.fc3(x)\n\n        iden = Variable(torch.from_numpy(np.array([1,0,0,0,1,0,0,0,1]).astype(np.float32))).view(1,9).repeat(batchsize,1)\n        if x.is_cuda:\n            iden = iden.cuda()\n        x = x + iden\n        x = x.view(-1, 3, 3)\n        return x\n\n\nclass PointNetfeat(nn.Module):\n    def __init__(self, global_feat = True):\n        super(PointNetfeat, self).__init__()\n        self.stn = STN3d()\n        self.conv1 = torch.nn.Conv1d(3, 64, 1)\n        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n        # self.bn1 = nn.BatchNorm1d(64)\n        # self.bn2 = nn.BatchNorm1d(128)\n        # self.bn3 = nn.BatchNorm1d(1024)\n        self.global_feat = global_feat\n    def forward(self, x):\n        batchsize = x.size()[0]\n        n_pts = x.size()[2]\n        trans = self.stn(x)\n        x = x.transpose(2,1)\n        x = torch.bmm(x, trans)\n        x = x.transpose(2,1)\n        x = F.selu(self.conv1(x))\n        pointfeat = x\n        x = F.selu(self.conv2(x))\n        x = self.conv3(x)\n        x = torch.max(x, 2, keepdim=True)[0]\n        x = x.view(-1, 1024)\n        if self.global_feat:\n            return x, trans\n        else:\n            x = x.view(-1, 1024, 1).repeat(1, 1, n_pts)\n            return torch.cat([x, pointfeat], 1), trans\n\nclass PointNetCls(nn.Module):\n    def __init__(self, k = 2):\n        super(PointNetCls, self).__init__()\n        self.feat = PointNetfeat(global_feat=True)\n        self.fc1 = nn.Linear(1024, 512)\n        self.fc2 = nn.Linear(512, 256)\n        self.fc3 = nn.Linear(256, k)\n        self.fc_drop = nn.Dropout2d()\n        # self.bn1 = nn.BatchNorm1d(512)\n        # self.bn2 = nn.BatchNorm1d(256)\n        self.relu = nn.ReLU()\n    def forward(self, x):\n        x, trans = self.feat(x)\n        x = F.selu(self.fc1(x))\n        x = F.selu(self.fc2(x))\n        # x = F.relu(self.bn2(self.fc2(self.fc_drop(x)))) # added dropout\n        # x = F.dropout(x, training=self.training)\n\n        x = self.fc3(x)\n        return F.log_softmax(x, dim=0), trans\n\nclass PointNetDenseCls(nn.Module):\n    def __init__(self, k = 2):\n        super(PointNetDenseCls, self).__init__()\n        self.k = k\n        self.feat = PointNetfeat(global_feat=False)\n        self.conv1 = torch.nn.Conv1d(1088, 512, 1)\n        self.conv2 = torch.nn.Conv1d(512, 256, 1)\n        self.conv3 = torch.nn.Conv1d(256, 128, 1)\n        self.conv4 = torch.nn.Conv1d(128, self.k, 1)\n        # self.bn1 = nn.BatchNorm1d(512)\n        # self.bn2 = nn.BatchNorm1d(256)\n        # self.bn3 = nn.BatchNorm1d(128)\n\n    def forward(self, x):\n        batchsize = x.size()[0]\n        n_pts = x.size()[2]\n        x, trans = self.feat(x)\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.relu(self.bn3(self.conv3(x)))\n        x = self.conv4(x)\n        x = x.transpose(2,1).contiguous()\n        x = F.log_softmax(x.view(-1,self.k), dim=-1)\n        x = x.view(batchsize, n_pts, self.k)\n        return x, trans\n\n\nif __name__ == '__main__':\n    sim_data = Variable(torch.rand(32,3,2500))\n    trans = STN3d()\n    out = trans(sim_data)\n    print('stn', out.size())\n\n    pointfeat = PointNetfeat(global_feat=True)\n    out, _ = pointfeat(sim_data)\n    print('global feat', out.size())\n\n    pointfeat = PointNetfeat(global_feat=False)\n    out, _ = pointfeat(sim_data)\n    print('point feat', out.size())\n\n    cls = PointNetCls(k = 5)\n    out, _ = cls(sim_data)\n    print('class', out.size())\n\n    seg = PointNetDenseCls(k = 3)\n    out, _ = seg(sim_data)\n    print('seg', out.size())\n"""
utils/torchnet/pointnet_small.py,1,"b""from __future__ import print_function\nimport argparse\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torch.utils.data\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nfrom torch.autograd import Variable\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pdb\nimport torch.nn.functional as F\n\n\nclass STN3d(nn.Module):\n    def __init__(self):\n        super(STN3d, self).__init__()\n        self.conv1 = torch.nn.Conv1d(3, 64, 1)\n        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n        self.conv3 = torch.nn.Conv1d(128, 256, 1)\n        self.fc1 = nn.Linear(256, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, 9)\n        self.relu = nn.ReLU()\n\n        self.bn1 = nn.BatchNorm1d(64)\n        self.bn2 = nn.BatchNorm1d(128)\n        self.bn3 = nn.BatchNorm1d(256)\n        self.bn4 = nn.BatchNorm1d(128)\n        self.bn5 = nn.BatchNorm1d(64)\n\n\n    def forward(self, x):\n        batchsize = x.size()[0]\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.relu(self.bn3(self.conv3(x)))\n        x = torch.max(x, 2, keepdim=True)[0]\n        x = x.view(-1, 256)\n\n        x = F.relu(self.bn4(self.fc1(x)))\n        x = F.relu(self.bn5(self.fc2(x)))\n        x = self.fc3(x)\n\n        iden = Variable(torch.from_numpy(np.array([1,0,0,0,1,0,0,0,1]).astype(np.float32))).view(1,9).repeat(batchsize,1)\n        if x.is_cuda:\n            iden = iden.cuda()\n        x = x + iden\n        x = x.view(-1, 3, 3)\n        return x\n\n\nclass PointNetfeat(nn.Module):\n    def __init__(self, global_feat = True):\n        super(PointNetfeat, self).__init__()\n        self.stn = STN3d()\n        self.conv1 = torch.nn.Conv1d(3, 64, 1)\n        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n        self.conv3 = torch.nn.Conv1d(128, 256, 1)\n        self.bn1 = nn.BatchNorm1d(64)\n        self.bn2 = nn.BatchNorm1d(128)\n        self.bn3 = nn.BatchNorm1d(256)\n        self.global_feat = global_feat\n    def forward(self, x):\n        batchsize = x.size()[0]\n        n_pts = x.size()[2]\n        trans = self.stn(x)\n        x = x.transpose(2,1)\n        x = torch.bmm(x, trans)\n        x = x.transpose(2,1)\n        x = F.relu(self.bn1(self.conv1(x)))\n        pointfeat = x\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = self.bn3(self.conv3(x))\n        x = torch.max(x, 2, keepdim=True)[0]\n        x = x.view(-1, 256)\n        if self.global_feat:\n            return x, trans\n        else:\n            x = x.view(-1, 256, 1).repeat(1, 1, n_pts)\n            return torch.cat([x, pointfeat], 1), trans\n\nclass PointNetCls(nn.Module):\n    def __init__(self, k = 2):\n        super(PointNetCls, self).__init__()\n        self.feat = PointNetfeat(global_feat=True)\n        self.fc1 = nn.Linear(256, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, k)\n        self.fc_drop = nn.Dropout2d()\n        self.bn1 = nn.BatchNorm1d(128)\n        self.bn2 = nn.BatchNorm1d(64)\n        self.relu = nn.ReLU()\n    def forward(self, x):\n        x, trans = self.feat(x)\n        x = F.relu(self.bn1(self.fc1(x)))\n        x = F.relu(self.bn2(self.fc2(x)))\n        x = self.fc3(x)\n        return F.log_softmax(x, dim=0), trans\n\nclass PointNetDenseCls(nn.Module):\n    def __init__(self, k = 2):\n        super(PointNetDenseCls, self).__init__()\n        self.k = k\n        self.feat = PointNetfeat(global_feat=False)\n        self.conv1 = torch.nn.Conv1d(1088, 512, 1)\n        self.conv2 = torch.nn.Conv1d(512, 256, 1)\n        self.conv3 = torch.nn.Conv1d(256, 128, 1)\n        self.conv4 = torch.nn.Conv1d(128, self.k, 1)\n        self.bn1 = nn.BatchNorm1d(512)\n        self.bn2 = nn.BatchNorm1d(256)\n        self.bn3 = nn.BatchNorm1d(128)\n\n    def forward(self, x):\n        batchsize = x.size()[0]\n        n_pts = x.size()[2]\n        x, trans = self.feat(x)\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.relu(self.bn3(self.conv3(x)))\n        x = self.conv4(x)\n        x = x.transpose(2,1).contiguous()\n        x = F.log_softmax(x.view(-1,self.k), dim=-1)\n        x = x.view(batchsize, n_pts, self.k)\n        return x, trans\n\n\nif __name__ == '__main__':\n    sim_data = Variable(torch.rand(32,3,2500))\n    trans = STN3d()\n    out = trans(sim_data)\n    print('stn', out.size())\n\n    pointfeat = PointNetfeat(global_feat=True)\n    out, _ = pointfeat(sim_data)\n    print('global feat', out.size())\n\n    pointfeat = PointNetfeat(global_feat=False)\n    out, _ = pointfeat(sim_data)\n    print('point feat', out.size())\n\n    cls = PointNetCls(k = 5)\n    out, _ = cls(sim_data)\n    print('class', out.size())\n\n    seg = PointNetDenseCls(k = 3)\n    out, _ = seg(sim_data)\n    print('seg', out.size())\n"""
utils/torchnet/pointnet_small_dropout.py,1,"b""from __future__ import print_function\nimport argparse\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torch.utils.data\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nfrom torch.autograd import Variable\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pdb\nimport torch.nn.functional as F\n\n\nclass STN3d(nn.Module):\n    def __init__(self):\n        super(STN3d, self).__init__()\n        self.conv1 = torch.nn.Conv1d(3, 64, 1)\n        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n        self.conv3 = torch.nn.Conv1d(128, 256, 1)\n        self.fc1 = nn.Linear(256, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, 9)\n        self.relu = nn.ReLU()\n\n        self.bn1 = nn.BatchNorm1d(64)\n        self.bn2 = nn.BatchNorm1d(128)\n        self.bn3 = nn.BatchNorm1d(256)\n        self.bn4 = nn.BatchNorm1d(128)\n        self.bn5 = nn.BatchNorm1d(64)\n\n\n    def forward(self, x):\n        batchsize = x.size()[0]\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.relu(self.bn3(self.conv3(x)))\n        x = torch.max(x, 2, keepdim=True)[0]\n        x = x.view(-1, 256)\n\n        x = F.relu(self.bn4(self.fc1(x)))\n        x = F.relu(self.bn5(self.fc2(x)))\n        x = self.fc3(x)\n\n        iden = Variable(torch.from_numpy(np.array([1,0,0,0,1,0,0,0,1]).astype(np.float32))).view(1,9).repeat(batchsize,1)\n        if x.is_cuda:\n            iden = iden.cuda()\n        x = x + iden\n        x = x.view(-1, 3, 3)\n        return x\n\n\nclass PointNetfeat(nn.Module):\n    def __init__(self, global_feat = True):\n        super(PointNetfeat, self).__init__()\n        self.stn = STN3d()\n        self.conv1 = torch.nn.Conv1d(3, 64, 1)\n        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n        self.conv3 = torch.nn.Conv1d(128, 256, 1)\n        self.bn1 = nn.BatchNorm1d(64)\n        self.bn2 = nn.BatchNorm1d(128)\n        self.bn3 = nn.BatchNorm1d(256)\n        self.global_feat = global_feat\n    def forward(self, x):\n        batchsize = x.size()[0]\n        n_pts = x.size()[2]\n        trans = self.stn(x)\n        x = x.transpose(2,1)\n        x = torch.bmm(x, trans)\n        x = x.transpose(2,1)\n        x = F.relu(self.bn1(self.conv1(x)))\n        pointfeat = x\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = self.bn3(self.conv3(x))\n        x = torch.max(x, 2, keepdim=True)[0]\n        x = x.view(-1, 256)\n        if self.global_feat:\n            return x, trans\n        else:\n            x = x.view(-1, 256, 1).repeat(1, 1, n_pts)\n            return torch.cat([x, pointfeat], 1), trans\n\nclass PointNetCls(nn.Module):\n    def __init__(self, k = 2):\n        super(PointNetCls, self).__init__()\n        self.feat = PointNetfeat(global_feat=True)\n        self.fc1 = nn.Linear(256, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, k)\n        self.fc_drop = nn.Dropout2d()\n        self.bn1 = nn.BatchNorm1d(128)\n        self.bn2 = nn.BatchNorm1d(64)\n        self.relu = nn.ReLU()\n    def forward(self, x):\n        x, trans = self.feat(x)\n        x = F.relu(self.bn1(self.fc1(self.fc_drop(x)))) # added dropout\n        x = F.dropout(x, training=self.training)\n        x = F.relu(self.bn2(self.fc2(x)))\n        x = self.fc3(x)\n        return F.log_softmax(x, dim=0), trans\n\nclass PointNetDenseCls(nn.Module):\n    def __init__(self, k = 2):\n        super(PointNetDenseCls, self).__init__()\n        self.k = k\n        self.feat = PointNetfeat(global_feat=False)\n        self.conv1 = torch.nn.Conv1d(1088, 512, 1)\n        self.conv2 = torch.nn.Conv1d(512, 256, 1)\n        self.conv3 = torch.nn.Conv1d(256, 128, 1)\n        self.conv4 = torch.nn.Conv1d(128, self.k, 1)\n        self.bn1 = nn.BatchNorm1d(512)\n        self.bn2 = nn.BatchNorm1d(256)\n        self.bn3 = nn.BatchNorm1d(128)\n\n    def forward(self, x):\n        batchsize = x.size()[0]\n        n_pts = x.size()[2]\n        x, trans = self.feat(x)\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.relu(self.bn3(self.conv3(x)))\n        x = self.conv4(x)\n        x = x.transpose(2,1).contiguous()\n        x = F.log_softmax(x.view(-1,self.k), dim=-1)\n        x = x.view(batchsize, n_pts, self.k)\n        return x, trans\n\n\nif __name__ == '__main__':\n    sim_data = Variable(torch.rand(32,3,2500))\n    trans = STN3d()\n    out = trans(sim_data)\n    print('stn', out.size())\n\n    pointfeat = PointNetfeat(global_feat=True)\n    out, _ = pointfeat(sim_data)\n    print('global feat', out.size())\n\n    pointfeat = PointNetfeat(global_feat=False)\n    out, _ = pointfeat(sim_data)\n    print('point feat', out.size())\n\n    cls = PointNetCls(k = 5)\n    out, _ = cls(sim_data)\n    print('class', out.size())\n\n    seg = PointNetDenseCls(k = 3)\n    out, _ = seg(sim_data)\n    print('seg', out.size())\n"""
utils/torchnet/train_classification.py,10,"b'from __future__ import print_function\nimport argparse\nimport os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torch.utils.data\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nfrom torch.autograd import Variable\nfrom datasets import PartDataset\nfrom pointnet import PointNetCls\nimport torch.nn.functional as F\nimport glob\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--batchSize\', type=int, default=32, help=\'input batch size\')\nparser.add_argument(\'--num_points\', type=int, default=2500, help=\'input batch size\')\nparser.add_argument(\'--workers\', type=int, help=\'number of data loading workers\', default=4)\nparser.add_argument(\'--nepoch\', type=int, default=25, help=\'number of epochs to train for\')\nparser.add_argument(\'--outf\', type=str, default=\'cls\',  help=\'output folder\')\nparser.add_argument(\'--model\', type=str, default = \'\',  help=\'model path\')\nparser.add_argument(\'--classes\', type=int, default = 2,  help=\'num classes\')\n\nopt = parser.parse_args()\nprint (opt)\n\nblue = lambda x:\'\\033[94m\' + x + \'\\033[0m\'\n\nopt.manualSeed = random.randint(1, 10000) # fix seed\nprint(""Random Seed: "", opt.manualSeed)\nrandom.seed(opt.manualSeed)\ntorch.manual_seed(opt.manualSeed)\n\n# Literal nonsense:\n\n# load arrays from file_path and label with class_num\ndef loadData(file_path, class_num,  num_points=2500, file_type=""*.npy""):\n    #helper function for subsampling\n    def subsampleColumns(arr, num_points):\n        subsampling = np.random.choice([i for i in range(len(arr[0]))], num_points)\n        return arr[:,subsampling]\n    \n    filenames = glob.glob(file_path + file_type)\n    datum = np.load(filenames[0])\n    datum = subsampleColumns(datum, num_points)\n    \n    # add class label\n    datum = np.concatenate((datum,np.array([[class_num],[class_num],[class_num]])), axis=1)\n    datum = np.expand_dims(datum, axis=0)\n    print(datum.shape)\n\n    for name in filenames[1:]:\n        arr = np.load(name)\n        arr = subsampleColumns(arr, num_points)\n        arr = np.concatenate((arr,np.array([[class_num],[class_num],[class_num]])), axis=1)\n        arr = np.expand_dims(arr, axis=0)\n        datum = np.concatenate((datum,arr),axis=0)\n    print(datum.shape)\n\n    return datum\n    \ndata = np.concatenate((loadData(""../train/open1/npys/"", 0), (loadData(""../train/thumbdown1/npys/"", 1))), axis=0)\ndataset = Variable(torch.from_numpy(data))\n\ntest = np.concatenate((loadData(""../train/open2/npys/"", 0), (loadData(""../train/thumbdown2/npys/"", 1))), axis=0)\ntest_dataset = Variable(torch.from_numpy(test))\n\n\n\n# dataset = PartDataset(root = \'shapenetcore_partanno_segmentation_benchmark_v0\', classification = True, npoints = opt.num_points)\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=opt.batchSize,\n                                          shuffle=True, num_workers=int(opt.workers))\n\n# test_dataset = PartDataset(root = \'shapenetcore_partanno_segmentation_benchmark_v0\', classification = True, train = False, npoints = opt.num_points)\ntestdataloader = torch.utils.data.DataLoader(test_dataset, batch_size=opt.batchSize,\n                                          shuffle=True, num_workers=int(opt.workers))\n\nprint(len(dataset), len(test_dataset))\nnum_classes = opt.classes\nprint(\'classes\', num_classes)\n\ntry:\n    os.makedirs(opt.outf)\nexcept OSError:\n    pass\n\n\nclassifier = PointNetCls(k = num_classes)\n\n\nif opt.model != \'\':\n    classifier.load_state_dict(torch.load(opt.model))\n\n\noptimizer = optim.SGD(classifier.parameters(), lr=0.01, momentum=0.9)\nclassifier.cuda()\n\nnum_batch = len(dataset)/opt.batchSize\n\nfor epoch in range(opt.nepoch):\n    for i, data in enumerate(dataloader, 0):\n        \n        # Ungodly indexing magic\n        points, target = data[:,:,0:opt.num_points].type(torch.FloatTensor), data[:,:,opt.num_points:opt.num_points+1][:,0].type(torch.LongTensor)\n        # print(points)\n        print(target)\n\n        points, target = Variable(points), Variable(target[:,0])\n        # points = points.transpose(2,1)\n        # print(points.shape)\n        points, target = points.cuda(), target.cuda()\n        # print(target)\n        # raise Exception(""no"")\n        optimizer.zero_grad()\n        classifier = classifier.train()\n        pred, _ = classifier(points)\n        print(pred)\n        loss = F.nll_loss(pred, target)\n        loss.backward()\n        optimizer.step()\n        pred_choice = pred.data.max(1)[1]\n        correct = pred_choice.eq(target.data).cpu().sum()\n        print(\'[%d: %d/%d] train loss: %f accuracy: %f\' %(epoch, i, num_batch, loss.item(),correct.item() / float(opt.batchSize)))\n\n        if i % 10 == 0:\n            j, data = next(enumerate(testdataloader, 0))\n            points, target = data[:,:,0:opt.num_points].type(torch.FloatTensor), data[:,:,opt.num_points:opt.num_points+1][:,0].type(torch.LongTensor)\n            points, target = Variable(points), Variable(target[:,0])\n            # points = points.transpose(2,1)\n            points, target = points.cuda(), target.cuda()\n            classifier = classifier.eval()\n            pred, _ = classifier(points)\n            loss = F.nll_loss(pred, target)\n            pred_choice = pred.data.max(1)[1]\n            correct = pred_choice.eq(target.data).cpu().sum()\n            print(\'[%d: %d/%d] %s loss: %f accuracy: %f\' %(epoch, i, num_batch, blue(\'test\'), loss.item(), correct.item()/float(opt.batchSize)))\n\n\n    # out, _ = classifier.sim_data\n    # print(out[0:10])\n    torch.save(classifier.state_dict(),""model"")# \'%s/cls_model_%d.pth\' % (opt.outf, epoch))\n'"
