file_path,api_count,code
setup.py,0,"b'\xef\xbb\xbf# -*- coding: utf-8 -*-\n# *****************************************************************************\n# Copyright (c) 2017-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nfrom setuptools import setup, Extension, find_packages, Command\nimport platform\nimport os\nimport sys\nfrom docs.source.buildscripts.sdc_build_doc import SDCBuildDoc\n\n\n# Note we don\'t import Numpy at the toplevel, since setup.py\n# should be able to run without Numpy for pip to discover the\n# build dependencies\nimport numpy.distutils.misc_util as np_misc\nimport versioneer\n\n# String constants for Intel SDC project configuration\n# This name is used for wheel package build\nSDC_NAME_STR = \'sdc\'\n\n# Inject required options for extensions compiled against the Numpy\n# C API (include dirs, library dirs etc.)\nnp_compile_args = np_misc.get_info(\'npymath\')\n\nis_win = platform.system() == \'Windows\'\n\n\ndef readme():\n    with open(\'README.rst\', encoding=\'utf-8\') as f:\n        return f.read()\n\n\n# package environment variable is PREFIX during build time\nif \'CONDA_BUILD\' in os.environ:\n    PREFIX_DIR = os.environ[\'PREFIX\']\nelse:\n    PREFIX_DIR = os.environ[\'CONDA_PREFIX\']\n    # C libraries are in \\Library on Windows\n    if is_win:\n        PREFIX_DIR += r\'\\Library\'\n\n\ntry:\n    import pyarrow\nexcept ImportError:\n    _has_pyarrow = False\nelse:\n    _has_pyarrow = True\n\n\n# Copypaste from numba\ndef check_file_at_path(path2file):\n    """"""\n    Takes a list as a path, a single glob (*) is permitted as an entry which\n    indicates that expansion at this location is required (i.e. version\n    might not be known).\n    """"""\n    found = None\n    path2check = [os.path.split(os.path.split(sys.executable)[0])[0]]\n    path2check += [os.getenv(n, \'\') for n in [\'CONDA_PREFIX\', \'PREFIX\']]\n    if sys.platform.startswith(\'win\'):\n        path2check += [os.path.join(p, \'Library\') for p in path2check]\n    for p in path2check:\n        if p:\n            if \'*\' in path2file:\n                globloc = path2file.index(\'*\')\n                searchroot = os.path.join(*path2file[:globloc])\n                try:\n                    potential_locs = os.listdir(os.path.join(p, searchroot))\n                except BaseException:\n                    continue\n                searchfor = path2file[globloc + 1:]\n                for x in potential_locs:\n                    potpath = os.path.join(p, searchroot, x, *searchfor)\n                    if os.path.isfile(potpath):\n                        found = p  # the latest is used\n            elif os.path.isfile(os.path.join(p, *path2file)):\n                found = p  # the latest is used\n    return found\n\n\n# Search for Intel TBB, first check env var TBBROOT then conda locations\ntbb_root = os.getenv(\'TBBROOT\')\nif not tbb_root:\n    tbb_root = check_file_at_path([\'include\', \'tbb\', \'tbb.h\'])\n\nind = [PREFIX_DIR + \'/include\', ]\nlid = [PREFIX_DIR + \'/lib\', ]\neca = [\'-std=c++11\', ""-O3""]  # \'-g\', \'-O0\']\nela = [\'-std=c++11\', ]\n\nio_libs = []\n\next_io = Extension(name=""sdc.hio"",\n                   sources=[""sdc/io/_io.cpp""],\n                   depends=[""sdc/_hpat_common.h"", ""sdc/_distributed.h"",\n                            ""sdc/_import_py.h"",\n                            ""sdc/_datetime_ext.h""],\n                   include_dirs=ind + np_compile_args[\'include_dirs\'],\n                   library_dirs=lid,\n                   extra_compile_args=eca,\n                   extra_link_args=ela,\n                   language=""c++""\n                   )\n\next_transport_seq = Extension(name=""sdc.transport_seq"",\n                              sources=[""sdc/transport/hpat_transport_single_process.cpp""],\n                              depends=[""sdc/_distributed.h""],\n                              include_dirs=ind,\n                              library_dirs=lid,\n                              extra_compile_args=eca,\n                              extra_link_args=ela,\n                              language=""c++""\n                              )\n\next_hdist = Extension(name=""sdc.hdist"",\n                      sources=[""sdc/_distributed.cpp""],\n                      depends=[""sdc/_hpat_common.h""],\n                      extra_compile_args=eca,\n                      extra_link_args=ela,\n                      include_dirs=ind,\n                      library_dirs=lid,\n                      )\n\next_chiframes = Extension(name=""sdc.chiframes"",\n                          sources=[""sdc/_hiframes.cpp""],\n                          depends=[""sdc/_hpat_sort.h""],\n                          extra_compile_args=eca,\n                          extra_link_args=ela,\n                          include_dirs=ind,\n                          library_dirs=lid,\n                          )\n\next_set = Extension(name=""sdc.hset_ext"",\n                    sources=[""sdc/_set_ext.cpp""],\n                    extra_compile_args=eca,\n                    extra_link_args=ela,\n                    include_dirs=ind,\n                    library_dirs=lid,\n                    )\n\next_sort = Extension(name=""sdc.concurrent_sort"",\n                     sources=[\n                        ""sdc/native/sort.cpp"",\n                        ""sdc/native/stable_sort.cpp"",\n                        ""sdc/native/module.cpp"",\n                        ""sdc/native/utils.cpp""],\n                     extra_compile_args=eca,\n                     extra_link_args=ela,\n                     libraries=[\'tbb\'],\n                     include_dirs=ind + [""sdc/native/"", os.path.join(tbb_root, \'include\')],\n                     library_dirs=lid + [\n                        # for Linux\n                        os.path.join(tbb_root, \'lib\', \'intel64\', \'gcc4.4\'),\n                        # for MacOS\n                        os.path.join(tbb_root, \'lib\'),\n                        # for Windows\n                        os.path.join(tbb_root, \'lib\', \'intel64\', \'vc_mt\'),\n                     ],\n                     language=""c++""\n                     )\n\nstr_libs = np_compile_args[\'libraries\']\n\next_str = Extension(name=""sdc.hstr_ext"",\n                    sources=[""sdc/_str_ext.cpp""],\n                    libraries=str_libs,\n                    define_macros=np_compile_args[\'define_macros\'],\n                    extra_compile_args=eca,\n                    extra_link_args=ela,\n                    include_dirs=np_compile_args[\'include_dirs\'] + ind,\n                    library_dirs=np_compile_args[\'library_dirs\'] + lid,\n                    )\n\next_dt = Extension(name=""sdc.hdatetime_ext"",\n                   sources=[""sdc/_datetime_ext.cpp""],\n                   libraries=np_compile_args[\'libraries\'],\n                   define_macros=np_compile_args[\'define_macros\'],\n                   extra_compile_args=[\'-std=c++11\'],\n                   extra_link_args=[\'-std=c++11\'],\n                   include_dirs=np_compile_args[\'include_dirs\'],\n                   library_dirs=np_compile_args[\'library_dirs\'],\n                   language=""c++""\n                   )\n\npq_libs = [\'arrow\', \'parquet\']\n\next_parquet = Extension(name=""sdc.parquet_cpp"",\n                        sources=[""sdc/io/_parquet.cpp""],\n                        libraries=pq_libs,\n                        include_dirs=[\'.\'] + ind,\n                        define_macros=[(\'BUILTIN_PARQUET_READER\', None)],\n                        extra_compile_args=eca,\n                        extra_link_args=ela,\n                        library_dirs=lid,\n                        )\n\n_ext_mods = [ext_hdist, ext_chiframes, ext_set, ext_str, ext_dt, ext_io, ext_transport_seq, ext_sort]\n\n# Support of Parquet is disabled because HPAT pipeline does not work now\n# if _has_pyarrow:\n#     _ext_mods.append(ext_parquet)\n\n\nclass style(Command):\n    """""" Command to check and adjust code style\n    Usage:\n        To check style: python ./setup.py style\n        To fix style: python ./setup.py style -a\n    """"""\n    user_options = [\n        (\'apply\', \'a\', \'Apply codestyle changes to sources.\')\n    ]\n    description = ""Code style check and apply (with -a)""\n    boolean_options = []\n\n    _result_marker = ""Result:""\n    _project_directory_excluded = [\'build\', \'.git\']\n\n    _c_formatter = \'clang-format-6.0\'\n    _c_formatter_install_msg = \'pip install clang\'\n    _c_formatter_command_line = [_c_formatter, \'-style=file\']\n    _c_file_extensions = [\'.h\', \'.c\', \'.hpp\', \'.cpp\']\n\n    _py_checker = \'pycodestyle\'\n    _py_formatter = \'autopep8\'\n    _py_formatter_install_msg = \'pip install --upgrade autopep8\\npip install --upgrade pycodestyle\'\n    # E265 and W503 excluded because I didn\'t find a way to apply changes automatically\n    _py_checker_command_line = [_py_checker, \'--ignore=E265,W503\']\n    _py_formatter_command_line = [_py_formatter, \'--in-place\', \'--aggressive\', \'--aggressive\']\n    _py_file_extensions = [\'.py\']\n\n    def _get_file_list(self, path, search_extentions):\n        """""" Return file list to be adjusted or checked\n\n        path - is the project base path\n        search_extentions - list of strings with files extension to search recurcivly\n        """"""\n        files = []\n        exluded_directories_full_path = [os.path.join(path, excluded_dir)\n                                         for excluded_dir in self._project_directory_excluded]\n\n        # r=root, d=directories, f = files\n        for r, d, f in os.walk(path):\n            # match exclude pattern in current directory\n            found = False\n            for excluded_dir in exluded_directories_full_path:\n                if r.find(excluded_dir) >= 0:\n                    found = True\n\n            if found:\n                continue\n\n            for file in f:\n                filename, extention = os.path.splitext(file)\n                if extention in search_extentions:\n                    files.append(os.path.join(r, file))\n\n        return files\n\n    def initialize_options(self):\n        self.apply = 0\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        root_dir = versioneer.get_root()\n        print(""Project directory is: %s"" % root_dir)\n\n        if self.apply:\n            self._c_formatter_command_line += [\'-i\']\n        else:\n            self._c_formatter_command_line += [\'-output-replacements-xml\']\n\n        import subprocess\n\n        bad_style_file_names = []\n\n        # C files handling\n        c_files = self._get_file_list(root_dir, self._c_file_extensions)\n        try:\n            for f in c_files:\n                command_output = subprocess.Popen(self._c_formatter_command_line + [f], stdout=subprocess.PIPE)\n                command_cout, command_cerr = command_output.communicate()\n                if not self.apply:\n                    if command_cout.find(b\'<replacement \') > 0:\n                        bad_style_file_names.append(f)\n        except BaseException as original_error:\n            print(""%s is not installed.\\nPlease use: %s"" % (self._c_formatter, self._c_formatter_install_msg))\n            print(""Original error message is:\\n"", original_error)\n            exit(1)\n\n        # Python files handling\n        py_files = self._get_file_list(root_dir, self._py_file_extensions)\n        try:\n            for f in py_files:\n                if not self.apply:\n                    command_output = subprocess.Popen(\n                        self._py_checker_command_line + [f])\n                    returncode = command_output.wait()\n                    if returncode != 0:\n                        bad_style_file_names.append(f)\n                else:\n                    command_output = subprocess.Popen(\n                        self._py_formatter_command_line + [f])\n                    command_output.wait()\n        except BaseException as original_error:\n            print(""%s is not installed.\\nPlease use: %s"" % (self._py_formatter, self._py_formatter_install_msg))\n            print(""Original error message is:\\n"", original_error)\n            exit(1)\n\n        if bad_style_file_names:\n            print(""Following files style need to be adjusted:"")\n            for line in bad_style_file_names:\n                print(line)\n            print(""%s Style check failed"" % self._result_marker)\n        else:\n            print(""%s Style check passed"" % self._result_marker)\n\n\n# Custom build commands\n#\n# These commands extend standard setuptools build procedure\n#\nsdc_build_commands = versioneer.get_cmdclass()\nsdc_build_commands[\'build_doc\'] = SDCBuildDoc\nsdc_build_commands.update({\'style\': style})\nsdc_version = versioneer.get_version()\nsdc_release = \'Alpha ({})\'.format(versioneer.get_version())\n\nsetup(name=SDC_NAME_STR,\n      version=sdc_version,\n      description=\'Numba* extension for compiling Pandas* operations\',\n      long_description=readme(),\n      classifiers=[\n          ""Development Status :: 2 - Pre-Alpha"",\n          ""Intended Audience :: Developers"",\n          ""Operating System :: POSIX :: Linux"",\n          ""Programming Language :: Python"",\n          ""Programming Language :: Python :: 3.7"",\n          ""Topic :: Software Development :: Compilers"",\n          ""Topic :: System :: Distributed Computing"",\n      ],\n      keywords=\'data analytics distributed Pandas Numba\',\n      url=\'https://github.com/IntelPython/sdc\',\n      author=\'Intel Corporation\',\n      packages=find_packages(),\n      package_data={\'sdc.tests\': [\'*.bz2\'], },\n      install_requires=[\n          \'numpy>=1.16\',\n          \'pandas==0.25.3\',\n          \'pyarrow==0.17.0\',\n          \'numba>=0.49.1,<0.50.0\',\n          \'tbb\'\n          ],\n      cmdclass=sdc_build_commands,\n      ext_modules=_ext_mods,\n      entry_points={\n          ""numba_extensions"": [\n              ""init = sdc:_init_extension"",\n          ]},\n      )\n'"
versioneer.py,0,"b'\n# Version: 0.18\n\n""""""The Versioneer - like a rocketeer, but for versions.\n\nThe Versioneer\n==============\n\n* like a rocketeer, but for versions!\n* https://github.com/warner/python-versioneer\n* Brian Warner\n* License: Public Domain\n* Compatible With: python2.6, 2.7, 3.2, 3.3, 3.4, 3.5, 3.6, and pypy\n* [![Latest Version]\n(https://pypip.in/version/versioneer/badge.svg?style=flat)\n](https://pypi.python.org/pypi/versioneer/)\n* [![Build Status]\n(https://travis-ci.org/warner/python-versioneer.png?branch=master)\n](https://travis-ci.org/warner/python-versioneer)\n\nThis is a tool for managing a recorded version number in distutils-based\npython projects. The goal is to remove the tedious and error-prone ""update\nthe embedded version string"" step from your release process. Making a new\nrelease should be as easy as recording a new tag in your version-control\nsystem, and maybe making new tarballs.\n\n\n## Quick Install\n\n* `pip install versioneer` to somewhere to your $PATH\n* add a `[versioneer]` section to your setup.cfg (see below)\n* run `versioneer install` in your source tree, commit the results\n\n## Version Identifiers\n\nSource trees come from a variety of places:\n\n* a version-control system checkout (mostly used by developers)\n* a nightly tarball, produced by build automation\n* a snapshot tarball, produced by a web-based VCS browser, like github\'s\n  ""tarball from tag"" feature\n* a release tarball, produced by ""setup.py sdist"", distributed through PyPI\n\nWithin each source tree, the version identifier (either a string or a number,\nthis tool is format-agnostic) can come from a variety of places:\n\n* ask the VCS tool itself, e.g. ""git describe"" (for checkouts), which knows\n  about recent ""tags"" and an absolute revision-id\n* the name of the directory into which the tarball was unpacked\n* an expanded VCS keyword ($Id$, etc)\n* a `_version.py` created by some earlier build step\n\nFor released software, the version identifier is closely related to a VCS\ntag. Some projects use tag names that include more than just the version\nstring (e.g. ""myproject-1.2"" instead of just ""1.2""), in which case the tool\nneeds to strip the tag prefix to extract the version identifier. For\nunreleased software (between tags), the version identifier should provide\nenough information to help developers recreate the same tree, while also\ngiving them an idea of roughly how old the tree is (after version 1.2, before\nversion 1.3). Many VCS systems can report a description that captures this,\nfor example `git describe --tags --dirty --always` reports things like\n""0.7-1-g574ab98-dirty"" to indicate that the checkout is one revision past the\n0.7 tag, has a unique revision id of ""574ab98"", and is ""dirty"" (it has\nuncommitted changes.\n\nThe version identifier is used for multiple purposes:\n\n* to allow the module to self-identify its version: `myproject.__version__`\n* to choose a name and prefix for a \'setup.py sdist\' tarball\n\n## Theory of Operation\n\nVersioneer works by adding a special `_version.py` file into your source\ntree, where your `__init__.py` can import it. This `_version.py` knows how to\ndynamically ask the VCS tool for version information at import time.\n\n`_version.py` also contains `$Revision$` markers, and the installation\nprocess marks `_version.py` to have this marker rewritten with a tag name\nduring the `git archive` command. As a result, generated tarballs will\ncontain enough information to get the proper version.\n\nTo allow `setup.py` to compute a version too, a `versioneer.py` is added to\nthe top level of your source tree, next to `setup.py` and the `setup.cfg`\nthat configures it. This overrides several distutils/setuptools commands to\ncompute the version when invoked, and changes `setup.py build` and `setup.py\nsdist` to replace `_version.py` with a small static file that contains just\nthe generated version data.\n\n## Installation\n\nSee [INSTALL.md](./INSTALL.md) for detailed installation instructions.\n\n## Version-String Flavors\n\nCode which uses Versioneer can learn about its version string at runtime by\nimporting `_version` from your main `__init__.py` file and running the\n`get_versions()` function. From the ""outside"" (e.g. in `setup.py`), you can\nimport the top-level `versioneer.py` and run `get_versions()`.\n\nBoth functions return a dictionary with different flavors of version\ninformation:\n\n* `[\'version\']`: A condensed version string, rendered using the selected\n  style. This is the most commonly used value for the project\'s version\n  string. The default ""pep440"" style yields strings like `0.11`,\n  `0.11+2.g1076c97`, or `0.11+2.g1076c97.dirty`. See the ""Styles"" section\n  below for alternative styles.\n\n* `[\'full-revisionid\']`: detailed revision identifier. For Git, this is the\n  full SHA1 commit id, e.g. ""1076c978a8d3cfc70f408fe5974aa6c092c949ac"".\n\n* `[\'date\']`: Date and time of the latest `HEAD` commit. For Git, it is the\n  commit date in ISO 8601 format. This will be None if the date is not\n  available.\n\n* `[\'dirty\']`: a boolean, True if the tree has uncommitted changes. Note that\n  this is only accurate if run in a VCS checkout, otherwise it is likely to\n  be False or None\n\n* `[\'error\']`: if the version string could not be computed, this will be set\n  to a string describing the problem, otherwise it will be None. It may be\n  useful to throw an exception in setup.py if this is set, to avoid e.g.\n  creating tarballs with a version string of ""unknown"".\n\nSome variants are more useful than others. Including `full-revisionid` in a\nbug report should allow developers to reconstruct the exact code being tested\n(or indicate the presence of local changes that should be shared with the\ndevelopers). `version` is suitable for display in an ""about"" box or a CLI\n`--version` output: it can be easily compared against release notes and lists\nof bugs fixed in various releases.\n\nThe installer adds the following text to your `__init__.py` to place a basic\nversion in `YOURPROJECT.__version__`:\n\n    from ._version import get_versions\n    __version__ = get_versions()[\'version\']\n    del get_versions\n\n## Styles\n\nThe setup.cfg `style=` configuration controls how the VCS information is\nrendered into a version string.\n\nThe default style, ""pep440"", produces a PEP440-compliant string, equal to the\nun-prefixed tag name for actual releases, and containing an additional ""local\nversion"" section with more detail for in-between builds. For Git, this is\nTAG[+DISTANCE.gHEX[.dirty]] , using information from `git describe --tags\n--dirty --always`. For example ""0.11+2.g1076c97.dirty"" indicates that the\ntree is like the ""1076c97"" commit but has uncommitted changes ("".dirty""), and\nthat this commit is two revisions (""+2"") beyond the ""0.11"" tag. For released\nsoftware (exactly equal to a known tag), the identifier will only contain the\nstripped tag, e.g. ""0.11"".\n\nOther styles are available. See [details.md](details.md) in the Versioneer\nsource tree for descriptions.\n\n## Debugging\n\nVersioneer tries to avoid fatal errors: if something goes wrong, it will tend\nto return a version of ""0+unknown"". To investigate the problem, run `setup.py\nversion`, which will run the version-lookup code in a verbose mode, and will\ndisplay the full contents of `get_versions()` (including the `error` string,\nwhich may help identify what went wrong).\n\n## Known Limitations\n\nSome situations are known to cause problems for Versioneer. This details the\nmost significant ones. More can be found on Github\n[issues page](https://github.com/warner/python-versioneer/issues).\n\n### Subprojects\n\nVersioneer has limited support for source trees in which `setup.py` is not in\nthe root directory (e.g. `setup.py` and `.git/` are *not* siblings). The are\ntwo common reasons why `setup.py` might not be in the root:\n\n* Source trees which contain multiple subprojects, such as\n  [Buildbot](https://github.com/buildbot/buildbot), which contains both\n  ""master"" and ""slave"" subprojects, each with their own `setup.py`,\n  `setup.cfg`, and `tox.ini`. Projects like these produce multiple PyPI\n  distributions (and upload multiple independently-installable tarballs).\n* Source trees whose main purpose is to contain a C library, but which also\n  provide bindings to Python (and perhaps other langauges) in subdirectories.\n\nVersioneer will look for `.git` in parent directories, and most operations\nshould get the right version string. However `pip` and `setuptools` have bugs\nand implementation details which frequently cause `pip install .` from a\nsubproject directory to fail to find a correct version string (so it usually\ndefaults to `0+unknown`).\n\n`pip install --editable .` should work correctly. `setup.py install` might\nwork too.\n\nPip-8.1.1 is known to have this problem, but hopefully it will get fixed in\nsome later version.\n\n[Bug #38](https://github.com/warner/python-versioneer/issues/38) is tracking\nthis issue. The discussion in\n[PR #61](https://github.com/warner/python-versioneer/pull/61) describes the\nissue from the Versioneer side in more detail.\n[pip PR#3176](https://github.com/pypa/pip/pull/3176) and\n[pip PR#3615](https://github.com/pypa/pip/pull/3615) contain work to improve\npip to let Versioneer work correctly.\n\nVersioneer-0.16 and earlier only looked for a `.git` directory next to the\n`setup.cfg`, so subprojects were completely unsupported with those releases.\n\n### Editable installs with setuptools <= 18.5\n\n`setup.py develop` and `pip install --editable .` allow you to install a\nproject into a virtualenv once, then continue editing the source code (and\ntest) without re-installing after every change.\n\n""Entry-point scripts"" (`setup(entry_points={""console_scripts"": ..})`) are a\nconvenient way to specify executable scripts that should be installed along\nwith the python package.\n\nThese both work as expected when using modern setuptools. When using\nsetuptools-18.5 or earlier, however, certain operations will cause\n`pkg_resources.DistributionNotFound` errors when running the entrypoint\nscript, which must be resolved by re-installing the package. This happens\nwhen the install happens with one version, then the egg_info data is\nregenerated while a different version is checked out. Many setup.py commands\ncause egg_info to be rebuilt (including `sdist`, `wheel`, and installing into\na different virtualenv), so this can be surprising.\n\n[Bug #83](https://github.com/warner/python-versioneer/issues/83) describes\nthis one, but upgrading to a newer version of setuptools should probably\nresolve it.\n\n### Unicode version strings\n\nWhile Versioneer works (and is continually tested) with both Python 2 and\nPython 3, it is not entirely consistent with bytes-vs-unicode distinctions.\nNewer releases probably generate unicode version strings on py2. It\'s not\nclear that this is wrong, but it may be surprising for applications when then\nwrite these strings to a network connection or include them in bytes-oriented\nAPIs like cryptographic checksums.\n\n[Bug #71](https://github.com/warner/python-versioneer/issues/71) investigates\nthis question.\n\n\n## Updating Versioneer\n\nTo upgrade your project to a new release of Versioneer, do the following:\n\n* install the new Versioneer (`pip install -U versioneer` or equivalent)\n* edit `setup.cfg`, if necessary, to include any new configuration settings\n  indicated by the release notes. See [UPGRADING](./UPGRADING.md) for details.\n* re-run `versioneer install` in your source tree, to replace\n  `SRC/_version.py`\n* commit any changed files\n\n## Future Directions\n\nThis tool is designed to make it easily extended to other version-control\nsystems: all VCS-specific components are in separate directories like\nsrc/git/ . The top-level `versioneer.py` script is assembled from these\ncomponents by running make-versioneer.py . In the future, make-versioneer.py\nwill take a VCS name as an argument, and will construct a version of\n`versioneer.py` that is specific to the given VCS. It might also take the\nconfiguration arguments that are currently provided manually during\ninstallation by editing setup.py . Alternatively, it might go the other\ndirection and include code from all supported VCS systems, reducing the\nnumber of intermediate scripts.\n\n\n## License\n\nTo make Versioneer easier to embed, all its code is dedicated to the public\ndomain. The `_version.py` that it creates is also in the public domain.\nSpecifically, both are released under the Creative Commons ""Public Domain\nDedication"" license (CC0-1.0), as described in\nhttps://creativecommons.org/publicdomain/zero/1.0/ .\n\n""""""\n\nfrom __future__ import print_function\ntry:\n    import configparser\nexcept ImportError:\n    import ConfigParser as configparser\nimport errno\nimport json\nimport os\nimport re\nimport subprocess\nimport sys\n\n\nclass VersioneerConfig:\n    """"""Container for Versioneer configuration parameters.""""""\n\n\ndef get_root():\n    """"""Get the project root directory.\n\n    We require that all commands are run from the project root, i.e. the\n    directory that contains setup.py, setup.cfg, and versioneer.py .\n    """"""\n    root = os.path.realpath(os.path.abspath(os.getcwd()))\n    setup_py = os.path.join(root, ""setup.py"")\n    versioneer_py = os.path.join(root, ""versioneer.py"")\n    if not (os.path.exists(setup_py) or os.path.exists(versioneer_py)):\n        # allow \'python path/to/setup.py COMMAND\'\n        root = os.path.dirname(os.path.realpath(os.path.abspath(sys.argv[0])))\n        setup_py = os.path.join(root, ""setup.py"")\n        versioneer_py = os.path.join(root, ""versioneer.py"")\n    if not (os.path.exists(setup_py) or os.path.exists(versioneer_py)):\n        err = (""Versioneer was unable to run the project root directory. ""\n               ""Versioneer requires setup.py to be executed from ""\n               ""its immediate directory (like \'python setup.py COMMAND\'), ""\n               ""or in a way that lets it use sys.argv[0] to find the root ""\n               ""(like \'python path/to/setup.py COMMAND\')."")\n        raise VersioneerBadRootError(err)\n    try:\n        # Certain runtime workflows (setup.py install/develop in a setuptools\n        # tree) execute all dependencies in a single python process, so\n        # ""versioneer"" may be imported multiple times, and python\'s shared\n        # module-import table will cache the first one. So we can\'t use\n        # os.path.dirname(__file__), as that will find whichever\n        # versioneer.py was first imported, even in later projects.\n        me = os.path.realpath(os.path.abspath(__file__))\n        me_dir = os.path.normcase(os.path.splitext(me)[0])\n        vsr_dir = os.path.normcase(os.path.splitext(versioneer_py)[0])\n        if me_dir != vsr_dir:\n            print(""Warning: build in %s is using versioneer.py from %s""\n                  % (os.path.dirname(me), versioneer_py))\n    except NameError:\n        pass\n    return root\n\n\ndef get_config_from_root(root):\n    """"""Read the project setup.cfg file to determine Versioneer config.""""""\n    # This might raise EnvironmentError (if setup.cfg is missing), or\n    # configparser.NoSectionError (if it lacks a [versioneer] section), or\n    # configparser.NoOptionError (if it lacks ""VCS=""). See the docstring at\n    # the top of versioneer.py for instructions on writing your setup.cfg .\n    setup_cfg = os.path.join(root, ""setup.cfg"")\n    parser = configparser.SafeConfigParser()\n    with open(setup_cfg, ""r"") as f:\n        parser.readfp(f)\n    VCS = parser.get(""versioneer"", ""VCS"")  # mandatory\n\n    def get(parser, name):\n        if parser.has_option(""versioneer"", name):\n            return parser.get(""versioneer"", name)\n        return None\n    cfg = VersioneerConfig()\n    cfg.VCS = VCS\n    cfg.style = get(parser, ""style"") or """"\n    cfg.versionfile_source = get(parser, ""versionfile_source"")\n    cfg.versionfile_build = get(parser, ""versionfile_build"")\n    cfg.tag_prefix = get(parser, ""tag_prefix"")\n    if cfg.tag_prefix in (""\'\'"", \'""""\'):\n        cfg.tag_prefix = """"\n    cfg.parentdir_prefix = get(parser, ""parentdir_prefix"")\n    cfg.verbose = get(parser, ""verbose"")\n    return cfg\n\n\nclass NotThisMethod(Exception):\n    """"""Exception raised if a method is not valid for the current scenario.""""""\n\n\n# these dictionaries contain VCS-specific tools\nLONG_VERSION_PY = {}\nHANDLERS = {}\n\n\ndef register_vcs_handler(vcs, method):  # decorator\n    """"""Decorator to mark a method as the handler for a particular VCS.""""""\n    def decorate(f):\n        """"""Store f in HANDLERS[vcs][method].""""""\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False,\n                env=None):\n    """"""Call the given command(s).""""""\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(""unable to run %s"" % dispcmd)\n                print(e)\n            return None, None\n    else:\n        if verbose:\n            print(""unable to find command, tried %s"" % (commands,))\n        return None, None\n    stdout = p.communicate()[0].strip()\n    if sys.version_info[0] >= 3:\n        stdout = stdout.decode()\n    if p.returncode != 0:\n        if verbose:\n            print(""unable to run %s (error)"" % dispcmd)\n            print(""stdout was %s"" % stdout)\n        return None, p.returncode\n    return stdout, p.returncode\n\n\nLONG_VERSION_PY[\'git\'] = \'\'\'\n# This file helps to compute a version number in source trees obtained from\n# git-archive tarball (such as those provided by githubs download-from-tag\n# feature). Distribution tarballs (built by setup.py sdist) and build\n# directories (produced by setup.py build) will contain a much shorter file\n# that just contains the computed version number.\n\n# This file is released into the public domain. Generated by\n# versioneer-0.18 (https://github.com/warner/python-versioneer)\n\n""""""Git implementation of _version.py.""""""\n\nimport errno\nimport os\nimport re\nimport subprocess\nimport sys\n\n\ndef get_keywords():\n    """"""Get the keywords needed to look up the version information.""""""\n    # these strings will be replaced by git during git-archive.\n    # setup.py/versioneer.py will grep for the variable names, so they must\n    # each be defined on a line of their own. _version.py will just call\n    # get_keywords().\n    git_refnames = ""%(DOLLAR)sFormat:%%d%(DOLLAR)s""\n    git_full = ""%(DOLLAR)sFormat:%%H%(DOLLAR)s""\n    git_date = ""%(DOLLAR)sFormat:%%ci%(DOLLAR)s""\n    keywords = {""refnames"": git_refnames, ""full"": git_full, ""date"": git_date}\n    return keywords\n\n\nclass VersioneerConfig:\n    """"""Container for Versioneer configuration parameters.""""""\n\n\ndef get_config():\n    """"""Create, populate and return the VersioneerConfig() object.""""""\n    # these strings are filled in when \'setup.py versioneer\' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = ""git""\n    cfg.style = ""%(STYLE)s""\n    cfg.tag_prefix = ""%(TAG_PREFIX)s""\n    cfg.parentdir_prefix = ""%(PARENTDIR_PREFIX)s""\n    cfg.versionfile_source = ""%(VERSIONFILE_SOURCE)s""\n    cfg.verbose = False\n    return cfg\n\n\nclass NotThisMethod(Exception):\n    """"""Exception raised if a method is not valid for the current scenario.""""""\n\n\nLONG_VERSION_PY = {}\nHANDLERS = {}\n\n\ndef register_vcs_handler(vcs, method):  # decorator\n    """"""Decorator to mark a method as the handler for a particular VCS.""""""\n    def decorate(f):\n        """"""Store f in HANDLERS[vcs][method].""""""\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False,\n                env=None):\n    """"""Call the given command(s).""""""\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(""unable to run %%s"" %% dispcmd)\n                print(e)\n            return None, None\n    else:\n        if verbose:\n            print(""unable to find command, tried %%s"" %% (commands,))\n        return None, None\n    stdout = p.communicate()[0].strip()\n    if sys.version_info[0] >= 3:\n        stdout = stdout.decode()\n    if p.returncode != 0:\n        if verbose:\n            print(""unable to run %%s (error)"" %% dispcmd)\n            print(""stdout was %%s"" %% stdout)\n        return None, p.returncode\n    return stdout, p.returncode\n\n\ndef versions_from_parentdir(parentdir_prefix, root, verbose):\n    """"""Try to determine the version from the parent directory name.\n\n    Source tarballs conventionally unpack into a directory that includes both\n    the project name and a version string. We will also support searching up\n    two directory levels for an appropriately named parent directory\n    """"""\n    rootdirs = []\n\n    for i in range(3):\n        dirname = os.path.basename(root)\n        if dirname.startswith(parentdir_prefix):\n            return {""version"": dirname[len(parentdir_prefix):],\n                    ""full-revisionid"": None,\n                    ""dirty"": False, ""error"": None, ""date"": None}\n        else:\n            rootdirs.append(root)\n            root = os.path.dirname(root)  # up a level\n\n    if verbose:\n        print(""Tried directories %%s but none started with prefix %%s"" %%\n              (str(rootdirs), parentdir_prefix))\n    raise NotThisMethod(""rootdir doesn\'t start with parentdir_prefix"")\n\n\n@register_vcs_handler(""git"", ""get_keywords"")\ndef git_get_keywords(versionfile_abs):\n    """"""Extract version information from the given file.""""""\n    # the code embedded in _version.py can just fetch the value of these\n    # keywords. When used from setup.py, we don\'t want to import _version.py,\n    # so we do it with a regexp instead. This function is not used from\n    # _version.py.\n    keywords = {}\n    try:\n        f = open(versionfile_abs, ""r"")\n        for line in f.readlines():\n            if line.strip().startswith(""git_refnames =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""refnames""] = mo.group(1)\n            if line.strip().startswith(""git_full =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""full""] = mo.group(1)\n            if line.strip().startswith(""git_date =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""date""] = mo.group(1)\n        f.close()\n    except EnvironmentError:\n        pass\n    return keywords\n\n\n@register_vcs_handler(""git"", ""keywords"")\ndef git_versions_from_keywords(keywords, tag_prefix, verbose):\n    """"""Get version information from git keywords.""""""\n    if not keywords:\n        raise NotThisMethod(""no keywords at all, weird"")\n    date = keywords.get(""date"")\n    if date is not None:\n        # git-2.2.0 added ""%%cI"", which expands to an ISO-8601 -compliant\n        # datestamp. However we prefer ""%%ci"" (which expands to an ""ISO-8601\n        # -like"" string, which we must then edit to make compliant), because\n        # it\'s been around since git-1.5.3, and it\'s too difficult to\n        # discover which version we\'re using, or to work around using an\n        # older one.\n        date = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n    refnames = keywords[""refnames""].strip()\n    if refnames.startswith(""$Format""):\n        if verbose:\n            print(""keywords are unexpanded, not using"")\n        raise NotThisMethod(""unexpanded keywords, not a git-archive tarball"")\n    refs = set([r.strip() for r in refnames.strip(""()"").split("","")])\n    # starting in git-1.8.3, tags are listed as ""tag: foo-1.0"" instead of\n    # just ""foo-1.0"". If we see a ""tag: "" prefix, prefer those.\n    TAG = ""tag: ""\n    tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])\n    if not tags:\n        # Either we\'re using git < 1.8.3, or there really are no tags. We use\n        # a heuristic: assume all version tags have a digit. The old git %%d\n        # expansion behaves like git log --decorate=short and strips out the\n        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n        # between branches and tags. By ignoring refnames without digits, we\n        # filter out many common branch names like ""release"" and\n        # ""stabilization"", as well as ""HEAD"" and ""master"".\n        tags = set([r for r in refs if re.search(r\'\\d\', r)])\n        if verbose:\n            print(""discarding \'%%s\', no digits"" %% "","".join(refs - tags))\n    if verbose:\n        print(""likely tags: %%s"" %% "","".join(sorted(tags)))\n    for ref in sorted(tags):\n        # sorting will prefer e.g. ""2.0"" over ""2.0rc1""\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix):]\n            if verbose:\n                print(""picking %%s"" %% r)\n            return {""version"": r,\n                    ""full-revisionid"": keywords[""full""].strip(),\n                    ""dirty"": False, ""error"": None,\n                    ""date"": date}\n    # no suitable tags, so version is ""0+unknown"", but full hex is still there\n    if verbose:\n        print(""no suitable tags, using unknown + full revision id"")\n    return {""version"": ""0+unknown"",\n            ""full-revisionid"": keywords[""full""].strip(),\n            ""dirty"": False, ""error"": ""no suitable tags"", ""date"": None}\n\n\n@register_vcs_handler(""git"", ""pieces_from_vcs"")\ndef git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n    """"""Get version from \'git describe\' in the root of the source tree.\n\n    This only gets called if the git-archive \'subst\' keywords were *not*\n    expanded, and _version.py hasn\'t already been rewritten with a short\n    version string, meaning we\'re inside a checked out source tree.\n    """"""\n    GITS = [""git""]\n    if sys.platform == ""win32"":\n        GITS = [""git.cmd"", ""git.exe""]\n\n    out, rc = run_command(GITS, [""rev-parse"", ""--git-dir""], cwd=root,\n                          hide_stderr=True)\n    if rc != 0:\n        if verbose:\n            print(""Directory %%s not under git control"" %% root)\n        raise NotThisMethod(""\'git rev-parse --git-dir\' returned error"")\n\n    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n    # if there isn\'t one, this yields HEX[-dirty] (no NUM)\n    describe_out, rc = run_command(GITS, [""describe"", ""--tags"", ""--dirty"",\n                                          ""--always"", ""--long"",\n                                          ""--match"", ""%%s*"" %% tag_prefix],\n                                   cwd=root)\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(""\'git describe\' failed"")\n    describe_out = describe_out.strip()\n    full_out, rc = run_command(GITS, [""rev-parse"", ""HEAD""], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(""\'git rev-parse\' failed"")\n    full_out = full_out.strip()\n\n    pieces = {}\n    pieces[""long""] = full_out\n    pieces[""short""] = full_out[:7]  # maybe improved later\n    pieces[""error""] = None\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(""-dirty"")\n    pieces[""dirty""] = dirty\n    if dirty:\n        git_describe = git_describe[:git_describe.rindex(""-dirty"")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if ""-"" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r\'^(.+)-(\\d+)-g([0-9a-f]+)$\', git_describe)\n        if not mo:\n            # unparseable. Maybe git-describe is misbehaving?\n            pieces[""error""] = (""unable to parse git-describe output: \'%%s\'""\n                               %% describe_out)\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = ""tag \'%%s\' doesn\'t start with prefix \'%%s\'""\n                print(fmt %% (full_tag, tag_prefix))\n            pieces[""error""] = (""tag \'%%s\' doesn\'t start with prefix \'%%s\'""\n                               %% (full_tag, tag_prefix))\n            return pieces\n        pieces[""closest-tag""] = full_tag[len(tag_prefix):]\n\n        # distance: number of commits since tag\n        pieces[""distance""] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[""short""] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[""closest-tag""] = None\n        count_out, rc = run_command(GITS, [""rev-list"", ""HEAD"", ""--count""],\n                                    cwd=root)\n        pieces[""distance""] = int(count_out)  # total number of commits\n\n    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n    date = run_command(GITS, [""show"", ""-s"", ""--format=%%ci"", ""HEAD""],\n                       cwd=root)[0].strip()\n    pieces[""date""] = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n\n    return pieces\n\n\ndef plus_or_dot(pieces):\n    """"""Return a + if we don\'t already have one, else return a .""""""\n    if ""+"" in pieces.get(""closest-tag"", """"):\n        return "".""\n    return ""+""\n\n\ndef render_pep440(pieces):\n    """"""Build up version string, with post-release ""local version identifier"".\n\n    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    get a tagged build and then dirty it, you\'ll get TAG+0.gHEX.dirty\n\n    Exceptions:\n    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += plus_or_dot(pieces)\n            rendered += ""%%d.g%%s"" %% (pieces[""distance""], pieces[""short""])\n            if pieces[""dirty""]:\n                rendered += "".dirty""\n    else:\n        # exception #1\n        rendered = ""0+untagged.%%d.g%%s"" %% (pieces[""distance""],\n                                          pieces[""short""])\n        if pieces[""dirty""]:\n            rendered += "".dirty""\n    return rendered\n\n\ndef render_pep440_pre(pieces):\n    """"""TAG[.post.devDISTANCE] -- No -dirty.\n\n    Exceptions:\n    1: no tags. 0.post.devDISTANCE\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += "".post.dev%%d"" %% pieces[""distance""]\n    else:\n        # exception #1\n        rendered = ""0.post.dev%%d"" %% pieces[""distance""]\n    return rendered\n\n\ndef render_pep440_post(pieces):\n    """"""TAG[.postDISTANCE[.dev0]+gHEX] .\n\n    The "".dev0"" means dirty. Note that .dev0 sorts backwards\n    (a dirty tree will appear ""older"" than the corresponding clean one),\n    but you shouldn\'t be releasing software with -dirty anyways.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%%d"" %% pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n            rendered += plus_or_dot(pieces)\n            rendered += ""g%%s"" %% pieces[""short""]\n    else:\n        # exception #1\n        rendered = ""0.post%%d"" %% pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n        rendered += ""+g%%s"" %% pieces[""short""]\n    return rendered\n\n\ndef render_pep440_old(pieces):\n    """"""TAG[.postDISTANCE[.dev0]] .\n\n    The "".dev0"" means dirty.\n\n    Eexceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%%d"" %% pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n    else:\n        # exception #1\n        rendered = ""0.post%%d"" %% pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n    return rendered\n\n\ndef render_git_describe(pieces):\n    """"""TAG[-DISTANCE-gHEX][-dirty].\n\n    Like \'git describe --tags --dirty --always\'.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += ""-%%d-g%%s"" %% (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render_git_describe_long(pieces):\n    """"""TAG-DISTANCE-gHEX[-dirty].\n\n    Like \'git describe --tags --dirty --always -long\'.\n    The distance/hash is unconditional.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        rendered += ""-%%d-g%%s"" %% (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render(pieces, style):\n    """"""Render the given version pieces into the requested style.""""""\n    if pieces[""error""]:\n        return {""version"": ""unknown"",\n                ""full-revisionid"": pieces.get(""long""),\n                ""dirty"": None,\n                ""error"": pieces[""error""],\n                ""date"": None}\n\n    if not style or style == ""default"":\n        style = ""pep440""  # the default\n\n    if style == ""pep440"":\n        rendered = render_pep440(pieces)\n    elif style == ""pep440-pre"":\n        rendered = render_pep440_pre(pieces)\n    elif style == ""pep440-post"":\n        rendered = render_pep440_post(pieces)\n    elif style == ""pep440-old"":\n        rendered = render_pep440_old(pieces)\n    elif style == ""git-describe"":\n        rendered = render_git_describe(pieces)\n    elif style == ""git-describe-long"":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(""unknown style \'%%s\'"" %% style)\n\n    return {""version"": rendered, ""full-revisionid"": pieces[""long""],\n            ""dirty"": pieces[""dirty""], ""error"": None,\n            ""date"": pieces.get(""date"")}\n\n\ndef get_versions():\n    """"""Get version information or return default if unable to do so.""""""\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don\'t do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n                                          verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the .git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for i in cfg.versionfile_source.split(\'/\'):\n            root = os.path.dirname(root)\n    except NameError:\n        return {""version"": ""0+unknown"", ""full-revisionid"": None,\n                ""dirty"": None,\n                ""error"": ""unable to find root of source tree"",\n                ""date"": None}\n\n    try:\n        pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)\n        return render(pieces, cfg.style)\n    except NotThisMethod:\n        pass\n\n    try:\n        if cfg.parentdir_prefix:\n            return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n    except NotThisMethod:\n        pass\n\n    return {""version"": ""0+unknown"", ""full-revisionid"": None,\n            ""dirty"": None,\n            ""error"": ""unable to compute version"", ""date"": None}\n\'\'\'\n\n\n@register_vcs_handler(""git"", ""get_keywords"")\ndef git_get_keywords(versionfile_abs):\n    """"""Extract version information from the given file.""""""\n    # the code embedded in _version.py can just fetch the value of these\n    # keywords. When used from setup.py, we don\'t want to import _version.py,\n    # so we do it with a regexp instead. This function is not used from\n    # _version.py.\n    keywords = {}\n    try:\n        f = open(versionfile_abs, ""r"")\n        for line in f.readlines():\n            if line.strip().startswith(""git_refnames =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""refnames""] = mo.group(1)\n            if line.strip().startswith(""git_full =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""full""] = mo.group(1)\n            if line.strip().startswith(""git_date =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""date""] = mo.group(1)\n        f.close()\n    except EnvironmentError:\n        pass\n    return keywords\n\n\n@register_vcs_handler(""git"", ""keywords"")\ndef git_versions_from_keywords(keywords, tag_prefix, verbose):\n    """"""Get version information from git keywords.""""""\n    if not keywords:\n        raise NotThisMethod(""no keywords at all, weird"")\n    date = keywords.get(""date"")\n    if date is not None:\n        # git-2.2.0 added ""%cI"", which expands to an ISO-8601 -compliant\n        # datestamp. However we prefer ""%ci"" (which expands to an ""ISO-8601\n        # -like"" string, which we must then edit to make compliant), because\n        # it\'s been around since git-1.5.3, and it\'s too difficult to\n        # discover which version we\'re using, or to work around using an\n        # older one.\n        date = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n    refnames = keywords[""refnames""].strip()\n    if refnames.startswith(""$Format""):\n        if verbose:\n            print(""keywords are unexpanded, not using"")\n        raise NotThisMethod(""unexpanded keywords, not a git-archive tarball"")\n    refs = set([r.strip() for r in refnames.strip(""()"").split("","")])\n    # starting in git-1.8.3, tags are listed as ""tag: foo-1.0"" instead of\n    # just ""foo-1.0"". If we see a ""tag: "" prefix, prefer those.\n    TAG = ""tag: ""\n    tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])\n    if not tags:\n        # Either we\'re using git < 1.8.3, or there really are no tags. We use\n        # a heuristic: assume all version tags have a digit. The old git %d\n        # expansion behaves like git log --decorate=short and strips out the\n        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n        # between branches and tags. By ignoring refnames without digits, we\n        # filter out many common branch names like ""release"" and\n        # ""stabilization"", as well as ""HEAD"" and ""master"".\n        tags = set([r for r in refs if re.search(r\'\\d\', r)])\n        if verbose:\n            print(""discarding \'%s\', no digits"" % "","".join(refs - tags))\n    if verbose:\n        print(""likely tags: %s"" % "","".join(sorted(tags)))\n    for ref in sorted(tags):\n        # sorting will prefer e.g. ""2.0"" over ""2.0rc1""\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix):]\n            if verbose:\n                print(""picking %s"" % r)\n            return {""version"": r,\n                    ""full-revisionid"": keywords[""full""].strip(),\n                    ""dirty"": False, ""error"": None,\n                    ""date"": date}\n    # no suitable tags, so version is ""0+unknown"", but full hex is still there\n    if verbose:\n        print(""no suitable tags, using unknown + full revision id"")\n    return {""version"": ""0+unknown"",\n            ""full-revisionid"": keywords[""full""].strip(),\n            ""dirty"": False, ""error"": ""no suitable tags"", ""date"": None}\n\n\n@register_vcs_handler(""git"", ""pieces_from_vcs"")\ndef git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n    """"""Get version from \'git describe\' in the root of the source tree.\n\n    This only gets called if the git-archive \'subst\' keywords were *not*\n    expanded, and _version.py hasn\'t already been rewritten with a short\n    version string, meaning we\'re inside a checked out source tree.\n    """"""\n    GITS = [""git""]\n    if sys.platform == ""win32"":\n        GITS = [""git.cmd"", ""git.exe""]\n\n    out, rc = run_command(GITS, [""rev-parse"", ""--git-dir""], cwd=root,\n                          hide_stderr=True)\n    if rc != 0:\n        if verbose:\n            print(""Directory %s not under git control"" % root)\n        raise NotThisMethod(""\'git rev-parse --git-dir\' returned error"")\n\n    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n    # if there isn\'t one, this yields HEX[-dirty] (no NUM)\n    describe_out, rc = run_command(GITS, [""describe"", ""--tags"", ""--dirty"",\n                                          ""--always"", ""--long"",\n                                          ""--match"", ""%s*"" % tag_prefix],\n                                   cwd=root)\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(""\'git describe\' failed"")\n    describe_out = describe_out.strip()\n    full_out, rc = run_command(GITS, [""rev-parse"", ""HEAD""], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(""\'git rev-parse\' failed"")\n    full_out = full_out.strip()\n\n    pieces = {}\n    pieces[""long""] = full_out\n    pieces[""short""] = full_out[:7]  # maybe improved later\n    pieces[""error""] = None\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(""-dirty"")\n    pieces[""dirty""] = dirty\n    if dirty:\n        git_describe = git_describe[:git_describe.rindex(""-dirty"")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if ""-"" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r\'^(.+)-(\\d+)-g([0-9a-f]+)$\', git_describe)\n        if not mo:\n            # unparseable. Maybe git-describe is misbehaving?\n            pieces[""error""] = (""unable to parse git-describe output: \'%s\'""\n                               % describe_out)\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = ""tag \'%s\' doesn\'t start with prefix \'%s\'""\n                print(fmt % (full_tag, tag_prefix))\n            pieces[""error""] = (""tag \'%s\' doesn\'t start with prefix \'%s\'""\n                               % (full_tag, tag_prefix))\n            return pieces\n        pieces[""closest-tag""] = full_tag[len(tag_prefix):]\n\n        # distance: number of commits since tag\n        pieces[""distance""] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[""short""] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[""closest-tag""] = None\n        count_out, rc = run_command(GITS, [""rev-list"", ""HEAD"", ""--count""],\n                                    cwd=root)\n        pieces[""distance""] = int(count_out)  # total number of commits\n\n    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n    date = run_command(GITS, [""show"", ""-s"", ""--format=%ci"", ""HEAD""],\n                       cwd=root)[0].strip()\n    pieces[""date""] = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n\n    return pieces\n\n\ndef do_vcs_install(manifest_in, versionfile_source, ipy):\n    """"""Git-specific installation logic for Versioneer.\n\n    For Git, this means creating/changing .gitattributes to mark _version.py\n    for export-subst keyword substitution.\n    """"""\n    GITS = [""git""]\n    if sys.platform == ""win32"":\n        GITS = [""git.cmd"", ""git.exe""]\n    files = [manifest_in, versionfile_source]\n    if ipy:\n        files.append(ipy)\n    try:\n        me = __file__\n        if me.endswith("".pyc"") or me.endswith("".pyo""):\n            me = os.path.splitext(me)[0] + "".py""\n        versioneer_file = os.path.relpath(me)\n    except NameError:\n        versioneer_file = ""versioneer.py""\n    files.append(versioneer_file)\n    present = False\n    try:\n        f = open("".gitattributes"", ""r"")\n        for line in f.readlines():\n            if line.strip().startswith(versionfile_source):\n                if ""export-subst"" in line.strip().split()[1:]:\n                    present = True\n        f.close()\n    except EnvironmentError:\n        pass\n    if not present:\n        f = open("".gitattributes"", ""a+"")\n        f.write(""%s export-subst\\n"" % versionfile_source)\n        f.close()\n        files.append("".gitattributes"")\n    run_command(GITS, [""add"", ""--""] + files)\n\n\ndef versions_from_parentdir(parentdir_prefix, root, verbose):\n    """"""Try to determine the version from the parent directory name.\n\n    Source tarballs conventionally unpack into a directory that includes both\n    the project name and a version string. We will also support searching up\n    two directory levels for an appropriately named parent directory\n    """"""\n    rootdirs = []\n\n    for i in range(3):\n        dirname = os.path.basename(root)\n        if dirname.startswith(parentdir_prefix):\n            return {""version"": dirname[len(parentdir_prefix):],\n                    ""full-revisionid"": None,\n                    ""dirty"": False, ""error"": None, ""date"": None}\n        else:\n            rootdirs.append(root)\n            root = os.path.dirname(root)  # up a level\n\n    if verbose:\n        print(""Tried directories %s but none started with prefix %s"" %\n              (str(rootdirs), parentdir_prefix))\n    raise NotThisMethod(""rootdir doesn\'t start with parentdir_prefix"")\n\n\nSHORT_VERSION_PY = """"""\n# This file was generated by \'versioneer.py\' (0.18) from\n# revision-control system data, or from the parent directory name of an\n# unpacked source archive. Distribution tarballs contain a pre-generated copy\n# of this file.\n\nimport json\n\nversion_json = \'\'\'\n%s\n\'\'\'  # END VERSION_JSON\n\n\ndef get_versions():\n    return json.loads(version_json)\n""""""\n\n\ndef versions_from_file(filename):\n    """"""Try to determine the version from _version.py if present.""""""\n    try:\n        with open(filename) as f:\n            contents = f.read()\n    except EnvironmentError:\n        raise NotThisMethod(""unable to read _version.py"")\n    mo = re.search(r""version_json = \'\'\'\\n(.*)\'\'\'  # END VERSION_JSON"",\n                   contents, re.M | re.S)\n    if not mo:\n        mo = re.search(r""version_json = \'\'\'\\r\\n(.*)\'\'\'  # END VERSION_JSON"",\n                       contents, re.M | re.S)\n    if not mo:\n        raise NotThisMethod(""no version_json in _version.py"")\n    return json.loads(mo.group(1))\n\n\ndef write_to_version_file(filename, versions):\n    """"""Write the given version number to the given _version.py file.""""""\n    os.unlink(filename)\n    contents = json.dumps(versions, sort_keys=True,\n                          indent=1, separators=("","", "": ""))\n    with open(filename, ""w"") as f:\n        f.write(SHORT_VERSION_PY % contents)\n\n    print(""set %s to \'%s\'"" % (filename, versions[""version""]))\n\n\ndef plus_or_dot(pieces):\n    """"""Return a + if we don\'t already have one, else return a .""""""\n    if ""+"" in pieces.get(""closest-tag"", """"):\n        return "".""\n    return ""+""\n\n\ndef render_pep440(pieces):\n    """"""Build up version string, with post-release ""local version identifier"".\n\n    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    get a tagged build and then dirty it, you\'ll get TAG+0.gHEX.dirty\n\n    Exceptions:\n    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += plus_or_dot(pieces)\n            rendered += ""%d.g%s"" % (pieces[""distance""], pieces[""short""])\n            if pieces[""dirty""]:\n                rendered += "".dirty""\n    else:\n        # exception #1\n        rendered = ""0+untagged.%d.g%s"" % (pieces[""distance""],\n                                          pieces[""short""])\n        if pieces[""dirty""]:\n            rendered += "".dirty""\n    return rendered\n\n\ndef render_pep440_pre(pieces):\n    """"""TAG[.post.devDISTANCE] -- No -dirty.\n\n    Exceptions:\n    1: no tags. 0.post.devDISTANCE\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += "".post.dev%d"" % pieces[""distance""]\n    else:\n        # exception #1\n        rendered = ""0.post.dev%d"" % pieces[""distance""]\n    return rendered\n\n\ndef render_pep440_post(pieces):\n    """"""TAG[.postDISTANCE[.dev0]+gHEX] .\n\n    The "".dev0"" means dirty. Note that .dev0 sorts backwards\n    (a dirty tree will appear ""older"" than the corresponding clean one),\n    but you shouldn\'t be releasing software with -dirty anyways.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%d"" % pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n            rendered += plus_or_dot(pieces)\n            rendered += ""g%s"" % pieces[""short""]\n    else:\n        # exception #1\n        rendered = ""0.post%d"" % pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n        rendered += ""+g%s"" % pieces[""short""]\n    return rendered\n\n\ndef render_pep440_old(pieces):\n    """"""TAG[.postDISTANCE[.dev0]] .\n\n    The "".dev0"" means dirty.\n\n    Eexceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%d"" % pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n    else:\n        # exception #1\n        rendered = ""0.post%d"" % pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n    return rendered\n\n\ndef render_git_describe(pieces):\n    """"""TAG[-DISTANCE-gHEX][-dirty].\n\n    Like \'git describe --tags --dirty --always\'.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += ""-%d-g%s"" % (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render_git_describe_long(pieces):\n    """"""TAG-DISTANCE-gHEX[-dirty].\n\n    Like \'git describe --tags --dirty --always -long\'.\n    The distance/hash is unconditional.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        rendered += ""-%d-g%s"" % (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render(pieces, style):\n    """"""Render the given version pieces into the requested style.""""""\n    if pieces[""error""]:\n        return {""version"": ""unknown"",\n                ""full-revisionid"": pieces.get(""long""),\n                ""dirty"": None,\n                ""error"": pieces[""error""],\n                ""date"": None}\n\n    if not style or style == ""default"":\n        style = ""pep440""  # the default\n\n    if style == ""pep440"":\n        rendered = render_pep440(pieces)\n    elif style == ""pep440-pre"":\n        rendered = render_pep440_pre(pieces)\n    elif style == ""pep440-post"":\n        rendered = render_pep440_post(pieces)\n    elif style == ""pep440-old"":\n        rendered = render_pep440_old(pieces)\n    elif style == ""git-describe"":\n        rendered = render_git_describe(pieces)\n    elif style == ""git-describe-long"":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(""unknown style \'%s\'"" % style)\n\n    return {""version"": rendered, ""full-revisionid"": pieces[""long""],\n            ""dirty"": pieces[""dirty""], ""error"": None,\n            ""date"": pieces.get(""date"")}\n\n\nclass VersioneerBadRootError(Exception):\n    """"""The project root directory is unknown or missing key files.""""""\n\n\ndef get_versions(verbose=False):\n    """"""Get the project version from whatever source is available.\n\n    Returns dict with two keys: \'version\' and \'full\'.\n    """"""\n    if ""versioneer"" in sys.modules:\n        # see the discussion in cmdclass.py:get_cmdclass()\n        del sys.modules[""versioneer""]\n\n    root = get_root()\n    cfg = get_config_from_root(root)\n\n    assert cfg.VCS is not None, ""please set [versioneer]VCS= in setup.cfg""\n    handlers = HANDLERS.get(cfg.VCS)\n    assert handlers, ""unrecognized VCS \'%s\'"" % cfg.VCS\n    verbose = verbose or cfg.verbose\n    assert cfg.versionfile_source is not None, \\\n        ""please set versioneer.versionfile_source""\n    assert cfg.tag_prefix is not None, ""please set versioneer.tag_prefix""\n\n    versionfile_abs = os.path.join(root, cfg.versionfile_source)\n\n    # extract version from first of: _version.py, VCS command (e.g. \'git\n    # describe\'), parentdir. This is meant to work for developers using a\n    # source checkout, for users of a tarball created by \'setup.py sdist\',\n    # and for users of a tarball/zipball created by \'git archive\' or github\'s\n    # download-from-tag feature or the equivalent in other VCSes.\n\n    get_keywords_f = handlers.get(""get_keywords"")\n    from_keywords_f = handlers.get(""keywords"")\n    if get_keywords_f and from_keywords_f:\n        try:\n            keywords = get_keywords_f(versionfile_abs)\n            ver = from_keywords_f(keywords, cfg.tag_prefix, verbose)\n            if verbose:\n                print(""got version from expanded keyword %s"" % ver)\n            return ver\n        except NotThisMethod:\n            pass\n\n    try:\n        ver = versions_from_file(versionfile_abs)\n        if verbose:\n            print(""got version from file %s %s"" % (versionfile_abs, ver))\n        return ver\n    except NotThisMethod:\n        pass\n\n    from_vcs_f = handlers.get(""pieces_from_vcs"")\n    if from_vcs_f:\n        try:\n            pieces = from_vcs_f(cfg.tag_prefix, root, verbose)\n            ver = render(pieces, cfg.style)\n            if verbose:\n                print(""got version from VCS %s"" % ver)\n            return ver\n        except NotThisMethod:\n            pass\n\n    try:\n        if cfg.parentdir_prefix:\n            ver = versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n            if verbose:\n                print(""got version from parentdir %s"" % ver)\n            return ver\n    except NotThisMethod:\n        pass\n\n    if verbose:\n        print(""unable to compute version"")\n\n    return {""version"": ""0+unknown"", ""full-revisionid"": None,\n            ""dirty"": None, ""error"": ""unable to compute version"",\n            ""date"": None}\n\n\ndef get_version():\n    """"""Get the short version string for this project.""""""\n    return get_versions()[""version""]\n\n\ndef get_cmdclass():\n    """"""Get the custom setuptools/distutils subclasses used by Versioneer.""""""\n    if ""versioneer"" in sys.modules:\n        del sys.modules[""versioneer""]\n        # this fixes the ""python setup.py develop"" case (also \'install\' and\n        # \'easy_install .\'), in which subdependencies of the main project are\n        # built (using setup.py bdist_egg) in the same python process. Assume\n        # a main project A and a dependency B, which use different versions\n        # of Versioneer. A\'s setup.py imports A\'s Versioneer, leaving it in\n        # sys.modules by the time B\'s setup.py is executed, causing B to run\n        # with the wrong versioneer. Setuptools wraps the sub-dep builds in a\n        # sandbox that restores sys.modules to it\'s pre-build state, so the\n        # parent is protected against the child\'s ""import versioneer"". By\n        # removing ourselves from sys.modules here, before the child build\n        # happens, we protect the child from the parent\'s versioneer too.\n        # Also see https://github.com/warner/python-versioneer/issues/52\n\n    cmds = {}\n\n    # we add ""version"" to both distutils and setuptools\n    from distutils.core import Command\n\n    class cmd_version(Command):\n        description = ""report generated version string""\n        user_options = []\n        boolean_options = []\n\n        def initialize_options(self):\n            pass\n\n        def finalize_options(self):\n            pass\n\n        def run(self):\n            vers = get_versions(verbose=True)\n            print(""Version: %s"" % vers[""version""])\n            print("" full-revisionid: %s"" % vers.get(""full-revisionid""))\n            print("" dirty: %s"" % vers.get(""dirty""))\n            print("" date: %s"" % vers.get(""date""))\n            if vers[""error""]:\n                print("" error: %s"" % vers[""error""])\n    cmds[""version""] = cmd_version\n\n    # we override ""build_py"" in both distutils and setuptools\n    #\n    # most invocation pathways end up running build_py:\n    #  distutils/build -> build_py\n    #  distutils/install -> distutils/build ->..\n    #  setuptools/bdist_wheel -> distutils/install ->..\n    #  setuptools/bdist_egg -> distutils/install_lib -> build_py\n    #  setuptools/install -> bdist_egg ->..\n    #  setuptools/develop -> ?\n    #  pip install:\n    #   copies source tree to a tempdir before running egg_info/etc\n    #   if .git isn\'t copied too, \'git describe\' will fail\n    #   then does setup.py bdist_wheel, or sometimes setup.py install\n    #  setup.py egg_info -> ?\n\n    # we override different ""build_py"" commands for both environments\n    if ""setuptools"" in sys.modules:\n        from setuptools.command.build_py import build_py as _build_py\n    else:\n        from distutils.command.build_py import build_py as _build_py\n\n    class cmd_build_py(_build_py):\n        def run(self):\n            root = get_root()\n            cfg = get_config_from_root(root)\n            versions = get_versions()\n            _build_py.run(self)\n            # now locate _version.py in the new build/ directory and replace\n            # it with an updated value\n            if cfg.versionfile_build:\n                target_versionfile = os.path.join(self.build_lib,\n                                                  cfg.versionfile_build)\n                print(""UPDATING %s"" % target_versionfile)\n                write_to_version_file(target_versionfile, versions)\n    cmds[""build_py""] = cmd_build_py\n\n    if ""cx_Freeze"" in sys.modules:  # cx_freeze enabled?\n        from cx_Freeze.dist import build_exe as _build_exe\n        # nczeczulin reports that py2exe won\'t like the pep440-style string\n        # as FILEVERSION, but it can be used for PRODUCTVERSION, e.g.\n        # setup(console=[{\n        #   ""version"": versioneer.get_version().split(""+"", 1)[0], # FILEVERSION\n        #   ""product_version"": versioneer.get_version(),\n        #   ...\n\n        class cmd_build_exe(_build_exe):\n            def run(self):\n                root = get_root()\n                cfg = get_config_from_root(root)\n                versions = get_versions()\n                target_versionfile = cfg.versionfile_source\n                print(""UPDATING %s"" % target_versionfile)\n                write_to_version_file(target_versionfile, versions)\n\n                _build_exe.run(self)\n                os.unlink(target_versionfile)\n                with open(cfg.versionfile_source, ""w"") as f:\n                    LONG = LONG_VERSION_PY[cfg.VCS]\n                    f.write(LONG %\n                            {""DOLLAR"": ""$"",\n                             ""STYLE"": cfg.style,\n                             ""TAG_PREFIX"": cfg.tag_prefix,\n                             ""PARENTDIR_PREFIX"": cfg.parentdir_prefix,\n                             ""VERSIONFILE_SOURCE"": cfg.versionfile_source,\n                             })\n        cmds[""build_exe""] = cmd_build_exe\n        del cmds[""build_py""]\n\n    if \'py2exe\' in sys.modules:  # py2exe enabled?\n        try:\n            from py2exe.distutils_buildexe import py2exe as _py2exe  # py3\n        except ImportError:\n            from py2exe.build_exe import py2exe as _py2exe  # py2\n\n        class cmd_py2exe(_py2exe):\n            def run(self):\n                root = get_root()\n                cfg = get_config_from_root(root)\n                versions = get_versions()\n                target_versionfile = cfg.versionfile_source\n                print(""UPDATING %s"" % target_versionfile)\n                write_to_version_file(target_versionfile, versions)\n\n                _py2exe.run(self)\n                os.unlink(target_versionfile)\n                with open(cfg.versionfile_source, ""w"") as f:\n                    LONG = LONG_VERSION_PY[cfg.VCS]\n                    f.write(LONG %\n                            {""DOLLAR"": ""$"",\n                             ""STYLE"": cfg.style,\n                             ""TAG_PREFIX"": cfg.tag_prefix,\n                             ""PARENTDIR_PREFIX"": cfg.parentdir_prefix,\n                             ""VERSIONFILE_SOURCE"": cfg.versionfile_source,\n                             })\n        cmds[""py2exe""] = cmd_py2exe\n\n    # we override different ""sdist"" commands for both environments\n    if ""setuptools"" in sys.modules:\n        from setuptools.command.sdist import sdist as _sdist\n    else:\n        from distutils.command.sdist import sdist as _sdist\n\n    class cmd_sdist(_sdist):\n        def run(self):\n            versions = get_versions()\n            self._versioneer_generated_versions = versions\n            # unless we update this, the command will keep using the old\n            # version\n            self.distribution.metadata.version = versions[""version""]\n            return _sdist.run(self)\n\n        def make_release_tree(self, base_dir, files):\n            root = get_root()\n            cfg = get_config_from_root(root)\n            _sdist.make_release_tree(self, base_dir, files)\n            # now locate _version.py in the new base_dir directory\n            # (remembering that it may be a hardlink) and replace it with an\n            # updated value\n            target_versionfile = os.path.join(base_dir, cfg.versionfile_source)\n            print(""UPDATING %s"" % target_versionfile)\n            write_to_version_file(target_versionfile,\n                                  self._versioneer_generated_versions)\n    cmds[""sdist""] = cmd_sdist\n\n    return cmds\n\n\nCONFIG_ERROR = """"""\nsetup.cfg is missing the necessary Versioneer configuration. You need\na section like:\n\n [versioneer]\n VCS = git\n style = pep440\n versionfile_source = src/myproject/_version.py\n versionfile_build = myproject/_version.py\n tag_prefix =\n parentdir_prefix = myproject-\n\nYou will also need to edit your setup.py to use the results:\n\n import versioneer\n setup(version=versioneer.get_version(),\n       cmdclass=versioneer.get_cmdclass(), ...)\n\nPlease read the docstring in ./versioneer.py for configuration instructions,\nedit setup.cfg, and re-run the installer or \'python versioneer.py setup\'.\n""""""\n\nSAMPLE_CONFIG = """"""\n# See the docstring in versioneer.py for instructions. Note that you must\n# re-run \'versioneer.py setup\' after changing this section, and commit the\n# resulting files.\n\n[versioneer]\n#VCS = git\n#style = pep440\n#versionfile_source =\n#versionfile_build =\n#tag_prefix =\n#parentdir_prefix =\n\n""""""\n\nINIT_PY_SNIPPET = """"""\nfrom ._version import get_versions\n__version__ = get_versions()[\'version\']\ndel get_versions\n""""""\n\n\ndef do_setup():\n    """"""Main VCS-independent setup function for installing Versioneer.""""""\n    root = get_root()\n    try:\n        cfg = get_config_from_root(root)\n    except (EnvironmentError, configparser.NoSectionError,\n            configparser.NoOptionError) as e:\n        if isinstance(e, (EnvironmentError, configparser.NoSectionError)):\n            print(""Adding sample versioneer config to setup.cfg"",\n                  file=sys.stderr)\n            with open(os.path.join(root, ""setup.cfg""), ""a"") as f:\n                f.write(SAMPLE_CONFIG)\n        print(CONFIG_ERROR, file=sys.stderr)\n        return 1\n\n    print("" creating %s"" % cfg.versionfile_source)\n    with open(cfg.versionfile_source, ""w"") as f:\n        LONG = LONG_VERSION_PY[cfg.VCS]\n        f.write(LONG % {""DOLLAR"": ""$"",\n                        ""STYLE"": cfg.style,\n                        ""TAG_PREFIX"": cfg.tag_prefix,\n                        ""PARENTDIR_PREFIX"": cfg.parentdir_prefix,\n                        ""VERSIONFILE_SOURCE"": cfg.versionfile_source,\n                        })\n\n    ipy = os.path.join(os.path.dirname(cfg.versionfile_source),\n                       ""__init__.py"")\n    if os.path.exists(ipy):\n        try:\n            with open(ipy, ""r"") as f:\n                old = f.read()\n        except EnvironmentError:\n            old = """"\n        if INIT_PY_SNIPPET not in old:\n            print("" appending to %s"" % ipy)\n            with open(ipy, ""a"") as f:\n                f.write(INIT_PY_SNIPPET)\n        else:\n            print("" %s unmodified"" % ipy)\n    else:\n        print("" %s doesn\'t exist, ok"" % ipy)\n        ipy = None\n\n    # Make sure both the top-level ""versioneer.py"" and versionfile_source\n    # (PKG/_version.py, used by runtime code) are in MANIFEST.in, so\n    # they\'ll be copied into source distributions. Pip won\'t be able to\n    # install the package without this.\n    manifest_in = os.path.join(root, ""MANIFEST.in"")\n    simple_includes = set()\n    try:\n        with open(manifest_in, ""r"") as f:\n            for line in f:\n                if line.startswith(""include ""):\n                    for include in line.split()[1:]:\n                        simple_includes.add(include)\n    except EnvironmentError:\n        pass\n    # That doesn\'t cover everything MANIFEST.in can do\n    # (http://docs.python.org/2/distutils/sourcedist.html#commands), so\n    # it might give some false negatives. Appending redundant \'include\'\n    # lines is safe, though.\n    if ""versioneer.py"" not in simple_includes:\n        print("" appending \'versioneer.py\' to MANIFEST.in"")\n        with open(manifest_in, ""a"") as f:\n            f.write(""include versioneer.py\\n"")\n    else:\n        print("" \'versioneer.py\' already in MANIFEST.in"")\n    if cfg.versionfile_source not in simple_includes:\n        print("" appending versionfile_source (\'%s\') to MANIFEST.in"" %\n              cfg.versionfile_source)\n        with open(manifest_in, ""a"") as f:\n            f.write(""include %s\\n"" % cfg.versionfile_source)\n    else:\n        print("" versionfile_source already in MANIFEST.in"")\n\n    # Make VCS-specific changes. For git, this means creating/changing\n    # .gitattributes to mark _version.py for export-subst keyword\n    # substitution.\n    do_vcs_install(manifest_in, cfg.versionfile_source, ipy)\n    return 0\n\n\ndef scan_setup_py():\n    """"""Validate the contents of setup.py against Versioneer\'s expectations.""""""\n    found = set()\n    setters = False\n    errors = 0\n    with open(""setup.py"", ""r"") as f:\n        for line in f.readlines():\n            if ""import versioneer"" in line:\n                found.add(""import"")\n            if ""versioneer.get_cmdclass()"" in line:\n                found.add(""cmdclass"")\n            if ""versioneer.get_version()"" in line:\n                found.add(""get_version"")\n            if ""versioneer.VCS"" in line:\n                setters = True\n            if ""versioneer.versionfile_source"" in line:\n                setters = True\n    if len(found) != 3:\n        print("""")\n        print(""Your setup.py appears to be missing some important items"")\n        print(""(but I might be wrong). Please make sure it has something"")\n        print(""roughly like the following:"")\n        print("""")\n        print("" import versioneer"")\n        print("" setup( version=versioneer.get_version(),"")\n        print(""        cmdclass=versioneer.get_cmdclass(),  ...)"")\n        print("""")\n        errors += 1\n    if setters:\n        print(""You should remove lines like \'versioneer.VCS = \' and"")\n        print(""\'versioneer.versionfile_source = \' . This configuration"")\n        print(""now lives in setup.cfg, and should be removed from setup.py"")\n        print("""")\n        errors += 1\n    return errors\n\n\nif __name__ == ""__main__"":\n    cmd = sys.argv[1]\n    if cmd == ""setup"":\n        errors = do_setup()\n        errors += scan_setup_py()\n        if errors:\n            sys.exit(1)\n'"
buildscripts/autogen_sources.py,0,"b'# -*- coding: utf-8 -*-\n# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\n\n| This script can be used to auto-generate SDC source files from common templates\n\n""""""\n\nimport sys\nimport inspect\nfrom pathlib import Path\n\nimport sdc.sdc_function_templates as templates_module\n\nencoding_info = \'# -*- coding: utf-8 -*-\'\n\ncopyright_header = \'\'\'\\\n# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\'\'\'\n\ndocstring_header = \'\'\'\\\n""""""\n\n| This file contains overloads for various extension types auto-generated with autogen_sources.py\n\n""""""\n\'\'\'\n\narithmetic_binops_symbols = {\n    \'add\': \'+\',\n    \'div\': \'/\',\n    \'sub\': \'-\',\n    \'mul\': \'*\',\n    \'truediv\': \'/\',\n    \'floordiv\': \'//\',\n    \'mod\': \'%\',\n    \'pow\': \'**\',\n}\n\ncomparison_binops_symbols = {\n    \'lt\': \'<\',\n    \'gt\': \'>\',\n    \'le\': \'<=\',\n    \'ge\': \'>=\',\n    \'ne\': \'!=\',\n    \'eq\': \'==\',\n}\n\ntarget_rel_filename = \'sdc/sdc_autogenerated.py\'\n\n\nif __name__ == \'__main__\':\n\n    sdc_root_path = Path(__file__).absolute().parents[1]\n    target_file_path = sdc_root_path.joinpath(target_rel_filename)\n\n    # read templates_module as text and extract import section to be copied into auto-generated target file\n    module_text = inspect.getsource(templates_module)\n    module_text_lines = module_text.splitlines(keepends=True)\n\n    # extract copyright text from templates file\n    copyright_line_numbers = [k for k, line in enumerate(module_text_lines) if \'# *****\' in line]\n    copyright_section_text = \'\'.join(module_text_lines[copyright_line_numbers[0]: copyright_line_numbers[1] + 1])\n\n    # extract copyright text from templates file - this only works if imports in it\n    # are placed contigiously, i.e. at one place and not intermixed with code\n    imports_line_numbers = [k for k, line in enumerate(module_text_lines) if \'import \' in line]\n    imports_start_line, import_end_line = min(imports_line_numbers), max(imports_line_numbers)\n    import_section_text = \'\'.join(module_text_lines[imports_start_line: import_end_line + 1])\n\n    # read function templates for arithmetic and comparison operators from templates module\n    template_series_binop = inspect.getsource(templates_module.sdc_pandas_series_binop)\n    template_series_comp_binop = inspect.getsource(templates_module.sdc_pandas_series_comp_binop)\n    template_series_operator = inspect.getsource(templates_module.sdc_pandas_series_operator_binop)\n    template_series_comp_operator = inspect.getsource(templates_module.sdc_pandas_series_operator_comp_binop)\n    template_str_arr_comp_binop = inspect.getsource(templates_module.sdc_str_arr_operator_comp_binop)\n\n    exit_status = -1\n    try:\n        # open the target file for writing and do the main work\n        with target_file_path.open(\'w\', newline=\'\') as file:\n            file.write(f\'{encoding_info}\\n\')\n            file.write(f\'{copyright_section_text}\\n\')\n            file.write(f\'{docstring_header}\\n\')\n            file.write(import_section_text)\n\n            # certaing modifications are needed to be applied for templates, so\n            # verify correctness of produced code manually\n            for name in arithmetic_binops_symbols:\n                func_text = template_series_binop.replace(\'binop\', name)\n                func_text = func_text.replace(\' + \', f\' {arithmetic_binops_symbols[name]} \')\n                func_text = func_text.replace(\'def \', f""@sdc_overload_method(SeriesType, \'{name}\')\\ndef "", 1)\n                file.write(f\'\\n\\n{func_text}\')\n\n            for name in comparison_binops_symbols:\n                func_text = template_series_comp_binop.replace(\'comp_binop\', name)\n                func_text = func_text.replace(\' < \', f\' {comparison_binops_symbols[name]} \')\n                func_text = func_text.replace(\'def \', f""@sdc_overload_method(SeriesType, \'{name}\')\\ndef "", 1)\n                file.write(f\'\\n\\n{func_text}\')\n\n            for name in arithmetic_binops_symbols:\n                if name != ""div"":\n                    func_text = template_series_operator.replace(\'binop\', name)\n                    func_text = func_text.replace(\' + \', f\' {arithmetic_binops_symbols[name]} \')\n                    func_text = func_text.replace(\'def \', f\'@sdc_overload(operator.{name})\\ndef \', 1)\n                    file.write(f\'\\n\\n{func_text}\')\n\n            for name in comparison_binops_symbols:\n                func_text = template_series_comp_operator.replace(\'comp_binop\', name)\n                func_text = func_text.replace(\' < \', f\' {comparison_binops_symbols[name]} \')\n                func_text = func_text.replace(\'def \', f\'@sdc_overload(operator.{name})\\ndef \', 1)\n                file.write(f\'\\n\\n{func_text}\')\n\n            for name in comparison_binops_symbols:\n                func_text = template_str_arr_comp_binop.replace(\'comp_binop\', name)\n                func_text = func_text.replace(\' < \', f\' {comparison_binops_symbols[name]} \')\n                if name == \'ne\':\n                    func_text = func_text.replace(\'and not\', \'or\')\n                func_text = func_text.replace(\'def \', f\'@sdc_overload(operator.{name})\\ndef \', 1)\n                file.write(f\'\\n\\n{func_text}\')\n\n    except Exception as e:\n        print(\'Exception of type {}: {} \\nwhile writing to a file: {}\\n\'.format(\n            type(e).__name__, e, target_file_path), file=sys.stderr)\n        exit_status = 1\n    else:\n        exit_status = 0\n\n    if not exit_status:\n        print(\'Auto-generation sctipt completed successfully\')\n    else:\n        print(\'Auto-generation failed, exit_status: {}\'.format(exit_status))\n    sys.exit(exit_status)\n'"
buildscripts/build.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\nimport argparse\nimport os\nimport platform\n\nfrom pathlib import Path\nfrom utilities import SDC_Build_Utilities\n\n\ndef build(sdc_utils):\n    os.chdir(str(sdc_utils.src_path))\n\n    sdc_utils.log_info(\'Start Intel SDC build\', separate=True)\n    conda_build_cmd = \' \'.join([\n        \'conda build\',\n        \'--no-test\',\n        f\'--python {sdc_utils.python}\',\n        f\'--numpy {sdc_utils.numpy}\',\n        f\'--output-folder {str(sdc_utils.output_folder)}\',\n        sdc_utils.channels,\n        \'--override-channels\',\n        str(sdc_utils.recipe)\n    ])\n    sdc_utils.run_command(conda_build_cmd)\n    sdc_utils.log_info(\'Intel SDC build SUCCESSFUL\', separate=True)\n\n    return\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--python\', required=True, choices=[\'3.6\', \'3.7\', \'3.8\'],\n                        help=\'Python version\')\n    parser.add_argument(\'--numpy\', required=True, choices=[\'1.16\', \'1.17\', \'1.18\'],\n                        help=\'Numpy version\')\n\n    args = parser.parse_args()\n\n    sdc_utils = SDC_Build_Utilities(args.python)\n    sdc_utils.numpy = args.numpy\n    sdc_utils.log_info(\'Build Intel(R) SDC conda and wheel packages\', separate=True)\n    sdc_utils.log_info(sdc_utils.line_double)\n\n    sdc_env_packages = [\'conda-build\']\n    if platform.system() == \'Windows\':\n        sdc_env_packages += [\'conda-verify\', \'vc\', \'vs2015_runtime\', \'vs2015_win-64\', \'pywin32=223\']\n    # Install conda-build and other packages from anaconda channel due to issue with wheel\n    # output build if use intel channels first\n    sdc_utils.create_environment(sdc_env_packages + [\'-c\', \'anaconda\'])\n\n    build(sdc_utils)\n'"
buildscripts/build_doc.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\nimport argparse\nimport os\nimport shutil\n\nfrom pathlib import Path\nfrom utilities import SDC_Build_Utilities\n\n\ndef build_doc(sdc_utils):\n    os.chdir(str(sdc_utils.doc_path))\n\n    # Set NUMBA_DISABLE_PERFORMANCE_WARNINGS to disable warnings in examples output\n    os.environ[\'NUMBA_DISABLE_PERFORMANCE_WARNINGS\'] = \'1\'\n\n    sdc_utils.log_info(\'Start documentation build\', separate=True)\n    sdc_utils.run_command(\'make html\')\n    sdc_utils.log_info(\'Documentation build SUCCESSFUL\', separate=True)\n\n    return\n\n\ndef publish_doc(sdc_utils):\n    doc_local_build = str(sdc_utils.doc_path / \'build\' / \'html\')\n    doc_repo_build = str(sdc_utils.doc_path / sdc_utils.doc_repo_name / sdc_utils.doc_tag)\n\n    git_email = os.environ[\'SDC_GIT_EMAIL\']\n    git_username = os.environ[\'SDC_GIT_USERNAME\']\n    git_access_token = os.environ[\'SDC_GIT_TOKEN\']\n    git_credentials_file = str(Path.home() / \'.git-credentials\')\n    git_credentials = f\'https://{git_access_token}:x-oauth-basic@github.com\\n\'\n\n    sdc_utils.log_info(f\'Start documentation publish to {sdc_utils.doc_repo_link}\', separate=True)\n\n    os.chdir(str(sdc_utils.doc_path))\n    sdc_utils.run_command(f\'git clone {sdc_utils.doc_repo_link}\')\n    os.chdir(str(sdc_utils.doc_repo_name))\n\n    # Set local git options\n    sdc_utils.run_command(\'git config --local credential.helper store\')\n    with open(git_credentials_file, ""w"") as fp:\n        fp.write(git_credentials)\n    sdc_utils.run_command(f\'git config --local user.email ""{git_email}""\')\n    sdc_utils.run_command(f\'git config --local user.name ""{git_username}""\')\n\n    sdc_utils.run_command(f\'git checkout {sdc_utils.doc_repo_branch}\')\n    shutil.rmtree(doc_repo_build)\n    shutil.copytree(doc_local_build, doc_repo_build)\n    sdc_utils.run_command(f\'git add -A {sdc_utils.doc_tag}\')\n    # Check if there is changes\n    output = sdc_utils.get_command_output(f\'git commit -m ""Updated doc release: {sdc_utils.doc_tag}""\')\n    if \'nothing to commit, working tree clean\' not in output:\n        sdc_utils.run_command(\'git push origin HEAD\')\n        sdc_utils.log_info(\'Documentation publish SUCCESSFUL\', separate=True)\n    else:\n        sdc_utils.log_info(\'No changes in documentation\', separate=True)\n\n    return\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--python\', default=\'3.7\', choices=[\'3.6\', \'3.7\', \'3.8\'],\n                        help=\'Python version, default = 3.7\')\n    parser.add_argument(\'--sdc-channel\', default=None, help=\'Intel SDC channel\')\n    parser.add_argument(\'--publish\', action=\'store_true\', help=\'Publish documentation to sdc-doc\')\n\n    args = parser.parse_args()\n\n    sdc_utils = SDC_Build_Utilities(args.python, args.sdc_channel)\n    sdc_utils.log_info(\'Build Intel(R) SDC documentation\', separate=True)\n    sdc_utils.log_info(sdc_utils.line_double)\n    sdc_utils.create_environment([\'sphinx\', \'sphinxcontrib-programoutput\'])\n    sdc_utils.install_conda_package([\'sdc\'])\n\n    build_doc(sdc_utils)\n    if args.publish:\n        publish_doc(sdc_utils)\n'"
buildscripts/run_benchmarks.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\nimport argparse\nimport os\nimport shutil\n\nfrom pathlib import Path\nfrom utilities import SDC_Build_Utilities\n\n\ndef run_benchmarks(sdc_utils, args_list, num_threads_list):\n    os.chdir(str(sdc_utils.src_path.parent))\n\n    for args_set in args_list:\n        for num_threads in num_threads_list:\n            os.environ[\'NUMBA_NUM_THREADS\'] = num_threads\n            sdc_utils.log_info(f\'Run Intel SDC benchmarks on {num_threads} threads\', separate=True)\n            sdc_utils.run_command(f\'python -W ignore -m sdc.runtests {args_set}\')\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--python\', default=\'3.7\', choices=[\'3.6\', \'3.7\', \'3.8\'],\n                        help=\'Python version, default = 3.7\')\n    parser.add_argument(\'--sdc-channel\', default=None, help=\'Intel SDC channel\')\n    parser.add_argument(\'--args-list\', required=True, nargs=\'+\', help=\'List of arguments sets for benchmarks\')\n    parser.add_argument(\'--num-threads-list\', required=True, nargs=\'+\',\n                        help=\'List of values for NUMBA_NUM_THREADS env variable\')\n\n    args = parser.parse_args()\n\n    sdc_utils = SDC_Build_Utilities(args.python, args.sdc_channel)\n    sdc_utils.log_info(\'Run Intel(R) SDC benchmarks\', separate=True)\n    sdc_utils.log_info(sdc_utils.line_double)\n    sdc_utils.create_environment([\'openpyxl\', \'xlrd\'])\n    sdc_utils.install_conda_package([\'sdc\'])\n\n    run_benchmarks(sdc_utils, args.args_list, args.num_threads_list)\n'"
buildscripts/run_examples.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\nimport argparse\nimport os\nimport shutil\nimport traceback\n\nfrom pathlib import Path\nfrom utilities import SDC_Build_Utilities\n\n\nEXAMPLES_TO_SKIP = {\'basic_usage_nyse_predict.py\'}\n\n\ndef run_examples(sdc_utils):\n    total = 0\n    passed = 0\n    failed = 0\n    skipped = 0\n    failed_examples = []\n\n    os.chdir(str(sdc_utils.examples_path))\n    for sdc_example in Path(\'.\').glob(\'**/*.py\'):\n        total += 1\n\n        if sdc_example.name in EXAMPLES_TO_SKIP:\n            skipped += 1\n            continue\n\n        sdc_example = str(sdc_example)\n        try:\n            sdc_utils.log_info(sdc_utils.line_double)\n            sdc_utils.run_command(f\'python {str(sdc_example)}\')\n        except Exception:\n            failed += 1\n            failed_examples.append(sdc_example)\n            sdc_utils.log_info(f\'{sdc_example} FAILED\')\n            traceback.print_exc()\n        else:\n            passed += 1\n            sdc_utils.log_info(f\'{sdc_example} PASSED\')\n\n    summary_msg = f\'SDC examples summary: {total} RUN, {passed} PASSED, {failed} FAILED, {skipped} SKIPPED\'\n    sdc_utils.log_info(summary_msg, separate=True)\n    for failed_example in failed_examples:\n        sdc_utils.log_info(f\'FAILED: {failed_example}\')\n\n    if failed > 0:\n        sdc_utils.log_info(\'Intel SDC examples FAILED\', separate=True)\n        exit(-1)\n    sdc_utils.log_info(\'Intel SDC examples PASSED\', separate=True)\n\n    return\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--python\', default=\'3.7\', choices=[\'3.6\', \'3.7\', \'3.8\'],\n                        help=\'Python version, default = 3.7\')\n    parser.add_argument(\'--sdc-channel\', default=None, help=\'Intel SDC channel\')\n\n    args = parser.parse_args()\n\n    sdc_utils = SDC_Build_Utilities(args.python, args.sdc_channel)\n    sdc_utils.log_info(\'Run Intel(R) SDC examples\', separate=True)\n    sdc_utils.log_info(sdc_utils.line_double)\n    sdc_utils.create_environment()\n    sdc_package = f\'sdc={sdc_utils.get_sdc_version_from_channel()}\'\n    sdc_utils.install_conda_package([sdc_package])\n\n    run_examples(sdc_utils)\n'"
buildscripts/test.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\nimport argparse\nimport os\n\nfrom pathlib import Path\nfrom utilities import SDC_Build_Utilities\n\n\ndef run_tests(sdc_utils):\n    os.chdir(str(sdc_utils.src_path))\n\n    sdc_utils.log_info(\'Run Intel SDC conda tests\', separate=True)\n    sdc_conda_package = str(list(sdc_utils.src_path.glob(\'**/sdc*.tar.bz2\'))[0])\n    conda_test_command = f\'conda build --test {sdc_utils.channels} {sdc_conda_package}\'\n    sdc_utils.run_command(conda_test_command)\n    sdc_utils.log_info(\'Intel SDC tests PASSED\', separate=True)\n\n    return\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--python\', default=\'3.7\', choices=[\'3.6\', \'3.7\', \'3.8\'],\n                        help=\'Python version, default = 3.7\')\n    parser.add_argument(\'--sdc-channel\', required=True, help=\'Local Intel SDC channel\')\n\n    args = parser.parse_args()\n\n    sdc_utils = SDC_Build_Utilities(args.python, args.sdc_channel)\n    sdc_utils.log_info(\'Run Intel(R) SDC tests\', separate=True)\n    sdc_utils.log_info(sdc_utils.line_double)\n    sdc_utils.create_environment([\'conda-build\'])\n\n    run_tests(sdc_utils)\n'"
buildscripts/utilities.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\n\nimport json\nimport os\nimport platform\nimport re\nimport subprocess\nimport time\nimport traceback\n\nfrom pathlib import Path\n\nfrom conda.cli.python_api import Commands as Conda_Commands\nfrom conda.cli.python_api import run_command as exec_conda_command\n\n\nclass SDC_Build_Utilities:\n    def __init__(self, python, sdc_local_channel=None):\n        self.src_path = Path(__file__).resolve().parent.parent\n        self.env_name = \'sdc_env\'\n        self.python = python\n        self.output_folder = self.src_path / \'sdc-build\'\n        self.recipe = self.src_path / \'conda-recipe\'\n\n        self.line_double = \'=\'*80\n        self.line_single = \'-\'*80\n\n        # Set channels\n        self.channel_list = [\'-c\', \'intel/label/beta\', \'-c\', \'defaults\', \'-c\', \'conda-forge\']\n        if sdc_local_channel:\n            sdc_local_channel = Path(sdc_local_channel).resolve().as_uri()\n            self.channel_list = [\'-c\', sdc_local_channel] + self.channel_list\n        self.channels = \' \'.join(self.channel_list)\n\n        # Conda activate command and conda croot (build) folder\n        if platform.system() == \'Windows\':\n            self.croot_folder = \'C:\\\\cb\'\n            self.env_activate = f\'activate {self.env_name}\'\n        else:\n            self.croot_folder = \'/cb\'\n            self.env_activate = f\'source activate {self.env_name}\'\n\n        # build_doc vars\n        self.doc_path = self.src_path / \'docs\'\n        self.doc_tag = \'dev\'\n        self.doc_repo_name = \'sdc-doc\'\n        self.doc_repo_link = \'https://github.com/IntelPython/sdc-doc.git\'\n        self.doc_repo_branch = \'gh-pages\'\n\n        # run_examples vars\n        self.examples_path = self.src_path / \'examples\'\n\n    def create_environment(self, packages_list=[]):\n        assert type(packages_list) == list, \'Argument should be a list\'\n\n        self.log_info(f\'Create {self.env_name} conda environment\')\n\n        # Clear Intel SDC environment\n        remove_args = [\'-q\', \'-y\', \'--name\', self.env_name, \'--all\']\n        self.__run_conda_command(Conda_Commands.REMOVE, remove_args)\n\n        # Create Intel SDC environment\n        create_args = [\'-q\', \'-y\', \'-n\', self.env_name, f\'python={self.python}\']\n        create_args += packages_list + self.channel_list + [\'--override-channels\']\n        self.log_info(self.__run_conda_command(Conda_Commands.CREATE, create_args))\n\n        return\n\n    def install_conda_package(self, packages_list):\n        assert type(packages_list) == list, \'Argument should be a list\'\n\n        self.log_info(f\'Install {"" "".join(packages_list)} to {self.env_name} conda environment\')\n        install_args = [\'-n\', self.env_name]\n        install_args += self.channel_list + [\'--override-channels\', \'-q\', \'-y\'] + packages_list\n        self.log_info(self.__run_conda_command(Conda_Commands.INSTALL, install_args))\n\n        return\n\n    def install_wheel_package(self, packages_list):\n        return\n\n    def __run_conda_command(self, conda_command, command_args):\n        self.log_info(f\'conda {conda_command} {"" "".join(command_args)}\')\n        output, errors, return_code = exec_conda_command(conda_command, *command_args, use_exception_handler=True)\n        if return_code != 0:\n            raise Exception(output + errors + f\'Return code: {str(return_code)}\')\n        return output\n\n    def run_command(self, command):\n        self.log_info(command)\n        self.log_info(self.line_single)\n        if platform.system() == \'Windows\':\n            subprocess.check_call(f\'{self.env_activate} && {command}\', stdout=None, stderr=None, shell=True)\n        else:\n            subprocess.check_call(f\'{self.env_activate} && {command}\', executable=\'/bin/bash\',\n                                  stdout=None, stderr=None, shell=True)\n\n    def get_command_output(self, command):\n        self.log_info(command)\n        self.log_info(self.line_single)\n        if platform.system() == \'Windows\':\n            output = subprocess.check_output(f\'{self.env_activate} && {command}\', universal_newlines=True, shell=True)\n        else:\n            output = subprocess.check_output(f\'{self.env_activate} && {command}\', executable=\'/bin/bash\',\n                                             universal_newlines=True, shell=True)\n        print(output, flush=True)\n        return output\n\n    def log_info(self, msg, separate=False):\n        if separate:\n            print(f\'{time.strftime(""%d/%m/%Y %H:%M:%S"")}: {self.line_double}\', flush=True)\n        print(f\'{time.strftime(""%d/%m/%Y %H:%M:%S"")}: {msg}\', flush=True)\n\n    def get_sdc_version_from_channel(self):\n        python_version = \'py\' + self.python.replace(\'.\', \'\')\n\n        # Get Intel SDC version from first channel in channel_list\n        search_args = [\'sdc\', \'-c\', self.channel_list[1], \'--override-channels\', \'--json\']\n        search_result = self.__run_conda_command(Conda_Commands.SEARCH, search_args)\n\n        repo_data = json.loads(search_result)\n        for package_data in repo_data[\'sdc\']:\n            sdc_version = package_data[\'version\']\n            sdc_build = package_data[\'build\']\n            if python_version in sdc_build:\n                break\n\n        return f\'{sdc_version}={sdc_build}\'\n'"
examples/basic_usage_nyse_predict.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport time\nimport pandas as pd\nfrom numba import njit\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split\n\n\n@njit(parallel=True)\ndef preprocess_data():\n    # Reading stock prices from CSV file\n    df_prices = pd.read_csv(\'./prices.csv\')  # Public dataset from https://www.kaggle.com/dgawlik/nyse\n\n    # Select stock of interest (INTC)\n    df_prices_intc = df_prices[df_prices[\'symbol\'] == \'INTC\']\n\n    # Remove unused columns\n    df_prices_intc = df_prices_intc.drop(columns=(\'symbol\', \'volume\'))\n\n    # The year of interest is 2012\n    df_prices_intc_2012 = df_prices_intc[df_prices_intc[\'date\'] <= \'2012-12-31\']\n    df_prices_intc_2012 = df_prices_intc_2012[df_prices_intc[\'date\'] >= \'2012-01-01\']\n\n    # Pearson correlation between open and close prices for 2012\n    corr_open_close_2012 = df_prices_intc_2012[\'open\'].corr(df_prices_intc_2012[\'close\'])\n\n    # Keep days when started low and finished high in 2012\n    df_prices_intc_2012_low2high = df_prices_intc_2012[df_prices_intc[\'open\'] <= df_prices_intc[\'low\']*1.005]\n    df_prices_intc_2012_low2high = df_prices_intc_2012_low2high[df_prices_intc[\'close\'] >= df_prices_intc[\'high\']*0.995]\n\n    # Prepare data for forecasting\n    x = df_prices_intc_2012[\'open\'].values.reshape(-1, 1)\n    y = df_prices_intc_2012[\'close\']\n\n    return df_prices_intc_2012, corr_open_close_2012, df_prices_intc_2012_low2high, x, y\n\n\nt_start = time.time()\ndata2012, coc12, data2012_low2high, x, y = preprocess_data()\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, shuffle=False)\nregr = GradientBoostingRegressor()\nmy_fit = regr.fit(x_train, y_train)\ny_pred = regr.predict(x_test)\nsc = regr.score(x_test, y_test)\nt_end = time.time()\n\nprint(\'2012\')\nprint(data2012.head())\n\nprint(\'Pearson correlation\')\nprint(coc12)\n\nprint(\'Days traded low to high\')\nprint(data2012_low2high)\n\nprint(y_pred)\nprint(sc)\n\nprint(\'Execution time: \', t_end - t_start)\n'"
examples/basic_workflow.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n# Dataset for analysis\nFNAME = ""employees.csv""\n\n\n# This function gets compiled by Numba*\n@njit\ndef get_analyzed_data():\n    df = pd.read_csv(FNAME)\n    s_bonus = pd.Series(df[\'Bonus %\'])\n    s_first_name = pd.Series(df[\'First Name\'])\n    m = s_bonus.mean()\n    names = s_first_name.sort_values()\n    return m, names\n\n\n# Printing names and their average bonus percent\nmean_bonus, sorted_first_names = get_analyzed_data()\nprint(sorted_first_names)\nprint(\'Average Bonus %:\', mean_bonus)\n'"
examples/basic_workflow_batch.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\nimport numpy as np\n\n\n# Datasets for analysis\nfile_names = [\n    ""employees_batch1.csv"",\n    ""employees_batch2.csv"",\n]\n\n\n# This function gets compiled by Numba*\n# For scalability use @njit(parallel=True)\n@njit\ndef get_analyzed_data(file_name):\n    df = pd.read_csv(file_name,\n                     dtype={\'Bonus %\': np.float64, \'First Name\': str},\n                     usecols=[\'Bonus %\', \'First Name\'])\n    s_bonus = pd.Series(df[\'Bonus %\'])\n    s_first_name = pd.Series(df[\'First Name\'])\n    m = s_bonus.mean()\n    names = s_first_name.sort_values()\n    return m, names\n\n\n# Printing names and their average bonus percent\nfor file_name in file_names:\n    mean_bonus, sorted_first_names = get_analyzed_data(file_name)\n    print(file_name)\n    print(sorted_first_names)\n    print(\'Average Bonus %:\', mean_bonus)\n'"
examples/basic_workflow_parallel.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit, prange\n\n# Dataset for analysis\nFNAME = ""employees.csv""\n\n\n# This function gets compiled by Numba* and multi-threaded\n@njit(parallel=True)\ndef get_analyzed_data():\n    df = pd.read_csv(FNAME)\n    s_bonus = pd.Series(df[\'Bonus %\'])\n    s_first_name = pd.Series(df[\'First Name\'])\n\n    # Use explicit loop to compute the mean. It will be compiled as parallel loop\n    m = 0.0\n    for i in prange(s_bonus.size):\n        m += s_bonus.values[i]\n    m /= s_bonus.size\n\n    names = s_first_name.sort_values()\n    return m, names\n\n\n# Printing names and their average bonus percent\nmean_bonus, sorted_first_names = get_analyzed_data()\nprint(sorted_first_names)\nprint(\'Average Bonus %:\', mean_bonus)\n'"
generate_data/gen_employees_csv.py,4,"b'import argparse\nimport pandas as pd\nfrom datetime import date, time, timedelta\nfrom random import randrange, uniform, random, choices\nimport numpy as np\n\n""""""\nThis file is used to generate file employees.csv used elsewhere in Pandas API examples for\nIntel(R) Scalable Dataframe Compiler.\n\nscript gen_employees_csv.py [-h] [--nrows N] [--fname FILE_NAME]\n""""""\n\n# Constants\nNROWS = 10  # Default number of rows in the generated CSV file\n\nHELP_STR_ROWS = \'Number of rows in the generated file (default:\'+str(NROWS)+\')\'\nHELP_STR_FNAME = \'File name of the generated file (default: employees.csv)\'\n\nMIN_YEAR = 1989\nMAX_YEAR = 2019\nMIN_SALARY = 37000\nMAX_SALARY = 150000\nMIN_BONUS = 1.0\nMAX_BONUS = 20.0\n\nMANAGEMENT_FREQ = 0.3\n\nTEAM_LIST = [\'Marketing\', \'Finance\', \'Client Services\', \'Legal\', \'Product\', \'Engineering\', \'Business Development\',\n             \'Human Resources\', \'Sales\', \'Distribution\']\nTEAM_WEIGHTS = [2, 1, 4, 1, 3, 6, 1, 1, 4, 3]\n\nMISSING_NAME_PROPORTION = 0.1\nMISSING_GENDER_PROPORTION = 0.1\nMISSING_SENIOR_MANAGEMENT_PROPORTION = 0.1\nMISSING_TEAM_PROPORTION = 0.1\n\n\ndef random_date():\n    """"""\n    Function generates random date for \'Start Date\' column\n    """"""\n    start = date(MIN_YEAR, 1, 1)\n    years = MAX_YEAR - MIN_YEAR + 1\n    end = start + timedelta(days=365 * years)\n    delta = end - start\n    int_delta = delta.days\n    random_days = randrange(int_delta)\n    return start + timedelta(days=random_days)\n\n\ndef random_time():\n    """"""\n    Function generates random time for \'Last Login Time\' column\n    """"""\n    return timedelta(seconds=randrange(60*60*24))\n\n\ndef random_management():\n    """"""\n    Function generates random \'true\' or \'false\' for \'Senior Management\' column\n    """"""\n    return \'true\' if random() < MANAGEMENT_FREQ else \'false\'\n\n\n# Argument parser\nparser = argparse.ArgumentParser(description=\'The employee.csv data generator\')\nparser.add_argument(\'--nrows\', default=NROWS, help=HELP_STR_ROWS, type=int)\nparser.add_argument(\'--fname\', default=\'employees.csv\', help=HELP_STR_FNAME, type=argparse.FileType(\'w\'))\n\nargs = parser.parse_args()\nnrows = args.nrows\nfname = args.fname\n\n# Read required dataset for names generation\n# The Top25BabyNames-By-Sex-2005-2017.csv is the public dataset from Open Government data.gov\n#\n# It has been downloaded from:\n# https://data.chhs.ca.gov/dataset/4a8cb74f-c4fa-458a-8ab1-5f2c0b2e22e3/resource/2bb8036b-8ce5-42e2-98e0-85ee2dca4093/\n# download/top25babynames-by-sex-2005-2017.csv\nnames_df = pd.read_csv(""data/top25babynames-by-sex-2005-2017.csv"")  # Reading names-gender dataset\nnames_df = names_df.rename(columns={\'Name\': \'First Name\'})  # Replacing column \'Name\' with \'First Name\'\ncounts = names_df[\'Count\']  # Will use \'Count"" column as weights for sampling\nnames_df = names_df.drop([\'YEAR\', \'RANK\', \'Count\'], axis=1)  # These columns are not used for employees.csv generation\nnames_df = names_df.append({\'Gender\': \'\', \'First Name\': \'Female\'}, ignore_index=True)\n\nemployees_df = names_df.sample(nrows, weights=counts, replace=True)  # Sampling names to create dataframe with nrows\nemployees_df[\'Start Date\'] = [random_date() for i in range(nrows)]  # Adding random start dates\nemployees_df[\'Last Login Time\'] = [random_time() for i in range(nrows)]  # Adding random login times\nemployees_df[\'Salary\'] = [randrange(MIN_SALARY, MAX_SALARY) for i in range(nrows)]  # Random salary\nemployees_df[\'Bonus %\'] = [uniform(MIN_BONUS, MAX_BONUS) for i in range(nrows)]  # Random bonus %\nemployees_df = employees_df.round({\'Bonus %\': 3})  # Rounding \'Bonus %\' column to 3 decimals\nemployees_df[\'Senior Management\'] = [random_management() for i in range(nrows)]  # True or False\nemployees_df[\'Team\'] = choices(TEAM_LIST, k=nrows, weights=TEAM_WEIGHTS)\nemployees_df.index = [i for i in range(nrows)]\n\n# Generate missing values\nemployees_df[\'First Name\'].values[np.random.choice(nrows, int(nrows*MISSING_NAME_PROPORTION))] = \'\'\nemployees_df[\'Gender\'].values[np.random.choice(nrows, int(nrows*MISSING_GENDER_PROPORTION))] = \'\'\nemployees_df[\'Team\'].values[np.random.choice(nrows, int(nrows*MISSING_TEAM_PROPORTION))] = \'\'\nemployees_df[\'Senior Management\'].values[np.random.choice(nrows, int(nrows*MISSING_SENIOR_MANAGEMENT_PROPORTION))] = \'\'\n\n# Writing dataframe to employee.csv file\nemployees_df.to_csv(fname)\n'"
generate_data/gen_kde_pq.py,2,"b'# *****************************************************************************\n# Copyright (c) 2017-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\nimport pyarrow.parquet as pq\nimport pyarrow as pa\nimport numpy as np\nimport pandas as pd\nimport argparse\nimport time\nimport sdc\n\n\ndef gen_kde(N, file_name):\n    # np.random.seed(0)\n    df = pd.DataFrame({\'points\': np.random.random(N)})\n    table = pa.Table.from_pandas(df)\n    row_group_size = 128\n    pq.write_table(table, \'kde.parquet\', row_group_size)\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'Gen KDE.\')\n    parser.add_argument(\'--size\', dest=\'size\', type=int, default=2000)\n    parser.add_argument(\'--file\', dest=\'file\', type=str, default=""kde.hdf5"")\n    args = parser.parse_args()\n    N = args.size\n    file_name = args.file\n\n    gen_kde(N, file_name)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
sdc/__init__.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numba\n\n# re-export from Numba\nfrom numba import (typeof, prange, pndindex, gdb, gdb_breakpoint, gdb_init,\n                   stencil, threading_layer, jitclass, objmode)\n\nimport sdc.config\nimport sdc.set_ext\nimport sdc.io\nimport sdc.io.np_io\nimport sdc.hiframes.pd_timestamp_ext\nimport sdc.hiframes.boxing\nimport sdc.timsort\nfrom sdc.decorators import jit\n\nimport sdc.datatypes.hpat_pandas_dataframe_rolling_functions\nimport sdc.datatypes.hpat_pandas_series_functions\nimport sdc.datatypes.hpat_pandas_series_rolling_functions\nimport sdc.datatypes.hpat_pandas_stringmethods_functions\nimport sdc.datatypes.hpat_pandas_groupby_functions\nimport sdc.datatypes.categorical.init\nimport sdc.datatypes.series.init\n\nimport sdc.extensions.indexes.range_index_ext\n\nfrom ._version import get_versions\n\nif not sdc.config.config_pipeline_hpat_default:\n    """"""\n    Overload Numba function to allow call SDC pass in Numba compiler pipeline\n    Functions are:\n    - Numba DefaultPassBuilder define_nopython_pipeline()\n\n    TODO: Needs to detect \'import Pandas\' and align initialization according to it\n    """"""\n\n    # sdc.config.numba_compiler_define_nopython_pipeline_orig = \\\n    # numba.core.compiler.DefaultPassBuilder.define_nopython_pipeline\n    # numba.core.compiler.DefaultPassBuilder.define_nopython_pipeline = \\\n    # sdc.datatypes.hpat_pandas_dataframe_pass.sdc_nopython_pipeline_lite_register\n\n    import sdc.rewrites.dataframe_constructor\n    import sdc.rewrites.read_csv_consts\n    import sdc.rewrites.dataframe_getitem_attribute\n    import sdc.datatypes.hpat_pandas_functions\n    import sdc.datatypes.hpat_pandas_dataframe_functions\nelse:\n    import sdc.compiler\n\nmultithread_mode = False\n\n\n__version__ = get_versions()[\'version\']\ndel get_versions\n\ndef _init_extension():\n    \'\'\'Register Pandas classes and functions with Numba.\n\n    This exntry_point is called by Numba when it initializes.\n    \'\'\'\n    # Importing SDC is already happened\n    pass\n'"
sdc/_version.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\n\n# This file helps to compute a version number in source trees obtained from\n# git-archive tarball (such as those provided by githubs download-from-tag\n# feature). Distribution tarballs (built by setup.py sdist) and build\n# directories (produced by setup.py build) will contain a much shorter file\n# that just contains the computed version number.\n\n# This file is released into the public domain. Generated by\n# versioneer-0.18 (https://github.com/warner/python-versioneer)\n\n""""""Git implementation of _version.py.""""""\n\nimport errno\nimport os\nimport re\nimport subprocess\nimport sys\n\n\ndef get_keywords():\n    """"""Get the keywords needed to look up the version information.""""""\n    # these strings will be replaced by git during git-archive.\n    # setup.py/versioneer.py will grep for the variable names, so they must\n    # each be defined on a line of their own. _version.py will just call\n    # get_keywords().\n    git_refnames = ""$Format:%d$""\n    git_full = ""$Format:%H$""\n    git_date = ""$Format:%ci$""\n    keywords = {""refnames"": git_refnames, ""full"": git_full, ""date"": git_date}\n    return keywords\n\n\nclass VersioneerConfig:\n    """"""Container for Versioneer configuration parameters.""""""\n\n\ndef get_config():\n    """"""Create, populate and return the VersioneerConfig() object.""""""\n    # these strings are filled in when \'setup.py versioneer\' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = ""git""\n    cfg.style = ""pep440""\n    cfg.tag_prefix = """"\n    cfg.parentdir_prefix = ""sdc-""\n    cfg.versionfile_source = ""sdc/_version.py""\n    cfg.verbose = False\n    return cfg\n\n\nclass NotThisMethod(Exception):\n    """"""Exception raised if a method is not valid for the current scenario.""""""\n\n\nLONG_VERSION_PY = {}\nHANDLERS = {}\n\n\ndef register_vcs_handler(vcs, method):  # decorator\n    """"""Decorator to mark a method as the handler for a particular VCS.""""""\n    def decorate(f):\n        """"""Store f in HANDLERS[vcs][method].""""""\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate\n\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False,\n                env=None):\n    """"""Call the given command(s).""""""\n    assert isinstance(commands, list)\n    p = None\n    for c in commands:\n        try:\n            dispcmd = str([c] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            p = subprocess.Popen([c] + args, cwd=cwd, env=env,\n                                 stdout=subprocess.PIPE,\n                                 stderr=(subprocess.PIPE if hide_stderr\n                                         else None))\n            break\n        except EnvironmentError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(""unable to run %s"" % dispcmd)\n                print(e)\n            return None, None\n    else:\n        if verbose:\n            print(""unable to find command, tried %s"" % (commands,))\n        return None, None\n    stdout = p.communicate()[0].strip()\n    if sys.version_info[0] >= 3:\n        stdout = stdout.decode()\n    if p.returncode != 0:\n        if verbose:\n            print(""unable to run %s (error)"" % dispcmd)\n            print(""stdout was %s"" % stdout)\n        return None, p.returncode\n    return stdout, p.returncode\n\n\ndef versions_from_parentdir(parentdir_prefix, root, verbose):\n    """"""Try to determine the version from the parent directory name.\n\n    Source tarballs conventionally unpack into a directory that includes both\n    the project name and a version string. We will also support searching up\n    two directory levels for an appropriately named parent directory\n    """"""\n    rootdirs = []\n\n    for i in range(3):\n        dirname = os.path.basename(root)\n        if dirname.startswith(parentdir_prefix):\n            return {""version"": dirname[len(parentdir_prefix):],\n                    ""full-revisionid"": None,\n                    ""dirty"": False, ""error"": None, ""date"": None}\n        else:\n            rootdirs.append(root)\n            root = os.path.dirname(root)  # up a level\n\n    if verbose:\n        print(""Tried directories %s but none started with prefix %s"" %\n              (str(rootdirs), parentdir_prefix))\n    raise NotThisMethod(""rootdir doesn\'t start with parentdir_prefix"")\n\n\n@register_vcs_handler(""git"", ""get_keywords"")\ndef git_get_keywords(versionfile_abs):\n    """"""Extract version information from the given file.""""""\n    # the code embedded in _version.py can just fetch the value of these\n    # keywords. When used from setup.py, we don\'t want to import _version.py,\n    # so we do it with a regexp instead. This function is not used from\n    # _version.py.\n    keywords = {}\n    # Don\'t open symlinks for security reasons.\n    if os.path.islink(versionfile_abs):\n        return keywords\n    try:\n        f = open(versionfile_abs, ""r"")\n        for line in f.readlines():\n            if line.strip().startswith(""git_refnames =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""refnames""] = mo.group(1)\n            if line.strip().startswith(""git_full =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""full""] = mo.group(1)\n            if line.strip().startswith(""git_date =""):\n                mo = re.search(r\'=\\s*""(.*)""\', line)\n                if mo:\n                    keywords[""date""] = mo.group(1)\n        f.close()\n    except EnvironmentError:\n        pass\n    return keywords\n\n\n@register_vcs_handler(""git"", ""keywords"")\ndef git_versions_from_keywords(keywords, tag_prefix, verbose):\n    """"""Get version information from git keywords.""""""\n    if not keywords:\n        raise NotThisMethod(""no keywords at all, weird"")\n    date = keywords.get(""date"")\n    if date is not None:\n        # git-2.2.0 added ""%cI"", which expands to an ISO-8601 -compliant\n        # datestamp. However we prefer ""%ci"" (which expands to an ""ISO-8601\n        # -like"" string, which we must then edit to make compliant), because\n        # it\'s been around since git-1.5.3, and it\'s too difficult to\n        # discover which version we\'re using, or to work around using an\n        # older one.\n        date = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n    refnames = keywords[""refnames""].strip()\n    if refnames.startswith(""$Format""):\n        if verbose:\n            print(""keywords are unexpanded, not using"")\n        raise NotThisMethod(""unexpanded keywords, not a git-archive tarball"")\n    refs = set([r.strip() for r in refnames.strip(""()"").split("","")])\n    # starting in git-1.8.3, tags are listed as ""tag: foo-1.0"" instead of\n    # just ""foo-1.0"". If we see a ""tag: "" prefix, prefer those.\n    TAG = ""tag: ""\n    tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])\n    if not tags:\n        # Either we\'re using git < 1.8.3, or there really are no tags. We use\n        # a heuristic: assume all version tags have a digit. The old git %d\n        # expansion behaves like git log --decorate=short and strips out the\n        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n        # between branches and tags. By ignoring refnames without digits, we\n        # filter out many common branch names like ""release"" and\n        # ""stabilization"", as well as ""HEAD"" and ""master"".\n        tags = set([r for r in refs if re.search(r\'\\d\', r)])\n        if verbose:\n            print(""discarding \'%s\', no digits"" % "","".join(refs - tags))\n    if verbose:\n        print(""likely tags: %s"" % "","".join(sorted(tags)))\n    for ref in sorted(tags):\n        # sorting will prefer e.g. ""2.0"" over ""2.0rc1""\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix):]\n            if verbose:\n                print(""picking %s"" % r)\n            return {""version"": r,\n                    ""full-revisionid"": keywords[""full""].strip(),\n                    ""dirty"": False, ""error"": None,\n                    ""date"": date}\n    # no suitable tags, so version is ""0+unknown"", but full hex is still there\n    if verbose:\n        print(""no suitable tags, using unknown + full revision id"")\n    return {""version"": ""0+unknown"",\n            ""full-revisionid"": keywords[""full""].strip(),\n            ""dirty"": False, ""error"": ""no suitable tags"", ""date"": None}\n\n\n@register_vcs_handler(""git"", ""pieces_from_vcs"")\ndef git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n    """"""Get version from \'git describe\' in the root of the source tree.\n\n    This only gets called if the git-archive \'subst\' keywords were *not*\n    expanded, and _version.py hasn\'t already been rewritten with a short\n    version string, meaning we\'re inside a checked out source tree.\n    """"""\n    GITS = [""git""]\n    if sys.platform == ""win32"":\n        GITS = [""git.cmd"", ""git.exe""]\n\n    out, rc = run_command(GITS, [""rev-parse"", ""--git-dir""], cwd=root,\n                          hide_stderr=True)\n    if rc != 0:\n        if verbose:\n            print(""Directory %s not under git control"" % root)\n        raise NotThisMethod(""\'git rev-parse --git-dir\' returned error"")\n\n    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n    # if there isn\'t one, this yields HEX[-dirty] (no NUM)\n    describe_out, rc = run_command(GITS, [""describe"", ""--tags"", ""--dirty"",\n                                          ""--always"", ""--long"",\n                                          ""--match"", ""%s*"" % tag_prefix],\n                                   cwd=root)\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(""\'git describe\' failed"")\n    describe_out = describe_out.strip()\n    full_out, rc = run_command(GITS, [""rev-parse"", ""HEAD""], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(""\'git rev-parse\' failed"")\n    full_out = full_out.strip()\n\n    pieces = {}\n    pieces[""long""] = full_out\n    pieces[""short""] = full_out[:7]  # maybe improved later\n    pieces[""error""] = None\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(""-dirty"")\n    pieces[""dirty""] = dirty\n    if dirty:\n        git_describe = git_describe[:git_describe.rindex(""-dirty"")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if ""-"" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r\'^(.+)-(\\d+)-g([0-9a-f]+)$\', git_describe)\n        if not mo:\n            # unparseable. Maybe git-describe is misbehaving?\n            pieces[""error""] = (""unable to parse git-describe output: \'%s\'""\n                               % describe_out)\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = ""tag \'%s\' doesn\'t start with prefix \'%s\'""\n                print(fmt % (full_tag, tag_prefix))\n            pieces[""error""] = (""tag \'%s\' doesn\'t start with prefix \'%s\'""\n                               % (full_tag, tag_prefix))\n            return pieces\n        pieces[""closest-tag""] = full_tag[len(tag_prefix):]\n\n        # distance: number of commits since tag\n        pieces[""distance""] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[""short""] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[""closest-tag""] = None\n        count_out, rc = run_command(GITS, [""rev-list"", ""HEAD"", ""--count""],\n                                    cwd=root)\n        pieces[""distance""] = int(count_out)  # total number of commits\n\n    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n    date = run_command(GITS, [""show"", ""-s"", ""--format=%ci"", ""HEAD""],\n                       cwd=root)[0].strip()\n    pieces[""date""] = date.strip().replace("" "", ""T"", 1).replace("" "", """", 1)\n\n    return pieces\n\n\ndef plus_or_dot(pieces):\n    """"""Return a + if we don\'t already have one, else return a .""""""\n    if ""+"" in pieces.get(""closest-tag"", """"):\n        return "".""\n    return ""+""\n\n\ndef render_pep440(pieces):\n    """"""Build up version string, with post-release ""local version identifier"".\n\n    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    get a tagged build and then dirty it, you\'ll get TAG+0.gHEX.dirty\n\n    Exceptions:\n    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += plus_or_dot(pieces)\n            rendered += ""%d.g%s"" % (pieces[""distance""], pieces[""short""])\n            if pieces[""dirty""]:\n                rendered += "".dirty""\n    else:\n        # exception #1\n        rendered = ""0+untagged.%d.g%s"" % (pieces[""distance""],\n                                          pieces[""short""])\n        if pieces[""dirty""]:\n            rendered += "".dirty""\n    return rendered\n\n\ndef render_pep440_pre(pieces):\n    """"""TAG[.post.devDISTANCE] -- No -dirty.\n\n    Exceptions:\n    1: no tags. 0.post.devDISTANCE\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += "".post.dev%d"" % pieces[""distance""]\n    else:\n        # exception #1\n        rendered = ""0.post.dev%d"" % pieces[""distance""]\n    return rendered\n\n\ndef render_pep440_post(pieces):\n    """"""TAG[.postDISTANCE[.dev0]+gHEX] .\n\n    The "".dev0"" means dirty. Note that .dev0 sorts backwards\n    (a dirty tree will appear ""older"" than the corresponding clean one),\n    but you shouldn\'t be releasing software with -dirty anyways.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%d"" % pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n            rendered += plus_or_dot(pieces)\n            rendered += ""g%s"" % pieces[""short""]\n    else:\n        # exception #1\n        rendered = ""0.post%d"" % pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n        rendered += ""+g%s"" % pieces[""short""]\n    return rendered\n\n\ndef render_pep440_old(pieces):\n    """"""TAG[.postDISTANCE[.dev0]] .\n\n    The "".dev0"" means dirty.\n\n    Eexceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""] or pieces[""dirty""]:\n            rendered += "".post%d"" % pieces[""distance""]\n            if pieces[""dirty""]:\n                rendered += "".dev0""\n    else:\n        # exception #1\n        rendered = ""0.post%d"" % pieces[""distance""]\n        if pieces[""dirty""]:\n            rendered += "".dev0""\n    return rendered\n\n\ndef render_git_describe(pieces):\n    """"""TAG[-DISTANCE-gHEX][-dirty].\n\n    Like \'git describe --tags --dirty --always\'.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        if pieces[""distance""]:\n            rendered += ""-%d-g%s"" % (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render_git_describe_long(pieces):\n    """"""TAG-DISTANCE-gHEX[-dirty].\n\n    Like \'git describe --tags --dirty --always -long\'.\n    The distance/hash is unconditional.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no \'g\' prefix)\n    """"""\n    if pieces[""closest-tag""]:\n        rendered = pieces[""closest-tag""]\n        rendered += ""-%d-g%s"" % (pieces[""distance""], pieces[""short""])\n    else:\n        # exception #1\n        rendered = pieces[""short""]\n    if pieces[""dirty""]:\n        rendered += ""-dirty""\n    return rendered\n\n\ndef render(pieces, style):\n    """"""Render the given version pieces into the requested style.""""""\n    if pieces[""error""]:\n        return {""version"": ""unknown"",\n                ""full-revisionid"": pieces.get(""long""),\n                ""dirty"": None,\n                ""error"": pieces[""error""],\n                ""date"": None}\n\n    if not style or style == ""default"":\n        style = ""pep440""  # the default\n\n    if style == ""pep440"":\n        rendered = render_pep440(pieces)\n    elif style == ""pep440-pre"":\n        rendered = render_pep440_pre(pieces)\n    elif style == ""pep440-post"":\n        rendered = render_pep440_post(pieces)\n    elif style == ""pep440-old"":\n        rendered = render_pep440_old(pieces)\n    elif style == ""git-describe"":\n        rendered = render_git_describe(pieces)\n    elif style == ""git-describe-long"":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(""unknown style \'%s\'"" % style)\n\n    return {""version"": rendered, ""full-revisionid"": pieces[""long""],\n            ""dirty"": pieces[""dirty""], ""error"": None,\n            ""date"": pieces.get(""date"")}\n\n\ndef get_versions():\n    """"""Get version information or return default if unable to do so.""""""\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don\'t do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n                                          verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the .git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for i in cfg.versionfile_source.split(\'/\'):\n            root = os.path.dirname(root)\n    except NameError:\n        return {""version"": ""0+unknown"", ""full-revisionid"": None,\n                ""dirty"": None,\n                ""error"": ""unable to find root of source tree"",\n                ""date"": None}\n\n    try:\n        pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)\n        return render(pieces, cfg.style)\n    except NotThisMethod:\n        pass\n\n    try:\n        if cfg.parentdir_prefix:\n            return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n    except NotThisMethod:\n        pass\n\n    return {""version"": ""0+unknown"", ""full-revisionid"": None,\n            ""dirty"": None,\n            ""error"": ""unable to compute version"", ""date"": None}\n'"
sdc/compiler.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\nfrom llvmlite import binding\nimport sdc\nimport sdc.hiframes\nimport sdc.hiframes.hiframes_untyped\nimport sdc.hiframes.hiframes_typed\nfrom sdc.hiframes.hiframes_untyped import HiFramesPass\nfrom sdc.hiframes.hiframes_typed import HiFramesTypedPass\nfrom sdc.hiframes.dataframe_pass import DataFramePass\nimport numba\nimport numba.core.compiler\nfrom numba.core.compiler import DefaultPassBuilder\nfrom numba import ir_utils, ir, postproc\nfrom numba.core.registry import CPUDispatcher\nfrom numba.core.ir_utils import guard, get_definition\nfrom numba.core.inline_closurecall import inline_closure_call\nfrom numba.typed_passes import (NopythonTypeInference, AnnotateTypes, ParforPass, IRLegalization)\nfrom numba.untyped_passes import (DeadBranchPrune, InlineInlinables, InlineClosureLikes)\nfrom sdc import config\nfrom sdc.distributed import DistributedPass\n\nfrom numba.core.compiler_machinery import FunctionPass, register_pass\n\n# workaround for Numba #3876 issue with large labels in mortgage benchmark\nbinding.set_option(""tmp"", ""-non-global-value-max-name-size=2048"")\n\n\ndef inline_calls(func_ir, _locals):\n    work_list = list(func_ir.blocks.items())\n    while work_list:\n        label, block = work_list.pop()\n        for i, instr in enumerate(block.body):\n            if isinstance(instr, ir.Assign):\n                lhs = instr.target\n                expr = instr.value\n                if isinstance(expr, ir.Expr) and expr.op == \'call\':\n                    func_def = guard(get_definition, func_ir, expr.func)\n                    if (isinstance(func_def, (ir.Global, ir.FreeVar))\n                            and isinstance(func_def.value, CPUDispatcher)):\n                        py_func = func_def.value.py_func\n                        inline_out = inline_closure_call(\n                            func_ir, py_func.__globals__, block, i, py_func,\n                            work_list=work_list)\n\n                        # TODO remove if when inline_closure_call() output fix\n                        # is merged in Numba\n                        if isinstance(inline_out, tuple):\n                            var_dict = inline_out[1]\n                            # TODO: update \'##distributed\' and \'##threaded\' in _locals\n                            _locals.update((var_dict[k].name, v)\n                                           for k, v in func_def.value.locals.items()\n                                           if k in var_dict)\n                        # for block in new_blocks:\n                        #     work_list.append(block)\n                        # current block is modified, skip the rest\n                        # (included in new blocks)\n                        break\n\n    # sometimes type inference fails after inlining since blocks are inserted\n    # at the end and there are agg constraints (categorical_split case)\n    # CFG simplification fixes this case\n    func_ir.blocks = ir_utils.simplify_CFG(func_ir.blocks)\n\n# TODO: remove these helper functions when Numba provide appropriate way to manipulate passes\ndef pass_position(pm, location):\n    assert pm.passes\n    pm._validate_pass(location)\n    for idx, (x, _) in enumerate(pm.passes):\n        if x == location:\n            return idx\n\n    raise ValueError(""Could not find pass %s"" % location)\n\n\ndef add_pass_before(pm, pass_cls, location):\n    assert pm.passes\n    pm._validate_pass(pass_cls)\n    position = pass_position(pm, location)\n    pm.passes.insert(position, (pass_cls, str(pass_cls)))\n\n    # if a pass has been added, it\'s not finalized\n    pm._finalized = False\n\n\ndef replace_pass(pm, pass_cls, location):\n    assert pm.passes\n    pm._validate_pass(pass_cls)\n    position = pass_position(pm, location)\n    pm[position] = (pass_cls, str(pass_cls))\n\n    # if a pass has been added, it\'s not finalized\n    pm._finalized = False\n\n\n@register_pass(mutates_CFG=True, analysis_only=False)\nclass InlinePass(FunctionPass):\n    _name = ""sdc_extention_inline_pass""\n\n    def __init__(self):\n        pass\n\n    def run_pass(self, state):\n        inline_calls(state.func_ir, state.locals)\n        return True\n\n\n@register_pass(mutates_CFG=True, analysis_only=False)\nclass PostprocessorPass(FunctionPass):\n    _name = ""sdc_extention_postprocessor_pass""\n\n    def __init__(self):\n        pass\n\n    def run_pass(self, state):\n        post_proc = postproc.PostProcessor(state.func_ir)\n        post_proc.run()\n        return True\n\n\nclass SDCPipeline(numba.core.compiler.CompilerBase):\n    """"""SDC compiler pipeline\n    """"""\n\n    def define_pipelines(self):\n        name = \'sdc_extention_pipeline_distributed\'\n        pm = DefaultPassBuilder.define_nopython_pipeline(self.state)\n\n        add_pass_before(pm, InlinePass, InlineClosureLikes)\n        pm.add_pass_after(HiFramesPass, InlinePass)\n        pm.add_pass_after(DataFramePass, AnnotateTypes)\n        pm.add_pass_after(PostprocessorPass, AnnotateTypes)\n        pm.add_pass_after(HiFramesTypedPass, DataFramePass)\n        pm.add_pass_after(DistributedPass, ParforPass)\n        pm.finalize()\n\n        return [pm]\n\n\n@register_pass(mutates_CFG=True, analysis_only=False)\nclass ParforSeqPass(FunctionPass):\n    _name = ""sdc_extention_parfor_seq_pass""\n\n    def __init__(self):\n        pass\n\n    def run_pass(self, state):\n        numba.parfors.parfor.lower_parfor_sequential(\n            state.typingctx, state.func_ir, state.typemap, state.calltypes)\n\n        return True\n\n\nclass SDCPipelineSeq(SDCPipeline):\n    """"""SDC pipeline without the distributed pass (used in rolling kernels)\n    """"""\n\n    def define_pipelines(self):\n        name = \'sdc_extention_pipeline_seq\'\n        pm = DefaultPassBuilder.define_nopython_pipeline(self.state)\n\n        add_pass_before(pm, InlinePass, InlineClosureLikes)\n        pm.add_pass_after(HiFramesPass, InlinePass)\n        pm.add_pass_after(DataFramePass, AnnotateTypes)\n        pm.add_pass_after(PostprocessorPass, AnnotateTypes)\n        pm.add_pass_after(HiFramesTypedPass, DataFramePass)\n        add_pass_before(pm, ParforSeqPass, IRLegalization)\n        pm.finalize()\n\n        return [pm]\n'"
sdc/config.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\'\'\'\nThis is a set of configuration variables in SDC initialized at startup\n\'\'\'\n\n\nimport os\nfrom distutils import util as distutils_util\n\ntry:\n    import pyarrow\nexcept ImportError:\n    _has_pyarrow = False\nelse:\n    _has_pyarrow = True\n\n\ndef strtobool(val):\n    \'\'\'Convert string to True or False\'\'\'\n    return bool(distutils_util.strtobool(val))\n\n\nconfig_transport_mpi_default = strtobool(os.getenv(\'SDC_CONFIG_MPI\', \'True\'))\n\'\'\'\nDefault value for transport used if no function decorator controls the transport\n\'\'\'\n\nconfig_transport_mpi = config_transport_mpi_default\n\'\'\'\nCurrent value for transport controlled by decorator need to initialize this here\nbecause decorator called later then modules have been initialized\n\'\'\'\n\nconfig_pipeline_hpat_default = strtobool(os.getenv(\'SDC_CONFIG_PIPELINE_SDC\', \'False\'))\n\'\'\'\nDefault value used to select compiler pipeline in a function decorator\n\'\'\'\n\nconfig_use_parallel_overloads = strtobool(os.getenv(\'SDC_AUTO_PARALLEL\', \'True\'))\n\'\'\'\nDefault value used to select whether auto parallel would be applied to sdc functions\n\'\'\'\n\nconfig_inline_overloads = strtobool(os.getenv(\'SDC_AUTO_INLINE\', \'False\'))\n\'\'\'\nDefault value used to select whether sdc functions would inline\n\'\'\'\n\nif not config_pipeline_hpat_default:\n    # avoid using MPI transport if no SDC compiler pipeline used\n    config_transport_mpi_default = False\n    config_transport_mpi = config_transport_mpi_default\n\nnumba_compiler_define_nopython_pipeline_orig = None\n\'\'\'\nDefault value for a pointer intended to use as Numba.DefaultPassBuilder.define_nopython_pipeline() in overloaded function\n\'\'\'\n\ntest_expected_failure = strtobool(os.getenv(\'SDC_TEST_EXPECTED_FAILURE\', \'False\'))\n\'\'\'\nIf True then replaces skip decorators to expectedFailure decorator.\n\'\'\'\n'"
sdc/cv_ext.py,5,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\nimport numba\nimport sdc\nfrom numba import types\nfrom numba.core.typing.templates import infer_global, AbstractTemplate, infer, signature\nfrom numba.extending import lower_builtin, overload, intrinsic\nfrom numba.core import cgutils\nfrom sdc.str_ext import string_type\nfrom numba.core.imputils import impl_ret_new_ref, impl_ret_borrowed\nfrom numba.np.arrayobj import _empty_nd_impl\n\nimport cv2\nimport numpy as np\n\nfrom llvmlite import ir as lir\nimport llvmlite.binding as ll\nfrom . import cv_wrapper\nll.add_symbol(\'cv_imread\', cv_wrapper.cv_imread)\nll.add_symbol(\'cv_resize\', cv_wrapper.cv_resize)\nll.add_symbol(\'cv_imdecode\', cv_wrapper.cv_imdecode)\nll.add_symbol(\'cv_mat_release\', cv_wrapper.cv_mat_release)\nll.add_symbol(\'cv_delete_buf\', cv_wrapper.cv_delete_buf)\n\n\n@infer_global(cv2.imread, typing_key=\'cv2imread\')\nclass ImreadInfer(AbstractTemplate):\n    def generic(self, args, kws):\n        if not kws and len(args) == 1 and args[0] == string_type:\n            return signature(types.Array(types.uint8, 3, \'C\'), *args)\n\n# @infer_global(cv2.resize, typing_key=\'cv2resize\')\n# class ImreadInfer(AbstractTemplate):\n#     def generic(self, args, kws):\n#         if not kws and len(args) == 2 and args[0] == types.Array(types.uint8, 3, \'C\'):\n#             return signature(types.Array(types.uint8, 3, \'C\'), *args)\n\n\n@lower_builtin(\'cv2imread\', string_type)\ndef lower_cv2_imread(context, builder, sig, args):\n    fname = args[0]\n    arrtype = sig.return_type\n\n    # read shapes and data pointer\n    ll_shty = lir.ArrayType(cgutils.intp_t, arrtype.ndim)\n    shapes_array = cgutils.alloca_once(builder, ll_shty)\n    data = cgutils.alloca_once(builder, lir.IntType(8).as_pointer())\n\n    fnty = lir.FunctionType(lir.IntType(8).as_pointer(),\n                            [ll_shty.as_pointer(),\n                             lir.IntType(8).as_pointer().as_pointer(),\n                             lir.IntType(8).as_pointer()])\n    fn_imread = builder.module.get_or_insert_function(fnty, name=""cv_imread"")\n    img = builder.call(fn_imread, [shapes_array, data, fname])\n\n    return _image_to_array(context, builder, shapes_array, arrtype, data, img)\n\n\n# @lower_builtin(\'cv2resize\', types.Array, types.UniTuple)\n# def lower_cv2_resize(context, builder, sig, args):\n#     #\n#     in_arrtype = sig.args[0]\n#     in_array = context.make_array(in_arrtype)(context, builder, args[0])\n#     in_shapes = cgutils.unpack_tuple(builder, in_array.shape)\n#     # cgutils.printf(builder, ""%lld\\n"", in_shapes[2])\n#\n#     new_sizes = cgutils.unpack_tuple(builder, args[1])\n#\n#     ary = _empty_nd_impl(context, builder, in_arrtype, [new_sizes[1], new_sizes[0], in_shapes[2]])\n#\n#     fnty = lir.FunctionType(lir.VoidType(),\n#                             [lir.IntType(64), lir.IntType(64),\n#                              lir.IntType(8).as_pointer(),\n#                              lir.IntType(8).as_pointer(),\n#                              lir.IntType(64),\n#                              lir.IntType(64)])\n#     fn_resize = builder.module.get_or_insert_function(fnty, name=""cv_resize"")\n#     img = builder.call(fn_resize, [new_sizes[1], new_sizes[0], ary.data, in_array.data,\n#                                     in_shapes[0], in_shapes[1]])\n#\n#     return impl_ret_new_ref(context, builder, in_arrtype, ary._getvalue())\n\n\ndef _image_to_array(context, builder, shapes_array, arrtype, data, img):\n    # allocate array\n    shapes = cgutils.unpack_tuple(builder, builder.load(shapes_array))\n    ary = _empty_nd_impl(context, builder, arrtype, shapes)\n    cgutils.raw_memcpy(builder, ary.data, builder.load(data), ary.nitems,\n                       ary.itemsize, align=1)\n\n    # clean up cv::Mat image\n    fnty = lir.FunctionType(lir.VoidType(), [lir.IntType(8).as_pointer()])\n    fn_release = builder.module.get_or_insert_function(fnty, name=""cv_mat_release"")\n    builder.call(fn_release, [img])\n\n    return impl_ret_new_ref(context, builder, arrtype, ary._getvalue())\n\n\n@overload(cv2.resize)\ndef resize_overload(A_t, resize_shape_t):\n    n_channels = 3\n    if not (isinstance(A_t, types.Array)\n            and A_t.ndim == n_channels\n            and A_t.dtype == types.uint8\n            and resize_shape_t == types.UniTuple(types.int64, 2)):\n        raise ValueError(""Unsupported cv2.resize() with types {} {}"".format(\n                         A_t, resize_shape_t))\n\n    dtype = A_t.dtype\n    sig = types.void(\n        types.intp,             # new num rows\n        types.intp,             # new num cols\n        types.CPointer(dtype),  # output data\n        types.CPointer(dtype),  # input data\n        types.intp,             # num rows\n        types.intp,             # num cols\n    )\n    cv_resize = types.ExternalFunction(""cv_resize"", sig)\n\n    def resize_imp(in_arr, resize_shape):\n        A = np.ascontiguousarray(in_arr)\n        n_channels = A.shape[2]\n        # cv Size object has column first\n        B = np.empty((resize_shape[1], resize_shape[0], n_channels), A.dtype)\n\n        cv_resize(resize_shape[1], resize_shape[0], B.ctypes,\n                  A.ctypes, A.shape[0], A.shape[1])\n        return B\n\n    return resize_imp\n\n\n@overload(cv2.imdecode)\ndef imdecode_overload(A_t, flags_t):\n\n    if (isinstance(A_t, types.Array) and A_t.ndim == 1\n            and A_t.dtype == types.uint8 and flags_t == types.intp):\n        in_dtype = A_t.dtype\n        out_dtype = A_t.dtype\n\n        sig = types.CPointer(out_dtype)(\n            types.CPointer(types.intp),  # output shape\n            types.CPointer(in_dtype),   # input array\n            types.intp,                # input size (num_bytes)\n            types.intp,                # flags\n        )\n        cv_imdecode = types.ExternalFunction(""cv_imdecode"", sig)\n\n        def imdecode_imp(A, flags):\n            out_shape = np.empty(2, dtype=np.int64)\n            data = cv_imdecode(out_shape.ctypes, A.ctypes, len(A), flags)\n            n_channels = 3\n            out_shape_tup = (out_shape[0], out_shape[1], n_channels)\n            img = wrap_array(data, out_shape_tup)\n            return img\n\n        return imdecode_imp\n\n\n@intrinsic\ndef wrap_array(typingctx, data_ptr, shape_tup):\n    """"""create an array from data_ptr with shape_tup as shape\n    """"""\n    assert isinstance(data_ptr, types.CPointer), ""invalid data pointer""\n    assert (isinstance(shape_tup, types.UniTuple)\n            and shape_tup.dtype == np.intp), ""invalid shape tuple""\n    dtype = data_ptr.dtype\n    arr_typ = types.Array(dtype, shape_tup.count, \'C\')\n\n    def codegen(context, builder, sig, args):\n        assert(len(args) == 2)\n        data = args[0]\n        shape = args[1]\n        # XXX: unnecessary allocation and copy, reuse data pointer\n        shape_list = cgutils.unpack_tuple(builder, shape, shape.type.count)\n        ary = _empty_nd_impl(context, builder, arr_typ, shape_list)\n        cgutils.raw_memcpy(builder, ary.data, data, ary.nitems, ary.itemsize, align=1)\n\n        # clean up image buffer\n        fnty = lir.FunctionType(lir.VoidType(), [lir.IntType(8).as_pointer()])\n        fn_release = builder.module.get_or_insert_function(fnty, name=""cv_delete_buf"")\n        builder.call(fn_release, [data])\n\n        return impl_ret_new_ref(context, builder, sig.return_type, ary._getvalue())\n\n        # # cgutils.printf(builder, ""%d"", shape)\n        # retary = context.make_array(arr_typ)(context, builder)\n        # itemsize = context.get_abi_sizeof(context.get_data_type(dtype))\n        # shape_list = cgutils.unpack_tuple(builder, shape, shape.type.count)\n        # strides = [context.get_constant(types.intp, itemsize)]\n        # for dimension_size in reversed(shape_list[1:]):\n        #     strides.append(builder.mul(strides[-1], dimension_size))\n        # strides = tuple(reversed(strides))\n        # #import pdb; pdb.set_trace()\n        # context.populate_array(retary,\n        #            data=data,\n        #            shape=shape,\n        #            strides=strides,\n        #            itemsize=itemsize,\n        #            meminfo=None)\n        # return retary._getvalue()\n\n    return signature(arr_typ, data_ptr, shape_tup), codegen\n'"
sdc/decorators.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\'\'\'\nThis is a function decorator definition\n\'\'\'\n\nimport numba\nimport sdc\n\n\ndef jit(signature_or_function=None, **options):\n\n    if \'nopython\' not in options:\n        \'\'\'\n        Always use @jit(noPython=True) in SDC by default\n        \'\'\'\n        options[\'nopython\'] = True\n\n    if not sdc.config.config_pipeline_hpat_default:\n        \'\'\'\n        Use Numba compiler pipeline\n        \'\'\'\n        return numba.jit(signature_or_function, **options)\n\n    _locals = options.pop(\'locals\', {})\n    assert isinstance(_locals, dict)\n\n    # put pivots in locals TODO: generalize numba.jit options\n    pivots = options.pop(\'pivots\', {})\n    assert isinstance(pivots, dict)\n    for var, vals in pivots.items():\n        _locals[var + "":pivot""] = vals\n\n    distributed = set(options.pop(\'distributed\', set()))\n    assert isinstance(distributed, (set, list))\n    _locals[""##distributed""] = distributed\n\n    threaded = set(options.pop(\'threaded\', set()))\n    assert isinstance(threaded, (set, list))\n    _locals[""##threaded""] = threaded\n\n    options[\'locals\'] = _locals\n\n    #options[\'parallel\'] = True\n    options[\'parallel\'] = {\'comprehension\': True,\n                           \'setitem\': False,  # FIXME: support parallel setitem\n                           \'reduction\': True,\n                           \'numpy\': True,\n                           \'stencil\': True,\n                           \'fusion\': True,\n                           }\n\n    # Option MPI is boolean and true by default\n    # it means MPI transport will be used\n    mpi_transport_requested = options.pop(\'MPI\', sdc.config.config_transport_mpi_default)\n    if not isinstance(mpi_transport_requested, (int, bool)):\n        raise ValueError(""Option MPI or SDC_CONFIG_MPI environment variable should be boolean"")\n\n    return numba.jit(signature_or_function, pipeline_class=sdc.compiler.SDCPipeline, **options)\n'"
sdc/distributed.py,9,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\n""""""\n.. module:: distributed.py\n\nThe description of the entire module will be here.\nSupported and unsupported list can also be added here\n""""""\nfrom __future__ import print_function, division, absolute_import\nimport operator\nimport types as pytypes  # avoid confusion with numba.types\nimport copy\nimport warnings\nfrom collections import defaultdict\nimport numpy as np\n\nimport numba\nfrom numba import types\nfrom numba.core import ir, ir_utils\nfrom numba.core import postproc\nfrom numba.core.ir_utils import (\n    mk_unique_var,\n    replace_vars_inner,\n    find_topo_order,\n    dprint_func_ir,\n    remove_dead,\n    mk_alloc,\n    get_global_func_typ,\n    get_name_var_table,\n    get_call_table,\n    get_tuple_table,\n    remove_dels,\n    compile_to_numba_ir,\n    replace_arg_nodes,\n    guard,\n    get_definition,\n    require,\n    GuardException,\n    find_callname,\n    build_definitions,\n    find_build_sequence,\n    find_const,\n    is_get_setitem)\nfrom numba.core.inline_closurecall import inline_closure_call\nfrom numba.parfors.parfor import (\n    Parfor,\n    lower_parfor_sequential,\n    get_parfor_reductions,\n    get_parfor_params,\n    wrap_parfor_blocks,\n    unwrap_parfor_blocks)\n\nfrom numba.core.compiler_machinery import FunctionPass, register_pass\n\nimport sdc\nimport sdc.utilities.utils\nfrom sdc import distributed_api, distributed_lower\nfrom sdc.str_ext import string_type\nfrom sdc.str_arr_type import string_array_type\nfrom sdc.distributed_api import Reduce_Type\nfrom sdc.distributed_analysis import Distribution, DistributedAnalysis\nfrom sdc.utilities.utils import (\n    is_alloc_callname,\n    is_whole_slice,\n    is_array_container,\n    get_slice_step,\n    is_array,\n    is_np_array,\n    find_build_tuple,\n    debug_prints,\n    ReplaceFunc,\n    gen_getitem,\n    is_call,\n    is_const_slice,\n    update_globals)\nfrom sdc.hiframes.pd_dataframe_ext import DataFrameType\n\ndistributed_run_extensions = {}\n\n# analysis data for debugging\ndist_analysis = None\nfir_text = None\n\n\n@register_pass(mutates_CFG=True, analysis_only=False)\nclass DistributedPass(FunctionPass):\n    """"""The summary of the class should be here for example below is the summary line for this class\n\n    This is an adapter for a new numba passes interface. Numba pass must be stateless. This class wraps statefull DistributedPassImpl\n    """"""\n\n    _name = ""sdc_extention_distributed_pass""\n\n    def __init__(self):\n        pass\n\n    def run_pass(self, state):\n        return DistributedPassImpl(state).run_pass()\n\n\nclass DistributedPassImpl(object):\n    """"""The summary of the class should be here for example below is the summary line for this class\n\n    This class analyzes program and transforms to distributed\n    """"""\n\n    def __init__(self, state):\n        self._dist_analysis = None\n        self._T_arrs = None  # set of transposed arrays (taken from analysis)\n\n        self._rank_var = None  # will be set in run\n        self._size_var = None\n        self._g_dist_var = None\n        self._set1_var = None  # variable set to 1\n        self._set0_var = None  # variable set to 0\n        self._array_starts = {}\n        self._array_counts = {}\n\n        # keep shape attr calls on parallel arrays like X.shape\n        self._shape_attrs = {}\n        # keep array sizes of parallel arrays to handle shape attrs\n        self._array_sizes = {}\n        # save output of converted 1DVar array len() variables\n        # which are global sizes, in order to recover local\n        # size for 1DVar allocs and parfors\n        self.oneDVar_len_vars = {}\n\n        self.state = state\n\n    def run_pass(self):\n        remove_dels(self.state.func_ir.blocks)\n        dprint_func_ir(self.state.func_ir, ""starting distributed pass"")\n        self.state.func_ir._definitions = build_definitions(self.state.func_ir.blocks)\n        dist_analysis_pass = DistributedAnalysis(\n            self.state.func_ir, self.state.typemap, self.state.calltypes, self.state.typingctx,\n            self.state.metadata)\n        self._dist_analysis = dist_analysis_pass.run()\n        # dprint_func_ir(self.state.func_ir, ""after analysis distributed"")\n\n        self._T_arrs = dist_analysis_pass._T_arrs\n        self._parallel_accesses = dist_analysis_pass._parallel_accesses\n        if debug_prints():  # pragma: no cover\n            print(""distributions: "", self._dist_analysis)\n\n        self._gen_dist_inits()\n        self.state.func_ir._definitions = build_definitions(self.state.func_ir.blocks)\n        self.state.func_ir.blocks = self._run_dist_pass(self.state.func_ir.blocks)\n        self.state.func_ir.blocks = self._dist_prints(self.state.func_ir.blocks)\n        remove_dead(self.state.func_ir.blocks, self.state.func_ir.arg_names, self.state.func_ir, self.state.typemap)\n        dprint_func_ir(self.state.func_ir, ""after distributed pass"")\n        lower_parfor_sequential(\n            self.state.typingctx, self.state.func_ir, self.state.typemap, self.state.calltypes)\n        if sdc.multithread_mode:\n            # parfor params need to be updated for multithread_mode since some\n            # new variables like alloc_start are introduced by distributed pass\n            # and are used in later parfors\n            parfor_ids = get_parfor_params(\n                self.state.func_ir.blocks, True, defaultdict(list))\n        post_proc = postproc.PostProcessor(self.state.func_ir)\n        post_proc.run()\n\n        # save data for debug and test\n        global dist_analysis, fir_text\n        dist_analysis = self._dist_analysis\n        import io\n        str_io = io.StringIO()\n        self.state.func_ir.dump(str_io)\n        fir_text = str_io.getvalue()\n        str_io.close()\n\n        return True\n\n    def _run_dist_pass(self, blocks):\n        """"""This function does something""""""\n        topo_order = find_topo_order(blocks)\n        namevar_table = get_name_var_table(blocks)\n        work_list = list((l, blocks[l]) for l in reversed(topo_order))\n        while work_list:\n            label, block = work_list.pop()\n            new_body = []\n            replaced = False\n            for i, inst in enumerate(block.body):\n                out_nodes = None\n                if type(inst) in distributed_run_extensions:\n                    f = distributed_run_extensions[type(inst)]\n                    out_nodes = f(inst, self._dist_analysis.array_dists,\n                                  self.state.typemap, self.state.calltypes, self.state.typingctx,\n                                  self.state.targetctx, self)\n                elif isinstance(inst, Parfor):\n                    out_nodes = self._run_parfor(inst, namevar_table)\n                    # run dist pass recursively\n                    p_blocks = wrap_parfor_blocks(inst)\n                    # build_definitions(p_blocks, self.state.func_ir._definitions)\n                    self._run_dist_pass(p_blocks)\n                    unwrap_parfor_blocks(inst)\n                elif isinstance(inst, ir.Assign):\n                    lhs = inst.target.name\n                    rhs = inst.value\n                    if isinstance(rhs, ir.Expr):\n                        out_nodes = self._run_expr(inst, namevar_table)\n                    elif isinstance(rhs, ir.Var) and (self._is_1D_arr(rhs.name)\n                                                      and not is_array_container(self.state.typemap, rhs.name)):\n                        self._array_starts[lhs] = self._array_starts[rhs.name]\n                        self._array_counts[lhs] = self._array_counts[rhs.name]\n                        self._array_sizes[lhs] = self._array_sizes[rhs.name]\n                    elif isinstance(rhs, ir.Arg):\n                        out_nodes = self._run_arg(inst)\n                elif isinstance(inst, (ir.StaticSetItem, ir.SetItem)):\n                    if isinstance(inst, ir.SetItem):\n                        index = inst.index\n                    else:\n                        index = inst.index_var\n                    out_nodes = self._run_getsetitem(inst.target,\n                                                     index, inst, inst)\n                elif isinstance(inst, ir.Return):\n                    out_nodes = self._gen_barrier() + [inst]\n\n                if out_nodes is None:\n                    new_body.append(inst)\n                elif isinstance(out_nodes, list):\n                    new_body += out_nodes\n                elif isinstance(out_nodes, ReplaceFunc):\n                    rp_func = out_nodes\n                    if rp_func.pre_nodes is not None:\n                        new_body.extend(rp_func.pre_nodes)\n                    # inline_closure_call expects a call assignment\n                    dummy_call = ir.Expr.call(\n                        ir.Var(block.scope, ""dummy"", inst.loc),\n                        rp_func.args, (), inst.loc)\n                    if isinstance(inst, ir.Assign):\n                        # replace inst.value to a call with target args\n                        # as expected by inline_closure_call\n                        inst.value = dummy_call\n                    else:\n                        # replace inst with dummy assignment\n                        # for cases like SetItem\n                        loc = block.loc\n                        dummy_var = ir.Var(\n                            block.scope, mk_unique_var(""r_dummy""), loc)\n                        block.body[i] = ir.Assign(dummy_call, dummy_var, loc)\n                    block.body = new_body + block.body[i:]\n                    # TODO: use Parfor loop blocks when replacing funcs in\n                    # parfor loop body\n                    update_globals(rp_func.func, rp_func.glbls)\n                    inline_closure_call(self.state.func_ir, rp_func.glbls,\n                                        block, len(new_body), rp_func.func, self.state.typingctx,\n                                        rp_func.arg_types,\n                                        self.state.typemap, self.state.calltypes, work_list)\n                    replaced = True\n                    break\n                else:\n                    assert False, ""invalid dist pass out nodes""\n\n            if not replaced:\n                blocks[label].body = new_body\n\n        return blocks\n\n    def _run_expr(self, inst, namevar_table):\n        lhs = inst.target.name\n        rhs = inst.value\n        nodes = [inst]\n        if rhs.op == \'call\':\n            return self._run_call(inst)\n        # we save array start/count for data pointer to enable\n        # file read\n        if (rhs.op == \'getattr\' and rhs.attr == \'ctypes\'\n                and (self._is_1D_arr(rhs.value.name))):\n            arr_name = rhs.value.name\n            self._array_starts[lhs] = self._array_starts[arr_name]\n            self._array_counts[lhs] = self._array_counts[arr_name]\n            self._array_sizes[lhs] = self._array_sizes[arr_name]\n        if (rhs.op == \'getattr\'\n                and (self._is_1D_arr(rhs.value.name)\n                     or self._is_1D_Var_arr(rhs.value.name))\n                and rhs.attr == \'size\'):\n            return self._run_array_size(inst.target, rhs.value)\n        if (rhs.op == \'static_getitem\'\n                and rhs.value.name in self._shape_attrs):\n            arr = self._shape_attrs[rhs.value.name]\n            ndims = self.state.typemap[arr].ndim\n            if arr not in self._T_arrs and rhs.index == 0:\n                # return parallel size\n                if self._is_1D_arr(arr):\n                    # XXX hack for array container case, TODO: handle properly\n                    if arr not in self._array_sizes:\n                        arr_var = namevar_table[arr]\n                        nodes = self._gen_1D_Var_len(arr_var)\n                        nodes[-1].target = inst.target\n                        return nodes\n                    inst.value = self._array_sizes[arr][rhs.index]\n                else:\n                    assert self._is_1D_Var_arr(arr)\n                    arr_var = namevar_table[arr]\n                    nodes = self._gen_1D_Var_len(arr_var)\n                    nodes[-1].target = inst.target\n                    # save output of converted 1DVar array len() variables\n                    # which are global sizes, in order to recover local\n                    # size for 1DVar allocs and parfors\n                    self.oneDVar_len_vars[inst.target.name] = arr_var\n                    return nodes\n            # last dimension of transposed arrays is partitioned\n            if arr in self._T_arrs and rhs.index == ndims - 1:\n                assert not self._is_1D_Var_arr(\n                    arr), ""1D_Var arrays cannot transpose""\n                inst.value = self._array_sizes[arr][rhs.index]\n        if rhs.op in [\'getitem\', \'static_getitem\']:\n            if rhs.op == \'getitem\':\n                index = rhs.index\n            else:\n                index = rhs.index_var\n            return self._run_getsetitem(rhs.value, index, rhs, inst)\n        if (rhs.op == \'getattr\'\n                and (self._is_1D_arr(rhs.value.name)\n                     or self._is_1D_Var_arr(rhs.value.name))\n                and rhs.attr == \'shape\'):\n            # XXX: return a new tuple using sizes here?\n            self._shape_attrs[lhs] = rhs.value.name\n        if (rhs.op == \'getattr\'\n                and self._is_1D_arr(rhs.value.name)\n                and rhs.attr == \'T\'):\n            assert lhs in self._T_arrs\n            orig_arr = rhs.value.name\n            self._array_starts[lhs] = copy.copy(\n                self._array_starts[orig_arr]).reverse()\n            self._array_counts[lhs] = copy.copy(\n                self._array_counts[orig_arr]).reverse()\n            self._array_sizes[lhs] = copy.copy(\n                self._array_sizes[orig_arr]).reverse()\n        if (rhs.op == \'exhaust_iter\'\n                and rhs.value.name in self._shape_attrs):\n            self._shape_attrs[lhs] = self._shape_attrs[rhs.value.name]\n        if rhs.op == \'inplace_binop\' and self._is_1D_arr(rhs.lhs.name):\n            self._array_starts[lhs] = self._array_starts[rhs.lhs.name]\n            self._array_counts[lhs] = self._array_counts[rhs.lhs.name]\n            self._array_sizes[lhs] = self._array_sizes[rhs.lhs.name]\n\n        return nodes\n\n    def _gen_1D_Var_len(self, arr):\n        def f(A, op):  # pragma: no cover\n            c = len(A)\n            res = sdc.distributed_api.dist_reduce(c, op)\n        f_block = compile_to_numba_ir(f, {\'sdc\': sdc}, self.state.typingctx,\n                                      (self.state.typemap[arr.name], types.int32),\n                                      self.state.typemap, self.state.calltypes).blocks.popitem()[1]\n        replace_arg_nodes(\n            f_block, [arr, ir.Const(Reduce_Type.Sum.value, arr.loc)])\n        nodes = f_block.body[:-3]  # remove none return\n        return nodes\n\n    def _gen_dist_inits(self):\n        # add initializations\n        topo_order = find_topo_order(self.state.func_ir.blocks)\n        first_block = self.state.func_ir.blocks[topo_order[0]]\n        # set scope and loc of generated code to the first variable in block\n        scope = first_block.scope\n        loc = first_block.loc\n        out = []\n        self._set1_var = ir.Var(scope, mk_unique_var(""$const_parallel""), loc)\n        self.state.typemap[self._set1_var.name] = types.int64\n        set1_assign = ir.Assign(ir.Const(1, loc), self._set1_var, loc)\n        out.append(set1_assign)\n        self._set0_var = ir.Var(scope, mk_unique_var(""$const_parallel""), loc)\n        self.state.typemap[self._set0_var.name] = types.int64\n        set0_assign = ir.Assign(ir.Const(0, loc), self._set0_var, loc)\n        out.append(set0_assign)\n        # g_dist_var = Global(sdc.distributed_api)\n        g_dist_var = ir.Var(scope, mk_unique_var(""$distributed_g_var""), loc)\n        self._g_dist_var = g_dist_var\n        self.state.typemap[g_dist_var.name] = types.misc.Module(sdc.distributed_api)\n        g_dist = ir.Global(\'distributed_api\', sdc.distributed_api, loc)\n        g_dist_assign = ir.Assign(g_dist, g_dist_var, loc)\n        # attr call: rank_attr = getattr(g_dist_var, get_rank)\n        rank_attr_call = ir.Expr.getattr(g_dist_var, ""get_rank"", loc)\n        rank_attr_var = ir.Var(scope, mk_unique_var(""$get_rank_attr""), loc)\n        self.state.typemap[rank_attr_var.name] = get_global_func_typ(\n            distributed_api.get_rank)\n        rank_attr_assign = ir.Assign(rank_attr_call, rank_attr_var, loc)\n        # rank_var = sdc.distributed_api.get_rank()\n        rank_var = ir.Var(scope, mk_unique_var(""$rank""), loc)\n        self.state.typemap[rank_var.name] = types.int32\n        rank_call = ir.Expr.call(rank_attr_var, [], (), loc)\n        self.state.calltypes[rank_call] = self.state.typemap[rank_attr_var.name].get_call_type(\n            self.state.typingctx, [], {})\n        rank_assign = ir.Assign(rank_call, rank_var, loc)\n        self._rank_var = rank_var\n        out += [g_dist_assign, rank_attr_assign, rank_assign]\n\n        # attr call: size_attr = getattr(g_dist_var, get_size)\n        size_attr_call = ir.Expr.getattr(g_dist_var, ""get_size"", loc)\n        size_attr_var = ir.Var(scope, mk_unique_var(""$get_size_attr""), loc)\n        self.state.typemap[size_attr_var.name] = get_global_func_typ(\n            distributed_api.get_size)\n        size_attr_assign = ir.Assign(size_attr_call, size_attr_var, loc)\n        # size_var = sdc.distributed_api.get_size()\n        size_var = ir.Var(scope, mk_unique_var(""$dist_size""), loc)\n        self.state.typemap[size_var.name] = types.int32\n        size_call = ir.Expr.call(size_attr_var, [], (), loc)\n        self.state.calltypes[size_call] = self.state.typemap[size_attr_var.name].get_call_type(\n            self.state.typingctx, [], {})\n        size_assign = ir.Assign(size_call, size_var, loc)\n        self._size_var = size_var\n        out += [size_attr_assign, size_assign]\n        first_block.body = out + first_block.body\n\n    def _run_call(self, assign):\n        lhs = assign.target.name\n        rhs = assign.value\n        func_var = rhs.func.name\n        scope = assign.target.scope\n        loc = assign.target.loc\n        out = [assign]\n\n        func_name = """"\n        func_mod = """"\n        fdef = guard(find_callname, self.state.func_ir, rhs, self.state.typemap)\n        if fdef is None:\n            # FIXME: since parfors are transformed and then processed\n            # recursively, some funcs don\'t have definitions. The generated\n            # arrays should be assigned REP and the var definitions added.\n            # warnings.warn(\n            #     ""function call couldn\'t be found for distributed pass"")\n            return out\n        else:\n            func_name, func_mod = fdef\n\n        # divide 1D alloc\n        # XXX allocs should be matched before going to _run_call_np\n        if self._is_1D_arr(lhs) and is_alloc_callname(func_name, func_mod):\n            # XXX for pre_alloc_string_array(n, nc), we assume nc is local\n            # value (updated only in parfor like _str_replace_regex_impl)\n            size_var = rhs.args[0]\n            out, new_size_var = self._run_alloc(size_var, lhs, scope, loc)\n            # empty_inferred is tuple for some reason\n            rhs.args = list(rhs.args)\n            rhs.args[0] = new_size_var\n            out.append(assign)\n            return out\n\n        # fix 1D_Var allocs in case global len of another 1DVar is used\n        if self._is_1D_Var_arr(lhs) and is_alloc_callname(func_name, func_mod):\n            size_var = rhs.args[0]\n            out, new_size_var = self._fix_1D_Var_alloc(\n                size_var, lhs, scope, loc)\n            # empty_inferred is tuple for some reason\n            rhs.args = list(rhs.args)\n            rhs.args[0] = new_size_var\n            out.append(assign)\n            return out\n\n        # numpy direct functions\n        if isinstance(func_mod, str) and func_mod == \'numpy\':\n            return self._run_call_np(lhs, func_name, assign, rhs.args)\n\n        # array.func calls\n        if isinstance(func_mod, ir.Var) and is_np_array(self.state.typemap, func_mod.name):\n            return self._run_call_array(lhs, func_mod, func_name, assign, rhs.args)\n\n        # df.func calls\n        if isinstance(func_mod, ir.Var) and isinstance(self.state.typemap[func_mod.name], DataFrameType):\n            return self._run_call_df(lhs, func_mod, func_name, assign, rhs.args)\n\n        # string_array.func_calls\n        if (self._is_1D_arr(lhs) and isinstance(func_mod, ir.Var)\n                and self.state.typemap[func_mod.name] == string_array_type):\n            if func_name == \'copy\':\n                self._array_starts[lhs] = self._array_starts[func_mod.name]\n                self._array_counts[lhs] = self._array_counts[func_mod.name]\n                self._array_sizes[lhs] = self._array_sizes[func_mod.name]\n\n        if fdef == (\'permutation\', \'numpy.random\'):\n            if self.state.typemap[rhs.args[0].name] == types.int64:\n                self._array_sizes[lhs] = [rhs.args[0]]\n                return self._run_permutation_int(assign, rhs.args)\n\n        # len(A) if A is 1D\n        if fdef == (\'len\', \'builtins\') and rhs.args and self._is_1D_arr(rhs.args[0].name):\n            arr = rhs.args[0].name\n            assign.value = self._array_sizes[arr][0]\n\n        # len(A) if A is 1D_Var\n        if fdef == (\'len\', \'builtins\') and rhs.args and self._is_1D_Var_arr(rhs.args[0].name):\n            arr_var = rhs.args[0]\n            out = self._gen_1D_Var_len(arr_var)\n            out[-1].target = assign.target\n            self.oneDVar_len_vars[assign.target.name] = arr_var\n\n        if (sdc.config._has_pyarrow\n                and fdef == (\'read_parquet\', \'sdc.io.parquet_pio\')\n                and self._is_1D_arr(rhs.args[2].name)):\n            arr = rhs.args[2].name\n            assert len(self._array_starts[arr]) == 1, ""only 1D arrs in parquet""\n            start_var = self._array_starts[arr][0]\n            count_var = self._array_counts[arr][0]\n            rhs.args += [start_var, count_var]\n\n            def f(fname, cindex, arr, out_dtype, start, count):  # pragma: no cover\n                return sdc.io.parquet_pio.read_parquet_parallel(fname, cindex,\n                                                                 arr, out_dtype, start, count)\n\n            return self._replace_func(f, rhs.args)\n\n        if (sdc.config._has_pyarrow\n                and fdef == (\'read_parquet_str\', \'sdc.io.parquet_pio\')\n                and self._is_1D_arr(lhs)):\n            arr = lhs\n            size_var = rhs.args[2]\n            assert self.state.typemap[size_var.name] == types.intp\n            self._array_sizes[arr] = [size_var]\n            out, start_var, count_var = self._gen_1D_div(size_var, scope, loc,\n                                                         ""$alloc"", ""get_node_portion"", distributed_api.get_node_portion)\n            self._array_starts[lhs] = [start_var]\n            self._array_counts[lhs] = [count_var]\n            rhs.args[2] = start_var\n            rhs.args.append(count_var)\n\n            def f(fname, cindex, start, count):  # pragma: no cover\n                return sdc.io.parquet_pio.read_parquet_str_parallel(fname, cindex,\n                                                                     start, count)\n\n            f_block = compile_to_numba_ir(f, {\'sdc\': sdc}, self.state.typingctx,\n                                          (self.state.typemap[rhs.args[0].name], types.intp,\n                                           types.intp, types.intp),\n                                          self.state.typemap, self.state.calltypes).blocks.popitem()[1]\n            replace_arg_nodes(f_block, rhs.args)\n            out += f_block.body[:-2]\n            out[-1].target = assign.target\n\n        if (func_mod == \'sdc.hiframes.api\' and func_name in (\n                \'to_arr_from_series\', \'ts_series_to_arr_typ\',\n                \'to_date_series_type\', \'init_series\')\n                and self._is_1D_arr(rhs.args[0].name)):\n            # TODO: handle index\n            in_arr = rhs.args[0].name\n            self._array_starts[lhs] = self._array_starts[in_arr]\n            self._array_counts[lhs] = self._array_counts[in_arr]\n            self._array_sizes[lhs] = self._array_sizes[in_arr]\n\n        if (fdef == (\'init_dataframe\', \'sdc.hiframes.pd_dataframe_ext\')\n                and self._is_1D_arr(rhs.args[0].name)):\n            in_arr = rhs.args[0].name\n            self._array_starts[lhs] = self._array_starts[in_arr]\n            self._array_counts[lhs] = self._array_counts[in_arr]\n            self._array_sizes[lhs] = self._array_sizes[in_arr]\n\n        if (fdef == (\'compute_split_view\', \'sdc.hiframes.split_impl\')\n                and self._is_1D_arr(rhs.args[0].name)):\n            in_arr = rhs.args[0].name\n            self._array_starts[lhs] = self._array_starts[in_arr]\n            self._array_counts[lhs] = self._array_counts[in_arr]\n            self._array_sizes[lhs] = self._array_sizes[in_arr]\n\n        if (fdef == (\'get_split_view_index\', \'sdc.hiframes.split_impl\')\n                and self._is_1D_arr(rhs.args[0].name)):\n            arr = rhs.args[0]\n            index_var = rhs.args[1]\n            sub_nodes = self._get_ind_sub(\n                index_var, self._array_starts[arr.name][0])\n            out = sub_nodes\n            rhs.args[1] = sub_nodes[-1].target\n            out.append(assign)\n            return out\n\n        if (fdef == (\'setitem_str_arr_ptr\', \'sdc.str_arr_ext\')\n                and self._is_1D_arr(rhs.args[0].name)):\n            arr = rhs.args[0]\n            index_var = rhs.args[1]\n            sub_nodes = self._get_ind_sub(\n                index_var, self._array_starts[arr.name][0])\n            out = sub_nodes\n            rhs.args[1] = sub_nodes[-1].target\n            out.append(assign)\n            return out\n\n        if (fdef == (\'str_arr_item_to_numeric\', \'sdc.str_arr_ext\')\n                and self._is_1D_arr(rhs.args[0].name)):\n            # TODO: test parallel\n            arr = rhs.args[0]\n            index_var = rhs.args[1]\n            sub_nodes = self._get_ind_sub(\n                index_var, self._array_starts[arr.name][0])\n            out = sub_nodes\n            rhs.args[1] = sub_nodes[-1].target\n            # input string array\n            arr = rhs.args[2]\n            index_var = rhs.args[3]\n            sub_nodes = self._get_ind_sub(\n                index_var, self._array_starts[arr.name][0])\n            out += sub_nodes\n            rhs.args[3] = sub_nodes[-1].target\n            out.append(assign)\n            return out\n\n        if fdef == (\'isna\', \'sdc.hiframes.api\') and self._is_1D_arr(rhs.args[0].name):\n            # fix index in call to isna\n            arr = rhs.args[0]\n            ind = rhs.args[1]\n            out = self._get_ind_sub(ind, self._array_starts[arr.name][0])\n            rhs.args[1] = out[-1].target\n            out.append(assign)\n\n        if fdef == (\'rolling_fixed\', \'sdc.hiframes.rolling\') and (\n                self._is_1D_arr(rhs.args[0].name)\n                or self._is_1D_Var_arr(rhs.args[0].name)):\n            in_arr = rhs.args[0].name\n            if self._is_1D_arr(in_arr):\n                self._array_starts[lhs] = self._array_starts[in_arr]\n                self._array_counts[lhs] = self._array_counts[in_arr]\n                self._array_sizes[lhs] = self._array_sizes[in_arr]\n            # set parallel flag to true\n            true_var = ir.Var(scope, mk_unique_var(""true_var""), loc)\n            self.state.typemap[true_var.name] = types.boolean\n            rhs.args[3] = true_var\n            out = [ir.Assign(ir.Const(True, loc), true_var, loc), assign]\n\n        if fdef == (\'rolling_variable\', \'sdc.hiframes.rolling\') and (\n                self._is_1D_arr(rhs.args[0].name)\n                or self._is_1D_Var_arr(rhs.args[0].name)):\n            in_arr = rhs.args[0].name\n            if self._is_1D_arr(in_arr):\n                self._array_starts[lhs] = self._array_starts[in_arr]\n                self._array_counts[lhs] = self._array_counts[in_arr]\n                self._array_sizes[lhs] = self._array_sizes[in_arr]\n            # set parallel flag to true\n            true_var = ir.Var(scope, mk_unique_var(""true_var""), loc)\n            self.state.typemap[true_var.name] = types.boolean\n            rhs.args[4] = true_var\n            out = [ir.Assign(ir.Const(True, loc), true_var, loc), assign]\n\n        if (func_mod == \'sdc.hiframes.rolling\'\n            and func_name in (\'shift\', \'pct_change\')\n            and (self._is_1D_arr(rhs.args[0].name)\n                 or self._is_1D_Var_arr(rhs.args[0].name))):\n            in_arr = rhs.args[0].name\n            if self._is_1D_arr(in_arr):\n                self._array_starts[lhs] = self._array_starts[in_arr]\n                self._array_counts[lhs] = self._array_counts[in_arr]\n                self._array_sizes[lhs] = self._array_sizes[in_arr]\n            # set parallel flag to true\n            true_var = ir.Var(scope, mk_unique_var(""true_var""), loc)\n            self.state.typemap[true_var.name] = types.boolean\n            rhs.args[2] = true_var\n            out = [ir.Assign(ir.Const(True, loc), true_var, loc), assign]\n\n        if fdef == (\'quantile\', \'sdc.hiframes.api\') and (self._is_1D_arr(rhs.args[0].name)\n                                                          or self._is_1D_Var_arr(rhs.args[0].name)):\n            arr = rhs.args[0].name\n            if arr in self._array_sizes:\n                assert len(self._array_sizes[arr]\n                           ) == 1, ""only 1D arrs in quantile""\n                size_var = self._array_sizes[arr][0]\n            else:\n                size_var = self._set0_var\n            rhs.args += [size_var]\n\n            def f(arr, q, size):\n                return sdc.hiframes.api.quantile_parallel(arr, q, size)\n            return self._replace_func(f, rhs.args)\n\n        if fdef == (\n            \'nunique\', \'sdc.hiframes.api\') and (\n            self._is_1D_arr(\n                rhs.args[0].name) or self._is_1D_Var_arr(\n                rhs.args[0].name)):\n\n            def f(arr):\n                return sdc.hiframes.api.nunique_parallel(arr)\n\n            return self._replace_func(f, rhs.args)\n\n        if fdef == (\n            \'unique\', \'sdc.hiframes.api\') and (\n            self._is_1D_arr(\n                rhs.args[0].name) or self._is_1D_Var_arr(\n                rhs.args[0].name)):\n\n            def f(arr):\n                return sdc.hiframes.api.unique_parallel(arr)\n\n            return self._replace_func(f, rhs.args)\n\n        if fdef == (\n            \'nlargest\', \'sdc.hiframes.api\') and (\n            self._is_1D_arr(\n                rhs.args[0].name) or self._is_1D_Var_arr(\n                rhs.args[0].name)):\n\n            def f(arr, k, i, f):\n                return sdc.hiframes.api.nlargest_parallel(arr, k, i, f)\n\n            return self._replace_func(f, rhs.args)\n\n        if fdef == (\n            \'median\', \'sdc.hiframes.api\') and (\n            self._is_1D_arr(\n                rhs.args[0].name) or self._is_1D_Var_arr(\n                rhs.args[0].name)):\n\n            def f(arr):\n                return sdc.hiframes.api.median(arr, True)\n\n            return self._replace_func(f, rhs.args)\n\n        if fdef == (\'convert_rec_to_tup\', \'sdc.hiframes.api\'):\n            # optimize Series back to back map pattern with tuples\n            # TODO: create another optimization pass?\n            arg_def = guard(get_definition, self.state.func_ir, rhs.args[0])\n            if (is_call(arg_def) and guard(find_callname, self.state.func_ir, arg_def)\n                    == (\'convert_tup_to_rec\', \'sdc.hiframes.api\')):\n                assign.value = arg_def.args[0]\n            return out\n\n        if fdef == (\'dist_return\', \'sdc.distributed_api\'):\n            # always rebalance returned distributed arrays\n            # TODO: need different flag for 1D_Var return (distributed_var)?\n            # TODO: rebalance strings?\n            # return [assign]  # self._run_call_rebalance_array(lhs, assign, rhs.args)\n            assign.value = rhs.args[0]\n            return [assign]\n\n        if ((fdef == (\'get_series_data\', \'sdc.hiframes.api\')\n             or fdef == (\'get_series_index\', \'sdc.hiframes.api\')\n             or fdef == (\'get_dataframe_data\', \'sdc.hiframes.pd_dataframe_ext\'))):\n            out = [assign]\n            arr = assign.target\n            # gen len() using 1D_Var reduce approach.\n            # TODO: refactor to avoid reduction for 1D\n            # arr_typ = self.state.typemap[arr.name]\n            ndim = 1\n            out += self._gen_1D_Var_len(arr)\n            total_length = out[-1].target\n            div_nodes, start_var, count_var = self._gen_1D_div(\n                total_length, arr.scope, arr.loc, ""$input"", ""get_node_portion"", distributed_api.get_node_portion)\n            out += div_nodes\n\n            # XXX: get sizes in lower dimensions\n            self._array_starts[lhs] = [-1] * ndim\n            self._array_counts[lhs] = [-1] * ndim\n            self._array_sizes[lhs] = [-1] * ndim\n            self._array_starts[lhs][0] = start_var\n            self._array_counts[lhs][0] = count_var\n            self._array_sizes[lhs][0] = total_length\n\n            return out\n\n        if fdef == (\'threaded_return\', \'sdc.distributed_api\'):\n            assign.value = rhs.args[0]\n            return [assign]\n\n        if fdef == (\'rebalance_array\', \'sdc.distributed_api\'):\n            return self._run_call_rebalance_array(lhs, assign, rhs.args)\n\n        if fdef == (\'file_read\', \'sdc.io.np_io\') and rhs.args[1].name in self._array_starts:\n            _fname = rhs.args[0]\n            _data_ptr = rhs.args[1]\n            _start = self._array_starts[_data_ptr.name][0]\n            _count = self._array_counts[_data_ptr.name][0]\n\n            def f(fname, data_ptr, start, count):  # pragma: no cover\n                return sdc.io.np_io.file_read_parallel(fname, data_ptr, start, count)\n            return self._replace_func(f, [_fname, _data_ptr, _start, _count])\n\n        return out\n\n    def _run_call_np(self, lhs, func_name, assign, args):\n        """"""transform np.func() calls\n        """"""\n        # allocs are handled separately\n        is_1D_bool = (self._is_1D_Var_arr(lhs) or self._is_1D_arr(lhs))\n        err_str = ""allocation calls handled separately \'empty\', \'zeros\', \'ones\', \'full\' etc.""\n        assert not (is_1D_bool and func_name in sdc.utilities.utils.np_alloc_callnames), err_str\n\n        out = [assign]\n        scope = assign.target.scope\n        loc = assign.loc\n\n        # numba doesn\'t support np.reshape() form yet\n        # if func_name == \'reshape\':\n        #     size_var = args[1]\n        #     # handle reshape like new allocation\n        #     out, new_size_var = self._run_alloc(size_var, lhs)\n        #     args[1] = new_size_var\n        #     out.append(assign)\n\n        if (func_name == \'array\' and is_array(self.state.typemap, args[0].name) and self._is_1D_arr(args[0].name)):\n            in_arr = args[0].name\n            self._array_starts[lhs] = self._array_starts[in_arr]\n            self._array_counts[lhs] = self._array_counts[in_arr]\n            self._array_sizes[lhs] = self._array_sizes[in_arr]\n\n        # output array has same properties (starts etc.) as input array\n        if (func_name in [\'cumsum\', \'cumprod\', \'empty_like\', \'zeros_like\', \'ones_like\',\n                          \'full_like\', \'copy\', \'ravel\', \'ascontiguousarray\'] and self._is_1D_arr(args[0].name)):\n            if func_name == \'ravel\':\n                assert self.state.typemap[args[0].name].ndim == 1, ""only 1D ravel supported""\n            in_arr = args[0].name\n            self._array_starts[lhs] = self._array_starts[in_arr]\n            self._array_counts[lhs] = self._array_counts[in_arr]\n            self._array_sizes[lhs] = self._array_sizes[in_arr]\n\n        if (func_name in [\'cumsum\', \'cumprod\'] and self._is_1D_arr(args[0].name)):\n            in_arr = args[0].name\n            in_arr_var = args[0]\n            lhs_var = assign.target\n            # allocate output array\n            # TODO: compute inplace if input array is dead\n            out = mk_alloc(\n                self.state.typemap,\n                self.state.calltypes,\n                lhs_var,\n                tuple(\n                    self._array_sizes[in_arr]),\n                self.state.typemap[in_arr].dtype,\n                scope,\n                loc)\n            # generate distributed call\n            dist_attr_var = ir.Var(scope, mk_unique_var(""$dist_attr""), loc)\n            dist_func_name = ""dist_"" + func_name\n            dist_func = getattr(distributed_api, dist_func_name)\n            dist_attr_call = ir.Expr.getattr(self._g_dist_var, dist_func_name, loc)\n            self.state.typemap[dist_attr_var.name] = get_global_func_typ(dist_func)\n            dist_func_assign = ir.Assign(dist_attr_call, dist_attr_var, loc)\n            err_var = ir.Var(scope, mk_unique_var(""$dist_err_var""), loc)\n            self.state.typemap[err_var.name] = types.int32\n            dist_call = ir.Expr.call(dist_attr_var, [in_arr_var, lhs_var], (), loc)\n            self.state.calltypes[dist_call] = self.state.typemap[dist_attr_var.name].get_call_type(\n                self.state.typingctx, [self.state.typemap[in_arr], self.state.typemap[lhs]], {})\n            dist_assign = ir.Assign(dist_call, err_var, loc)\n            return out + [dist_func_assign, dist_assign]\n\n        # sum over the first axis is distributed, A.sum(0)\n        if func_name == \'sum\' and len(args) == 2:\n            axis_def = guard(get_definition, self.state.func_ir, args[1])\n            if isinstance(axis_def, ir.Const) and axis_def.value == 0:\n                reduce_op = Reduce_Type.Sum\n                reduce_var = assign.target\n                return out + self._gen_reduce(reduce_var, reduce_op, scope, loc)\n\n        if func_name == \'dot\':\n            return self._run_call_np_dot(lhs, assign, args)\n\n        if func_name == \'stack\' and self._is_1D_arr(lhs):\n            # TODO: generalize\n            in_arrs, _ = guard(find_build_sequence, self.state.func_ir, args[0])\n            arr0 = in_arrs[0].name\n            self._array_starts[lhs] = [self._array_starts[arr0][0], None]\n            self._array_counts[lhs] = [self._array_counts[arr0][0], None]\n            self._array_sizes[lhs] = [self._array_sizes[arr0][0], None]\n\n        return out\n\n    def _run_call_array(self, lhs, arr, func_name, assign, args):\n        #\n        out = [assign]\n        if func_name in (\'astype\', \'copy\') and self._is_1D_arr(lhs):\n            self._array_starts[lhs] = self._array_starts[arr.name]\n            self._array_counts[lhs] = self._array_counts[arr.name]\n            self._array_sizes[lhs] = self._array_sizes[arr.name]\n\n        # HACK support A.reshape(n, 1) for 1D_Var\n        if func_name == \'reshape\' and self._is_1D_Var_arr(arr.name):\n            assert len(args) == 2 and guard(find_const, self.state.func_ir, args[1]) == 1\n            assert args[0].name in self.oneDVar_len_vars\n            size_var = args[0]\n            out, new_size_var = self._fix_1D_Var_alloc(size_var, lhs, assign.target.scope, assign.loc)\n            # empty_inferred is tuple for some reason\n            assign.value.args = list(args)\n            assign.value.args[0] = new_size_var\n            out.append(assign)\n            return out\n\n        if func_name == \'reshape\' and self._is_1D_arr(arr.name):\n            return self._run_reshape(assign, arr, args)\n\n        if func_name == \'transpose\' and self._is_1D_arr(lhs):\n            # Currently only 1D arrays are supported\n            assert self._is_1D_arr(arr.name)\n            ndim = self.state.typemap[arr.name].ndim\n            self._array_starts[lhs] = [-1] * ndim\n            self._array_counts[lhs] = [-1] * ndim\n            self._array_sizes[lhs] = [-1] * ndim\n            self._array_starts[lhs][0] = self._array_starts[arr.name][0]\n            self._array_counts[lhs][0] = self._array_counts[arr.name][0]\n            self._array_sizes[lhs][0] = self._array_sizes[arr.name][0]\n\n        # TODO: refactor\n        # TODO: add unittest\n        if func_name == \'tofile\':\n            if self._is_1D_arr(arr.name):\n                _fname = args[0]\n                _start = self._array_starts[arr.name][0]\n                _count = self._array_counts[arr.name][0]\n\n                def f(fname, arr, start, count):  # pragma: no cover\n                    return sdc.io.np_io.file_write_parallel(fname, arr, start, count)\n\n                return self._replace_func(f, [_fname, arr, _start, _count])\n\n            if self._is_1D_Var_arr(arr.name):\n                _fname = args[0]\n\n                def f(fname, arr):  # pragma: no cover\n                    count = len(arr)\n                    start = sdc.distributed_api.dist_exscan(count)\n                    return sdc.io.np_io.file_write_parallel(fname, arr, start, count)\n\n                return self._replace_func(f, [_fname, arr])\n\n        return out\n\n    def _run_call_df(self, lhs, df, func_name, assign, args):\n        if func_name == \'to_csv\' and self._is_1D_arr(df.name):\n            # set index to proper range if None\n            # avoid header for non-zero ranks\n            # write to string then parallel file write\n            # df.to_csv(fname) ->\n            # l = len(df)\n            # index_start = dist_exscan(l)\n            # df2 = df(index=range(index_start, index_start+l))\n            # header = header and is_root  # only first line has header\n            # str_out = df2.to_csv(None, header=header)\n            # sdc.io.np_io._file_write_parallel(fname, str_out)\n\n            df_typ = self.state.typemap[df.name]\n            rhs = assign.value\n            fname = args[0]\n\n            # update df index and get to_csv from new df\n            nodes = self._fix_parallel_df_index(df)\n            new_df = nodes[-1].target\n            new_df_typ = self.state.typemap[new_df.name]\n            new_to_csv = ir.Expr.getattr(new_df, \'to_csv\', new_df.loc)\n            new_func = ir.Var(new_df.scope, mk_unique_var(\'func\'), new_df.loc)\n            self.state.typemap[new_func.name] = self.state.typingctx.resolve_getattr(\n                new_df_typ, \'to_csv\')\n            nodes.append(ir.Assign(new_to_csv, new_func, new_df.loc))\n            rhs.func = new_func\n\n            # # header = header and is_root\n            kws = dict(rhs.kws)\n            true_var = ir.Var(assign.target.scope, mk_unique_var(\'true\'), rhs.loc)\n            self.state.typemap[true_var.name] = types.bool_\n            nodes.append(\n                ir.Assign(ir.Const(True, new_df.loc), true_var, new_df.loc))\n            header_var = self._get_arg(\n                \'to_csv\', rhs.args, kws, 5, \'header\', true_var)\n            nodes += self._gen_is_root_and_cond(header_var)\n            header_var = nodes[-1].target\n            if len(rhs.args) > 5:\n                rhs.args[5] = header_var\n            else:\n                kws[\'header\'] = header_var\n                rhs.kws = kws\n\n            # fix to_csv() type to have None as 1st arg\n            call_type = self.state.calltypes.pop(rhs)\n            arg_typs = list((types.none,) + call_type.args[1:])\n            arg_typs[5] = types.bool_\n            arg_typs = tuple(arg_typs)\n            # self.state.calltypes[rhs] = self.state.typemap[rhs.func.name].get_call_type(\n            #      self.state.typingctx, arg_typs, {})\n            self.state.calltypes[rhs] = numba.core.typing.Signature(\n                string_type, arg_typs, new_df_typ,\n                call_type.pysig)\n\n            # None as 1st arg\n            none_var = ir.Var(assign.target.scope, mk_unique_var(\'none\'), rhs.loc)\n            self.state.typemap[none_var.name] = types.none\n            none_assign = ir.Assign(ir.Const(None, rhs.loc), none_var, rhs.loc)\n            nodes.append(none_assign)\n            rhs.args[0] = none_var\n\n            # str_out = df.to_csv(None)\n            str_out = ir.Var(assign.target.scope, mk_unique_var(\'write_csv\'), rhs.loc)\n            self.state.typemap[str_out.name] = string_type\n            new_assign = ir.Assign(rhs, str_out, rhs.loc)\n            nodes.append(new_assign)\n\n            # print_node = ir.Print([str_out], None, rhs.loc)\n            # self.state.calltypes[print_node] = signature(types.none, string_type)\n            # nodes.append(print_node)\n\n            # TODO: fix lazy IO load\n            from . import hio\n            import llvmlite.binding as ll\n            ll.add_symbol(\'file_write_parallel\', hio.file_write_parallel)\n            # HACK use the string in a dummy function to avoid refcount issues\n            # TODO: fix string data reference count\n            dummy_use = numba.njit(lambda a: None)\n\n            def f(fname, str_out):  # pragma: no cover\n                count = len(str_out)\n                start = sdc.distributed_api.dist_exscan(count)\n                sdc.io.np_io._file_write_parallel(\n                    fname._data, str_out._data, start, count, 1)\n                dummy_use(str_out)\n\n            return self._replace_func(\n                f, [fname, str_out], pre_nodes=nodes)\n\n        return [assign]\n\n    def _gen_is_root_and_cond(self, cond_var):\n        def f(cond):\n            return cond & (sdc.distributed_api.get_rank() == 0)\n        f_block = compile_to_numba_ir(f, {\'sdc\': sdc},\n                                      self.state.typingctx,\n                                      (self.state.typemap[cond_var.name],),\n                                      self.state.typemap,\n                                      self.state.calltypes).blocks.popitem()[1]\n        replace_arg_nodes(f_block, [cond_var])\n        nodes = f_block.body[:-2]\n        return nodes\n\n    def _fix_parallel_df_index(self, df):\n        def f(df):  # pragma: no cover\n            length = len(df)\n            start = sdc.distributed_api.dist_exscan(length)\n            ind = np.arange(start, start + length)\n            df2 = sdc.hiframes.pd_dataframe_ext.set_df_index(df, ind)\n            return df2\n\n        f_block = compile_to_numba_ir(f, {\'sdc\': sdc, \'np\': np},\n                                      self.state.typingctx,\n                                      (self.state.typemap[df.name],),\n                                      self.state.typemap,\n                                      self.state.calltypes).blocks.popitem()[1]\n        replace_arg_nodes(f_block, [df])\n        nodes = f_block.body[:-2]\n        return nodes\n\n    def _run_permutation_int(self, assign, args):\n        lhs = assign.target\n        n = args[0]\n\n        def f(lhs, n):\n            sdc.distributed_lower.dist_permutation_int(lhs, n)\n\n        f_block = compile_to_numba_ir(f, {\'sdc\': sdc},\n                                      self.state.typingctx,\n                                      (self.state.typemap[lhs.name], types.intp),\n                                      self.state.typemap,\n                                      self.state.calltypes).blocks.popitem()[1]\n        replace_arg_nodes(f_block, [lhs, n])\n        f_block.body = [assign] + f_block.body\n        return f_block.body[:-3]\n\n    # Returns an IR node that defines a variable holding the size of |dtype|.\n    def dtype_size_assign_ir(self, dtype, scope, loc):\n        context = numba.targets.cpu.CPUContext(self.state.typingctx)\n        dtype_size = context.get_abi_sizeof(context.get_data_type(dtype))\n        dtype_size_var = ir.Var(scope, mk_unique_var(""dtype_size""), loc)\n        self.state.typemap[dtype_size_var.name] = types.intp\n        return ir.Assign(ir.Const(dtype_size, loc), dtype_size_var, loc)\n\n    def _run_permutation_array_index(self, lhs, rhs, idx):\n        scope, loc = lhs.scope, lhs.loc\n        dtype = self.state.typemap[lhs.name].dtype\n        out = mk_alloc(self.state.typemap, self.state.calltypes, lhs,\n                       (self._array_counts[lhs.name][0],\n                        *self._array_sizes[lhs.name][1:]), dtype, scope, loc)\n\n        def f(lhs, lhs_len, dtype_size, rhs, idx, idx_len):\n            sdc.distributed_lower.dist_permutation_array_index(\n                lhs, lhs_len, dtype_size, rhs, idx, idx_len)\n\n        f_block = compile_to_numba_ir(f, {\'sdc\': sdc},\n                                      self.state.typingctx,\n                                      (self.state.typemap[lhs.name],\n                                       types.intp,\n                                       types.intp,\n                                       self.state.typemap[rhs.name],\n                                       self.state.typemap[idx.name],\n                                       types.intp),\n                                      self.state.typemap,\n                                      self.state.calltypes).blocks.popitem()[1]\n        dtype_ir = self.dtype_size_assign_ir(dtype, scope, loc)\n        out.append(dtype_ir)\n        replace_arg_nodes(f_block, [lhs, self._array_sizes[lhs.name][0],\n                                    dtype_ir.target, rhs, idx,\n                                    self._array_sizes[idx.name][0]])\n        f_block.body = out + f_block.body\n        return f_block.body[:-3]\n\n    def _run_reshape(self, assign, in_arr, args):\n        lhs = assign.target\n        scope = assign.target.scope\n        loc = assign.target.loc\n        if len(args) == 1:\n            new_shape = args[0]\n        else:\n            # reshape can take list of ints\n            new_shape = args\n        # TODO: avoid alloc and copy if no communication necessary\n        # get new local shape in reshape and set start/count vars like new allocation\n        out, new_local_shape_var = self._run_alloc(new_shape, lhs.name, scope, loc)\n        # get actual tuple for mk_alloc\n        if len(args) != 1:\n            sh_list = guard(find_build_tuple, self.state.func_ir, new_local_shape_var)\n            assert sh_list is not None, ""invalid shape in reshape""\n            new_local_shape_var = tuple(sh_list)\n        dtype = self.state.typemap[in_arr.name].dtype\n        out += mk_alloc(self.state.typemap, self.state.calltypes, lhs,\n                        new_local_shape_var, dtype, scope, loc)\n\n        def f(lhs, in_arr, new_0dim_global_len, old_0dim_global_len, dtype_size):  # pragma: no cover\n            sdc.distributed_lower.dist_oneD_reshape_shuffle(\n                lhs, in_arr, new_0dim_global_len, old_0dim_global_len, dtype_size)\n\n        f_block = compile_to_numba_ir(f, {\'sdc\': sdc},\n                                      self.state.typingctx,\n                                      (self.state.typemap[lhs.name], self.state.typemap[in_arr.name],\n                                       types.intp, types.intp, types.intp),\n                                      self.state.typemap, self.state.calltypes).blocks.popitem()[1]\n        dtype_ir = self.dtype_size_assign_ir(dtype, scope, loc)\n        out.append(dtype_ir)\n        replace_arg_nodes(f_block, [lhs, in_arr, self._array_sizes[lhs.name][0],\n                                    self._array_sizes[in_arr.name][0],\n                                    dtype_ir.target])\n        out += f_block.body[:-3]\n        return out\n        # if len(args) == 1:\n        #     args[0] = new_size_var\n        # else:\n        #     args[0] = self._tuple_table[new_size_var.name][0]\n        # out.append(assign)\n\n    def _run_call_rebalance_array(self, lhs, assign, args):\n        out = [assign]\n        if not self._is_1D_Var_arr(args[0].name):\n            if self._is_1D_arr(args[0].name):\n                in_1d_arr = args[0].name\n                self._array_starts[lhs] = self._array_starts[in_1d_arr]\n                self._array_counts[lhs] = self._array_counts[in_1d_arr]\n                self._array_sizes[lhs] = self._array_sizes[in_1d_arr]\n            else:\n                warnings.warn(""array {} is not 1D_Block_Var"".format(args[0].name))\n            return out\n\n        arr = args[0]\n        ndim = self.state.typemap[arr.name].ndim\n        out = self._gen_1D_Var_len(arr)\n        total_length = out[-1].target\n        div_nodes, start_var, count_var = self._gen_1D_div(\n            total_length, arr.scope, arr.loc, ""$rebalance"", ""get_node_portion"",\n            distributed_api.get_node_portion)\n        out += div_nodes\n\n        # XXX: get sizes in lower dimensions\n        self._array_starts[lhs] = [-1] * ndim\n        self._array_counts[lhs] = [-1] * ndim\n        self._array_sizes[lhs] = [-1] * ndim\n        self._array_starts[lhs][0] = start_var\n        self._array_counts[lhs][0] = count_var\n        self._array_sizes[lhs][0] = total_length\n\n        def f(arr, count):  # pragma: no cover\n            b_arr = sdc.distributed_api.rebalance_array_parallel(arr, count)\n\n        f_block = compile_to_numba_ir(f, {\'sdc\': sdc}, self.state.typingctx,\n                                      (self.state.typemap[arr.name], types.intp),\n                                      self.state.typemap, self.state.calltypes).blocks.popitem()[1]\n        replace_arg_nodes(f_block, [arr, count_var])\n        out += f_block.body[:-3]\n        out[-1].target = assign.target\n        return out\n\n    def _run_call_np_dot(self, lhs, assign, args):\n        out = [assign]\n        arg0 = args[0].name\n        arg1 = args[1].name\n        ndim0 = self.state.typemap[arg0].ndim\n        ndim1 = self.state.typemap[arg1].ndim\n        t0 = arg0 in self._T_arrs\n        t1 = arg1 in self._T_arrs\n\n        # reduction across dataset\n        if self._is_1D_arr(arg0) and self._is_1D_arr(arg1):\n            dprint(""run dot dist reduce:"", arg0, arg1)\n            reduce_op = Reduce_Type.Sum\n            reduce_var = assign.target\n            out += self._gen_reduce(reduce_var, reduce_op, reduce_var.scope,\n                                    reduce_var.loc)\n\n        # assign starts/counts/sizes data structures for output array\n        if ndim0 == 2 and ndim1 == 1 and not t0 and self._is_1D_arr(arg0):\n            # special case were arg1 vector is treated as column vector\n            # samples dot weights: np.dot(X,w)\n            # output is 1D array same size as dim 0 of X\n            assert self.state.typemap[lhs].ndim == 1\n            assert self._is_1D_arr(lhs)\n            self._array_starts[lhs] = [self._array_starts[arg0][0]]\n            self._array_counts[lhs] = [self._array_counts[arg0][0]]\n            self._array_sizes[lhs] = [self._array_sizes[arg0][0]]\n            dprint(""run dot case 1 Xw:"", arg0, arg1)\n        if ndim0 == 2 and ndim1 == 2 and not t0 and not t1:\n            # samples dot weights: np.dot(X,w)\n            assert self._is_1D_arr(lhs)\n            # first dimension is same as X\n            # second dimension not needed\n            self._array_starts[lhs] = [self._array_starts[arg0][0], -1]\n            self._array_counts[lhs] = [self._array_counts[arg0][0], -1]\n            self._array_sizes[lhs] = [self._array_sizes[arg0][0], -1]\n            dprint(""run dot case 4 Xw:"", arg0, arg1)\n\n        return out\n\n    def _run_alloc(self, size_var, lhs, scope, loc):\n        """""" divides array sizes and assign its sizes/starts/counts attributes\n        returns generated nodes and the new size variable to enable update of\n        the alloc call.\n        """"""\n        out = []\n        new_size_var = None\n\n        # size is single int var\n        if isinstance(size_var, ir.Var) and isinstance(self.state.typemap[size_var.name], types.Integer):\n            self._array_sizes[lhs] = [size_var]\n            out, start_var, end_var = self._gen_1D_div(size_var, scope, loc,\n                                                       ""$alloc"", ""get_node_portion"", distributed_api.get_node_portion)\n            self._array_starts[lhs] = [start_var]\n            self._array_counts[lhs] = [end_var]\n            new_size_var = end_var\n            return out, new_size_var\n\n        # tuple variable of ints\n        if isinstance(size_var, ir.Var):\n            # see if size_var is a 1D array\'s shape\n            # it is already the local size, no need to transform\n            var_def = guard(get_definition, self.state.func_ir, size_var)\n            oned_varnames = set(v for v in self._dist_analysis.array_dists\n                                if self._dist_analysis.array_dists[v] == Distribution.OneD)\n            if (isinstance(var_def, ir.Expr) and var_def.op == \'getattr\'\n                    and var_def.attr == \'shape\' and var_def.value.name in oned_varnames):\n                prev_arr = var_def.value.name\n                self._array_starts[lhs] = self._array_starts[prev_arr]\n                self._array_counts[lhs] = self._array_counts[prev_arr]\n                self._array_sizes[lhs] = self._array_sizes[prev_arr]\n                return out, size_var\n\n            # size should be either int or tuple of ints\n            #assert size_var.name in self._tuple_table\n            # self._tuple_table[size_var.name]\n            size_list = self._get_tuple_varlist(size_var, out)\n            size_list = [ir_utils.convert_size_to_var(s, self.state.typemap, scope, loc, out)\n                         for s in size_list]\n        # tuple of int vars\n        else:\n            assert isinstance(size_var, (tuple, list))\n            size_list = list(size_var)\n\n        self._array_sizes[lhs] = size_list\n        gen_nodes, start_var, end_var = self._gen_1D_div(size_list[0], scope, loc,\n                                                         ""$alloc"", ""get_node_portion"", distributed_api.get_node_portion)\n        out += gen_nodes\n        ndims = len(size_list)\n        new_size_list = copy.copy(size_list)\n        new_size_list[0] = end_var\n        tuple_var = ir.Var(scope, mk_unique_var(""$tuple_var""), loc)\n        self.state.typemap[tuple_var.name] = types.containers.UniTuple(\n            types.intp, ndims)\n        tuple_call = ir.Expr.build_tuple(new_size_list, loc)\n        tuple_assign = ir.Assign(tuple_call, tuple_var, loc)\n        out.append(tuple_assign)\n        self.state.func_ir._definitions[tuple_var.name] = [tuple_call]\n        self._array_starts[lhs] = [self._set0_var] * ndims\n        self._array_starts[lhs][0] = start_var\n        self._array_counts[lhs] = new_size_list\n        new_size_var = tuple_var\n        return out, new_size_var\n\n    def _fix_1D_Var_alloc(self, size_var, lhs, scope, loc):\n        """""" OneD_Var allocs use sizes of other OneD_var variables,\n        so find the local size of those variables (since we transform\n        to use global size)\n        """"""\n        out = []\n        new_size_var = None\n\n        # size is single int var\n        if isinstance(size_var, ir.Var) and isinstance(self.state.typemap[size_var.name], types.Integer):\n            # array could be allocated inside 1D_Var nodes like sort\n            if size_var.name not in self.oneDVar_len_vars:\n                return [], size_var\n            # assert size_var.name in self.oneDVar_len_vars, ""invalid 1DVar alloc""\n            arr_var = self.oneDVar_len_vars[size_var.name]\n\n            def f(oneD_var_arr):  # pragma: no cover\n                arr_len = len(oneD_var_arr)\n            f_block = compile_to_numba_ir(f, {\'sdc\': sdc}, self.state.typingctx,\n                                          (self.state.typemap[arr_var.name],),\n                                          self.state.typemap, self.state.calltypes).blocks.popitem()[1]\n            replace_arg_nodes(f_block, [arr_var])\n            out = f_block.body[:-3]  # remove none return\n            new_size_var = out[-1].target\n            return out, new_size_var\n\n        # tuple variable of ints\n        if isinstance(size_var, ir.Var):\n            # see if size_var is a 1D_Var array\'s shape\n            # it is already the local size, no need to transform\n            var_def = guard(get_definition, self.state.func_ir, size_var)\n            oned_var_varnames = set(v for v in self._dist_analysis.array_dists\n                                    if self._dist_analysis.array_dists[v] == Distribution.OneD_Var)\n            if (isinstance(var_def, ir.Expr) and var_def.op == \'getattr\'\n                    and var_def.attr == \'shape\' and var_def.value.name in oned_var_varnames):\n                return out, size_var\n            # size should be either int or tuple of ints\n            #assert size_var.name in self._tuple_table\n            # self._tuple_table[size_var.name]\n            size_list = self._get_tuple_varlist(size_var, out)\n            size_list = [ir_utils.convert_size_to_var(s, self.state.typemap, scope, loc, out)\n                         for s in size_list]\n        # tuple of int vars\n        else:\n            assert isinstance(size_var, (tuple, list))\n            size_list = list(size_var)\n\n        arr_var = self.oneDVar_len_vars[size_list[0].name]\n\n        def f(oneD_var_arr):  # pragma: no cover\n            arr_len = len(oneD_var_arr)\n        f_block = compile_to_numba_ir(f, {\'sdc\': sdc}, self.state.typingctx,\n                                      (self.state.typemap[arr_var.name],),\n                                      self.state.typemap, self.state.calltypes).blocks.popitem()[1]\n        replace_arg_nodes(f_block, [arr_var])\n        nodes = f_block.body[:-3]  # remove none return\n        new_size_var = nodes[-1].target\n        out += nodes\n\n        ndims = len(size_list)\n        new_size_list = copy.copy(size_list)\n        new_size_list[0] = new_size_var\n        tuple_var = ir.Var(scope, mk_unique_var(""$tuple_var""), loc)\n        self.state.typemap[tuple_var.name] = types.containers.UniTuple(\n            types.intp, ndims)\n        tuple_call = ir.Expr.build_tuple(new_size_list, loc)\n        tuple_assign = ir.Assign(tuple_call, tuple_var, loc)\n        out.append(tuple_assign)\n        self.state.func_ir._definitions[tuple_var.name] = [tuple_call]\n        return out, tuple_var\n\n    # new_body += self._run_1D_array_shape(\n    #                                inst.target, rhs.value)\n    # def _run_1D_array_shape(self, lhs, arr):\n    #     """"""return shape tuple with global size of 1D/1D_Var arrays\n    #     """"""\n    #     nodes = []\n    #     if self._is_1D_arr(arr.name):\n    #         dim1_size = self._array_sizes[arr.name][0]\n    #     else:\n    #         assert self._is_1D_Var_arr(arr.name)\n    #         nodes += self._gen_1D_Var_len(arr)\n    #         dim1_size = nodes[-1].target\n    #\n    #     ndim = self._get_arr_ndim(arr.name)\n    #\n    #     func_text = ""def f(arr, dim1):\\n""\n    #     func_text += ""    s = (dim1, {})\\n"".format(\n    #         "","".join([""arr.shape[{}]"".format(i) for i in range(1, ndim)]))\n    #     loc_vars = {}\n    #     exec(func_text, {}, loc_vars)\n    #     f = loc_vars[\'f\']\n    #\n    #     f_ir = compile_to_numba_ir(f, {\'np\': np}, self.state.typingctx,\n    #                                (self.state.typemap[arr.name], types.intp),\n    #                                self.state.typemap, self.state.calltypes)\n    #     f_block = f_ir.blocks.popitem()[1]\n    #     replace_arg_nodes(f_block, [arr, dim1_size])\n    #     nodes += f_block.body[:-3]\n    #     nodes[-1].target = lhs\n    #     return nodes\n\n    def _run_array_size(self, lhs, arr):\n        # get total size by multiplying all dimension sizes\n        nodes = []\n        if self._is_1D_arr(arr.name):\n            dim1_size = self._array_sizes[arr.name][0]\n        else:\n            assert self._is_1D_Var_arr(arr.name)\n            nodes += self._gen_1D_Var_len(arr)\n            dim1_size = nodes[-1].target\n\n        def f(arr, dim1):  # pragma: no cover\n            sizes = np.array(arr.shape)\n            sizes[0] = dim1\n            s = sizes.prod()\n\n        f_ir = compile_to_numba_ir(f, {\'np\': np}, self.state.typingctx,\n                                   (self.state.typemap[arr.name], types.intp),\n                                   self.state.typemap, self.state.calltypes)\n        f_block = f_ir.blocks.popitem()[1]\n        replace_arg_nodes(f_block, [arr, dim1_size])\n        nodes += f_block.body[:-3]\n        nodes[-1].target = lhs\n        return nodes\n\n    def _run_getsetitem(self, arr, index_var, node, full_node):\n        out = [full_node]\n        # 1D_Var arrays need adjustment for 1D_Var parfors as well\n        if ((self._is_1D_arr(arr.name) or (self._is_1D_Var_arr(arr.name) and arr.name in self._array_starts))\n                and ((arr.name, index_var.name) in self._parallel_accesses)):\n            scope = index_var.scope\n            loc = index_var.loc\n            #ndims = self._get_arr_ndim(arr.name)\n            # if ndims==1:\n            # multi-dimensional array could be indexed with 1D index\n            if isinstance(self.state.typemap[index_var.name], types.Integer):\n                sub_nodes = self._get_ind_sub(\n                    index_var, self._array_starts[arr.name][0])\n                out = sub_nodes\n                _set_getsetitem_index(node, sub_nodes[-1].target)\n            else:\n                index_list = guard(find_build_tuple, self.state.func_ir, index_var)\n                assert index_list is not None\n                sub_nodes = self._get_ind_sub(\n                    index_list[0], self._array_starts[arr.name][0])\n                out = sub_nodes\n                new_index_list = copy.copy(index_list)\n                new_index_list[0] = sub_nodes[-1].target\n                tuple_var = ir.Var(scope, mk_unique_var(""$tuple_var""), loc)\n                self.state.typemap[tuple_var.name] = self.state.typemap[index_var.name]\n                tuple_call = ir.Expr.build_tuple(new_index_list, loc)\n                tuple_assign = ir.Assign(tuple_call, tuple_var, loc)\n                out.append(tuple_assign)\n                _set_getsetitem_index(node, tuple_var)\n\n            out.append(full_node)\n\n        elif self._is_1D_arr(arr.name) and isinstance(node, (ir.StaticSetItem, ir.SetItem)):\n            is_multi_dim = False\n            # we only consider 1st dimension for multi-dim arrays\n            inds = guard(find_build_tuple, self.state.func_ir, index_var)\n            if inds is not None:\n                index_var = inds[0]\n                is_multi_dim = True\n\n            # no need for transformation for whole slices\n            if guard(is_whole_slice, self.state.typemap, self.state.func_ir, index_var):\n                return out\n\n            # TODO: support multi-dim slice setitem like X[a:b, c:d]\n            assert not is_multi_dim\n            start = self._array_starts[arr.name][0]\n            count = self._array_counts[arr.name][0]\n\n            if isinstance(self.state.typemap[index_var.name], types.Integer):\n                def f(A, val, index, chunk_start, chunk_count):  # pragma: no cover\n                    sdc.distributed_lower._set_if_in_range(\n                        A, val, index, chunk_start, chunk_count)\n\n                return self._replace_func(\n                    f, [arr, node.value, index_var, start, count])\n\n            assert isinstance(self.state.typemap[index_var.name],\n                              types.misc.SliceType), ""slice index expected""\n\n            # convert setitem with global range to setitem with local range\n            # that overlaps with the local array chunk\n            def f(A, val, start, stop, chunk_start, chunk_count):  # pragma: no cover\n                loc_start, loc_stop = sdc.distributed_lower._get_local_range(\n                    start, stop, chunk_start, chunk_count)\n                A[loc_start:loc_stop] = val\n\n            slice_call = get_definition(self.state.func_ir, index_var)\n            slice_start = slice_call.args[0]\n            slice_stop = slice_call.args[1]\n            return self._replace_func(\n                f, [arr, node.value, slice_start, slice_stop, start, count])\n            # print_node = ir.Print([start_var, end_var], None, loc)\n            # self.state.calltypes[print_node] = signature(types.none, types.int64, types.int64)\n            # out.append(print_node)\n            #\n            # setitem_attr_var = ir.Var(scope, mk_unique_var(""$setitem_attr""), loc)\n            # setitem_attr_call = ir.Expr.getattr(self._g_dist_var, ""dist_setitem"", loc)\n            # self.state.typemap[setitem_attr_var.name] = get_global_func_typ(\n            #                                 distributed_api.dist_setitem)\n            # setitem_assign = ir.Assign(setitem_attr_call, setitem_attr_var, loc)\n            # out = [setitem_assign]\n            # setitem_call = ir.Expr.call(setitem_attr_var,\n            #                     [arr, index_var, node.value, start, count], (), loc)\n            # self.state.calltypes[setitem_call] = self.state.typemap[setitem_attr_var.name].get_call_type(\n            #     self.state.typingctx, [self.state.typemap[arr.name],\n            #     self.state.typemap[index_var.name], self.state.typemap[node.value.name],\n            #     types.intp, types.intp], {})\n            # err_var = ir.Var(scope, mk_unique_var(""$setitem_err_var""), loc)\n            # self.state.typemap[err_var.name] = types.int32\n            # setitem_assign = ir.Assign(setitem_call, err_var, loc)\n            # out.append(setitem_assign)\n\n        elif self._is_1D_arr(arr.name) and node.op in [\'getitem\', \'static_getitem\']:\n            is_multi_dim = False\n            lhs = full_node.target\n\n            # we only consider 1st dimension for multi-dim arrays\n            inds = guard(find_build_tuple, self.state.func_ir, index_var)\n            if inds is not None:\n                index_var = inds[0]\n                is_multi_dim = True\n\n            arr_def = guard(get_definition, self.state.func_ir, index_var)\n            if isinstance(arr_def, ir.Expr) and arr_def.op == \'call\':\n                fdef = guard(find_callname, self.state.func_ir, arr_def, self.state.typemap)\n                if fdef == (\'permutation\', \'numpy.random\'):\n                    self._array_starts[lhs.name] = self._array_starts[arr.name]\n                    self._array_counts[lhs.name] = self._array_counts[arr.name]\n                    self._array_sizes[lhs.name] = self._array_sizes[arr.name]\n                    out = self._run_permutation_array_index(lhs, arr, index_var)\n\n            # no need for transformation for whole slices\n            if guard(is_whole_slice, self.state.typemap, self.state.func_ir, index_var):\n                # A = X[:,3]\n                self._array_starts[lhs.name] = [self._array_starts[arr.name][0]]\n                self._array_counts[lhs.name] = [self._array_counts[arr.name][0]]\n                self._array_sizes[lhs.name] = [self._array_sizes[arr.name][0]]\n\n            # strided whole slice\n            # e.g. A = X[::2,5]\n            elif guard(is_whole_slice, self.state.typemap, self.state.func_ir, index_var, accept_stride=True):\n                # FIXME: we use rebalance array to handle the output array\n                # TODO: convert to neighbor exchange\n                # on each processor, the slice has to start from an offset:\n                # |step-(start%step)|\n                in_arr = full_node.value.value\n                start = self._array_starts[in_arr.name][0]\n                step = get_slice_step(self.state.typemap, self.state.func_ir, index_var)\n\n                def f(A, start, step):\n                    offset = abs(step - (start % step)) % step\n                    B = A[offset::step]\n\n                f_block = compile_to_numba_ir(f, {}, self.state.typingctx,\n                                              (self.state.typemap[in_arr.name], types.intp, types.intp),\n                                              self.state.typemap, self.state.calltypes).blocks.popitem()[1]\n                replace_arg_nodes(f_block, [in_arr, start, step])\n                out = f_block.body[:-3]  # remove none return\n                imb_arr = out[-1].target\n\n                # call rebalance\n                self._dist_analysis.array_dists[imb_arr.name] = Distribution.OneD_Var\n                out += self._run_call_rebalance_array(lhs.name, full_node, [imb_arr])\n                out[-1].target = lhs\n\n            elif self._is_REP(lhs.name) and guard(\n                    is_const_slice, self.state.typemap, self.state.func_ir, index_var):\n                # cases like S.head()\n                # bcast if all in rank 0, otherwise gatherv\n                in_arr = full_node.value.value\n                start = self._array_starts[in_arr.name][0]\n                count = self._array_counts[in_arr.name][0]\n                return self._replace_func(\n                    lambda arr, slice_index, start, count: sdc.distributed_api.const_slice_getitem(\n                        arr, slice_index, start, count), [in_arr, index_var, start, count])\n\n        return out\n\n    def _run_parfor(self, parfor, namevar_table):\n        # stencil_accesses, neighborhood = get_stencil_accesses(\n        #     parfor, self.state.typemap)\n\n        # Thread and 1D parfors turn to gufunc in multithread mode\n        if (sdc.multithread_mode\n                and self._dist_analysis.parfor_dists[parfor.id]\n                != Distribution.REP):\n            parfor.no_sequential_lowering = True\n\n        if self._dist_analysis.parfor_dists[parfor.id] == Distribution.OneD_Var:\n            return self._run_parfor_1D_Var(parfor, namevar_table)\n\n        if self._dist_analysis.parfor_dists[parfor.id] != Distribution.OneD:\n            if debug_prints():  # pragma: no cover\n                print(""parfor "" + str(parfor.id) + "" not parallelized."")\n            return [parfor]\n\n        scope = parfor.init_block.scope\n        loc = parfor.init_block.loc\n        range_size = parfor.loop_nests[0].stop\n        out = []\n\n        # return range to original size of array\n        # if stencil_accesses:\n        #     #right_length = neighborhood[1][0]\n        #     left_length, right_length = self._get_stencil_border_length(\n        #         neighborhood)\n        #     if right_length:\n        #         new_range_size = ir.Var(\n        #             scope, mk_unique_var(""new_range_size""), loc)\n        #         self.state.typemap[new_range_size.name] = types.intp\n        #         index_const = ir.Var(scope, mk_unique_var(""index_const""), loc)\n        #         self.state.typemap[index_const.name] = types.intp\n        #         out.append(\n        #             ir.Assign(ir.Const(right_length, loc), index_const, loc))\n        #         calc_call = ir.Expr.binop(\'+\', range_size, index_const, loc)\n        #         self.state.calltypes[calc_call] = ir_utils.find_op_typ(\'+\',\n        #                                                          [types.intp, types.intp])\n        #         out.append(ir.Assign(calc_call, new_range_size, loc))\n        #         range_size = new_range_size\n\n        div_nodes, start_var, end_var = self._gen_1D_div(range_size, scope, loc,\n                                                         ""$loop"", ""get_end"", distributed_api.get_end)\n        out += div_nodes\n        # print_node = ir.Print([start_var, end_var, range_size], None, loc)\n        # self.state.calltypes[print_node] = signature(types.none, types.int64, types.int64, types.intp)\n        # out.append(print_node)\n\n        parfor.loop_nests[0].start = start_var\n        parfor.loop_nests[0].stop = end_var\n\n        # if stencil_accesses:\n        #     # TODO assuming single array in stencil\n        #     arr_set = set(stencil_accesses.values())\n        #     arr = arr_set.pop()\n        #     assert not arr_set  # only one array\n        #     self._run_parfor_stencil(parfor, out, start_var, end_var,\n        #                              neighborhood, namevar_table[arr])\n        # else:\n        #     out.append(parfor)\n        out.append(parfor)\n\n        init_reduce_nodes, reduce_nodes = self._gen_parfor_reductions(\n            parfor, namevar_table)\n        parfor.init_block.body += init_reduce_nodes\n        out += reduce_nodes\n        return out\n\n    def _run_parfor_1D_Var(self, parfor, namevar_table):\n        # recover range of 1DVar parfors coming from converted 1DVar array len()\n        prepend = []\n        for l in parfor.loop_nests:\n            if l.stop.name in self.oneDVar_len_vars:\n                arr_var = self.oneDVar_len_vars[l.stop.name]\n\n                def f(A):  # pragma: no cover\n                    arr_len = len(A)\n                f_block = compile_to_numba_ir(f, {\'sdc\': sdc}, self.state.typingctx,\n                                              (self.state.typemap[arr_var.name],),\n                                              self.state.typemap, self.state.calltypes).blocks.popitem()[1]\n                replace_arg_nodes(f_block, [arr_var])\n                nodes = f_block.body[:-3]  # remove none return\n                l.stop = nodes[-1].target\n                prepend += nodes\n\n        # see if parfor index is used in compute other than array access\n        # (e.g. argmin)\n        l_nest = parfor.loop_nests[0]\n        ind_varname = l_nest.index_variable.name\n        ind_used = False\n        for block in parfor.loop_body.values():\n            for stmt in block.body:\n                if not is_get_setitem(stmt) and ind_varname in (v.name for v in stmt.list_vars()):\n                    ind_used = True\n                    dprint(""index of 1D_Var pafor {} used in {}"".format(\n                        parfor.id, stmt))\n                break\n\n        # fix parfor start and stop bounds using ex_scan on ranges\n        if ind_used:\n            scope = l_nest.index_variable.scope\n            loc = l_nest.index_variable.loc\n            if isinstance(l_nest.start, int):\n                start_var = ir.Var(scope, mk_unique_var(""loop_start""), loc)\n                self.state.typemap[start_var.name] = types.intp\n                prepend.append(ir.Assign(\n                    ir.Const(l_nest.start, loc), start_var, loc))\n                l_nest.start = start_var\n\n            def _fix_ind_bounds(start, stop):\n                prefix = sdc.distributed_api.dist_exscan(stop - start)\n                # rank = sdc.distributed_api.get_rank()\n                # print(rank, prefix, start, stop)\n                return start + prefix, stop + prefix\n\n            f_block = compile_to_numba_ir(_fix_ind_bounds, {\'sdc\': sdc},\n                                          self.state.typingctx, (types.intp, types.intp), self.state.typemap,\n                                          self.state.calltypes).blocks.popitem()[1]\n            replace_arg_nodes(f_block, [l_nest.start, l_nest.stop])\n            nodes = f_block.body[:-2]\n            ret_var = nodes[-1].target\n            gen_getitem(l_nest.start, ret_var, 0, self.state.calltypes, nodes)\n            gen_getitem(l_nest.stop, ret_var, 1, self.state.calltypes, nodes)\n            prepend += nodes\n\n            array_accesses = ir_utils.get_array_accesses(parfor.loop_body)\n            for (arr, index) in array_accesses:\n                if self._index_has_par_index(index, ind_varname):\n                    self._array_starts[arr] = [l_nest.start]\n\n        init_reduce_nodes, reduce_nodes = self._gen_parfor_reductions(\n            parfor, namevar_table)\n        parfor.init_block.body += init_reduce_nodes\n        out = prepend + [parfor] + reduce_nodes\n        return out\n\n    def _run_arg(self, assign):\n        rhs = assign.value\n        out = [assign]\n\n        if rhs.name not in self.state.metadata[\'distributed\']:\n            return None\n\n        arr = assign.target\n        typ = self.state.typemap[arr.name]\n        if is_array_container(self.state.typemap, arr.name):\n            return None\n\n        # TODO: comprehensive support for Series vars\n        from sdc.hiframes.pd_series_ext import SeriesType\n        if isinstance(typ, (SeriesType, sdc.hiframes.pd_dataframe_ext.DataFrameType)):\n            return None\n\n        # gen len() using 1D_Var reduce approach.\n        # TODO: refactor to avoid reduction\n        ndim = self.state.typemap[arr.name].ndim\n        out += self._gen_1D_Var_len(arr)\n        total_length = out[-1].target\n        div_nodes, start_var, count_var = self._gen_1D_div(\n            total_length, arr.scope, arr.loc, ""$input"", ""get_node_portion"",\n            distributed_api.get_node_portion)\n        out += div_nodes\n\n        # XXX: get sizes in lower dimensions\n        self._array_starts[arr.name] = [-1] * ndim\n        self._array_counts[arr.name] = [-1] * ndim\n        self._array_sizes[arr.name] = [-1] * ndim\n        self._array_starts[arr.name][0] = start_var\n        self._array_counts[arr.name][0] = count_var\n        self._array_sizes[arr.name][0] = total_length\n        return out\n\n    def _index_has_par_index(self, index, other_index):\n        if index == other_index:\n            return True\n        # multi-dim case\n        tup_list = guard(find_build_tuple, self.state.func_ir, index)\n        if tup_list is not None:\n            index_tuple = [var.name for var in tup_list]\n            if index_tuple[0] == index:\n                return True\n        return False\n\n    def _gen_parfor_reductions(self, parfor, namevar_table):\n        scope = parfor.init_block.scope\n        loc = parfor.init_block.loc\n        pre = []\n        out = []\n        _, reductions = get_parfor_reductions(\n            self.state.func_ir, parfor, parfor.params, self.state.calltypes)\n\n        for reduce_varname, (init_val, reduce_nodes, _) in reductions.items():\n            reduce_op = guard(self._get_reduce_op, reduce_nodes)\n            # TODO: initialize reduction vars (arrays)\n            reduce_var = namevar_table[reduce_varname]\n            pre += self._gen_init_reduce(reduce_var, reduce_op)\n            out += self._gen_reduce(reduce_var, reduce_op, scope, loc)\n\n        return pre, out\n\n    # def _get_var_const_val(self, var):\n    #     if isinstance(var, int):\n    #         return var\n    #     node = guard(get_definition, self.state.func_ir, var)\n    #     if isinstance(node, ir.Const):\n    #         return node.value\n    #     if isinstance(node, ir.Expr):\n    #         if node.op == \'unary\' and node.fn == \'-\':\n    #             return -self._get_var_const_val(node.value)\n    #         if node.op == \'binop\':\n    #             lhs = self._get_var_const_val(node.lhs)\n    #             rhs = self._get_var_const_val(node.rhs)\n    #             if node.fn == \'+\':\n    #                 return lhs + rhs\n    #             if node.fn == \'-\':\n    #                 return lhs - rhs\n    #             if node.fn == \'//\':\n    #                 return lhs // rhs\n    #     return None\n\n    def _gen_1D_div(self, size_var, scope, loc, prefix, end_call_name, end_call):\n        div_nodes = []\n        if isinstance(size_var, int):\n            new_size_var = ir.Var(\n                scope, mk_unique_var(prefix + ""_size_var""), loc)\n            self.state.typemap[new_size_var.name] = types.int64\n            size_assign = ir.Assign(ir.Const(size_var, loc), new_size_var, loc)\n            div_nodes.append(size_assign)\n            size_var = new_size_var\n\n        # attr call: start_attr = getattr(g_dist_var, get_start)\n        start_attr_call = ir.Expr.getattr(self._g_dist_var, ""get_start"", loc)\n        start_attr_var = ir.Var(scope, mk_unique_var(""$get_start_attr""), loc)\n        self.state.typemap[start_attr_var.name] = get_global_func_typ(\n            distributed_api.get_start)\n        start_attr_assign = ir.Assign(start_attr_call, start_attr_var, loc)\n\n        # start_var = get_start(size, rank, pes)\n        start_var = ir.Var(scope, mk_unique_var(prefix + ""_start_var""), loc)\n        self.state.typemap[start_var.name] = types.int64\n        start_expr = ir.Expr.call(start_attr_var, [size_var,\n                                                   self._size_var, self._rank_var], (), loc)\n        self.state.calltypes[start_expr] = self.state.typemap[start_attr_var.name].get_call_type(\n            self.state.typingctx, [types.int64, types.int32, types.int32], {})\n        start_assign = ir.Assign(start_expr, start_var, loc)\n\n        # attr call: end_attr = getattr(g_dist_var, get_end)\n        end_attr_call = ir.Expr.getattr(self._g_dist_var, end_call_name, loc)\n        end_attr_var = ir.Var(scope, mk_unique_var(""$get_end_attr""), loc)\n        self.state.typemap[end_attr_var.name] = get_global_func_typ(end_call)\n        end_attr_assign = ir.Assign(end_attr_call, end_attr_var, loc)\n\n        end_var = ir.Var(scope, mk_unique_var(prefix + ""_end_var""), loc)\n        self.state.typemap[end_var.name] = types.int64\n        end_expr = ir.Expr.call(end_attr_var, [size_var, self._size_var, self._rank_var], (), loc)\n        self.state.calltypes[end_expr] = self.state.typemap[end_attr_var.name].get_call_type(\n            self.state.typingctx, [types.int64, types.int32, types.int32], {})\n        end_assign = ir.Assign(end_expr, end_var, loc)\n        div_nodes += [start_attr_assign, start_assign, end_attr_assign, end_assign]\n        return div_nodes, start_var, end_var\n\n    def _get_ind_sub(self, ind_var, start_var):\n        if (isinstance(ind_var, slice)\n                or isinstance(self.state.typemap[ind_var.name],\n                              types.misc.SliceType)):\n            return self._get_ind_sub_slice(ind_var, start_var)\n        # gen sub\n        f_ir = compile_to_numba_ir(lambda ind, start: ind - start, {}, self.state.typingctx,\n                                   (types.intp, types.intp), self.state.typemap, self.state.calltypes)\n        block = f_ir.blocks.popitem()[1]\n        replace_arg_nodes(block, [ind_var, start_var])\n        return block.body[:-2]\n\n    def _get_ind_sub_slice(self, slice_var, offset_var):\n        if isinstance(slice_var, slice):\n            f_text = """"""def f(offset):\n                return slice({} - offset, {} - offset)\n            """""".format(slice_var.start, slice_var.stop)\n            loc = {}\n            exec(f_text, {}, loc)\n            f = loc[\'f\']\n            args = [offset_var]\n            arg_typs = (types.intp,)\n        else:\n            def f(old_slice, offset):  # pragma: no cover\n                return slice(old_slice.start - offset, old_slice.stop - offset)\n            args = [slice_var, offset_var]\n            slice_type = self.state.typemap[slice_var.name]\n            arg_typs = (slice_type, types.intp,)\n        _globals = self.state.func_ir.func_id.func.__globals__\n        f_ir = compile_to_numba_ir(f, _globals, self.state.typingctx, arg_typs,\n                                   self.state.typemap, self.state.calltypes)\n        _, block = f_ir.blocks.popitem()\n        replace_arg_nodes(block, args)\n        return block.body[:-2]  # ignore return nodes\n\n    def _dist_prints(self, blocks):\n        new_blocks = {}\n        for (block_label, block) in blocks.items():\n            scope = block.scope\n            i = _find_first_print(block.body)\n            while i != -1:\n                inst = block.body[i]\n                loc = inst.loc\n                # split block across print\n                prev_block = ir.Block(scope, loc)\n                new_blocks[block_label] = prev_block\n                block_label = ir_utils.next_label()\n                print_label = ir_utils.next_label()\n\n                prev_block.body = block.body[:i]\n                rank_comp_var = ir.Var(scope, mk_unique_var(""$rank_comp""), loc)\n                self.state.typemap[rank_comp_var.name] = types.boolean\n                comp_expr = ir.Expr.binop(operator.eq, self._rank_var, self._set0_var, loc)\n                expr_typ = self.state.typingctx.resolve_function_type(operator.eq, (types.int32, types.int64), {})\n                #expr_typ = find_op_typ(operator.eq, [types.int32, types.int64])\n                self.state.calltypes[comp_expr] = expr_typ\n                comp_assign = ir.Assign(comp_expr, rank_comp_var, loc)\n                prev_block.body.append(comp_assign)\n                print_branch = ir.Branch(rank_comp_var, print_label, block_label, loc)\n                prev_block.body.append(print_branch)\n\n                print_block = ir.Block(scope, loc)\n                print_block.body.append(inst)\n                print_block.body.append(ir.Jump(block_label, loc))\n                new_blocks[print_label] = print_block\n\n                block.body = block.body[i + 1:]\n                i = _find_first_print(block.body)\n            new_blocks[block_label] = block\n        return new_blocks\n\n    def _gen_barrier(self):\n        def f():  # pragma: no cover\n            return sdc.distributed_api.barrier()\n\n        f_blocks = compile_to_numba_ir(f, {\'sdc\': sdc}, self.state.typingctx, {},\n                                       self.state.typemap, self.state.calltypes).blocks\n        block = f_blocks[min(f_blocks.keys())]\n        return block.body[:-2]  # remove return\n\n    def _gen_reduce(self, reduce_var, reduce_op, scope, loc):\n        op_var = ir.Var(scope, mk_unique_var(""$reduce_op""), loc)\n        self.state.typemap[op_var.name] = types.int32\n        op_assign = ir.Assign(ir.Const(reduce_op.value, loc), op_var, loc)\n\n        def f(val, op):  # pragma: no cover\n            sdc.distributed_api.dist_reduce(val, op)\n\n        f_ir = compile_to_numba_ir(f,\n                                   {\'sdc\': sdc},\n                                   self.state.typingctx,\n                                   (self.state.typemap[reduce_var.name], types.int32),\n                                   self.state.typemap,\n                                   self.state.calltypes)\n        _, block = f_ir.blocks.popitem()\n\n        replace_arg_nodes(block, [reduce_var, op_var])\n        dist_reduce_nodes = [op_assign] + block.body[:-3]\n        dist_reduce_nodes[-1].target = reduce_var\n        return dist_reduce_nodes\n\n    def _get_reduce_op(self, reduce_nodes):\n        require(len(reduce_nodes) == 2)\n        require(isinstance(reduce_nodes[0], ir.Assign))\n        require(isinstance(reduce_nodes[1], ir.Assign))\n        require(isinstance(reduce_nodes[0].value, ir.Expr))\n        require(isinstance(reduce_nodes[1].value, ir.Var))\n        rhs = reduce_nodes[0].value\n\n        if rhs.op == \'inplace_binop\':\n            if rhs.fn in (\'+=\', operator.iadd):\n                return Reduce_Type.Sum\n            if rhs.fn in (\'|=\', operator.ior):\n                return Reduce_Type.Or\n            if rhs.fn in (\'*=\', operator.imul):\n                return Reduce_Type.Prod\n\n        if rhs.op == \'call\':\n            func = find_callname(self.state.func_ir, rhs, self.state.typemap)\n            if func == (\'min\', \'builtins\'):\n                if isinstance(self.state.typemap[rhs.args[0].name], numba.core.typing.builtins.IndexValueType):\n                    return Reduce_Type.Argmin\n                return Reduce_Type.Min\n            if func == (\'max\', \'builtins\'):\n                if isinstance(self.state.typemap[rhs.args[0].name], numba.core.typing.builtins.IndexValueType):\n                    return Reduce_Type.Argmax\n                return Reduce_Type.Max\n\n        raise GuardException  # pragma: no cover\n\n    def _gen_init_reduce(self, reduce_var, reduce_op):\n        """"""generate code to initialize reduction variables on non-root\n        processors.\n        """"""\n        red_var_typ = self.state.typemap[reduce_var.name]\n        el_typ = red_var_typ\n        if is_np_array(self.state.typemap, reduce_var.name):\n            el_typ = red_var_typ.dtype\n        init_val = None\n        pre_init_val = """"\n\n        if reduce_op in [Reduce_Type.Sum, Reduce_Type.Or]:\n            init_val = str(el_typ(0))\n        if reduce_op == Reduce_Type.Prod:\n            init_val = str(el_typ(1))\n        if reduce_op == Reduce_Type.Min:\n            init_val = ""numba.targets.builtins.get_type_max_value(np.ones(1,dtype=np.{}).dtype)"".format(el_typ)\n        if reduce_op == Reduce_Type.Max:\n            init_val = ""numba.targets.builtins.get_type_min_value(np.ones(1,dtype=np.{}).dtype)"".format(el_typ)\n        if reduce_op in [Reduce_Type.Argmin, Reduce_Type.Argmax]:\n            # don\'t generate initialization for argmin/argmax since they are not\n            # initialized by user and correct initialization is already there\n            return []\n\n        assert init_val is not None\n\n        if is_np_array(self.state.typemap, reduce_var.name):\n            pre_init_val = ""v = np.full_like(s, {}, s.dtype)"".format(init_val)\n            init_val = ""v""\n\n        f_text = ""def f(s):\\n  {}\\n  s = sdc.distributed_lower._root_rank_select(s, {})"".format(pre_init_val, init_val)\n        loc_vars = {}\n        exec(f_text, {\'sdc\': sdc}, loc_vars)\n        f = loc_vars[\'f\']\n\n        f_block = compile_to_numba_ir(f,\n                                      {\'sdc\': sdc, \'numba\': numba, \'np\': np},\n                                      self.state.typingctx,\n                                      (red_var_typ,),\n                                      self.state.typemap,\n                                      self.state.calltypes).blocks.popitem()[1]\n        replace_arg_nodes(f_block, [reduce_var])\n        nodes = f_block.body[:-3]\n        nodes[-1].target = reduce_var\n        return nodes\n\n    def _get_tuple_varlist(self, tup_var, out):\n        """""" get the list of variables that hold values in the tuple variable.\n        add node to out if code generation needed.\n        """"""\n        t_list = guard(find_build_tuple, self.state.func_ir, tup_var)\n        if t_list is not None:\n            return t_list\n        assert isinstance(self.state.typemap[tup_var.name], types.UniTuple)\n        ndims = self.state.typemap[tup_var.name].count\n        f_text = ""def f(tup_var):\\n""\n        for i in range(ndims):\n            f_text += ""  val{} = tup_var[{}]\\n"".format(i, i)\n        loc_vars = {}\n        exec(f_text, {}, loc_vars)\n        f = loc_vars[\'f\']\n        f_block = compile_to_numba_ir(f,\n                                      {},\n                                      self.state.typingctx,\n                                      (self.state.typemap[tup_var.name],\n                                       ),\n                                      self.state.typemap,\n                                      self.state.calltypes).blocks.popitem()[1]\n        replace_arg_nodes(f_block, [tup_var])\n        nodes = f_block.body[:-3]\n        vals_list = []\n        for stmt in nodes:\n            assert isinstance(stmt, ir.Assign)\n            rhs = stmt.value\n            assert isinstance(rhs, (ir.Var, ir.Const, ir.Expr))\n            if isinstance(rhs, ir.Expr):\n                assert rhs.op == \'static_getitem\'\n                vals_list.append(stmt.target)\n        out += nodes\n        return vals_list\n\n    def _get_arg(self, f_name, args, kws, arg_no, arg_name, default=None, err_msg=None):\n        arg = None\n        if len(args) > arg_no:\n            arg = args[arg_no]\n        elif arg_name in kws:\n            arg = kws[arg_name]\n\n        if arg is None:\n            if default is not None:\n                return default\n            if err_msg is None:\n                err_msg = ""{} requires \'{}\' argument"".format(f_name, arg_name)\n            raise ValueError(err_msg)\n        return arg\n\n    def _replace_func(self, func, args, const=False, pre_nodes=None, extra_globals=None):\n        glbls = {\'numba\': numba, \'np\': np, \'sdc\': sdc}\n        if extra_globals is not None:\n            glbls.update(extra_globals)\n        arg_typs = tuple(self.state.typemap[v.name] for v in args)\n        if const:\n            new_args = []\n            for i, arg in enumerate(args):\n                val = guard(find_const, self.state.func_ir, arg)\n                if val:\n                    new_args.append(types.literal(val))\n                else:\n                    new_args.append(arg_typs[i])\n            arg_typs = tuple(new_args)\n        return ReplaceFunc(func, arg_typs, args, glbls, pre_nodes)\n\n    def _get_arr_ndim(self, arrname):\n        if self.state.typemap[arrname] == string_array_type:\n            return 1\n        return self.state.typemap[arrname].ndim\n\n    def _is_1D_arr(self, arr_name):\n        # some arrays like stencil buffers are added after analysis so\n        # they are not in dists list\n        return ((arr_name in self._dist_analysis.array_dists\n                 and self._dist_analysis.array_dists[arr_name] == Distribution.OneD))\n\n    def _is_1D_Var_arr(self, arr_name):\n        # some arrays like stencil buffers are added after analysis so\n        # they are not in dists list\n        return ((arr_name in self._dist_analysis.array_dists\n                 and self._dist_analysis.array_dists[arr_name] == Distribution.OneD_Var))\n\n    def _is_REP(self, arr_name):\n        return ((arr_name not in self._dist_analysis.array_dists\n                 or self._dist_analysis.array_dists[arr_name] == Distribution.REP))\n\n\ndef _find_first_print(body):\n    """""" This function finds the first print of something """"""\n    for (i, inst) in enumerate(body):\n        if isinstance(inst, ir.Print):\n            return i\n    return -1\n\n\ndef _set_getsetitem_index(node, new_ind):\n    if ((isinstance(node, ir.Expr) and node.op == \'static_getitem\') or isinstance(node, ir.StaticSetItem)):\n        node.index_var = new_ind\n        node.index = None\n        return\n\n    assert ((isinstance(node, ir.Expr) and node.op == \'getitem\') or isinstance(node, ir.SetItem))\n    node.index = new_ind\n\n\ndef dprint(*s):  # pragma: no cover\n    if debug_prints():\n        print(*s)\n'"
sdc/distributed_analysis.py,11,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\nfrom __future__ import print_function, division, absolute_import\nfrom collections import namedtuple\nimport copy\nimport warnings\n\nimport numba\nfrom numba import types\nfrom numba.core import ir, ir_utils\nfrom numba.core.ir_utils import (find_topo_order, guard, get_definition, require,\n                            find_callname, mk_unique_var, compile_to_numba_ir,\n                            replace_arg_nodes, build_definitions,\n                            find_build_sequence, find_const)\nfrom numba.parfors.parfor import Parfor\nfrom numba.parfors.parfor import wrap_parfor_blocks, unwrap_parfor_blocks\nfrom numba.core import analysis\n\nimport sdc\nimport sdc.io\nimport sdc.io.np_io\nfrom sdc.hiframes.pd_series_ext import SeriesType\nfrom sdc.utilities.utils import (get_constant, is_alloc_callname,\n                                 is_whole_slice, is_array, is_array_container,\n                                 is_np_array, find_build_tuple, debug_prints,\n                                 is_const_slice)\nfrom sdc.hiframes.pd_dataframe_ext import DataFrameType\nfrom enum import Enum\n\n\nclass Distribution(Enum):\n    REP = 1\n    Thread = 2\n    TwoD = 3\n    OneD_Var = 4\n    OneD = 5\n\n\n_dist_analysis_result = namedtuple(\n    \'dist_analysis_result\', \'array_dists,parfor_dists\')\n\ndistributed_analysis_extensions = {}\nauto_rebalance = False\n\n\nclass DistributedAnalysis(object):\n    """"""Analyze program for distributed transformation""""""\n\n    _extra_call = {}\n\n    @classmethod\n    def add_call_analysis(cls, typ, func, analysis_func):\n        \'\'\'\n        External modules/packages (like daal4py) can register their own call-analysis.\n        Analysis funcs are stored in a dict with keys (typ, funcname)\n        \'\'\'\n        assert (typ, func) not in cls._extra_call\n        cls._extra_call[(typ, func)] = analysis_func\n\n    def __init__(self, func_ir, typemap, calltypes, typingctx, metadata):\n        self.func_ir = func_ir\n        self.typemap = typemap\n        self.calltypes = calltypes\n        self.typingctx = typingctx\n        self.metadata = metadata\n\n    def _init_run(self):\n        self.func_ir._definitions = build_definitions(self.func_ir.blocks)\n        self._parallel_accesses = set()\n        self._T_arrs = set()\n        self.second_pass = False\n        self.in_parallel_parfor = -1\n\n    def run(self):\n        self._init_run()\n        blocks = self.func_ir.blocks\n        array_dists = {}\n        parfor_dists = {}\n        topo_order = find_topo_order(blocks)\n        self._run_analysis(self.func_ir.blocks, topo_order,\n                           array_dists, parfor_dists)\n        self.second_pass = True\n        self._run_analysis(self.func_ir.blocks, topo_order,\n                           array_dists, parfor_dists)\n        # rebalance arrays if necessary\n        if auto_rebalance and Distribution.OneD_Var in array_dists.values():\n            changed = self._rebalance_arrs(array_dists, parfor_dists)\n            if changed:\n                return self.run()\n\n        return _dist_analysis_result(array_dists=array_dists, parfor_dists=parfor_dists)\n\n    def _run_analysis(self, blocks, topo_order, array_dists, parfor_dists):\n        save_array_dists = {}\n        save_parfor_dists = {1: 1}  # dummy value\n        # fixed-point iteration\n        while array_dists != save_array_dists or parfor_dists != save_parfor_dists:\n            save_array_dists = copy.copy(array_dists)\n            save_parfor_dists = copy.copy(parfor_dists)\n            for label in topo_order:\n                self._analyze_block(blocks[label], array_dists, parfor_dists)\n\n    def _analyze_block(self, block, array_dists, parfor_dists):\n        for inst in block.body:\n            if isinstance(inst, ir.Assign):\n                self._analyze_assign(inst, array_dists, parfor_dists)\n            elif isinstance(inst, Parfor):\n                self._analyze_parfor(inst, array_dists, parfor_dists)\n            elif isinstance(inst, (ir.SetItem, ir.StaticSetItem)):\n                self._analyze_setitem(inst, array_dists)\n            # elif isinstance(inst, ir.Print):\n            #     continue\n            elif type(inst) in distributed_analysis_extensions:\n                # let external calls handle stmt if type matches\n                f = distributed_analysis_extensions[type(inst)]\n                f(inst, array_dists)\n            else:\n                self._set_REP(inst.list_vars(), array_dists)\n\n    def _analyze_assign(self, inst, array_dists, parfor_dists):\n        lhs = inst.target.name\n        rhs = inst.value\n        # treat return casts like assignments\n        if isinstance(rhs, ir.Expr) and rhs.op == \'cast\':\n            rhs = rhs.value\n\n        if isinstance(rhs, ir.Var) and (is_array(self.typemap, lhs)\n                                        or isinstance(self.typemap[lhs], (SeriesType, DataFrameType))\n                                        or is_array_container(self.typemap, lhs)):\n            self._meet_array_dists(lhs, rhs.name, array_dists)\n            return\n        elif (is_array(self.typemap, lhs)\n                and isinstance(rhs, ir.Expr)\n                and rhs.op == \'inplace_binop\'):\n            # distributions of all 3 variables should meet (lhs, arg1, arg2)\n            arg1 = rhs.lhs.name\n            arg2 = rhs.rhs.name\n            dist = self._meet_array_dists(arg1, arg2, array_dists)\n            dist = self._meet_array_dists(arg1, lhs, array_dists, dist)\n            self._meet_array_dists(arg1, arg2, array_dists, dist)\n            return\n        elif isinstance(rhs, ir.Expr) and rhs.op in [\'getitem\', \'static_getitem\']:\n            self._analyze_getitem(inst, lhs, rhs, array_dists)\n            return\n        elif isinstance(rhs, ir.Expr) and rhs.op == \'build_tuple\':\n            # parallel arrays can be packed and unpacked from tuples\n            # e.g. boolean array index in test_getitem_multidim\n            return\n        elif (isinstance(rhs, ir.Expr) and rhs.op == \'getattr\' and rhs.attr == \'T\'\n              and is_array(self.typemap, lhs)):\n            # array and its transpose have same distributions\n            arr = rhs.value.name\n            self._meet_array_dists(lhs, arr, array_dists)\n            # keep lhs in table for dot() handling\n            self._T_arrs.add(lhs)\n            return\n        elif (isinstance(rhs, ir.Expr) and rhs.op == \'getattr\'\n                and isinstance(self.typemap[rhs.value.name], DataFrameType)\n                and rhs.attr == \'to_csv\'):\n            return\n        elif (isinstance(rhs, ir.Expr) and rhs.op == \'getattr\'\n                and rhs.attr in [\'shape\', \'ndim\', \'size\', \'strides\', \'dtype\',\n                                 \'itemsize\', \'astype\', \'reshape\', \'ctypes\',\n                                 \'transpose\', \'tofile\', \'copy\']):\n            pass  # X.shape doesn\'t affect X distribution\n        elif isinstance(rhs, ir.Expr) and rhs.op == \'call\':\n            self._analyze_call(lhs, rhs, rhs.func.name, rhs.args, array_dists)\n        # handle for A in arr_container: ...\n        # A = pair_first(iternext(getiter(arr_container)))\n        # TODO: support getitem of container\n        elif isinstance(rhs, ir.Expr) and rhs.op == \'pair_first\' and is_array(self.typemap, lhs):\n            arr_container = guard(_get_pair_first_container, self.func_ir, rhs)\n            if arr_container is not None:\n                self._meet_array_dists(lhs, arr_container.name, array_dists)\n                return\n        elif isinstance(rhs, ir.Expr) and rhs.op in (\'getiter\', \'iternext\'):\n            # analyze array container access in pair_first\n            return\n\n        elif isinstance(rhs, ir.Arg):\n            distributed_key = \'distributed\'\n            threaded_key = \'threaded\'\n\n            if distributed_key not in self.metadata.keys():\n                self.metadata[distributed_key] = {}\n\n            if threaded_key not in self.metadata.keys():\n                self.metadata[threaded_key] = {}\n\n            if rhs.name in self.metadata[distributed_key]:\n                if lhs not in array_dists:\n                    array_dists[lhs] = Distribution.OneD\n\n            elif rhs.name in self.metadata[threaded_key]:\n                if lhs not in array_dists:\n                    array_dists[lhs] = Distribution.Thread\n\n            else:\n                dprint(""replicated input "", rhs.name, lhs)\n                self._set_REP([inst.target], array_dists)\n\n        else:\n            self._set_REP(inst.list_vars(), array_dists)\n        return\n\n    def _analyze_parfor(self, parfor, array_dists, parfor_dists):\n        if parfor.id not in parfor_dists:\n            parfor_dists[parfor.id] = Distribution.OneD\n\n        # analyze init block first to see array definitions\n        self._analyze_block(parfor.init_block, array_dists, parfor_dists)\n        out_dist = Distribution.OneD\n        if self.in_parallel_parfor != -1:\n            out_dist = Distribution.REP\n\n        parfor_arrs = set()  # arrays this parfor accesses in parallel\n        array_accesses = _get_array_accesses(\n            parfor.loop_body, self.func_ir, self.typemap)\n        par_index_var = parfor.loop_nests[0].index_variable.name\n        #stencil_accesses, _ = get_stencil_accesses(parfor, self.typemap)\n        for (arr, index) in array_accesses:\n            # XXX sometimes copy propagation doesn\'t work for parfor indices\n            # so see if the index has a single variable definition and use it\n            # e.g. test_to_numeric\n            ind_def = self.func_ir._definitions[index]\n            if len(ind_def) == 1 and isinstance(ind_def[0], ir.Var):\n                index = ind_def[0].name\n            if index == par_index_var:  # or index in stencil_accesses:\n                parfor_arrs.add(arr)\n                self._parallel_accesses.add((arr, index))\n\n            # multi-dim case\n            tup_list = guard(find_build_tuple, self.func_ir, index)\n            if tup_list is not None:\n                index_tuple = [var.name for var in tup_list]\n                if index_tuple[0] == par_index_var:\n                    parfor_arrs.add(arr)\n                    self._parallel_accesses.add((arr, index))\n                if par_index_var in index_tuple[1:]:\n                    out_dist = Distribution.REP\n            # TODO: check for index dependency\n\n        for arr in parfor_arrs:\n            if arr in array_dists:\n                out_dist = Distribution(\n                    min(out_dist.value, array_dists[arr].value))\n        parfor_dists[parfor.id] = out_dist\n        for arr in parfor_arrs:\n            if arr in array_dists:\n                array_dists[arr] = out_dist\n\n        # TODO: find prange actually coming from user\n        # for pattern in parfor.patterns:\n        #     if pattern[0] == \'prange\' and not self.in_parallel_parfor:\n        #         parfor_dists[parfor.id] = Distribution.OneD\n\n        # run analysis recursively on parfor body\n        if self.second_pass and out_dist in [Distribution.OneD,\n                                             Distribution.OneD_Var]:\n            self.in_parallel_parfor = parfor.id\n        blocks = wrap_parfor_blocks(parfor)\n        for b in blocks.values():\n            self._analyze_block(b, array_dists, parfor_dists)\n        unwrap_parfor_blocks(parfor)\n        if self.in_parallel_parfor == parfor.id:\n            self.in_parallel_parfor = -1\n        return\n\n    def _analyze_call(self, lhs, rhs, func_var, args, array_dists):\n        """"""analyze array distributions in function calls\n        """"""\n        func_name = """"\n        func_mod = """"\n        fdef = guard(find_callname, self.func_ir, rhs, self.typemap)\n        if fdef is None:\n            # check ObjModeLiftedWith, we assume distribution doesn\'t change\n            # blocks of data are passed in, TODO: document\n            func_def = guard(get_definition, self.func_ir, rhs.func)\n            if isinstance(func_def, ir.Const) and isinstance(func_def.value,\n                                                             numba.dispatcher.ObjModeLiftedWith):\n                return\n            warnings.warn(\n                ""function call couldn\'t be found for distributed analysis"")\n            self._analyze_call_set_REP(lhs, args, array_dists, fdef)\n            return\n        else:\n            func_name, func_mod = fdef\n\n        if is_alloc_callname(func_name, func_mod):\n            if lhs not in array_dists:\n                array_dists[lhs] = Distribution.OneD\n            return\n\n        # numpy direct functions\n        if isinstance(func_mod, str) and func_mod == \'numpy\':\n            self._analyze_call_np(lhs, func_name, args, array_dists)\n            return\n\n        # handle array.func calls\n        if isinstance(func_mod, ir.Var) and is_array(self.typemap, func_mod.name):\n            self._analyze_call_array(lhs, func_mod, func_name, args, array_dists)\n            return\n\n        # handle df.func calls\n        if isinstance(func_mod, ir.Var) and isinstance(\n                self.typemap[func_mod.name], DataFrameType):\n            self._analyze_call_df(lhs, func_mod, func_name, args, array_dists)\n            return\n\n        # sdc.distributed_api functions\n        if isinstance(func_mod, str) and func_mod == \'sdc.distributed_api\':\n            self._analyze_call_hpat_dist(lhs, func_name, args, array_dists)\n            return\n\n        # len()\n        if func_name == \'len\' and func_mod in (\'__builtin__\', \'builtins\'):\n            return\n\n        if fdef == (\'quantile\', \'sdc.hiframes.api\'):\n            # quantile doesn\'t affect input\'s distribution\n            return\n\n        if fdef == (\'nunique\', \'sdc.hiframes.api\'):\n            # nunique doesn\'t affect input\'s distribution\n            return\n\n        if fdef == (\'unique\', \'sdc.hiframes.api\'):\n            # doesn\'t affect distribution of input since input can stay 1D\n            if lhs not in array_dists:\n                array_dists[lhs] = Distribution.OneD_Var\n\n            new_dist = Distribution(min(array_dists[lhs].value,\n                                        array_dists[rhs.args[0].name].value))\n            array_dists[lhs] = new_dist\n            return\n\n        if fdef == (\'rolling_fixed\', \'sdc.hiframes.rolling\'):\n            self._meet_array_dists(lhs, rhs.args[0].name, array_dists)\n            return\n\n        if fdef == (\'rolling_variable\', \'sdc.hiframes.rolling\'):\n            # lhs, in_arr, on_arr should have the same distribution\n            new_dist = self._meet_array_dists(lhs, rhs.args[0].name, array_dists)\n            new_dist = self._meet_array_dists(lhs, rhs.args[1].name, array_dists, new_dist)\n            array_dists[rhs.args[0].name] = new_dist\n            return\n\n        if fdef == (\'shift\', \'sdc.hiframes.rolling\'):\n            self._meet_array_dists(lhs, rhs.args[0].name, array_dists)\n            return\n\n        if fdef == (\'pct_change\', \'sdc.hiframes.rolling\'):\n            self._meet_array_dists(lhs, rhs.args[0].name, array_dists)\n            return\n\n        if fdef == (\'nlargest\', \'sdc.hiframes.api\'):\n            # output of nlargest is REP\n            array_dists[lhs] = Distribution.REP\n            return\n\n        if fdef == (\'median\', \'sdc.hiframes.api\'):\n            return\n\n        if fdef == (\'concat\', \'sdc.hiframes.api\'):\n            # hiframes concat is similar to np.concatenate\n            self._analyze_call_np_concatenate(lhs, args, array_dists)\n            return\n\n        if fdef == (\'isna\', \'sdc.hiframes.api\'):\n            return\n\n        if fdef == (\'get_series_name\', \'sdc.hiframes.api\'):\n            return\n\n        # dummy hiframes functions\n        if func_mod == \'sdc.hiframes.api\' and func_name in (\'get_series_data\',\n                                                             \'get_series_index\',\n                                                             \'to_arr_from_series\', \'ts_series_to_arr_typ\',\n                                                             \'to_date_series_type\', \'dummy_unbox_series\',\n                                                             \'parallel_fix_df_array\'):\n            # TODO: support Series type similar to Array\n            self._meet_array_dists(lhs, rhs.args[0].name, array_dists)\n            return\n\n        if fdef == (\'init_series\', \'sdc.hiframes.api\'):\n            # lhs, in_arr, and index should have the same distribution\n            new_dist = self._meet_array_dists(lhs, rhs.args[0].name, array_dists)\n            if len(rhs.args) > 1 and self.typemap[rhs.args[1].name] != types.none:\n                new_dist = self._meet_array_dists(lhs, rhs.args[1].name, array_dists, new_dist)\n                array_dists[rhs.args[0].name] = new_dist\n            return\n\n        if fdef == (\'init_dataframe\', \'sdc.hiframes.pd_dataframe_ext\'):\n            # lhs, data arrays, and index should have the same distribution\n            df_typ = self.typemap[lhs]\n            n_cols = len(df_typ.columns)\n            for i in range(n_cols):\n                new_dist = self._meet_array_dists(lhs, rhs.args[i].name, array_dists)\n            # handle index\n            if len(rhs.args) > n_cols and self.typemap[rhs.args[n_cols].name] != types.none:\n                new_dist = self._meet_array_dists(lhs, rhs.args[n_cols].name, array_dists, new_dist)\n            for i in range(n_cols):\n                array_dists[rhs.args[i].name] = new_dist\n            return\n\n        if fdef == (\'get_dataframe_data\', \'sdc.hiframes.pd_dataframe_ext\'):\n            self._meet_array_dists(lhs, rhs.args[0].name, array_dists)\n            return\n\n        if fdef == (\'compute_split_view\', \'sdc.hiframes.split_impl\'):\n            self._meet_array_dists(lhs, rhs.args[0].name, array_dists)\n            return\n\n        if fdef == (\'get_split_view_index\', \'sdc.hiframes.split_impl\'):\n            # just used in str.get() implementation for now so we know it is\n            # parallel\n            # TODO: handle index similar to getitem to support more cases\n            return\n\n        if fdef == (\'get_split_view_data_ptr\', \'sdc.hiframes.split_impl\'):\n            return\n\n        if fdef == (\'setitem_str_arr_ptr\', \'sdc.str_arr_ext\'):\n            return\n\n        if fdef == (\'num_total_chars\', \'sdc.str_arr_ext\'):\n            return\n\n        if fdef == (\'_series_dropna_str_alloc_impl_inner\', \'sdc.hiframes.series_kernels\'):\n            if lhs not in array_dists:\n                array_dists[lhs] = Distribution.OneD_Var\n            in_dist = array_dists[rhs.args[0].name]\n            out_dist = array_dists[lhs]\n            out_dist = Distribution(min(out_dist.value, in_dist.value))\n            array_dists[lhs] = out_dist\n            # output can cause input REP\n            if out_dist != Distribution.OneD_Var:\n                array_dists[rhs.args[0].name] = out_dist\n            return\n\n        if (fdef == (\'copy_non_null_offsets\', \'sdc.str_arr_ext\')\n                or fdef == (\'copy_data\', \'sdc.str_arr_ext\')):\n            out_arrname = rhs.args[0].name\n            in_arrname = rhs.args[1].name\n            self._meet_array_dists(out_arrname, in_arrname, array_dists)\n            return\n\n        if fdef == (\'str_arr_item_to_numeric\', \'sdc.str_arr_ext\'):\n            out_arrname = rhs.args[0].name\n            in_arrname = rhs.args[2].name\n            self._meet_array_dists(out_arrname, in_arrname, array_dists)\n            return\n\n        # np.fromfile()\n        if fdef == (\'file_read\', \'sdc.io.np_io\'):\n            return\n\n        if sdc.config._has_pyarrow and fdef == (\'read_parquet\', \'sdc.io.parquet_pio\'):\n            return\n\n        if sdc.config._has_pyarrow and fdef == (\'read_parquet_str\', \'sdc.io.parquet_pio\'):\n            # string read creates array in output\n            if lhs not in array_dists:\n                array_dists[lhs] = Distribution.OneD\n            return\n\n        # TODO: make sure assert_equiv is not generated unnecessarily\n        # TODO: fix assert_equiv for np.stack from df.value\n        if fdef == (\'assert_equiv\', \'numba.parfors.parfor.array_analysis\'):\n            return\n\n        # we perform call-analysis from external at the end\n        if isinstance(func_mod, ir.Var):\n            ky = (self.typemap[func_mod.name], func_name)\n            if ky in DistributedAnalysis._extra_call:\n                if DistributedAnalysis._extra_call[ky](lhs, func_mod, *ky, args, array_dists):\n                    return\n\n        # set REP if not found\n        self._analyze_call_set_REP(lhs, args, array_dists, fdef)\n\n    def _analyze_call_np(self, lhs, func_name, args, array_dists):\n        """"""analyze distributions of numpy functions (np.func_name)\n        """"""\n\n        if func_name == \'ascontiguousarray\':\n            self._meet_array_dists(lhs, args[0].name, array_dists)\n            return\n\n        if func_name == \'ravel\':\n            self._meet_array_dists(lhs, args[0].name, array_dists)\n            return\n\n        if func_name == \'concatenate\':\n            self._analyze_call_np_concatenate(lhs, args, array_dists)\n            return\n\n        if func_name == \'array\' and is_array(self.typemap, args[0].name):\n            self._meet_array_dists(lhs, args[0].name, array_dists)\n            return\n\n        # sum over the first axis is distributed, A.sum(0)\n        if func_name == \'sum\' and len(args) == 2:\n            axis_def = guard(get_definition, self.func_ir, args[1])\n            if isinstance(axis_def, ir.Const) and axis_def.value == 0:\n                array_dists[lhs] = Distribution.REP\n                return\n\n        if func_name == \'dot\':\n            self._analyze_call_np_dot(lhs, args, array_dists)\n            return\n\n        # used in df.values\n        if func_name == \'stack\':\n            seq_info = guard(find_build_sequence, self.func_ir, args[0])\n            if seq_info is None:\n                self._analyze_call_set_REP(lhs, args, array_dists, \'np.\' + func_name)\n                return\n            in_arrs, _ = seq_info\n\n            axis = 0\n            # TODO: support kws\n            # if \'axis\' in kws:\n            #     axis = find_const(self.func_ir, kws[\'axis\'])\n            if len(args) > 1:\n                axis = find_const(self.func_ir, args[1])\n\n            # parallel if args are 1D and output is 2D and axis == 1\n            if axis is not None and axis == 1 and self.typemap[lhs].ndim == 2:\n                for v in in_arrs:\n                    self._meet_array_dists(lhs, v.name, array_dists)\n                return\n\n        if (func_name in [\'cumsum\', \'cumprod\', \'empty_like\',\n                          \'zeros_like\', \'ones_like\', \'full_like\', \'copy\']):\n            in_arr = args[0].name\n            self._meet_array_dists(lhs, in_arr, array_dists)\n            return\n\n        # set REP if not found\n        self._analyze_call_set_REP(lhs, args, array_dists, \'np.\' + func_name)\n\n    def _analyze_call_array(self, lhs, arr, func_name, args, array_dists):\n        """"""analyze distributions of array functions (arr.func_name)\n        """"""\n        if func_name == \'transpose\':\n            if len(args) == 0:\n                raise ValueError(""Transpose with no arguments is not""\n                                 "" supported"")\n            in_arr_name = arr.name\n            arg0 = guard(get_constant, self.func_ir, args[0])\n            if isinstance(arg0, tuple):\n                arg0 = arg0[0]\n            if arg0 != 0:\n                raise ValueError(""Transpose with non-zero first argument""\n                                 "" is not supported"")\n            self._meet_array_dists(lhs, in_arr_name, array_dists)\n            return\n\n        if func_name in (\'astype\', \'reshape\', \'copy\'):\n            in_arr_name = arr.name\n            self._meet_array_dists(lhs, in_arr_name, array_dists)\n            # TODO: support 1D_Var reshape\n            if func_name == \'reshape\' and array_dists[lhs] == Distribution.OneD_Var:\n                # HACK support A.reshape(n, 1) for 1D_Var\n                if len(args) == 2 and guard(find_const, self.func_ir, args[1]) == 1:\n                    return\n                self._analyze_call_set_REP(lhs, args, array_dists, \'array.\' + func_name)\n            return\n\n        # Array.tofile() is supported for all distributions\n        if func_name == \'tofile\':\n            return\n\n        # set REP if not found\n        self._analyze_call_set_REP(lhs, args, array_dists, \'array.\' + func_name)\n\n    def _analyze_call_df(self, lhs, arr, func_name, args, array_dists):\n        # to_csv() can be parallelized\n        if func_name == \'to_csv\':\n            return\n\n        # set REP if not found\n        self._analyze_call_set_REP(lhs, args, array_dists, \'df.\' + func_name)\n\n    def _analyze_call_hpat_dist(self, lhs, func_name, args, array_dists):\n        """"""analyze distributions of hpat distributed functions\n        (sdc.distributed_api.func_name)\n        """"""\n        if func_name == \'local_len\':\n            return\n        if func_name == \'parallel_print\':\n            return\n\n        if func_name == \'dist_return\':\n            arr_name = args[0].name\n            assert arr_name in array_dists, ""array distribution not found""\n            if array_dists[arr_name] == Distribution.REP:\n                raise ValueError(""distributed return of array {} not valid""\n                                 "" since it is replicated"".format(arr_name))\n            return\n\n        if func_name == \'threaded_return\':\n            arr_name = args[0].name\n            assert arr_name in array_dists, ""array distribution not found""\n            if array_dists[arr_name] == Distribution.REP:\n                raise ValueError(""threaded return of array {} not valid""\n                                 "" since it is replicated"")\n            array_dists[arr_name] = Distribution.Thread\n            return\n\n        if func_name == \'rebalance_array\':\n            if lhs not in array_dists:\n                array_dists[lhs] = Distribution.OneD\n            in_arr = args[0].name\n            if array_dists[in_arr] == Distribution.OneD_Var:\n                array_dists[lhs] = Distribution.OneD\n            else:\n                self._meet_array_dists(lhs, in_arr, array_dists)\n            return\n\n        # set REP if not found\n        self._analyze_call_set_REP(lhs, args, array_dists, \'sdc.distributed_api.\' + func_name)\n\n    def _analyze_call_np_concatenate(self, lhs, args, array_dists):\n        assert len(args) == 1\n        tup_def = guard(get_definition, self.func_ir, args[0])\n        assert isinstance(tup_def, ir.Expr) and tup_def.op == \'build_tuple\'\n        in_arrs = tup_def.items\n        # input arrays have same distribution\n        in_dist = Distribution.OneD\n        for v in in_arrs:\n            in_dist = Distribution(\n                min(in_dist.value, array_dists[v.name].value))\n        # OneD_Var since sum of block sizes might not be exactly 1D\n        out_dist = Distribution.OneD_Var\n        out_dist = Distribution(min(out_dist.value, in_dist.value))\n        array_dists[lhs] = out_dist\n        # output can cause input REP\n        if out_dist != Distribution.OneD_Var:\n            in_dist = out_dist\n        for v in in_arrs:\n            array_dists[v.name] = in_dist\n        return\n\n    def _analyze_call_np_dot(self, lhs, args, array_dists):\n\n        arg0 = args[0].name\n        arg1 = args[1].name\n        ndim0 = self.typemap[arg0].ndim\n        ndim1 = self.typemap[arg1].ndim\n        dist0 = array_dists[arg0]\n        dist1 = array_dists[arg1]\n        # Fortran layout is caused by X.T and means transpose\n        t0 = arg0 in self._T_arrs\n        t1 = arg1 in self._T_arrs\n        if ndim0 == 1 and ndim1 == 1:\n            # vector dot, both vectors should have same layout\n            new_dist = Distribution(min(array_dists[arg0].value,\n                                        array_dists[arg1].value))\n            array_dists[arg0] = new_dist\n            array_dists[arg1] = new_dist\n            return\n        if ndim0 == 2 and ndim1 == 1 and not t0:\n            # special case were arg1 vector is treated as column vector\n            # samples dot weights: np.dot(X,w)\n            # w is always REP\n            array_dists[arg1] = Distribution.REP\n            if lhs not in array_dists:\n                array_dists[lhs] = Distribution.OneD\n            # lhs and X have same distribution\n            self._meet_array_dists(lhs, arg0, array_dists)\n            dprint(""dot case 1 Xw:"", arg0, arg1)\n            return\n        if ndim0 == 1 and ndim1 == 2 and not t1:\n            # reduction across samples np.dot(Y,X)\n            # lhs is always REP\n            array_dists[lhs] = Distribution.REP\n            # Y and X have same distribution\n            self._meet_array_dists(arg0, arg1, array_dists)\n            dprint(""dot case 2 YX:"", arg0, arg1)\n            return\n        if ndim0 == 2 and ndim1 == 2 and t0 and not t1:\n            # reduction across samples np.dot(X.T,Y)\n            # lhs is always REP\n            array_dists[lhs] = Distribution.REP\n            # Y and X have same distribution\n            self._meet_array_dists(arg0, arg1, array_dists)\n            dprint(""dot case 3 XtY:"", arg0, arg1)\n            return\n        if ndim0 == 2 and ndim1 == 2 and not t0 and not t1:\n            # samples dot weights: np.dot(X,w)\n            # w is always REP\n            array_dists[arg1] = Distribution.REP\n            self._meet_array_dists(lhs, arg0, array_dists)\n            dprint(""dot case 4 Xw:"", arg0, arg1)\n            return\n\n        # set REP if no pattern matched\n        self._analyze_call_set_REP(lhs, args, array_dists, \'np.dot\')\n\n    def _analyze_call_set_REP(self, lhs, args, array_dists, fdef=None):\n        for v in args:\n            if (is_array(self.typemap, v.name)\n                    or is_array_container(self.typemap, v.name)\n                    or isinstance(self.typemap[v.name], DataFrameType)):\n                dprint(""dist setting call arg REP {} in {}"".format(v.name, fdef))\n                array_dists[v.name] = Distribution.REP\n        if (is_array(self.typemap, lhs)\n                or is_array_container(self.typemap, lhs)\n                or isinstance(self.typemap[lhs], DataFrameType)):\n            dprint(""dist setting call out REP {} in {}"".format(lhs, fdef))\n            array_dists[lhs] = Distribution.REP\n\n    def _analyze_getitem(self, inst, lhs, rhs, array_dists):\n        # selecting an array from a tuple\n        if (rhs.op == \'static_getitem\'\n                and isinstance(self.typemap[rhs.value.name], types.BaseTuple)\n                and isinstance(rhs.index, int)):\n            seq_info = guard(find_build_sequence, self.func_ir, rhs.value)\n            if seq_info is not None:\n                in_arrs, _ = seq_info\n                arr = in_arrs[rhs.index]\n                self._meet_array_dists(lhs, arr.name, array_dists)\n                return\n\n        if rhs.op == \'static_getitem\':\n            if rhs.index_var is None:\n                # TODO: things like A[0] need broadcast\n                self._set_REP(inst.list_vars(), array_dists)\n                return\n            index_var = rhs.index_var\n        else:\n            assert rhs.op == \'getitem\'\n            index_var = rhs.index\n\n        if (rhs.value.name, index_var.name) in self._parallel_accesses:\n            # XXX: is this always valid? should be done second pass?\n            self._set_REP([inst.target], array_dists)\n            return\n\n        # in multi-dimensional case, we only consider first dimension\n        # TODO: extend to 2D distribution\n        tup_list = guard(find_build_tuple, self.func_ir, index_var)\n        if tup_list is not None:\n            index_var = tup_list[0]\n            # rest of indices should be replicated if array\n            other_ind_vars = tup_list[1:]\n            self._set_REP(other_ind_vars, array_dists)\n\n        if isinstance(index_var, int):\n            self._set_REP(inst.list_vars(), array_dists)\n            return\n        assert isinstance(index_var, ir.Var)\n\n        # array selection with boolean index\n        if (is_np_array(self.typemap, index_var.name)\n                and self.typemap[index_var.name].dtype == types.boolean):\n            # input array and bool index have the same distribution\n            new_dist = self._meet_array_dists(index_var.name, rhs.value.name,\n                                              array_dists)\n            array_dists[lhs] = Distribution(min(Distribution.OneD_Var.value,\n                                                new_dist.value))\n            return\n\n        # array selection with permutation array index\n        if is_np_array(self.typemap, index_var.name):\n            arr_def = guard(get_definition, self.func_ir, index_var)\n            if isinstance(arr_def, ir.Expr) and arr_def.op == \'call\':\n                fdef = guard(find_callname, self.func_ir, arr_def, self.typemap)\n                if fdef == (\'permutation\', \'numpy.random\'):\n                    self._meet_array_dists(lhs, rhs.value.name, array_dists)\n                    return\n\n        # whole slice or strided slice access\n        # for example: A = X[:,5], A = X[::2,5]\n        if guard(is_whole_slice, self.typemap, self.func_ir, index_var,\n                 accept_stride=True):\n            self._meet_array_dists(lhs, rhs.value.name, array_dists)\n            return\n\n        # output of operations like S.head() is REP since it\'s a ""small"" slice\n        # input can remain 1D\n        if guard(is_const_slice, self.typemap, self.func_ir, index_var):\n            array_dists[lhs] = Distribution.REP\n            return\n\n        self._set_REP(inst.list_vars(), array_dists)\n        return\n\n    def _analyze_setitem(self, inst, array_dists):\n        if isinstance(inst, ir.SetItem):\n            index_var = inst.index\n        else:\n            index_var = inst.index_var\n\n        if ((inst.target.name, index_var.name) in self._parallel_accesses):\n            # no parallel to parallel array set (TODO)\n            return\n\n        tup_list = guard(find_build_tuple, self.func_ir, index_var)\n        if tup_list is not None:\n            index_var = tup_list[0]\n            # rest of indices should be replicated if array\n            self._set_REP(tup_list[1:], array_dists)\n\n        if guard(is_whole_slice, self.typemap, self.func_ir, index_var):\n            # for example: X[:,3] = A\n            self._meet_array_dists(\n                inst.target.name, inst.value.name, array_dists)\n            return\n\n        self._set_REP([inst.value], array_dists)\n\n    def _meet_array_dists(self, arr1, arr2, array_dists, top_dist=None):\n        if top_dist is None:\n            top_dist = Distribution.OneD\n        if arr1 not in array_dists:\n            array_dists[arr1] = top_dist\n        if arr2 not in array_dists:\n            array_dists[arr2] = top_dist\n\n        new_dist = Distribution(min(array_dists[arr1].value,\n                                    array_dists[arr2].value))\n        new_dist = Distribution(min(new_dist.value, top_dist.value))\n        array_dists[arr1] = new_dist\n        array_dists[arr2] = new_dist\n        return new_dist\n\n    def _set_REP(self, var_list, array_dists):\n        for var in var_list:\n            varname = var.name\n            # Handle SeriesType since it comes from Arg node and it could\n            # have user-defined distribution\n            if (is_array(self.typemap, varname)\n                    or is_array_container(self.typemap, varname)\n                    or isinstance(\n                        self.typemap[varname], (SeriesType, DataFrameType))):\n                dprint(""dist setting REP {}"".format(varname))\n                array_dists[varname] = Distribution.REP\n            # handle tuples of arrays\n            var_def = guard(get_definition, self.func_ir, var)\n            if (var_def is not None and isinstance(var_def, ir.Expr)\n                    and var_def.op == \'build_tuple\'):\n                tuple_vars = var_def.items\n                self._set_REP(tuple_vars, array_dists)\n\n    def _rebalance_arrs(self, array_dists, parfor_dists):\n        # rebalance an array if it is accessed in a parfor that has output\n        # arrays or is in a loop\n\n        # find sequential loop bodies\n        cfg = analysis.compute_cfg_from_blocks(self.func_ir.blocks)\n        loop_bodies = set()\n        for loop in cfg.loops().values():\n            loop_bodies |= loop.body\n\n        rebalance_arrs = set()\n\n        for label, block in self.func_ir.blocks.items():\n            for inst in block.body:\n                # TODO: handle hiframes filter etc.\n                if (isinstance(inst, Parfor)\n                        and parfor_dists[inst.id] == Distribution.OneD_Var):\n                    array_accesses = _get_array_accesses(\n                        inst.loop_body, self.func_ir, self.typemap)\n                    onedv_arrs = set(arr for (arr, ind) in array_accesses\n                                     if arr in array_dists and array_dists[arr] == Distribution.OneD_Var)\n                    if (label in loop_bodies\n                            or _arrays_written(onedv_arrs, inst.loop_body)):\n                        rebalance_arrs |= onedv_arrs\n\n        if len(rebalance_arrs) != 0:\n            self._gen_rebalances(rebalance_arrs, self.func_ir.blocks)\n            return True\n\n        return False\n\n    def _gen_rebalances(self, rebalance_arrs, blocks):\n        #\n        for block in blocks.values():\n            new_body = []\n            for inst in block.body:\n                # TODO: handle hiframes filter etc.\n                if isinstance(inst, Parfor):\n                    self._gen_rebalances(rebalance_arrs, {0: inst.init_block})\n                    self._gen_rebalances(rebalance_arrs, inst.loop_body)\n                if isinstance(inst, ir.Assign) and inst.target.name in rebalance_arrs:\n                    out_arr = inst.target\n                    self.func_ir._definitions[out_arr.name].remove(inst.value)\n                    # hold inst results in tmp array\n                    tmp_arr = ir.Var(out_arr.scope,\n                                     mk_unique_var(""rebalance_tmp""),\n                                     out_arr.loc)\n                    self.typemap[tmp_arr.name] = self.typemap[out_arr.name]\n                    inst.target = tmp_arr\n                    nodes = [inst]\n\n                    def f(in_arr):  # pragma: no cover\n                        out_a = sdc.distributed_api.rebalance_array(in_arr)\n                    f_block = compile_to_numba_ir(f, {\'sdc\': sdc}, self.typingctx,\n                                                  (self.typemap[tmp_arr.name],),\n                                                  self.typemap, self.calltypes).blocks.popitem()[1]\n                    replace_arg_nodes(f_block, [tmp_arr])\n                    nodes += f_block.body[:-3]  # remove none return\n                    nodes[-1].target = out_arr\n                    # update definitions\n                    dumm_block = ir.Block(out_arr.scope, out_arr.loc)\n                    dumm_block.body = nodes\n                    build_definitions({0: dumm_block}, self.func_ir._definitions)\n                    new_body += nodes\n                else:\n                    new_body.append(inst)\n\n            block.body = new_body\n\n\ndef _get_pair_first_container(func_ir, rhs):\n    assert isinstance(rhs, ir.Expr) and rhs.op == \'pair_first\'\n    iternext = get_definition(func_ir, rhs.value)\n    require(isinstance(iternext, ir.Expr) and iternext.op == \'iternext\')\n    getiter = get_definition(func_ir, iternext.value)\n    require(isinstance(iternext, ir.Expr) and getiter.op == \'getiter\')\n    return getiter.value\n\n\ndef _arrays_written(arrs, blocks):\n    for block in blocks.values():\n        for inst in block.body:\n            if isinstance(inst, Parfor) and _arrays_written(arrs, inst.loop_body):\n                return True\n            if (isinstance(inst, (ir.SetItem, ir.StaticSetItem))\n                    and inst.target.name in arrs):\n                return True\n    return False\n\n# def get_stencil_accesses(parfor, typemap):\n#     # if a parfor has stencil pattern, see which accesses depend on loop index\n#     # XXX: assuming loop index is not used for non-stencil arrays\n#     # TODO support recursive parfor, multi-D, mutiple body blocks\n\n#     # no access if not stencil\n#     is_stencil = False\n#     for pattern in parfor.patterns:\n#         if pattern[0] == \'stencil\':\n#             is_stencil = True\n#             neighborhood = pattern[1]\n#     if not is_stencil:\n#         return {}, None\n\n#     par_index_var = parfor.loop_nests[0].index_variable\n#     body = parfor.loop_body\n#     body_defs = build_definitions(body)\n\n#     stencil_accesses = {}\n\n#     for block in body.values():\n#         for stmt in block.body:\n#             if isinstance(stmt, ir.Assign) and isinstance(stmt.value, ir.Expr):\n#                 lhs = stmt.target.name\n#                 rhs = stmt.value\n#                 if (rhs.op == \'getitem\' and is_array(typemap, rhs.value.name)\n#                         and vars_dependent(body_defs, rhs.index, par_index_var)):\n#                     stencil_accesses[rhs.index.name] = rhs.value.name\n\n#     return stencil_accesses, neighborhood\n\n\n# def vars_dependent(defs, var1, var2):\n#     # see if var1 depends on var2 based on definitions in defs\n#     if len(defs[var1.name]) != 1:\n#         return False\n\n#     vardef = defs[var1.name][0]\n#     if isinstance(vardef, ir.Var) and vardef.name == var2.name:\n#         return True\n#     if isinstance(vardef, ir.Expr):\n#         for invar in vardef.list_vars():\n#             if invar.name == var2.name or vars_dependent(defs, invar, var2):\n#                 return True\n#     return False\n\n\n# array access code is copied from ir_utils to be able to handle specialized\n# array access calls such as get_split_view_index()\n# TODO: implement extendable version in ir_utils\ndef get_parfor_array_accesses(parfor, func_ir, typemap, accesses=None):\n    if accesses is None:\n        accesses = set()\n    blocks = wrap_parfor_blocks(parfor)\n    accesses = _get_array_accesses(blocks, func_ir, typemap, accesses)\n    unwrap_parfor_blocks(parfor)\n    return accesses\n\n\narray_accesses_extensions = {}\narray_accesses_extensions[Parfor] = get_parfor_array_accesses\n\n\ndef _get_array_accesses(blocks, func_ir, typemap, accesses=None):\n    """"""returns a set of arrays accessed and their indices.\n    """"""\n    if accesses is None:\n        accesses = set()\n\n    for block in blocks.values():\n        for inst in block.body:\n            if isinstance(inst, ir.SetItem):\n                accesses.add((inst.target.name, inst.index.name))\n            if isinstance(inst, ir.StaticSetItem):\n                accesses.add((inst.target.name, inst.index_var.name))\n            if isinstance(inst, ir.Assign):\n                lhs = inst.target.name\n                rhs = inst.value\n                if isinstance(rhs, ir.Expr) and rhs.op == \'getitem\':\n                    accesses.add((rhs.value.name, rhs.index.name))\n                if isinstance(rhs, ir.Expr) and rhs.op == \'static_getitem\':\n                    index = rhs.index\n                    # slice is unhashable, so just keep the variable\n                    if index is None or ir_utils.is_slice_index(index):\n                        index = rhs.index_var.name\n                    accesses.add((rhs.value.name, index))\n                if isinstance(rhs, ir.Expr) and rhs.op == \'call\':\n                    fdef = guard(find_callname, func_ir, rhs, typemap)\n                    if fdef is not None:\n                        if fdef == (\'get_split_view_index\', \'sdc.hiframes.split_impl\'):\n                            accesses.add((rhs.args[0].name, rhs.args[1].name))\n                        if fdef == (\'setitem_str_arr_ptr\', \'sdc.str_arr_ext\'):\n                            accesses.add((rhs.args[0].name, rhs.args[1].name))\n                        if fdef == (\'str_arr_item_to_numeric\', \'sdc.str_arr_ext\'):\n                            accesses.add((rhs.args[0].name, rhs.args[1].name))\n                            accesses.add((rhs.args[2].name, rhs.args[3].name))\n            for T, f in array_accesses_extensions.items():\n                if isinstance(inst, T):\n                    f(inst, func_ir, typemap, accesses)\n    return accesses\n\n\ndef dprint(*s):\n    if debug_prints():\n        print(*s)\n'"
sdc/distributed_api.py,37,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\n""""""\n.. module:: distributed_api.py\n\nThe description of the distributed_api module will be here.\n""""""\nimport time\nfrom enum import Enum\nimport llvmlite.binding as ll\nimport operator\nimport numpy as np\n\nimport numba\nfrom numba import types\nfrom numba.extending import models, overload, register_model\nfrom numba.core.typing import signature\nfrom numba.core.typing.templates import AbstractTemplate, infer_global\n\nimport sdc\nfrom sdc.str_arr_ext import (string_array_type, num_total_chars, StringArray,\n                             pre_alloc_string_array, get_offset_ptr,\n                             get_data_ptr, convert_len_arr_to_offset)\nfrom sdc.utilities.utils import (debug_prints, empty_like_type, _numba_to_c_type_map, unliteral_all)\n\nfrom . import transport_seq as transport\n\n\nll.add_symbol(\'c_alltoall\', transport.c_alltoall)\nll.add_symbol(\'c_gather_scalar\', transport.c_gather_scalar)\nll.add_symbol(\'c_gatherv\', transport.c_gatherv)\nll.add_symbol(\'c_bcast\', transport.c_bcast)\nll.add_symbol(\'c_recv\', transport.hpat_dist_recv)\nll.add_symbol(\'c_send\', transport.hpat_dist_send)\n\n\n# get size dynamically from C code (mpich 3.2 is 4 bytes but openmpi 1.6 is 8)\nmpi_req_numba_type = getattr(types, ""int"" + str(8 * transport.mpi_req_num_bytes))\n\nMPI_ROOT = 0\n\n\nclass Reduce_Type(Enum):\n    Sum = 0\n    Prod = 1\n    Min = 2\n    Max = 3\n    Argmin = 4\n    Argmax = 5\n    Or = 6\n\n\ndef get_type_enum(arr):\n    return np.int32(-1)\n\n\n@overload(get_type_enum)\ndef get_type_enum_overload(arr):\n    dtype = arr.dtype\n    if isinstance(dtype, sdc.hiframes.pd_categorical_ext.PDCategoricalDtype):\n        dtype = sdc.hiframes.pd_categorical_ext.get_categories_int_type(dtype)\n\n    typ_val = _numba_to_c_type_map[dtype]\n    return lambda arr: np.int32(typ_val)\n\n\nINT_MAX = np.iinfo(np.int32).max\n\n_send = types.ExternalFunction(""c_send"", types.void(types.voidptr, types.int32, types.int32, types.int32, types.int32))\n\n\n@numba.njit\ndef send(val, rank, tag):\n    # dummy array for val\n    send_arr = np.full(1, val)\n    type_enum = get_type_enum(send_arr)\n    _send(send_arr.ctypes, 1, type_enum, rank, tag)\n\n\n_recv = types.ExternalFunction(""c_recv"", types.void(types.voidptr, types.int32, types.int32, types.int32, types.int32))\n\n\n@numba.njit\ndef recv(dtype, rank, tag):\n    # dummy array for val\n    recv_arr = np.empty(1, dtype)\n    type_enum = get_type_enum(recv_arr)\n    _recv(recv_arr.ctypes, 1, type_enum, rank, tag)\n    return recv_arr[0]\n\n\n_alltoall = types.ExternalFunction(""c_alltoall"", types.void(types.voidptr, types.voidptr, types.int32, types.int32))\n\n\n@numba.njit\ndef alltoall(send_arr, recv_arr, count):\n    # TODO: handle int64 counts\n    assert count < INT_MAX\n    type_enum = get_type_enum(send_arr)\n    _alltoall(send_arr.ctypes, recv_arr.ctypes, np.int32(count), type_enum)\n\n\ndef gather_scalar(data):  # pragma: no cover\n    return np.ones(1)\n\n\nc_gather_scalar = types.ExternalFunction(""c_gather_scalar"", types.void(types.voidptr, types.voidptr, types.int32))\n\n\n# TODO: test\n@overload(gather_scalar)\ndef gather_scalar_overload(val):\n    assert isinstance(val, (types.Integer, types.Float))\n    # TODO: other types like boolean\n    typ_val = _numba_to_c_type_map[val]\n\n    func_text = (\n        ""def gather_scalar_impl(val):\\n""\n        ""  n_pes = sdc.distributed_api.get_size()\\n""\n        ""  rank = sdc.distributed_api.get_rank()\\n""\n        ""  send = np.full(1, val, np.{})\\n""\n        ""  res_size = n_pes if rank == {} else 0\\n""\n        ""  res = np.empty(res_size, np.{})\\n""\n        ""  c_gather_scalar(send.ctypes, res.ctypes, np.int32({}))\\n""\n        ""  return res\\n"").format(val, MPI_ROOT, val, typ_val)\n\n    loc_vars = {}\n    exec(func_text, {\'sdc\': sdc, \'np\': np, \'c_gather_scalar\': c_gather_scalar}, loc_vars)\n    gather_impl = loc_vars[\'gather_scalar_impl\']\n    return gather_impl\n\n# TODO: test\n\n\ndef gatherv(data):  # pragma: no cover\n    return data\n\n\n# sendbuf, sendcount, recvbuf, recv_counts, displs, dtype\nc_gatherv = types.ExternalFunction(\n    ""c_gatherv"",\n    types.void(\n        types.voidptr,\n        types.int32,\n        types.voidptr,\n        types.voidptr,\n        types.voidptr,\n        types.int32))\n\n\n@overload(gatherv)\ndef gatherv_overload(data):\n    if isinstance(data, types.Array):\n        # TODO: other types like boolean\n        typ_val = _numba_to_c_type_map[data.dtype]\n\n        def gatherv_impl(data):\n            rank = sdc.distributed_api.get_rank()\n            n_loc = len(data)\n            recv_counts = gather_scalar(np.int32(n_loc))\n            n_total = recv_counts.sum()\n            all_data = empty_like_type(n_total, data)\n            # displacements\n            displs = np.empty(1, np.int32)\n            if rank == MPI_ROOT:\n                displs = sdc.hiframes.join.calc_disp(recv_counts)\n            c_gatherv(\n                data.ctypes,\n                np.int32(n_loc),\n                all_data.ctypes,\n                recv_counts.ctypes,\n                displs.ctypes,\n                np.int32(typ_val))\n            return all_data\n\n        return gatherv_impl\n\n    if data == string_array_type:\n        int32_typ_enum = np.int32(_numba_to_c_type_map[types.int32])\n        char_typ_enum = np.int32(_numba_to_c_type_map[types.uint8])\n\n        def gatherv_str_arr_impl(data):\n            rank = sdc.distributed_api.get_rank()\n            n_loc = len(data)\n            n_all_chars = num_total_chars(data)\n\n            # allocate send lens arrays\n            send_arr_lens = np.empty(n_loc, np.uint32)  # XXX offset type is uint32\n            send_data_ptr = get_data_ptr(data)\n\n            for i in range(n_loc):\n                _str = data[i]\n                send_arr_lens[i] = len(_str)\n\n            recv_counts = gather_scalar(np.int32(n_loc))\n            recv_counts_char = gather_scalar(np.int32(n_all_chars))\n            n_total = recv_counts.sum()\n            n_total_char = recv_counts_char.sum()\n\n            # displacements\n            all_data = StringArray([\'\'])  # dummy arrays on non-root PEs\n            displs = np.empty(0, np.int32)\n            displs_char = np.empty(0, np.int32)\n\n            if rank == MPI_ROOT:\n                all_data = pre_alloc_string_array(n_total, n_total_char)\n                displs = sdc.hiframes.join.calc_disp(recv_counts)\n                displs_char = sdc.hiframes.join.calc_disp(recv_counts_char)\n\n            offset_ptr = get_offset_ptr(all_data)\n            data_ptr = get_data_ptr(all_data)\n            c_gatherv(\n                send_arr_lens.ctypes,\n                np.int32(n_loc),\n                offset_ptr,\n                recv_counts.ctypes,\n                displs.ctypes,\n                int32_typ_enum)\n            c_gatherv(\n                send_data_ptr,\n                np.int32(n_all_chars),\n                data_ptr,\n                recv_counts_char.ctypes,\n                displs_char.ctypes,\n                char_typ_enum)\n            convert_len_arr_to_offset(offset_ptr, n_total)\n            return all_data\n\n        return gatherv_str_arr_impl\n\n\n# TODO: test\n# TODO: large BCast\n\ndef bcast(data):  # pragma: no cover\n    return\n\n\n@overload(bcast)\ndef bcast_overload(data):\n    if isinstance(data, types.Array):\n        def bcast_impl(data):\n            typ_enum = get_type_enum(data)\n            count = len(data)\n            assert count < INT_MAX\n            c_bcast(data.ctypes, np.int32(count), typ_enum)\n            return\n        return bcast_impl\n\n    if data == string_array_type:\n        int32_typ_enum = np.int32(_numba_to_c_type_map[types.int32])\n        char_typ_enum = np.int32(_numba_to_c_type_map[types.uint8])\n\n        def bcast_str_impl(data):\n            rank = sdc.distributed_api.get_rank()\n            n_loc = len(data)\n            n_all_chars = num_total_chars(data)\n            assert n_loc < INT_MAX\n            assert n_all_chars < INT_MAX\n\n            offset_ptr = get_offset_ptr(data)\n            data_ptr = get_data_ptr(data)\n\n            if rank == MPI_ROOT:\n                send_arr_lens = np.empty(n_loc, np.uint32)  # XXX offset type is uint32\n                for i in range(n_loc):\n                    _str = data[i]\n                    send_arr_lens[i] = len(_str)\n\n                c_bcast(send_arr_lens.ctypes, np.int32(n_loc), int32_typ_enum)\n            else:\n                c_bcast(offset_ptr, np.int32(n_loc), int32_typ_enum)\n\n            c_bcast(data_ptr, np.int32(n_all_chars), char_typ_enum)\n            if rank != MPI_ROOT:\n                convert_len_arr_to_offset(offset_ptr, n_loc)\n\n        return bcast_str_impl\n\n\n# sendbuf, sendcount, dtype\nc_bcast = types.ExternalFunction(""c_bcast"", types.void(types.voidptr, types.int32, types.int32))\n\n\ndef bcast_scalar(val):  # pragma: no cover\n    return val\n\n# TODO: test\n@overload(bcast_scalar)\ndef bcast_scalar_overload(val):\n    assert isinstance(val, (types.Integer, types.Float))\n    # TODO: other types like boolean\n    typ_val = _numba_to_c_type_map[val]\n    # TODO: fix np.full and refactor\n    func_text = (\n        ""def bcast_scalar_impl(val):\\n""\n        ""  send = np.full(1, val, np.{})\\n""\n        ""  c_bcast(send.ctypes, np.int32(1), np.int32({}))\\n""\n        ""  return send[0]\\n"").format(val, typ_val)\n\n    loc_vars = {}\n    exec(func_text, {\'sdc\': sdc, \'np\': np, \'c_bcast\': c_bcast}, loc_vars)\n    bcast_scalar_impl = loc_vars[\'bcast_scalar_impl\']\n    return bcast_scalar_impl\n\n# if arr is string array, pre-allocate on non-root the same size as root\n\n\ndef prealloc_str_for_bcast(arr):\n    return arr\n\n\n@overload(prealloc_str_for_bcast)\ndef prealloc_str_for_bcast_overload(arr):\n    if arr == string_array_type:\n        def prealloc_impl(arr):\n            rank = sdc.distributed_api.get_rank()\n            n_loc = bcast_scalar(len(arr))\n            n_all_char = bcast_scalar(np.int64(num_total_chars(arr)))\n            if rank != MPI_ROOT:\n                arr = pre_alloc_string_array(n_loc, n_all_char)\n            return arr\n\n        return prealloc_impl\n\n    return lambda arr: arr\n\n\n# assuming start and step are None\ndef const_slice_getitem(arr, slice_index, start, count):\n    return arr[slice_index]\n\n\n@overload(const_slice_getitem)\ndef const_slice_getitem_overload(arr, slice_index, start, count):\n    \'\'\'Provides parallel implementation of getting a const slice from arrays of different types\n\n    Arguments:\n    arr -- part of the input array processed by this processor\n    slice_index -- start and stop of the slice in the input array (same on all ranks)\n    start -- position of first arr element in the input array\n    count -- lenght of the part of the array processed by this processor\n\n    Return value:\n    Function providing implementation basing on arr type. The function should implement\n    logic of fetching const slice from the array distributed over multiple processes.\n    \'\'\'\n\n    # TODO: should this also handle slices not staring from zero?\n    if arr == string_array_type:\n        reduce_op = Reduce_Type.Sum.value\n\n        def getitem_str_impl(arr, slice_index, start, count):\n            rank = sdc.distributed_api.get_rank()\n            k = slice_index.stop\n\n            # get total characters for allocation\n            n_chars = np.uint64(0)\n            if k > start:\n                # if slice end is beyond the start of this subset we have to send our elements\n                my_end = min(count, k - start)\n                my_arr = arr[:my_end]\n            else:\n                my_arr = arr[:0]\n\n            # get the total number of chars in our array, then gather all arrays into one\n            # and compute total number of chars in all arrays\n            n_chars = num_total_chars(my_arr)\n            my_arr = sdc.distributed_api.gatherv(my_arr)\n            n_chars = sdc.distributed_api.dist_reduce(n_chars, np.int32(reduce_op))\n\n            if rank != 0:\n                out_arr = pre_alloc_string_array(k, n_chars)\n            else:\n                out_arr = my_arr\n\n            # actual communication\n            sdc.distributed_api.bcast(out_arr)\n            return out_arr\n\n        return getitem_str_impl\n\n    def getitem_impl(arr, slice_index, start, count):\n        rank = sdc.distributed_api.get_rank()\n        k = slice_index.stop\n\n        out_arr = np.empty(k, arr.dtype)\n        if k > start:\n            # if slice end is beyond the start of this subset we have to send our elements\n            my_end = min(count, k - start)\n            my_arr = arr[:my_end]\n        else:\n            my_arr = arr[:0]\n\n        # gather all subsets from all processors\n        my_arr = sdc.distributed_api.gatherv(my_arr)\n\n        if rank == 0:\n            out_arr = my_arr\n\n        # actual communication\n        sdc.distributed_api.bcast(out_arr)\n        return out_arr\n\n    return getitem_impl\n\n\n@numba.njit\ndef local_len(A):\n    return len(A)\n\n\n# send_data, recv_data, send_counts, recv_counts, send_disp, recv_disp, typ_enum\nc_alltoallv = types.ExternalFunction(\n    ""c_alltoallv"",\n    types.void(\n        types.voidptr,\n        types.voidptr,\n        types.voidptr,\n        types.voidptr,\n        types.voidptr,\n        types.voidptr,\n        types.int32))\n\n# TODO: test\n# TODO: big alltoallv\n@numba.njit\ndef alltoallv(send_data, out_data, send_counts, recv_counts, send_disp, recv_disp):  # pragma: no cover\n    typ_enum = get_type_enum(send_data)\n    typ_enum_o = get_type_enum(out_data)\n    assert typ_enum == typ_enum_o\n\n    c_alltoallv(\n        send_data.ctypes,\n        out_data.ctypes,\n        send_counts.ctypes,\n        recv_counts.ctypes,\n        send_disp.ctypes,\n        recv_disp.ctypes,\n        typ_enum)\n    return\n\n\ndef alltoallv_tup(send_data, out_data, send_counts, recv_counts, send_disp, recv_disp):  # pragma: no cover\n    return\n\n\n@overload(alltoallv_tup)\ndef alltoallv_tup_overload(send_data, out_data, send_counts, recv_counts, send_disp, recv_disp):\n\n    count = send_data.count\n    assert out_data.count == count\n\n    func_text = ""def f(send_data, out_data, send_counts, recv_counts, send_disp, recv_disp):\\n""\n    for i in range(count):\n        func_text += ""  alltoallv(send_data[{}], out_data[{}],\\n"".format(i, i)\n        func_text += ""            send_counts, recv_counts, send_disp, recv_disp)\\n""\n    func_text += ""  return\\n""\n\n    loc_vars = {}\n    exec(func_text, {\'alltoallv\': alltoallv}, loc_vars)\n    a2a_impl = loc_vars[\'f\']\n    return a2a_impl\n\n\ndef get_rank():  # pragma: no cover\n    """"""dummy function for C mpi get_rank""""""\n    return 0\n\n\ndef barrier():  # pragma: no cover\n    return 0\n\n\ndef get_size():  # pragma: no cover\n    """"""dummy function for C mpi get_size""""""\n    return 0\n\n\ndef get_start(total_size, pes, rank):  # pragma: no cover\n    """"""get end point of range for parfor division""""""\n    return 0\n\n\ndef get_end(total_size, pes, rank):  # pragma: no cover\n    """"""get end point of range for parfor division""""""\n    return 0\n\n\ndef get_node_portion(total_size, pes, rank):  # pragma: no cover\n    """"""get portion of size for alloc division""""""\n    return 0\n\n\ndef dist_reduce(value, op):  # pragma: no cover\n    """"""dummy to implement simple reductions""""""\n    return value\n\n\ndef dist_arr_reduce(arr):  # pragma: no cover\n    """"""dummy to implement array reductions""""""\n    return -1\n\n\ndef dist_cumsum(arr):  # pragma: no cover\n    """"""dummy to implement cumsum""""""\n    return arr\n\n\ndef dist_cumprod(arr):  # pragma: no cover\n    """"""dummy to implement cumprod""""""\n    return arr\n\n\ndef dist_exscan(value):  # pragma: no cover\n    """"""dummy to implement simple exscan""""""\n    return value\n\n\ndef dist_setitem(arr, index, val):  # pragma: no cover\n    return 0\n\n\ndef allgather(arr, val):  # pragma: no cover\n    arr[0] = val\n\n\ndef dist_time():  # pragma: no cover\n    return time.time()\n\n\ndef dist_return(A):  # pragma: no cover\n    return A\n\n\ndef threaded_return(A):  # pragma: no cover\n    return A\n\n\ndef rebalance_array(A):\n    return A\n\n\ndef rebalance_array_parallel(A):\n    return A\n\n\n@overload(rebalance_array)\ndef dist_return_overload(A):\n    return dist_return\n\n# TODO: move other funcs to old API?\n@infer_global(threaded_return)\n@infer_global(dist_return)\nclass ThreadedRetTyper(AbstractTemplate):\n    def generic(self, args, kws):\n        assert not kws\n        assert len(args) == 1  # array\n        return signature(args[0], *args)\n\n\n@numba.njit\ndef parallel_print(s):\n    print(s)\n\n\ndef irecv():  # pragma: no cover\n    return 0\n\n\ndef isend():  # pragma: no cover\n    return 0\n\n\ndef wait():  # pragma: no cover\n    return 0\n\n\ndef waitall():  # pragma: no cover\n    return 0\n\n\n@infer_global(allgather)\nclass DistAllgather(AbstractTemplate):\n    def generic(self, args, kws):\n        assert not kws\n        assert len(args) == 2  # array and val\n        return signature(types.none, *unliteral_all(args))\n\n\n@infer_global(rebalance_array_parallel)\nclass DistRebalanceParallel(AbstractTemplate):\n    def generic(self, args, kws):\n        assert not kws\n        assert len(args) == 2  # array and count\n        return signature(args[0], *unliteral_all(args))\n\n\n@infer_global(get_rank)\nclass DistRank(AbstractTemplate):\n    def generic(self, args, kws):\n        assert not kws\n        assert len(args) == 0\n        return signature(types.int32, *unliteral_all(args))\n\n\n@infer_global(get_size)\nclass DistSize(AbstractTemplate):\n    def generic(self, args, kws):\n        assert not kws\n        assert len(args) == 0\n        return signature(types.int32, *unliteral_all(args))\n\n\n@infer_global(get_start)\nclass DistStart(AbstractTemplate):\n    def generic(self, args, kws):\n        assert not kws\n        assert len(args) == 3\n        return signature(types.int64, *unliteral_all(args))\n\n\n@infer_global(get_end)\nclass DistEnd(AbstractTemplate):\n    def generic(self, args, kws):\n        assert not kws\n        assert len(args) == 3\n        return signature(types.int64, *unliteral_all(args))\n\n\n@infer_global(get_node_portion)\nclass DistPortion(AbstractTemplate):\n    def generic(self, args, kws):\n        assert not kws\n        assert len(args) == 3\n        return signature(types.int64, *unliteral_all(args))\n\n\n@infer_global(dist_reduce)\nclass DistReduce(AbstractTemplate):\n    def generic(self, args, kws):\n        assert not kws\n        assert len(args) == 2  # value and reduce_op\n        return signature(args[0], *unliteral_all(args))\n\n\n@infer_global(dist_exscan)\nclass DistExscan(AbstractTemplate):\n    def generic(self, args, kws):\n        assert not kws\n        assert len(args) == 1\n        return signature(args[0], *unliteral_all(args))\n\n\n@infer_global(dist_arr_reduce)\nclass DistArrReduce(AbstractTemplate):\n    def generic(self, args, kws):\n        assert not kws\n        assert len(args) == 2  # value and reduce_op\n        return signature(types.int32, *unliteral_all(args))\n\n\n@infer_global(time.time)\nclass DistTime(AbstractTemplate):\n    def generic(self, args, kws):\n        assert not kws\n        assert len(args) == 0\n        return signature(types.float64, *unliteral_all(args))\n\n\n@infer_global(dist_time)\nclass DistDistTime(AbstractTemplate):\n    def generic(self, args, kws):\n        assert not kws\n        assert len(args) == 0\n        return signature(types.float64, *unliteral_all(args))\n\n\n@infer_global(barrier)\nclass DistBarrier(AbstractTemplate):\n    def generic(self, args, kws):\n        assert not kws\n        assert len(args) == 0\n        return signature(types.int32, *unliteral_all(args))\n\n\n@infer_global(dist_cumsum)\n@infer_global(dist_cumprod)\nclass DistCumsumprod(AbstractTemplate):\n    def generic(self, args, kws):\n        assert not kws\n        assert len(args) == 2\n        return signature(types.int32, *unliteral_all(args))\n\n\n@infer_global(irecv)\n@infer_global(isend)\nclass DistIRecv(AbstractTemplate):\n    def generic(self, args, kws):\n        assert not kws\n        assert len(args) in [4, 5]\n        return signature(mpi_req_numba_type, *unliteral_all(args))\n\n\n@infer_global(wait)\nclass DistWait(AbstractTemplate):\n    def generic(self, args, kws):\n        assert not kws\n        assert len(args) == 2\n        return signature(types.int32, *unliteral_all(args))\n\n\n@infer_global(waitall)\nclass DistWaitAll(AbstractTemplate):\n    def generic(self, args, kws):\n        assert not kws\n        assert len(args) == 2 and args == (types.int32, req_array_type)\n        return signature(types.none, *unliteral_all(args))\n\n# @infer_global(dist_setitem)\n# class DistSetitem(AbstractTemplate):\n#     def generic(self, args, kws):\n#         assert not kws\n#         assert len(args)==5\n#         return signature(types.int32, *unliteral_all(args))\n\n\nclass ReqArrayType(types.Type):\n    def __init__(self):\n        super(ReqArrayType, self).__init__(\n            name=\'ReqArrayType()\')\n\n\nreq_array_type = ReqArrayType()\nregister_model(ReqArrayType)(models.OpaqueModel)\n\n\ndef comm_req_alloc():\n    return 0\n\n\ndef comm_req_dealloc():\n    return 0\n\n\n@infer_global(comm_req_alloc)\nclass DistCommReqAlloc(AbstractTemplate):\n    def generic(self, args, kws):\n        assert not kws\n        assert len(args) == 1 and args[0] == types.int32\n        return signature(req_array_type, *unliteral_all(args))\n\n\n@infer_global(comm_req_dealloc)\nclass DistCommReqDeAlloc(AbstractTemplate):\n    def generic(self, args, kws):\n        assert not kws\n        assert len(args) == 1 and args[0] == req_array_type\n        return signature(types.none, *unliteral_all(args))\n\n\n@infer_global(operator.setitem)\nclass SetItemReqArray(AbstractTemplate):\n    def generic(self, args, kws):\n        assert not kws\n        [ary, idx, val] = args\n        if isinstance(ary, ReqArrayType) and idx == types.intp and val == mpi_req_numba_type:\n            return signature(types.none, *unliteral_all(args))\n'"
sdc/distributed_lower.py,13,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\nimport time\nimport atexit\nimport sys\nimport operator\nimport numpy as np\nfrom llvmlite import ir as lir\nimport llvmlite.binding as ll\n\nimport numba.np.arrayobj\nfrom numba import types\nfrom numba.core import cgutils\nfrom numba.extending import overload\nfrom numba.core.imputils import impl_ret_borrowed, lower_builtin\nfrom numba.np.arrayobj import make_array\nfrom numba.core.typing import signature\nfrom numba.core.typing.templates import infer_global, AbstractTemplate\nfrom numba.core.typing.builtins import IndexValueType\n\nimport sdc\nfrom sdc import distributed_api\nfrom sdc.utilities.utils import _numba_to_c_type_map\nfrom sdc.distributed_api import mpi_req_numba_type, ReqArrayType, req_array_type\nfrom . import hdist\n\nfrom . import transport_seq as transport\n\n\nll.add_symbol(\'hpat_dist_get_rank\', transport.hpat_dist_get_rank)\nll.add_symbol(\'hpat_dist_get_size\', transport.hpat_dist_get_size)\nll.add_symbol(\'hpat_dist_get_time\', transport.hpat_dist_get_time)\nll.add_symbol(\'hpat_get_time\', transport.hpat_get_time)\nll.add_symbol(\'hpat_barrier\', transport.hpat_barrier)\nll.add_symbol(\'hpat_dist_reduce\', transport.hpat_dist_reduce)\nll.add_symbol(\'hpat_dist_arr_reduce\', transport.hpat_dist_arr_reduce)\nll.add_symbol(\'hpat_dist_exscan_i4\', transport.hpat_dist_exscan_i4)\nll.add_symbol(\'hpat_dist_exscan_i8\', transport.hpat_dist_exscan_i8)\nll.add_symbol(\'hpat_dist_exscan_f4\', transport.hpat_dist_exscan_f4)\nll.add_symbol(\'hpat_dist_exscan_f8\', transport.hpat_dist_exscan_f8)\nll.add_symbol(\'hpat_dist_irecv\', transport.hpat_dist_irecv)\nll.add_symbol(\'hpat_dist_isend\', transport.hpat_dist_isend)\nll.add_symbol(\'hpat_dist_wait\', transport.hpat_dist_wait)\nll.add_symbol(\'allgather\', transport.allgather)\nll.add_symbol(\'comm_req_alloc\', transport.comm_req_alloc)\nll.add_symbol(\'comm_req_dealloc\', transport.comm_req_dealloc)\nll.add_symbol(\'req_array_setitem\', transport.req_array_setitem)\nll.add_symbol(\'hpat_dist_waitall\', transport.hpat_dist_waitall)\nll.add_symbol(\'oneD_reshape_shuffle\', transport.oneD_reshape_shuffle)\nll.add_symbol(\'permutation_int\', transport.permutation_int)\nll.add_symbol(\'permutation_array_index\', transport.permutation_array_index)\nll.add_symbol(\'hpat_finalize\', transport.hpat_finalize)\n\n# get size dynamically from C code\nmpi_req_llvm_type = lir.IntType(8 * transport.mpi_req_num_bytes)\n\nll.add_symbol(\'hpat_dist_get_item_pointer\', hdist.hpat_dist_get_item_pointer)\nll.add_symbol(\'hpat_get_dummy_ptr\', hdist.hpat_get_dummy_ptr)\nll.add_symbol(\'hpat_dist_get_start\', hdist.hpat_dist_get_start)\nll.add_symbol(\'hpat_dist_get_end\', hdist.hpat_dist_get_end)\nll.add_symbol(\'hpat_dist_get_node_portion\', hdist.hpat_dist_get_node_portion)\n\n\n@lower_builtin(distributed_api.get_rank)\ndef dist_get_rank(context, builder, sig, args):\n    fnty = lir.FunctionType(lir.IntType(32), [])\n    fn = builder.module.get_or_insert_function(fnty, name=""hpat_dist_get_rank"")\n    return builder.call(fn, [])\n\n\n@lower_builtin(distributed_api.get_size)\ndef dist_get_size(context, builder, sig, args):\n    fnty = lir.FunctionType(lir.IntType(32), [])\n    fn = builder.module.get_or_insert_function(fnty, name=""hpat_dist_get_size"")\n    return builder.call(fn, [])\n\n\n@lower_builtin(distributed_api.get_start, types.int64, types.int32, types.int32)\ndef dist_get_start(context, builder, sig, args):\n    fnty = lir.FunctionType(lir.IntType(64), [lir.IntType(64),\n                                              lir.IntType(32), lir.IntType(32)])\n    fn = builder.module.get_or_insert_function(\n        fnty, name=""hpat_dist_get_start"")\n    return builder.call(fn, [args[0], args[1], args[2]])\n\n\n@lower_builtin(distributed_api.get_end, types.int64, types.int32, types.int32)\ndef dist_get_end(context, builder, sig, args):\n    fnty = lir.FunctionType(lir.IntType(64), [lir.IntType(64),\n                                              lir.IntType(32), lir.IntType(32)])\n    fn = builder.module.get_or_insert_function(fnty, name=""hpat_dist_get_end"")\n    return builder.call(fn, [args[0], args[1], args[2]])\n\n\n@lower_builtin(distributed_api.get_node_portion, types.int64, types.int32, types.int32)\ndef dist_get_portion(context, builder, sig, args):\n    fnty = lir.FunctionType(lir.IntType(64), [lir.IntType(64),\n                                              lir.IntType(32), lir.IntType(32)])\n    fn = builder.module.get_or_insert_function(\n        fnty, name=""hpat_dist_get_node_portion"")\n    return builder.call(fn, [args[0], args[1], args[2]])\n\n\n@lower_builtin(distributed_api.dist_reduce, types.int8, types.int32)\n@lower_builtin(distributed_api.dist_reduce, types.uint8, types.int32)\n@lower_builtin(distributed_api.dist_reduce, types.int64, types.int32)\n@lower_builtin(distributed_api.dist_reduce, types.int32, types.int32)\n@lower_builtin(distributed_api.dist_reduce, types.float32, types.int32)\n@lower_builtin(distributed_api.dist_reduce, types.float64, types.int32)\n@lower_builtin(distributed_api.dist_reduce, IndexValueType, types.int32)\n@lower_builtin(distributed_api.dist_reduce, types.uint64, types.int32)\ndef lower_dist_reduce(context, builder, sig, args):\n    val_typ = args[0].type\n    op_typ = args[1].type\n\n    target_typ = sig.args[0]\n    if isinstance(target_typ, IndexValueType):\n        target_typ = target_typ.val_typ\n        supported_typs = [types.int32, types.float32, types.float64]\n        import sys\n        if not sys.platform.startswith(\'win\'):\n            # long is 4 byte on Windows\n            supported_typs.append(types.int64)\n        if target_typ not in supported_typs:  # pragma: no cover\n            raise TypeError(""argmin/argmax not supported for type {}"".format(\n                target_typ))\n\n    in_ptr = cgutils.alloca_once(builder, val_typ)\n    out_ptr = cgutils.alloca_once(builder, val_typ)\n    builder.store(args[0], in_ptr)\n    # cast to char *\n    in_ptr = builder.bitcast(in_ptr, lir.IntType(8).as_pointer())\n    out_ptr = builder.bitcast(out_ptr, lir.IntType(8).as_pointer())\n\n    typ_enum = _numba_to_c_type_map[target_typ]\n    typ_arg = cgutils.alloca_once_value(builder, lir.Constant(lir.IntType(32),\n                                                              typ_enum))\n\n    fnty = lir.FunctionType(lir.VoidType(), [lir.IntType(8).as_pointer(),\n                                             lir.IntType(8).as_pointer(), op_typ, lir.IntType(32)])\n    fn = builder.module.get_or_insert_function(fnty, name=""hpat_dist_reduce"")\n    builder.call(fn, [in_ptr, out_ptr, args[1], builder.load(typ_arg)])\n    # cast back to value type\n    out_ptr = builder.bitcast(out_ptr, val_typ.as_pointer())\n    return builder.load(out_ptr)\n\n\n@lower_builtin(distributed_api.dist_reduce, types.npytypes.Array, types.int32)\ndef lower_dist_arr_reduce(context, builder, sig, args):\n\n    op_typ = args[1].type\n\n    # store an int to specify data type\n    typ_enum = _numba_to_c_type_map[sig.args[0].dtype]\n    typ_arg = cgutils.alloca_once_value(\n        builder, lir.Constant(lir.IntType(32), typ_enum))\n    ndims = sig.args[0].ndim\n\n    out = make_array(sig.args[0])(context, builder, args[0])\n    # store size vars array struct to pointer\n    size_ptr = cgutils.alloca_once(builder, out.shape.type)\n    builder.store(out.shape, size_ptr)\n    size_arg = builder.bitcast(size_ptr, lir.IntType(64).as_pointer())\n\n    ndim_arg = cgutils.alloca_once_value(\n        builder, lir.Constant(lir.IntType(32), sig.args[0].ndim))\n    call_args = [builder.bitcast(out.data, lir.IntType(8).as_pointer()),\n                 size_arg, builder.load(ndim_arg), args[1], builder.load(typ_arg)]\n\n    # array, shape, ndim, extra last arg type for type enum\n    arg_typs = [lir.IntType(8).as_pointer(), lir.IntType(64).as_pointer(),\n                lir.IntType(32), op_typ, lir.IntType(32)]\n    fnty = lir.FunctionType(lir.IntType(32), arg_typs)\n    fn = builder.module.get_or_insert_function(\n        fnty, name=""hpat_dist_arr_reduce"")\n    builder.call(fn, call_args)\n    res = out._getvalue()\n    return impl_ret_borrowed(context, builder, sig.return_type, res)\n\n\n@lower_builtin(time.time)\ndef dist_get_time(context, builder, sig, args):\n    fnty = lir.FunctionType(lir.DoubleType(), [])\n    fn = builder.module.get_or_insert_function(fnty, name=""hpat_get_time"")\n    return builder.call(fn, [])\n\n\n@lower_builtin(distributed_api.dist_time)\ndef dist_get_dist_time(context, builder, sig, args):\n    fnty = lir.FunctionType(lir.DoubleType(), [])\n    fn = builder.module.get_or_insert_function(fnty, name=""hpat_dist_get_time"")\n    return builder.call(fn, [])\n\n\n@lower_builtin(distributed_api.barrier)\ndef dist_barrier(context, builder, sig, args):\n    fnty = lir.FunctionType(lir.IntType(32), [])\n    fn = builder.module.get_or_insert_function(fnty, name=""hpat_barrier"")\n    return builder.call(fn, [])\n\n\n@lower_builtin(distributed_api.dist_cumsum, types.npytypes.Array, types.npytypes.Array)\ndef lower_dist_cumsum(context, builder, sig, args):\n\n    dtype = sig.args[0].dtype\n    zero = dtype(0)\n\n    def cumsum_impl(in_arr, out_arr):  # pragma: no cover\n        c = zero\n        for v in np.nditer(in_arr):\n            c += v.item()\n        prefix_var = distributed_api.dist_exscan(c)\n        for i in range(in_arr.size):\n            prefix_var += in_arr[i]\n            out_arr[i] = prefix_var\n        return 0\n\n    res = context.compile_internal(builder, cumsum_impl, sig, args,\n                                   locals=dict(c=dtype,\n                                               prefix_var=dtype))\n    return res\n\n\n@lower_builtin(distributed_api.dist_exscan, types.int64)\n@lower_builtin(distributed_api.dist_exscan, types.int32)\n@lower_builtin(distributed_api.dist_exscan, types.float32)\n@lower_builtin(distributed_api.dist_exscan, types.float64)\ndef lower_dist_exscan(context, builder, sig, args):\n    ltyp = args[0].type\n    fnty = lir.FunctionType(ltyp, [ltyp])\n    typ_map = {types.int32: ""i4"", types.int64: ""i8"",\n               types.float32: ""f4"", types.float64: ""f8""}\n    typ_str = typ_map[sig.args[0]]\n    fn = builder.module.get_or_insert_function(\n        fnty, name=""hpat_dist_exscan_{}"".format(typ_str))\n    return builder.call(fn, [args[0]])\n\n# array, size, pe, tag, cond\n@lower_builtin(distributed_api.irecv, types.npytypes.Array, types.int32, types.int32, types.int32)\n@lower_builtin(distributed_api.irecv, types.npytypes.Array, types.int32, types.int32, types.int32, types.boolean)\ndef lower_dist_irecv(context, builder, sig, args):\n    # store an int to specify data type\n    typ_enum = _numba_to_c_type_map[sig.args[0].dtype]\n    typ_arg = context.get_constant(types.int32, typ_enum)\n    out = make_array(sig.args[0])(context, builder, args[0])\n    size_arg = args[1]\n    if len(args) == 4:\n        cond_arg = context.get_constant(types.boolean, True)\n    else:\n        cond_arg = args[4]\n\n    call_args = [builder.bitcast(out.data, lir.IntType(8).as_pointer()),\n                 size_arg, typ_arg,\n                 args[2], args[3], cond_arg]\n\n    # array, size, extra arg type for type enum\n    # pe, tag, cond\n    arg_typs = [lir.IntType(8).as_pointer(),\n                lir.IntType(32), lir.IntType(\n                    32), lir.IntType(32), lir.IntType(32),\n                lir.IntType(1)]\n    fnty = lir.FunctionType(mpi_req_llvm_type, arg_typs)\n    fn = builder.module.get_or_insert_function(fnty, name=""hpat_dist_irecv"")\n    return builder.call(fn, call_args)\n\n# array, size, pe, tag, cond\n@lower_builtin(distributed_api.isend, types.npytypes.Array, types.int32, types.int32, types.int32)\n@lower_builtin(distributed_api.isend, types.npytypes.Array, types.int32, types.int32, types.int32, types.boolean)\ndef lower_dist_isend(context, builder, sig, args):\n    # store an int to specify data type\n    typ_enum = _numba_to_c_type_map[sig.args[0].dtype]\n    typ_arg = context.get_constant(types.int32, typ_enum)\n    out = make_array(sig.args[0])(context, builder, args[0])\n\n    if len(args) == 4:\n        cond_arg = context.get_constant(types.boolean, True)\n    else:\n        cond_arg = args[4]\n\n    call_args = [builder.bitcast(out.data, lir.IntType(8).as_pointer()),\n                 args[1], typ_arg,\n                 args[2], args[3], cond_arg]\n\n    # array, size, extra arg type for type enum\n    # pe, tag, cond\n    arg_typs = [lir.IntType(8).as_pointer(),\n                lir.IntType(32), lir.IntType(\n                    32), lir.IntType(32), lir.IntType(32),\n                lir.IntType(1)]\n    fnty = lir.FunctionType(mpi_req_llvm_type, arg_typs)\n    fn = builder.module.get_or_insert_function(fnty, name=""hpat_dist_isend"")\n    return builder.call(fn, call_args)\n\n\n@lower_builtin(distributed_api.wait, mpi_req_numba_type, types.boolean)\ndef lower_dist_wait(context, builder, sig, args):\n    fnty = lir.FunctionType(lir.IntType(32), [mpi_req_llvm_type, lir.IntType(1)])\n    fn = builder.module.get_or_insert_function(fnty, name=""hpat_dist_wait"")\n    return builder.call(fn, args)\n\n\n@lower_builtin(distributed_api.waitall, types.int32, req_array_type)\ndef lower_dist_waitall(context, builder, sig, args):\n    fnty = lir.FunctionType(lir.VoidType(),\n                            [lir.IntType(32), lir.IntType(8).as_pointer()])\n    fn = builder.module.get_or_insert_function(fnty, name=""hpat_dist_waitall"")\n    builder.call(fn, args)\n    return context.get_dummy_value()\n\n\n@lower_builtin(distributed_api.rebalance_array_parallel, types.Array, types.intp)\ndef lower_dist_rebalance_array_parallel(context, builder, sig, args):\n\n    arr_typ = sig.args[0]\n    ndim = arr_typ.ndim\n    # TODO: support string type\n\n    shape_tup = "","".join([""count""] + [""in_arr.shape[{}]"".format(i) for i in range(1, ndim)])\n    alloc_text = ""np.empty(({}), in_arr.dtype)"".format(shape_tup)\n\n    func_text = """"""def f(in_arr, count):\n    n_pes = sdc.distributed_api.get_size()\n    my_rank = sdc.distributed_api.get_rank()\n    out_arr = {}\n    # copy old data\n    old_len = len(in_arr)\n    out_ind = 0\n    for i in range(min(old_len, count)):\n        out_arr[i] = in_arr[i]\n        out_ind += 1\n    # get diff data for all procs\n    my_diff = old_len - count\n    all_diffs = np.empty(n_pes, np.int64)\n    sdc.distributed_api.allgather(all_diffs, my_diff)\n    # alloc comm requests\n    comm_req_ind = 0\n    comm_reqs = sdc.distributed_api.comm_req_alloc(n_pes)\n    req_ind = 0\n    # for each potential receiver\n    for i in range(n_pes):\n        # if receiver\n        if all_diffs[i] < 0:\n            # for each potential sender\n            for j in range(n_pes):\n                # if sender\n                if all_diffs[j] > 0:\n                    send_size = min(all_diffs[j], -all_diffs[i])\n                    # if I\'m receiver\n                    if my_rank == i:\n                        buff = out_arr[out_ind:(out_ind+send_size)]\n                        comm_reqs[comm_req_ind] = sdc.distributed_api.irecv(\n                            buff, np.int32(buff.size), np.int32(j), np.int32(9))\n                        comm_req_ind += 1\n                        out_ind += send_size\n                    # if I\'m sender\n                    if my_rank == j:\n                        buff = np.ascontiguousarray(in_arr[out_ind:(out_ind+send_size)])\n                        comm_reqs[comm_req_ind] = sdc.distributed_api.isend(\n                            buff, np.int32(buff.size), np.int32(i), np.int32(9))\n                        comm_req_ind += 1\n                        out_ind += send_size\n                    # update sender and receivers remaining counts\n                    all_diffs[i] += send_size\n                    all_diffs[j] -= send_size\n                    # if receiver is done, stop sender search\n                    if all_diffs[i] == 0: break\n    sdc.distributed_api.waitall(np.int32(comm_req_ind), comm_reqs)\n    sdc.distributed_api.comm_req_dealloc(comm_reqs)\n    return out_arr\n    """""".format(alloc_text)\n\n    loc = {}\n    exec(func_text, {\'sdc\': sdc, \'np\': np}, loc)\n    rebalance_impl = loc[\'f\']\n\n    res = context.compile_internal(builder, rebalance_impl, sig, args)\n    return impl_ret_borrowed(context, builder, sig.return_type, res)\n\n\n@lower_builtin(distributed_api.allgather, types.Array, types.Any)\ndef lower_dist_allgather(context, builder, sig, args):\n    arr_typ = sig.args[0]\n    val_typ = sig.args[1]\n    assert val_typ == arr_typ.dtype\n\n    # type enum arg\n    assert val_typ in _numba_to_c_type_map, ""invalid allgather type""\n    typ_enum = _numba_to_c_type_map[val_typ]\n    typ_arg = context.get_constant(types.int32, typ_enum)\n\n    # size arg is 1 for now\n    size_arg = context.get_constant(types.int32, 1)\n\n    val_ptr = cgutils.alloca_once_value(builder, args[1])\n\n    out = make_array(sig.args[0])(context, builder, args[0])\n\n    call_args = [builder.bitcast(out.data, lir.IntType(8).as_pointer()),\n                 size_arg, val_ptr, typ_arg]\n\n    fnty = lir.FunctionType(lir.VoidType(), [lir.IntType(8).as_pointer(),\n                                             lir.IntType(32), val_ptr.type, lir.IntType(32)])\n    fn = builder.module.get_or_insert_function(fnty, name=""allgather"")\n    builder.call(fn, call_args)\n    return context.get_dummy_value()\n\n\n@lower_builtin(distributed_api.comm_req_alloc, types.int32)\ndef lower_dist_comm_req_alloc(context, builder, sig, args):\n    fnty = lir.FunctionType(lir.IntType(8).as_pointer(), [lir.IntType(32)])\n    fn = builder.module.get_or_insert_function(fnty, name=""comm_req_alloc"")\n    return builder.call(fn, args)\n\n\n@lower_builtin(distributed_api.comm_req_dealloc, req_array_type)\ndef lower_dist_comm_req_dealloc(context, builder, sig, args):\n    fnty = lir.FunctionType(lir.VoidType(), [lir.IntType(8).as_pointer()])\n    fn = builder.module.get_or_insert_function(fnty, name=""comm_req_dealloc"")\n    builder.call(fn, args)\n    return context.get_dummy_value()\n\n\n@lower_builtin(operator.setitem, ReqArrayType, types.intp, mpi_req_numba_type)\ndef setitem_req_array(context, builder, sig, args):\n    fnty = lir.FunctionType(lir.VoidType(), [lir.IntType(8).as_pointer(),\n                                             lir.IntType(64),\n                                             mpi_req_llvm_type])\n    fn = builder.module.get_or_insert_function(\n        fnty, name=""req_array_setitem"")\n    return builder.call(fn, args)\n\n# @lower_builtin(distributed_api.dist_setitem, types.Array, types.Any, types.Any,\n#     types.intp, types.intp)\n# def dist_setitem_array(context, builder, sig, args):\n#     """"""add check for access to be in processor bounds of the array""""""\n#     # TODO: replace array shape if array is small\n#     #  (processor chuncks smaller than setitem range causes index normalization)\n#     # remove start and count args to call regular get_item_pointer2\n#     count = args.pop()\n#     start = args.pop()\n#     sig.args = tuple([sig.args[0], sig.args[1], sig.args[2]])\n#     regular_get_item_pointer2 = cgutils.get_item_pointer2\n#     # add bounds check for distributed access,\n#     # return a dummy pointer if out of bounds\n#     def dist_get_item_pointer2(builder, data, shape, strides, layout, inds,\n#                       wraparound=False):\n#         # get local index or -1 if out of bounds\n#         fnty = lir.FunctionType(lir.IntType(64), [lir.IntType(64), lir.IntType(64), lir.IntType(64)])\n#         fn = builder.module.get_or_insert_function(fnty, name=""hpat_dist_get_item_pointer"")\n#         first_ind = builder.call(fn, [inds[0], start, count])\n#         inds = tuple([first_ind, *inds[1:]])\n#         # regular local pointer with new indices\n#         in_ptr = regular_get_item_pointer2(builder, data, shape, strides, layout, inds, wraparound)\n#         ret_ptr = cgutils.alloca_once(builder, in_ptr.type)\n#         builder.store(in_ptr, ret_ptr)\n#         not_inbound = builder.icmp_signed(\'==\', first_ind, lir.Constant(lir.IntType(64), -1))\n#         # get dummy pointer\n#         dummy_fnty  = lir.FunctionType(lir.IntType(8).as_pointer(), [])\n#         dummy_fn = builder.module.get_or_insert_function(dummy_fnty, name=""hpat_get_dummy_ptr"")\n#         dummy_ptr = builder.bitcast(builder.call(dummy_fn, []), in_ptr.type)\n#         with builder.if_then(not_inbound, likely=True):\n#             builder.store(dummy_ptr, ret_ptr)\n#         return builder.load(ret_ptr)\n#\n#     # replace inner array access call for setitem generation\n#     cgutils.get_item_pointer2 = dist_get_item_pointer2\n#     numba.np.arrayobj.setitem_array(context, builder, sig, args)\n#     cgutils.get_item_pointer2 = regular_get_item_pointer2\n#     return lir.Constant(lir.IntType(32), 0)\n\n# find overlapping range of an input range (start:stop) and a chunk range\n# (chunk_start:chunk_start+chunk_count). Inputs are assumed positive.\n# output is set to empty range of local range goes out of bounds\n\n\n@numba.njit\ndef _get_local_range(start, stop, chunk_start, chunk_count):  # pragma: no cover\n    assert start >= 0 and stop > 0\n    new_start = max(start, chunk_start)\n    new_stop = min(stop, chunk_start + chunk_count)\n    loc_start = new_start - chunk_start\n    loc_stop = new_stop - chunk_start\n    if loc_start < 0 or loc_stop < 0:\n        loc_start = 1\n        loc_stop = 0\n    return loc_start, loc_stop\n\n\n@numba.njit\ndef _set_if_in_range(A, val, index, chunk_start, chunk_count):  # pragma: no cover\n    if index >= chunk_start and index < chunk_start + chunk_count:\n        A[index - chunk_start] = val\n\n\n@numba.njit\ndef _root_rank_select(old_val, new_val):  # pragma: no cover\n    if distributed_api.get_rank() == 0:\n        return old_val\n    return new_val\n\n\ndef get_tuple_prod(t):\n    return np.prod(t)\n\n\n@overload(get_tuple_prod)\ndef get_tuple_prod_overload(t):\n    # handle empty tuple seperately since empty getiter doesn\'t work\n    if t == numba.types.containers.Tuple(()):\n        return lambda t: 1\n\n    def get_tuple_prod_impl(t):\n        res = 1\n        for a in t:\n            res *= a\n        return res\n\n    return get_tuple_prod_impl\n\n\nsig = types.void(\n    types.voidptr,  # output array\n    types.voidptr,  # input array\n    types.intp,     # old_len\n    types.intp,     # new_len\n    types.intp,     # input lower_dim size in bytes\n    types.intp,     # output lower_dim size in bytes\n)\n\noneD_reshape_shuffle = types.ExternalFunction(""oneD_reshape_shuffle"", sig)\n\n\n@numba.njit\ndef dist_oneD_reshape_shuffle(lhs, in_arr, new_0dim_global_len, old_0dim_global_len, dtype_size):  # pragma: no cover\n    c_in_arr = np.ascontiguousarray(in_arr)\n    in_lower_dims_size = get_tuple_prod(c_in_arr.shape[1:])\n    out_lower_dims_size = get_tuple_prod(lhs.shape[1:])\n    # print(c_in_arr)\n    # print(new_0dim_global_len, old_0dim_global_len, out_lower_dims_size, in_lower_dims_size)\n    oneD_reshape_shuffle(lhs.ctypes, c_in_arr.ctypes,\n                         new_0dim_global_len, old_0dim_global_len,\n                         dtype_size * out_lower_dims_size,\n                         dtype_size * in_lower_dims_size)\n    # print(in_arr)\n\n\npermutation_int = types.ExternalFunction(""permutation_int"",\n                                         types.void(types.voidptr, types.intp))\n\n\n@numba.njit\ndef dist_permutation_int(lhs, n):\n    permutation_int(lhs.ctypes, n)\n\n\npermutation_array_index = types.ExternalFunction(""permutation_array_index"",\n                                                 types.void(types.voidptr,\n                                                            types.intp,\n                                                            types.intp,\n                                                            types.voidptr,\n                                                            types.voidptr,\n                                                            types.intp))\n\n\n@numba.njit\ndef dist_permutation_array_index(lhs, lhs_len, dtype_size, rhs, p, p_len):\n    c_rhs = np.ascontiguousarray(rhs)\n    lower_dims_size = get_tuple_prod(c_rhs.shape[1:])\n    elem_size = dtype_size * lower_dims_size\n    permutation_array_index(lhs.ctypes, lhs_len, elem_size, c_rhs.ctypes,\n                            p.ctypes, p_len)\n\n# ********* finalize MPI when exiting ********************\n\n\ndef hpat_finalize():\n    return 0\n\n\n@infer_global(hpat_finalize)\nclass FinalizeInfer(AbstractTemplate):\n    def generic(self, args, kws):\n        assert not kws\n        assert len(args) == 0\n        return signature(types.int32, *args)\n\n\n@lower_builtin(hpat_finalize)\ndef lower_hpat_finalize(context, builder, sig, args):\n    fnty = lir.FunctionType(lir.IntType(32), [])\n    fn = builder.module.get_or_insert_function(fnty, name=""hpat_finalize"")\n    return builder.call(fn, args)\n\n\n@numba.njit\ndef call_finalize():\n    hpat_finalize()\n\n\nif sdc.config.config_pipeline_hpat_default:\n    atexit.register(call_finalize)\n    # flush output before finalize\n    atexit.register(sys.stdout.flush)\n'"
sdc/runtests.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\nimport os\nimport unittest\nimport sdc.tests\nfrom sdc.tests.test_basic import get_rank\n\n""""""\n    Every test in suite can be executed specified times using\n    desired value for SDC_REPEAT_TEST_NUMBER environment variable.\n    This can be used to locate scpecific failures occured\n    on next execution of affected test.\n\n    loadTestsFromModule returns TestSuite obj with _tests member\n    which contains further TestSuite instanses for each found testCase:\n    hpat_tests = TestSuite(sdc.tests)\n    TestSuite(sdc.tests)._tests = [TestSuite(sdc.tests.TestBasic), TestSuite(sdc.tests.TestDataFrame), ...]\n    TestSuite(sdc.tests.TestBasic)._tests = [TestBasic testMethod=test_array_reduce, ...]\n""""""\n\n\ndef load_tests(loader, tests, pattern):\n    suite = unittest.TestSuite()\n    hpat_tests = loader.loadTestsFromModule(sdc.tests)\n    repeat_test_number = int(os.getenv(\'SDC_REPEAT_TEST_NUMBER\', \'1\'))\n\n    if repeat_test_number > 1:\n        for i, test_case in enumerate(hpat_tests):\n            extended_tests = []\n            for test in test_case:\n                for _ in range(repeat_test_number):\n                    extended_tests.append(test)\n            hpat_tests._tests[i]._tests = extended_tests\n\n    suite.addTests(hpat_tests)\n    return suite\n\n\nif __name__ == \'__main__\':\n    # initialize MPI to avoid ""Attempting to use an MPI routine before initializing MPICH"" in any pipeline\n    get_rank()\n\n    unittest.main()\n'"
sdc/sdc_autogenerated.py,0,"b'# -*- coding: utf-8 -*-\n# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\n\n| This file contains overloads for various extension types auto-generated with autogen_sources.py\n\n""""""\n\nimport numba\nimport numpy\nimport operator\nimport pandas\n\nfrom numba.core.errors import TypingError\nfrom numba import types\n\nfrom sdc.utilities.sdc_typing_utils import (TypeChecker, check_index_is_numeric, check_types_comparable,\n                                            find_common_dtype_from_numpy_dtypes)\nfrom sdc.datatypes.common_functions import (sdc_join_series_indexes, sdc_check_indexes_equal)\nfrom sdc.hiframes.pd_series_type import SeriesType\nfrom sdc.str_arr_ext import (string_array_type, str_arr_is_na)\nfrom sdc.utilities.utils import sdc_overload, sdc_overload_method\nfrom sdc.functions import numpy_like\n\n\n@sdc_overload_method(SeriesType, \'add\')\ndef sdc_pandas_series_add(self, other, level=None, fill_value=None, axis=0):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.add\n\n    Limitations\n    -----------\n    Parameters ``level`` and ``axis`` are currently unsupported by Intel Scalable Dataframe Compiler\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_add.py\n       :language: python\n       :lines: 27-\n       :caption:\n       :name: ex_series_add\n\n    .. command-output:: python ./series/series_add.py\n       :cwd: ../../../examples\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.add` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests sdc.tests.test_series.TestSeries.test_series_op5\n    """"""\n\n    ty_checker = TypeChecker(\'Method add().\')\n    self_is_series, other_is_series = isinstance(self, SeriesType), isinstance(other, SeriesType)\n    if not (self_is_series or other_is_series):\n        return None\n\n    # this overload is not for string series\n    self_is_string_series = self_is_series and isinstance(self.dtype, types.UnicodeType)\n    other_is_string_series = other_is_series and isinstance(other.dtype, types.UnicodeType)\n    if self_is_string_series or other_is_string_series:\n        return None\n\n    if not isinstance(self, (SeriesType, types.Number)):\n        ty_checker.raise_exc(self, \'pandas.series or scalar\', \'self\')\n\n    if not isinstance(other, (SeriesType, types.Number)):\n        ty_checker.raise_exc(other, \'pandas.series or scalar\', \'other\')\n\n    operands_are_series = self_is_series and other_is_series\n    if operands_are_series:\n        none_or_numeric_indexes = ((isinstance(self.index, types.NoneType) or check_index_is_numeric(self))\n                                   and (isinstance(other.index, types.NoneType) or check_index_is_numeric(other)))\n        series_indexes_comparable = check_types_comparable(self.index, other.index) or none_or_numeric_indexes\n        if not series_indexes_comparable:\n            raise TypingError(\'{} Not implemented for series with not-comparable indexes. \\\n            Given: self.index={}, other.index={}\'.format(_func_name, self.index, other.index))\n\n    series_data_comparable = check_types_comparable(self, other)\n    if not series_data_comparable:\n        raise TypingError(\'{} Not supported for not-comparable operands. \\\n        Given: self={}, other={}\'.format(_func_name, self, other))\n\n    if not isinstance(level, types.Omitted) and level is not None:\n        ty_checker.raise_exc(level, \'None\', \'level\')\n\n    if not isinstance(fill_value, (types.Omitted, types.Number, types.NoneType)) and fill_value is not None:\n        ty_checker.raise_exc(fill_value, \'number\', \'fill_value\')\n\n    if not isinstance(axis, types.Omitted) and axis != 0:\n        ty_checker.raise_exc(axis, \'int\', \'axis\')\n    fill_value_is_none = isinstance(fill_value, (types.NoneType, types.Omitted)) or fill_value is None\n    # specializations for numeric series only\n    if not operands_are_series:\n        def _series_add_scalar_impl(self, other, level=None, fill_value=None, axis=0):\n            if self_is_series == True:  # noqa\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                result_data = numpy.empty(len(self._data), dtype=numpy.float64)\n                result_data[:] = self._data + numpy.float64(other)\n                return pandas.Series(result_data, index=self._index, name=self._name)\n            else:\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n                result_data = numpy.empty(len(other._data), dtype=numpy.float64)\n                result_data[:] = numpy.float64(self) + other._data\n                return pandas.Series(result_data, index=other._index, name=other._name)\n\n        return _series_add_scalar_impl\n\n    else:   # both operands are numeric series\n        # optimization for series with default indexes, that can be aligned differently\n        if (isinstance(self.index, types.NoneType) and isinstance(other.index, types.NoneType)):\n            def _series_add_none_indexes_impl(self, other, level=None, fill_value=None, axis=0):\n                _fill_value = numpy.nan if fill_value_is_none == True else fill_value  # noqa\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n\n                if (len(self._data) == len(other._data)):\n                    result_data = numpy_like.astype(self._data, numpy.float64)\n                    result_data = result_data + other._data\n                    return pandas.Series(result_data)\n                else:\n                    left_size, right_size = len(self._data), len(other._data)\n                    min_data_size = min(left_size, right_size)\n                    max_data_size = max(left_size, right_size)\n                    result_data = numpy.empty(max_data_size, dtype=numpy.float64)\n                    if (left_size == min_data_size):\n                        result_data[:min_data_size] = self._data\n                        for i in range(min_data_size, len(result_data)):\n                            result_data[i] = _fill_value\n                        result_data = result_data + other._data\n                    else:\n                        result_data[:min_data_size] = other._data\n                        for i in range(min_data_size, len(result_data)):\n                            result_data[i] = _fill_value\n                        result_data = self._data + result_data\n\n                    return pandas.Series(result_data)\n\n            return _series_add_none_indexes_impl\n        else:\n            # for numeric indexes find common dtype to be used when creating joined index\n            if none_or_numeric_indexes:\n                ty_left_index_dtype = types.int64 if isinstance(self.index, types.NoneType) else self.index.dtype\n                ty_right_index_dtype = types.int64 if isinstance(other.index, types.NoneType) else other.index.dtype\n                numba_index_common_dtype = find_common_dtype_from_numpy_dtypes(\n                    [ty_left_index_dtype, ty_right_index_dtype], [])\n\n            def _series_add_common_impl(self, other, level=None, fill_value=None, axis=0):\n                left_index, right_index = self.index, other.index\n                _fill_value = numpy.nan if fill_value_is_none == True else fill_value  # noqa\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n                # check if indexes are equal and series don\'t have to be aligned\n                if sdc_check_indexes_equal(left_index, right_index):\n                    result_data = numpy.empty(len(self._data), dtype=numpy.float64)\n                    result_data[:] = self._data + other._data\n\n                    if none_or_numeric_indexes == True:  # noqa\n                        result_index = numpy_like.astype(left_index, numba_index_common_dtype)\n                    else:\n                        result_index = self._index\n\n                    return pandas.Series(result_data, index=result_index)\n\n                # TODO: replace below with core join(how=\'outer\', return_indexers=True) when implemented\n                joined_index, left_indexer, right_indexer = sdc_join_series_indexes(left_index, right_index)\n                result_size = len(joined_index)\n                left_values = numpy.empty(result_size, dtype=numpy.float64)\n                right_values = numpy.empty(result_size, dtype=numpy.float64)\n                for i in range(result_size):\n                    left_pos, right_pos = left_indexer[i], right_indexer[i]\n                    left_values[i] = self._data[left_pos] if left_pos != -1 else _fill_value\n                    right_values[i] = other._data[right_pos] if right_pos != -1 else _fill_value\n                result_data = left_values + right_values\n                return pandas.Series(result_data, joined_index)\n\n            return _series_add_common_impl\n\n    return None\n\n\n@sdc_overload_method(SeriesType, \'div\')\ndef sdc_pandas_series_div(self, other, level=None, fill_value=None, axis=0):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.div\n\n    Limitations\n    -----------\n    Parameters ``level`` and ``axis`` are currently unsupported by Intel Scalable Dataframe Compiler\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_div.py\n       :language: python\n       :lines: 27-\n       :caption:\n       :name: ex_series_div\n\n    .. command-output:: python ./series/series_div.py\n       :cwd: ../../../examples\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.div` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests sdc.tests.test_series.TestSeries.test_series_op5\n    """"""\n\n    ty_checker = TypeChecker(\'Method div().\')\n    self_is_series, other_is_series = isinstance(self, SeriesType), isinstance(other, SeriesType)\n    if not (self_is_series or other_is_series):\n        return None\n\n    # this overload is not for string series\n    self_is_string_series = self_is_series and isinstance(self.dtype, types.UnicodeType)\n    other_is_string_series = other_is_series and isinstance(other.dtype, types.UnicodeType)\n    if self_is_string_series or other_is_string_series:\n        return None\n\n    if not isinstance(self, (SeriesType, types.Number)):\n        ty_checker.raise_exc(self, \'pandas.series or scalar\', \'self\')\n\n    if not isinstance(other, (SeriesType, types.Number)):\n        ty_checker.raise_exc(other, \'pandas.series or scalar\', \'other\')\n\n    operands_are_series = self_is_series and other_is_series\n    if operands_are_series:\n        none_or_numeric_indexes = ((isinstance(self.index, types.NoneType) or check_index_is_numeric(self))\n                                   and (isinstance(other.index, types.NoneType) or check_index_is_numeric(other)))\n        series_indexes_comparable = check_types_comparable(self.index, other.index) or none_or_numeric_indexes\n        if not series_indexes_comparable:\n            raise TypingError(\'{} Not implemented for series with not-comparable indexes. \\\n            Given: self.index={}, other.index={}\'.format(_func_name, self.index, other.index))\n\n    series_data_comparable = check_types_comparable(self, other)\n    if not series_data_comparable:\n        raise TypingError(\'{} Not supported for not-comparable operands. \\\n        Given: self={}, other={}\'.format(_func_name, self, other))\n\n    if not isinstance(level, types.Omitted) and level is not None:\n        ty_checker.raise_exc(level, \'None\', \'level\')\n\n    if not isinstance(fill_value, (types.Omitted, types.Number, types.NoneType)) and fill_value is not None:\n        ty_checker.raise_exc(fill_value, \'number\', \'fill_value\')\n\n    if not isinstance(axis, types.Omitted) and axis != 0:\n        ty_checker.raise_exc(axis, \'int\', \'axis\')\n    fill_value_is_none = isinstance(fill_value, (types.NoneType, types.Omitted)) or fill_value is None\n    # specializations for numeric series only\n    if not operands_are_series:\n        def _series_div_scalar_impl(self, other, level=None, fill_value=None, axis=0):\n            if self_is_series == True:  # noqa\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                result_data = numpy.empty(len(self._data), dtype=numpy.float64)\n                result_data[:] = self._data / numpy.float64(other)\n                return pandas.Series(result_data, index=self._index, name=self._name)\n            else:\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n                result_data = numpy.empty(len(other._data), dtype=numpy.float64)\n                result_data[:] = numpy.float64(self) / other._data\n                return pandas.Series(result_data, index=other._index, name=other._name)\n\n        return _series_div_scalar_impl\n\n    else:   # both operands are numeric series\n        # optimization for series with default indexes, that can be aligned differently\n        if (isinstance(self.index, types.NoneType) and isinstance(other.index, types.NoneType)):\n            def _series_div_none_indexes_impl(self, other, level=None, fill_value=None, axis=0):\n                _fill_value = numpy.nan if fill_value_is_none == True else fill_value  # noqa\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n\n                if (len(self._data) == len(other._data)):\n                    result_data = numpy_like.astype(self._data, numpy.float64)\n                    result_data = result_data / other._data\n                    return pandas.Series(result_data)\n                else:\n                    left_size, right_size = len(self._data), len(other._data)\n                    min_data_size = min(left_size, right_size)\n                    max_data_size = max(left_size, right_size)\n                    result_data = numpy.empty(max_data_size, dtype=numpy.float64)\n                    if (left_size == min_data_size):\n                        result_data[:min_data_size] = self._data\n                        for i in range(min_data_size, len(result_data)):\n                            result_data[i] = _fill_value\n                        result_data = result_data / other._data\n                    else:\n                        result_data[:min_data_size] = other._data\n                        for i in range(min_data_size, len(result_data)):\n                            result_data[i] = _fill_value\n                        result_data = self._data / result_data\n\n                    return pandas.Series(result_data)\n\n            return _series_div_none_indexes_impl\n        else:\n            # for numeric indexes find common dtype to be used when creating joined index\n            if none_or_numeric_indexes:\n                ty_left_index_dtype = types.int64 if isinstance(self.index, types.NoneType) else self.index.dtype\n                ty_right_index_dtype = types.int64 if isinstance(other.index, types.NoneType) else other.index.dtype\n                numba_index_common_dtype = find_common_dtype_from_numpy_dtypes(\n                    [ty_left_index_dtype, ty_right_index_dtype], [])\n\n            def _series_div_common_impl(self, other, level=None, fill_value=None, axis=0):\n                left_index, right_index = self.index, other.index\n                _fill_value = numpy.nan if fill_value_is_none == True else fill_value  # noqa\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n                # check if indexes are equal and series don\'t have to be aligned\n                if sdc_check_indexes_equal(left_index, right_index):\n                    result_data = numpy.empty(len(self._data), dtype=numpy.float64)\n                    result_data[:] = self._data / other._data\n\n                    if none_or_numeric_indexes == True:  # noqa\n                        result_index = numpy_like.astype(left_index, numba_index_common_dtype)\n                    else:\n                        result_index = self._index\n\n                    return pandas.Series(result_data, index=result_index)\n\n                # TODO: replace below with core join(how=\'outer\', return_indexers=True) when implemented\n                joined_index, left_indexer, right_indexer = sdc_join_series_indexes(left_index, right_index)\n                result_size = len(joined_index)\n                left_values = numpy.empty(result_size, dtype=numpy.float64)\n                right_values = numpy.empty(result_size, dtype=numpy.float64)\n                for i in range(result_size):\n                    left_pos, right_pos = left_indexer[i], right_indexer[i]\n                    left_values[i] = self._data[left_pos] if left_pos != -1 else _fill_value\n                    right_values[i] = other._data[right_pos] if right_pos != -1 else _fill_value\n                result_data = left_values / right_values\n                return pandas.Series(result_data, joined_index)\n\n            return _series_div_common_impl\n\n    return None\n\n\n@sdc_overload_method(SeriesType, \'sub\')\ndef sdc_pandas_series_sub(self, other, level=None, fill_value=None, axis=0):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.sub\n\n    Limitations\n    -----------\n    Parameters ``level`` and ``axis`` are currently unsupported by Intel Scalable Dataframe Compiler\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_sub.py\n       :language: python\n       :lines: 27-\n       :caption:\n       :name: ex_series_sub\n\n    .. command-output:: python ./series/series_sub.py\n       :cwd: ../../../examples\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.sub` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests sdc.tests.test_series.TestSeries.test_series_op5\n    """"""\n\n    ty_checker = TypeChecker(\'Method sub().\')\n    self_is_series, other_is_series = isinstance(self, SeriesType), isinstance(other, SeriesType)\n    if not (self_is_series or other_is_series):\n        return None\n\n    # this overload is not for string series\n    self_is_string_series = self_is_series and isinstance(self.dtype, types.UnicodeType)\n    other_is_string_series = other_is_series and isinstance(other.dtype, types.UnicodeType)\n    if self_is_string_series or other_is_string_series:\n        return None\n\n    if not isinstance(self, (SeriesType, types.Number)):\n        ty_checker.raise_exc(self, \'pandas.series or scalar\', \'self\')\n\n    if not isinstance(other, (SeriesType, types.Number)):\n        ty_checker.raise_exc(other, \'pandas.series or scalar\', \'other\')\n\n    operands_are_series = self_is_series and other_is_series\n    if operands_are_series:\n        none_or_numeric_indexes = ((isinstance(self.index, types.NoneType) or check_index_is_numeric(self))\n                                   and (isinstance(other.index, types.NoneType) or check_index_is_numeric(other)))\n        series_indexes_comparable = check_types_comparable(self.index, other.index) or none_or_numeric_indexes\n        if not series_indexes_comparable:\n            raise TypingError(\'{} Not implemented for series with not-comparable indexes. \\\n            Given: self.index={}, other.index={}\'.format(_func_name, self.index, other.index))\n\n    series_data_comparable = check_types_comparable(self, other)\n    if not series_data_comparable:\n        raise TypingError(\'{} Not supported for not-comparable operands. \\\n        Given: self={}, other={}\'.format(_func_name, self, other))\n\n    if not isinstance(level, types.Omitted) and level is not None:\n        ty_checker.raise_exc(level, \'None\', \'level\')\n\n    if not isinstance(fill_value, (types.Omitted, types.Number, types.NoneType)) and fill_value is not None:\n        ty_checker.raise_exc(fill_value, \'number\', \'fill_value\')\n\n    if not isinstance(axis, types.Omitted) and axis != 0:\n        ty_checker.raise_exc(axis, \'int\', \'axis\')\n    fill_value_is_none = isinstance(fill_value, (types.NoneType, types.Omitted)) or fill_value is None\n    # specializations for numeric series only\n    if not operands_are_series:\n        def _series_sub_scalar_impl(self, other, level=None, fill_value=None, axis=0):\n            if self_is_series == True:  # noqa\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                result_data = numpy.empty(len(self._data), dtype=numpy.float64)\n                result_data[:] = self._data - numpy.float64(other)\n                return pandas.Series(result_data, index=self._index, name=self._name)\n            else:\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n                result_data = numpy.empty(len(other._data), dtype=numpy.float64)\n                result_data[:] = numpy.float64(self) - other._data\n                return pandas.Series(result_data, index=other._index, name=other._name)\n\n        return _series_sub_scalar_impl\n\n    else:   # both operands are numeric series\n        # optimization for series with default indexes, that can be aligned differently\n        if (isinstance(self.index, types.NoneType) and isinstance(other.index, types.NoneType)):\n            def _series_sub_none_indexes_impl(self, other, level=None, fill_value=None, axis=0):\n                _fill_value = numpy.nan if fill_value_is_none == True else fill_value  # noqa\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n\n                if (len(self._data) == len(other._data)):\n                    result_data = numpy_like.astype(self._data, numpy.float64)\n                    result_data = result_data - other._data\n                    return pandas.Series(result_data)\n                else:\n                    left_size, right_size = len(self._data), len(other._data)\n                    min_data_size = min(left_size, right_size)\n                    max_data_size = max(left_size, right_size)\n                    result_data = numpy.empty(max_data_size, dtype=numpy.float64)\n                    if (left_size == min_data_size):\n                        result_data[:min_data_size] = self._data\n                        for i in range(min_data_size, len(result_data)):\n                            result_data[i] = _fill_value\n                        result_data = result_data - other._data\n                    else:\n                        result_data[:min_data_size] = other._data\n                        for i in range(min_data_size, len(result_data)):\n                            result_data[i] = _fill_value\n                        result_data = self._data - result_data\n\n                    return pandas.Series(result_data)\n\n            return _series_sub_none_indexes_impl\n        else:\n            # for numeric indexes find common dtype to be used when creating joined index\n            if none_or_numeric_indexes:\n                ty_left_index_dtype = types.int64 if isinstance(self.index, types.NoneType) else self.index.dtype\n                ty_right_index_dtype = types.int64 if isinstance(other.index, types.NoneType) else other.index.dtype\n                numba_index_common_dtype = find_common_dtype_from_numpy_dtypes(\n                    [ty_left_index_dtype, ty_right_index_dtype], [])\n\n            def _series_sub_common_impl(self, other, level=None, fill_value=None, axis=0):\n                left_index, right_index = self.index, other.index\n                _fill_value = numpy.nan if fill_value_is_none == True else fill_value  # noqa\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n                # check if indexes are equal and series don\'t have to be aligned\n                if sdc_check_indexes_equal(left_index, right_index):\n                    result_data = numpy.empty(len(self._data), dtype=numpy.float64)\n                    result_data[:] = self._data - other._data\n\n                    if none_or_numeric_indexes == True:  # noqa\n                        result_index = numpy_like.astype(left_index, numba_index_common_dtype)\n                    else:\n                        result_index = self._index\n\n                    return pandas.Series(result_data, index=result_index)\n\n                # TODO: replace below with core join(how=\'outer\', return_indexers=True) when implemented\n                joined_index, left_indexer, right_indexer = sdc_join_series_indexes(left_index, right_index)\n                result_size = len(joined_index)\n                left_values = numpy.empty(result_size, dtype=numpy.float64)\n                right_values = numpy.empty(result_size, dtype=numpy.float64)\n                for i in range(result_size):\n                    left_pos, right_pos = left_indexer[i], right_indexer[i]\n                    left_values[i] = self._data[left_pos] if left_pos != -1 else _fill_value\n                    right_values[i] = other._data[right_pos] if right_pos != -1 else _fill_value\n                result_data = left_values - right_values\n                return pandas.Series(result_data, joined_index)\n\n            return _series_sub_common_impl\n\n    return None\n\n\n@sdc_overload_method(SeriesType, \'mul\')\ndef sdc_pandas_series_mul(self, other, level=None, fill_value=None, axis=0):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.mul\n\n    Limitations\n    -----------\n    Parameters ``level`` and ``axis`` are currently unsupported by Intel Scalable Dataframe Compiler\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_mul.py\n       :language: python\n       :lines: 27-\n       :caption:\n       :name: ex_series_mul\n\n    .. command-output:: python ./series/series_mul.py\n       :cwd: ../../../examples\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.mul` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests sdc.tests.test_series.TestSeries.test_series_op5\n    """"""\n\n    ty_checker = TypeChecker(\'Method mul().\')\n    self_is_series, other_is_series = isinstance(self, SeriesType), isinstance(other, SeriesType)\n    if not (self_is_series or other_is_series):\n        return None\n\n    # this overload is not for string series\n    self_is_string_series = self_is_series and isinstance(self.dtype, types.UnicodeType)\n    other_is_string_series = other_is_series and isinstance(other.dtype, types.UnicodeType)\n    if self_is_string_series or other_is_string_series:\n        return None\n\n    if not isinstance(self, (SeriesType, types.Number)):\n        ty_checker.raise_exc(self, \'pandas.series or scalar\', \'self\')\n\n    if not isinstance(other, (SeriesType, types.Number)):\n        ty_checker.raise_exc(other, \'pandas.series or scalar\', \'other\')\n\n    operands_are_series = self_is_series and other_is_series\n    if operands_are_series:\n        none_or_numeric_indexes = ((isinstance(self.index, types.NoneType) or check_index_is_numeric(self))\n                                   and (isinstance(other.index, types.NoneType) or check_index_is_numeric(other)))\n        series_indexes_comparable = check_types_comparable(self.index, other.index) or none_or_numeric_indexes\n        if not series_indexes_comparable:\n            raise TypingError(\'{} Not implemented for series with not-comparable indexes. \\\n            Given: self.index={}, other.index={}\'.format(_func_name, self.index, other.index))\n\n    series_data_comparable = check_types_comparable(self, other)\n    if not series_data_comparable:\n        raise TypingError(\'{} Not supported for not-comparable operands. \\\n        Given: self={}, other={}\'.format(_func_name, self, other))\n\n    if not isinstance(level, types.Omitted) and level is not None:\n        ty_checker.raise_exc(level, \'None\', \'level\')\n\n    if not isinstance(fill_value, (types.Omitted, types.Number, types.NoneType)) and fill_value is not None:\n        ty_checker.raise_exc(fill_value, \'number\', \'fill_value\')\n\n    if not isinstance(axis, types.Omitted) and axis != 0:\n        ty_checker.raise_exc(axis, \'int\', \'axis\')\n    fill_value_is_none = isinstance(fill_value, (types.NoneType, types.Omitted)) or fill_value is None\n    # specializations for numeric series only\n    if not operands_are_series:\n        def _series_mul_scalar_impl(self, other, level=None, fill_value=None, axis=0):\n            if self_is_series == True:  # noqa\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                result_data = numpy.empty(len(self._data), dtype=numpy.float64)\n                result_data[:] = self._data * numpy.float64(other)\n                return pandas.Series(result_data, index=self._index, name=self._name)\n            else:\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n                result_data = numpy.empty(len(other._data), dtype=numpy.float64)\n                result_data[:] = numpy.float64(self) * other._data\n                return pandas.Series(result_data, index=other._index, name=other._name)\n\n        return _series_mul_scalar_impl\n\n    else:   # both operands are numeric series\n        # optimization for series with default indexes, that can be aligned differently\n        if (isinstance(self.index, types.NoneType) and isinstance(other.index, types.NoneType)):\n            def _series_mul_none_indexes_impl(self, other, level=None, fill_value=None, axis=0):\n                _fill_value = numpy.nan if fill_value_is_none == True else fill_value  # noqa\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n\n                if (len(self._data) == len(other._data)):\n                    result_data = numpy_like.astype(self._data, numpy.float64)\n                    result_data = result_data * other._data\n                    return pandas.Series(result_data)\n                else:\n                    left_size, right_size = len(self._data), len(other._data)\n                    min_data_size = min(left_size, right_size)\n                    max_data_size = max(left_size, right_size)\n                    result_data = numpy.empty(max_data_size, dtype=numpy.float64)\n                    if (left_size == min_data_size):\n                        result_data[:min_data_size] = self._data\n                        for i in range(min_data_size, len(result_data)):\n                            result_data[i] = _fill_value\n                        result_data = result_data * other._data\n                    else:\n                        result_data[:min_data_size] = other._data\n                        for i in range(min_data_size, len(result_data)):\n                            result_data[i] = _fill_value\n                        result_data = self._data * result_data\n\n                    return pandas.Series(result_data)\n\n            return _series_mul_none_indexes_impl\n        else:\n            # for numeric indexes find common dtype to be used when creating joined index\n            if none_or_numeric_indexes:\n                ty_left_index_dtype = types.int64 if isinstance(self.index, types.NoneType) else self.index.dtype\n                ty_right_index_dtype = types.int64 if isinstance(other.index, types.NoneType) else other.index.dtype\n                numba_index_common_dtype = find_common_dtype_from_numpy_dtypes(\n                    [ty_left_index_dtype, ty_right_index_dtype], [])\n\n            def _series_mul_common_impl(self, other, level=None, fill_value=None, axis=0):\n                left_index, right_index = self.index, other.index\n                _fill_value = numpy.nan if fill_value_is_none == True else fill_value  # noqa\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n                # check if indexes are equal and series don\'t have to be aligned\n                if sdc_check_indexes_equal(left_index, right_index):\n                    result_data = numpy.empty(len(self._data), dtype=numpy.float64)\n                    result_data[:] = self._data * other._data\n\n                    if none_or_numeric_indexes == True:  # noqa\n                        result_index = numpy_like.astype(left_index, numba_index_common_dtype)\n                    else:\n                        result_index = self._index\n\n                    return pandas.Series(result_data, index=result_index)\n\n                # TODO: replace below with core join(how=\'outer\', return_indexers=True) when implemented\n                joined_index, left_indexer, right_indexer = sdc_join_series_indexes(left_index, right_index)\n                result_size = len(joined_index)\n                left_values = numpy.empty(result_size, dtype=numpy.float64)\n                right_values = numpy.empty(result_size, dtype=numpy.float64)\n                for i in range(result_size):\n                    left_pos, right_pos = left_indexer[i], right_indexer[i]\n                    left_values[i] = self._data[left_pos] if left_pos != -1 else _fill_value\n                    right_values[i] = other._data[right_pos] if right_pos != -1 else _fill_value\n                result_data = left_values * right_values\n                return pandas.Series(result_data, joined_index)\n\n            return _series_mul_common_impl\n\n    return None\n\n\n@sdc_overload_method(SeriesType, \'truediv\')\ndef sdc_pandas_series_truediv(self, other, level=None, fill_value=None, axis=0):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.truediv\n\n    Limitations\n    -----------\n    Parameters ``level`` and ``axis`` are currently unsupported by Intel Scalable Dataframe Compiler\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_truediv.py\n       :language: python\n       :lines: 27-\n       :caption:\n       :name: ex_series_truediv\n\n    .. command-output:: python ./series/series_truediv.py\n       :cwd: ../../../examples\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.truediv` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests sdc.tests.test_series.TestSeries.test_series_op5\n    """"""\n\n    ty_checker = TypeChecker(\'Method truediv().\')\n    self_is_series, other_is_series = isinstance(self, SeriesType), isinstance(other, SeriesType)\n    if not (self_is_series or other_is_series):\n        return None\n\n    # this overload is not for string series\n    self_is_string_series = self_is_series and isinstance(self.dtype, types.UnicodeType)\n    other_is_string_series = other_is_series and isinstance(other.dtype, types.UnicodeType)\n    if self_is_string_series or other_is_string_series:\n        return None\n\n    if not isinstance(self, (SeriesType, types.Number)):\n        ty_checker.raise_exc(self, \'pandas.series or scalar\', \'self\')\n\n    if not isinstance(other, (SeriesType, types.Number)):\n        ty_checker.raise_exc(other, \'pandas.series or scalar\', \'other\')\n\n    operands_are_series = self_is_series and other_is_series\n    if operands_are_series:\n        none_or_numeric_indexes = ((isinstance(self.index, types.NoneType) or check_index_is_numeric(self))\n                                   and (isinstance(other.index, types.NoneType) or check_index_is_numeric(other)))\n        series_indexes_comparable = check_types_comparable(self.index, other.index) or none_or_numeric_indexes\n        if not series_indexes_comparable:\n            raise TypingError(\'{} Not implemented for series with not-comparable indexes. \\\n            Given: self.index={}, other.index={}\'.format(_func_name, self.index, other.index))\n\n    series_data_comparable = check_types_comparable(self, other)\n    if not series_data_comparable:\n        raise TypingError(\'{} Not supported for not-comparable operands. \\\n        Given: self={}, other={}\'.format(_func_name, self, other))\n\n    if not isinstance(level, types.Omitted) and level is not None:\n        ty_checker.raise_exc(level, \'None\', \'level\')\n\n    if not isinstance(fill_value, (types.Omitted, types.Number, types.NoneType)) and fill_value is not None:\n        ty_checker.raise_exc(fill_value, \'number\', \'fill_value\')\n\n    if not isinstance(axis, types.Omitted) and axis != 0:\n        ty_checker.raise_exc(axis, \'int\', \'axis\')\n    fill_value_is_none = isinstance(fill_value, (types.NoneType, types.Omitted)) or fill_value is None\n    # specializations for numeric series only\n    if not operands_are_series:\n        def _series_truediv_scalar_impl(self, other, level=None, fill_value=None, axis=0):\n            if self_is_series == True:  # noqa\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                result_data = numpy.empty(len(self._data), dtype=numpy.float64)\n                result_data[:] = self._data / numpy.float64(other)\n                return pandas.Series(result_data, index=self._index, name=self._name)\n            else:\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n                result_data = numpy.empty(len(other._data), dtype=numpy.float64)\n                result_data[:] = numpy.float64(self) / other._data\n                return pandas.Series(result_data, index=other._index, name=other._name)\n\n        return _series_truediv_scalar_impl\n\n    else:   # both operands are numeric series\n        # optimization for series with default indexes, that can be aligned differently\n        if (isinstance(self.index, types.NoneType) and isinstance(other.index, types.NoneType)):\n            def _series_truediv_none_indexes_impl(self, other, level=None, fill_value=None, axis=0):\n                _fill_value = numpy.nan if fill_value_is_none == True else fill_value  # noqa\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n\n                if (len(self._data) == len(other._data)):\n                    result_data = numpy_like.astype(self._data, numpy.float64)\n                    result_data = result_data / other._data\n                    return pandas.Series(result_data)\n                else:\n                    left_size, right_size = len(self._data), len(other._data)\n                    min_data_size = min(left_size, right_size)\n                    max_data_size = max(left_size, right_size)\n                    result_data = numpy.empty(max_data_size, dtype=numpy.float64)\n                    if (left_size == min_data_size):\n                        result_data[:min_data_size] = self._data\n                        for i in range(min_data_size, len(result_data)):\n                            result_data[i] = _fill_value\n                        result_data = result_data / other._data\n                    else:\n                        result_data[:min_data_size] = other._data\n                        for i in range(min_data_size, len(result_data)):\n                            result_data[i] = _fill_value\n                        result_data = self._data / result_data\n\n                    return pandas.Series(result_data)\n\n            return _series_truediv_none_indexes_impl\n        else:\n            # for numeric indexes find common dtype to be used when creating joined index\n            if none_or_numeric_indexes:\n                ty_left_index_dtype = types.int64 if isinstance(self.index, types.NoneType) else self.index.dtype\n                ty_right_index_dtype = types.int64 if isinstance(other.index, types.NoneType) else other.index.dtype\n                numba_index_common_dtype = find_common_dtype_from_numpy_dtypes(\n                    [ty_left_index_dtype, ty_right_index_dtype], [])\n\n            def _series_truediv_common_impl(self, other, level=None, fill_value=None, axis=0):\n                left_index, right_index = self.index, other.index\n                _fill_value = numpy.nan if fill_value_is_none == True else fill_value  # noqa\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n                # check if indexes are equal and series don\'t have to be aligned\n                if sdc_check_indexes_equal(left_index, right_index):\n                    result_data = numpy.empty(len(self._data), dtype=numpy.float64)\n                    result_data[:] = self._data / other._data\n\n                    if none_or_numeric_indexes == True:  # noqa\n                        result_index = numpy_like.astype(left_index, numba_index_common_dtype)\n                    else:\n                        result_index = self._index\n\n                    return pandas.Series(result_data, index=result_index)\n\n                # TODO: replace below with core join(how=\'outer\', return_indexers=True) when implemented\n                joined_index, left_indexer, right_indexer = sdc_join_series_indexes(left_index, right_index)\n                result_size = len(joined_index)\n                left_values = numpy.empty(result_size, dtype=numpy.float64)\n                right_values = numpy.empty(result_size, dtype=numpy.float64)\n                for i in range(result_size):\n                    left_pos, right_pos = left_indexer[i], right_indexer[i]\n                    left_values[i] = self._data[left_pos] if left_pos != -1 else _fill_value\n                    right_values[i] = other._data[right_pos] if right_pos != -1 else _fill_value\n                result_data = left_values / right_values\n                return pandas.Series(result_data, joined_index)\n\n            return _series_truediv_common_impl\n\n    return None\n\n\n@sdc_overload_method(SeriesType, \'floordiv\')\ndef sdc_pandas_series_floordiv(self, other, level=None, fill_value=None, axis=0):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.floordiv\n\n    Limitations\n    -----------\n    Parameters ``level`` and ``axis`` are currently unsupported by Intel Scalable Dataframe Compiler\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_floordiv.py\n       :language: python\n       :lines: 27-\n       :caption:\n       :name: ex_series_floordiv\n\n    .. command-output:: python ./series/series_floordiv.py\n       :cwd: ../../../examples\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.floordiv` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests sdc.tests.test_series.TestSeries.test_series_op5\n    """"""\n\n    ty_checker = TypeChecker(\'Method floordiv().\')\n    self_is_series, other_is_series = isinstance(self, SeriesType), isinstance(other, SeriesType)\n    if not (self_is_series or other_is_series):\n        return None\n\n    # this overload is not for string series\n    self_is_string_series = self_is_series and isinstance(self.dtype, types.UnicodeType)\n    other_is_string_series = other_is_series and isinstance(other.dtype, types.UnicodeType)\n    if self_is_string_series or other_is_string_series:\n        return None\n\n    if not isinstance(self, (SeriesType, types.Number)):\n        ty_checker.raise_exc(self, \'pandas.series or scalar\', \'self\')\n\n    if not isinstance(other, (SeriesType, types.Number)):\n        ty_checker.raise_exc(other, \'pandas.series or scalar\', \'other\')\n\n    operands_are_series = self_is_series and other_is_series\n    if operands_are_series:\n        none_or_numeric_indexes = ((isinstance(self.index, types.NoneType) or check_index_is_numeric(self))\n                                   and (isinstance(other.index, types.NoneType) or check_index_is_numeric(other)))\n        series_indexes_comparable = check_types_comparable(self.index, other.index) or none_or_numeric_indexes\n        if not series_indexes_comparable:\n            raise TypingError(\'{} Not implemented for series with not-comparable indexes. \\\n            Given: self.index={}, other.index={}\'.format(_func_name, self.index, other.index))\n\n    series_data_comparable = check_types_comparable(self, other)\n    if not series_data_comparable:\n        raise TypingError(\'{} Not supported for not-comparable operands. \\\n        Given: self={}, other={}\'.format(_func_name, self, other))\n\n    if not isinstance(level, types.Omitted) and level is not None:\n        ty_checker.raise_exc(level, \'None\', \'level\')\n\n    if not isinstance(fill_value, (types.Omitted, types.Number, types.NoneType)) and fill_value is not None:\n        ty_checker.raise_exc(fill_value, \'number\', \'fill_value\')\n\n    if not isinstance(axis, types.Omitted) and axis != 0:\n        ty_checker.raise_exc(axis, \'int\', \'axis\')\n    fill_value_is_none = isinstance(fill_value, (types.NoneType, types.Omitted)) or fill_value is None\n    # specializations for numeric series only\n    if not operands_are_series:\n        def _series_floordiv_scalar_impl(self, other, level=None, fill_value=None, axis=0):\n            if self_is_series == True:  # noqa\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                result_data = numpy.empty(len(self._data), dtype=numpy.float64)\n                result_data[:] = self._data // numpy.float64(other)\n                return pandas.Series(result_data, index=self._index, name=self._name)\n            else:\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n                result_data = numpy.empty(len(other._data), dtype=numpy.float64)\n                result_data[:] = numpy.float64(self) // other._data\n                return pandas.Series(result_data, index=other._index, name=other._name)\n\n        return _series_floordiv_scalar_impl\n\n    else:   # both operands are numeric series\n        # optimization for series with default indexes, that can be aligned differently\n        if (isinstance(self.index, types.NoneType) and isinstance(other.index, types.NoneType)):\n            def _series_floordiv_none_indexes_impl(self, other, level=None, fill_value=None, axis=0):\n                _fill_value = numpy.nan if fill_value_is_none == True else fill_value  # noqa\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n\n                if (len(self._data) == len(other._data)):\n                    result_data = numpy_like.astype(self._data, numpy.float64)\n                    result_data = result_data // other._data\n                    return pandas.Series(result_data)\n                else:\n                    left_size, right_size = len(self._data), len(other._data)\n                    min_data_size = min(left_size, right_size)\n                    max_data_size = max(left_size, right_size)\n                    result_data = numpy.empty(max_data_size, dtype=numpy.float64)\n                    if (left_size == min_data_size):\n                        result_data[:min_data_size] = self._data\n                        for i in range(min_data_size, len(result_data)):\n                            result_data[i] = _fill_value\n                        result_data = result_data // other._data\n                    else:\n                        result_data[:min_data_size] = other._data\n                        for i in range(min_data_size, len(result_data)):\n                            result_data[i] = _fill_value\n                        result_data = self._data // result_data\n\n                    return pandas.Series(result_data)\n\n            return _series_floordiv_none_indexes_impl\n        else:\n            # for numeric indexes find common dtype to be used when creating joined index\n            if none_or_numeric_indexes:\n                ty_left_index_dtype = types.int64 if isinstance(self.index, types.NoneType) else self.index.dtype\n                ty_right_index_dtype = types.int64 if isinstance(other.index, types.NoneType) else other.index.dtype\n                numba_index_common_dtype = find_common_dtype_from_numpy_dtypes(\n                    [ty_left_index_dtype, ty_right_index_dtype], [])\n\n            def _series_floordiv_common_impl(self, other, level=None, fill_value=None, axis=0):\n                left_index, right_index = self.index, other.index\n                _fill_value = numpy.nan if fill_value_is_none == True else fill_value  # noqa\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n                # check if indexes are equal and series don\'t have to be aligned\n                if sdc_check_indexes_equal(left_index, right_index):\n                    result_data = numpy.empty(len(self._data), dtype=numpy.float64)\n                    result_data[:] = self._data // other._data\n\n                    if none_or_numeric_indexes == True:  # noqa\n                        result_index = numpy_like.astype(left_index, numba_index_common_dtype)\n                    else:\n                        result_index = self._index\n\n                    return pandas.Series(result_data, index=result_index)\n\n                # TODO: replace below with core join(how=\'outer\', return_indexers=True) when implemented\n                joined_index, left_indexer, right_indexer = sdc_join_series_indexes(left_index, right_index)\n                result_size = len(joined_index)\n                left_values = numpy.empty(result_size, dtype=numpy.float64)\n                right_values = numpy.empty(result_size, dtype=numpy.float64)\n                for i in range(result_size):\n                    left_pos, right_pos = left_indexer[i], right_indexer[i]\n                    left_values[i] = self._data[left_pos] if left_pos != -1 else _fill_value\n                    right_values[i] = other._data[right_pos] if right_pos != -1 else _fill_value\n                result_data = left_values // right_values\n                return pandas.Series(result_data, joined_index)\n\n            return _series_floordiv_common_impl\n\n    return None\n\n\n@sdc_overload_method(SeriesType, \'mod\')\ndef sdc_pandas_series_mod(self, other, level=None, fill_value=None, axis=0):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.mod\n\n    Limitations\n    -----------\n    Parameters ``level`` and ``axis`` are currently unsupported by Intel Scalable Dataframe Compiler\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_mod.py\n       :language: python\n       :lines: 27-\n       :caption:\n       :name: ex_series_mod\n\n    .. command-output:: python ./series/series_mod.py\n       :cwd: ../../../examples\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.mod` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests sdc.tests.test_series.TestSeries.test_series_op5\n    """"""\n\n    ty_checker = TypeChecker(\'Method mod().\')\n    self_is_series, other_is_series = isinstance(self, SeriesType), isinstance(other, SeriesType)\n    if not (self_is_series or other_is_series):\n        return None\n\n    # this overload is not for string series\n    self_is_string_series = self_is_series and isinstance(self.dtype, types.UnicodeType)\n    other_is_string_series = other_is_series and isinstance(other.dtype, types.UnicodeType)\n    if self_is_string_series or other_is_string_series:\n        return None\n\n    if not isinstance(self, (SeriesType, types.Number)):\n        ty_checker.raise_exc(self, \'pandas.series or scalar\', \'self\')\n\n    if not isinstance(other, (SeriesType, types.Number)):\n        ty_checker.raise_exc(other, \'pandas.series or scalar\', \'other\')\n\n    operands_are_series = self_is_series and other_is_series\n    if operands_are_series:\n        none_or_numeric_indexes = ((isinstance(self.index, types.NoneType) or check_index_is_numeric(self))\n                                   and (isinstance(other.index, types.NoneType) or check_index_is_numeric(other)))\n        series_indexes_comparable = check_types_comparable(self.index, other.index) or none_or_numeric_indexes\n        if not series_indexes_comparable:\n            raise TypingError(\'{} Not implemented for series with not-comparable indexes. \\\n            Given: self.index={}, other.index={}\'.format(_func_name, self.index, other.index))\n\n    series_data_comparable = check_types_comparable(self, other)\n    if not series_data_comparable:\n        raise TypingError(\'{} Not supported for not-comparable operands. \\\n        Given: self={}, other={}\'.format(_func_name, self, other))\n\n    if not isinstance(level, types.Omitted) and level is not None:\n        ty_checker.raise_exc(level, \'None\', \'level\')\n\n    if not isinstance(fill_value, (types.Omitted, types.Number, types.NoneType)) and fill_value is not None:\n        ty_checker.raise_exc(fill_value, \'number\', \'fill_value\')\n\n    if not isinstance(axis, types.Omitted) and axis != 0:\n        ty_checker.raise_exc(axis, \'int\', \'axis\')\n    fill_value_is_none = isinstance(fill_value, (types.NoneType, types.Omitted)) or fill_value is None\n    # specializations for numeric series only\n    if not operands_are_series:\n        def _series_mod_scalar_impl(self, other, level=None, fill_value=None, axis=0):\n            if self_is_series == True:  # noqa\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                result_data = numpy.empty(len(self._data), dtype=numpy.float64)\n                result_data[:] = self._data % numpy.float64(other)\n                return pandas.Series(result_data, index=self._index, name=self._name)\n            else:\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n                result_data = numpy.empty(len(other._data), dtype=numpy.float64)\n                result_data[:] = numpy.float64(self) % other._data\n                return pandas.Series(result_data, index=other._index, name=other._name)\n\n        return _series_mod_scalar_impl\n\n    else:   # both operands are numeric series\n        # optimization for series with default indexes, that can be aligned differently\n        if (isinstance(self.index, types.NoneType) and isinstance(other.index, types.NoneType)):\n            def _series_mod_none_indexes_impl(self, other, level=None, fill_value=None, axis=0):\n                _fill_value = numpy.nan if fill_value_is_none == True else fill_value  # noqa\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n\n                if (len(self._data) == len(other._data)):\n                    result_data = numpy_like.astype(self._data, numpy.float64)\n                    result_data = result_data % other._data\n                    return pandas.Series(result_data)\n                else:\n                    left_size, right_size = len(self._data), len(other._data)\n                    min_data_size = min(left_size, right_size)\n                    max_data_size = max(left_size, right_size)\n                    result_data = numpy.empty(max_data_size, dtype=numpy.float64)\n                    if (left_size == min_data_size):\n                        result_data[:min_data_size] = self._data\n                        for i in range(min_data_size, len(result_data)):\n                            result_data[i] = _fill_value\n                        result_data = result_data % other._data\n                    else:\n                        result_data[:min_data_size] = other._data\n                        for i in range(min_data_size, len(result_data)):\n                            result_data[i] = _fill_value\n                        result_data = self._data % result_data\n\n                    return pandas.Series(result_data)\n\n            return _series_mod_none_indexes_impl\n        else:\n            # for numeric indexes find common dtype to be used when creating joined index\n            if none_or_numeric_indexes:\n                ty_left_index_dtype = types.int64 if isinstance(self.index, types.NoneType) else self.index.dtype\n                ty_right_index_dtype = types.int64 if isinstance(other.index, types.NoneType) else other.index.dtype\n                numba_index_common_dtype = find_common_dtype_from_numpy_dtypes(\n                    [ty_left_index_dtype, ty_right_index_dtype], [])\n\n            def _series_mod_common_impl(self, other, level=None, fill_value=None, axis=0):\n                left_index, right_index = self.index, other.index\n                _fill_value = numpy.nan if fill_value_is_none == True else fill_value  # noqa\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n                # check if indexes are equal and series don\'t have to be aligned\n                if sdc_check_indexes_equal(left_index, right_index):\n                    result_data = numpy.empty(len(self._data), dtype=numpy.float64)\n                    result_data[:] = self._data % other._data\n\n                    if none_or_numeric_indexes == True:  # noqa\n                        result_index = numpy_like.astype(left_index, numba_index_common_dtype)\n                    else:\n                        result_index = self._index\n\n                    return pandas.Series(result_data, index=result_index)\n\n                # TODO: replace below with core join(how=\'outer\', return_indexers=True) when implemented\n                joined_index, left_indexer, right_indexer = sdc_join_series_indexes(left_index, right_index)\n                result_size = len(joined_index)\n                left_values = numpy.empty(result_size, dtype=numpy.float64)\n                right_values = numpy.empty(result_size, dtype=numpy.float64)\n                for i in range(result_size):\n                    left_pos, right_pos = left_indexer[i], right_indexer[i]\n                    left_values[i] = self._data[left_pos] if left_pos != -1 else _fill_value\n                    right_values[i] = other._data[right_pos] if right_pos != -1 else _fill_value\n                result_data = left_values % right_values\n                return pandas.Series(result_data, joined_index)\n\n            return _series_mod_common_impl\n\n    return None\n\n\n@sdc_overload_method(SeriesType, \'pow\')\ndef sdc_pandas_series_pow(self, other, level=None, fill_value=None, axis=0):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.pow\n\n    Limitations\n    -----------\n    Parameters ``level`` and ``axis`` are currently unsupported by Intel Scalable Dataframe Compiler\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_pow.py\n       :language: python\n       :lines: 27-\n       :caption:\n       :name: ex_series_pow\n\n    .. command-output:: python ./series/series_pow.py\n       :cwd: ../../../examples\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.pow` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests sdc.tests.test_series.TestSeries.test_series_op5\n    """"""\n\n    ty_checker = TypeChecker(\'Method pow().\')\n    self_is_series, other_is_series = isinstance(self, SeriesType), isinstance(other, SeriesType)\n    if not (self_is_series or other_is_series):\n        return None\n\n    # this overload is not for string series\n    self_is_string_series = self_is_series and isinstance(self.dtype, types.UnicodeType)\n    other_is_string_series = other_is_series and isinstance(other.dtype, types.UnicodeType)\n    if self_is_string_series or other_is_string_series:\n        return None\n\n    if not isinstance(self, (SeriesType, types.Number)):\n        ty_checker.raise_exc(self, \'pandas.series or scalar\', \'self\')\n\n    if not isinstance(other, (SeriesType, types.Number)):\n        ty_checker.raise_exc(other, \'pandas.series or scalar\', \'other\')\n\n    operands_are_series = self_is_series and other_is_series\n    if operands_are_series:\n        none_or_numeric_indexes = ((isinstance(self.index, types.NoneType) or check_index_is_numeric(self))\n                                   and (isinstance(other.index, types.NoneType) or check_index_is_numeric(other)))\n        series_indexes_comparable = check_types_comparable(self.index, other.index) or none_or_numeric_indexes\n        if not series_indexes_comparable:\n            raise TypingError(\'{} Not implemented for series with not-comparable indexes. \\\n            Given: self.index={}, other.index={}\'.format(_func_name, self.index, other.index))\n\n    series_data_comparable = check_types_comparable(self, other)\n    if not series_data_comparable:\n        raise TypingError(\'{} Not supported for not-comparable operands. \\\n        Given: self={}, other={}\'.format(_func_name, self, other))\n\n    if not isinstance(level, types.Omitted) and level is not None:\n        ty_checker.raise_exc(level, \'None\', \'level\')\n\n    if not isinstance(fill_value, (types.Omitted, types.Number, types.NoneType)) and fill_value is not None:\n        ty_checker.raise_exc(fill_value, \'number\', \'fill_value\')\n\n    if not isinstance(axis, types.Omitted) and axis != 0:\n        ty_checker.raise_exc(axis, \'int\', \'axis\')\n    fill_value_is_none = isinstance(fill_value, (types.NoneType, types.Omitted)) or fill_value is None\n    # specializations for numeric series only\n    if not operands_are_series:\n        def _series_pow_scalar_impl(self, other, level=None, fill_value=None, axis=0):\n            if self_is_series == True:  # noqa\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                result_data = numpy.empty(len(self._data), dtype=numpy.float64)\n                result_data[:] = self._data ** numpy.float64(other)\n                return pandas.Series(result_data, index=self._index, name=self._name)\n            else:\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n                result_data = numpy.empty(len(other._data), dtype=numpy.float64)\n                result_data[:] = numpy.float64(self) ** other._data\n                return pandas.Series(result_data, index=other._index, name=other._name)\n\n        return _series_pow_scalar_impl\n\n    else:   # both operands are numeric series\n        # optimization for series with default indexes, that can be aligned differently\n        if (isinstance(self.index, types.NoneType) and isinstance(other.index, types.NoneType)):\n            def _series_pow_none_indexes_impl(self, other, level=None, fill_value=None, axis=0):\n                _fill_value = numpy.nan if fill_value_is_none == True else fill_value  # noqa\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n\n                if (len(self._data) == len(other._data)):\n                    result_data = numpy_like.astype(self._data, numpy.float64)\n                    result_data = result_data ** other._data\n                    return pandas.Series(result_data)\n                else:\n                    left_size, right_size = len(self._data), len(other._data)\n                    min_data_size = min(left_size, right_size)\n                    max_data_size = max(left_size, right_size)\n                    result_data = numpy.empty(max_data_size, dtype=numpy.float64)\n                    if (left_size == min_data_size):\n                        result_data[:min_data_size] = self._data\n                        for i in range(min_data_size, len(result_data)):\n                            result_data[i] = _fill_value\n                        result_data = result_data ** other._data\n                    else:\n                        result_data[:min_data_size] = other._data\n                        for i in range(min_data_size, len(result_data)):\n                            result_data[i] = _fill_value\n                        result_data = self._data ** result_data\n\n                    return pandas.Series(result_data)\n\n            return _series_pow_none_indexes_impl\n        else:\n            # for numeric indexes find common dtype to be used when creating joined index\n            if none_or_numeric_indexes:\n                ty_left_index_dtype = types.int64 if isinstance(self.index, types.NoneType) else self.index.dtype\n                ty_right_index_dtype = types.int64 if isinstance(other.index, types.NoneType) else other.index.dtype\n                numba_index_common_dtype = find_common_dtype_from_numpy_dtypes(\n                    [ty_left_index_dtype, ty_right_index_dtype], [])\n\n            def _series_pow_common_impl(self, other, level=None, fill_value=None, axis=0):\n                left_index, right_index = self.index, other.index\n                _fill_value = numpy.nan if fill_value_is_none == True else fill_value  # noqa\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n                # check if indexes are equal and series don\'t have to be aligned\n                if sdc_check_indexes_equal(left_index, right_index):\n                    result_data = numpy.empty(len(self._data), dtype=numpy.float64)\n                    result_data[:] = self._data ** other._data\n\n                    if none_or_numeric_indexes == True:  # noqa\n                        result_index = numpy_like.astype(left_index, numba_index_common_dtype)\n                    else:\n                        result_index = self._index\n\n                    return pandas.Series(result_data, index=result_index)\n\n                # TODO: replace below with core join(how=\'outer\', return_indexers=True) when implemented\n                joined_index, left_indexer, right_indexer = sdc_join_series_indexes(left_index, right_index)\n                result_size = len(joined_index)\n                left_values = numpy.empty(result_size, dtype=numpy.float64)\n                right_values = numpy.empty(result_size, dtype=numpy.float64)\n                for i in range(result_size):\n                    left_pos, right_pos = left_indexer[i], right_indexer[i]\n                    left_values[i] = self._data[left_pos] if left_pos != -1 else _fill_value\n                    right_values[i] = other._data[right_pos] if right_pos != -1 else _fill_value\n                result_data = left_values ** right_values\n                return pandas.Series(result_data, joined_index)\n\n            return _series_pow_common_impl\n\n    return None\n\n\n@sdc_overload_method(SeriesType, \'lt\')\ndef sdc_pandas_series_lt(self, other, level=None, fill_value=None, axis=0):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.lt\n\n    Limitations\n    -----------\n    Parameters ``level`` and ``axis`` are currently unsupported by Intel Scalable Dataframe Compiler\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_lt.py\n       :language: python\n       :lines: 27-\n       :caption:\n       :name: ex_series_lt\n\n    .. command-output:: python ./series/series_lt.py\n       :cwd: ../../../examples\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.lt` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_op8\n    """"""\n\n    _func_name = \'Method lt().\'\n\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    if not (isinstance(level, types.Omitted) or level is None):\n        ty_checker.raise_exc(level, \'None\', \'level\')\n\n    if not isinstance(fill_value, (types.Omitted, types.Number, types.NoneType)) and fill_value is not None:\n        ty_checker.raise_exc(fill_value, \'number\', \'fill_value\')\n\n    if not (isinstance(axis, types.Omitted) or axis == 0):\n        ty_checker.raise_exc(axis, \'int\', \'axis\')\n\n    self_is_series, other_is_series = isinstance(self, SeriesType), isinstance(other, SeriesType)\n    if not (self_is_series or other_is_series):\n        return None\n\n    if not isinstance(self, (SeriesType, types.Number, types.UnicodeType)):\n        ty_checker.raise_exc(self, \'pandas.series or scalar\', \'self\')\n\n    if not isinstance(other, (SeriesType, types.Number, types.UnicodeType)):\n        ty_checker.raise_exc(other, \'pandas.series or scalar\', \'other\')\n\n    operands_are_series = self_is_series and other_is_series\n    if operands_are_series:\n        none_or_numeric_indexes = ((isinstance(self.index, types.NoneType) or check_index_is_numeric(self))\n                                   and (isinstance(other.index, types.NoneType) or check_index_is_numeric(other)))\n        series_indexes_comparable = check_types_comparable(self.index, other.index) or none_or_numeric_indexes\n        if not series_indexes_comparable:\n            raise TypingError(\'{} Not implemented for series with not-comparable indexes. \\\n            Given: self.index={}, other.index={}\'.format(_func_name, self.index, other.index))\n\n    series_data_comparable = check_types_comparable(self, other)\n    if not series_data_comparable:\n        raise TypingError(\'{} Not supported for not-comparable operands. \\\n        Given: self={}, other={}\'.format(_func_name, self, other))\n\n    fill_value_is_none = isinstance(fill_value, (types.NoneType, types.Omitted)) or fill_value is None\n    if not operands_are_series:\n        def _series_lt_scalar_impl(self, other, level=None, fill_value=None, axis=0):\n            if self_is_series == True:  # noqa\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                return pandas.Series(self._data < other, index=self._index, name=self._name)\n            else:\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n                return pandas.Series(self < other._data, index=other._index, name=other._name)\n\n        return _series_lt_scalar_impl\n\n    else:\n\n        # optimization for series with default indexes, that can be aligned differently\n        if (isinstance(self.index, types.NoneType) and isinstance(other.index, types.NoneType)):\n            def _series_lt_none_indexes_impl(self, other, level=None, fill_value=None, axis=0):\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n                left_size, right_size = len(self._data), len(other._data)\n                if (left_size == right_size):\n                    return pandas.Series(self._data < other._data)\n                else:\n                    raise ValueError(""Can only compare identically-labeled Series objects"")\n\n            return _series_lt_none_indexes_impl\n        else:\n\n            if none_or_numeric_indexes:\n                ty_left_index_dtype = types.int64 if isinstance(self.index, types.NoneType) else self.index.dtype\n                ty_right_index_dtype = types.int64 if isinstance(other.index, types.NoneType) else other.index.dtype\n                numba_index_common_dtype = find_common_dtype_from_numpy_dtypes(\n                    [ty_left_index_dtype, ty_right_index_dtype], [])\n\n            def _series_lt_common_impl(self, other, level=None, fill_value=None, axis=0):\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n                left_index, right_index = self.index, other.index\n\n                if sdc_check_indexes_equal(left_index, right_index):\n                    if none_or_numeric_indexes == True:  # noqa\n                        new_index = numpy_like.astype(left_index, numba_index_common_dtype)\n                    else:\n                        new_index = self._index\n                    return pandas.Series(self._data < other._data,\n                                         new_index)\n                else:\n                    raise ValueError(""Can only compare identically-labeled Series objects"")\n\n            return _series_lt_common_impl\n\n    return None\n\n\n@sdc_overload_method(SeriesType, \'gt\')\ndef sdc_pandas_series_gt(self, other, level=None, fill_value=None, axis=0):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.gt\n\n    Limitations\n    -----------\n    Parameters ``level`` and ``axis`` are currently unsupported by Intel Scalable Dataframe Compiler\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_gt.py\n       :language: python\n       :lines: 27-\n       :caption:\n       :name: ex_series_gt\n\n    .. command-output:: python ./series/series_gt.py\n       :cwd: ../../../examples\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.gt` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_op8\n    """"""\n\n    _func_name = \'Method gt().\'\n\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    if not (isinstance(level, types.Omitted) or level is None):\n        ty_checker.raise_exc(level, \'None\', \'level\')\n\n    if not isinstance(fill_value, (types.Omitted, types.Number, types.NoneType)) and fill_value is not None:\n        ty_checker.raise_exc(fill_value, \'number\', \'fill_value\')\n\n    if not (isinstance(axis, types.Omitted) or axis == 0):\n        ty_checker.raise_exc(axis, \'int\', \'axis\')\n\n    self_is_series, other_is_series = isinstance(self, SeriesType), isinstance(other, SeriesType)\n    if not (self_is_series or other_is_series):\n        return None\n\n    if not isinstance(self, (SeriesType, types.Number, types.UnicodeType)):\n        ty_checker.raise_exc(self, \'pandas.series or scalar\', \'self\')\n\n    if not isinstance(other, (SeriesType, types.Number, types.UnicodeType)):\n        ty_checker.raise_exc(other, \'pandas.series or scalar\', \'other\')\n\n    operands_are_series = self_is_series and other_is_series\n    if operands_are_series:\n        none_or_numeric_indexes = ((isinstance(self.index, types.NoneType) or check_index_is_numeric(self))\n                                   and (isinstance(other.index, types.NoneType) or check_index_is_numeric(other)))\n        series_indexes_comparable = check_types_comparable(self.index, other.index) or none_or_numeric_indexes\n        if not series_indexes_comparable:\n            raise TypingError(\'{} Not implemented for series with not-comparable indexes. \\\n            Given: self.index={}, other.index={}\'.format(_func_name, self.index, other.index))\n\n    series_data_comparable = check_types_comparable(self, other)\n    if not series_data_comparable:\n        raise TypingError(\'{} Not supported for not-comparable operands. \\\n        Given: self={}, other={}\'.format(_func_name, self, other))\n\n    fill_value_is_none = isinstance(fill_value, (types.NoneType, types.Omitted)) or fill_value is None\n    if not operands_are_series:\n        def _series_gt_scalar_impl(self, other, level=None, fill_value=None, axis=0):\n            if self_is_series == True:  # noqa\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                return pandas.Series(self._data > other, index=self._index, name=self._name)\n            else:\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n                return pandas.Series(self > other._data, index=other._index, name=other._name)\n\n        return _series_gt_scalar_impl\n\n    else:\n\n        # optimization for series with default indexes, that can be aligned differently\n        if (isinstance(self.index, types.NoneType) and isinstance(other.index, types.NoneType)):\n            def _series_gt_none_indexes_impl(self, other, level=None, fill_value=None, axis=0):\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n                left_size, right_size = len(self._data), len(other._data)\n                if (left_size == right_size):\n                    return pandas.Series(self._data > other._data)\n                else:\n                    raise ValueError(""Can only compare identically-labeled Series objects"")\n\n            return _series_gt_none_indexes_impl\n        else:\n\n            if none_or_numeric_indexes:\n                ty_left_index_dtype = types.int64 if isinstance(self.index, types.NoneType) else self.index.dtype\n                ty_right_index_dtype = types.int64 if isinstance(other.index, types.NoneType) else other.index.dtype\n                numba_index_common_dtype = find_common_dtype_from_numpy_dtypes(\n                    [ty_left_index_dtype, ty_right_index_dtype], [])\n\n            def _series_gt_common_impl(self, other, level=None, fill_value=None, axis=0):\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n                left_index, right_index = self.index, other.index\n\n                if sdc_check_indexes_equal(left_index, right_index):\n                    if none_or_numeric_indexes == True:  # noqa\n                        new_index = numpy_like.astype(left_index, numba_index_common_dtype)\n                    else:\n                        new_index = self._index\n                    return pandas.Series(self._data > other._data,\n                                         new_index)\n                else:\n                    raise ValueError(""Can only compare identically-labeled Series objects"")\n\n            return _series_gt_common_impl\n\n    return None\n\n\n@sdc_overload_method(SeriesType, \'le\')\ndef sdc_pandas_series_le(self, other, level=None, fill_value=None, axis=0):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.le\n\n    Limitations\n    -----------\n    Parameters ``level`` and ``axis`` are currently unsupported by Intel Scalable Dataframe Compiler\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_le.py\n       :language: python\n       :lines: 27-\n       :caption:\n       :name: ex_series_le\n\n    .. command-output:: python ./series/series_le.py\n       :cwd: ../../../examples\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.le` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_op8\n    """"""\n\n    _func_name = \'Method le().\'\n\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    if not (isinstance(level, types.Omitted) or level is None):\n        ty_checker.raise_exc(level, \'None\', \'level\')\n\n    if not isinstance(fill_value, (types.Omitted, types.Number, types.NoneType)) and fill_value is not None:\n        ty_checker.raise_exc(fill_value, \'number\', \'fill_value\')\n\n    if not (isinstance(axis, types.Omitted) or axis == 0):\n        ty_checker.raise_exc(axis, \'int\', \'axis\')\n\n    self_is_series, other_is_series = isinstance(self, SeriesType), isinstance(other, SeriesType)\n    if not (self_is_series or other_is_series):\n        return None\n\n    if not isinstance(self, (SeriesType, types.Number, types.UnicodeType)):\n        ty_checker.raise_exc(self, \'pandas.series or scalar\', \'self\')\n\n    if not isinstance(other, (SeriesType, types.Number, types.UnicodeType)):\n        ty_checker.raise_exc(other, \'pandas.series or scalar\', \'other\')\n\n    operands_are_series = self_is_series and other_is_series\n    if operands_are_series:\n        none_or_numeric_indexes = ((isinstance(self.index, types.NoneType) or check_index_is_numeric(self))\n                                   and (isinstance(other.index, types.NoneType) or check_index_is_numeric(other)))\n        series_indexes_comparable = check_types_comparable(self.index, other.index) or none_or_numeric_indexes\n        if not series_indexes_comparable:\n            raise TypingError(\'{} Not implemented for series with not-comparable indexes. \\\n            Given: self.index={}, other.index={}\'.format(_func_name, self.index, other.index))\n\n    series_data_comparable = check_types_comparable(self, other)\n    if not series_data_comparable:\n        raise TypingError(\'{} Not supported for not-comparable operands. \\\n        Given: self={}, other={}\'.format(_func_name, self, other))\n\n    fill_value_is_none = isinstance(fill_value, (types.NoneType, types.Omitted)) or fill_value is None\n    if not operands_are_series:\n        def _series_le_scalar_impl(self, other, level=None, fill_value=None, axis=0):\n            if self_is_series == True:  # noqa\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                return pandas.Series(self._data <= other, index=self._index, name=self._name)\n            else:\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n                return pandas.Series(self <= other._data, index=other._index, name=other._name)\n\n        return _series_le_scalar_impl\n\n    else:\n\n        # optimization for series with default indexes, that can be aligned differently\n        if (isinstance(self.index, types.NoneType) and isinstance(other.index, types.NoneType)):\n            def _series_le_none_indexes_impl(self, other, level=None, fill_value=None, axis=0):\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n                left_size, right_size = len(self._data), len(other._data)\n                if (left_size == right_size):\n                    return pandas.Series(self._data <= other._data)\n                else:\n                    raise ValueError(""Can only compare identically-labeled Series objects"")\n\n            return _series_le_none_indexes_impl\n        else:\n\n            if none_or_numeric_indexes:\n                ty_left_index_dtype = types.int64 if isinstance(self.index, types.NoneType) else self.index.dtype\n                ty_right_index_dtype = types.int64 if isinstance(other.index, types.NoneType) else other.index.dtype\n                numba_index_common_dtype = find_common_dtype_from_numpy_dtypes(\n                    [ty_left_index_dtype, ty_right_index_dtype], [])\n\n            def _series_le_common_impl(self, other, level=None, fill_value=None, axis=0):\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n                left_index, right_index = self.index, other.index\n\n                if sdc_check_indexes_equal(left_index, right_index):\n                    if none_or_numeric_indexes == True:  # noqa\n                        new_index = numpy_like.astype(left_index, numba_index_common_dtype)\n                    else:\n                        new_index = self._index\n                    return pandas.Series(self._data <= other._data,\n                                         new_index)\n                else:\n                    raise ValueError(""Can only compare identically-labeled Series objects"")\n\n            return _series_le_common_impl\n\n    return None\n\n\n@sdc_overload_method(SeriesType, \'ge\')\ndef sdc_pandas_series_ge(self, other, level=None, fill_value=None, axis=0):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.ge\n\n    Limitations\n    -----------\n    Parameters ``level`` and ``axis`` are currently unsupported by Intel Scalable Dataframe Compiler\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_ge.py\n       :language: python\n       :lines: 27-\n       :caption:\n       :name: ex_series_ge\n\n    .. command-output:: python ./series/series_ge.py\n       :cwd: ../../../examples\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.ge` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_op8\n    """"""\n\n    _func_name = \'Method ge().\'\n\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    if not (isinstance(level, types.Omitted) or level is None):\n        ty_checker.raise_exc(level, \'None\', \'level\')\n\n    if not isinstance(fill_value, (types.Omitted, types.Number, types.NoneType)) and fill_value is not None:\n        ty_checker.raise_exc(fill_value, \'number\', \'fill_value\')\n\n    if not (isinstance(axis, types.Omitted) or axis == 0):\n        ty_checker.raise_exc(axis, \'int\', \'axis\')\n\n    self_is_series, other_is_series = isinstance(self, SeriesType), isinstance(other, SeriesType)\n    if not (self_is_series or other_is_series):\n        return None\n\n    if not isinstance(self, (SeriesType, types.Number, types.UnicodeType)):\n        ty_checker.raise_exc(self, \'pandas.series or scalar\', \'self\')\n\n    if not isinstance(other, (SeriesType, types.Number, types.UnicodeType)):\n        ty_checker.raise_exc(other, \'pandas.series or scalar\', \'other\')\n\n    operands_are_series = self_is_series and other_is_series\n    if operands_are_series:\n        none_or_numeric_indexes = ((isinstance(self.index, types.NoneType) or check_index_is_numeric(self))\n                                   and (isinstance(other.index, types.NoneType) or check_index_is_numeric(other)))\n        series_indexes_comparable = check_types_comparable(self.index, other.index) or none_or_numeric_indexes\n        if not series_indexes_comparable:\n            raise TypingError(\'{} Not implemented for series with not-comparable indexes. \\\n            Given: self.index={}, other.index={}\'.format(_func_name, self.index, other.index))\n\n    series_data_comparable = check_types_comparable(self, other)\n    if not series_data_comparable:\n        raise TypingError(\'{} Not supported for not-comparable operands. \\\n        Given: self={}, other={}\'.format(_func_name, self, other))\n\n    fill_value_is_none = isinstance(fill_value, (types.NoneType, types.Omitted)) or fill_value is None\n    if not operands_are_series:\n        def _series_ge_scalar_impl(self, other, level=None, fill_value=None, axis=0):\n            if self_is_series == True:  # noqa\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                return pandas.Series(self._data >= other, index=self._index, name=self._name)\n            else:\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n                return pandas.Series(self >= other._data, index=other._index, name=other._name)\n\n        return _series_ge_scalar_impl\n\n    else:\n\n        # optimization for series with default indexes, that can be aligned differently\n        if (isinstance(self.index, types.NoneType) and isinstance(other.index, types.NoneType)):\n            def _series_ge_none_indexes_impl(self, other, level=None, fill_value=None, axis=0):\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n                left_size, right_size = len(self._data), len(other._data)\n                if (left_size == right_size):\n                    return pandas.Series(self._data >= other._data)\n                else:\n                    raise ValueError(""Can only compare identically-labeled Series objects"")\n\n            return _series_ge_none_indexes_impl\n        else:\n\n            if none_or_numeric_indexes:\n                ty_left_index_dtype = types.int64 if isinstance(self.index, types.NoneType) else self.index.dtype\n                ty_right_index_dtype = types.int64 if isinstance(other.index, types.NoneType) else other.index.dtype\n                numba_index_common_dtype = find_common_dtype_from_numpy_dtypes(\n                    [ty_left_index_dtype, ty_right_index_dtype], [])\n\n            def _series_ge_common_impl(self, other, level=None, fill_value=None, axis=0):\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n                left_index, right_index = self.index, other.index\n\n                if sdc_check_indexes_equal(left_index, right_index):\n                    if none_or_numeric_indexes == True:  # noqa\n                        new_index = numpy_like.astype(left_index, numba_index_common_dtype)\n                    else:\n                        new_index = self._index\n                    return pandas.Series(self._data >= other._data,\n                                         new_index)\n                else:\n                    raise ValueError(""Can only compare identically-labeled Series objects"")\n\n            return _series_ge_common_impl\n\n    return None\n\n\n@sdc_overload_method(SeriesType, \'ne\')\ndef sdc_pandas_series_ne(self, other, level=None, fill_value=None, axis=0):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.ne\n\n    Limitations\n    -----------\n    Parameters ``level`` and ``axis`` are currently unsupported by Intel Scalable Dataframe Compiler\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_ne.py\n       :language: python\n       :lines: 27-\n       :caption:\n       :name: ex_series_ne\n\n    .. command-output:: python ./series/series_ne.py\n       :cwd: ../../../examples\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.ne` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_op8\n    """"""\n\n    _func_name = \'Method ne().\'\n\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    if not (isinstance(level, types.Omitted) or level is None):\n        ty_checker.raise_exc(level, \'None\', \'level\')\n\n    if not isinstance(fill_value, (types.Omitted, types.Number, types.NoneType)) and fill_value is not None:\n        ty_checker.raise_exc(fill_value, \'number\', \'fill_value\')\n\n    if not (isinstance(axis, types.Omitted) or axis == 0):\n        ty_checker.raise_exc(axis, \'int\', \'axis\')\n\n    self_is_series, other_is_series = isinstance(self, SeriesType), isinstance(other, SeriesType)\n    if not (self_is_series or other_is_series):\n        return None\n\n    if not isinstance(self, (SeriesType, types.Number, types.UnicodeType)):\n        ty_checker.raise_exc(self, \'pandas.series or scalar\', \'self\')\n\n    if not isinstance(other, (SeriesType, types.Number, types.UnicodeType)):\n        ty_checker.raise_exc(other, \'pandas.series or scalar\', \'other\')\n\n    operands_are_series = self_is_series and other_is_series\n    if operands_are_series:\n        none_or_numeric_indexes = ((isinstance(self.index, types.NoneType) or check_index_is_numeric(self))\n                                   and (isinstance(other.index, types.NoneType) or check_index_is_numeric(other)))\n        series_indexes_comparable = check_types_comparable(self.index, other.index) or none_or_numeric_indexes\n        if not series_indexes_comparable:\n            raise TypingError(\'{} Not implemented for series with not-comparable indexes. \\\n            Given: self.index={}, other.index={}\'.format(_func_name, self.index, other.index))\n\n    series_data_comparable = check_types_comparable(self, other)\n    if not series_data_comparable:\n        raise TypingError(\'{} Not supported for not-comparable operands. \\\n        Given: self={}, other={}\'.format(_func_name, self, other))\n\n    fill_value_is_none = isinstance(fill_value, (types.NoneType, types.Omitted)) or fill_value is None\n    if not operands_are_series:\n        def _series_ne_scalar_impl(self, other, level=None, fill_value=None, axis=0):\n            if self_is_series == True:  # noqa\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                return pandas.Series(self._data != other, index=self._index, name=self._name)\n            else:\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n                return pandas.Series(self != other._data, index=other._index, name=other._name)\n\n        return _series_ne_scalar_impl\n\n    else:\n\n        # optimization for series with default indexes, that can be aligned differently\n        if (isinstance(self.index, types.NoneType) and isinstance(other.index, types.NoneType)):\n            def _series_ne_none_indexes_impl(self, other, level=None, fill_value=None, axis=0):\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n                left_size, right_size = len(self._data), len(other._data)\n                if (left_size == right_size):\n                    return pandas.Series(self._data != other._data)\n                else:\n                    raise ValueError(""Can only compare identically-labeled Series objects"")\n\n            return _series_ne_none_indexes_impl\n        else:\n\n            if none_or_numeric_indexes:\n                ty_left_index_dtype = types.int64 if isinstance(self.index, types.NoneType) else self.index.dtype\n                ty_right_index_dtype = types.int64 if isinstance(other.index, types.NoneType) else other.index.dtype\n                numba_index_common_dtype = find_common_dtype_from_numpy_dtypes(\n                    [ty_left_index_dtype, ty_right_index_dtype], [])\n\n            def _series_ne_common_impl(self, other, level=None, fill_value=None, axis=0):\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n                left_index, right_index = self.index, other.index\n\n                if sdc_check_indexes_equal(left_index, right_index):\n                    if none_or_numeric_indexes == True:  # noqa\n                        new_index = numpy_like.astype(left_index, numba_index_common_dtype)\n                    else:\n                        new_index = self._index\n                    return pandas.Series(self._data != other._data,\n                                         new_index)\n                else:\n                    raise ValueError(""Can only compare identically-labeled Series objects"")\n\n            return _series_ne_common_impl\n\n    return None\n\n\n@sdc_overload_method(SeriesType, \'eq\')\ndef sdc_pandas_series_eq(self, other, level=None, fill_value=None, axis=0):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.eq\n\n    Limitations\n    -----------\n    Parameters ``level`` and ``axis`` are currently unsupported by Intel Scalable Dataframe Compiler\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_eq.py\n       :language: python\n       :lines: 27-\n       :caption:\n       :name: ex_series_eq\n\n    .. command-output:: python ./series/series_eq.py\n       :cwd: ../../../examples\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.eq` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_op8\n    """"""\n\n    _func_name = \'Method eq().\'\n\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    if not (isinstance(level, types.Omitted) or level is None):\n        ty_checker.raise_exc(level, \'None\', \'level\')\n\n    if not isinstance(fill_value, (types.Omitted, types.Number, types.NoneType)) and fill_value is not None:\n        ty_checker.raise_exc(fill_value, \'number\', \'fill_value\')\n\n    if not (isinstance(axis, types.Omitted) or axis == 0):\n        ty_checker.raise_exc(axis, \'int\', \'axis\')\n\n    self_is_series, other_is_series = isinstance(self, SeriesType), isinstance(other, SeriesType)\n    if not (self_is_series or other_is_series):\n        return None\n\n    if not isinstance(self, (SeriesType, types.Number, types.UnicodeType)):\n        ty_checker.raise_exc(self, \'pandas.series or scalar\', \'self\')\n\n    if not isinstance(other, (SeriesType, types.Number, types.UnicodeType)):\n        ty_checker.raise_exc(other, \'pandas.series or scalar\', \'other\')\n\n    operands_are_series = self_is_series and other_is_series\n    if operands_are_series:\n        none_or_numeric_indexes = ((isinstance(self.index, types.NoneType) or check_index_is_numeric(self))\n                                   and (isinstance(other.index, types.NoneType) or check_index_is_numeric(other)))\n        series_indexes_comparable = check_types_comparable(self.index, other.index) or none_or_numeric_indexes\n        if not series_indexes_comparable:\n            raise TypingError(\'{} Not implemented for series with not-comparable indexes. \\\n            Given: self.index={}, other.index={}\'.format(_func_name, self.index, other.index))\n\n    series_data_comparable = check_types_comparable(self, other)\n    if not series_data_comparable:\n        raise TypingError(\'{} Not supported for not-comparable operands. \\\n        Given: self={}, other={}\'.format(_func_name, self, other))\n\n    fill_value_is_none = isinstance(fill_value, (types.NoneType, types.Omitted)) or fill_value is None\n    if not operands_are_series:\n        def _series_eq_scalar_impl(self, other, level=None, fill_value=None, axis=0):\n            if self_is_series == True:  # noqa\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                return pandas.Series(self._data == other, index=self._index, name=self._name)\n            else:\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n                return pandas.Series(self == other._data, index=other._index, name=other._name)\n\n        return _series_eq_scalar_impl\n\n    else:\n\n        # optimization for series with default indexes, that can be aligned differently\n        if (isinstance(self.index, types.NoneType) and isinstance(other.index, types.NoneType)):\n            def _series_eq_none_indexes_impl(self, other, level=None, fill_value=None, axis=0):\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n                left_size, right_size = len(self._data), len(other._data)\n                if (left_size == right_size):\n                    return pandas.Series(self._data == other._data)\n                else:\n                    raise ValueError(""Can only compare identically-labeled Series objects"")\n\n            return _series_eq_none_indexes_impl\n        else:\n\n            if none_or_numeric_indexes:\n                ty_left_index_dtype = types.int64 if isinstance(self.index, types.NoneType) else self.index.dtype\n                ty_right_index_dtype = types.int64 if isinstance(other.index, types.NoneType) else other.index.dtype\n                numba_index_common_dtype = find_common_dtype_from_numpy_dtypes(\n                    [ty_left_index_dtype, ty_right_index_dtype], [])\n\n            def _series_eq_common_impl(self, other, level=None, fill_value=None, axis=0):\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n                left_index, right_index = self.index, other.index\n\n                if sdc_check_indexes_equal(left_index, right_index):\n                    if none_or_numeric_indexes == True:  # noqa\n                        new_index = numpy_like.astype(left_index, numba_index_common_dtype)\n                    else:\n                        new_index = self._index\n                    return pandas.Series(self._data == other._data,\n                                         new_index)\n                else:\n                    raise ValueError(""Can only compare identically-labeled Series objects"")\n\n            return _series_eq_common_impl\n\n    return None\n\n\n@sdc_overload(operator.add)\ndef sdc_pandas_series_operator_add(self, other):\n    """"""\n    Pandas Series operator :attr:`pandas.Series.add` implementation\n\n    Note: Currently implemented for numeric Series only.\n        Differs from Pandas in returning Series with fixed dtype :obj:`float64`\n\n    .. only:: developer\n\n    **Test**: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_op1*\n              python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_op2*\n              python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_operator_add*\n\n    Parameters\n    ----------\n    series: :obj:`pandas.Series`\n        Input series\n    other: :obj:`pandas.Series` or :obj:`scalar`\n        Series or scalar value to be used as a second argument of binary operation\n\n    Returns\n    -------\n    :obj:`pandas.Series`\n        The result of the operation\n    """"""\n\n    ty_checker = TypeChecker(\'Operator add().\')\n    self_is_series, other_is_series = isinstance(self, SeriesType), isinstance(other, SeriesType)\n    if not (self_is_series or other_is_series):\n        return None\n\n    # this overload is not for string series\n    self_is_string_series = self_is_series and isinstance(self.dtype, types.UnicodeType)\n    other_is_string_series = other_is_series and isinstance(other.dtype, types.UnicodeType)\n    if self_is_string_series or other_is_string_series:\n        return None\n\n    if not isinstance(self, (SeriesType, types.Number)):\n        ty_checker.raise_exc(self, \'pandas.series or scalar\', \'self\')\n\n    if not isinstance(other, (SeriesType, types.Number)):\n        ty_checker.raise_exc(other, \'pandas.series or scalar\', \'other\')\n\n    operands_are_series = self_is_series and other_is_series\n    if operands_are_series:\n        none_or_numeric_indexes = ((isinstance(self.index, types.NoneType) or check_index_is_numeric(self))\n                                   and (isinstance(other.index, types.NoneType) or check_index_is_numeric(other)))\n        series_indexes_comparable = check_types_comparable(self.index, other.index) or none_or_numeric_indexes\n        if not series_indexes_comparable:\n            raise TypingError(\'{} Not implemented for series with not-comparable indexes. \\\n            Given: self.index={}, other.index={}\'.format(_func_name, self.index, other.index))\n\n    series_data_comparable = check_types_comparable(self, other)\n    if not series_data_comparable:\n        raise TypingError(\'{} Not supported for not-comparable operands. \\\n        Given: self={}, other={}\'.format(_func_name, self, other))\n\n    def sdc_pandas_series_operator_add_impl(self, other):\n        return self.add(other)\n\n    return sdc_pandas_series_operator_add_impl\n\n\n@sdc_overload(operator.sub)\ndef sdc_pandas_series_operator_sub(self, other):\n    """"""\n    Pandas Series operator :attr:`pandas.Series.sub` implementation\n\n    Note: Currently implemented for numeric Series only.\n        Differs from Pandas in returning Series with fixed dtype :obj:`float64`\n\n    .. only:: developer\n\n    **Test**: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_op1*\n              python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_op2*\n              python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_operator_sub*\n\n    Parameters\n    ----------\n    series: :obj:`pandas.Series`\n        Input series\n    other: :obj:`pandas.Series` or :obj:`scalar`\n        Series or scalar value to be used as a second argument of binary operation\n\n    Returns\n    -------\n    :obj:`pandas.Series`\n        The result of the operation\n    """"""\n\n    ty_checker = TypeChecker(\'Operator sub().\')\n    self_is_series, other_is_series = isinstance(self, SeriesType), isinstance(other, SeriesType)\n    if not (self_is_series or other_is_series):\n        return None\n\n    # this overload is not for string series\n    self_is_string_series = self_is_series and isinstance(self.dtype, types.UnicodeType)\n    other_is_string_series = other_is_series and isinstance(other.dtype, types.UnicodeType)\n    if self_is_string_series or other_is_string_series:\n        return None\n\n    if not isinstance(self, (SeriesType, types.Number)):\n        ty_checker.raise_exc(self, \'pandas.series or scalar\', \'self\')\n\n    if not isinstance(other, (SeriesType, types.Number)):\n        ty_checker.raise_exc(other, \'pandas.series or scalar\', \'other\')\n\n    operands_are_series = self_is_series and other_is_series\n    if operands_are_series:\n        none_or_numeric_indexes = ((isinstance(self.index, types.NoneType) or check_index_is_numeric(self))\n                                   and (isinstance(other.index, types.NoneType) or check_index_is_numeric(other)))\n        series_indexes_comparable = check_types_comparable(self.index, other.index) or none_or_numeric_indexes\n        if not series_indexes_comparable:\n            raise TypingError(\'{} Not implemented for series with not-comparable indexes. \\\n            Given: self.index={}, other.index={}\'.format(_func_name, self.index, other.index))\n\n    series_data_comparable = check_types_comparable(self, other)\n    if not series_data_comparable:\n        raise TypingError(\'{} Not supported for not-comparable operands. \\\n        Given: self={}, other={}\'.format(_func_name, self, other))\n\n    def sdc_pandas_series_operator_sub_impl(self, other):\n        return self.sub(other)\n\n    return sdc_pandas_series_operator_sub_impl\n\n\n@sdc_overload(operator.mul)\ndef sdc_pandas_series_operator_mul(self, other):\n    """"""\n    Pandas Series operator :attr:`pandas.Series.mul` implementation\n\n    Note: Currently implemented for numeric Series only.\n        Differs from Pandas in returning Series with fixed dtype :obj:`float64`\n\n    .. only:: developer\n\n    **Test**: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_op1*\n              python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_op2*\n              python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_operator_mul*\n\n    Parameters\n    ----------\n    series: :obj:`pandas.Series`\n        Input series\n    other: :obj:`pandas.Series` or :obj:`scalar`\n        Series or scalar value to be used as a second argument of binary operation\n\n    Returns\n    -------\n    :obj:`pandas.Series`\n        The result of the operation\n    """"""\n\n    ty_checker = TypeChecker(\'Operator mul().\')\n    self_is_series, other_is_series = isinstance(self, SeriesType), isinstance(other, SeriesType)\n    if not (self_is_series or other_is_series):\n        return None\n\n    # this overload is not for string series\n    self_is_string_series = self_is_series and isinstance(self.dtype, types.UnicodeType)\n    other_is_string_series = other_is_series and isinstance(other.dtype, types.UnicodeType)\n    if self_is_string_series or other_is_string_series:\n        return None\n\n    if not isinstance(self, (SeriesType, types.Number)):\n        ty_checker.raise_exc(self, \'pandas.series or scalar\', \'self\')\n\n    if not isinstance(other, (SeriesType, types.Number)):\n        ty_checker.raise_exc(other, \'pandas.series or scalar\', \'other\')\n\n    operands_are_series = self_is_series and other_is_series\n    if operands_are_series:\n        none_or_numeric_indexes = ((isinstance(self.index, types.NoneType) or check_index_is_numeric(self))\n                                   and (isinstance(other.index, types.NoneType) or check_index_is_numeric(other)))\n        series_indexes_comparable = check_types_comparable(self.index, other.index) or none_or_numeric_indexes\n        if not series_indexes_comparable:\n            raise TypingError(\'{} Not implemented for series with not-comparable indexes. \\\n            Given: self.index={}, other.index={}\'.format(_func_name, self.index, other.index))\n\n    series_data_comparable = check_types_comparable(self, other)\n    if not series_data_comparable:\n        raise TypingError(\'{} Not supported for not-comparable operands. \\\n        Given: self={}, other={}\'.format(_func_name, self, other))\n\n    def sdc_pandas_series_operator_mul_impl(self, other):\n        return self.mul(other)\n\n    return sdc_pandas_series_operator_mul_impl\n\n\n@sdc_overload(operator.truediv)\ndef sdc_pandas_series_operator_truediv(self, other):\n    """"""\n    Pandas Series operator :attr:`pandas.Series.truediv` implementation\n\n    Note: Currently implemented for numeric Series only.\n        Differs from Pandas in returning Series with fixed dtype :obj:`float64`\n\n    .. only:: developer\n\n    **Test**: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_op1*\n              python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_op2*\n              python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_operator_truediv*\n\n    Parameters\n    ----------\n    series: :obj:`pandas.Series`\n        Input series\n    other: :obj:`pandas.Series` or :obj:`scalar`\n        Series or scalar value to be used as a second argument of binary operation\n\n    Returns\n    -------\n    :obj:`pandas.Series`\n        The result of the operation\n    """"""\n\n    ty_checker = TypeChecker(\'Operator truediv().\')\n    self_is_series, other_is_series = isinstance(self, SeriesType), isinstance(other, SeriesType)\n    if not (self_is_series or other_is_series):\n        return None\n\n    # this overload is not for string series\n    self_is_string_series = self_is_series and isinstance(self.dtype, types.UnicodeType)\n    other_is_string_series = other_is_series and isinstance(other.dtype, types.UnicodeType)\n    if self_is_string_series or other_is_string_series:\n        return None\n\n    if not isinstance(self, (SeriesType, types.Number)):\n        ty_checker.raise_exc(self, \'pandas.series or scalar\', \'self\')\n\n    if not isinstance(other, (SeriesType, types.Number)):\n        ty_checker.raise_exc(other, \'pandas.series or scalar\', \'other\')\n\n    operands_are_series = self_is_series and other_is_series\n    if operands_are_series:\n        none_or_numeric_indexes = ((isinstance(self.index, types.NoneType) or check_index_is_numeric(self))\n                                   and (isinstance(other.index, types.NoneType) or check_index_is_numeric(other)))\n        series_indexes_comparable = check_types_comparable(self.index, other.index) or none_or_numeric_indexes\n        if not series_indexes_comparable:\n            raise TypingError(\'{} Not implemented for series with not-comparable indexes. \\\n            Given: self.index={}, other.index={}\'.format(_func_name, self.index, other.index))\n\n    series_data_comparable = check_types_comparable(self, other)\n    if not series_data_comparable:\n        raise TypingError(\'{} Not supported for not-comparable operands. \\\n        Given: self={}, other={}\'.format(_func_name, self, other))\n\n    def sdc_pandas_series_operator_truediv_impl(self, other):\n        return self.truediv(other)\n\n    return sdc_pandas_series_operator_truediv_impl\n\n\n@sdc_overload(operator.floordiv)\ndef sdc_pandas_series_operator_floordiv(self, other):\n    """"""\n    Pandas Series operator :attr:`pandas.Series.floordiv` implementation\n\n    Note: Currently implemented for numeric Series only.\n        Differs from Pandas in returning Series with fixed dtype :obj:`float64`\n\n    .. only:: developer\n\n    **Test**: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_op1*\n              python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_op2*\n              python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_operator_floordiv*\n\n    Parameters\n    ----------\n    series: :obj:`pandas.Series`\n        Input series\n    other: :obj:`pandas.Series` or :obj:`scalar`\n        Series or scalar value to be used as a second argument of binary operation\n\n    Returns\n    -------\n    :obj:`pandas.Series`\n        The result of the operation\n    """"""\n\n    ty_checker = TypeChecker(\'Operator floordiv().\')\n    self_is_series, other_is_series = isinstance(self, SeriesType), isinstance(other, SeriesType)\n    if not (self_is_series or other_is_series):\n        return None\n\n    # this overload is not for string series\n    self_is_string_series = self_is_series and isinstance(self.dtype, types.UnicodeType)\n    other_is_string_series = other_is_series and isinstance(other.dtype, types.UnicodeType)\n    if self_is_string_series or other_is_string_series:\n        return None\n\n    if not isinstance(self, (SeriesType, types.Number)):\n        ty_checker.raise_exc(self, \'pandas.series or scalar\', \'self\')\n\n    if not isinstance(other, (SeriesType, types.Number)):\n        ty_checker.raise_exc(other, \'pandas.series or scalar\', \'other\')\n\n    operands_are_series = self_is_series and other_is_series\n    if operands_are_series:\n        none_or_numeric_indexes = ((isinstance(self.index, types.NoneType) or check_index_is_numeric(self))\n                                   and (isinstance(other.index, types.NoneType) or check_index_is_numeric(other)))\n        series_indexes_comparable = check_types_comparable(self.index, other.index) or none_or_numeric_indexes\n        if not series_indexes_comparable:\n            raise TypingError(\'{} Not implemented for series with not-comparable indexes. \\\n            Given: self.index={}, other.index={}\'.format(_func_name, self.index, other.index))\n\n    series_data_comparable = check_types_comparable(self, other)\n    if not series_data_comparable:\n        raise TypingError(\'{} Not supported for not-comparable operands. \\\n        Given: self={}, other={}\'.format(_func_name, self, other))\n\n    def sdc_pandas_series_operator_floordiv_impl(self, other):\n        return self.floordiv(other)\n\n    return sdc_pandas_series_operator_floordiv_impl\n\n\n@sdc_overload(operator.mod)\ndef sdc_pandas_series_operator_mod(self, other):\n    """"""\n    Pandas Series operator :attr:`pandas.Series.mod` implementation\n\n    Note: Currently implemented for numeric Series only.\n        Differs from Pandas in returning Series with fixed dtype :obj:`float64`\n\n    .. only:: developer\n\n    **Test**: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_op1*\n              python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_op2*\n              python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_operator_mod*\n\n    Parameters\n    ----------\n    series: :obj:`pandas.Series`\n        Input series\n    other: :obj:`pandas.Series` or :obj:`scalar`\n        Series or scalar value to be used as a second argument of binary operation\n\n    Returns\n    -------\n    :obj:`pandas.Series`\n        The result of the operation\n    """"""\n\n    ty_checker = TypeChecker(\'Operator mod().\')\n    self_is_series, other_is_series = isinstance(self, SeriesType), isinstance(other, SeriesType)\n    if not (self_is_series or other_is_series):\n        return None\n\n    # this overload is not for string series\n    self_is_string_series = self_is_series and isinstance(self.dtype, types.UnicodeType)\n    other_is_string_series = other_is_series and isinstance(other.dtype, types.UnicodeType)\n    if self_is_string_series or other_is_string_series:\n        return None\n\n    if not isinstance(self, (SeriesType, types.Number)):\n        ty_checker.raise_exc(self, \'pandas.series or scalar\', \'self\')\n\n    if not isinstance(other, (SeriesType, types.Number)):\n        ty_checker.raise_exc(other, \'pandas.series or scalar\', \'other\')\n\n    operands_are_series = self_is_series and other_is_series\n    if operands_are_series:\n        none_or_numeric_indexes = ((isinstance(self.index, types.NoneType) or check_index_is_numeric(self))\n                                   and (isinstance(other.index, types.NoneType) or check_index_is_numeric(other)))\n        series_indexes_comparable = check_types_comparable(self.index, other.index) or none_or_numeric_indexes\n        if not series_indexes_comparable:\n            raise TypingError(\'{} Not implemented for series with not-comparable indexes. \\\n            Given: self.index={}, other.index={}\'.format(_func_name, self.index, other.index))\n\n    series_data_comparable = check_types_comparable(self, other)\n    if not series_data_comparable:\n        raise TypingError(\'{} Not supported for not-comparable operands. \\\n        Given: self={}, other={}\'.format(_func_name, self, other))\n\n    def sdc_pandas_series_operator_mod_impl(self, other):\n        return self.mod(other)\n\n    return sdc_pandas_series_operator_mod_impl\n\n\n@sdc_overload(operator.pow)\ndef sdc_pandas_series_operator_pow(self, other):\n    """"""\n    Pandas Series operator :attr:`pandas.Series.pow` implementation\n\n    Note: Currently implemented for numeric Series only.\n        Differs from Pandas in returning Series with fixed dtype :obj:`float64`\n\n    .. only:: developer\n\n    **Test**: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_op1*\n              python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_op2*\n              python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_operator_pow*\n\n    Parameters\n    ----------\n    series: :obj:`pandas.Series`\n        Input series\n    other: :obj:`pandas.Series` or :obj:`scalar`\n        Series or scalar value to be used as a second argument of binary operation\n\n    Returns\n    -------\n    :obj:`pandas.Series`\n        The result of the operation\n    """"""\n\n    ty_checker = TypeChecker(\'Operator pow().\')\n    self_is_series, other_is_series = isinstance(self, SeriesType), isinstance(other, SeriesType)\n    if not (self_is_series or other_is_series):\n        return None\n\n    # this overload is not for string series\n    self_is_string_series = self_is_series and isinstance(self.dtype, types.UnicodeType)\n    other_is_string_series = other_is_series and isinstance(other.dtype, types.UnicodeType)\n    if self_is_string_series or other_is_string_series:\n        return None\n\n    if not isinstance(self, (SeriesType, types.Number)):\n        ty_checker.raise_exc(self, \'pandas.series or scalar\', \'self\')\n\n    if not isinstance(other, (SeriesType, types.Number)):\n        ty_checker.raise_exc(other, \'pandas.series or scalar\', \'other\')\n\n    operands_are_series = self_is_series and other_is_series\n    if operands_are_series:\n        none_or_numeric_indexes = ((isinstance(self.index, types.NoneType) or check_index_is_numeric(self))\n                                   and (isinstance(other.index, types.NoneType) or check_index_is_numeric(other)))\n        series_indexes_comparable = check_types_comparable(self.index, other.index) or none_or_numeric_indexes\n        if not series_indexes_comparable:\n            raise TypingError(\'{} Not implemented for series with not-comparable indexes. \\\n            Given: self.index={}, other.index={}\'.format(_func_name, self.index, other.index))\n\n    series_data_comparable = check_types_comparable(self, other)\n    if not series_data_comparable:\n        raise TypingError(\'{} Not supported for not-comparable operands. \\\n        Given: self={}, other={}\'.format(_func_name, self, other))\n\n    def sdc_pandas_series_operator_pow_impl(self, other):\n        return self.pow(other)\n\n    return sdc_pandas_series_operator_pow_impl\n\n\n@sdc_overload(operator.lt)\ndef sdc_pandas_series_operator_lt(self, other):\n    """"""\n    Pandas Series operator :attr:`pandas.Series.lt` implementation\n\n    .. only:: developer\n\n    **Test**: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_op7*\n              python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_operator_lt*\n\n    Parameters\n    ----------\n    series: :obj:`pandas.Series`\n        Input series\n    other: :obj:`pandas.Series` or :obj:`scalar`\n        Series or scalar value to be used as a second argument of binary operation\n\n    Returns\n    -------\n    :obj:`pandas.Series`\n        The result of the operation\n    """"""\n\n    ty_checker = TypeChecker(\'Operator lt().\')\n    self_is_series, other_is_series = isinstance(self, SeriesType), isinstance(other, SeriesType)\n    if not (self_is_series or other_is_series):\n        return None\n\n    if not isinstance(self, (SeriesType, types.Number, types.UnicodeType)):\n        ty_checker.raise_exc(self, \'pandas.series or scalar\', \'self\')\n\n    if not isinstance(other, (SeriesType, types.Number, types.UnicodeType)):\n        ty_checker.raise_exc(other, \'pandas.series or scalar\', \'other\')\n\n    operands_are_series = self_is_series and other_is_series\n    if operands_are_series:\n        none_or_numeric_indexes = ((isinstance(self.index, types.NoneType) or check_index_is_numeric(self))\n                                   and (isinstance(other.index, types.NoneType) or check_index_is_numeric(other)))\n        series_indexes_comparable = check_types_comparable(self.index, other.index) or none_or_numeric_indexes\n        if not series_indexes_comparable:\n            raise TypingError(\'{} Not implemented for series with not-comparable indexes. \\\n            Given: self.index={}, other.index={}\'.format(_func_name, self.index, other.index))\n\n    series_data_comparable = check_types_comparable(self, other)\n    if not series_data_comparable:\n        raise TypingError(\'{} Not supported for not-comparable operands. \\\n        Given: self={}, other={}\'.format(_func_name, self, other))\n\n    def sdc_pandas_series_operator_lt_impl(self, other):\n        return self.lt(other)\n\n    return sdc_pandas_series_operator_lt_impl\n\n\n@sdc_overload(operator.gt)\ndef sdc_pandas_series_operator_gt(self, other):\n    """"""\n    Pandas Series operator :attr:`pandas.Series.gt` implementation\n\n    .. only:: developer\n\n    **Test**: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_op7*\n              python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_operator_gt*\n\n    Parameters\n    ----------\n    series: :obj:`pandas.Series`\n        Input series\n    other: :obj:`pandas.Series` or :obj:`scalar`\n        Series or scalar value to be used as a second argument of binary operation\n\n    Returns\n    -------\n    :obj:`pandas.Series`\n        The result of the operation\n    """"""\n\n    ty_checker = TypeChecker(\'Operator gt().\')\n    self_is_series, other_is_series = isinstance(self, SeriesType), isinstance(other, SeriesType)\n    if not (self_is_series or other_is_series):\n        return None\n\n    if not isinstance(self, (SeriesType, types.Number, types.UnicodeType)):\n        ty_checker.raise_exc(self, \'pandas.series or scalar\', \'self\')\n\n    if not isinstance(other, (SeriesType, types.Number, types.UnicodeType)):\n        ty_checker.raise_exc(other, \'pandas.series or scalar\', \'other\')\n\n    operands_are_series = self_is_series and other_is_series\n    if operands_are_series:\n        none_or_numeric_indexes = ((isinstance(self.index, types.NoneType) or check_index_is_numeric(self))\n                                   and (isinstance(other.index, types.NoneType) or check_index_is_numeric(other)))\n        series_indexes_comparable = check_types_comparable(self.index, other.index) or none_or_numeric_indexes\n        if not series_indexes_comparable:\n            raise TypingError(\'{} Not implemented for series with not-comparable indexes. \\\n            Given: self.index={}, other.index={}\'.format(_func_name, self.index, other.index))\n\n    series_data_comparable = check_types_comparable(self, other)\n    if not series_data_comparable:\n        raise TypingError(\'{} Not supported for not-comparable operands. \\\n        Given: self={}, other={}\'.format(_func_name, self, other))\n\n    def sdc_pandas_series_operator_gt_impl(self, other):\n        return self.gt(other)\n\n    return sdc_pandas_series_operator_gt_impl\n\n\n@sdc_overload(operator.le)\ndef sdc_pandas_series_operator_le(self, other):\n    """"""\n    Pandas Series operator :attr:`pandas.Series.le` implementation\n\n    .. only:: developer\n\n    **Test**: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_op7*\n              python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_operator_le*\n\n    Parameters\n    ----------\n    series: :obj:`pandas.Series`\n        Input series\n    other: :obj:`pandas.Series` or :obj:`scalar`\n        Series or scalar value to be used as a second argument of binary operation\n\n    Returns\n    -------\n    :obj:`pandas.Series`\n        The result of the operation\n    """"""\n\n    ty_checker = TypeChecker(\'Operator le().\')\n    self_is_series, other_is_series = isinstance(self, SeriesType), isinstance(other, SeriesType)\n    if not (self_is_series or other_is_series):\n        return None\n\n    if not isinstance(self, (SeriesType, types.Number, types.UnicodeType)):\n        ty_checker.raise_exc(self, \'pandas.series or scalar\', \'self\')\n\n    if not isinstance(other, (SeriesType, types.Number, types.UnicodeType)):\n        ty_checker.raise_exc(other, \'pandas.series or scalar\', \'other\')\n\n    operands_are_series = self_is_series and other_is_series\n    if operands_are_series:\n        none_or_numeric_indexes = ((isinstance(self.index, types.NoneType) or check_index_is_numeric(self))\n                                   and (isinstance(other.index, types.NoneType) or check_index_is_numeric(other)))\n        series_indexes_comparable = check_types_comparable(self.index, other.index) or none_or_numeric_indexes\n        if not series_indexes_comparable:\n            raise TypingError(\'{} Not implemented for series with not-comparable indexes. \\\n            Given: self.index={}, other.index={}\'.format(_func_name, self.index, other.index))\n\n    series_data_comparable = check_types_comparable(self, other)\n    if not series_data_comparable:\n        raise TypingError(\'{} Not supported for not-comparable operands. \\\n        Given: self={}, other={}\'.format(_func_name, self, other))\n\n    def sdc_pandas_series_operator_le_impl(self, other):\n        return self.le(other)\n\n    return sdc_pandas_series_operator_le_impl\n\n\n@sdc_overload(operator.ge)\ndef sdc_pandas_series_operator_ge(self, other):\n    """"""\n    Pandas Series operator :attr:`pandas.Series.ge` implementation\n\n    .. only:: developer\n\n    **Test**: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_op7*\n              python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_operator_ge*\n\n    Parameters\n    ----------\n    series: :obj:`pandas.Series`\n        Input series\n    other: :obj:`pandas.Series` or :obj:`scalar`\n        Series or scalar value to be used as a second argument of binary operation\n\n    Returns\n    -------\n    :obj:`pandas.Series`\n        The result of the operation\n    """"""\n\n    ty_checker = TypeChecker(\'Operator ge().\')\n    self_is_series, other_is_series = isinstance(self, SeriesType), isinstance(other, SeriesType)\n    if not (self_is_series or other_is_series):\n        return None\n\n    if not isinstance(self, (SeriesType, types.Number, types.UnicodeType)):\n        ty_checker.raise_exc(self, \'pandas.series or scalar\', \'self\')\n\n    if not isinstance(other, (SeriesType, types.Number, types.UnicodeType)):\n        ty_checker.raise_exc(other, \'pandas.series or scalar\', \'other\')\n\n    operands_are_series = self_is_series and other_is_series\n    if operands_are_series:\n        none_or_numeric_indexes = ((isinstance(self.index, types.NoneType) or check_index_is_numeric(self))\n                                   and (isinstance(other.index, types.NoneType) or check_index_is_numeric(other)))\n        series_indexes_comparable = check_types_comparable(self.index, other.index) or none_or_numeric_indexes\n        if not series_indexes_comparable:\n            raise TypingError(\'{} Not implemented for series with not-comparable indexes. \\\n            Given: self.index={}, other.index={}\'.format(_func_name, self.index, other.index))\n\n    series_data_comparable = check_types_comparable(self, other)\n    if not series_data_comparable:\n        raise TypingError(\'{} Not supported for not-comparable operands. \\\n        Given: self={}, other={}\'.format(_func_name, self, other))\n\n    def sdc_pandas_series_operator_ge_impl(self, other):\n        return self.ge(other)\n\n    return sdc_pandas_series_operator_ge_impl\n\n\n@sdc_overload(operator.ne)\ndef sdc_pandas_series_operator_ne(self, other):\n    """"""\n    Pandas Series operator :attr:`pandas.Series.ne` implementation\n\n    .. only:: developer\n\n    **Test**: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_op7*\n              python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_operator_ne*\n\n    Parameters\n    ----------\n    series: :obj:`pandas.Series`\n        Input series\n    other: :obj:`pandas.Series` or :obj:`scalar`\n        Series or scalar value to be used as a second argument of binary operation\n\n    Returns\n    -------\n    :obj:`pandas.Series`\n        The result of the operation\n    """"""\n\n    ty_checker = TypeChecker(\'Operator ne().\')\n    self_is_series, other_is_series = isinstance(self, SeriesType), isinstance(other, SeriesType)\n    if not (self_is_series or other_is_series):\n        return None\n\n    if not isinstance(self, (SeriesType, types.Number, types.UnicodeType)):\n        ty_checker.raise_exc(self, \'pandas.series or scalar\', \'self\')\n\n    if not isinstance(other, (SeriesType, types.Number, types.UnicodeType)):\n        ty_checker.raise_exc(other, \'pandas.series or scalar\', \'other\')\n\n    operands_are_series = self_is_series and other_is_series\n    if operands_are_series:\n        none_or_numeric_indexes = ((isinstance(self.index, types.NoneType) or check_index_is_numeric(self))\n                                   and (isinstance(other.index, types.NoneType) or check_index_is_numeric(other)))\n        series_indexes_comparable = check_types_comparable(self.index, other.index) or none_or_numeric_indexes\n        if not series_indexes_comparable:\n            raise TypingError(\'{} Not implemented for series with not-comparable indexes. \\\n            Given: self.index={}, other.index={}\'.format(_func_name, self.index, other.index))\n\n    series_data_comparable = check_types_comparable(self, other)\n    if not series_data_comparable:\n        raise TypingError(\'{} Not supported for not-comparable operands. \\\n        Given: self={}, other={}\'.format(_func_name, self, other))\n\n    def sdc_pandas_series_operator_ne_impl(self, other):\n        return self.ne(other)\n\n    return sdc_pandas_series_operator_ne_impl\n\n\n@sdc_overload(operator.eq)\ndef sdc_pandas_series_operator_eq(self, other):\n    """"""\n    Pandas Series operator :attr:`pandas.Series.eq` implementation\n\n    .. only:: developer\n\n    **Test**: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_op7*\n              python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_operator_eq*\n\n    Parameters\n    ----------\n    series: :obj:`pandas.Series`\n        Input series\n    other: :obj:`pandas.Series` or :obj:`scalar`\n        Series or scalar value to be used as a second argument of binary operation\n\n    Returns\n    -------\n    :obj:`pandas.Series`\n        The result of the operation\n    """"""\n\n    ty_checker = TypeChecker(\'Operator eq().\')\n    self_is_series, other_is_series = isinstance(self, SeriesType), isinstance(other, SeriesType)\n    if not (self_is_series or other_is_series):\n        return None\n\n    if not isinstance(self, (SeriesType, types.Number, types.UnicodeType)):\n        ty_checker.raise_exc(self, \'pandas.series or scalar\', \'self\')\n\n    if not isinstance(other, (SeriesType, types.Number, types.UnicodeType)):\n        ty_checker.raise_exc(other, \'pandas.series or scalar\', \'other\')\n\n    operands_are_series = self_is_series and other_is_series\n    if operands_are_series:\n        none_or_numeric_indexes = ((isinstance(self.index, types.NoneType) or check_index_is_numeric(self))\n                                   and (isinstance(other.index, types.NoneType) or check_index_is_numeric(other)))\n        series_indexes_comparable = check_types_comparable(self.index, other.index) or none_or_numeric_indexes\n        if not series_indexes_comparable:\n            raise TypingError(\'{} Not implemented for series with not-comparable indexes. \\\n            Given: self.index={}, other.index={}\'.format(_func_name, self.index, other.index))\n\n    series_data_comparable = check_types_comparable(self, other)\n    if not series_data_comparable:\n        raise TypingError(\'{} Not supported for not-comparable operands. \\\n        Given: self={}, other={}\'.format(_func_name, self, other))\n\n    def sdc_pandas_series_operator_eq_impl(self, other):\n        return self.eq(other)\n\n    return sdc_pandas_series_operator_eq_impl\n\n\n@sdc_overload(operator.lt)\ndef sdc_str_arr_operator_lt(self, other):\n\n    self_is_str_arr = self == string_array_type\n    other_is_str_arr = other == string_array_type\n    operands_are_arrays = self_is_str_arr and other_is_str_arr\n\n    if not (operands_are_arrays\n            or (self_is_str_arr and isinstance(other, types.UnicodeType))\n            or (isinstance(self, types.UnicodeType) and other_is_str_arr)):\n        return None\n\n    if operands_are_arrays:\n        def _sdc_str_arr_operator_lt_impl(self, other):\n            if len(self) != len(other):\n                raise ValueError(""Mismatch of String Arrays sizes in operator.lt"")\n            n = len(self)\n            out_list = [False] * n\n            for i in numba.prange(n):\n                out_list[i] = (self[i] < other[i]\n                               and not (str_arr_is_na(self, i) or str_arr_is_na(other, i)))\n            return out_list\n\n    elif self_is_str_arr:\n        def _sdc_str_arr_operator_lt_impl(self, other):\n            n = len(self)\n            out_list = [False] * n\n            for i in numba.prange(n):\n                out_list[i] = (self[i] < other and not (str_arr_is_na(self, i)))\n            return out_list\n\n    elif other_is_str_arr:\n        def _sdc_str_arr_operator_lt_impl(self, other):\n            n = len(other)\n            out_list = [False] * n\n            for i in numba.prange(n):\n                out_list[i] = (self < other[i] and not (str_arr_is_na(other, i)))\n            return out_list\n    else:\n        return None\n\n    return _sdc_str_arr_operator_lt_impl\n\n\n@sdc_overload(operator.gt)\ndef sdc_str_arr_operator_gt(self, other):\n\n    self_is_str_arr = self == string_array_type\n    other_is_str_arr = other == string_array_type\n    operands_are_arrays = self_is_str_arr and other_is_str_arr\n\n    if not (operands_are_arrays\n            or (self_is_str_arr and isinstance(other, types.UnicodeType))\n            or (isinstance(self, types.UnicodeType) and other_is_str_arr)):\n        return None\n\n    if operands_are_arrays:\n        def _sdc_str_arr_operator_gt_impl(self, other):\n            if len(self) != len(other):\n                raise ValueError(""Mismatch of String Arrays sizes in operator.gt"")\n            n = len(self)\n            out_list = [False] * n\n            for i in numba.prange(n):\n                out_list[i] = (self[i] > other[i]\n                               and not (str_arr_is_na(self, i) or str_arr_is_na(other, i)))\n            return out_list\n\n    elif self_is_str_arr:\n        def _sdc_str_arr_operator_gt_impl(self, other):\n            n = len(self)\n            out_list = [False] * n\n            for i in numba.prange(n):\n                out_list[i] = (self[i] > other and not (str_arr_is_na(self, i)))\n            return out_list\n\n    elif other_is_str_arr:\n        def _sdc_str_arr_operator_gt_impl(self, other):\n            n = len(other)\n            out_list = [False] * n\n            for i in numba.prange(n):\n                out_list[i] = (self > other[i] and not (str_arr_is_na(other, i)))\n            return out_list\n    else:\n        return None\n\n    return _sdc_str_arr_operator_gt_impl\n\n\n@sdc_overload(operator.le)\ndef sdc_str_arr_operator_le(self, other):\n\n    self_is_str_arr = self == string_array_type\n    other_is_str_arr = other == string_array_type\n    operands_are_arrays = self_is_str_arr and other_is_str_arr\n\n    if not (operands_are_arrays\n            or (self_is_str_arr and isinstance(other, types.UnicodeType))\n            or (isinstance(self, types.UnicodeType) and other_is_str_arr)):\n        return None\n\n    if operands_are_arrays:\n        def _sdc_str_arr_operator_le_impl(self, other):\n            if len(self) != len(other):\n                raise ValueError(""Mismatch of String Arrays sizes in operator.le"")\n            n = len(self)\n            out_list = [False] * n\n            for i in numba.prange(n):\n                out_list[i] = (self[i] <= other[i]\n                               and not (str_arr_is_na(self, i) or str_arr_is_na(other, i)))\n            return out_list\n\n    elif self_is_str_arr:\n        def _sdc_str_arr_operator_le_impl(self, other):\n            n = len(self)\n            out_list = [False] * n\n            for i in numba.prange(n):\n                out_list[i] = (self[i] <= other and not (str_arr_is_na(self, i)))\n            return out_list\n\n    elif other_is_str_arr:\n        def _sdc_str_arr_operator_le_impl(self, other):\n            n = len(other)\n            out_list = [False] * n\n            for i in numba.prange(n):\n                out_list[i] = (self <= other[i] and not (str_arr_is_na(other, i)))\n            return out_list\n    else:\n        return None\n\n    return _sdc_str_arr_operator_le_impl\n\n\n@sdc_overload(operator.ge)\ndef sdc_str_arr_operator_ge(self, other):\n\n    self_is_str_arr = self == string_array_type\n    other_is_str_arr = other == string_array_type\n    operands_are_arrays = self_is_str_arr and other_is_str_arr\n\n    if not (operands_are_arrays\n            or (self_is_str_arr and isinstance(other, types.UnicodeType))\n            or (isinstance(self, types.UnicodeType) and other_is_str_arr)):\n        return None\n\n    if operands_are_arrays:\n        def _sdc_str_arr_operator_ge_impl(self, other):\n            if len(self) != len(other):\n                raise ValueError(""Mismatch of String Arrays sizes in operator.ge"")\n            n = len(self)\n            out_list = [False] * n\n            for i in numba.prange(n):\n                out_list[i] = (self[i] >= other[i]\n                               and not (str_arr_is_na(self, i) or str_arr_is_na(other, i)))\n            return out_list\n\n    elif self_is_str_arr:\n        def _sdc_str_arr_operator_ge_impl(self, other):\n            n = len(self)\n            out_list = [False] * n\n            for i in numba.prange(n):\n                out_list[i] = (self[i] >= other and not (str_arr_is_na(self, i)))\n            return out_list\n\n    elif other_is_str_arr:\n        def _sdc_str_arr_operator_ge_impl(self, other):\n            n = len(other)\n            out_list = [False] * n\n            for i in numba.prange(n):\n                out_list[i] = (self >= other[i] and not (str_arr_is_na(other, i)))\n            return out_list\n    else:\n        return None\n\n    return _sdc_str_arr_operator_ge_impl\n\n\n@sdc_overload(operator.ne)\ndef sdc_str_arr_operator_ne(self, other):\n\n    self_is_str_arr = self == string_array_type\n    other_is_str_arr = other == string_array_type\n    operands_are_arrays = self_is_str_arr and other_is_str_arr\n\n    if not (operands_are_arrays\n            or (self_is_str_arr and isinstance(other, types.UnicodeType))\n            or (isinstance(self, types.UnicodeType) and other_is_str_arr)):\n        return None\n\n    if operands_are_arrays:\n        def _sdc_str_arr_operator_ne_impl(self, other):\n            if len(self) != len(other):\n                raise ValueError(""Mismatch of String Arrays sizes in operator.ne"")\n            n = len(self)\n            out_list = [False] * n\n            for i in numba.prange(n):\n                out_list[i] = (self[i] != other[i]\n                               or (str_arr_is_na(self, i) or str_arr_is_na(other, i)))\n            return out_list\n\n    elif self_is_str_arr:\n        def _sdc_str_arr_operator_ne_impl(self, other):\n            n = len(self)\n            out_list = [False] * n\n            for i in numba.prange(n):\n                out_list[i] = (self[i] != other or (str_arr_is_na(self, i)))\n            return out_list\n\n    elif other_is_str_arr:\n        def _sdc_str_arr_operator_ne_impl(self, other):\n            n = len(other)\n            out_list = [False] * n\n            for i in numba.prange(n):\n                out_list[i] = (self != other[i] or (str_arr_is_na(other, i)))\n            return out_list\n    else:\n        return None\n\n    return _sdc_str_arr_operator_ne_impl\n\n\n@sdc_overload(operator.eq)\ndef sdc_str_arr_operator_eq(self, other):\n\n    self_is_str_arr = self == string_array_type\n    other_is_str_arr = other == string_array_type\n    operands_are_arrays = self_is_str_arr and other_is_str_arr\n\n    if not (operands_are_arrays\n            or (self_is_str_arr and isinstance(other, types.UnicodeType))\n            or (isinstance(self, types.UnicodeType) and other_is_str_arr)):\n        return None\n\n    if operands_are_arrays:\n        def _sdc_str_arr_operator_eq_impl(self, other):\n            if len(self) != len(other):\n                raise ValueError(""Mismatch of String Arrays sizes in operator.eq"")\n            n = len(self)\n            out_list = [False] * n\n            for i in numba.prange(n):\n                out_list[i] = (self[i] == other[i]\n                               and not (str_arr_is_na(self, i) or str_arr_is_na(other, i)))\n            return out_list\n\n    elif self_is_str_arr:\n        def _sdc_str_arr_operator_eq_impl(self, other):\n            n = len(self)\n            out_list = [False] * n\n            for i in numba.prange(n):\n                out_list[i] = (self[i] == other and not (str_arr_is_na(self, i)))\n            return out_list\n\n    elif other_is_str_arr:\n        def _sdc_str_arr_operator_eq_impl(self, other):\n            n = len(other)\n            out_list = [False] * n\n            for i in numba.prange(n):\n                out_list[i] = (self == other[i] and not (str_arr_is_na(other, i)))\n            return out_list\n    else:\n        return None\n\n    return _sdc_str_arr_operator_eq_impl\n'"
sdc/sdc_function_templates.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\n\n| This file contains function templates used by the auto-generation script\n\n""""""\n\n# below imports are copied into the auto-generated source file as-is\n# for the auto-generation script to work ensure they are not mixed up with code\nimport numba\nimport numpy\nimport operator\nimport pandas\n\nfrom numba.core.errors import TypingError\nfrom numba import types\n\nfrom sdc.utilities.sdc_typing_utils import (TypeChecker, check_index_is_numeric, check_types_comparable,\n                                            find_common_dtype_from_numpy_dtypes)\nfrom sdc.datatypes.common_functions import (sdc_join_series_indexes, sdc_check_indexes_equal)\nfrom sdc.hiframes.pd_series_type import SeriesType\nfrom sdc.str_arr_ext import (string_array_type, str_arr_is_na)\nfrom sdc.utilities.utils import sdc_overload, sdc_overload_method\nfrom sdc.functions import numpy_like\n\n\ndef sdc_pandas_series_binop(self, other, level=None, fill_value=None, axis=0):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.binop\n\n    Limitations\n    -----------\n    Parameters ``level`` and ``axis`` are currently unsupported by Intel Scalable Dataframe Compiler\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_binop.py\n       :language: python\n       :lines: 27-\n       :caption:\n       :name: ex_series_binop\n\n    .. command-output:: python ./series/series_binop.py\n       :cwd: ../../../examples\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.binop` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests sdc.tests.test_series.TestSeries.test_series_op5\n    """"""\n\n    ty_checker = TypeChecker(\'Method binop().\')\n    self_is_series, other_is_series = isinstance(self, SeriesType), isinstance(other, SeriesType)\n    if not (self_is_series or other_is_series):\n        return None\n\n    # this overload is not for string series\n    self_is_string_series = self_is_series and isinstance(self.dtype, types.UnicodeType)\n    other_is_string_series = other_is_series and isinstance(other.dtype, types.UnicodeType)\n    if self_is_string_series or other_is_string_series:\n        return None\n\n    if not isinstance(self, (SeriesType, types.Number)):\n        ty_checker.raise_exc(self, \'pandas.series or scalar\', \'self\')\n\n    if not isinstance(other, (SeriesType, types.Number)):\n        ty_checker.raise_exc(other, \'pandas.series or scalar\', \'other\')\n\n    operands_are_series = self_is_series and other_is_series\n    if operands_are_series:\n        none_or_numeric_indexes = ((isinstance(self.index, types.NoneType) or check_index_is_numeric(self))\n                                   and (isinstance(other.index, types.NoneType) or check_index_is_numeric(other)))\n        series_indexes_comparable = check_types_comparable(self.index, other.index) or none_or_numeric_indexes\n        if not series_indexes_comparable:\n            raise TypingError(\'{} Not implemented for series with not-comparable indexes. \\\n            Given: self.index={}, other.index={}\'.format(_func_name, self.index, other.index))\n\n    series_data_comparable = check_types_comparable(self, other)\n    if not series_data_comparable:\n        raise TypingError(\'{} Not supported for not-comparable operands. \\\n        Given: self={}, other={}\'.format(_func_name, self, other))\n\n    if not isinstance(level, types.Omitted) and level is not None:\n        ty_checker.raise_exc(level, \'None\', \'level\')\n\n    if not isinstance(fill_value, (types.Omitted, types.Number, types.NoneType)) and fill_value is not None:\n        ty_checker.raise_exc(fill_value, \'number\', \'fill_value\')\n\n    if not isinstance(axis, types.Omitted) and axis != 0:\n        ty_checker.raise_exc(axis, \'int\', \'axis\')\n    fill_value_is_none = isinstance(fill_value, (types.NoneType, types.Omitted)) or fill_value is None\n    # specializations for numeric series only\n    if not operands_are_series:\n        def _series_binop_scalar_impl(self, other, level=None, fill_value=None, axis=0):\n            if self_is_series == True:  # noqa\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                result_data = numpy.empty(len(self._data), dtype=numpy.float64)\n                result_data[:] = self._data + numpy.float64(other)\n                return pandas.Series(result_data, index=self._index, name=self._name)\n            else:\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n                result_data = numpy.empty(len(other._data), dtype=numpy.float64)\n                result_data[:] = numpy.float64(self) + other._data\n                return pandas.Series(result_data, index=other._index, name=other._name)\n\n        return _series_binop_scalar_impl\n\n    else:   # both operands are numeric series\n        # optimization for series with default indexes, that can be aligned differently\n        if (isinstance(self.index, types.NoneType) and isinstance(other.index, types.NoneType)):\n            def _series_binop_none_indexes_impl(self, other, level=None, fill_value=None, axis=0):\n                _fill_value = numpy.nan if fill_value_is_none == True else fill_value  # noqa\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n\n                if (len(self._data) == len(other._data)):\n                    result_data = numpy_like.astype(self._data, numpy.float64)\n                    result_data = result_data + other._data\n                    return pandas.Series(result_data)\n                else:\n                    left_size, right_size = len(self._data), len(other._data)\n                    min_data_size = min(left_size, right_size)\n                    max_data_size = max(left_size, right_size)\n                    result_data = numpy.empty(max_data_size, dtype=numpy.float64)\n                    if (left_size == min_data_size):\n                        result_data[:min_data_size] = self._data\n                        for i in range(min_data_size, len(result_data)):\n                            result_data[i] = _fill_value\n                        result_data = result_data + other._data\n                    else:\n                        result_data[:min_data_size] = other._data\n                        for i in range(min_data_size, len(result_data)):\n                            result_data[i] = _fill_value\n                        result_data = self._data + result_data\n\n                    return pandas.Series(result_data)\n\n            return _series_binop_none_indexes_impl\n        else:\n            # for numeric indexes find common dtype to be used when creating joined index\n            if none_or_numeric_indexes:\n                ty_left_index_dtype = types.int64 if isinstance(self.index, types.NoneType) else self.index.dtype\n                ty_right_index_dtype = types.int64 if isinstance(other.index, types.NoneType) else other.index.dtype\n                numba_index_common_dtype = find_common_dtype_from_numpy_dtypes(\n                    [ty_left_index_dtype, ty_right_index_dtype], [])\n\n            def _series_binop_common_impl(self, other, level=None, fill_value=None, axis=0):\n                left_index, right_index = self.index, other.index\n                _fill_value = numpy.nan if fill_value_is_none == True else fill_value  # noqa\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n                # check if indexes are equal and series don\'t have to be aligned\n                if sdc_check_indexes_equal(left_index, right_index):\n                    result_data = numpy.empty(len(self._data), dtype=numpy.float64)\n                    result_data[:] = self._data + other._data\n\n                    if none_or_numeric_indexes == True:  # noqa\n                        result_index = numpy_like.astype(left_index, numba_index_common_dtype)\n                    else:\n                        result_index = self._index\n\n                    return pandas.Series(result_data, index=result_index)\n\n                # TODO: replace below with core join(how=\'outer\', return_indexers=True) when implemented\n                joined_index, left_indexer, right_indexer = sdc_join_series_indexes(left_index, right_index)\n                result_size = len(joined_index)\n                left_values = numpy.empty(result_size, dtype=numpy.float64)\n                right_values = numpy.empty(result_size, dtype=numpy.float64)\n                for i in range(result_size):\n                    left_pos, right_pos = left_indexer[i], right_indexer[i]\n                    left_values[i] = self._data[left_pos] if left_pos != -1 else _fill_value\n                    right_values[i] = other._data[right_pos] if right_pos != -1 else _fill_value\n                result_data = left_values + right_values\n                return pandas.Series(result_data, joined_index)\n\n            return _series_binop_common_impl\n\n    return None\n\n\ndef sdc_pandas_series_comp_binop(self, other, level=None, fill_value=None, axis=0):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.comp_binop\n\n    Limitations\n    -----------\n    Parameters ``level`` and ``axis`` are currently unsupported by Intel Scalable Dataframe Compiler\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_comp_binop.py\n       :language: python\n       :lines: 27-\n       :caption:\n       :name: ex_series_comp_binop\n\n    .. command-output:: python ./series/series_comp_binop.py\n       :cwd: ../../../examples\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.comp_binop` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_op8\n    """"""\n\n    _func_name = \'Method comp_binop().\'\n\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    if not (isinstance(level, types.Omitted) or level is None):\n        ty_checker.raise_exc(level, \'None\', \'level\')\n\n    if not isinstance(fill_value, (types.Omitted, types.Number, types.NoneType)) and fill_value is not None:\n        ty_checker.raise_exc(fill_value, \'number\', \'fill_value\')\n\n    if not (isinstance(axis, types.Omitted) or axis == 0):\n        ty_checker.raise_exc(axis, \'int\', \'axis\')\n\n    self_is_series, other_is_series = isinstance(self, SeriesType), isinstance(other, SeriesType)\n    if not (self_is_series or other_is_series):\n        return None\n\n    if not isinstance(self, (SeriesType, types.Number, types.UnicodeType)):\n        ty_checker.raise_exc(self, \'pandas.series or scalar\', \'self\')\n\n    if not isinstance(other, (SeriesType, types.Number, types.UnicodeType)):\n        ty_checker.raise_exc(other, \'pandas.series or scalar\', \'other\')\n\n    operands_are_series = self_is_series and other_is_series\n    if operands_are_series:\n        none_or_numeric_indexes = ((isinstance(self.index, types.NoneType) or check_index_is_numeric(self))\n                                   and (isinstance(other.index, types.NoneType) or check_index_is_numeric(other)))\n        series_indexes_comparable = check_types_comparable(self.index, other.index) or none_or_numeric_indexes\n        if not series_indexes_comparable:\n            raise TypingError(\'{} Not implemented for series with not-comparable indexes. \\\n            Given: self.index={}, other.index={}\'.format(_func_name, self.index, other.index))\n\n    series_data_comparable = check_types_comparable(self, other)\n    if not series_data_comparable:\n        raise TypingError(\'{} Not supported for not-comparable operands. \\\n        Given: self={}, other={}\'.format(_func_name, self, other))\n\n    fill_value_is_none = isinstance(fill_value, (types.NoneType, types.Omitted)) or fill_value is None\n    if not operands_are_series:\n        def _series_comp_binop_scalar_impl(self, other, level=None, fill_value=None, axis=0):\n            if self_is_series == True:  # noqa\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                return pandas.Series(self._data < other, index=self._index, name=self._name)\n            else:\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n                return pandas.Series(self < other._data, index=other._index, name=other._name)\n\n        return _series_comp_binop_scalar_impl\n\n    else:\n\n        # optimization for series with default indexes, that can be aligned differently\n        if (isinstance(self.index, types.NoneType) and isinstance(other.index, types.NoneType)):\n            def _series_comp_binop_none_indexes_impl(self, other, level=None, fill_value=None, axis=0):\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n                left_size, right_size = len(self._data), len(other._data)\n                if (left_size == right_size):\n                    return pandas.Series(self._data < other._data)\n                else:\n                    raise ValueError(""Can only compare identically-labeled Series objects"")\n\n            return _series_comp_binop_none_indexes_impl\n        else:\n\n            if none_or_numeric_indexes:\n                ty_left_index_dtype = types.int64 if isinstance(self.index, types.NoneType) else self.index.dtype\n                ty_right_index_dtype = types.int64 if isinstance(other.index, types.NoneType) else other.index.dtype\n                numba_index_common_dtype = find_common_dtype_from_numpy_dtypes(\n                    [ty_left_index_dtype, ty_right_index_dtype], [])\n\n            def _series_comp_binop_common_impl(self, other, level=None, fill_value=None, axis=0):\n                if not (fill_value is None or numpy.isnan(fill_value)):\n                    numpy_like.fillna(self._data, inplace=True, value=fill_value)\n                    numpy_like.fillna(other._data, inplace=True, value=fill_value)\n                left_index, right_index = self.index, other.index\n\n                if sdc_check_indexes_equal(left_index, right_index):\n                    if none_or_numeric_indexes == True:  # noqa\n                        new_index = numpy_like.astype(left_index, numba_index_common_dtype)\n                    else:\n                        new_index = self._index\n                    return pandas.Series(self._data < other._data,\n                                         new_index)\n                else:\n                    raise ValueError(""Can only compare identically-labeled Series objects"")\n\n            return _series_comp_binop_common_impl\n\n    return None\n\n\ndef sdc_pandas_series_operator_binop(self, other):\n    """"""\n    Pandas Series operator :attr:`pandas.Series.binop` implementation\n\n    Note: Currently implemented for numeric Series only.\n        Differs from Pandas in returning Series with fixed dtype :obj:`float64`\n\n    .. only:: developer\n\n    **Test**: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_op1*\n              python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_op2*\n              python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_operator_binop*\n\n    Parameters\n    ----------\n    series: :obj:`pandas.Series`\n        Input series\n    other: :obj:`pandas.Series` or :obj:`scalar`\n        Series or scalar value to be used as a second argument of binary operation\n\n    Returns\n    -------\n    :obj:`pandas.Series`\n        The result of the operation\n    """"""\n\n    ty_checker = TypeChecker(\'Operator binop().\')\n    self_is_series, other_is_series = isinstance(self, SeriesType), isinstance(other, SeriesType)\n    if not (self_is_series or other_is_series):\n        return None\n\n    # this overload is not for string series\n    self_is_string_series = self_is_series and isinstance(self.dtype, types.UnicodeType)\n    other_is_string_series = other_is_series and isinstance(other.dtype, types.UnicodeType)\n    if self_is_string_series or other_is_string_series:\n        return None\n\n    if not isinstance(self, (SeriesType, types.Number)):\n        ty_checker.raise_exc(self, \'pandas.series or scalar\', \'self\')\n\n    if not isinstance(other, (SeriesType, types.Number)):\n        ty_checker.raise_exc(other, \'pandas.series or scalar\', \'other\')\n\n    operands_are_series = self_is_series and other_is_series\n    if operands_are_series:\n        none_or_numeric_indexes = ((isinstance(self.index, types.NoneType) or check_index_is_numeric(self))\n                                   and (isinstance(other.index, types.NoneType) or check_index_is_numeric(other)))\n        series_indexes_comparable = check_types_comparable(self.index, other.index) or none_or_numeric_indexes\n        if not series_indexes_comparable:\n            raise TypingError(\'{} Not implemented for series with not-comparable indexes. \\\n            Given: self.index={}, other.index={}\'.format(_func_name, self.index, other.index))\n\n    series_data_comparable = check_types_comparable(self, other)\n    if not series_data_comparable:\n        raise TypingError(\'{} Not supported for not-comparable operands. \\\n        Given: self={}, other={}\'.format(_func_name, self, other))\n\n    def sdc_pandas_series_operator_binop_impl(self, other):\n        return self.binop(other)\n\n    return sdc_pandas_series_operator_binop_impl\n\n\ndef sdc_pandas_series_operator_comp_binop(self, other):\n    """"""\n    Pandas Series operator :attr:`pandas.Series.comp_binop` implementation\n\n    .. only:: developer\n\n    **Test**: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_op7*\n              python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_operator_comp_binop*\n\n    Parameters\n    ----------\n    series: :obj:`pandas.Series`\n        Input series\n    other: :obj:`pandas.Series` or :obj:`scalar`\n        Series or scalar value to be used as a second argument of binary operation\n\n    Returns\n    -------\n    :obj:`pandas.Series`\n        The result of the operation\n    """"""\n\n    ty_checker = TypeChecker(\'Operator comp_binop().\')\n    self_is_series, other_is_series = isinstance(self, SeriesType), isinstance(other, SeriesType)\n    if not (self_is_series or other_is_series):\n        return None\n\n    if not isinstance(self, (SeriesType, types.Number, types.UnicodeType)):\n        ty_checker.raise_exc(self, \'pandas.series or scalar\', \'self\')\n\n    if not isinstance(other, (SeriesType, types.Number, types.UnicodeType)):\n        ty_checker.raise_exc(other, \'pandas.series or scalar\', \'other\')\n\n    operands_are_series = self_is_series and other_is_series\n    if operands_are_series:\n        none_or_numeric_indexes = ((isinstance(self.index, types.NoneType) or check_index_is_numeric(self))\n                                   and (isinstance(other.index, types.NoneType) or check_index_is_numeric(other)))\n        series_indexes_comparable = check_types_comparable(self.index, other.index) or none_or_numeric_indexes\n        if not series_indexes_comparable:\n            raise TypingError(\'{} Not implemented for series with not-comparable indexes. \\\n            Given: self.index={}, other.index={}\'.format(_func_name, self.index, other.index))\n\n    series_data_comparable = check_types_comparable(self, other)\n    if not series_data_comparable:\n        raise TypingError(\'{} Not supported for not-comparable operands. \\\n        Given: self={}, other={}\'.format(_func_name, self, other))\n\n    def sdc_pandas_series_operator_comp_binop_impl(self, other):\n        return self.comp_binop(other)\n\n    return sdc_pandas_series_operator_comp_binop_impl\n\n\ndef sdc_str_arr_operator_comp_binop(self, other):\n\n    self_is_str_arr = self == string_array_type\n    other_is_str_arr = other == string_array_type\n    operands_are_arrays = self_is_str_arr and other_is_str_arr\n\n    if not (operands_are_arrays\n            or (self_is_str_arr and isinstance(other, types.UnicodeType))\n            or (isinstance(self, types.UnicodeType) and other_is_str_arr)):\n        return None\n\n    if operands_are_arrays:\n        def _sdc_str_arr_operator_comp_binop_impl(self, other):\n            if len(self) != len(other):\n                raise ValueError(""Mismatch of String Arrays sizes in operator.comp_binop"")\n            n = len(self)\n            out_list = [False] * n\n            for i in numba.prange(n):\n                out_list[i] = (self[i] < other[i]\n                               and not (str_arr_is_na(self, i) or str_arr_is_na(other, i)))\n            return out_list\n\n    elif self_is_str_arr:\n        def _sdc_str_arr_operator_comp_binop_impl(self, other):\n            n = len(self)\n            out_list = [False] * n\n            for i in numba.prange(n):\n                out_list[i] = (self[i] < other and not (str_arr_is_na(self, i)))\n            return out_list\n\n    elif other_is_str_arr:\n        def _sdc_str_arr_operator_comp_binop_impl(self, other):\n            n = len(other)\n            out_list = [False] * n\n            for i in numba.prange(n):\n                out_list[i] = (self < other[i] and not (str_arr_is_na(other, i)))\n            return out_list\n    else:\n        return None\n\n    return _sdc_str_arr_operator_comp_binop_impl\n'"
sdc/set_ext.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\nfrom sdc.str_arr_ext import (StringArray, StringArrayType, string_array_type,\n                              pre_alloc_string_array, StringArrayPayloadType,\n                              is_str_arr_typ)\nfrom sdc.str_ext import string_type, gen_get_unicode_chars\nfrom sdc.utilities.utils import to_array\nimport sdc\nimport operator\nimport numba\nfrom numba import types, generated_jit\nfrom numba.core import typing\nfrom numba.extending import box, unbox, NativeValue\nfrom numba.extending import models, register_model\nfrom numba.extending import lower_builtin, overload_method, overload, intrinsic\nfrom numba.core.imputils import (impl_ret_new_ref, impl_ret_borrowed,\n                                    iternext_impl, impl_ret_untracked, RefType)\nfrom numba.core import cgutils\nfrom numba.core.typing.templates import signature, AbstractTemplate, infer, infer_global\n\nfrom llvmlite import ir as lir\nimport llvmlite.binding as ll\nfrom . import hset_ext\nll.add_symbol(\'init_set_string\', hset_ext.init_set_string)\nll.add_symbol(\'insert_set_string\', hset_ext.insert_set_string)\nll.add_symbol(\'len_set_string\', hset_ext.len_set_string)\nll.add_symbol(\'set_in_string\', hset_ext.set_in_string)\nll.add_symbol(\'set_iterator_string\', hset_ext.set_iterator_string)\nll.add_symbol(\'set_itervalid_string\', hset_ext.set_itervalid_string)\nll.add_symbol(\'set_nextval_string\', hset_ext.set_nextval_string)\nll.add_symbol(\'num_total_chars_set_string\', hset_ext.num_total_chars_set_string)\nll.add_symbol(\'populate_str_arr_from_set\', hset_ext.populate_str_arr_from_set)\n\n\n# similar to types.Container.Set\nclass SetType(types.Container):\n    def __init__(self, dtype):\n        self.dtype = dtype\n        super(SetType, self).__init__(\n            name=\'SetType({})\'.format(dtype))\n\n    @property\n    def key(self):\n        return self.dtype\n\n    @property\n    def iterator_type(self):\n        return SetIterType(self)\n\n    def is_precise(self):\n        return self.dtype.is_precise()\n\n\nset_string_type = SetType(string_type)\n\n\nclass SetIterType(types.BaseContainerIterator):\n    container_class = SetType\n\n\nregister_model(SetType)(models.OpaqueModel)\n\n\n_init_set_string = types.ExternalFunction(""init_set_string"",\n                                          set_string_type())\n\n\ndef init_set_string():\n    return set()\n\n\n@overload(init_set_string)\ndef init_set_overload():\n    return lambda: _init_set_string()\n\n\nadd_set_string = types.ExternalFunction(""insert_set_string"",\n                                        types.void(set_string_type, types.voidptr))\n\nlen_set_string = types.ExternalFunction(""len_set_string"",\n                                        types.intp(set_string_type))\n\nnum_total_chars_set_string = types.ExternalFunction(""num_total_chars_set_string"",\n                                                    types.intp(set_string_type))\n\n# TODO: box set(string)\n\n\n@generated_jit(nopython=True, cache=True)\ndef build_set(A):\n    if is_str_arr_typ(A):\n        return _build_str_set_impl\n    else:\n        return lambda A: set(A)\n\n\ndef _build_str_set_impl(A):\n    str_arr = sdc.hiframes.api.dummy_unbox_series(A)\n    str_set = init_set_string()\n    n = len(str_arr)\n    for i in range(n):\n        _str = str_arr[i]\n        str_set.add(_str)\n    return str_set\n\n# TODO: remove since probably unused\n@overload(set)\ndef init_set_string_array(A):\n    if is_str_arr_typ(A):\n        return _build_str_set_impl\n\n\n@overload_method(SetType, \'add\')\ndef set_add_overload(set_obj, item):\n    # TODO: expand to other set types\n    assert set_obj == set_string_type and item == string_type\n\n    def add_impl(set_obj, item):\n        return add_set_string(set_obj, item._data)\n    return add_impl\n\n\n@overload(len)\ndef len_set_str_overload(A):\n    if A == set_string_type:\n        def len_impl(A):\n            return len_set_string(A)\n        return len_impl\n\n# FIXME: overload fails in lowering sometimes!\n@lower_builtin(len, set_string_type)\ndef lower_len_set_impl(context, builder, sig, args):\n\n    def len_impl(str_set):\n        return len_set_string(str_set)\n\n    res = context.compile_internal(builder, len_impl, sig, args)\n    return impl_ret_untracked(context, builder, sig.return_type, res)\n\n\n@infer\nclass InSet(AbstractTemplate):\n    key = ""in""\n\n    def generic(self, args, kws):\n        _, cont_typ = args\n        if cont_typ == set_string_type:\n            return signature(types.boolean, cont_typ.dtype, cont_typ)\n\n\n@infer_global(operator.contains)\nclass InSetOp(AbstractTemplate):\n    def generic(self, args, kws):\n        cont_typ, _ = args\n        if cont_typ == set_string_type:\n            return signature(types.boolean, cont_typ, cont_typ.dtype)\n\n\n@lower_builtin(""in"", string_type, set_string_type)\ndef lower_dict_in(context, builder, sig, args):\n    fnty = lir.FunctionType(lir.IntType(1), [lir.IntType(8).as_pointer(),\n                                             lir.IntType(8).as_pointer()])\n    fn = builder.module.get_or_insert_function(fnty, name=""set_in_string"")\n    return builder.call(fn, args)\n\n\n@lower_builtin(operator.contains, set_string_type, string_type)\ndef lower_dict_in_op(context, builder, sig, args):\n    set_str, unicode_str = args\n    char_str = gen_get_unicode_chars(context, builder, unicode_str)\n    fnty = lir.FunctionType(lir.IntType(1), [lir.IntType(8).as_pointer(),\n                                             lir.IntType(8).as_pointer()])\n    fn = builder.module.get_or_insert_function(fnty, name=""set_in_string"")\n    return builder.call(fn, [char_str, set_str])\n\n\n@overload(to_array)\ndef to_array_overload(A):\n    if A == set_string_type:\n        #\n        def set_string_to_array(A):\n            # TODO: support unicode\n            num_total_chars = num_total_chars_set_string(A)\n            num_strs = len(A)\n            str_arr = pre_alloc_string_array(num_strs, num_total_chars)\n            populate_str_arr_from_set(A, str_arr)\n            return str_arr\n\n        return set_string_to_array\n\n\n@intrinsic\ndef populate_str_arr_from_set(typingctx, in_set_typ, in_str_arr_typ=None):\n    assert in_set_typ == set_string_type\n    assert is_str_arr_typ(in_str_arr_typ)\n\n    def codegen(context, builder, sig, args):\n        in_set, in_str_arr = args\n\n        string_array = context.make_helper(builder, string_array_type, in_str_arr)\n\n        fnty = lir.FunctionType(lir.VoidType(),\n                                [lir.IntType(8).as_pointer(),\n                                 lir.IntType(32).as_pointer(),\n                                 lir.IntType(8).as_pointer(),\n                                 ])\n        fn_getitem = builder.module.get_or_insert_function(fnty,\n                                                           name=""populate_str_arr_from_set"")\n        builder.call(fn_getitem, [in_set, string_array.offsets,\n                                  string_array.data])\n        return context.get_dummy_value()\n\n    return types.void(set_string_type, string_array_type), codegen\n\n# TODO: delete iterator\n\n\n@register_model(SetIterType)\nclass StrSetIteratorModel(models.StructModel):\n    def __init__(self, dmm, fe_type):\n        members = [(\'itp\', types.Opaque(\'SetIterPtr\')),\n                   (\'set\', set_string_type)]\n        super(StrSetIteratorModel, self).__init__(dmm, fe_type, members)\n\n\n@lower_builtin(\'getiter\', SetType)\ndef getiter_set(context, builder, sig, args):\n    fnty = lir.FunctionType(lir.IntType(8).as_pointer(),\n                            [lir.IntType(8).as_pointer()])\n    fn = builder.module.get_or_insert_function(fnty, name=""set_iterator_string"")\n    itp = builder.call(fn, args)\n\n    iterobj = context.make_helper(builder, sig.return_type)\n\n    iterobj.itp = itp\n    iterobj.set = args[0]\n\n    return iterobj._getvalue()\n\n\n@lower_builtin(\'iternext\', SetIterType)\n@iternext_impl(RefType.NEW)\ndef iternext_setiter(context, builder, sig, args, result):\n    iterty, = sig.args\n    it, = args\n    iterobj = context.make_helper(builder, iterty, value=it)\n\n    fnty = lir.FunctionType(lir.IntType(1),\n                            [lir.IntType(8).as_pointer(), lir.IntType(8).as_pointer()])\n    fn = builder.module.get_or_insert_function(fnty, name=""set_itervalid_string"")\n    is_valid = builder.call(fn, [iterobj.itp, iterobj.set])\n    result.set_valid(is_valid)\n\n    fnty = lir.FunctionType(lir.IntType(8).as_pointer(),\n                            [lir.IntType(8).as_pointer()])\n    fn = builder.module.get_or_insert_function(fnty, name=""set_nextval_string"")\n    kind = numba.cpython.unicode.PY_UNICODE_1BYTE_KIND\n\n    def std_str_to_unicode(std_str):\n        length = sdc.str_ext.get_std_str_len(std_str)\n        ret = numba.cpython.unicode._empty_string(kind, length)\n        sdc.str_arr_ext._memcpy(\n            ret._data, sdc.str_ext.get_c_str(std_str), length, 1)\n        sdc.str_ext.del_str(std_str)\n        return ret\n\n    with builder.if_then(is_valid):\n        val = builder.call(fn, [iterobj.itp])\n        val = context.compile_internal(\n            builder,\n            std_str_to_unicode,\n            string_type(sdc.str_ext.std_str_type),\n            [val])\n        result.yield_(val)\n'"
sdc/shuffle_utils.py,19,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\nfrom collections import namedtuple\nimport numpy as np\n\nfrom numba import types\nfrom numba.extending import overload\n\nimport sdc\nfrom sdc.utilities.utils import get_ctypes_ptr, _numba_to_c_type_map\nfrom sdc.timsort import getitem_arr_tup\nfrom sdc.str_ext import string_type\nfrom sdc.str_arr_ext import (string_array_type, to_string_list,\n                              get_offset_ptr, get_data_ptr, convert_len_arr_to_offset,\n                              pre_alloc_string_array, num_total_chars)\n\n\n# metadata required for shuffle\n# send_counts -> pre, single\n# recv_counts -> single\n# send_buff\n# out_arr\n# n_send  -> single\n# n_out  -> single\n# send_disp -> single\n# recv_disp -> single\n# tmp_offset -> single\n# string arrays\n# send_counts_char -> pre\n# recv_counts_char\n# send_arr_lens -> pre\n# send_arr_chars\n# send_disp_char\n# recv_disp_char\n# tmp_offset_char\n# dummy array to key reference count alive, since ArrayCTypes can\'t be\n# passed to jitclass TODO: update\n# send_arr_chars_arr\n\n\nPreShuffleMeta = namedtuple(\'PreShuffleMeta\',\n                            \'send_counts, send_counts_char_tup, send_arr_lens_tup\')\n\nShuffleMeta = namedtuple(\'ShuffleMeta\',\n                         (\'send_counts, recv_counts, n_send, n_out, send_disp, recv_disp, \'\n                          \'tmp_offset, send_buff_tup, out_arr_tup, send_counts_char_tup, \'\n                          \'recv_counts_char_tup, send_arr_lens_tup, send_arr_chars_tup, \'\n                          \'send_disp_char_tup, recv_disp_char_tup, tmp_offset_char_tup, \'\n                          \'send_arr_chars_arr_tup\'))\n\n\n# before shuffle, \'send_counts\' is needed as well as\n# \'send_counts_char\' and \'send_arr_lens\' for every string type\ndef alloc_pre_shuffle_metadata(arr, data, n_pes, is_contig):\n    return PreShuffleMeta(np.zeros(n_pes, np.int32), ())\n\n\n@overload(alloc_pre_shuffle_metadata)\ndef alloc_pre_shuffle_metadata_overload(key_arrs, data, n_pes, is_contig):\n\n    func_text = ""def f(key_arrs, data, n_pes, is_contig):\\n""\n    # send_counts\n    func_text += ""  send_counts = np.zeros(n_pes, np.int32)\\n""\n\n    # send_counts_char, send_arr_lens for strings\n    n_keys = len(key_arrs.types)\n    n_str = 0\n    for i, typ in enumerate(key_arrs.types + data.types):\n        if typ == string_array_type:\n            func_text += (""  arr = key_arrs[{}]\\n"".format(i) if i < n_keys else ""  arr = data[{}]\\n"".format(i - n_keys))\n            func_text += ""  send_counts_char_{} = np.zeros(n_pes, np.int32)\\n"".format(n_str)\n            func_text += ""  send_arr_lens_{} = np.empty(1, np.uint32)\\n"".format(n_str)\n            # needs allocation since written in update before finalize\n            func_text += ""  if is_contig:\\n""\n            func_text += ""    send_arr_lens_{} = np.empty(len(arr), np.uint32)\\n"".format(n_str)\n            n_str += 1\n\n    count_char_tup = "", "".join(""send_counts_char_{}"".format(i) for i in range(n_str))\n    lens_tup = "", "".join(""send_arr_lens_{}"".format(i) for i in range(n_str))\n    extra_comma = "","" if n_str == 1 else """"\n    func_text += ""  return PreShuffleMeta(send_counts, ({}{}), ({}{}))\\n"".format(\n        count_char_tup, extra_comma, lens_tup, extra_comma)\n\n    # print(func_text)\n\n    loc_vars = {}\n    exec(func_text, {\'np\': np, \'PreShuffleMeta\': PreShuffleMeta}, loc_vars)\n    alloc_impl = loc_vars[\'f\']\n    return alloc_impl\n\n\n# \'send_counts\' is updated, and \'send_counts_char\' and \'send_arr_lens\'\n# for every string type\ndef update_shuffle_meta(pre_shuffle_meta, node_id, ind, val, data, is_contig=True):\n    pre_shuffle_meta.send_counts[node_id] += 1\n\n\n@overload(update_shuffle_meta)\ndef update_shuffle_meta_overload(pre_shuffle_meta, node_id, ind, val, data, is_contig=True):\n    func_text = ""def f(pre_shuffle_meta, node_id, ind, val, data, is_contig=True):\\n""\n    func_text += ""  pre_shuffle_meta.send_counts[node_id] += 1\\n""\n    n_keys = len(val.types)\n    n_str = 0\n    for i, typ in enumerate(val.types + data.types):\n        if typ in (string_type, string_array_type):\n            val_or_data = \'val[{}]\'.format(i) if i < n_keys else \'data[{}]\'.format(i - n_keys)\n            func_text += ""  n_chars = len({})\\n"".format(val_or_data)\n            func_text += ""  pre_shuffle_meta.send_counts_char_tup[{}][node_id] += n_chars\\n"".format(n_str)\n            func_text += ""  if is_contig:\\n""\n            func_text += ""    pre_shuffle_meta.send_arr_lens_tup[{}][ind] = n_chars\\n"".format(n_str)\n            n_str += 1\n\n    # print(func_text)\n\n    loc_vars = {}\n    exec(func_text, {}, loc_vars)\n    update_impl = loc_vars[\'f\']\n    return update_impl\n\n\ndef finalize_shuffle_meta(arrs, data, pre_shuffle_meta, n_pes, is_contig, init_vals=()):\n    return ShuffleMeta()\n\n\n@overload(finalize_shuffle_meta)\ndef finalize_shuffle_meta_overload(key_arrs, data, pre_shuffle_meta, n_pes, is_contig, init_vals=()):\n\n    func_text = ""def f(key_arrs, data, pre_shuffle_meta, n_pes, is_contig, init_vals=()):\\n""\n    # common metas: send_counts, recv_counts, tmp_offset, n_out, n_send, send_disp, recv_disp\n    func_text += ""  send_counts = pre_shuffle_meta.send_counts\\n""\n    func_text += ""  recv_counts = np.empty(n_pes, np.int32)\\n""\n    func_text += ""  tmp_offset = np.zeros(n_pes, np.int32)\\n""  # for non-contig\n    func_text += ""  sdc.distributed_api.alltoall(send_counts, recv_counts, 1)\\n""\n    func_text += ""  n_out = recv_counts.sum()\\n""\n    func_text += ""  n_send = send_counts.sum()\\n""\n    func_text += ""  send_disp = sdc.hiframes.join.calc_disp(send_counts)\\n""\n    func_text += ""  recv_disp = sdc.hiframes.join.calc_disp(recv_counts)\\n""\n\n    n_keys = len(key_arrs.types)\n    n_all = len(key_arrs.types + data.types)\n    n_str = 0\n\n    for i, typ in enumerate(key_arrs.types + data.types):\n        func_text += (""  arr = key_arrs[{}]\\n"".format(i) if i < n_keys\n                      else ""  arr = data[{}]\\n"".format(i - n_keys))\n        if isinstance(typ, types.Array):\n            func_text += ""  out_arr_{} = fix_cat_array_type(np.empty(n_out, arr.dtype))\\n"".format(i)\n            func_text += ""  send_buff_{} = arr\\n"".format(i)\n            func_text += ""  if not is_contig:\\n""\n            if i >= n_keys and init_vals != ():\n                func_text += ""    send_buff_{} = fix_cat_array_type(np.full(n_send,\\n"".format(i)\n                func_text += ""                                              init_vals[{}],\\n"".format(i - n_keys)\n                func_text += ""                                              arr.dtype))\\n""\n            else:\n                func_text += ""    send_buff_{} = fix_cat_array_type(np.empty(n_send, arr.dtype))\\n"".format(i)\n        else:\n            assert typ == string_array_type\n            # send_buff is None for strings\n            func_text += ""  send_buff_{} = None\\n"".format(i)\n            # send/recv counts\n            func_text += ""  send_counts_char_{} = pre_shuffle_meta.send_counts_char_tup[{}]\\n"".format(n_str, n_str)\n            func_text += ""  recv_counts_char_{} = np.empty(n_pes, np.int32)\\n"".format(n_str)\n            func_text += (""  sdc.distributed_api.alltoall(""\n                          ""send_counts_char_{}, recv_counts_char_{}, 1)\\n"").format(n_str, n_str)\n            # alloc output\n            func_text += ""  n_all_chars = recv_counts_char_{}.sum()\\n"".format(n_str)\n            func_text += ""  out_arr_{} = pre_alloc_string_array(n_out, n_all_chars)\\n"".format(i)\n            # send/recv disp\n            func_text += (""  send_disp_char_{} = sdc.hiframes.join.""\n                          ""calc_disp(send_counts_char_{})\\n"").format(n_str, n_str)\n            func_text += (""  recv_disp_char_{} = sdc.hiframes.join.""\n                          ""calc_disp(recv_counts_char_{})\\n"").format(n_str, n_str)\n\n            # tmp_offset_char, send_arr_lens\n            func_text += ""  tmp_offset_char_{} = np.zeros(n_pes, np.int32)\\n"".format(n_str)\n            func_text += ""  send_arr_lens_{} = pre_shuffle_meta.send_arr_lens_tup[{}]\\n"".format(n_str, n_str)\n            # send char arr\n            # TODO: arr refcount if arr is not stored somewhere?\n            func_text += ""  send_arr_chars_arr_{} = np.empty(1, np.uint8)\\n"".format(n_str)\n            func_text += ""  send_arr_chars_{} = get_ctypes_ptr(get_data_ptr(arr))\\n"".format(n_str)\n            func_text += ""  if not is_contig:\\n""\n            func_text += ""    send_arr_lens_{} = np.empty(n_send, np.uint32)\\n"".format(n_str)\n            func_text += ""    s_n_all_chars = send_counts_char_{}.sum()\\n"".format(n_str)\n            func_text += ""    send_arr_chars_arr_{} = np.empty(s_n_all_chars, np.uint8)\\n"".format(n_str)\n            func_text += ""    send_arr_chars_{} = get_ctypes_ptr(send_arr_chars_arr_{}.ctypes)\\n"".format(n_str, n_str)\n            n_str += 1\n\n    send_buffs = "", "".join(""send_buff_{}"".format(i) for i in range(n_all))\n    out_arrs = "", "".join(""out_arr_{}"".format(i) for i in range(n_all))\n    all_comma = "","" if n_all == 1 else """"\n    send_counts_chars = "", "".join(""send_counts_char_{}"".format(i) for i in range(n_str))\n    recv_counts_chars = "", "".join(""recv_counts_char_{}"".format(i) for i in range(n_str))\n    send_arr_lens = "", "".join(""send_arr_lens_{}"".format(i) for i in range(n_str))\n    send_arr_chars = "", "".join(""send_arr_chars_{}"".format(i) for i in range(n_str))\n    send_disp_chars = "", "".join(""send_disp_char_{}"".format(i) for i in range(n_str))\n    recv_disp_chars = "", "".join(""recv_disp_char_{}"".format(i) for i in range(n_str))\n    tmp_offset_chars = "", "".join(""tmp_offset_char_{}"".format(i) for i in range(n_str))\n    send_arr_chars_arrs = "", "".join(""send_arr_chars_arr_{}"".format(i) for i in range(n_str))\n    str_comma = "","" if n_str == 1 else """"\n\n    func_text += (\'  return ShuffleMeta(send_counts, recv_counts, n_send, n_out, send_disp, recv_disp, tmp_offset,\\n\')\n    func_text += (\'                     ({}{}), ({}{}), ({}{}), ({}{}), ({}{}),\\n\').format(send_buffs, all_comma,\n                                                                                           out_arrs,\n                                                                                           all_comma,\n                                                                                           send_counts_chars,\n                                                                                           str_comma,\n                                                                                           recv_counts_chars,\n                                                                                           str_comma,\n                                                                                           send_arr_lens,\n                                                                                           str_comma)\n    func_text += (\'                     ({}{}), ({}{}), ({}{}), ({}{}), ({}{}), )\\n\').format(send_arr_chars,\n                                                                                             str_comma,\n                                                                                             send_disp_chars,\n                                                                                             str_comma,\n                                                                                             recv_disp_chars,\n                                                                                             str_comma,\n                                                                                             tmp_offset_chars,\n                                                                                             str_comma,\n                                                                                             send_arr_chars_arrs,\n                                                                                             str_comma)\n\n    loc_vars = {}\n    exec(func_text, {\'np\': np, \'sdc\': sdc,\n                     \'pre_alloc_string_array\': pre_alloc_string_array,\n                     \'num_total_chars\': num_total_chars,\n                     \'get_data_ptr\': get_data_ptr,\n                     \'ShuffleMeta\': ShuffleMeta,\n                     \'get_ctypes_ptr\': get_ctypes_ptr,\n                     \'fix_cat_array_type\':\n                     sdc.hiframes.pd_categorical_ext.fix_cat_array_type}, loc_vars)\n    finalize_impl = loc_vars[\'f\']\n    return finalize_impl\n\n\ndef alltoallv(arr, m):\n    return\n\n\n@overload(alltoallv)\ndef alltoallv_impl(arr, metadata):\n    if isinstance(arr, types.Array):\n        def a2av_impl(arr, metadata):\n            sdc.distributed_api.alltoallv(\n                metadata.send_buff, metadata.out_arr, metadata.send_counts,\n                metadata.recv_counts, metadata.send_disp, metadata.recv_disp)\n        return a2av_impl\n\n    assert arr == string_array_type\n    int32_typ_enum = np.int32(_numba_to_c_type_map[types.int32])\n    char_typ_enum = np.int32(_numba_to_c_type_map[types.uint8])\n\n    def a2av_str_impl(arr, metadata):\n        # TODO: increate refcount?\n        offset_ptr = get_offset_ptr(metadata.out_arr)\n        sdc.distributed_api.c_alltoallv(\n            metadata.send_arr_lens.ctypes,\n            offset_ptr,\n            metadata.send_counts.ctypes,\n            metadata.recv_counts.ctypes,\n            metadata.send_disp.ctypes,\n            metadata.recv_disp.ctypes,\n            int32_typ_enum)\n        sdc.distributed_api.c_alltoallv(\n            metadata.send_arr_chars,\n            get_data_ptr(\n                metadata.out_arr),\n            metadata.send_counts_char.ctypes,\n            metadata.recv_counts_char.ctypes,\n            metadata.send_disp_char.ctypes,\n            metadata.recv_disp_char.ctypes,\n            char_typ_enum)\n        convert_len_arr_to_offset(offset_ptr, metadata.n_out)\n    return a2av_str_impl\n\n\ndef alltoallv_tup(arrs, shuffle_meta):\n    return arrs\n\n\n@overload(alltoallv_tup)\ndef alltoallv_tup_overload(arrs, meta):\n    func_text = ""def f(arrs, meta):\\n""\n    n_str = 0\n    for i, typ in enumerate(arrs.types):\n        if isinstance(typ, types.Array):\n            func_text += (""  sdc.distributed_api.alltoallv(""\n                          ""meta.send_buff_tup[{}], meta.out_arr_tup[{}], meta.send_counts,""\n                          ""meta.recv_counts, meta.send_disp, meta.recv_disp)\\n"").format(i, i)\n        else:\n            assert typ == string_array_type\n            func_text += ""  offset_ptr_{} = get_offset_ptr(meta.out_arr_tup[{}])\\n"".format(i, i)\n\n            func_text += (""  sdc.distributed_api.c_alltoallv(""\n                          ""meta.send_arr_lens_tup[{}].ctypes, offset_ptr_{}, meta.send_counts.ctypes, ""\n                          ""meta.recv_counts.ctypes, meta.send_disp.ctypes, ""\n                          ""meta.recv_disp.ctypes, int32_typ_enum)\\n"").format(n_str, i)\n\n            func_text += (""  sdc.distributed_api.c_alltoallv(""\n                          ""meta.send_arr_chars_tup[{}], get_data_ptr(meta.out_arr_tup[{}]),""\n                          ""meta.send_counts_char_tup[{}].ctypes, meta.recv_counts_char_tup[{}].ctypes,""\n                          ""meta.send_disp_char_tup[{}].ctypes, meta.recv_disp_char_tup[{}].ctypes,""\n                          ""char_typ_enum)\\n"").format(n_str, i, n_str, n_str, n_str, n_str)\n\n            func_text += ""  convert_len_arr_to_offset(offset_ptr_{}, meta.n_out)\\n"".format(i)\n            n_str += 1\n\n    func_text += ""  return ({}{})\\n"".format(\n        \',\'.join([\'meta.out_arr_tup[{}]\'.format(i) for i in range(arrs.count)]),\n        "","" if arrs.count == 1 else """")\n\n    int32_typ_enum = np.int32(_numba_to_c_type_map[types.int32])\n    char_typ_enum = np.int32(_numba_to_c_type_map[types.uint8])\n    loc_vars = {}\n    exec(func_text, {\'sdc\': sdc, \'get_offset_ptr\': get_offset_ptr,\n                     \'get_data_ptr\': get_data_ptr, \'int32_typ_enum\': int32_typ_enum,\n                     \'char_typ_enum\': char_typ_enum,\n                     \'convert_len_arr_to_offset\': convert_len_arr_to_offset}, loc_vars)\n    a2a_impl = loc_vars[\'f\']\n    return a2a_impl\n\n\ndef _get_keys_tup(recvs, key_arrs):\n    return recvs[:len(key_arrs)]\n\n\n@overload(_get_keys_tup)\ndef _get_keys_tup_overload(recvs, key_arrs):\n    n_keys = len(key_arrs.types)\n    func_text = ""def f(recvs, key_arrs):\\n""\n    res = "","".join(""recvs[{}]"".format(i) for i in range(n_keys))\n    func_text += ""  return ({}{})\\n"".format(res, "","" if n_keys == 1 else """")\n    loc_vars = {}\n    exec(func_text, {}, loc_vars)\n    impl = loc_vars[\'f\']\n    return impl\n\n\ndef _get_data_tup(recvs, key_arrs):\n    return recvs[len(key_arrs):]\n\n\n@overload(_get_data_tup)\ndef _get_data_tup_overload(recvs, key_arrs):\n    n_keys = len(key_arrs.types)\n    n_all = len(recvs.types)\n    n_data = n_all - n_keys\n    func_text = ""def f(recvs, key_arrs):\\n""\n    res = "","".join(""recvs[{}]"".format(i) for i in range(n_keys, n_all))\n    func_text += ""  return ({}{})\\n"".format(res, "","" if n_data == 1 else """")\n    loc_vars = {}\n    exec(func_text, {}, loc_vars)\n    impl = loc_vars[\'f\']\n    return impl\n\n\n# returns scalar instead of tuple if only one array\ndef getitem_arr_tup_single(arrs, i):\n    return arrs[0][i]\n\n\n@overload(getitem_arr_tup_single)\ndef getitem_arr_tup_single_overload(arrs, i):\n    if len(arrs.types) == 1:\n        return lambda arrs, i: arrs[0][i]\n    return lambda arrs, i: getitem_arr_tup(arrs, i)\n\n\ndef val_to_tup(val):\n    return (val,)\n\n\n@overload(val_to_tup)\ndef val_to_tup_overload(val):\n    if isinstance(val, types.BaseTuple):\n        return lambda val: val\n    return lambda val: (val,)\n'"
sdc/str_arr_ext.py,8,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\nimport llvmlite.binding as ll\nimport llvmlite.llvmpy.core as lc\nimport numba\nimport numpy as np\nimport operator\nimport sdc\n\nfrom sdc import hstr_ext\nfrom glob import glob\nfrom llvmlite import ir as lir\nfrom numba import types\nfrom numba.core import cgutils\nfrom numba.extending import (typeof_impl, type_callable, models, register_model, NativeValue,\n                             lower_builtin, box, unbox, lower_getattr, intrinsic,\n                             overload_method, overload, overload_attribute)\nfrom numba.cpython.hashing import _Py_hash_t\nfrom numba.core.imputils import (impl_ret_new_ref, impl_ret_borrowed, iternext_impl, RefType)\nfrom numba.cpython.listobj import ListInstance\nfrom numba.core.typing.templates import (infer_global, AbstractTemplate, infer,\n                                         signature, AttributeTemplate, infer_getattr, bound_function)\nfrom numba import prange\n\nfrom sdc.str_ext import string_type\nfrom sdc.str_arr_type import (StringArray, string_array_type, StringArrayType,\n                              StringArrayPayloadType, str_arr_payload_type, StringArrayIterator,\n                              is_str_arr_typ, offset_typ, data_ctypes_type, offset_ctypes_type)\nfrom sdc.utilities.sdc_typing_utils import check_is_array_of_dtype\n\n\n@typeof_impl.register(StringArray)\ndef typeof_string_array(val, c):\n    return string_array_type\n\n# @type_callable(StringArray)\n# def type_string_array_call(context):\n#     def typer(offset, data):\n#         return string_array_type\n#     return typer\n\n\n@type_callable(StringArray)\ndef type_string_array_call2(context):\n    def typer(string_list=None):\n        return string_array_type\n    return typer\n\n\ndef iternext_str_array(context, builder, sig, args, result):\n    """"""\n    Implementation of iternext() for the StringArrayIterator type\n    :param context: context descriptor\n    :param builder: llvmlite IR Builder\n    :param sig: iterator signature\n    :param args: tuple with iterator arguments, such as instruction, operands and types\n    :param result: iternext result\n    """"""\n\n    [itertype] = sig.args\n    [iter_arg] = args\n\n    iterobj = context.make_helper(builder, itertype, value=iter_arg)\n    len_sig = signature(types.intp, string_array_type)\n    nitems = context.compile_internal(builder, lambda a: len(a), len_sig, [iterobj.array])\n\n    index = builder.load(iterobj.index)\n    is_valid = builder.icmp(lc.ICMP_SLT, index, nitems)\n    result.set_valid(is_valid)\n\n    with builder.if_then(is_valid):\n        getitem_sig = signature(string_type, string_array_type, types.intp)\n        value = context.compile_internal(builder, lambda a, i: a[i], getitem_sig, [iterobj.array, index])\n        result.yield_(value)\n        nindex = cgutils.increment_index(builder, index)\n        builder.store(nindex, iterobj.index)\n\n\n@register_model(StringArrayIterator)\nclass StrArrayIteratorModel(models.StructModel):\n    def __init__(self, dmm, fe_type):\n        # We use an unsigned index to avoid the cost of negative index tests.\n        members = [(\'index\', types.EphemeralPointer(types.uintp)),\n                   (\'array\', string_array_type)]\n        super(StrArrayIteratorModel, self).__init__(dmm, fe_type, members)\n\n\nlower_builtin(\'getiter\', string_array_type)(numba.np.arrayobj.getiter_array)\nlower_builtin(\'iternext\', StringArrayIterator)(iternext_impl(RefType.NEW)(iternext_str_array))\n\n\n@intrinsic\ndef num_total_chars(typingctx, str_arr_typ=None):\n    # None default to make IntelliSense happy\n    assert is_str_arr_typ(str_arr_typ)\n\n    def codegen(context, builder, sig, args):\n        in_str_arr, = args\n        string_array = context.make_helper(builder, string_array_type, in_str_arr)\n        return string_array.num_total_chars\n\n    return types.uint64(string_array_type), codegen\n\n\n@intrinsic\ndef get_offset_ptr(typingctx, str_arr_typ=None):\n    assert is_str_arr_typ(str_arr_typ)\n\n    def codegen(context, builder, sig, args):\n        in_str_arr, = args\n\n        string_array = context.make_helper(builder, string_array_type, in_str_arr)\n        # return string_array.offsets\n        # # Create new ArrayCType structure\n        ctinfo = context.make_helper(builder, offset_ctypes_type)\n        ctinfo.data = builder.bitcast(string_array.offsets, lir.IntType(32).as_pointer())\n        ctinfo.meminfo = string_array.meminfo\n        res = ctinfo._getvalue()\n        return impl_ret_borrowed(context, builder, offset_ctypes_type, res)\n\n    return offset_ctypes_type(string_array_type), codegen\n\n\n@intrinsic\ndef get_data_ptr(typingctx, str_arr_typ=None):\n    assert is_str_arr_typ(str_arr_typ)\n\n    def codegen(context, builder, sig, args):\n        in_str_arr, = args\n\n        string_array = context.make_helper(builder, string_array_type, in_str_arr)\n        # return string_array.data\n        # Create new ArrayCType structure\n        # TODO: put offset/data in main structure since immutable\n        ctinfo = context.make_helper(builder, data_ctypes_type)\n        ctinfo.data = string_array.data\n        ctinfo.meminfo = string_array.meminfo\n        res = ctinfo._getvalue()\n        return impl_ret_borrowed(context, builder, data_ctypes_type, res)\n\n    return data_ctypes_type(string_array_type), codegen\n\n\n@intrinsic\ndef get_data_ptr_ind(typingctx, str_arr_typ, int_t=None):\n    assert is_str_arr_typ(str_arr_typ)\n\n    def codegen(context, builder, sig, args):\n        in_str_arr, ind = args\n\n        string_array = context.make_helper(\n            builder, string_array_type, in_str_arr)\n        # Create new ArrayCType structure\n        # TODO: put offset/data in main structure since immutable\n        ctinfo = context.make_helper(builder, data_ctypes_type)\n        ctinfo.data = builder.gep(string_array.data, [ind])\n        ctinfo.meminfo = string_array.meminfo\n        res = ctinfo._getvalue()\n        return impl_ret_borrowed(context, builder, data_ctypes_type, res)\n\n    return data_ctypes_type(string_array_type, types.intp), codegen\n\n\n@intrinsic\ndef getitem_str_offset(typingctx, str_arr_typ, ind_t=None):\n    def codegen(context, builder, sig, args):\n        in_str_arr, ind = args\n\n        string_array = context.make_helper(builder, string_array_type, in_str_arr)\n        offsets = builder.bitcast(string_array.offsets, lir.IntType(32).as_pointer())\n        return builder.load(builder.gep(offsets, [ind]))\n\n    return types.uint32(string_array_type, ind_t), codegen\n\n# TODO: fix this for join\n@intrinsic\ndef setitem_str_offset(typingctx, str_arr_typ, ind_t, val_t=None):\n    def codegen(context, builder, sig, args):\n        in_str_arr, ind, val = args\n\n        string_array = context.make_helper(builder, string_array_type, in_str_arr)\n        offsets = builder.bitcast(string_array.offsets, lir.IntType(32).as_pointer())\n        builder.store(val, builder.gep(offsets, [ind]))\n        return context.get_dummy_value()\n\n    return types.void(string_array_type, ind_t, types.uint32), codegen\n\n\n@intrinsic\ndef copy_str_arr_slice(typingctx, str_arr_typ, out_str_arr_typ, ind_t=None):\n    def codegen(context, builder, sig, args):\n        out_str_arr, in_str_arr, ind = args\n\n        in_string_array = context.make_helper(builder, string_array_type, in_str_arr)\n\n        out_string_array = context.make_helper(builder, string_array_type, out_str_arr)\n\n        in_offsets = builder.bitcast(in_string_array.offsets, lir.IntType(32).as_pointer())\n        out_offsets = builder.bitcast(out_string_array.offsets, lir.IntType(32).as_pointer())\n\n        ind_p1 = builder.add(ind, context.get_constant(types.intp, 1))\n        cgutils.memcpy(builder, out_offsets, in_offsets, ind_p1)\n        cgutils.memcpy(\n            builder,\n            out_string_array.data,\n            in_string_array.data,\n            builder.load(\n                builder.gep(\n                    in_offsets,\n                    [ind])))\n        # n_bytes = (num_strings+sizeof(uint8_t)-1)/sizeof(uint8_t)\n        ind_p7 = builder.add(ind, lir.Constant(lir.IntType(64), 7))\n        n_bytes = builder.lshr(ind_p7, lir.Constant(lir.IntType(64), 3))\n        # assuming rest of last byte is set to all ones (e.g. from prealloc)\n        cgutils.memcpy(builder, out_string_array.null_bitmap, in_string_array.null_bitmap, n_bytes)\n        return context.get_dummy_value()\n\n    return types.void(string_array_type, string_array_type, ind_t), codegen\n\n\n@intrinsic\ndef copy_data(typingctx, str_arr_typ, out_str_arr_typ=None):\n    # precondition: output is allocated with data the same size as input\'s data\n    def codegen(context, builder, sig, args):\n        out_str_arr, in_str_arr = args\n\n        in_string_array = context.make_helper(builder, string_array_type, in_str_arr)\n        out_string_array = context.make_helper(builder, string_array_type, out_str_arr)\n\n        cgutils.memcpy(builder, out_string_array.data, in_string_array.data,\n                       in_string_array.num_total_chars)\n        return context.get_dummy_value()\n\n    return types.void(string_array_type, string_array_type), codegen\n\n\n@intrinsic\ndef copy_non_null_offsets(typingctx, str_arr_typ, out_str_arr_typ=None):\n    # precondition: output is allocated with offset the size non-nulls in input\n    def codegen(context, builder, sig, args):\n        out_str_arr, in_str_arr = args\n\n        in_string_array = context.make_helper(builder, string_array_type, in_str_arr)\n        out_string_array = context.make_helper(builder, string_array_type, out_str_arr)\n        n = in_string_array.num_items\n        zero = context.get_constant(offset_typ, 0)\n        curr_offset_ptr = cgutils.alloca_once_value(builder, zero)\n        # XXX: assuming last offset is already set by allocate_string_array\n\n        # for i in range(n)\n        #   if not isna():\n        #     out_offset[curr] = offset[i]\n        with cgutils.for_range(builder, n) as loop:\n            isna = lower_is_na(context, builder, in_string_array.null_bitmap, loop.index)\n            with cgutils.if_likely(builder, builder.not_(isna)):\n                in_val = builder.load(builder.gep(in_string_array.offsets, [loop.index]))\n                curr_offset = builder.load(curr_offset_ptr)\n                builder.store(in_val, builder.gep(out_string_array.offsets, [curr_offset]))\n                builder.store(\n                    builder.add(\n                        curr_offset,\n                        lir.Constant(\n                            context.get_data_type(offset_typ),\n                            1)),\n                    curr_offset_ptr)\n\n        # set last offset\n        curr_offset = builder.load(curr_offset_ptr)\n        in_val = builder.load(builder.gep(in_string_array.offsets, [n]))\n        builder.store(in_val, builder.gep(out_string_array.offsets, [curr_offset]))\n        return context.get_dummy_value()\n\n    return types.void(string_array_type, string_array_type), codegen\n\n\n@intrinsic\ndef str_copy(typingctx, buff_arr_typ, ind_typ, str_typ, len_typ=None):\n    def codegen(context, builder, sig, args):\n        buff_arr, ind, str, len_str = args\n        buff_arr = context.make_array(sig.args[0])(context, builder, buff_arr)\n        ptr = builder.gep(buff_arr.data, [ind])\n        cgutils.raw_memcpy(builder, ptr, str, len_str, 1)\n        return context.get_dummy_value()\n\n    return types.void(types.Array(types.uint8, 1, \'C\'), types.intp, types.voidptr, types.intp), codegen\n\n\n@intrinsic\ndef str_copy_ptr(typingctx, ptr_typ, ind_typ, str_typ, len_typ=None):\n    def codegen(context, builder, sig, args):\n        ptr, ind, _str, len_str = args\n        ptr = builder.gep(ptr, [ind])\n        cgutils.raw_memcpy(builder, ptr, _str, len_str, 1)\n        return context.get_dummy_value()\n\n    return types.void(types.voidptr, types.intp, types.voidptr, types.intp), codegen\n\n\n# convert array to list of strings if it is StringArray\n# just return it otherwise\ndef to_string_list(arr):\n    return arr\n\n\n@overload(to_string_list)\ndef to_string_list_overload(data):\n    if is_str_arr_typ(data):\n        def to_string_impl(data):\n            n = len(data)\n            l_str = []\n            for i in range(n):\n                l_str.append(data[i])\n            return l_str\n        return to_string_impl\n\n    if isinstance(data, (types.Tuple, types.UniTuple)):\n        count = data.count\n\n        func_text = ""def f(data):\\n""\n        func_text += ""  return ({}{})\\n"".format(\',\'.join([""to_string_list(data[{}])"".format(\n            i) for i in range(count)]),\n            "","" if count == 1 else """")  # single value needs comma to become tuple\n\n        loc_vars = {}\n        exec(func_text, {\'to_string_list\': to_string_list}, loc_vars)\n        to_str_impl = loc_vars[\'f\']\n        return to_str_impl\n\n    return lambda data: data\n\n\ndef cp_str_list_to_array(str_arr, str_list):\n    return\n\n\n@overload(cp_str_list_to_array)\ndef cp_str_list_to_array_overload(str_arr, list_data):\n    if is_str_arr_typ(str_arr):\n        def cp_str_list_impl(str_arr, list_data):\n            n = len(list_data)\n            for i in range(n):\n                _str = list_data[i]\n                str_arr[i] = _str\n\n        return cp_str_list_impl\n\n    if isinstance(str_arr, (types.Tuple, types.UniTuple)):\n        count = str_arr.count\n\n        func_text = ""def f(str_arr, list_data):\\n""\n        for i in range(count):\n            func_text += ""  cp_str_list_to_array(str_arr[{}], list_data[{}])\\n"".format(i, i)\n        func_text += ""  return\\n""\n\n        loc_vars = {}\n        exec(func_text, {\'cp_str_list_to_array\': cp_str_list_to_array}, loc_vars)\n        cp_str_impl = loc_vars[\'f\']\n        return cp_str_impl\n\n    return lambda str_arr, list_data: None\n\n\ndef str_list_to_array(str_list):\n    return str_list\n\n\n@overload(str_list_to_array)\ndef str_list_to_array_overload(str_list):\n    if str_list == types.List(string_type):\n        def str_list_impl(str_list):\n            n = len(str_list)\n            n_char = 0\n            for i in range(n):\n                _str = str_list[i]\n                n_char += get_utf8_size(_str)\n            str_arr = pre_alloc_string_array(n, n_char)\n            for i in range(n):\n                _str = str_list[i]\n                str_arr[i] = _str\n            return str_arr\n\n        return str_list_impl\n\n    return lambda str_list: str_list\n\n\nif sdc.config.config_pipeline_hpat_default:\n    @infer_global(operator.getitem)\n    class GetItemStringArray(AbstractTemplate):\n        key = operator.getitem\n\n        def generic(self, args, kws):\n            assert not kws\n            [ary, idx] = args\n            if isinstance(ary, StringArrayType):\n                if isinstance(idx, types.SliceType):\n                    return signature(string_array_type, *args)\n                # elif isinstance(idx, types.Integer):\n                #     return signature(string_type, *args)\n\n                elif idx == types.Array(types.bool_, 1, \'C\'):\n                    return signature(string_array_type, *args)\n                elif idx == types.Array(types.intp, 1, \'C\'):\n                    return signature(string_array_type, *args)\nelse:\n    # use old-implementation in the new pipeline if idx is of types.SliceType type\n    @infer_global(operator.getitem)\n    class GetItemStringArray(AbstractTemplate):\n        key = operator.getitem\n\n        def generic(self, args, kws):\n            assert not kws\n            [ary, idx] = args\n            if isinstance(ary, StringArrayType):\n                if isinstance(idx, types.SliceType):\n                    return signature(string_array_type, *args)\n\n\n@infer_global(operator.setitem)\nclass SetItemStringArray(AbstractTemplate):\n    def generic(self, args, kws):\n        assert not kws\n        ary, idx, val = args\n        if (ary == string_array_type and isinstance(idx, types.Integer)\n                and val == string_type):\n            return signature(types.none, *args)\n\n\nif sdc.config.config_pipeline_hpat_default:\n    @infer\n    @infer_global(operator.eq)\n    @infer_global(operator.ne)\n    @infer_global(operator.ge)\n    @infer_global(operator.gt)\n    @infer_global(operator.le)\n    @infer_global(operator.lt)\n    class CmpOpEqStringArray(AbstractTemplate):\n        key = operator.eq\n\n        def generic(self, args, kws):\n            assert not kws\n            [va, vb] = args\n            # if one of the inputs is string array\n            if va == string_array_type or vb == string_array_type:\n                # inputs should be either string array or string\n                assert is_str_arr_typ(va) or va == string_type\n                assert is_str_arr_typ(vb) or vb == string_type\n                return signature(types.Array(types.boolean, 1, \'C\'), va, vb)\n\n    @infer\n    class CmpOpNEqStringArray(CmpOpEqStringArray):\n        key = \'!=\'\n\n    @infer\n    class CmpOpGEStringArray(CmpOpEqStringArray):\n        key = \'>=\'\n\n    @infer\n    class CmpOpGTStringArray(CmpOpEqStringArray):\n        key = \'>\'\n\n    @infer\n    class CmpOpLEStringArray(CmpOpEqStringArray):\n        key = \'<=\'\n\n    @infer\n    class CmpOpLTStringArray(CmpOpEqStringArray):\n        key = \'<\'\n\n\n# def is_str_arr_typ(typ):\n#     from sdc.hiframes.pd_series_ext import is_str_series_typ\n#     return typ == string_array_type or is_str_series_typ(typ)\n\n# @infer_global(len)\n# class LenStringArray(AbstractTemplate):\n#     def generic(self, args, kws):\n#         if not kws and len(args)==1 and args[0]==string_array_type:\n#             return signature(types.intp, *args)\n\n# XXX: should these be exposed?\n# make_attribute_wrapper(StringArrayType, \'num_items\', \'num_items\')\n# make_attribute_wrapper(StringArrayType, \'num_total_chars\', \'num_total_chars\')\n# make_attribute_wrapper(StringArrayType, \'offsets\', \'offsets\')\n# make_attribute_wrapper(StringArrayType, \'data\', \'data\')\n\n# make_attribute_wrapper(StringArrayPayloadType, \'offsets\', \'offsets\')\n# make_attribute_wrapper(StringArrayPayloadType, \'data\', \'data\')\n\n# XXX can\'t use this with overload_method\n@infer_getattr\nclass StrArrayAttribute(AttributeTemplate):\n    key = StringArrayType\n\n    def resolve_size(self, ctflags):\n        return types.intp\n\n    @bound_function(""str_arr.copy"")\n    def resolve_copy(self, ary, args, kws):\n        return signature(string_array_type, *args)\n\n\n@lower_builtin(""str_arr.copy"", string_array_type)\ndef str_arr_copy_impl(context, builder, sig, args):\n    return context.compile_internal(builder, copy_impl, sig, args)\n\n\ndef copy_impl(arr):\n    n = len(arr)\n    n_chars = num_total_chars(arr)\n    new_arr = pre_alloc_string_array(n, np.int64(n_chars))\n    copy_str_arr_slice(new_arr, arr, n)\n    return new_arr\n\n# @overload_method(StringArrayType, \'copy\')\n# def string_array_copy(arr_t):\n#     return copy_impl\n\n\n# @overload_attribute(string_array_type, \'size\')\n# def string_array_attr_size(arr_t):\n#     return get_str_arr_size\n\n# def get_str_arr_size(arr):  # pragma: no cover\n#     return len(arr)\n\n# @infer_global(get_str_arr_size)\n# class StrArrSizeInfer(AbstractTemplate):\n#     def generic(self, args, kws):\n#         assert not kws\n#         assert len(args) == 1 and args[0] == string_array_type\n#         return signature(types.intp, *args)\n\n# @lower_builtin(get_str_arr_size, string_array_type)\n# def str_arr_size_impl(context, builder, sig, args):\n\n@lower_getattr(string_array_type, \'size\')\ndef str_arr_size_impl(context, builder, typ, val):\n    string_array = context.make_helper(builder, string_array_type, val)\n\n    attrval = string_array.num_items\n    attrty = types.intp\n    return impl_ret_borrowed(context, builder, attrty, attrval)\n\n# @lower_builtin(StringArray, types.Type, types.Type)\n# def impl_string_array(context, builder, sig, args):\n#     typ = sig.return_type\n#     offsets, data = args\n#     string_array = cgutils.create_struct_proxy(typ)(context, builder)\n#     string_array.offsets = offsets\n#     string_array.data = data\n#     return string_array._getvalue()\n\n\n@overload(len)\ndef str_arr_len_overload(str_arr):\n    if is_str_arr_typ(str_arr):\n        def str_arr_len(str_arr):\n            return str_arr.size\n        return str_arr_len\n\n\nll.add_symbol(\'get_str_len\', hstr_ext.get_str_len)\nll.add_symbol(\'allocate_string_array\', hstr_ext.allocate_string_array)\nll.add_symbol(\'setitem_string_array\', hstr_ext.setitem_string_array)\nll.add_symbol(\'getitem_string_array\', hstr_ext.getitem_string_array)\nll.add_symbol(\'getitem_string_array_std\', hstr_ext.getitem_string_array_std)\nll.add_symbol(\'is_na\', hstr_ext.is_na)\nll.add_symbol(\'string_array_from_sequence\', hstr_ext.string_array_from_sequence)\nll.add_symbol(\'np_array_from_string_array\', hstr_ext.np_array_from_string_array)\nll.add_symbol(\'print_int\', hstr_ext.print_int)\nll.add_symbol(\'convert_len_arr_to_offset\', hstr_ext.convert_len_arr_to_offset)\nll.add_symbol(\'set_string_array_range\', hstr_ext.set_string_array_range)\nll.add_symbol(\'str_arr_to_int64\', hstr_ext.str_arr_to_int64)\nll.add_symbol(\'str_arr_to_float64\', hstr_ext.str_arr_to_float64)\nll.add_symbol(\'dtor_string_array\', hstr_ext.dtor_string_array)\nll.add_symbol(\'c_glob\', hstr_ext.c_glob)\nll.add_symbol(\'decode_utf8\', hstr_ext.decode_utf8)\nll.add_symbol(\'get_utf8_size\', hstr_ext.get_utf8_size)\n\nconvert_len_arr_to_offset = types.ExternalFunction(""convert_len_arr_to_offset"", types.void(types.voidptr, types.intp))\n\n\nsetitem_string_array = types.ExternalFunction(""setitem_string_array"",\n                                              types.void(types.voidptr, types.voidptr, types.intp, string_type,\n                                                         types.intp))\n_get_utf8_size = types.ExternalFunction(""get_utf8_size"",\n                                        types.intp(types.voidptr, types.intp, types.int32))\n\n\ndef construct_string_array(context, builder):\n    """"""Creates meminfo and sets dtor.\n    """"""\n    alloc_type = context.get_data_type(str_arr_payload_type)\n    alloc_size = context.get_abi_sizeof(alloc_type)\n\n    llvoidptr = context.get_value_type(types.voidptr)\n    llsize = context.get_value_type(types.uintp)\n    dtor_ftype = lir.FunctionType(lir.VoidType(),\n                                  [llvoidptr, llsize, llvoidptr])\n    dtor_fn = builder.module.get_or_insert_function(\n        dtor_ftype, name=""dtor_string_array"")\n\n    meminfo = context.nrt.meminfo_alloc_dtor(\n        builder,\n        context.get_constant(types.uintp, alloc_size),\n        dtor_fn,\n    )\n    meminfo_data_ptr = context.nrt.meminfo_data(builder, meminfo)\n    meminfo_data_ptr = builder.bitcast(meminfo_data_ptr,\n                                       alloc_type.as_pointer())\n\n    # Nullify all data\n    # builder.store( cgutils.get_null_value(alloc_type),\n    #             meminfo_data_ptr)\n    return meminfo, meminfo_data_ptr\n\n\n# TODO: overload of constructor doesn\'t work\n# @overload(StringArray)\n# def string_array_const(in_list=None):\n#     if in_list is None:\n#         return lambda: pre_alloc_string_array(0, 0)\n\n#     def str_arr_from_list(in_list):\n#         n_strs = len(in_list)\n#         total_chars = 0\n#         # TODO: use vector to avoid two passes?\n#         # get total number of chars\n#         for s in in_list:\n#             total_chars += len(s)\n\n#         A = pre_alloc_string_array(n_strs, total_chars)\n#         for i in range(n_strs):\n#             A[i] = in_list[i]\n\n#         return A\n\n#     return str_arr_from_list\n\n\n# used in pd.DataFrame() and pd.Series() to convert list of strings\n@lower_builtin(StringArray)\n@lower_builtin(StringArray, types.List)\n@lower_builtin(StringArray, types.UniTuple)\n@lower_builtin(StringArray, types.Tuple)\ndef impl_string_array_single(context, builder, sig, args):\n\n    arg = args[0]\n    if isinstance(arg, (types.UniTuple, types.List)):\n        assert (arg.dtype == string_type\n                or (isinstance(arg.dtype, types.Optional) and arg.dtype.type == string_type))\n\n    # FIXME: doesn\'t work for Tuple with None values\n    if isinstance(arg, types.Tuple):\n        for i in arg:\n            assert i.dtype == string_type or i.dtype == types.StringLiteral\n\n    if not sig.args:  # return empty string array if no args\n        res = context.compile_internal(\n            builder, lambda: pre_alloc_string_array(0, 0), sig, args)\n        return res\n\n    def str_arr_from_sequence(in_list):\n        n_strs = len(in_list)\n        total_chars = 0\n        # TODO: use vector to avoid two passes?\n        # get total number of chars\n        nan_mask = np.zeros(n_strs, dtype=np.bool_)\n        for i in numba.prange(n_strs):\n            s = in_list[i]\n            if s is None:\n                nan_mask[i] = True\n            else:\n                total_chars += get_utf8_size(s)\n\n        A = pre_alloc_string_array(n_strs, total_chars)\n        for i in np.arange(n_strs):\n            A[i] = \'\' if nan_mask[i] else in_list[i]\n        str_arr_set_na_by_mask(A, nan_mask)\n\n        return A\n\n    res = context.compile_internal(builder, str_arr_from_sequence, sig, args)\n    return res\n\n# @lower_builtin(StringArray)\n# @lower_builtin(StringArray, types.List)\n# def impl_string_array_single(context, builder, sig, args):\n#     typ = sig.return_type\n#     zero = context.get_constant(types.intp, 0)\n#     meminfo, meminfo_data_ptr = construct_string_array(context, builder)\n\n#     str_arr_payload = cgutils.create_struct_proxy(str_arr_payload_type)(context, builder)\n#     if not sig.args:  # return empty string array if no args\n#         # XXX alloc empty arrays for dtor to safely delete?\n#         builder.store(str_arr_payload._getvalue(), meminfo_data_ptr)\n#         string_array = context.make_helper(builder, typ)\n#         string_array.meminfo = meminfo\n#         string_array.num_items = zero\n#         string_array.num_total_chars = zero\n#         ret = string_array._getvalue()\n#         #context.nrt.decref(builder, ty, ret)\n\n#         return impl_ret_new_ref(context, builder, typ, ret)\n\n#     string_list = ListInstance(context, builder, sig.args[0], args[0])\n\n#     # get total size of string buffer\n#     fnty = lir.FunctionType(lir.IntType(64),\n#                             [lir.IntType(8).as_pointer()])\n#     fn_len = builder.module.get_or_insert_function(fnty, name=""get_str_len"")\n#     total_size = cgutils.alloca_once_value(builder, zero)\n\n#     # loop through all strings and get length\n#     with cgutils.for_range(builder, string_list.size) as loop:\n#         str_value = string_list.getitem(loop.index)\n#         str_len = builder.call(fn_len, [str_value])\n#         builder.store(builder.add(builder.load(total_size), str_len), total_size)\n\n#     # allocate string array\n#     fnty = lir.FunctionType(lir.VoidType(),\n#                             [lir.IntType(32).as_pointer().as_pointer(),\n#                              lir.IntType(8).as_pointer().as_pointer(),\n#                              lir.IntType(8).as_pointer().as_pointer(),\n#                              lir.IntType(64),\n#                              lir.IntType(64)])\n#     fn_alloc = builder.module.get_or_insert_function(fnty,\n#                                                      name=""allocate_string_array"")\n#     builder.call(fn_alloc, [str_arr_payload._get_ptr_by_name(\'offsets\'),\n#                             str_arr_payload._get_ptr_by_name(\'data\'),\n#                             str_arr_payload._get_ptr_by_name(\'null_bitmap\'),\n#                             string_list.size, builder.load(total_size)])\n\n#     # set string array values\n#     fnty = lir.FunctionType(lir.VoidType(),\n#                             [lir.IntType(32).as_pointer(),\n#                              lir.IntType(8).as_pointer(),\n#                              lir.IntType(8).as_pointer(),\n#                              lir.IntType(64)])\n#     fn_setitem = builder.module.get_or_insert_function(fnty,\n#                                                        name=""setitem_string_array"")\n\n#     with cgutils.for_range(builder, string_list.size) as loop:\n#         str_value = string_list.getitem(loop.index)\n#         builder.call(fn_setitem, [str_arr_payload.offsets, str_arr_payload.data,\n#                                   str_value, loop.index])\n\n#     builder.store(str_arr_payload._getvalue(), meminfo_data_ptr)\n\n#     string_array = context.make_helper(builder, typ)\n#     string_array.num_items = string_list.size\n#     string_array.num_total_chars = builder.load(total_size)\n#     #cgutils.printf(builder, ""str %d %d\\n"", string_array.num_items, string_array.num_total_chars)\n#     string_array.offsets = str_arr_payload.offsets\n#     string_array.data = str_arr_payload.data\n#     string_array.null_bitmap = str_arr_payload.null_bitmap\n#     string_array.meminfo = meminfo\n#     ret = string_array._getvalue()\n#     #context.nrt.decref(builder, ty, ret)\n\n#     return impl_ret_new_ref(context, builder, typ, ret)\n\n\n@intrinsic\ndef pre_alloc_string_array(typingctx, num_strs_typ, num_total_chars_typ=None):\n    assert isinstance(num_strs_typ, types.Integer) and isinstance(num_total_chars_typ, types.Integer)\n\n    def codegen(context, builder, sig, args):\n        num_strs, num_total_chars = args\n        meminfo, meminfo_data_ptr = construct_string_array(context, builder)\n\n        str_arr_payload = cgutils.create_struct_proxy(str_arr_payload_type)(context, builder)\n\n        # allocate string array\n        fnty = lir.FunctionType(lir.VoidType(),\n                                [lir.IntType(32).as_pointer().as_pointer(),\n                                 lir.IntType(8).as_pointer().as_pointer(),\n                                 lir.IntType(8).as_pointer().as_pointer(),\n                                 lir.IntType(64),\n                                 lir.IntType(64)])\n        fn_alloc = builder.module.get_or_insert_function(fnty,\n                                                         name=""allocate_string_array"")\n        builder.call(fn_alloc, [str_arr_payload._get_ptr_by_name(\'offsets\'),\n                                str_arr_payload._get_ptr_by_name(\'data\'),\n                                str_arr_payload._get_ptr_by_name(\'null_bitmap\'),\n                                num_strs,\n                                num_total_chars])\n\n        builder.store(str_arr_payload._getvalue(), meminfo_data_ptr)\n        string_array = context.make_helper(builder, string_array_type)\n        string_array.num_items = num_strs\n        string_array.num_total_chars = num_total_chars\n        string_array.offsets = str_arr_payload.offsets\n        string_array.data = str_arr_payload.data\n        string_array.null_bitmap = str_arr_payload.null_bitmap\n        string_array.meminfo = meminfo\n        ret = string_array._getvalue()\n        # context.nrt.decref(builder, ty, ret)\n\n        return impl_ret_new_ref(context, builder, string_array_type, ret)\n\n    return string_array_type(types.intp, types.intp), codegen\n\n\n@intrinsic\ndef set_string_array_range(typingctx, out_typ, in_typ, curr_str_typ, curr_chars_typ=None):\n    assert is_str_arr_typ(out_typ) and is_str_arr_typ(in_typ)\n    assert curr_str_typ == types.intp and curr_chars_typ == types.intp\n\n    def codegen(context, builder, sig, args):\n        out_arr, in_arr, curr_str_ind, curr_chars_ind = args\n\n        # get input/output struct\n        out_string_array = context.make_helper(builder, string_array_type, out_arr)\n        in_string_array = context.make_helper(builder, string_array_type, in_arr)\n\n        fnty = lir.FunctionType(lir.VoidType(),\n                                [lir.IntType(32).as_pointer(),\n                                 lir.IntType(8).as_pointer(),\n                                 lir.IntType(32).as_pointer(),\n                                 lir.IntType(8).as_pointer(),\n                                 lir.IntType(64),\n                                 lir.IntType(64),\n                                 lir.IntType(64),\n                                 lir.IntType(64), ])\n        fn_alloc = builder.module.get_or_insert_function(fnty,\n                                                         name=""set_string_array_range"")\n        builder.call(fn_alloc, [out_string_array.offsets,\n                                out_string_array.data,\n                                in_string_array.offsets,\n                                in_string_array.data,\n                                curr_str_ind,\n                                curr_chars_ind,\n                                in_string_array.num_items,\n                                in_string_array.num_total_chars,\n                                ])\n\n        return context.get_dummy_value()\n\n    return types.void(string_array_type, string_array_type, types.intp, types.intp), codegen\n\n# box series calls this too\n@box(StringArrayType)\ndef box_str_arr(typ, val, c):\n    """"""\n    """"""\n\n    string_array = c.context.make_helper(c.builder, string_array_type, val)\n\n    fnty = lir.FunctionType(c.context.get_argument_type(types.pyobject),  # lir.IntType(8).as_pointer(),\n                            [lir.IntType(64),\n                             lir.IntType(32).as_pointer(),\n                             lir.IntType(8).as_pointer(),\n                             lir.IntType(8).as_pointer(),\n                             ])\n    fn_get = c.builder.module.get_or_insert_function(fnty, name=""np_array_from_string_array"")\n    arr = c.builder.call(fn_get, [string_array.num_items, string_array.offsets,\n                                  string_array.data, string_array.null_bitmap])\n\n    # TODO: double check refcounting here\n    # c.context.nrt.decref(c.builder, typ, val)\n    return arr  # c.builder.load(arr)\n\n\n@intrinsic\ndef str_arr_is_na(typingctx, str_arr_typ, ind_typ=None):\n    # None default to make IntelliSense happy\n    assert is_str_arr_typ(str_arr_typ)\n\n    def codegen(context, builder, sig, args):\n        in_str_arr, ind = args\n        string_array = context.make_helper(builder, string_array_type, in_str_arr)\n\n        # (null_bitmap[i / 8] & kBitmask[i % 8]) == 0;\n        byte_ind = builder.lshr(ind, lir.Constant(lir.IntType(64), 3))\n        bit_ind = builder.urem(ind, lir.Constant(lir.IntType(64), 8))\n        byte = builder.load(builder.gep(string_array.null_bitmap, [byte_ind], inbounds=True))\n        ll_typ_mask = lir.ArrayType(lir.IntType(8), 8)\n        mask_tup = cgutils.alloca_once_value(builder, lir.Constant(ll_typ_mask, (1, 2, 4, 8, 16, 32, 64, 128)))\n        mask = builder.load(builder.gep(mask_tup, [lir.Constant(lir.IntType(64), 0), bit_ind], inbounds=True))\n        return builder.icmp_unsigned(\'==\', builder.and_(byte, mask), lir.Constant(lir.IntType(8), 0))\n\n    return types.bool_(string_array_type, types.intp), codegen\n\n\n@intrinsic\ndef str_arr_set_na(typingctx, str_arr_typ, ind_typ=None):\n    # None default to make IntelliSense happy\n    assert is_str_arr_typ(str_arr_typ)\n\n    def codegen(context, builder, sig, args):\n        in_str_arr, ind = args\n        string_array = context.make_helper(builder, string_array_type, in_str_arr)\n\n        # bits[i / 8] |= kBitmask[i % 8];\n        byte_ind = builder.lshr(ind, lir.Constant(lir.IntType(64), 3))\n        bit_ind = builder.urem(ind, lir.Constant(lir.IntType(64), 8))\n        byte_ptr = builder.gep(string_array.null_bitmap, [byte_ind], inbounds=True)\n        byte = builder.load(byte_ptr)\n        ll_typ_mask = lir.ArrayType(lir.IntType(8), 8)\n        mask_tup = cgutils.alloca_once_value(builder, lir.Constant(ll_typ_mask, (1, 2, 4, 8, 16, 32, 64, 128)))\n        mask = builder.load(builder.gep(mask_tup, [lir.Constant(lir.IntType(64), 0), bit_ind], inbounds=True))\n        # flip all bits of mask e.g. 11111101\n        mask = builder.xor(mask, lir.Constant(lir.IntType(8), -1))\n        # unset masked bit\n        builder.store(builder.and_(byte, mask), byte_ptr)\n        return context.get_dummy_value()\n\n    return types.void(string_array_type, types.intp), codegen\n\n\n@intrinsic\ndef set_null_bits(typingctx, str_arr_typ=None):\n    assert is_str_arr_typ(str_arr_typ)\n\n    def codegen(context, builder, sig, args):\n        in_str_arr, = args\n        string_array = context.make_helper(builder, string_array_type, in_str_arr)\n        # n_bytes = (num_strings+sizeof(uint8_t)-1)/sizeof(uint8_t);\n        n_bytes = builder.udiv(\n            builder.add(\n                string_array.num_items, lir.Constant(\n                    lir.IntType(64), 7)), lir.Constant(\n                lir.IntType(64), 8))\n        cgutils.memset(builder, string_array.null_bitmap, n_bytes, -1)\n        return context.get_dummy_value()\n\n    return types.none(string_array_type), codegen\n\n# XXX: setitem works only if value is same size as the previous value\n@lower_builtin(operator.setitem, StringArrayType, types.Integer, string_type)\ndef setitem_str_arr(context, builder, sig, args):\n    arr, ind, val = args\n    uni_str = cgutils.create_struct_proxy(string_type)(\n        context, builder, value=val)\n    string_array = context.make_helper(builder, string_array_type, arr)\n    fnty = lir.FunctionType(lir.VoidType(),\n                            [lir.IntType(32).as_pointer(),\n                             lir.IntType(8).as_pointer(),\n                             lir.IntType(64),\n                             lir.IntType(8).as_pointer(),\n                             lir.IntType(64),\n                             lir.IntType(32),\n                             lir.IntType(32),\n                             lir.IntType(64)])\n    fn_setitem = builder.module.get_or_insert_function(\n        fnty, name=""setitem_string_array"")\n    builder.call(fn_setitem, [string_array.offsets, string_array.data,\n                              string_array.num_total_chars,\n                              uni_str.data, uni_str.length, uni_str.kind,\n                              uni_str.is_ascii, ind])\n    return context.get_dummy_value()\n\n\n@numba.njit(no_cpython_wrapper=True)\ndef get_utf8_size(s):\n    if s._is_ascii == 1:\n        return len(s)\n    return _get_utf8_size(s._data, s._length, s._kind)\n\n\n@intrinsic\ndef setitem_str_arr_ptr(typingctx, str_arr_t, ind_t, ptr_t, len_t=None):\n    def codegen(context, builder, sig, args):\n        arr, ind, ptr, length = args\n        string_array = context.make_helper(builder, string_array_type, arr)\n        fnty = lir.FunctionType(lir.VoidType(),\n                                [lir.IntType(32).as_pointer(),\n                                 lir.IntType(8).as_pointer(),\n                                 lir.IntType(64),\n                                 lir.IntType(8).as_pointer(),\n                                 lir.IntType(64),\n                                 lir.IntType(32),\n                                 lir.IntType(32),\n                                 lir.IntType(64)])\n        fn_setitem = builder.module.get_or_insert_function(\n            fnty, name=""setitem_string_array"")\n        # kind doesn\'t matter since input is ASCII\n        kind = context.get_constant(types.int32, -1)\n        is_ascii = context.get_constant(types.int32, 1)\n        builder.call(fn_setitem, [string_array.offsets, string_array.data,\n                                  string_array.num_total_chars,\n                                  builder.extract_value(ptr, 0), length, kind, is_ascii, ind\n                                  ])\n        return context.get_dummy_value()\n\n    return types.void(str_arr_t, ind_t, ptr_t, len_t), codegen\n\n\ndef lower_is_na(context, builder, bull_bitmap, ind):\n    fnty = lir.FunctionType(lir.IntType(1),\n                            [lir.IntType(8).as_pointer(),\n                             lir.IntType(64)])\n    fn_getitem = builder.module.get_or_insert_function(fnty,\n                                                       name=""is_na"")\n    return builder.call(fn_getitem, [bull_bitmap,\n                                     ind])\n\n\n@intrinsic\ndef _memcpy(typingctx, dest_t, src_t, count_t, item_size_t=None):\n    def codegen(context, builder, sig, args):\n        dst, src, count, itemsize = args\n        # buff_arr = context.make_array(sig.args[0])(context, builder, buff_arr)\n        # ptr = builder.gep(buff_arr.data, [ind])\n        cgutils.raw_memcpy(builder, dst, src, count, itemsize)\n        return context.get_dummy_value()\n\n    return types.void(types.voidptr, types.voidptr, types.intp, types.intp), codegen\n\n\n# TODO: use overload for all getitem cases (currently implemented via lower_builtin)\n@overload(operator.getitem)\ndef str_arr_getitem_int(A, arg):\n\n    if (A != string_array_type):\n        return None\n\n    if isinstance(arg, types.Integer):\n        def str_arr_getitem_by_integer_impl(A, arg):\n            if arg < 0 or arg >= len(A):\n                raise IndexError(""StringArray getitem with index out of bounds"")\n\n            start_offset = getitem_str_offset(A, arg)\n            end_offset = getitem_str_offset(A, arg + 1)\n            length = end_offset - start_offset\n            ptr = get_data_ptr_ind(A, start_offset)\n            ret = decode_utf8(ptr, length)\n            # ret = numba.cpython.unicode._empty_string(kind, length)\n            # _memcpy(ret._data, ptr, length, 1)\n            return ret\n\n        return str_arr_getitem_by_integer_impl\n    elif (isinstance(arg, types.Array) and isinstance(arg.dtype, (types.Boolean, types.Integer))):\n        def str_arr_getitem_by_array_impl(A, arg):\n\n            if len(A) != len(arg):\n                raise IndexError(""Mismatch of boolean index and indexed array sizes"")\n\n            idxs = np.arange(len(A))\n            taken_idxs = idxs[arg]\n\n            result_size = len(taken_idxs)\n            total_chars = 0\n            for i in prange(result_size):\n                total_chars += len(A[taken_idxs[i]])\n\n            ret = pre_alloc_string_array(result_size, total_chars)\n            for i in prange(result_size):\n                ret[i] = A[taken_idxs[i]]\n                if str_arr_is_na(A, taken_idxs[i]):\n                    str_arr_set_na(ret, i)\n\n            return ret\n\n        return str_arr_getitem_by_array_impl\n\n    return None\n\n\n@intrinsic\ndef decode_utf8(typingctx, ptr_t, len_t=None):\n    def codegen(context, builder, sig, args):\n        ptr, length = args\n\n        # create str and call decode with internal pointers\n        uni_str = cgutils.create_struct_proxy(string_type)(context, builder)\n        fnty = lir.FunctionType(lir.VoidType(), [lir.IntType(8).as_pointer(),\n                                                 lir.IntType(64),\n                                                 lir.IntType(32).as_pointer(),\n                                                 lir.IntType(32).as_pointer(),\n                                                 lir.IntType(64).as_pointer(),\n                                                 uni_str.meminfo.type.as_pointer()])\n        fn_decode = builder.module.get_or_insert_function(\n            fnty, name=""decode_utf8"")\n        builder.call(fn_decode, [ptr, length,\n                                 uni_str._get_ptr_by_name(\'kind\'),\n                                 uni_str._get_ptr_by_name(\'is_ascii\'),\n                                 uni_str._get_ptr_by_name(\'length\'),\n                                 uni_str._get_ptr_by_name(\'meminfo\')])\n        uni_str.hash = context.get_constant(_Py_hash_t, -1)\n        uni_str.data = context.nrt.meminfo_data(builder, uni_str.meminfo)\n        # Set parent to NULL\n        uni_str.parent = cgutils.get_null_value(uni_str.parent.type)\n        return uni_str._getvalue()\n\n    return string_type(types.voidptr, types.intp), codegen\n\n\n# @lower_builtin(operator.getitem, StringArrayType, types.Integer)\n# @lower_builtin(operator.getitem, StringArrayType, types.IntegerLiteral)\n# def lower_string_arr_getitem(context, builder, sig, args):\n#     # TODO: support multibyte unicode\n#     # TODO: support Null\n#     kind = numba.cpython.unicode.PY_UNICODE_1BYTE_KIND\n#     def str_arr_getitem_impl(A, i):\n#         start_offset = getitem_str_offset(A, i)\n#         end_offset = getitem_str_offset(A, i + 1)\n#         length = end_offset - start_offset\n#         ret = numba.cpython.unicode._empty_string(kind, length)\n#         ptr = get_data_ptr_ind(A, start_offset)\n#         _memcpy(ret._data, ptr, length, 1)\n#         return ret\n\n#     res = context.compile_internal(builder, str_arr_getitem_impl, sig, args)\n#     return res\n\n    # typ = sig.args[0]\n    # ind = args[1]\n\n    # string_array = context.make_helper(builder, typ, args[0])\n\n    # # check for NA\n    # # i/8, XXX: lshr since always positive\n    # #byte_ind = builder.lshr(ind, lir.Constant(lir.IntType(64), 3))\n    # #bit_ind = builder.srem\n\n    # # cgutils.printf(builder, ""calling bitmap\\n"")\n    # # with cgutils.if_unlikely(builder, lower_is_na(context, builder, string_array.null_bitmap, ind)):\n    # #     cgutils.printf(builder, ""is_na %d \\n"", ind)\n    # # cgutils.printf(builder, ""calling bitmap done\\n"")\n\n    # fnty = lir.FunctionType(lir.IntType(8).as_pointer(),\n    #                         [lir.IntType(32).as_pointer(),\n    #                          lir.IntType(8).as_pointer(),\n    #                          lir.IntType(64)])\n    # fn_getitem = builder.module.get_or_insert_function(fnty,\n    #                                                    name=""getitem_string_array_std"")\n    # return builder.call(fn_getitem, [string_array.offsets,\n    #                                  string_array.data, args[1]])\n\n\nif sdc.config.config_pipeline_hpat_default:\n    # FIXME: old-style getitem implementations copy strings but not null bits\n    @lower_builtin(operator.getitem, StringArrayType, types.Array(types.bool_, 1, \'C\'))\n    def lower_string_arr_getitem_bool(context, builder, sig, args):\n        def str_arr_bool_impl(str_arr, bool_arr):\n            n = len(str_arr)\n            if n != len(bool_arr):\n                raise IndexError(""boolean index did not match indexed array along dimension 0"")\n            n_strs = 0\n            n_chars = 0\n            for i in range(n):\n                if bool_arr[i]:\n                    # TODO: use get_cstr_and_len instead of getitem\n                    _str = str_arr[i]\n                    n_strs += 1\n                    n_chars += get_utf8_size(_str)\n            out_arr = pre_alloc_string_array(n_strs, n_chars)\n            str_ind = 0\n            for i in range(n):\n                if bool_arr[i]:\n                    _str = str_arr[i]\n                    out_arr[str_ind] = _str\n                    str_ind += 1\n            return out_arr\n        res = context.compile_internal(builder, str_arr_bool_impl, sig, args)\n        return res\n\n    @lower_builtin(operator.getitem, StringArrayType, types.Array(types.intp, 1, \'C\'))\n    def lower_string_arr_getitem_arr(context, builder, sig, args):\n        def str_arr_arr_impl(str_arr, ind_arr):\n            n = len(ind_arr)\n            # get lengths\n            n_strs = 0\n            n_chars = 0\n            for i in range(n):\n                # TODO: use get_cstr_and_len instead of getitem\n                _str = str_arr[ind_arr[i]]\n                n_strs += 1\n                n_chars += get_utf8_size(_str)\n\n            out_arr = pre_alloc_string_array(n_strs, n_chars)\n            str_ind = 0\n            for i in range(n):\n                _str = str_arr[ind_arr[i]]\n                out_arr[str_ind] = _str\n                str_ind += 1\n            return out_arr\n        res = context.compile_internal(builder, str_arr_arr_impl, sig, args)\n        return res\n\n\n@lower_builtin(operator.getitem, StringArrayType, types.SliceType)\ndef lower_string_arr_getitem_slice(context, builder, sig, args):\n    def str_arr_slice_impl(str_arr, idx):\n        n = len(str_arr)\n        slice_idx = numba.cpython.unicode._normalize_slice(idx, n)\n        span = numba.cpython.unicode._slice_span(slice_idx)\n\n        if slice_idx.step == 1:\n            start_offset = getitem_str_offset(str_arr, slice_idx.start)\n            end_offset = getitem_str_offset(str_arr, slice_idx.stop)\n            n_chars = end_offset - start_offset\n            new_arr = pre_alloc_string_array(span, np.int64(n_chars))\n            # TODO: more efficient copy\n            for i in range(span):\n                new_arr[i] = str_arr[slice_idx.start + i]\n            return new_arr\n        else:  # TODO: test\n            # get number of chars\n            n_chars = 0\n            for i in range(slice_idx.start, slice_idx.stop, slice_idx.step):\n                _str = str_arr[i]\n                n_chars += get_utf8_size(_str)\n            new_arr = pre_alloc_string_array(span, np.int64(n_chars))\n            # TODO: more efficient copy\n            for i in range(span):\n                new_arr[i] = str_arr[slice_idx.start + i * slice_idx.step]\n            return new_arr\n\n    res = context.compile_internal(builder, str_arr_slice_impl, sig, args)\n    return res\n\n\n@numba.njit(no_cpython_wrapper=True)\ndef str_arr_item_to_numeric(out_arr, out_ind, str_arr, ind):\n    return _str_arr_item_to_numeric(sdc.hiframes.split_impl.get_c_arr_ptr(\n        out_arr.ctypes, out_ind), str_arr, ind, out_arr.dtype)\n\n\n@intrinsic\ndef _str_arr_item_to_numeric(typingctx, out_ptr_t, str_arr_t, ind_t,\n                             out_dtype_t=None):\n    assert str_arr_t == string_array_type\n    assert ind_t == types.int64\n\n    def codegen(context, builder, sig, args):\n        # TODO: return tuple with value and error and avoid array arg?\n        out_ptr, arr, ind, _dtype = args\n        string_array = context.make_helper(builder, string_array_type, arr)\n        fnty = lir.FunctionType(\n            lir.IntType(32),\n            [out_ptr.type,\n             lir.IntType(32).as_pointer(),\n             lir.IntType(8).as_pointer(),\n             lir.IntType(64)])\n        fname = \'str_arr_to_int64\'\n        if sig.args[3].dtype == types.float64:\n            fname = \'str_arr_to_float64\'\n        else:\n            assert sig.args[3].dtype == types.int64\n        fn_to_numeric = builder.module.get_or_insert_function(fnty, fname)\n        return builder.call(\n            fn_to_numeric,\n            [out_ptr, string_array.offsets, string_array.data, ind])\n\n    return types.int32(\n        out_ptr_t, string_array_type, types.int64, out_dtype_t), codegen\n\n\n# TODO: support array of strings\n# @typeof_impl.register(np.ndarray)\n# def typeof_np_string(val, c):\n#     arr_typ = numba.core.typing.typeof._typeof_ndarray(val, c)\n#     # match string dtype\n#     if isinstance(arr_typ.dtype, (types.UnicodeCharSeq, types.CharSeq)):\n#         return string_array_type\n#     return arr_typ\n\n\n@unbox(StringArrayType)\ndef unbox_str_series(typ, val, c):\n    """"""\n    Unbox a Pandas String Series. We just redirect to StringArray implementation.\n    """"""\n    dtype = StringArrayPayloadType()\n    payload = cgutils.create_struct_proxy(dtype)(c.context, c.builder)\n    string_array = c.context.make_helper(c.builder, typ)\n\n    # function signature of string_array_from_sequence\n    # we use void* instead of PyObject*\n    fnty = lir.FunctionType(lir.VoidType(),\n                            [lir.IntType(8).as_pointer(),\n                             lir.IntType(64).as_pointer(),\n                             lir.IntType(32).as_pointer().as_pointer(),\n                             lir.IntType(8).as_pointer().as_pointer(),\n                             lir.IntType(8).as_pointer().as_pointer(),\n                             ])\n    fn = c.builder.module.get_or_insert_function(fnty, name=""string_array_from_sequence"")\n    c.builder.call(fn, [val,\n                        string_array._get_ptr_by_name(\'num_items\'),\n                        payload._get_ptr_by_name(\'offsets\'),\n                        payload._get_ptr_by_name(\'data\'),\n                        payload._get_ptr_by_name(\'null_bitmap\'),\n                        ])\n\n    # the raw data is now copied to payload\n    # The native representation is a proxy to the payload, we need to\n    # get a proxy and attach the payload and meminfo\n    meminfo, meminfo_data_ptr = construct_string_array(c.context, c.builder)\n    c.builder.store(payload._getvalue(), meminfo_data_ptr)\n\n    string_array.meminfo = meminfo\n    string_array.offsets = payload.offsets\n    string_array.data = payload.data\n    string_array.null_bitmap = payload.null_bitmap\n    string_array.num_total_chars = c.builder.zext(c.builder.load(\n        c.builder.gep(string_array.offsets, [string_array.num_items])), lir.IntType(64))\n\n    # FIXME how to check that the returned size is > 0?\n    is_error = cgutils.is_not_null(c.builder, c.pyapi.err_occurred())\n    return NativeValue(string_array._getvalue(), is_error=is_error)\n\n# zero = context.get_constant(types.intp, 0)\n# cond = builder.icmp_signed(\'>=\', size, zero)\n# with cgutils.if_unlikely(builder, cond):\n# http://llvmlite.readthedocs.io/en/latest/user-guide/ir/ir-builder.html#comparisons\n\n\n# *** glob support *****\n\n@infer_global(glob)\nclass GlobInfer(AbstractTemplate):\n    def generic(self, args, kws):\n        if not kws and len(args) == 1 and args[0] == string_type:\n            return signature(string_array_type, *args)\n\n\n@lower_builtin(glob, string_type)\ndef lower_glob(context, builder, sig, args):\n    path = args[0]\n    uni_str = cgutils.create_struct_proxy(string_type)(\n        context, builder, value=path)\n    path = uni_str.data\n    typ = sig.return_type\n    dtype = StringArrayPayloadType()\n    meminfo, meminfo_data_ptr = construct_string_array(context, builder)\n    string_array = context.make_helper(builder, typ)\n    str_arr_payload = cgutils.create_struct_proxy(dtype)(context, builder)\n\n    # call glob in C\n    fnty = lir.FunctionType(lir.VoidType(),\n                            [lir.IntType(32).as_pointer().as_pointer(),\n                             lir.IntType(8).as_pointer().as_pointer(),\n                             lir.IntType(8).as_pointer().as_pointer(),\n                             lir.IntType(64).as_pointer(),\n                             lir.IntType(8).as_pointer()])\n    fn = builder.module.get_or_insert_function(fnty, name=""c_glob"")\n    builder.call(fn, [str_arr_payload._get_ptr_by_name(\'offsets\'),\n                      str_arr_payload._get_ptr_by_name(\'data\'),\n                      str_arr_payload._get_ptr_by_name(\'null_bitmap\'),\n                      string_array._get_ptr_by_name(\'num_items\'),\n                      path])\n\n    builder.store(str_arr_payload._getvalue(), meminfo_data_ptr)\n\n    string_array.meminfo = meminfo\n    string_array.offsets = str_arr_payload.offsets\n    string_array.data = str_arr_payload.data\n    string_array.null_bitmap = str_arr_payload.null_bitmap\n    string_array.num_total_chars = builder.zext(builder.load(\n        builder.gep(string_array.offsets, [string_array.num_items])), lir.IntType(64))\n\n    # cgutils.printf(builder, ""n %d\\n"", string_array.num_items)\n    ret = string_array._getvalue()\n    # context.nrt.decref(builder, ty, ret)\n\n    return impl_ret_new_ref(context, builder, typ, ret)\n\n\n@numba.njit(no_cpython_wrapper=True)\ndef append_string_array_to(result, pos, A):\n    # precondition: result is allocated with the size enough to contain A\n    i, j = 0, pos\n    for str in A:\n        result[j] = str\n        if str_arr_is_na(A, i):\n            sdc.str_arr_ext.str_arr_set_na(result, j)\n        i += 1\n        j += 1\n\n    return i\n\n\n@numba.njit(no_cpython_wrapper=True)\ndef create_str_arr_from_list(str_list):\n\n    n = len(str_list)\n    data_total_chars = 0\n    for i in numba.prange(n):\n        data_total_chars += get_utf8_size(str_list[i])\n    str_arr = pre_alloc_string_array(n, data_total_chars)\n    cp_str_list_to_array(str_arr, str_list)\n\n    return str_arr\n\n\n@numba.njit(no_cpython_wrapper=True)\ndef str_arr_set_na_by_mask(str_arr, nan_mask):\n    # precondition: (1) str_arr and nan_mask have the same size\n    #               (2) elements for which na bits are set all have zero lenght\n    for i in numba.prange(len(str_arr)):\n        if nan_mask[i]:\n            str_arr_set_na(str_arr, i)\n\n    return str_arr\n\n\n@overload(operator.add)\ndef sdc_str_arr_operator_add(self, other):\n\n    self_is_str_arr = self == string_array_type\n    other_is_str_arr = other == string_array_type\n    operands_are_str_arr = self_is_str_arr and other_is_str_arr\n\n    if not (operands_are_str_arr\n            or (self_is_str_arr and isinstance(other, types.UnicodeType))\n            or (isinstance(self, types.UnicodeType) and other_is_str_arr)):\n        return None\n\n    if operands_are_str_arr:\n        def _sdc_str_arr_operator_add_impl(self, other):\n            size_self, size_other = len(self), len(other)\n            if size_self != size_other:\n                raise ValueError(""Mismatch of String Arrays sizes in operator.add"")\n\n            res_total_chars = 0\n            for i in numba.prange(size_self):\n                if not str_arr_is_na(self, i) and not str_arr_is_na(other, i):\n                    res_total_chars += (get_utf8_size(self[i]) + get_utf8_size(other[i]))\n            res_arr = pre_alloc_string_array(size_self, res_total_chars)\n\n            for i in numba.prange(size_self):\n                if not (str_arr_is_na(self, i) or str_arr_is_na(other, i)):\n                    res_arr[i] = self[i] + other[i]\n                else:\n                    res_arr[i] = \'\'\n                    str_arr_set_na(res_arr, i)\n\n            return res_arr\n\n    elif self_is_str_arr:\n        def _sdc_str_arr_operator_add_impl(self, other):\n            res_size = len(self)\n            res_total_chars = 0\n            for i in numba.prange(res_size):\n                if not str_arr_is_na(self, i):\n                    res_total_chars += get_utf8_size(self[i]) + get_utf8_size(other)\n            res_arr = pre_alloc_string_array(res_size, res_total_chars)\n\n            for i in numba.prange(res_size):\n                if not str_arr_is_na(self, i):\n                    res_arr[i] = self[i] + other\n                else:\n                    res_arr[i] = \'\'\n                    str_arr_set_na(res_arr, i)\n\n            return res_arr\n\n    elif other_is_str_arr:\n        def _sdc_str_arr_operator_add_impl(self, other):\n            res_size = len(other)\n            res_total_chars = 0\n            for i in numba.prange(res_size):\n                if not str_arr_is_na(other, i):\n                    res_total_chars += get_utf8_size(other[i]) + get_utf8_size(self)\n            res_arr = pre_alloc_string_array(res_size, res_total_chars)\n\n            for i in numba.prange(res_size):\n                if not str_arr_is_na(other, i):\n                    res_arr[i] = self + other[i]\n                else:\n                    res_arr[i] = \'\'\n                    str_arr_set_na(res_arr, i)\n\n            return res_arr\n\n    else:\n        return None\n\n    return _sdc_str_arr_operator_add_impl\n\n\n@overload(operator.mul)\ndef sdc_str_arr_operator_mul(self, other):\n\n    self_is_str_arr = self == string_array_type\n    other_is_str_arr = other == string_array_type\n    if not ((self_is_str_arr and check_is_array_of_dtype(other, types.Integer)\n             or self_is_str_arr and isinstance(other, types.Integer)\n             or other_is_str_arr and check_is_array_of_dtype(self, types.Integer)\n             or other_is_str_arr and isinstance(self, types.Integer))):\n        return None\n\n    one_operand_is_scalar = isinstance(self, types.Integer) or isinstance(other, types.Integer)\n\n    def _sdc_str_arr_operator_mul_impl(self, other):\n\n        _self, _other = (self, other) if self_is_str_arr == True else (other, self)  # noqa\n        res_size = len(_self)\n        if one_operand_is_scalar != True:  # noqa\n            if res_size != len(_other):\n                raise ValueError(""Mismatch of String Array and Integer array sizes in operator.mul"")\n\n        res_total_chars = 0\n        for i in numba.prange(res_size):\n            if not str_arr_is_na(_self, i):\n                if one_operand_is_scalar == True:  # noqa\n                    res_total_chars += get_utf8_size(_self[i]) * max(0, _other)\n                else:\n                    res_total_chars += get_utf8_size(_self[i]) * max(0, _other[i])\n        res_arr = pre_alloc_string_array(res_size, res_total_chars)\n\n        for i in numba.prange(res_size):\n            if not str_arr_is_na(_self, i):\n                if one_operand_is_scalar == True:  # noqa\n                    set_value = _self[i] * _other\n                    res_arr[i] = _self[i] * _other\n                else:\n                    set_value = _self[i] * _other[i]\n                    res_arr[i] = _self[i] * _other[i]\n            else:\n                res_arr[i] = \'\'\n                str_arr_set_na(res_arr, i)\n\n        return res_arr\n\n    return _sdc_str_arr_operator_mul_impl\n\n\n@lower_builtin(operator.is_, StringArrayType, StringArrayType)\ndef sdc_str_arr_operator_is(context, builder, sig, args):\n\n    # meminfo ptr uniquely identifies each StringArray allocation\n    a = context.make_helper(builder, string_array_type, args[0])\n    b = context.make_helper(builder, string_array_type, args[1])\n    ma = builder.ptrtoint(a.meminfo, cgutils.intp_t)\n    mb = builder.ptrtoint(b.meminfo, cgutils.intp_t)\n    return builder.icmp_signed(\'==\', ma, mb)\n'"
sdc/str_arr_type.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\nfrom numba import types\nfrom numba.extending import (models, register_model, make_attribute_wrapper)\nfrom sdc.str_ext import string_type\n\n\nchar_typ = types.uint8\noffset_typ = types.uint32\n\ndata_ctypes_type = types.ArrayCTypes(types.Array(char_typ, 1, \'C\'))\noffset_ctypes_type = types.ArrayCTypes(types.Array(offset_typ, 1, \'C\'))\n\n\nclass StringArray(object):\n    def __init__(self, str_list):\n        # dummy constructor\n        self.num_strings = len(str_list)\n        self.offsets = str_list\n        self.data = str_list\n\n    def __repr__(self):\n        return \'StringArray({})\'.format(self.data)\n\n\nclass StringArrayType(types.IterableType):\n    def __init__(self):\n        super(StringArrayType, self).__init__(\n            name=\'StringArrayType()\')\n\n    @property\n    def dtype(self):\n        return string_type\n\n    @property\n    def iterator_type(self):\n        return StringArrayIterator()\n\n    def copy(self):\n        return StringArrayType()\n\n\nstring_array_type = StringArrayType()\n\n\nclass StringArrayPayloadType(types.Type):\n    def __init__(self):\n        super(StringArrayPayloadType, self).__init__(\n            name=\'StringArrayPayloadType()\')\n\n\nstr_arr_payload_type = StringArrayPayloadType()\n\n\n# XXX: C equivalent in _str_ext.cpp\n@register_model(StringArrayPayloadType)\nclass StringArrayPayloadModel(models.StructModel):\n    def __init__(self, dmm, fe_type):\n        members = [\n            (\'offsets\', types.CPointer(offset_typ)),\n            (\'data\', types.CPointer(char_typ)),\n            (\'null_bitmap\', types.CPointer(char_typ)),\n        ]\n        models.StructModel.__init__(self, dmm, fe_type, members)\n\n\nstr_arr_model_members = [\n    (\'num_items\', types.uint64),\n    (\'num_total_chars\', types.uint64),\n    (\'offsets\', types.CPointer(offset_typ)),\n    (\'data\', types.CPointer(char_typ)),\n    (\'null_bitmap\', types.CPointer(char_typ)),\n    (\'meminfo\', types.MemInfoPointer(str_arr_payload_type)),\n]\n\n\nmake_attribute_wrapper(StringArrayType, \'null_bitmap\', \'null_bitmap\')\n\n\n@register_model(StringArrayType)\nclass StringArrayModel(models.StructModel):\n    def __init__(self, dmm, fe_type):\n\n        models.StructModel.__init__(self, dmm, fe_type, str_arr_model_members)\n\n\nclass StringArrayIterator(types.SimpleIteratorType):\n    """"""\n    Type class for iterators of string arrays.\n    """"""\n\n    def __init__(self):\n        name = ""iter(String)""\n        yield_type = string_type\n        super(StringArrayIterator, self).__init__(name, yield_type)\n\n\n@register_model(StringArrayIterator)\nclass StrArrayIteratorModel(models.StructModel):\n    def __init__(self, dmm, fe_type):\n        # We use an unsigned index to avoid the cost of negative index tests.\n        members = [(\'index\', types.EphemeralPointer(types.uintp)),\n                   (\'array\', string_array_type)]\n        super(StrArrayIteratorModel, self).__init__(dmm, fe_type, members)\n\n\ndef is_str_arr_typ(typ):\n    from sdc.hiframes.pd_series_ext import is_str_series_typ\n    return typ == string_array_type or is_str_series_typ(typ)\n'"
sdc/str_ext.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\nimport operator\nimport re\nimport llvmlite.llvmpy.core as lc\nfrom llvmlite import ir as lir\nimport llvmlite.binding as ll\n\nimport numba\nfrom numba.core import cgutils, types\nfrom numba.extending import (box, unbox, typeof_impl, register_model, models,\n                             NativeValue, lower_builtin, lower_cast, overload,\n                             type_callable, overload_method, intrinsic)\nimport numba.cpython.hashing\nfrom numba.core.imputils import lower_constant, impl_ret_new_ref, impl_ret_untracked\nfrom numba.core.typing.templates import (signature, AbstractTemplate, infer, infer_getattr,\n                                    ConcreteTemplate, AttributeTemplate, bound_function, infer_global)\n\nimport sdc\nfrom sdc import hstr_ext\n# from sdc.utilities.utils import unliteral_all\n# TODO: resolve import conflict\n\n\ndef unliteral_all(args):\n    return tuple(types.unliteral(a) for a in args)\n\n\n# relative import seems required for C extensions\nll.add_symbol(\'get_char_from_string\', hstr_ext.get_char_from_string)\nll.add_symbol(\'get_char_ptr\', hstr_ext.get_char_ptr)\nll.add_symbol(\'del_str\', hstr_ext.del_str)\nll.add_symbol(\'_hash_str\', hstr_ext.hash_str)\n\n\nstring_type = types.unicode_type\n\n\n# XXX setting hash secret for hash(unicode_type) to be consistent across\n# processes. Other wise, shuffle operators like unique_str_parallel will fail.\n# TODO: use a seperate implementation?\n# TODO: make sure hash(str) is not already instantiated in overloads\n# def _rm_hash_str_overload():\n#     try:\n#         fn = numba.core.registry.cpu_target.typing_context.resolve_value_type(hash)\n#         sig = signature(types.int64, types.unicode_type)\n#         key = fn.get_impl_key(sig)\n#         numba.core.registry.cpu_target.target_context._defns[key]._cache.pop(sig.args)\n#     except:\n#         pass\n\n# _rm_hash_str_overload()\n\nnumba.cpython.hashing._Py_HashSecret_djbx33a_suffix = 0\nnumba.cpython.hashing._Py_HashSecret_siphash_k0 = 0\nnumba.cpython.hashing._Py_HashSecret_siphash_k1 = 0\n\n\n# use objmode for string methods for now\n\n# string methods that take no arguments and return a string\nstr2str_noargs = (\'capitalize\', \'casefold\', \'lower\', \'swapcase\', \'title\', \'upper\')\n\n\ndef str_overload_noargs(method):\n    @overload_method(types.UnicodeType, method)\n    def str_overload(in_str):\n        def _str_impl(in_str):\n            with numba.objmode(out=\'unicode_type\'):\n                out = getattr(in_str, method)()\n            return out\n\n        return _str_impl\n\n\nfor method in str2str_noargs:\n    str_overload_noargs(method)\n\n# strip string methods that take one argument and return a string\n# Numba bug https://github.com/numba/numba/issues/4731\nstr2str_1arg = (\'lstrip\', \'rstrip\', \'strip\')\n\n\ndef str_overload_1arg(method):\n    @overload_method(types.UnicodeType, method)\n    def str_overload(in_str, arg1):\n        def _str_impl(in_str, arg1):\n            with numba.objmode(out=\'unicode_type\'):\n                out = getattr(in_str, method)(arg1)\n            return out\n\n        return _str_impl\n\n\nfor method in str2str_1arg:\n    str_overload_1arg(method)\n\n\n@overload_method(types.UnicodeType, \'replace\')\ndef str_replace_overload(in_str, old, new, count=-1):\n\n    def _str_replace_impl(in_str, old, new, count=-1):\n        with numba.objmode(out=\'unicode_type\'):\n            out = in_str.replace(old, new, count)\n        return out\n\n    return _str_replace_impl\n\n\n# ********************  re support  *******************\n\nclass RePatternType(types.Opaque):\n    def __init__(self):\n        super(RePatternType, self).__init__(name=\'RePatternType\')\n\n\nre_pattern_type = RePatternType()\ntypes.re_pattern_type = re_pattern_type\n\nregister_model(RePatternType)(models.OpaqueModel)\n\n\n@box(RePatternType)\ndef box_re_pattern(typ, val, c):\n    # TODO: fix\n    c.pyapi.incref(val)\n    return val\n\n\n@unbox(RePatternType)\ndef unbox_re_pattern(typ, obj, c):\n    # TODO: fix\n    c.pyapi.incref(obj)\n    return NativeValue(obj)\n\n\n# jitoptions until numba #4020 is resolved\n@overload(re.compile, jit_options={\'no_cpython_wrapper\': False})\ndef re_compile_overload(pattern, flags=0):\n    def _re_compile_impl(pattern, flags=0):\n        with numba.objmode(pat=\'re_pattern_type\'):\n            pat = re.compile(pattern, flags)\n        return pat\n    return _re_compile_impl\n\n\n@overload_method(RePatternType, \'sub\')\ndef re_sub_overload(p, repl, string, count=0):\n    def _re_sub_impl(p, repl, string, count=0):\n        with numba.objmode(out=\'unicode_type\'):\n            out = p.sub(repl, string, count)\n        return out\n    return _re_sub_impl\n\n\n# **********************  type for std string pointer  ************************\n\n\nclass StringType(types.Opaque, types.Hashable):\n    def __init__(self):\n        super(StringType, self).__init__(name=\'StringType\')\n\n\nstd_str_type = StringType()\n\n# XXX enabling this turns on old std::string implementation\n# string_type = StringType()\n\n# @typeof_impl.register(str)\n# def _typeof_str(val, c):\n#     return string_type\n\n\nregister_model(StringType)(models.OpaqueModel)\n\n# XXX: should be subtype of StringType?\n\n\nclass CharType(types.Type):\n    def __init__(self):\n        super(CharType, self).__init__(name=\'CharType\')\n        self.bitwidth = 8\n\n\nchar_type = CharType()\nregister_model(CharType)(models.IntegerModel)\n\n\n@overload(operator.getitem)\ndef char_getitem_overload(_str, ind):\n    if _str == std_str_type and isinstance(ind, types.Integer):\n        sig = char_type(\n            std_str_type,   # string\n            types.intp,    # index\n        )\n        get_char_from_string = types.ExternalFunction(""get_char_from_string"", sig)\n\n        def impl(_str, ind):\n            return get_char_from_string(_str, ind)\n\n        return impl\n\n# XXX: fix overload for getitem and use it\n@lower_builtin(operator.getitem, StringType, types.Integer)\ndef getitem_string(context, builder, sig, args):\n    fnty = lir.FunctionType(lir.IntType(8),\n                            [lir.IntType(8).as_pointer(), lir.IntType(64)])\n    fn = builder.module.get_or_insert_function(fnty, name=""get_char_from_string"")\n    return builder.call(fn, args)\n\n\n@box(CharType)\ndef box_char(typ, val, c):\n    """"""\n    """"""\n    fnty = lir.FunctionType(lir.IntType(8).as_pointer(),\n                            [lir.IntType(8)])\n    fn = c.builder.module.get_or_insert_function(fnty, name=""get_char_ptr"")\n    c_str = c.builder.call(fn, [val])\n    pystr = c.pyapi.string_from_string_and_size(c_str, c.context.get_constant(types.intp, 1))\n    # TODO: delete ptr\n    return pystr\n\n\ndel_str = types.ExternalFunction(""del_str"", types.void(std_str_type))\n_hash_str = types.ExternalFunction(""_hash_str"", types.int64(std_str_type))\nget_c_str = types.ExternalFunction(""get_c_str"", types.voidptr(std_str_type))\n\n\n@overload_method(StringType, \'c_str\')\ndef str_c_str(str_typ):\n    return lambda s: get_c_str(s)\n\n\n@overload_method(StringType, \'join\')\ndef str_join(str_typ, iterable_typ):\n    # TODO: more efficient implementation (e.g. C++ string buffer)\n    def str_join_impl(sep_str, str_container):\n        res = """"\n        counter = 0\n        for s in str_container:\n            if counter != 0:\n                res += sep_str\n            counter += 1\n            res += s\n        return res\n    return str_join_impl\n\n\n# TODO: using lower_builtin since overload fails for str tuple\n# TODO: constant hash like hash(""ss"",) fails\n# @overload(hash)\n# def hash_overload(str_typ):\n#     if str_typ == string_type:\n#         return lambda s: _hash_str(s)\n\n@lower_builtin(hash, std_str_type)\ndef hash_str_lower(context, builder, sig, args):\n    return context.compile_internal(\n        builder, lambda s: _hash_str(s), sig, args)\n\n# XXX: use Numba\'s hash(str) when available\n@lower_builtin(hash, string_type)\ndef hash_unicode_lower(context, builder, sig, args):\n    std_str = gen_unicode_to_std_str(context, builder, args[0])\n    return hash_str_lower(\n        context, builder, signature(sig.return_type, std_str_type), [std_str])\n\n\n@infer\n@infer_global(operator.add)\n@infer_global(operator.iadd)\nclass StringAdd(ConcreteTemplate):\n    key = ""+""\n    cases = [signature(std_str_type, std_str_type, std_str_type)]\n\n\n@infer\n@infer_global(operator.eq)\n@infer_global(operator.ne)\n@infer_global(operator.ge)\n@infer_global(operator.gt)\n@infer_global(operator.le)\n@infer_global(operator.lt)\nclass StringOpEq(AbstractTemplate):\n    key = \'==\'\n\n    def generic(self, args, kws):\n        assert not kws\n        (arg1, arg2) = args\n        if isinstance(arg1, StringType) and isinstance(arg2, StringType):\n            return signature(types.boolean, arg1, arg2)\n        if arg1 == char_type and arg2 == char_type:\n            return signature(types.boolean, arg1, arg2)\n\n\n@infer\nclass StringOpNotEq(StringOpEq):\n    key = \'!=\'\n\n\n@infer\nclass StringOpGE(StringOpEq):\n    key = \'>=\'\n\n\n@infer\nclass StringOpGT(StringOpEq):\n    key = \'>\'\n\n\n@infer\nclass StringOpLE(StringOpEq):\n    key = \'<=\'\n\n\n@infer\nclass StringOpLT(StringOpEq):\n    key = \'<\'\n\n\n@infer_getattr\nclass StringAttribute(AttributeTemplate):\n    key = StringType\n\n    @bound_function(""str.split"")\n    def resolve_split(self, dict, args, kws):\n        assert not kws\n        assert len(args) == 1\n        return signature(types.List(std_str_type), types.unliteral(args[0]))\n\n\n# @infer_global(operator.getitem)\n# class GetItemString(AbstractTemplate):\n#     key = operator.getitem\n#\n#     def generic(self, args, kws):\n#         assert not kws\n#         if (len(args) == 2 and isinstance(args[0], StringType)\n#                 and isinstance(args[1], types.Integer)):\n#             return signature(args[0], *args)\n\n\n# @infer_global(len)\n# class LenStringArray(AbstractTemplate):\n#     def generic(self, args, kws):\n#         if not kws and len(args) == 1 and args[0] == std_str_type:\n#             return signature(types.intp, *args)\n\n\n@overload(int)\ndef int_str_overload(in_str):\n    if in_str == string_type:\n        def _str_to_int_impl(in_str):\n            return _str_to_int64(in_str._data, in_str._length)\n\n        return _str_to_int_impl\n\n\n# @infer_global(int)\n# class StrToInt(AbstractTemplate):\n#     def generic(self, args, kws):\n#         assert not kws\n#         [arg] = args\n#         if isinstance(arg, StringType):\n#             return signature(types.intp, arg)\n#         # TODO: implement int(str) in Numba\n#         if arg == string_type:\n#             return signature(types.intp, arg)\n\n\n@infer_global(float)\nclass StrToFloat(AbstractTemplate):\n    def generic(self, args, kws):\n        assert not kws\n        [arg] = args\n        if isinstance(arg, StringType):\n            return signature(types.float64, arg)\n        # TODO: implement int(str) in Numba\n        if arg == string_type:\n            return signature(types.float64, arg)\n\n\n@infer_global(str)\nclass StrConstInfer(AbstractTemplate):\n    def generic(self, args, kws):\n        assert not kws\n        assert len(args) == 1\n        assert args[0] in [types.int32, types.int64, types.float32, types.float64, string_type]\n        return signature(string_type, *args)\n\n\nclass RegexType(types.Opaque):\n    def __init__(self):\n        super(RegexType, self).__init__(name=\'RegexType\')\n\n\nregex_type = RegexType()\n\nregister_model(RegexType)(models.OpaqueModel)\n\n\ndef compile_regex(pat):\n    return 0\n\n\n@infer_global(compile_regex)\nclass CompileRegexInfer(AbstractTemplate):\n    def generic(self, args, kws):\n        assert not kws\n        assert len(args) == 1\n        return signature(regex_type, *unliteral_all(args))\n\n\ndef contains_regex(str, pat):\n    return False\n\n\ndef contains_noregex(str, pat):\n    return False\n\n\n@infer_global(contains_regex)\n@infer_global(contains_noregex)\nclass ContainsInfer(AbstractTemplate):\n    def generic(self, args, kws):\n        assert not kws\n        assert len(args) == 2\n        return signature(types.boolean, *unliteral_all(args))\n\n\nll.add_symbol(\'init_string\', hstr_ext.init_string)\nll.add_symbol(\'init_string_const\', hstr_ext.init_string_const)\nll.add_symbol(\'get_c_str\', hstr_ext.get_c_str)\nll.add_symbol(\'str_concat\', hstr_ext.str_concat)\nll.add_symbol(\'str_compare\', hstr_ext.str_compare)\nll.add_symbol(\'str_equal\', hstr_ext.str_equal)\nll.add_symbol(\'str_equal_cstr\', hstr_ext.str_equal_cstr)\nll.add_symbol(\'str_split\', hstr_ext.str_split)\nll.add_symbol(\'str_substr_int\', hstr_ext.str_substr_int)\nll.add_symbol(\'str_to_int64\', hstr_ext.str_to_int64)\nll.add_symbol(\'std_str_to_int64\', hstr_ext.std_str_to_int64)\nll.add_symbol(\'str_to_float64\', hstr_ext.str_to_float64)\nll.add_symbol(\'get_str_len\', hstr_ext.get_str_len)\nll.add_symbol(\'compile_regex\', hstr_ext.compile_regex)\nll.add_symbol(\'str_contains_regex\', hstr_ext.str_contains_regex)\nll.add_symbol(\'str_contains_noregex\', hstr_ext.str_contains_noregex)\nll.add_symbol(\'str_replace_regex\', hstr_ext.str_replace_regex)\nll.add_symbol(\'str_from_int32\', hstr_ext.str_from_int32)\nll.add_symbol(\'str_from_int64\', hstr_ext.str_from_int64)\nll.add_symbol(\'str_from_float32\', hstr_ext.str_from_float32)\nll.add_symbol(\'str_from_float64\', hstr_ext.str_from_float64)\n\nget_std_str_len = types.ExternalFunction(\n    ""get_str_len"", signature(types.intp, std_str_type))\ninit_string_from_chars = types.ExternalFunction(\n    ""init_string_const"", std_str_type(types.voidptr, types.intp))\n\n_str_to_int64 = types.ExternalFunction(\n    ""str_to_int64"", signature(types.intp, types.voidptr, types.intp))\n\nstr_replace_regex = types.ExternalFunction(\n    ""str_replace_regex"", std_str_type(std_str_type, regex_type, std_str_type))\n\n\ndef gen_unicode_to_std_str(context, builder, unicode_val):\n    #\n    uni_str = cgutils.create_struct_proxy(string_type)(\n        context, builder, value=unicode_val)\n    fnty = lir.FunctionType(lir.IntType(8).as_pointer(),\n                            [lir.IntType(8).as_pointer(), lir.IntType(64)])\n    fn = builder.module.get_or_insert_function(fnty, name=""init_string_const"")\n    return builder.call(fn, [uni_str.data, uni_str.length])\n\n\ndef gen_std_str_to_unicode(context, builder, std_str_val, del_str=False):\n    kind = numba.cpython.unicode.PY_UNICODE_1BYTE_KIND\n\n    def _std_str_to_unicode(std_str):\n        length = sdc.str_ext.get_std_str_len(std_str)\n        ret = numba.cpython.unicode._empty_string(kind, length)\n        sdc.str_arr_ext._memcpy(\n            ret._data, sdc.str_ext.get_c_str(std_str), length, 1)\n        if del_str:\n            sdc.str_ext.del_str(std_str)\n        return ret\n    val = context.compile_internal(\n        builder,\n        _std_str_to_unicode,\n        string_type(sdc.str_ext.std_str_type),\n        [std_str_val])\n    return val\n\n\ndef gen_get_unicode_chars(context, builder, unicode_val):\n    uni_str = cgutils.create_struct_proxy(string_type)(\n        context, builder, value=unicode_val)\n    return uni_str.data\n\n\ndef unicode_to_char_ptr(in_str):\n    return in_str\n\n\n@overload(unicode_to_char_ptr)\ndef unicode_to_char_ptr_overload(a):\n    # str._data is not safe since str might be literal\n    # overload resolves str literal to unicode_type\n    if a == string_type:\n        return lambda a: a._data\n\n\n@intrinsic\ndef unicode_to_std_str(typingctx, unicode_t=None):\n    def codegen(context, builder, sig, args):\n        return gen_unicode_to_std_str(context, builder, args[0])\n    return std_str_type(string_type), codegen\n\n\n@intrinsic\ndef std_str_to_unicode(typingctx, unicode_t=None):\n    def codegen(context, builder, sig, args):\n        return gen_std_str_to_unicode(context, builder, args[0], True)\n    return string_type(std_str_type), codegen\n\n\n@intrinsic\ndef alloc_str_list(typingctx, n_t=None):\n    def codegen(context, builder, sig, args):\n        nitems = args[0]\n        list_type = types.List(string_type)\n        result = numba.cpython.listobj.ListInstance.allocate(context, builder, list_type, nitems)\n        result.size = nitems\n        return impl_ret_new_ref(context, builder, list_type, result.value)\n    return types.List(string_type)(types.intp), codegen\n\n\n# XXX using list of list string instead of array of list string since Numba\'s\n# arrays can\'t store lists\nlist_string_array_type = types.List(types.List(string_type))\n\n\n@intrinsic\ndef alloc_list_list_str(typingctx, n_t=None):\n    def codegen(context, builder, sig, args):\n        nitems = args[0]\n        list_type = list_string_array_type\n        result = numba.cpython.listobj.ListInstance.allocate(context, builder, list_type, nitems)\n        result.size = nitems\n        return impl_ret_new_ref(context, builder, list_type, result.value)\n    return list_string_array_type(types.intp), codegen\n\n\n@unbox(StringType)\ndef unbox_string(typ, obj, c):\n    """"""\n    """"""\n    ok, buffer, size = c.pyapi.string_as_string_and_size(obj)\n\n    fnty = lir.FunctionType(lir.IntType(8).as_pointer(),\n                            [lir.IntType(8).as_pointer(), lir.IntType(64)])\n    fn = c.builder.module.get_or_insert_function(fnty, name=""init_string"")\n    ret = c.builder.call(fn, [buffer, size])\n\n    return NativeValue(ret, is_error=c.builder.not_(ok))\n\n\n@box(StringType)\ndef box_str(typ, val, c):\n    """"""\n    """"""\n    fnty = lir.FunctionType(lir.IntType(8).as_pointer(),\n                            [lir.IntType(8).as_pointer()])\n    fn = c.builder.module.get_or_insert_function(fnty, name=""get_c_str"")\n    c_str = c.builder.call(fn, [val])\n    pystr = c.pyapi.string_from_string(c_str)\n    return pystr\n\n\ndef getpointer(str):\n    pass\n\n\n@type_callable(getpointer)\ndef type_string_getpointer(context):\n    def typer(val):\n        return types.voidptr\n    return typer\n\n\n@lower_builtin(getpointer, StringType)\ndef getpointer_from_string(context, builder, sig, args):\n    val = args[0]\n    fnty = lir.FunctionType(lir.IntType(8).as_pointer(),\n                            [lir.IntType(8).as_pointer()])\n    fn = builder.module.get_or_insert_function(fnty, name=""get_c_str"")\n    c_str = builder.call(fn, [val])\n    return c_str\n\n\n@lower_builtin(getpointer, types.StringLiteral)\ndef getpointer_from_string_literal(context, builder, sig, args):\n    cstr = context.insert_const_string(builder.module, sig.args[0].literal_value)\n    return cstr\n\n\n@lower_cast(StringType, types.StringLiteral)\ndef string_type_to_const(context, builder, fromty, toty, val):\n    # calling str() since the const value can be non-str like tuple const (CSV)\n    cstr = context.insert_const_string(builder.module, str(toty.literal_value))\n    # check to make sure Const value matches stored string\n    # call str == cstr\n    fnty = lir.FunctionType(lir.IntType(1),\n                            [lir.IntType(8).as_pointer(), lir.IntType(8).as_pointer()])\n    fn = builder.module.get_or_insert_function(fnty, name=""str_equal_cstr"")\n    match = builder.call(fn, [val, cstr])\n    with cgutils.if_unlikely(builder, builder.not_(match)):\n        # Raise RuntimeError about the assumption violation\n        usermsg = ""constant string assumption violated""\n        errmsg = ""{}: expecting {}"".format(usermsg, toty.literal_value)\n        context.call_conv.return_user_exc(builder, RuntimeError, (errmsg,))\n\n    return impl_ret_untracked(context, builder, toty, cstr)\n\n\n@lower_constant(StringType)\ndef const_string(context, builder, ty, pyval):\n    cstr = context.insert_const_string(builder.module, pyval)\n    length = context.get_constant(types.intp, len(pyval))\n\n    fnty = lir.FunctionType(lir.IntType(8).as_pointer(),\n                            [lir.IntType(8).as_pointer(), lir.IntType(64)])\n    fn = builder.module.get_or_insert_function(fnty, name=""init_string_const"")\n    ret = builder.call(fn, [cstr, length])\n    return ret\n\n\n@lower_cast(types.StringLiteral, StringType)\ndef const_to_string_type(context, builder, fromty, toty, val):\n    cstr = context.insert_const_string(builder.module, fromty.literal_value)\n    length = context.get_constant(types.intp, len(fromty.literal_value))\n\n    fnty = lir.FunctionType(lir.IntType(8).as_pointer(),\n                            [lir.IntType(8).as_pointer(), lir.IntType(64)])\n    fn = builder.module.get_or_insert_function(fnty, name=""init_string_const"")\n    ret = builder.call(fn, [cstr, length])\n    return ret\n\n\n@lower_builtin(str, types.Any)\ndef string_from_impl(context, builder, sig, args):\n    in_typ = sig.args[0]\n    if in_typ == string_type:\n        return args[0]\n    ll_in_typ = context.get_value_type(sig.args[0])\n    fnty = lir.FunctionType(lir.IntType(8).as_pointer(), [ll_in_typ])\n    fn = builder.module.get_or_insert_function(\n        fnty, name=""str_from_"" + str(in_typ))\n    std_str = builder.call(fn, args)\n    return gen_std_str_to_unicode(context, builder, std_str)\n\n\n@lower_builtin(operator.add, std_str_type, std_str_type)\n@lower_builtin(""+"", std_str_type, std_str_type)\ndef impl_string_concat(context, builder, sig, args):\n    fnty = lir.FunctionType(lir.IntType(8).as_pointer(),\n                            [lir.IntType(8).as_pointer(), lir.IntType(8).as_pointer()])\n    fn = builder.module.get_or_insert_function(fnty, name=""str_concat"")\n    return builder.call(fn, args)\n\n\n@lower_builtin(operator.eq, std_str_type, std_str_type)\n@lower_builtin(\'==\', std_str_type, std_str_type)\ndef string_eq_impl(context, builder, sig, args):\n    fnty = lir.FunctionType(lir.IntType(1),\n                            [lir.IntType(8).as_pointer(), lir.IntType(8).as_pointer()])\n    fn = builder.module.get_or_insert_function(fnty, name=""str_equal"")\n    return builder.call(fn, args)\n\n\n@lower_builtin(operator.eq, char_type, char_type)\n@lower_builtin(\'==\', char_type, char_type)\ndef char_eq_impl(context, builder, sig, args):\n    def char_eq_impl(c1, c2):\n        return c1 == c2\n    new_sig = signature(sig.return_type, types.uint8, types.uint8)\n    res = context.compile_internal(builder, char_eq_impl, new_sig, args)\n    return res\n\n\n@lower_builtin(operator.ne, std_str_type, std_str_type)\n@lower_builtin(\'!=\', std_str_type, std_str_type)\ndef string_neq_impl(context, builder, sig, args):\n    fnty = lir.FunctionType(lir.IntType(1),\n                            [lir.IntType(8).as_pointer(), lir.IntType(8).as_pointer()])\n    fn = builder.module.get_or_insert_function(fnty, name=""str_equal"")\n    return builder.not_(builder.call(fn, args))\n\n\n@lower_builtin(operator.ge, std_str_type, std_str_type)\n@lower_builtin(\'>=\', std_str_type, std_str_type)\ndef string_ge_impl(context, builder, sig, args):\n    fnty = lir.FunctionType(lir.IntType(32),\n                            [lir.IntType(8).as_pointer(), lir.IntType(8).as_pointer()])\n    fn = builder.module.get_or_insert_function(fnty, name=""str_compare"")\n    comp_val = builder.call(fn, args)\n    zero = context.get_constant(types.int32, 0)\n    res = builder.icmp(lc.ICMP_SGE, comp_val, zero)\n    return res\n\n\n@lower_builtin(operator.gt, std_str_type, std_str_type)\n@lower_builtin(\'>\', std_str_type, std_str_type)\ndef string_gt_impl(context, builder, sig, args):\n    fnty = lir.FunctionType(lir.IntType(32),\n                            [lir.IntType(8).as_pointer(), lir.IntType(8).as_pointer()])\n    fn = builder.module.get_or_insert_function(fnty, name=""str_compare"")\n    comp_val = builder.call(fn, args)\n    zero = context.get_constant(types.int32, 0)\n    res = builder.icmp(lc.ICMP_SGT, comp_val, zero)\n    return res\n\n\n@lower_builtin(operator.le, std_str_type, std_str_type)\n@lower_builtin(\'<=\', std_str_type, std_str_type)\ndef string_le_impl(context, builder, sig, args):\n    fnty = lir.FunctionType(lir.IntType(32),\n                            [lir.IntType(8).as_pointer(), lir.IntType(8).as_pointer()])\n    fn = builder.module.get_or_insert_function(fnty, name=""str_compare"")\n    comp_val = builder.call(fn, args)\n    zero = context.get_constant(types.int32, 0)\n    res = builder.icmp(lc.ICMP_SLE, comp_val, zero)\n    return res\n\n\n@lower_builtin(operator.lt, std_str_type, std_str_type)\n@lower_builtin(\'<\', std_str_type, std_str_type)\ndef string_lt_impl(context, builder, sig, args):\n    fnty = lir.FunctionType(lir.IntType(32),\n                            [lir.IntType(8).as_pointer(), lir.IntType(8).as_pointer()])\n    fn = builder.module.get_or_insert_function(fnty, name=""str_compare"")\n    comp_val = builder.call(fn, args)\n    zero = context.get_constant(types.int32, 0)\n    res = builder.icmp(lc.ICMP_SLT, comp_val, zero)\n    return res\n\n\n@lower_builtin(""str.split"", std_str_type, std_str_type)\ndef string_split_impl(context, builder, sig, args):\n    nitems = cgutils.alloca_once(builder, lir.IntType(64))\n    # input str, sep, size pointer\n    fnty = lir.FunctionType(lir.IntType(8).as_pointer().as_pointer(),\n                            [lir.IntType(8).as_pointer(), lir.IntType(8).as_pointer(),\n                             lir.IntType(64).as_pointer()])\n    fn = builder.module.get_or_insert_function(fnty, name=""str_split"")\n    ptr = builder.call(fn, args + [nitems])\n    size = builder.load(nitems)\n    # TODO: use ptr instead of allocating and copying, use NRT_MemInfo_new\n    # TODO: deallocate ptr\n    _list = numba.cpython.listobj.ListInstance.allocate(context, builder,\n                                                        sig.return_type, size)\n    _list.size = size\n    with cgutils.for_range(builder, size) as loop:\n        value = builder.load(cgutils.gep_inbounds(builder, ptr, loop.index))\n        # TODO: refcounted str\n        _list.setitem(loop.index, value, incref=False)\n    return impl_ret_new_ref(context, builder, sig.return_type, _list.value)\n\n\n# @lower_builtin(operator.getitem, StringType, types.Integer)\n# def getitem_string(context, builder, sig, args):\n#     fnty = lir.FunctionType(lir.IntType(8).as_pointer(),\n#                             [lir.IntType(8).as_pointer(), lir.IntType(64)])\n#     fn = builder.module.get_or_insert_function(fnty, name=""str_substr_int"")\n#     # TODO: handle reference counting\n#     # return impl_ret_new_ref(builder.call(fn, args))\n#     return (builder.call(fn, args))\n\n\n@lower_cast(StringType, types.int64)\ndef cast_str_to_int64(context, builder, fromty, toty, val):\n    fnty = lir.FunctionType(lir.IntType(64), [lir.IntType(8).as_pointer()])\n    fn = builder.module.get_or_insert_function(fnty, name=""std_str_to_int64"")\n    return builder.call(fn, (val,))\n\n# # XXX handle unicode until Numba supports int(str)\n# @lower_cast(string_type, types.int64)\n# def cast_unicode_str_to_int64(context, builder, fromty, toty, val):\n#     std_str = gen_unicode_to_std_str(context, builder, val)\n#     return cast_str_to_int64(context, builder, std_str_type, toty, std_str)\n\n\n@lower_cast(StringType, types.float64)\ndef cast_str_to_float64(context, builder, fromty, toty, val):\n    fnty = lir.FunctionType(lir.DoubleType(), [lir.IntType(8).as_pointer()])\n    fn = builder.module.get_or_insert_function(fnty, name=""str_to_float64"")\n    return builder.call(fn, (val,))\n\n# XXX handle unicode until Numba supports float(str)\n@lower_cast(string_type, types.float64)\ndef cast_unicode_str_to_float64(context, builder, fromty, toty, val):\n    std_str = gen_unicode_to_std_str(context, builder, val)\n    return cast_str_to_float64(context, builder, std_str_type, toty, std_str)\n\n# @lower_builtin(len, StringType)\n# def len_string(context, builder, sig, args):\n#     fnty = lir.FunctionType(lir.IntType(64),\n#                             [lir.IntType(8).as_pointer()])\n#     fn = builder.module.get_or_insert_function(fnty, name=""get_str_len"")\n#     return (builder.call(fn, args))\n\n\n@lower_builtin(compile_regex, std_str_type)\ndef lower_compile_regex(context, builder, sig, args):\n    fnty = lir.FunctionType(lir.IntType(8).as_pointer(),\n                            [lir.IntType(8).as_pointer()])\n    fn = builder.module.get_or_insert_function(fnty, name=""compile_regex"")\n    return builder.call(fn, args)\n\n\n@lower_builtin(compile_regex, string_type)\ndef lower_compile_regex_unicode(context, builder, sig, args):\n    val = args[0]\n    std_val = gen_unicode_to_std_str(context, builder, val)\n    return lower_compile_regex(context, builder, sig, [std_val])\n\n\n@lower_builtin(contains_regex, std_str_type, regex_type)\ndef impl_string_contains_regex(context, builder, sig, args):\n    fnty = lir.FunctionType(lir.IntType(1),\n                            [lir.IntType(8).as_pointer(), lir.IntType(8).as_pointer()])\n    fn = builder.module.get_or_insert_function(fnty, name=""str_contains_regex"")\n    return builder.call(fn, args)\n\n\n@lower_builtin(contains_regex, string_type, regex_type)\ndef impl_unicode_string_contains_regex(context, builder, sig, args):\n    val, reg = args\n    std_val = gen_unicode_to_std_str(context, builder, val)\n    return impl_string_contains_regex(\n        context, builder, sig, [std_val, reg])\n\n\n@lower_builtin(contains_noregex, std_str_type, std_str_type)\ndef impl_string_contains_noregex(context, builder, sig, args):\n    fnty = lir.FunctionType(lir.IntType(1),\n                            [lir.IntType(8).as_pointer(), lir.IntType(8).as_pointer()])\n    fn = builder.module.get_or_insert_function(\n        fnty, name=""str_contains_noregex"")\n    return builder.call(fn, args)\n\n\n@lower_builtin(contains_noregex, string_type, string_type)\ndef impl_unicode_string_contains_noregex(context, builder, sig, args):\n    val, pat = args\n    std_val = gen_unicode_to_std_str(context, builder, val)\n    std_pat = gen_unicode_to_std_str(context, builder, pat)\n    return impl_string_contains_noregex(\n        context, builder, sig, [std_val, std_pat])\n'"
sdc/timsort.py,10,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\nimport numpy as np\nimport pandas as pd\nimport numba\nfrom numba.extending import overload\nfrom sdc.utilities.utils import empty_like_type, alloc_arr_tup\n\n# ported from Spark to Numba-compilable Python\n# A port of the Android TimSort class, which utilizes a ""stable, adaptive, iterative mergesort.""\n# https:# github.com/apache/spark/blob/master/core/src/main/java/org/apache/spark/util/collection/TimSort.java\n# https:# github.com/python/cpython/blob/master/Objects/listobject.c\n\n\n# This is the minimum sized sequence that will be merged.  Shorter\n# sequences will be lengthened by calling binarySort.  If the entire\n# array is less than this length, no merges will be performed.\n# # This constant should be a power of two.  It was 64 in Tim Peter\'s C\n# implementation, but 32 was empirically determined to work better in\n# this implementation.  In the unlikely event that you set this constant\n# to be a number that\'s not a power of two, you\'ll need to change the\n# minRunLength computation.\n# # If you decrease this constant, you must change the stackLen\n# computation in the TimSort constructor, or you risk an\n# ArrayOutOfBounds exception.  See listsort.txt for a discussion\n# of the minimum stack length required as a function of the length\n# of the array being sorted and the minimum merge sequence length.\n\nMIN_MERGE = 32\n\n\n# A stable, adaptive, iterative mergesort that requires far fewer than\n# n lg(n) comparisons when running on partially sorted arrays, while\n# offering performance comparable to a traditional mergesort when run\n# on random arrays.  Like all proper mergesorts, this sort is stable and\n# runs O(n log n) time (worst case).  In the worst case, this sort requires\n# temporary storage space for n/2 object references, in the best case,\n# it requires only a small constant amount of space.\n#\n# This implementation was adapted from Tim Peters\'s list sort for\n# Python, which is described in detail here:\n#\n#  http:# svn.python.org/projects/python/trunk/Objects/listsort.txt\n#\n# Tim\'s C code may be found here:\n#\n#  http:# svn.python.org/projects/python/trunk/Objects/listobject.c\n#\n# The underlying techniques are described in this paper (and may have\n# even earlier origins):\n#\n# ""Optimistic Sorting and Information Theoretic Complexity""\n# Peter McIlroy\n# SODA (Fourth Annual ACM-SIAM Symposium on Discrete Algorithms),\n# pp 467-474, Austin, Texas, 25-27 January 1993.\n#\n# While the API to this class consists solely of static methods, it is\n# (privately) instantiable, a TimSort instance holds the state of an ongoing\n# sort, assuming the input array is large enough to warrant the full-blown\n# TimSort. Small arrays are sorted in place, using a binary insertion sort.\n\n@numba.njit(no_cpython_wrapper=True, cache=True)\ndef sort(key_arrs, lo, hi, data):  # pragma: no cover\n\n    nRemaining = hi - lo\n    if nRemaining < 2:\n        return  # Arrays of size 0 and 1 are always sorted\n\n    # If array is small, do a ""mini-TimSort"" with no merges\n    if nRemaining < MIN_MERGE:\n        initRunLen = countRunAndMakeAscending(key_arrs, lo, hi, data)\n        binarySort(key_arrs, lo, hi, lo + initRunLen, data)\n        return\n\n    # March over the array once, left to right, finding natural runs,\n    # extending short natural runs to minRun elements, and merging runs\n    # to maintain stack invariant.\n\n    (stackSize, runBase, runLen, tmpLength, tmp, tmp_data,\n        minGallop) = init_sort_start(key_arrs, data)\n\n    minRun = minRunLength(nRemaining)\n    while True:  # emulating do-while\n        # Identify next run\n        run_len = countRunAndMakeAscending(key_arrs, lo, hi, data)\n\n        # If run is short, extend to min(minRun, nRemaining)\n        if run_len < minRun:\n            force = nRemaining if nRemaining <= minRun else minRun\n            binarySort(key_arrs, lo, lo + force, lo + run_len, data)\n            run_len = force\n\n        # Push run onto pending-run stack, and maybe merge\n        stackSize = pushRun(stackSize, runBase, runLen, lo, run_len)\n        stackSize, tmpLength, tmp, tmp_data, minGallop = mergeCollapse(\n            stackSize, runBase, runLen, key_arrs, data, tmpLength, tmp,\n            tmp_data, minGallop)\n\n        # Advance to find next run\n        lo += run_len\n        nRemaining -= run_len\n        if nRemaining == 0:\n            break\n\n    # Merge all remaining runs to complete sort\n    assert lo == hi\n    stackSize, tmpLength, tmp, tmp_data, minGallop = mergeForceCollapse(\n        stackSize, runBase, runLen, key_arrs, data, tmpLength, tmp,\n        tmp_data, minGallop)\n    assert stackSize == 1\n\n\n# Sorts the specified portion of the specified array using a binary\n# insertion sort.  This is the best method for sorting small numbers\n# of elements.  It requires O(n log n) compares, but O(n^2) data\n# movement (worst case).\n#\n# If the initial part of the specified range is already sorted,\n# this method can take advantage of it: the method assumes that the\n# elements from index {@code lo}, inclusive, to {@code start},\n# exclusive are already sorted.\n#\n# @param a the array in which a range is to be sorted\n# @param lo the index of the first element in the range to be sorted\n# @param hi the index after the last element in the range to be sorted\n# @param start the index of the first element in the range that is\n#       not already known to be sorted ({@code lo <= start <= hi})\n# @param c comparator to used for the sort\n\n@numba.njit(no_cpython_wrapper=True, cache=True)\ndef binarySort(key_arrs, lo, hi, start, data):  # pragma: no cover\n    assert lo <= start and start <= hi\n    if start == lo:\n        start += 1\n\n    # Buffer pivotStore = s.allocate(1)\n\n    while start < hi:\n        # pivotStore = key_arrs[start]  # TODO: copy data to pivot\n        pivot = getitem_arr_tup(key_arrs, start)\n        pivot_data = getitem_arr_tup(data, start)\n\n        # Set left (and right) to the index where key_arrs[start] (pivot) belongs\n        left = lo\n        right = start\n        assert left <= right\n\n        # Invariants:\n        #  pivot >= all in [lo, left).\n        #  pivot <  all in [right, start).\n\n        while left < right:\n            mid = (left + right) >> 1\n            if pivot < getitem_arr_tup(key_arrs, mid):\n                right = mid\n            else:\n                left = mid + 1\n\n        assert left == right\n\n        # The invariants still hold: pivot >= all in [lo, left) and\n        # pivot < all in [left, start), so pivot belongs at left.  Note\n        # that if there are elements equal to pivot, left points to the\n        # first slot after them -- that\'s why this sort is stable.\n        # Slide elements over to make room for pivot.\n\n        n = start - left  # The number of elements to move\n        # TODO: optimize for n==1 and n==2\n        # TODO: data\n        # FIXME: is slicing ok?\n        #key_arrs[left+1:left+1+n] = key_arrs[left:left+n]\n        copyRange_tup(key_arrs, left, key_arrs, left + 1, n)\n        copyRange_tup(data, left, data, left + 1, n)\n\n        #copyElement(pivotStore, 0, key_arrs, left)\n        setitem_arr_tup(key_arrs, left, pivot)\n        setitem_arr_tup(data, left, pivot_data)\n        start += 1\n\n\n# Returns the length of the run beginning at the specified position in\n# the specified array and reverses the run if it is descending (ensuring\n# that the run will always be ascending when the method returns).\n#\n# A run is the longest ascending sequence with:\n#\n#   a[lo] <= a[lo + 1] <= a[lo + 2] <= ...\n#\n# or the longest descending sequence with:\n#\n#   a[lo] >  a[lo + 1] >  a[lo + 2] >  ...\n#\n# For its intended use in a stable mergesort, the strictness of the\n# definition of ""descending"" is needed so that the call can safely\n# reverse a descending sequence without violating stability.\n#\n# @param a the array in which a run is to be counted and possibly reversed\n# @param lo index of the first element in the run\n# @param hi index after the last element that may be contained in the run.\n# It is required that {@code lo < hi}.\n# @param c the comparator to used for the sort\n# @return  the length of the run beginning at the specified position in\n#         the specified array\n\n@numba.njit(no_cpython_wrapper=True, cache=True)\ndef countRunAndMakeAscending(key_arrs, lo, hi, data):  # pragma: no cover\n    assert lo < hi\n    runHi = lo + 1\n    if runHi == hi:\n        return 1\n\n    # Find end of run, and reverse range if descending\n    if getitem_arr_tup(key_arrs, runHi) < getitem_arr_tup(key_arrs, lo):  # Descending\n        runHi += 1\n        while runHi < hi and getitem_arr_tup(key_arrs, runHi) < getitem_arr_tup(key_arrs, runHi - 1):\n            runHi += 1\n        reverseRange(key_arrs, lo, runHi, data)\n    else:                     # Ascending\n        runHi += 1\n        while runHi < hi and getitem_arr_tup(key_arrs, runHi) >= getitem_arr_tup(key_arrs, runHi - 1):\n            runHi += 1\n\n    return runHi - lo\n\n\n# Reverse the specified range of the specified array.\n# @param a the array in which a range is to be reversed\n# @param lo the index of the first element in the range to be reversed\n# @param hi the index after the last element in the range to be reversed\n\n@numba.njit(no_cpython_wrapper=True, cache=True)\ndef reverseRange(key_arrs, lo, hi, data):  # pragma: no cover\n    hi -= 1\n    while lo < hi:\n        # swap, TODO: copy data\n        tmp = getitem_arr_tup(key_arrs, lo)\n        setitem_arr_tup(key_arrs, lo, getitem_arr_tup(key_arrs, hi))\n        setitem_arr_tup(key_arrs, hi, tmp)\n\n        # TODO: add support for map and use it\n        swap_arrs(data, lo, hi)\n        # for arr in data:\n        #     tmp_v = arr[lo]\n        #     arr[lo] = arr[hi]\n        #     arr[hi] = tmp_v\n\n        lo += 1\n        hi -= 1\n\n\n# Returns the minimum acceptable run length for an array of the specified\n# length. Natural runs shorter than this will be extended with\n# {@link #binarySort}.\n#\n# Roughly speaking, the computation is:\n#\n# If n < MIN_MERGE, return n (it\'s too small to bother with fancy stuff).\n# Else if n is an exact power of 2, return MIN_MERGE/2.\n# Else return an k, MIN_MERGE/2 <= k <= MIN_MERGE, such that n/k\n#  is close to, but strictly less than, an exact power of 2.\n#\n# For the rationale, see listsort.txt.\n#\n# @param n the length of the array to be sorted\n# @return the length of the minimum run to be merged\n\n\n@numba.njit(no_cpython_wrapper=True, cache=True)\ndef minRunLength(n):  # pragma: no cover\n    assert n >= 0\n    r = 0      # Becomes 1 if any 1 bits are shifted off\n    while n >= MIN_MERGE:\n        r |= (n & 1)\n        n >>= 1\n\n    return n + r\n\n# When we get into galloping mode, we stay there until both runs win less\n# often than MIN_GALLOP consecutive times.\n\n\nMIN_GALLOP = 7\n\n# Maximum initial size of tmp array, which is used for merging.  The array\n# can grow to accommodate demand.\n#\n# Unlike Tim\'s original C version, we do not allocate this much storage\n# when sorting smaller arrays.  This change was required for performance.\n\nINITIAL_TMP_STORAGE_LENGTH = 256\n\n\n@numba.njit(no_cpython_wrapper=True, cache=True)\ndef init_sort_start(key_arrs, data):\n\n    # This controls when we get *into* galloping mode.  It is initialized\n    # to MIN_GALLOP.  The mergeLo and mergeHi methods nudge it higher for\n    # random data, and lower for highly structured data.\n    minGallop = MIN_GALLOP\n\n    arr_len = len(key_arrs[0])\n    # Allocate temp storage (which may be increased later if necessary)\n    tmpLength = (arr_len >> 1 if arr_len < 2 * INITIAL_TMP_STORAGE_LENGTH\n                 else INITIAL_TMP_STORAGE_LENGTH)\n    tmp = alloc_arr_tup(tmpLength, key_arrs)\n    tmp_data = alloc_arr_tup(tmpLength, data)\n\n    # A stack of pending runs yet to be merged.  Run i starts at\n    # address base[i] and extends for len[i] elements.  It\'s always\n    # true (so long as the indices are in bounds) that:\n    #\n    #    runBase[i] + runLen[i] == runBase[i + 1]\n    #\n    # so we could cut the storage for this, but it\'s a minor amount,\n    # and keeping all the info explicit simplifies the code.\n\n    # Allocate runs-to-be-merged stack (which cannot be expanded).  The\n    # stack length requirements are described in listsort.txt.  The C\n    # version always uses the same stack length (85), but this was\n    # measured to be too expensive when sorting ""mid-sized"" arrays (e.g.,\n    # 100 elements) in Java.  Therefore, we use smaller (but sufficiently\n    # large) stack lengths for smaller arrays.  The ""magic numbers"" in the\n    # computation below must be changed if MIN_MERGE is decreased.  See\n    # the MIN_MERGE declaration above for more information.\n\n    stackSize = 0  # Number of pending runs on stack\n    stackLen = 5 if arr_len < 120 else (\n        10 if arr_len < 1542 else (\n            19 if arr_len < 119151 else 40\n        ))\n    runBase = np.empty(stackLen, np.int64)\n    runLen = np.empty(stackLen, np.int64)\n\n    return stackSize, runBase, runLen, tmpLength, tmp, tmp_data, minGallop\n\n\n# Pushes the specified run onto the pending-run stack.\n\n# @param runBase index of the first element in the run\n# @param runLen  the number of elements in the run\n@numba.njit(no_cpython_wrapper=True, cache=True)\ndef pushRun(stackSize, runBase, runLen, runBase_val, runLen_val):\n    runBase[stackSize] = runBase_val\n    runLen[stackSize] = runLen_val\n    stackSize += 1\n    return stackSize\n\n# Examines the stack of runs waiting to be merged and merges adjacent runs\n# until the stack invariants are reestablished:\n\n#    1. runLen[i - 3] > runLen[i - 2] + runLen[i - 1]\n#    2. runLen[i - 2] > runLen[i - 1]\n\n# This method is called each time a new run is pushed onto the stack,\n# so the invariants are guaranteed to hold for i < stackSize upon\n# entry to the method.\n@numba.njit(no_cpython_wrapper=True, cache=True)\ndef mergeCollapse(stackSize, runBase, runLen, key_arrs, data, tmpLength, tmp,\n                  tmp_data, minGallop):\n    while stackSize > 1:\n        n = stackSize - 2\n        if ((n >= 1 and runLen[n - 1] <= runLen[n] + runLen[n + 1])\n                or (n >= 2 and runLen[n - 2] <= runLen[n] + runLen[n - 1])):\n            if runLen[n - 1] < runLen[n + 1]:\n                n -= 1\n        elif runLen[n] > runLen[n + 1]:\n            break  # Invariant is established\n\n        stackSize, tmpLength, tmp, tmp_data, minGallop = mergeAt(\n            stackSize, runBase, runLen, key_arrs, data,\n            tmpLength, tmp, tmp_data, minGallop, n)\n\n    return stackSize, tmpLength, tmp, tmp_data, minGallop\n\n# Merges all runs on the stack until only one remains.  This method is\n# called once, to complete the sort.\n@numba.njit(no_cpython_wrapper=True, cache=True)\ndef mergeForceCollapse(stackSize, runBase, runLen, key_arrs, data, tmpLength,\n                       tmp, tmp_data, minGallop):\n    while stackSize > 1:\n        n = stackSize - 2\n        if n > 0 and runLen[n - 1] < runLen[n + 1]:\n            n -= 1\n        stackSize, tmpLength, tmp, tmp_data, minGallop = mergeAt(\n            stackSize, runBase, runLen, key_arrs, data,\n            tmpLength, tmp, tmp_data, minGallop, n)\n\n    return stackSize, tmpLength, tmp, tmp_data, minGallop\n\n\n# Merges the two runs at stack indices i and i+1.  Run i must be\n# the penultimate or antepenultimate run on the stack.  In other words,\n# i must be equal to stackSize-2 or stackSize-3.\n\n# @param i stack index of the first of the two runs to merge\n@numba.njit(no_cpython_wrapper=True, cache=True)\ndef mergeAt(stackSize, runBase, runLen, key_arrs, data, tmpLength, tmp,\n            tmp_data, minGallop, i):\n    assert stackSize >= 2\n    assert i >= 0\n    assert i == stackSize - 2 or i == stackSize - 3\n\n    base1 = runBase[i]\n    len1 = runLen[i]\n    base2 = runBase[i + 1]\n    len2 = runLen[i + 1]\n    assert len1 > 0 and len2 > 0\n    assert base1 + len1 == base2\n\n    # Record the length of the combined runs. if i is the 3rd-last\n    # run now, also slide over the last run (which isn\'t involved\n    # in this merge).  The current run (i+1) goes away in any case.\n\n    runLen[i] = len1 + len2\n    if i == stackSize - 3:\n        runBase[i + 1] = runBase[i + 2]\n        runLen[i + 1] = runLen[i + 2]\n\n    stackSize -= 1\n\n    # Find where the first element of run2 goes in run1. Prior elements\n    # in run1 can be ignored (because they\'re already in place).\n\n    k = gallopRight(getitem_arr_tup(key_arrs, base2), key_arrs, base1, len1, 0)\n    assert k >= 0\n    base1 += k\n    len1 -= k\n    if len1 == 0:\n        return stackSize, tmpLength, tmp, tmp_data, minGallop\n\n    # Find where the last element of run1 goes in run2. Subsequent elements\n    # in run2 can be ignored (because they\'re already in place).\n\n    len2 = gallopLeft(getitem_arr_tup(key_arrs, base1 + len1 - 1), key_arrs, base2,\n                      len2, len2 - 1)\n    assert len2 >= 0\n    if len2 == 0:\n        return stackSize, tmpLength, tmp, tmp_data, minGallop\n\n    # Merge remaining runs, using tmp array with min(len1, len2) elements\n    if len1 <= len2:\n        tmpLength, tmp, tmp_data = ensureCapacity(\n            tmpLength, tmp, tmp_data, key_arrs, data,\n            len1)\n        minGallop = mergeLo(key_arrs, data, tmp,\n                            tmp_data, minGallop, base1, len1, base2, len2)\n    else:\n        tmpLength, tmp, tmp_data = ensureCapacity(\n            tmpLength, tmp, tmp_data, key_arrs, data,\n            len2)\n        minGallop = mergeHi(key_arrs, data, tmp,\n                            tmp_data, minGallop, base1, len1, base2, len2)\n\n    return stackSize, tmpLength, tmp, tmp_data, minGallop\n\n\n# Locates the position at which to insert the specified key into the\n# specified sorted range, if the range contains an element equal to key,\n# returns the index of the leftmost equal element.\n\n# @param key the key whose insertion point to search for\n# @param a the array in which to search\n# @param base the index of the first element in the range\n# @param len the length of the range, must be > 0\n# @param hint the index at which to begin the search, 0 <= hint < n.\n#    The closer hint is to the result, the faster this method will run.\n# @param c the comparator used to order the range, and to search\n# @return the k,  0 <= k <= n such that a[b + k - 1] < key <= a[b + k],\n#   pretending that a[b - 1] is minus infinity and a[b + n] is infinity.\n#   In other words, key belongs at index b + k, or in other words,\n#   the first k elements of a should precede key, and the last n - k\n#   should follow it.\n@numba.njit(no_cpython_wrapper=True, cache=True)\ndef gallopLeft(key, arr, base, _len, hint):\n    assert _len > 0 and hint >= 0 and hint < _len\n    lastOfs = 0\n    ofs = 1\n\n    if key > getitem_arr_tup(arr, base + hint):\n        # Gallop right until a[base+hint+lastOfs] < key <= a[base+hint+ofs]\n        maxOfs = _len - hint\n        while ofs < maxOfs and key > getitem_arr_tup(arr, base + hint + ofs):\n            lastOfs = ofs\n            ofs = (ofs << 1) + 1\n            if ofs <= 0:   # overflow\n                ofs = maxOfs\n\n        if ofs > maxOfs:\n            ofs = maxOfs\n\n        # Make offsets relative to base\n        lastOfs += hint\n        ofs += hint\n    else:  # key <= a[base + hint]\n        # Gallop left until a[base+hint-ofs] < key <= a[base+hint-lastOfs]\n        maxOfs = hint + 1\n        while ofs < maxOfs and key <= getitem_arr_tup(arr, base + hint - ofs):\n            lastOfs = ofs\n            ofs = (ofs << 1) + 1\n            if ofs <= 0:   # overflow\n                ofs = maxOfs\n\n        if ofs > maxOfs:\n            ofs = maxOfs\n\n        # Make offsets relative to base\n        tmp = lastOfs\n        lastOfs = hint - ofs\n        ofs = hint - tmp\n\n    assert -1 <= lastOfs and lastOfs < ofs and ofs <= _len\n\n    # Now a[base+lastOfs] < key <= a[base+ofs], so key belongs somewhere\n    # to the right of lastOfs but no farther right than ofs.  Do a binary\n    # search, with invariant a[base + lastOfs - 1] < key <= a[base + ofs].\n\n    lastOfs += 1\n    while lastOfs < ofs:\n        m = lastOfs + ((ofs - lastOfs) >> 1)\n\n        if key > getitem_arr_tup(arr, base + m):\n            lastOfs = m + 1  # a[base + m] < key\n        else:\n            ofs = m          # key <= a[base + m]\n\n    assert lastOfs == ofs    # so a[base + ofs - 1] < key <= a[base + ofs]\n    return ofs\n\n\n# Like gallopLeft, except that if the range contains an element equal to\n# key, gallopRight returns the index after the rightmost equal element.\n\n# @param key the key whose insertion point to search for\n# @param a the array in which to search\n# @param base the index of the first element in the range\n# @param len the length of the range must be > 0\n# @param hint the index at which to begin the search, 0 <= hint < n.\n#    The closer hint is to the result, the faster this method will run.\n# @param c the comparator used to order the range, and to search\n# @return the k,  0 <= k <= n such that a[b + k - 1] <= key < a[b + k]\n@numba.njit(no_cpython_wrapper=True, cache=True)\ndef gallopRight(key, arr, base, _len, hint):\n    assert _len > 0 and hint >= 0 and hint < _len\n\n    ofs = 1\n    lastOfs = 0\n\n    if key < getitem_arr_tup(arr, base + hint):\n        # Gallop left until a[b+hint - ofs] <= key < a[b+hint - lastOfs]\n        maxOfs = hint + 1\n        while ofs < maxOfs and key < getitem_arr_tup(arr, base + hint - ofs):\n            lastOfs = ofs\n            ofs = (ofs << 1) + 1\n            if ofs <= 0:   # overflow\n                ofs = maxOfs\n\n        if ofs > maxOfs:\n            ofs = maxOfs\n\n        # Make offsets relative to b\n        tmp = lastOfs\n        lastOfs = hint - ofs\n        ofs = hint - tmp\n    else:  # a[b + hint] <= key\n        # Gallop right until a[b+hint + lastOfs] <= key < a[b+hint + ofs]\n        maxOfs = _len - hint\n        while ofs < maxOfs and key >= getitem_arr_tup(arr, base + hint + ofs):\n            lastOfs = ofs\n            ofs = (ofs << 1) + 1\n            if ofs <= 0:   # overflow\n                ofs = maxOfs\n\n        if ofs > maxOfs:\n            ofs = maxOfs\n\n        # Make offsets relative to b\n        lastOfs += hint\n        ofs += hint\n\n    assert -1 <= lastOfs and lastOfs < ofs and ofs <= _len\n\n    # Now a[b + lastOfs] <= key < a[b + ofs], so key belongs somewhere to\n    # the right of lastOfs but no farther right than ofs.  Do a binary\n    # search, with invariant a[b + lastOfs - 1] <= key < a[b + ofs].\n\n    lastOfs += 1\n    while lastOfs < ofs:\n        m = lastOfs + ((ofs - lastOfs) >> 1)\n\n        if key < getitem_arr_tup(arr, base + m):\n            ofs = m          # key < a[b + m]\n        else:\n            lastOfs = m + 1  # a[b + m] <= key\n\n    assert lastOfs == ofs    # so a[b + ofs - 1] <= key < a[b + ofs]\n    return ofs\n\n# Merges two adjacent runs in place, in a stable fashion.  The first\n# element of the first run must be greater than the first element of the\n# second run (a[base1] > a[base2]), and the last element of the first run\n# (a[base1 + len1-1]) must be greater than all elements of the second run.\n\n# For performance, this method should be called only when len1 <= len2\n# its twin, mergeHi should be called if len1 >= len2.  (Either method\n# may be called if len1 == len2.)\n\n# @param base1 index of first element in first run to be merged\n# @param len1  length of first run to be merged (must be > 0)\n# @param base2 index of first element in second run to be merged\n#       (must be aBase + aLen)\n# @param len2  length of second run to be merged (must be > 0)\n@numba.njit(no_cpython_wrapper=True, cache=True)\ndef mergeLo(key_arrs, data, tmp, tmp_data, minGallop, base1, len1, base2, len2):\n    assert len1 > 0 and len2 > 0 and base1 + len1 == base2\n\n    # Copy first run into temp array\n    arr = key_arrs\n    arr_data = data\n    copyRange_tup(arr, base1, tmp, 0, len1)\n    #tmp[:len1] = arr[base1:base1+len1]\n    copyRange_tup(arr_data, base1, tmp_data, 0, len1)\n\n    cursor1 = 0       # Indexes into tmp array\n    cursor2 = base2   # Indexes a\n    dest = base1      # Indexes a\n\n    # Move first element of second run and deal with degenerate cases\n    # copyElement(arr, cursor2, arr, dest)\n    setitem_arr_tup(arr, dest, getitem_arr_tup(arr, cursor2))\n    copyElement_tup(arr_data, cursor2, arr_data, dest)\n\n    cursor2 += 1\n    dest += 1\n    len2 -= 1\n    if len2 == 0:\n        copyRange_tup(tmp, cursor1, arr, dest, len1)\n        copyRange_tup(tmp_data, cursor1, arr_data, dest, len1)\n        #arr[dest:dest+len1] = tmp[cursor1:cursor1+len1]\n        return minGallop\n\n    if len1 == 1:\n        copyRange_tup(arr, cursor2, arr, dest, len2)\n        copyRange_tup(arr_data, cursor2, arr_data, dest, len2)\n        copyElement_tup(tmp, cursor1, arr, dest + len2)  # Last elt of run 1 to end of merge\n        copyElement_tup(tmp_data, cursor1, arr_data, dest + len2)\n        return minGallop\n\n    # XXX *************** refactored nested break into func\n    len1, len2, cursor1, cursor2, dest, minGallop = mergeLo_inner(\n        key_arrs, data, tmp_data,\n        len1, len2, tmp, cursor1, cursor2, dest, minGallop)\n    # XXX *****************\n\n    minGallop = 1 if minGallop < 1 else minGallop  # Write back to field\n\n    if len1 == 1:\n        assert len2 > 0\n        copyRange_tup(arr, cursor2, arr, dest, len2)\n        copyRange_tup(arr_data, cursor2, arr_data, dest, len2)\n        copyElement_tup(tmp, cursor1, arr, dest + len2)  # Last elt of run 1 to end of merge\n        copyElement_tup(tmp_data, cursor1, arr_data, dest + len2)\n    elif len1 == 0:\n        raise ValueError(""Comparison method violates its general contract!"")\n    else:\n        assert len2 == 0\n        assert len1 > 1\n        copyRange_tup(tmp, cursor1, arr, dest, len1)\n        copyRange_tup(tmp_data, cursor1, arr_data, dest, len1)\n\n    return minGallop\n\n\n@numba.njit(no_cpython_wrapper=True, cache=True)\ndef mergeLo_inner(arr, arr_data, tmp_data, len1, len2, tmp, cursor1, cursor2,\n                  dest, minGallop):\n\n    while True:\n        count1 = 0  # Number of times in a row that first run won\n        count2 = 0  # Number of times in a row that second run won\n\n        # Do the straightforward thing until (if ever) one run starts\n        # winning consistently.\n\n        while True:\n            assert len1 > 1 and len2 > 0\n            if getitem_arr_tup(arr, cursor2) < getitem_arr_tup(tmp, cursor1):\n                copyElement_tup(arr, cursor2, arr, dest)\n                copyElement_tup(arr_data, cursor2, arr_data, dest)\n                cursor2 += 1\n                dest += 1\n                count2 += 1\n                count1 = 0\n                len2 -= 1\n                if len2 == 0:\n                    return len1, len2, cursor1, cursor2, dest, minGallop\n            else:\n                copyElement_tup(tmp, cursor1, arr, dest)\n                copyElement_tup(tmp_data, cursor1, arr_data, dest)\n                cursor1 += 1\n                dest += 1\n                count1 += 1\n                count2 = 0\n                len1 -= 1\n                if len1 == 1:\n                    return len1, len2, cursor1, cursor2, dest, minGallop\n\n            if not ((count1 | count2) < minGallop):\n                break\n\n        # One run is winning so consistently that galloping may be a\n        # huge win. So try that, and continue galloping until (if ever)\n        # neither run appears to be winning consistently anymore.\n\n        while True:\n            assert len1 > 1 and len2 > 0\n            count1 = gallopRight(\n                getitem_arr_tup(arr, cursor2), tmp, cursor1, len1, 0)\n            if count1 != 0:\n                copyRange_tup(tmp, cursor1, arr, dest, count1)\n                copyRange_tup(tmp_data, cursor1, arr_data, dest, count1)\n                dest += count1\n                cursor1 += count1\n                len1 -= count1\n                if len1 <= 1:  # len1 == 1 or len1 == 0\n                    return len1, len2, cursor1, cursor2, dest, minGallop\n\n            copyElement_tup(arr, cursor2, arr, dest)\n            copyElement_tup(arr_data, cursor2, arr_data, dest)\n            cursor2 += 1\n            dest += 1\n            len2 -= 1\n            if len2 == 0:\n                return len1, len2, cursor1, cursor2, dest, minGallop\n\n            count2 = gallopLeft(\n                getitem_arr_tup(tmp, cursor1), arr, cursor2, len2, 0)\n            if count2 != 0:\n                copyRange_tup(arr, cursor2, arr, dest, count2)\n                copyRange_tup(arr_data, cursor2, arr_data, dest, count2)\n                dest += count2\n                cursor2 += count2\n                len2 -= count2\n                if len2 == 0:\n                    return len1, len2, cursor1, cursor2, dest, minGallop\n\n            copyElement_tup(tmp, cursor1, arr, dest)\n            copyElement_tup(tmp_data, cursor1, arr_data, dest)\n            cursor1 += 1\n            dest += 1\n            len1 -= 1\n            if len1 == 1:\n                return len1, len2, cursor1, cursor2, dest, minGallop\n            minGallop -= 1\n\n            if not (count1 >= MIN_GALLOP | count2 >= MIN_GALLOP):\n                break\n\n        if minGallop < 0:\n            minGallop = 0\n\n        minGallop += 2  # Penalize for leaving gallop mode\n\n    return len1, len2, cursor1, cursor2, dest, minGallop\n\n\n# Like mergeLo, except that this method should be called only if\n# len1 >= len2 mergeLo should be called if len1 <= len2.  (Either method\n# may be called if len1 == len2.)\n\n# @param base1 index of first element in first run to be merged\n# @param len1  length of first run to be merged (must be > 0)\n# @param base2 index of first element in second run to be merged\n#       (must be aBase + aLen)\n# @param len2  length of second run to be merged (must be > 0)\n@numba.njit(no_cpython_wrapper=True, cache=True)\ndef mergeHi(key_arrs, data, tmp, tmp_data, minGallop, base1, len1, base2, len2):\n    assert len1 > 0 and len2 > 0 and base1 + len1 == base2\n\n    # Copy second run into temp array\n    arr = key_arrs\n    arr_data = data\n    copyRange_tup(arr, base2, tmp, 0, len2)\n    copyRange_tup(arr_data, base2, tmp_data, 0, len2)\n\n    cursor1 = base1 + len1 - 1  # Indexes into arr\n    cursor2 = len2 - 1          # Indexes into tmp array\n    dest = base2 + len2 - 1     # Indexes into arr\n\n    # Move last element of first run and deal with degenerate cases\n    copyElement_tup(arr, cursor1, arr, dest)\n    copyElement_tup(arr_data, cursor1, arr_data, dest)\n    cursor1 -= 1\n    dest -= 1\n    len1 -= 1\n    if len1 == 0:\n        copyRange_tup(tmp, 0, arr, dest - (len2 - 1), len2)\n        copyRange_tup(tmp_data, 0, arr_data, dest - (len2 - 1), len2)\n        return minGallop\n\n    if len2 == 1:\n        dest -= len1\n        cursor1 -= len1\n        copyRange_tup(arr, cursor1 + 1, arr, dest + 1, len1)\n        copyRange_tup(arr_data, cursor1 + 1, arr_data, dest + 1, len1)\n        copyElement_tup(tmp, cursor2, arr, dest)\n        copyElement_tup(tmp_data, cursor2, arr_data, dest)\n        return minGallop\n\n    # XXX *************** refactored nested break into func\n    len1, len2, tmp, cursor1, cursor2, dest, minGallop = mergeHi_inner(\n        key_arrs, data, tmp_data,\n        base1, len1, len2, tmp, cursor1, cursor2, dest, minGallop)\n    # XXX *****************\n\n    minGallop = 1 if minGallop < 1 else minGallop  # Write back to field\n\n    if len2 == 1:\n        assert len1 > 0\n        dest -= len1\n        cursor1 -= len1\n        copyRange_tup(arr, cursor1 + 1, arr, dest + 1, len1)\n        copyRange_tup(arr_data, cursor1 + 1, arr_data, dest + 1, len1)\n        copyElement_tup(tmp, cursor2, arr, dest)  # Move first elt of run2 to front of merge\n        copyElement_tup(tmp_data, cursor2, arr_data, dest)\n    elif len2 == 0:\n        raise ValueError(""Comparison method violates its general contract!"")\n    else:\n        assert len1 == 0\n        assert len2 > 0\n        copyRange_tup(tmp, 0, arr, dest - (len2 - 1), len2)\n        copyRange_tup(tmp_data, 0, arr_data, dest - (len2 - 1), len2)\n\n    return minGallop\n\n\n# XXX refactored nested loop break\n@numba.njit(no_cpython_wrapper=True, cache=True)\ndef mergeHi_inner(arr, arr_data, tmp_data, base1, len1, len2, tmp, cursor1,\n                  cursor2, dest, minGallop):\n\n    while True:\n        count1 = 0  # Number of times in a row that first run won\n        count2 = 0  # Number of times in a row that second run won\n\n        # Do the straightforward thing until (if ever) one run\n        # appears to win consistently.\n\n        while True:\n            assert len1 > 0 and len2 > 1\n            if getitem_arr_tup(tmp, cursor2) < getitem_arr_tup(arr, cursor1):\n                copyElement_tup(arr, cursor1, arr, dest)\n                copyElement_tup(arr_data, cursor1, arr_data, dest)\n                cursor1 -= 1\n                dest -= 1\n                count1 += 1\n                count2 = 0\n                len1 -= 1\n                if len1 == 0:\n                    return len1, len2, tmp, cursor1, cursor2, dest, minGallop\n            else:\n                copyElement_tup(tmp, cursor2, arr, dest)\n                copyElement_tup(tmp_data, cursor2, arr_data, dest)\n                cursor2 -= 1\n                dest -= 1\n                count2 += 1\n                count1 = 0\n                len2 -= 1\n                if len2 == 1:\n                    return len1, len2, tmp, cursor1, cursor2, dest, minGallop\n\n            if not ((count1 | count2) < minGallop):\n                break\n\n        # One run is winning so consistently that galloping may be a\n        # huge win. So try that, and continue galloping until (if ever)\n        # neither run appears to be winning consistently anymore.\n\n        while True:\n            assert len1 > 0 and len2 > 1\n            count1 = len1 - gallopRight(getitem_arr_tup(tmp, cursor2), arr,\n                                        base1, len1, len1 - 1)\n            if count1 != 0:\n                dest -= count1\n                cursor1 -= count1\n                len1 -= count1\n                copyRange_tup(arr, cursor1 + 1, arr, dest + 1, count1)\n                copyRange_tup(arr_data, cursor1 + 1, arr_data, dest + 1, count1)\n                if len1 == 0:\n                    return len1, len2, tmp, cursor1, cursor2, dest, minGallop\n\n            copyElement_tup(tmp, cursor2, arr, dest)\n            copyElement_tup(tmp_data, cursor2, arr_data, dest)\n            cursor2 -= 1\n            dest -= 1\n            len2 -= 1\n            if len2 == 1:\n                return len1, len2, tmp, cursor1, cursor2, dest, minGallop\n\n            count2 = len2 - gallopLeft(getitem_arr_tup(arr, cursor1), tmp, 0,\n                                       len2, len2 - 1)\n            if count2 != 0:\n                dest -= count2\n                cursor2 -= count2\n                len2 -= count2\n                copyRange_tup(tmp, cursor2 + 1, arr, dest + 1, count2)\n                copyRange_tup(tmp_data, cursor2 + 1, arr_data, dest + 1, count2)\n                if len2 <= 1:  # len2 == 1 or len2 == 0\n                    return len1, len2, tmp, cursor1, cursor2, dest, minGallop\n\n            copyElement_tup(arr, cursor1, arr, dest)\n            copyElement_tup(arr_data, cursor1, arr_data, dest)\n            cursor1 -= 1\n            dest -= 1\n            len1 -= 1\n            if len1 == 0:\n                return len1, len2, tmp, cursor1, cursor2, dest, minGallop\n            minGallop -= 1\n            if not (count1 >= MIN_GALLOP | count2 >= MIN_GALLOP):\n                break\n\n        if minGallop < 0:\n            minGallop = 0\n        minGallop += 2  # Penalize for leaving gallop mode\n\n    return len1, len2, tmp, cursor1, cursor2, dest, minGallop\n\n\n# Ensures that the external array tmp has at least the specified\n# number of elements, increasing its size if necessary.  The size\n# increases exponentially to ensure amortized linear time complexity.\n\n# @param minCapacity the minimum required capacity of the tmp array\n# @return tmp, whether or not it grew\n\n@numba.njit(no_cpython_wrapper=True, cache=True)\ndef ensureCapacity(tmpLength, tmp, tmp_data, key_arrs, data, minCapacity):\n    aLength = len(key_arrs[0])\n    if tmpLength < minCapacity:\n        # Compute smallest power of 2 > minCapacity\n        newSize = minCapacity\n        newSize |= newSize >> 1\n        newSize |= newSize >> 2\n        newSize |= newSize >> 4\n        newSize |= newSize >> 8\n        newSize |= newSize >> 16\n        newSize += 1\n\n        if newSize < 0:  # Not bloody likely!\n            newSize = minCapacity\n        else:\n            newSize = min(newSize, aLength >> 1)\n\n        tmp = alloc_arr_tup(newSize, key_arrs)\n        tmp_data = alloc_arr_tup(newSize, data)\n        tmpLength = newSize\n\n    return tmpLength, tmp, tmp_data\n\n\n# ***************** Utils *************\n\n\ndef swap_arrs(data, lo, hi):  # pragma: no cover\n    for arr in data:\n        tmp_v = arr[lo]\n        arr[lo] = arr[hi]\n        arr[hi] = tmp_v\n\n\n@overload(swap_arrs)\ndef swap_arrs_overload(arr_tup, lo, hi):\n    count = arr_tup.count\n\n    func_text = ""def f(arr_tup, lo, hi):\\n""\n    for i in range(count):\n        func_text += ""  tmp_v_{} = arr_tup[{}][lo]\\n"".format(i, i)\n        func_text += ""  arr_tup[{}][lo] = arr_tup[{}][hi]\\n"".format(i, i)\n        func_text += ""  arr_tup[{}][hi] = tmp_v_{}\\n"".format(i, i)\n    func_text += ""  return\\n""\n\n    loc_vars = {}\n    exec(func_text, {}, loc_vars)\n    swap_impl = loc_vars[\'f\']\n    return swap_impl\n\n\n@numba.njit(no_cpython_wrapper=True, cache=True)\ndef copyRange(src_arr, src_pos, dst_arr, dst_pos, n):  # pragma: no cover\n    dst_arr[dst_pos:dst_pos + n] = src_arr[src_pos:src_pos + n]\n\n\ndef copyRange_tup(src_arr_tup, src_pos, dst_arr_tup, dst_pos, n):  # pragma: no cover\n    for src_arr, dst_arr in zip(src_arr_tup, dst_arr_tup):\n        dst_arr[dst_pos:dst_pos + n] = src_arr[src_pos:src_pos + n]\n\n\n@overload(copyRange_tup)\ndef copyRange_tup_overload(src_arr_tup, src_pos, dst_arr_tup, dst_pos, n):\n    count = src_arr_tup.count\n    assert count == dst_arr_tup.count\n\n    func_text = ""def f(src_arr_tup, src_pos, dst_arr_tup, dst_pos, n):\\n""\n    for i in range(count):\n        func_text += ""  copyRange(src_arr_tup[{}], src_pos, dst_arr_tup[{}], dst_pos, n)\\n"".format(i, i)\n    func_text += ""  return\\n""\n\n    loc_vars = {}\n    exec(func_text, {\'copyRange\': copyRange}, loc_vars)\n    copy_impl = loc_vars[\'f\']\n    return copy_impl\n\n\n@numba.njit(no_cpython_wrapper=True, cache=True)\ndef copyElement(src_arr, src_pos, dst_arr, dst_pos):  # pragma: no cover\n    dst_arr[dst_pos] = src_arr[src_pos]\n\n\ndef copyElement_tup(src_arr_tup, src_pos, dst_arr_tup, dst_pos):  # pragma: no cover\n    for src_arr, dst_arr in zip(src_arr_tup, dst_arr_tup):\n        dst_arr[dst_pos] = src_arr[src_pos]\n\n\n@overload(copyElement_tup)\ndef copyElement_tup_overload(src_arr_tup, src_pos, dst_arr_tup, dst_pos):\n    count = src_arr_tup.count\n    assert count == dst_arr_tup.count\n\n    func_text = ""def f(src_arr_tup, src_pos, dst_arr_tup, dst_pos):\\n""\n    for i in range(count):\n        func_text += ""  copyElement(src_arr_tup[{}], src_pos, dst_arr_tup[{}], dst_pos)\\n"".format(i, i)\n    func_text += ""  return\\n""\n\n    loc_vars = {}\n    exec(func_text, {\'copyElement\': copyElement}, loc_vars)\n    copy_impl = loc_vars[\'f\']\n    return copy_impl\n\n\ndef getitem_arr_tup(arr_tup, ind):  # pragma: no cover\n    result = [arr[ind] for arr in arr_tup]\n    return tuple(result)\n\n\n@overload(getitem_arr_tup)\ndef getitem_arr_tup_overload(arr_tup, ind):\n    count = arr_tup.count\n\n    func_text = ""def f(arr_tup, ind):\\n""\n    func_text += ""  return ({}{})\\n"".format(\n        \',\'.join([""arr_tup[{}][ind]"".format(i) for i in range(count)]),\n        "","" if count == 1 else """")  # single value needs comma to become tuple\n\n    loc_vars = {}\n    exec(func_text, {}, loc_vars)\n    impl = loc_vars[\'f\']\n    return impl\n\n\ndef setitem_arr_tup(arr_tup, ind, val_tup):  # pragma: no cover\n    for arr, val in zip(arr_tup, val_tup):\n        arr[ind] = val\n\n\n@overload(setitem_arr_tup)\ndef setitem_arr_tup_overload(arr_tup, ind, val_tup):\n    count = arr_tup.count\n\n    func_text = ""def f(arr_tup, ind, val_tup):\\n""\n    for i in range(count):\n        func_text += ""  arr_tup[{}][ind] = val_tup[{}]\\n"".format(i, i)\n    func_text += ""  return\\n""\n\n    loc_vars = {}\n    exec(func_text, {}, loc_vars)\n    impl = loc_vars[\'f\']\n    return impl\n\n\ndef test():  # pragma: no cover\n    import time\n    # warm up\n    t1 = time.time()\n    T = np.ones(3)\n    data = (np.arange(3), np.ones(3),)\n    sort((T,), 0, 3, data)\n    print(""compile time"", time.time() - t1)\n    n = 210000\n    np.random.seed(2)\n    data = (np.arange(n), np.random.ranf(n))\n    A = np.random.ranf(n)\n    df = pd.DataFrame({\'A\': A, \'B\': data[0], \'C\': data[1]})\n    t1 = time.time()\n    #B = np.sort(A)\n    df2 = df.sort_values(\'A\', inplace=False)\n    t2 = time.time()\n    sort((A,), 0, n, data)\n    print(""SDC"", time.time() - t2, ""Numpy"", t2 - t1)\n    # print(df2.B)\n    # print(data)\n    np.testing.assert_almost_equal(data[0], df2.B.values)\n    np.testing.assert_almost_equal(data[1], df2.C.values)\n\n\nif __name__ == \'__main__\':  # pragma: no cover\n    test()\n'"
sdc/types.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nfrom .datatypes.categorical.types import *\nfrom .datatypes.series.types import *\n'"
docs/source/conf.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\n# coding: utf-8\n# Configuration file for the Sphinx documentation builder.\n#\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\n# import os\n# import sys\n# sys.path.insert(0, os.path.abspath(\'.\'))\n\n\n# -- Import sdc package to build API Reference -------------------------------\nimport os\nimport sys\nimport shutil\n\nSDC_DOC_NO_API_REF_STR = \'SDC_DOC_NO_API_REF\'\nSDC_DOC_APIREF_DIR = \'_api_ref\'\n\nsys.path.insert(0, os.path.relpath(\'buildscripts\'))\nsdc_doc_no_api_ref = False  # Generate API Reference by default\n\nif SDC_DOC_NO_API_REF_STR in os.environ:\n    sdc_doc_no_api_ref = os.environ[SDC_DOC_NO_API_REF_STR] == \'1\'\n\nif not sdc_doc_no_api_ref:\n    if os.path.exists(SDC_DOC_APIREF_DIR):\n        shutil.rmtree(SDC_DOC_APIREF_DIR)\n\n    try:\n        import sdc\n    except ImportError:\n        raise ImportError(\'Cannot import sdc.\\n\'\n                          \'Documentation generator for API Reference for a given module expects that module \'\n                          \'to be installed. Use conda/pip install SDC to install it prior to using API Reference \'\n                          \'generation. If you want to disable API Reference generation, set the environment \'\n                          \'variable SDC_DOC_NO_API_REF=1\')\n\n    try:\n        from apiref_generator import generate_api_reference\n    except ImportError:\n        raise ImportError(\'Cannot import apiref_generator\', os.getcwd())\n\n    generate_api_reference()\n\nSDC_DOC_NO_EXAMPLES_STR = \'SDC_DOC_NO_EXAMPLES\'\nSDC_DOC_EXAMPLES_DIR = \'_examples\'\n\nsdc_doc_no_examples = False  # Generate examples list by default\nif SDC_DOC_NO_EXAMPLES_STR in os.environ:\n    sdc_doc_no_examples = os.environ[SDC_DOC_NO_EXAMPLES_STR] == \'1\'\n\nif not sdc_doc_no_examples:\n    if os.path.exists(SDC_DOC_EXAMPLES_DIR):\n        shutil.rmtree(SDC_DOC_EXAMPLES_DIR)\n\n    try:\n        import sdc\n    except ImportError:\n        raise ImportError(\'Cannot import sdc.\\n\'\n                          \'Documentation generator for Examples for a given module expects that module \'\n                          \'to be installed. Use conda/pip install SDC to install it prior to using API Examples \'\n                          \'generation. If you want to disable Examples generation, set the environment \'\n                          \'variable SDC_DOC_NO_EXAMPLES_STR=1\')\n\n    try:\n        from examples_generator import generate_examples\n    except ImportError:\n        raise ImportError(\'Cannot import examples_generator\', os.getcwd())\n\n    generate_examples()\n\n# -- Project information -----------------------------------------------------\n\nproject = \'Intel\xc2\xae Scalable Dataframe Compiler\'\ncopyright = \'2019-2020, Intel Corporation\'\nauthor = \'Intel Corporation\'\n\n# The full version, including alpha/beta/rc tags\nrelease = \'0.1\'\n\n\n# -- General configuration ----------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.todo\',\n    \'sphinx.ext.intersphinx\',\n    \'sphinx.ext.extlinks\',\n    \'sphinx.ext.githubpages\',\n    \'sphinx.ext.napoleon\',\n    \'sphinxcontrib.programoutput\',\n]\n\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = []\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sdc-sphinx-theme\'\n\nhtml_theme_path = [\'.\']\n\nhtml_theme_options = {\n}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = []\n\nhtml_sidebars = {\n   \'**\': [\'globaltoc.html\', \'sourcelink.html\', \'searchbox.html\', \'relations.html\'],\n }\n\nhtml_show_sourcelink = False\n\n# -- Todo extension configuration  ----------------------------------------------\ntodo_include_todos = True\ntodo_link_only = True\n\n# -- InterSphinx configuration: looks for objects in external projects -----\n# Add here external classes you want to link from Intel SDC documentation\n# Each entry of the dictionary has the following format:\n#      \'class name\': (\'link to object.inv file for that class\', None)\nintersphinx_mapping = {\n    \'pandas\': (\'https://pandas.pydata.org/pandas-docs/stable/\', None),\n    \'python\': (\'http://docs.python.org/2\', None),\n    \'numpy\': (\'http://docs.scipy.org/doc/numpy\', None)\n}\n\n# -- Napoleon extension configuration (Numpy and Google docstring options) -------\nnapoleon_google_docstring = True\nnapoleon_numpy_docstring = True\nnapoleon_include_init_with_doc = True\nnapoleon_include_private_with_doc = True\nnapoleon_include_special_with_doc = True\nnapoleon_use_admonition_for_examples = False\nnapoleon_use_admonition_for_notes = False\nnapoleon_use_admonition_for_references = False\nnapoleon_use_ivar = False\nnapoleon_use_param = True\nnapoleon_use_rtype = True\n\n# -- Prepend module name to an object name or not -----------------------------------\nadd_module_names = False\n'"
examples/dataframe/dataframe_append.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\n    Expected result:\n         A  B    C\n    0  1.0  3  NaN\n    1  2.0  4  NaN\n    2  NaN  5  7.0\n    3  NaN  6  8.0\n\n""""""\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef dataframe_append():\n    df = pd.DataFrame({\'A\': [1, 2], \'B\': [3, 4]})\n    df2 = pd.DataFrame({\'B\': [5, 6], \'C\': [7, 8]})\n    result = df.append(df2)\n\n    return result\n\n\nprint(dataframe_append())\n'"
examples/dataframe/dataframe_at.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef dataframe_at():\n    df = pd.DataFrame({\'A\': [1.0, 2.0, 3.0, 1.0], \'B\': [4, 5, 6, 7], \'C\': [\'a\', \'b\', \'c\', \'d\']})\n\n    return df.at[1, \'C\']  # [\'b\']\n\n\nprint(dataframe_at())\n'"
examples/dataframe/dataframe_copy.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\n   Expected new DataFrame:\n        A  B\n    0  1.0  4\n    1  2.0  5\n    2  3.0  6\n    3  1.0  7\n""""""\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef dataframe_copy():\n    df = pd.DataFrame({\'A\': [1.0, 2.0, 3.0, 1.0], \'B\': [4, 5, 6, 7]})\n    new_df = df.copy(deep=True)\n    return new_df\n\n\nprint(dataframe_copy())\n'"
examples/dataframe/dataframe_count.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n# result\n# A    4\n# B    4\n# C    3\n# dtype: int64\n\nimport pandas as pd\nimport numpy as np\nfrom numba import njit\n\n\n@njit\ndef dataframe_count():\n    df = pd.DataFrame({""A"": [.2, .0, .6, .2],\n                       ""B"": [2, 0, 6, 2],\n                       ""C"": [-1, np.nan, 1, np.inf]})\n\n    return df.count()\n\n\nprint(dataframe_count())\n'"
examples/dataframe/dataframe_drop.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\n   Expected result:\n       B  C\n   0  4  a\n   1  5  b\n   2  6  c\n   3  7  d\n   dtype: object\n""""""\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef dataframe_drop():\n    df = pd.DataFrame({\'A\': [1.0, 2.0, 3.0, 1.0], \'B\': [4, 5, 6, 7], \'C\': [\'a\', \'b\', \'c\', \'d\']})\n\n    return df.drop(columns=\'A\')\n\n\nprint(dataframe_drop())\n'"
examples/dataframe/dataframe_head.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\n   Expected result:\n          animal\n    0  alligator\n    1        bee\n    2     falcon\n    3       lion\n    4     monkey\n    5     parrot\n""""""\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef dataframe_head():\n    df = pd.DataFrame({\'animal\': [\'alligator\', \'bee\', \'falcon\', \'lion\',\n                                  \'monkey\', \'parrot\', \'shark\', \'whale\', \'zebra\']})\n\n    return df.head(n=6)\n\n\nprint(dataframe_head())\n'"
examples/dataframe/dataframe_iat.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef dataframe_iat():\n    df = pd.DataFrame({\'A\': [1.0, 2.0, 3.0, 1.0], \'B\': [4, 5, 6, 7], \'C\': [\'a\', \'b\', \'c\', \'d\']})\n\n    return df.iat[1, 2]  # value b\n\n\nprint(dataframe_iat())\n'"
examples/dataframe/dataframe_iloc.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\n""""""\n   Expected result:\n    A    2.0\n    B    5.0\n    Name: 1, dtype: float64\n""""""\n\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef dataframe_iloc():\n    df = pd.DataFrame({\'A\': [1.0, 2.0, 3.0, 1.0], \'B\': [4, 5, 6, 7]})\n\n    return df.iloc[1]\n\n\nprint(dataframe_iloc())\n'"
examples/dataframe/dataframe_index.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef dataframe_index():\n    df = pd.DataFrame({\'A\': [1, 2], \'B\': [3, 4]}, index=[\'a\', \'b\'])\n    result = df.index\n\n    return result  # Numpy array of index values [\'a\', \'b\']\n\n\nprint(dataframe_index())\n'"
examples/dataframe/dataframe_isna.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\n   Expected result:\n        A      B      C\n    0  False  False   True\n    1   True  False  False\n    2  False  False  False\n    3  False  False  False\n""""""\n\nimport pandas as pd\nimport numpy as np\nfrom numba import njit\n\n\n@njit\ndef dataframe_isna():\n    df = pd.DataFrame({\'A\': [1.0, np.nan, 3.0, 1.0], \'B\': [4, 5, 6, 7], \'C\': [None, \'b\', \'c\', \'d\']})\n\n    return df.isna()\n\n\nprint(dataframe_isna())\n'"
examples/dataframe/dataframe_loc.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\n""""""\n   Expected result:\n        A  B  C\n    2  3.0  6  2\n""""""\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef dataframe_loc():\n    df = pd.DataFrame({\'A\': [1.0, 2.0, 3.0, 1.0], \'B\': [4, 5, 6, 7], \'C\': [4, 5, 2, 1]})\n\n    return df.loc[2]\n\n\nprint(dataframe_loc())\n'"
examples/dataframe/dataframe_max.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\n   Expected result:\n    A    0.6\n    B    6.0\n    C    inf\n    dtype: float64\n""""""\n\nimport pandas as pd\nimport numpy as np\nfrom numba import njit\n\n\n@njit\ndef dataframe_max():\n    df = pd.DataFrame({""A"": [.2, .0, .6, .2],\n                       ""B"": [2, 0, 6, 2],\n                       ""C"": [-1, np.nan, 1, np.inf]})\n\n    return df.max()\n\n\nprint(dataframe_max())\n'"
examples/dataframe/dataframe_mean.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\n   Expected result:\n    A    0.25\n    B    2.50\n    C     inf\n    dtype: float64\n""""""\n\nimport pandas as pd\nimport numpy as np\nfrom numba import njit\n\n\n@njit\ndef dataframe_mean():\n    df = pd.DataFrame({""A"": [.2, .0, .6, .2],\n                       ""B"": [2, 0, 6, 2],\n                       ""C"": [-1, np.nan, 1, np.inf]})\n\n    return df.mean()\n\n\nprint(dataframe_mean())\n'"
examples/dataframe/dataframe_median.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\n   Expected result:\n    A    0.2\n    B    2.0\n    C    1.0\n    dtype: float64\n""""""\n\nimport pandas as pd\nimport numpy as np\nfrom numba import njit\n\n\n@njit\ndef dataframe_median():\n    df = pd.DataFrame({""A"": [.2, .0, .6, .2],\n                       ""B"": [2, 0, 6, 2],\n                       ""C"": [-1, np.nan, 1, np.inf]})\n\n    return df.median()\n\n\nprint(dataframe_median())\n'"
examples/dataframe/dataframe_min.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\n   Expected result:\n    A    0.0\n    B    0.0\n    C   -1.0\n    dtype: float64\n""""""\n\nimport pandas as pd\nimport numpy as np\nfrom numba import njit\n\n\n@njit\ndef dataframe_min():\n    df = pd.DataFrame({""A"": [.2, .0, .6, .2],\n                       ""B"": [2, 0, 6, 2],\n                       ""C"": [-1, np.nan, 1, np.inf]})\n\n    return df.min()\n\n\nprint(dataframe_min())\n'"
examples/dataframe/dataframe_pct_change.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n# result DataFrame\n#            A          B         C         D\n# 0        NaN        NaN       NaN       NaN\n# 1  -0.714286  -0.600000  0.000000 -0.785714\n# 2   0.250000  26.000000 -0.650000  1.000000\n# 3  -0.200000  -0.944444  2.000000 -0.666667\n# 4  -0.750000  -0.333333 -0.619048  2.000000\n# 5  54.000000  15.000000 -0.375000 -0.333333\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef dataframe_pct_change():\n    df = pd.DataFrame({""A"": [14, 4, 5, 4, 1, 55],\n                       ""B"": [5, 2, 54, 3, 2, 32],\n                       ""C"": [20, 20, 7, 21, 8, 5],\n                       ""D"": [14, 3, 6, 2, 6, 4]})\n    out_df = df.pct_change()\n\n    return out_df\n\n\nprint(dataframe_pct_change())\n'"
examples/dataframe/dataframe_prod.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\n   Expected result:\n    A    0.0\n    B    0.0\n    C   -inf\n    dtype: float64\n""""""\n\nimport pandas as pd\nimport numpy as np\nfrom numba import njit\n\n\n@njit\ndef dataframe_prod():\n    df = pd.DataFrame({""A"": [.2, .0, .6, .2],\n                       ""B"": [2, 0, 6, 2],\n                       ""C"": [-1, np.nan, 1, np.inf]})\n\n    return df.prod()\n\n\nprint(dataframe_prod())\n'"
examples/dataframe/dataframe_reset_index_drop_False.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n# result DataFrame\n#    index   A\n# 0      5  14\n# 1      2   4\n# 2    -11   5\n# 3      0   4\n# 4     13   1\n# 5      9  55\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef dataframe_reset_index():\n    df = pd.DataFrame({""A"": [14, 4, 5, 4, 1, 55]}, index=[5, 2, -11, 0, 13, 9])\n\n    return df.reset_index(drop=False)\n\n\nprint(dataframe_reset_index())\n'"
examples/dataframe/dataframe_reset_index_drop_True.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n# result DataFrame\n#     A\n# 0  14\n# 1   4\n# 2   5\n# 3   4\n# 4   1\n# 5  55\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef dataframe_reset_index():\n    df = pd.DataFrame({""A"": [14, 4, 5, 4, 1, 55]}, index=[5, 2, -11, 0, 13, 9])\n\n    return df.reset_index(drop=True)\n\n\nprint(dataframe_reset_index())\n'"
examples/dataframe/dataframe_rolling_sum.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef df_rolling_sum():\n    df = pd.DataFrame({\'A\': [4, 3, 5, 2, 6], \'B\': [-4, -3, -5, -2, -6]})\n    out_df = df.rolling(3).sum()\n\n    # Expect DataFrame of\n    # {\'A\': [NaN, NaN, 12.0, 10.0, 13.0], \'B\': [NaN, NaN, -12.0, -10.0, -13.0]}\n    return out_df\n\n\nprint(df_rolling_sum())\n'"
examples/dataframe/dataframe_std.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\n   Expected result:\n    A    0.251661\n    B    2.516611\n    C         NaN\n    dtype: float64\n""""""\n\nimport pandas as pd\nimport numpy as np\nfrom numba import njit\n\n\n@njit\ndef dataframe_std():\n    df = pd.DataFrame({""A"": [.2, .0, .6, .2],\n                       ""B"": [2, 0, 6, 2],\n                       ""C"": [-1, np.nan, 1, np.inf]})\n\n    return df.std()\n\n\nprint(dataframe_std())\n'"
examples/dataframe/dataframe_sum.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\n   Expected result:\n    A     1.0\n    B    10.0\n    C     inf\n    dtype: float64\n""""""\n\nimport pandas as pd\nimport numpy as np\nfrom numba import njit\n\n\n@njit\ndef dataframe_sum():\n    df = pd.DataFrame({""A"": [.2, .0, .6, .2],\n                       ""B"": [2, 0, 6, 2],\n                       ""C"": [-1, np.nan, 1, np.inf]})\n\n    return df.sum()\n\n\nprint(dataframe_sum())\n'"
examples/dataframe/dataframe_values.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef dataframe_values():\n    df = pd.DataFrame({\'age\': [3,  29], \'height\': [94, 170], \'weight\': [31, 115]})\n    result = df.values\n\n    return result  # Numpy array of dataframe values: array([[3, 94, 31], [29, 170, 115]], dtype=int64)\n\n\nprint(dataframe_values())\n'"
examples/dataframe/dataframe_var.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\n   Expected result:\n    A    0.063333\n    B    6.333333\n    C         NaN\n    dtype: float64\n""""""\n\nimport pandas as pd\nimport numpy as np\nfrom numba import njit\n\n\n@njit\ndef dataframe_var():\n    df = pd.DataFrame({""A"": [.2, .0, .6, .2],\n                       ""B"": [2, 0, 6, 2],\n                       ""C"": [-1, np.nan, 1, np.inf]})\n\n    return df.var()\n\n\nprint(dataframe_var())\n'"
examples/series/series_T.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_T():\n    series = pd.Series(np.arange(5))\n\n    return series.T  # Expect array of 0, 1, 2, 3, 4\n\n\nprint(series_T())\n'"
examples/series/series_abs.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_abs():\n    s = pd.Series([-1.10, 2, -3.33])\n    out_series = s.abs()\n\n    return out_series  # Expect series of 1.10, 2.00, 3.33\n\n\nprint(series_abs())\n'"
examples/series/series_add.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_add():\n    s1 = pd.Series([1, 2, 3])\n    s2 = pd.Series([4, 5, 6])\n    out_series = s1.add(s2)\n\n    return out_series  # Expect series of 5, 7, 9\n\n\nprint(series_add())\n'"
examples/series/series_append.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\nExpect Series\n0      one\n1      two\n2    three\n0     four\n1     five\n2      six\ndtype: object\n""""""\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_append():\n    s1 = pd.Series([\'one\', \'two\', \'three\'])\n    s2 = pd.Series([\'four\', \'five\', \'six\'])\n\n    return s1.append(s2)\n\n\nprint(series_append())\n'"
examples/series/series_apply.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n# Expected:\n# London      400\n# New York    441\n# Helsinki    144\n# dtype: int64\n\nimport pandas as pd\nimport numpy as np\nfrom numba import njit\n\n\n@njit\ndef series_apply():\n    s = pd.Series([20, 21, 12],\n                  index=[\'London\', \'New York\', \'Helsinki\'])\n\n    def square(x):\n        return x ** 2\n\n    return s.apply(square)\n\n\nprint(series_apply())\n'"
examples/series/series_apply_lambda.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n# Expected:\n# London      400\n# New York    441\n# Helsinki    144\n# dtype: int64\n\nimport pandas as pd\nimport numpy as np\nfrom numba import njit\n\n\n@njit\ndef series_apply():\n    s = pd.Series([20, 21, 12],\n                  index=[\'London\', \'New York\', \'Helsinki\'])\n\n    return s.apply(lambda x: x ** 2)\n\n\nprint(series_apply())\n'"
examples/series/series_apply_log.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n# Expected:\n# London      2.995732\n# New York    3.044522\n# Helsinki    2.484907\n# dtype: float64\n\nimport pandas as pd\nimport numpy as np\nfrom numba import njit\n\n\n@njit\ndef series_apply():\n    s = pd.Series([20, 21, 12],\n                  index=[\'London\', \'New York\', \'Helsinki\'])\n\n    return s.apply(np.log)\n\n\nprint(series_apply())\n'"
examples/series/series_argsort.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_argsort():\n    s = pd.Series([3, -10, np.nan, 0, 92])\n\n    return s.argsort()  # Expect series of 1, 2, -1, 0, 3\n\n\nprint(series_argsort())\n'"
examples/series/series_astype.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\nExpect Series\n0      3.000000\n1    -10.000000\n2           nan\n3      0.000000\n4     92.000000\ndtype: object\n""""""\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_astype():\n    series = pd.Series([3, -10, np.nan, 0, 92])\n\n    return series.astype(str)\n\n\nprint(series_astype())\n'"
examples/series/series_copy.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_copy():\n    s1 = pd.Series(np.arange(5))\n    s2 = s1.copy()\n\n    return s2  # Expect new series of 0, 1, 2, 3, 4\n\n\nprint(series_copy())\n'"
examples/series/series_corr.py,2,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_corr():\n    s1 = pd.Series([3.2, -10, np.nan, 0.23, 9.2])\n    s2 = pd.Series([5., 0, 3.3, np.nan, 9.2])\n\n    return s1.corr(s2)  # Expect value: 0.98673...\n\n\nprint(series_corr())\n'"
examples/series/series_count.py,1,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nimport numpy as np\nfrom numba import njit\n\n\n@njit\ndef series_count():\n    s = pd.Series([1, 2, np.nan])\n    out_series = s.count()\n\n    return out_series  # Expect the number of non-Nan values == \'2\'\n\n\nprint(series_count())\n'"
examples/series/series_cov.py,2,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_cov():\n    s1 = pd.Series([3.2, -10, np.nan, 0.23, 9.2])\n    s2 = pd.Series([5., 0, 3.3, np.nan, 9.2])\n\n    return s1.cov(s2)  # Expect value: 44.639...\n\n\nprint(series_cov())\n'"
examples/series/series_cumsum.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_cumsum():\n    s = pd.Series([1, 2, 3, 4])\n\n    return s.cumsum()  # Expect series of 1, 3, 6, 10\n\n\nprint(series_cumsum())\n'"
examples/series/series_describe.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\nExpect Series\ncount    5.000000\nmean     4.380000\nstd      3.315419\nmin      0.000000\n25%      3.300000\n50%      4.400000\n75%      5.000000\nmax      9.200000\ndtype: float64\n""""""\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_describe():\n    s = pd.Series([5., 0, 3.3, 4.4, 9.2])\n\n    return s.describe()\n\n\nprint(series_describe())\n'"
examples/series/series_div.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_div():\n    s1 = pd.Series([1, 2, 4])\n    s2 = pd.Series([4, 4, 16])\n    out_series = s1.div(s2)\n\n    return out_series  # Expect series of 0.25, 0.50, 0.25\n\n\nprint(series_div())\n'"
examples/series/series_dropna.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\nExpect Series\n0    4.0\n2    2.0\n3    1.0\ndtype: float64\n""""""\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_dropna():\n    s = pd.Series([4, np.nan, 2, 1])\n\n    return s.dropna()\n\n\nprint(series_dropna())\n'"
examples/series/series_eq.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_eq():\n    s1 = pd.Series([5, 4, 3, 2, 1])\n    s2 = pd.Series([0, 2, 3, 6, 8])\n\n    return s1.eq(s2)  # Expect series of False, False, True, False, False\n\n\nprint(series_eq())\n'"
examples/series/series_fillna.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\nExpect Series\n0    4.0\n1    0.0\n2    2.0\n3    1.0\ndtype: float64\n""""""\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_fillna():\n    s = pd.Series([4, np.nan, 2, 1])\n\n    return s.fillna(0)\n\n\nprint(series_fillna())\n'"
examples/series/series_floordiv.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_floordiv():\n    s1 = pd.Series([5, 4, 3, 2, 1])\n    s2 = pd.Series([0, 2, 3, 6, 8])\n\n    return s1.floordiv(s2)  # Expect series of 0, 2, 1, 0, 0\n\n\nprint(series_floordiv())\n'"
examples/series/series_ge.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_ge():\n    s1 = pd.Series([5, 4, 3, 2, 1])\n    s2 = pd.Series([0, 2, 3, 6, 8])\n\n    return s1.ge(s2)  # Expect series of True, True, True, False, False\n\n\nprint(series_ge())\n'"
examples/series/series_groupby.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\nExpect Series\na    210.0\nb    185.0\ndtype: float64\n""""""\nimport pandas as pd\nimport numpy as np\nfrom numba import njit\n\n\n@njit\ndef series_groupby():\n    S = pd.Series([390., 350., 30., 20.])\n    by = np.asarray([0, 1, 0, 1])\n\n    # Expect Series of pd.Series([210.0, 185.0], index=[0, 1])\n    return S.groupby(by).mean()\n\n\nprint(series_groupby())\n'"
examples/series/series_gt.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_gt():\n    s1 = pd.Series([5, 4, 3, 2, 1])\n    s2 = pd.Series([0, 2, 3, 6, 8])\n\n    return s1.gt(s2)  # Expect series of True, True, False, False, False\n\n\nprint(series_gt())\n'"
examples/series/series_head.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\nExpected Series\n0    7\n2    6\n4    5\ndtype: int64\n""""""\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_head():\n    s = pd.Series([7, 6, 5, 4, 3, 2, 1], index=[0, 2, 4, 6, 8, 10, 12])\n\n    return s.head(3)\n\n\nprint(series_head())\n'"
examples/series/series_iat.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\nfrom numba import njit\n\n\n@njit\ndef series_iat():\n    series = pd.Series([5, 4, 3, 2, 1], index=[0, 2, 4, 6, 8])\n\n    return series.iat[4]  # Expect value: 1\n\n\nprint(series_iat())\n'"
examples/series/series_idxmax.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_idxmax():\n    s = pd.Series([4, np.nan, 2, 1], index=[\'A\', \'B\', \'C\', \'D\'])\n\n    return s.idxmax()  # Expect index of maximum value A\n\n\nprint(series_idxmax())\n'"
examples/series/series_idxmin.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_idxmin():\n    s = pd.Series([4, np.nan, 2, 1], index=[\'A\', \'B\', \'C\', \'D\'])\n\n    return s.idxmin()  # Expect index of minimum value D\n\n\nprint(series_idxmin())\n'"
examples/series/series_index.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_index():\n    series = pd.Series(np.arange(5), index=[\'one\', \'two\', \'three\', \'four\', \'five\'])\n\n    return series.index  # Expect array of \'one\' \'two\' \'three\' \'four\' \'five\'\n\n\nprint(series_index())\n'"
examples/series/series_isin.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_isin():\n    s = pd.Series([4, np.nan, 2, 1])\n\n    return s.isin([4, 1])  # Expect series of True, False, False, True\n\n\nprint(series_isin())\n'"
examples/series/series_isna.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_isna():\n    s = pd.Series([4, np.nan, 2, 1])\n\n    return s.isna()  # Expect series of False, True, False, False\n\n\nprint(series_isna())\n'"
examples/series/series_isnull.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_isnull():\n    s = pd.Series([4, np.nan, 2, 1])\n\n    return s.isnull()  # Expect series of False, True, False, False\n\n\nprint(series_isnull())\n'"
examples/series/series_le.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_le():\n    s1 = pd.Series([5, 4, 3, 2, 1])\n    s2 = pd.Series([0, 2, 3, 6, 8])\n\n    return s1.le(s2)  # Expect series of False, False, True, True, True\n\n\nprint(series_le())\n'"
examples/series/series_lt.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_lt():\n    s1 = pd.Series([5, 4, 3, 2, 1])\n    s2 = pd.Series([0, 2, 3, 6, 8])\n\n    return s1.lt(s2)  # Expect series of False, False, False, True, True\n\n\nprint(series_lt())\n'"
examples/series/series_map.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""Expected:\n0     1.0\n1     4.0\n2     9.0\n3    16.0\n4    25.0\ndtype: float64\n""""""\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_map():\n    s = pd.Series([1., 2., 3., 4., 5.])\n    return s.map(lambda x: x ** 2)\n\n\nprint(series_map())\n'"
examples/series/series_max.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_max():\n    s = pd.Series([1, 4, 2, 0])\n    out_series = s.max()\n\n    return out_series  # Expect maximum value 4\n\n\nprint(series_max())\n'"
examples/series/series_mean.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_mean():\n    series = pd.Series([3.2, -10, np.nan, 0.23, 9.2])\n\n    return series.mean()  # Expect value: 0.6575\n\n\nprint(series_mean())\n'"
examples/series/series_median.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_median():\n    series = pd.Series([1, 2, 3, 4])\n\n    return series.median()  # Expect value: 2.5\n\n\nprint(series_median())\n'"
examples/series/series_min.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_min():\n    series = pd.Series([4, np.nan, 2, 1])\n\n    return series.min()  # Expect minimum value 1.0\n\n\nprint(series_min())\n'"
examples/series/series_mod.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_mod():\n    s1 = pd.Series([5, 4, 3, 2, 1])\n    s2 = pd.Series([0, 2, 3, 6, 8])\n\n    return s1.mod(s2)  # Expect series of 0, 0, 0, 2, 1\n\n\nprint(series_mod())\n'"
examples/series/series_mul.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_mul():\n    s1 = pd.Series([1, 3, 100])\n    s2 = pd.Series([0, 1, 2])\n    out_series = s1.mul(s2)\n\n    return out_series  # Expect series of 0, 3, 200\n\n\nprint(series_mul())\n'"
examples/series/series_ndim.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_ndim():\n    series = pd.Series(np.arange(10))\n\n    return series.ndim  # Expect value: 1\n\n\nprint(series_ndim())\n'"
examples/series/series_ne.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_ne():\n    s1 = pd.Series([5, 4, 3, 2, 1])\n    s2 = pd.Series([0, 2, 3, 6, 8])\n\n    return s1.ne(s2)  # Expect series of True, True, False, True, True\n\n\nprint(series_ne())\n'"
examples/series/series_nlargest.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_nlargest():\n    series = pd.Series(np.arange(10))\n\n    return series.nlargest(4)  # Expect series of 9, 8, 7, 6\n\n\nprint(series_nlargest())\n'"
examples/series/series_notna.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_notna():\n    series = pd.Series([4, np.nan, 2, 1])\n\n    return series.notna()  # Expect series of True, False, True, True\n\n\nprint(series_notna())\n'"
examples/series/series_nsmallest.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_nsmallest():\n    series = pd.Series(np.arange(10))\n\n    return series.nsmallest(4)  # Expect series of 0, 1, 2, 3\n\n\nprint(series_nsmallest())\n'"
examples/series/series_nunique.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_nunique():\n    series = pd.Series([2, 8, 2, 1])\n\n    return series.nunique()  # Expect value: 3\n\n\nprint(series_nunique())\n'"
examples/series/series_pct_change.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\nExpect Series\n0         NaN\n1         NaN\n2   -0.340000\n3         NaN\n4    1.787879\ndtype: float64\n""""""\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_pct_change():\n    s = pd.Series([5., 0, 3.3, np.nan, 9.2])\n\n    return s.pct_change(periods=2, fill_method=None, limit=None, freq=None)\n\n\nprint(series_pct_change())\n'"
examples/series/series_pow.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_pow():\n    s1 = pd.Series([5, 4, 3, 2, 1])\n    s2 = pd.Series([0, 2, 3, 6, 8])\n\n    return s1.pow(s2)  # Expect series of 1, 16, 27, 64, 1\n\n\nprint(series_pow())\n'"
examples/series/series_prod.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_prod():\n    series = pd.Series([3.2, -10, np.nan, 0.23, 9.2])\n\n    return series.prod()  # Expect value: -67.712\n\n\nprint(series_prod())\n'"
examples/series/series_quantile.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_quantile():\n    s = pd.Series([1, 2, 3, 4])\n    median = .5  # compute median\n    out_series = s.quantile(median)\n\n    return out_series # Expect median value == 2.5\n\n\nprint(series_quantile())\n'"
examples/series/series_rename.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\nExpect Series\n0    0\n1    1\n2    2\n3    3\n4    4\nName: new_series, dtype: int64\n""""""\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_rename():\n    s = pd.Series(np.arange(5))\n    s.rename(""new_series"")\n\n    return s\n\n\nprint(series_rename())\n'"
examples/series/series_setitem_int.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numpy as np\nimport pandas as pd\n\nfrom numba import njit\n\n\n@njit\ndef series_setitem():\n    value = 0\n    series = pd.Series(np.arange(5, 0, -1))  # Series of 5, 4, 3, 2, 1\n\n    series[0] = value\n\n    return series   # result Series of 0, 4, 3, 2, 1\n\n\nprint(series_setitem())\n'"
examples/series/series_setitem_series.py,2,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numpy as np\nimport pandas as pd\n\nfrom numba import njit\n\n\n@njit\ndef series_setitem():\n    value = 0\n    series = pd.Series(np.arange(5, 0, -1))  # Series of 5, 4, 3, 2, 1\n\n    indices = pd.Series(np.asarray([1, 3]))\n    series[indices] = value\n\n    return series       # result Series of 5, 0, 3, 0, 1\n\n\nprint(series_setitem())\n'"
examples/series/series_setitem_slice.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numpy as np\nimport pandas as pd\n\nfrom numba import njit\n\n\n@njit\ndef series_setitem():\n    value = 0\n    series = pd.Series(np.arange(5, 0, -1))  # Series of 5, 4, 3, 2, 1\n\n    series[2:5] = value\n\n    return series   # result Series of 5, 4, 0, 0, 0\n\n\nprint(series_setitem())\n'"
examples/series/series_shape.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_shape():\n    series = pd.Series(np.arange(10))\n\n    return series.shape  # Expect (10,)\n\n\nprint(series_shape())\n'"
examples/series/series_shift.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\nExpect Series\n0     NaN\n1     3.0\n2   -10.0\n3     NaN\n4     0.0\ndtype: float64\n""""""\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_shift():\n    series = pd.Series([3, -10, np.nan, 0, 92])\n\n    return series.shift()\n\n\nprint(series_shift())\n'"
examples/series/series_size.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_size():\n    series = pd.Series(np.arange(10))\n\n    return series.size  # Expect value: 10\n\n\nprint(series_size())\n'"
examples/series/series_skew.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nimport numpy as np\nfrom numba import njit\n\n\n@njit\ndef series_skew():\n    s = pd.Series([np.nan, -2., 3., 5.0])\n\n    return s.skew()  # Expect -1.1520696383139375\n\n\nprint(series_skew())\n'"
examples/series/series_sort_values.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\nExpect Series\n1   -10.0\n3     0.0\n0     3.0\n4    92.0\n2     NaN\ndtype: float64\n""""""\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_sort_values():\n    series = pd.Series([3, -10, np.nan, 0, 92])\n\n    return series.sort_values()\n\n\nprint(series_sort_values())\n'"
examples/series/series_std.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_std():\n    series = pd.Series(np.arange(10))\n\n    return series.std()  # Expect value: 3.0276503540974917\n\n\nprint(series_std())\n'"
examples/series/series_sub.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_sub():\n    s1 = pd.Series([5, 4, 3, 2, 1])\n    s2 = pd.Series([0, 2, 3, 6, 8])\n\n    return s1.sub(s2)  # Expect series of 5, 2, 0, -4, -7\n\n\nprint(series_sub())\n'"
examples/series/series_sum.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_sum():\n    series = pd.Series([5, 4, 3, 2, 1])\n\n    return series.sum()  # Expect value: 15\n\n\nprint(series_sum())\n'"
examples/series/series_take.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_take():\n    series = pd.Series([5, 4, 3, 2, 1])\n\n    return series.take([4, 1])  # Expect series of 4, 1\n\n\nprint(series_take())\n'"
examples/series/series_truediv.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_truediv():\n    s1 = pd.Series([1, 2, 4])\n    s2 = pd.Series([4, 4, 16])\n    out_series = s1.truediv(s2)\n\n    return out_series  # Expect series of 0.25, 0.50, 0.25\n\n\nprint(series_truediv())\n'"
examples/series/series_unique.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_unique():\n    s = pd.Series([2, 1, 3, 3])\n    out_series = s.unique()\n\n    return out_series  # Expect array of unique values [1, 2, 3]\n\n\nprint(series_unique())\n'"
examples/series/series_value_counts.py,1,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\nExpect Series:\n3.0    2\n4.0    1\n2.0    1\n1.0    1\ndtype: int64\n""""""\nimport pandas as pd\nimport numpy as np\nfrom numba import njit\n\n\n@njit\ndef series_value_counts():\n    s = pd.Series([3, 1, 2, 3, 4, np.nan])\n    out_series = s.value_counts()\n\n    return out_series\n\n\nprint(series_value_counts())\n'"
examples/series/series_values.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_values():\n    series = pd.Series(np.arange(5))\n\n    return series.values  # Expect array of 0, 1, 2, 3, 4\n\n\nprint(series_values())\n'"
examples/series/series_var.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_var():\n    series = pd.Series(np.arange(10))\n\n    return series.var()  # Expect value: 9.16666...\n\n\nprint(series_var())\n'"
sdc/datatypes/__init__.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n'"
sdc/datatypes/common_functions.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\n\n| This file contains SDC overloads for common algorithms used internally\n\n""""""\n\nimport numpy\nimport pandas\nfrom pandas.core.indexing import IndexingError\n\nimport numba\nfrom numba.misc import quicksort\nfrom numba import types\nfrom numba.core.errors import TypingError\nfrom numba.extending import register_jitable\nfrom numba.np import numpy_support\nfrom numba.typed import Dict\n\nimport sdc\nfrom sdc.hiframes.api import isna\nfrom sdc.hiframes.pd_series_type import SeriesType\nfrom sdc.str_arr_type import string_array_type\nfrom sdc.str_arr_ext import (num_total_chars, append_string_array_to,\n                             str_arr_is_na, pre_alloc_string_array, str_arr_set_na, string_array_type,\n                             cp_str_list_to_array, create_str_arr_from_list, get_utf8_size,\n                             str_arr_set_na_by_mask)\nfrom sdc.utilities.prange_utils import parallel_chunks\nfrom sdc.utilities.utils import sdc_overload, sdc_register_jitable\nfrom sdc.utilities.sdc_typing_utils import (find_common_dtype_from_numpy_dtypes,\n                                            TypeChecker)\n\n\nclass SDCLimitation(Exception):\n    """"""Exception to be raised in case of SDC limitation""""""\n    pass\n\n\ndef hpat_arrays_append(A, B):\n    pass\n\n\n@sdc_overload(hpat_arrays_append, jit_options={\'parallel\': False})\ndef hpat_arrays_append_overload(A, B):\n    """"""Function for appending underlying arrays (A and B) or list/tuple of arrays B to an array A""""""\n\n    if isinstance(A, types.Array):\n        if isinstance(B, types.Array):\n            def _append_single_numeric_impl(A, B):\n                return numpy.concatenate((A, B,))\n\n            return _append_single_numeric_impl\n        elif isinstance(B, (types.UniTuple, types.List)):\n            # TODO: this heavily relies on B being a homogeneous tuple/list - find a better way\n            # to resolve common dtype of heterogeneous sequence of arrays\n            numba_common_dtype = find_common_dtype_from_numpy_dtypes([A.dtype, B.dtype.dtype], [])\n\n            # TODO: refactor to use numpy.concatenate when Numba supports building a tuple at runtime\n            def _append_list_numeric_impl(A, B):\n\n                total_length = len(A) + numpy.array([len(arr) for arr in B]).sum()\n                new_data = numpy.empty(total_length, numba_common_dtype)\n\n                stop = len(A)\n                new_data[:stop] = A\n                for arr in B:\n                    start = stop\n                    stop = start + len(arr)\n                    new_data[start:stop] = arr\n                return new_data\n\n            return _append_list_numeric_impl\n\n    elif A == string_array_type:\n        if B == string_array_type:\n            def _append_single_string_array_impl(A, B):\n                total_size = len(A) + len(B)\n                total_chars = num_total_chars(A) + num_total_chars(B)\n                new_data = sdc.str_arr_ext.pre_alloc_string_array(total_size, total_chars)\n\n                pos = 0\n                pos += append_string_array_to(new_data, pos, A)\n                pos += append_string_array_to(new_data, pos, B)\n\n                return new_data\n\n            return _append_single_string_array_impl\n        elif (isinstance(B, (types.UniTuple, types.List)) and B.dtype == string_array_type):\n            def _append_list_string_array_impl(A, B):\n                array_list = [A] + list(B)\n                total_size = numpy.array([len(arr) for arr in array_list]).sum()\n                total_chars = numpy.array([num_total_chars(arr) for arr in array_list]).sum()\n\n                new_data = sdc.str_arr_ext.pre_alloc_string_array(total_size, total_chars)\n\n                pos = 0\n                pos += append_string_array_to(new_data, pos, A)\n                for arr in B:\n                    pos += append_string_array_to(new_data, pos, arr)\n\n                return new_data\n\n            return _append_list_string_array_impl\n\n\n@sdc_register_jitable\ndef fill_array(data, size, fill_value=numpy.nan, push_back=True):\n    """"""\n    Fill array with given values to reach the size\n    """"""\n\n    if push_back:\n        return numpy.append(data, numpy.repeat(fill_value, size - data.size))\n\n    return numpy.append(numpy.repeat(fill_value, size - data.size), data)\n\n\n@sdc_register_jitable\ndef fill_str_array(data, size, push_back=True):\n    """"""\n    Fill StringArrayType array with given values to reach the size\n    """"""\n\n    string_array_size = len(data)\n    nan_array_size = size - string_array_size\n    num_chars = sdc.str_arr_ext.num_total_chars(data)\n\n    result_data = sdc.str_arr_ext.pre_alloc_string_array(size, num_chars)\n\n    # Keep NaN values of initial array\n    arr_is_na_mask = numpy.array([sdc.hiframes.api.isna(data, i) for i in range(string_array_size)])\n    data_str_list = sdc.str_arr_ext.to_string_list(data)\n    nan_list = [\'\'] * nan_array_size\n\n    result_list = data_str_list + nan_list if push_back else nan_list + data_str_list\n    cp_str_list_to_array(result_data, result_list)\n\n    # Batch=64 iteration to avoid threads competition\n    batch_size = 64\n    if push_back:\n        for i in numba.prange(size//batch_size + 1):\n            for j in range(i*batch_size, min((i+1)*batch_size, size)):\n                if j < string_array_size:\n                    if arr_is_na_mask[j]:\n                        str_arr_set_na(result_data, j)\n                else:\n                    str_arr_set_na(result_data, j)\n\n    else:\n        for i in numba.prange(size//batch_size + 1):\n            for j in range(i*batch_size, min((i+1)*batch_size, size)):\n                if j < nan_array_size:\n                    str_arr_set_na(result_data, j)\n                else:\n                    str_arr_j = j - nan_array_size\n                    if arr_is_na_mask[str_arr_j]:\n                        str_arr_set_na(result_data, j)\n\n    return result_data\n\n\n@numba.njit\ndef _hpat_ensure_array_capacity(new_size, arr):\n    """""" Function ensuring that the size of numpy array is at least as specified\n        Returns newly allocated array of bigger size with copied elements if existing size is less than requested\n    """"""\n\n    k = len(arr)\n    if k >= new_size:\n        return arr\n\n    n = k\n    while n < new_size:\n        n = 2 * n\n    res = numpy.empty(n, arr.dtype)\n    res[:k] = arr[:k]\n    return res\n\n\ndef sdc_join_series_indexes(left, right):\n    pass\n\n\n@sdc_overload(sdc_join_series_indexes, jit_options={\'parallel\': False})\ndef sdc_join_series_indexes_overload(left, right):\n    """"""Function for joining arrays left and right in a way similar to pandas.join \'outer\' algorithm""""""\n\n    # TODO: eliminate code duplication by merging implementations for numeric and StringArray\n    # requires equivalents of numpy.arsort and _hpat_ensure_array_capacity for StringArrays\n    if (isinstance(left, types.Array) and isinstance(right, types.Array)):\n\n        numba_common_dtype = find_common_dtype_from_numpy_dtypes([left.dtype, right.dtype], [])\n        if isinstance(numba_common_dtype, types.Number):\n\n            def sdc_join_series_indexes_impl(left, right):\n\n                # allocate result arrays\n                lsize = len(left)\n                rsize = len(right)\n                est_total_size = int(1.1 * (lsize + rsize))\n\n                lidx = numpy.empty(est_total_size, numpy.int64)\n                ridx = numpy.empty(est_total_size, numpy.int64)\n                joined = numpy.empty(est_total_size, numba_common_dtype)\n\n                left_nan = []\n                right_nan = []\n                for i in range(lsize):\n                    if numpy.isnan(left[i]):\n                        left_nan.append(i)\n                for i in range(rsize):\n                    if numpy.isnan(right[i]):\n                        right_nan.append(i)\n\n                # sort arrays saving the old positions\n                sorted_left = numpy.argsort(left, kind=\'mergesort\')\n                sorted_right = numpy.argsort(right, kind=\'mergesort\')\n                # put the position of the nans in an increasing sequence\n                sorted_left[lsize-len(left_nan):] = left_nan\n                sorted_right[rsize-len(right_nan):] = right_nan\n\n                i, j, k = 0, 0, 0\n                while (i < lsize and j < rsize):\n                    joined = _hpat_ensure_array_capacity(k + 1, joined)\n                    lidx = _hpat_ensure_array_capacity(k + 1, lidx)\n                    ridx = _hpat_ensure_array_capacity(k + 1, ridx)\n\n                    left_index = left[sorted_left[i]]\n                    right_index = right[sorted_right[j]]\n\n                    if (left_index < right_index) or numpy.isnan(right_index):\n                        joined[k] = left_index\n                        lidx[k] = sorted_left[i]\n                        ridx[k] = -1\n                        i += 1\n                        k += 1\n                    elif (left_index > right_index) or numpy.isnan(left_index):\n                        joined[k] = right_index\n                        lidx[k] = -1\n                        ridx[k] = sorted_right[j]\n                        j += 1\n                        k += 1\n                    else:\n                        # find ends of sequences of equal index values in left and right\n                        ni, nj = i, j\n                        while (ni < lsize and left[sorted_left[ni]] == left_index):\n                            ni += 1\n                        while (nj < rsize and right[sorted_right[nj]] == right_index):\n                            nj += 1\n\n                        # join the blocks found into results\n                        for s in numpy.arange(i, ni, 1):\n                            block_size = nj - j\n                            to_joined = numpy.repeat(left_index, block_size)\n                            to_lidx = numpy.repeat(sorted_left[s], block_size)\n                            to_ridx = numpy.array([sorted_right[k] for k in numpy.arange(j, nj, 1)], numpy.int64)\n\n                            joined = _hpat_ensure_array_capacity(k + block_size, joined)\n                            lidx = _hpat_ensure_array_capacity(k + block_size, lidx)\n                            ridx = _hpat_ensure_array_capacity(k + block_size, ridx)\n\n                            joined[k:k + block_size] = to_joined\n                            lidx[k:k + block_size] = to_lidx\n                            ridx[k:k + block_size] = to_ridx\n                            k += block_size\n                        i = ni\n                        j = nj\n\n                # fill the end of joined with remaining part of left or right\n                if i < lsize:\n                    block_size = lsize - i\n                    joined = _hpat_ensure_array_capacity(k + block_size, joined)\n                    lidx = _hpat_ensure_array_capacity(k + block_size, lidx)\n                    ridx = _hpat_ensure_array_capacity(k + block_size, ridx)\n                    ridx[k: k + block_size] = numpy.repeat(-1, block_size)\n                    while i < lsize:\n                        joined[k] = left[sorted_left[i]]\n                        lidx[k] = sorted_left[i]\n                        i += 1\n                        k += 1\n\n                elif j < rsize:\n                    block_size = rsize - j\n                    joined = _hpat_ensure_array_capacity(k + block_size, joined)\n                    lidx = _hpat_ensure_array_capacity(k + block_size, lidx)\n                    ridx = _hpat_ensure_array_capacity(k + block_size, ridx)\n                    lidx[k: k + block_size] = numpy.repeat(-1, block_size)\n                    while j < rsize:\n                        joined[k] = right[sorted_right[j]]\n                        ridx[k] = sorted_right[j]\n                        j += 1\n                        k += 1\n\n                return joined[:k], lidx[:k], ridx[:k]\n\n            return sdc_join_series_indexes_impl\n\n        else:\n            # TODO: support joining indexes with common dtype=object - requires Numba\n            # support of such numpy arrays in nopython mode, for now just return None\n            return None\n\n    elif (left == string_array_type and right == string_array_type):\n\n        def sdc_join_series_indexes_impl(left, right):\n\n            # allocate result arrays\n            lsize = len(left)\n            rsize = len(right)\n            est_total_size = int(1.1 * (lsize + rsize))\n\n            lidx = numpy.empty(est_total_size, numpy.int64)\n            ridx = numpy.empty(est_total_size, numpy.int64)\n\n            # use Series.sort_values since argsort for StringArrays not implemented\n            original_left_series = pandas.Series(left)\n            original_right_series = pandas.Series(right)\n\n            # sort arrays saving the old positions\n            left_series = original_left_series.sort_values(kind=\'mergesort\')\n            right_series = original_right_series.sort_values(kind=\'mergesort\')\n            sorted_left = left_series._index\n            sorted_right = right_series._index\n\n            i, j, k = 0, 0, 0\n            while (i < lsize and j < rsize):\n                lidx = _hpat_ensure_array_capacity(k + 1, lidx)\n                ridx = _hpat_ensure_array_capacity(k + 1, ridx)\n\n                left_index = left[sorted_left[i]]\n                right_index = right[sorted_right[j]]\n\n                if (left_index < right_index):\n                    lidx[k] = sorted_left[i]\n                    ridx[k] = -1\n                    i += 1\n                    k += 1\n                elif (left_index > right_index):\n                    lidx[k] = -1\n                    ridx[k] = sorted_right[j]\n                    j += 1\n                    k += 1\n                else:\n                    # find ends of sequences of equal index values in left and right\n                    ni, nj = i, j\n                    while (ni < lsize and left[sorted_left[ni]] == left_index):\n                        ni += 1\n                    while (nj < rsize and right[sorted_right[nj]] == right_index):\n                        nj += 1\n\n                    # join the blocks found into results\n                    for s in numpy.arange(i, ni, 1):\n                        block_size = nj - j\n                        to_lidx = numpy.repeat(sorted_left[s], block_size)\n                        to_ridx = numpy.array([sorted_right[k] for k in numpy.arange(j, nj, 1)], numpy.int64)\n\n                        lidx = _hpat_ensure_array_capacity(k + block_size, lidx)\n                        ridx = _hpat_ensure_array_capacity(k + block_size, ridx)\n\n                        lidx[k:k + block_size] = to_lidx\n                        ridx[k:k + block_size] = to_ridx\n                        k += block_size\n                    i = ni\n                    j = nj\n\n            # fill the end of joined with remaining part of left or right\n            if i < lsize:\n                block_size = lsize - i\n                lidx = _hpat_ensure_array_capacity(k + block_size, lidx)\n                ridx = _hpat_ensure_array_capacity(k + block_size, ridx)\n                ridx[k: k + block_size] = numpy.repeat(-1, block_size)\n                while i < lsize:\n                    lidx[k] = sorted_left[i]\n                    i += 1\n                    k += 1\n\n            elif j < rsize:\n                block_size = rsize - j\n                lidx = _hpat_ensure_array_capacity(k + block_size, lidx)\n                ridx = _hpat_ensure_array_capacity(k + block_size, ridx)\n                lidx[k: k + block_size] = numpy.repeat(-1, block_size)\n                while j < rsize:\n                    ridx[k] = sorted_right[j]\n                    j += 1\n                    k += 1\n\n            # count total number of characters and allocate joined array\n            total_joined_size = k\n            num_chars_in_joined = 0\n            for i in numpy.arange(total_joined_size):\n                if lidx[i] != -1:\n                    num_chars_in_joined += len(left[lidx[i]])\n                elif ridx[i] != -1:\n                    num_chars_in_joined += len(right[ridx[i]])\n\n            joined = pre_alloc_string_array(total_joined_size, num_chars_in_joined)\n\n            # iterate over joined and fill it with indexes using lidx and ridx indexers\n            for i in numpy.arange(total_joined_size):\n                if lidx[i] != -1:\n                    joined[i] = left[lidx[i]]\n                    if (str_arr_is_na(left, lidx[i])):\n                        str_arr_set_na(joined, i)\n                elif ridx[i] != -1:\n                    joined[i] = right[ridx[i]]\n                    if (str_arr_is_na(right, ridx[i])):\n                        str_arr_set_na(joined, i)\n                else:\n                    str_arr_set_na(joined, i)\n\n            return joined, lidx, ridx\n\n        return sdc_join_series_indexes_impl\n\n    return None\n\n\ndef sdc_check_indexes_equal(left, right):\n    pass\n\n\n@sdc_overload(sdc_check_indexes_equal, jit_options={\'parallel\': False})\ndef sdc_check_indexes_equal_overload(A, B):\n    """"""Function for checking arrays A and B of the same type are equal""""""\n\n    if isinstance(A, types.Array):\n        def sdc_check_indexes_equal_numeric_impl(A, B):\n            return numpy.array_equal(A, B)\n        return sdc_check_indexes_equal_numeric_impl\n\n    elif A == string_array_type:\n        def sdc_check_indexes_equal_string_impl(A, B):\n            # TODO: replace with StringArrays comparison\n            is_index_equal = (len(A) == len(B)\n                              and num_total_chars(A) == num_total_chars(B))\n            for i in numpy.arange(len(A)):\n                if (A[i] != B[i]\n                        or str_arr_is_na(A, i) is not str_arr_is_na(B, i)):\n                    return False\n\n            return is_index_equal\n\n        return sdc_check_indexes_equal_string_impl\n\n\n@numba.njit\ndef _sdc_pandas_format_percentiles(arr):\n    """""" Function converting float array of percentiles to a list of strings formatted\n        the same as in pandas.io.formats.format.format_percentiles\n    """"""\n\n    percentiles_strs = []\n    for percentile in arr:\n        p_as_string = str(percentile * 100)\n\n        trim_index = len(p_as_string) - 1\n        while trim_index >= 0:\n            if p_as_string[trim_index] == \'0\':\n                trim_index -= 1\n                continue\n            elif p_as_string[trim_index] == \'.\':\n                break\n\n            trim_index += 1\n            break\n\n        if trim_index < 0:\n            p_as_string_trimmed = \'0\'\n        else:\n            p_as_string_trimmed = p_as_string[:trim_index]\n\n        percentiles_strs.append(p_as_string_trimmed + \'%\')\n\n    return percentiles_strs\n\n\ndef sdc_arrays_argsort(A, kind=\'quicksort\'):\n    pass\n\n\n@sdc_overload(sdc_arrays_argsort, jit_options={\'parallel\': False})\ndef sdc_arrays_argsort_overload(A, kind=\'quicksort\'):\n    """"""Function providing pandas argsort implementation for different 1D array types""""""\n\n    # kind is not known at compile time, so get this function here and use in impl if needed\n    quicksort_func = quicksort.make_jit_quicksort().run_quicksort\n\n    kind_is_default = isinstance(kind, str)\n    if isinstance(A, types.Array):\n        def _sdc_arrays_argsort_array_impl(A, kind=\'quicksort\'):\n            _kind = \'quicksort\' if kind_is_default == True else kind  # noqa\n            return numpy.argsort(A, kind=_kind)\n\n        return _sdc_arrays_argsort_array_impl\n\n    elif A == string_array_type:\n        def _sdc_arrays_argsort_str_arr_impl(A, kind=\'quicksort\'):\n\n            nan_mask = sdc.hiframes.api.get_nan_mask(A)\n            idx = numpy.arange(len(A))\n            old_nan_positions = idx[nan_mask]\n\n            data = A[~nan_mask]\n            keys = idx[~nan_mask]\n            if kind == \'quicksort\':\n                zipped = list(zip(list(data), list(keys)))\n                zipped = quicksort_func(zipped)\n                argsorted = [zipped[i][1] for i in numpy.arange(len(data))]\n            elif kind == \'mergesort\':\n                sdc.hiframes.sort.local_sort((data, ), (keys, ))\n                argsorted = list(keys)\n            else:\n                raise ValueError(""Unrecognized kind of sort in sdc_arrays_argsort"")\n\n            argsorted.extend(old_nan_positions)\n            return numpy.asarray(argsorted, dtype=numpy.int32)\n\n        return _sdc_arrays_argsort_str_arr_impl\n\n    elif isinstance(A, types.List):\n        return None\n\n    return None\n\n\ndef _sdc_pandas_series_check_axis(axis):\n    pass\n\n\n@sdc_overload(_sdc_pandas_series_check_axis, jit_options={\'parallel\': False})\ndef _sdc_pandas_series_check_axis_overload(axis):\n    if isinstance(axis, types.UnicodeType):\n        def _sdc_pandas_series_check_axis_impl(axis):\n            if axis != \'index\':\n                raise ValueError(""Method sort_values(). Unsupported parameter. Given axis != \'index\'"")\n        return _sdc_pandas_series_check_axis_impl\n\n    elif isinstance(axis, types.Integer):\n        def _sdc_pandas_series_check_axis_impl(axis):\n            if axis != 0:\n                raise ValueError(""Method sort_values(). Unsupported parameter. Given axis != 0"")\n        return _sdc_pandas_series_check_axis_impl\n\n    return None\n\n\ndef _sdc_asarray(data):\n    pass\n\n\n@sdc_overload(_sdc_asarray)\ndef _sdc_asarray_overload(data):\n\n    # TODO: extend with other types\n    if not isinstance(data, types.List):\n        return None\n\n    if isinstance(data.dtype, types.UnicodeType):\n        def _sdc_asarray_impl(data):\n            return create_str_arr_from_list(data)\n\n        return _sdc_asarray_impl\n\n    else:\n        result_dtype = data.dtype\n\n        def _sdc_asarray_impl(data):\n            # TODO: check if elementwise copy is needed at all\n            res_size = len(data)\n            res_arr = numpy.empty(res_size, dtype=result_dtype)\n            for i in numba.prange(res_size):\n                res_arr[i] = data[i]\n            return res_arr\n\n        return _sdc_asarray_impl\n\n    return None\n\n\ndef _sdc_take(data, indexes):\n    pass\n\n\n@sdc_overload(_sdc_take, jit_options={\'parallel\': True})\ndef _sdc_take_overload(data, indexes):\n    if isinstance(indexes.dtype, types.ListType) and isinstance(data, (types.Array, types.List)):\n        arr_dtype = data.dtype\n\n        def _sdc_take_list_impl(data, indexes):\n            res_size = 0\n            for i in numba.prange(len(indexes)):\n                res_size += len(indexes[i])\n            res_arr = numpy.empty(res_size, dtype=arr_dtype)\n            for i in numba.prange(len(indexes)):\n                start = 0\n                for l in range(len(indexes[0:i])):\n                    start += len(indexes[l])\n                current_pos = start\n                for j in range(len(indexes[i])):\n                    res_arr[current_pos] = data[indexes[i][j]]\n                    current_pos += 1\n            return res_arr\n\n        return _sdc_take_list_impl\n\n    elif isinstance(indexes.dtype, types.ListType) and data == string_array_type:\n        def _sdc_take_list_str_impl(data, indexes):\n            res_size = 0\n            for i in numba.prange(len(indexes)):\n                res_size += len(indexes[i])\n            nan_mask = numpy.zeros(res_size, dtype=numpy.bool_)\n            num_total_bytes = 0\n            for i in numba.prange(len(indexes)):\n                start = 0\n                for l in range(len(indexes[0:i])):\n                    start += len(indexes[l])\n                current_pos = start\n                for j in range(len(indexes[i])):\n                    num_total_bytes += get_utf8_size(data[indexes[i][j]])\n                    if isna(data, indexes[i][j]):\n                        nan_mask[current_pos] = True\n                    current_pos += 1\n            res_arr = pre_alloc_string_array(res_size, num_total_bytes)\n            for i in numba.prange(len(indexes)):\n                start = 0\n                for l in range(len(indexes[0:i])):\n                    start += len(indexes[l])\n                current_pos = start\n                for j in range(len(indexes[i])):\n                    res_arr[current_pos] = data[indexes[i][j]]\n                    if nan_mask[current_pos]:\n                        str_arr_set_na(res_arr, current_pos)\n                    current_pos += 1\n\n            return res_arr\n\n        return _sdc_take_list_str_impl\n\n    elif isinstance(data, types.Array):\n        arr_dtype = data.dtype\n\n        def _sdc_take_array_impl(data, indexes):\n            res_size = len(indexes)\n            res_arr = numpy.empty(res_size, dtype=arr_dtype)\n            for i in numba.prange(res_size):\n                res_arr[i] = data[indexes[i]]\n            return res_arr\n\n        return _sdc_take_array_impl\n\n    elif data == string_array_type:\n        def _sdc_take_str_arr_impl(data, indexes):\n            res_size = len(indexes)\n            nan_mask = numpy.zeros(res_size, dtype=numpy.bool_)\n            num_total_bytes = 0\n            for i in numba.prange(res_size):\n                num_total_bytes += get_utf8_size(data[indexes[i]])\n                if isna(data, indexes[i]):\n                    nan_mask[i] = True\n\n            res_arr = pre_alloc_string_array(res_size, num_total_bytes)\n            for i in numpy.arange(res_size):\n                res_arr[i] = data[indexes[i]]\n                if nan_mask[i]:\n                    str_arr_set_na(res_arr, i)\n\n            return res_arr\n\n        return _sdc_take_str_arr_impl\n\n    elif (isinstance(data, types.RangeType) and isinstance(data.dtype, types.Integer)):\n        arr_dtype = data.dtype\n\n        def _sdc_take_array_impl(data, indexes):\n            res_size = len(indexes)\n            index_errors = 0\n            res_arr = numpy.empty(res_size, dtype=arr_dtype)\n            for i in numba.prange(res_size):\n                value = data.start + data.step * indexes[i]\n                if value >= data.stop:\n                    index_errors += 1\n                res_arr[i] = value\n            if index_errors:\n                raise IndexError(""_sdc_take: index out-of-bounds"")\n            return res_arr\n\n        return _sdc_take_array_impl\n\n    return None\n\n\ndef _almost_equal(x, y):\n    """"""Check if floats are almost equal based on the float epsilon""""""\n    pass\n\n\n@sdc_overload(_almost_equal)\ndef _almost_equal_overload(x, y):\n    ty_checker = TypeChecker(\'Function sdc.common_functions._almost_equal_overload().\')\n    ty_checker.check(x, types.Float)\n    ty_checker.check(x, types.Float)\n\n    common_dtype = numpy.find_common_type([], [x.name, y.name])\n\n    def _almost_equal_impl(x, y):\n        return abs(x - y) <= numpy.finfo(common_dtype).eps\n\n    return _almost_equal_impl\n\n\ndef sdc_reindex_series(arr, index, name, by_index):\n    pass\n\n\n@sdc_overload(sdc_reindex_series)\ndef sdc_reindex_series_overload(arr, index, name, by_index):\n    """""" Reindexes series data by new index following the logic of pandas.core.indexing.check_bool_indexer """"""\n\n    same_index_types = index is by_index\n    data_dtype, index_dtype = arr.dtype, index.dtype\n    data_is_str_arr = isinstance(arr.dtype, types.UnicodeType)\n\n    def sdc_reindex_series_impl(arr, index, name, by_index):\n\n        # if index types are the same, we may not reindex if indexes are the same\n        if same_index_types == True:  # noqa\n            if index is by_index:\n                return pandas.Series(data=arr, index=index, name=name)\n\n        if data_is_str_arr == True:  # noqa\n            _res_data = [\'\'] * len(by_index)\n            res_data_nan_mask = numpy.zeros(len(by_index), dtype=types.bool_)\n        else:\n            _res_data = numpy.empty(len(by_index), dtype=data_dtype)\n\n        # build a dict of self.index values to their positions:\n        map_index_to_position = Dict.empty(\n            key_type=index_dtype,\n            value_type=types.int32\n        )\n\n        for i, value in enumerate(index):\n            if value in map_index_to_position:\n                raise ValueError(""cannot reindex from a duplicate axis"")\n            else:\n                map_index_to_position[value] = i\n\n        index_mismatch = 0\n        for i in numba.prange(len(by_index)):\n            if by_index[i] in map_index_to_position:\n                pos_in_self = map_index_to_position[by_index[i]]\n                _res_data[i] = arr[pos_in_self]\n                if data_is_str_arr == True:  # noqa\n                    res_data_nan_mask[i] = isna(arr, i)\n            else:\n                index_mismatch += 1\n        if index_mismatch:\n            msg = ""Unalignable boolean Series provided as indexer "" + \\\n                  ""(index of the boolean Series and of the indexed object do not match).""\n            raise IndexingError(msg)\n\n        if data_is_str_arr == True:  # noqa\n            res_data = create_str_arr_from_list(_res_data)\n            str_arr_set_na_by_mask(res_data, res_data_nan_mask)\n        else:\n            res_data = _res_data\n\n        return pandas.Series(data=res_data, index=by_index, name=name)\n\n    return sdc_reindex_series_impl\n'"
sdc/datatypes/hpat_pandas_dataframe_functions.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\'\'\'\n| :class:`pandas.DataFrame` functions and operators implementations in Intel SDC\n| Also, it contains Numba internal operators which are required for DataFrame type handling\n\'\'\'\n\nimport numba\nimport numpy\nimport operator\nimport pandas\nimport sdc\n\nfrom pandas.core.indexing import IndexingError\n\nfrom numba import types\nfrom numba import literally\nfrom numba.typed import List, Dict\nfrom numba.core.errors import TypingError\nfrom pandas.core.indexing import IndexingError\n\nfrom sdc.hiframes.pd_dataframe_ext import DataFrameType\nfrom sdc.hiframes.pd_series_type import SeriesType\nfrom sdc.utilities.sdc_typing_utils import (TypeChecker, check_index_is_numeric,\n                                            check_types_comparable, kwsparams2list,\n                                            gen_impl_generator, find_common_dtype_from_numpy_dtypes)\nfrom sdc.str_arr_ext import StringArrayType\n\nfrom sdc.hiframes.pd_dataframe_type import DataFrameType\nfrom sdc.datatypes.hpat_pandas_dataframe_getitem_types import (DataFrameGetitemAccessorType,\n                                                               dataframe_getitem_accessor_init)\nfrom sdc.datatypes.common_functions import SDCLimitation\nfrom sdc.datatypes.hpat_pandas_dataframe_rolling_types import _hpat_pandas_df_rolling_init\nfrom sdc.datatypes.hpat_pandas_rolling_types import (\n    gen_sdc_pandas_rolling_overload_body, sdc_pandas_rolling_docstring_tmpl)\nfrom sdc.datatypes.hpat_pandas_groupby_functions import init_dataframe_groupby, merge_groupby_dicts_inplace\nfrom sdc.hiframes.pd_dataframe_ext import get_dataframe_data\nfrom sdc.utilities.utils import sdc_overload, sdc_overload_method, sdc_overload_attribute\nfrom sdc.hiframes.api import isna\nfrom sdc.functions.numpy_like import getitem_by_mask\nfrom sdc.datatypes.common_functions import _sdc_take, sdc_reindex_series\nfrom sdc.utilities.prange_utils import parallel_chunks\nfrom sdc.functions.numpy_like import find_idx\n\n\n@sdc_overload_attribute(DataFrameType, \'index\')\ndef hpat_pandas_dataframe_index(df):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n    Pandas API: pandas.DataFrame.index\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/dataframe/dataframe_index.py\n        :language: python\n        :lines: 27-\n        :caption: The index (row labels) of the DataFrame.\n        :name: ex_dataframe_index\n\n    .. command-output:: python ./dataframe/dataframe_index.py\n        :cwd: ../../../examples\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas DataFrame attribute :attr:`pandas.DataFrame.index` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_dataframe.TestDataFrame.test_index*\n    """"""\n\n    ty_checker = TypeChecker(\'Attribute index.\')\n    ty_checker.check(df, DataFrameType)\n\n    if isinstance(df.index, types.NoneType) or df.index is None:\n        empty_df = not df.columns\n\n        def hpat_pandas_df_index_none_impl(df):\n            df_len = len(df._data[0][0]) if empty_df == False else 0  # noqa\n\n            return numpy.arange(df_len)\n\n        return hpat_pandas_df_index_none_impl\n    else:\n\n        def hpat_pandas_df_index_impl(df):\n            return df._index\n\n        return hpat_pandas_df_index_impl\n\n\ndef sdc_pandas_dataframe_values_codegen(self, numba_common_dtype):\n    """"""\n    Example of generated implementation:\n        def sdc_pandas_dataframe_values_impl(self):\n          length = len(self._data[0][0])\n          col_data_0 = self._data[0][0]\n          col_data_1 = self._data[1][0]\n          col_data_2 = self._data[0][1]\n          values = numpy.empty(length*3, numpy.dtype(""float64""))\n          for i in range(length):\n            values[i*3+0] = col_data_0[i]\n            values[i*3+1] = col_data_1[i]\n            values[i*3+2] = col_data_2[i]\n          return values.reshape(length, 3)\n    """"""\n    columns_data = []\n    columns_num = len(self.columns)\n    func_lines = [\n        f\'def sdc_pandas_dataframe_values_impl(self):\',\n        f\'  length = {df_length_expr(self)}\',\n    ]\n    for i, col in enumerate(self.columns):\n        col_loc = self.column_loc[col]\n        type_id, col_id = col_loc.type_id, col_loc.col_id\n        func_lines += [\n            f\'  col_data_{i} = self._data[{type_id}][{col_id}]\',\n        ]\n        columns_data.append(f\'col_data_{i}\')\n\n    func_lines += [\n        f\'  values = numpy.empty(length*{columns_num}, numpy.dtype(""{numba_common_dtype}""))\',\n        f\'  for i in range(length):\',\n    ]\n    func_lines += [\'\\n\'.join([\n        f\'    values[i*{columns_num}+{j}] = {columns_data[j]}[i]\',\n    ]) for j in range(columns_num)]\n    func_lines += [\n        f\'  return values.reshape(length, {columns_num})\\n\'\n    ]\n    func_text = \'\\n\'.join(func_lines)\n    global_vars = {\'pandas\': pandas, \'numpy\': numpy}\n\n    return func_text, global_vars\n\n\n@sdc_overload_attribute(DataFrameType, \'values\')\ndef hpat_pandas_dataframe_values(self):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n    Pandas API: pandas.DataFrame.values\n\n    Limitations\n    -----------\n    - Only numeric values supported as an output\n    - The dtype will be a lower-common-denominator dtype (implicit upcasting);\n    that is to say if the dtypes (even of numeric types) are mixed, the one that accommodates all will be chosen.\n    Use this with care if you are not dealing with the blocks.\n    e.g. If the dtypes are float16 and float32, dtype will be upcast to float32. If dtypes are int32 and uint8,\n    dtype will be upcast to int32. By numpy.find_common_type() convention,\n    mixing int64 and uint64 will result in a float64 dtype.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/dataframe/dataframe_values.py\n      :language: python\n      :lines: 27-\n      :caption: The values data of the DataFrame.\n      :name: ex_dataframe_values\n\n    .. command-output:: python ./dataframe/dataframe_values.py\n       :cwd: ../../../examples\n\n    .. seealso::\n\n        :ref:`DataFrame.to_numpy <pandas.DataFrame.to_numpy>`\n            Recommended alternative to this method.\n        :ref:`DataFrame.index <pandas.DataFrame.index>`\n            Retrieve the index labels.\n        :ref:`DataFrame.columns <pandas.DataFrame.columns>`\n            Retrieving the column names.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas DataFrame attribute :attr:`pandas.DataFrame.values` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_dataframe.TestDataFrame.test_df_values*\n    """"""\n\n    func_name = \'Attribute values.\'\n    ty_checker = TypeChecker(func_name)\n    ty_checker.check(self, DataFrameType)\n\n    # TODO: Handle StringArrayType\n    for i, column in enumerate(self.data):\n        if isinstance(column, StringArrayType):\n            ty_checker.raise_exc(column, \'Numeric type\', f\'df.data[""{self.columns[i]}""]\')\n\n    numba_common_dtype = find_common_dtype_from_numpy_dtypes([column.dtype for column in self.data], [])\n\n    def hpat_pandas_df_values_impl(self, numba_common_dtype):\n        loc_vars = {}\n        func_text, global_vars = sdc_pandas_dataframe_values_codegen(self, numba_common_dtype)\n\n        exec(func_text, global_vars, loc_vars)\n        _values_impl = loc_vars[\'sdc_pandas_dataframe_values_impl\']\n        return _values_impl\n\n    return hpat_pandas_df_values_impl(self, numba_common_dtype)\n\n\ndef sdc_pandas_dataframe_append_codegen(df, other, _func_name, ignore_index_value, indexes_comparable, args):\n    """"""\n    Example of generated implementation:\n    def sdc_pandas_dataframe_append_impl(df, other, ignore_index=False, verify_integrity=False, sort=None):\n        len_df = len(df._data[0][0])\n        len_other = len(other._data[0][0])\n        new_col_0_data_df = df._data[0][0]\n        new_col_0_data_other = other._data[0][0]\n        new_col_0 = init_series(new_col_0_data_df).append(init_series(new_col_0_data_other))._data\n        new_col_1_data_df = df._data[0][1]\n        new_col_1_data_other = other._data[0][1]\n        new_col_1 = init_series(new_col_1_data_df).append(init_series(new_col_1_data_other))._data\n        return pandas.DataFrame({""A"": new_col_0, ""B"": new_col_1})\n    """"""\n    indent = 4 * \' \'\n    func_args = [\'df\', \'other\'] + kwsparams2list(args)\n\n    df_columns_indx = {col_name: i for i, col_name in enumerate(df.columns)}\n    other_columns_indx = {col_name: i for i, col_name in enumerate(other.columns)}\n\n    # Keep columns that are StringArrayType\n    string_type_columns = set(col_name for typ, col_name in zip(df.data, df.columns)\n                              if isinstance(typ, StringArrayType))\n\n    for typ, col_name in zip(other.data, other.columns):\n        if isinstance(typ, StringArrayType):\n            string_type_columns.add(col_name)\n\n    func_definition = [f\'def sdc_pandas_dataframe_{_func_name}_impl({"", "".join(func_args)}):\']\n    func_text = []\n    column_list = []\n\n    func_text.append(f\'len_df = len(df._data[0][0])\')\n    func_text.append(f\'len_other = len(other._data[0][0])\')\n\n    for col_name, idx in df_columns_indx.items():\n        col_loc = df.column_loc[col_name]\n        type_id, col_id = col_loc.type_id, col_loc.col_id\n        func_text.append(f\'new_col_{idx}_data_df = df._data[{type_id}][{col_id}]\')\n        if col_name in other_columns_indx:\n            other_col_loc = other.column_loc[col_name]\n            other_type_id, other_col_id = other_col_loc.type_id, other_col_loc.col_id\n            func_text.append(f\'new_col_{idx}_data_other = \'\n                             f\'other._data[{other_type_id}][{other_col_id}]\')\n            s1 = f\'init_series(new_col_{idx}_data_df)\'\n            s2 = f\'init_series(new_col_{idx}_data_other)\'\n            func_text.append(f\'new_col_{idx} = {s1}.append({s2})._data\')\n        else:\n            func_text.append(f\'new_col_{idx}_data = init_series(new_col_{idx}_data_df)._data\')\n            if col_name in string_type_columns:\n                func_text.append(f\'new_col_{idx} = fill_str_array(new_col_{idx}_data, len_df+len_other)\')\n            else:\n                func_text.append(f\'new_col_{idx} = fill_array(new_col_{idx}_data, len_df+len_other)\')\n        column_list.append((f\'new_col_{idx}\', col_name))\n\n    for col_name, idx in other_columns_indx.items():\n        if col_name not in df_columns_indx:\n            other_col_loc = other.column_loc[col_name]\n            other_type_id, other_col_id = other_col_loc.type_id, other_col_loc.col_id\n            func_text.append(f\'new_col_{idx}_data_other = other._data[{other_type_id}][{other_col_id}]\')\n            func_text.append(f\'new_col_{idx}_data = init_series(new_col_{idx}_data_other)._data\')\n            if col_name in string_type_columns:\n                func_text.append(\n                    f\'new_col_{idx}_other = \'\n                    f\'fill_str_array(new_col_{idx}_data, len_df+len_other, push_back=False)\')\n            else:\n                func_text.append(f\'new_col_{idx}_other = \'\n                                 f\'fill_array(new_col_{idx}_data, len_df+len_other, push_back=False)\')\n            column_list.append((f\'new_col_{idx}_other\', col_name))\n\n    data = \', \'.join(f\'""{column_name}"": {column}\' for column, column_name in column_list)\n\n    if ignore_index_value == True:  # noqa\n        func_text.append(f\'return pandas.DataFrame({{{data}}})\\n\')\n    else:\n        if indexes_comparable == False:  # noqa\n            func_text.append(f\'raise SDCLimitation(""Indexes of dataframes are expected to have comparable \'\n                             f\'(both Numeric or String) types if parameter ignore_index is set to False."")\')\n        else:\n            func_text += [f\'joined_index = hpat_arrays_append(df.index, other.index)\\n\',\n                          f\'return pandas.DataFrame({{{data}}}, index=joined_index)\\n\']\n\n    func_definition.extend([indent + func_line for func_line in func_text])\n    func_def = \'\\n\'.join(func_definition)\n\n    global_vars = {\'pandas\': pandas,\n                   \'init_series\': sdc.hiframes.api.init_series,\n                   \'fill_array\': sdc.datatypes.common_functions.fill_array,\n                   \'fill_str_array\': sdc.datatypes.common_functions.fill_str_array,\n                   \'hpat_arrays_append\': sdc.datatypes.common_functions.hpat_arrays_append,\n                   \'SDCLimitation\': SDCLimitation}\n\n    return func_def, global_vars\n\n\n@sdc_overload_method(DataFrameType, \'append\')\ndef sdc_pandas_dataframe_append(df, other, ignore_index=False, verify_integrity=False, sort=None):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.DataFrame.append\n\n    Limitations\n    -----------\n    - Parameters ``verify_integrity`` and ``sort`` are unsupported.\n    - Parameter ``other`` can be only :obj:`pandas.DataFrame`.\n    - Indexes of dataframes are expected to have comparable (both Numeric or String) types if parameter ignore_index\n    is set to False.\n    - This function may reveal slower performance than Pandas* on user system. Users should exercise a tradeoff\n    between staying in JIT-region with that function or going back to interpreter mode.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/dataframe/dataframe_append.py\n        :language: python\n        :lines: 37-\n        :caption: Appending rows of other to the end of caller, returning a new object. Columns in other that are not\n                  in the caller are added as new columns.\n        :name: ex_dataframe_append\n\n    .. command-output:: python ./dataframe/dataframe_append.py\n        :cwd: ../../../examples\n\n    .. seealso::\n        `pandas.concat <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html>`_\n            General function to concatenate DataFrame or Series objects.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas DataFrame method :meth:`pandas.DataFrame.append` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_dataframe.TestDataFrame.test_append*\n    """"""\n\n    _func_name = \'append\'\n\n    ty_checker = TypeChecker(f\'Method {_func_name}().\')\n    ty_checker.check(df, DataFrameType)\n    # TODO: support other array-like types\n    ty_checker.check(other, DataFrameType)\n\n    if not isinstance(verify_integrity, (bool, types.Boolean, types.Omitted)) and verify_integrity:\n        ty_checker.raise_exc(verify_integrity, \'boolean\', \'verify_integrity\')\n\n    if not isinstance(sort, (bool, types.Boolean, types.Omitted)) and sort is not None:\n        ty_checker.raise_exc(sort, \'boolean, None\', \'sort\')\n\n    if not isinstance(ignore_index, (bool, types.Boolean, types.Omitted)):\n        ty_checker.raise_exc(ignore_index, \'boolean\', \'ignore_index\')\n\n    none_or_numeric_indexes = ((isinstance(df.index, types.NoneType) or isinstance(df.index, types.Number)) and\n                               (isinstance(other.index, types.NoneType) or isinstance(other.index, types.Number)))\n    indexes_comparable = check_types_comparable(df.index, other.index) or none_or_numeric_indexes\n\n    if isinstance(ignore_index, types.Literal):\n        ignore_index = ignore_index.literal_value\n    elif not (ignore_index is False or isinstance(ignore_index, types.Omitted)):\n        raise SDCLimitation(""Parameter ignore_index should be Literal"")\n\n    args = {\'ignore_index\': False, \'verify_integrity\': False, \'sort\': None}\n\n    def sdc_pandas_dataframe_append_impl(df, other, _func_name, ignore_index, indexes_comparable, args):\n        loc_vars = {}\n        func_def, global_vars = sdc_pandas_dataframe_append_codegen(df, other, _func_name, ignore_index,\n                                                                    indexes_comparable, args)\n        exec(func_def, global_vars, loc_vars)\n        _append_impl = loc_vars[\'sdc_pandas_dataframe_append_impl\']\n        return _append_impl\n\n    return sdc_pandas_dataframe_append_impl(df, other, _func_name, ignore_index, indexes_comparable, args)\n\n# Example func_text for func_name=\'count\' columns=(\'A\', \'B\'):\n#\n#         def _df_count_impl(df, axis=0, level=None, numeric_only=False):\n#           series_A = init_series(df._data[0])\n#           result_A = series_A.count(level=level)\n#           series_B = init_series(df._data[1])\n#           result_B = series_B.count(level=level)\n#           return pandas.Series([result_A, result_B], [\'A\', \'B\'])\n\n\ndef _dataframe_reduce_columns_codegen(func_name, func_params, series_params, columns, column_loc):\n    result_name_list = []\n    joined = \', \'.join(func_params)\n    func_lines = [f\'def _df_{func_name}_impl({joined}):\']\n    for i, c in enumerate(columns):\n        col_loc = column_loc[c]\n        type_id, col_id = col_loc.type_id, col_loc.col_id\n        result_c = f\'result_{i}\'\n        func_lines += [f\'  series_{i} = pandas.Series({func_params[0]}._data[{type_id}][{col_id}])\',\n                       f\'  {result_c} = series_{i}.{func_name}({series_params})\']\n        result_name_list.append(result_c)\n    all_results = \', \'.join(result_name_list)\n    all_columns = \', \'.join([f""\'{c}\'"" for c in columns])\n\n    func_lines += [f\'  return pandas.Series([{all_results}], [{all_columns}])\']\n    func_text = \'\\n\'.join(func_lines)\n\n    global_vars = {\'pandas\': pandas}\n\n    return func_text, global_vars\n\n\ndef sdc_pandas_dataframe_reduce_columns(df, func_name, params, ser_params):\n    all_params = [\'df\']\n    ser_par = []\n\n    for key, value in params.items():\n        all_params.append(\'{}={}\'.format(key, value))\n    for key, value in ser_params.items():\n        ser_par.append(\'{}={}\'.format(key, value))\n\n    s_par = \'{}\'.format(\', \'.join(ser_par[:]))\n\n    df_func_name = f\'_df_{func_name}_impl\'\n\n    func_text, global_vars = _dataframe_reduce_columns_codegen(func_name, all_params, s_par, df.columns,\n                                                               df.column_loc)\n    loc_vars = {}\n    exec(func_text, global_vars, loc_vars)\n    _reduce_impl = loc_vars[df_func_name]\n\n    return _reduce_impl\n\n\ndef _dataframe_reduce_columns_codegen_head(func_name, func_params, series_params, df):\n    """"""\n    Example func_text for func_name=\'head\' columns=(\'float\', \'string\'):\n        def _df_head_impl(df, n=5):\n          data_0 = df._data[0][0]\n          series_0 = pandas.Series(data_0)\n          result_0 = series_0.head(n=n)\n          data_1 = df._data[1][0]\n          series_1 = pandas.Series(data_1)\n          result_1 = series_1.head(n=n)\n          return pandas.DataFrame({""float"": result_0, ""string"": result_1}, index=df._index[:n])\n    """"""\n    results = []\n    joined = \', \'.join(func_params)\n    func_lines = [f\'def _df_{func_name}_impl(df, {joined}):\']\n    ind = df_index_codegen_head(df)\n    for i, c in enumerate(df.columns):\n        col_loc = df.column_loc[c]\n        type_id, col_id = col_loc.type_id, col_loc.col_id\n        result_c = f\'result_{i}\'\n        func_lines += [f\'  data_{i} = df._data[{type_id}][{col_id}]\',\n                       f\'  series_{i} = pandas.Series(data_{i})\',\n                       f\'  {result_c} = series_{i}.{func_name}({series_params})\']\n        results.append((df.columns[i], result_c))\n\n    data = \', \'.join(f\'""{col}"": {data}\' for col, data in results)\n    func_lines += [f\'  return pandas.DataFrame({{{data}}}, {ind})\']\n    func_text = \'\\n\'.join(func_lines)\n    global_vars = {\'pandas\': pandas}\n\n    return func_text, global_vars\n\n\ndef sdc_pandas_dataframe_head_codegen(df, func_name, params, ser_params):\n    all_params = kwsparams2list(params)\n    ser_par = kwsparams2list(ser_params)\n    s_par = \', \'.join(ser_par)\n\n    df_func_name = f\'_df_{func_name}_impl\'\n    func_text, global_vars = _dataframe_reduce_columns_codegen_head(func_name, all_params, s_par, df)\n    loc_vars = {}\n    exec(func_text, global_vars, loc_vars)\n    _reduce_impl = loc_vars[df_func_name]\n\n    return _reduce_impl\n\n\ndef df_index_codegen_head(self):\n    # TODO: Rewrite when DF constructor will be fixed with index=None\n    if isinstance(self.index, types.NoneType):\n        return \'\'\n\n    return \'index=df._index[:n]\'\n\n\n@sdc_overload_method(DataFrameType, \'head\')\ndef head_overload(df, n=5):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.DataFrame.head\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/dataframe/dataframe_head.py\n       :language: python\n       :lines: 37-\n       :caption: Return the first n rows.\n       :name: ex_dataframe_head\n\n    .. command-output:: python ./dataframe/dataframe_head.py\n       :cwd: ../../../examples\n\n    .. seealso::\n\n        :ref:`DataFrame.tail <pandas.DataFrame.tail>`\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.head` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_dataframe.TestDataFrame.test_df_head*\n    """"""\n    name = \'head\'\n\n    if isinstance(n, types.Omitted):\n        n = n.value\n\n    params = {\'n\': 5}\n    ser_par = {\'n\': \'n\'}\n    return sdc_pandas_dataframe_head_codegen(df, name, params, ser_par)\n\n\ndef _dataframe_codegen_copy(func_params, series_params, df):\n    """"""\n    Example func_text for func_name=\'copy\' columns=(\'A\', \'B\'):\n        def _df_copy_impl(df, deep=True):\n          data_0 = df._data[0][0]\n          series_0 = pandas.Series(data_0, name=\'A\')\n          result_0 = series_0.copy(deep=deep)\n          data_1 = df._data[1][0]\n          series_1 = pandas.Series(data_1, name=\'B\')\n          result_1 = series_1.copy(deep=deep)\n          return pandas.DataFrame({""A"": result_0, ""B"": result_1}, index=df._index)\n    """"""\n    results = []\n    series_params_str = \', \'.join(kwsparams2list(series_params))\n    func_params_str = \', \'.join(kwsparams2list(func_params))\n    func_lines = [f""def _df_copy_impl(df, {func_params_str}):""]\n    index = df_index_codegen_all(df)\n    for i, c in enumerate(df.columns):\n        col_loc = df.column_loc[c]\n        type_id, col_id = col_loc.type_id, col_loc.col_id\n        result_c = f""result_{i}""\n        func_lines += [f""  data_{i} = df._data[{type_id}][{col_id}]"",\n                       f""  series_{i} = pandas.Series(data_{i}, name=\'{c}\')"",\n                       f""  {result_c} = series_{i}.copy({series_params_str})""]\n        results.append((c, result_c))\n\n    data = \', \'.join(f\'""{col}"": {data}\' for col, data in results)\n    func_lines += [f""  return pandas.DataFrame({{{data}}}, {index})""]\n    func_text = \'\\n\'.join(func_lines)\n    global_vars = {\'pandas\': pandas}\n\n    return func_text, global_vars\n\n\ndef sdc_pandas_dataframe_copy_codegen(df, params, series_params):\n    func_text, global_vars = _dataframe_codegen_copy(params, series_params, df)\n    loc_vars = {}\n    exec(func_text, global_vars, loc_vars)\n    _reduce_impl = loc_vars[\'_df_copy_impl\']\n\n    return _reduce_impl\n\n\ndef df_index_codegen_all(self):\n    if isinstance(self.index, types.NoneType):\n        return \'\'\n\n    return \'index=df._index\'\n\n\n@sdc_overload_method(DataFrameType, \'copy\')\ndef copy_overload(df, deep=True):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.DataFrame.copy\n\n    Limitations\n    -----------\n    - Parameter deep=False is currently unsupported for indexes by Intel Scalable Dataframe Compiler\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/dataframe/dataframe_copy.py\n       :language: python\n       :lines: 36-\n       :caption: Make a copy of this object\xe2\x80\x99s indices and data.\n       :name: ex_dataframe_copy\n\n    .. command-output:: python ./dataframe/dataframe_copy.py\n       :cwd: ../../../examples\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas DataFrame method :meth:`pandas.Series.copy` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_dataframe.TestDataFrame.test_df_copy*\n    """"""\n    ty_checker = TypeChecker(""Method copy()."")\n    ty_checker.check(df, DataFrameType)\n\n    if not (isinstance(deep, (types.Omitted, types.Boolean, types.NoneType)) or deep is True):\n        ty_checker.raise_exc(deep, \'boolean\', \'deep\')\n\n    params = {\'deep\': True}\n    series_params = {\'deep\': \'deep\'}\n    return sdc_pandas_dataframe_copy_codegen(df, params, series_params)\n\n\ndef _dataframe_apply_columns_codegen(func_name, func_params, series_params, columns, column_loc):\n    result_name = []\n    joined = \', \'.join(func_params)\n    func_lines = [f\'def _df_{func_name}_impl({joined}):\']\n    for i, c in enumerate(columns):\n        col_loc = column_loc[c]\n        type_id, col_id = col_loc.type_id, col_loc.col_id\n        result_c = f\'result_{i}\'\n        func_lines += [f\'  series_{i} = pandas.Series({func_params[0]}._data[{type_id}][{col_id}])\',\n                       f\'  {result_c} = series_{i}.{func_name}({series_params})\']\n        result_name.append((result_c, c))\n\n    data = \', \'.join(f\'""{column_name}"": {column}\' for column, column_name in result_name)\n\n    func_lines += [f\'  return pandas.DataFrame({{{data}}})\']\n    func_text = \'\\n\'.join(func_lines)\n\n    global_vars = {\'pandas\': pandas}\n\n    return func_text, global_vars\n\n\ndef sdc_pandas_dataframe_apply_columns(df, func_name, params, ser_params):\n    all_params = [\'df\']\n    ser_par = []\n\n    for key, value in params.items():\n        all_params.append(\'{}={}\'.format(key, value))\n    for key, value in ser_params.items():\n        ser_par.append(\'{}={}\'.format(key, value))\n\n    s_par = \', \'.join(ser_par)\n\n    df_func_name = f\'_df_{func_name}_impl\'\n\n    func_text, global_vars = _dataframe_apply_columns_codegen(func_name, all_params, s_par,\n                                                              df.columns, df.column_loc)\n    loc_vars = {}\n    exec(func_text, global_vars, loc_vars)\n    _reduce_impl = loc_vars[df_func_name]\n\n    return _reduce_impl\n\n\ndef check_type(name, df, axis=None, skipna=None, level=None, numeric_only=None, ddof=1, min_count=0):\n    ty_checker = TypeChecker(\'Method {}().\'.format(name))\n    ty_checker.check(df, DataFrameType)\n\n    if not (isinstance(axis, types.Omitted) or axis is None):\n        ty_checker.raise_exc(axis, \'unsupported\', \'axis\')\n\n    if not (isinstance(skipna, (types.Omitted, types.NoneType, types.Boolean)) or skipna is None):\n        ty_checker.raise_exc(skipna, \'bool\', \'skipna\')\n\n    if not (isinstance(level, types.Omitted) or level is None):\n        ty_checker.raise_exc(level, \'unsupported\', \'level\')\n\n    if not (isinstance(numeric_only, types.Omitted) or numeric_only is None):\n        ty_checker.raise_exc(numeric_only, \'unsupported\', \'numeric_only\')\n\n    if not (isinstance(ddof, (types.Omitted, types.Integer)) or ddof == 1):\n        ty_checker.raise_exc(ddof, \'int\', \'ddof\')\n\n    if not (isinstance(min_count, types.Omitted) or min_count == 0):\n        ty_checker.raise_exc(min_count, \'unsupported\', \'min_count\')\n\n\n@sdc_overload_method(DataFrameType, \'median\')\ndef median_overload(df, axis=None, skipna=None, level=None, numeric_only=None):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.DataFrame.median\n\n    Limitations\n    -----------\n    Parameters ``axis``, ``level`` and ``numeric_only`` are unsupported.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/dataframe/dataframe_median.py\n       :language: python\n       :lines: 35-\n       :caption: Return the median of the values for the columns.\n       :name: ex_dataframe_median\n\n    .. command-output:: python ./dataframe/dataframe_median.py\n       :cwd: ../../../examples\n\n    .. seealso::\n        :ref:`Series.median <pandas.Series.median>`\n            Returns the median of the values for the Series.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n\n    Pandas DataFrame method :meth:`pandas.DataFrame.median` implementation.\n\n    .. only:: developer\n\n        Test: python -m sdc.runtests -k sdc.tests.test_dataframe.TestDataFrame.test_median*\n    """"""\n\n    name = \'median\'\n\n    check_type(name, df, axis=axis, skipna=skipna, level=level, numeric_only=numeric_only)\n\n    params = {\'axis\': None, \'skipna\': None, \'level\': None, \'numeric_only\': None}\n    ser_par = {\'skipna\': \'skipna\', \'level\': \'level\'}\n\n    return sdc_pandas_dataframe_reduce_columns(df, name, params, ser_par)\n\n\n@sdc_overload_method(DataFrameType, \'mean\')\ndef mean_overload(df, axis=None, skipna=None, level=None, numeric_only=None):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.DataFrame.mean\n\n    Limitations\n    -----------\n    Parameters ``axis``, ``level`` and ``numeric_only`` are unsupported.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/dataframe/dataframe_mean.py\n       :language: python\n       :lines: 35-\n       :caption: Return the mean of the values for the columns.\n       :name: ex_dataframe_mean\n\n    .. command-output:: python ./dataframe/dataframe_mean.py\n       :cwd: ../../../examples\n\n    .. seealso::\n        :ref:`Series.mean <pandas.Series.mean>`\n            Return the mean of the values for the Series.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n\n    Pandas DataFrame method :meth:`pandas.DataFrame.mean` implementation.\n\n    .. only:: developer\n\n        Test: python -m sdc.runtests -k sdc.tests.test_dataframe.TestDataFrame.test_mean*\n    """"""\n\n    name = \'mean\'\n\n    check_type(name, df, axis=axis, skipna=skipna, level=level, numeric_only=numeric_only)\n\n    params = {\'axis\': None, \'skipna\': None, \'level\': None, \'numeric_only\': None}\n    ser_par = {\'skipna\': \'skipna\', \'level\': \'level\'}\n\n    return sdc_pandas_dataframe_reduce_columns(df, name, params, ser_par)\n\n\nsdc_pandas_dataframe_rolling = sdc_overload_method(DataFrameType, \'rolling\')(\n    gen_sdc_pandas_rolling_overload_body(_hpat_pandas_df_rolling_init, DataFrameType))\nsdc_pandas_dataframe_rolling.__doc__ = sdc_pandas_rolling_docstring_tmpl.format(\n    ty=\'DataFrame\', ty_lower=\'dataframe\')\n\n\n@sdc_overload_method(DataFrameType, \'std\')\ndef std_overload(df, axis=None, skipna=None, level=None, ddof=1, numeric_only=None):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.DataFrame.std\n\n    Limitations\n    -----------\n    Parameters ``axis``, ``level`` and ``numeric_only`` are unsupported.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/dataframe/dataframe_std.py\n       :language: python\n       :lines: 35-\n       :caption: Return sample standard deviation over columns.\n       :name: ex_dataframe_std\n\n    .. command-output:: python ./dataframe/dataframe_std.py\n       :cwd: ../../../examples\n\n    .. seealso::\n        :ref:`Series.std <pandas.Series.std>`\n            Returns sample standard deviation over Series.\n        :ref:`Series.var <pandas.Series.var>`\n            Returns unbiased variance over Series.\n        :ref:`DataFrame.var <pandas.DataFrame.var>`\n            Returns unbiased variance over DataFrame.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n\n    Pandas DataFrame method :meth:`pandas.DataFrame.std` implementation.\n\n    .. only:: developer\n\n        Test: python -m sdc.runtests -k sdc.tests.test_dataframe.TestDataFrame.test_std*\n    """"""\n\n    name = \'std\'\n\n    check_type(name, df, axis=axis, skipna=skipna, level=level, numeric_only=numeric_only, ddof=ddof)\n\n    params = {\'axis\': None, \'skipna\': None, \'level\': None, \'ddof\': 1, \'numeric_only\': None}\n    ser_par = {\'skipna\': \'skipna\', \'level\': \'level\', \'ddof\': \'ddof\'}\n\n    return sdc_pandas_dataframe_reduce_columns(df, name, params, ser_par)\n\n\n@sdc_overload_method(DataFrameType, \'var\')\ndef var_overload(df, axis=None, skipna=None, level=None, ddof=1, numeric_only=None):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.DataFrame.var\n\n    Limitations\n    -----------\n    Parameters\xc2\xa0``axis``,\xc2\xa0``level``\xc2\xa0and\xc2\xa0``numeric_only``\xc2\xa0are\xc2\xa0unsupported.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/dataframe/dataframe_var.py\n       :language: python\n       :lines: 35-\n       :caption: Return unbiased variance over requested axis.\n       :name: ex_dataframe_var\n\n    .. command-output:: python ./dataframe/dataframe_var.py\n       :cwd: ../../../examples\n\n    .. seealso::\n        :ref:`Series.std <pandas.Series.std>`\n            Returns sample standard deviation over Series.\n        :ref:`Series.var<pandas.Series.var>`\n            Returns unbiased variance over Series.\n        :ref:`DataFrame.std <pandas.DataFrame.std>`\n            Returns sample standard deviation over DataFrame.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n\n    Pandas DataFrame method :meth:`pandas.DataFrame.var` implementation.\n\n    .. only:: developer\n\n        Test: python -m sdc.runtests -k sdc.tests.test_dataframe.TestDataFrame.test_var*\n    """"""\n\n    name = \'var\'\n\n    check_type(name, df, axis=axis, skipna=skipna, level=level, numeric_only=numeric_only, ddof=ddof)\n\n    params = {\'axis\': None, \'skipna\': None, \'level\': None, \'ddof\': 1, \'numeric_only\': None}\n    ser_par = {\'skipna\': \'skipna\', \'level\': \'level\', \'ddof\': \'ddof\'}\n\n    return sdc_pandas_dataframe_reduce_columns(df, name, params, ser_par)\n\n\n@sdc_overload_method(DataFrameType, \'max\')\ndef max_overload(df, axis=None, skipna=None, level=None, numeric_only=None):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.DataFrame.max\n\n    Limitations\n    -----------\n    Parameters ``axis``, ``level`` and  ``numeric_only`` are unsupported.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/dataframe/dataframe_max.py\n       :language: python\n       :lines: 35-\n       :caption: Return the maximum of the values for the columns.\n       :name: ex_dataframe_max\n\n    .. command-output:: python ./dataframe/dataframe_max.py\n       :cwd: ../../../examples\n\n    .. seealso::\n        :ref:`Series.sum <pandas.Series.sum>`\n            Return the sum.\n        :ref:`Series.max <pandas.Series.max>`\n            Return the maximum.\n        :ref:`Series.idxmin <pandas.Series.idxmin>`\n            Return the index of the minimum.\n        :ref:`Series.idxmax <pandas.Series.idxmax>`\n            Return the index of the maximum.\n        :ref:`DataFrame.sum <pandas.DataFrame.sum>`\n            Return the sum over the requested axis.\n        :ref:`DataFrame.min <pandas.DataFrame.min>`\n            Return the minimum over the requested axis.\n        :ref:`DataFrame.idxmin <pandas.DataFrame.idxmin>`\n            Return the index of the minimum over the requested axis.\n        :ref:`DataFrame.idxmax <pandas.DataFrame.idxmax>`\n            Return the index of the maximum over the requested axis.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n\n    Pandas DataFrame method :meth:`pandas.DataFrame.max` implementation.\n\n    .. only:: developer\n\n        Test: python -m sdc.runtests -k sdc.tests.test_dataframe.TestDataFrame.test_max*\n    """"""\n\n    name = \'max\'\n\n    check_type(name, df, axis=axis, skipna=skipna, level=level, numeric_only=numeric_only)\n\n    params = {\'axis\': None, \'skipna\': None, \'level\': None, \'numeric_only\': None}\n    ser_par = {\'skipna\': \'skipna\', \'level\': \'level\'}\n\n    return sdc_pandas_dataframe_reduce_columns(df, name, params, ser_par)\n\n\n@sdc_overload_method(DataFrameType, \'min\')\ndef min_overload(df, axis=None, skipna=None, level=None, numeric_only=None):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.DataFrame.min\n\n    Limitations\n    -----------\n    Parameters ``axis``, ``level`` and ``numeric_only`` are unsupported.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/dataframe/dataframe_min.py\n       :language: python\n       :lines: 35-\n       :caption: Return the minimum of the values for the columns.\n       :name: ex_dataframe_min\n\n    .. command-output:: python ./dataframe/dataframe_min.py\n       :cwd: ../../../examples\n\n    .. seealso::\n        :ref:`Series.sum <pandas.Series.sum>`\n            Return the sum.\n        :ref:`Series.min <pandas.Series.min>`\n            Return the minimum.\n        :ref:`Series.max <pandas.Series.max>`\n            Return the maximum.\n        :ref:`Series.idxmin <pandas.Series.idxmin>`\n            Return the index of the minimum.\n        :ref:`Series.idxmax <pandas.Series.idxmax>`\n            Return the index of the maximum.\n        :ref:`DataFrame.sum <pandas.DataFrame.sum>`\n            Return the sum over the requested axis.\n        :ref:`DataFrame.min <pandas.DataFrame.min>`\n            Return the minimum over the requested axis.\n        :ref:`DataFrame.max <pandas.DataFrame.max>`\n            Return the maximum over the requested axis.\n        :ref:`DataFrame.idxmin <pandas.DataFrame.idxmin>`\n            Return the index of the minimum over the requested axis.\n        :ref:`DataFrame.idxmax <pandas.DataFrame.idxmax>`\n            Return the index of the maximum over the requested axis.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n\n    Pandas DataFrame method :meth:`pandas.DataFrame.min` implementation.\n\n    .. only:: developer\n\n        Test: python -m sdc.runtests -k sdc.tests.test_dataframe.TestDataFrame.test_min*\n    """"""\n\n    name = \'min\'\n\n    check_type(name, df, axis=axis, skipna=skipna, level=level, numeric_only=numeric_only)\n\n    params = {\'axis\': None, \'skipna\': None, \'level\': None, \'numeric_only\': None}\n    ser_par = {\'skipna\': \'skipna\', \'level\': \'level\'}\n\n    return sdc_pandas_dataframe_reduce_columns(df, name, params, ser_par)\n\n\n@sdc_overload_method(DataFrameType, \'sum\')\ndef sum_overload(df, axis=None, skipna=None, level=None, numeric_only=None, min_count=0):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.DataFrame.sum\n\n    Limitations\n    -----------\n    Parameters ``axis``, ``level``, ``numeric_only`` and ``min_count`` are unsupported.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/dataframe/dataframe_sum.py\n       :language: python\n       :lines: 35-\n       :caption: Return the sum of the values for the columns.\n       :name: ex_dataframe_sum\n\n    .. command-output:: python ./dataframe/dataframe_sum.py\n       :cwd: ../../../examples\n\n    .. seealso::\n\n        :ref:`Series.sum <pandas.Series.sum>`\n            Return the sum.\n\n        :ref:`Series.min <pandas.Series.min>`\n            Return the minimum.\n\n        :ref:`Series.max <pandas.Series.max>`\n            Return the maximum.\n\n        :ref:`Series.idxmin <pandas.Series.idxmin>`\n            Return the index of the minimum.\n\n        :ref:`Series.idxmax <pandas.Series.idxmax>`\n            Return the index of the maximum.\n\n        :ref:`DataFrame.sum <pandas.DataFrame.sum>`\n            Return the sum over the requested axis.\n\n        :ref:`DataFrame.min <pandas.DataFrame.min>`\n            Return the minimum over the requested axis.\n\n        :ref:`DataFrame.max <pandas.DataFrame.max>`\n            Return the maximum over the requested axis.\n\n        :ref:`DataFrame.idxmin <pandas.DataFrame.idxmin>`\n            Return the index of the minimum over the requested axis.\n\n        :ref:`DataFrame.idxmax <pandas.DataFrame.idxmax>`\n            Return the index of the maximum over the requested axis.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n\n    Pandas DataFrame method :meth:`pandas.DataFrame.sum` implementation.\n\n    .. only:: developer\n\n        Test: python -m sdc.runtests -k sdc.tests.test_dataframe.TestDataFrame.test_sum*\n    """"""\n\n    name = \'sum\'\n\n    check_type(name, df, axis=axis, skipna=skipna, level=level, numeric_only=numeric_only, min_count=min_count)\n\n    params = {\'axis\': None, \'skipna\': None, \'level\': None, \'numeric_only\': None, \'min_count\': 0}\n    ser_par = {\'skipna\': \'skipna\', \'level\': \'level\', \'min_count\': \'min_count\'}\n\n    return sdc_pandas_dataframe_reduce_columns(df, name, params, ser_par)\n\n\n@sdc_overload_method(DataFrameType, \'prod\')\ndef prod_overload(df, axis=None, skipna=None, level=None, numeric_only=None, min_count=0):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.DataFrame.prod\n\n    Limitations\n    -----------\n    Parameters ``axis``, ``level``, ``numeric_only`` and ``min_count`` are unsupported.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/dataframe/dataframe_prod.py\n       :language: python\n       :lines: 35-\n       :caption: Return the product of the values for the columns.\n       :name: ex_dataframe_prod\n\n    .. command-output:: python ./dataframe/dataframe_prod.py\n       :cwd: ../../../examples\n\n    .. seealso::\n        :ref:`Series.prod <pandas.Series.prod>`\n            Returns the product of the values for the Series.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n\n    Pandas DataFrame method :meth:`pandas.DataFrame.prod` implementation.\n\n    .. only:: developer\n\n        Test: python -m sdc.runtests -k sdc.tests.test_dataframe.TestDataFrame.test_prod*\n    """"""\n\n    name = \'prod\'\n\n    check_type(name, df, axis=axis, skipna=skipna, level=level, numeric_only=numeric_only, min_count=min_count)\n\n    params = {\'axis\': None, \'skipna\': None, \'level\': None, \'numeric_only\': None, \'min_count\': 0}\n    ser_par = {\'skipna\': \'skipna\', \'level\': \'level\', \'min_count\': \'min_count\'}\n\n    return sdc_pandas_dataframe_reduce_columns(df, name, params, ser_par)\n\n\n@sdc_overload_method(DataFrameType, \'count\')\ndef count_overload(df, axis=0, level=None, numeric_only=False):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.DataFrame.count\n\n    Limitations\n    -----------\n    Parameters ``axis``, ``level`` and ``numeric_only`` are unsupported.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/dataframe/dataframe_count.py\n       :language: python\n       :lines: 33-\n       :caption: Count non-NA cells for each column or row.\n       :name: ex_dataframe_count\n\n    .. command-output:: python ./dataframe/dataframe_count.py\n       :cwd: ../../../examples\n\n    .. seealso::\n        :ref:`Series.count <pandas.Series.count>`\n            Number of non-NA elements in a Series.\n        :ref:`DataFrame.shape <pandas.DataFrame.shape>`\n            Number of DataFrame rows and columns (including NA elements).\n        :ref:`DataFrame.isna <pandas.DataFrame.isna>`\n            Boolean same-sized DataFrame showing places of NA elements.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n\n    Pandas DataFrame method :meth:`pandas.DataFrame.count` implementation.\n\n        .. only:: developer\n\n            Test: python -m sdc.runtests -k sdc.tests.test_dataframe.TestDataFrame.test_count*\n    """"""\n\n    name = \'count\'\n\n    ty_checker = TypeChecker(\'Method {}().\'.format(name))\n    ty_checker.check(df, DataFrameType)\n\n    if not (isinstance(axis, types.Omitted) or axis == 0):\n        ty_checker.raise_exc(axis, \'unsupported\', \'axis\')\n\n    if not (isinstance(level, types.Omitted) or level is None):\n        ty_checker.raise_exc(level, \'unsupported\', \'level\')\n\n    if not (isinstance(numeric_only, types.Omitted) or numeric_only is False):\n        ty_checker.raise_exc(numeric_only, \'unsupported\', \'numeric_only\')\n\n    params = {\'axis\': 0, \'level\': None, \'numeric_only\': False}\n    ser_par = {\'level\': \'level\'}\n\n    return sdc_pandas_dataframe_reduce_columns(df, name, params, ser_par)\n\n\ndef _dataframe_codegen_isna(func_name, columns, df):\n    """"""\n    Example if generated implementation\n        def _df_isna_impl(df):\n          data_0 = df._data[0][0]\n          series_0 = pandas.Series(data_0)\n          result_0 = series_0.isna()\n          data_1 = df._data[1][0]\n          series_1 = pandas.Series(data_1)\n          result_1 = series_1.isna()\n          return pandas.DataFrame({""A"": result_0, ""B"": result_1}, index=df._index)\n    """"""\n    results = []\n    func_lines = [f\'def _df_{func_name}_impl(df):\']\n    index = df_index_codegen_all(df)\n    for i, c in enumerate(columns):\n        col_loc = df.column_loc[c]\n        type_id, col_id = col_loc.type_id, col_loc.col_id\n        result_c = f\'result_{i}\'\n        func_lines += [f\'  data_{i} = df._data[{type_id}][{col_id}]\',\n                       f\'  series_{i} = pandas.Series(data_{i})\',\n                       f\'  {result_c} = series_{i}.{func_name}()\']\n        results.append((columns[i], result_c))\n\n    data = \', \'.join(f\'""{col}"": {data}\' for col, data in results)\n    func_lines += [f\'  return pandas.DataFrame({{{data}}}, {index})\']\n    func_text = \'\\n\'.join(func_lines)\n    global_vars = {\'pandas\': pandas}\n\n    return func_text, global_vars\n\n\ndef sdc_pandas_dataframe_isna_codegen(df, func_name):\n    df_func_name = f\'_df_{func_name}_impl\'\n    func_text, global_vars = _dataframe_codegen_isna(func_name, df.columns, df)\n    loc_vars = {}\n    exec(func_text, global_vars, loc_vars)\n    _reduce_impl = loc_vars[df_func_name]\n\n    return _reduce_impl\n\n\ndef df_index_codegen_all(self):\n    if isinstance(self.index, types.NoneType):\n        return \'\'\n    return \'index=df._index\'\n\n\n@sdc_overload_method(DataFrameType, \'isna\')\ndef isna_overload(df):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.DataFrame.isna\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/dataframe/dataframe_isna.py\n       :language: python\n       :lines: 35-\n       :caption: Detect missing values.\n       :name: ex_dataframe_isna\n\n    .. command-output:: python ./dataframe/dataframe_isna.py\n       :cwd: ../../../examples\n\n    .. seealso::\n\n        :ref:`DataFrame.isnull <pandas.DataFrame.isnull>`\n            Alias of isna.\n\n        :ref:`DataFrame.notna <pandas.DataFrame.notna>`\n            Boolean inverse of isna.\n\n        :ref:`DataFrame.dropna <pandas.DataFrame.dropna>`\n            Omit axes labels with missing values.\n\n        `pandas.isna <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.isna.html#pandas.isna>`_\n            Top-level isna.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas DataFrame method :meth:`pandas.DataFrame.isna` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_dataframe.TestDataFrame.test_df_isna*\n    """"""\n\n    return sdc_pandas_dataframe_isna_codegen(df, \'isna\')\n\n\ndef sdc_pandas_dataframe_drop_codegen(func_name, func_args, df, drop_cols):\n    """"""\n    Example of generated implementation:\n        def sdc_pandas_dataframe_drop_impl(df, labels=None, axis=0, index=None, columns=None,\n                                           level=None, inplace=False, errors=""raise""):\n            new_col_0_data_df = df._data[1][0]\n            new_col_1_data_df = df._data[0][1]\n            return pandas.DataFrame({""B"": new_col_0_data_df, ""C"": new_col_1_data_df}, index=df.index)\n    """"""\n    indent = 4 * \' \'\n    saved_df_columns = [column for column in df.columns if column not in drop_cols]\n    func_definition = [f\'def sdc_pandas_dataframe_{func_name}_impl({"", "".join(func_args)}):\']\n    func_text = []\n    column_list = []\n\n    for label in drop_cols:\n        if label not in df.columns:\n            func_text.append(f\'if errors == ""raise"":\')\n            func_text.append(indent + f\'raise ValueError(""The label {label} is not found in the selected axis"")\')\n            break\n\n    for column_id, column_name in enumerate(saved_df_columns):\n        col_loc = df.column_loc[column_name]\n        type_id, col_id = col_loc.type_id, col_loc.col_id\n        func_text.append(f\'new_col_{column_id}_data_df = df._data[{type_id}][{col_id}]\')\n        column_list.append((f\'new_col_{column_id}_data_df\', column_name))\n\n    data = \', \'.join(f\'""{column_name}"": {column}\' for column, column_name in column_list)\n    index = \'df.index\'\n    func_text.append(f""return pandas.DataFrame({{{data}}}, index={index})\\n"")\n    func_definition.extend([indent + func_line for func_line in func_text])\n    func_def = \'\\n\'.join(func_definition)\n\n    global_vars = {\'pandas\': pandas}\n\n    return func_def, global_vars\n\n\n@sdc_overload_method(DataFrameType, \'drop\')\ndef sdc_pandas_dataframe_drop(df, labels=None, axis=0, index=None, columns=None, level=None, inplace=False,\n                              errors=\'raise\'):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n    Pandas API: pandas.DataFrame.drop\n\n    Limitations\n    -----------\n    - Parameters ``labels``, ``axis``, ``index``, ``level`` and ``inplace`` are currently unsupported.\n    - Parameter ``columns`` is required and is expected to be a Literal value with one column name\n    or Tuple with columns names.\n    - Supported ``errors`` can be {``raise``, ``ignore``}, default ``raise``. If ``ignore``, suppress error and only\n    existing labels are dropped.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/dataframe/dataframe_drop.py\n        :language: python\n        :lines: 37-\n        :caption: Drop specified columns from DataFrame.\n        :name: ex_dataframe_drop\n\n    .. command-output:: python ./dataframe/dataframe_drop.py\n        :cwd: ../../../examples\n\n    .. seealso::\n        :ref:`DataFrame.loc <pandas.DataFrame.loc>`\n            Label-location based indexer for selection by label.\n        :ref:`DataFrame.dropna <pandas.DataFrame.dropna>`\n            Return DataFrame with labels on given axis omitted where (all or any) data are missing.\n        :ref:`DataFrame.drop_duplicates <pandas.DataFrame.drop_duplicates>`\n            Return DataFrame with duplicate rows removed, optionally only considering certain columns.\n        :ref:`Series.drop <pandas.Series.drop>`\n            Return Series with specified index labels removed.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas DataFrame method :meth:`pandas.DataFrame.drop` implementation.\n    .. only:: developer\n    Test: python -m sdc.runtests -k sdc.tests.test_dataframe.TestDataFrame.test_df_drop*\n\n    """"""\n\n    _func_name = \'drop\'\n\n    ty_checker = TypeChecker(f\'Method {_func_name}().\')\n    ty_checker.check(df, DataFrameType)\n\n    if not isinstance(labels, types.Omitted) and labels is not None:\n        ty_checker.raise_exc(labels, \'None\', \'labels\')\n\n    if not isinstance(axis, (int, types.Omitted)):\n        ty_checker.raise_exc(axis, \'int\', \'axis\')\n\n    if not isinstance(index, types.Omitted) and index is not None:\n        ty_checker.raise_exc(index, \'None\', \'index\')\n\n    if not isinstance(columns, (types.Omitted, types.Tuple, types.Literal)):\n        ty_checker.raise_exc(columns, \'str, tuple of str\', \'columns\')\n\n    if not isinstance(level, (types.Omitted, types.Literal)) and level is not None:\n        ty_checker.raise_exc(level, \'None\', \'level\')\n\n    if not isinstance(inplace, (bool, types.Omitted)) and inplace:\n        ty_checker.raise_exc(inplace, \'bool\', \'inplace\')\n\n    if not isinstance(errors, (str, types.Omitted, types.Literal)):\n        ty_checker.raise_exc(errors, \'str\', \'errors\')\n\n    args = {\'labels\': None, \'axis\': 0, \'index\': None, \'columns\': None, \'level\': None, \'inplace\': False,\n            \'errors\': f\'""raise""\'}\n\n    def sdc_pandas_dataframe_drop_impl(df, _func_name, args, columns):\n        func_args = [\'df\']\n        for key, value in args.items():\n            if key not in func_args:\n                if isinstance(value, types.Literal):\n                    value = value.literal_value\n                func_args.append(f\'{key}={value}\')\n\n        if isinstance(columns, types.StringLiteral):\n            drop_cols = (columns.literal_value,)\n        elif isinstance(columns, types.Tuple):\n            drop_cols = tuple(column.literal_value for column in columns)\n        else:\n            raise ValueError(\'Only drop by one column or tuple of columns is currently supported in df.drop()\')\n\n        func_def, global_vars = sdc_pandas_dataframe_drop_codegen(_func_name, func_args, df, drop_cols)\n        loc_vars = {}\n        exec(func_def, global_vars, loc_vars)\n        _drop_impl = loc_vars[\'sdc_pandas_dataframe_drop_impl\']\n        return _drop_impl\n\n    return sdc_pandas_dataframe_drop_impl(df, _func_name, args, columns)\n\n\ndef df_length_expr(self):\n    """"""Generate expression to get length of DF""""""\n    if self.columns:\n        return \'len(self._data[0][0])\'\n\n    return \'0\'\n\n\ndef df_index_expr(self, length_expr=None, as_range=False):\n    """"""Generate expression to get or create index of DF""""""\n    if isinstance(self.index, types.NoneType):\n        if length_expr is None:\n            length_expr = df_length_expr(self)\n\n        if as_range:\n            return f\'range({length_expr})\'\n        else:\n            return f\'numpy.arange({length_expr})\'\n\n    return \'self._index\'\n\n\ndef df_getitem_slice_idx_main_codelines(self, idx):\n    """"""Generate main code lines for df.getitem with idx of slice""""""\n    results = []\n    func_lines = [\n        f\'  self_index = {df_index_expr(self)}\',\n        f\'  index = self_index[idx]\',\n    ]\n    for i, col in enumerate(self.columns):\n        col_loc = self.column_loc[col]\n        type_id, col_id = col_loc.type_id, col_loc.col_id\n        res_data = f\'res_data_{i}\'\n        func_lines += [\n            f\'  data_{i} = self._data[{type_id}][{col_id}][idx]\',\n            f\'  {res_data} = pandas.Series(data_{i}, index=index, name=""{col}"")\',\n        ]\n        results.append((col, res_data))\n\n    data = \', \'.join(f\'""{col}"": {data}\' for col, data in results)\n    func_lines += [f\'  return pandas.DataFrame({{{data}}}, index=index)\']\n\n    return func_lines\n\n\ndef df_getitem_tuple_idx_main_codelines(self, literal_idx):\n    """"""Generate main code lines for df.getitem with idx of tuple""""""\n    results = []\n    func_lines = [f\'  res_index = {df_index_expr(self)}\']\n    needed_cols = {col: i for i, col in enumerate(self.columns) if col in literal_idx}\n    for col, i in needed_cols.items():\n        col_loc = self.column_loc[col]\n        type_id, col_id = col_loc.type_id, col_loc.col_id\n        res_data = f\'res_data_{i}\'\n        func_lines += [\n            f\'  data_{i} = self._data[{type_id}][{col_id}]\',\n            f\'  {res_data} = pandas.Series(data_{i}, index=res_index, name=""{col}"")\'\n        ]\n        results.append((col, res_data))\n\n    data = \', \'.join(f\'""{col}"": {data}\' for col, data in results)\n    func_lines += [f\'  return pandas.DataFrame({{{data}}}, index=res_index)\']\n\n    return func_lines\n\n\ndef df_getitem_bool_series_idx_main_codelines(self, idx):\n    """"""Generate main code lines for df.getitem""""""\n    length_expr = df_length_expr(self)\n\n    # optimization for default indexes in df and idx when index alignment is trivial\n    if isinstance(self.index, types.NoneType) and isinstance(idx.index, types.NoneType):\n        func_lines = [\n            f\'  length = {length_expr}\',\n            f\'  self_index = {df_index_expr(self, length_expr=length_expr, as_range=True)}\',\n            f\'  if length > len(idx):\',\n            f\'    msg = ""Unalignable boolean Series provided as indexer "" + \\\\\',\n            f\'          ""(index of the boolean Series and of the indexed object do not match).""\',\n            f\'    raise IndexingError(msg)\',\n            f\'  # do not trim idx._data to length as getitem_by_mask handles such case\',\n            f\'  res_index = getitem_by_mask(self_index, idx._data)\',\n            f\'  # df index is default, same as positions so it can be used in take\'\n        ]\n        results = []\n        for i, col in enumerate(self.columns):\n            col_loc = self.column_loc[col]\n            type_id, col_id = col_loc.type_id, col_loc.col_id\n            res_data = f\'res_data_{i}\'\n            func_lines += [\n                f\'  data_{i} = self._data[{type_id}][{col_id}]\',\n                f\'  {res_data} = sdc_take(data_{i}, res_index)\'\n            ]\n            results.append((col, res_data))\n\n        data = \', \'.join(f\'""{col}"": {data}\' for col, data in results)\n        func_lines += [\n            f\'  return pandas.DataFrame({{{data}}}, index=res_index)\'\n        ]\n    else:\n        func_lines = [\n            f\'  length = {length_expr}\',\n            f\'  self_index = self.index\',\n            f\'  reindexed_idx = sdc_reindex_series(idx._data, idx.index, idx._name, self_index)\',\n            f\'  res_index = getitem_by_mask(self_index, reindexed_idx._data)\',\n            f\'  selected_pos = getitem_by_mask(numpy.arange(length), reindexed_idx._data)\'\n        ]\n        results = []\n        for i, col in enumerate(self.columns):\n            col_loc = self.column_loc[col]\n            type_id, col_id = col_loc.type_id, col_loc.col_id\n            res_data = f\'res_data_{i}\'\n            func_lines += [\n                f\'  data_{i} = self._data[{type_id}][{col_id}]\',\n                f\'  {res_data} = sdc_take(data_{i}, selected_pos)\'\n            ]\n            results.append((col, res_data))\n\n        data = \', \'.join(f\'""{col}"": {data}\' for col, data in results)\n        func_lines += [\n            f\'  return pandas.DataFrame({{{data}}}, index=res_index)\'\n        ]\n\n    return func_lines\n\n\ndef df_getitem_bool_array_idx_main_codelines(self, idx):\n    """"""Generate main code lines for df.getitem""""""\n\n    func_lines = [f\'  length = {df_length_expr(self)}\',\n                  f\'  if length != len(idx):\',\n                  f\'    raise ValueError(""Item wrong length."")\',\n                  f\'  self_index = {df_index_expr(self, as_range=True)}\',\n                  f\'  taken_pos = getitem_by_mask(self_index, idx)\',\n                  f\'  res_index = sdc_take(self_index, taken_pos)\']\n    results = []\n    for i, col in enumerate(self.columns):\n        col_loc = self.column_loc[col]\n        type_id, col_id = col_loc.type_id, col_loc.col_id\n        res_data = f\'res_data_{i}\'\n        func_lines += [\n            f\'  data_{i} = self._data[{type_id}][{col_id}]\',\n            f\'  {res_data} = sdc_take(data_{i}, taken_pos)\'\n        ]\n        results.append((col, res_data))\n\n    data = \', \'.join(f\'""{col}"": {data}\' for col, data in results)\n    func_lines += [\n        f\'  return pandas.DataFrame({{{data}}}, index=res_index)\'\n    ]\n\n    return func_lines\n\n\ndef df_getitem_key_error_codelines():\n    """"""Generate code lines to raise KeyError""""""\n    return [\'  raise KeyError(""Column is not in the DataFrame"")\']\n\n\ndef df_getitem_slice_idx_codegen(self, idx):\n    """"""\n    Example of generated implementation with provided index:\n        def _df_getitem_slice_idx_impl(self, idx):\n          self_index = numpy.arange(len(self._data[0][0]))\n          index = self_index[idx]\n          data_0 = self._data[0][0][idx]\n          res_data_0 = pandas.Series(data_0, index=index, name=""A"")\n          data_1 = self._data[1][0][idx]\n          return pandas.DataFrame({""A"": res_data_0, ""B"": res_data_1}, index=index)\n    """"""\n    func_lines = [\'def _df_getitem_slice_idx_impl(self, idx):\']\n    if self.columns:\n        func_lines += df_getitem_slice_idx_main_codelines(self, idx)\n    else:\n        # raise KeyError if input DF is empty\n        func_lines += df_getitem_key_error_codelines()\n    func_text = \'\\n\'.join(func_lines)\n    global_vars = {\'pandas\': pandas, \'numpy\': numpy}\n\n    return func_text, global_vars\n\n\ndef df_getitem_tuple_idx_codegen(self, idx):\n    """"""\n    Example of generated implementation with provided index:\n        def _df_getitem_tuple_idx_impl(self, idx):\n          res_index = numpy.arange(len(self._data[0][0]))\n          data_0 = self._data[0][0]\n          res_data_0 = pandas.Series(data_0, index=res_index, name=""A"")\n          data_2 = self._data[0][1]\n          res_data_2 = pandas.Series(data_2, index=res_index, name=""C"")\n          return pandas.DataFrame({""A"": res_data_0, ""C"": res_data_2}, index=res_index)\n    """"""\n    func_lines = [\'def _df_getitem_tuple_idx_impl(self, idx):\']\n    literal_idx = {col.literal_value for col in idx}\n    key_error = any(i not in self.columns for i in literal_idx)\n\n    if self.columns and not key_error:\n        func_lines += df_getitem_tuple_idx_main_codelines(self, literal_idx)\n    else:\n        # raise KeyError if input DF is empty or idx is invalid\n        func_lines += df_getitem_key_error_codelines()\n\n    func_text = \'\\n\'.join(func_lines)\n    global_vars = {\'pandas\': pandas, \'numpy\': numpy}\n\n    return func_text, global_vars\n\n\ndef df_getitem_bool_series_idx_codegen(self, idx):\n    """"""\n    Example of generated implementation with provided index:\n        def _df_getitem_bool_series_idx_impl(self, idx):\n          length = len(self._data[0][0])\n          self_index = range(len(self._data[0][0]))\n          if length > len(idx):\n            msg = ""Unalignable boolean Series provided as indexer "" + \\\n                  ""(index of the boolean Series and of the indexed object do not match).""\n            raise IndexingError(msg)\n          # do not trim idx._data to length as getitem_by_mask handles such case\n          res_index = getitem_by_mask(self_index, idx._data)\n          # df index is default, same as positions so it can be used in take\n          data_0 = self._data[0][0]\n          res_data_0 = sdc_take(data_0, res_index)\n          data_1 = self._data[1][0]\n          res_data_1 = sdc_take(data_1, res_index)\n          return pandas.DataFrame({""A"": res_data_0, ""B"": res_data_1}, index=res_index)\n    """"""\n    func_lines = [\'def _df_getitem_bool_series_idx_impl(self, idx):\']\n    func_lines += df_getitem_bool_series_idx_main_codelines(self, idx)\n    func_text = \'\\n\'.join(func_lines)\n    global_vars = {\'pandas\': pandas, \'numpy\': numpy,\n                   \'getitem_by_mask\': getitem_by_mask,\n                   \'sdc_take\': _sdc_take,\n                   \'sdc_reindex_series\': sdc_reindex_series,\n                   \'IndexingError\': IndexingError}\n\n    return func_text, global_vars\n\n\ndef df_getitem_bool_array_idx_codegen(self, idx):\n    """"""\n    Example of generated implementation with provided index:\n        def _df_getitem_bool_array_idx_impl(self, idx):\n          length = len(self._data[0][0])\n          if length != len(idx):\n            raise ValueError(""Item wrong length."")\n          self_index = range(len(self._data[0][0]))\n          taken_pos = getitem_by_mask(self_index, idx)\n          res_index = sdc_take(self_index, taken_pos)\n          data_0 = self._data[0][0]\n          res_data_0 = sdc_take(data_0, taken_pos)\n          data_1 = self._data[1][0]\n          res_data_1 = sdc_take(data_1, taken_pos)\n          return pandas.DataFrame({""A"": res_data_0, ""B"": res_data_1}, index=res_index)\n    """"""\n    func_lines = [\'def _df_getitem_bool_array_idx_impl(self, idx):\']\n    func_lines += df_getitem_bool_array_idx_main_codelines(self, idx)\n    func_text = \'\\n\'.join(func_lines)\n    global_vars = {\'pandas\': pandas, \'numpy\': numpy,\n                   \'getitem_by_mask\': getitem_by_mask,\n                   \'sdc_take\': _sdc_take}\n\n    return func_text, global_vars\n\n\ngen_df_getitem_slice_idx_impl = gen_impl_generator(\n    df_getitem_slice_idx_codegen, \'_df_getitem_slice_idx_impl\')\ngen_df_getitem_tuple_idx_impl = gen_impl_generator(\n    df_getitem_tuple_idx_codegen, \'_df_getitem_tuple_idx_impl\')\ngen_df_getitem_bool_series_idx_impl = gen_impl_generator(\n    df_getitem_bool_series_idx_codegen, \'_df_getitem_bool_series_idx_impl\')\ngen_df_getitem_bool_array_idx_impl = gen_impl_generator(\n    df_getitem_bool_array_idx_codegen, \'_df_getitem_bool_array_idx_impl\')\n\n\n@sdc_overload(operator.getitem)\ndef sdc_pandas_dataframe_getitem(self, idx):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n    Pandas API: pandas.DataFrame.getitem\n\n    Get data from a DataFrame by indexer.\n\n    Limitations\n    -----------\n    Supported ``key`` can be one of the following:\n\n    * String literal, e.g. :obj:`df[\'A\']`\n    * A slice, e.g. :obj:`df[2:5]`\n    * A tuple of string, e.g. :obj:`df[(\'A\', \'B\')]`\n    * An array of booleans, e.g. :obj:`df[True,False]`\n    * A series of booleans, e.g. :obj:`df(series([True,False]))`\n\n    Supported getting a column through getting attribute.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/dataframe/getitem/df_getitem_attr.py\n       :language: python\n       :lines: 37-\n       :caption: Getting Pandas DataFrame column through getting attribute.\n       :name: ex_dataframe_getitem\n\n    .. command-output:: python ./dataframe/getitem/df_getitem_attr.py\n       :cwd: ../../../examples\n\n    .. literalinclude:: ../../../examples/dataframe/getitem/df_getitem.py\n       :language: python\n       :lines: 37-\n       :caption: Getting Pandas DataFrame column where key is a string.\n       :name: ex_dataframe_getitem\n\n    .. command-output:: python ./dataframe/getitem/df_getitem.py\n       :cwd: ../../../examples\n\n    .. literalinclude:: ../../../examples/dataframe/getitem/df_getitem_slice.py\n       :language: python\n       :lines: 34-\n       :caption: Getting slice of Pandas DataFrame.\n       :name: ex_dataframe_getitem\n\n    .. command-output:: python ./dataframe/getitem/df_getitem_slice.py\n       :cwd: ../../../examples\n\n    .. literalinclude:: ../../../examples/dataframe/getitem/df_getitem_tuple.py\n       :language: python\n       :lines: 37-\n       :caption: Getting Pandas DataFrame elements where key is a tuple of strings.\n       :name: ex_dataframe_getitem\n\n    .. command-output:: python ./dataframe/getitem/df_getitem_tuple.py\n       :cwd: ../../../examples\n\n    .. literalinclude:: ../../../examples/dataframe/getitem/df_getitem_array.py\n       :language: python\n       :lines: 34-\n       :caption: Getting Pandas DataFrame elements where key is an array of booleans.\n       :name: ex_dataframe_getitem\n\n    .. command-output:: python ./dataframe/getitem/df_getitem_array.py\n       :cwd: ../../../examples\n\n    .. literalinclude:: ../../../examples/dataframe/getitem/df_getitem_series.py\n       :language: python\n       :lines: 34-\n       :caption: Getting Pandas DataFrame elements where key is series of booleans.\n       :name: ex_dataframe_getitem\n\n    .. command-output:: python ./dataframe/getitem/df_getitem_series.py\n       :cwd: ../../../examples\n\n    .. seealso::\n        :ref:`Series.getitem <pandas.Series.getitem>`\n            Get value(s) of Series by key.\n        :ref:`Series.setitem <pandas.Series.setitem>`\n            Set value to Series by index\n        :ref:`Series.loc <pandas.Series.loc>`\n            Access a group of rows and columns by label(s) or a boolean array.\n        :ref:`Series.iloc <pandas.Series.iloc>`\n            Purely integer-location based indexing for selection by position.\n        :ref:`Series.at <pandas.Series.at>`\n            Access a single value for a row/column label pair.\n        :ref:`Series.iat <pandas.Series.iat>`\n            Access a single value for a row/column pair by integer position.\n        :ref:`DataFrame.setitem <pandas.DataFrame.setitem>`\n            Set value to DataFrame by index\n        :ref:`DataFrame.loc <pandas.DataFrame.loc>`\n            Access a group of rows and columns by label(s) or a boolean array.\n        :ref:`DataFrame.iloc <pandas.DataFrame.iloc>`\n            Purely integer-location based indexing for selection by position.\n        :ref:`DataFrame.at <pandas.DataFrame.at>`\n            Access a single value for a row/column label pair.\n        :ref:`DataFrame.iat <pandas.DataFrame.iat>`\n            Access a single value for a row/column pair by integer position.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n\n    Pandas DataFrame method :meth:`pandas.DataFrame.getitem` implementation.\n\n    .. only:: developer\n\n    Test: python -m sdc.runtests -k sdc.tests.test_dataframe.TestDataFrame.test_df_getitem*\n    """"""\n    ty_checker = TypeChecker(\'Operator getitem().\')\n\n    if not isinstance(self, DataFrameType):\n        return None\n\n    if isinstance(idx, types.StringLiteral):\n        col_loc = self.column_loc.get(idx.literal_value)\n        if col_loc is None:\n            key_error = True\n        else:\n            type_id, col_id = col_loc.type_id, col_loc.col_id\n            key_error = False\n\n        def _df_getitem_str_literal_idx_impl(self, idx):\n            if key_error == False:  # noqa\n                data = self._data[type_id][col_id]\n                return pandas.Series(data, index=self._index, name=idx)\n            else:\n                raise KeyError(\'Column is not in the DataFrame\')\n\n        return _df_getitem_str_literal_idx_impl\n\n    if isinstance(idx, types.UnicodeType):\n\n        def _df_getitem_unicode_idx_impl(self, idx):\n            # http://numba.pydata.org/numba-doc/dev/developer/literal.html#specifying-for-literal-typing\n            # literally raises special exception to call getitem with literal idx value got from unicode\n            return literally(idx)\n\n        return _df_getitem_unicode_idx_impl\n\n    if isinstance(idx, types.Tuple):\n        if all([isinstance(item, types.StringLiteral) for item in idx]):\n            return gen_df_getitem_tuple_idx_impl(self, idx)\n\n    if isinstance(idx, types.SliceType):\n        return gen_df_getitem_slice_idx_impl(self, idx)\n\n    if isinstance(idx, SeriesType) and isinstance(idx.dtype, types.Boolean):\n        self_index_is_none = isinstance(self.index, types.NoneType)\n        idx_index_is_none = isinstance(idx.index, types.NoneType)\n\n        if self_index_is_none and not idx_index_is_none:\n            if not check_index_is_numeric(idx):\n                ty_checker.raise_exc(idx.index.dtype, \'number\', \'idx.index.dtype\')\n\n        if not self_index_is_none and idx_index_is_none:\n            if not check_index_is_numeric(self):\n                ty_checker.raise_exc(idx.index.dtype, self.index.dtype, \'idx.index.dtype\')\n\n        if not self_index_is_none and not idx_index_is_none:\n            if not check_types_comparable(self.index, idx.index):\n                ty_checker.raise_exc(idx.index.dtype, self.index.dtype, \'idx.index.dtype\')\n\n        return gen_df_getitem_bool_series_idx_impl(self, idx)\n\n    if isinstance(idx, types.Array) and isinstance(idx.dtype, types.Boolean):\n        return gen_df_getitem_bool_array_idx_impl(self, idx)\n\n    ty_checker = TypeChecker(\'Operator getitem().\')\n    expected_types = \'str, tuple(str), slice, series(bool), array(bool)\'\n    ty_checker.raise_exc(idx, expected_types, \'idx\')\n\n\ndef df_getitem_tuple_at_codegen(self, row, col):\n    """"""\n    Example of generated implementation:\n        def _df_getitem_tuple_at_impl(self, idx):\n            row, _ = idx\n            data = self._dataframe._data[1][0]\n            res_data = pandas.Series(data, index=self._dataframe.index)\n            return res_data.at[row]\n    """"""\n    func_lines = [\'def _df_getitem_tuple_at_impl(self, idx):\',\n                  \'  row, _ = idx\']\n    check = False\n    for i in range(len(self.columns)):\n        if self.columns[i] == col:\n            col_loc = self.column_loc[col]\n            type_id, col_id = col_loc.type_id, col_loc.col_id\n            check = True\n            func_lines += [\n                f\'  data = self._dataframe._data[{type_id}][{col_id}]\',\n                f\'  res_data = pandas.Series(data, index=self._dataframe.index)\',\n                \'  return res_data.at[row]\',\n            ]\n    if check == False:  # noqa\n        raise KeyError(\'Column is not in the DataFrame\')\n\n    func_text = \'\\n\'.join(func_lines)\n    global_vars = {\'pandas\': pandas}\n\n    return func_text, global_vars\n\n\ndef df_getitem_single_label_loc_codegen(self, idx):\n    """"""\n    Example of generated implementation:\n        def _df_getitem_single_label_loc_impl(self, idx):\n            idx_list = find_idx(self._dataframe._index, idx)\n            data_0 = _sdc_take(self._dataframe._data[0][0], idx_list)\n            res_data_0 = pandas.Series(data_0)\n            data_1 = _sdc_take(self._dataframe._data[1][0], idx_list)\n            res_data_1 = pandas.Series(data_1)\n            if len(idx_list) < 1:\n                raise KeyError(\'Index is not in the DataFrame\')\n            new_index = _sdc_take(self._dataframe._index, idx_list)\n            return pandas.DataFrame({""A"": res_data_0, ""B"": res_data_1}, index=new_index)\n    """"""\n    if isinstance(self.index, types.NoneType):\n        fill_list = [\'  idx_list =  numpy.array([idx])\']\n        new_index = [\'  new_index = numpy.array([idx])\']\n\n    else:\n        fill_list = [\'  idx_list = find_idx(self._dataframe._index, idx)\']\n        new_index = [\'  new_index = _sdc_take(self._dataframe._index, idx_list)\']\n\n    fill_list_text = \'\\n\'.join(fill_list)\n    new_index_text = \'\\n\'.join(new_index)\n    func_lines = [\'def _df_getitem_single_label_loc_impl(self, idx):\',\n                  f\'{fill_list_text}\']\n    results = []\n    for i, c in enumerate(self.columns):\n        col_loc = self.column_loc[c]\n        type_id, col_id = col_loc.type_id, col_loc.col_id\n        data = f\'data_{i}\'\n        res_data = f\'res_data_{i}\'\n        func_lines += [f\'  {data} = _sdc_take(self._dataframe._data[{type_id}][{col_id}], idx_list)\',\n                       f\'  {res_data} = pandas.Series({data})\']\n        results.append((c, res_data))\n\n    func_lines += [\'  if len(idx_list) < 1:\',\n                   ""    raise KeyError(\'Index is not in the DataFrame\')""]\n\n    data = \', \'.join(f\'""{col}"": {data}\' for col, data in results)\n    func_lines += [f\'{new_index_text}\',\n                   f\'  return pandas.DataFrame({{{data}}}, index=new_index)\']\n\n    func_text = \'\\n\'.join(func_lines)\n    global_vars = {\'pandas\': pandas, \'numpy\': numpy,\n                   \'numba\': numba,\n                   \'_sdc_take\': _sdc_take,\n                   \'find_idx\': find_idx,\n                   \'KeyError\': KeyError}\n\n    return func_text, global_vars\n\n\ndef df_getitem_int_iloc_codegen(self, idx):\n    """"""\n    Example of generated implementation:\n        def _df_getitem_int_iloc_impl(self, idx):\n            if -1 < idx < len(self._dataframe.index):\n                data_0 = pandas.Series(self._dataframe._data[0][0])\n                result_0 = data_0.iat[idx]\n                data_1 = pandas.Series(self._dataframe._data[0][1])\n                result_1 = data_1.iat[idx]\n                return pandas.Series(data=[result_0, result_1], index=[\'A\', \'B\'], name=str(idx))\n            raise IndexingError(\'Index is out of bounds for axis\')\n    """"""\n    func_lines = [\'def _df_getitem_int_iloc_impl(self, idx):\',\n                  \'  if -1 < idx < len(self._dataframe.index):\']\n    results = []\n    index = []\n    name = \'self._dataframe._index[idx]\'\n    if isinstance(self.index, types.NoneType):\n        name = \'idx\'\n    for i, c in enumerate(self.columns):\n        col_loc = self.column_loc[c]\n        type_id, col_id = col_loc.type_id, col_loc.col_id\n        result_c = f""result_{i}""\n        func_lines += [f""    data_{i} = pandas.Series(self._dataframe._data[{type_id}][{col_id}])"",\n                       f""    {result_c} = data_{i}.iat[idx]""]\n        results.append(result_c)\n        index.append(c)\n    data = \', \'.join(col for col in results)\n    func_lines += [f""    return pandas.Series(data=[{data}], index={index}, name=str({name}))"",\n                   f""  raise IndexingError(\'Index is out of bounds for axis\')""]\n\n    func_text = \'\\n\'.join(func_lines)\n    global_vars = {\'pandas\': pandas, \'numpy\': numpy, \'IndexingError\': IndexingError}\n\n    return func_text, global_vars\n\n\ndef df_getitem_slice_iloc_codegen(self, idx):\n    """"""\n    Example of generated implementation:\n        def _df_getitem_slice_iloc_impl(self, idx):\n            data_0 = pandas.Series(self._dataframe._data[0][0])\n            result_0 = data_0.iloc[idx]\n            data_1 = pandas.Series(self._dataframe._data[1][0])\n            result_1 = data_1.iloc[idx]\n            return pandas.DataFrame(data={""A"": result_0, ""B"": result_1}, index=self._dataframe.index[idx])\n    """"""\n    func_lines = [\'def _df_getitem_slice_iloc_impl(self, idx):\']\n    results = []\n    for i, c in enumerate(self.columns):\n        col_loc = self.column_loc[c]\n        type_id, col_id = col_loc.type_id, col_loc.col_id\n        result_c = f""result_{i}""\n        func_lines += [f""  data_{i} = pandas.Series(self._dataframe._data[{type_id}][{col_id}])"",\n                       f""  {result_c} = data_{i}.iloc[idx]""]\n        results.append((c, result_c))\n    data = \', \'.join(f\'""{col}"": {data}\' for col, data in results)\n    func_lines += [f""  return pandas.DataFrame(data={{{data}}}, index=self._dataframe.index[idx])""]\n\n    func_text = \'\\n\'.join(func_lines)\n    global_vars = {\'pandas\': pandas, \'numpy\': numpy}\n\n    return func_text, global_vars\n\n\ndef df_getitem_list_iloc_codegen(self, idx):\n    """"""\n    Example of generated implementation:\n        def _df_getitem_list_iloc_impl(self, idx):\n            check_idx = False\n            for i in idx:\n                if -1 < i < len(self._dataframe.index):\n                    check_idx = True\n            if check_idx == True:\n                data_0 = pandas.Series(self._dataframe._data[0][0])\n                result_0 = data_0.iloc[numpy.array(idx)]\n                data_1 = pandas.Series(self._dataframe._data[1][0])\n                result_1 = data_1.iloc[numpy.array(idx)]\n                return pandas.DataFrame(data={""A"": result_0, ""B"": result_1}, index=idx)\n            raise IndexingError(\'Index is out of bounds for axis\')\n    """"""\n    func_lines = [\'def _df_getitem_list_iloc_impl(self, idx):\',\n                  \'  check_idx = False\',\n                  \'  for i in idx:\',\n                  \'    if -1 < i < len(self._dataframe.index):\',\n                  \'      check_idx = True\',\n                  \'  if check_idx == True:\']\n    results = []\n    index = \'[self._dataframe._index[i] for i in idx]\'\n    if isinstance(self.index, types.NoneType):\n        index = \'idx\'\n    for i, c in enumerate(self.columns):\n        col_loc = self.column_loc[c]\n        type_id, col_id = col_loc.type_id, col_loc.col_id\n        result_c = f""result_{i}""\n        func_lines += [f""    data_{i} = pandas.Series(self._dataframe._data[{type_id}][{col_id}])"",\n                       f""    {result_c} = data_{i}.iloc[numpy.array(idx)]""]\n        results.append((c, result_c))\n    data = \', \'.join(f\'""{col}"": {data}\' for col, data in results)\n    func_lines += [f""    return pandas.DataFrame(data={{{data}}}, index={index})"",\n                   f""  raise IndexingError(\'Index is out of bounds for axis\')""]\n\n    func_text = \'\\n\'.join(func_lines)\n    global_vars = {\'pandas\': pandas, \'numpy\': numpy, \'IndexingError\': IndexingError}\n\n    return func_text, global_vars\n\n\ndef df_getitem_list_bool_iloc_codegen(self, idx):\n    """"""\n    Example of generated implementation:\n        def _df_getitem_list_bool_iloc_impl(self, idx):\n            if len(self._dataframe.index) == len(idx):\n                data_0 = self._dataframe._data[0][0]\n                result_0 = pandas.Series(data_0[numpy.array(idx)])\n                data_1 = self._dataframe._data[1][0]\n                result_1 = pandas.Series(data_1[numpy.array(idx)])\n                return pandas.DataFrame(data={""A"": result_0, ""B"": result_1},\n                    index=self._dataframe.index[numpy.array(idx)])\n            raise IndexingError(\'Item wrong length\')\n    """"""\n    func_lines = [\'def _df_getitem_list_bool_iloc_impl(self, idx):\']\n    results = []\n    index = \'self._dataframe.index[numpy.array(idx)]\'\n    func_lines += [\'  if len(self._dataframe.index) == len(idx):\']\n    for i, c in enumerate(self.columns):\n        col_loc = self.column_loc[c]\n        type_id, col_id = col_loc.type_id, col_loc.col_id\n        result_c = f""result_{i}""\n        func_lines += [f""    data_{i} = self._dataframe._data[{type_id}][{col_id}]"",\n                       f""    {result_c} = pandas.Series(data_{i}[numpy.array(idx)])""]\n        results.append((c, result_c))\n    data = \', \'.join(f\'""{col}"": {data}\' for col, data in results)\n    func_lines += [f""    return pandas.DataFrame(data={{{data}}}, index={index})"",\n                   f""  raise IndexingError(\'Item wrong length\')""]\n\n    func_text = \'\\n\'.join(func_lines)\n    global_vars = {\'pandas\': pandas, \'numpy\': numpy, \'IndexingError\': IndexingError}\n\n    return func_text, global_vars\n\n\ndef gen_df_getitem_tuple_at_impl(self, row, col):\n    func_text, global_vars = df_getitem_tuple_at_codegen(self, row, col)\n    loc_vars = {}\n    exec(func_text, global_vars, loc_vars)\n    _reduce_impl = loc_vars[\'_df_getitem_tuple_at_impl\']\n\n    return _reduce_impl\n\n\ngen_df_getitem_loc_single_label_impl = gen_impl_generator(\n    df_getitem_single_label_loc_codegen, \'_df_getitem_single_label_loc_impl\')\n\ngen_df_getitem_iloc_int_impl = gen_impl_generator(\n    df_getitem_int_iloc_codegen, \'_df_getitem_int_iloc_impl\')\n\ngen_df_getitem_iloc_slice_impl = gen_impl_generator(\n    df_getitem_slice_iloc_codegen, \'_df_getitem_slice_iloc_impl\')\n\ngen_df_getitem_iloc_list_impl = gen_impl_generator(\n    df_getitem_list_iloc_codegen, \'_df_getitem_list_iloc_impl\')\n\ngen_df_getitem_iloc_list_bool_impl = gen_impl_generator(\n    df_getitem_list_bool_iloc_codegen, \'_df_getitem_list_bool_iloc_impl\')\n\n\n@sdc_overload(operator.getitem)\ndef sdc_pandas_dataframe_accessor_getitem(self, idx):\n    if not isinstance(self, DataFrameGetitemAccessorType):\n        return None\n\n    accessor = self.accessor.literal_value\n\n    if accessor == \'at\':\n        num_idx = isinstance(idx[0], types.Number) and isinstance(self.dataframe.index, (types.Array, types.NoneType))\n        str_idx = (isinstance(idx[0], (types.UnicodeType, types.StringLiteral))\n                   and isinstance(self.dataframe.index, StringArrayType))\n        if isinstance(idx, types.Tuple) and isinstance(idx[1], types.StringLiteral):\n            if num_idx or str_idx:\n                row = idx[0]\n                col = idx[1].literal_value\n                return gen_df_getitem_tuple_at_impl(self.dataframe, row, col)\n\n            raise TypingError(\'Attribute at(). The row parameter type ({}) is different from the index type\\\n                              ({})\'.format(type(idx[0]), type(self.dataframe.index)))\n\n        raise TypingError(\'Attribute at(). The index must be a row and literal column. Given: {}\'.format(idx))\n\n    if accessor == \'loc\':\n        if isinstance(idx, (types.Integer, types.UnicodeType, types.StringLiteral)):\n            return gen_df_getitem_loc_single_label_impl(self.dataframe, idx)\n\n        ty_checker = TypeChecker(\'Attribute loc().\')\n        ty_checker.raise_exc(idx, \'int or str\', \'idx\')\n\n    if accessor == \'iat\':\n        if isinstance(idx, types.Tuple) and isinstance(idx[1], types.Literal):\n            col = idx[1].literal_value\n            if -1 < col < len(self.dataframe.columns):\n                col_loc = self.dataframe.column_loc[self.dataframe.columns[col]]\n                type_id, col_id = col_loc.type_id, col_loc.col_id\n\n                def df_getitem_iat_tuple_impl(self, idx):\n                    row, _ = idx\n                    if -1 < row < len(self._dataframe.index):\n                        data = self._dataframe._data[type_id][col_id]\n                        res_data = pandas.Series(data)\n                        return res_data.iat[row]\n\n                    raise IndexingError(\'Index is out of bounds for axis\')\n\n                return df_getitem_iat_tuple_impl\n\n            raise IndexingError(\'Index is out of bounds for axis\')\n\n        raise TypingError(\'Operator getitem(). The index must be a row and literal column. Given: {}\'.format(idx))\n\n    if accessor == \'iloc\':\n        if isinstance(idx, types.SliceType):\n            return gen_df_getitem_iloc_slice_impl(self.dataframe, idx)\n\n        if (\n            isinstance(idx, (types.List, types.Array)) and\n            isinstance(idx.dtype, (types.Boolean, bool))\n        ):\n            return gen_df_getitem_iloc_list_bool_impl(self.dataframe, idx)\n\n        if isinstance(idx, types.List):\n            return gen_df_getitem_iloc_list_impl(self.dataframe, idx)\n\n        if isinstance(idx, types.Integer):\n            return gen_df_getitem_iloc_int_impl(self.dataframe, idx)\n\n        if isinstance(idx, (types.Tuple, types.UniTuple)):\n            def df_getitem_tuple_iat_impl(self, idx):\n                return self._dataframe.iat[idx]\n\n            return df_getitem_tuple_iat_impl\n\n        raise TypingError(\'Attribute iloc(). The index must be an integer, a list or array of integers,\\\n                          a slice object with ints or a boolean array.\\\n                          Given: {}\'.format(idx))\n\n    raise TypingError(\'Operator getitem(). Unknown accessor. Only ""loc"", ""iloc"", ""at"", ""iat"" are supported.\\\n                      Given: {}\'.format(accessor))\n\n\n@sdc_overload_attribute(DataFrameType, \'iloc\')\ndef sdc_pandas_dataframe_iloc(self):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.DataFrame.iloc\n\n    Limitations\n    -----------\n    - Parameter ``\'name\'`` in new DataFrame can be String only\n    - Column can be literal value only, in DataFrame.iloc[row, column]\n    - Iloc works with basic cases only: an integer, a list or array of integers,\n        a slice object with ints, a boolean array\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/dataframe/dataframe_iloc.py\n       :language: python\n       :lines: 36-\n       :caption: Get value at specified index position.\n       :name: ex_dataframe_iloc\n\n    .. command-output:: python ./dataframe/dataframe_iloc.py\n       :cwd: ../../../examples\n\n    .. seealso::\n\n        :ref:`DataFrame.iat <pandas.DataFrame.iat>`\n            Fast integer location scalar accessor.\n\n        :ref:`DataFrame.loc <pandas.DataFrame.loc>`\n            Purely label-location based indexer for selection by label.\n\n        :ref:`Series.iloc <pandas.Series.iloc>`\n            Purely integer-location based indexing for selection by position.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas DataFrame method :meth:`pandas.DataFrame.iloc` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_dataframe.TestDataFrame.test_df_iloc*\n    """"""\n\n    ty_checker = TypeChecker(\'Attribute iloc().\')\n    ty_checker.check(self, DataFrameType)\n\n    def sdc_pandas_dataframe_iloc_impl(self):\n        return dataframe_getitem_accessor_init(self, \'iloc\')\n\n    return sdc_pandas_dataframe_iloc_impl\n\n\n@sdc_overload_attribute(DataFrameType, \'iat\')\ndef sdc_pandas_dataframe_iat(self):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.DataFrame.iat\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/dataframe/dataframe_iat.py\n       :language: python\n       :lines: 28-\n       :caption: Get value at specified index position.\n       :name: ex_dataframe_iat\n\n    .. command-output:: python ./dataframe/dataframe_iat.py\n       :cwd: ../../../examples\n\n    .. seealso::\n\n        :ref:`DataFrame.at <pandas.DataFrame.at>`\n            Access a single value for a row/column label pair.\n\n        :ref:`DataFrame.loc <pandas.DataFrame.loc>`\n            Purely label-location based indexer for selection by label.\n\n        :ref:`DataFrame.iloc <pandas.DataFrame.iloc>`\n            Access group of rows and columns by integer position(s).\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas DataFrame method :meth:`pandas.DataFrame.iat` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_dataframe.TestDataFrame.test_dataframe_iat*\n    """"""\n\n    ty_checker = TypeChecker(\'Attribute iat().\')\n    ty_checker.check(self, DataFrameType)\n\n    def sdc_pandas_dataframe_iat_impl(self):\n        return dataframe_getitem_accessor_init(self, \'iat\')\n\n    return sdc_pandas_dataframe_iat_impl\n\n\n@sdc_overload_attribute(DataFrameType, \'at\')\ndef sdc_pandas_dataframe_at(self):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Limitations\n    -----------\n    - ``Dataframe.at`` always returns ``array``.\n    - Parameter ``column`` in ``idx`` must be a literal value.\n\n    Pandas API: pandas.DataFrame.at\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/dataframe/dataframe_at.py\n       :language: python\n       :lines: 28-\n       :caption: Access a single value for a row/column label pair.\n       :name: ex_dataframe_at\n\n    .. command-output:: python ./dataframe/dataframe_at.py\n       :cwd: ../../../examples\n\n    .. seealso::\n\n        :ref:`DataFrame.iat <pandas.DataFrame.iat>`\n            Access a single value for a row/column pair by integer position.\n\n        :ref:`DataFrame.loc <pandas.DataFrame.loc>`\n            Access a group of rows and columns by label(s).\n\n        :ref:`Series.at <pandas.Series.at>`\n            Access a single value using a label.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas DataFrame method :meth:`pandas.DataFrame.at` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_dataframe.TestDataFrame.test_df_at*\n    """"""\n\n    ty_checker = TypeChecker(\'Attribute at().\')\n    ty_checker.check(self, DataFrameType)\n\n    def sdc_pandas_dataframe_at_impl(self):\n        return dataframe_getitem_accessor_init(self, \'at\')\n\n    return sdc_pandas_dataframe_at_impl\n\n\n@sdc_overload_attribute(DataFrameType, \'loc\')\ndef sdc_pandas_dataframe_loc(self):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.DataFrame.loc\n\n    Limitations\n    -----------\n    - Loc always returns Dataframe.\n    - Parameter ``idx`` is supported only to be a single value, e.g. :obj:`df.loc[\'A\']`.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/dataframe/dataframe_loc.py\n       :language: python\n       :lines: 36-\n       :caption: Access a group of rows and columns by label(s) or a boolean array.\n       :name: ex_dataframe_loc\n\n    .. command-output:: python ./dataframe/dataframe_loc.py\n       :cwd: ../../../examples\n\n    .. seealso::\n        :ref:`DataFrame.at <pandas.DataFrame.at>`\n            Access a single value for a row/column label pair.\n        :ref:`DataFrame.iloc <pandas.DataFrame.iloc>`\n            Access group of rows and columns by integer position(s).\n        :ref:`DataFrame.xs <pandas.DataFrame.xs>`\n            Returns a cross-section (row(s) or column(s)) from the Series/DataFrame.\n        :ref:`Series.loc <pandas.Series.loc>`\n            Access group of values using labels.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas DataFrame method :meth:`pandas.DataFrame.loc` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_dataframe.TestDataFrame.test_df_loc*\n    """"""\n\n    ty_checker = TypeChecker(\'Attribute loc().\')\n    ty_checker.check(self, DataFrameType)\n\n    def sdc_pandas_dataframe_loc_impl(self):\n        return sdc.datatypes.hpat_pandas_dataframe_getitem_types.dataframe_getitem_accessor_init(self, \'loc\')\n\n    return sdc_pandas_dataframe_loc_impl\n\n\n@sdc_overload_method(DataFrameType, \'pct_change\')\ndef pct_change_overload(df, periods=1, fill_method=\'pad\', limit=None, freq=None):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.DataFrame.pct_change\n\n    Limitations\n    -----------\n    Parameters ``limit`` and ``freq`` are supported only with default value ``None``.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/dataframe/dataframe_pct_change.py\n        :language: python\n        :lines: 36-\n        :caption: Percentage change between the current and a prior element.\n        :name: ex_dataframe_pct_change\n\n    .. command-output:: python ./dataframe/dataframe_pct_change.py\n        :cwd: ../../../examples\n\n    .. seealso::\n\n        :ref:`Series.diff <pandas.Series.diff>`\n            Compute the difference of two elements in a Series.\n\n        :ref:`DataFrame.diff <pandas.DataFrame.diff>`\n            Compute the difference of two elements in a DataFrame.\n\n        :ref:`Series.shift <pandas.Series.shift>`\n            Shift the index by some number of periods.\n\n        :ref:`DataFrame.shift <pandas.DataFrame.shift>`\n            Shift the index by some number of periods.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas DataFrame method :meth:`pandas.DataFrame.pct_change` implementation.\n\n    .. only:: developer\n\n      Test: python -m sdc.runtests -k sdc.tests.test_dataframe.TestDataFrame.test_pct_change*\n    """"""\n\n    name = \'pct_change\'\n\n    ty_checker = TypeChecker(\'Method {}().\'.format(name))\n    ty_checker.check(df, DataFrameType)\n\n    if not isinstance(periods, (types.Integer, types.Omitted)) and periods != 1:\n        ty_checker.raise_exc(periods, \'int64\', \'periods\')\n\n    if not isinstance(fill_method, (str, types.UnicodeType, types.StringLiteral, types.NoneType, types.Omitted)):\n        ty_checker.raise_exc(fill_method, \'string\', \'fill_method\')\n\n    if not isinstance(limit, (types.Omitted, types.NoneType)) and limit is not None:\n        ty_checker.raise_exc(limit, \'None\', \'limit\')\n\n    if not isinstance(freq, (types.Omitted, types.NoneType)) and freq is not None:\n        ty_checker.raise_exc(freq, \'None\', \'freq\')\n\n    params = {\'periods\': 1, \'fill_method\': \'""pad""\', \'limit\': None, \'freq\': None}\n    ser_par = {\'periods\': \'periods\', \'fill_method\': \'fill_method\', \'limit\': \'limit\', \'freq\': \'freq\'}\n\n    return sdc_pandas_dataframe_apply_columns(df, name, params, ser_par)\n\n\n@sdc_overload_method(DataFrameType, \'groupby\')\ndef sdc_pandas_dataframe_groupby(self, by=None, axis=0, level=None, as_index=True, sort=True,\n                                 group_keys=True, squeeze=False, observed=False):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n    Pandas API: pandas.DataFrame.groupby\n\n    Limitations\n    -----------\n    - Parameters ``axis``, ``level``, ``as_index``, ``group_keys``, ``squeeze`` and ``observed`` \\\nare currently unsupported by Intel Scalable Dataframe Compiler\n    - Parameter ``by`` is supported as single literal column name only\n    - Mutating the contents of a DataFrame between creating a groupby object and calling it\'s methods is unsupported\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/dataframe/groupby/dataframe_groupby_min.py\n       :language: python\n       :lines: 27-\n       :caption: Groupby and calculate the minimum in each group.\n       :name: ex_dataframe_groupby\n\n    .. command-output:: python ./dataframe/groupby/dataframe_groupby_min.py\n       :cwd: ../../../examples\n\n    .. seealso::\n        :ref:`resample <pandas.DataFrame.resample>`\n            Resample time-series data.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n\n    Pandas DataFrame attribute :meth:`pandas.DataFrame.groupby` implementation\n    .. only:: developer\n\n    Test: python -m sdc.runtests -k sdc.tests.test_groupby.TestGroupBy.test_dataframe_groupby*\n\n    Parameters\n    ----------\n\n    self: :obj:`pandas.DataFrame`\n        Input DataFrame.\n    by: :obj:`mapping`, :obj:`function`, :obj:`string` or :obj:`list`\n        Used to determine the groups for the groupby.\n    axis : :obj:`int` or :obj:`string`, default 0\n        Split along rows (0) or columns (1).\n    level : :obj:`int` or :obj:`str`, default None\n        If the axis is a MultiIndex (hierarchical), group by a particular\n        level or levels.\n    as_index : :obj:`bool`, default True\n        For aggregated output, return object with group labels as the\n        index.\n    sort : :obj:`bool`, default True\n        Sort group keys. Get better performance by turning this off.\n        Note this does not influence the order of observations within each\n        group. Groupby preserves the order of rows within each group.\n    group_keys : :obj:`bool`, default True\n        When calling apply, add group keys to index to identify pieces.\n    squeeze : :obj:`bool`, default False\n        Reduce the dimensionality of the return type if possible,\n        otherwise return a consistent type.\n    observed : :obj:`bool`, default False\n        This only applies if any of the groupers are Categoricals.\n        If True: only show observed values for categorical groupers.\n        If False: show all values for categorical groupers.\n\n    Returns\n    -------\n    :class:`pandas.DataFrameGroupBy`\n        Returns a groupby object that contains information about the groups.\n""""""\n\n    if not isinstance(by, types.StringLiteral):\n        return None\n\n    column_id = self.columns.index(by.literal_value)\n    list_type = types.ListType(types.int64)\n    by_type = self.data[column_id].dtype\n\n    col_loc = self.column_loc[by.literal_value]\n    type_id, col_id = col_loc.type_id, col_loc.col_id\n\n    def sdc_pandas_dataframe_groupby_impl(self, by=None, axis=0, level=None, as_index=True, sort=True,\n                                          group_keys=True, squeeze=False, observed=False):\n\n        by_column_data = self._data[type_id][col_id]\n        chunks = parallel_chunks(len(by_column_data))\n        dict_parts = [Dict.empty(by_type, list_type) for _ in range(len(chunks))]\n\n        # filling separate dict of by_value -> positions for each chunk of initial array\n        for i in numba.prange(len(chunks)):\n            chunk = chunks[i]\n            res = dict_parts[i]\n            for j in range(chunk.start, chunk.stop):\n                if isna(by_column_data, j):\n                    continue\n                value = by_column_data[j]\n                group_list = res.get(value)\n                if group_list is None:\n                    new_group_list = List.empty_list(types.int64)\n                    new_group_list.append(j)\n                    res[value] = new_group_list\n                else:\n                    group_list.append(j)\n\n        # merging all dict parts into a single resulting dict\n        res_dict = dict_parts[0]\n        for i in range(1, len(chunks)):\n            res_dict = merge_groupby_dicts_inplace(res_dict, dict_parts[i])\n\n        return init_dataframe_groupby(self, column_id, res_dict, sort)\n\n    return sdc_pandas_dataframe_groupby_impl\n\n\ndef df_set_column_index_codelines(self):\n    """"""Generate code lines with definition of resulting index for DF set_column""""""\n    func_lines = []\n    if self.columns:\n        func_lines += [\n            f\'  length = {df_length_expr(self)}\',\n            f\'  if length == 0:\',\n            f\'    raise SDCLimitation(""Could not set item for DataFrame with empty columns"")\',\n            f\'  elif length != len(value):\',\n            f\'    raise ValueError(""Length of values does not match length of index"")\',\n        ]\n    else:\n        func_lines += [\'  length = len(value)\']\n    func_lines += [f\'  res_index = {df_index_expr(self, length_expr=""length"")}\']\n\n    return func_lines\n\n\ndef df_add_column_codelines(self, key):\n    """"""Generate code lines to add new column to DF""""""\n    func_lines = df_set_column_index_codelines(self)  # provide res_index = ...\n\n    results = []\n    for i, col in enumerate(self.columns):\n        col_loc = self.column_loc[col]\n        type_id, col_id = col_loc.type_id, col_loc.col_id\n        res_data = f\'res_data_{i}\'\n        func_lines += [\n            f\'  data_{i} = self._data[{type_id}][{col_id}]\',\n            f\'  {res_data} = pandas.Series(data_{i}, index=res_index, name=""{col}"")\',\n        ]\n        results.append((col, res_data))\n\n    res_data = \'new_res_data\'\n    literal_key = key.literal_value\n    func_lines += [f\'  {res_data} = pandas.Series(value, index=res_index, name=""{literal_key}"")\']\n    results.append((literal_key, res_data))\n\n    data = \', \'.join(f\'""{col}"": {data}\' for col, data in results)\n    func_lines += [f\'  return pandas.DataFrame({{{data}}}, index=res_index)\']\n\n    return func_lines\n\n\ndef df_replace_column_codelines(self, key):\n    """"""Generate code lines to replace existing column in DF""""""\n    func_lines = df_set_column_index_codelines(self)  # provide res_index = ...\n\n    results = []\n    literal_key = key.literal_value\n    for i, col in enumerate(self.columns):\n        if literal_key == col:\n            func_lines += [f\'  data_{i} = value\']\n        else:\n            col_loc = self.column_loc[col]\n            type_id, col_id = col_loc.type_id, col_loc.col_id\n            func_lines += [f\'  data_{i} = self._data[{type_id}][{col_id}]\']\n\n        res_data = f\'res_data_{i}\'\n        func_lines += [\n            f\'  {res_data} = pandas.Series(data_{i}, index=res_index, name=""{col}"")\',\n        ]\n        results.append((col, res_data))\n\n    data = \', \'.join(f\'""{col}"": {data}\' for col, data in results)\n    func_lines += [f\'  return pandas.DataFrame({{{data}}}, index=res_index)\']\n\n    return func_lines\n\n\ndef df_add_column_codegen(self, key):\n    """"""\n    Example of generated implementation:\n    def _df_add_column_impl(self, key, value):\n      length = len(self._data[0])\n      if length == 0:\n        raise SDCLimitation(""Could not set item for empty DataFrame"")\n      elif length != len(value):\n        raise ValueError(""Length of values does not match length of index"")\n      res_index = numpy.arange(length)\n      data_0 = self._data[0]\n      res_data_0 = pandas.Series(data_0, index=res_index, name=""A"")\n      data_1 = self._data[1]\n      res_data_1 = pandas.Series(data_1, index=res_index, name=""C"")\n      new_res_data = pandas.Series(value, index=res_index, name=""B"")\n      return pandas.DataFrame({""A"": res_data_0, ""C"": res_data_1, ""B"": new_res_data}, index=res_index)\n    """"""\n    func_lines = [f\'def _df_add_column_impl(self, key, value):\']\n    func_lines += df_add_column_codelines(self, key)\n\n    func_text = \'\\n\'.join(func_lines)\n    global_vars = {\'pandas\': pandas, \'numpy\': numpy,\n                   \'SDCLimitation\': SDCLimitation}\n\n    return func_text, global_vars\n\n\ndef df_replace_column_codegen(self, key):\n    """"""\n    Example of generated implementation:\n    def _df_replace_column_impl(self, key, value):\n      length = len(self._data[0])\n      if length == 0:\n        raise SDCLimitation(""Could not set item for DataFrame with empty columns"")\n      elif length != len(value):\n        raise ValueError(""Length of values does not match length of index"")\n      res_index = numpy.arange(length)\n      data_0 = value\n      res_data_0 = pandas.Series(data_0, index=res_index, name=""A"")\n      data_1 = self._data[1]\n      res_data_1 = pandas.Series(data_1, index=res_index, name=""C"")\n      return pandas.DataFrame({""A"": res_data_0, ""C"": res_data_1}, index=res_index)\n    """"""\n    func_lines = [f\'def _df_replace_column_impl(self, key, value):\']\n    func_lines += df_replace_column_codelines(self, key)\n\n    func_text = \'\\n\'.join(func_lines)\n    global_vars = {\'pandas\': pandas, \'numpy\': numpy,\n                   \'SDCLimitation\': SDCLimitation}\n\n    return func_text, global_vars\n\n\ngen_df_add_column_impl = gen_impl_generator(\n    df_add_column_codegen, \'_df_add_column_impl\')\ngen_df_replace_column_impl = gen_impl_generator(\n    df_replace_column_codegen, \'_df_replace_column_impl\')\n\n\n@sdc_overload_method(DataFrameType, \'_set_column\')\ndef df_set_column_overload(self, key, value):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n    Pandas API: pandas.DataFrame.setitem\n\n    Set data to a DataFrame by indexer.\n\n    Limitations\n    -----------\n    - Supported setting a column in a DataFrame through private method ``df._set_column(key, value)``.\n    - DataFrame passed into jit region as a parameter is not changed outside of the region.\n    New DataFrame should be returned from the region in this case.\n    - Supported setting a column in a non-empty DataFrame as a 1D array only.\n\n    .. literalinclude:: ../../../examples/dataframe/setitem/df_set_new_column.py\n       :language: python\n       :lines: 37-\n       :caption: Setting new column to the DataFrame.\n       :name: ex_dataframe_set_new_column\n\n    .. command-output:: python ./dataframe/setitem/df_set_new_column.py\n       :cwd: ../../../examples\n\n    .. literalinclude:: ../../../examples/dataframe/setitem/df_set_existing_column.py\n       :language: python\n       :lines: 37-\n       :caption: Setting data to existing column of the DataFrame.\n       :name: ex_dataframe_set_existing_column\n\n    .. command-output:: python ./dataframe/setitem/df_set_existing_column.py\n       :cwd: ../../../examples\n\n    .. seealso::\n        :ref:`Series.getitem <pandas.Series.getitem>`\n            Get value(s) of Series by key.\n        :ref:`Series.setitem <pandas.Series.setitem>`\n            Set value to Series by index\n        :ref:`Series.loc <pandas.Series.loc>`\n            Access a group of rows and columns by label(s) or a boolean array.\n        :ref:`Series.iloc <pandas.Series.iloc>`\n            Purely integer-location based indexing for selection by position.\n        :ref:`Series.at <pandas.Series.at>`\n            Access a single value for a row/column label pair.\n        :ref:`Series.iat <pandas.Series.iat>`\n            Access a single value for a row/column pair by integer position.\n        :ref:`DataFrame.getitem <pandas.DataFrame.getitem>`\n            Set value to DataFrame by index\n        :ref:`DataFrame.loc <pandas.DataFrame.loc>`\n            Access a group of rows and columns by label(s) or a boolean array.\n        :ref:`DataFrame.iloc <pandas.DataFrame.iloc>`\n            Purely integer-location based indexing for selection by position.\n        :ref:`DataFrame.at <pandas.DataFrame.at>`\n            Access a single value for a row/column label pair.\n        :ref:`DataFrame.iat <pandas.DataFrame.iat>`\n            Access a single value for a row/column pair by integer position.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n\n    Test: python -m sdc.runtests -k sdc.tests.test_dataframe.TestDataFrame.test_df_add_column\n    Test: python -m sdc.runtests -k sdc.tests.test_dataframe.TestDataFrame.test_df_replace_column\n    """"""\n    if not isinstance(self, DataFrameType):\n        return None\n\n    if isinstance(key, types.StringLiteral):\n        try:\n            self.columns.index(key.literal_value)\n        except ValueError:\n            return gen_df_add_column_impl(self, key)\n        else:\n            return gen_df_replace_column_impl(self, key)\n\n    if isinstance(key, types.UnicodeType):\n\n        def _df_set_column_unicode_key_impl(self, key, value):\n            # http://numba.pydata.org/numba-doc/dev/developer/literal.html#specifying-for-literal-typing\n            # literally raises special exception to call df._set_column with literal idx value got from unicode\n            return literally(key)\n\n        return _df_set_column_unicode_key_impl\n\n    ty_checker = TypeChecker(\'Method _set_column().\')\n    ty_checker.raise_exc(key, \'str\', \'key\')\n\n\ndef sdc_pandas_dataframe_reset_index_codegen(drop, all_params, columns, column_loc):\n    """"""\n    Example of generated implementation:\n        def _df_reset_index_impl(self, level=None, drop=False, inplace=False, col_level=0, col_fill=""""):\n          result_0 = self._data[0][0]\n          result_1 = self._data[0][1]\n          return pandas.DataFrame({""A"": result_0, ""B"": result_1})\n    """"""\n    result_name = []\n    all_params_str = \', \'.join(all_params)\n    func_lines = [f\'def _df_reset_index_impl({all_params_str}):\']\n    if not drop:\n        old_index = \'old_index\'\n        func_lines += [f\'  {old_index} = self.index\']\n        result_name.append((old_index, \'index\'))\n    for i, c in enumerate(columns):\n        col_loc = column_loc[c]\n        type_id, col_id = col_loc.type_id, col_loc.col_id\n        result_c = f\'result_{i}\'\n        func_lines += [\n            f\'  result_{i} = self._data[{type_id}][{col_id}]\'\n        ]\n        result_name.append((result_c, c))\n    data = \', \'.join(f\'""{column_name}"": {column}\' for column, column_name in result_name)\n    func_lines += [f\'  return pandas.DataFrame({{{data}}})\']\n    func_text = \'\\n\'.join(func_lines)\n\n    global_vars = {\'pandas\': pandas,\n                   \'numpy\': numpy}\n\n    return func_text, global_vars\n\n\ndef sdc_pandas_dataframe_reset_index_impl(self, drop=False):\n    all_params = [\'self\', \'level=None\', \'drop=False\', \'inplace=False\', \'col_level=0\', \'col_fill=""""\']\n    func_text, global_vars = sdc_pandas_dataframe_reset_index_codegen(drop, all_params,\n                                                                      self.columns, self.column_loc)\n    loc_vars = {}\n    exec(func_text, global_vars, loc_vars)\n    _apply_impl = loc_vars[f\'_df_reset_index_impl\']\n\n    return _apply_impl\n\n\n@sdc_overload_method(DataFrameType, \'reset_index\')\ndef sdc_pandas_dataframe_reset_index(self, level=None, drop=False, inplace=False, col_level=0, col_fill=\'\'):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n    Pandas API: pandas.DataFrame.reset_index\n\n    Limitations\n    -----------\n    - Reset the index of the DataFrame, and use the default one instead.\n    - Parameters level, inplacem col_level, col_fill unsupported.\n    - Parameter drop can be only literal value or default value.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/dataframe/dataframe_reset_index_drop_False.py\n        :language: python\n        :lines: 36-\n        :caption: Reset the index of the DataFrame, and use the default one instead.\n                  The old index becomes the first column.\n        :name: ex_dataframe_reset_index\n\n    .. command-output:: python ./dataframe/dataframe_reset_index_drop_False.py\n        :cwd: ../../../examples\n\n    .. literalinclude:: ../../../examples/dataframe/dataframe_reset_index_drop_True.py\n        :language: python\n        :lines: 36-\n        :caption: Reset the index of the DataFrame, and use the default one instead.\n        :name: ex_dataframe_reset_index\n\n    .. command-output:: python ./dataframe/dataframe_reset_index_drop_True.py\n        :cwd: ../../../examples\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas DataFrame method :meth:`pandas.DataFrame.reset_index` implementation.\n\n   .. only:: developer\n\n       Test: python -m sdc.runtests -k sdc.tests.test_dataframe.TestDataFrame.test_df_reset_index*\n   """"""\n\n    func_name = \'reset_index\'\n\n    ty_checker = TypeChecker(\'Method {}().\'.format(func_name))\n    ty_checker.check(self, DataFrameType)\n\n    if not (level is None or isinstance(level, types.Omitted)):\n        raise TypingError(\'{} Unsupported parameter level. Given: {}\'.format(func_name, level))\n\n    if not isinstance(drop, (types.Omitted, types.Boolean, bool)):\n        ty_checker.raise_exc(drop, \'bool\', \'drop\')\n\n    if not (inplace is False or isinstance(inplace, types.Omitted)):\n        raise TypingError(\'{} Unsupported parameter inplace. Given: {}\'.format(func_name, inplace))\n\n    if not (col_level == 0 or isinstance(col_level, types.Omitted)):\n        raise TypingError(\'{} Unsupported parameter col_level. Given: {}\'.format(func_name, col_level))\n\n    if not (col_fill == \'\' or isinstance(col_fill, types.Omitted)):\n        raise TypingError(\'{} Unsupported parameter col_fill. Given: {}\'.format(func_name, col_fill))\n\n    if isinstance(drop, types.Literal):\n        literal_drop = drop.literal_value\n        return sdc_pandas_dataframe_reset_index_impl(self, drop=literal_drop)\n    elif isinstance(drop, types.Omitted):\n        return sdc_pandas_dataframe_reset_index_impl(self, drop=drop.value)\n    elif isinstance(drop, bool):\n        return sdc_pandas_dataframe_reset_index_impl(self, drop=drop)\n\n    raise SDCLimitation(\'Method {}(). Parameter drop is only supported as a literal.\'.format(func_name))\n\n'"
sdc/datatypes/hpat_pandas_dataframe_getitem_types.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\nimport pandas\n\nfrom numba import types\nfrom numba.core import cgutils\nfrom numba.extending import models, overload, register_model, make_attribute_wrapper, intrinsic\nfrom numba.core.datamodel import register_default, StructModel\nfrom numba.core.typing.templates import signature\n\n\nclass DataFrameGetitemAccessorType(types.Type):\n    def __init__(self, dataframe, accessor):\n        self.dataframe = dataframe\n        self.accessor = accessor\n        super(DataFrameGetitemAccessorType, self).__init__(\'DataFrameGetitemAccessorType({}, {})\\\n            \'.format(dataframe, accessor))\n\n\n@register_model(DataFrameGetitemAccessorType)\nclass DataFrameGetitemAccessorTypeModel(StructModel):\n    def __init__(self, dmm, fe_type):\n        members = [\n            (\'dataframe\', fe_type.dataframe),\n        ]\n        models.StructModel.__init__(self, dmm, fe_type, members)\n\n\nmake_attribute_wrapper(DataFrameGetitemAccessorType, \'dataframe\', \'_dataframe\')\n\n\n@intrinsic\ndef dataframe_getitem_accessor_init(typingctx, dataframe, accessor):\n    def dataframe_getitem_accessor_init_codegen(context, builder, signature, args):\n        dataframe_val, accessor_val = args\n        getitem_accessor = cgutils.create_struct_proxy(\n            signature.return_type)(context, builder)\n        getitem_accessor.dataframe = dataframe_val\n\n        if context.enable_nrt:\n            context.nrt.incref(builder, signature.args[0], dataframe_val)\n\n        return getitem_accessor._getvalue()\n\n    ret_typ = DataFrameGetitemAccessorType(dataframe, accessor)\n    sig = signature(ret_typ, dataframe, accessor)\n\n    return sig, dataframe_getitem_accessor_init_codegen\n'"
sdc/datatypes/hpat_pandas_dataframe_pass.py,5,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\'\'\'\n| Procedures are required for SDC DataFrameType handling in Numba\n\'\'\'\n\nimport numpy\nimport pandas\n\nimport numba\nfrom numba import types\nfrom numba.core import ir, ir_utils\nfrom numba.core.compiler_machinery import FunctionPass, register_pass\nfrom numba.core.ir_utils import find_topo_order, build_definitions, guard, find_callname\n\nimport sdc\n\n\n@register_pass(mutates_CFG=True, analysis_only=False)\nclass SDC_Pandas_DataFrame_TransformationPass_Stage2(FunctionPass):\n    """"""\n    This transformation pass replaces known function definitions with implementation\n    """"""\n\n    _name = ""sdc_dataframe_transformationpass_stage2""\n\n    def __init__(self):\n        FunctionPass.__init__(self)\n\n    def run_pass(self, state):\n        self.state = state\n\n        blocks = self.state.func_ir.blocks\n\n        topo_order = find_topo_order(blocks)\n        work_list = list((l, blocks[l]) for l in reversed(topo_order))\n        while work_list:\n            label, block = work_list.pop()\n            new_body = []\n            replaced = False\n            for i, inst in enumerate(block.body):\n                out_nodes = [inst]\n\n                if isinstance(inst, ir.Assign):\n                    if inst.value in self.state.func_ir._definitions[inst.target.name]:\n                        self.state.func_ir._definitions[inst.target.name].remove(inst.value)\n                    out_nodes = self._run_assign(inst)\n\n                if isinstance(out_nodes, list):\n                    new_body.extend(out_nodes)\n                    self._update_definitions(out_nodes)\n\n                if isinstance(out_nodes, sdc.utilities.utils.ReplaceFunc):\n                    rp_func = out_nodes\n                    if rp_func.pre_nodes is not None:\n                        new_body.extend(rp_func.pre_nodes)\n                        self._update_definitions(rp_func.pre_nodes)\n\n                    inst.value = ir.Expr.call(ir.Var(block.scope, ""dummy"", inst.loc), rp_func.args, (), inst.loc)\n                    block.body = new_body + block.body[i:]\n                    sdc.utilities.utils.update_globals(rp_func.func, rp_func.glbls)\n                    numba.core.inline_closurecall.inline_closure_call(self.state.func_ir,\n                                                                 rp_func.glbls,\n                                                                 block,\n                                                                 len(new_body),\n                                                                 rp_func.func,\n                                                                 self.state.typingctx,\n                                                                 rp_func.arg_types,\n                                                                 self.state.typemap,\n                                                                 self.state.calltypes,\n                                                                 work_list)\n                    replaced = True\n                    break\n                if isinstance(out_nodes, dict):\n                    block.body = new_body + block.body[i:]\n                    sdc.utilities.utils.inline_new_blocks(self.state.func_ir, block, i, out_nodes, work_list)\n                    replaced = True\n                    break\n\n            if not replaced:\n                blocks[label].body = new_body\n\n        # XXX remove slice() of h5 read due to Numba\'s #3380 bug\n        self.state.func_ir.blocks = ir_utils.simplify_CFG(self.state.func_ir.blocks)\n        while ir_utils.remove_dead(self.state.func_ir.blocks,\n                                   self.state.func_ir.arg_names,\n                                   self.state.func_ir,\n                                   self.state.typemap):\n            pass\n\n        self.state.func_ir._definitions = build_definitions(self.state.func_ir.blocks)\n        ir_utils.dprint_func_ir(self.state.func_ir, self._name)\n\n        return True\n\n    def _run_assign(self, assign):\n        lhs = assign.target.name\n        rhs = assign.value\n\n        if isinstance(rhs, ir.Expr):\n            if rhs.op == \'getattr\':\n                return self._run_getattr(assign, rhs)\n\n            if rhs.op == \'call\':\n                return self._run_call(assign, lhs, rhs)\n\n        return [assign]\n\n    def _run_getattr(self, assign, rhs):\n        rhs_type = self.state.typemap[rhs.value.name]  # get type of rhs value ""S""\n\n        if (rhs.attr == \'dtype\'\n            and (sdc.hiframes.pd_series_ext.is_series_type(rhs_type) or isinstance(rhs_type, types.Array))\n            and isinstance(rhs_type.dtype, (types.NPDatetime, types.NPTimedelta))):\n\n            assign.value = ir.Global(""numpy.datetime64"", rhs_type.dtype, rhs.loc)\n            return [assign]\n\n        if (rhs.attr == \'dtype\' and isinstance(sdc.hiframes.pd_series_ext.if_series_to_array_type(rhs_type), types.Array)):\n            typ_str = str(rhs_type.dtype)\n            assign.value = ir.Global(""numpy.dtype({})"".format(typ_str), numpy.dtype(typ_str), rhs.loc)\n            return [assign]\n\n        # PR135. This needs to be commented out\n        if isinstance(rhs_type, sdc.hiframes.pd_series_ext.SeriesType) and rhs.attr == \'values\':\n            # simply return the column\n            nodes = []\n            var = self._get_series_data(rhs.value, nodes)\n            assign.value = var\n            nodes.append(assign)\n            return nodes\n\n        return [assign]\n\n    def _run_call(self, assign, lhs, rhs):\n        func_name = None  # literal name of analyzed function\n        func_mod = None  # literal name of analyzed module\n\n        fdef = guard(find_callname, self.state.func_ir, rhs, self.state.typemap)\n        if fdef is None:\n            return [assign]\n        else:\n            func_name, func_mod = fdef\n\n        if func_mod == \'sdc.hiframes.api\':\n            return self._run_call_hiframes(assign, assign.target, rhs, func_name)\n\n        return [assign]\n\n    def _run_call_hiframes(self, assign, lhs, rhs, func_name):\n        if func_name == \'df_isin\':\n            nodes = []\n            data, other = rhs.args\n\n            def _isin_series(A, B):\n                numba.parfors.parfor.init_prange()\n                n = len(A)\n                m = len(B)\n                S = numpy.empty(n, numpy.bool_)\n\n                for i in numba.parfors.parfor.internal_prange(n):\n                    S[i] = (A[i] == B[i] if i < m else False)\n\n                return S\n\n            return self._replace_func(_isin_series, [data, other], pre_nodes=nodes)\n\n        if func_name == \'df_isin_vals\':\n            nodes = []\n            data = rhs.args[0]\n\n            def _isin_series(A, vals):\n                numba.parfors.parfor.init_prange()\n                n = len(A)\n                S = numpy.empty(n, numpy.bool_)\n\n                for i in numba.parfors.parfor.internal_prange(n):\n                    S[i] = A[i] in vals\n\n                return S\n\n            return self._replace_func(_isin_series, [data, rhs.args[1]], pre_nodes=nodes)\n\n        if func_name == \'series_filter_bool\':\n            return self._handle_df_col_filter(assign, lhs, rhs)\n\n        return [assign]\n\n    def _handle_df_col_filter(self, assign, lhs, rhs):\n        nodes = []\n        in_arr = rhs.args[0]\n        bool_arr = rhs.args[1]\n        if sdc.hiframes.pd_series_ext.is_series_type(self.state.typemap[in_arr.name]):\n            in_arr = self._get_series_data(in_arr, nodes)\n        if sdc.hiframes.pd_series_ext.is_series_type(self.state.typemap[bool_arr.name]):\n            bool_arr = self._get_series_data(bool_arr, nodes)\n\n        return self._replace_func(sdc.hiframes.series_kernels._column_filter_impl, [in_arr, bool_arr], pre_nodes=nodes)\n\n    def _get_series_data(self, series_var, nodes):\n        var_def = guard(ir_utils.get_definition, self.state.func_ir, series_var)\n        call_def = guard(find_callname, self.state.func_ir, var_def)\n\n        if call_def == (\'init_series\', \'sdc.hiframes.api\'):\n            return var_def.args[0]\n\n        def _get_series_data_lambda(S):\n            return sdc.hiframes.api.get_series_data(S)\n\n        f_block = ir_utils.compile_to_numba_ir(_get_series_data_lambda,\n                                               {\'sdc\': sdc},\n                                               self.state.typingctx,\n                                               (self.state.typemap[series_var.name], ),\n                                               self.state.typemap,\n                                               self.state.calltypes).blocks.popitem()[1]\n        ir_utils.replace_arg_nodes(f_block, [series_var])\n        nodes += f_block.body[:-2]\n        return nodes[-1].target\n\n    def _replace_func(self, func, args, const=False, pre_nodes=None, extra_globals=None, pysig=None, kws=None):\n        glbls = {\'numba\': numba, \'numpy\': numpy, \'sdc\': sdc}\n        if extra_globals is not None:\n            glbls.update(extra_globals)\n\n        if pysig is not None:\n            pre_nodes = [] if pre_nodes is None else pre_nodes\n            scope = next(iter(self.state.func_ir.blocks.values())).scope\n            loc = scope.loc\n\n            def normal_handler(index, param, default):\n                return default\n\n            def default_handler(index, param, default):\n                d_var = ir.Var(scope, ir_utils.mk_unique_var(\'defaults\'), loc)\n                self.state.typemap[d_var.name] = numba.typeof(default)\n                node = ir.Assign(ir.Const(default, loc), d_var, loc)\n                pre_nodes.append(node)\n\n                return d_var\n\n            args = numba.core.typing.fold_arguments(pysig, args, kws, normal_handler, default_handler, normal_handler)\n\n        arg_typs = tuple(self.state.typemap[v.name] for v in args)\n\n        if const:\n            new_args = []\n\n            for i, arg in enumerate(args):\n                val = guard(ir_utils.find_const, self.state.func_ir, arg)\n                if val:\n                    new_args.append(types.literal(val))\n                else:\n                    new_args.append(arg_typs[i])\n            arg_typs = tuple(new_args)\n\n        return sdc.utilities.utils.ReplaceFunc(func, arg_typs, args, glbls, pre_nodes)\n\n    def _update_definitions(self, node_list):\n        loc = ir.Loc("""", 0)\n        dumm_block = ir.Block(ir.Scope(None, loc), loc)\n        dumm_block.body = node_list\n        build_definitions({0: dumm_block}, self.state.func_ir._definitions)\n\n        return\n\n@register_pass(mutates_CFG=True, analysis_only=False)\nclass SDC_Pandas_DataFrame_TransformationPass_Stage1(FunctionPass):\n    """"""\n    This transformation pass replaces known function definitions with implementation\n    """"""\n\n    _name = ""sdc_dataframe_transformationpass_stage1""\n\n    def __init__(self):\n        FunctionPass.__init__(self)\n\n    def run_pass(self, state):\n        self.state = state\n        # replace inst variables as determined previously during the pass\n        # currently use to keep lhs of Arg nodes intact\n        self.replace_var_dict = {}\n\n        # df_var -> {col1:col1_var ...}\n        self.df_vars = {}\n        # df_var -> label where it is defined\n        self.df_labels = {}\n\n        ir_utils._max_label = max(self.state.func_ir.blocks.keys())  # shssf:  is it still needed?\n\n        # FIXME: see why this breaks test_kmeans\n        # remove_dels(self.state.func_ir.blocks)\n        ir_utils.dprint_func_ir(self.state.func_ir, self._name)\n        blocks = self.state.func_ir.blocks\n        # call build definition since rewrite pass doesn\'t update definitions\n        # e.g. getitem to static_getitem in test_column_list_select2\n        self.state.func_ir._definitions = build_definitions(blocks)\n        # topo_order necessary since df vars need to be found before use\n        topo_order = find_topo_order(blocks)\n        work_list = list((l, blocks[l]) for l in reversed(topo_order))\n        while work_list:\n            label, block = work_list.pop()\n            new_body = []\n            replaced = False\n            self._working_body = new_body\n            for i, inst in enumerate(block.body):\n                self._replace_vars(inst)\n                out_nodes = [inst]\n\n                # handle potential dataframe set column here\n                # df[\'col\'] = arr\n                if (isinstance(inst, ir.StaticSetItem)\n                        and isinstance(inst.index, str)):\n                    # cfg needed for set df column\n                    cfg = ir_utils.compute_cfg_from_blocks(blocks)\n                    out_nodes = self._run_df_set_column(inst, label, cfg)\n                elif isinstance(inst, ir.Assign):\n                    self.state.func_ir._definitions[inst.target.name].remove(inst.value)\n                    out_nodes = self._run_assign(inst, label)\n\n                if isinstance(out_nodes, list):\n                    # TODO: fix scope/loc\n                    new_body.extend(out_nodes)\n                    self._update_definitions(out_nodes)\n                if isinstance(out_nodes, sdc.utilities.utils.ReplaceFunc):\n                    rp_func = out_nodes\n                    if rp_func.pre_nodes is not None:\n                        new_body.extend(rp_func.pre_nodes)\n                        self._update_definitions(rp_func.pre_nodes)\n                    # replace inst.value to a call with target args\n                    # as expected by numba.core.inline_closurecall.inline_closure_call\n                    # TODO: inst other than Assign?\n                    inst.value = ir.Expr.call(\n                        ir.Var(block.scope, ""dummy"", inst.loc),\n                        rp_func.args, (), inst.loc)\n                    block.body = new_body + block.body[i:]\n                    sdc.utilities.utils.update_globals(rp_func.func, rp_func.glbls)\n                    numba.core.inline_closurecall.inline_closure_call(self.state.func_ir, rp_func.glbls,\n                                        block, len(new_body), rp_func.func, work_list=work_list)\n                    replaced = True\n                    break\n                if isinstance(out_nodes, dict):\n                    block.body = new_body + block.body[i:]\n                    # TODO: insert new blocks in current spot of work_list\n                    # instead of append?\n                    # TODO: rename variables, fix scope/loc\n                    sdc.utilities.utils.inline_new_blocks(\n                        self.state.func_ir, block, len(new_body), out_nodes, work_list)\n                    replaced = True\n                    break\n            if not replaced:\n                blocks[label].body = new_body\n\n        self.state.func_ir.blocks = ir_utils.simplify_CFG(self.state.func_ir.blocks)\n        # self.state.func_ir._definitions = build_definitions(blocks)\n        # XXX: remove dead here fixes h5 slice issue\n        # iterative remove dead to make sure all extra code (e.g. df vars) is removed\n        # while remove_dead(blocks, self.state.func_ir.arg_names, self.state.func_ir):\n        #     pass\n        self.state.func_ir._definitions = build_definitions(blocks)\n        ir_utils.dprint_func_ir(self.state.func_ir, self._name)\n\n        return True\n\n    def _replace_vars(self, inst):\n        # variable replacement can affect definitions so handling assignment\n        # values specifically\n        if sdc.utilities.utils.is_assign(inst):\n            lhs = inst.target.name\n            self.state.func_ir._definitions[lhs].remove(inst.value)\n\n        ir_utils.replace_vars_stmt(inst, self.replace_var_dict)\n\n        if sdc.utilities.utils.is_assign(inst):\n            self.state.func_ir._definitions[lhs].append(inst.value)\n            # if lhs changed, TODO: test\n            if inst.target.name != lhs:\n                self.state.func_ir._definitions[inst.target.name] = self.state.func_ir._definitions[lhs]\n\n    def _run_assign(self, assign, label):\n        lhs = assign.target.name\n        rhs = assign.value\n\n        if isinstance(rhs, ir.Expr):\n            if rhs.op == \'call\':\n                return self._run_call(assign, label)\n\n            # HACK: delete pd.DataFrame({}) nodes to avoid typing errors\n            # TODO: remove when dictionaries are implemented and typing works\n            if rhs.op == \'getattr\':\n                val_def = guard(ir_utils.get_definition, self.state.func_ir, rhs.value)\n                if (isinstance(val_def, ir.Global) and val_def.value == pandas and rhs.attr in (\'DataFrame\', \'read_csv\', \'read_parquet\', \'to_numeric\')):\n                    # TODO: implement to_numeric in typed pass?\n                    # put back the definition removed earlier but remove node\n                    # enables function matching without node in IR\n                    self.state.func_ir._definitions[lhs].append(rhs)\n                    return []\n\n            if rhs.op == \'getattr\':\n                val_def = guard(ir_utils.get_definition, self.state.func_ir, rhs.value)\n                if (isinstance(val_def, ir.Global) and val_def.value == numpy and rhs.attr == \'fromfile\'):\n                    # put back the definition removed earlier but remove node\n                    self.state.func_ir._definitions[lhs].append(rhs)\n                    return []\n\n            if rhs.op == \'make_function\':\n                # HACK make globals availabe for typing in series.map()\n                rhs.globals = self.state.func_ir.func_id.func.__globals__\n\n        return [assign]\n\n    def _run_call(self, assign, label):\n        """"""handle calls and return new nodes if needed\n        """"""\n        lhs = assign.target\n        rhs = assign.value\n\n        func_name = None\n        func_mod = None\n        fdef = guard(find_callname, self.state.func_ir, rhs)\n\n        if fdef is None:\n            return [assign]\n        else:\n            func_name, func_mod = fdef\n\n        # handling pd.DataFrame() here since input can be constant dictionary\n        if fdef == (\'DataFrame\', \'pandas\'):\n            return self._handle_pd_DataFrame(assign, lhs, rhs, label)\n\n        if fdef == (\'concat\', \'pandas\'):\n            return self._handle_concat(assign, lhs, rhs, label)\n\n        if isinstance(func_mod, ir.Var) and self._is_df_var(func_mod):\n            return self._run_call_df(assign, lhs, rhs, func_mod, func_name, label)\n\n        if func_name == \'drop\' and isinstance(func_mod, ir.Var):\n            # handle potential df.drop(inplace=True) here since it needs\n            # variable replacement\n            return self._handle_df_drop(assign, lhs, rhs, func_mod)\n\n        # groupby aggregate\n        # e.g. df.groupby(\'A\')[\'B\'].agg(lambda x: x.max()-x.min())\n        if isinstance(func_mod, ir.Var) and self._is_df_obj_call(func_mod, \'groupby\'):\n            return self._handle_aggregate(lhs, rhs, func_mod, func_name, label)\n\n        # rolling window\n        # e.g. df.rolling(2).sum\n        if isinstance(func_mod, ir.Var) and self._is_df_obj_call(func_mod, \'rolling\'):\n            return self._handle_rolling(lhs, rhs, func_mod, func_name, label)\n\n        return [assign]\n\n    def _run_call_df(self, assign, lhs, rhs, df_var, func_name, label):\n\n        # df.isin()\n        if func_name == \'isin\':\n            return self._handle_df_isin(lhs, rhs, df_var, label)\n\n        # df.append()\n        if func_name == \'append\':\n            return self._handle_df_append(lhs, rhs, df_var, label)\n\n        # df.fillna()\n        if func_name == \'fillna\':\n            return self._handle_df_fillna(lhs, rhs, df_var, label)\n\n        if func_name not in (\'groupby\', \'rolling\'):\n            raise NotImplementedError(\n                ""data frame function {} not implemented yet"".format(func_name))\n\n        return [assign]\n\n    def _handle_df_isin(self, lhs, rhs, df_var, label):\n        other = self._get_arg(\'isin\', rhs.args, dict(rhs.kws), 0, \'values\')\n        other_colmap = {}\n        df_col_map = self._get_df_cols(df_var)\n        nodes = []\n        df_case = False\n\n        # dataframe case\n        if self._is_df_var(other):\n            df_case = True\n            arg_df_map = self._get_df_cols(other)\n            for cname in df_col_map:\n                if cname in arg_df_map:\n                    other_colmap[cname] = arg_df_map[cname]\n        else:\n            other_def = guard(ir_utils.get_definition, self.state.func_ir, other)\n            # dict case\n            if isinstance(other_def, ir.Expr) and other_def.op == \'build_map\':\n                for c, v in other_def.items:\n                    cname = guard(ir_utils.find_const, self.state.func_ir, c)\n                    if not isinstance(cname, str):\n                        raise ValueError(""dictionary argument to isin() should have constant keys"")\n                    other_colmap[cname] = v\n                    # HACK replace build_map to avoid inference errors\n                    other_def.op = \'build_list\'\n                    other_def.items = [v[0] for v in other_def.items]\n            else:\n                # general iterable (e.g. list, set) case\n                # TODO: handle passed in dict case (pass colname to func?)\n                other_colmap = {c: other for c in df_col_map.keys()}\n\n        out_df_map = {}\n\n        def isin_func(A, B):\n            # XXX df isin is different than Series.isin, df.isin considers\n            #  index but Series.isin ignores it (everything is set)\n            # TODO: support strings and other types\n            nodes = []\n            data, other = rhs.args\n\n            def _isin_series(A, B):\n                numba.parfors.parfor.init_prange()\n                n = len(A)\n                m = len(B)\n                S = numpy.empty(n, np.bool_)\n                for i in numba.parfors.parfor.internal_prange(n):\n                    S[i] = (A[i] == B[i] if i < m else False)\n                return S\n\n            return self._replace_func(_isin_series, [data, other], pre_nodes=nodes)\n            # return sdc.hiframes.api.df_isin(A, B)\n\n        def isin_vals_func(A, B):\n            return sdc.hiframes.api.df_isin_vals(A, B)\n        # create array of False values used when other col not available\n        def bool_arr_func(A):\n            return sdc.hiframes.api.init_series(np.zeros(len(A), np.bool_))\n        # use the first array of df to get len. TODO: check for empty df\n        false_arr_args = [list(df_col_map.values())[0]]\n\n        for cname, in_var in self.df_vars[df_var.name].items():\n            if cname in other_colmap:\n                if df_case:\n                    func = isin_func\n                else:\n                    func = isin_vals_func\n                other_col_var = other_colmap[cname]\n                args = [in_var, other_col_var]\n            else:\n                func = bool_arr_func\n                args = false_arr_args\n            f_block = ir_utils.compile_to_numba_ir(func, {\'sdc\': sdc, \'numpy\': numpy}).blocks.popitem()[1]\n            ir_utils.replace_arg_nodes(f_block, args)\n            nodes += f_block.body[:-2]\n            out_df_map[cname] = nodes[-1].target\n\n        self._create_df(lhs.name, out_df_map, label)\n        return nodes\n\n    def _handle_df_append(self, lhs, rhs, df_var, label):\n        other = self._get_arg(\'append\', rhs.args, dict(rhs.kws), 0, \'other\')\n        # only handles df or list of df input\n        # TODO: check for series/dict/list input\n        # TODO: enforce ignore_index=True?\n        # single df case\n        if self._is_df_var(other):\n            return self._handle_concat_df(lhs, [df_var, other], label)\n        # list of dfs\n        df_list = guard(ir_utils.get_definition, self.state.func_ir, other)\n        if len(df_list.items) > 0 and self._is_df_var(df_list.items[0]):\n            return self._handle_concat_df(lhs, [df_var] + df_list.items, label)\n        raise ValueError(""invalid df.append() input. Only dataframe and list of dataframes supported"")\n\n    def _handle_df_fillna(self, lhs, rhs, df_var, label):\n        nodes = []\n        inplace_default = ir.Var(lhs.scope, ir_utils.mk_unique_var(""fillna_default""), lhs.loc)\n        nodes.append(ir.Assign(ir.Const(False, lhs.loc), inplace_default, lhs.loc))\n        val_var = self._get_arg(\'fillna\', rhs.args, dict(rhs.kws), 0, \'value\')\n        inplace_var = self._get_arg(\'fillna\', rhs.args, dict(rhs.kws), 3, \'inplace\', default=inplace_default)\n\n        def _fillna_func(A, val, inplace):\n            return A.fillna(val, inplace=inplace)\n\n        out_col_map = {}\n        for cname, in_var in self._get_df_cols(df_var).items():\n            f_block = ir_utils.compile_to_numba_ir(_fillna_func, {}).blocks.popitem()[1]\n            ir_utils.replace_arg_nodes(f_block, [in_var, val_var, inplace_var])\n            nodes += f_block.body[:-2]\n            out_col_map[cname] = nodes[-1].target\n\n        # create output df if not inplace\n        if (inplace_var.name == inplace_default.name or\n                guard(ir_utils.find_const, self.state.func_ir, inplace_var) is False):\n            self._create_df(lhs.name, out_col_map, label)\n        return nodes\n\n    def _handle_df_dropna(self, lhs, rhs, df_var, label):\n        nodes = []\n        inplace_default = ir.Var(lhs.scope, ir_utils.mk_unique_var(""dropna_default""), lhs.loc)\n        nodes.append(ir.Assign(ir.Const(False, lhs.loc), inplace_default, lhs.loc))\n        inplace_var = self._get_arg(\'dropna\', rhs.args, dict(rhs.kws), 4, \'inplace\', default=inplace_default)\n\n        col_names = self._get_df_col_names(df_var)\n        col_vars = self._get_df_col_vars(df_var)\n        arg_names = "", "".join([ir_utils.mk_unique_var(cname).replace(\'.\', \'_\') for cname in col_names])\n        out_names = "", "".join([ir_utils.mk_unique_var(cname).replace(\'.\', \'_\') for cname in col_names])\n\n        func_text = ""def _dropna_imp({}, inplace):\\n"".format(arg_names)\n        func_text += ""  ({},) = sdc.hiframes.api.dropna(({},), inplace)\\n"".format(\n            out_names, arg_names)\n        loc_vars = {}\n        exec(func_text, {\'sdc\': sdc}, loc_vars)\n        _dropna_imp = loc_vars[\'_dropna_imp\']\n\n        f_block = ir_utils.compile_to_numba_ir(_dropna_imp, {\'sdc\': sdc}).blocks.popitem()[1]\n        ir_utils.replace_arg_nodes(f_block, col_vars + [inplace_var])\n        nodes += f_block.body[:-3]\n\n        # extract column vars from output\n        out_col_map = {}\n        for i, cname in enumerate(col_names):\n            out_col_map[cname] = nodes[-len(col_names) + i].target\n\n        # create output df if not inplace\n        if (inplace_var.name == inplace_default.name or\n                guard(ir_utils.find_const, self.state.func_ir, inplace_var) is False):\n            self._create_df(lhs.name, out_col_map, label)\n        else:\n            # assign back to column vars for inplace case\n            for i in range(len(col_vars)):\n                c_var = col_vars[i]\n                dropped_var = list(out_col_map.values())[i]\n                nodes.append(ir.Assign(dropped_var, c_var, lhs.loc))\n        return nodes\n\n    def _handle_df_drop(self, assign, lhs, rhs, df_var):\n        """"""handle possible df.drop(inplace=True)\n        lhs = A.drop(inplace=True) -> A1, lhs = drop_inplace(...)\n        replace A with A1\n        """"""\n        kws = dict(rhs.kws)\n        inplace_var = self._get_arg(\'drop\', rhs.args, kws, 5, \'inplace\', \'\')\n        inplace = guard(ir_utils.find_const, self.state.func_ir, inplace_var)\n        if inplace is not None and inplace:\n            # TODO: make sure call post dominates df_var definition or df_var\n            # is not used in other code paths\n            # replace func variable with drop_inplace\n            f_block = ir_utils.compile_to_numba_ir(lambda: sdc.hiframes.api.drop_inplace, {\'sdc\': sdc}\n                                                   ).blocks.popitem()[1]\n            nodes = f_block.body[:-2]\n            new_func_var = nodes[-1].target\n            rhs.func = new_func_var\n            rhs.args.insert(0, df_var)\n            # new tuple return\n            ret_tup = ir.Var(lhs.scope, ir_utils.mk_unique_var(\'drop_ret\'), lhs.loc)\n            assign.target = ret_tup\n            nodes.append(assign)\n            new_df_var = ir.Var(df_var.scope, ir_utils.mk_unique_var(df_var.name), df_var.loc)\n            zero_var = ir.Var(df_var.scope, ir_utils.mk_unique_var(\'zero\'), df_var.loc)\n            one_var = ir.Var(df_var.scope, ir_utils.mk_unique_var(\'one\'), df_var.loc)\n            nodes.append(ir.Assign(ir.Const(0, lhs.loc), zero_var, lhs.loc))\n            nodes.append(ir.Assign(ir.Const(1, lhs.loc), one_var, lhs.loc))\n            getitem0 = ir.Expr.static_getitem(ret_tup, 0, zero_var, lhs.loc)\n            nodes.append(ir.Assign(getitem0, new_df_var, lhs.loc))\n            getitem1 = ir.Expr.static_getitem(ret_tup, 1, one_var, lhs.loc)\n            nodes.append(ir.Assign(getitem1, lhs, lhs.loc))\n            # replace old variable with new one\n            self.replace_var_dict[df_var.name] = new_df_var\n            return nodes\n\n        return [assign]\n\n        # df.drop(labels=None, axis=0, index=None, columns=None, level=None,\n        #         inplace=False, errors=\'raise\')\n        labels_var = self._get_arg(\'drop\', rhs.args, kws, 0, \'labels\', \'\')\n        axis_var = self._get_arg(\'drop\', rhs.args, kws, 1, \'axis\', \'\')\n        labels = self._get_str_or_list(labels_var, default=\'\')\n        axis = guard(ir_utils.find_const, self.state.func_ir, axis_var)\n\n        if labels != \'\' and axis is not None:\n            if axis != 1:\n                raise ValueError(""only dropping columns (axis=1) supported"")\n            columns = labels\n        else:\n            columns_var = self._get_arg(\'drop\', rhs.args, kws, 3, \'columns\', \'\')\n            err_msg = (""columns argument (constant string list) ""\n                       ""or labels and axis required"")\n            columns = self._get_str_or_list(columns_var, err_msg=err_msg)\n\n        inplace_var = self._get_arg(\'drop\', rhs.args, kws, 5, \'inplace\', \'\')\n        inplace = guard(ir_utils.find_const, self.state.func_ir, inplace_var)\n\n        if inplace is not None and inplace:\n            df_label = self.df_labels[df_var.name]\n            cfg = ir_utils.compute_cfg_from_blocks(self.state.func_ir.blocks)\n            # dropping columns inplace possible only when it dominates the df\n            # creation to keep schema consistent\n            if label not in cfg.backbone() and label not in cfg.post_dominators()[df_label]:\n                raise ValueError(""dropping dataframe columns inplace inside ""\n                                 ""conditionals and loops not supported yet"")\n            # TODO: rename df name\n            # TODO: support dropping columns of input dfs (reflection)\n            for cname in columns:\n                self.df_vars[df_var.name].pop(cname)\n            return []\n\n        in_df_map = self._get_df_cols(df_var)\n        nodes = []\n        out_df_map = {c: _gen_arr_copy(in_df_map[c], nodes)\n                      for c in in_df_map.keys() if c not in columns}\n        self._create_df(lhs.name, out_df_map, label)\n        return nodes\n\n    def _handle_pd_DataFrame(self, assign, lhs, rhs, label):\n        """"""\n        Handle pd.DataFrame({\'A\': A}) call\n        """"""\n        kws = dict(rhs.kws)\n        if \'data\' in kws:\n            data = kws[\'data\']\n            if len(rhs.args) != 0:\n                raise ValueError(\n                    ""only data argument suppoted in pd.DataFrame()"")\n        else:\n            if len(rhs.args) != 1:\n                raise ValueError(\n                    ""data argument in pd.DataFrame() expected"")\n            data = rhs.args[0]\n\n        arg_def = guard(ir_utils.get_definition, self.state.func_ir, data)\n        if (not isinstance(arg_def, ir.Expr) or arg_def.op != \'build_map\'):\n            raise ValueError(""Invalid DataFrame() arguments (constant dict of columns expected)"")\n\n        nodes, items = self._fix_df_arrays(arg_def.items)\n\n        # HACK replace build_map to avoid inference errors\n        arg_def.op = \'build_list\'\n        arg_def.items = [v[0] for v in arg_def.items]\n\n        n_cols = len(items)\n        data_args = "", "".join(\'data{}\'.format(i) for i in range(n_cols))\n        col_args = "", "".join(\'col{}\'.format(i) for i in range(n_cols))\n\n        func_text = ""def _init_df({}, index, {}):\\n"".format(data_args, col_args)\n        func_text += ""  return sdc.hiframes.pd_dataframe_ext.init_dataframe({}, index, {})\\n"".format(data_args, col_args)\n        loc_vars = {}\n        exec(func_text, {\'sdc\': sdc}, loc_vars)\n        _init_df = loc_vars[\'_init_df\']\n\n        # TODO: support index var\n        index = ir.Var(lhs.scope, ir_utils.mk_unique_var(\'df_index_none\'), lhs.loc)\n        nodes.append(ir.Assign(ir.Const(None, lhs.loc), index, lhs.loc))\n        data_vars = [a[1] for a in items]\n        col_vars = [a[0] for a in items]\n        args = data_vars + [index] + col_vars\n\n        return self._replace_func(_init_df, args, pre_nodes=nodes)\n\n    def _get_csv_col_info(self, dtype_map, date_cols, col_names, lhs):\n        if isinstance(dtype_map, types.Type):\n            typ = dtype_map\n            data_arrs = [ir.Var(lhs.scope, ir_utils.mk_unique_var(cname), lhs.loc)\n                         for cname in col_names]\n            return col_names, data_arrs, [typ] * len(col_names)\n\n        columns = []\n        data_arrs = []\n        out_types = []\n        for i, (col_name, typ) in enumerate(dtype_map.items()):\n            columns.append(col_name)\n            # get array dtype\n            if i in date_cols:\n                typ = types.Array(types.NPDatetime(\'ns\'), 1, \'C\')\n            out_types.append(typ)\n            # output array variable\n            data_arrs.append(\n                ir.Var(lhs.scope, ir_utils.mk_unique_var(col_name), lhs.loc))\n\n        return columns, data_arrs, out_types\n\n    def _get_const_dtype(self, dtype_var):\n        dtype_def = guard(ir_utils.get_definition, self.state.func_ir, dtype_var)\n        if isinstance(dtype_def, ir.Const) and isinstance(dtype_def.value, str):\n            typ_name = dtype_def.value\n            if typ_name == \'str\':\n                return string_array_type\n            typ_name = \'int64\' if typ_name == \'int\' else typ_name\n            typ_name = \'float64\' if typ_name == \'float\' else typ_name\n            typ = getattr(types, typ_name)\n            typ = types.Array(typ, 1, \'C\')\n            return typ\n\n        # str case\n        if isinstance(dtype_def, ir.Global) and dtype_def.value == str:\n            return string_array_type\n        # categorical case\n        if isinstance(dtype_def, ir.Expr) and dtype_def.op == \'call\':\n            if (not guard(find_callname, self.state.func_ir, dtype_def)\n                    == (\'category\', \'pandas.core.dtypes.dtypes\')):\n                raise ValueError(""pd.read_csv() invalid dtype ""\n                                 ""(built using a call but not Categorical)"")\n            cats_var = self._get_arg(\'CategoricalDtype\', dtype_def.args,\n                                     dict(dtype_def.kws), 0, \'categories\')\n            err_msg = ""categories should be constant list""\n            cats = self._get_str_or_list(cats_var, list_only=True, err_msg=err_msg)\n            typ = PDCategoricalDtype(cats)\n            return CategoricalArray(typ)\n        if not isinstance(dtype_def, ir.Expr) or dtype_def.op != \'getattr\':\n            raise ValueError(""pd.read_csv() invalid dtype"")\n        glob_def = guard(ir_utils.get_definition, self.state.func_ir, dtype_def.value)\n        if not isinstance(glob_def, ir.Global) or glob_def.value != numpy:\n            raise ValueError(""pd.read_csv() invalid dtype"")\n        # TODO: extend to other types like string and date, check error\n        typ_name = dtype_def.attr\n        typ_name = \'int64\' if typ_name == \'int\' else typ_name\n        typ_name = \'float64\' if typ_name == \'float\' else typ_name\n        typ = getattr(types, typ_name)\n        typ = types.Array(typ, 1, \'C\')\n        return typ\n\n    def _handle_concat(self, assign, lhs, rhs, label):\n        # converting build_list to build_tuple before type inference to avoid\n        # errors\n        kws = dict(rhs.kws)\n        objs_arg = self._get_arg(\'concat\', rhs.args, kws, 0, \'objs\')\n\n        df_list = guard(ir_utils.get_definition, self.state.func_ir, objs_arg)\n        if not isinstance(df_list, ir.Expr) or not (df_list.op\n                                                    in [\'build_tuple\', \'build_list\']):\n            raise ValueError(""pd.concat input should be constant list or tuple"")\n\n        # XXX convert build_list to build_tuple since Numba doesn\'t handle list of\n        # arrays for np.concatenate()\n        if df_list.op == \'build_list\':\n            df_list.op = \'build_tuple\'\n\n        if len(df_list.items) == 0:\n            # copied error from pandas\n            raise ValueError(""No objects to concatenate"")\n\n        return [assign]\n\n    def _handle_concat_df(self, lhs, df_list, label):\n        # TODO: handle non-numerical (e.g. string, datetime) columns\n        nodes = []\n\n        # get output column names\n        all_colnames = []\n        for df in df_list:\n            all_colnames.extend(self._get_df_col_names(df))\n        # TODO: verify how Pandas sorts column names\n        all_colnames = sorted(set(all_colnames))\n\n        # generate a concat call for each output column\n        # TODO: support non-numericals like string\n        def gen_nan_func(A): return np.full(len(A), np.nan)\n        # gen concat function\n        arg_names = "", "".join([\'in{}\'.format(i) for i in range(len(df_list))])\n        func_text = ""def _concat_imp({}):\\n"".format(arg_names)\n        func_text += ""    return sdc.hiframes.api.init_series(sdc.hiframes.api.concat(({})))\\n"".format(\n            arg_names)\n        loc_vars = {}\n        exec(func_text, {\'sdc\': sdc}, loc_vars)\n        _concat_imp = loc_vars[\'_concat_imp\']\n\n        done_cols = {}\n        for cname in all_colnames:\n            # arguments to the generated function\n            args = []\n            # get input columns\n            for df in df_list:\n                df_col_map = self._get_df_cols(df)\n                # generate full NaN column\n                if cname not in df_col_map:\n                    # use a df column just for len()\n                    len_arr = list(df_col_map.values())[0]\n                    f_block = ir_utils.compile_to_numba_ir(gen_nan_func,\n                                                  {\'sdc\': sdc, \'numpy\': numpy}).blocks.popitem()[1]\n                    ir_utils.replace_arg_nodes(f_block, [len_arr])\n                    nodes += f_block.body[:-2]\n                    args.append(nodes[-1].target)\n                else:\n                    args.append(df_col_map[cname])\n\n            f_block = ir_utils.compile_to_numba_ir(_concat_imp,\n                                          {\'sdc\': sdc, \'numpy\': numpy}).blocks.popitem()[1]\n            ir_utils.replace_arg_nodes(f_block, args)\n            nodes += f_block.body[:-2]\n            done_cols[cname] = nodes[-1].target\n\n        self._create_df(lhs.name, done_cols, label)\n        return nodes\n\n    def _handle_concat_series(self, lhs, rhs):\n        # defer to typed pass since the type might be non-numerical\n        def f(arr_list):\n            return sdc.hiframes.api.init_series(sdc.hiframes.api.concat(arr_list))\n        return self._replace_func(f, rhs.args)\n\n    def _fix_df_arrays(self, items_list):\n        nodes = []\n        new_list = []\n        for item in items_list:\n            col_varname = item[0]\n            col_arr = item[1]\n            # fix list(multi-dim arrays) (packing images)\n            # FIXME: does this break for list(other things)?\n            col_arr = self._fix_df_list_of_array(col_arr)\n\n            def f(arr):\n                df_arr = sdc.hiframes.api.fix_df_array(arr)\n            f_block = ir_utils.compile_to_numba_ir(\n                f, {\'sdc\': sdc}).blocks.popitem()[1]\n            ir_utils.replace_arg_nodes(f_block, [col_arr])\n            nodes += f_block.body[:-3]  # remove none return\n            new_col_arr = nodes[-1].target\n            new_list.append((col_varname, new_col_arr))\n        return nodes, new_list\n\n    def _fix_df_list_of_array(self, col_arr):\n        list_call = guard(ir_utils.get_definition, self.state.func_ir, col_arr)\n        if guard(find_callname, self.state.func_ir, list_call) == (\'list\', \'builtins\'):\n            return list_call.args[0]\n        return col_arr\n\n    def _process_df_build_map(self, items_list):\n        df_cols = {}\n        nodes = []\n        for item in items_list:\n            col_var = item[0]\n            if isinstance(col_var, str):\n                col_name = col_var\n            else:\n                col_name = get_constant(self.state.func_ir, col_var)\n                if col_name is NOT_CONSTANT:\n                    raise ValueError(\n                        ""data frame column names should be constant"")\n            # cast to series type\n\n            def f(arr):\n                df_arr = sdc.hiframes.api.init_series(arr)\n            f_block = ir_utils.compile_to_numba_ir(\n                f, {\'sdc\': sdc}).blocks.popitem()[1]\n            ir_utils.replace_arg_nodes(f_block, [item[1]])\n            nodes += f_block.body[:-3]  # remove none return\n            new_col_arr = nodes[-1].target\n            df_cols[col_name] = new_col_arr\n        return nodes, df_cols\n\n    def _is_df_obj_call(self, call_var, obj_name):\n        """"""\n        determines whether variable is coming from groupby() or groupby()[], rolling(), rolling()[]\n        """"""\n\n        var_def = guard(ir_utils.get_definition, self.state.func_ir, call_var)\n        # groupby()[\'B\'] case\n        if (isinstance(var_def, ir.Expr)\n                and var_def.op in [\'getitem\', \'static_getitem\']):\n            return self._is_df_obj_call(var_def.value, obj_name)\n        # groupby() called on column or df\n        call_def = guard(find_callname, self.state.func_ir, var_def)\n        if (call_def is not None and call_def[0] == obj_name\n                and isinstance(call_def[1], ir.Var)\n                and self._is_df_var(call_def[1])):\n            return True\n        return False\n\n    def _get_str_arg(self, f_name, args, kws, arg_no, arg_name, default=None, err_msg=None):\n        arg = None\n        if len(args) > arg_no:\n            arg = guard(ir_utils.find_const, self.state.func_ir, args[arg_no])\n        elif arg_name in kws:\n            arg = guard(ir_utils.find_const, self.state.func_ir, kws[arg_name])\n\n        if arg is None:\n            if default is not None:\n                return default\n            if err_msg is None:\n                err_msg = (""SDC error. {} requires \'{}\' argument as a constant string"").format(f_name, arg_name)\n            raise ValueError(err_msg)\n        return arg\n\n    def _get_arg(self, f_name, args, kws, arg_no, arg_name, default=None, err_msg=None):\n        arg = None\n        if len(args) > arg_no:\n            arg = args[arg_no]\n        elif arg_name in kws:\n            arg = kws[arg_name]\n\n        if arg is None:\n            if default is not None:\n                return default\n            if err_msg is None:\n                err_msg = ""SDC error. {} requires \'{}\' argument"".format(f_name, arg_name)\n            raise ValueError(err_msg)\n        return arg\n\n    def _handle_aggregate(self, lhs, rhs, obj_var, func_name, label):\n        # format df.groupby(\'A\')[\'B\'].agg(lambda x: x.max()-x.min())\n        # TODO: support aggregation functions sum, count, etc.\n        if func_name not in supported_agg_funcs:\n            raise ValueError(""only {} supported in groupby"".format("", "".join(supported_agg_funcs)))\n\n        # find selected output columns\n        df_var, out_colnames, explicit_select, obj_var = self._get_df_obj_select(obj_var, \'groupby\')\n        key_colnames, as_index = self._get_agg_obj_args(obj_var)\n        if out_colnames is None:\n            out_colnames = list(self.df_vars[df_var.name].keys())\n            # key arr is not output by default\n            # as_index should be handled separately since it just returns keys\n            for k in key_colnames:\n                out_colnames.remove(k)\n\n        # find input vars\n        in_vars = {out_cname: self.df_vars[df_var.name][out_cname]\n                   for out_cname in out_colnames}\n\n        nodes, agg_func, out_tp_vars = self._handle_agg_func(\n            in_vars, out_colnames, func_name, lhs, rhs)\n\n        # output column map, create dataframe if multiple outputs\n        out_key_vars = None\n        # XXX output becomes series if single output and explicitly selected\n        if len(out_colnames) == 1 and explicit_select and as_index:\n            df_col_map = {out_colnames[0]: lhs}\n        else:\n            out_df = {}\n            # keys come first in column list\n            if as_index is False:\n                out_key_vars = []\n                for k in key_colnames:\n                    out_key_var = ir.Var(lhs.scope, ir_utils.mk_unique_var(k), lhs.loc)\n                    out_df[k] = out_key_var\n                    out_key_vars.append(out_key_var)\n            df_col_map = ({col: ir.Var(lhs.scope, ir_utils.mk_unique_var(col), lhs.loc)\n                           for col in out_colnames})\n            out_df.update(df_col_map)\n\n            self._create_df(lhs.name, out_df, label)\n\n        in_key_vars = [self.df_vars[df_var.name][k] for k in key_colnames]\n\n        agg_node = aggregate.Aggregate(lhs.name, df_var.name, key_colnames, out_key_vars, df_col_map, in_vars, in_key_vars, agg_func, out_tp_vars, lhs.loc)\n        nodes.append(agg_node)\n        return nodes\n\n    def _handle_agg_func(self, in_vars, out_colnames, func_name, lhs, rhs):\n        agg_func = get_agg_func(self.state.func_ir, func_name, rhs)\n        out_tp_vars = {}\n\n        # sdc.jit() instead of numba.njit() to handle str arrs etc\n        agg_func_dis = sdc.jit(agg_func)\n        #agg_func_dis = numba.njit(agg_func)\n        agg_gb_var = ir.Var(lhs.scope, ir_utils.mk_unique_var(""agg_gb""), lhs.loc)\n        nodes = [ir.Assign(ir.Global(""agg_gb"", agg_func_dis, lhs.loc), agg_gb_var, lhs.loc)]\n        for out_cname in out_colnames:\n            in_var = in_vars[out_cname]\n\n            def to_arr(a, _agg_f):\n                b = sdc.hiframes.api.to_arr_from_series(a)\n                res = sdc.hiframes.api.init_series(sdc.hiframes.api.agg_typer(b, _agg_f))\n            f_block = ir_utils.compile_to_numba_ir(to_arr, {\'sdc\': sdc, \'numpy\': numpy}).blocks.popitem()[1]\n            ir_utils.replace_arg_nodes(f_block, [in_var, agg_gb_var])\n            nodes += f_block.body[:-3]  # remove none return\n            out_tp_vars[out_cname] = nodes[-1].target\n        return nodes, agg_func, out_tp_vars\n\n    def _get_agg_obj_args(self, agg_var):\n        # find groupby key and as_index\n        groubpy_call = guard(ir_utils.get_definition, self.state.func_ir, agg_var)\n        assert isinstance(groubpy_call, ir.Expr) and groubpy_call.op == \'call\'\n        kws = dict(groubpy_call.kws)\n        as_index = True\n        if \'as_index\' in kws:\n            as_index = guard(ir_utils.find_const, self.state.func_ir, kws[\'as_index\'])\n            if as_index is None:\n                raise ValueError(""groupby as_index argument should be constant"")\n        if len(groubpy_call.args) == 1:\n            by_arg = groubpy_call.args[0]\n        elif \'by\' in kws:\n            by_arg = kws[\'by\']\n        else:\n            raise ValueError(""by argument for groupby() required"")\n\n        err_msg = (""groupby() by argument should be list of column names or a column name"")\n        key_colnames = self._get_str_or_list(by_arg, True, err_msg=err_msg)\n\n        return key_colnames, as_index\n\n    def _get_str_or_list(self, by_arg, list_only=False, default=None, err_msg=None, typ=None):\n        typ = str if typ is None else typ\n        by_arg_def = guard(find_build_sequence, self.state.func_ir, by_arg)\n\n        if by_arg_def is None:\n            # try add_consts_to_type\n            by_arg_call = guard(ir_utils.get_definition, self.state.func_ir, by_arg)\n            if guard(find_callname, self.state.func_ir, by_arg_call) == (\'add_consts_to_type\', \'sdc.hiframes.api\'):\n                by_arg_def = guard(find_build_sequence, self.state.func_ir, by_arg_call.args[0])\n\n        if by_arg_def is None:\n            # try dict.keys()\n            by_arg_call = guard(ir_utils.get_definition, self.state.func_ir, by_arg)\n            call_name = guard(find_callname, self.state.func_ir, by_arg_call)\n            if (call_name is not None and len(call_name) == 2\n                    and call_name[0] == \'keys\'\n                    and isinstance(call_name[1], ir.Var)):\n                var_def = guard(ir_utils.get_definition, self.state.func_ir, call_name[1])\n                if isinstance(var_def, ir.Expr) and var_def.op == \'build_map\':\n                    by_arg_def = [v[0] for v in var_def.items], \'build_map\'\n                    # HACK replace dict.keys getattr to avoid typing errors\n                    keys_getattr = guard(\n                        ir_utils.get_definition, self.state.func_ir, by_arg_call.func)\n                    assert isinstance(\n                        keys_getattr, ir.Expr) and keys_getattr.attr == \'keys\'\n                    keys_getattr.attr = \'copy\'\n\n        if by_arg_def is None:\n            # try single key column\n            by_arg_def = guard(ir_utils.find_const, self.state.func_ir, by_arg)\n            if by_arg_def is None:\n                if default is not None:\n                    return default\n                raise ValueError(err_msg)\n            key_colnames = [by_arg_def]\n        else:\n            if list_only and by_arg_def[1] != \'build_list\':\n                if default is not None:\n                    return default\n                raise ValueError(err_msg)\n            key_colnames = [guard(ir_utils.find_const, self.state.func_ir, v) for v in by_arg_def[0]]\n            if any(not isinstance(v, typ) for v in key_colnames):\n                if default is not None:\n                    return default\n                raise ValueError(err_msg)\n        return key_colnames\n\n    def _get_df_obj_select(self, obj_var, obj_name):\n        """"""analyze selection of columns in after groupby() or rolling()\n        e.g. groupby(\'A\')[\'B\'], groupby(\'A\')[\'B\', \'C\'], groupby(\'A\')\n        """"""\n        select_def = guard(ir_utils.get_definition, self.state.func_ir, obj_var)\n        out_colnames = None\n        explicit_select = False\n        if isinstance(select_def, ir.Expr) and select_def.op in (\'getitem\', \'static_getitem\'):\n            obj_var = select_def.value\n            out_colnames = (select_def.index\n                            if select_def.op == \'static_getitem\'\n                            else guard(ir_utils.find_const, self.state.func_ir, select_def.index))\n            if not isinstance(out_colnames, (str, tuple)):\n                raise ValueError(""{} output column names should be constant"".format(obj_name))\n            if isinstance(out_colnames, str):\n                out_colnames = [out_colnames]\n            explicit_select = True\n\n        obj_call = guard(ir_utils.get_definition, self.state.func_ir, obj_var)\n        # find dataframe\n        call_def = guard(find_callname, self.state.func_ir, obj_call)\n        assert (call_def is not None and call_def[0] == obj_name\n                and isinstance(call_def[1], ir.Var)\n                and self._is_df_var(call_def[1]))\n        df_var = call_def[1]\n\n        return df_var, out_colnames, explicit_select, obj_var\n\n    def _handle_rolling(self, lhs, rhs, obj_var, func_name, label):\n        # format df.rolling(w)[\'B\'].sum()\n        # TODO: support aggregation functions sum, count, etc.\n        if func_name not in sdc.hiframes.rolling.supported_rolling_funcs:\n            raise ValueError(""only ({}) supported in rolling"".format("", "".join(sdc.hiframes.rolling.supported_rolling_funcs)))\n\n        nodes = []\n        # find selected output columns\n        df_var, out_colnames, explicit_select, obj_var = self._get_df_obj_select(obj_var, \'rolling\')\n        rolling_call = guard(ir_utils.get_definition, self.state.func_ir, obj_var)\n        window, center, on = get_rolling_setup_args(self.state.func_ir, rolling_call, False)\n        on_arr = self.df_vars[df_var.name][on] if on is not None else None\n        if not isinstance(center, ir.Var):\n            center_var = ir.Var(lhs.scope, ir_utils.mk_unique_var(""center""), lhs.loc)\n            nodes.append(ir.Assign(ir.Const(center, lhs.loc), center_var, lhs.loc))\n            center = center_var\n        if not isinstance(window, ir.Var):\n            window_var = ir.Var(lhs.scope, ir_utils.mk_unique_var(""window""), lhs.loc)\n            nodes.append(ir.Assign(ir.Const(window, lhs.loc), window_var, lhs.loc))\n            window = window_var\n        # TODO: get \'on\' arg for offset case\n        if out_colnames is None:\n            out_colnames = list(self.df_vars[df_var.name].keys())\n            # TODO: remove index col for offset case\n\n        nan_cols = []\n        if func_name in (\'cov\', \'corr\'):\n            if len(rhs.args) != 1:\n                raise ValueError(""rolling {} requires one argument (other)"".format(func_name))\n            # XXX pandas only accepts variable window cov/corr\n            # when both inputs have time index\n            if on_arr is not None:\n                raise ValueError(""variable window rolling {} not supported yet."".format(func_name))\n            # TODO: support variable window rolling cov/corr which is only\n            # possible in pandas with time index\n            other = rhs.args[0]\n            if self._is_df_var(other):\n                # df on df cov/corr returns common columns only (without\n                # pairwise flag)\n                # TODO: support pairwise arg\n                col_set1 = set(out_colnames)\n                col_set2 = set(self._get_df_col_names(other))\n                out_colnames = list(col_set1 & col_set2)\n                # Pandas makes non-common columns NaNs\n                nan_cols = list(col_set1 ^ col_set2)\n\n        # output column map, create dataframe if multiple outputs\n        out_df = None\n        if len(out_colnames) == 1 and explicit_select:\n            df_col_map = {out_colnames[0]: lhs}\n        else:\n            df_col_map = ({col: ir.Var(lhs.scope, ir_utils.mk_unique_var(col), lhs.loc)\n                           for col in out_colnames})\n            if on is not None:\n                df_col_map[on] = on_arr\n            out_df = df_col_map.copy()\n            # TODO: add datetime index for offset case\n\n        args = rhs.args\n        for cname, out_col_var in df_col_map.items():\n            if cname == on:\n                continue\n            in_col_var = self.df_vars[df_var.name][cname]\n            if func_name in (\'cov\', \'corr\'):\n                args[0] = self.df_vars[other.name][cname]\n            nodes += self._gen_rolling_call(in_col_var, out_col_var, window, center, args, func_name, on_arr)\n\n        # create NaN columns for cov/corr case\n        len_arr = self.df_vars[df_var.name][out_colnames[0]]\n        for cname in nan_cols:\n            def f(arr):\n                nan_arr = numpy.full(len(arr), np.nan)\n            f_block = ir_utils.compile_to_numba_ir(f, {\'numpy\': numpy}).blocks.popitem()[1]\n            ir_utils.replace_arg_nodes(f_block, [len_arr])\n            nodes += f_block.body[:-3]  # remove none return\n            out_df[cname] = nodes[-1].target\n        if out_df is not None:\n            # Pandas sorts the output column names _flex_binary_moment\n            # line: res_columns = arg1.columns.union(arg2.columns)\n            self._create_df(lhs.name, dict(sorted(out_df.items())), label)\n\n        return nodes\n\n    def _gen_rolling_call(self, in_col_var, out_col_var, window, center, args, func_name, on_arr):\n        nodes = []\n        if func_name in (\'cov\', \'corr\'):\n            other = args[0]\n            if on_arr is not None:\n                if func_name == \'cov\':\n                    def f(arr, other, on_arr, w, center):\n                        df_arr = sdc.hiframes.api.init_series(sdc.hiframes.rolling.rolling_cov(arr, other, on_arr, w, center))\n                if func_name == \'corr\':\n                    def f(arr, other, on_arr, w, center):\n                        df_arr = sdc.hiframes.api.init_series(sdc.hiframes.rolling.rolling_corr(arr, other, on_arr, w, center))\n                args = [in_col_var, other, on_arr, window, center]\n            else:\n                if func_name == \'cov\':\n                    def f(arr, other, w, center):\n                        df_arr = sdc.hiframes.api.init_series(sdc.hiframes.rolling.rolling_cov(arr, other, w, center))\n                if func_name == \'corr\':\n                    def f(arr, other, w, center):\n                        df_arr = sdc.hiframes.api.init_series(sdc.hiframes.rolling.rolling_corr(arr, other, w, center))\n                args = [in_col_var, other, window, center]\n        # variable window case\n        elif on_arr is not None:\n            if func_name == \'apply\':\n                def f(arr, on_arr, w, center, func):\n                    df_arr = sdc.hiframes.api.init_series(sdc.hiframes.rolling.rolling_variable(arr, on_arr, w, center, False, func))\n                args = [in_col_var, on_arr, window, center, args[0]]\n            else:\n                def f(arr, on_arr, w, center):\n                    df_arr = sdc.hiframes.api.init_series(sdc.hiframes.rolling.rolling_variable(arr, on_arr, w, center, False, _func_name))\n                args = [in_col_var, on_arr, window, center]\n        else:  # fixed window\n            # apply case takes the passed function instead of just name\n            if func_name == \'apply\':\n                def f(arr, w, center, func):\n                    df_arr = sdc.hiframes.api.init_series(sdc.hiframes.rolling.rolling_fixed(arr, w, center, False, func))\n                args = [in_col_var, window, center, args[0]]\n            else:\n                def f(arr, w, center):\n                    df_arr = sdc.hiframes.api.init_series(sdc.hiframes.rolling.rolling_fixed(arr, w, center, False, _func_name))\n                args = [in_col_var, window, center]\n\n        f_block = ir_utils.compile_to_numba_ir(f, {\'sdc\': sdc, \'_func_name\': func_name}).blocks.popitem()[1]\n        ir_utils.replace_arg_nodes(f_block, args)\n        nodes += f_block.body[:-3]  # remove none return\n        nodes[-1].target = out_col_var\n\n        return nodes\n\n    def _run_df_set_column(self, inst, label, cfg):\n        """"""replace setitem of string index with a call to handle possible\n        dataframe case where schema is changed:\n        df[\'new_col\'] = arr  ->  df2 = set_df_col(df, \'new_col\', arr)\n        dataframe_pass will replace set_df_col() with regular setitem if target\n        is not dataframe\n        """"""\n\n        df_var = inst.target\n        # create var for string index\n        cname_var = ir.Var(inst.value.scope, ir_utils.mk_unique_var(""$cname_const""), inst.loc)\n        nodes = [ir.Assign(ir.Const(inst.index, inst.loc), cname_var, inst.loc)]\n\n        def func(df, cname, arr):\n            return sdc.hiframes.api.set_df_col(df, cname, arr)\n\n        f_block = ir_utils.compile_to_numba_ir(func, {\'sdc\': sdc}).blocks.popitem()[1]\n        ir_utils.replace_arg_nodes(f_block, [df_var, cname_var, inst.value])\n        nodes += f_block.body[:-2]\n\n        # rename the dataframe variable to keep schema static\n        new_df_var = ir.Var(df_var.scope, ir_utils.mk_unique_var(df_var.name), df_var.loc)\n        nodes[-1].target = new_df_var\n        self.replace_var_dict[df_var.name] = new_df_var\n\n        return nodes\n\n    def _replace_func(self, func, args, const=False, array_typ_convert=True, pre_nodes=None, extra_globals=None):\n        glbls = {\'numba\': numba, \'numpy\': numpy, \'sdc\': sdc}\n\n        if extra_globals is not None:\n            glbls.update(extra_globals)\n\n        return sdc.utilities.utils.ReplaceFunc(func, None, args, glbls, pre_nodes)\n\n    def _create_df(self, df_varname, df_col_map, label):\n        # order is important for proper handling of itertuples, apply, etc.\n        # starting pandas 0.23 and Python 3.6, regular dict order is OK\n        # for <0.23 ordered_df_map = OrderedDict(sorted(df_col_map.items()))\n\n        self.df_vars[df_varname] = df_col_map\n        self.df_labels[df_varname] = label\n\n    def _is_df_var(self, var):\n        assert isinstance(var, ir.Var)\n        return (var.name in self.df_vars)\n\n    def _get_df_cols(self, df_var):\n        #\n        assert isinstance(df_var, ir.Var)\n        df_var_renamed = self._get_renamed_df(df_var)\n        return self.df_vars[df_var_renamed.name]\n\n    def _get_df_col_names(self, df_var):\n        assert isinstance(df_var, ir.Var)\n        df_var_renamed = self._get_renamed_df(df_var)\n        return list(self.df_vars[df_var_renamed.name].keys())\n\n    def _get_df_col_vars(self, df_var):\n        #\n        assert isinstance(df_var, ir.Var)\n        df_var_renamed = self._get_renamed_df(df_var)\n        return list(self.df_vars[df_var_renamed.name].values())\n\n    def _get_renamed_df(self, df_var):\n        # XXX placeholder for df variable renaming\n        assert isinstance(df_var, ir.Var)\n        return df_var\n\n    def _update_definitions(self, node_list):\n        loc = ir.Loc("""", 0)\n        dumm_block = ir.Block(ir.Scope(None, loc), loc)\n        dumm_block.body = node_list\n        build_definitions({0: dumm_block}, self.state.func_ir._definitions)\n        return\n\ndef _gen_arr_copy(in_arr, nodes):\n    f_block = ir_utils.compile_to_numba_ir(lambda A: A.copy(), {}).blocks.popitem()[1]\n    ir_utils.replace_arg_nodes(f_block, [in_arr])\n    nodes += f_block.body[:-2]\n    return nodes[-1].target\n\ndef sdc_nopython_pipeline_lite_register(state, name=\'nopython\'):\n    """"""\n    This is to register some sub set of Intel SDC compiler passes in Numba NoPython pipeline\n    Each pass, enabled here, is expected to be called many times on every decorated function including\n    functions which are not related to Pandas.\n\n    Test: SDC_CONFIG_PIPELINE_SDC=0 python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_sort_values1\n\n    This function needs to be removed if SDC DataFrame support\n    no more needs Numba IR transformations via DataFramePass\n    """"""\n\n    if sdc.config.numba_compiler_define_nopython_pipeline_orig is None:\n        raise ValueError(""Intel SDC. Unexpected usage of DataFrame passes registration function."")\n\n    numba_pass_manager = sdc.config.numba_compiler_define_nopython_pipeline_orig(state, name)\n\n    numba_pass_manager.add_pass_after(SDC_Pandas_DataFrame_TransformationPass_Stage1, numba.untyped_passes.InlineClosureLikes)\n\n    numba_pass_manager.add_pass_after(sdc.hiframes.dataframe_pass.DataFramePass, numba.typed_passes.AnnotateTypes)\n    numba_pass_manager.add_pass_after(sdc.compiler.PostprocessorPass, numba.typed_passes.AnnotateTypes)\n\n    numba_pass_manager.add_pass_after(SDC_Pandas_DataFrame_TransformationPass_Stage2, sdc.hiframes.dataframe_pass.DataFramePass)\n\n    numba_pass_manager.finalize()\n\n    return numba_pass_manager\n'"
sdc/datatypes/hpat_pandas_dataframe_rolling_functions.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\nimport numpy\nimport pandas\n\nfrom numba.core.types import (float64, Boolean, Integer, Number, Omitted,\n                         NoneType, StringLiteral, UnicodeType)\nfrom sdc.utilities.sdc_typing_utils import TypeChecker, kwsparams2list\nfrom sdc.datatypes.hpat_pandas_dataframe_rolling_types import DataFrameRollingType\nfrom sdc.hiframes.pd_dataframe_ext import get_dataframe_data\nfrom sdc.hiframes.pd_dataframe_type import DataFrameType\nfrom sdc.hiframes.pd_series_type import SeriesType\nfrom sdc.utilities.utils import sdc_overload_method\n\n\nsdc_pandas_dataframe_rolling_docstring_tmpl = """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n    Pandas API: pandas.core.window.Rolling.{method_name}\n{limitations_block}\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/dataframe/rolling/dataframe_rolling_{method_name}.py\n       :language: python\n       :lines: 27-\n       :caption: {example_caption}\n       :name: ex_dataframe_rolling_{method_name}\n\n    .. command-output:: python ./dataframe/rolling/dataframe_rolling_{method_name}.py\n       :cwd: ../../../examples\n\n    .. seealso::\n        :ref:`DataFrame.rolling <pandas.DataFrame.rolling>`\n            Calling object with a DataFrame.\n        :ref:`DataFrame.rolling <pandas.DataFrame.rolling>`\n            Calling object with a DataFrame.\n        :ref:`DataFrame.{method_name} <pandas.DataFrame.{method_name}>`\n            Similar method for DataFrame.\n        :ref:`DataFrame.{method_name} <pandas.DataFrame.{method_name}>`\n            Similar method for DataFrame.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n\n    Pandas DataFrame method :meth:`pandas.DataFrame.rolling.{method_name}()` implementation.\n\n    .. only:: developer\n\n    Test: python -m sdc.runtests -k sdc.tests.test_rolling.TestRolling.test_dataframe_rolling_{method_name}\n\n    Parameters\n    ----------\n    self: :class:`pandas.DataFrame.rolling`\n        input arg{extra_params}\n\n    Returns\n    -------\n    :obj:`pandas.DataFrame`\n         returns :obj:`pandas.DataFrame` object\n""""""\n\n\ndef df_rolling_params_codegen():\n    """"""Generate rolling parameters""""""\n    params = [\'window\', \'min_periods\', \'center\', \'win_type\', \'on\', \'axis\', \'closed\']\n    return \', \'.join(f\'self._{p}\' for p in params)\n\n\ndef df_rolling_method_other_df_codegen(method_name, self, other, args=None, kws=None):\n    args = args or []\n    kwargs = kws or {}\n\n    rolling_params = df_rolling_params_codegen()\n    method_kws = {k: k for k in kwargs}\n    impl_params = [\'self\'] + args + kwsparams2list(kwargs)\n    impl_params_as_str = \', \'.join(impl_params)\n\n    data_columns = {col: idx for idx, col in enumerate(self.data.columns)}\n    other_columns = {col: idx for idx, col in enumerate(other.columns)}\n\n    # columns order matters\n    common_columns = [col for col in data_columns if col in other_columns]\n    all_columns = [col for col in data_columns]\n    for col in other_columns:\n        if col in all_columns:\n            continue\n        all_columns.append(col)\n\n    results = []\n    impl_name = f\'_df_rolling_{method_name}_other_df_impl\'\n    func_lines = [f\'def {impl_name}({impl_params_as_str}):\']\n\n    if \'pairwise\' in kwargs:\n        func_lines += [\n            \'  if pairwise is None:\',\n            \'    _pairwise = False\',\n            \'  else:\',\n            \'    _pairwise = pairwise\',\n            \'  if _pairwise:\',\n            f\'    raise ValueError(""Method rolling.{method_name}(). The object pairwise\\\\n expected: False, None"")\'\n        ]\n\n    data_length = \'len(self._data._data[0][0])\' if data_columns else \'0\'\n    other_length = \'len(other._data[0][0])\' if other_columns else \'0\'\n    func_lines += [f\'  length = max([{data_length}, {other_length}])\']\n\n    for col in all_columns:\n        res_data = f\'result_data_{col}\'\n        if col in common_columns:\n            col_loc = self.data.column_loc[col]\n            type_id, col_id = col_loc.type_id, col_loc.col_id\n            other_col_loc = other.column_loc[col]\n            other_type_id = other_col_loc.type_id\n            other_col_id = other_col_loc.col_id\n\n            other_series = f\'other_series_{col}\'\n            method_kws[\'other\'] = other_series\n            method_params = \', \'.join(args + kwsparams2list(method_kws))\n            func_lines += [\n                f\'  data_{col} = self._data._data[{type_id}][{col_id}]\',\n                f\'  other_data_{col} = other._data[{other_type_id}][{other_col_id}]\',\n                f\'  series_{col} = pandas.Series(data_{col})\',\n                f\'  {other_series} = pandas.Series(other_data_{col})\',\n                f\'  rolling_{col} = series_{col}.rolling({rolling_params})\',\n                f\'  result_{col} = rolling_{col}.{method_name}({method_params})\',\n                f\'  {res_data} = result_{col}._data[:length]\'\n            ]\n        else:\n            func_lines += [\n                f\'  {res_data} = numpy.empty(length, dtype=float64)\',\n                f\'  {res_data}[:] = numpy.nan\'\n            ]\n        results.append((col, res_data))\n\n    data = \', \'.join(f\'""{col}"": {data}\' for col, data in results)\n    func_lines += [f\'  return pandas.DataFrame({{{data}}})\']\n    func_text = \'\\n\'.join(func_lines)\n\n    global_vars = {\'numpy\': numpy, \'pandas\': pandas, \'float64\': float64}\n\n    return func_text, global_vars\n\n\ndef df_rolling_method_main_codegen(method_params, df_columns, column_loc, method_name):\n    rolling_params = df_rolling_params_codegen()\n    method_params_as_str = \', \'.join(method_params)\n\n    results = []\n    func_lines = []\n    for idx, col in enumerate(df_columns):\n        col_loc = column_loc[col]\n        type_id, col_id = col_loc.type_id, col_loc.col_id\n        res_data = f\'result_data_{col}\'\n        func_lines += [\n            f\'  data_{col} = self._data._data[{type_id}][{col_id}]\',\n            f\'  series_{col} = pandas.Series(data_{col})\',\n            f\'  rolling_{col} = series_{col}.rolling({rolling_params})\',\n            f\'  result_{col} = rolling_{col}.{method_name}({method_params_as_str})\',\n            f\'  {res_data} = result_{col}._data[:len(data_{col})]\'\n        ]\n        results.append((col, res_data))\n\n    data = \', \'.join(f\'""{col}"": {data}\' for col, data in results)\n    func_lines += [f\'  return pandas.DataFrame({{{data}}})\']\n\n    return func_lines\n\n\ndef gen_df_rolling_method_other_none_codegen(rewrite_name=None):\n    """"""Generate df.rolling method code generator based on name of the method""""""\n    def df_rolling_method_other_none_codegen(method_name, self, args=None, kws=None):\n        _method_name = rewrite_name or method_name\n        args = args or []\n        kwargs = kws or {}\n\n        impl_params = [\'self\'] + args + kwsparams2list(kwargs)\n        impl_params_as_str = \', \'.join(impl_params)\n\n        impl_name = f\'_df_rolling_{_method_name}_other_none_impl\'\n        func_lines = [f\'def {impl_name}({impl_params_as_str}):\']\n\n        if \'pairwise\' in kwargs:\n            func_lines += [\n                \'  if pairwise is None:\',\n                \'    _pairwise = True\',\n                \'  else:\',\n                \'    _pairwise = pairwise\',\n                \'  if _pairwise:\',\n                f\'    raise ValueError(""Method rolling.{_method_name}(). The object pairwise\\\\n expected: False"")\'\n            ]\n        method_params = args + [\'{}={}\'.format(k, k) for k in kwargs if k != \'other\']\n        func_lines += df_rolling_method_main_codegen(method_params, self.data.columns, self.data.column_loc,\n                                                     method_name)\n\n        func_text = \'\\n\'.join(func_lines)\n\n        global_vars = {\'pandas\': pandas}\n\n        return func_text, global_vars\n\n    return df_rolling_method_other_none_codegen\n\n\ndf_rolling_method_other_none_codegen = gen_df_rolling_method_other_none_codegen()\ndf_rolling_cov_other_none_codegen = gen_df_rolling_method_other_none_codegen(\'cov\')\n\n\ndef df_rolling_method_codegen(method_name, self, args=None, kws=None):\n    args = args or []\n    kwargs = kws or {}\n\n    impl_params = [\'self\'] + args + kwsparams2list(kwargs)\n    impl_params_as_str = \', \'.join(impl_params)\n\n    impl_name = f\'_df_rolling_{method_name}_impl\'\n    func_lines = [f\'def {impl_name}({impl_params_as_str}):\']\n\n    method_params = args + [\'{}={}\'.format(k, k) for k in kwargs]\n    func_lines += df_rolling_method_main_codegen(method_params, self.data.columns,\n                                                 self.data.column_loc, method_name)\n    func_text = \'\\n\'.join(func_lines)\n\n    global_vars = {\'pandas\': pandas}\n\n    return func_text, global_vars\n\n\ndef gen_df_rolling_method_other_df_impl(method_name, self, other, args=None, kws=None):\n    func_text, global_vars = df_rolling_method_other_df_codegen(method_name, self, other,\n                                                                args=args, kws=kws)\n    loc_vars = {}\n    exec(func_text, global_vars, loc_vars)\n    _impl = loc_vars[f\'_df_rolling_{method_name}_other_df_impl\']\n\n    return _impl\n\n\ndef gen_df_rolling_method_other_none_impl(method_name, self, args=None, kws=None):\n    func_text, global_vars = df_rolling_method_other_none_codegen(method_name, self,\n                                                                  args=args, kws=kws)\n    loc_vars = {}\n    exec(func_text, global_vars, loc_vars)\n    _impl = loc_vars[f\'_df_rolling_{method_name}_other_none_impl\']\n\n    return _impl\n\n\ndef gen_df_rolling_cov_other_none_impl(method_name, self, args=None, kws=None):\n    func_text, global_vars = df_rolling_cov_other_none_codegen(method_name, self,\n                                                               args=args, kws=kws)\n    loc_vars = {}\n    exec(func_text, global_vars, loc_vars)\n    _impl = loc_vars[f\'_df_rolling_cov_other_none_impl\']\n\n    return _impl\n\n\ndef gen_df_rolling_method_impl(method_name, self, args=None, kws=None):\n    func_text, global_vars = df_rolling_method_codegen(method_name, self,\n                                                       args=args, kws=kws)\n    loc_vars = {}\n    exec(func_text, global_vars, loc_vars)\n    _impl = loc_vars[f\'_df_rolling_{method_name}_impl\']\n\n    return _impl\n\n\n@sdc_overload_method(DataFrameRollingType, \'apply\')\ndef sdc_pandas_dataframe_rolling_apply(self, func, raw=None):\n\n    ty_checker = TypeChecker(\'Method rolling.apply().\')\n    ty_checker.check(self, DataFrameRollingType)\n\n    raw_accepted = (Omitted, NoneType, Boolean)\n    if not isinstance(raw, raw_accepted) and raw is not None:\n        ty_checker.raise_exc(raw, \'bool\', \'raw\')\n\n    return gen_df_rolling_method_impl(\'apply\', self, args=[\'func\'],\n                                      kws={\'raw\': \'None\'})\n\n\n@sdc_overload_method(DataFrameRollingType, \'corr\')\ndef sdc_pandas_dataframe_rolling_corr(self, other=None, pairwise=None):\n\n    ty_checker = TypeChecker(\'Method rolling.corr().\')\n    ty_checker.check(self, DataFrameRollingType)\n\n    accepted_other = (Omitted, NoneType, DataFrameType, SeriesType)\n    if not isinstance(other, accepted_other) and other is not None:\n        ty_checker.raise_exc(other, \'DataFrame, Series\', \'other\')\n\n    accepted_pairwise = (bool, Boolean, Omitted, NoneType)\n    if not isinstance(pairwise, accepted_pairwise) and pairwise is not None:\n        ty_checker.raise_exc(pairwise, \'bool\', \'pairwise\')\n\n    none_other = isinstance(other, (Omitted, NoneType)) or other is None\n    kws = {\'other\': \'None\', \'pairwise\': \'None\'}\n\n    if none_other:\n        return gen_df_rolling_method_other_none_impl(\'corr\', self, kws=kws)\n\n    if isinstance(other, DataFrameType):\n        return gen_df_rolling_method_other_df_impl(\'corr\', self, other, kws=kws)\n\n    return gen_df_rolling_method_impl(\'corr\', self, kws=kws)\n\n\n@sdc_overload_method(DataFrameRollingType, \'count\')\ndef sdc_pandas_dataframe_rolling_count(self):\n\n    ty_checker = TypeChecker(\'Method rolling.count().\')\n    ty_checker.check(self, DataFrameRollingType)\n\n    return gen_df_rolling_method_impl(\'count\', self)\n\n\n@sdc_overload_method(DataFrameRollingType, \'cov\')\ndef sdc_pandas_dataframe_rolling_cov(self, other=None, pairwise=None, ddof=1):\n\n    ty_checker = TypeChecker(\'Method rolling.cov().\')\n    ty_checker.check(self, DataFrameRollingType)\n\n    accepted_other = (Omitted, NoneType, DataFrameType, SeriesType)\n    if not isinstance(other, accepted_other) and other is not None:\n        ty_checker.raise_exc(other, \'DataFrame, Series\', \'other\')\n\n    accepted_pairwise = (bool, Boolean, Omitted, NoneType)\n    if not isinstance(pairwise, accepted_pairwise) and pairwise is not None:\n        ty_checker.raise_exc(pairwise, \'bool\', \'pairwise\')\n\n    if not isinstance(ddof, (int, Integer, Omitted)):\n        ty_checker.raise_exc(ddof, \'int\', \'ddof\')\n\n    none_other = isinstance(other, (Omitted, NoneType)) or other is None\n    kws = {\'other\': \'None\', \'pairwise\': \'None\', \'ddof\': \'1\'}\n\n    if none_other:\n        # method _df_cov in comparison to method cov doesn\'t align input data\n        # by replacing infinite and matched finite values with nans\n        return gen_df_rolling_cov_other_none_impl(\'_df_cov\', self, kws=kws)\n\n    if isinstance(other, DataFrameType):\n        return gen_df_rolling_method_other_df_impl(\'cov\', self, other, kws=kws)\n\n    return gen_df_rolling_method_impl(\'cov\', self, kws=kws)\n\n\n@sdc_overload_method(DataFrameRollingType, \'kurt\')\ndef sdc_pandas_dataframe_rolling_kurt(self):\n\n    ty_checker = TypeChecker(\'Method rolling.kurt().\')\n    ty_checker.check(self, DataFrameRollingType)\n\n    return gen_df_rolling_method_impl(\'kurt\', self)\n\n\n@sdc_overload_method(DataFrameRollingType, \'max\')\ndef sdc_pandas_dataframe_rolling_max(self):\n\n    ty_checker = TypeChecker(\'Method rolling.max().\')\n    ty_checker.check(self, DataFrameRollingType)\n\n    return gen_df_rolling_method_impl(\'max\', self)\n\n\n@sdc_overload_method(DataFrameRollingType, \'mean\')\ndef sdc_pandas_dataframe_rolling_mean(self):\n\n    ty_checker = TypeChecker(\'Method rolling.mean().\')\n    ty_checker.check(self, DataFrameRollingType)\n\n    return gen_df_rolling_method_impl(\'mean\', self)\n\n\n@sdc_overload_method(DataFrameRollingType, \'median\')\ndef sdc_pandas_dataframe_rolling_median(self):\n\n    ty_checker = TypeChecker(\'Method rolling.median().\')\n    ty_checker.check(self, DataFrameRollingType)\n\n    return gen_df_rolling_method_impl(\'median\', self)\n\n\n@sdc_overload_method(DataFrameRollingType, \'min\')\ndef sdc_pandas_dataframe_rolling_min(self):\n\n    ty_checker = TypeChecker(\'Method rolling.min().\')\n    ty_checker.check(self, DataFrameRollingType)\n\n    return gen_df_rolling_method_impl(\'min\', self)\n\n\n@sdc_overload_method(DataFrameRollingType, \'quantile\')\ndef sdc_pandas_dataframe_rolling_quantile(self, quantile, interpolation=\'linear\'):\n\n    ty_checker = TypeChecker(\'Method rolling.quantile().\')\n    ty_checker.check(self, DataFrameRollingType)\n\n    if not isinstance(quantile, Number):\n        ty_checker.raise_exc(quantile, \'float\', \'quantile\')\n\n    str_types = (Omitted, StringLiteral, UnicodeType)\n    if not isinstance(interpolation, str_types) and interpolation != \'linear\':\n        ty_checker.raise_exc(interpolation, \'str\', \'interpolation\')\n\n    return gen_df_rolling_method_impl(\'quantile\', self, args=[\'quantile\'],\n                                      kws={\'interpolation\': \'""linear""\'})\n\n\n@sdc_overload_method(DataFrameRollingType, \'skew\')\ndef sdc_pandas_dataframe_rolling_skew(self):\n\n    ty_checker = TypeChecker(\'Method rolling.skew().\')\n    ty_checker.check(self, DataFrameRollingType)\n\n    return gen_df_rolling_method_impl(\'skew\', self)\n\n\n@sdc_overload_method(DataFrameRollingType, \'std\')\ndef sdc_pandas_dataframe_rolling_std(self, ddof=1):\n\n    ty_checker = TypeChecker(\'Method rolling.std().\')\n    ty_checker.check(self, DataFrameRollingType)\n\n    if not isinstance(ddof, (int, Integer, Omitted)):\n        ty_checker.raise_exc(ddof, \'int\', \'ddof\')\n\n    return gen_df_rolling_method_impl(\'std\', self, kws={\'ddof\': \'1\'})\n\n\n@sdc_overload_method(DataFrameRollingType, \'sum\')\ndef sdc_pandas_dataframe_rolling_sum(self):\n\n    ty_checker = TypeChecker(\'Method rolling.sum().\')\n    ty_checker.check(self, DataFrameRollingType)\n\n    return gen_df_rolling_method_impl(\'sum\', self)\n\n\n@sdc_overload_method(DataFrameRollingType, \'var\')\ndef sdc_pandas_dataframe_rolling_var(self, ddof=1):\n\n    ty_checker = TypeChecker(\'Method rolling.var().\')\n    ty_checker.check(self, DataFrameRollingType)\n\n    if not isinstance(ddof, (int, Integer, Omitted)):\n        ty_checker.raise_exc(ddof, \'int\', \'ddof\')\n\n    return gen_df_rolling_method_impl(\'var\', self, kws={\'ddof\': \'1\'})\n\n\nsdc_pandas_dataframe_rolling_apply.__doc__ = sdc_pandas_dataframe_rolling_docstring_tmpl.format(**{\n    \'method_name\': \'apply\',\n    \'example_caption\': \'Calculate the rolling apply.\',\n    \'limitations_block\':\n    """"""\n    Limitations\n    -----------\n    - This function may reveal slower performance than Pandas* on user system. Users should exercise a tradeoff\n    between staying in JIT-region with that function or going back to interpreter mode.\n    - Supported ``raw`` only can be `None` or `True`. Parameters ``args``, ``kwargs`` unsupported.\n    - DataFrame elements cannot be max/min float/integer. Otherwise SDC and Pandas results are different.\n    """""",\n    \'extra_params\':\n    """"""\n    func:\n        A single value producer\n    raw: :obj:`bool`\n        False : passes each row or column as a Series to the function.\n        True or None : the passed function will receive ndarray objects instead.\n    """"""\n})\n\nsdc_pandas_dataframe_rolling_corr.__doc__ = sdc_pandas_dataframe_rolling_docstring_tmpl.format(**{\n    \'method_name\': \'corr\',\n    \'example_caption\': \'Calculate rolling correlation.\',\n    \'limitations_block\':\n    """"""\n    Limitations\n    -----------\n    DataFrame elements cannot be max/min float/integer. Otherwise SDC and Pandas results are different.\n    """""",\n    \'extra_params\':\n    """"""\n    other: :obj:`Series` or :obj:`DataFrame`\n        Other Series or DataFrame.\n    pairwise: :obj:`bool`\n        Calculate pairwise combinations of columns within a DataFrame.\n    """"""\n})\n\nsdc_pandas_dataframe_rolling_count.__doc__ = sdc_pandas_dataframe_rolling_docstring_tmpl.format(**{\n    \'method_name\': \'count\',\n    \'example_caption\': \'Count of any non-NaN observations inside the window.\',\n    \'limitations_block\': \'\',\n    \'extra_params\': \'\'\n})\n\nsdc_pandas_dataframe_rolling_cov.__doc__ = sdc_pandas_dataframe_rolling_docstring_tmpl.format(**{\n    \'method_name\': \'cov\',\n    \'example_caption\': \'Calculate rolling covariance.\',\n    \'limitations_block\':\n    """"""\n    Limitations\n    -----------\n    DataFrame elements cannot be max/min float/integer. Otherwise SDC and Pandas results are different.\n    Different size of `self` and `other` can produce result different from the result of Pandas\n    due to different float rounding in Python and SDC.\n    """""",\n    \'extra_params\':\n    """"""\n    other: :obj:`Series` or :obj:`DataFrame`\n        Other Series or DataFrame.\n    pairwise: :obj:`bool`\n        Calculate pairwise combinations of columns within a DataFrame.\n    ddof: :obj:`int`\n        Delta Degrees of Freedom.\n    """"""\n})\n\nsdc_pandas_dataframe_rolling_kurt.__doc__ = sdc_pandas_dataframe_rolling_docstring_tmpl.format(**{\n    \'method_name\': \'kurt\',\n    \'example_caption\': \'Calculate unbiased rolling kurtosis.\',\n    \'limitations_block\': \'\',\n    \'extra_params\': \'\'\n})\n\nsdc_pandas_dataframe_rolling_max.__doc__ = sdc_pandas_dataframe_rolling_docstring_tmpl.format(**{\n    \'method_name\': \'max\',\n    \'example_caption\': \'Calculate the rolling maximum.\',\n    \'limitations_block\': \'\',\n    \'extra_params\': \'\'\n})\n\nsdc_pandas_dataframe_rolling_mean.__doc__ = sdc_pandas_dataframe_rolling_docstring_tmpl.format(**{\n    \'method_name\': \'mean\',\n    \'example_caption\': \'Calculate the rolling mean of the values.\',\n    \'limitations_block\':\n    """"""\n    Limitations\n    -----------\n    DataFrame elements cannot be max/min float/integer. Otherwise SDC and Pandas results are different.\n    """""",\n    \'extra_params\': \'\'\n})\n\nsdc_pandas_dataframe_rolling_median.__doc__ = sdc_pandas_dataframe_rolling_docstring_tmpl.format(**{\n    \'method_name\': \'median\',\n    \'example_caption\': \'Calculate the rolling median.\',\n    \'limitations_block\':\n    """"""\n    Limitations\n    -----------\n    This function may reveal slower performance than Pandas* on user system. Users should exercise a tradeoff\n    between staying in JIT-region with that function or going back to interpreter mode.\n    """""",\n    \'extra_params\': \'\'\n})\n\nsdc_pandas_dataframe_rolling_min.__doc__ = sdc_pandas_dataframe_rolling_docstring_tmpl.format(**{\n    \'method_name\': \'min\',\n    \'example_caption\': \'Calculate the rolling minimum.\',\n    \'limitations_block\': \'\',\n    \'extra_params\': \'\'\n})\n\nsdc_pandas_dataframe_rolling_quantile.__doc__ = sdc_pandas_dataframe_rolling_docstring_tmpl.format(**{\n    \'method_name\': \'quantile\',\n    \'example_caption\': \'Calculate the rolling quantile.\',\n    \'limitations_block\':\n    """"""\n    Limitations\n    -----------\n    - This function may reveal slower performance than Pandas* on user system. Users should exercise a tradeoff\n    between staying in JIT-region with that function or going back to interpreter mode.\n    - Supported ``interpolation`` only can be `\'linear\'`.\n    - DataFrame elements cannot be max/min float/integer. Otherwise SDC and Pandas results are different.\n    """""",\n    \'extra_params\':\n    """"""\n    quantile: :obj:`float`\n        Quantile to compute. 0 <= quantile <= 1.\n    interpolation: :obj:`str`\n        This optional parameter specifies the interpolation method to use.\n    """"""\n})\n\nsdc_pandas_dataframe_rolling_skew.__doc__ = sdc_pandas_dataframe_rolling_docstring_tmpl.format(**{\n    \'method_name\': \'skew\',\n    \'example_caption\': \'Unbiased rolling skewness.\',\n    \'limitations_block\': \'\',\n    \'extra_params\': \'\'\n})\n\nsdc_pandas_dataframe_rolling_std.__doc__ = sdc_pandas_dataframe_rolling_docstring_tmpl.format(**{\n    \'method_name\': \'std\',\n    \'example_caption\': \'Calculate rolling standard deviation.\',\n    \'limitations_block\':\n    """"""\n    Limitations\n    -----------\n    DataFrame elements cannot be max/min float/integer. Otherwise SDC and Pandas results are different.\n    """""",\n    \'extra_params\':\n    """"""\n    ddof: :obj:`int`\n        Delta Degrees of Freedom.\n    """"""\n})\n\nsdc_pandas_dataframe_rolling_sum.__doc__ = sdc_pandas_dataframe_rolling_docstring_tmpl.format(**{\n    \'method_name\': \'sum\',\n    \'example_caption\': \'Calculate rolling sum of given Series.\',\n    \'limitations_block\':\n    """"""\n    Limitations\n    -----------\n    DataFrame elements cannot be max/min float/integer. Otherwise SDC and Pandas results are different.\n    """""",\n    \'extra_params\': \'\'\n})\n\nsdc_pandas_dataframe_rolling_var.__doc__ = sdc_pandas_dataframe_rolling_docstring_tmpl.format(**{\n    \'method_name\': \'var\',\n    \'example_caption\': \'Calculate unbiased rolling variance.\',\n    \'limitations_block\':\n    """"""\n    Limitations\n    -----------\n    DataFrame elements cannot be max/min float/integer. Otherwise SDC and Pandas results are different.\n    """""",\n    \'extra_params\':\n    """"""\n    ddof: :obj:`int`\n        Delta Degrees of Freedom.\n    """"""\n})\n'"
sdc/datatypes/hpat_pandas_dataframe_rolling_types.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nfrom numba.extending import intrinsic, register_model\nfrom sdc.datatypes.hpat_pandas_rolling_types import (\n    gen_hpat_pandas_rolling_init, RollingType, RollingTypeModel)\n\n\nclass DataFrameRollingType(RollingType):\n    """"""Type definition for pandas.DataFrame.rolling functions handling.""""""\n    def __init__(self, data, win_type=None, on=None, closed=None):\n        super(DataFrameRollingType, self).__init__(\'DataFrameRollingType\',\n                                                   data, win_type=win_type,\n                                                   on=on, closed=closed)\n\n\n@register_model(DataFrameRollingType)\nclass DataFrameRollingTypeModel(RollingTypeModel):\n    """"""Model for DataFrameRollingType type.""""""\n    def __init__(self, dmm, fe_type):\n        super(DataFrameRollingTypeModel, self).__init__(dmm, fe_type)\n\n\n_hpat_pandas_df_rolling_init = intrinsic(gen_hpat_pandas_rolling_init(\n    DataFrameRollingType))\n'"
sdc/datatypes/hpat_pandas_dataframe_types.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\n\n| :class:`pandas.DataFrame` type implementation in Intel SDC\n| Also, it contains related types and iterators for DataFrame type handling\n\n""""""\n\n\nimport operator\nimport pandas\n\nfrom numba import types\nfrom numba.core import cgutils\nfrom numba.extending import (models, overload, register_model, make_attribute_wrapper, intrinsic, box, unbox)\nfrom numba.core.datamodel import register_default, StructModel\nfrom numba.core.typing.templates import signature, infer_global, AbstractTemplate\n\nfrom sdc.config import config_pipeline_hpat_default\n\n\nclass DataFrameTypeIterator(types.SimpleIteratorType):\n    """"""\n    Iterator type for DataFrameType type\n\n    Members\n    ----------\n    _data: :class:`DataFrameType`\n        input arg\n    """"""\n\n    def __init__(self, data=None):\n        self.data = data\n\n        super(DataFrameTypeIterator, self).__init__(""DataFrameTypeIterator(data={})"".format(self.data), data)\n\n\n@register_default(DataFrameTypeIterator)\nclass DataFrameTypeIteratorModel(StructModel):\n    """"""\n    Model for DataFrameTypeIterator type\n    All members must be the same as main type for this model\n\n    Test:\n    """"""\n\n    def __init__(self, dmm, fe_type):\n        members = [\n            (\'data\', fe_type.data),\n        ]\n        super(DataFrameTypeIteratorModel, self).__init__(dmm, fe_type, members)\n\n\nmake_attribute_wrapper(DataFrameTypeIterator, \'data\', \'_data\')\n\n\nclass DataFrameType(types.IterableType):\n    """"""\n    Type definition for DataFrame functions handling.\n\n    Members\n    ----------\n    data: Dictinary of :class:`SeriesType`\n        input arg\n\n    index: DataFrame index\n        *unsupported*\n\n        Dictinary looks a bit ambigous due to keys are column names which already presented in Series.\n        This type selected due to pandas.DataFrame interprets input :class:`SeriesType` as rows instead\n        expected columns if passed as a list.\n        This data is interpreted as columns if passed as a dictinary only.\n\n    Test: python -m sdc.runtests sdc.tests.test_dataframe.TestDataFrame.test_create\n    """"""\n\n    def __init__(self, data=None):\n\n        self.data = data\n\n        type_str = ""DataFrameType(data={})"".format(self.data)\n        super(DataFrameType, self).__init__(type_str)\n\n    @property\n    def iterator_type(self):\n        return DataFrameTypeIterator(self)\n\n\nif not config_pipeline_hpat_default:\n    @register_model(DataFrameType)\n    class DataFrameTypeModel(StructModel):\n        """"""\n        Model for DataFrameType type\n        All members must be the same as main type for this model\n\n        Test: python -m sdc.runtests sdc.tests.test_dataframe.TestDataFrame.test_create_numeric_column\n        """"""\n\n        def __init__(self, dmm, fe_type):\n            members = [\n                (\'data\', fe_type.data)\n            ]\n            models.StructModel.__init__(self, dmm, fe_type, members)\n\n    make_attribute_wrapper(DataFrameType, \'data\', \'_data\')\n\n\n@intrinsic\ndef _hpat_pandas_dataframe_init(typingctx, data=None):\n    """"""\n    Internal Numba required function to register DataFrameType and\n    connect it with corresponding Python type mentioned in @overload(pandas.DataFrame)\n    """"""\n\n    def _hpat_pandas_dataframe_init_codegen(context, builder, signature, args):\n        """"""\n        It is looks like it creates DataFrameModel structure\n\n        - Fixed number of parameters. Must be 4\n        - increase reference counr for the data\n        """"""\n\n        [data_val] = args\n\n        dataframe = cgutils.create_struct_proxy(signature.return_type)(context, builder)\n        dataframe.data = data_val\n\n        if context.enable_nrt:\n            context.nrt.incref(builder, data, dataframe.data)\n\n        return dataframe._getvalue()\n\n    ret_typ = DataFrameType(data)\n    sig = signature(ret_typ, data)\n    """"""\n    Construct signature of the Numba DataFrameType::ctor()\n    """"""\n\n    return sig, _hpat_pandas_dataframe_init_codegen\n\n\nif not config_pipeline_hpat_default:\n    @overload(pandas.DataFrame)\n    def hpat_pandas_dataframe(data=None, index=None, columns=None, dtype=None, copy=False):\n        """"""\n        Special Numba procedure to overload Python type pandas.DataFrame::ctor() with Numba registered model\n        """"""\n\n        if isinstance(data, types.DictType):\n            def hpat_pandas_dataframe_impl(data=None, index=None, columns=None, dtype=None, copy=False):\n                series_dict = {}\n                series_list = []\n\n                for key, value in data.items():\n                    """"""\n                    Convert input dictionary with:\n                        key - unicode string\n                        value - array\n                    into dictinary of pandas.Series with same names and values\n                    """"""\n\n                    series_item = pandas.Series(data=value, name=key)\n                    series_dict[key] = series_item\n                    series_list.append(series_item)\n\n                # return _hpat_pandas_dataframe_init(series_dict)\n                return _hpat_pandas_dataframe_init(series_list)\n\n            return hpat_pandas_dataframe_impl\n\n    @box(DataFrameType)\n    def hpat_pandas_dataframe_box(typ, val, c):\n        """"""\n        This method is to copy data from JITted region data structure\n        to new Python object data structure.\n        Python object data structure has creating in this procedure.\n        """"""\n\n        dataframe = cgutils.create_struct_proxy(typ)(c.context, c.builder, value=val)\n\n        ir_ptr_data = c.box(typ.data, dataframe.data)\n\n        dataframe_ctor_args = c.pyapi.tuple_pack([ir_ptr_data, ])\n        # dataframe_ctor_kwargs = c.pyapi.dict_pack([(""data"", ir_ptr_data), ])\n        """"""\n        It is better to use kwargs but it fails into SIGSEGV\n        """"""\n\n        dataframe_ctor_fn = c.pyapi.unserialize(c.pyapi.serialize_object(pandas.DataFrame))\n        """"""\n        Create a pandas.DataFrame ctor() function pointer\n        """"""\n\n        df_obj = c.pyapi.call(dataframe_ctor_fn, dataframe_ctor_args)  # kws=dataframe_ctor_kwargs)\n        """"""\n        Call pandas.DataFrame function pointer with parameters\n        """"""\n\n        c.pyapi.decref(ir_ptr_data)\n        c.pyapi.decref(dataframe_ctor_args)\n        c.pyapi.decref(dataframe_ctor_fn)\n\n        return df_obj\n'"
sdc/datatypes/hpat_pandas_functions.py,3,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\'\'\'\n| :module:`pandas` functions and operators implementations in Intel SDC\n\'\'\'\n\nimport pandas as pd\nimport numpy as np\n\nimport numba\nfrom numba import types\nfrom numba.np import numpy_support\nfrom numba.core.errors import TypingError\nfrom numba.extending import overload\n\nfrom sdc.io.csv_ext import (\n    _gen_csv_reader_py_pyarrow_py_func,\n    _gen_csv_reader_py_pyarrow_func_text_dataframe,\n)\nfrom sdc.str_arr_ext import string_array_type\n\nfrom sdc.hiframes import join, aggregate, sort\nfrom sdc.types import CategoricalDtypeType, Categorical\n\n\ndef get_numba_array_types_for_csv(df):\n    """"""Extracts Numba array types from the given DataFrame.""""""\n    result = []\n    for numpy_type in df.dtypes.values:\n        try:\n            numba_type = numpy_support.from_dtype(numpy_type)\n        except NotImplementedError:\n            numba_type = None\n\n        if numba_type and numba_type != types.pyobject:\n            array_type = types.Array(numba_type, 1, \'C\')\n        else:\n            # default type for CSV is string\n            array_type = string_array_type\n\n        result.append(array_type)\n    return result\n\n\ndef infer_column_names_and_types_from_constant_filename(fname_const, delimiter, names, usecols, skiprows):\n    rows_to_read = 100  # TODO: tune this\n    df = pd.read_csv(fname_const, delimiter=delimiter, names=names,\n                     usecols=usecols, skiprows=skiprows, nrows=rows_to_read)\n    # TODO: string_array, categorical, etc.\n    col_names = df.columns.to_list()\n    col_typs = get_numba_array_types_for_csv(df)\n    return col_names, col_typs\n\n\n@overload(pd.read_csv)\ndef sdc_pandas_read_csv(\n    filepath_or_buffer,\n    sep=\',\',\n    delimiter=None,\n    # Column and Index Locations and Names\n    header=""infer"",\n    names=None,\n    index_col=None,\n    usecols=None,\n    squeeze=False,\n    prefix=None,\n    mangle_dupe_cols=True,\n    # General Parsing Configuration\n    dtype=None,\n    engine=None,\n    converters=None,\n    true_values=None,\n    false_values=None,\n    skipinitialspace=False,\n    skiprows=None,\n    skipfooter=0,\n    nrows=None,\n    # NA and Missing Data Handling\n    na_values=None,\n    keep_default_na=True,\n    na_filter=True,\n    verbose=False,\n    skip_blank_lines=True,\n    # Datetime Handling\n    parse_dates=False,\n    infer_datetime_format=False,\n    keep_date_col=False,\n    date_parser=None,\n    dayfirst=False,\n    cache_dates=True,\n    # Iteration\n    iterator=False,\n    chunksize=None,\n    # Quoting, Compression, and File Format\n    compression=""infer"",\n    thousands=None,\n    decimal=b""."",\n    lineterminator=None,\n    quotechar=\'""\',\n    # quoting=csv.QUOTE_MINIMAL,  # not supported\n    doublequote=True,\n    escapechar=None,\n    comment=None,\n    encoding=None,\n    dialect=None,\n    # Error Handling\n    error_bad_lines=True,\n    warn_bad_lines=True,\n    # Internal\n    delim_whitespace=False,\n    # low_memory=_c_parser_defaults[""low_memory""],  # not supported\n    memory_map=False,\n    float_precision=None,\n):\n    signature = """"""\n        filepath_or_buffer,\n        sep=\',\',\n        delimiter=None,\n        # Column and Index Locations and Names\n        header=""infer"",\n        names=None,\n        index_col=None,\n        usecols=None,\n        squeeze=False,\n        prefix=None,\n        mangle_dupe_cols=True,\n        # General Parsing Configuration\n        dtype=None,\n        engine=None,\n        converters=None,\n        true_values=None,\n        false_values=None,\n        skipinitialspace=False,\n        skiprows=None,\n        skipfooter=0,\n        nrows=None,\n        # NA and Missing Data Handling\n        na_values=None,\n        keep_default_na=True,\n        na_filter=True,\n        verbose=False,\n        skip_blank_lines=True,\n        # Datetime Handling\n        parse_dates=False,\n        infer_datetime_format=False,\n        keep_date_col=False,\n        date_parser=None,\n        dayfirst=False,\n        cache_dates=True,\n        # Iteration\n        iterator=False,\n        chunksize=None,\n        # Quoting, Compression, and File Format\n        compression=""infer"",\n        thousands=None,\n        decimal=b""."",\n        lineterminator=None,\n        quotechar=\'""\',\n        # quoting=csv.QUOTE_MINIMAL,  # not supported\n        doublequote=True,\n        escapechar=None,\n        comment=None,\n        encoding=None,\n        dialect=None,\n        # Error Handling\n        error_bad_lines=True,\n        warn_bad_lines=True,\n        # Internal\n        delim_whitespace=False,\n        # low_memory=_c_parser_defaults[""low_memory""],  # not supported\n        memory_map=False,\n        float_precision=None,\n    """"""\n\n    # read_csv can infer result DataFrame type from file or from params\n\n    # for inferring from file this parameters should be literal or omitted\n    infer_from_file = all([\n        isinstance(filepath_or_buffer, types.Literal),\n        isinstance(sep, (types.Literal, types.Omitted)) or sep == \',\',\n        isinstance(delimiter, (types.Literal, types.Omitted)) or delimiter is None,\n        isinstance(names, (types.Tuple, types.Omitted, type(None))),\n        isinstance(usecols, (types.Tuple, types.Omitted, type(None))),\n        isinstance(skiprows, (types.Literal, types.Omitted)) or skiprows is None,\n    ])\n\n    # for inference from params dtype and (names or usecols) shoud present\n    # names, dtype and usecols should be literal tuples after rewrite pass (see. RewriteReadCsv)\n    # header not supported\n    infer_from_params = all([\n        isinstance(dtype, types.Tuple),\n        any([\n            isinstance(names, types.Tuple) and isinstance(usecols, types.Tuple),\n            isinstance(names, types.Tuple) and isinstance(usecols, (types.Omitted, type(None))),\n            isinstance(names, (types.Omitted, type(None))) and isinstance(usecols, types.Tuple),\n        ]),\n        isinstance(header, types.Omitted) or header == \'infer\',\n    ])\n\n    # cannot create function if parameters provide not enough info\n    if not any([infer_from_file, infer_from_params]):\n        msg = ""Cannot infer resulting DataFrame from constant file or parameters.""\n        raise TypingError(msg)\n\n    if infer_from_file:\n        # parameters should be constants and are important only for inference from file\n\n        if isinstance(filepath_or_buffer, types.Literal):\n            filepath_or_buffer = filepath_or_buffer.literal_value\n\n        if isinstance(sep, types.Literal):\n            sep = sep.literal_value\n\n        if isinstance(delimiter, types.Literal):\n            delimiter = delimiter.literal_value\n\n        # Alias sep -> delimiter.\n        if delimiter is None:\n            delimiter = sep\n\n        if isinstance(skiprows, types.Literal):\n            skiprows = skiprows.literal_value\n\n    # names and usecols influence on both inferencing from file and from params\n    if isinstance(names, types.Tuple):\n        assert all(isinstance(name, types.Literal) for name in names)\n        names = [name.literal_value for name in names]\n\n    if isinstance(usecols, types.Tuple):\n        assert all(isinstance(col, types.Literal) for col in usecols)\n        usecols = [col.literal_value for col in usecols]\n\n    if infer_from_params:\n        # dtype should be constants and is important only for inference from params\n        if isinstance(dtype, types.Tuple):\n            assert all(isinstance(key, types.Literal) for key in dtype[::2])\n            keys = (k.literal_value for k in dtype[::2])\n\n            values = dtype[1::2]\n            values = [v.typing_key if isinstance(v, types.Function) else v for v in values]\n            values = [types.Array(numba.from_dtype(np.dtype(v.literal_value)), 1, \'C\')\n                      if isinstance(v, types.Literal) else v for v in values]\n            values = [types.Array(types.int_, 1, \'C\') if v == int else v for v in values]\n            values = [types.Array(types.float64, 1, \'C\') if v == float else v for v in values]\n            values = [string_array_type if v == str else v for v in values]\n            values = [Categorical(v) if isinstance(v, CategoricalDtypeType) else v for v in values]\n\n            dtype = dict(zip(keys, values))\n\n    # in case of both are available\n    # inferencing from params has priority over inferencing from file\n    if infer_from_params:\n        col_names = names\n        # all names should be in dtype\n        return_columns = usecols if usecols else names\n        col_typs = [dtype[n] for n in return_columns]\n\n    elif infer_from_file:\n        col_names, col_typs = infer_column_names_and_types_from_constant_filename(\n            filepath_or_buffer, delimiter, names, usecols, skiprows)\n\n    else:\n        return None\n\n    dtype_present = not isinstance(dtype, (types.Omitted, type(None)))\n\n    # generate function text with signature and returning DataFrame\n    func_text, func_name = _gen_csv_reader_py_pyarrow_func_text_dataframe(\n        col_names, col_typs, dtype_present, usecols, signature)\n\n    # compile with Python\n    csv_reader_py = _gen_csv_reader_py_pyarrow_py_func(func_text, func_name)\n\n    return csv_reader_py\n\n\nsdc_pandas_read_csv.__doc__ = r""""""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.read_csv\n\n    Limitations\n    -----------\n    - Parameters \\\n        ``header``, \\\n        ``index_col``, \\\n        ``squeeze``, \\\n        ``prefix``, \\\n        ``mangle_dupe_cols``, \\\n        ``engine``, \\\n        ``converters``, \\\n        ``true_values``, \\\n        ``false_values``, \\\n        ``skipinitialspace``, \\\n        ``skipfooter``, \\\n        ``nrows``, \\\n        ``na_values``, \\\n        ``keep_default_na``, \\\n        ``na_filter``, \\\n        ``verbose``, \\\n        ``skip_blank_lines``, \\\n        ``parse_dates``, \\\n        ``infer_datetime_format``, \\\n        ``keep_date_col``, \\\n        ``date_parser``, \\\n        ``dayfirst``, \\\n        ``cache_dates``, \\\n        ``iterator``, \\\n        ``chunksize``, \\\n        ``compression``, \\\n        ``thousands``, \\\n        ``decimal``, \\\n        ``lineterminator``, \\\n        ``quotechar``, \\\n        ``quoting``, \\\n        ``doublequote``, \\\n        ``escapechar``, \\\n        ``comment``, \\\n        ``encoding``, \\\n        ``dialect``, \\\n        ``error_bad_lines``, \\\n        ``warn_bad_lines``, \\\n        ``delim_whitespace``, \\\n        ``low_memory``, \\\n        ``memory_map`` and \\\n        ``float_precision`` \\\n        are currently unsupported by Intel Scalable Dataframe Compiler.\n    - Resulting DataFrame type could be inferred from constant file name of from parameters. \\\n        ``filepath_or_buffer`` could be constant for inferencing from file. \\\n        ``filepath_or_buffer`` could be variable for inferencing from parameters if ``dtype`` is constant. \\\n        If both ``filepath_or_buffer`` and ``dtype`` are constants then default is inferencing from parameters.\n    - For inferring from parameters ``names`` or ``usecols`` should be provided additionally to ``dtype``.\n    - For inferring from file ``sep``, ``delimiter`` and ``skiprows`` should be constants or omitted.\n    - ``names`` and ``usecols`` should be constants or omitted for both types of inferrencing.\n    - ``usecols`` with list of ints is unsupported by Intel Scalable Dataframe Compiler.\n\n    Examples\n    --------\n    Inference from file. File name is constant. \\\n    Resulting DataFrame depends on CSV file content at the moment of compilation.\n\n    >>> pd.read_csv(\'data.csv\')  # doctest: +SKIP\n\n    Inference from file. File name, ``names``, ``usecols``, ``delimiter`` and ``skiprow`` are constants. \\\n    Resulting DataFrame contains one column ``A`` \\\n    with type of column depending on CSV file content at the moment of compilation.\n\n    >>> pd.read_csv(\'data.csv\', names=[\'A\',\'B\'], usecols=[\'A\'], delimiter=\';\', skiprows=2)  # doctest: +SKIP\n\n    Inference from parameters. File name, ``delimiter`` and ``skiprow`` are variables. \\\n    ``names``, ``usecols`` and ``dtype`` are constants. \\\n    Resulting DataFrame contains column ``A`` with type ``np.float64``.\n\n    >>> pd.read_csv(file_name, names=[\'A\',\'B\'], usecols=[\'A\'], dtype={\'A\': np.float64}, \\\n                    delimiter=some_char, skiprows=some_int)  # doctest: +SKIP\n""""""\n'"
sdc/datatypes/hpat_pandas_getitem_types.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\nimport pandas\n\nfrom numba import types\nfrom numba.core import cgutils\nfrom numba.extending import (models, overload, register_model, make_attribute_wrapper, intrinsic)\nfrom numba.core.datamodel import (register_default, StructModel)\nfrom numba.core.typing.templates import signature\n\n\nclass SeriesGetitemAccessorType(types.Type):\n    def __init__(self, series, accessor):\n        self.series = series\n        self.accessor = accessor\n        super(SeriesGetitemAccessorType, self).__init__(\'SeriesGetitemAccessorType({}, {})\\\n            \'.format(series, accessor))\n\n\n@register_model(SeriesGetitemAccessorType)\nclass SeriesGetitemAccessorTypeModel(StructModel):\n    def __init__(self, dmm, fe_type):\n        members = [\n            (\'series\', fe_type.series),\n        ]\n        models.StructModel.__init__(self, dmm, fe_type, members)\n\n\nmake_attribute_wrapper(SeriesGetitemAccessorType, \'series\', \'_series\')\n\n\n@intrinsic\ndef series_getitem_accessor_init(typingctx, series, accessor):\n    def series_getitem_accessor_init_codegen(context, builder, signature, args):\n        series_val, accessor_val = args\n        getitem_accessor = cgutils.create_struct_proxy(\n            signature.return_type)(context, builder)\n        getitem_accessor.series = series_val\n\n        if context.enable_nrt:\n            context.nrt.incref(builder, signature.args[0], series_val)\n\n        return getitem_accessor._getvalue()\n\n    ret_typ = SeriesGetitemAccessorType(series, accessor)\n    sig = signature(ret_typ, series, accessor)\n\n    return sig, series_getitem_accessor_init_codegen\n'"
sdc/datatypes/hpat_pandas_groupby_functions.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\n| :class:`pandas.DataFrame.GroupBy` functions and operators implementations in Intel SDC\n""""""\n\nimport pandas\nimport numba\nimport numpy\nimport operator\nimport sdc\n\nfrom numba import types\nfrom numba.core import cgutils\nfrom numba.extending import intrinsic\nfrom numba.core.registry import cpu_target\nfrom numba.core.typing import signature\nfrom numba import literally\n\nfrom sdc.datatypes.common_functions import sdc_arrays_argsort, _sdc_asarray, _sdc_take\nfrom sdc.datatypes.hpat_pandas_groupby_types import DataFrameGroupByType, SeriesGroupByType\nfrom sdc.utilities.sdc_typing_utils import TypeChecker, kwsparams2list, sigparams2list\nfrom sdc.utilities.utils import (sdc_overload, sdc_overload_method, sdc_register_jitable)\nfrom sdc.hiframes.pd_series_type import SeriesType\nfrom sdc.str_ext import string_type\n\n\nperformance_limitation = ""This function may reveal slower performance than Pandas* on user system.\\\n    Users should exercise a tradeoff between staying in JIT-region with that function or\\\n    going back to interpreter mode.""\n\n\n@sdc_register_jitable\ndef merge_groupby_dicts_inplace(left, right):\n    """""" Merges one internal groupby dictionary (right) into another one (left) and returns\n    the merging result. Used for aggregating partial dicts into single result. """"""\n\n    if not right:\n        return left\n\n    for key, right_group_list in right.items():\n        left_group_list = left.get(key)\n        if left_group_list is None:\n            left[key] = right_group_list\n        else:\n            left_group_list.extend(right_group_list)\n\n    return left\n\n\n@intrinsic\ndef init_dataframe_groupby(typingctx, parent, column_id, data, sort, target_columns=None):\n\n    target_columns = types.none if target_columns is None else target_columns\n    if isinstance(target_columns, types.NoneType):\n        target_not_specified = True\n        selected_col_names = tuple([a for i, a in enumerate(parent.columns) if i != column_id.literal_value])\n    else:\n        target_not_specified = False\n        selected_col_names = tuple([a.literal_value for a in target_columns])\n\n    n_target_cols = len(selected_col_names)\n    def codegen(context, builder, signature, args):\n        parent_val, column_id_val, data_val, sort_val, target_columns = args\n        # create series struct and store values\n        groupby_obj = cgutils.create_struct_proxy(\n            signature.return_type)(context, builder)\n        groupby_obj.parent = parent_val\n        groupby_obj.col_id = column_id_val\n        groupby_obj.data = data_val\n        groupby_obj.sort = sort_val\n        groupby_obj.target_default = context.get_constant(types.bool_, target_not_specified)\n\n        column_strs = [numba.cpython.unicode.make_string_from_constant(\n            context, builder, string_type, c) for c in selected_col_names]\n        column_tup = context.make_tuple(\n            builder, types.UniTuple(string_type, n_target_cols), column_strs)\n\n        groupby_obj.target_columns = column_tup\n\n        # increase refcount of stored values\n        if context.enable_nrt:\n            context.nrt.incref(builder, signature.args[0], parent_val)\n            context.nrt.incref(builder, signature.args[2], data_val)\n            for var in column_strs:\n                context.nrt.incref(builder, string_type, var)\n\n        return groupby_obj._getvalue()\n\n    ret_typ = DataFrameGroupByType(parent, column_id, selected_col_names)\n    sig = signature(ret_typ, parent, column_id, data, sort, target_columns)\n    return sig, codegen\n\n\n@intrinsic\ndef init_series_groupby(typingctx, parent, by_data, data, sort):\n\n    def codegen(context, builder, signature, args):\n        parent_val, _, data_val, sort_val = args\n        # create series struct and store values\n        groupby_obj = cgutils.create_struct_proxy(\n            signature.return_type)(context, builder)\n        groupby_obj.parent = parent_val\n        groupby_obj.data = data_val\n        groupby_obj.sort = sort_val\n\n        # increase refcount of stored values\n        if context.enable_nrt:\n            context.nrt.incref(builder, signature.args[0], parent_val)\n            context.nrt.incref(builder, signature.args[2], data_val)\n\n        return groupby_obj._getvalue()\n\n    ret_typ = SeriesGroupByType(parent, by_data)\n    sig = signature(ret_typ, parent, by_data, data, sort)\n    return sig, codegen\n\n\n@sdc_overload(operator.getitem)\ndef sdc_pandas_dataframe_getitem(self, idx):\n\n    if not isinstance(self, DataFrameGroupByType):\n        return None\n\n    idx_is_literal_str = isinstance(idx, types.StringLiteral)\n    if (idx_is_literal_str\n        or (isinstance(idx, types.Tuple)\n            and all(isinstance(a, types.StringLiteral) for a in idx))):\n\n        by_col_id_literal = self.col_id.literal_value\n        by_col_loc = self.parent.column_loc[self.parent.columns[by_col_id_literal]]\n        by_type_id, by_col_id = by_col_loc.type_id, by_col_loc.col_id\n\n        if idx_is_literal_str:\n            target_col_id_literal = self.parent.columns.index(idx.literal_value)\n            target_col_loc = self.parent.column_loc[self.parent.columns[target_col_id_literal]]\n            target_type_id, target_col_id = target_col_loc.type_id, target_col_loc.col_id\n\n        def sdc_pandas_dataframe_getitem_common_impl(self, idx):\n\n            # calling getitem twice raises IndexError, just as in pandas\n            if not self._target_default:\n                raise IndexError(""DataFrame.GroupBy.getitem: Columns already selected"")\n\n            if idx_is_literal_str == True:  # noqa\n                # no need to pass index into this series, as we group by array\n                target_series = pandas.Series(\n                    data=self._parent._data[target_type_id][target_col_id],\n                    name=self._parent._columns[target_col_id_literal]\n                )\n                by_arr_data = self._parent._data[by_type_id][by_col_id]\n                return init_series_groupby(target_series, by_arr_data, self._data, self._sort)\n            else:\n                return init_dataframe_groupby(self._parent, by_col_id_literal, self._data, self._sort, idx)\n\n        return sdc_pandas_dataframe_getitem_common_impl\n\n    if isinstance(idx, types.UnicodeType):\n        def sdc_pandas_dataframe_getitem_idx_unicode_str_impl(self, idx):\n            # just call literally as it will raise and compilation will continue via common impl\n            return literally(idx)\n        return sdc_pandas_dataframe_getitem_idx_unicode_str_impl\n\n    return None\n\n\ndef _sdc_pandas_groupby_generic_func_codegen(func_name, columns, column_loc,\n                                             func_params, defaults, impl_params):\n    all_params_as_str = \', \'.join(sigparams2list(func_params, defaults))\n    extra_impl_params = \', \'.join(kwsparams2list(impl_params))\n\n    groupby_obj = f\'{func_params[0]}\'\n    df = f\'{groupby_obj}._parent\'\n    groupby_dict = f\'{groupby_obj}._data\'\n    groupby_param_sort = f\'{groupby_obj}._sort\'\n    column_names, column_ids = tuple(zip(*columns))\n\n    func_lines = [\n        f\'def _dataframe_groupby_{func_name}_impl({all_params_as_str}):\',\n        f\'  group_keys = _sdc_asarray([key for key in {groupby_dict}])\',\n        f\'  res_index_len = len(group_keys)\',\n        f\'  if {groupby_param_sort}:\',\n        f\'    argsorted_index = sdc_arrays_argsort(group_keys, kind=\\\'mergesort\\\')\',\n    ]\n\n    # TODO: remove conversion from Numba typed.List to reflected one while creating group_arr_{i}\n    for i in range(len(columns)):\n        col_loc = column_loc[column_names[i]]\n        type_id, col_id = col_loc.type_id, col_loc.col_id\n        func_lines += [\n            f\'  result_data_{i} = numpy.empty(res_index_len, dtype=res_arrays_dtypes[{i}])\',\n            f\'  column_data_{i} = {df}._data[{type_id}][{col_id}]\',\n            f\'  for j in numpy.arange(res_index_len):\',\n            f\'    idx = argsorted_index[j] if {groupby_param_sort} else j\',\n            f\'    group_arr_{i} = _sdc_take(column_data_{i}, list({groupby_dict}[group_keys[idx]]))\',\n            f\'    group_series_{i} = pandas.Series(group_arr_{i})\',\n            f\'    result_data_{i}[j] = group_series_{i}.{func_name}({extra_impl_params})\',\n        ]\n\n    data = \', \'.join(f\'\\\'{column_names[i]}\\\': result_data_{i}\' for i in range(len(columns)))\n    func_lines.extend([\'\\n\'.join([\n        f\'  if {groupby_param_sort}:\',\n        f\'    res_index = _sdc_take(group_keys, argsorted_index)\',\n        f\'  else:\',\n        f\'    res_index = group_keys\',\n        f\'  return pandas.DataFrame({{{data}}}, index=res_index)\'\n    ])])\n\n    func_text = \'\\n\'.join(func_lines)\n    global_vars = {\'pandas\': pandas,\n                   \'numpy\': numpy,\n                   \'_sdc_asarray\': _sdc_asarray,\n                   \'_sdc_take\': _sdc_take,\n                   \'sdc_arrays_argsort\': sdc_arrays_argsort}\n\n    return func_text, global_vars\n\n\ndef _sdc_pandas_series_groupby_generic_func_codegen(func_name, func_params, defaults, impl_params):\n\n    all_params_as_str = \', \'.join(sigparams2list(func_params, defaults))\n    extra_impl_params = \', \'.join(kwsparams2list(impl_params))\n\n    groupby_obj = f\'{func_params[0]}\'\n    series = f\'{groupby_obj}._parent\'\n    groupby_dict = f\'{groupby_obj}._data\'\n    groupby_param_sort = f\'{groupby_obj}._sort\'\n\n    # TODO: remove conversion from Numba typed.List to reflected one while creating group_arr_{i}\n    func_lines = [\n        f\'def _series_groupby_{func_name}_impl({all_params_as_str}):\',\n        f\'  group_keys = _sdc_asarray([key for key in {groupby_dict}])\',\n        f\'  res_index_len = len(group_keys)\',\n        f\'  if {groupby_param_sort}:\',\n        f\'    argsorted_index = sdc_arrays_argsort(group_keys, kind=\\\'mergesort\\\')\',\n        f\'  result_data = numpy.empty(res_index_len, dtype=res_dtype)\',\n        f\'  for j in numpy.arange(res_index_len):\',\n        f\'    idx = argsorted_index[j] if {groupby_param_sort} else j\',\n        f\'    group_arr = _sdc_take({series}._data, list({groupby_dict}[group_keys[idx]]))\',\n        f\'    group_series = pandas.Series(group_arr)\',\n        f\'    result_data[j] = group_series.{func_name}({extra_impl_params})\',\n        f\'  if {groupby_param_sort}:\',\n        f\'    res_index = _sdc_take(group_keys, argsorted_index)\',\n        f\'  else:\',\n        f\'    res_index = group_keys\',\n        f\'  return pandas.Series(data=result_data, index=res_index, name={series}._name)\'\n    ]\n\n    func_text = \'\\n\'.join(func_lines)\n    global_vars = {\'pandas\': pandas,\n                   \'numpy\': numpy,\n                   \'_sdc_asarray\': _sdc_asarray,\n                   \'_sdc_take\': _sdc_take,\n                   \'sdc_arrays_argsort\': sdc_arrays_argsort}\n\n    return func_text, global_vars\n\n\nseries_method_to_func = {\n    \'count\': lambda S: S.count(),\n    \'max\': lambda S: S.max(),\n    \'mean\': lambda S: S.mean(),\n    \'median\': lambda S: S.median(),\n    \'min\': lambda S: S.min(),\n    \'prod\': lambda S: S.prod(),\n    \'std\': lambda S: S.std(),\n    \'sum\': lambda S: S.sum(),\n    \'var\': lambda S: S.var()\n}\n\n\ndef _groupby_resolve_impl_func_type(series_dtype, method_name):\n    """""" Used for typing result value of functions implementing groupby methods,\n        assuming that these implementation call method of Series.\n    """"""\n    ty_series = SeriesType(series_dtype)\n    jitted_func = numba.njit(series_method_to_func[method_name])\n    return cpu_target.typing_context.resolve_function_type(jitted_func, (ty_series, ), {})\n\n\ndef sdc_pandas_dataframe_groupby_apply_func(self, func_name, func_args, defaults=None, impl_args=None):\n\n    defaults = defaults or {}\n    impl_args = impl_args or {}\n\n    df_column_types = self.parent.data\n    df_column_names = self.parent.columns\n    by_column_id = self.col_id.literal_value\n    selected_cols_set = set(self.target_columns)\n    subject_columns = [(name, i) for i, name in enumerate(df_column_names) if name in selected_cols_set]\n\n    # resolve types of result dataframe columns\n    res_arrays_dtypes = tuple(\n        _groupby_resolve_impl_func_type(\n            ty_arr.dtype, func_name\n            ).return_type for i, ty_arr in enumerate(df_column_types) if i != by_column_id)\n\n    groupby_func_name = f\'_dataframe_groupby_{func_name}_impl\'\n    func_text, global_vars = _sdc_pandas_groupby_generic_func_codegen(\n        func_name, subject_columns, self.parent.column_loc, func_args, defaults, impl_args)\n\n    # capture result column types into generated func context\n    global_vars[\'res_arrays_dtypes\'] = res_arrays_dtypes\n\n    loc_vars = {}\n    exec(func_text, global_vars, loc_vars)\n    _groupby_method_impl = loc_vars[groupby_func_name]\n\n    return _groupby_method_impl\n\n\ndef sdc_pandas_series_groupby_apply_func(self, func_name, func_args, defaults=None, impl_args=None):\n\n    defaults = defaults or {}\n    impl_args = impl_args or {}\n\n    # resolve type of result series\n    res_dtype = _groupby_resolve_impl_func_type(self.parent.dtype, func_name).return_type\n\n    groupby_func_name = f\'_series_groupby_{func_name}_impl\'\n    func_text, global_vars = _sdc_pandas_series_groupby_generic_func_codegen(\n        func_name, func_args, defaults, impl_args)\n\n    # capture result column types into generated func context\n    global_vars[\'res_dtype\'] = res_dtype\n\n    loc_vars = {}\n    exec(func_text, global_vars, loc_vars)\n    _groupby_method_impl = loc_vars[groupby_func_name]\n\n    return _groupby_method_impl\n\n\n@sdc_overload_method(DataFrameGroupByType, \'count\')\ndef sdc_pandas_dataframe_groupby_count(self):\n\n    method_name = \'GroupBy.count().\'\n    ty_checker = TypeChecker(method_name)\n    ty_checker.check(self, DataFrameGroupByType)\n\n    method_args = [\'self\']\n    applied_func_name = \'count\'\n    return sdc_pandas_dataframe_groupby_apply_func(self, applied_func_name, method_args)\n\n\n@sdc_overload_method(DataFrameGroupByType, \'max\')\ndef sdc_pandas_dataframe_groupby_max(self):\n\n    method_name = \'GroupBy.max().\'\n    ty_checker = TypeChecker(method_name)\n    ty_checker.check(self, DataFrameGroupByType)\n\n    method_args = [\'self\']\n    applied_func_name = \'max\'\n    return sdc_pandas_dataframe_groupby_apply_func(self, applied_func_name, method_args)\n\n\n@sdc_overload_method(DataFrameGroupByType, \'mean\')\ndef sdc_pandas_dataframe_groupby_mean(self, *args):\n\n    method_name = \'GroupBy.mean().\'\n    ty_checker = TypeChecker(method_name)\n    ty_checker.check(self, DataFrameGroupByType)\n\n    method_args = [\'self\', \'*args\']\n    applied_func_name = \'mean\'\n    return sdc_pandas_dataframe_groupby_apply_func(self, applied_func_name, method_args)\n\n\n@sdc_overload_method(DataFrameGroupByType, \'median\')\ndef sdc_pandas_dataframe_groupby_median(self):\n\n    method_name = \'GroupBy.median().\'\n    ty_checker = TypeChecker(method_name)\n    ty_checker.check(self, DataFrameGroupByType)\n\n    method_args = [\'self\']\n    applied_func_name = \'median\'\n    return sdc_pandas_dataframe_groupby_apply_func(self, applied_func_name, method_args)\n\n\n@sdc_overload_method(DataFrameGroupByType, \'min\')\ndef sdc_pandas_dataframe_groupby_min(self):\n\n    method_name = \'GroupBy.min().\'\n    ty_checker = TypeChecker(method_name)\n    ty_checker.check(self, DataFrameGroupByType)\n\n    method_args = [\'self\']\n    applied_func_name = \'min\'\n    return sdc_pandas_dataframe_groupby_apply_func(self, applied_func_name, method_args)\n\n\n@sdc_overload_method(DataFrameGroupByType, \'prod\')\ndef sdc_pandas_dataframe_groupby_prod(self):\n\n    method_name = \'GroupBy.prod().\'\n    ty_checker = TypeChecker(method_name)\n    ty_checker.check(self, DataFrameGroupByType)\n\n    method_args = [\'self\']\n    applied_func_name = \'prod\'\n    return sdc_pandas_dataframe_groupby_apply_func(self, applied_func_name, method_args)\n\n\n@sdc_overload_method(DataFrameGroupByType, \'std\')\ndef sdc_pandas_dataframe_groupby_std(self, ddof=1, *args):\n\n    method_name = \'GroupBy.std().\'\n    ty_checker = TypeChecker(method_name)\n    ty_checker.check(self, DataFrameGroupByType)\n\n    if not isinstance(ddof, (types.Omitted, int, types.Integer)):\n        ty_checker.raise_exc(ddof, \'int\', \'ddof\')\n\n    method_args = [\'self\', \'ddof\', \'*args\']\n    default_values = {\'ddof\': 1}\n    impl_used_params = {\'ddof\': \'ddof\'}\n\n    applied_func_name = \'std\'\n    return sdc_pandas_dataframe_groupby_apply_func(\n        self, applied_func_name, method_args, default_values, impl_used_params)\n\n\n@sdc_overload_method(DataFrameGroupByType, \'sum\')\ndef sdc_pandas_dataframe_groupby_sum(self):\n\n    method_name = \'GroupBy.sum().\'\n    ty_checker = TypeChecker(method_name)\n    ty_checker.check(self, DataFrameGroupByType)\n\n    method_args = [\'self\']\n    applied_func_name = \'sum\'\n    return sdc_pandas_dataframe_groupby_apply_func(self, applied_func_name, method_args)\n\n\n@sdc_overload_method(DataFrameGroupByType, \'var\')\ndef sdc_pandas_dataframe_groupby_var(self, ddof=1, *args):\n\n    method_name = \'GroupBy.var().\'\n    ty_checker = TypeChecker(method_name)\n    ty_checker.check(self, DataFrameGroupByType)\n\n    if not isinstance(ddof, (types.Omitted, int, types.Integer)):\n        ty_checker.raise_exc(ddof, \'int\', \'ddof\')\n\n    method_args = [\'self\', \'ddof\', \'*args\']\n    default_values = {\'ddof\': 1}\n    impl_used_params = {\'ddof\': \'ddof\'}\n\n    applied_func_name = \'var\'\n    return sdc_pandas_dataframe_groupby_apply_func(\n        self, applied_func_name, method_args, default_values, impl_used_params)\n\n\n@sdc_overload_method(SeriesGroupByType, \'count\')\ndef sdc_pandas_series_groupby_count(self):\n\n    method_name = \'GroupBy.count().\'\n    ty_checker = TypeChecker(method_name)\n    ty_checker.check(self, SeriesGroupByType)\n\n    method_args = [\'self\']\n    applied_func_name = \'count\'\n    return sdc_pandas_series_groupby_apply_func(self, applied_func_name, method_args)\n\n\n@sdc_overload_method(SeriesGroupByType, \'max\')\ndef sdc_pandas_series_groupby_max(self):\n\n    method_name = \'GroupBy.max().\'\n    ty_checker = TypeChecker(method_name)\n    ty_checker.check(self, SeriesGroupByType)\n\n    method_args = [\'self\']\n    applied_func_name = \'max\'\n    return sdc_pandas_series_groupby_apply_func(self, applied_func_name, method_args)\n\n\n@sdc_overload_method(SeriesGroupByType, \'mean\')\ndef sdc_pandas_series_groupby_mean(self, *args):\n\n    method_name = \'GroupBy.mean().\'\n    ty_checker = TypeChecker(method_name)\n    ty_checker.check(self, SeriesGroupByType)\n\n    method_args = [\'self\', \'*args\']\n    applied_func_name = \'mean\'\n    return sdc_pandas_series_groupby_apply_func(self, applied_func_name, method_args)\n\n\n@sdc_overload_method(SeriesGroupByType, \'median\')\ndef sdc_pandas_series_groupby_median(self):\n\n    method_name = \'GroupBy.median().\'\n    ty_checker = TypeChecker(method_name)\n    ty_checker.check(self, SeriesGroupByType)\n\n    method_args = [\'self\']\n    applied_func_name = \'median\'\n    return sdc_pandas_series_groupby_apply_func(self, applied_func_name, method_args)\n\n\n@sdc_overload_method(SeriesGroupByType, \'min\')\ndef sdc_pandas_series_groupby_min(self):\n\n    method_name = \'GroupBy.min().\'\n    ty_checker = TypeChecker(method_name)\n    ty_checker.check(self, SeriesGroupByType)\n\n    method_args = [\'self\']\n    applied_func_name = \'min\'\n    return sdc_pandas_series_groupby_apply_func(self, applied_func_name, method_args)\n\n\n@sdc_overload_method(SeriesGroupByType, \'prod\')\ndef sdc_pandas_series_groupby_prod(self):\n\n    method_name = \'GroupBy.prod().\'\n    ty_checker = TypeChecker(method_name)\n    ty_checker.check(self, SeriesGroupByType)\n\n    method_args = [\'self\']\n    applied_func_name = \'prod\'\n    return sdc_pandas_series_groupby_apply_func(self, applied_func_name, method_args)\n\n\n@sdc_overload_method(SeriesGroupByType, \'std\')\ndef sdc_pandas_series_groupby_std(self, ddof=1, *args):\n\n    method_name = \'GroupBy.std().\'\n    ty_checker = TypeChecker(method_name)\n    ty_checker.check(self, SeriesGroupByType)\n\n    if not isinstance(ddof, (types.Omitted, int, types.Integer)):\n        ty_checker.raise_exc(ddof, \'int\', \'ddof\')\n\n    method_args = [\'self\', \'ddof\', \'*args\']\n    default_values = {\'ddof\': 1}\n    impl_used_params = {\'ddof\': \'ddof\'}\n\n    applied_func_name = \'std\'\n    return sdc_pandas_series_groupby_apply_func(self, applied_func_name, method_args, default_values, impl_used_params)\n\n\n@sdc_overload_method(SeriesGroupByType, \'sum\')\ndef sdc_pandas_series_groupby_sum(self):\n\n    method_name = \'GroupBy.sum().\'\n    ty_checker = TypeChecker(method_name)\n    ty_checker.check(self, SeriesGroupByType)\n\n    method_args = [\'self\']\n    applied_func_name = \'sum\'\n    return sdc_pandas_series_groupby_apply_func(self, applied_func_name, method_args)\n\n\n@sdc_overload_method(SeriesGroupByType, \'var\')\ndef sdc_pandas_series_groupby_var(self, ddof=1, *args):\n\n    method_name = \'GroupBy.var().\'\n    ty_checker = TypeChecker(method_name)\n    ty_checker.check(self, SeriesGroupByType)\n\n    if not isinstance(ddof, (types.Omitted, int, types.Integer)):\n        ty_checker.raise_exc(ddof, \'int\', \'ddof\')\n\n    method_args = [\'self\', \'ddof\', \'*args\']\n    default_values = {\'ddof\': 1}\n    impl_used_params = {\'ddof\': \'ddof\'}\n\n    applied_func_name = \'var\'\n    return sdc_pandas_series_groupby_apply_func(self, applied_func_name, method_args, default_values, impl_used_params)\n\n\nsdc_pandas_dataframe_groupby_docstring_tmpl = """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n    Pandas API: pandas.core.groupby.GroupBy.{method_name}\n{limitations_block}\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/dataframe/groupby/dataframe_groupby_{method_name}.py\n       :language: python\n       :lines: 27-\n       :caption: {example_caption}\n       :name: ex_dataframe_groupby_{method_name}\n\n    .. command-output:: python ./dataframe/groupby/dataframe_groupby_{method_name}.py\n       :cwd: ../../../examples\n{see_also}\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n\n    Pandas DataFrame method :meth:`pandas.DataFrame.groupby.{method_name}()` implementation.\n\n    .. only:: developer\n\n    Test: python -m sdc.runtests -k sdc.tests.test_groupby.TestGroupBy.test_dataframe_groupby_{method_name}*\n\n    Parameters\n    ----------\n    self: :class:`pandas.DataFrame.groupby`\n        input arg{extra_params}\n\n    Returns\n    -------\n    :obj:`pandas.DataFrame`\n         returns :obj:`pandas.DataFrame` object\n""""""\n\n\nsdc_pandas_dataframe_groupby_count.__doc__ = sdc_pandas_dataframe_groupby_docstring_tmpl.format(**{\n    \'method_name\': \'count\',\n    \'example_caption\': \'Compute count of group, excluding missing values.\',\n    \'limitations_block\':\n        f""""""\n        Limitations\n        -----------\n        - {performance_limitation}\n        """""",\n    \'see_also\':\n    """"""\n    .. seealso::\n        :ref:`Series.groupby <pandas.Series.groupby>`\n            Group Series using a mapper or by a Series of columns.\n        :ref:`DataFrame.groupby <pandas.DataFrame.groupby>`\n            Group DataFrame using a mapper or by a Series of columns.\n    """""",\n    \'extra_params\': \'\'\n})\n\n\nsdc_pandas_dataframe_groupby_max.__doc__ = sdc_pandas_dataframe_groupby_docstring_tmpl.format(**{\n    \'method_name\': \'max\',\n    \'example_caption\': \'Compute max of group values.\',\n    \'limitations_block\':\n        f""""""\n        Limitations\n        -----------\n        - {performance_limitation}\n        """""",\n    \'see_also\': \'\',\n    \'extra_params\': \'\'\n})\n\n\nsdc_pandas_dataframe_groupby_mean.__doc__ = sdc_pandas_dataframe_groupby_docstring_tmpl.format(**{\n    \'method_name\': \'mean\',\n    \'example_caption\': \'Compute mean of groups, excluding missing values.\',\n    \'limitations_block\':\n        f""""""\n        Limitations\n        -----------\n        - {performance_limitation}\n        """""",\n    \'see_also\':\n    """"""\n    .. seealso::\n        :ref:`Series.groupby <pandas.Series.groupby>`\n            Group Series using a mapper or by a Series of columns.\n        :ref:`DataFrame.groupby <pandas.DataFrame.groupby>`\n            Group DataFrame using a mapper or by a Series of columns.\n    """""",\n    \'extra_params\': \'\'\n})\n\n\nsdc_pandas_dataframe_groupby_median.__doc__ = sdc_pandas_dataframe_groupby_docstring_tmpl.format(**{\n    \'method_name\': \'median\',\n    \'example_caption\': \'Compute median of groups, excluding missing values.\',\n    \'limitations_block\':\n        f""""""\n        Limitations\n        -----------\n        - {performance_limitation}\n        """""",\n    \'see_also\':\n    """"""\n    .. seealso::\n        :ref:`Series.groupby <pandas.Series.groupby>`\n            Group Series using a mapper or by a Series of columns.\n        :ref:`DataFrame.groupby <pandas.DataFrame.groupby>`\n            Group DataFrame using a mapper or by a Series of columns.\n    """""",\n    \'extra_params\': \'\'\n})\n\n\nsdc_pandas_dataframe_groupby_min.__doc__ = sdc_pandas_dataframe_groupby_docstring_tmpl.format(**{\n    \'method_name\': \'min\',\n    \'example_caption\': \'Compute min of group values.\',\n    \'limitations_block\':\n        f""""""\n        Limitations\n        -----------\n        - {performance_limitation}\n        """""",\n    \'see_also\': \'\',\n    \'extra_params\': \'\'\n})\n\n\nsdc_pandas_dataframe_groupby_prod.__doc__ = sdc_pandas_dataframe_groupby_docstring_tmpl.format(**{\n    \'method_name\': \'prod\',\n    \'example_caption\': \'Compute prod of group values.\',\n    \'limitations_block\':\n        f""""""\n        Limitations\n        -----------\n        - {performance_limitation}\n        """""",\n    \'see_also\': \'\',\n    \'extra_params\': \'\'\n})\n\n\nsdc_pandas_dataframe_groupby_std.__doc__ = sdc_pandas_dataframe_groupby_docstring_tmpl.format(**{\n    \'method_name\': \'std\',\n    \'example_caption\': \'Compute standard deviation of groups, excluding missing values.\',\n    \'limitations_block\':\n        f""""""\n        Limitations\n        -----------\n        - {performance_limitation}\n        """""",\n    \'see_also\':\n    """"""\n    .. seealso::\n        :ref:`Series.groupby <pandas.Series.groupby>`\n            Group Series using a mapper or by a Series of columns.\n        :ref:`DataFrame.groupby <pandas.DataFrame.groupby>`\n            Group DataFrame using a mapper or by a Series of columns.\n    """""",\n    \'extra_params\': \'\'\n})\n\n\nsdc_pandas_dataframe_groupby_sum.__doc__ = sdc_pandas_dataframe_groupby_docstring_tmpl.format(**{\n    \'method_name\': \'sum\',\n    \'example_caption\': \'Compute sum of groups, excluding missing values.\',\n    \'limitations_block\':\n        f""""""\n        Limitations\n        -----------\n        - {performance_limitation}\n        """""",\n    \'see_also\': \'\',\n    \'extra_params\': \'\'\n})\n\n\nsdc_pandas_dataframe_groupby_var.__doc__ = sdc_pandas_dataframe_groupby_docstring_tmpl.format(**{\n    \'method_name\': \'var\',\n    \'example_caption\': \'Compute variance of groups, excluding missing values.\',\n    \'limitations_block\':\n        f""""""\n        Limitations\n        -----------\n        - {performance_limitation}\n        """""",\n    \'see_also\':\n    """"""\n    .. seealso::\n        :ref:`Series.groupby <pandas.Series.groupby>`\n            Group Series using a mapper or by a Series of columns.\n        :ref:`DataFrame.groupby <pandas.DataFrame.groupby>`\n            Group DataFrame using a mapper or by a Series of columns.\n    """""",\n    \'extra_params\': \'\'\n})\n'"
sdc/datatypes/hpat_pandas_groupby_types.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\nimport numba\nfrom numba import types\nfrom numba.extending import (models, register_model, make_attribute_wrapper)\nfrom sdc.str_ext import string_type\n\n\nclass DataFrameGroupByType(types.Type):\n    """"""\n    Type definition for DataFrameGroupBy functions handling.\n    """"""\n\n    def __init__(self, parent, col_id, target_columns):\n        self.parent = parent\n        self.col_id = col_id\n        self.target_columns = target_columns\n        super(DataFrameGroupByType, self).__init__(\n            name=""DataFrameGroupByType({}, {})"".format(parent, col_id, target_columns))\n\n    @property\n    def key(self):\n        return self.parent, self.col_id, self.target_columns\n\n\n@register_model(DataFrameGroupByType)\nclass DataFrameGroupByModel(models.StructModel):\n    def __init__(self, dmm, fe_type):\n        by_series_dtype = fe_type.parent.data[fe_type.col_id.literal_value].dtype\n        ty_data = types.containers.DictType(\n            by_series_dtype,\n            types.containers.ListType(types.int64)\n        )\n\n        n_target_cols = len(fe_type.target_columns)\n        members = [\n            (\'parent\', fe_type.parent),\n            (\'col_id\', types.int64),\n            (\'data\', ty_data),\n            (\'sort\', types.bool_),\n            (\'target_default\', types.bool_),\n            (\'target_columns\', types.UniTuple(string_type, n_target_cols))\n        ]\n        super(DataFrameGroupByModel, self).__init__(dmm, fe_type, members)\n\n\nmake_attribute_wrapper(DataFrameGroupByType, \'parent\', \'_parent\')\nmake_attribute_wrapper(DataFrameGroupByType, \'col_id\', \'_col_id\')\nmake_attribute_wrapper(DataFrameGroupByType, \'data\', \'_data\')\nmake_attribute_wrapper(DataFrameGroupByType, \'sort\', \'_sort\')\nmake_attribute_wrapper(DataFrameGroupByType, \'target_default\', \'_target_default\')\nmake_attribute_wrapper(DataFrameGroupByType, \'target_columns\', \'_target_columns\')\n\n\nclass SeriesGroupByType(types.Type):\n    """"""\n    Type definition for SeriesGroupBy functions handling.\n    """"""\n\n    def __init__(self, parent, by_data):\n        self.parent = parent\n        self.by_data = by_data\n        super(SeriesGroupByType, self).__init__(\n            name=""SeriesGroupByType({}, {})"".format(parent, by_data))\n\n    @property\n    def key(self):\n        return self.parent, self.by_data\n\n\n@register_model(SeriesGroupByType)\nclass SeriesGroupByModel(models.StructModel):\n    def __init__(self, dmm, fe_type):\n        by_dtype = fe_type.by_data.dtype\n        ty_data = types.containers.DictType(\n            by_dtype,\n            types.containers.ListType(types.int64)\n        )\n\n        members = [\n            (\'parent\', fe_type.parent),\n            (\'data\', ty_data),\n            (\'sort\', types.bool_)\n        ]\n        super(SeriesGroupByModel, self).__init__(dmm, fe_type, members)\n\n\nmake_attribute_wrapper(SeriesGroupByType, \'parent\', \'_parent\')\nmake_attribute_wrapper(SeriesGroupByType, \'data\', \'_data\')\nmake_attribute_wrapper(SeriesGroupByType, \'sort\', \'_sort\')\n'"
sdc/datatypes/hpat_pandas_rolling_types.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nfrom numba.core import cgutils, types\nfrom numba.core.datamodel import StructModel\nfrom numba.extending import make_attribute_wrapper, models\nfrom numba.core.typing.templates import signature\nfrom sdc.utilities.sdc_typing_utils import TypeChecker\n\n\nclass RollingType(types.Type):\n    """"""Type definition for pandas.rolling functions handling.""""""\n    def __init__(self, ty, data, win_type=None, on=None, closed=None):\n        self.data = data\n        self.win_type = win_type or types.none\n        self.on = on or types.none\n        self.closed = closed or types.none\n\n        name_tmpl = \'{}({}, win_type={}, on={}, closed={})\'\n        name = name_tmpl.format(ty, data, self.win_type, self.on, self.closed)\n        super(RollingType, self).__init__(name)\n\n\nclass RollingTypeModel(StructModel):\n    """"""Model for RollingType type.""""""\n    def __init__(self, dmm, fe_type):\n        members = [\n            (\'data\', fe_type.data),\n            # window is able to be offset\n            (\'window\', types.intp),\n            (\'min_periods\', types.intp),\n            (\'center\', types.boolean),\n            (\'win_type\', fe_type.win_type),\n            (\'on\', fe_type.on),\n            # axis is able to be unicode type\n            (\'axis\', types.intp),\n            (\'closed\', fe_type.closed),\n        ]\n        models.StructModel.__init__(self, dmm, fe_type, members)\n\n\nmake_attribute_wrapper(RollingType, \'data\', \'_data\')\nmake_attribute_wrapper(RollingType, \'window\', \'_window\')\nmake_attribute_wrapper(RollingType, \'min_periods\', \'_min_periods\')\nmake_attribute_wrapper(RollingType, \'center\', \'_center\')\nmake_attribute_wrapper(RollingType, \'win_type\', \'_win_type\')\nmake_attribute_wrapper(RollingType, \'on\', \'_on\')\nmake_attribute_wrapper(RollingType, \'axis\', \'_axis\')\nmake_attribute_wrapper(RollingType, \'closed\', \'_closed\')\n\n\ndef gen_hpat_pandas_rolling_init(ty):\n    """"""Generate rolling initializer based on data type""""""\n    def _hpat_pandas_rolling_init(typingctx, self, window, min_periods=None,\n                                  center=False, win_type=None,\n                                  on=None, axis=0, closed=None):\n        """"""Internal Numba required function to register RollingType.""""""\n        ret_typ = ty(self, win_type, on, closed)\n        sig = signature(ret_typ, self, window, min_periods,\n                        center, win_type, on, axis, closed)\n\n        def _codegen(context, builder, sig, args):\n            """"""Create DataFrameRollingTypeModel structure.""""""\n            data, window, min_periods, center, win_type, on, axis, closed = args\n            rolling = cgutils.create_struct_proxy(sig.return_type)(context, builder)\n            rolling.data = data\n            rolling.window = window\n            rolling.min_periods = min_periods\n            rolling.center = center\n            rolling.win_type = win_type\n            rolling.on = on\n            rolling.axis = axis\n            rolling.closed = closed\n\n            if context.enable_nrt:\n                context.nrt.incref(builder, self, rolling.data)\n\n            return rolling._getvalue()\n\n        return sig, _codegen\n\n    return _hpat_pandas_rolling_init\n\n\ndef gen_sdc_pandas_rolling_overload_body(initializer, ty):\n    """"""Generate code of the overloaded method using associated DataType and constructor.""""""\n    def sdc_pandas_rolling(self, window, min_periods=None, center=False,\n                           win_type=None, on=None, axis=0, closed=None):\n        ty_checker = TypeChecker(\'Method rolling().\')\n        ty_checker.check(self, ty)\n\n        if not isinstance(window, types.Integer):\n            ty_checker.raise_exc(window, \'int\', \'window\')\n\n        minp_accepted = (types.Omitted, types.NoneType, types.Integer)\n        if not isinstance(min_periods, minp_accepted) and min_periods is not None:\n            ty_checker.raise_exc(min_periods, \'None, int\', \'min_periods\')\n\n        center_accepted = (types.Omitted, types.Boolean)\n        if not isinstance(center, center_accepted) and center is not False:\n            ty_checker.raise_exc(center, \'bool\', \'center\')\n\n        str_types = (types.Omitted, types.NoneType, types.StringLiteral, types.UnicodeType)\n        if not isinstance(win_type, str_types) and win_type is not None:\n            ty_checker.raise_exc(win_type, \'str\', \'win_type\')\n\n        if not isinstance(on, str_types) and on is not None:\n            ty_checker.raise_exc(on, \'str\', \'on\')\n\n        axis_accepted = (types.Omitted, types.Integer, types.StringLiteral, types.UnicodeType)\n        if not isinstance(axis, axis_accepted) and axis != 0:\n            ty_checker.raise_exc(axis, \'int, str\', \'axis\')\n\n        if not isinstance(closed, str_types) and closed is not None:\n            ty_checker.raise_exc(closed, \'str\', \'closed\')\n\n        nan_minp = isinstance(min_periods, (types.Omitted, types.NoneType)) or min_periods is None\n\n        def sdc_pandas_rolling_impl(self, window, min_periods=None, center=False,\n                                    win_type=None, on=None, axis=0, closed=None):\n            if window < 0:\n                raise ValueError(\'window must be non-negative\')\n\n            if nan_minp == True:  # noqa\n                minp = window\n            else:\n                minp = min_periods\n\n            if minp < 0:\n                raise ValueError(\'min_periods must be >= 0\')\n            if minp > window:\n                raise ValueError(\'min_periods must be <= window\')\n\n            if center != False:  # noqa\n                raise ValueError(\'Method rolling(). The object center\\n expected: False\')\n\n            if win_type is not None:\n                raise ValueError(\'Method rolling(). The object win_type\\n expected: None\')\n\n            if on is not None:\n                raise ValueError(\'Method rolling(). The object on\\n expected: None\')\n\n            if axis != 0:\n                raise ValueError(\'Method rolling(). The object axis\\n expected: 0\')\n\n            if closed is not None:\n                raise ValueError(\'Method rolling(). The object closed\\n expected: None\')\n\n            return initializer(self, window, minp, center, win_type, on, axis, closed)\n\n        return sdc_pandas_rolling_impl\n\n    return sdc_pandas_rolling\n\n\nsdc_pandas_rolling_docstring_tmpl = """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n    Pandas API: pandas.{ty}.rolling\n\n    Limitations\n    -----------\n    Parameters ``center``, ``win_type``, ``on``, ``axis`` and ``closed`` are supported only with default values.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/{ty_lower}/rolling/{ty_lower}_rolling_min.py\n       :language: python\n       :lines: 27-\n       :caption: Calculate the rolling minimum.\n       :name: ex_{ty_lower}_rolling\n\n    .. command-output:: python ./{ty_lower}/rolling/{ty_lower}_rolling_min.py\n       :cwd: ../../../examples\n\n    .. todo:: Add support of parameters ``center``, ``win_type``, ``on``, ``axis`` and ``closed``\n\n    .. seealso::\n        :ref:`expanding <pandas.{ty}.expanding>`\n            Provides expanding transformations.\n        :ref:`ewm <pandas.{ty}.ewm>`\n            Provides exponential weighted functions.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n\n    Pandas {ty} attribute :attr:`pandas.{ty}.rolling` implementation\n    .. only:: developer\n\n    Test: python -m sdc.runtests -k sdc.tests.test_rolling.TestRolling.test_{ty_lower}_rolling\n\n    Parameters\n    ----------\n    self: :obj:`pandas.{ty}`\n        Input {ty}.\n    window: :obj:`int` or :obj:`offset`\n        Size of the moving window.\n    min_periods: :obj:`int`\n        Minimum number of observations in window required to have a value.\n    center: :obj:`bool`\n        Set the labels at the center of the window.\n        *unsupported*\n    win_type: :obj:`str`\n        Provide a window type.\n        *unsupported*\n    on: :obj:`str`\n        Column on which to calculate the rolling window.\n        *unsupported*\n    axis: :obj:`int`, :obj:`str`\n        Axis along which the operation acts\n        0/None/\'index\' - row-wise operation\n        1/\'columns\'    - column-wise operation\n        *unsupported*\n    closed: :obj:`str`\n        Make the interval closed on the \xe2\x80\x98right\xe2\x80\x99, \xe2\x80\x98left\xe2\x80\x99, \xe2\x80\x98both\xe2\x80\x99 or \xe2\x80\x98neither\xe2\x80\x99 endpoints.\n        *unsupported*\n\n    Returns\n    -------\n    :class:`pandas.{ty}.rolling`\n        Output class to manipulate with input data.\n""""""\n'"
sdc/datatypes/hpat_pandas_series_functions.py,8,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\n| :class:`pandas.Series` functions and operators implementations in SDC\n| Also, it contains Numba internal operators which are required for Series type handling\n""""""\n\nimport numba\nimport numpy\nimport operator\nimport pandas\nimport math\nimport sys\n\nfrom numba.core.errors import TypingError\nfrom numba.core.typing import signature\nfrom numba.extending import intrinsic\nfrom numba import types\nfrom numba.core import cgutils\nfrom numba.np import numpy_support\nfrom numba.typed import List, Dict\nfrom numba import prange\nfrom numba.np.arraymath import get_isnan\nfrom pandas.core.indexing import IndexingError\n\nimport sdc\nimport sdc.datatypes.common_functions as common_functions\nfrom sdc.utilities.sdc_typing_utils import (TypeChecker, check_index_is_numeric, check_types_comparable,\n                                            find_common_dtype_from_numpy_dtypes, has_literal_value,\n                                            has_python_value)\nfrom sdc.datatypes.common_functions import (sdc_join_series_indexes, sdc_arrays_argsort, sdc_check_indexes_equal,\n                                            sdc_reindex_series)\nfrom sdc.datatypes.hpat_pandas_rolling_types import (\n    gen_sdc_pandas_rolling_overload_body, sdc_pandas_rolling_docstring_tmpl)\nfrom sdc.datatypes.hpat_pandas_series_rolling_types import _hpat_pandas_series_rolling_init\nfrom sdc.datatypes.hpat_pandas_stringmethods_types import StringMethodsType\nfrom sdc.datatypes.hpat_pandas_getitem_types import SeriesGetitemAccessorType\nfrom sdc.hiframes.pd_series_type import SeriesType\nfrom sdc.str_arr_type import (StringArrayType, string_array_type)\nfrom sdc.str_arr_ext import (str_arr_is_na, str_arr_set_na, num_total_chars,\n                             pre_alloc_string_array, cp_str_list_to_array,\n                             create_str_arr_from_list, str_arr_set_na_by_mask,\n                             str_list_to_array)\nfrom sdc.utilities.utils import to_array, sdc_overload, sdc_overload_method, sdc_overload_attribute\nfrom sdc import sdc_autogenerated\nfrom sdc.functions import numpy_like\nfrom sdc.hiframes.api import isna\nfrom sdc.datatypes.hpat_pandas_groupby_functions import init_series_groupby\nfrom sdc.utilities.prange_utils import parallel_chunks\n\nfrom .pandas_series_functions import apply\nfrom .pandas_series_functions import map as _map\n\n\n@sdc_overload(operator.getitem)\ndef hpat_pandas_series_accessor_getitem(self, idx):\n    """"""\n    Pandas Series operator :attr:`pandas.Series.__getitem__` implementation\n    for methods `pandas.Series.iloc`, `pandas.Series.loc`, `pandas.Series.iat`, `pandas.Series.at`\n\n    Parameters\n    ----------\n    self: :class:`pandas.Series`\n        input Series\n    other: :obj:`pandas.Series`, :obj:`int` or :obj:`float`, :obj:`slice`, :obj:`list`\n        input arg\n\n    Returns\n    -------\n    :class:`pandas.Series` or a value of :obj:`pandas.Series.dtype`\n        returns object\n    """"""\n\n    _func_name = \'Operator getitem().\'\n\n    if not isinstance(self, SeriesGetitemAccessorType):\n        return None\n\n    accessor = self.accessor.literal_value\n\n    if accessor == \'iloc\':\n        if isinstance(idx, (types.List, types.Array, types.SliceType)):\n            def hpat_pandas_series_iloc_list_slice_impl(self, idx):\n                result_data = self._series._data[idx]\n                result_index = self._series.index[idx]\n                return pandas.Series(data=result_data, index=result_index, name=self._series._name)\n\n            return hpat_pandas_series_iloc_list_slice_impl\n\n        if isinstance(idx, (int, types.Integer)):\n            def hpat_pandas_series_iloc_impl(self, idx):\n                return self._series._data[idx]\n\n            return hpat_pandas_series_iloc_impl\n\n        def hpat_pandas_series_iloc_callable_impl(self, idx):\n            index = numpy.asarray(list(map(idx, self._series._data)))\n            return pandas.Series(\n                data=self._series._data[index],\n                index=self._series.index[index],\n                name=self._series._name\n            )\n\n        return hpat_pandas_series_iloc_callable_impl\n\n        raise TypingError(\'{} The index must be an Integer, Slice or List of Integer or a callable.\\\n                    Given: {}\'.format(_func_name, idx))\n\n    if accessor == \'iat\':\n        if isinstance(idx, (int, types.Integer)):\n            def hpat_pandas_series_iat_impl(self, idx):\n                return self._series._data[idx]\n\n            return hpat_pandas_series_iat_impl\n\n        raise TypingError(\'{} The index must be a Integer. Given: {}\'.format(_func_name, idx))\n\n    if accessor == \'loc\':\n        # Note: Loc return Series\n        # Note: Loc slice and callable with String is not implemented\n        # Note: Loc slice without start is not supported\n        min_int64 = numpy.iinfo(\'int64\').min\n        max_int64 = numpy.iinfo(\'int64\').max\n        index_is_none = (self.series.index is None or\n                         isinstance(self.series.index, numba.types.misc.NoneType))\n        if isinstance(idx, types.SliceType) and not index_is_none:\n            def hpat_pandas_series_loc_slice_impl(self, idx):\n                series = self._series\n                index = series.index\n                start_position = len(index)\n                stop_position = 0\n                max_diff = 0\n                min_diff = 0\n                start_position_inc = len(index)\n                start_position_dec = len(index)\n                stop_position_inc = 0\n                stop_position_dec = 0\n                idx_start = idx.start\n                idx_stop = idx.stop\n                for i in numba.prange(len(index)):\n                    if index[i] >= idx_start:\n                        start_position_inc = min(start_position_inc, i)\n                    if index[i] <= idx_start:\n                        start_position_dec = min(start_position_dec, i)\n                    if index[i] <= idx_stop:\n                        stop_position_inc = max(stop_position_inc, i)\n                    if index[i] >= idx_stop:\n                        stop_position_dec = max(stop_position_dec, i)\n                    if i > 0:\n                        max_diff = max(max_diff, index[i] - index[i - 1])\n                        min_diff = min(min_diff, index[i] - index[i - 1])\n\n                if max_diff*min_diff < 0:\n                    raise ValueError(""Index must be monotonic increasing or decreasing"")\n\n                if max_diff > 0:\n                    start_position = start_position_inc\n                    stop_position = stop_position_inc\n                    if idx_stop < index[0]:\n                        return pandas.Series(data=series._data[:0], index=series._index[:0], name=series._name)\n                else:\n                    start_position = start_position_dec\n                    stop_position = stop_position_dec if idx.stop != max_int64 else len(index)\n                    if idx_stop > index[0] and idx_stop != max_int64:\n                        return pandas.Series(data=series._data[:0], index=series._index[:0], name=series._name)\n\n                stop_position = min(stop_position + 1, len(index))\n\n                if (\n                    start_position >= len(index) or stop_position <= 0 or stop_position <= start_position\n                ):\n                    return pandas.Series(data=series._data[:0], index=series._index[:0], name=series._name)\n\n                return pandas.Series(data=series._data[start_position:stop_position],\n                                     index=index[start_position:stop_position],\n                                     name=series._name)\n\n            return hpat_pandas_series_loc_slice_impl\n\n        if isinstance(idx, types.SliceType) and index_is_none:\n            def hpat_pandas_series_loc_slice_noidx_impl(self, idx):\n                max_slice = sys.maxsize\n                start = idx.start\n                stop = idx.stop\n                if idx.stop == max_slice:\n                    stop = max_slice - 1\n                result_data = self._series._data[start:stop+1]\n                result_index = numpy.arange(start, stop + 1)\n                return pandas.Series(data=result_data, index=result_index, name=self._series._name)\n\n            return hpat_pandas_series_loc_slice_noidx_impl\n\n        if isinstance(idx, (types.Array, types.List)):\n            def hpat_pandas_series_loc_array_impl(self, idx):\n                index = self._series.index\n                data = self._series._data\n                size = len(index)\n                data_res = []\n                index_res = []\n                for value in idx:\n                    mask = numpy.zeros(shape=size, dtype=numpy.bool_)\n                    for i in numba.prange(size):\n                        mask[i] = index[i] == value\n                    data_res.extend(data[mask])\n                    index_res.extend(index[mask])\n\n                return pandas.Series(data=data_res, index=index_res, name=self._series._name)\n\n            return hpat_pandas_series_loc_array_impl\n\n        if isinstance(idx, (int, types.Integer, types.UnicodeType, types.StringLiteral)):\n            def hpat_pandas_series_loc_impl(self, idx):\n                index = self._series.index\n                mask = numpy.empty(len(self._series._data), numpy.bool_)\n                for i in numba.prange(len(index)):\n                    mask[i] = index[i] == idx\n                return pandas.Series(data=self._series._data[mask], index=index[mask], name=self._series._name)\n\n            return hpat_pandas_series_loc_impl\n\n        raise TypingError(\'{} The index must be an Number, Slice, String, List, Array or a callable.\\\n                          Given: {}\'.format(_func_name, idx))\n\n    if accessor == \'at\':\n        if isinstance(idx, (int, types.Integer, types.UnicodeType, types.StringLiteral)):\n            def hpat_pandas_series_at_impl(self, idx):\n                index = self._series.index\n                count = 0\n                mask = numpy.empty(len(self._series._data), numpy.bool_)\n                for i in numba.prange(len(index)):\n                    mask[i] = index[i] == idx\n                    if mask[i] == True:  # noqa\n                        count += 1\n                if count == 0:  # noqa\n                    raise ValueError(""Index is not in the Series"")\n                return self._series._data[mask]\n\n            return hpat_pandas_series_at_impl\n\n        raise TypingError(\'{} The index must be a Number or String. Given: {}\'.format(_func_name, idx))\n\n    raise TypingError(\'{} Unknown accessor. Only ""loc"", ""iloc"", ""at"", ""iat"" are supported.\\\n                      Given: {}\'.format(_func_name, accessor))\n\n\n@sdc_overload(operator.getitem)\ndef hpat_pandas_series_getitem(self, idx):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n    Pandas API: pandas.Series.getitem\n\n    Get value(s) of Series by key.\n\n    Limitations\n    -----------\n    Supported ``key`` can be one of the following:\n        - Integer scalar, e.g. :obj:`series[0]`\n        - An array or a list, e.g. :obj:`series[0,2,5]`\n        - A list of booleans, e.g. :obj:`series[True,False]`\n        - A slice, e.g. :obj:`series[2:5]`\n        - Another series\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_getitem/series_getitem_scalar_single_result.py\n       :language: python\n       :lines: 32-\n       :caption: Getting Pandas Series elements. Returns single value.\n       :name: ex_series_getitem\n\n    .. command-output:: python ./series/series_getitem/series_getitem_scalar_single_result.py\n       :cwd: ../../../examples\n\n    .. literalinclude:: ../../../examples/series/series_getitem/series_getitem_scalar_multiple_result.py\n       :language: python\n       :lines: 34-\n       :caption: Getting Pandas Series elements. Returns multiple value.\n       :name: ex_series_getitem\n\n    .. command-output:: python ./series/series_getitem/series_getitem_scalar_multiple_result.py\n       :cwd: ../../../examples\n\n    .. literalinclude:: ../../../examples/series/series_getitem/series_getitem_slice.py\n       :language: python\n       :lines: 35-\n       :caption: Getting Pandas Series elements by slice.\n       :name: ex_series_getitem\n\n    .. command-output:: python ./series/series_getitem/series_getitem_slice.py\n       :cwd: ../../../examples\n\n    .. literalinclude:: ../../../examples/series/series_getitem/series_getitem_bool_array.py\n       :language: python\n       :lines: 37-\n       :caption: Getting Pandas Series elements by array of booleans.\n       :name: ex_series_getitem\n\n    .. command-output:: python ./series/series_getitem/series_getitem_bool_array.py\n       :cwd: ../../../examples\n\n    .. literalinclude:: ../../../examples/series/series_getitem/series_getitem_series.py\n       :language: python\n       :lines: 36-\n       :caption: Getting Pandas Series elements by another Series.\n       :name: ex_series_getitem\n\n    .. command-output:: python ./series/series_getitem/series_getitem_series.py\n       :cwd: ../../../examples\n\n    .. seealso::\n        :ref:`Series.setitem <pandas.Series.setitem>`\n            Set value to Series by index\n        :ref:`Series.loc <pandas.Series.loc>`\n            Access a group of rows and columns by label(s) or a boolean array.\n        :ref:`Series.iloc <pandas.Series.iloc>`\n            Purely integer-location based indexing for selection by position.\n        :ref:`Series.at <pandas.Series.at>`\n            Access a single value for a row/column label pair.\n        :ref:`Series.iat <pandas.Series.iat>`\n            Access a single value for a row/column pair by integer position.\n        :ref:`DataFrame.getitem <pandas.DataFrame.getitem>`\n            Get data from a DataFrame by indexer.\n        :ref:`DataFrame.setitem <pandas.DataFrame.setitem>`\n            Set value to DataFrame by index\n        :ref:`DataFrame.loc <pandas.DataFrame.loc>`\n            Access a group of rows and columns by label(s) or a boolean array.\n        :ref:`DataFrame.iloc <pandas.DataFrame.iloc>`\n            Purely integer-location based indexing for selection by position.\n        :ref:`DataFrame.at <pandas.DataFrame.at>`\n            Access a single value for a row/column label pair.\n        :ref:`DataFrame.iat <pandas.DataFrame.iat>`\n            Access a single value for a row/column pair by integer position.\n\n    .. todo:: Fix SDC behavior and add the expected output of the > python ./series_getitem.py to the docstring\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series operator :attr:`pandas.Series.__getitem__` implementation\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_getitem*\n    """"""\n\n    _func_name = \'Operator getitem().\'\n\n    if not isinstance(self, SeriesType):\n        return None\n\n    # Note: Getitem return Series\n    index_is_none = isinstance(self.index, numba.types.misc.NoneType)\n    index_is_none_or_numeric = index_is_none or (self.index and isinstance(self.index.dtype, types.Number))\n    index_is_string = not index_is_none and isinstance(self.index.dtype, (types.UnicodeType, types.StringLiteral))\n\n    if (\n        isinstance(idx, types.Number) and index_is_none_or_numeric or\n        (isinstance(idx, (types.UnicodeType, types.StringLiteral)) and index_is_string)\n    ):\n        def hpat_pandas_series_getitem_index_impl(self, idx):\n            index = self.index\n            mask = numpy.empty(len(self._data), numpy.bool_)\n            for i in numba.prange(len(index)):\n                mask[i] = index[i] == idx\n            return pandas.Series(data=self._data[mask], index=index[mask], name=self._name)\n\n        return hpat_pandas_series_getitem_index_impl\n\n    if (isinstance(idx, types.Integer) and index_is_string):\n        def hpat_pandas_series_idx_impl(self, idx):\n            return self._data[idx]\n\n        return hpat_pandas_series_idx_impl\n\n    if isinstance(idx, types.SliceType):\n        # Return slice for str values not implement\n        def hpat_pandas_series_getitem_idx_slice_impl(self, idx):\n            return pandas.Series(data=self._data[idx], index=self.index[idx], name=self._name)\n\n        return hpat_pandas_series_getitem_idx_slice_impl\n\n    if (isinstance(idx, (types.List, types.Array))\n            and isinstance(idx.dtype, (types.Boolean, bool))):\n        def hpat_pandas_series_getitem_idx_list_impl(self, idx):\n\n            if len(self) != len(idx):\n                raise IndexError(""Item wrong length"")\n\n            return pandas.Series(\n                data=numpy_like.getitem_by_mask(self._data, idx),\n                index=numpy_like.getitem_by_mask(self.index, idx),\n                name=self._name\n            )\n\n        return hpat_pandas_series_getitem_idx_list_impl\n\n    # idx is Series and it\'s index is any, idx.dtype is Boolean\n    if (isinstance(idx, SeriesType) and isinstance(idx.dtype, types.Boolean)):\n\n        none_indexes = isinstance(self.index, types.NoneType) and isinstance(idx.index, types.NoneType)\n        none_or_numeric_indexes = ((isinstance(self.index, types.NoneType) or check_index_is_numeric(self))\n                                   and (isinstance(idx.index, types.NoneType) or check_index_is_numeric(idx)))\n        if not (none_or_numeric_indexes\n                or check_types_comparable(self.index, idx.index)):\n            msg = \'{} The index of boolean indexer is not comparable to Series index.\' + \\\n                  \' Given: self.index={}, idx.index={}\'\n            raise TypingError(msg.format(_func_name, self.index, idx.index))\n\n        def _series_getitem_idx_bool_indexer_impl(self, idx):\n\n            if none_indexes == True:  # noqa\n                if len(self) > len(idx):\n                    msg = ""Unalignable boolean Series provided as indexer "" + \\\n                          ""(index of the boolean Series and of the indexed object do not match).""\n                    raise IndexingError(msg)\n\n                self_index = range(len(self))\n                reindexed_idx = idx\n            else:\n                self_index = self.index\n                reindexed_idx = sdc_reindex_series(idx._data, idx.index, idx._name, self_index)\n\n            return pandas.Series(\n                data=numpy_like.getitem_by_mask(self._data, reindexed_idx._data),\n                index=numpy_like.getitem_by_mask(self_index, reindexed_idx._data),\n                name=self._name\n            )\n\n        return _series_getitem_idx_bool_indexer_impl\n\n    # idx is Series and it\'s index is None, idx.dtype is not Boolean\n    if (isinstance(idx, SeriesType) and index_is_none\n            and not isinstance(idx.data.dtype, (types.Boolean, bool))):\n        def hpat_pandas_series_getitem_idx_list_impl(self, idx):\n            res = numpy.copy(self._data[:len(idx._data)])\n            index = numpy.arange(len(self._data))\n            for i in numba.prange(len(res)):\n                for j in numba.prange(len(index)):\n                    if j == idx._data[i]:\n                        res[i] = self._data[j]\n            return pandas.Series(data=res, index=index[idx._data], name=self._name)\n        return hpat_pandas_series_getitem_idx_list_impl\n\n    # idx is Series and it\'s index is not None, idx.dtype is not Boolean\n    if (isinstance(idx, SeriesType) and not isinstance(self.index, types.NoneType)\n            and not isinstance(idx.data.dtype, (types.Boolean, bool))):\n        def hpat_pandas_series_getitem_idx_series_impl(self, idx):\n            index = self.index\n            data = self._data\n            size = len(index)\n            data_res = []\n            index_res = []\n            for value in idx._data:\n                mask = numpy.zeros(shape=size, dtype=numpy.bool_)\n                for i in numba.prange(size):\n                    mask[i] = index[i] == value\n\n                data_res.extend(data[mask])\n                index_res.extend(index[mask])\n\n            return pandas.Series(data=data_res, index=index_res, name=self._name)\n\n        return hpat_pandas_series_getitem_idx_series_impl\n\n\n    raise TypingError(\'{} The index must be an Number, Slice, String, Boolean Array or a Series.\\\n                    Given: {}\'.format(_func_name, idx))\n\n\n@sdc_overload(operator.setitem)\ndef sdc_pandas_series_setitem(self, idx, value):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n    Pandas API: pandas.Series.setitem\n\n    Set value to Series by index\n\n    Limitations\n    -----------\n       - Not supported for idx as a string slice, e.g. S[\'a\':\'f\'] = value\n       - Not supported for string series\n       - Not supported for a case of setting value for non existing index\n       - Not supported for cases when setting causes change of the Series dtype\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_setitem_int.py\n       :language: python\n       :lines: 27-\n       :caption: Setting Pandas Series elements\n       :name: ex_series_setitem\n\n    .. command-output:: python ./series/series_setitem_int.py\n       :cwd: ../../../examples\n\n    .. literalinclude:: ../../../examples/series/series_setitem_slice.py\n       :language: python\n       :lines: 27-\n       :caption: Setting Pandas Series elements by slice\n       :name: ex_series_setitem\n\n    .. command-output:: python ./series/series_setitem_slice.py\n       :cwd: ../../../examples\n\n    .. literalinclude:: ../../../examples/series/series_setitem_series.py\n       :language: python\n       :lines: 27-\n       :caption: Setting Pandas Series elements by series\n       :name: ex_series_setitem\n\n    .. command-output:: python ./series/series_setitem_series.py\n       :cwd: ../../../examples\n\n    .. seealso::\n            :ref:`Series.getitem <pandas.Series.getitem>`\n                Get value(s) of Series by key.\n            :ref:`Series.loc <pandas.Series.loc>`\n                Access a group of rows and columns by label(s) or a boolean array.\n            :ref:`Series.iloc <pandas.Series.iloc>`\n                Purely integer-location based indexing for selection by position.\n            :ref:`Series.at <pandas.Series.at>`\n                Access a single value for a row/column label pair.\n            :ref:`Series.iat <pandas.Series.iat>`\n                Access a single value for a row/column pair by integer position.\n            :ref:`DataFrame.getitem <pandas.DataFrame.getitem>`\n                Get data from a DataFrame by indexer.\n            :ref:`DataFrame.setitem <pandas.DataFrame.setitem>`\n                Set value to DataFrame by index\n            :ref:`DataFrame.loc <pandas.DataFrame.loc>`\n                Access a group of rows and columns by label(s) or a boolean array.\n            :ref:`DataFrame.iloc <pandas.DataFrame.iloc>`\n                Purely integer-location based indexing for selection by position.\n            :ref:`DataFrame.at <pandas.DataFrame.at>`\n                Access a single value for a row/column label pair.\n            :ref:`DataFrame.iat <pandas.DataFrame.iat>`\n                Access a single value for a row/column pair by integer position.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series operator :attr:`pandas.Series.set` implementation\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_setitem*\n    """"""\n\n    _func_name = \'Operator setitem().\'\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    if not (isinstance(idx, (types.Number, types.UnicodeType, types.SliceType, types.Array, SeriesType))):\n        ty_checker.raise_exc(idx, \'scalar, Slice, Array or Series\', \'idx\')\n\n    all_supported_scalar_types = (types.Number, types.UnicodeType, types.Boolean)\n    if not (isinstance(value, all_supported_scalar_types) or isinstance(value, (SeriesType, types.Array))):\n        ty_checker.raise_exc(value, \'scalar, Array or Series\', \'value\')\n\n    if not check_types_comparable(self, value):\n        msg = \'{} The value and Series data must be comparable. Given: self.dtype={}, value={}\'\n        raise TypingError(msg.format(_func_name, self.dtype, value))\n\n    # idx is not necessarily of the same dtype as self.index, e.g. it might be a Boolean indexer or a Slice\n    if not (check_types_comparable(idx, self.index)\n            or isinstance(idx, (types.Integer, types.SliceType))\n            or (isinstance(idx, (SeriesType, types.Array)) and isinstance(idx.dtype, (types.Integer, types.Boolean)))):\n        msg = \'{} The idx is not comparable to Series index, not a Boolean or integer indexer or a Slice. \' + \\\n              \'Given: self.index={}, idx={}\'\n        raise TypingError(msg.format(_func_name, self.index, idx))\n\n    value_is_series = isinstance(value, SeriesType)\n    value_is_array = isinstance(value, types.Array)\n\n    # for many cases pandas setitem assigns values along positions in self._data\n    # not considering Series index, so a common implementation exists\n    idx_is_boolean_array = isinstance(idx, types.Array) and isinstance(idx.dtype, types.Boolean)\n    idx_is_boolean_series = isinstance(idx, SeriesType) and isinstance(idx.dtype, types.Boolean)\n    idx_and_self_index_comparable = check_types_comparable(self.index, idx)\n    self_index_is_none = isinstance(self.index, types.NoneType)\n    assign_along_positions = ((self_index_is_none\n                               or isinstance(idx, types.SliceType)\n                               or not idx_and_self_index_comparable)\n                              and not idx_is_boolean_series\n                              and not idx_is_boolean_array)\n\n    idx_is_scalar = isinstance(idx, (types.Number, types.UnicodeType))\n    if assign_along_positions or idx_is_scalar:\n\n        idx_is_numeric_or_boolean_series = (isinstance(idx, SeriesType)\n                                            and isinstance(idx.dtype, (types.Number, types.Boolean)))\n        assign_via_idx_mask = idx_is_scalar and idx_and_self_index_comparable\n        assign_via_idx_data = idx_is_numeric_or_boolean_series and not idx_and_self_index_comparable\n\n        def sdc_pandas_series_setitem_no_reindexing_impl(self, idx, value):\n\n            if assign_via_idx_mask == True:  # noqa\n                _idx = self._index == idx\n            elif assign_via_idx_data == True:  # noqa\n                _idx = idx._data\n            else:\n                _idx = idx\n\n            _value = value._data if value_is_series == True else value  # noqa\n            self._data[_idx] = _value\n            return self\n\n        return sdc_pandas_series_setitem_no_reindexing_impl\n\n    if (idx_is_boolean_array or idx_is_boolean_series) and value_is_series:\n\n        self_index_dtype = types.int64 if isinstance(self.index, types.NoneType) else self.index.dtype\n        value_index_dtype = types.int64 if isinstance(value.index, types.NoneType) else value.index.dtype\n        if (isinstance(self_index_dtype, types.Number) and isinstance(value_index_dtype, types.Number)):\n            indexes_common_dtype = find_common_dtype_from_numpy_dtypes([self_index_dtype, value_index_dtype], [])\n        elif (isinstance(self_index_dtype, types.UnicodeType) and isinstance(value_index_dtype, types.UnicodeType)):\n            indexes_common_dtype = types.unicode_type\n        else:\n            msg = \'{} The self and value indexes must be comparable. Given: self.dtype={}, value.dtype={}\'\n            raise TypingError(msg.format(_func_name, self_index_dtype, value_index_dtype))\n\n    if idx_is_boolean_array:\n\n        def sdc_pandas_series_setitem_idx_bool_array_align_impl(self, idx, value):\n\n            # if idx is a Boolean array (and value is a series) it\'s used as a mask for self.index\n            # and filtered indexes are looked in value.index, and if found corresponding value is set\n            if value_is_series == True:  # noqa\n                value_index, self_index = value.index, self.index\n                unique_value_indices, unique_self_indices = set(value_index), set(self_index)\n\n                # pandas behaves differently if value.index has duplicates and if it has no\n                # in case of duplicates in value.index assignment is made via positions\n                # in case there are no duplicates, value.index is used as reindexer\n                self_index_has_duplicates = len(unique_self_indices) != len(self_index)\n                value_index_has_duplicates = len(unique_value_indices) != len(value_index)\n                if (self_index_has_duplicates or value_index_has_duplicates):\n                    self._data[idx] = value._data\n                else:\n                    map_index_to_position = Dict.empty(\n                        key_type=indexes_common_dtype,\n                        value_type=types.int32\n                    )\n                    for i, index_value in enumerate(value_index):\n                        map_index_to_position[index_value] = types.int32(i)\n\n                    # such iterative setitem on a StringArray will be inefficient\n                    # TODO: refactor this when str_arr setitem is fully supported\n                    for i in numba.prange(len(self_index)):\n                        if idx[i]:\n                            self_index_value = self_index[i]\n                            if self_index_value in map_index_to_position:\n                                self._data[i] = value._data[map_index_to_position[self_index_value]]\n                            else:\n                                sdc.hiframes.join.setitem_arr_nan(self._data, i)\n\n            else:\n                # if value has no index - nothing to reindex and assignment is made along positions set by idx mask\n                self._data[idx] = value\n\n            return self\n\n        return sdc_pandas_series_setitem_idx_bool_array_align_impl\n\n    elif idx_is_boolean_series:\n\n        def sdc_pandas_series_setitem_idx_bool_series_align_impl(self, idx, value):\n\n            self_index, idx_index = self.index, idx.index\n            # FIXME: for now just use sorted, as == is not implemented for sets of unicode strings\n            if (sorted(self_index) != sorted(idx_index)):\n                msg = ""Unalignable boolean Series provided as indexer "" + \\\n                      ""(index of the boolean Series and of the indexed object do not match)""\n                raise ValueError(msg)\n\n            # if idx is a Boolean Series it\'s data is used as a mask for it\'s index\n            # and filtered indexes are either looked in value.index (if value is a Series)\n            # or in self.index (if value is scalar or array)\n            filtered_idx_indices = idx_index[idx._data]\n            filtered_idx_indices_set = set(filtered_idx_indices)\n            if value_is_series == True:  # noqa\n\n                if len(filtered_idx_indices_set) != len(filtered_idx_indices):\n                    raise ValueError(""cannot reindex from a duplicate axis"")\n\n                map_self_index_to_position = Dict.empty(\n                    key_type=indexes_common_dtype,\n                    value_type=types.int32\n                )\n                for i, index_value in enumerate(self_index):\n                    map_self_index_to_position[index_value] = types.int32(i)\n\n                value_index = value.index\n                map_value_index_to_position = Dict.empty(\n                    key_type=indexes_common_dtype,\n                    value_type=types.int32\n                )\n                for i, index_value in enumerate(value_index):\n                    map_value_index_to_position[index_value] = types.int32(i)\n\n                # for all index values in filtered index assign element of value with this index\n                # to element of self with this index\n                for i in numba.prange(len(filtered_idx_indices)):\n                    idx_index_value = filtered_idx_indices[i]\n                    if idx_index_value in map_value_index_to_position:\n                        self_index_pos = map_self_index_to_position[idx_index_value]\n                        value_index_pos = map_value_index_to_position[idx_index_value]\n                        self._data[self_index_pos] = value._data[value_index_pos]\n                    else:\n                        sdc.hiframes.join.setitem_arr_nan(self._data, map_self_index_to_position[idx_index_value])\n            else:\n                # use filtered index values to create a set mask, then make assignment to self\n                # using this mask (i.e. the order of filtered indices in self.index does not matter)\n                self_index_size = len(self_index)\n                set_mask = numpy.zeros(self_index_size, dtype=numpy.bool_)\n                for i in numba.prange(self_index_size):\n                    if self_index[i] in filtered_idx_indices_set:\n                        set_mask[i] = True\n                self._data[set_mask] = value\n\n            return self\n\n        return sdc_pandas_series_setitem_idx_bool_series_align_impl\n\n    elif isinstance(idx, (SeriesType, types.Array)) and idx_and_self_index_comparable:\n\n        # idx is numeric Series or array comparable with self.index, hence reindexing is possible\n        if isinstance(self.index.dtype, types.Number):\n\n            idx_is_series = isinstance(idx, SeriesType)\n            value_is_scalar = not (value_is_series or value_is_array)\n            def sdc_pandas_series_setitem_idx_int_series_align_impl(self, idx, value):\n\n                _idx = idx._data if idx_is_series == True else idx  # noqa\n                _value = value._data if value_is_series == True else value  # noqa\n\n                self_index_size = len(self._index)\n                idx_size = len(_idx)\n                valid_indices = numpy.repeat(-1, self_index_size)\n                for i in numba.prange(self_index_size):\n                    for j in numpy.arange(idx_size):\n                        if self._index[i] == _idx[j]:\n                            valid_indices[i] = j\n\n                valid_indices_positions = numpy.arange(self_index_size)[valid_indices != -1]\n                valid_indices_masked = valid_indices[valid_indices != -1]\n\n                indexes_found = self._index[valid_indices_positions]\n                if len(numpy.unique(indexes_found)) != len(indexes_found):\n                    raise ValueError(""Reindexing only valid with uniquely valued Index objects"")\n\n                if len(valid_indices_masked) != idx_size:\n                    raise ValueError(""Reindexing not possible: idx has index not found in Series"")\n\n                if value_is_scalar == True:  # noqa\n                    self._data[valid_indices_positions] = _value\n                else:\n                    self._data[valid_indices_positions] = numpy.take(_value, valid_indices_masked)\n\n                return self\n\n            return sdc_pandas_series_setitem_idx_int_series_align_impl\n\n        elif isinstance(self.index.dtype, types.UnicodeType):\n\n            def sdc_pandas_series_setitem_idx_str_series_align_impl(self, idx, value):\n\n                map_index_to_position = Dict.empty(\n                    key_type=types.unicode_type,\n                    value_type=types.int32\n                )\n                for i, index_value in enumerate(self._index):\n                    if index_value in map_index_to_position:\n                        raise ValueError(""Reindexing only valid with uniquely valued Index objects"")\n                    map_index_to_position[index_value] = types.int32(i)\n\n                idx_data_size = len(idx._data)\n                number_of_found = 0\n                set_positions = numpy.empty(idx_data_size, dtype=types.int32)\n                for i in numba.prange(len(idx._data)):\n                    index_value = idx._data[i]\n                    if index_value in map_index_to_position:\n                        number_of_found += 1\n                        set_positions[i] = map_index_to_position[index_value]\n\n                if number_of_found != idx_data_size:\n                    raise ValueError(""Reindexing not possible: idx has index not found in Series"")\n\n                if value_is_series == True:  # noqa\n                    self._data[set_positions] = value._data\n                else:\n                    self._data[set_positions] = value\n                return self\n\n            return sdc_pandas_series_setitem_idx_str_series_align_impl\n\n        else:  # self.index.dtype other than types.Number or types.Unicode\n            return None\n\n    return None\n\n\n@sdc_overload_attribute(SeriesType, \'iloc\')\ndef hpat_pandas_series_iloc(self):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.iloc\n\n    Limitations\n    -----------\n    Iloc always returns Series.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_iloc/series_iloc_value.py\n       :language: python\n       :lines: 27-\n       :caption: With a scalar integer.\n       :name: ex_series_iloc\n\n    .. command-output:: python ./series/series_iloc/series_iloc_value.py\n       :cwd: ../../../examples\n\n    .. literalinclude:: ../../../examples/series/series_iloc/series_iloc_slice.py\n       :language: python\n       :lines: 33-\n       :caption: With a slice object.\n       :name: ex_series_iloc\n\n    .. command-output:: python ./series/series_iloc/series_iloc_slice.py\n       :cwd: ../../../examples\n\n    .. seealso::\n\n        :ref:`DataFrame.iat <pandas.DataFrame.iat>`\n            Fast integer location scalar accessor.\n\n        :ref:`DataFrame.loc <pandas.DataFrame.loc>`\n            Purely label-location based indexer for selection by label.\n\n        :ref:`Series.iloc <pandas.Series.iloc>`\n            Purely integer-location based indexing for selection by position.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.iloc` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_iloc*\n    """"""\n\n    _func_name = \'Attribute iloc().\'\n\n    if not isinstance(self, SeriesType):\n        raise TypingError(\'{} The object must be a pandas.series. Given: {}\'.format(_func_name, self))\n\n    def hpat_pandas_series_iloc_impl(self):\n        return sdc.datatypes.hpat_pandas_getitem_types.series_getitem_accessor_init(self, \'iloc\')\n\n    return hpat_pandas_series_iloc_impl\n\n\n@sdc_overload_attribute(SeriesType, \'loc\')\ndef hpat_pandas_series_loc(self):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.loc\n\n    Limitations\n    -----------\n    - Loc always returns Series.\n    - Loc slice is supported only with numeric values and specified ``start``.\n    - Loc callable is not supported yet.\n    - This function may reveal slower performance than Pandas* on user system. Users should exercise a tradeoff\n    between staying in JIT-region with that function or going back to interpreter mode.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_loc/series_loc_single_result.py\n       :language: python\n       :lines: 32-\n       :caption: With a scalar integer. Returns single value.\n       :name: ex_series_loc\n\n    .. command-output:: python ./series/series_loc/series_loc_single_result.py\n       :cwd: ../../../examples\n\n    .. literalinclude:: ../../../examples/series/series_loc/series_loc_multiple_result.py\n       :language: python\n       :lines: 34-\n       :caption: With a scalar integer. Returns multiple value.\n       :name: ex_series_loc\n\n    .. command-output:: python ./series/series_loc/series_loc_multiple_result.py\n       :cwd: ../../../examples\n\n    .. literalinclude:: ../../../examples/series/series_loc/series_loc_slice.py\n       :language: python\n       :lines: 34-\n       :caption: With a slice object. Returns multiple value.\n       :name: ex_series_loc\n\n    .. command-output:: python ./series/series_loc/series_loc_slice.py\n       :cwd: ../../../examples\n\n    .. seealso::\n\n        :ref:`DataFrame.at <pandas.DataFrame.at>`\n            Access a single value for a row/column label pair.\n\n        :ref:`DataFrame.iloc <pandas.DataFrame.iloc>`\n            Access group of rows and columns by integer position(s).\n\n        :ref:`DataFrame.xs <pandas.DataFrame.xs>`\n            Returns a cross-section (row(s) or column(s)) from the Series/DataFrame.\n\n        :ref:`Series.loc <pandas.Series.loc>`\n            Access group of values using labels.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.loc` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_loc*\n    """"""\n\n    _func_name = \'Attribute loc().\'\n\n    if not isinstance(self, SeriesType):\n        raise TypingError(\'{} The object must be a pandas.series. Given: {}\'.format(_func_name, self))\n\n    def hpat_pandas_series_loc_impl(self):\n        return sdc.datatypes.hpat_pandas_getitem_types.series_getitem_accessor_init(self, \'loc\')\n\n    return hpat_pandas_series_loc_impl\n\n\n@sdc_overload_attribute(SeriesType, \'iat\')\ndef hpat_pandas_series_iat(self):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.iat\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_iat.py\n       :language: python\n       :lines: 27-\n       :caption: Get value at specified index position.\n       :name: ex_series_iat\n\n    .. command-output:: python ./series/series_iat.py\n       :cwd: ../../../examples\n\n    .. seealso::\n\n        :ref:`DataFrame.at <pandas.DataFrame.at>`\n            Access a single value for a row/column label pair.\n\n        :ref:`DataFrame.loc <pandas.DataFrame.loc>`\n            Purely label-location based indexer for selection by label.\n\n        :ref:`DataFrame.iloc <pandas.DataFrame.iloc>`\n            Access group of rows and columns by integer position(s).\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.iat` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_iat*\n    """"""\n\n    _func_name = \'Attribute iat().\'\n\n    if not isinstance(self, SeriesType):\n        raise TypingError(\'{} The object must be a pandas.series. Given: {}\'.format(_func_name, self))\n\n    def hpat_pandas_series_iat_impl(self):\n        return sdc.datatypes.hpat_pandas_getitem_types.series_getitem_accessor_init(self, \'iat\')\n\n    return hpat_pandas_series_iat_impl\n\n\n@sdc_overload_attribute(SeriesType, \'at\')\ndef hpat_pandas_series_at(self):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.at\n\n    Limitations\n    -----------\n    - This function may reveal slower performance than Pandas* on user system. Users should exercise a tradeoff\n    between staying in JIT-region with that function or going back to interpreter mode.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_at/series_at_single_result.py\n       :language: python\n       :lines: 27-\n       :caption: With a scalar integer. Returns single value.\n       :name: ex_series_at\n\n    .. command-output:: python ./series/series_at/series_at_single_result.py\n       :cwd: ../../../examples\n\n    .. literalinclude:: ../../../examples/series/series_at/series_at_multiple_result.py\n       :language: python\n       :lines: 27-\n       :caption: With a scalar integer. Returns multiple value.\n       :name: ex_series_at\n\n    .. command-output:: python ./series/series_at/series_at_multiple_result.py\n       :cwd: ../../../examples\n\n    .. seealso::\n\n        :ref:`DataFrame.iat <pandas.DataFrame.iat>`\n            Access a single value for a row/column pair by integer position.\n\n        :ref:`DataFrame.loc <pandas.DataFrame.loc>`\n            Access a group of rows and columns by label(s).\n\n        :ref:`Series.at <pandas.Series.at>`\n            Access a single value using a label.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.at` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_at*\n    """"""\n\n    _func_name = \'Attribute at().\'\n\n    if not isinstance(self, SeriesType):\n        raise TypingError(\'{} The object must be a pandas.series. Given: {}\'.format(_func_name, self))\n\n    def hpat_pandas_series_at_impl(self):\n        return sdc.datatypes.hpat_pandas_getitem_types.series_getitem_accessor_init(self, \'at\')\n\n    return hpat_pandas_series_at_impl\n\n\n@sdc_overload_method(SeriesType, \'nsmallest\')\ndef hpat_pandas_series_nsmallest(self, n=5, keep=\'first\'):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.nsmallest\n\n    Limitations\n    -----------\n    - Parameter ``keep`` is supported only with default value ``\'first\'``.\n    - This function may reveal slower performance than Pandas* on user system. Users should exercise a tradeoff\n    between staying in JIT-region with that function or going back to interpreter mode.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_nsmallest.py\n       :language: python\n       :lines: 27-\n       :caption: Returns the smallest n elements.\n       :name: ex_series_nsmallest\n\n    .. command-output:: python ./series/series_nsmallest.py\n       :cwd: ../../../examples\n\n    .. seealso::\n\n        :ref:`Series.nlargest <pandas.Series.nlargest>`\n            Get the n largest elements.\n\n        :ref:`Series.sort_values <pandas.Series.sort_values>`\n            Sort Series by values.\n\n        :ref:`Series.head <pandas.Series.head>`\n            Return the first n rows.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n\n    Pandas Series method :meth:`pandas.Series.nsmallest` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_nsmallest*\n    """"""\n\n    _func_name = \'Method nsmallest().\'\n\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    if not isinstance(n, (types.Omitted, int, types.Integer)):\n        ty_checker.raise_exc(n, \'int\', \'n\')\n\n    if not isinstance(keep, (types.Omitted, str, types.UnicodeType, types.StringLiteral)):\n        ty_checker.raise_exc(keep, \'str\', \'keep\')\n\n    def hpat_pandas_series_nsmallest_impl(self, n=5, keep=\'first\'):\n        if keep != \'first\':\n            raise ValueError(""Method nsmallest(). Unsupported parameter. Given \'keep\' != \'first\'"")\n\n        # mergesort is used for stable sorting of repeated values\n        indices = self._data.argsort(kind=\'mergesort\')[:max(n, 0)]\n\n        return self.take(indices)\n\n    return hpat_pandas_series_nsmallest_impl\n\n\n@sdc_overload_method(SeriesType, \'nlargest\')\ndef hpat_pandas_series_nlargest(self, n=5, keep=\'first\'):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.nlargest\n\n    Limitations\n    -----------\n    - Parameter ``keep`` is supported only with default value ``\'first\'``.\n    - This function may reveal slower performance than Pandas* on user system. Users should exercise a tradeoff\n    between staying in JIT-region with that function or going back to interpreter mode.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_nlargest.py\n       :language: python\n       :lines: 27-\n       :caption: Returns the largest n elements.\n       :name: ex_series_nlargest\n\n    .. command-output:: python ./series/series_nlargest.py\n       :cwd: ../../../examples\n\n    .. seealso::\n\n        :ref:`Series.nsmallest <pandas.Series.nsmallest>`\n            Get the n smallest elements.\n\n        :ref:`Series.sort_values <pandas.Series.sort_values>`\n            Sort Series by values.\n\n        :ref:`Series.head <pandas.Series.head>`\n            Return the first n rows.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n\n    Pandas Series method :meth:`pandas.Series.nlargest` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_nlargest*\n    """"""\n\n    _func_name = \'Method nlargest().\'\n\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    if not isinstance(n, (types.Omitted, int, types.Integer)):\n        ty_checker.raise_exc(n, \'int\', \'n\')\n\n    if not isinstance(keep, (types.Omitted, str, types.UnicodeType, types.StringLiteral)):\n        ty_checker.raise_exc(keep, \'str\', \'keep\')\n\n    def hpat_pandas_series_nlargest_impl(self, n=5, keep=\'first\'):\n        if keep != \'first\':\n            raise ValueError(""Method nlargest(). Unsupported parameter. Given \'keep\' != \'first\'"")\n\n        # data: [0, 1, -1, 1, 0] -> [1, 1, 0, 0, -1]\n        # index: [0, 1,  2, 3, 4] -> [1, 3, 0, 4,  2] (not [3, 1, 4, 0, 2])\n        # subtract 1 to ensure reverse ordering at boundaries\n        indices = (-self._data - 1).argsort(kind=\'mergesort\')[:max(n, 0)]\n\n        return self.take(indices)\n\n    return hpat_pandas_series_nlargest_impl\n\n\n@sdc_overload_attribute(SeriesType, \'shape\')\ndef hpat_pandas_series_shape(self):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.shape\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_shape.py\n       :language: python\n       :lines: 27-\n       :caption: Return a tuple of the shape of the underlying data.\n       :name: ex_series_shape\n\n    .. command-output:: python ./series/series_shape.py\n       :cwd: ../../../examples\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series attribute :attr:`pandas.Series.shape` implementation\n\n    .. only:: developer\n        Test: python -m sdc.runtests sdc.tests.test_series.TestSeries.test_series_shape1\n    """"""\n\n    _func_name = \'Attribute shape.\'\n\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    def hpat_pandas_series_shape_impl(self):\n        return self._data.shape\n\n    return hpat_pandas_series_shape_impl\n\n\n@sdc_overload_method(SeriesType, \'std\')\ndef hpat_pandas_series_std(self, axis=None, skipna=None, level=None, ddof=1, numeric_only=None):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.std\n\n    Limitations\n    -----------\n    Parameters ``axis``, ``level`` and ``numeric_only`` are supported only with default value ``None``.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_std.py\n       :language: python\n       :lines: 27-\n       :caption: Returns sample standard deviation over Series.\n       :name: ex_series_std\n\n    .. command-output:: python ./series/series_std.py\n       :cwd: ../../../examples\n\n    .. seealso::\n\n        :ref:`Series.var <pandas.Series.var>`\n            Returns unbiased variance over Series.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n\n    Pandas Series method :meth:`pandas.Series.std` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_std\n    """"""\n\n    _func_name = \'Method std().\'\n\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    if not isinstance(self.data.dtype, types.Number):\n        ty_checker.raise_exc(self.data, \'number\', \'self.data\')\n\n    if not isinstance(skipna, (types.Omitted, types.Boolean, types.NoneType)) and skipna is not None:\n        ty_checker.raise_exc(skipna, \'bool\', \'skipna\')\n\n    if not isinstance(ddof, (types.Omitted, int, types.Integer)):\n        ty_checker.raise_exc(ddof, \'int\', \'ddof\')\n\n    if not isinstance(axis, (types.Omitted, types.NoneType)) and axis is not None:\n        ty_checker.raise_exc(axis, \'None\', \'axis\')\n\n    if not isinstance(level, (types.Omitted, types.NoneType)) and level is not None:\n        ty_checker.raise_exc(level, \'None\', \'level\')\n\n    if not isinstance(numeric_only, (types.Omitted, types.NoneType)) and numeric_only is not None:\n        ty_checker.raise_exc(numeric_only, \'None\', \'numeric_only\')\n\n    def hpat_pandas_series_std_impl(self, axis=None, skipna=None, level=None, ddof=1, numeric_only=None):\n        var = self.var(axis=axis, skipna=skipna, level=level, ddof=ddof, numeric_only=numeric_only)\n        return var ** 0.5\n\n    return hpat_pandas_series_std_impl\n\n\n@sdc_overload_attribute(SeriesType, \'values\')\ndef hpat_pandas_series_values(self):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.values\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_values.py\n       :language: python\n       :lines: 27-\n       :caption: Return Series as ndarray or ndarray-like depending on the dtype.\n       :name: ex_series_values\n\n    .. command-output:: python ./series/series_values.py\n       :cwd: ../../../examples\n\n    .. seealso::\n\n        :ref:`Series.array <pandas.Series.array>`\n            Reference to the underlying data.\n\n        :ref:`Series.to_numpy <pandas.Series.to_numpy>`\n            A NumPy array representing the underlying data.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series attribute \'values\' implementation.\n\n    .. only:: developer\n        Test:  python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_values*\n    """"""\n\n    _func_name = \'Attribute values.\'\n\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    def hpat_pandas_series_values_impl(self):\n        return self._data\n\n    return hpat_pandas_series_values_impl\n\n\n@sdc_overload_method(SeriesType, \'value_counts\')\ndef hpat_pandas_series_value_counts(self, normalize=False, sort=True, ascending=False, bins=None, dropna=True):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.value_counts\n\n    Limitations\n    -----------\n    - Parameters ``normalize`` and ``bins`` are currently unsupported.\n    - Parameter ``dropna`` is unsupported for String Series.\n    - Elements with the same count might appear in result in a different order than in Pandas.\n    - This function may reveal slower performance than Pandas* on user system. Users should exercise a tradeoff\n    between staying in JIT-region with that function or going back to interpreter mode.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_value_counts.py\n       :language: python\n       :lines: 35-\n       :caption: Getting the number of values excluding NaNs\n       :name: ex_series_value_counts\n\n    .. command-output:: python ./series/series_value_counts.py\n       :cwd: ../../../examples\n\n    .. seealso::\n\n        :ref:`Series.count <pandas.Series.count>`\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.value_counts` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_value_counts*\n    """"""\n\n    _func_name = \'Method value_counts().\'\n\n    ty_checker = TypeChecker(\'Method value_counts().\')\n    ty_checker.check(self, SeriesType)\n\n    if not isinstance(normalize, (types.Omitted, types.Boolean, bool)) and normalize is True:\n        ty_checker.raise_exc(normalize, \'boolean\', \'normalize\')\n\n    if not isinstance(sort, (types.Omitted, types.Boolean, bool)):\n        ty_checker.raise_exc(sort, \'boolean\', \'sort\')\n\n    if not isinstance(ascending, (types.Omitted, types.Boolean, bool)):\n        ty_checker.raise_exc(ascending, \'boolean\', \'ascending\')\n\n    if not isinstance(bins, (types.Omitted, types.NoneType)) and bins is not None:\n        ty_checker.raise_exc(bins, \'boolean\', \'bins\')\n\n    if not isinstance(dropna, (types.Omitted, types.Boolean, bool)):\n        ty_checker.raise_exc(dropna, \'boolean\', \'dropna\')\n\n    if isinstance(self.data, StringArrayType):\n        def hpat_pandas_series_value_counts_str_impl(\n                self, normalize=False, sort=True, ascending=False, bins=None, dropna=True):\n\n            value_counts_dict = Dict.empty(\n                key_type=types.unicode_type,\n                value_type=types.intp\n            )\n\n            nan_counts = 0\n            for i, value in enumerate(self._data):\n                if str_arr_is_na(self._data, i):\n                    if not dropna:\n                        nan_counts += 1\n                    continue\n\n                value_counts_dict[value] = value_counts_dict.get(value, 0) + 1\n\n            need_add_nan_count = not dropna and nan_counts\n\n            values = [key for key in value_counts_dict]\n            counts_as_list = [value_counts_dict[key] for key in value_counts_dict.keys()]\n            values_len = len(values)\n\n            if need_add_nan_count:\n                # append a separate empty string for NaN elements\n                values_len += 1\n                values.append(\'\')\n                counts_as_list.append(nan_counts)\n\n            counts = numpy.asarray(counts_as_list, dtype=numpy.intp)\n            indexes_order = numpy.arange(values_len)\n            if sort:\n                indexes_order = counts.argsort()\n                if not ascending:\n                    indexes_order = indexes_order[::-1]\n\n            counts_sorted = numpy.take(counts, indexes_order)\n            values_sorted_by_count = [values[i] for i in indexes_order]\n\n            # allocate the result index as a StringArray and copy values to it\n            result_index = create_str_arr_from_list(values_sorted_by_count)\n            if need_add_nan_count:\n                # set null bit for StringArray element corresponding to NaN element (was added as last in values)\n                index_previous_nan_pos = values_len - 1\n                for i in numpy.arange(values_len):\n                    if indexes_order[i] == index_previous_nan_pos:\n                        str_arr_set_na(result_index, i)\n                        break\n\n            return pandas.Series(counts_sorted, index=result_index, name=self._name)\n\n        return hpat_pandas_series_value_counts_str_impl\n\n    elif isinstance(self.dtype, (types.Number, types.Boolean)):\n\n        series_dtype = self.dtype\n        def hpat_pandas_series_value_counts_number_impl(\n                self, normalize=False, sort=True, ascending=False, bins=None, dropna=True):\n\n            value_counts_dict = Dict.empty(\n                key_type=series_dtype,\n                value_type=types.intp\n            )\n\n            zero_counts = 0\n            is_zero_found = False\n            for value in self._data:\n                if (dropna and numpy.isnan(value)):\n                    continue\n\n                # Pandas hash-based value_count_float64 function doesn\'t distinguish between\n                # positive and negative zeros, hence we count zero values separately and store\n                # as a key the first zero value found in the Series\n                if not value:\n                    zero_counts += 1\n                    if not is_zero_found:\n                        zero_value = value\n                        is_zero_found = True\n                    continue\n\n                value_counts_dict[value] = value_counts_dict.get(value, 0) + 1\n\n            if zero_counts:\n                value_counts_dict[zero_value] = zero_counts\n\n            unique_values = numpy.asarray(\n                list(value_counts_dict),\n                dtype=self._data.dtype\n            )\n            value_counts = numpy.asarray(\n                [value_counts_dict[key] for key in value_counts_dict],\n                dtype=numpy.intp\n            )\n\n            indexes_order = numpy.arange(len(value_counts))\n            if sort:\n                indexes_order = value_counts.argsort()\n                if not ascending:\n                    indexes_order = indexes_order[::-1]\n\n            sorted_unique_values = numpy.take(unique_values, indexes_order)\n            sorted_value_counts = numpy.take(value_counts, indexes_order)\n\n            return pandas.Series(sorted_value_counts, index=sorted_unique_values, name=self._name)\n\n        return hpat_pandas_series_value_counts_number_impl\n\n    return None\n\n\n@sdc_overload_method(SeriesType, \'var\')\ndef hpat_pandas_series_var(self, axis=None, skipna=None, level=None, ddof=1, numeric_only=None):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.var\n\n    Limitations\n    -----------\n    Parameters ``axis``, ``level`` and ``numeric_only`` are supported only with default value ``None``.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_var.py\n       :language: python\n       :lines: 27-\n       :caption: Returns unbiased variance over Series.\n       :name: ex_series_var\n\n    .. command-output:: python ./series/series_var.py\n       :cwd: ../../../examples\n\n    .. seealso::\n\n        :ref:`Series.std <pandas.Series.std>`\n            Returns sample standard deviation over Series.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n\n    Pandas Series method :meth:`pandas.Series.var` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_var\n    """"""\n\n    _func_name = \'Method var().\'\n\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    if not isinstance(self.data.dtype, types.Number):\n        ty_checker.raise_exc(self.data, \'number\', \'self.data\')\n\n    if not isinstance(skipna, (types.Omitted, types.Boolean, types.NoneType)) and skipna is not None:\n        ty_checker.raise_exc(skipna, \'bool\', \'skipna\')\n\n    if not isinstance(ddof, (types.Omitted, int, types.Integer)):\n        ty_checker.raise_exc(ddof, \'int\', \'ddof\')\n\n    if not isinstance(axis, (types.Omitted, types.NoneType)) and axis is not None:\n        ty_checker.raise_exc(axis, \'None\', \'axis\')\n\n    if not isinstance(level, (types.Omitted, types.NoneType)) and level is not None:\n        ty_checker.raise_exc(level, \'None\', \'level\')\n\n    if not isinstance(numeric_only, (types.Omitted, types.NoneType)) and numeric_only is not None:\n        ty_checker.raise_exc(numeric_only, \'None\', \'numeric_only\')\n\n    def hpat_pandas_series_var_impl(self, axis=None, skipna=None, level=None, ddof=1, numeric_only=None):\n        if skipna is None:\n            skipna = True\n\n        if skipna:\n            valuable_length = len(self._data) - numpy.sum(numpy.isnan(self._data))\n            if valuable_length <= ddof:\n                return numpy.nan\n\n            return numpy_like.nanvar(self._data) * valuable_length / (valuable_length - ddof)\n\n        if len(self._data) <= ddof:\n            return numpy.nan\n\n        return self._data.var() * len(self._data) / (len(self._data) - ddof)\n\n    return hpat_pandas_series_var_impl\n\n\n@sdc_overload_attribute(SeriesType, \'index\')\ndef hpat_pandas_series_index(self):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.index\n\n    Limitations\n    -----------\n    - This function may reveal slower performance than Pandas* on user system. Users should exercise a tradeoff\n    between staying in JIT-region with that function or going back to interpreter mode.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_index.py\n       :language: python\n       :lines: 27-\n       :caption: The index (axis labels) of the Series.\n       :name: ex_series_index\n\n    .. command-output:: python ./series/series_index.py\n       :cwd: ../../../examples\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series attribute :attr:`pandas.Series.index` implementation\n\n    .. only:: developer\n        Test: python -m sdc.runtests sdc.tests.test_series.TestSeries.test_series_index1\n        Test: python -m sdc.runtests sdc.tests.test_series.TestSeries.test_series_index2\n    """"""\n\n    _func_name = \'Attribute index.\'\n\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    if isinstance(self.index, types.NoneType) or self.index is None:\n        def hpat_pandas_series_index_none_impl(self):\n            return numpy.arange(len(self._data))\n\n        return hpat_pandas_series_index_none_impl\n    else:\n        def hpat_pandas_series_index_impl(self):\n            return self._index\n\n        return hpat_pandas_series_index_impl\n\n\nhpat_pandas_series_rolling = sdc_overload_method(SeriesType, \'rolling\')(\n    gen_sdc_pandas_rolling_overload_body(_hpat_pandas_series_rolling_init, SeriesType))\nhpat_pandas_series_rolling.__doc__ = sdc_pandas_rolling_docstring_tmpl.format(\n    ty=\'Series\', ty_lower=\'series\')\n\n\n@sdc_overload_attribute(SeriesType, \'size\')\ndef hpat_pandas_series_size(self):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.size\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_size.py\n       :language: python\n       :lines: 27-\n       :caption: Return the number of elements in the underlying data.\n       :name: ex_series_size\n\n    .. command-output:: python ./series/series_size.py\n       :cwd: ../../../examples\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series attribute :attr:`pandas.Series.size` implementation\n\n    .. only:: developer\n        Test: python -m sdc.runtests sdc.tests.test_series.TestSeries.test_series_size\n    """"""\n\n    _func_name = \'Attribute size.\'\n\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    def hpat_pandas_series_size_impl(self):\n        return len(self._data)\n\n    return hpat_pandas_series_size_impl\n\n\n@sdc_overload_attribute(SeriesType, \'str\')\ndef hpat_pandas_series_str(self):\n    """"""\n    Pandas Series attribute :attr:`pandas.Series.str` implementation\n\n    .. only:: developer\n        Test: python -m sdc.runtests sdc.tests.test_hiframes.TestHiFrames.test_str_get\n    """"""\n\n    _func_name = \'Attribute str.\'\n\n    if not isinstance(self, SeriesType):\n        raise TypingError(\'{} The object must be a pandas.series. Given: {}\'.format(_func_name, self))\n\n    if not isinstance(self.data.dtype, (types.List, types.UnicodeType)):\n        msg = \'{}  Can only use .str accessor with string values. Given: {}\'\n        raise TypingError(msg.format(_func_name, self.data.dtype))\n\n    def hpat_pandas_series_str_impl(self):\n        return pandas.core.strings.StringMethods(self)\n\n    return hpat_pandas_series_str_impl\n\n\n@sdc_overload_attribute(SeriesType, \'ndim\')\ndef hpat_pandas_series_ndim(self):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.ndim\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_ndim.py\n       :language: python\n       :lines: 27-\n       :caption: Number of dimensions of the underlying data, by definition 1.\n       :name: ex_series_ndim\n\n    .. command-output:: python ./series/series_ndim.py\n       :cwd: ../../../examples\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series attribute :attr:`pandas.Series.ndim` implementation\n\n    .. only:: developer\n        Test: python -m sdc.runtests sdc.tests.test_series.TestSeries.test_series_getattr_ndim\n    """"""\n\n    _func_name = \'Attribute ndim.\'\n\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    def hpat_pandas_series_ndim_impl(self):\n        return 1\n\n    return hpat_pandas_series_ndim_impl\n\n\n@sdc_overload_attribute(SeriesType, \'T\')\ndef hpat_pandas_series_T(self):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.T\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_T.py\n       :language: python\n       :lines: 27-\n       :caption: Return the transpose, which is by definition self.\n       :name: ex_series_T\n\n    .. command-output:: python ./series/series_T.py\n       :cwd: ../../../examples\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series attribute :attr:`pandas.Series.T` implementation\n\n    .. only:: developer\n        Test: python -m sdc.runtests sdc.tests.test_series.TestSeries.test_series_getattr_T\n    """"""\n\n    _func_name = \'Attribute T.\'\n\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    def hpat_pandas_series_T_impl(self):\n        return self._data\n\n    return hpat_pandas_series_T_impl\n\n\n@sdc_overload(len)\ndef hpat_pandas_series_len(self):\n    """"""\n    Pandas Series operator :func:`len` implementation\n\n    .. only:: developer\n        Test: python -m sdc.runtests sdc.tests.test_series.TestSeries.test_series_len\n    """"""\n\n    _func_name = \'Operator len().\'\n\n    if not isinstance(self, SeriesType):\n        raise TypingError(\'{} The object must be a pandas.series. Given: {}\'.format(_func_name, self))\n\n    def hpat_pandas_series_len_impl(self):\n        return len(self._data)\n\n    return hpat_pandas_series_len_impl\n\n\n@sdc_overload_method(SeriesType, \'astype\', parallel=False)\ndef hpat_pandas_series_astype(self, dtype, copy=True, errors=\'raise\'):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.astype\n\n    Limitations\n    -----------\n    - Parameter ``copy`` is supported only with default value ``True``.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_astype.py\n       :language: python\n       :lines: 36-\n       :caption: Cast a pandas object to a specified dtype.\n       :name: ex_series_astype\n\n    .. command-output:: python ./series/series_astype.py\n       :cwd: ../../../examples\n\n    .. seealso::\n\n        `pandas.to_datetime\n        <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html#pandas.to_datetime>`_\n            Convert argument to datetime.\n\n        `pandas.to_timedelta\n        <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_timedelta.html#pandas.to_timedelta>`_\n            Convert argument to timedelta.\n\n        `pandas.to_numeric\n        <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_numeric.html#pandas.to_numeric>`_\n            Convert argument to a numeric type.\n\n        `numpy.ndarray.astype\n        <https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.astype.html#numpy.ndarray.astype>`_\n            Copy of the array, cast to a specified type.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.astype` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_astype*\n    """"""\n\n    _func_name = \'Method astype().\'\n\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    if not isinstance(copy, (types.Omitted, bool, types.Boolean)):\n        ty_checker.raise_exc(copy, \'bool\', \'copy\')\n\n    if (not isinstance(errors, (types.Omitted, str, types.UnicodeType, types.StringLiteral)) and\n        errors in (\'raise\', \'ignore\')):\n        ty_checker.raise_exc(errors, \'str\', \'errors\')\n\n    # Return npytypes.Array from npytypes.Array for astype(types.functions.NumberClass), example - astype(np.int64)\n    # Return npytypes.Array from npytypes.Array for astype(types.StringLiteral), example - astype(\'int64\')\n    def hpat_pandas_series_astype_numba_impl(self, dtype, copy=True, errors=\'raise\'):\n        return pandas.Series(data=numpy_like.astype_no_inline(self._data, dtype), index=self._index, name=self._name)\n\n    # Return self\n    def hpat_pandas_series_astype_no_modify_impl(self, dtype, copy=True, errors=\'raise\'):\n        return pandas.Series(data=self._data, index=self._index, name=self._name)\n\n    str_check = ((isinstance(dtype, types.Function) and dtype.typing_key == str) or\n                 (isinstance(dtype, types.StringLiteral) and dtype.literal_value == \'str\'))\n\n    # Needs Numba astype impl support converting unicode_type to NumberClass and other types\n    if (isinstance(self.data, StringArrayType) and not str_check):\n        if isinstance(dtype, types.functions.NumberClass) and errors == \'raise\':\n            raise TypingError(f\'Needs Numba astype impl support converting unicode_type to {dtype}\')\n        if isinstance(dtype, types.StringLiteral) and errors == \'raise\':\n            try:\n                literal_value = numpy.dtype(dtype.literal_value)\n            except:\n                pass # Will raise the exception later\n            else:\n                raise TypingError(f\'Needs Numba astype impl support converting unicode_type to {dtype.literal_value}\')\n\n    data_narr = isinstance(self.data, types.npytypes.Array)\n    dtype_num_liter = isinstance(dtype, (types.functions.NumberClass, types.StringLiteral))\n\n    if data_narr and dtype_num_liter or str_check:\n        return hpat_pandas_series_astype_numba_impl\n\n    if errors == \'raise\':\n        raise TypingError(f\'{_func_name} The object must be a supported type. Given dtype: {dtype}\')\n    else:\n        return hpat_pandas_series_astype_no_modify_impl\n\n\n@sdc_overload_method(SeriesType, \'shift\')\ndef hpat_pandas_series_shift(self, periods=1, freq=None, axis=0, fill_value=None):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.shift\n\n    Limitations\n    -----------\n    Parameters ``freq`` and ``axis`` are supported only with default values ``None`` and ``0`` respectively.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_shift.py\n       :language: python\n       :lines: 36-\n       :caption: Shift index by desired number of periods with an optional time freq.\n       :name: ex_series_shift\n\n    .. command-output:: python ./series/series_shift.py\n       :cwd: ../../../examples\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n\n    Pandas Series method :meth:`pandas.Series.shift` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_shift*\n    """"""\n\n    _func_name = \'Method shift().\'\n\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    if not isinstance(self.data.dtype, types.Number):\n        ty_checker.raise_exc(self.data.dtype, \'number\', \'self.data.dtype\')\n\n    if not isinstance(fill_value, (types.Omitted, types.Number, types.NoneType)) and fill_value is not None:\n        ty_checker.raise_exc(fill_value, \'number\', \'fill_value\')\n\n    if not isinstance(freq, (types.Omitted, types.NoneType)) and freq is not None:\n        ty_checker.raise_exc(freq, \'None\', \'freq\')\n\n    if not isinstance(axis, (types.Omitted, int, types.Integer)) and not axis:\n        ty_checker.raise_exc(axis, \'int\', \'axis\')\n\n    fill_is_default = isinstance(fill_value, (types.Omitted, types.NoneType)) or fill_value is None\n    series_np_dtype = [numpy_support.as_dtype(self.data.dtype)]\n    fill_np_dtype = [numpy.float64 if fill_is_default else numpy_support.as_dtype(fill_value)]\n\n    fill_dtype = types.float64 if fill_is_default else fill_value\n    common_dtype = find_common_dtype_from_numpy_dtypes([], [self.data.dtype, fill_dtype])\n\n    if fill_is_default:\n        def hpat_pandas_series_shift_impl(self, periods=1, freq=None, axis=0, fill_value=None):\n            if axis != 0:\n                raise TypingError(\'Method shift(). Unsupported parameters. Given axis != 0\')\n\n            arr = numpy.empty(shape=len(self._data), dtype=common_dtype)\n            if periods > 0:\n                arr[:periods] = numpy.nan\n                arr[periods:] = self._data[:-periods]\n            elif periods < 0:\n                arr[periods:] = numpy.nan\n                arr[:periods] = self._data[-periods:]\n            else:\n                arr[:] = self._data\n\n            return pandas.Series(data=arr, index=self._index, name=self._name)\n\n        return hpat_pandas_series_shift_impl\n\n    def hpat_pandas_series_shift_impl(self, periods=1, freq=None, axis=0, fill_value=None):\n        if axis != 0:\n            raise TypingError(\'Method shift(). Unsupported parameters. Given axis != 0\')\n\n        arr = numpy.empty(len(self._data), dtype=common_dtype)\n        if periods > 0:\n            arr[:periods] = fill_value\n            arr[periods:] = self._data[:-periods]\n        elif periods < 0:\n            arr[periods:] = fill_value\n            arr[:periods] = self._data[-periods:]\n        else:\n            arr[:] = self._data\n\n        return pandas.Series(data=arr, index=self._index, name=self._name)\n\n    return hpat_pandas_series_shift_impl\n\n\n@sdc_overload_method(SeriesType, \'isin\')\ndef hpat_pandas_series_isin(self, values):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.isin\n\n    Limitations\n    -----------\n    - This function may reveal slower performance than Pandas* on user system. Users should exercise a tradeoff\n    between staying in JIT-region with that function or going back to interpreter mode.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_isin.py\n       :language: python\n       :lines: 27-\n       :caption: Check whether values are contained in Series.\n       :name: ex_series_isin\n\n    .. command-output:: python ./series/series_isin.py\n       :cwd: ../../../examples\n\n    .. seealso::\n\n        :ref:`DataFrame.isin <pandas.DataFrame.isin>`\n            Equivalent method on DataFrame.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.isin` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_isin*\n    """"""\n\n    _func_name = \'Method isin().\'\n\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    if not isinstance(values, (types.Set, types.List)):\n        ty_checker.raise_exc(values, \'set or list\', \'values\')\n\n    if isinstance(values.dtype, (types.UnicodeType, types.StringLiteral)):\n        def hpat_pandas_series_isin_impl(self, values):\n            # TODO: replace with below line when Numba supports np.isin in nopython mode\n            # return pandas.Series (np.isin (self._data, values))\n\n            values = str_list_to_array(list(values))\n            values = set(values)\n            data_len = len(self._data)\n            result = numpy.empty(data_len, dtype=numpy.bool_)\n            for i in prange(data_len):\n                result[i] = self._data[i] in values\n\n            return pandas.Series(data=result, index=self._index, name=self._name)\n    else:\n        def hpat_pandas_series_isin_impl(self, values):\n            # TODO: replace with below line when Numba supports np.isin in nopython mode\n            # return pandas.Series (np.isin (self._data, values))\n\n            values = set(values)\n            data_len = len(self._data)\n            result = numpy.empty(data_len, dtype=numpy.bool_)\n            for i in prange(data_len):\n                result[i] = self._data[i] in values\n\n            return pandas.Series(data=result, index=self._index, name=self._name)\n\n    return hpat_pandas_series_isin_impl\n\n\n@sdc_overload_method(SeriesType, \'append\')\ndef hpat_pandas_series_append(self, to_append, ignore_index=False, verify_integrity=False):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.append\n\n    Limitations\n    -----------\n    - Parameter ``verify_integrity`` is currently unsupported by Intel Scalable Dataframe Compiler\n    - Parameter ``ignore_index`` is supported as literal value only\n    - This function may reveal slower performance than Pandas* on user system. Users should exercise a tradeoff\n    between staying in JIT-region with that function or going back to interpreter mode.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_append.py\n       :language: python\n       :lines: 37-\n       :caption: Concatenate two or more Series.\n       :name: ex_series_append\n\n    .. command-output:: python ./series/series_append.py\n       :cwd: ../../../examples\n\n    .. seealso::\n\n        `pandas.absolute\n        <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html#pandas.concat>`_\n            General function to concatenate DataFrame or Series objects.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.append` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_append*\n    """"""\n\n    _func_name = \'Method append().\'\n\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    if not (isinstance(to_append, SeriesType)\n            or (isinstance(to_append, (types.UniTuple, types.List)) and isinstance(to_append.dtype, SeriesType))):\n        ty_checker.raise_exc(to_append, \'series or list/tuple of series\', \'to_append\')\n\n    # currently we will always raise this in the end, i.e. if no impl was found\n    # TODO: find a way to stop compilation early and not proceed with unliteral step\n    if not (isinstance(ignore_index, types.Literal) and isinstance(ignore_index, types.Boolean)\n            or isinstance(ignore_index, types.Omitted)\n            or ignore_index is False):\n        ty_checker.raise_exc(ignore_index, \'literal Boolean constant\', \'ignore_index\')\n\n    if not (verify_integrity is False or isinstance(verify_integrity, types.Omitted)):\n        ty_checker.raise_exc(verify_integrity, \'bool\', \'verify_integrity\')\n\n    # ignore_index value has to be known at compile time to select between implementations with different signatures\n    ignore_index_is_false = (has_literal_value(ignore_index, False)\n                             or has_python_value(ignore_index, False)\n                             or isinstance(ignore_index, types.Omitted))\n    to_append_is_series = isinstance(to_append, SeriesType)\n\n    if ignore_index_is_false:\n        def hpat_pandas_series_append_impl(self, to_append, ignore_index=False, verify_integrity=False):\n            if to_append_is_series == True:  # noqa\n                new_data = common_functions.hpat_arrays_append(self._data, to_append._data)\n                new_index = common_functions.hpat_arrays_append(self.index, to_append.index)\n            else:\n                data_arrays_to_append = [series._data for series in to_append]\n                index_arrays_to_append = [series.index for series in to_append]\n                new_data = common_functions.hpat_arrays_append(self._data, data_arrays_to_append)\n                new_index = common_functions.hpat_arrays_append(self.index, index_arrays_to_append)\n\n            return pandas.Series(new_data, new_index)\n\n        return hpat_pandas_series_append_impl\n\n    else:\n        def hpat_pandas_series_append_ignore_index_impl(self, to_append, ignore_index=False, verify_integrity=False):\n\n            if to_append_is_series == True:  # noqa\n                new_data = common_functions.hpat_arrays_append(self._data, to_append._data)\n            else:\n                arrays_to_append = [series._data for series in to_append]\n                new_data = common_functions.hpat_arrays_append(self._data, arrays_to_append)\n\n            return pandas.Series(new_data, None)\n\n        return hpat_pandas_series_append_ignore_index_impl\n\n\n@sdc_overload_method(SeriesType, \'copy\')\ndef hpat_pandas_series_copy(self, deep=True):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.copy\n\n    Limitations\n    -----------\n    - When ``deep=False``, a new object will be created without copying the calling object\xe2\x80\x99s data\n    and with a copy of the calling object\xe2\x80\x99s indices.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_copy.py\n       :language: python\n       :lines: 27-\n       :caption: Make a copy of this object\xe2\x80\x99s indices and data.\n       :name: ex_series_copy\n\n    .. command-output:: python ./series/series_copy.py\n       :cwd: ../../../examples\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.copy` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests sdc.tests.test_series -k series_copy\n    """"""\n\n    ty_checker = TypeChecker(\'Method Series.copy().\')\n    ty_checker.check(self, SeriesType)\n\n    if not isinstance(deep, (types.Omitted, types.Boolean)) and not deep:\n        ty_checker.raise_exc(deep, \'boolean\', \'deep\')\n\n    if isinstance(self.index, types.NoneType):\n        def hpat_pandas_series_copy_impl(self, deep=True):\n            if deep:\n                return pandas.Series(data=numpy_like.copy(self._data), name=self._name)\n            else:\n                return pandas.Series(data=self._data, name=self._name)\n        return hpat_pandas_series_copy_impl\n    else:\n        def hpat_pandas_series_copy_impl(self, deep=True):\n            if deep:\n                return pandas.Series(data=numpy_like.copy(self._data), index=numpy_like.copy(self._index),\n                                     name=self._name)\n            else:\n                # Shallow copy of index is not supported yet\n                return pandas.Series(data=self._data, index=numpy_like.copy(self._index), name=self._name)\n        return hpat_pandas_series_copy_impl\n\n\n@sdc_overload_method(SeriesType, \'corr\')\ndef hpat_pandas_series_corr(self, other, method=\'pearson\', min_periods=None):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.corr\n\n    Limitations\n    -----------\n    - Parameter ``method`` is supported only with default value \'pearson\'\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_corr.py\n       :language: python\n       :lines: 27-\n       :caption: Compute correlation with other Series, excluding missing values.\n       :name: ex_series_corr\n\n    .. command-output:: python ./series/series_corr.py\n       :cwd: ../../../examples\n\n    .. seealso::\n        :ref:`Series.cov <pandas.Series.cov>`\n            Compute covariance with Series, excluding missing values.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.corr` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_corr*\n    """"""\n\n    _func_name = \'Method corr().\'\n\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    ty_checker.check(other, SeriesType)\n\n    if not isinstance(self.data.dtype, types.Number):\n        ty_checker.raise_exc(self.data, \'number\', \'self.data\')\n\n    if not isinstance(other.data.dtype, types.Number):\n        ty_checker.raise_exc(other.data, \'number\', \'other.data\')\n\n    if not isinstance(min_periods, (int, types.Integer, types.Omitted, types.NoneType)) and min_periods is not None:\n        ty_checker.raise_exc(min_periods, \'int64\', \'min_periods\')\n\n    def hpat_pandas_series_corr_impl(self, other, method=\'pearson\', min_periods=None):\n        return numpy_like.corr(self, other, method, min_periods)\n\n    return hpat_pandas_series_corr_impl\n\n\n@sdc_overload_method(SeriesType, \'head\')\ndef hpat_pandas_series_head(self, n=5):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.head\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_head.py\n       :language: python\n       :lines: 34-\n       :caption: Getting the first n rows.\n       :name: ex_series_head\n\n    .. command-output:: python ./series/series_head.py\n       :cwd: ../../../examples\n\n    .. seealso::\n\n        :ref:`DataFrame.tail <pandas.DataFrame.tail>`\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.head` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_head*\n    """"""\n\n    _func_name = \'Method head().\'\n\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    if not isinstance(n, (types.Integer, types.Omitted, types.NoneType)) and n != 5:\n        ty_checker.raise_exc(n, \'int\', \'n\')\n\n    if isinstance(self.index, types.NoneType):\n        def hpat_pandas_series_head_impl(self, n=5):\n            return pandas.Series(data=self._data[:n], name=self._name)\n\n        return hpat_pandas_series_head_impl\n    else:\n        def hpat_pandas_series_head_index_impl(self, n=5):\n            return pandas.Series(data=self._data[:n], index=self._index[:n], name=self._name)\n\n        return hpat_pandas_series_head_index_impl\n\n\n@sdc_overload_method(SeriesType, \'isnull\')\ndef hpat_pandas_series_isnull(self):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.isnull\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_isnull.py\n       :language: python\n       :lines: 27-\n       :caption: Detect missing values.\n       :name: ex_series_isnull\n\n    .. command-output:: python ./series/series_isnull.py\n       :cwd: ../../../examples\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.isnull` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_isnull*\n    """"""\n\n    _func_name = \'Method isnull().\'\n\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    if isinstance(self.data.dtype, (types.Number, types.Boolean, bool)):\n        def hpat_pandas_series_isnull_impl(self):\n            return pandas.Series(data=numpy_like.isnan(self._data), index=self._index, name=self._name)\n\n        return hpat_pandas_series_isnull_impl\n\n    if isinstance(self.data.dtype, types.UnicodeType):\n        def hpat_pandas_series_isnull_impl(self):\n            result = numpy.empty(len(self._data), numpy.bool_)\n            byte_size = 8\n            # iterate over bits in StringArrayType null_bitmap and fill array indicating if array\'s element are NaN\n            for i in range(len(self._data)):\n                bmap_idx = i // byte_size\n                bit_idx = i % byte_size\n                bmap = self._data.null_bitmap[bmap_idx]\n                bit_value = (bmap >> bit_idx) & 1\n                result[i] = bit_value == 0\n            return pandas.Series(result, index=self._index, name=self._name)\n\n        return hpat_pandas_series_isnull_impl\n\n\n@sdc_overload_method(SeriesType, \'isna\')\ndef hpat_pandas_series_isna(self):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.isna\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_isna.py\n       :language: python\n       :lines: 27-\n       :caption: Detect missing values.\n       :name: ex_series_isna\n\n    .. command-output:: python ./series/series_isna.py\n       :cwd: ../../../examples\n\n    .. seealso::\n\n        :ref:`Series.isnull <pandas.Series.isnull>`\n            Alias of isna.\n\n        :ref:`Series.notna <pandas.Series.notna>`\n            Boolean inverse of isna.\n\n        :ref:`Series.dropna <pandas.Series.dropna>`\n            Omit axes labels with missing values.\n\n        `pandas.absolute <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.isna.html#pandas.isna>`_\n            Top-level isna.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.isna` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_isna*\n    """"""\n\n    _func_name = \'Method isna().\'\n\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    if isinstance(self.data.dtype, (types.Number, types.Boolean, bool)):\n        def hpat_pandas_series_isna_impl(self):\n            return pandas.Series(data=numpy_like.isnan(self._data), index=self._index, name=self._name)\n\n        return hpat_pandas_series_isna_impl\n\n    if isinstance(self.data.dtype, types.UnicodeType):\n        def hpat_pandas_series_isna_impl(self):\n            result = numpy.empty(len(self._data), numpy.bool_)\n            byte_size = 8\n            # iterate over bits in StringArrayType null_bitmap and fill array indicating if array\'s element are NaN\n            for i in range(len(self._data)):\n                bmap_idx = i // byte_size\n                bit_idx = i % byte_size\n                bmap = self._data.null_bitmap[bmap_idx]\n                bit_value = (bmap >> bit_idx) & 1\n                result[i] = bit_value == 0\n            return pandas.Series(result, index=self._index, name=self._name)\n\n        return hpat_pandas_series_isna_impl\n\n\n@sdc_overload_method(SeriesType, \'notna\')\ndef hpat_pandas_series_notna(self):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.notna\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_notna.py\n       :language: python\n       :lines: 27-\n       :caption: Detect existing (non-missing) values.\n       :name: ex_series_notna\n\n    .. command-output:: python ./series/series_notna.py\n       :cwd: ../../../examples\n\n    .. seealso::\n\n        :ref:`Series.notnull <pandas.Series.notnull>`\n            Alias of notna.\n\n        :ref:`Series.isna <pandas.Series.isna>`\n            Boolean inverse of notna.\n\n        :ref:`Series.dropna <pandas.Series.dropna>`\n            Omit axes labels with missing values.\n\n        `pandas.absolute <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.notna.html#pandas.notna>`_\n            Top-level notna.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.notna` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_notna*\n    """"""\n\n    _func_name = \'Method notna().\'\n\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    if isinstance(self.data.dtype, (types.Number, types.Boolean, bool)):\n        def hpat_pandas_series_notna_impl(self):\n            return pandas.Series(numpy_like.notnan(self._data), index=self._index, name=self._name)\n\n        return hpat_pandas_series_notna_impl\n\n    if isinstance(self.data.dtype, types.UnicodeType):\n        def hpat_pandas_series_notna_impl(self):\n            result = self.isna()\n            return pandas.Series(numpy.invert(result._data), index=self._index, name=self._name)\n\n        return hpat_pandas_series_notna_impl\n\n\n@sdc_overload_method(SeriesType, \'sum\')\ndef hpat_pandas_series_sum(\n    self,\n    axis=None,\n    skipna=None,\n    level=None,\n    numeric_only=None,\n    min_count=0,\n):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.sum\n\n    Limitations\n    -----------\n    - Parameters ``axis``, ``level``, ``numeric_only`` and ``min_count`` \\\n        are currently unsupported by Intel Scalable Dataframe Compiler\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_sum.py\n       :language: python\n       :lines: 27-\n       :caption: Return the sum of the values for the requested axis.\n       :name: ex_series_sum\n\n    .. command-output:: python ./series/series_sum.py\n       :cwd: ../../../examples\n\n    .. seealso::\n\n        :ref:`Series.sum <pandas.Series.sum>`\n            Return the sum.\n\n        :ref:`Series.min <pandas.Series.min>`\n            Return the minimum.\n\n        :ref:`Series.max <pandas.Series.max>`\n            Return the maximum.\n\n        :ref:`Series.idxmin <pandas.Series.idxmin>`\n            Return the index of the minimum.\n\n        :ref:`Series.idxmax <pandas.Series.idxmax>`\n            Return the index of the maximum.\n\n        :ref:`DataFrame.sum <pandas.DataFrame.sum>`\n            Return the sum over the requested axis.\n\n        :ref:`DataFrame.min <pandas.DataFrame.min>`\n            Return the minimum over the requested axis.\n\n        :ref:`DataFrame.max <pandas.DataFrame.max>`\n            Return the maximum over the requested axis.\n\n        :ref:`DataFrame.idxmin <pandas.DataFrame.idxmin>`\n            Return the index of the minimum over the requested axis.\n\n        :ref:`DataFrame.idxmax <pandas.DataFrame.idxmax>`\n            Return index of first occurrence of maximum over requested axis.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.sum` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests sdc.tests.test_series -k series_sum\n    """"""\n\n    _func_name = \'Method sum().\'\n\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    if not (isinstance(axis, (types.Integer, types.Omitted)) or axis is None):\n        ty_checker.raise_exc(axis, \'int\', \'axis\')\n\n    if not (isinstance(skipna, (types.Boolean, types.Omitted, types.NoneType)) or skipna is None):\n        ty_checker.raise_exc(skipna, \'bool\', \'skipna\')\n\n    if not (isinstance(level, (types.Integer, types.StringLiteral, types.Omitted, types.NoneType)) or level is None):\n        ty_checker.raise_exc(level, \'int or str\', \'level\')\n\n    if not (isinstance(numeric_only, (types.Boolean, types.Omitted)) or numeric_only is None):\n        ty_checker.raise_exc(numeric_only, \'bool\', \'numeric_only\')\n\n    if not (isinstance(min_count, (types.Integer, types.Omitted)) or min_count == 0):\n        ty_checker.raise_exc(min_count, \'int\', \'min_count\')\n\n    def hpat_pandas_series_sum_impl(\n        self,\n        axis=None,\n        skipna=None,\n        level=None,\n        numeric_only=None,\n        min_count=0,\n    ):\n\n        if skipna is None:\n            _skipna = True\n        else:\n            _skipna = skipna\n\n        if _skipna:\n            return numpy_like.nansum(self._data)\n        return numpy_like.sum(self._data)\n\n    return hpat_pandas_series_sum_impl\n\n\n@sdc_overload_method(SeriesType, \'take\')\ndef hpat_pandas_series_take(self, indices, axis=0, is_copy=False):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.take\n\n    Limitations\n    -----------\n    Parameter ``axis`` is supported only with default values ``0`` and ``\'index\'``.\n    Parameter ``is_copy`` is supported only with default value ``False``.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_take.py\n       :language: python\n       :lines: 27-\n       :caption: Return the elements in the given positional indices along an axis.\n       :name: ex_series_take\n\n    .. command-output:: python ./series/series_take.py\n       :cwd: ../../../examples\n\n    .. seealso::\n        :ref:`DataFrame.loc <pandas.DataFrame.loc>`\n            Select a subset of a DataFrame by labels.\n        :ref:`DataFrame.iloc <pandas.DataFrame.iloc>`\n            Select a subset of a DataFrame by positions.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n\n    Pandas Series method :meth:`pandas.Series.take` implementation.\n\n    .. only:: developer\n\n        Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_take_index_*\n    """"""\n\n    _func_name = \'Method take().\'\n\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    if (not isinstance(axis, (int, types.Integer, str, types.UnicodeType, types.StringLiteral, types.Omitted))\n        and axis not in (0, \'index\')):\n        ty_checker.raise_exc(axis, \'integer or string\', \'axis\')\n\n    if not isinstance(is_copy, (bool, types.Boolean, types.Omitted)) and is_copy is not False:\n        ty_checker.raise_exc(is_copy, \'boolean\', \'is_copy\')\n\n    if not isinstance(indices, (types.List, types.Array)):\n        ty_checker.raise_exc(indices, \'array-like\', \'indices\')\n\n    if isinstance(self.index, types.NoneType) or self.index is None:\n        def hpat_pandas_series_take_noindex_impl(self, indices, axis=0, is_copy=False):\n            local_data = [self._data[i] for i in indices]\n\n            return pandas.Series(local_data, indices)\n\n        return hpat_pandas_series_take_noindex_impl\n\n    def hpat_pandas_series_take_impl(self, indices, axis=0, is_copy=False):\n        local_data = [self._data[i] for i in indices]\n        local_index = [self._index[i] for i in indices]\n\n        return pandas.Series(local_data, local_index)\n\n    return hpat_pandas_series_take_impl\n\n\n@sdc_overload_method(SeriesType, \'idxmax\')\ndef hpat_pandas_series_idxmax(self, axis=None, skipna=None):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.idxmax\n\n    Limitations\n    -----------\n    Parameter ``axis`` is supported only with default value ``None``.\n    Parameter ``skipna`` cannot be ``False`` with data of string type.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_idxmax.py\n       :language: python\n       :lines: 27-\n       :caption: Getting the row label of the maximum value.\n       :name: ex_series_idxmax\n\n    .. command-output:: python ./series/series_idxmax.py\n       :cwd: ../../../examples\n\n    .. seealso::\n\n        :ref:`Series.idxmin <pandas.Series.idxmin>`\n            Return index label of the first occurrence of minimum of values.\n\n        `numpy.absolute <https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.html#numpy.argmax>`_\n            Return indices of the maximum values along the given axis.\n\n        :ref:`DataFrame.idxmax <pandas.DataFrame.idxmax>`\n            Return index of first occurrence of maximum over requested axis.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.idxmax` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_idxmax*\n    """"""\n\n    _func_name = \'Method idxmax().\'\n\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    if not isinstance(self.data.dtype, types.Number):\n        ty_checker.raise_exc(self.data.dtype, \'int, float\', \'self.data.dtype\')\n\n    if not (isinstance(skipna, (types.Omitted, types.Boolean, bool)) or skipna is None):\n        ty_checker.raise_exc(skipna, \'bool\', \'skipna\')\n\n    if not (isinstance(axis, types.Omitted) or axis is None):\n        ty_checker.raise_exc(axis, \'None\', \'axis\')\n\n    none_index = isinstance(self.index, types.NoneType) or self.index is None\n    if isinstance(self.data, StringArrayType):\n        def hpat_pandas_series_idxmax_str_impl(self, axis=None, skipna=None):\n            if skipna is None:\n                _skipna = True\n            else:\n                raise ValueError(""Method idxmax(). Unsupported parameter \'skipna\'=False with str data"")\n\n            result = numpy.argmax(self._data)\n            if none_index == True:  # noqa\n                return result\n            else:\n                return self._index[int(result)]\n\n        return hpat_pandas_series_idxmax_str_impl\n\n    def hpat_pandas_series_idxmax_impl(self, axis=None, skipna=None):\n        # return numpy.argmax(self._data)\n        if skipna is None:\n            _skipna = True\n        else:\n            _skipna = skipna\n\n        if _skipna:\n            result = numpy_like.nanargmax(self._data)\n        else:\n            result = numpy_like.argmax(self._data)\n\n        if none_index == True:  # noqa\n            return result\n        else:\n            return self._index[int(result)]\n\n        return numpy_like.argmax(self._data)\n\n    return hpat_pandas_series_idxmax_impl\n\n\n@sdc_overload_method(SeriesType, \'prod\')\ndef hpat_pandas_series_prod(self, axis=None, skipna=None, level=None, numeric_only=None, min_count=0):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.prod\n\n    Limitations\n    -----------\n    - Parameters ``axis``, ``level``, ``numeric_only`` and ``min_count`` \\\n        are currently unsupported by Intel Scalable Dataframe Compiler\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_prod.py\n       :language: python\n       :lines: 27-\n       :caption: Return the product of the values.\n       :name: ex_series_prod\n\n    .. command-output:: python ./series/series_prod.py\n       :cwd: ../../../examples\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.prod` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests sdc.tests.test_series -k series_prod\n    """"""\n\n    _func_name = \'Method prod().\'\n\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    if not isinstance(self.data.dtype, (types.Integer, types.Float)):\n        ty_checker.raise_exc(self.data.dtype, \'numeric\', \'self.data.dtype\')\n\n    if not (isinstance(axis, (types.Integer, types.Omitted)) or axis is None):\n        ty_checker.raise_exc(axis, \'int\', \'axis\')\n\n    if not (isinstance(skipna, (types.Omitted, types.Boolean, types.NoneType)) or skipna is None or skipna is True):\n        ty_checker.raise_exc(skipna, \'bool\', \'skipna\')\n\n    if not (isinstance(level, (types.Integer, types.StringLiteral, types.Omitted, types.NoneType)) or level is None):\n        ty_checker.raise_exc(level, \'int or str\', \'level\')\n\n    if not (isinstance(numeric_only, (types.Boolean, types.Omitted)) or numeric_only is None):\n        ty_checker.raise_exc(numeric_only, \'bool\', \'numeric_only\')\n\n    if not (isinstance(min_count, (types.Integer, types.Omitted)) or min_count == 0):\n        ty_checker.raise_exc(min_count, \'int\', \'min_count\')\n\n    def hpat_pandas_series_prod_impl(self, axis=None, skipna=None, level=None, numeric_only=None, min_count=0):\n        if skipna is None:\n            _skipna = True\n        else:\n            _skipna = skipna\n\n        if _skipna:\n            return numpy_like.nanprod(self._data)\n        else:\n            return numpy.prod(self._data)\n\n    return hpat_pandas_series_prod_impl\n\n\n@sdc_overload_method(SeriesType, \'quantile\')\ndef hpat_pandas_series_quantile(self, q=0.5, interpolation=\'linear\'):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.quantile\n\n    Limitations\n    -----------\n    Parameter ``interpolation`` is currently unsupported.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_quantile.py\n       :language: python\n       :lines: 27-\n       :caption: Computing quantile for the Series\n       :name: ex_series_quantile\n\n    .. command-output:: python ./series/series_quantile.py\n       :cwd: ../../../examples\n\n    .. seealso::\n\n        :ref:`core.window.Rolling.quantile <pandas.core.window.Rolling.quantile>`\n            Calculate the rolling quantile.\n\n        `numpy.percentile\n        <https://docs.scipy.org/doc/numpy/reference/generated/numpy.percentile.html#numpy.percentile>`_\n            Compute the q-th percentile of the data along the specified axis.\n\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.quantile` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_quantile*\n    """"""\n\n    _func_name = \'Method quantile().\'\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    if not isinstance(interpolation, types.Omitted) and interpolation != \'linear\':\n        ty_checker.raise_exc(interpolation, \'str\', \'interpolation\')\n\n    if not isinstance(q, (int, float, list, types.Number, types.Omitted, types.List)):\n        ty_checker.raise_exc(q, \'int, float, list\', \'q\')\n\n    def hpat_pandas_series_quantile_impl(self, q=0.5, interpolation=\'linear\'):\n\n        return numpy.quantile(self._data, q)\n\n    return hpat_pandas_series_quantile_impl\n\n\n@sdc_overload_method(SeriesType, \'rename\')\ndef hpat_pandas_series_rename(self, index=None, copy=True, inplace=False, level=None):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.rename\n\n    Limitations\n    -----------\n    - Parameter ``level`` is currently unsupported by Intel Scalable Dataframe Compiler.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_rename.py\n       :language: python\n       :lines: 36-\n       :caption: Alter Series index labels or name.\n       :name: ex_series_rename\n\n    .. command-output:: python ./series/series_rename.py\n       :cwd: ../../../examples\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.rename` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_rename\n    """"""\n\n    ty_checker = TypeChecker(\'Method rename().\')\n    ty_checker.check(self, SeriesType)\n\n    if not isinstance(index, (types.Omitted, types.UnicodeType,\n                              types.StringLiteral, str,\n                              types.Integer, types.Boolean,\n                              types.Hashable, types.Float,\n                              types.NPDatetime, types.NPTimedelta,\n                              types.Number)) and index is not None:\n        ty_checker.raise_exc(index, \'string\', \'index\')\n\n    if not isinstance(copy, (types.Omitted, types.Boolean, bool)):\n        ty_checker.raise_exc(copy, \'boolean\', \'copy\')\n\n    if not isinstance(inplace, (types.Omitted, types.Boolean, bool)):\n        ty_checker.raise_exc(inplace, \'boolean\', \'inplace\')\n\n    if not isinstance(level, (types.Omitted, types.UnicodeType,\n                              types.StringLiteral, types.Integer)) and level is not None:\n        ty_checker.raise_exc(level, \'Integer or string\', \'level\')\n\n    def hpat_pandas_series_rename_idx_impl(self, index=None, copy=True, inplace=False, level=None):\n        if copy is True:\n            series_data = self._data.copy()\n            series_index = self._index.copy()\n        else:\n            series_data = self._data\n            series_index = self._index\n\n        return pandas.Series(data=series_data, index=series_index, name=index)\n\n    def hpat_pandas_series_rename_noidx_impl(self, index=None, copy=True, inplace=False, level=None):\n        if copy is True:\n            series_data = self._data.copy()\n        else:\n            series_data = self._data\n\n        return pandas.Series(data=series_data, index=self._index, name=index)\n\n    if isinstance(self.index, types.NoneType):\n        return hpat_pandas_series_rename_noidx_impl\n    return hpat_pandas_series_rename_idx_impl\n\n\n@sdc_overload_method(SeriesType, \'min\')\ndef hpat_pandas_series_min(self, axis=None, skipna=None, level=None, numeric_only=None):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.min\n\n    Limitations\n    -----------\n    Parameters ``level``, ``numeric_only`` and ``axis`` are currently unsupported by Intel Scalable Dataframe Compiler.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_min.py\n       :language: python\n       :lines: 27-\n       :caption: Getting the minimum value of Series elements\n       :name: ex_series_min\n\n    .. command-output:: python ./series/series_min.py\n       :cwd: ../../../examples\n\n    .. seealso::\n\n        :ref:`Series.sum <pandas.Series.sum>`\n            Return the sum.\n\n        :ref:`Series.min <pandas.Series.min>`\n            Return the minimum.\n\n        :ref:`Series.max <pandas.Series.max>`\n            Return the maximum.\n\n        :ref:`Series.idxmin <pandas.Series.idxmin>`\n            Return the index of the minimum.\n\n        :ref:`Series.idxmax <pandas.Series.idxmax>`\n            Return the index of the maximum.\n\n        :ref:`DataFrame.sum <pandas.DataFrame.sum>`\n            Return the sum over the requested axis.\n\n        :ref:`DataFrame.min <pandas.DataFrame.min>`\n            Return the minimum over the requested axis.\n\n        :ref:`DataFrame.max <pandas.DataFrame.max>`\n            Return the maximum over the requested axis.\n\n        :ref:`DataFrame.idxmin <pandas.DataFrame.idxmin>`\n            Return the index of the minimum over the requested axis.\n\n        :ref:`DataFrame.idxmax <pandas.DataFrame.idxmax>`\n            Return the index of the maximum over the requested axis.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.min` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_min*\n    """"""\n\n    _func_name = \'Method min().\'\n\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    if not isinstance(self.data.dtype, (types.Integer, types.Float)):\n        ty_checker.raise_exc(self.data.dtype, \'int, float\', \'self.data.dtype\')\n\n    if not (isinstance(skipna, (types.Omitted, types.Boolean, types.NoneType)) or skipna is True or skipna is None):\n        ty_checker.raise_exc(skipna, \'bool\', \'skipna\')\n\n    if not isinstance(axis, types.Omitted) and axis is not None:\n        ty_checker.raise_exc(axis, \'None\', \'axis\')\n\n    if not isinstance(level, (types.Omitted, types.NoneType)) and level is not None:\n        ty_checker.raise_exc(level, \'None\', \'level\')\n\n    if not isinstance(numeric_only, types.Omitted) and numeric_only is not None:\n        ty_checker.raise_exc(numeric_only, \'None\', \'numeric_only\')\n\n    def hpat_pandas_series_min_impl(self, axis=None, skipna=None, level=None, numeric_only=None):\n        if skipna is None:\n            _skipna = True\n        else:\n            _skipna = skipna\n\n        if _skipna:\n            return numpy_like.nanmin(self._data)\n\n        return self._data.min()\n\n    return hpat_pandas_series_min_impl\n\n\n@sdc_overload_method(SeriesType, \'max\')\ndef hpat_pandas_series_max(self, axis=None, skipna=None, level=None, numeric_only=None):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.max\n\n    Limitations\n    -----------\n    Parameters ``axis``, ``level`` and ``numeric_only`` are currently unsupported.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_max.py\n       :language: python\n       :lines: 27-\n       :caption: Getting the maximum value of Series elements\n       :name: ex_series_max\n\n    .. command-output:: python ./series/series_max.py\n       :cwd: ../../../examples\n\n    .. seealso::\n\n        :ref:`Series.sum <pandas.Series.sum>`\n            Return the sum.\n        :ref:`Series.min <pandas.Series.min>`\n            Return the minimum.\n        :ref:`Series.max <pandas.Series.max>`\n            Return the maximum.\n        :ref:`Series.idxmin <pandas.Series.idxmin>`\n            Return the index of the minimum.\n        :ref:`Series.idxmax <pandas.Series.idxmax>`\n            Return the index of the maximum.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.max` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_max*\n    """"""\n\n    _func_name = \'Method max().\'\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    if not isinstance(self.data.dtype, (types.Integer, types.Float)):\n        ty_checker.raise_exc(self.data.dtype, \'int, float\', \'self.data.dtype\')\n\n    if not (isinstance(skipna, (types.Omitted, types.Boolean, types.NoneType)) or skipna is True or skipna is None):\n        ty_checker.raise_exc(skipna, \'bool\', \'skipna\')\n\n    if not isinstance(axis, types.Omitted) and axis is not None:\n        ty_checker.raise_exc(axis, \'None\', \'axis\')\n\n    if not isinstance(level, (types.Omitted, types.NoneType)) and level is not None:\n        ty_checker.raise_exc(level, \'None\', \'level\')\n\n    if not isinstance(numeric_only, types.Omitted) and numeric_only is not None:\n        ty_checker.raise_exc(numeric_only, \'None\', \'numeric_only\')\n\n    def hpat_pandas_series_max_impl(self, axis=None, skipna=None, level=None, numeric_only=None):\n        if skipna is None:\n            _skipna = True\n        else:\n            _skipna = skipna\n\n        if _skipna:\n            return numpy_like.nanmax(self._data)\n\n        return self._data.max()\n\n    return hpat_pandas_series_max_impl\n\n\n@sdc_overload_method(SeriesType, \'mean\')\ndef hpat_pandas_series_mean(self, axis=None, skipna=None, level=None, numeric_only=None):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.mean\n\n    Limitations\n    -----------\n    - Parameters ``axis``, ``level`` and ``numeric_only`` \\\n        are currently unsupported by Intel Scalable Dataframe Compiler.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_mean.py\n       :language: python\n       :lines: 27-\n       :caption: Return the mean of the values.\n       :name: ex_series_mean\n\n    .. command-output:: python ./series/series_mean.py\n       :cwd: ../../../examples\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.mean` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests sdc.tests.test_series -k series_mean\n    """"""\n\n    _func_name = \'Method mean().\'\n\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    if not isinstance(self.data.dtype, types.Number):\n        ty_checker.raise_exc(self.data.dtype, \'numeric\', \'self.data.dtype\')\n\n    if not isinstance(skipna, (types.Omitted, types.Boolean, types.NoneType)) and skipna is not None:\n        ty_checker.raise_exc(skipna, \'bool\', \'skipna\')\n\n    if not isinstance(axis, types.Omitted) and axis is not None:\n        ty_checker.raise_exc(axis, \'None\', \'axis\')\n\n    if not isinstance(level, (types.Omitted, types.NoneType)) and level is not None:\n        ty_checker.raise_exc(level, \'None\', \'level\')\n\n    if not isinstance(numeric_only, types.Omitted) and numeric_only is not None:\n        ty_checker.raise_exc(numeric_only, \'None\', \'numeric_only\')\n\n    def hpat_pandas_series_mean_impl(self, axis=None, skipna=None, level=None, numeric_only=None):\n        if skipna is None:\n            _skipna = True\n        else:\n            _skipna = skipna\n\n        if _skipna:\n            return numpy_like.nanmean(self._data)\n\n        return self._data.mean()\n\n    return hpat_pandas_series_mean_impl\n\n\n@sdc_overload_method(SeriesType, \'idxmin\')\ndef hpat_pandas_series_idxmin(self, axis=None, skipna=None):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.idxmin\n\n    Limitations\n    -----------\n    - Parameter ``axis`` is supported only with default value ``None``.\n    - Parameter ``skipna`` cannot be ``False`` with data of string type.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_idxmin.py\n       :language: python\n       :lines: 27-\n       :caption: Getting the row label of the minimum value.\n       :name: ex_series_idxmin\n\n    .. command-output:: python ./series/series_idxmin.py\n       :cwd: ../../../examples\n\n    .. seealso::\n\n        :ref:`Series.idxmax <pandas.Series.idxmax>`\n            Return index label of the first occurrence of maximum of values.\n\n        `numpy.argmin <https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmin.html#numpy.argmin>`_\n            Return indices of the minimum values along the given axis.\n\n        :ref:`DataFrame.idxmin <pandas.DataFrame.idxmin>`\n            Return index of first occurrence of minimum over requested axis.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.idxmin` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_idxmin*\n    """"""\n\n    _func_name = \'Method idxmin().\'\n\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    if not isinstance(self.data.dtype, types.Number):\n        ty_checker.raise_exc(self.data.dtype, \'int, float\', \'self.data.dtype\')\n\n    if not (isinstance(skipna, (types.Omitted, types.Boolean, bool)) or skipna is None):\n        ty_checker.raise_exc(skipna, \'bool\', \'skipna\')\n\n    if not (isinstance(axis, types.Omitted) or axis is None):\n        ty_checker.raise_exc(axis, \'None\', \'axis\')\n\n    none_index = isinstance(self.index, types.NoneType) or self.index is None\n    if isinstance(self.data, StringArrayType):\n        def hpat_pandas_series_idxmin_str_impl(self, axis=None, skipna=None):\n            if skipna is None:\n                _skipna = True\n            else:\n                raise ValueError(""Method idxmin(). Unsupported parameter \'skipna\'=False with str data"")\n\n            result = numpy.argmin(self._data)\n            if none_index == True:  # noqa\n                return result\n            else:\n                return self._index[int(result)]\n\n        return hpat_pandas_series_idxmin_str_impl\n\n    def hpat_pandas_series_idxmin_impl(self, axis=None, skipna=None):\n        # return numpy.argmin(self._data)\n        if skipna is None:\n            _skipna = True\n        else:\n            _skipna = skipna\n\n        if _skipna:\n            result = numpy_like.nanargmin(self._data)\n        else:\n            result = numpy_like.argmin(self._data)\n\n        if none_index == True:  # noqa\n            return result\n        else:\n            return self._index[int(result)]\n\n        return numpy_like.argmin(self._data)\n\n    return hpat_pandas_series_idxmin_impl\n\n\n@sdc_overload_method(SeriesType, \'abs\')\ndef hpat_pandas_series_abs(self):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.abs\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_abs.py\n       :language: python\n       :lines: 27-\n       :caption: Getting the absolute value of each element in Series\n       :name: ex_series_abs\n\n    .. command-output:: python ./series/series_abs.py\n       :cwd: ../../../examples\n\n    .. seealso::\n\n        `numpy.absolute <https://docs.scipy.org/doc/numpy/reference/generated/numpy.absolute.html>`_\n            Calculate the absolute value element-wise.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.abs` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests sdc.tests.test_series.TestSeries.test_series_abs1\n    """"""\n\n    _func_name = \'Method abs().\'\n\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    if not isinstance(self.dtype, (types.Integer, types.Float)):\n        raise TypingError(\n            \'{} The function only applies to elements that are all numeric. Given data type: {}\'.format(_func_name,\n                                                                                                        self.dtype))\n\n    def hpat_pandas_series_abs_impl(self):\n        return pandas.Series(numpy.abs(self._data))\n\n    return hpat_pandas_series_abs_impl\n\n\n@sdc_overload_method(SeriesType, \'unique\')\ndef hpat_pandas_series_unique(self):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.unique\n\n    Limitations\n    -----------\n    - Return values order is unspecified\n    - This function may reveal slower performance than Pandas* on user system. Users should exercise a tradeoff\n    between staying in JIT-region with that function or going back to interpreter mode.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_unique.py\n       :language: python\n       :lines: 27-\n       :caption: Getting unique values in Series\n       :name: ex_series_unique\n\n    .. command-output:: python ./series/series_unique.py\n       :cwd: ../../../examples\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.unique` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests sdc.tests.test_series.TestSeries.test_unique_sorted\n    """"""\n\n    ty_checker = TypeChecker(\'Method unique().\')\n    ty_checker.check(self, SeriesType)\n\n    if isinstance(self.data, StringArrayType):\n        def hpat_pandas_series_unique_str_impl(self):\n            \'\'\'\n            Returns sorted unique elements of an array\n            Note: Can\'t use Numpy due to StringArrayType has no ravel() for noPython mode.\n            Also, NotImplementedError: unicode_type cannot be represented as a Numpy dtype\n\n            Test: python -m sdc.runtests sdc.tests.test_series.TestSeries.test_unique_str\n            \'\'\'\n\n            str_set = set(self._data)\n            return to_array(str_set)\n\n        return hpat_pandas_series_unique_str_impl\n\n    def hpat_pandas_series_unique_impl(self):\n        \'\'\'\n        Returns sorted unique elements of an array\n\n        Test: python -m sdc.runtests sdc.tests.test_series.TestSeries.test_unique\n        \'\'\'\n\n        return numpy.unique(self._data)\n\n    return hpat_pandas_series_unique_impl\n\n\n@sdc_overload_method(SeriesType, \'cumsum\')\ndef hpat_pandas_series_cumsum(self, axis=None, skipna=True):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.cumsum\n\n    Limitations\n    -----------\n    Parameter ``axis`` is supported only with default value ``None``.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_cumsum.py\n       :language: python\n       :lines: 27-\n       :caption: Returns cumulative sum over Series.\n       :name: ex_series_cumsum\n\n    .. command-output:: python ./series/series_cumsum.py\n       :cwd: ../../../examples\n\n    .. seealso::\n        :ref:`Series.sum <pandas.Series.sum>`\n            Return the sum over Series.\n        :ref:`Series.cummax <pandas.Series.cummax>`\n            Return cumulative maximum over Series.\n        :ref:`Series.cummin <pandas.Series.cummin>`\n            Return cumulative minimum over Series.\n        :ref:`Series.cumprod <pandas.Series.cumprod>`\n            Return cumulative product over Series.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n\n    Pandas Series method :meth:`pandas.Series.cumsum` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_cumsum*\n    """"""\n\n    _func_name = \'Method cumsum().\'\n\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    if not isinstance(self.data.dtype, types.Number):\n        ty_checker.raise_exc(self.data.dtype, \'numeric\', \'self.data.dtype\')\n\n    if not isinstance(axis, (types.Omitted, types.NoneType)) and axis is not None:\n        ty_checker.raise_exc(axis, \'None\', \'axis\')\n\n    def hpat_pandas_series_cumsum_impl(self, axis=None, skipna=True):\n        if skipna:\n            return pandas.Series(numpy_like.nancumsum(self._data, like_pandas=True))\n        return pandas.Series(numpy_like.cumsum(self._data))\n\n    return hpat_pandas_series_cumsum_impl\n\n\n@sdc_overload_method(SeriesType, \'nunique\')\ndef hpat_pandas_series_nunique(self, dropna=True):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.nunique\n\n    Limitations\n    -----------\n    - This function may reveal slower performance than Pandas* on user system. Users should exercise a tradeoff\n    between staying in JIT-region with that function or going back to interpreter mode.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_nunique.py\n       :language: python\n       :lines: 27-\n       :caption: Return number of unique elements in the object.\n       :name: ex_series_nunique\n\n    .. command-output:: python ./series/series_nunique.py\n       :cwd: ../../../examples\n\n    .. seealso::\n        :ref:`DataFrame.nunique <pandas.DataFrame.nunique>`\n            Method nunique for DataFrame.\n        :ref:`Series.count <pandas.Series.count>`\n            Count non-NA/null observations in the Series.\n        :ref:`DatatFrame.count <pandas.DataFrame.count>`\n            Count non-NA cells for each column\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.nunique` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests sdc.tests.test_series.TestSeries.test_series_nunique\n    """"""\n\n    _func_name = \'Method nunique().\'\n\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    if isinstance(self.data, StringArrayType):\n        def hpat_pandas_series_nunique_str_impl(self, dropna=True):\n            """"""\n            It is better to merge with Numeric branch\n            """"""\n            data = self._data\n            if dropna:\n                nan_mask = self.isna()\n                data = self._data[~nan_mask._data]\n            unique_values = set(data)\n            return len(unique_values)\n\n        return hpat_pandas_series_nunique_str_impl\n\n    def hpat_pandas_series_nunique_impl(self, dropna=True):\n        """"""\n        This function for Numeric data because NumPy dosn\'t support StringArrayType\n        Algo looks a bit ambigous because, currently, set() can not be used with NumPy with Numba JIT\n        """"""\n        data_mask_for_nan = numpy.isnan(self._data)\n        nan_exists = numpy.any(data_mask_for_nan)\n        data_no_nan = self._data[~data_mask_for_nan]\n        data_set = set(data_no_nan)\n        if dropna or not nan_exists:\n            return len(data_set)\n        else:\n            return len(data_set) + 1\n\n    return hpat_pandas_series_nunique_impl\n\n\n@sdc_overload_method(SeriesType, \'count\')\ndef hpat_pandas_series_count(self, level=None):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.count\n\n    Limitations\n    -----------\n    Parameter ``level`` is currently unsupported.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_count.py\n       :language: python\n       :lines: 27-\n       :caption: Counting non-NaN values in Series\n       :name: ex_series_count\n\n    .. command-output:: python ./series/series_count.py\n       :cwd: ../../../examples\n\n    .. seealso::\n\n        :ref:`Series.value_counts <pandas.Series.value_counts>`\n            Return a Series containing counts of unique values.\n\n        :ref:`Series.str.len <pandas.Series.str.len>`\n            Count the length of each element in the Series.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.count` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests sdc.tests.test_series.TestSeries.test_series_count\n    """"""\n\n    _func_name = \'Method count().\'\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    if not isinstance(level, (types.Omitted, types.NoneType)) and level is not None:\n        ty_checker.raise_exc(level, \'None\', \'level\')\n\n    if isinstance(self.data, StringArrayType):\n        def hpat_pandas_series_count_str_impl(self, level=None):\n\n            nan_mask = self.isna()\n            return numpy.sum(nan_mask._data == 0)\n\n        return hpat_pandas_series_count_str_impl\n\n    if isinstance(self.data, types.Array) and isinstance(self.data.dtype, types.Integer):\n        def hpat_pandas_series_count_int_impl(self, level=None):\n            return len(self._data)\n        return hpat_pandas_series_count_int_impl\n\n    def hpat_pandas_series_count_impl(self, level=None):\n        """"""\n        Return number of non-NA/null observations in the object\n        Returns number of unique elements in the object\n        Test: python -m sdc.runtests sdc.tests.test_series.TestSeries.test_series_count\n        """"""\n        result = 0\n        for i in prange(len(self._data)):\n            if not numpy.isnan(self._data[i]):\n                result = result + 1\n        return result\n\n    return hpat_pandas_series_count_impl\n\n\n@sdc_overload_method(SeriesType, \'median\')\ndef hpat_pandas_series_median(self, axis=None, skipna=None, level=None, numeric_only=None):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.median\n\n    Limitations\n    -----------\n    - Parameters ``axis``, ``level`` and ``numeric_only`` are supported only with default value ``None``.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_median.py\n       :language: python\n       :lines: 27-\n       :caption: Return the median of the values for the requested axis.\n       :name: ex_series_median\n\n    .. command-output:: python ./series/series_median.py\n       :cwd: ../../../examples\n\n    .. seealso::\n        :ref:`DataFrame.median <pandas.DataFrame.median>`\n            Return the median of the values for the columns.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.median` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_median1*\n    """"""\n\n    _func_name = \'Method median().\'\n\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    if not isinstance(self.dtype, types.Number):\n        ty_checker.raise_exc(self.dtype, \'numeric\', \'self.dtype\')\n\n    if not (isinstance(axis, (types.Integer, types.UnicodeType, types.Omitted)) or axis is None):\n        ty_checker.raise_exc(axis, \'int or str\', \'axis\')\n\n    if not (isinstance(skipna, (types.Boolean, types.Omitted, types.NoneType)) or skipna or skipna is None):\n        ty_checker.raise_exc(skipna, \'bool\', \'skipna\')\n\n    if not isinstance(axis, types.Omitted) and axis is not None:\n        ty_checker.raise_exc(axis, \'None\', \'axis\')\n\n    if not isinstance(level, (types.Omitted, types.NoneType)) and level is not None:\n        ty_checker.raise_exc(level, \'None\', \'level\')\n\n    if not isinstance(numeric_only, types.Omitted) and numeric_only is not None:\n        ty_checker.raise_exc(numeric_only, \'None\', \'numeric_only\')\n\n    def hpat_pandas_series_median_impl(self, axis=None, skipna=None, level=None, numeric_only=None):\n        if skipna is None:\n            _skipna = True\n        else:\n            _skipna = skipna\n\n        if _skipna:\n            return numpy.nanmedian(self._data)\n\n        return numpy.median(self._data)\n\n    return hpat_pandas_series_median_impl\n\n\n@sdc_overload_method(SeriesType, \'argsort\')\ndef hpat_pandas_series_argsort(self, axis=0, kind=\'quicksort\', order=None):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.argsort\n\n    Limitations\n    -----------\n    - Parameter ``axis`` is supported only with default value ``0``.\n    - Parameter ``order`` is supported only with default value ``None``.\n    - Parameter ``kind`` is supported only with values ``\'mergesort\'`` and ``\'quicksort\'``.\n    - This function may reveal slower performance than Pandas* on user system. Users should exercise a tradeoff\n    between staying in JIT-region with that function or going back to interpreter mode.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_argsort.py\n       :language: python\n       :lines: 27-\n       :caption: Override ndarray.argsort.\n       :name: ex_series_argsort\n\n    .. command-output:: python ./series/series_argsort.py\n       :cwd: ../../../examples\n\n    .. seealso::\n\n        `numpy.ndarray.argsort\n        <https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.argsort.html#numpy.ndarray.argsort>`_\n            Return indices of the minimum values along the given axis.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.argsort` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_argsort*\n    """"""\n\n    _func_name = \'Method argsort().\'\n\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    if not isinstance(self.data.dtype, types.Number):\n        ty_checker.raise_exc(self.data.dtype, \'int, float\', \'self.data.dtype\')\n\n    if not (isinstance(axis, types.Omitted) or isinstance(axis, types.Integer) or axis == 0):\n        ty_checker.raise_exc(axis, \'int64\', \'axis\')\n\n    if not isinstance(kind, (types.Omitted, str, types.UnicodeType, types.StringLiteral)):\n        ty_checker.raise_exc(kind, \'quicksort\', \'kind\')\n\n    if not isinstance(order, (str, types.UnicodeType, types.StringLiteral, types.Omitted, types.NoneType, types.List))\\\n            and order is not None:\n        ty_checker.raise_exc(order, \'None\', \'order\')\n\n    if not isinstance(self.index, types.NoneType):\n        def hpat_pandas_series_argsort_idx_impl(self, axis=0, kind=\'quicksort\', order=None):\n            if kind != \'quicksort\' and kind != \'mergesort\':\n                raise ValueError(""Method argsort(). Unsupported parameter. Given \'kind\' != \'quicksort\' or \'mergesort\'"")\n            if kind == \'mergesort\':\n                #It is impossible to use numpy.argsort(self._data, kind=kind) since numba gives typing error\n                sort = numpy.argsort(self._data, kind=\'mergesort\')\n            else:\n                sort = numpy.argsort(self._data)\n            na = self.isna().sum()\n            result = numpy.empty(len(self._data), dtype=numpy.int64)\n            na_data_arr = sdc.hiframes.api.get_nan_mask(self._data)\n            if kind == \'mergesort\':\n                sort_nona = numpy.argsort(self._data[~na_data_arr], kind=\'mergesort\')\n            else:\n                sort_nona = numpy.argsort(self._data[~na_data_arr])\n            q = 0\n            for id, i in enumerate(sort):\n                if id in set(sort[len(self._data) - na:]):\n                    q += 1\n                else:\n                    result[id] = sort_nona[id - q]\n            for i in sort[len(self._data) - na:]:\n                result[i] = -1\n\n            return pandas.Series(result, self._index)\n\n        return hpat_pandas_series_argsort_idx_impl\n\n    def hpat_pandas_series_argsort_noidx_impl(self, axis=0, kind=\'quicksort\', order=None):\n        if kind != \'quicksort\' and kind != \'mergesort\':\n            raise ValueError(""Method argsort(). Unsupported parameter. Given \'kind\' != \'quicksort\' or \'mergesort\'"")\n        if kind == \'mergesort\':\n            sort = numpy.argsort(self._data, kind=\'mergesort\')\n        else:\n            sort = numpy.argsort(self._data)\n        na = self.isna().sum()\n        result = numpy.empty(len(self._data), dtype=numpy.int64)\n        na_data_arr = sdc.hiframes.api.get_nan_mask(self._data)\n        if kind == \'mergesort\':\n            sort_nona = numpy.argsort(self._data[~na_data_arr], kind=\'mergesort\')\n        else:\n            sort_nona = numpy.argsort(self._data[~na_data_arr])\n        q = 0\n        for id, i in enumerate(sort):\n            if id in set(sort[len(self._data) - na:]):\n                q += 1\n            else:\n                result[id] = sort_nona[id - q]\n        for i in sort[len(self._data) - na:]:\n            result[i] = -1\n\n        return pandas.Series(result)\n\n    return hpat_pandas_series_argsort_noidx_impl\n\n\n@sdc_overload_method(SeriesType, \'sort_values\', parallel=False)\ndef hpat_pandas_series_sort_values(self, axis=0, ascending=True, inplace=False, kind=\'quicksort\', na_position=\'last\'):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.sort_values\n\n    Limitations\n    -----------\n    - Parameter ``inplace`` is supported only with default value ``False``.\n    - Parameter ``axis`` is currently unsupported by Intel Scalable Dataframe Compiler.\n    - Parameter ``kind`` is supported only with values ``\'mergesort\'`` and ``\'quicksort\'``.\n    - This function may reveal slower performance than Pandas* on user system. Users should exercise a tradeoff\n    between staying in JIT-region with that function or going back to interpreter mode.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_sort_values.py\n       :language: python\n       :lines: 36-\n       :caption: Sort by the values.\n       :name: ex_series_sort_values\n\n    .. command-output:: python ./series/series_sort_values.py\n       :cwd: ../../../examples\n\n    .. seealso::\n\n        :ref:`Series.sort_index <pandas.Series.sort_index>`\n            Sort by the Series indices.\n\n        :ref:`DataFrame.sort_values <pandas.DataFrame.sort_values>`\n            Sort DataFrame by the values along either axis.\n\n        :ref:`DataFrame.sort_index <pandas.DataFrame.sort_index>`\n            Sort DataFrame by indices.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.sort_values` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_sort_values*\n    """"""\n\n    _func_name = \'Method sort_values().\'\n\n    ty_checker = TypeChecker(\'Method sort_values().\')\n    ty_checker.check(self, SeriesType)\n\n    axis_supported_types = (int, types.Omitted, types.Integer, types.StringLiteral, types.UnicodeType)\n    if not isinstance(axis, axis_supported_types):\n        ty_checker.raise_exc(axis, \'integer or string\', \'axis\')\n\n    ascending_supported_types = (bool, types.Omitted, types.Boolean)\n    if not isinstance(ascending, ascending_supported_types):\n        ty_checker.raise_exc(ascending, \'boolean\', \'ascending\')\n\n    kind_supported_types = (str, types.Omitted, types.NoneType, types.StringLiteral, types.UnicodeType)\n    if not isinstance(kind, kind_supported_types):\n        ty_checker.raise_exc(kind, \'string\', \'kind\')\n    kind_is_none_or_default = isinstance(kind, (str, types.Omitted, types.NoneType))\n\n    na_position_supported_types = (str, types.Omitted, types.StringLiteral, types.UnicodeType)\n    if not isinstance(na_position, na_position_supported_types):\n        ty_checker.raise_exc(na_position, \'string\', \'na_position\')\n\n    if not (inplace is False or isinstance(inplace, types.Omitted)):\n        raise TypingError(\'{} Unsupported parameters. Given inplace: {}\'.format(_func_name, inplace))\n\n    def _sdc_pandas_series_sort_values_impl(\n            self, axis=0, ascending=True, inplace=False, kind=\'quicksort\', na_position=\'last\'):\n\n        common_functions._sdc_pandas_series_check_axis(axis)\n\n        if not (kind_is_none_or_default or kind in (\'quicksort\', \'mergesort\')):\n            raise ValueError(""Method sort_values(). Unsupported parameter. Given kind != \'quicksort\', \'mergesort\'"")\n\n        if na_position not in (\'last\', \'first\'):\n            raise ValueError(""Method sort_values(). Unsupported parameter. Given na_position != \'last\', \'first\'"")\n\n        data_nan_mask = sdc.hiframes.api.get_nan_mask(self._data)\n        good = ~data_nan_mask\n\n        if kind_is_none_or_default == True:  # noqa\n            argsort_res = sdc_arrays_argsort(self._data[good], kind=\'quicksort\')\n        else:\n            argsort_res = sdc_arrays_argsort(self._data[good], kind=kind)\n        if not ascending:\n            argsort_res = argsort_res[::-1]\n\n        idx = numpy.arange(len(self), dtype=numpy.int32)\n        sorted_index = numpy.empty(len(self), dtype=numpy.int32)\n        if na_position == ""last"":\n            nans_start, nans_stop = good.sum(), len(self)\n            sorted_index[:nans_start] = idx[good][argsort_res]\n            sorted_index[nans_start: nans_stop] = idx[data_nan_mask]\n        elif na_position == ""first"":\n            nans_start, nans_stop = 0, data_nan_mask.sum()\n            sorted_index[nans_stop:] = idx[good][argsort_res]\n            sorted_index[:nans_stop] = idx[data_nan_mask]\n\n        result_data = self._data[sorted_index]\n        result_index = self.index[sorted_index]\n\n        return pandas.Series(data=result_data, index=result_index, name=self._name)\n\n    return _sdc_pandas_series_sort_values_impl\n\n\n@sdc_overload_method(SeriesType, \'dropna\')\ndef hpat_pandas_series_dropna(self, axis=0, inplace=False):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.dropna\n\n    Limitations\n    -----------\n    - Parameters ``inplace`` and ``axis`` are currently unsupported by Intel Scalable Dataframe Compiler.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_dropna.py\n       :language: python\n       :lines: 34-\n       :caption: Return a new Series with missing values removed.\n       :name: ex_series_dropna\n\n    .. command-output:: python ./series/series_dropna.py\n       :cwd: ../../../examples\n\n    .. seealso::\n\n        :ref:`Series.isna <pandas.Series.isna>`\n            Indicate missing values.\n\n        :ref:`Series.notna <pandas.Series.notna>`\n            Indicate existing (non-missing) values.\n\n        :ref:`Series.fillna <pandas.Series.fillna>`\n            Replace missing values.\n\n        :ref:`DataFrame.dropna <pandas.DataFrame.dropna>`\n            Drop rows or columns which contain NA values.\n\n        `pandas.Index.dropna\n        <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.dropna.html#pandas.Index.dropna>`_\n            Return Index without NA/NaN values\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.dropna` implementation.\n\n    .. only:: developer\n       Tests: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_dropna*\n    """"""\n\n    _func_name = \'Method dropna().\'\n\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    if not (isinstance(axis, (types.Integer, types.StringLiteral, types.UnicodeType, types.Omitted)) or axis == 0):\n        ty_checker.raise_exc(axis, \'int or str\', \'axis\')\n\n    if not (inplace is False or isinstance(inplace, types.Omitted)):\n        ty_checker.raise_exc(inplace, \'bool\', \'inplace\')\n\n    if isinstance(self.data.dtype, types.Number) and isinstance(self.index, (types.Number, types.NoneType)):\n        def hpat_pandas_series_dropna_impl(self, axis=0, inplace=False):\n            index = self.index\n            return numpy_like.dropna(self._data, index, self._name)\n\n        return hpat_pandas_series_dropna_impl\n\n    else:\n        def hpat_pandas_series_dropna_str_impl(self, axis=0, inplace=False):\n            # generate Series index if needed by using SeriesType.index (i.e. not self._index)\n            na_data_arr = sdc.hiframes.api.get_nan_mask(self._data)\n            data = self._data[~na_data_arr]\n            index = self.index[~na_data_arr]\n            return pandas.Series(data, index, self._name)\n\n        return hpat_pandas_series_dropna_str_impl\n\n\n@sdc_overload_method(SeriesType, \'fillna\')\ndef hpat_pandas_series_fillna(self, value=None, method=None, axis=None, inplace=False, limit=None, downcast=None):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.fillna\n\n    Limitations\n    -----------\n    - Parameters ``method``, ``limit`` and ``downcast`` are currently unsupported\n    by Intel Scalable Dataframe Compiler.\n    - Parameter ``inplace`` is supported with literal value only.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_fillna.py\n       :language: python\n       :lines: 35-\n       :caption: Fill NA/NaN values using the specified method.\n       :name: ex_series_fillna\n\n    .. command-output:: python ./series/series_fillna.py\n       :cwd: ../../../examples\n\n    .. seealso::\n\n        `pandas.interpolate\n        <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.interpolate.html#pandas.Series.interpolate>`_\n            Fill NaN values using interpolation.\n\n        `pandas.reindex\n        <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.reindex.html#pandas.Series.reindex>`_\n            Conform object to new index.\n\n        `pandas.asfreq\n        <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.asfreq.html#pandas.Series.asfreq>`_\n            Convert TimeSeries to specified frequency.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.fillna` implementation.\n\n    .. only:: developer\n       Tests: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_fillna*\n    """"""\n\n    _func_name = \'Method fillna().\'\n\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    if not (isinstance(axis, (types.Integer, types.StringLiteral, types.UnicodeType, types.Omitted)) or axis is None):\n        ty_checker.raise_exc(axis, \'int or str\', \'axis\')\n\n    if not (isinstance(inplace, types.Literal) and isinstance(inplace, types.Boolean)\n            or isinstance(inplace, types.Omitted)\n            or inplace is False):\n        ty_checker.raise_exc(inplace, \'bool\', \'inplace\')\n\n    if not isinstance(method, (types.Omitted, types.NoneType)) and method is not None:\n        ty_checker.raise_exc(method, \'None\', \'method\')\n\n    if not isinstance(limit, (types.Omitted, types.NoneType)) and limit is not None:\n        ty_checker.raise_exc(limit, \'None\', \'limit\')\n\n    if not isinstance(downcast, (types.Omitted, types.NoneType)) and downcast is not None:\n        ty_checker.raise_exc(downcast, \'None\', \'downcast\')\n\n    # inplace value has to be known at compile time to select between implementations with different signatures\n    if ((isinstance(inplace, types.Literal) and inplace.literal_value == True)\n        or (isinstance(inplace, bool) and inplace == True)):\n        # do operation inplace, fill the NA/NaNs in the same array and return None\n        if isinstance(self.dtype, types.UnicodeType):\n            # TODO: StringArrayType cannot resize inplace, and assigning a copy back to self._data is not possible now\n            raise TypingError(\'{} Not implemented when Series dtype is {} and\\\n                 inplace={}\'.format(_func_name, self.dtype, inplace))\n\n        else:\n            def hpat_pandas_series_fillna_impl(self, value=None, method=None, axis=None, inplace=False,\n                                               limit=None, downcast=None):\n                return numpy_like.fillna(self._data, inplace=inplace, value=value)\n\n            return hpat_pandas_series_fillna_impl\n\n    else:\n        # non inplace implementations, copy array, fill the NA/NaN and return a new Series\n        if isinstance(self.dtype, types.UnicodeType):\n            # For StringArrayType implementation is taken from _series_fillna_str_alloc_impl\n            # (can be called directly when it\'s index handling is fixed)\n            def hpat_pandas_series_str_fillna_impl(self, value=None, method=None, axis=None,\n                                                   inplace=False, limit=None, downcast=None):\n                return pandas.Series(data=numpy_like.fillna(self._data, inplace=inplace, value=value),\n                                     index=self._index,\n                                     name=self._name)\n\n            return hpat_pandas_series_str_fillna_impl\n\n        elif isinstance(self.dtype, (types.Integer, types.Boolean)):\n            def hpat_pandas_series_no_nan_fillna_impl(self, value=None, method=None, axis=None, inplace=False, limit=None, downcast=None):\n                return pandas.Series(data=numpy_like.fillna(self._data, inplace=inplace, value=value),\n                                     index=self._index,\n                                     name=self._name)\n\n            return hpat_pandas_series_no_nan_fillna_impl\n\n        else:\n            def hpat_pandas_series_fillna_impl(self, value=None, method=None, axis=None, inplace=False, limit=None, downcast=None):\n                filled_data = numpy_like.fillna(self._data, inplace=inplace, value=value)\n                return pandas.Series(data=filled_data,\n                                     index=self._index,\n                                     name=self._name)\n\n            return hpat_pandas_series_fillna_impl\n\n\n@sdc_overload_method(SeriesType, \'cov\')\ndef hpat_pandas_series_cov(self, other, min_periods=None):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.cov\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_cov.py\n       :language: python\n       :lines: 27-\n       :caption: Compute covariance with Series, excluding missing values.\n       :name: ex_series_cov\n\n    .. command-output:: python ./series/series_cov.py\n       :cwd: ../../../examples\n\n    .. seealso::\n        :ref:`Series.corr <pandas.Series.corr>`\n            Compute correlation with other Series, excluding missing values.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.cov` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_cov*\n    """"""\n\n    ty_checker = TypeChecker(\'Method cov().\')\n    ty_checker.check(self, SeriesType)\n\n    ty_checker.check(other, SeriesType)\n\n    if not isinstance(self.data.dtype, types.Number):\n        ty_checker.raise_exc(self.data.dtype, \'number\', \'self.data\')\n\n    if not isinstance(other.data.dtype, types.Number):\n        ty_checker.raise_exc(other.data.dtype, \'number\', \'other.data\')\n\n    if not isinstance(min_periods, (types.Integer, types.Omitted, types.NoneType)) and min_periods is not None:\n        ty_checker.raise_exc(min_periods, \'int64\', \'min_periods\')\n\n    def hpat_pandas_series_cov_impl(self, other, min_periods=None):\n\n        if min_periods is None:\n            min_periods = 2\n\n        if min_periods < 2:\n            min_periods = 2\n\n        min_len = min(len(self._data), len(other._data))\n\n        if min_len == 0:\n            return numpy.nan\n\n        other_sum = 0.\n        self_sum = 0.\n        self_other_sum = 0.\n        total_count = 0\n        for i in prange(min_len):\n            s = self._data[i]\n            o = other._data[i]\n            if not (numpy.isnan(s) or numpy.isnan(o)):\n                self_sum += s\n                other_sum += o\n                self_other_sum += s*o\n                total_count += 1\n\n        if total_count < min_periods:\n            return numpy.nan\n\n        return (self_other_sum - self_sum*other_sum/total_count)/(total_count - 1)\n\n    return hpat_pandas_series_cov_impl\n\n\n@sdc_overload_method(SeriesType, \'pct_change\', parallel=False)\ndef hpat_pandas_series_pct_change(self, periods=1, fill_method=\'pad\', limit=None, freq=None):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.pct_change\n\n    Limitations\n    -----------\n    - Parameters limit, freq are currently unsupported by Intel Scalable Dataframe Compiler\n    - This function may reveal slower performance than Pandas* on user system. Users should exercise a tradeoff\n    between staying in JIT-region with that function or going back to interpreter mode.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_pct_change.py\n       :language: python\n       :lines: 36-\n       :caption: Percentage change between the current and a prior element.\n       :name: ex_series_pct_change\n\n    .. command-output:: python ./series/series_pct_change.py\n       :cwd: ../../../examples\n\n    .. seealso::\n\n        :ref:`Series.diff <pandas.Series.diff>`\n            Compute the difference of two elements in a Series.\n\n        :ref:`DataFrame.diff <pandas.DataFrame.diff>`\n            Compute the difference of two elements in a DataFrame.\n\n        :ref:`Series.shift <pandas.Series.shift>`\n            Shift the index by some number of periods.\n\n        :ref:`DataFrame.shift <pandas.DataFrame.shift>`\n            Shift the index by some number of periods.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.pct_change` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_pct_change\n    """"""\n\n    ty_checker = TypeChecker(\'Method pct_change().\')\n    ty_checker.check(self, SeriesType)\n\n    if not isinstance(self.data.dtype, types.Number):\n        ty_checker.raise_exc(self.data.dtype, \'number\', \'self.data\')\n\n    if not isinstance(periods, (types.Integer, types.Omitted)):\n        ty_checker.raise_exc(periods, \'int64\', \'periods\')\n\n    if not isinstance(fill_method, (str, types.UnicodeType, types.StringLiteral, types.NoneType, types.Omitted)):\n        ty_checker.raise_exc(fill_method, \'string\', \'fill_method\')\n\n    if not isinstance(limit, (types.Omitted, types.NoneType)):\n        ty_checker.raise_exc(limit, \'None\', \'limit\')\n\n    if not isinstance(freq, (types.Omitted, types.NoneType)):\n        ty_checker.raise_exc(freq, \'None\', \'freq\')\n\n    def hpat_pandas_series_pct_change_impl(self, periods=1, fill_method=\'pad\', limit=None, freq=None):\n        if not (fill_method is None or fill_method in [\'pad\', \'ffill\', \'backfill\', \'bfill\']):\n            raise ValueError(\n                ""Method pct_change(). Unsupported parameter. The function uses fill_method pad (ffill) or backfill (bfill) or None."")\n        local_series = self.copy()\n        if fill_method is not None:\n            # replacement method fillna for given method\n            # =========================================\n            # Example:\n            # s = [1.1, 0.3, np.nan, 1, np.inf, 0, 1.1, np.nan, 2.2, np.inf, 2, 2]\n            # result = [1.1, 0.3, 0.3, 1, inf, 0, 1.1, 1.1, 2.2, inf, 2, 2]\n            # ==========================================\n            for i in range(len(local_series._data)):\n                # check each element on numpy.nan\n                if numpy.isnan(local_series._data[i]):\n                    if fill_method in [\'pad\', \'ffill\']:\n                        # if it first element is nan, element will be is nan\n                        # if it not first element, element will be is nearest is not nan element\n                        # take a step back while will not find is not nan element\n                        # if before the first element you did not find one, the element will be equal nan\n                        if i == 0:\n                            local_series._data[i] = numpy.nan\n                        else:\n                            k = 1\n                            while numpy.isnan(local_series._data[i - k]):\n                                if i - k == 0:\n                                    local_series._data[i] = numpy.nan\n                                    break\n                                k += 1\n                            local_series._data[i] = local_series._data[i - k]\n                    elif fill_method in [\'backfill\', \'bfill\']:\n                        # if it last element is nan, element will be is nan\n                        # if it not last element, element will be is nearest is not nan element\n                        # take a step front while will not find is not nan element\n                        # if before the last element you did not find one, the element will be equal nan\n                        if i == len(local_series._data)-1:\n                            local_series._data[i] = numpy.nan\n                        else:\n                            k = 1\n                            while numpy.isnan(local_series._data[i + k]):\n                                if i + k == len(local_series._data) - 1:\n                                    local_series._data[i] = numpy.nan\n                                    break\n                                k += 1\n                            local_series._data[i] = local_series._data[i + k]\n        rshift = local_series.shift(periods=periods, freq=freq)\n        rdiv = local_series.div(rshift)\n        result = rdiv._data - 1\n        return pandas.Series(result)\n\n    return hpat_pandas_series_pct_change_impl\n\n\n@sdc_overload_method(SeriesType, \'describe\')\ndef hpat_pandas_series_describe(self, percentiles=None, include=None, exclude=None):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.describe\n\n    Limitations\n    -----------\n    - Parameters ``include`` and ``exclude`` are currently unsupported by Intel Scalable Dataframe Compiler.\n    - For string Series resulting values are returned as strings.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_describe.py\n       :language: python\n       :lines: 39-\n       :caption: Generate descriptive statistics.\n       :name: ex_series_describe\n\n    .. command-output:: python ./series/series_describe.py\n       :cwd: ../../../examples\n\n    .. seealso::\n\n        :ref:`DataFrame.count <pandas.DataFrame.count>`\n            Count number of non-NA/null observations.\n\n        :ref:`DataFrame.max <pandas.DataFrame.max>`\n            Maximum of the values in the object.\n\n        :ref:`DataFrame.min <pandas.DataFrame.min>`\n            Minimum of the values in the object.\n\n        :ref:`DataFrame.mean <pandas.DataFrame.mean>`\n            Mean of the values.\n\n        :ref:`DataFrame.std <pandas.DataFrame.std>`\n            Standard deviation of the observations.\n\n        :ref:`DataFrame.select_dtypes <pandas.DataFrame.select_dtypes>`\n            Subset of a DataFrame including/excluding columns based on their dtype.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.describe` implementation.\n\n    .. only:: developer\n\n       Tests: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_describe*\n    """"""\n\n    _func_name = \'Method describe().\'\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    if not (isinstance(percentiles, (types.List, types.Array, types.UniTuple))\n            and isinstance(percentiles.dtype, types.Number)\n            or isinstance(percentiles, (types.Omitted, types.NoneType))\n            or percentiles is None):\n        ty_checker.raise_exc(percentiles, \'list-like\', \'percentiles\')\n\n    if not (isinstance(include, (types.Omitted, types.NoneType)) or include is None):\n        raise TypingError(\'{} Unsupported parameters. Given include: {}\'.format(_func_name, include))\n\n    if not (isinstance(exclude, (types.Omitted, types.NoneType)) or exclude is None):\n        raise TypingError(\'{} Unsupported parameters. Given exclude: {}\'.format(_func_name, exclude))\n\n    is_percentiles_none = percentiles is None or isinstance(percentiles, (types.Omitted, types.NoneType))\n\n    if isinstance(self.dtype, types.Number):\n        def hpat_pandas_series_describe_numeric_impl(self, percentiles=None, include=None, exclude=None):\n\n            if is_percentiles_none == False:  # noqa\n                percentiles_list = list(percentiles)\n                median_in_percentiles = 0.5 in percentiles_list\n                if not median_in_percentiles:\n                    percentiles_list.append(0.5)\n                sorted_percentiles = sorted(percentiles_list)\n\n                # check percentiles have correct values:\n                arr = numpy.asarray(sorted_percentiles)\n                if len(numpy.unique(arr)) != len(arr):\n                    raise ValueError(""percentiles cannot contain duplicates"")\n                if numpy.any(arr[(arr < 0) * (arr > 1)]):\n                    raise ValueError(""percentiles should all be in the interval [0, 1]."")\n\n                # TODO: support proper rounding of percentiles like in pandas.io.formats.format.format_percentiles\n                # requires numpy.round(precision), numpy.isclose to be supported by Numba\n                percentiles_indexes = common_functions._sdc_pandas_format_percentiles(arr)\n            else:\n                sorted_percentiles = [0.25, 0.5, 0.75]\n                percentiles_indexes = [\'25%\', \'50%\', \'75%\']\n\n            index_strings = [\'count\', \'mean\', \'std\', \'min\']\n            index_strings.extend(percentiles_indexes)\n            index_strings.append(\'max\')\n\n            values = []\n            values.append(numpy.float64(self.count()))\n            values.append(self.mean())\n            values.append(self.std())\n            values.append(self.min())\n            for p in sorted_percentiles:\n                values.append(self.quantile(p))\n            values.append(self.max())\n\n            return pandas.Series(values, index_strings)\n\n        return hpat_pandas_series_describe_numeric_impl\n\n    elif isinstance(self.dtype, types.UnicodeType):\n        def hpat_pandas_series_describe_string_impl(self, percentiles=None, include=None, exclude=None):\n\n            objcounts = self.value_counts()\n            index_strings = [\'count\', \'unique\', \'top\', \'freq\']\n\n            # use list of strings for the output series, since Numba doesn\'t support np.arrays with object dtype\n            values = []\n            values.append(str(self.count()))\n            values.append(str(len(self.unique())))\n            values.append(str(objcounts.index[0]))\n            values.append(str(objcounts.iloc[0]))\n\n            return pandas.Series(values, index_strings)\n\n        return hpat_pandas_series_describe_string_impl\n\n    elif isinstance(self.dtype, (types.NPDatetime, types.NPTimedelta)):\n        # TODO: provide specialization for (types.NPDatetime, types.NPTimedelta)\n        # needs dropna for date-time series, conversion to int and tz_convert to be implemented\n        return None\n\n    return None\n\n\n@sdc_overload(operator.add, parallel=False)\ndef sdc_pandas_str_series_operator_add(self, other):\n    """"""\n    Additional overload of operator.add for Series of strings.\n    For generic overload see - autogenerated one in sdc_autogenerated.py\n    """"""\n\n    _func_name = \'Operator add().\'\n\n    ty_checker = TypeChecker(\'Operator add().\')\n    self_is_series, other_is_series = isinstance(self, SeriesType), isinstance(other, SeriesType)\n    if not (self_is_series or other_is_series):\n        return None\n\n    # this overload is for string series only (so check that at least one is series of strings)\n    self_is_string_series = self_is_series and isinstance(self.dtype, types.UnicodeType)\n    other_is_string_series = other_is_series and isinstance(other.dtype, types.UnicodeType)\n    if not (self_is_string_series or other_is_string_series):\n        return None\n\n    if not isinstance(self, (SeriesType, types.Number, types.UnicodeType)):\n        ty_checker.raise_exc(self, \'pandas.series or string\', \'self\')\n\n    if not isinstance(other, (SeriesType, types.Number, types.UnicodeType)):\n        ty_checker.raise_exc(other, \'pandas.series or string\', \'other\')\n\n    operands_are_series = self_is_series and other_is_series\n    if operands_are_series:\n        none_or_numeric_indexes = ((isinstance(self.index, types.NoneType) or check_index_is_numeric(self))\n                                   and (isinstance(other.index, types.NoneType) or check_index_is_numeric(other)))\n        series_indexes_comparable = check_types_comparable(self.index, other.index) or none_or_numeric_indexes\n        if not series_indexes_comparable:\n            raise TypingError(\'{} Not implemented for series with not-comparable indexes. \\\n            Given: self.index={}, other.index={}\'.format(_func_name, self.index, other.index))\n\n    series_data_comparable = check_types_comparable(self, other)\n    if not series_data_comparable:\n        raise TypingError(\'{} Not supported for not-comparable operands. \\\n        Given: self={}, other={}\'.format(_func_name, self, other))\n\n    if not operands_are_series:\n        def _series_operator_add_scalar_impl(self, other):\n            if self_is_series == True:  # noqa\n                result_data = self._data + other\n                return pandas.Series(result_data, index=self._index, name=self._name)\n            else:\n                result_data = self + other._data\n                return pandas.Series(result_data, index=other._index, name=other._name)\n\n        return _series_operator_add_scalar_impl\n    else:   # both operands are string series\n\n        # optimization for series with default indexes, that can be aligned differently\n        if (isinstance(self.index, types.NoneType) and isinstance(other.index, types.NoneType)):\n            def _series_operator_add_none_indexes_impl(self, other):\n\n                if (len(self._data) == len(other._data)):\n                    result_data = self._data + other._data\n                    return pandas.Series(result_data)\n                else:\n                    left_size, right_size = len(self._data), len(other._data)\n                    min_data_size = min(left_size, right_size)\n                    max_data_size = max(left_size, right_size)\n\n                    result_data_as_list = []\n                    result_nan_mask = numpy.zeros(max_data_size, dtype=numpy.bool_)\n                    result_nan_mask[min_data_size:] = True\n                    for i in numpy.arange(min_data_size):\n                        if str_arr_is_na(self._data, i) or str_arr_is_na(other._data, i):\n                            result_nan_mask[i] = True\n                            result_data_as_list.append(\'\')\n                        else:\n                            result_data_as_list.append(self._data[i] + other._data[i])\n                    result_data_as_list.extend([\'\'] * (max_data_size - min_data_size))\n                    result_data = create_str_arr_from_list(result_data_as_list)\n                    str_arr_set_na_by_mask(result_data, result_nan_mask)\n\n                    return pandas.Series(result_data, self._index)\n\n            return _series_operator_add_none_indexes_impl\n        else:\n            # for numeric indexes find common dtype to be used when creating joined index\n            if none_or_numeric_indexes:\n                ty_left_index_dtype = types.int64 if isinstance(self.index, types.NoneType) else self.index.dtype\n                ty_right_index_dtype = types.int64 if isinstance(other.index, types.NoneType) else other.index.dtype\n                numba_index_common_dtype = find_common_dtype_from_numpy_dtypes(\n                    [ty_left_index_dtype, ty_right_index_dtype], [])\n\n            def _series_operator_add_common_impl(self, other):\n                left_index, right_index = self.index, other.index\n\n                # check if indexes are equal and series don\'t have to be aligned\n                if sdc_check_indexes_equal(left_index, right_index):\n                    result_data = self._data + other._data\n\n                    if none_or_numeric_indexes == True:  # noqa\n                        result_index = left_index.astype(numba_index_common_dtype)\n                    else:  # case of string indices\n                        result_index = self._index\n\n                    return pandas.Series(result_data, index=result_index)\n\n                # TODO: replace below with core join(how=\'outer\', return_indexers=True) when implemented\n                joined_index, left_indexer, right_indexer = sdc_join_series_indexes(left_index, right_index)\n\n                result_size = len(joined_index)\n                result_nan_mask = numpy.zeros(result_size, dtype=numpy.bool_)\n                result_data_as_list = []\n                for i in numpy.arange(result_size):\n                    if (left_indexer[i] == -1\n                            or right_indexer[i] == -1\n                            or str_arr_is_na(self._data, left_indexer[i])\n                            or str_arr_is_na(other._data, right_indexer[i])):\n                        result_nan_mask[i] = True\n                        result_data_as_list.append(\'\')\n                    else:\n                        result_data_as_list.append(self._data[left_indexer[i]] + other._data[right_indexer[i]])\n\n                result_data = create_str_arr_from_list(result_data_as_list)\n                str_arr_set_na_by_mask(result_data, result_nan_mask)\n\n                return pandas.Series(result_data, joined_index)\n\n            return _series_operator_add_common_impl\n\n    return None\n\n\n@sdc_overload(operator.mul, parallel=False)\ndef sdc_pandas_str_series_operator_mul(self, other):\n    """"""\n    Additional overload of operator.add for Series of strings.\n    For generic overload see - autogenerated one in sdc_autogenerated.py\n    """"""\n\n    _func_name = \'Operator mul().\'\n\n    ty_checker = TypeChecker(_func_name)\n    self_is_series, other_is_series = isinstance(self, SeriesType), isinstance(other, SeriesType)\n    if not (self_is_series or other_is_series):\n        return None\n\n    # this overload is for string series only (but another might well be series of integers)\n    self_is_string_series = self_is_series and isinstance(self.dtype, types.UnicodeType)\n    other_is_string_series = other_is_series and isinstance(other.dtype, types.UnicodeType)\n    if not (self_is_string_series or other_is_string_series):\n        return None\n\n    if not isinstance(self, (SeriesType, types.Number, types.UnicodeType)):\n        ty_checker.raise_exc(self, \'pandas.series or string\', \'self\')\n\n    if not isinstance(other, (SeriesType, types.Number, types.UnicodeType)):\n        ty_checker.raise_exc(other, \'pandas.series or string\', \'other\')\n\n    operands_are_series = self_is_series and other_is_series\n    if operands_are_series:\n        none_or_numeric_indexes = ((isinstance(self.index, types.NoneType) or check_index_is_numeric(self))\n                                   and (isinstance(other.index, types.NoneType) or check_index_is_numeric(other)))\n        series_indexes_comparable = check_types_comparable(self.index, other.index) or none_or_numeric_indexes\n        if not series_indexes_comparable:\n            raise TypingError(\'{} Not implemented for series with not-comparable indexes. \\\n            Given: self.index={}, other.index={}\'.format(_func_name, self.index, other.index))\n\n    # check operation is allowed\n    self_is_int_series = self_is_series and isinstance(self.dtype, types.Integer)\n    other_is_int_series = other_is_series and isinstance(other.dtype, types.Integer)\n    if not (self_is_string_series and (other_is_int_series or isinstance(other, types.Integer))\n            or (other_is_string_series and (self_is_int_series or isinstance(self, types.Integer)))):\n        raise TypingError(\'{} Not supported between operands of types: self={}, \\\n        other={}\'.format(_func_name, self, other))\n\n    if not operands_are_series:\n        def _series_operator_mul_scalar_impl(self, other):\n            if self_is_series == True:  # noqa\n                result_data = self._data * other\n                return pandas.Series(result_data, index=self._index, name=self._name)\n            else:\n                result_data = self * other._data\n                return pandas.Series(result_data, index=other._index, name=other._name)\n\n        return _series_operator_mul_scalar_impl\n    else:   # both operands are series (one is integer and other is string)\n\n        self_is_series = isinstance(self, SeriesType)\n        # optimization for series with default indexes, that can be aligned differently\n        if (isinstance(self.index, types.NoneType) and isinstance(other.index, types.NoneType)):\n            def _series_operator_mul_none_indexes_impl(self, other):\n\n                series_operand = self if self_is_series == True else other  # noqa\n                if (len(self._data) == len(other._data)):\n                    result_data = self._data * other._data\n                    return pandas.Series(result_data)\n                else:\n                    left_size, right_size = len(self._data), len(other._data)\n                    min_data_size = min(left_size, right_size)\n                    max_data_size = max(left_size, right_size)\n\n                    result_data_as_list = []\n                    result_nan_mask = numpy.zeros(max_data_size, dtype=numpy.bool_)\n                    result_nan_mask[min_data_size:] = True\n                    for i in numpy.arange(min_data_size):\n                        if str_arr_is_na(series_operand._data, i):\n                            result_nan_mask[i] = True\n                            result_data_as_list.append(\'\')\n                        result_data_as_list.append(self._data[i] * other._data[i])\n                    result_data_as_list.extend([\'\'] * (max_data_size - min_data_size))\n                    result_data = create_str_arr_from_list(result_data_as_list)\n                    str_arr_set_na_by_mask(result_data, result_nan_mask)\n\n                    return pandas.Series(result_data, self._index)\n\n            return _series_operator_mul_none_indexes_impl\n        else:\n            # for numeric indexes find common dtype to be used when creating joined index\n            if none_or_numeric_indexes:\n                ty_left_index_dtype = types.int64 if isinstance(self.index, types.NoneType) else self.index.dtype\n                ty_right_index_dtype = types.int64 if isinstance(other.index, types.NoneType) else other.index.dtype\n                numba_index_common_dtype = find_common_dtype_from_numpy_dtypes(\n                    [ty_left_index_dtype, ty_right_index_dtype], [])\n\n            def _series_operator_mul_common_impl(self, other):\n                left_index, right_index = self.index, other.index\n\n                # check if indexes are equal and series don\'t have to be aligned\n                if sdc_check_indexes_equal(left_index, right_index):\n                    result_data = self._data * other._data\n\n                    if none_or_numeric_indexes == True:  # noqa\n                        result_index = left_index.astype(numba_index_common_dtype)\n                    else:  # case of string indices\n                        result_index = self._index\n\n                    return pandas.Series(result_data, index=result_index)\n\n                # TODO: replace below with core join(how=\'outer\', return_indexers=True) when implemented\n                joined_index, left_indexer, right_indexer = sdc_join_series_indexes(left_index, right_index)\n\n                str_series_operand = self if self_is_string_series == True else other  # noqa\n                str_series_indexer = left_indexer if self_is_string_series == True else right_indexer  # noqa\n                result_size = len(joined_index)\n                result_nan_mask = numpy.zeros(result_size, dtype=numpy.bool_)\n                result_data_as_list = []\n                for i in numpy.arange(result_size):\n                    if (left_indexer[i] == -1\n                            or right_indexer[i] == -1\n                            or str_arr_is_na(str_series_operand._data, str_series_indexer[i])):\n                        result_nan_mask[i] = True\n                        result_data_as_list.append(\'\')\n                    else:\n                        result_data_as_list.append(self._data[left_indexer[i]] * other._data[right_indexer[i]])\n\n                result_data = create_str_arr_from_list(result_data_as_list)\n                str_arr_set_na_by_mask(result_data, result_nan_mask)\n\n                return pandas.Series(result_data, joined_index)\n\n            return _series_operator_mul_common_impl\n\n\n@sdc_overload_method(SeriesType, \'groupby\')\ndef sdc_pandas_series_groupby(self, by=None, axis=0, level=None, as_index=True, sort=True,\n                              group_keys=True, squeeze=False, observed=False):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.groupby\n\n    Limitations\n    -----------\n    - Parameters ``axis``, ``level``, ``as_index``, ``group_keys``, ``squeeze`` and ``observed`` \\\nare currently unsupported by Intel Scalable Dataframe Compiler\n    - Parameter ``by`` is supported as single literal column name only\n    - Mutating the contents of a DataFrame between creating a groupby object and calling it\'s methods is unsupported\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_groupby.py\n       :language: python\n       :lines: 33-\n       :caption: Return the mean of the values grouped by numpy array.\n       :name: ex_series_groupby\n\n    .. command-output:: python ./series/series_groupby.py\n       :cwd: ../../../examples\n\n    .. seealso::\n\n        :ref:`Series.resample <pandas.Series.resample>`\n            Resample time-series data.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Pandas Series method :meth:`pandas.Series.groupby` implementation.\n\n    .. only:: developer\n        Test: python -m sdc.runtests sdc.tests.test_groupby.TestGroupBy.test_series_groupby*\n    """"""\n\n    _func_name = \'Method Series.groupby().\'\n\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    # we support only simpliest case of by being 1D array (a column of a DataFrame)\n    # TODO: extend and support fully functional SeriesGroupBy\n    if not ((isinstance(by, types.Array) and by.ndim == 1)\n            or by == string_array_type):\n        return None\n\n    if not (isinstance(axis, (types.Integer, types.UnicodeType, types.Omitted)) or axis == 0):\n        ty_checker.raise_exc(axis, \'int or str\', \'axis\')\n\n    if not (level is None or isinstance(level, types.Omitted)):\n        raise TypingError(\'{} Unsupported parameters. Given inplace: {}\'.format(_func_name, level))\n\n    if not (as_index is True or isinstance(as_index, types.Omitted)):\n        raise TypingError(\'{} Unsupported parameters. Given inplace: {}\'.format(_func_name, as_index))\n\n    if not (isinstance(sort, (types.Omitted, types.Boolean)) or sort is True):\n        ty_checker.raise_exc(sort, \'bool\', \'sort\')\n\n    if not (group_keys is True or isinstance(group_keys, types.Omitted)):\n        raise TypingError(\'{} Unsupported parameters. Given inplace: {}\'.format(_func_name, group_keys))\n\n    if not (squeeze is False or isinstance(squeeze, types.Omitted)):\n        raise TypingError(\'{} Unsupported parameters. Given inplace: {}\'.format(_func_name, squeeze))\n\n    if not (observed is False or isinstance(observed, types.Omitted)):\n        raise TypingError(\'{} Unsupported parameters. Given inplace: {}\'.format(_func_name, observed))\n\n    by_type = by.dtype\n    list_type = types.ListType(types.int64)\n    def sdc_pandas_series_groupby_impl(self, by=None, axis=0, level=None, as_index=True, sort=True,\n                                       group_keys=True, squeeze=False, observed=False):\n\n        if len(self) != len(by):\n            raise ValueError(""Series.groupby(). Grouper and axis must be same length"")\n\n        grouped = Dict.empty(by_type, list_type)\n        for i in numpy.arange(len(by)):\n            if isna(by, i):\n                continue\n            value = by[i]\n            group_list = grouped.get(value, List.empty_list(types.int64))\n            group_list.append(i)\n            grouped[value] = group_list\n\n        return init_series_groupby(self, by, grouped, sort)\n\n    return sdc_pandas_series_groupby_impl\n\n\n@sdc_overload_method(SeriesType, \'skew\')\ndef sdc_pandas_series_skew(self, axis=None, skipna=None, level=None, numeric_only=None):\n    """"""\n        Intel Scalable Dataframe Compiler User Guide\n        ********************************************\n\n        Pandas API: pandas.Series.skew\n\n        Limitations\n        -----------\n        - Parameters ``level`` and ``numeric_only`` are supported only with default value ``None``.\n\n        Examples\n        --------\n        .. literalinclude:: ../../../examples/series/series_skew.py\n           :language: python\n           :lines: 27-\n           :caption: Unbiased rolling skewness.\n           :name: ex_series_skew\n\n        .. command-output:: python ./series/series_skew.py\n           :cwd: ../../../examples\n\n        Intel Scalable Dataframe Compiler Developer Guide\n        *************************************************\n        Pandas Series method :meth:`pandas.Series.skew` implementation.\n\n        .. only:: developer\n            Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_skew*\n        """"""\n    _func_name = \'Method Series.skew()\'\n\n    ty_checker = TypeChecker(_func_name)\n    ty_checker.check(self, SeriesType)\n\n    if not isinstance(axis, (types.Integer, types.NoneType, types.Omitted)) and axis is not None:\n        ty_checker.raise_exc(axis, \'int64\', \'axis\')\n\n    if not isinstance(skipna, (types.Boolean, types.NoneType, types.Omitted)) and skipna is not None:\n        ty_checker.raise_exc(skipna, \'bool\', \'skipna\')\n\n    if not isinstance(level, (types.Omitted, types.NoneType)) and level is not None:\n        ty_checker.raise_exc(level, \'None\', \'level\')\n\n    if not isinstance(numeric_only, (types.Omitted, types.NoneType)) and numeric_only is not None:\n        ty_checker.raise_exc(numeric_only, \'None\', \'numeric_only\')\n\n    def sdc_pandas_series_skew_impl(self, axis=None, skipna=None, level=None, numeric_only=None):\n        if axis != 0 and axis is not None:\n            raise ValueError(\'Parameter axis must be only 0 or None.\')\n\n        if skipna is None:\n            _skipna = True\n        else:\n            _skipna = skipna\n\n        if _skipna:\n            return numpy_like.nanskew(self._data)\n\n        return numpy_like.skew(self._data)\n\n    return sdc_pandas_series_skew_impl\n'"
sdc/datatypes/hpat_pandas_series_rolling_functions.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numpy\nimport pandas\n\nfrom functools import partial\n\nfrom numba import prange\nfrom numba.extending import register_jitable\nfrom numba.core.types import (float64, Boolean, Integer, NoneType, Number,\n                         Omitted, StringLiteral, UnicodeType)\n\nfrom sdc.datatypes.common_functions import _almost_equal\nfrom sdc.datatypes.hpat_pandas_series_rolling_types import SeriesRollingType\nfrom sdc.functions.statistics import skew_formula\nfrom sdc.hiframes.pd_series_type import SeriesType\nfrom sdc.utilities.prange_utils import parallel_chunks\nfrom sdc.utilities.sdc_typing_utils import TypeChecker\nfrom sdc.utilities.utils import sdc_overload_method, sdc_register_jitable\n\n\n# disabling parallel execution for rolling due to numba issue https://github.com/numba/numba/issues/5098\nsdc_rolling_overload = partial(sdc_overload_method, parallel=False)\n\n\nhpat_pandas_series_rolling_docstring_tmpl = """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n    Pandas API: pandas.core.window.Rolling.{method_name}\n{limitations_block}\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/rolling/series_rolling_{method_name}.py\n       :language: python\n       :lines: 27-\n       :caption: {example_caption}\n       :name: ex_series_rolling_{method_name}\n\n    .. command-output:: python ./series/rolling/series_rolling_{method_name}.py\n       :cwd: ../../../examples\n\n    .. literalinclude:: ../../../examples/dataframe/rolling/dataframe_rolling_{method_name}.py\n       :language: python\n       :lines: 27-\n       :caption: {example_caption}\n       :name: ex_dataframe_rolling_{method_name}\n\n    .. command-output:: python ./dataframe/rolling/dataframe_rolling_{method_name}.py\n       :cwd: ../../../examples\n\n    .. seealso::\n        :ref:`Series.rolling <pandas.Series.rolling>`\n            Calling object with a Series.\n        :ref:`DataFrame.rolling <pandas.DataFrame.rolling>`\n            Calling object with a DataFrame.\n        :ref:`Series.{method_name} <pandas.Series.{method_name}>`\n            Similar method for Series.\n        :ref:`DataFrame.{method_name} <pandas.DataFrame.{method_name}>`\n            Similar method for DataFrame.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n\n    Pandas Series method :meth:`pandas.Series.rolling.{method_name}()` implementation.\n\n    .. only:: developer\n\n    Test: python -m sdc.runtests -k sdc.tests.test_rolling.TestRolling.test_series_rolling_{method_name}\n\n    Parameters\n    ----------\n    self: :class:`pandas.Series.rolling`\n        input arg{extra_params}\n\n    Returns\n    -------\n    :obj:`pandas.Series`\n         returns :obj:`pandas.Series` object\n""""""\n\n\n@sdc_register_jitable\ndef arr_apply(arr, func):\n    """"""Apply function for values""""""\n    return func(arr)\n\n\n@sdc_register_jitable\ndef arr_mean(arr):\n    """"""Calculate mean of values""""""\n    if len(arr) == 0:\n        return numpy.nan\n\n    return arr.mean()\n\n\n@sdc_register_jitable\ndef arr_median(arr):\n    """"""Calculate median of values""""""\n    if len(arr) == 0:\n        return numpy.nan\n\n    return numpy.median(arr)\n\n\n@sdc_register_jitable\ndef arr_quantile(arr, q):\n    """"""Calculate quantile of values""""""\n    if len(arr) == 0:\n        return numpy.nan\n\n    return numpy.quantile(arr, q)\n\n\ndef gen_hpat_pandas_series_rolling_impl(rolling_func):\n    """"""Generate series rolling methods implementations based on input func""""""\n    def impl(self):\n        win = self._window\n        minp = self._min_periods\n\n        input_series = self._data\n        input_arr = input_series._data\n        length = len(input_arr)\n        output_arr = numpy.empty(length, dtype=float64)\n\n        def apply_minp(arr, minp):\n            finite_arr = arr[numpy.isfinite(arr)]\n            if len(finite_arr) < minp:\n                return numpy.nan\n            else:\n                return rolling_func(finite_arr)\n\n        boundary = min(win, length)\n        for i in prange(boundary):\n            arr_range = input_arr[:i + 1]\n            output_arr[i] = apply_minp(arr_range, minp)\n\n        for i in prange(boundary, length):\n            arr_range = input_arr[i + 1 - win:i + 1]\n            output_arr[i] = apply_minp(arr_range, minp)\n\n        return pandas.Series(output_arr, input_series._index, name=input_series._name)\n\n    return impl\n\n\ndef gen_hpat_pandas_series_rolling_ddof_impl(rolling_func):\n    """"""Generate series rolling methods implementations with parameter ddof""""""\n    def impl(self, ddof=1):\n        win = self._window\n        minp = self._min_periods\n\n        input_series = self._data\n        input_arr = input_series._data\n        length = len(input_arr)\n        output_arr = numpy.empty(length, dtype=float64)\n\n        def apply_minp(arr, ddof, minp):\n            finite_arr = arr[numpy.isfinite(arr)]\n            if len(finite_arr) < minp:\n                return numpy.nan\n            else:\n                return rolling_func(finite_arr, ddof)\n\n        boundary = min(win, length)\n        for i in prange(boundary):\n            arr_range = input_arr[:i + 1]\n            output_arr[i] = apply_minp(arr_range, ddof, minp)\n\n        for i in prange(boundary, length):\n            arr_range = input_arr[i + 1 - win:i + 1]\n            output_arr[i] = apply_minp(arr_range, ddof, minp)\n\n        return pandas.Series(output_arr, input_series._index, name=input_series._name)\n\n    return impl\n\n\nhpat_pandas_rolling_series_median_impl = register_jitable(\n    gen_hpat_pandas_series_rolling_impl(arr_median))\n\n\n@sdc_register_jitable\ndef pop_corr(x, y, nfinite, result):\n    """"""Calculate the window sums for corr without old value.""""""\n    sum_x, sum_y, sum_xy, sum_xx, sum_yy = result\n    if numpy.isfinite(x) and numpy.isfinite(y):\n        nfinite -= 1\n        sum_x -= x\n        sum_y -= y\n        sum_xy -= x * y\n        sum_xx -= x * x\n        sum_yy -= y * y\n\n    return nfinite, (sum_x, sum_y, sum_xy, sum_xx, sum_yy)\n\n\n@sdc_register_jitable\ndef put_corr(x, y, nfinite, result):\n    """"""Calculate the window sums for corr with new value.""""""\n    sum_x, sum_y, sum_xy, sum_xx, sum_yy = result\n    if numpy.isfinite(x) and numpy.isfinite(y):\n        nfinite += 1\n        sum_x += x\n        sum_y += y\n        sum_xy += x * y\n        sum_xx += x * x\n        sum_yy += y * y\n\n    return nfinite, (sum_x, sum_y, sum_xy, sum_xx, sum_yy)\n\n\n@sdc_register_jitable\ndef pop_count(value, nfinite, result):\n    """"""Calculate the window count without old value.""""""\n    if numpy.isnan(value):\n        return nfinite, result\n\n    return nfinite, result - 1\n\n\n@sdc_register_jitable\ndef put_count(value, nfinite, result):\n    """"""Calculate the window count with new value.""""""\n    if numpy.isnan(value):\n        return nfinite, result\n\n    return nfinite, result + 1\n\n\n@sdc_register_jitable\ndef result(nfinite, minp, result):\n    """"""Get result.""""""\n    return result\n\n\n@sdc_register_jitable\ndef pop_cov(x, y, nfinite, result, align_finiteness=False):\n    """"""Calculate the window sums for cov without old value.""""""\n    sum_x, sum_y, sum_xy, count = result\n    if numpy.isfinite(x) and numpy.isfinite(y):\n        nfinite -= 1\n        sum_x -= x\n        sum_y -= y\n        sum_xy -= x * y\n\n        return nfinite, (sum_x, sum_y, sum_xy, count - 1)\n\n    if align_finiteness or numpy.isnan(x) or numpy.isnan(y):\n        # alignment by finiteness means replacement of all the infinite values with nans\n        # count should NOT be recalculated in case of nans\n        return nfinite, result\n\n    return nfinite, (sum_x, sum_y, sum_xy, count - 1)\n\n\n@sdc_register_jitable\ndef put_cov(x, y, nfinite, result, align_finiteness=False):\n    """"""Calculate the window sums for cov with new value.""""""\n    sum_x, sum_y, sum_xy, count = result\n    if numpy.isfinite(x) and numpy.isfinite(y):\n        nfinite += 1\n        sum_x += x\n        sum_y += y\n        sum_xy += x * y\n\n        return nfinite, (sum_x, sum_y, sum_xy, count + 1)\n\n    if align_finiteness or numpy.isnan(x) or numpy.isnan(y):\n        # alignment by finiteness means replacement of all the infinite values with nans\n        # count should NOT be recalculated in case of nans\n        return nfinite, result\n\n    return nfinite, (sum_x, sum_y, sum_xy, count + 1)\n\n\n@sdc_register_jitable\ndef put_kurt(value, nfinite, result):\n    """"""Calculate the window sums for kurt with new value.""""""\n    _sum, square_sum, cube_sum, fourth_degree_sum = result\n    if numpy.isfinite(value):\n        nfinite += 1\n        _sum += value\n        square_sum += value * value\n        cube_sum += value * value * value\n        fourth_degree_sum += value * value * value * value\n\n    return nfinite, (_sum, square_sum, cube_sum, fourth_degree_sum)\n\n\n@sdc_register_jitable\ndef pop_kurt(value, nfinite, result):\n    """"""Calculate the window sums for kurt without old value.""""""\n    _sum, square_sum, cube_sum, fourth_degree_sum = result\n    if numpy.isfinite(value):\n        nfinite -= 1\n        _sum -= value\n        square_sum -= value * value\n        cube_sum -= value * value * value\n        fourth_degree_sum -= value * value * value * value\n\n    return nfinite, (_sum, square_sum, cube_sum, fourth_degree_sum)\n\n\n@sdc_register_jitable\ndef calc_max(arr, idx, win_size):\n    """"""Recalculate the window max based on data, index and window size.""""""\n    start = max(0, idx - win_size + 1)\n    nfinite = 0\n    result = numpy.nan\n    for i in range(start, idx + 1):\n        value = arr[i]\n        nfinite, result = put_max(value, nfinite, result)\n\n    return nfinite, result\n\n\n@sdc_register_jitable\ndef pop_max(value, nfinite, result, arr, idx, win_size):\n    """"""Calculate the window max without old value.""""""\n    if numpy.isfinite(value):\n        nfinite -= 1\n        if nfinite:\n            if value == result:\n                return calc_max(arr, idx, win_size)\n        else:\n            result = numpy.nan\n\n    return nfinite, result\n\n\n@sdc_register_jitable\ndef put_max(value, nfinite, result):\n    """"""Calculate the window max with new value.""""""\n    if numpy.isfinite(value):\n        nfinite += 1\n        if numpy.isnan(result) or value > result:\n            result = value\n\n    return nfinite, result\n\n\n@sdc_register_jitable\ndef calc_min(arr, idx, win_size):\n    """"""Recalculate the window min based on data, index and window size.""""""\n    start = max(0, idx - win_size + 1)\n    nfinite = 0\n    result = numpy.nan\n    for i in range(start, idx + 1):\n        value = arr[i]\n        nfinite, result = put_min(value, nfinite, result)\n\n    return nfinite, result\n\n\n@sdc_register_jitable\ndef pop_min(value, nfinite, result, arr, idx, win_size):\n    """"""Calculate the window min without old value.""""""\n    if numpy.isfinite(value):\n        nfinite -= 1\n        if nfinite:\n            if value == result:\n                return calc_min(arr, idx, win_size)\n        else:\n            result = numpy.nan\n\n    return nfinite, result\n\n\n@sdc_register_jitable\ndef put_min(value, nfinite, result):\n    """"""Calculate the window min with new value.""""""\n    if numpy.isfinite(value):\n        nfinite += 1\n        if numpy.isnan(result) or value < result:\n            result = value\n\n    return nfinite, result\n\n\n@sdc_register_jitable\ndef put_skew(value, nfinite, result):\n    """"""Calculate the window sums for skew with new value.""""""\n    _sum, square_sum, cube_sum = result\n    if numpy.isfinite(value):\n        nfinite += 1\n        _sum += value\n        square_sum += value * value\n        cube_sum += value * value * value\n\n    return nfinite, (_sum, square_sum, cube_sum)\n\n\n@sdc_register_jitable\ndef pop_skew(value, nfinite, result):\n    """"""Calculate the window sums for skew without old value.""""""\n    _sum, square_sum, cube_sum = result\n    if numpy.isfinite(value):\n        nfinite -= 1\n        _sum -= value\n        square_sum -= value * value\n        cube_sum -= value * value * value\n\n    return nfinite, (_sum, square_sum, cube_sum)\n\n\n@sdc_register_jitable\ndef pop_sum(value, nfinite, result):\n    """"""Calculate the window sum without old value.""""""\n    if numpy.isfinite(value):\n        nfinite -= 1\n        result -= value\n\n    return nfinite, result\n\n\n@sdc_register_jitable\ndef put_sum(value, nfinite, result):\n    """"""Calculate the window sum with new value.""""""\n    if numpy.isfinite(value):\n        nfinite += 1\n        result += value\n\n    return nfinite, result\n\n\n@sdc_register_jitable\ndef pop_sum2(value, nfinite, result):\n    """"""Calculate the window sums of first/second degree without old value.""""""\n    _sum, square_sum = result\n    if numpy.isfinite(value):\n        nfinite -= 1\n        _sum -= value\n        square_sum -= value * value\n\n    return nfinite, (_sum, square_sum)\n\n\n@sdc_register_jitable\ndef put_sum2(value, nfinite, result):\n    """"""Calculate the window sums of first/second degree with new value.""""""\n    _sum, square_sum = result\n    if numpy.isfinite(value):\n        nfinite += 1\n        _sum += value\n        square_sum += value * value\n\n    return nfinite, (_sum, square_sum)\n\n\n@sdc_register_jitable\ndef result_or_nan(nfinite, minp, result):\n    """"""Get result taking into account min periods.""""""\n    if nfinite < minp:\n        return numpy.nan\n\n    return result\n\n\n@sdc_register_jitable\ndef corr_result_or_nan(nfinite, minp, result):\n    """"""Get result corr taking into account min periods.""""""\n    if nfinite < max(1, minp):\n        return numpy.nan\n\n    sum_x, sum_y, sum_xy, sum_xx, sum_yy = result\n\n    var_x = sum_xx - sum_x * sum_x / nfinite\n    if _almost_equal(var_x, 0.):\n        return numpy.nan\n\n    var_y = sum_yy - sum_y * sum_y / nfinite\n    if _almost_equal(var_y, 0.):\n        return numpy.nan\n\n    cov_xy = sum_xy - sum_x * sum_y / nfinite\n\n    return cov_xy / numpy.sqrt(var_x * var_y)\n\n\n@sdc_register_jitable\ndef cov_result_or_nan(nfinite, minp, result, ddof):\n    """"""Get result of covariance taking into account min periods.""""""\n    if nfinite < max(1, minp):\n        return numpy.nan\n\n    sum_x, sum_y, sum_xy, count = result\n    res = (sum_xy - sum_x * sum_y / nfinite) / nfinite\n\n    return ddof_result(count, minp, res, ddof)\n\n\n@sdc_register_jitable\ndef kurt_result_or_nan(nfinite, minp, result):\n    """"""Get result kurt taking into account min periods.""""""\n    if nfinite < max(4, minp):\n        return numpy.nan\n\n    _sum, square_sum, cube_sum, fourth_degree_sum = result\n\n    n = nfinite\n    m2 = (square_sum - _sum * _sum / n) / n\n    m4 = (fourth_degree_sum - 4*_sum*cube_sum/n + 6*_sum*_sum*square_sum/n/n - 3*_sum*_sum*_sum*_sum/n/n/n) / n\n    res = 0 if m2 == 0 else m4 / m2 ** 2.0\n\n    if (n > 2) & (m2 > 0):\n        res = 1.0/(n-2)/(n-3) * ((n**2-1.0)*m4/m2**2.0 - 3*(n-1)**2.0)\n\n    return res\n\n\n@sdc_register_jitable\ndef mean_result_or_nan(nfinite, minp, result):\n    """"""Get result mean taking into account min periods.""""""\n    if nfinite == 0 or nfinite < minp:\n        return numpy.nan\n\n    return result / nfinite\n\n\n@sdc_register_jitable\ndef skew_result_or_nan(nfinite, minp, result):\n    """"""Get result skew taking into account min periods.""""""\n    if nfinite < max(3, minp):\n        return numpy.nan\n\n    _sum, square_sum, cube_sum = result\n\n    return skew_formula(nfinite, _sum, square_sum, cube_sum)\n\n\n@sdc_register_jitable\ndef ddof_result(nfinite, minp, result, ddof):\n    """"""Get result taking into account ddof.""""""\n    if nfinite - ddof < 1:\n        return numpy.nan\n\n    return result * nfinite / (nfinite - ddof)\n\n\n@sdc_register_jitable\ndef var_result_or_nan(nfinite, minp, result, ddof):\n    """"""Get result var taking into account min periods.""""""\n    if nfinite < max(1, minp):\n        return numpy.nan\n\n    _sum, square_sum = result\n    res = (square_sum - _sum * _sum / nfinite) / nfinite\n\n    return ddof_result(nfinite, minp, res, ddof)\n\n\n@sdc_register_jitable\ndef std_result_or_nan(nfinite, minp, result, ddof):\n    """"""Get result std taking into account min periods.""""""\n    return var_result_or_nan(nfinite, minp, result, ddof) ** 0.5\n\n\ndef gen_sdc_pandas_series_rolling_impl(pop, put, get_result=result_or_nan,\n                                       init_result=numpy.nan):\n    """"""Generate series rolling methods implementations based on pop/put funcs""""""\n    def impl(self):\n        win = self._window\n        minp = self._min_periods\n\n        input_series = self._data\n        input_arr = input_series._data\n        length = len(input_arr)\n        output_arr = numpy.empty(length, dtype=float64)\n\n        chunks = parallel_chunks(length)\n        for i in prange(len(chunks)):\n            chunk = chunks[i]\n            nfinite = 0\n            result = init_result\n\n            if win == 0:\n                for idx in range(chunk.start, chunk.stop):\n                    output_arr[idx] = get_result(nfinite, minp, result)\n                continue\n\n            prelude_start = max(0, chunk.start - win + 1)\n            prelude_stop = chunk.start\n\n            interlude_start = prelude_stop\n            interlude_stop = min(prelude_start + win, chunk.stop)\n\n            for idx in range(prelude_start, prelude_stop):\n                value = input_arr[idx]\n                nfinite, result = put(value, nfinite, result)\n\n            for idx in range(interlude_start, interlude_stop):\n                value = input_arr[idx]\n                nfinite, result = put(value, nfinite, result)\n                output_arr[idx] = get_result(nfinite, minp, result)\n\n            for idx in range(interlude_stop, chunk.stop):\n                put_value = input_arr[idx]\n                pop_value = input_arr[idx - win]\n                nfinite, result = put(put_value, nfinite, result)\n                nfinite, result = pop(pop_value, nfinite, result)\n                output_arr[idx] = get_result(nfinite, minp, result)\n\n        return pandas.Series(output_arr, input_series._index,\n                             name=input_series._name)\n    return impl\n\n\ndef gen_sdc_pandas_series_rolling_minmax_impl(pop, put, init_result=numpy.nan):\n    """"""Generate series rolling min/max implementations based on pop/put funcs""""""\n    def impl(self):\n        win = self._window\n        minp = self._min_periods\n\n        input_series = self._data\n        input_arr = input_series._data\n        length = len(input_arr)\n        output_arr = numpy.empty(length, dtype=float64)\n\n        chunks = parallel_chunks(length)\n        for i in prange(len(chunks)):\n            chunk = chunks[i]\n            nfinite = 0\n            result = init_result\n\n            if win == 0:\n                for idx in range(chunk.start, chunk.stop):\n                    output_arr[idx] = result_or_nan(nfinite, minp, result)\n                continue\n\n            prelude_start = max(0, chunk.start - win + 1)\n            prelude_stop = chunk.start\n\n            interlude_start = prelude_stop\n            interlude_stop = min(prelude_start + win, chunk.stop)\n\n            for idx in range(prelude_start, prelude_stop):\n                value = input_arr[idx]\n                nfinite, result = put(value, nfinite, result)\n\n            for idx in range(interlude_start, interlude_stop):\n                value = input_arr[idx]\n                nfinite, result = put(value, nfinite, result)\n                output_arr[idx] = result_or_nan(nfinite, minp, result)\n\n            for idx in range(interlude_stop, chunk.stop):\n                put_value = input_arr[idx]\n                pop_value = input_arr[idx - win]\n                nfinite, result = put(put_value, nfinite, result)\n                nfinite, result = pop(pop_value, nfinite, result,\n                                      input_arr, idx, win)\n                output_arr[idx] = result_or_nan(nfinite, minp, result)\n\n        return pandas.Series(output_arr, input_series._index,\n                             name=input_series._name)\n    return impl\n\n\ndef gen_sdc_pandas_series_rolling_ddof_impl(pop, put, get_result=ddof_result,\n                                            init_result=numpy.nan):\n    """"""Generate series rolling ddof implementations based on pop/put funcs""""""\n    def impl(self, ddof=1):\n        win = self._window\n        minp = self._min_periods\n\n        input_series = self._data\n        input_arr = input_series._data\n        length = len(input_arr)\n        output_arr = numpy.empty(length, dtype=float64)\n\n        chunks = parallel_chunks(length)\n        for i in prange(len(chunks)):\n            chunk = chunks[i]\n            nfinite = 0\n            result = init_result\n\n            if win == 0:\n                for idx in range(chunk.start, chunk.stop):\n                    output_arr[idx] = get_result(nfinite, minp, result, ddof)\n                continue\n\n            prelude_start = max(0, chunk.start - win + 1)\n            prelude_stop = chunk.start\n\n            interlude_start = prelude_stop\n            interlude_stop = min(prelude_start + win, chunk.stop)\n\n            for idx in range(prelude_start, prelude_stop):\n                value = input_arr[idx]\n                nfinite, result = put(value, nfinite, result)\n\n            for idx in range(interlude_start, interlude_stop):\n                value = input_arr[idx]\n                nfinite, result = put(value, nfinite, result)\n                output_arr[idx] = get_result(nfinite, minp, result, ddof)\n\n            for idx in range(interlude_stop, chunk.stop):\n                put_value = input_arr[idx]\n                pop_value = input_arr[idx - win]\n                nfinite, result = put(put_value, nfinite, result)\n                nfinite, result = pop(pop_value, nfinite, result)\n                output_arr[idx] = get_result(nfinite, minp, result, ddof)\n\n        return pandas.Series(output_arr, input_series._index,\n                             name=input_series._name)\n    return impl\n\n\nsdc_pandas_series_rolling_count_impl = gen_sdc_pandas_series_rolling_impl(\n    pop_count, put_count, get_result=result, init_result=0.)\nsdc_pandas_series_rolling_kurt_impl = gen_sdc_pandas_series_rolling_impl(\n    pop_kurt, put_kurt, get_result=kurt_result_or_nan,\n    init_result=(0., 0., 0., 0.))\nsdc_pandas_series_rolling_max_impl = gen_sdc_pandas_series_rolling_minmax_impl(\n    pop_max, put_max)\nsdc_pandas_series_rolling_mean_impl = gen_sdc_pandas_series_rolling_impl(\n    pop_sum, put_sum, get_result=mean_result_or_nan, init_result=0.)\nsdc_pandas_series_rolling_min_impl = gen_sdc_pandas_series_rolling_minmax_impl(\n    pop_min, put_min)\nsdc_pandas_series_rolling_skew_impl = gen_sdc_pandas_series_rolling_impl(\n    pop_skew, put_skew, get_result=skew_result_or_nan, init_result=(0., 0., 0.))\nsdc_pandas_series_rolling_sum_impl = gen_sdc_pandas_series_rolling_impl(\n    pop_sum, put_sum, init_result=0.)\nsdc_pandas_series_rolling_var_impl = gen_sdc_pandas_series_rolling_ddof_impl(\n    pop_sum2, put_sum2, get_result=var_result_or_nan, init_result=(0., 0.))\nsdc_pandas_series_rolling_std_impl = gen_sdc_pandas_series_rolling_ddof_impl(\n    pop_sum2, put_sum2, get_result=std_result_or_nan, init_result=(0., 0.))\n\n\n@sdc_rolling_overload(SeriesRollingType, \'apply\')\ndef hpat_pandas_series_rolling_apply(self, func, raw=None):\n\n    ty_checker = TypeChecker(\'Method rolling.apply().\')\n    ty_checker.check(self, SeriesRollingType)\n\n    raw_accepted = (Omitted, NoneType, Boolean)\n    if not isinstance(raw, raw_accepted) and raw is not None:\n        ty_checker.raise_exc(raw, \'bool\', \'raw\')\n\n    def hpat_pandas_rolling_series_apply_impl(self, func, raw=None):\n        win = self._window\n        minp = self._min_periods\n\n        input_series = self._data\n        input_arr = input_series._data\n        length = len(input_arr)\n        output_arr = numpy.empty(length, dtype=float64)\n\n        def culc_apply(arr, func, minp):\n            finite_arr = arr.copy()\n            finite_arr[numpy.isinf(arr)] = numpy.nan\n            if len(finite_arr) < minp:\n                return numpy.nan\n            else:\n                return arr_apply(finite_arr, func)\n\n        boundary = min(win, length)\n        for i in prange(boundary):\n            arr_range = input_arr[:i + 1]\n            output_arr[i] = culc_apply(arr_range, func, minp)\n\n        for i in prange(boundary, length):\n            arr_range = input_arr[i + 1 - win:i + 1]\n            output_arr[i] = culc_apply(arr_range, func, minp)\n\n        return pandas.Series(output_arr, input_series._index, name=input_series._name)\n\n    return hpat_pandas_rolling_series_apply_impl\n\n\n@sdc_overload_method(SeriesRollingType, \'corr\')\ndef hpat_pandas_series_rolling_corr(self, other=None, pairwise=None):\n\n    ty_checker = TypeChecker(\'Method rolling.corr().\')\n    ty_checker.check(self, SeriesRollingType)\n\n    accepted_other = (bool, Omitted, NoneType, SeriesType)\n    if not isinstance(other, accepted_other) and other is not None:\n        ty_checker.raise_exc(other, \'Series\', \'other\')\n\n    accepted_pairwise = (bool, Boolean, Omitted, NoneType)\n    if not isinstance(pairwise, accepted_pairwise) and pairwise is not None:\n        ty_checker.raise_exc(pairwise, \'bool\', \'pairwise\')\n\n    nan_other = isinstance(other, (Omitted, NoneType)) or other is None\n\n    def hpat_pandas_rolling_series_corr_impl(self, other=None, pairwise=None):\n        win = self._window\n        minp = self._min_periods\n\n        main_series = self._data\n        main_arr = main_series._data\n\n        if nan_other == True:  # noqa\n            other_arr = main_arr\n        else:\n            other_arr = other._data\n\n        main_arr_length = len(main_arr)\n        other_arr_length = len(other_arr)\n        min_length = min(main_arr_length, other_arr_length)\n        length = max(main_arr_length, other_arr_length)\n        output_arr = numpy.empty(length, dtype=float64)\n\n        chunks = parallel_chunks(length)\n        for i in prange(len(chunks)):\n            chunk = chunks[i]\n            nfinite = 0\n            result = (0., 0., 0., 0., 0.)\n\n            if win == 0:\n                for idx in range(chunk.start, chunk.stop):\n                    output_arr[idx] = corr_result_or_nan(nfinite, minp, result)\n                continue\n\n            prelude_start = max(0, chunk.start - win + 1)\n            prelude_stop = min(chunk.start, min_length)\n\n            interlude_start = chunk.start\n            interlude_stop = min(prelude_start + win, chunk.stop, min_length)\n\n            postlude_start = min(prelude_start + win, chunk.stop)\n            postlude_stop = min(chunk.stop, min_length)\n\n            for idx in range(prelude_start, prelude_stop):\n                x, y = main_arr[idx], other_arr[idx]\n                nfinite, result = put_corr(x, y, nfinite, result)\n\n            for idx in range(interlude_start, interlude_stop):\n                x, y = main_arr[idx], other_arr[idx]\n                nfinite, result = put_corr(x, y, nfinite, result)\n                output_arr[idx] = corr_result_or_nan(nfinite, minp, result)\n\n            for idx in range(postlude_start, postlude_stop):\n                put_x, put_y = main_arr[idx], other_arr[idx]\n                pop_x, pop_y = main_arr[idx - win], other_arr[idx - win]\n                nfinite, result = put_corr(put_x, put_y, nfinite, result)\n                nfinite, result = pop_corr(pop_x, pop_y, nfinite, result)\n                output_arr[idx] = corr_result_or_nan(nfinite, minp, result)\n\n            last_start = max(min_length, interlude_start)\n            for idx in range(last_start, postlude_start):\n                output_arr[idx] = corr_result_or_nan(nfinite, minp, result)\n\n            last_start = max(min_length, postlude_start)\n            last_stop = min(min_length + win, chunk.stop)\n            for idx in range(last_start, last_stop):\n                x, y = main_arr[idx - win], other_arr[idx - win]\n                nfinite, result = pop_corr(x, y, nfinite, result)\n                output_arr[idx] = corr_result_or_nan(nfinite, minp, result)\n\n            for idx in range(last_stop, chunk.stop):\n                output_arr[idx] = numpy.nan\n\n        return pandas.Series(output_arr)\n\n    return hpat_pandas_rolling_series_corr_impl\n\n\n@sdc_overload_method(SeriesRollingType, \'count\')\ndef hpat_pandas_series_rolling_count(self):\n\n    ty_checker = TypeChecker(\'Method rolling.count().\')\n    ty_checker.check(self, SeriesRollingType)\n\n    return sdc_pandas_series_rolling_count_impl\n\n\ndef _hpat_pandas_series_rolling_cov_check_types(self, other=None,\n                                                pairwise=None, ddof=1):\n    """"""Check types of parameters of series.rolling.cov()""""""\n    ty_checker = TypeChecker(\'Method rolling.cov().\')\n    ty_checker.check(self, SeriesRollingType)\n\n    accepted_other = (bool, Omitted, NoneType, SeriesType)\n    if not isinstance(other, accepted_other) and other is not None:\n        ty_checker.raise_exc(other, \'Series\', \'other\')\n\n    accepted_pairwise = (bool, Boolean, Omitted, NoneType)\n    if not isinstance(pairwise, accepted_pairwise) and pairwise is not None:\n        ty_checker.raise_exc(pairwise, \'bool\', \'pairwise\')\n\n    if not isinstance(ddof, (int, Integer, Omitted)):\n        ty_checker.raise_exc(ddof, \'int\', \'ddof\')\n\n\ndef _gen_hpat_pandas_rolling_series_cov_impl(other, align_finiteness=False):\n    """"""Generate series.rolling.cov() implementation based on series alignment""""""\n    nan_other = isinstance(other, (Omitted, NoneType)) or other is None\n\n    def _impl(self, other=None, pairwise=None, ddof=1):\n        win = self._window\n        minp = self._min_periods\n\n        main_series = self._data\n        main_arr = main_series._data\n\n        if nan_other == True:  # noqa\n            other_arr = main_arr\n        else:\n            other_arr = other._data\n\n        main_arr_length = len(main_arr)\n        other_arr_length = len(other_arr)\n        min_length = min(main_arr_length, other_arr_length)\n        length = max(main_arr_length, other_arr_length)\n        output_arr = numpy.empty(length, dtype=float64)\n\n        chunks = parallel_chunks(length)\n        for i in prange(len(chunks)):\n            chunk = chunks[i]\n            nfinite = 0\n            result = (0., 0., 0., 0.)\n\n            if win == 0:\n                for idx in range(chunk.start, chunk.stop):\n                    output_arr[idx] = cov_result_or_nan(nfinite, minp, result, ddof)\n                continue\n\n            prelude_start = max(0, chunk.start - win + 1)\n            prelude_stop = min(chunk.start, min_length)\n\n            interlude_start = chunk.start\n            interlude_stop = min(prelude_start + win, chunk.stop, min_length)\n\n            postlude_start = min(prelude_start + win, chunk.stop)\n            postlude_stop = min(chunk.stop, min_length)\n\n            for idx in range(prelude_start, prelude_stop):\n                x, y = main_arr[idx], other_arr[idx]\n                nfinite, result = put_cov(x, y, nfinite, result,\n                                          align_finiteness=align_finiteness)\n\n            for idx in range(interlude_start, interlude_stop):\n                x, y = main_arr[idx], other_arr[idx]\n                nfinite, result = put_cov(x, y, nfinite, result,\n                                          align_finiteness=align_finiteness)\n                output_arr[idx] = cov_result_or_nan(nfinite, minp, result, ddof)\n\n            for idx in range(postlude_start, postlude_stop):\n                put_x, put_y = main_arr[idx], other_arr[idx]\n                pop_x, pop_y = main_arr[idx - win], other_arr[idx - win]\n                nfinite, result = put_cov(put_x, put_y, nfinite, result,\n                                          align_finiteness=align_finiteness)\n                nfinite, result = pop_cov(pop_x, pop_y, nfinite, result,\n                                          align_finiteness=align_finiteness)\n                output_arr[idx] = cov_result_or_nan(nfinite, minp, result, ddof)\n\n            last_start = max(min_length, interlude_start)\n            for idx in range(last_start, postlude_start):\n                output_arr[idx] = cov_result_or_nan(nfinite, minp, result, ddof)\n\n            last_start = max(min_length, postlude_start)\n            last_stop = min(min_length + win, chunk.stop)\n            for idx in range(last_start, last_stop):\n                x, y = main_arr[idx - win], other_arr[idx - win]\n                nfinite, result = pop_cov(x, y, nfinite, result,\n                                          align_finiteness=align_finiteness)\n                output_arr[idx] = cov_result_or_nan(nfinite, minp, result, ddof)\n\n            for idx in range(last_stop, chunk.stop):\n                output_arr[idx] = numpy.nan\n\n        return pandas.Series(output_arr)\n\n    return _impl\n\n\n@sdc_overload_method(SeriesRollingType, \'cov\')\ndef hpat_pandas_series_rolling_cov(self, other=None, pairwise=None, ddof=1):\n    _hpat_pandas_series_rolling_cov_check_types(self, other=other,\n                                                pairwise=pairwise, ddof=ddof)\n\n    return _gen_hpat_pandas_rolling_series_cov_impl(other, align_finiteness=True)\n\n\n@sdc_overload_method(SeriesRollingType, \'_df_cov\')\ndef hpat_pandas_series_rolling_cov(self, other=None, pairwise=None, ddof=1):\n    _hpat_pandas_series_rolling_cov_check_types(self, other=other,\n                                                pairwise=pairwise, ddof=ddof)\n\n    return _gen_hpat_pandas_rolling_series_cov_impl(other)\n\n\n@sdc_overload_method(SeriesRollingType, \'kurt\')\ndef hpat_pandas_series_rolling_kurt(self):\n\n    ty_checker = TypeChecker(\'Method rolling.kurt().\')\n    ty_checker.check(self, SeriesRollingType)\n\n    return sdc_pandas_series_rolling_kurt_impl\n\n\n@sdc_overload_method(SeriesRollingType, \'max\')\ndef hpat_pandas_series_rolling_max(self):\n\n    ty_checker = TypeChecker(\'Method rolling.max().\')\n    ty_checker.check(self, SeriesRollingType)\n\n    return sdc_pandas_series_rolling_max_impl\n\n\n@sdc_overload_method(SeriesRollingType, \'mean\')\ndef hpat_pandas_series_rolling_mean(self):\n\n    ty_checker = TypeChecker(\'Method rolling.mean().\')\n    ty_checker.check(self, SeriesRollingType)\n\n    return sdc_pandas_series_rolling_mean_impl\n\n\n@sdc_rolling_overload(SeriesRollingType, \'median\')\ndef hpat_pandas_series_rolling_median(self):\n\n    ty_checker = TypeChecker(\'Method rolling.median().\')\n    ty_checker.check(self, SeriesRollingType)\n\n    return hpat_pandas_rolling_series_median_impl\n\n\n@sdc_overload_method(SeriesRollingType, \'min\')\ndef hpat_pandas_series_rolling_min(self):\n\n    ty_checker = TypeChecker(\'Method rolling.min().\')\n    ty_checker.check(self, SeriesRollingType)\n\n    return sdc_pandas_series_rolling_min_impl\n\n@sdc_rolling_overload(SeriesRollingType, \'quantile\')\ndef hpat_pandas_series_rolling_quantile(self, quantile, interpolation=\'linear\'):\n\n    ty_checker = TypeChecker(\'Method rolling.quantile().\')\n    ty_checker.check(self, SeriesRollingType)\n\n    if not isinstance(quantile, Number):\n        ty_checker.raise_exc(quantile, \'float\', \'quantile\')\n\n    str_types = (Omitted, StringLiteral, UnicodeType)\n    if not isinstance(interpolation, str_types) and interpolation != \'linear\':\n        ty_checker.raise_exc(interpolation, \'str\', \'interpolation\')\n\n    def hpat_pandas_rolling_series_quantile_impl(self, quantile, interpolation=\'linear\'):\n        if quantile < 0 or quantile > 1:\n            raise ValueError(\'quantile value not in [0, 1]\')\n        if interpolation != \'linear\':\n            raise ValueError(\'interpolation value not ""linear""\')\n\n        win = self._window\n        minp = self._min_periods\n\n        input_series = self._data\n        input_arr = input_series._data\n        length = len(input_arr)\n        output_arr = numpy.empty(length, dtype=float64)\n\n        def calc_quantile(arr, quantile, minp):\n            finite_arr = arr[numpy.isfinite(arr)]\n            if len(finite_arr) < minp:\n                return numpy.nan\n            else:\n                return arr_quantile(finite_arr, quantile)\n\n        boundary = min(win, length)\n        for i in prange(boundary):\n            arr_range = input_arr[:i + 1]\n            output_arr[i] = calc_quantile(arr_range, quantile, minp)\n\n        for i in prange(boundary, length):\n            arr_range = input_arr[i + 1 - win:i + 1]\n            output_arr[i] = calc_quantile(arr_range, quantile, minp)\n\n        return pandas.Series(output_arr, input_series._index, name=input_series._name)\n\n    return hpat_pandas_rolling_series_quantile_impl\n\n\n@sdc_overload_method(SeriesRollingType, \'skew\')\ndef hpat_pandas_series_rolling_skew(self):\n\n    ty_checker = TypeChecker(\'Method rolling.skew().\')\n    ty_checker.check(self, SeriesRollingType)\n\n    return sdc_pandas_series_rolling_skew_impl\n\n\n@sdc_overload_method(SeriesRollingType, \'std\')\ndef hpat_pandas_series_rolling_std(self, ddof=1):\n\n    ty_checker = TypeChecker(\'Method rolling.std().\')\n    ty_checker.check(self, SeriesRollingType)\n\n    if not isinstance(ddof, (int, Integer, Omitted)):\n        ty_checker.raise_exc(ddof, \'int\', \'ddof\')\n\n    return sdc_pandas_series_rolling_std_impl\n\n\n@sdc_overload_method(SeriesRollingType, \'sum\')\ndef hpat_pandas_series_rolling_sum(self):\n\n    ty_checker = TypeChecker(\'Method rolling.sum().\')\n    ty_checker.check(self, SeriesRollingType)\n\n    return sdc_pandas_series_rolling_sum_impl\n\n\n@sdc_overload_method(SeriesRollingType, \'var\')\ndef hpat_pandas_series_rolling_var(self, ddof=1):\n\n    ty_checker = TypeChecker(\'Method rolling.var().\')\n    ty_checker.check(self, SeriesRollingType)\n\n    if not isinstance(ddof, (int, Integer, Omitted)):\n        ty_checker.raise_exc(ddof, \'int\', \'ddof\')\n\n    return sdc_pandas_series_rolling_var_impl\n\n\nhpat_pandas_series_rolling_apply.__doc__ = hpat_pandas_series_rolling_docstring_tmpl.format(**{\n    \'method_name\': \'apply\',\n    \'example_caption\': \'Calculate the rolling apply.\',\n    \'limitations_block\':\n    """"""\n    Limitations\n    -----------\n    - This function may reveal slower performance than Pandas* on user system. Users should exercise a tradeoff\n    between staying in JIT-region with that function or going back to interpreter mode.\n    - Supported ``raw`` only can be `None` or `True`. Parameters ``args``, ``kwargs`` unsupported.\n    - DataFrame/Series elements cannot be max/min float/integer. Otherwise SDC and Pandas results are different.\n    """""",\n    \'extra_params\':\n    """"""\n    func:\n        A single value producer\n    raw: :obj:`bool`\n        False : passes each row or column as a Series to the function.\n        True or None : the passed function will receive ndarray objects instead.\n    """"""\n})\n\nhpat_pandas_series_rolling_corr.__doc__ = hpat_pandas_series_rolling_docstring_tmpl.format(**{\n    \'method_name\': \'corr\',\n    \'example_caption\': \'Calculate rolling correlation.\',\n    \'limitations_block\':\n    """"""\n    Limitations\n    -----------\n    DataFrame/Series elements cannot be max/min float/integer. Otherwise SDC and Pandas results are different.\n    Resulting DataFrame/Series has default index and name.\n    """""",\n    \'extra_params\':\n    """"""\n    other: :obj:`DataFrame` or :obj:`Series`\n        Other DataFrame/Series.\n    pairwise: :obj:`bool`\n        Not relevant for Series.\n    """"""\n})\n\nhpat_pandas_series_rolling_count.__doc__ = hpat_pandas_series_rolling_docstring_tmpl.format(**{\n    \'method_name\': \'count\',\n    \'example_caption\': \'Count of any non-NaN observations inside the window.\',\n    \'limitations_block\': \'\',\n    \'extra_params\': \'\'\n})\n\nhpat_pandas_series_rolling_cov.__doc__ = hpat_pandas_series_rolling_docstring_tmpl.format(**{\n    \'method_name\': \'cov\',\n    \'example_caption\': \'Calculate rolling covariance.\',\n    \'limitations_block\':\n    """"""\n    Limitations\n    -----------\n    DataFrame/Series elements cannot be max/min float/integer. Otherwise SDC and Pandas results are different.\n    Resulting DataFrame/Series has default index and name.\n    """""",\n    \'extra_params\':\n    """"""\n    other: :obj:`DataFrame` or :obj:`Series`\n        Other DataFrame/Series.\n    pairwise: :obj:`bool`\n        Not relevant for Series.\n    ddof: :obj:`int`\n        Delta Degrees of Freedom.\n    """"""\n})\n\nhpat_pandas_series_rolling_kurt.__doc__ = hpat_pandas_series_rolling_docstring_tmpl.format(**{\n    \'method_name\': \'kurt\',\n    \'example_caption\': \'Calculate unbiased rolling kurtosis.\',\n    \'limitations_block\': \'\',\n    \'extra_params\': \'\'\n})\n\nhpat_pandas_series_rolling_max.__doc__ = hpat_pandas_series_rolling_docstring_tmpl.format(**{\n    \'method_name\': \'max\',\n    \'example_caption\': \'Calculate the rolling maximum.\',\n    \'limitations_block\': \'\',\n    \'extra_params\': \'\'\n})\n\nhpat_pandas_series_rolling_mean.__doc__ = hpat_pandas_series_rolling_docstring_tmpl.format(**{\n    \'method_name\': \'mean\',\n    \'example_caption\': \'Calculate the rolling mean of the values.\',\n    \'limitations_block\':\n    """"""\n    Limitations\n    -----------\n    DataFrame/Series elements cannot be max/min float/integer. Otherwise SDC and Pandas results are different.\n    """""",\n    \'extra_params\': \'\'\n})\n\nhpat_pandas_series_rolling_median.__doc__ = hpat_pandas_series_rolling_docstring_tmpl.format(**{\n    \'method_name\': \'median\',\n    \'example_caption\': \'Calculate the rolling median.\',\n    \'limitations_block\':\n    """"""\n    Limitations\n    -----------\n    This function may reveal slower performance than Pandas* on user system. Users should exercise a tradeoff\n    between staying in JIT-region with that function or going back to interpreter mode.\n    """""",\n    \'extra_params\': \'\'\n})\n\nhpat_pandas_series_rolling_min.__doc__ = hpat_pandas_series_rolling_docstring_tmpl.format(**{\n    \'method_name\': \'min\',\n    \'example_caption\': \'Calculate the rolling minimum.\',\n    \'limitations_block\': \'\',\n    \'extra_params\': \'\'\n})\n\nhpat_pandas_series_rolling_quantile.__doc__ = hpat_pandas_series_rolling_docstring_tmpl.format(**{\n    \'method_name\': \'quantile\',\n    \'example_caption\': \'Calculate the rolling quantile.\',\n    \'limitations_block\':\n    """"""\n    Limitations\n    -----------\n    This function may reveal slower performance than Pandas* on user system. Users should exercise a tradeoff\n    between staying in JIT-region with that function or going back to interpreter mode.\n    Supported ``interpolation`` only can be `\'linear\'`.\n    DataFrame/Series elements cannot be max/min float/integer. Otherwise SDC and Pandas results are different.\n    """""",\n    \'extra_params\':\n    """"""\n    quantile: :obj:`float`\n        Quantile to compute. 0 <= quantile <= 1.\n    interpolation: :obj:`str`\n        This optional parameter specifies the interpolation method to use.\n    """"""\n})\n\nhpat_pandas_series_rolling_skew.__doc__ = hpat_pandas_series_rolling_docstring_tmpl.format(**{\n    \'method_name\': \'skew\',\n    \'example_caption\': \'Unbiased rolling skewness.\',\n    \'limitations_block\': \'\',\n    \'extra_params\': \'\'\n})\n\nhpat_pandas_series_rolling_std.__doc__ = hpat_pandas_series_rolling_docstring_tmpl.format(**{\n    \'method_name\': \'std\',\n    \'example_caption\': \'Calculate rolling standard deviation.\',\n    \'limitations_block\':\n    """"""\n    Limitations\n    -----------\n    DataFrame/Series elements cannot be max/min float/integer. Otherwise SDC and Pandas results are different.\n    """""",\n    \'extra_params\':\n    """"""\n    ddof: :obj:`int`\n        Delta Degrees of Freedom.\n    """"""\n})\n\nhpat_pandas_series_rolling_sum.__doc__ = hpat_pandas_series_rolling_docstring_tmpl.format(**{\n    \'method_name\': \'sum\',\n    \'example_caption\': \'Calculate rolling sum\',\n    \'limitations_block\':\n    """"""\n    Limitations\n    -----------\n    DataFrame/Series elements cannot be max/min float/integer. Otherwise SDC and Pandas results are different.\n    """""",\n    \'extra_params\': \'\'\n})\n\nhpat_pandas_series_rolling_var.__doc__ = hpat_pandas_series_rolling_docstring_tmpl.format(**{\n    \'method_name\': \'var\',\n    \'example_caption\': \'Calculate unbiased rolling variance.\',\n    \'limitations_block\':\n    """"""\n    Limitations\n    -----------\n    DataFrame/Series elements cannot be max/min float/integer. Otherwise SDC and Pandas results are different.\n    """""",\n    \'extra_params\':\n    """"""\n    ddof: :obj:`int`\n        Delta Degrees of Freedom.\n    """"""\n})\n'"
sdc/datatypes/hpat_pandas_series_rolling_types.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nfrom numba.extending import intrinsic, register_model\nfrom sdc.datatypes.hpat_pandas_rolling_types import (\n    gen_hpat_pandas_rolling_init, RollingType, RollingTypeModel)\n\n\nclass SeriesRollingType(RollingType):\n    """"""Type definition for pandas.Series.rolling functions handling.""""""\n    def __init__(self, data, win_type=None, on=None, closed=None):\n        super(SeriesRollingType, self).__init__(\'SeriesRollingType\',\n                                                data, win_type=win_type,\n                                                on=on, closed=closed)\n\n\n@register_model(SeriesRollingType)\nclass SeriesRollingTypeModel(RollingTypeModel):\n    """"""Model for SeriesRollingType type.""""""\n    def __init__(self, dmm, fe_type):\n        super(SeriesRollingTypeModel, self).__init__(dmm, fe_type)\n\n\n_hpat_pandas_series_rolling_init = intrinsic(gen_hpat_pandas_rolling_init(\n    SeriesRollingType))\n'"
sdc/datatypes/hpat_pandas_stringmethods_functions.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\n\n| :class:`pandas.core.strings.StringMethods` functions and operators implementations in HPAT\n\n    .. only:: developer\n\n    This is autogenerated sources for all Unicode string functions supported by Numba.\n    Currently tested 45 functions only. List of functions obtained automatically from\n    `numba.types.misc.UnicodeType` class\n\n    Example of the generated method (for method upper()):\n    `hpat_pandas_stringmethods_upper_parallel_impl` is paralell version\n    (required additional import mentioned in the body)\n\n    @sdc_overload_method(StringMethodsType, \'upper\')\n    def hpat_pandas_stringmethods_upper(self):\n\n        ty_checker = TypeChecker(\'Method stringmethods.upper().\')\n        ty_checker.check(self, StringMethodsType)\n\n        def hpat_pandas_stringmethods_upper_parallel_impl(self):\n            from numba.parfors.parfor import (init_prange, min_checker, internal_prange)\n\n            init_prange()\n            result = []\n            item_count = len(self._data)\n            min_checker(item_count)\n            for i in internal_prange(item_count):\n                item = self._data[i]\n                item_method = item.upper()\n                result.append(item_method)\n\n            return pandas.Series(result)\n\n        return hpat_pandas_stringmethods_upper_parallel_impl\n\n        def hpat_pandas_stringmethods_upper_impl(self):\n            result = []\n            item_count = len(self._data)\n            for i in range(item_count):\n                item = self._data[i]\n                item_method = item.upper()\n                result.append(item_method)\n\n            return pandas.Series(result)\n\n        return hpat_pandas_stringmethods_upper_impl\n\n    Test: python -m sdc.runtests sdc.tests.test_hiframes.TestHiFrames.test_str_split_filter\n\n""""""\n\n\nimport numpy\nimport pandas\n\nimport numba\nfrom numba.core.types import (Boolean, Integer, NoneType,\n                         Omitted, StringLiteral, UnicodeType)\n\nfrom sdc.utilities.sdc_typing_utils import TypeChecker\nfrom sdc.datatypes.hpat_pandas_stringmethods_types import StringMethodsType\nfrom sdc.utilities.utils import sdc_overload_method, sdc_register_jitable\nfrom sdc.hiframes.api import get_nan_mask\nfrom sdc.str_arr_ext import str_arr_set_na_by_mask, create_str_arr_from_list\nfrom sdc.datatypes.common_functions import SDCLimitation\n\n\n@sdc_overload_method(StringMethodsType, \'center\')\ndef hpat_pandas_stringmethods_center(self, width, fillchar=\' \'):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n    Pandas API: pandas.Series.str.center\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/str/series_str_center.py\n       :language: python\n       :lines: 27-\n       :caption: Filling left and right side of strings in the Series with an additional character\n       :name: ex_series_str_center\n\n    .. command-output:: python ./series/str/series_str_center.py\n       :cwd: ../../../examples\n\n    .. seealso::\n        :ref:`Series.str.rjust <pandas.Series.str.rjust>`\n            Fills the left side of strings with an arbitrary character.\n        :ref:`Series.str.ljust <pandas.Series.str.ljust>`\n            Fills the right side of strings with an arbitrary character.\n\n    .. todo:: Add support of 32-bit Unicode for `str.center()`\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n\n    Pandas Series method :meth:`pandas.core.strings.StringMethods.center()` implementation.\n\n    .. only:: developer\n\n    Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_center\n    """"""\n\n    ty_checker = TypeChecker(\'Method center().\')\n    ty_checker.check(self, StringMethodsType)\n\n    if not isinstance(width, Integer):\n        ty_checker.raise_exc(width, \'int\', \'width\')\n\n    accepted_types = (Omitted, StringLiteral, UnicodeType)\n    if not isinstance(fillchar, accepted_types) and fillchar != \' \':\n        ty_checker.raise_exc(fillchar, \'str\', \'fillchar\')\n\n    def hpat_pandas_stringmethods_center_impl(self, width, fillchar=\' \'):\n        mask = get_nan_mask(self._data._data)\n        item_count = len(self._data)\n        res_list = [\'\'] * item_count\n        for idx in numba.prange(item_count):\n            res_list[idx] = self._data._data[idx].center(width, fillchar)\n        str_arr = create_str_arr_from_list(res_list)\n        result = str_arr_set_na_by_mask(str_arr, mask)\n\n        return pandas.Series(result, self._data._index, name=self._data._name)\n\n    return hpat_pandas_stringmethods_center_impl\n\n\n@sdc_overload_method(StringMethodsType, \'contains\')\ndef hpat_pandas_stringmethods_contains(self, pat, case=True, flags=0, na=None, regex=True):\n    """"""\n        Intel Scalable Dataframe Compiler User Guide\n        ********************************************\n        Pandas API: pandas.Series.str.contains\n\n        Limitations\n        -----------\n        - Series elements are expected to be Unicode strings. Elements cannot be `NaNs`.\n        - Parameter ``na`` is supported only with default value ``None``.\n        - Parameter ``flags`` is supported only with default value ``0``.\n        - Parameter ``regex`` is supported only with default value ``True``.\n\n        Examples\n        --------\n        .. literalinclude:: ../../../examples/series/str/series_str_contains.py\n           :language: python\n           :lines: 27-\n           :caption: Tests if string element contains a pattern.\n           :name: ex_series_str_contains\n\n        .. command-output:: python ./series/str/series_str_contains.py\n           :cwd: ../../../examples\n\n        .. seealso::\n            :ref:`Series.str.startswith <pandas.Series.str.startswith>`\n                Same as endswith, but tests the start of string.\n            :ref:`Series.str.endswith <pandas.Series.str.endswith>`\n                Same as startswith, but tests the end of string.\n\n        Intel Scalable Dataframe Compiler Developer Guide\n        *************************************************\n\n        Pandas Series method :meth:`pandas.core.strings.StringMethods.contains()` implementation.\n\n        .. only:: developer\n\n        Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_contains\n        """"""\n\n    ty_checker = TypeChecker(\'Method contains().\')\n    ty_checker.check(self, StringMethodsType)\n\n    if not isinstance(pat, (StringLiteral, UnicodeType)):\n        ty_checker.raise_exc(pat, \'str\', \'pat\')\n\n    if not isinstance(na, (Omitted, NoneType)) and na is not None:\n        ty_checker.raise_exc(na, \'none\', \'na\')\n\n    if not isinstance(case, (Boolean, Omitted)) and case is not True:\n        ty_checker.raise_exc(case, \'bool\', \'case\')\n\n    if not isinstance(flags, (Omitted, Integer)) and flags != 0:\n        ty_checker.raise_exc(flags, \'int64\', \'flags\')\n\n    if not isinstance(regex, (Omitted, Boolean)) and regex is not True:\n        ty_checker.raise_exc(regex, \'bool\', \'regex\')\n\n    def hpat_pandas_stringmethods_contains_impl(self, pat, case=True, flags=0, na=None, regex=True):\n        if flags != 0:\n            raise SDCLimitation(""Method contains(). Unsupported parameter. Given \'flags\' != 0"")\n\n        if not regex:\n            raise SDCLimitation(""Method contains(). Unsupported parameter. Given \'regex\' is False"")\n\n        if not case:\n            _pat = pat.lower()\n        else:\n            _pat = pat\n\n        len_data = len(self._data)\n        res_list = numpy.empty(len_data, numba.types.boolean)\n        for idx in numba.prange(len_data):\n            res_list[idx] = _pat in self._data._data[idx]\n\n        return pandas.Series(res_list, self._data._index, name=self._data._name)\n\n    return hpat_pandas_stringmethods_contains_impl\n\n\n@sdc_overload_method(StringMethodsType, \'endswith\')\ndef hpat_pandas_stringmethods_endswith(self, pat, na=None):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n    Pandas API: pandas.Series.str.endswith\n\n    Limitations\n    -----------\n    Series elements are expected to be Unicode strings. Elements cannot be `NaNs`.\n    Parameter ``na`` is supported only with default value ``None``.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/str/series_str_endswith.py\n       :language: python\n       :lines: 27-\n       :caption: Test if the end of each string element matches a string\n       :name: ex_series_str_endswith\n\n    .. command-output:: python ./series/str/series_str_endswith.py\n       :cwd: ../../../examples\n\n    .. seealso::\n        `str.endswith <https://docs.python.org/3/library/stdtypes.html#str.endswith>`_\n            Python standard library string method.\n        :ref:`Series.str.startswith <pandas.Series.str.startswith>`\n            Same as endswith, but tests the start of string.\n        :ref:`Series.str.contains <pandas.Series.str.contains>`\n            Tests if string element contains a pattern.\n\n    .. todo::\n        - Add support of matching the end of each string by a pattern\n        - Add support of parameter ``na``\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n\n    Pandas Series method :meth:`pandas.core.strings.StringMethods.endswith()` implementation.\n\n    .. only:: developer\n\n    Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_endswith\n    """"""\n\n    ty_checker = TypeChecker(\'Method endswith().\')\n    ty_checker.check(self, StringMethodsType)\n\n    if not isinstance(pat, (StringLiteral, UnicodeType)):\n        ty_checker.raise_exc(pat, \'str\', \'pat\')\n\n    if not isinstance(na, (Boolean, NoneType, Omitted)) and na is not None:\n        ty_checker.raise_exc(na, \'bool\', \'na\')\n\n    def hpat_pandas_stringmethods_endswith_impl(self, pat, na=None):\n        if na is not None:\n            msg = \'Method endswith(). The object na\\n expected: None\'\n            raise ValueError(msg)\n\n        item_endswith = len(self._data)\n        result = numpy.empty(item_endswith, numba.types.boolean)\n        for idx, item in enumerate(self._data._data):\n            result[idx] = item.endswith(pat)\n\n        return pandas.Series(result, self._data._index, name=self._data._name)\n\n    return hpat_pandas_stringmethods_endswith_impl\n\n\n@sdc_overload_method(StringMethodsType, \'find\')\ndef hpat_pandas_stringmethods_find(self, sub, start=0, end=None):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n    Pandas API: pandas.Series.str.find\n\n    Limitations\n    -----------\n    Series elements are expected to be Unicode strings. Elements cannot be `NaNs`.\n    Parameters ``start``, ``end`` are supported only with default value ``0`` and ``None`` respectively.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/str/series_str_find.py\n       :language: python\n       :lines: 27-\n       :caption: Return lowest indexes in each strings in the Series\n       :name: ex_series_str_find\n\n    .. command-output:: python ./series/str/series_str_find.py\n       :cwd: ../../../examples\n\n    .. seealso::\n        :ref:`Series.str.rfind <pandas.Series.str.rfind>`\n            Return highest indexes in each strings.\n\n    .. todo:: Add support of parameters ``start`` and ``end``\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n\n    Pandas Series method :meth:`pandas.core.strings.StringMethods.find()` implementation.\n\n    .. only:: developer\n\n    Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_find\n    """"""\n\n    ty_checker = TypeChecker(\'Method find().\')\n    ty_checker.check(self, StringMethodsType)\n\n    if not isinstance(sub, (StringLiteral, UnicodeType)):\n        ty_checker.raise_exc(sub, \'str\', \'sub\')\n\n    accepted_types = (Integer, NoneType, Omitted)\n    if not isinstance(start, accepted_types) and start != 0:\n        ty_checker.raise_exc(start, \'None, int\', \'start\')\n\n    if not isinstance(end, accepted_types) and end is not None:\n        ty_checker.raise_exc(end, \'None, int\', \'end\')\n\n    def hpat_pandas_stringmethods_find_impl(self, sub, start=0, end=None):\n        if start != 0:\n            raise ValueError(\'Method find(). The object start\\n expected: 0\')\n        if end is not None:\n            raise ValueError(\'Method find(). The object end\\n expected: None\')\n\n        item_count = len(self._data)\n        result = numpy.empty(item_count, numba.types.int64)\n        for idx, item in enumerate(self._data._data):\n            result[idx] = item.find(sub)\n\n        return pandas.Series(result, self._data._index, name=self._data._name)\n\n    return hpat_pandas_stringmethods_find_impl\n\n\n@sdc_overload_method(StringMethodsType, \'isupper\')\ndef hpat_pandas_stringmethods_isupper(self):\n    ty_checker = TypeChecker(\'Method isupper().\')\n    ty_checker.check(self, StringMethodsType)\n\n    def hpat_pandas_stringmethods_isupper_impl(self):\n        item_count = len(self._data)\n        result = numpy.empty(item_count, numba.types.boolean)\n        for idx, item in enumerate(self._data._data):\n            result[idx] = item.isupper()\n\n        return pandas.Series(result, self._data._index, name=self._data._name)\n\n    return hpat_pandas_stringmethods_isupper_impl\n\n\n@sdc_overload_method(StringMethodsType, \'len\')\ndef hpat_pandas_stringmethods_len(self):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n    Pandas API: pandas.Series.str.len\n\n    Limitations\n    -----------\n    Series elements are expected to be Unicode strings. Elements cannot be `NaNs`.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/str/series_str_len.py\n       :language: python\n       :lines: 27-\n       :caption: Compute the length of each element in the Series\n       :name: ex_series_str_len\n\n    .. command-output:: python ./series/str/series_str_len.py\n       :cwd: ../../../examples\n\n    .. seealso::\n        `str.len`\n            Python built-in function returning the length of an object.\n        :ref:`Series.size <pandas.Series.size>`\n            Returns the length of the Series.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n\n    Pandas Series method :meth:`pandas.core.strings.StringMethods.len()` implementation.\n\n    .. only:: developer\n\n    Test: python -m sdc.runtests sdc.tests.test_series.TestSeries.test_series_str_len1\n    """"""\n\n    ty_checker = TypeChecker(\'Method len().\')\n    ty_checker.check(self, StringMethodsType)\n\n    def hpat_pandas_stringmethods_len_impl(self):\n        item_count = len(self._data)\n        result = numpy.empty(item_count, numba.types.int64)\n        for idx, item in enumerate(self._data._data):\n            result[idx] = len(item)\n\n        return pandas.Series(result, self._data._index, name=self._data._name)\n\n    return hpat_pandas_stringmethods_len_impl\n\n\n@sdc_overload_method(StringMethodsType, \'ljust\')\ndef hpat_pandas_stringmethods_ljust(self, width, fillchar=\' \'):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n    Pandas API: pandas.Series.str.ljust\n\n    Limitations\n    -----------\n    - This function may reveal slower performance than Pandas* on user system. Users should exercise a tradeoff\n    between staying in JIT-region with that function or going back to interpreter mode.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/str/series_str_ljust.py\n       :language: python\n       :lines: 27-\n       :caption: Filling right side of strings in the Series with an additional character\n       :name: ex_series_str_ljust\n\n    .. command-output:: python ./series/str/series_str_ljust.py\n       :cwd: ../../../examples\n\n    .. seealso::\n        :ref:`Series.str.rjust <pandas.Series.str.rjust>`\n            Fills the left side of strings with an arbitrary character.\n        :ref:`Series.str.center <pandas.Series.str.center>`\n            Fills boths sides of strings with an arbitrary character.\n\n    .. todo:: Add support of 32-bit Unicode for `str.ljust()`\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n\n    Pandas Series method :meth:`pandas.core.strings.StringMethods.ljust()` implementation.\n\n    .. only:: developer\n\n    Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_ljust\n    """"""\n\n    ty_checker = TypeChecker(\'Method ljust().\')\n    ty_checker.check(self, StringMethodsType)\n\n    if not isinstance(width, Integer):\n        ty_checker.raise_exc(width, \'int\', \'width\')\n\n    accepted_types = (Omitted, StringLiteral, UnicodeType)\n    if not isinstance(fillchar, accepted_types) and fillchar != \' \':\n        ty_checker.raise_exc(fillchar, \'str\', \'fillchar\')\n\n    def hpat_pandas_stringmethods_ljust_impl(self, width, fillchar=\' \'):\n        mask = get_nan_mask(self._data._data)\n        item_count = len(self._data)\n        res_list = [\'\'] * item_count\n        for idx in numba.prange(item_count):\n            res_list[idx] = self._data._data[idx].ljust(width, fillchar)\n        str_arr = create_str_arr_from_list(res_list)\n        result = str_arr_set_na_by_mask(str_arr, mask)\n\n        return pandas.Series(result, self._data._index, name=self._data._name)\n\n    return hpat_pandas_stringmethods_ljust_impl\n\n\n@sdc_overload_method(StringMethodsType, \'rjust\')\ndef hpat_pandas_stringmethods_rjust(self, width, fillchar=\' \'):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n    Pandas API: pandas.Series.str.rjust\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/str/series_str_rjust.py\n       :language: python\n       :lines: 27-\n       :caption: Filling left side of strings in the Series with an additional character\n       :name: ex_series_str_rjust\n\n    .. command-output:: python ./series/str/series_str_rjust.py\n       :cwd: ../../../examples\n\n    .. seealso::\n        :ref:`Series.str.ljust <pandas.Series.str.ljust>`\n            Fills the right side of strings with an arbitrary character.\n        :ref:`Series.str.center <pandas.Series.str.center>`\n            Fills boths sides of strings with an arbitrary character.\n\n    .. todo:: Add support of 32-bit Unicode for `str.rjust()`\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n\n    Pandas Series method :meth:`pandas.core.strings.StringMethods.rjust()` implementation.\n\n    .. only:: developer\n\n    Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_rjust\n    """"""\n\n    ty_checker = TypeChecker(\'Method rjust().\')\n    ty_checker.check(self, StringMethodsType)\n\n    if not isinstance(width, Integer):\n        ty_checker.raise_exc(width, \'int\', \'width\')\n\n    accepted_types = (Omitted, StringLiteral, UnicodeType)\n    if not isinstance(fillchar, accepted_types) and fillchar != \' \':\n        ty_checker.raise_exc(fillchar, \'str\', \'fillchar\')\n\n    def hpat_pandas_stringmethods_rjust_impl(self, width, fillchar=\' \'):\n        mask = get_nan_mask(self._data._data)\n        item_count = len(self._data)\n        res_list = [\'\'] * item_count\n        for idx in numba.prange(item_count):\n            res_list[idx] = self._data._data[idx].rjust(width, fillchar)\n        str_arr = create_str_arr_from_list(res_list)\n        result = str_arr_set_na_by_mask(str_arr, mask)\n\n        return pandas.Series(result, self._data._index, name=self._data._name)\n\n    return hpat_pandas_stringmethods_rjust_impl\n\n\n@sdc_overload_method(StringMethodsType, \'startswith\')\ndef hpat_pandas_stringmethods_startswith(self, pat, na=None):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n    Pandas API: pandas.Series.str.startswith\n\n    Limitations\n    -----------\n    Series elements are expected to be Unicode strings. Elements cannot be `NaNs`.\n    Parameter ``na`` is supported only with default value ``None``.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/str/series_str_startswith.py\n       :language: python\n       :lines: 27-\n       :caption: Test if the start of each string element matches a string\n       :name: ex_series_str_startswith\n\n    .. command-output:: python ./series/str/series_str_startswith.py\n       :cwd: ../../../examples\n\n    .. seealso::\n        `str.startswith <https://docs.python.org/3/library/stdtypes.html#str.startswith>`_\n            Python standard library string method.\n        :ref:`Series.str.endswith <pandas.Series.str.endswith>`\n            Same as startswith, but tests the end of string.\n        :ref:`Series.str.contains <pandas.Series.str.contains>`\n            Tests if string element contains a pattern.\n\n    .. todo::\n        - Add support of matching the start of each string by a pattern\n        - Add support of parameter ``na``\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n\n    Pandas Series method :meth:`pandas.core.strings.StringMethods.startswith()` implementation.\n\n    .. only:: developer\n\n    Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_startswith\n    """"""\n\n    ty_checker = TypeChecker(\'Method startswith().\')\n    ty_checker.check(self, StringMethodsType)\n\n    if not isinstance(pat, (StringLiteral, UnicodeType)):\n        ty_checker.raise_exc(pat, \'str\', \'pat\')\n\n    if not isinstance(na, (Boolean, NoneType, Omitted)) and na is not None:\n        ty_checker.raise_exc(na, \'bool\', \'na\')\n\n    def hpat_pandas_stringmethods_startswith_impl(self, pat, na=None):\n        if na is not None:\n            msg = \'Method startswith(). The object na\\n expected: None\'\n            raise ValueError(msg)\n\n        item_startswith = len(self._data)\n        result = numpy.empty(item_startswith, numba.types.boolean)\n        for idx, item in enumerate(self._data._data):\n            result[idx] = item.startswith(pat)\n\n        return pandas.Series(result, self._data._index, name=self._data._name)\n\n    return hpat_pandas_stringmethods_startswith_impl\n\n\n@sdc_overload_method(StringMethodsType, \'zfill\')\ndef hpat_pandas_stringmethods_zfill(self, width):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n    Pandas API: pandas.Series.str.zfill\n\n    Limitations\n    -----------\n    A leading sign prefix (\'+\'/\'-\') is handled by inserting the padding after\n    the sign character rather than before.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/str/series_str_zfill.py\n       :language: python\n       :lines: 27-\n       :caption: Pad strings in the Series by prepending \'0\' characters\n       :name: ex_series_str_zfill\n\n    .. command-output:: python ./series/str/series_str_zfill.py\n       :cwd: ../../../examples\n\n    .. seealso::\n        :ref:`Series.str.rjust <pandas.Series.str.rjust>`\n            Fills the left side of strings with an arbitrary character.\n        :ref:`Series.str.ljust <pandas.Series.str.ljust>`\n            Fills the right side of strings with an arbitrary character.\n        :ref:`Series.str.pad <pandas.Series.str.pad>`\n            Fills the specified sides of strings with an arbitrary character.\n        :ref:`Series.str.center <pandas.Series.str.center>`\n            Fills boths sides of strings with an arbitrary character.\n\n    .. todo:: Add support of 32-bit Unicode for `str.zfill()`\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n\n    Pandas Series method :meth:`pandas.core.strings.StringMethods.zfill()` implementation.\n\n    .. only:: developer\n\n    Test: python -m sdc.runtests -k sdc.tests.test_series.TestSeries.test_series_zfill\n    """"""\n\n    ty_checker = TypeChecker(\'Method zfill().\')\n    ty_checker.check(self, StringMethodsType)\n\n    if not isinstance(width, Integer):\n        ty_checker.raise_exc(width, \'int\', \'width\')\n\n    def hpat_pandas_stringmethods_zfill_impl(self, width):\n        mask = get_nan_mask(self._data._data)\n        item_count = len(self._data)\n        res_list = [\'\'] * item_count\n        for idx in numba.prange(item_count):\n            res_list[idx] = self._data._data[idx].zfill(width)\n        str_arr = create_str_arr_from_list(res_list)\n        result = str_arr_set_na_by_mask(str_arr, mask)\n\n        return pandas.Series(result, self._data._index, name=self._data._name)\n\n    return hpat_pandas_stringmethods_zfill_impl\n\n\nsdc_pandas_series_str_docstring_template = """"""\n        Intel Scalable Dataframe Compiler User Guide\n        ********************************************\n        Pandas API: pandas.Series.str.{method_name}\n        {limitations}\n        Examples\n        --------\n        .. literalinclude:: ../../../examples/series/str/series_str_{method_name}.py\n           :language: python\n           :lines: 27-\n           :caption: {caption}\n           :name: ex_series_str_{method_name}\n\n        .. command-output:: python ./series/str/series_str_{method_name}.py\n           :cwd: ../../../examples\n        {seealso}\n        Intel Scalable Dataframe Compiler Developer Guide\n        *************************************************\n\n        Pandas Series method :meth:`pandas.core.strings.StringMethods.{method_name}()` implementation.\n\n        .. only:: developer\n\n        Test: python -m sdc.runtests sdc.tests.test_series.TestSeries.test_series_{method_name}_str\n""""""\n\n\n@sdc_overload_method(StringMethodsType, \'istitle\')\ndef hpat_pandas_stringmethods_istitle(self):\n\n    ty_checker = TypeChecker(\'Method istitle().\')\n    ty_checker.check(self, StringMethodsType)\n\n    def hpat_pandas_stringmethods_istitle_impl(self):\n        item_count = len(self._data)\n        result = numpy.empty(item_count, numba.types.boolean)\n        for idx, item in enumerate(self._data._data):\n            result[idx] = item.istitle()\n\n        return pandas.Series(result, self._data._index, name=self._data._name)\n\n    return hpat_pandas_stringmethods_istitle_impl\n\n\n@sdc_overload_method(StringMethodsType, \'isspace\')\ndef hpat_pandas_stringmethods_isspace(self):\n\n    ty_checker = TypeChecker(\'Method isspace().\')\n    ty_checker.check(self, StringMethodsType)\n\n    def hpat_pandas_stringmethods_isspace_impl(self):\n        item_count = len(self._data)\n        result = numpy.empty(item_count, numba.types.boolean)\n        for idx, item in enumerate(self._data._data):\n            result[idx] = item.isspace()\n\n        return pandas.Series(result, self._data._index, name=self._data._name)\n\n    return hpat_pandas_stringmethods_isspace_impl\n\n\n@sdc_overload_method(StringMethodsType, \'isalpha\')\ndef hpat_pandas_stringmethods_isalpha(self):\n\n    ty_checker = TypeChecker(\'Method isalpha().\')\n    ty_checker.check(self, StringMethodsType)\n\n    def hpat_pandas_stringmethods_isalpha_impl(self):\n        item_count = len(self._data)\n        result = numpy.empty(item_count, numba.types.boolean)\n        for idx, item in enumerate(self._data._data):\n            result[idx] = item.isalpha()\n\n        return pandas.Series(result, self._data._index, name=self._data._name)\n\n    return hpat_pandas_stringmethods_isalpha_impl\n\n\n@sdc_overload_method(StringMethodsType, \'islower\')\ndef hpat_pandas_stringmethods_islower(self):\n\n    ty_checker = TypeChecker(\'Method islower().\')\n    ty_checker.check(self, StringMethodsType)\n\n    def hpat_pandas_stringmethods_islower_impl(self):\n        item_count = len(self._data)\n        result = numpy.empty(item_count, numba.types.boolean)\n        for idx, item in enumerate(self._data._data):\n            result[idx] = item.islower()\n\n        return pandas.Series(result, self._data._index, name=self._data._name)\n\n    return hpat_pandas_stringmethods_islower_impl\n\n\n@sdc_overload_method(StringMethodsType, \'isalnum\')\ndef hpat_pandas_stringmethods_isalnum(self):\n\n    ty_checker = TypeChecker(\'Method isalnum().\')\n    ty_checker.check(self, StringMethodsType)\n\n    def hpat_pandas_stringmethods_isalnum_impl(self):\n        item_count = len(self._data)\n        result = numpy.empty(item_count, numba.types.boolean)\n        for idx, item in enumerate(self._data._data):\n            result[idx] = item.isalnum()\n\n        return pandas.Series(result, self._data._index, name=self._data._name)\n\n    return hpat_pandas_stringmethods_isalnum_impl\n\n\n@sdc_overload_method(StringMethodsType, \'isnumeric\')\ndef hpat_pandas_stringmethods_isnumeric(self):\n    ty_checker = TypeChecker(\'Method isnumeric().\')\n    ty_checker.check(self, StringMethodsType)\n\n    def hpat_pandas_stringmethods_isnumeric_impl(self):\n        item_count = len(self._data)\n        result = numpy.empty(item_count, numba.types.boolean)\n        for idx, item in enumerate(self._data._data):\n            result[idx] = item.isnumeric()\n\n        return pandas.Series(result, self._data._index, name=self._data._name)\n\n    return hpat_pandas_stringmethods_isnumeric_impl\n\n\n@sdc_overload_method(StringMethodsType, \'isdigit\')\ndef hpat_pandas_stringmethods_isdigit(self):\n    ty_checker = TypeChecker(\'Method isdigit().\')\n    ty_checker.check(self, StringMethodsType)\n\n    def hpat_pandas_stringmethods_isdigit_impl(self):\n        item_count = len(self._data)\n        result = numpy.empty(item_count, numba.types.boolean)\n        for idx, item in enumerate(self._data._data):\n            result[idx] = item.isdigit()\n\n        return pandas.Series(result, self._data._index, name=self._data._name)\n\n    return hpat_pandas_stringmethods_isdigit_impl\n\n\n@sdc_overload_method(StringMethodsType, \'isdecimal\')\ndef hpat_pandas_stringmethods_isdecimal(self):\n    ty_checker = TypeChecker(\'Method isdecimal().\')\n    ty_checker.check(self, StringMethodsType)\n\n    def hpat_pandas_stringmethods_isdecimal_impl(self):\n        item_count = len(self._data)\n        result = numpy.empty(item_count, numba.types.boolean)\n        for idx, item in enumerate(self._data._data):\n            result[idx] = item.isdecimal()\n\n        return pandas.Series(result, self._data._index, name=self._data._name)\n\n    return hpat_pandas_stringmethods_isdecimal_impl\n\n\n@sdc_overload_method(StringMethodsType, \'capitalize\')\ndef hpat_pandas_stringmethods_capitalize(self):\n    ty_checker = TypeChecker(\'Method capitalize().\')\n    ty_checker.check(self, StringMethodsType)\n\n    def hpat_pandas_stringmethods_capitalize_impl(self):\n        mask = get_nan_mask(self._data._data)\n        item_count = len(self._data)\n        res_list = [\'\'] * item_count\n        for idx in numba.prange(item_count):\n            res_list[idx] = self._data._data[idx].capitalize()\n        str_arr = create_str_arr_from_list(res_list)\n        result = str_arr_set_na_by_mask(str_arr, mask)\n\n        return pandas.Series(result, self._data._index, name=self._data._name)\n\n    return hpat_pandas_stringmethods_capitalize_impl\n\n\n@sdc_overload_method(StringMethodsType, \'title\')\ndef hpat_pandas_stringmethods_title(self):\n    ty_checker = TypeChecker(\'Method title().\')\n    ty_checker.check(self, StringMethodsType)\n\n    def hpat_pandas_stringmethods_title_impl(self):\n        mask = get_nan_mask(self._data._data)\n        item_count = len(self._data)\n        res_list = [\'\'] * item_count\n        for idx in numba.prange(item_count):\n            res_list[idx] = self._data._data[idx].title()\n        str_arr = create_str_arr_from_list(res_list)\n        result = str_arr_set_na_by_mask(str_arr, mask)\n\n        return pandas.Series(result, self._data._index, name=self._data._name)\n\n    return hpat_pandas_stringmethods_title_impl\n\n\n@sdc_overload_method(StringMethodsType, \'swapcase\')\ndef hpat_pandas_stringmethods_swapcase(self):\n    ty_checker = TypeChecker(\'Method swapcase().\')\n    ty_checker.check(self, StringMethodsType)\n\n    def hpat_pandas_stringmethods_swapcase_impl(self):\n        mask = get_nan_mask(self._data._data)\n        item_count = len(self._data)\n        res_list = [\'\'] * item_count\n        for idx in numba.prange(item_count):\n            res_list[idx] = self._data._data[idx].swapcase()\n        str_arr = create_str_arr_from_list(res_list)\n        result = str_arr_set_na_by_mask(str_arr, mask)\n\n        return pandas.Series(result, self._data._index, name=self._data._name)\n\n    return hpat_pandas_stringmethods_swapcase_impl\n\n\n@sdc_overload_method(StringMethodsType, \'casefold\')\ndef hpat_pandas_stringmethods_casefold(self):\n    ty_checker = TypeChecker(\'Method casefold().\')\n    ty_checker.check(self, StringMethodsType)\n\n    def hpat_pandas_stringmethods_casefold_impl(self):\n        mask = get_nan_mask(self._data._data)\n        item_count = len(self._data)\n        res_list = [\'\'] * item_count\n        for idx in numba.prange(item_count):\n            res_list[idx] = self._data._data[idx].casefold()\n        str_arr = create_str_arr_from_list(res_list)\n        result = str_arr_set_na_by_mask(str_arr, mask)\n\n        return pandas.Series(result, self._data._index, name=self._data._name)\n\n    return hpat_pandas_stringmethods_casefold_impl\n\n\n@sdc_overload_method(StringMethodsType, \'lower\')\ndef hpat_pandas_stringmethods_lower(self):\n    ty_checker = TypeChecker(\'Method lower().\')\n    ty_checker.check(self, StringMethodsType)\n\n    def hpat_pandas_stringmethods_lower_impl(self):\n        mask = get_nan_mask(self._data._data)\n        item_count = len(self._data)\n        res_list = [\'\'] * item_count\n\n        for it in range(item_count):\n            item = self._data._data[it]\n            if len(item) > 0:\n                res_list[it] = item.lower()\n            else:\n                res_list[it] = item\n\n        str_arr = create_str_arr_from_list(res_list)\n        result = str_arr_set_na_by_mask(str_arr, mask)\n\n        return pandas.Series(result, self._data._index, name=self._data._name)\n\n    return hpat_pandas_stringmethods_lower_impl\n\n\n@sdc_overload_method(StringMethodsType, \'upper\')\ndef hpat_pandas_stringmethods_upper(self):\n    ty_checker = TypeChecker(\'Method upper().\')\n    ty_checker.check(self, StringMethodsType)\n\n    def hpat_pandas_stringmethods_upper_impl(self):\n        mask = get_nan_mask(self._data._data)\n        item_count = len(self._data)\n        result = [\'\'] * item_count\n\n        for it in numba.prange(item_count):\n            item = self._data._data[it]\n            if len(item) > 0:\n                result[it] = item.upper()\n            else:\n                result[it] = item\n\n        str_arr = create_str_arr_from_list(result)\n        result = str_arr_set_na_by_mask(str_arr, mask)\n\n        return pandas.Series(result, self._data._index, name=self._data._name)\n\n    return hpat_pandas_stringmethods_upper_impl\n\n\n@sdc_register_jitable\ndef lstrip_usecase(s, to_strip):\n    return s.lstrip(to_strip)\n\n\n@sdc_register_jitable\ndef rstrip_usecase(s, to_strip):\n    return s.rstrip(to_strip)\n\n\n@sdc_register_jitable\ndef strip_usecase(s, to_strip):\n    return s.strip(to_strip)\n\n\ndef gen_sdc_pandas_series_str_strip_impl(usecase):\n    """"""Generate series.str.lstrip/rstrip/strip implementations based on usecase func""""""\n    def impl(self, to_strip=None):\n        mask = get_nan_mask(self._data._data)\n        item_count = len(self._data)\n        res_list = [\'\'] * item_count\n\n        for it in range(item_count):\n            item = self._data._data[it]\n            if len(item) > 0:\n                res_list[it] = usecase(item, to_strip)\n            else:\n                res_list[it] = item\n\n        str_arr = create_str_arr_from_list(res_list)\n        result = str_arr_set_na_by_mask(str_arr, mask)\n\n        return pandas.Series(result, self._data._index, name=self._data._name)\n\n    return impl\n\n\nsdc_pandas_series_str_lstrip_impl = gen_sdc_pandas_series_str_strip_impl(lstrip_usecase)\nsdc_pandas_series_str_rstrip_impl = gen_sdc_pandas_series_str_strip_impl(rstrip_usecase)\nsdc_pandas_series_str_strip_impl = gen_sdc_pandas_series_str_strip_impl(strip_usecase)\n\n\n@sdc_overload_method(StringMethodsType, \'lstrip\')\ndef hpat_pandas_stringmethods_lstrip(self, to_strip=None):\n    ty_checker = TypeChecker(\'Method strip().\')\n    ty_checker.check(self, StringMethodsType)\n\n    if not isinstance(to_strip, (NoneType, StringLiteral, UnicodeType, Omitted)) and to_strip is not None:\n        ty_checker.raise_exc(to_strip, \'str\', \'to_strip\')\n\n    return sdc_pandas_series_str_lstrip_impl\n\n\n@sdc_overload_method(StringMethodsType, \'rstrip\')\ndef hpat_pandas_stringmethods_rstrip(self, to_strip=None):\n    ty_checker = TypeChecker(\'Method rstrip().\')\n    ty_checker.check(self, StringMethodsType)\n\n    if not isinstance(to_strip, (NoneType, StringLiteral, UnicodeType, Omitted)) and to_strip is not None:\n        ty_checker.raise_exc(to_strip, \'str\', \'to_strip\')\n\n    return sdc_pandas_series_str_rstrip_impl\n\n\n@sdc_overload_method(StringMethodsType, \'strip\')\ndef hpat_pandas_stringmethods_strip(self, to_strip=None):\n    ty_checker = TypeChecker(\'Method strip().\')\n    ty_checker.check(self, StringMethodsType)\n\n    if not isinstance(to_strip, (NoneType, StringLiteral, UnicodeType, Omitted)) and to_strip is not None:\n        ty_checker.raise_exc(to_strip, \'str\', \'to_strip\')\n\n    return sdc_pandas_series_str_strip_impl\n\n\nseealso_check_methods = """"""\n        .. seealso::\n            :ref:`Series.str.isalpha <pandas.Series.str.isalpha>`\n                Checks whether all characters are alphabetic.\n            :ref:`Series.str.isnumeric <pandas.Series.str.isnumeric>`\n                Checks whether all characters are numeric.\n            :ref:`Series.str.isalnum <pandas.Series.str.isalnum>`\n                Checks whether all characters are alphanumeric.\n            :ref:`Series.str.isdigit <pandas.Series.str.isdigit>`\n                Checks whether all characters are digits.\n            :ref:`Series.str.isdecimal <pandas.Series.str.isdecimal>`\n                Checks whether all characters are decimal.\n            :ref:`Series.str.isspace <pandas.Series.str.isspace>`\n                Checks whether all characters are whitespace.\n            :ref:`Series.str.islower <pandas.Series.str.islower>`\n                Checks whether all characters are lowercase.\n            :ref:`Series.str.isupper <pandas.Series.str.isupper>`\n                Checks whether all characters are uppercase.\n            :ref:`Series.str.istitle <pandas.Series.str.istitle>`\n                Checks whether all characters are titlecase.\n""""""\n\nseealso_transform_methods = """"""\n        .. seealso::\n            :ref:`Series.str.lower <pandas.Series.str.lower>`\n                Converts all characters to lowercase.\n            :ref:`Series.str.upper <pandas.Series.str.upper>`\n                Converts all characters to uppercase.\n            :ref:`Series.str.title <pandas.Series.str.title>`\n                Converts first character of each word to uppercase and remaining to lowercase.\n            :ref:`Series.str.capitalize <pandas.Series.str.capitalize>`\n                Converts first character to uppercase and remaining to lowercase.\n            :ref:`Series.str.swapcase <pandas.Series.str.swapcase>`\n                Converts uppercase to lowercase and lowercase to uppercase.\n            :ref:`Series.str.casefold <pandas.Series.str.casefold>`\n                Removes all case distinctions in the string.\n""""""\n\nlimitation_nans_unsupported = """"""\n        Limitations\n        -----------\n        Series elements are expected to be Unicode strings. Elements cannot be `NaNs`.\n""""""\n\nlimitation_nans_supported = """"""\n        Limitations\n        -----------\n        All values in Series equal to `None` are converted to `NaNs`.\n""""""\n\nseealso_strip_methods = """"""\n        .. seealso::\n            :ref:`Series.str.strip <pandas.Series.str.strip>`\n                Remove leading and trailing characters in Series.\n            :ref:`Series.str.lstrip <pandas.Series.str.lstrip>`\n                Remove leading characters in Series.\n            :ref:`Series.str.strip <pandas.Series.str.strip>`\n                Remove trailing characters in Series.\n""""""\n\n\nstringmethods_funcs = {\n    \'istitle\': {\n        \'method\': hpat_pandas_stringmethods_istitle,\n        \'caption\': \'Check if each word start with an upper case letter\',\n        \'seealso\': seealso_check_methods,\n        \'limitations\':\n        """"""\n        Limitations\n        -----------\n        This function may reveal slower performance than Pandas* on user system. Users should exercise a tradeoff\n        between staying in JIT-region with that function or going back to interpreter mode.\n        """"""\n    },\n    \'isspace\': {\n        \'method\': hpat_pandas_stringmethods_isspace,\n        \'caption\': \'Check if all the characters in the text are whitespaces\',\n        \'seealso\': seealso_check_methods,\n        \'limitations\':\n        """"""\n        Limitations\n        -----------\n        This function may reveal slower performance than Pandas* on user system. Users should exercise a tradeoff\n        between staying in JIT-region with that function or going back to interpreter mode.\n        """"""\n    },\n    \'isalpha\': {\n        \'method\': hpat_pandas_stringmethods_isalpha,\n        \'caption\': \'Check whether all characters in each string are alphabetic\',\n        \'seealso\': seealso_check_methods,\n        \'limitations\':\n        """"""\n        Limitations\n        -----------\n        This function may reveal slower performance than Pandas* on user system. Users should exercise a tradeoff\n        between staying in JIT-region with that function or going back to interpreter mode.\n        """"""\n    },\n    \'islower\': {\n        \'method\': hpat_pandas_stringmethods_islower,\n        \'caption\': \'Check if all the characters in the text are alphanumeric\',\n        \'seealso\': seealso_check_methods,\n        \'limitations\':\n        """"""\n        Limitations\n        -----------\n        This function may reveal slower performance than Pandas* on user system. Users should exercise a tradeoff\n        between staying in JIT-region with that function or going back to interpreter mode.\n        """"""\n    },\n    \'isalnum\': {\n        \'method\': hpat_pandas_stringmethods_isalnum,\n        \'caption\': \'Check if all the characters in the text are alphanumeric\',\n        \'seealso\': seealso_check_methods,\n        \'limitations\':\n        """"""\n        Limitations\n        -----------\n        This function may reveal slower performance than Pandas* on user system. Users should exercise a tradeoff\n        between staying in JIT-region with that function or going back to interpreter mode.\n        """"""\n    },\n    \'isnumeric\': {\n        \'method\': hpat_pandas_stringmethods_isnumeric,\n        \'caption\': \'Check whether all characters in each string are numeric.\',\n        \'seealso\': seealso_check_methods,\n        \'limitations\': limitation_nans_unsupported\n    },\n    \'isdigit\': {\n        \'method\': hpat_pandas_stringmethods_isdigit,\n        \'caption\': \'Check whether all characters in each string in the Series are digits.\',\n        \'seealso\': seealso_check_methods,\n        \'limitations\': limitation_nans_unsupported\n    },\n    \'isdecimal\': {\n        \'method\': hpat_pandas_stringmethods_isdecimal,\n        \'caption\': \'Check whether all characters in each string are decimal.\',\n        \'seealso\': seealso_check_methods,\n        \'limitations\': limitation_nans_unsupported\n    },\n    \'isupper\': {\n        \'method\': hpat_pandas_stringmethods_isupper,\n        \'caption\': \'Check whether all characters in each string are uppercase.\',\n        \'seealso\': seealso_check_methods,\n        \'limitations\':\n        """"""\n        Limitations\n        -----------\n        This function may reveal slower performance than Pandas* on user system. Users should exercise a tradeoff\n        between staying in JIT-region with that function or going back to interpreter mode.\n        """"""\n    },\n    \'capitalize\': {\n        \'method\': hpat_pandas_stringmethods_capitalize,\n        \'caption\': \'Convert strings in the Series to be capitalized.\',\n        \'seealso\': seealso_transform_methods,\n        \'limitations\':\n        """"""\n        Limitations\n        -----------\n        - All values in Series equal to `None` are converted to `NaNs`.\n        - This function may reveal slower performance than Pandas* on user system. Users should exercise a tradeoff\n        between staying in JIT-region with that function or going back to interpreter mode.\n        """"""\n    },\n    \'title\': {\n        \'method\': hpat_pandas_stringmethods_title,\n        \'caption\': \'Convert strings in the Series to titlecase.\',\n        \'seealso\': seealso_transform_methods,\n        \'limitations\':\n        """"""\n        Limitations\n        -----------\n        - All values in Series equal to `None` are converted to `NaNs`.\n        - This function may reveal slower performance than Pandas* on user system. Users should exercise a tradeoff\n        between staying in JIT-region with that function or going back to interpreter mode.\n        """"""\n    },\n    \'swapcase\': {\n        \'method\': hpat_pandas_stringmethods_swapcase,\n        \'caption\': \'Convert strings in the Series to be swapcased.\',\n        \'seealso\': seealso_transform_methods,\n        \'limitations\':\n        """"""\n        Limitations\n        -----------\n        - All values in Series equal to `None` are converted to `NaNs`.\n        - This function may reveal slower performance than Pandas* on user system. Users should exercise a tradeoff\n        between staying in JIT-region with that function or going back to interpreter mode.\n        """"""\n    },\n    \'casefold\': {\n        \'method\': hpat_pandas_stringmethods_casefold,\n        \'caption\': \'Convert strings in the Series to be casefolded.\',\n        \'seealso\': seealso_transform_methods,\n        \'limitations\':\n        """"""\n        Limitations\n        -----------\n        - All values in Series equal to `None` are converted to `NaNs`.\n        - This function may reveal slower performance than Pandas* on user system. Users should exercise a tradeoff\n        between staying in JIT-region with that function or going back to interpreter mode.\n        """"""\n    },\n    \'strip\': {\n        \'method\': hpat_pandas_stringmethods_strip,\n        \'caption\': \'Remove leading and trailing characters.\',\n        \'seealso\': seealso_strip_methods,\n        \'limitations\':\n        """"""\n        Limitations\n        -----------\n        - All values in Series equal to `None` are converted to `NaNs`.\n        - This function may reveal slower performance than Pandas* on user system. Users should exercise a tradeoff\n        between staying in JIT-region with that function or going back to interpreter mode.\n        """"""\n    },\n    \'lstrip\': {\n        \'method\': hpat_pandas_stringmethods_lstrip,\n        \'caption\': \'Remove leading and trailing characters.\',\n        \'seealso\': seealso_strip_methods,\n        \'limitations\':\n        """"""\n        Limitations\n        -----------\n        - All values in Series equal to `None` are converted to `NaNs`.\n        - This function may reveal slower performance than Pandas* on user system. Users should exercise a tradeoff\n        between staying in JIT-region with that function or going back to interpreter mode.\n        """"""\n    },\n    \'rstrip\': {\n        \'method\': hpat_pandas_stringmethods_rstrip,\n        \'caption\': \'Remove leading and trailing characters.\',\n        \'seealso\': seealso_strip_methods,\n        \'limitations\':\n        """"""\n        Limitations\n        -----------\n        - All values in Series equal to `None` are converted to `NaNs`.\n        - This function may reveal slower performance than Pandas* on user system. Users should exercise a tradeoff\n        between staying in JIT-region with that function or going back to interpreter mode.\n        """"""\n    },\n    \'lower\': {\n        \'method\': hpat_pandas_stringmethods_lower,\n        \'caption\': \'Convert strings in the Series to lowercase.\',\n        \'seealso\': seealso_transform_methods,\n        \'limitations\':\n        """"""\n        Limitations\n        -----------\n        - All values in Series equal to `None` are converted to `NaNs`.\n        - This function may reveal slower performance than Pandas* on user system. Users should exercise a tradeoff\n        between staying in JIT-region with that function or going back to interpreter mode.\n        """"""\n    },\n    \'upper\': {\n        \'method\': hpat_pandas_stringmethods_upper,\n        \'caption\': \'Convert strings in the Series to upper case.\',\n        \'seealso\': seealso_transform_methods,\n        \'limitations\':\n        """"""\n        Limitations\n        -----------\n        - All values in Series equal to `None` are converted to `NaNs`.\n        - This function may reveal slower performance than Pandas* on user system. Users should exercise a tradeoff\n        between staying in JIT-region with that function or going back to interpreter mode.\n        """"""\n    },\n}\n\nfor name, data in stringmethods_funcs.items():\n    data[\'method\'].__doc__ = sdc_pandas_series_str_docstring_template.format(\n        **{\'method_name\': name,\n           \'caption\': data[\'caption\'],\n           \'seealso\': data[\'seealso\'],\n           \'limitations\': data[\'limitations\']\n           }\n    )\n'"
sdc/datatypes/hpat_pandas_stringmethods_types.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\n| :class:`pandas.core.strings.StringMethods` type implementation in HPAT\n| Also, it contains related types and iterators for StringMethods type handling\n""""""\n\n\nimport pandas\n\nfrom numba import types\nfrom numba.core import cgutils\nfrom numba.extending import (models, overload, register_model, make_attribute_wrapper, intrinsic)\nfrom numba.core.datamodel import (register_default, StructModel)\nfrom numba.core.typing.templates import signature\nfrom sdc.utilities.utils import sdc_overload\n\n\nclass StringMethodsType(types.IterableType):\n    """"""\n    Type definition for pandas.core.strings.StringMethods functions handling.\n\n    Members\n    ----------\n    _data: :class:`SeriesType`\n        input arg\n    """"""\n\n    def __init__(self, data):\n        self.data = data\n        name = \'StringMethodsType({})\'.format(self.data)\n        super(StringMethodsType, self).__init__(name)\n\n    @property\n    def iterator_type(self):\n        return None\n\n\n@register_model(StringMethodsType)\nclass StringMethodsTypeModel(StructModel):\n    """"""\n    Model for StringMethodsType type\n    All members must be the same as main type for this model\n    """"""\n\n    def __init__(self, dmm, fe_type):\n        members = [\n            (\'data\', fe_type.data)\n        ]\n        models.StructModel.__init__(self, dmm, fe_type, members)\n\n\nmake_attribute_wrapper(StringMethodsType, \'data\', \'_data\')\n\n\ndef _gen_hpat_pandas_stringmethods_init(string_methods_type=None):\n    string_methods_type = string_methods_type or StringMethodsType\n\n    def _hpat_pandas_stringmethods_init(typingctx, data):\n        """"""\n        Internal Numba required function to register StringMethodsType and\n        connect it with corresponding Python type mentioned in @overload(pandas.core.strings.StringMethods)\n        """"""\n\n        def _hpat_pandas_stringmethods_init_codegen(context, builder, signature, args):\n            """"""\n            It is looks like it creates StringMethodsModel structure\n\n            - Fixed number of parameters. Must be 4\n            - increase reference count for the data\n            """"""\n\n            [data_val] = args\n            stringmethod = cgutils.create_struct_proxy(signature.return_type)(context, builder)\n            stringmethod.data = data_val\n\n            if context.enable_nrt:\n                context.nrt.incref(builder, data, stringmethod.data)\n\n            return stringmethod._getvalue()\n\n        ret_typ = string_methods_type(data)\n        sig = signature(ret_typ, data)\n        """"""\n        Construct signature of the Numba SeriesGroupByType::ctor()\n        """"""\n\n        return sig, _hpat_pandas_stringmethods_init_codegen\n\n    return _hpat_pandas_stringmethods_init\n\n\n_hpat_pandas_stringmethods_init = intrinsic(\n    _gen_hpat_pandas_stringmethods_init(string_methods_type=StringMethodsType))\n\n\n@sdc_overload(pandas.core.strings.StringMethods)\ndef hpat_pandas_stringmethods(obj):\n    """"""\n    Special Numba procedure to overload Python type pandas.core.strings.StringMethods::ctor()\n    with Numba registered model\n    """"""\n\n    def hpat_pandas_stringmethods_impl(obj):\n        return _hpat_pandas_stringmethods_init(obj)\n\n    return hpat_pandas_stringmethods_impl\n'"
sdc/datatypes/range_index_type.py,0,"b'# -*- coding: utf-8 -*-\n# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nfrom numba import types\nfrom numba.extending import (\n    models,\n    register_model,\n    make_attribute_wrapper\n)\n\n\nRangeIndexDataType = types.range_state64_type\n\n\nclass RangeIndexType(types.IterableType):\n\n    dtype = types.int64\n\n    def __init__(self, is_named=False):\n        self.is_named = is_named\n        super(RangeIndexType, self).__init__(\n            name=\'RangeIndexType({})\'.format(is_named))\n\n    # TODO: provide iteration support by adding getiter and iternext\n    @property\n    def iterator_type(self):\n        return RangeIndexDataType.iterator_type()\n\n\n@register_model(RangeIndexType)\nclass RangeIndexModel(models.StructModel):\n    def __init__(self, dmm, fe_type):\n\n        name_type = types.unicode_type if fe_type.is_named else types.none\n        members = [\n            (\'data\', RangeIndexDataType),\n            (\'name\', name_type)\n        ]\n        models.StructModel.__init__(self, dmm, fe_type, members)\n\n\nmake_attribute_wrapper(RangeIndexType, \'data\', \'_data\')\nmake_attribute_wrapper(RangeIndexType, \'name\', \'_name\')\n'"
sdc/extensions/__init__.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n'"
sdc/functions/__init__.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n'"
sdc/functions/numpy_like.py,7,"b'# -*- coding: utf-8 -*-\n# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\nfrom sdc.hiframes.api import isna\n\n""""""\n\n| This file contains SDC numpy modified functions, rewritten according to concurrency\n\n""""""\n\nimport numba\nimport numpy\nimport sys\nimport pandas\nimport numpy as np\n\nfrom numba import types, prange, literally\nfrom numba.np import numpy_support\nfrom numba.np.arraymath import get_isnan\nfrom numba.typed import List\n\nimport sdc\nfrom sdc.functions.statistics import skew_formula\nfrom sdc.utilities.sdc_typing_utils import TypeChecker\nfrom sdc.utilities.utils import (sdc_overload, sdc_register_jitable,\n                                 min_dtype_int_val, max_dtype_int_val, min_dtype_float_val,\n                                 max_dtype_float_val)\nfrom sdc.str_arr_ext import (StringArrayType, pre_alloc_string_array, get_utf8_size,\n                             string_array_type, create_str_arr_from_list, str_arr_set_na_by_mask)\nfrom sdc.utilities.utils import sdc_overload, sdc_register_jitable\nfrom sdc.utilities.prange_utils import parallel_chunks\n\n\ndef astype(self, dtype):\n    pass\n\n\ndef astype_no_inline(self, dtype):\n    pass\n\n\ndef argmin(self):\n    pass\n\n\ndef argmax(self):\n    pass\n\n\ndef nanargmin(self):\n    pass\n\n\ndef nanargmax(self):\n    pass\n\n\ndef fillna(self, inplace=False, value=None):\n    pass\n\n\ndef copy(self):\n    pass\n\n\ndef isnan(self):\n    pass\n\n\ndef notnan(self):\n    pass\n\n\ndef sum(self):\n    pass\n\n\ndef nansum(self):\n    pass\n\n\n@sdc_overload(astype, inline=\'always\')\n@sdc_overload(astype_no_inline)\ndef sdc_astype_overload(self, dtype):\n    """"""\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Parallel replacement of numpy.astype.\n\n    .. only:: developer\n       Test: python -m sdc.runtests sdc.tests.test_sdc_numpy -k astype\n\n    """"""\n\n    ty_checker = TypeChecker(""numpy-like \'astype\'"")\n    if not isinstance(self, (types.Array, StringArrayType)):\n        return None\n\n    if not isinstance(dtype, (types.functions.NumberClass, types.Function, types.Literal)):\n        def impl(self, dtype):\n            return astype(self, literally(dtype))\n\n        return impl\n\n    if not isinstance(dtype, (types.StringLiteral, types.UnicodeType, types.Function, types.functions.NumberClass)):\n        ty_checker.raise_exc(dtype, \'string or type\', \'dtype\')\n\n    if (\n        (isinstance(dtype, types.Function) and dtype.typing_key == str) or\n        (isinstance(dtype, types.StringLiteral) and dtype.literal_value == \'str\')\n    ):\n        def sdc_astype_number_to_string_impl(self, dtype):\n            num_bytes = 0\n            arr_len = len(self)\n\n            # Get total bytes for new array\n            for i in prange(arr_len):\n                item = self[i]\n                num_bytes += get_utf8_size(str(item))\n\n            data = pre_alloc_string_array(arr_len, num_bytes)\n\n            for i in range(arr_len):\n                item = self[i]\n                data[i] = str(item)  # TODO: check NA\n\n            return data\n\n        return sdc_astype_number_to_string_impl\n\n    if (isinstance(self, types.Array) and isinstance(dtype, (types.StringLiteral, types.functions.NumberClass))):\n        def sdc_astype_number_impl(self, dtype):\n            arr = numpy.empty(len(self), dtype=numpy.dtype(dtype))\n            for i in numba.prange(len(self)):\n                arr[i] = self[i]\n\n            return arr\n\n        return sdc_astype_number_impl\n\n\ndef sdc_nanarg_overload(reduce_op):\n    def nanarg_impl(self):\n        """"""\n        Intel Scalable Dataframe Compiler Developer Guide\n        *************************************************\n        Parallel replacement of numpy.nanargmin/numpy.nanargmax.\n\n        .. only:: developer\n        Test: python -m sdc.runtests sdc.tests.test_sdc_numpy -k nanargmin\n        Test: python -m sdc.runtests sdc.tests.test_sdc_numpy -k nanargmax\n\n        """"""\n\n        ty_checker = TypeChecker(""numpy-like \'nanargmin\'/\'nanargmax\'"")\n        dtype = self.dtype\n        isnan = get_isnan(dtype)\n        max_int64 = max_dtype_int_val(numpy_support.from_dtype(numpy.int64))\n        if isinstance(dtype, types.Integer):\n            initial_result = {\n                min: max_dtype_int_val(dtype),\n                max: min_dtype_int_val(dtype),\n            }[reduce_op]\n\n        if isinstance(dtype, types.Float):\n            initial_result = {\n                min: max_dtype_float_val(dtype),\n                max: min_dtype_float_val(dtype),\n            }[reduce_op]\n\n        if not isinstance(self, types.Array):\n            return None\n\n        if isinstance(dtype, types.Number):\n            def sdc_nanargmin_impl(self):\n                chunks = parallel_chunks(len(self))\n                arr_res = numpy.empty(shape=len(chunks), dtype=dtype)\n                arr_pos = numpy.empty(shape=len(chunks), dtype=numpy.int64)\n                for i in prange(len(chunks)):\n                    chunk = chunks[i]\n                    res = initial_result\n                    pos = max_int64\n                    for j in range(chunk.start, chunk.stop):\n                        if reduce_op(res, self[j]) != self[j]:\n                            continue\n                        if isnan(self[j]):\n                            continue\n                        if res == self[j]:\n                            pos = min(pos, j)\n                        else:\n                            pos = j\n                            res = self[j]\n                    arr_res[i] = res\n                    arr_pos[i] = pos\n\n                general_res = initial_result\n                general_pos = max_int64\n                for i in range(len(chunks)):\n                    if reduce_op(general_res, arr_res[i]) != arr_res[i]:\n                        continue\n                    if general_res == arr_res[i]:\n                        general_pos = min(general_pos, arr_pos[i])\n                    else:\n                        general_pos = arr_pos[i]\n                        general_res = arr_res[i]\n\n                return general_pos\n\n            return sdc_nanargmin_impl\n\n        ty_checker.raise_exc(dtype, \'number\', \'self.dtype\')\n    return nanarg_impl\n\n\nsdc_overload(nanargmin)(sdc_nanarg_overload(min))\nsdc_overload(nanargmax)(sdc_nanarg_overload(max))\n\n\ndef sdc_arg_overload(reduce_op):\n    def arg_impl(self):\n        """"""\n        Intel Scalable Dataframe Compiler Developer Guide\n        *************************************************\n        Parallel replacement of numpy.argmin/numpy.argmax.\n\n        .. only:: developer\n        Test: python -m sdc.runtests sdc.tests.test_sdc_numpy -k argmin\n        Test: python -m sdc.runtests sdc.tests.test_sdc_numpy -k argmax\n\n        """"""\n\n        ty_checker = TypeChecker(""numpy-like \'argmin\'/\'argmax\'"")\n        dtype = self.dtype\n        isnan = get_isnan(dtype)\n        max_int64 = max_dtype_int_val(numpy_support.from_dtype(numpy.int64))\n        if isinstance(dtype, types.Integer):\n            initial_result = {\n                min: max_dtype_int_val(dtype),\n                max: min_dtype_int_val(dtype),\n            }[reduce_op]\n\n        if isinstance(dtype, types.Float):\n            initial_result = {\n                min: max_dtype_float_val(dtype),\n                max: min_dtype_float_val(dtype),\n            }[reduce_op]\n\n        if not isinstance(self, types.Array):\n            return None\n\n        if isinstance(dtype, types.Number):\n            def sdc_argmin_impl(self):\n                chunks = parallel_chunks(len(self))\n                arr_res = numpy.empty(shape=len(chunks), dtype=dtype)\n                arr_pos = numpy.empty(shape=len(chunks), dtype=numpy.int64)\n                for i in prange(len(chunks)):\n                    chunk = chunks[i]\n                    res = initial_result\n                    pos = max_int64\n                    for j in range(chunk.start, chunk.stop):\n                        if not isnan(self[j]):\n                            if reduce_op(res, self[j]) != self[j]:\n                                continue\n                            if res == self[j]:\n                                pos = min(pos, j)\n                            else:\n                                pos = j\n                                res = self[j]\n                        else:\n                            if numpy.isnan(res):\n                                pos = min(pos, j)\n                            else:\n                                pos = j\n                            res = self[j]\n\n                    arr_res[i] = res\n                    arr_pos[i] = pos\n                general_res = initial_result\n                general_pos = max_int64\n                for i in range(len(chunks)):\n                    if not isnan(arr_res[i]):\n                        if reduce_op(general_res, arr_res[i]) != arr_res[i]:\n                            continue\n                        if general_res == arr_res[i]:\n                            general_pos = min(general_pos, arr_pos[i])\n                        else:\n                            general_pos = arr_pos[i]\n                            general_res = arr_res[i]\n                    else:\n                        if numpy.isnan(general_res):\n                            general_pos = min(general_pos, arr_pos[i])\n                        else:\n                            general_pos = arr_pos[i]\n                        general_res = arr_res[i]\n                return general_pos\n\n            return sdc_argmin_impl\n\n        ty_checker.raise_exc(dtype, \'number\', \'self.dtype\')\n    return arg_impl\n\n\nsdc_overload(argmin)(sdc_arg_overload(min))\nsdc_overload(argmax)(sdc_arg_overload(max))\n\n\n@sdc_overload(copy)\ndef sdc_copy_overload(self):\n    """"""\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Parallel replacement of numpy.copy.\n\n    .. only:: developer\n       Test: python -m sdc.runtests sdc.tests.test_sdc_numpy -k copy\n    """"""\n\n    if not isinstance(self, (types.Array, StringArrayType)):\n        return None\n\n    dtype = self.dtype\n    if isinstance(dtype, (types.Number, types.Boolean, bool)):\n        def sdc_copy_number_impl(self):\n            length = len(self)\n            res = numpy.empty(length, dtype=dtype)\n            for i in prange(length):\n                res[i] = self[i]\n\n            return res\n\n        return sdc_copy_number_impl\n\n    if isinstance(dtype, (types.npytypes.UnicodeCharSeq, types.UnicodeType, types.StringLiteral)):\n        def sdc_copy_string_impl(self):\n            return self.copy()\n\n        return sdc_copy_string_impl\n\n\n@sdc_overload(notnan)\ndef sdc_notnan_overload(self):\n    """"""\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Parallel replacement of numpy.notnan.\n    .. only:: developer\n       Test: python -m sdc.runtests sdc.tests.test_sdc_numpy -k notnan\n    """"""\n\n    if not isinstance(self, types.Array):\n        return None\n\n    dtype = self.dtype\n    isnan = get_isnan(dtype)\n    if isinstance(dtype, (types.Integer, types.Boolean, bool)):\n        def sdc_notnan_int_impl(self):\n            length = len(self)\n            res = numpy.ones(shape=length, dtype=numpy.bool_)\n\n            return res\n\n        return sdc_notnan_int_impl\n\n    if isinstance(dtype, types.Float):\n        def sdc_notnan_float_impl(self):\n            length = len(self)\n            res = numpy.empty(shape=length, dtype=numpy.bool_)\n            for i in prange(length):\n                res[i] = not isnan(self[i])\n\n            return res\n\n        return sdc_notnan_float_impl\n\n    ty_checker.raise_exc(dtype, \'int or float\', \'self.dtype\')\n\n\n@sdc_overload(isnan)\ndef sdc_isnan_overload(self):\n    """"""\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Parallel replacement of numpy.isnan.\n    .. only:: developer\n       Test: python -m sdc.runtests sdc.tests.test_sdc_numpy -k isnan\n    """"""\n\n    if not isinstance(self, types.Array):\n        return None\n\n    dtype = self.dtype\n    isnan = get_isnan(dtype)\n    if isinstance(dtype, (types.Integer, types.Boolean, bool)):\n        def sdc_isnan_int_impl(self):\n            length = len(self)\n            res = numpy.zeros(shape=length, dtype=numpy.bool_)\n\n            return res\n\n        return sdc_isnan_int_impl\n\n    if isinstance(dtype, types.Float):\n        def sdc_isnan_float_impl(self):\n            length = len(self)\n            res = numpy.empty(shape=length, dtype=numpy.bool_)\n            for i in prange(length):\n                res[i] = isnan(self[i])\n\n            return res\n\n        return sdc_isnan_float_impl\n\n    ty_checker.raise_exc(dtype, \'int or float\', \'self.dtype\')\n\n\ndef gen_sum_bool_impl():\n    """"""Generate sum bool implementation.""""""\n    def _sum_bool_impl(self):\n        length = len(self)\n        result = 0\n        for i in prange(length):\n            result += self[i]\n\n        return result\n\n    return _sum_bool_impl\n\n\n@sdc_overload(sum)\ndef sdc_sum_overload(self):\n    """"""\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Parallel replacement of numpy.sum.\n    .. only:: developer\n       Test: python -m sdc.runtests sdc.tests.test_sdc_numpy -k sum\n    """"""\n\n    dtype = self.dtype\n    isnan = get_isnan(dtype)\n    if not isinstance(self, types.Array):\n        return None\n\n    if isinstance(dtype, types.Number):\n        def sdc_sum_number_impl(self):\n            length = len(self)\n            result = 0\n            for i in prange(length):\n                if not isnan(self[i]):\n                    result += self[i]\n                else:\n                    return numpy.nan\n\n            return result\n\n        return sdc_sum_number_impl\n\n    if isinstance(dtype, (types.Boolean, bool)):\n        return gen_sum_bool_impl()\n\n\n@sdc_overload(nansum)\ndef sdc_nansum_overload(self):\n    """"""\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Parallel replacement of numpy.nansum.\n    .. only:: developer\n       Test: python -m sdc.runtests sdc.tests.test_sdc_numpy -k nansum\n    """"""\n\n    dtype = self.dtype\n    isnan = get_isnan(dtype)\n    if not isinstance(self, types.Array):\n        return None\n\n    if isinstance(dtype, types.Number):\n        def sdc_nansum_number_impl(self):\n            length = len(self)\n            result = 0\n            for i in prange(length):\n                if not numpy.isnan(self[i]):\n                    result += self[i]\n\n            return result\n\n        return sdc_nansum_number_impl\n\n    if isinstance(dtype, (types.Boolean, bool)):\n        return gen_sum_bool_impl()\n\n\n@sdc_overload(fillna)\ndef sdc_fillna_overload(self, inplace=False, value=None):\n    """"""\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n    Parallel replacement of fillna.\n    .. only:: developer\n       Test: python -m sdc.runtests sdc.tests.test_sdc_numpy -k fillna\n    """"""\n    if not isinstance(self, (types.Array, StringArrayType)):\n        return None\n\n    dtype = self.dtype\n    isnan = get_isnan(dtype)\n    if (\n        (isinstance(inplace, types.Literal) and inplace.literal_value == True) or  # noqa\n        (isinstance(inplace, bool) and inplace == True)  # noqa\n    ):\n        if isinstance(dtype, (types.Integer, types.Boolean)):\n            def sdc_fillna_inplace_int_impl(self, inplace=False, value=None):\n                return None\n\n            return sdc_fillna_inplace_int_impl\n\n        def sdc_fillna_inplace_float_impl(self, inplace=False, value=None):\n            length = len(self)\n            for i in prange(length):\n                if isnan(self[i]):\n                    self[i] = value\n            return None\n\n        return sdc_fillna_inplace_float_impl\n\n    else:\n        if isinstance(self.dtype, types.UnicodeType):\n            def sdc_fillna_str_impl(self, inplace=False, value=None):\n                n = len(self)\n                num_chars = 0\n                # get total chars in new array\n                for i in prange(n):\n                    s = self[i]\n                    if sdc.hiframes.api.isna(self, i):\n                        num_chars += len(value)\n                    else:\n                        num_chars += len(s)\n\n                filled_data = pre_alloc_string_array(n, num_chars)\n                for i in prange(n):\n                    if sdc.hiframes.api.isna(self, i):\n                        filled_data[i] = value\n                    else:\n                        filled_data[i] = self[i]\n                return filled_data\n\n            return sdc_fillna_str_impl\n\n        if isinstance(dtype, (types.Integer, types.Boolean)):\n            def sdc_fillna_int_impl(self, inplace=False, value=None):\n                return copy(self)\n\n            return sdc_fillna_int_impl\n\n        def sdc_fillna_impl(self, inplace=False, value=None):\n            length = len(self)\n            filled_data = numpy.empty(length, dtype=dtype)\n            for i in prange(length):\n                if isnan(self[i]):\n                    filled_data[i] = value\n                else:\n                    filled_data[i] = self[i]\n            return filled_data\n\n        return sdc_fillna_impl\n\n\ndef nanmin(a):\n    pass\n\n\ndef nanmax(a):\n    pass\n\n\ndef nan_min_max_overload_factory(reduce_op):\n    def ov_impl(a):\n        if not isinstance(a, types.Array):\n            return\n\n        if isinstance(a.dtype, (types.Float, types.Complex)):\n            isnan = get_isnan(a.dtype)\n            initial_result = {\n                min: numpy.inf,\n                max: -numpy.inf,\n            }[reduce_op]\n\n            def impl(a):\n                result = initial_result\n                nan_count = 0\n                length = len(a)\n                for i in prange(length):\n                    v = a[i]\n                    if not isnan(v):\n                        result = reduce_op(result, v)\n                    else:\n                        nan_count += 1\n\n                if nan_count == length:\n                    return numpy.nan\n\n                return result\n            return impl\n        else:\n            def impl(a):\n                result = a[0]\n                for i in prange(len(a) - 1):\n                    result = reduce_op(result, a[i + 1])\n                return result\n            return impl\n\n    return ov_impl\n\n\nsdc_overload(nanmin)(nan_min_max_overload_factory(min))\nsdc_overload(nanmax)(nan_min_max_overload_factory(max))\n\n\ndef nanprod(a):\n    pass\n\n\n@sdc_overload(nanprod)\ndef np_nanprod(a):\n    """"""\n    Reimplemented with parfor from numba.np.arraymath.\n    """"""\n    if not isinstance(a, types.Array):\n        return\n    if isinstance(a.dtype, types.Integer):\n        retty = types.intp\n    else:\n        retty = a.dtype\n    one = retty(1)\n    isnan = get_isnan(a.dtype)\n\n    def nanprod_impl(a):\n        c = one\n        for i in prange(len(a)):\n            v = a[i]\n            if not isnan(v):\n                c *= v\n        return c\n\n    return nanprod_impl\n\n\ndef dropna(arr, idx, name):\n    pass\n\n\n@sdc_overload(dropna)\ndef dropna_overload(arr, idx, name):\n    dtype = arr.dtype\n    dtype_idx = idx.dtype\n    isnan = get_isnan(dtype)\n\n    def dropna_impl(arr, idx, name):\n        chunks = parallel_chunks(len(arr))\n        arr_len = numpy.empty(len(chunks), dtype=numpy.int64)\n        length = 0\n\n        for i in prange(len(chunks)):\n            chunk = chunks[i]\n            res = 0\n            for j in range(chunk.start, chunk.stop):\n                if not isnan(arr[j]):\n                    res += 1\n            length += res\n            arr_len[i] = res\n\n        result_data = numpy.empty(shape=length, dtype=dtype)\n        result_index = numpy.empty(shape=length, dtype=dtype_idx)\n        for i in prange(len(chunks)):\n            chunk = chunks[i]\n            new_start = int(sum(arr_len[0:i]))\n            new_stop = new_start + arr_len[i]\n            current_pos = new_start\n\n            for j in range(chunk.start, chunk.stop):\n                if not isnan(arr[j]):\n                    result_data[current_pos] = arr[j]\n                    result_index[current_pos] = idx[j]\n                    current_pos += 1\n\n        return pandas.Series(result_data, result_index, name)\n\n    return dropna_impl\n\n\ndef find_idx(arr, idx):\n    pass\n\n\n@sdc_overload(find_idx)\ndef find_idx_overload(arr, idx):\n    def find_idx_impl(arr, idx):\n        chunks = parallel_chunks(len(arr))\n        new_arr = [List.empty_list(types.int64) for i in range(len(chunks))]\n        for i in prange(len(chunks)):\n            chunk = chunks[i]\n            for j in range(chunk.start, chunk.stop):\n                if arr[j] == idx:\n                    new_arr[i].append(j)\n\n        return new_arr\n\n    return find_idx_impl\n\n\ndef nanmean(a):\n    pass\n\n\n@sdc_overload(nanmean)\ndef np_nanmean(a):\n    if not isinstance(a, types.Array):\n        return\n    isnan = get_isnan(a.dtype)\n\n    def nanmean_impl(a):\n        c = 0.0\n        count = 0\n        for i in prange(len(a)):\n            v = a[i]\n            if not isnan(v):\n                c += v\n                count += 1\n        # np.divide() doesn\'t raise ZeroDivisionError\n        return np.divide(c, count)\n\n    return nanmean_impl\n\n\ndef corr(self, other, method=\'pearson\', min_periods=None):\n    pass\n\n\n@sdc_overload(corr)\ndef corr_overload(self, other, method=\'pearson\', min_periods=None):\n    def corr_impl(self, other, method=\'pearson\', min_periods=None):\n        if method not in (\'pearson\', \'\'):\n            raise ValueError(""Method corr(). Unsupported parameter. Given method != \'pearson\'"")\n\n        if min_periods is None:\n            min_periods = 1\n\n        if min_periods < 1:\n            min_periods = 1\n\n        min_len = min(len(self._data), len(other._data))\n\n        if min_len == 0:\n            return numpy.nan\n\n        sum_y = 0.\n        sum_x = 0.\n        sum_xy = 0.\n        sum_xx = 0.\n        sum_yy = 0.\n        total_count = 0\n        for i in prange(min_len):\n            x = self._data[i]\n            y = other._data[i]\n            if not (numpy.isnan(x) or numpy.isnan(y)):\n                sum_x += x\n                sum_y += y\n                sum_xy += x * y\n                sum_xx += x * x\n                sum_yy += y * y\n                total_count += 1\n\n        if total_count < min_periods:\n            return numpy.nan\n\n        cov_xy = (sum_xy - sum_x * sum_y / total_count)\n        var_x = (sum_xx - sum_x * sum_x / total_count)\n        var_y = (sum_yy - sum_y * sum_y / total_count)\n        corr_xy = cov_xy / numpy.sqrt(var_x * var_y)\n\n        return corr_xy\n\n    return corr_impl\n\n\ndef nanvar(a):\n    pass\n\n\n@sdc_overload(nanvar)\ndef np_nanvar(a):\n    if not isinstance(a, types.Array):\n        return\n    isnan = get_isnan(a.dtype)\n\n    def nanvar_impl(a):\n        # Compute the mean\n        m = nanmean(a)\n\n        # Compute the sum of square diffs\n        ssd = 0.0\n        count = 0\n        for i in prange(len(a)):\n            v = a[i]\n            if not isnan(v):\n                val = (v.item() - m)\n                ssd += np.real(val * np.conj(val))\n                count += 1\n        # np.divide() doesn\'t raise ZeroDivisionError\n        return np.divide(ssd, count)\n\n    return nanvar_impl\n\n\ndef cumsum(a):\n    pass\n\n\ndef nancumsum(a):\n    pass\n\n\n@sdc_overload(cumsum)\ndef np_cumsum(arr):\n    if not isinstance(arr, types.Array):\n        return\n\n    retty = arr.dtype\n    zero = retty(0)\n\n    def cumsum_impl(arr):\n        chunks = parallel_chunks(len(arr))\n        partial_sum = numpy.zeros(len(chunks), dtype=retty)\n        result = numpy.empty_like(arr)\n\n        for i in prange(len(chunks)):\n            chunk = chunks[i]\n            partial = zero\n            for j in range(chunk.start, chunk.stop):\n                result[j] = partial + arr[j]\n                partial = result[j]\n            partial_sum[i] = partial\n\n        for i in prange(len(chunks)):\n            prefix = sum(partial_sum[0:i])\n            chunk = chunks[i]\n            for j in range(chunk.start, chunk.stop):\n                result[j] += prefix\n\n        return result\n\n    return cumsum_impl\n\n\n@sdc_overload(nancumsum)\ndef np_nancumsum(arr, like_pandas=False):\n    if not isinstance(arr, types.Array):\n        return\n\n    if isinstance(arr.dtype, (types.Boolean, types.Integer)):\n        # dtype cannot possibly contain NaN\n        return lambda arr, like_pandas=False: cumsum(arr)\n    else:\n        retty = arr.dtype\n        is_nan = get_isnan(retty)\n        zero = retty(0)\n\n        def nancumsum_impl(arr, like_pandas=False):\n            chunks = parallel_chunks(len(arr))\n            partial_sum = numpy.zeros(len(chunks), dtype=retty)\n            result = numpy.empty_like(arr)\n\n            for i in prange(len(chunks)):\n                chunk = chunks[i]\n                partial = zero\n                for j in range(chunk.start, chunk.stop):\n                    if like_pandas:\n                        result[j] = partial + arr[j]\n                        if ~is_nan(arr[j]):\n                            partial = result[j]\n                    else:\n                        if ~is_nan(arr[j]):\n                            partial += arr[j]\n                        result[j] = partial\n                partial_sum[i] = partial\n\n            for i in prange(len(chunks)):\n                prefix = sum(partial_sum[0:i])\n                chunk = chunks[i]\n                for j in range(chunk.start, chunk.stop):\n                    result[j] += prefix\n\n            return result\n\n        return nancumsum_impl\n\n\ndef getitem_by_mask(arr, idx):\n    pass\n\n\n@sdc_overload(getitem_by_mask)\ndef getitem_by_mask_overload(arr, idx):\n    """"""\n    Creates a new array from arr by selecting elements indicated by Boolean mask idx.\n\n    Parameters\n    -----------\n    arr: :obj:`Array` or :obj:`Range`\n        Input array or range\n    idx: :obj:`Array` of dtype :class:`bool`\n        Boolean mask\n\n    Returns\n    -------\n    :obj:`Array` of the same dtype as arr\n        Array with only elements indicated by mask left\n\n    """"""\n\n    is_range = isinstance(arr, types.RangeType) and isinstance(arr.dtype, types.Integer)\n    is_str_arr = arr == string_array_type\n    if not (isinstance(arr, types.Array) or is_str_arr or is_range):\n        return\n\n    res_dtype = arr.dtype\n    is_str_arr = arr == string_array_type\n    def getitem_by_mask_impl(arr, idx):\n        chunks = parallel_chunks(len(arr))\n        arr_len = numpy.empty(len(chunks), dtype=numpy.int64)\n        length = 0\n\n        for i in prange(len(chunks)):\n            chunk = chunks[i]\n            res = 0\n            for j in range(chunk.start, chunk.stop):\n                if idx[j]:\n                    res += 1\n            length += res\n            arr_len[i] = res\n\n        if is_str_arr == True:  # noqa\n            result_data = [\'\'] * length\n            result_nan_mask = numpy.empty(shape=length, dtype=types.bool_)\n        else:\n            result_data = numpy.empty(shape=length, dtype=res_dtype)\n        for i in prange(len(chunks)):\n            chunk = chunks[i]\n            new_start = int(sum(arr_len[0:i]))\n            current_pos = new_start\n\n            for j in range(chunk.start, chunk.stop):\n                if idx[j]:\n                    if is_range == True:  # noqa\n                        value = arr.start + arr.step * j\n                    else:\n                        value = arr[j]\n                    result_data[current_pos] = value\n                    if is_str_arr == True:  # noqa\n                        result_nan_mask[current_pos] = isna(arr, j)\n                    current_pos += 1\n\n        if is_str_arr == True:  # noqa\n            result_data_as_str_arr = create_str_arr_from_list(result_data)\n            str_arr_set_na_by_mask(result_data_as_str_arr, result_nan_mask)\n            return result_data_as_str_arr\n        else:\n            return result_data\n\n    return getitem_by_mask_impl\n\n\ndef skew(a):\n    pass\n\n\ndef nanskew(a):\n    pass\n\n\n@sdc_overload(skew)\ndef np_skew(arr):\n    if not isinstance(arr, types.Array):\n        return\n\n    def skew_impl(arr):\n        len_val = len(arr)\n        n = 0\n        _sum = 0.\n        square_sum = 0.\n        cube_sum = 0.\n\n        for idx in numba.prange(len_val):\n            if not numpy.isnan(arr[idx]):\n                n += 1\n                _sum += arr[idx]\n                square_sum += arr[idx] ** 2\n                cube_sum += arr[idx] ** 3\n\n        if n == 0 or n < len_val:\n            return numpy.nan\n\n        return skew_formula(n, _sum, square_sum, cube_sum)\n\n    return skew_impl\n\n\n@sdc_overload(nanskew)\ndef np_nanskew(arr):\n    if not isinstance(arr, types.Array):\n        return\n\n    def nanskew_impl(arr):\n        len_val = len(arr)\n        n = 0\n        _sum = 0.\n        square_sum = 0.\n        cube_sum = 0.\n\n        for idx in numba.prange(len_val):\n            if not numpy.isnan(arr[idx]):\n                n += 1\n                _sum += arr[idx]\n                square_sum += arr[idx] ** 2\n                cube_sum += arr[idx] ** 3\n\n        if n == 0:\n            return numpy.nan\n\n        return skew_formula(n, _sum, square_sum, cube_sum)\n\n    return nanskew_impl\n'"
sdc/functions/sort.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nfrom numba import njit, cfunc, literally\nfrom numba.extending import intrinsic, overload\nfrom numba import types\nfrom numba.core import cgutils\nfrom numba import typed\nfrom numba import config\nimport ctypes as ct\n\nfrom sdc import concurrent_sort\n\n\ndef bind(sym, sig):\n    # Returns ctypes binding to symbol sym with signature sig\n    addr = getattr(concurrent_sort, sym)\n    return ct.cast(addr, sig)\n\n\nparallel_sort_arithm_sig = ct.CFUNCTYPE(None, ct.c_void_p, ct.c_uint64)\n\nparallel_sort_sig = ct.CFUNCTYPE(None, ct.c_void_p, ct.c_uint64,\n                                 ct.c_uint64, ct.c_void_p,)\n\nparallel_sort_sym = bind(\'parallel_sort\',\n                         parallel_sort_sig)\n\nparallel_stable_sort_sym = bind(\'parallel_stable_sort\',\n                                parallel_sort_sig)\n\nparallel_sort_t_sig = ct.CFUNCTYPE(None, ct.c_void_p, ct.c_uint64)\n\nset_threads_count_sig = ct.CFUNCTYPE(None, ct.c_uint64)\nset_threads_count_sym = bind(\'set_number_of_threads\', set_threads_count_sig)\n\nset_threads_count_sym(config.NUMBA_NUM_THREADS)\n\n\ndef less(left, right):\n    pass\n\n\n@overload(less, jit_options={\'locals\': {\'result\': types.int8}})\ndef less_overload(left, right):\n    def less_impl(left, right):\n        result = left < right\n        return result\n\n    return less_impl\n\n\n@intrinsic\ndef adaptor(tyctx, thing, another):\n    # This function creates a call specialisation on ""custom_hash"" based on the\n    # type of ""thing"" and its literal value\n\n    # resolve to function type\n    sig = types.intp(thing, another)\n    fnty = tyctx.resolve_value_type(less)\n\n    def codegen(cgctx, builder, sig, args):\n        ty = sig.args[0]\n        # trigger resolution to get a ""custom_hash"" impl based on the call type\n        # ""ty"" and its literal value\n        # import pdb; pdb.set_trace()\n        lsig = fnty.get_call_type(tyctx, (ty, ty), {})\n        resolved = cgctx.get_function(fnty, lsig)\n\n        # close over resolved function, this is to deal with python scoping\n        def resolved_codegen(cgctx, builder, sig, args):\n            return resolved(builder, args)\n\n        # A python function ""wrapper"" is made for the `@cfunc` arg, this calls\n        # the jitted function ""wrappee"", which will be compiled as part of the\n        # compilation chain for the cfunc. In turn the wrappee jitted function\n        # has an intrinsic call which is holding reference to the resolved type\n        # specialised custom_hash call above.\n        @intrinsic\n        def dispatcher(_ityctx, _a, _b):\n            return types.int8(thing, another), resolved_codegen\n\n        @intrinsic\n        def deref(_ityctx, _x):\n            # to deref the void * passed. TODO: nrt awareness\n            catchthing = thing\n            sig = catchthing(_x)\n\n            def codegen(cgctx, builder, sig, args):\n                toty = cgctx.get_value_type(sig.return_type).as_pointer()\n                addressable = builder.bitcast(args[0], toty)\n                zero_intpt = cgctx.get_constant(types.intp, 0)\n                vref = builder.gep(addressable, [zero_intpt], inbounds=True)\n\n                return builder.load(vref)\n\n            return sig, codegen\n\n        @njit\n        def wrappee(ap, bp):\n            a = deref(ap)\n            b = deref(bp)\n            return dispatcher(a, b)\n\n        def wrapper(a, b):\n            return wrappee(a, b)\n\n        callback = cfunc(types.int8(types.voidptr, types.voidptr))(wrapper)\n\n        # bake in address as a int const\n        address = callback.address\n        return cgctx.get_constant(types.intp, address)\n\n    return sig, codegen\n\n\n@intrinsic\ndef asvoidp(tyctx, thing):\n    sig = types.voidptr(thing)\n\n    def codegen(cgctx, builder, sig, args):\n        dm_thing = cgctx.data_model_manager[sig.args[0]]\n        data_thing = dm_thing.as_data(builder, args[0])\n        ptr_thing = cgutils.alloca_once_value(builder, data_thing)\n\n        return builder.bitcast(ptr_thing, cgutils.voidptr_t)\n\n    return sig, codegen\n\n\n@intrinsic\ndef sizeof(context, t):\n    sig = types.uint64(t)\n\n    def codegen(cgctx, builder, sig, args):\n        size = cgctx.get_abi_sizeof(t)\n        return cgctx.get_constant(types.uint64, size)\n\n    return sig, codegen\n\n\ntypes_to_postfix = {types.int8: \'i8\',\n                    types.uint8: \'u8\',\n                    types.int16: \'i16\',\n                    types.uint16: \'u16\',\n                    types.int32: \'i32\',\n                    types.uint32: \'u32\',\n                    types.int64: \'i64\',\n                    types.uint64: \'u64\',\n                    types.float32: \'f32\',\n                    types.float64: \'f64\'}\n\n\ndef load_symbols(name, sig, types):\n    result = {}\n\n    func_text = \'\\n\'.join([f""result[{typ}] = bind(\'{name}_{pstfx}\', sig)"" for typ, pstfx in types.items()])\n    glbls = {f\'{typ}\': typ for typ in types.keys()}\n    glbls.update({\'result\': result, \'sig\': sig, \'bind\': bind})\n    exec(func_text, glbls)\n\n    return result\n\n\nsort_map = load_symbols(\'parallel_sort\', parallel_sort_arithm_sig, types_to_postfix)\nstable_sort_map = load_symbols(\'parallel_stable_sort\', parallel_sort_arithm_sig, types_to_postfix)\n\n\n@intrinsic\ndef list_itemsize(tyctx, list_ty):\n    sig = types.uint64(list_ty)\n\n    def codegen(cgctx, builder, sig, args):\n        nb_lty = sig.args[0]\n        nb_item_ty = nb_lty.item_type\n        ll_item_ty = cgctx.get_value_type(nb_item_ty)\n        item_size = cgctx.get_abi_sizeof(ll_item_ty)\n        return cgctx.get_constant(sig.return_type, item_size)\n\n    return sig, codegen\n\n\ndef itemsize(arr):\n    pass\n\n\n@overload(itemsize)\ndef itemsize_overload(arr):\n    if isinstance(arr, types.Array):\n        def itemsize_impl(arr):\n            return arr.itemsize\n\n        return itemsize_impl\n\n    if isinstance(arr, types.List):\n        def itemsize_impl(arr):\n            return list_itemsize(arr)\n\n        return itemsize_impl\n\n    raise NotImplementedError\n\n\ndef parallel_sort(arr):\n    pass\n\n\n@overload(parallel_sort)\ndef parallel_sort_overload(arr):\n\n    if not isinstance(arr, types.Array):\n        raise NotImplementedError\n\n    dt = arr.dtype\n\n    if dt in types_to_postfix.keys():\n        sort_f = sort_map[dt]\n\n        def parallel_sort_arithm_impl(arr):\n            return sort_f(arr.ctypes, len(arr))\n\n        return parallel_sort_arithm_impl\n\n    def parallel_sort_impl(arr):\n        item_size = itemsize(arr)\n        return parallel_sort_sym(arr.ctypes, len(arr), item_size, adaptor(arr[0], arr[0]))\n\n    return parallel_sort_impl\n\n\ndef parallel_stable_sort(arr):\n    pass\n\n\n@overload(parallel_stable_sort)\ndef parallel_stable_sort_overload(arr):\n\n    if not isinstance(arr, types.Array):\n        raise NotImplementedError\n\n    dt = arr.dtype\n\n    if dt in types_to_postfix.keys():\n        sort_f = stable_sort_map[dt]\n\n        def parallel_stable_sort_arithm_impl(arr):\n            return sort_f(arr.ctypes, len(arr))\n\n        return parallel_stable_sort_arithm_impl\n\n    def parallel_stable_sort_impl(arr):\n        item_size = itemsize(arr)\n        return parallel_stable_sort_sym(arr.ctypes, len(arr), item_size, adaptor(arr[0], arr[0]))\n\n    return parallel_stable_sort_impl\n'"
sdc/functions/statistics.py,0,"b'# -*- coding: utf-8 -*-\n# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numpy\nfrom sdc.utilities.utils import sdc_register_jitable\n\n\n@sdc_register_jitable\ndef skew_formula(n, _sum, square_sum, cube_sum):\n    m2 = (square_sum - _sum * _sum / n) / n\n    m3 = (cube_sum - 3. * _sum * square_sum / n + 2. * _sum * _sum * _sum / n / n) / n\n    res = numpy.nan if m2 == 0 else m3 / m2 ** 1.5\n\n    if (n > 2) & (m2 > 0):\n        res = numpy.sqrt((n - 1.) * n) / (n - 2.) * m3 / m2 ** 1.5\n\n    return res\n'"
sdc/hiframes/__init__.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n'"
sdc/hiframes/aggregate.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\nfrom __future__ import print_function, division, absolute_import\n\nimport numba\nfrom numba.core import types\nfrom numba.core.typing import signature\nfrom numba.core.typing.templates import infer_global, AbstractTemplate\nfrom numba.extending import lower_builtin\n\n\n@infer_global(bool)\nclass BoolNoneTyper(AbstractTemplate):\n    def generic(self, args, kws):\n        assert not kws\n        assert len(args) == 1\n        val_t = args[0]\n        if val_t == types.none:\n            return signature(types.boolean, *args)\n\n\n@lower_builtin(bool, types.none)\ndef lower_column_mean_impl(context, builder, sig, args):\n    res = context.compile_internal(builder, lambda a: False, sig, args)\n    return res  # impl_ret_untracked(context, builder, sig.return_type, res)\n'"
sdc/hiframes/api.py,9,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\nimport numpy as np\n\nimport numba\nfrom numba.core import cgutils, types\nfrom numba.parfors import array_analysis\nfrom numba.core.typing import signature\nfrom numba.core.typing.templates import infer_global, AbstractTemplate, CallableTemplate\nfrom numba.extending import overload, intrinsic\nfrom numba.core.imputils import (lower_builtin, impl_ret_borrowed)\n\nimport sdc\nfrom sdc.str_ext import string_type, list_string_array_type\nfrom sdc.str_arr_ext import (\n    StringArrayType,\n    string_array_type)\nfrom sdc.hiframes.pd_series_ext import (\n    SeriesType,\n    if_series_to_array_type)\nfrom numba.core.errors import TypingError\n\n\ndef isna(arr, i):\n    return False\n\n\n@overload(isna)\ndef isna_overload(arr, i):\n    if arr == string_array_type:\n        return lambda arr, i: sdc.str_arr_ext.str_arr_is_na(arr, i)\n    # TODO: support NaN in list(list(str))\n    if arr == list_string_array_type:\n        return lambda arr, i: False\n    # TODO: extend to other types\n    assert isinstance(arr, types.Array) or isinstance(arr, types.List)\n    dtype = arr.dtype\n    if isinstance(dtype, types.Float):\n        return lambda arr, i: np.isnan(arr[i])\n\n    # NaT for dt64\n    if isinstance(dtype, (types.NPDatetime, types.NPTimedelta)):\n        nat = dtype(\'NaT\')\n        # TODO: replace with np.isnat\n        return lambda arr, i: arr[i] == nat\n\n    # XXX integers don\'t have nans, extend to boolean\n    return lambda arr, i: False\n\n\ndef get_nan_mask(arr):\n    return np.zeros(len(arr), np.bool_)\n\n\n@overload(get_nan_mask)\ndef get_nan_mask_overload(arr):\n\n    _func_name = ""Function: get_nan_mask""\n    def get_nan_mask_via_isna_impl(arr):\n        len_arr = len(arr)\n        res = np.empty(len_arr, dtype=np.bool_)\n        for i in numba.prange(len_arr):\n            res[i] = isna(arr, i)\n        return res\n\n    if isinstance(arr, types.Array):\n        dtype = arr.dtype\n        if isinstance(dtype, types.Float):\n            return lambda arr: np.isnan(arr)\n        elif isinstance(dtype, (types.Boolean, types.Integer)):\n            return lambda arr: np.zeros(len(arr), np.bool_)\n        elif isinstance(dtype, (types.NPDatetime, types.NPTimedelta)):\n            return get_nan_mask_via_isna_impl\n        else:\n            raise TypingError(\'{} Not implemented for arrays with dtype: {}\'.format(_func_name, dtype))\n    else:\n        # for StringArrayType and other cases rely on isna implementation\n        return get_nan_mask_via_isna_impl\n\n\ndef fix_df_array(c):  # pragma: no cover\n    return c\n\n# the same as fix_df_array but can be parallel\n@numba.generated_jit(nopython=True)\ndef parallel_fix_df_array(c):  # pragma: no cover\n    return lambda c: fix_df_array(c)\n\n\ndef fix_rolling_array(c):  # pragma: no cover\n    return c\n\n\ndef dummy_unbox_series(arr):\n    return arr\n\n\n@infer_global(dummy_unbox_series)\nclass DummyToSeriesType(AbstractTemplate):\n    def generic(self, args, kws):\n        assert not kws\n        assert len(args) == 1\n        arr = if_series_to_array_type(args[0], True)\n        return signature(arr, *args)\n\n\n@lower_builtin(dummy_unbox_series, types.Any)\ndef dummy_unbox_series_impl(context, builder, sig, args):\n    return impl_ret_borrowed(context, builder, sig.return_type, args[0])\n\n\n# this function should be used for getting S._data for alias analysis to work\n# no_cpython_wrapper since Array(DatetimeDate) cannot be boxed\n@numba.generated_jit(nopython=True, no_cpython_wrapper=True)\ndef get_series_data(S):\n    return lambda S: S._data\n\n\n# XXX: use infer_global instead of overload, since overload fails if the same\n# user function is compiled twice\n@infer_global(fix_df_array)\nclass FixDfArrayType(AbstractTemplate):\n    def generic(self, args, kws):\n        assert not kws\n        assert len(args) == 1\n        column = types.unliteral(args[0])\n        ret_typ = column\n        if (isinstance(column, types.List)\n            and (isinstance(column.dtype, types.Number)\n                 or column.dtype == types.boolean)):\n            ret_typ = types.Array(column.dtype, 1, \'C\')\n        if (isinstance(column, types.List)\n            and (column.dtype == string_type\n                 or isinstance(column.dtype, types.Optional) and column.dtype.type == string_type)):\n            ret_typ = string_array_type\n        if isinstance(column, SeriesType):\n            ret_typ = column.data\n        # TODO: add other types\n        return signature(ret_typ, column)\n\n\n@lower_builtin(fix_df_array, types.Any)  # TODO: replace Any with types\ndef lower_fix_df_array(context, builder, sig, args):\n    func = fix_df_array_overload(sig.args[0])\n    res = context.compile_internal(builder, func, sig, args)\n    return impl_ret_borrowed(context, builder, sig.return_type, res)\n\n\ndef fix_df_array_overload(column):\n    # convert list of numbers/bools to numpy array\n    if (isinstance(column, types.List)\n            and (isinstance(column.dtype, types.Number)\n                 or column.dtype == types.boolean)):\n        def fix_df_array_list_impl(column):  # pragma: no cover\n            return np.array(column)\n        return fix_df_array_list_impl\n\n    # convert list of strings to string array\n    if (isinstance(column, types.List)\n        and (column.dtype == string_type\n             or isinstance(column.dtype, types.Optional) and column.dtype.type == string_type)):\n\n        def fix_df_array_str_impl(column):  # pragma: no cover\n            return sdc.str_arr_ext.StringArray(column)\n        return fix_df_array_str_impl\n\n    if isinstance(column, SeriesType):\n        return lambda column: sdc.hiframes.api.get_series_data(column)\n\n    # column is array if not list\n    assert isinstance(column, (types.Array, StringArrayType, SeriesType))\n\n    def fix_df_array_impl(column):  # pragma: no cover\n        return column\n    # FIXME: np.array() for everything else?\n    return fix_df_array_impl\n\n\n@infer_global(fix_rolling_array)\nclass FixDfRollingArrayType(AbstractTemplate):\n    def generic(self, args, kws):\n        assert not kws\n        assert len(args) == 1\n        column = args[0]\n        dtype = column.dtype\n        ret_typ = column\n        if dtype == types.boolean or isinstance(dtype, types.Integer):\n            ret_typ = types.Array(types.float64, 1, \'C\')\n        # TODO: add other types\n        return signature(ret_typ, column)\n\n\n@lower_builtin(fix_rolling_array, types.Any)  # TODO: replace Any with types\ndef lower_fix_rolling_array(context, builder, sig, args):\n    func = fix_rolling_array_overload(sig.args[0])\n    res = context.compile_internal(builder, func, sig, args)\n    return impl_ret_borrowed(context, builder, sig.return_type, res)\n\n\ndef fix_rolling_array_overload(column):\n    assert isinstance(column, types.Array)\n    dtype = column.dtype\n    # convert bool and integer to float64\n    if dtype == types.boolean or isinstance(dtype, types.Integer):\n        def fix_rolling_array_impl(column):  # pragma: no cover\n            return column.astype(np.float64)\n    else:\n        def fix_rolling_array_impl(column):  # pragma: no cover\n            return column\n    return fix_rolling_array_impl\n\n\n@intrinsic\ndef init_series(typingctx, data, index=None, name=None):\n    """"""Create a Series with provided data, index and name values.\n    Used as a single constructor for Series and assigning its data, so that\n    optimization passes can look for init_series() to see if underlying\n    data has changed, and get the array variables from init_series() args if\n    not changed.\n    """"""\n\n    index = types.none if index is None else index\n    name = types.none if name is None else name\n    is_named = False if name is types.none else True\n\n    def codegen(context, builder, signature, args):\n        data_val, index_val, name_val = args\n        # create series struct and store values\n        series = cgutils.create_struct_proxy(\n            signature.return_type)(context, builder)\n        series.data = data_val\n        series.index = index_val\n        if is_named:\n            if isinstance(name, types.StringLiteral):\n                series.name = numba.cpython.unicode.make_string_from_constant(\n                    context, builder, string_type, name.literal_value)\n            else:\n                series.name = name_val\n\n        # increase refcount of stored values\n        if context.enable_nrt:\n            context.nrt.incref(builder, signature.args[0], data_val)\n            context.nrt.incref(builder, signature.args[1], index_val)\n            if is_named:\n                context.nrt.incref(builder, signature.args[2], name_val)\n\n        return series._getvalue()\n\n    dtype = data.dtype\n    # XXX pd.DataFrame() calls init_series for even Series since it\'s untyped\n    data = if_series_to_array_type(data)\n    ret_typ = SeriesType(dtype, data, index, is_named)\n    sig = signature(ret_typ, data, index, name)\n    return sig, codegen\n'"
sdc/hiframes/boxing.py,5,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\n\nimport pandas as pd\nimport pandas.api.types\nimport numpy as np\nimport numba\nfrom numba.extending import (typeof_impl, unbox, register_model, models,\n                             NativeValue, box, intrinsic)\nfrom numba import types\nfrom numba.core import cgutils\nfrom numba.np import numpy_support\nfrom numba.core.typing import signature\nfrom numba.core.boxing import box_array, unbox_array, box_list\nfrom numba.core.boxing import _NumbaTypeHelper\nfrom numba.cpython import listobj\n\nfrom sdc.hiframes.pd_dataframe_type import DataFrameType\nfrom sdc.str_ext import string_type, list_string_array_type\nfrom sdc.str_arr_ext import (string_array_type, unbox_str_series, box_str_arr)\nfrom sdc.datatypes.categorical.types import CategoricalDtypeType, Categorical\nfrom sdc.datatypes.categorical.boxing import unbox_Categorical, box_Categorical\nfrom sdc.hiframes.pd_series_ext import SeriesType\nfrom sdc.hiframes.pd_series_type import _get_series_array_type\n\nfrom sdc.hiframes.pd_dataframe_ext import get_structure_maps\n\nfrom .. import hstr_ext\nimport llvmlite.binding as ll\nfrom llvmlite import ir as lir\nfrom llvmlite.llvmpy.core import Type as LLType\nll.add_symbol(\'array_size\', hstr_ext.array_size)\nll.add_symbol(\'array_getptr1\', hstr_ext.array_getptr1)\n\n\n@typeof_impl.register(pd.DataFrame)\ndef typeof_pd_dataframe(val, c):\n\n    col_names = tuple(val.columns.tolist())\n    # TODO: support other types like string and timestamp\n    col_types = get_hiframes_dtypes(val)\n    index_type = _infer_index_type(val.index)\n    column_loc, _, _ = get_structure_maps(col_types, col_names)\n\n    return DataFrameType(col_types, index_type, col_names, True, column_loc=column_loc)\n\n\n# register series types for import\n@typeof_impl.register(pd.Series)\ndef typeof_pd_str_series(val, c):\n    index_type = _infer_index_type(val.index)\n    is_named = val.name is not None\n    return SeriesType(\n        _infer_series_dtype(val), index=index_type, is_named=is_named)\n\n\n@unbox(DataFrameType)\ndef unbox_dataframe(typ, val, c):\n    """"""unbox dataframe to an empty DataFrame struct\n    columns will be extracted later if necessary.\n    """"""\n    n_cols = len(typ.columns)\n    column_strs = [numba.cpython.unicode.make_string_from_constant(\n        c.context, c.builder, string_type, a) for a in typ.columns]\n    # create dataframe struct and store values\n    dataframe = cgutils.create_struct_proxy(typ)(c.context, c.builder)\n\n    errorptr = cgutils.alloca_once_value(c.builder, cgutils.false_bit)\n\n    col_list_type = types.List(string_type)\n    ok, inst = listobj.ListInstance.allocate_ex(c.context, c.builder, col_list_type, n_cols)\n\n    with c.builder.if_else(ok, likely=True) as (if_ok, if_not_ok):\n        with if_ok:\n            inst.size = c.context.get_constant(types.intp, n_cols)\n            for i, column_str in enumerate(column_strs):\n                inst.setitem(c.context.get_constant(types.intp, i), column_str, incref=False)\n            dataframe.columns = inst.value\n\n        with if_not_ok:\n            c.builder.store(cgutils.true_bit, errorptr)\n\n    # If an error occurred, drop the whole native list\n    with c.builder.if_then(c.builder.load(errorptr)):\n        c.context.nrt.decref(c.builder, col_list_type, inst.value)\n\n    _, data_typs_map, types_order = get_structure_maps(typ.data, typ.columns)\n\n    for col_typ in types_order:\n        type_id, col_indices = data_typs_map[col_typ]\n        n_type_cols = len(col_indices)\n        list_type = types.List(col_typ)\n        ok, inst = listobj.ListInstance.allocate_ex(c.context, c.builder, list_type, n_type_cols)\n\n        with c.builder.if_else(ok, likely=True) as (if_ok, if_not_ok):\n            with if_ok:\n                inst.size = c.context.get_constant(types.intp, n_type_cols)\n                for i, col_idx in enumerate(col_indices):\n                    series_obj = c.pyapi.object_getattr_string(val, typ.columns[col_idx])\n                    arr_obj = c.pyapi.object_getattr_string(series_obj, ""values"")\n                    ty_series = typ.data[col_idx]\n                    if isinstance(ty_series, types.Array):\n                        native_val = unbox_array(typ.data[col_idx], arr_obj, c)\n                    elif ty_series == string_array_type:\n                        native_val = unbox_str_series(string_array_type, series_obj, c)\n\n                    inst.setitem(c.context.get_constant(types.intp, i), native_val.value, incref=False)\n\n                dataframe.data = c.builder.insert_value(dataframe.data, inst.value, type_id)\n\n            with if_not_ok:\n                c.builder.store(cgutils.true_bit, errorptr)\n\n        # If an error occurred, drop the whole native list\n        with c.builder.if_then(c.builder.load(errorptr)):\n            c.context.nrt.decref(c.builder, list_type, inst.value)\n\n    # TODO: support unboxing index\n    if typ.index == types.none:\n        dataframe.index = c.context.get_constant(types.none, None)\n    if typ.index == string_array_type:\n        index_obj = c.pyapi.object_getattr_string(val, ""index"")\n        dataframe.index = unbox_str_series(string_array_type, index_obj, c).value\n    if isinstance(typ.index, types.Array):\n        index_obj = c.pyapi.object_getattr_string(val, ""index"")\n        index_data = c.pyapi.object_getattr_string(index_obj, ""_data"")\n        dataframe.index = unbox_array(typ.index, index_data, c).value\n\n    dataframe.parent = val\n\n    # increase refcount of stored values\n    if c.context.enable_nrt:\n        # TODO: other objects?\n        for var in column_strs:\n            c.context.nrt.incref(c.builder, string_type, var)\n\n    return NativeValue(dataframe._getvalue(), is_error=c.builder.load(errorptr))\n\n\ndef get_hiframes_dtypes(df):\n    """"""get hiframe data types for a pandas dataframe\n    """"""\n    col_names = df.columns.tolist()\n    hi_typs = [_get_series_array_type(_infer_series_dtype(df[cname]))\n               for cname in col_names]\n    return tuple(hi_typs)\n\n\ndef _infer_series_dtype(S):\n    if S.dtype == np.dtype(\'O\'):\n        # XXX assuming the whole column is strings if 1st val is string\n        # TODO: handle NA as 1st value\n        i = 0\n        while i < len(S) and (S.iloc[i] is np.nan or S.iloc[i] is None):\n            i += 1\n        if i == len(S):\n            raise ValueError(\n                ""object dtype infer out of bounds for {}"".format(S.name))\n\n        first_val = S.iloc[i]\n        if isinstance(first_val, list):\n            return _infer_series_list_dtype(S)\n        elif isinstance(first_val, str):\n            return string_type\n        else:\n            raise ValueError(\n                ""object dtype infer: data type for column {} not supported"".format(S.name))\n    elif isinstance(S.dtype, pd.CategoricalDtype):\n        return numba.typeof(S.dtype)\n    # regular numpy types\n    try:\n        return numpy_support.from_dtype(S.dtype)\n    except NotImplementedError:\n        raise ValueError(""np dtype infer: data type for column {} not supported"".format(S.name))\n\n\ndef _infer_series_list_dtype(S):\n    for i in range(len(S)):\n        first_val = S.iloc[i]\n        if not isinstance(first_val, list):\n            raise ValueError(\n                ""data type for column {} not supported"".format(S.name))\n        if len(first_val) > 0:\n            # TODO: support more types\n            if isinstance(first_val[0], str):\n                return types.List(string_type)\n            else:\n                raise ValueError(\n                    ""data type for column {} not supported"".format(S.name))\n    raise ValueError(\n        ""data type for column {} not supported"".format(S.name))\n\n\ndef _infer_index_type(index):\n    \'\'\'\n    Convertion input index type into Numba known type\n    need to return instance of the type class\n    \'\'\'\n\n    if isinstance(index, (types.NoneType, pd.RangeIndex, pd.DatetimeIndex)) or index is None or len(index) == 0:\n        return types.none\n\n    if index.dtype == np.dtype(\'O\') and len(index) > 0:\n        first_val = index[0]\n        if isinstance(first_val, str):\n            return string_array_type\n\n    numba_index_type = numpy_support.from_dtype(index.dtype)\n    return types.Array(numba_index_type, 1, \'C\')\n\n\n@box(DataFrameType)\ndef box_dataframe(typ, val, c):\n    context = c.context\n    builder = c.builder\n\n    col_names = typ.columns\n    arr_typs = typ.data\n\n    dataframe = cgutils.create_struct_proxy(typ)(context, builder, value=val)\n\n    pyapi = c.pyapi\n    # gil_state = pyapi.gil_ensure()  # acquire GIL\n\n    mod_name = context.insert_const_string(c.builder.module, ""pandas"")\n    class_obj = pyapi.import_module_noblock(mod_name)\n    df_dict = pyapi.dict_new()\n\n    arrays_list_objs = {}\n    for cname, arr_typ in zip(col_names, arr_typs):\n        # df[\'cname\'] = boxed_arr\n        # TODO: datetime.date, DatetimeIndex?\n        name_str = context.insert_const_string(c.builder.module, cname)\n        cname_obj = pyapi.string_from_string(name_str)\n\n        col_loc = typ.column_loc[cname]\n        type_id, col_id = col_loc.type_id, col_loc.col_id\n\n        # dataframe.data looks like a tuple(list(array))\n        # e.g. ([array(int64, 1d, C), array(int64, 1d, C)], [array(float64, 1d, C)])\n        arrays_list_obj = arrays_list_objs.get(type_id)\n        if arrays_list_obj is None:\n            list_typ = types.List(arr_typ)\n            # extracting list from the tuple\n            list_val = builder.extract_value(dataframe.data, type_id)\n            # getting array from the list to box it then\n            arrays_list_obj = box_list(list_typ, list_val, c)\n            arrays_list_objs[type_id] = arrays_list_obj\n\n        # PyList_GetItem returns borrowed reference\n        arr_obj = pyapi.list_getitem(arrays_list_obj, col_id)\n        pyapi.dict_setitem(df_dict, cname_obj, arr_obj)\n\n        pyapi.decref(cname_obj)\n\n    df_obj = pyapi.call_method(class_obj, ""DataFrame"", (df_dict,))\n    pyapi.decref(df_dict)\n\n    # set df.index if necessary\n    if typ.index != types.none:\n        arr_obj = _box_series_data(typ.index.dtype, typ.index, dataframe.index, c)\n        pyapi.object_setattr_string(df_obj, \'index\', arr_obj)\n        pyapi.decref(arr_obj)\n\n    for arrays_list_obj in arrays_list_objs.values():\n        pyapi.decref(arrays_list_obj)\n\n    pyapi.decref(class_obj)\n    # pyapi.gil_release(gil_state)    # release GIL\n    return df_obj\n\n\n@intrinsic\ndef unbox_dataframe_column(typingctx, df, i=None):\n\n    def codegen(context, builder, sig, args):\n        pyapi = context.get_python_api(builder)\n        c = numba.pythonapi._UnboxContext(context, builder, pyapi)\n\n        df_typ = sig.args[0]\n        col_ind = sig.args[1].literal_value\n        data_typ = df_typ.data[col_ind]\n        col_name = df_typ.columns[col_ind]\n        # TODO: refcounts?\n\n        dataframe = cgutils.create_struct_proxy(\n            sig.args[0])(context, builder, value=args[0])\n        series_obj = c.pyapi.object_getattr_string(dataframe.parent, col_name)\n        arr_obj = c.pyapi.object_getattr_string(series_obj, ""values"")\n\n        # TODO: support column of tuples?\n        native_val = _unbox_series_data(\n            data_typ.dtype, data_typ, arr_obj, c)\n\n        c.pyapi.decref(series_obj)\n        c.pyapi.decref(arr_obj)\n        c.context.nrt.incref(builder, df_typ.index, dataframe.index)\n\n        # assign array and set unboxed flag\n        dataframe.data = builder.insert_value(\n            dataframe.data, native_val.value, col_ind)\n        return dataframe._getvalue()\n\n    return signature(df, df, i), codegen\n\n\n@unbox(SeriesType)\ndef unbox_series(typ, val, c):\n    arr_obj = c.pyapi.object_getattr_string(val, ""values"")\n    series = cgutils.create_struct_proxy(typ)(c.context, c.builder)\n    series.data = _unbox_series_data(typ.dtype, typ.data, arr_obj, c).value\n    # TODO: other indices\n    if typ.index == string_array_type:\n        index_obj = c.pyapi.object_getattr_string(val, ""index"")\n        series.index = unbox_str_series(string_array_type, index_obj, c).value\n\n    if isinstance(typ.index, types.Array):\n        index_obj = c.pyapi.object_getattr_string(val, ""index"")\n        index_data = c.pyapi.object_getattr_string(index_obj, ""_data"")\n        series.index = unbox_array(typ.index, index_data, c).value\n\n    if typ.is_named:\n        name_obj = c.pyapi.object_getattr_string(val, ""name"")\n        series.name = numba.cpython.unicode.unbox_unicode_str(\n            string_type, name_obj, c).value\n    # TODO: handle index and name\n    c.pyapi.decref(arr_obj)\n    return NativeValue(series._getvalue())\n\n\ndef _unbox_series_data(dtype, data_typ, arr_obj, c):\n    if data_typ == string_array_type:\n        return unbox_str_series(string_array_type, arr_obj, c)\n    elif data_typ == list_string_array_type:\n        return _unbox_array_list_str(arr_obj, c)\n    elif isinstance(dtype, CategoricalDtypeType):\n        return unbox_Categorical(data_typ, arr_obj, c)\n\n    # TODO: error handling like Numba callwrappers.py\n    return unbox_array(data_typ, arr_obj, c)\n\n\n@box(SeriesType)\ndef box_series(typ, val, c):\n    """"""\n    """"""\n    mod_name = c.context.insert_const_string(c.builder.module, ""pandas"")\n    pd_class_obj = c.pyapi.import_module_noblock(mod_name)\n    dtype = typ.dtype\n\n    series = cgutils.create_struct_proxy(\n        typ)(c.context, c.builder, val)\n\n    arr = _box_series_data(dtype, typ.data, series.data, c)\n\n    if typ.index is types.none:\n        index = c.pyapi.make_none()\n    else:\n        # TODO: index-specific boxing like RangeIndex() etc.\n        index = _box_series_data(\n            typ.index.dtype, typ.index, series.index, c)\n\n    if typ.is_named:\n        name = c.pyapi.from_native_value(string_type, series.name)\n    else:\n        name = c.pyapi.make_none()\n\n    dtype = c.pyapi.make_none()  # TODO: dtype\n    res = c.pyapi.call_method(\n        pd_class_obj, ""Series"", (arr, index, dtype, name))\n\n    c.pyapi.decref(arr)\n    c.pyapi.decref(index)\n    c.pyapi.decref(dtype)\n    c.pyapi.decref(name)\n    c.pyapi.decref(pd_class_obj)\n    return res\n\n\ndef _box_series_data(dtype, data_typ, val, c):\n\n    if isinstance(dtype, types.BaseTuple):\n        np_dtype = np.dtype(\n            \',\'.join(str(t) for t in dtype.types), align=True)\n        dtype = numba.np.numpy_support.from_dtype(np_dtype)\n\n    if dtype == string_type:\n        arr = box_str_arr(string_array_type, val, c)\n    elif isinstance(dtype, CategoricalDtypeType):\n        arr = box_Categorical(data_typ, val, c)\n    elif dtype == types.List(string_type):\n        arr = box_list(list_string_array_type, val, c)\n    else:\n        arr = box_array(data_typ, val, c)\n\n    if isinstance(dtype, types.Record):\n        o_str = c.context.insert_const_string(c.builder.module, ""O"")\n        o_str = c.pyapi.string_from_string(o_str)\n        arr = c.pyapi.call_method(arr, ""astype"", (o_str,))\n\n    return arr\n\n\ndef _unbox_array_list_str(obj, c):\n    #\n    typ = list_string_array_type\n    # from unbox_list\n    errorptr = cgutils.alloca_once_value(c.builder, cgutils.false_bit)\n    listptr = cgutils.alloca_once(c.builder, c.context.get_value_type(typ))\n\n    # get size of array\n    arr_size_fnty = LLType.function(c.pyapi.py_ssize_t, [c.pyapi.pyobj])\n    arr_size_fn = c.pyapi._get_function(arr_size_fnty, name=""array_size"")\n    size = c.builder.call(arr_size_fn, [obj])\n    # cgutils.printf(c.builder, \'size %d\\n\', size)\n\n    _python_array_obj_to_native_list(typ, obj, c, size, listptr, errorptr)\n\n    return NativeValue(c.builder.load(listptr),\n                       is_error=c.builder.load(errorptr))\n\n\ndef _python_array_obj_to_native_list(typ, obj, c, size, listptr, errorptr):\n    """"""\n    Construct a new native list from a Python array of objects.\n    copied from _python_list_to_native but list_getitem is converted to array\n    getitem.\n    """"""\n    def check_element_type(nth, itemobj, expected_typobj):\n        typobj = nth.typeof(itemobj)\n        # Check if *typobj* is NULL\n        with c.builder.if_then(\n                cgutils.is_null(c.builder, typobj),\n                likely=False,\n        ):\n            c.builder.store(cgutils.true_bit, errorptr)\n            loop.do_break()\n        # Mandate that objects all have the same exact type\n        type_mismatch = c.builder.icmp_signed(\'!=\', typobj, expected_typobj)\n\n        with c.builder.if_then(type_mismatch, likely=False):\n            c.builder.store(cgutils.true_bit, errorptr)\n            c.pyapi.err_format(\n                ""PyExc_TypeError"",\n                ""can\'t unbox heterogeneous list: %S != %S"",\n                expected_typobj, typobj,\n            )\n            c.pyapi.decref(typobj)\n            loop.do_break()\n        c.pyapi.decref(typobj)\n\n    # Allocate a new native list\n    ok, list = listobj.ListInstance.allocate_ex(c.context, c.builder, typ, size)\n    # Array getitem call\n    arr_get_fnty = LLType.function(LLType.pointer(c.pyapi.pyobj), [c.pyapi.pyobj, c.pyapi.py_ssize_t])\n    arr_get_fn = c.pyapi._get_function(arr_get_fnty, name=""array_getptr1"")\n\n    with c.builder.if_else(ok, likely=True) as (if_ok, if_not_ok):\n        with if_ok:\n            list.size = size\n            zero = lir.Constant(size.type, 0)\n            with c.builder.if_then(c.builder.icmp_signed(\'>\', size, zero),\n                                   likely=True):\n                # Traverse Python list and unbox objects into native list\n                with _NumbaTypeHelper(c) as nth:\n                    # Note: *expected_typobj* can\'t be NULL\n                    # TODO: enable type checking when emty list item in\n                    # list(list(str)) case can be handled\n                    # expected_typobj = nth.typeof(c.builder.load(\n                    #                 c.builder.call(arr_get_fn, [obj, zero])))\n                    with cgutils.for_range(c.builder, size) as loop:\n                        itemobj = c.builder.call(arr_get_fn, [obj, loop.index])\n                        # extra load since we have ptr to object\n                        itemobj = c.builder.load(itemobj)\n                        # c.pyapi.print_object(itemobj)\n                        # check_element_type(nth, itemobj, expected_typobj)\n                        # XXX we don\'t call native cleanup for each\n                        # list element, since that would require keeping\n                        # of which unboxings have been successful.\n                        native = c.unbox(typ.dtype, itemobj)\n                        with c.builder.if_then(native.is_error, likely=False):\n                            c.builder.store(cgutils.true_bit, errorptr)\n                            loop.do_break()\n                        # The object (e.g. string) is stored so incref=True\n                        list.setitem(loop.index, native.value, incref=True)\n                    # c.pyapi.decref(expected_typobj)\n            if typ.reflected:\n                list.parent = obj\n            # Stuff meminfo pointer into the Python object for\n            # later reuse.\n            with c.builder.if_then(c.builder.not_(c.builder.load(errorptr)),\n                                   likely=False):\n                c.pyapi.object_set_private_data(obj, list.meminfo)\n            list.set_dirty(False)\n            c.builder.store(list.value, listptr)\n\n        with if_not_ok:\n            c.builder.store(cgutils.true_bit, errorptr)\n\n    # If an error occurred, drop the whole native list\n    with c.builder.if_then(c.builder.load(errorptr)):\n        c.context.nrt.decref(c.builder, typ, list.value)\n'"
sdc/hiframes/join.py,1,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\nimport numpy as np\n\nfrom numba import types\nfrom numba.extending import overload\n\n\ndef setitem_arr_nan(arr, ind):\n    arr[ind] = np.nan\n\n\n@overload(setitem_arr_nan)\ndef setitem_arr_nan_overload(arr, ind):\n    if isinstance(arr.dtype, types.Float):\n        return setitem_arr_nan\n\n    return lambda arr, ind: None\n'"
sdc/hiframes/pd_categorical_ext.py,3,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\nimport numba\nfrom numba.extending import (box, unbox, typeof_impl, register_model, models,\n                             NativeValue, lower_builtin, lower_cast, overload,\n                             type_callable, overload_method, intrinsic)\nfrom numba import types\nfrom numba.core.boxing import box_array, unbox_array\n\nimport numpy as np\n\n\nclass PDCategoricalDtype(types.Opaque):\n    def __init__(self, _categories):\n        self.categories = _categories\n        name = \'PDCategoricalDtype({})\'.format(self.categories)\n        super(PDCategoricalDtype, self).__init__(name=name)\n\n\nclass CategoricalArray(types.Array):\n    def __init__(self, dtype):\n        self.dtype = dtype\n        super(CategoricalArray, self).__init__(\n            dtype, 1, \'C\', name=\'CategoricalArray({})\'.format(dtype))\n\n\n# @unbox(CategoricalArray)\ndef unbox_categorical_array(typ, val, c):\n    arr_obj = c.pyapi.object_getattr_string(val, ""codes"")\n    # c.pyapi.print_object(arr_obj)\n    dtype = get_categories_int_type(typ.dtype)\n    native_val = unbox_array(types.Array(dtype, 1, \'C\'), arr_obj, c)\n    c.pyapi.decref(arr_obj)\n    return native_val\n\n\ndef get_categories_int_type(cat_dtype):\n    dtype = types.int64\n    n_cats = len(cat_dtype.categories)\n    if n_cats < np.iinfo(np.int8).max:\n        dtype = types.int8\n    elif n_cats < np.iinfo(np.int16).max:\n        dtype = types.int16\n    elif n_cats < np.iinfo(np.int32).max:\n        dtype = types.int32\n    return dtype\n\n\n# @box(CategoricalArray)\ndef box_categorical_array(typ, val, c):\n    dtype = typ.dtype\n    mod_name = c.context.insert_const_string(c.builder.module, ""pandas"")\n    pd_class_obj = c.pyapi.import_module_noblock(mod_name)\n\n    # categories list e.g. [\'A\', \'B\', \'C\']\n    item_objs = _get_cat_obj_items(dtype.categories, c)\n    n = len(item_objs)\n    list_obj = c.pyapi.list_new(c.context.get_constant(types.intp, n))\n    for i in range(n):\n        idx = c.context.get_constant(types.intp, i)\n        c.pyapi.incref(item_objs[i])\n        c.pyapi.list_setitem(list_obj, idx, item_objs[i])\n\n\n    int_dtype = get_categories_int_type(dtype)\n    arr = box_array(types.Array(int_dtype, 1, \'C\'), val, c)\n\n    pdcat_cls_obj = c.pyapi.object_getattr_string(pd_class_obj, ""Categorical"")\n    cat_arr = c.pyapi.call_method(pdcat_cls_obj, ""from_codes"", (arr, list_obj))\n    c.pyapi.decref(pdcat_cls_obj)\n    c.pyapi.decref(arr)\n    c.pyapi.decref(list_obj)\n    for obj in item_objs:\n        c.pyapi.decref(obj)\n\n    c.pyapi.decref(pd_class_obj)\n    return cat_arr\n\n\ndef _get_cat_obj_items(categories, c):\n    assert len(categories) > 0\n    val = categories[0]\n    if isinstance(val, str):\n        return [c.pyapi.string_from_constant_string(item) for item in categories]\n\n    dtype = numba.typeof(val)\n    return [c.box(dtype, c.context.get_constant(dtype, item)) for item in categories]\n'"
sdc/hiframes/pd_dataframe_ext.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\nimport operator\nfrom typing import NamedTuple\n\nimport numba\nfrom numba import types\nfrom numba.core import cgutils\nfrom numba.extending import (models, register_model, lower_cast, infer_getattr,\n                             type_callable, infer, overload, intrinsic,\n                             lower_builtin, overload_method)\nfrom numba.core.typing.templates import (infer_global, AbstractTemplate, signature,\n                                    AttributeTemplate, bound_function)\nfrom numba.core.imputils import impl_ret_new_ref, impl_ret_borrowed\n\nfrom sdc.hiframes.pd_series_ext import SeriesType\nfrom sdc.hiframes.pd_dataframe_type import DataFrameType\nfrom sdc.str_ext import string_type\n\n\n@infer_getattr\nclass DataFrameAttribute(AttributeTemplate):\n    key = DataFrameType\n\n    def generic_resolve(self, df, attr):\n        if attr in df.columns:\n            ind = df.columns.index(attr)\n            arr_typ = df.data[ind]\n            return SeriesType(arr_typ.dtype, arr_typ, df.index, True)\n\n\nclass ColumnLoc(NamedTuple):\n    type_id: int\n    col_id: int\n\n\ndef get_structure_maps(col_types, col_names):\n    # Define map column name to column location ex. {\'A\': (0,0), \'B\': (1,0), \'C\': (0,1)}\n    column_loc = {}\n    # Store unique types of columns ex. {\'int64\': (0, [0, 2]), \'float64\': (1, [1])}\n    data_typs_map = {}\n    types_order = []\n    type_id = 0\n    for i, col_typ in enumerate(col_types):\n        col_name = col_names[i]\n\n        if col_typ not in data_typs_map:\n            data_typs_map[col_typ] = (type_id, [i])\n            # The first column in each type always has 0 index\n            column_loc[col_name] = ColumnLoc(type_id, 0)\n            types_order.append(col_typ)\n            type_id += 1\n        else:\n            # Get index of column in list of types\n            existing_type_id, col_indices = data_typs_map[col_typ]\n            col_id = len(col_indices)\n            column_loc[col_name] = ColumnLoc(existing_type_id, col_id)\n            col_indices.append(i)\n\n    return column_loc, data_typs_map, types_order\n\n\n@intrinsic\ndef init_dataframe(typingctx, *args):\n    """"""Create a DataFrame with provided data, index and columns values.\n    Used as a single constructor for DataFrame and assigning its data, so that\n    optimization passes can look for init_dataframe() to see if underlying\n    data has changed, and get the array variables from init_dataframe() args if\n    not changed.\n    """"""\n\n    n_cols = len(args) // 2\n    data_typs = tuple(args[:n_cols])\n    index_typ = args[n_cols]\n    column_names = tuple(a.literal_value for a in args[n_cols + 1:])\n\n    column_loc, data_typs_map, types_order = get_structure_maps(data_typs, column_names)\n\n    def codegen(context, builder, signature, args):\n        in_tup = args[0]\n        data_arrs = [builder.extract_value(in_tup, i) for i in range(n_cols)]\n        index = builder.extract_value(in_tup, n_cols)\n        column_strs = [numba.cpython.unicode.make_string_from_constant(\n            context, builder, string_type, c) for c in column_names]\n        # create dataframe struct and store values\n        dataframe = cgutils.create_struct_proxy(\n            signature.return_type)(context, builder)\n\n        data_list_type = [types.List(typ) for typ in types_order]\n\n        data_lists = []\n        for typ_id, typ in enumerate(types_order):\n            data_list_typ = context.build_list(builder, data_list_type[typ_id],\n                                               [data_arrs[data_id] for data_id in data_typs_map[typ][1]])\n            data_lists.append(data_list_typ)\n\n        data_tup = context.make_tuple(\n            builder, types.Tuple(data_list_type), data_lists)\n\n        col_list_type = types.List(string_type)\n        column_list = context.build_list(builder, col_list_type, column_strs)\n\n        dataframe.data = data_tup\n        dataframe.index = index\n        dataframe.columns = column_list\n        dataframe.parent = context.get_constant_null(types.pyobject)\n\n        # increase refcount of stored values\n        if context.enable_nrt:\n            context.nrt.incref(builder, index_typ, index)\n            for var, typ in zip(data_arrs, data_typs):\n                context.nrt.incref(builder, typ, var)\n            for var in column_strs:\n                context.nrt.incref(builder, string_type, var)\n\n        return dataframe._getvalue()\n\n    ret_typ = DataFrameType(data_typs, index_typ, column_names, column_loc=column_loc)\n    sig = signature(ret_typ, types.Tuple(args))\n    return sig, codegen\n\n\n# TODO: alias analysis\n# this function should be used for getting df._data for alias analysis to work\n# no_cpython_wrapper since Array(DatetimeDate) cannot be boxed\n@numba.njit(no_cpython_wrapper=True, inline=\'always\')\ndef get_dataframe_data(df, i):\n    return df._data[i]\n\n\n@overload(len)  # TODO: avoid lowering?\ndef df_len_overload(df):\n    if not isinstance(df, DataFrameType):\n        return\n\n    if len(df.columns) == 0:  # empty df\n        return lambda df: 0\n    return lambda df: len(df._data[0][0])\n\n\n# handle getitem for Tuples because sometimes df._data[i] in\n# get_dataframe_data() doesn\'t translate to \'static_getitem\' which causes\n# Numba to fail. See TestDataFrame.test_unbox1, TODO: find root cause in Numba\n# adapted from typing/builtins.py\n@infer_global(operator.getitem)\nclass GetItemTuple(AbstractTemplate):\n    key = operator.getitem\n\n    def generic(self, args, kws):\n        tup, idx = args\n        if (not isinstance(tup, types.BaseTuple) or not isinstance(idx, types.IntegerLiteral)):\n            return\n        idx_val = idx.literal_value\n        if isinstance(idx_val, int):\n            ret = tup.types[idx_val]\n        elif isinstance(idx_val, slice):\n            ret = types.BaseTuple.from_types(tup.types[idx_val])\n\n        return signature(ret, *args)\n\n\n# adapted from targets/tupleobj.py\n@lower_builtin(operator.getitem, types.BaseTuple, types.IntegerLiteral)\n@lower_builtin(operator.getitem, types.BaseTuple, types.SliceLiteral)\ndef getitem_tuple_lower(context, builder, sig, args):\n    tupty, idx = sig.args\n    idx = idx.literal_value\n    tup, _ = args\n    if isinstance(idx, int):\n        if idx < 0:\n            idx += len(tupty)\n        if not 0 <= idx < len(tupty):\n            raise IndexError(""cannot index at %d in %s"" % (idx, tupty))\n        res = builder.extract_value(tup, idx)\n    elif isinstance(idx, slice):\n        items = cgutils.unpack_tuple(builder, tup)[idx]\n        res = context.make_tuple(builder, sig.return_type, items)\n    else:\n        raise NotImplementedError(""unexpected index %r for %s"" % (idx, sig.args[0]))\n    return impl_ret_borrowed(context, builder, sig.return_type, res)\n'"
sdc/hiframes/pd_dataframe_type.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\nimport numba\nfrom numba import types\nfrom numba.core import cgutils\nfrom numba.extending import (models, register_model, make_attribute_wrapper)\n\nfrom sdc.str_ext import string_type\n\n\nclass DataFrameType(types.Type):  # TODO: IterableType over column names\n    """"""Temporary type class for DataFrame objects.\n    """"""\n\n    def __init__(self, data=None, index=None, columns=None, has_parent=False, column_loc=None):\n        self.data = data\n        if index is None:\n            index = types.none\n        self.index = index\n        self.columns = columns\n        # keeping whether it is unboxed from Python to enable reflection of new\n        # columns\n        self.has_parent = has_parent\n        self.column_loc = column_loc\n        super(DataFrameType, self).__init__(\n            name=""dataframe({}, {}, {}, {})"".format(data, index, columns, has_parent))\n\n    def copy(self, index=None, has_parent=None):\n        # XXX is copy necessary?\n        if index is None:\n            index = types.none if self.index == types.none else self.index.copy()\n        data = tuple(a.copy() for a in self.data)\n        if has_parent is None:\n            has_parent = self.has_parent\n        return DataFrameType(data, index, self.columns, has_parent)\n\n    @property\n    def key(self):\n        # needed?\n        return self.data, self.index, self.columns, self.has_parent\n\n    def unify(self, typingctx, other):\n        if (isinstance(other, DataFrameType)\n                and len(other.data) == len(self.data)\n                and other.columns == self.columns\n                and other.has_parent == self.has_parent):\n            new_index = types.none\n            if self.index != types.none and other.index != types.none:\n                new_index = self.index.unify(typingctx, other.index)\n            elif other.index != types.none:\n                new_index = other.index\n            elif self.index != types.none:\n                new_index = self.index\n\n            data = tuple(a.unify(typingctx, b) for a, b in zip(self.data, other.data))\n            return DataFrameType(data, new_index, self.columns, self.has_parent)\n\n    def is_precise(self):\n        return all(a.is_precise() for a in self.data) and self.index.is_precise()\n\n\n@register_model(DataFrameType)\nclass DataFrameModel(models.StructModel):\n    def __init__(self, dmm, fe_type):\n        types_unique = set()\n        df_types = []\n        for col_type in fe_type.data:\n            if col_type in types_unique:\n                continue\n            types_unique.add(col_type)\n            df_types.append(col_type)\n\n        members = [\n            (\'data\', types.Tuple([types.List(typ) for typ in df_types])),\n            (\'index\', fe_type.index),\n            (\'columns\', types.List(string_type)),\n            (\'parent\', types.pyobject),\n        ]\n        super(DataFrameModel, self).__init__(dmm, fe_type, members)\n\n\nmake_attribute_wrapper(DataFrameType, \'data\', \'_data\')\nmake_attribute_wrapper(DataFrameType, \'index\', \'_index\')\nmake_attribute_wrapper(DataFrameType, \'columns\', \'_columns\')\nmake_attribute_wrapper(DataFrameType, \'unboxed\', \'_unboxed\')\nmake_attribute_wrapper(DataFrameType, \'parent\', \'_parent\')\n'"
sdc/hiframes/pd_series_ext.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\n\nimport numba\nfrom numba import types\nfrom numba.core import cgutils\nfrom numba.extending import (\n    models,\n    register_model,\n    lower_cast,\n    lower_builtin,\n    infer_getattr,\n    type_callable,\n    infer,\n    overload)\n\nimport sdc\nfrom sdc.hiframes.pd_categorical_ext import PDCategoricalDtype\n\nfrom sdc.str_arr_ext import string_array_type\nfrom sdc.str_ext import string_type, list_string_array_type\n\nfrom sdc.hiframes.pd_series_type import SeriesType\nfrom sdc.datatypes.categorical.pdimpl import is_categoricaldtype\nfrom sdc.datatypes.series.pdimpl import _Series_category\n\n\ndef is_str_series_typ(t):\n    return isinstance(t, SeriesType) and t.dtype == string_type\n\n\ndef is_dt64_series_typ(t):\n    return isinstance(t, SeriesType) and t.dtype == types.NPDatetime(\'ns\')\n\n\ndef series_to_array_type(typ, replace_boxed=False):\n    return typ.data\n\n\ndef arr_to_series_type(arr):\n    series_type = None\n    if isinstance(arr, types.Array):\n        series_type = SeriesType(arr.dtype, arr)\n    elif arr == string_array_type:\n        # StringArray is readonly\n        series_type = SeriesType(string_type)\n    elif arr == list_string_array_type:\n        series_type = SeriesType(types.List(string_type))\n    return series_type\n\n\ndef if_series_to_array_type(typ, replace_boxed=False):\n    if isinstance(typ, SeriesType):\n        return series_to_array_type(typ, replace_boxed)\n\n    if isinstance(typ, (types.Tuple, types.UniTuple)):\n        return types.Tuple(\n            [if_series_to_array_type(t, replace_boxed) for t in typ.types])\n    if isinstance(typ, types.List):\n        return types.List(if_series_to_array_type(typ.dtype, replace_boxed))\n    if isinstance(typ, types.Set):\n        return types.Set(if_series_to_array_type(typ.dtype, replace_boxed))\n    # TODO: other types that can have Series inside?\n    return typ\n\n\ndef if_arr_to_series_type(typ):\n    if isinstance(typ, (types.Tuple, types.UniTuple)):\n        return types.Tuple([if_arr_to_series_type(t) for t in typ.types])\n    if isinstance(typ, types.List):\n        return types.List(if_arr_to_series_type(typ.dtype))\n    if isinstance(typ, types.Set):\n        return types.Set(if_arr_to_series_type(typ.dtype))\n    # TODO: other types that can have Arrays inside?\n    return typ\n\n\n@overload(pd.Series)\ndef pd_series_overload(data=None, index=None, dtype=None, name=None, copy=False, fastpath=False):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n    Pandas API: pandas.Series\n\n    Limitations\n    -----------\n    - Parameters ``dtype`` and ``copy`` are currently unsupported.\n    - Types iterable and dict as ``data`` parameter are currently unsupported.\n    - Categorical types (i.e. \'category\' and ``CategoricalDtype``) are supported in ``dtype``\n    only if they are provided as constants in jitted code.\n\n    Examples\n    --------\n    Create Series with data [1, 2, 3] and index [\'A\', \'B\', \'C\'].\n\n    >>> pd.Series([1, 2, 3], [\'A\', \'B\', \'C\'])\n\n    Create Series with categorical data:\n\n    >>> pd.Series([1, 2, 3], dtype=\'category\')\n    >>> pd.Series([1, 2, 3], dtype=CategoricalDtype([1, 2, 3]))\n\n    .. seealso::\n\n        :ref:`DataFrame <pandas.DataFrame>`\n            DataFrame constructor.\n    """"""\n\n    is_index_none = isinstance(index, types.NoneType) or index is None\n\n    if is_categoricaldtype(dtype):\n        return _Series_category(data, index, dtype, name, copy, fastpath)\n\n    def hpat_pandas_series_ctor_impl(data=None, index=None, dtype=None, name=None, copy=False, fastpath=False):\n\n        \'\'\'\' use binop here as otherwise Numba\'s dead branch pruning doesn\'t work\n        TODO: replace with \'if not is_index_none\' when resolved \'\'\'\n        if is_index_none == False:  # noqa\n            fix_index = sdc.hiframes.api.fix_df_array(index)\n        else:\n            fix_index = index\n\n        return sdc.hiframes.api.init_series(sdc.hiframes.api.fix_df_array(data), fix_index, name)\n\n    return hpat_pandas_series_ctor_impl\n'"
sdc/hiframes/pd_series_type.py,4,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport llvmlite.llvmpy.core as lc\n\nimport numpy as np\n\nfrom numba import types\nfrom numba.core import cgutils\nfrom numba.np.numpy_support import from_dtype\nfrom numba.extending import (models, register_model, make_attribute_wrapper, lower_builtin)\nfrom numba.core.imputils import (impl_ret_new_ref, iternext_impl, RefType)\nfrom numba.np.arrayobj import make_array, _getitem_array_single_int\n\nfrom sdc.str_ext import string_type, list_string_array_type\nfrom sdc.str_arr_ext import (string_array_type, iternext_str_array, StringArrayType)\nfrom sdc.datatypes.categorical.types import CategoricalDtypeType, Categorical\n\n\nclass SeriesType(types.IterableType):\n    """"""Temporary type class for Series objects.\n    """"""\n\n    def __init__(self, dtype, data=None, index=None, is_named=False):\n        # keeping data array in type since operators can make changes such\n        # as making array unaligned etc.\n        data = _get_series_array_type(dtype) if data is None else data\n        # convert Record to tuple (for tuple output of map)\n        # TODO: handle actual Record objects in Series?\n        self.dtype = (types.Tuple(list(dict(dtype.members).values()))\n                      if isinstance(dtype, types.Record) else dtype)\n        self.data = data\n        if index is None:\n            index = types.none\n        self.index = index\n        # keep is_named in type to enable boxing\n        self.is_named = is_named\n        super(SeriesType, self).__init__(\n            name=""series({}, {}, {}, {})"".format(dtype, data, index, is_named))\n\n    def copy(self, dtype=None):\n        # XXX is copy necessary?\n        index = types.none if self.index == types.none else self.index.copy()\n        dtype = dtype if dtype is not None else self.dtype\n        data = _get_series_array_type(dtype)\n        return SeriesType(dtype, data, index)\n\n    @property\n    def key(self):\n        # needed?\n        return self.dtype, self.data, self.index, self.is_named\n\n    @property\n    def ndim(self):\n        return self.data.ndim\n\n    def unify(self, typingctx, other):\n        if isinstance(other, SeriesType):\n            new_index = types.none\n            if self.index != types.none and other.index != types.none:\n                new_index = self.index.unify(typingctx, other.index)\n            elif other.index != types.none:\n                new_index = other.index\n            elif self.index != types.none:\n                new_index = self.index\n\n            # If dtype matches or other.dtype is undefined (inferred)\n            if other.dtype == self.dtype or not other.dtype.is_precise():\n                return SeriesType(\n                    self.dtype,\n                    self.data.unify(typingctx, other.data),\n                    new_index)\n\n        # XXX: unify Series/Array as Array\n        return super(SeriesType, self).unify(typingctx, other)\n\n    def can_convert_to(self, typingctx, other):\n        # same as types.Array\n        if (isinstance(other, SeriesType) and other.dtype == self.dtype):\n            # called for overload selection sometimes\n            # TODO: fix index\n            if self.index == types.none and other.index == types.none:\n                return self.data.can_convert_to(typingctx, other.data)\n            if self.index != types.none and other.index != types.none:\n                return max(self.data.can_convert_to(typingctx, other.data),\n                           self.index.can_convert_to(typingctx, other.index))\n\n    def is_precise(self):\n        # same as types.Array\n        return self.dtype.is_precise()\n\n    @property\n    def iterator_type(self):\n        # TODO: fix timestamp\n        return SeriesIterator(self)\n\n\n# register_model(SeriesType)(models.ArrayModel)\n# need to define model since fix_df_array overload goes to native code\n@register_model(SeriesType)\nclass SeriesModel(models.StructModel):\n    def __init__(self, dmm, fe_type):\n        name_typ = string_type if fe_type.is_named else types.none\n        members = [\n            (\'data\', fe_type.data),\n            (\'index\', fe_type.index),\n            (\'name\', name_typ),\n        ]\n        super(SeriesModel, self).__init__(dmm, fe_type, members)\n\n\nmake_attribute_wrapper(SeriesType, \'data\', \'_data\')\nmake_attribute_wrapper(SeriesType, \'index\', \'_index\')\nmake_attribute_wrapper(SeriesType, \'name\', \'_name\')\n\n\nclass SeriesIterator(types.SimpleIteratorType):\n    """"""\n    Type class for iterator over dataframe series.\n    """"""\n\n    def __init__(self, series_type):\n        self.series_type = series_type\n        self.array_type = series_type.data\n\n        name = f\'iter({self.series_type.data})\'\n        yield_type = series_type.dtype\n        super(SeriesIterator, self).__init__(name, yield_type)\n\n    @property\n    def _iternext(self):\n        if isinstance(self.array_type, StringArrayType):\n            return iternext_str_array\n        elif isinstance(self.array_type, types.Array):\n            return iternext_series_array\n\n\n@register_model(SeriesIterator)\nclass SeriesIteratorModel(models.StructModel):\n    def __init__(self, dmm, fe_type):\n        members = [(\'index\', types.EphemeralPointer(types.uintp)),\n                   (\'array\', fe_type.series_type.data)]\n\n        models.StructModel.__init__(self, dmm, fe_type, members)\n\n\n@lower_builtin(\'getiter\', SeriesType)\ndef getiter_series(context, builder, sig, args):\n    """"""\n    Getting iterator for the Series type\n\n    :param context: context descriptor\n    :param builder: llvmlite IR Builder\n    :param sig: iterator signature\n    :param args: tuple with iterator arguments, such as instruction, operands and types\n    :param result: iternext result\n    :return: reference to iterator\n    """"""\n\n    arraytype = sig.args[0].data\n\n    # Create instruction to get array to iterate\n    zero_member_pointer = context.get_constant(types.intp, 0)\n    zero_member = context.get_constant(types.int32, 0)\n    alloca = args[0].operands[0]\n    gep_result = builder.gep(alloca, [zero_member_pointer, zero_member])\n    array = builder.load(gep_result)\n\n    # TODO: call numba getiter with gep_result for array\n    iterobj = context.make_helper(builder, sig.return_type)\n    zero_index = context.get_constant(types.intp, 0)\n    indexptr = cgutils.alloca_once_value(builder, zero_index)\n\n    iterobj.index = indexptr\n    iterobj.array = array\n\n    if context.enable_nrt:\n        context.nrt.incref(builder, arraytype, array)\n\n    result = iterobj._getvalue()\n    # Note: a decref on the iterator will dereference all internal MemInfo*\n    out = impl_ret_new_ref(context, builder, sig.return_type, result)\n    return out\n\n\n# TODO: call it from numba.np.arrayobj, need separate function in numba\ndef iternext_series_array(context, builder, sig, args, result):\n    """"""\n    Implementation of iternext() for the ArrayIterator type\n\n    :param context: context descriptor\n    :param builder: llvmlite IR Builder\n    :param sig: iterator signature\n    :param args: tuple with iterator arguments, such as instruction, operands and types\n    :param result: iternext result\n    """"""\n\n    [iterty] = sig.args\n    [iter] = args\n    arrayty = iterty.array_type\n\n    if arrayty.ndim != 1:\n        raise NotImplementedError(""iterating over %dD array"" % arrayty.ndim)\n\n    iterobj = context.make_helper(builder, iterty, value=iter)\n    ary = make_array(arrayty)(context, builder, value=iterobj.array)\n\n    nitems, = cgutils.unpack_tuple(builder, ary.shape, count=1)\n\n    index = builder.load(iterobj.index)\n    is_valid = builder.icmp(lc.ICMP_SLT, index, nitems)\n    result.set_valid(is_valid)\n\n    with builder.if_then(is_valid):\n        value = _getitem_array_single_int(\n            context, builder, iterty.yield_type, arrayty, ary, index\n        )\n        result.yield_(value)\n        nindex = cgutils.increment_index(builder, index)\n        builder.store(nindex, iterobj.index)\n\n\n@lower_builtin(\'iternext\', SeriesIterator)\n@iternext_impl(RefType.BORROWED)\ndef iternext_series(context, builder, sig, args, result):\n    """"""\n    Iternext implementation depending on Array type\n\n    :param context: context descriptor\n    :param builder: llvmlite IR Builder\n    :param sig: iterator signature\n    :param args: tuple with iterator arguments, such as instruction, operands and types\n    :param result: iternext result\n    """"""\n    iternext_func = sig.args[0]._iternext\n    iternext_func(context=context, builder=builder, sig=sig, args=args, result=result)\n\n\ndef _get_series_array_type(dtype):\n    """"""get underlying default array type of series based on its dtype\n    """"""\n    # list(list(str))\n    if dtype == types.List(string_type):\n        # default data layout is list but split view is used if possible\n        return list_string_array_type\n    # string array\n    elif dtype == string_type:\n        return string_array_type\n\n    # categorical\n    if isinstance(dtype, CategoricalDtypeType):\n        # TODO: pass codes array if exists\n        return Categorical(dtype)\n\n    # use recarray data layout for series of tuples\n    if isinstance(dtype, types.BaseTuple):\n        if any(not isinstance(t, types.Number) for t in dtype.types):\n            # TODO: support more types. what types can be in recarrays?\n            raise ValueError(""series tuple dtype {} includes non-numerics"".format(dtype))\n        np_dtype = np.dtype(\n            \',\'.join(str(t) for t in dtype.types), align=True)\n        dtype = from_dtype(np_dtype)\n\n    # TODO: other types?\n    # regular numpy array\n    return types.Array(dtype, 1, \'C\')\n'"
sdc/hiframes/pd_timestamp_ext.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\nfrom numba import types\nfrom numba.extending import (typeof_impl, type_callable, models, register_model, NativeValue,\n                             make_attribute_wrapper, lower_builtin, box, unbox, lower_cast,\n                             lower_getattr, infer_getattr, overload_method, overload, intrinsic)\nfrom numba.core import cgutils\n\nfrom llvmlite import ir as lir\n\nimport pandas as pd\n\nimport datetime\nfrom .. import hdatetime_ext\nimport llvmlite.binding as ll\nll.add_symbol(\'parse_iso_8601_datetime\', hdatetime_ext.parse_iso_8601_datetime)\nll.add_symbol(\'convert_datetimestruct_to_datetime\', hdatetime_ext.convert_datetimestruct_to_datetime)\nll.add_symbol(\'np_datetime_date_array_from_packed_ints\', hdatetime_ext.np_datetime_date_array_from_packed_ints)\n\n\n# ---------------------------------------------------------------\n\n# datetime.date implementation that uses a single int to store year/month/day\n\n\nclass DatetimeDateType(types.Type):\n    def __init__(self):\n        super(DatetimeDateType, self).__init__(\n            name=\'DatetimeDateType()\')\n        self.bitwidth = 64\n\n\ndatetime_date_type = DatetimeDateType()\n\n\nregister_model(DatetimeDateType)(models.IntegerModel)\n\n\n@box(DatetimeDateType)\ndef box_datetime_date(typ, val, c):\n    year_obj = c.pyapi.long_from_longlong(c.builder.lshr(val, lir.Constant(lir.IntType(64), 32)))\n    month_obj = c.pyapi.long_from_longlong(\n        c.builder.and_(\n            c.builder.lshr(\n                val, lir.Constant(\n                    lir.IntType(64), 16)), lir.Constant(\n                lir.IntType(64), 0xFFFF)))\n    day_obj = c.pyapi.long_from_longlong(c.builder.and_(val, lir.Constant(lir.IntType(64), 0xFFFF)))\n\n    dt_obj = c.pyapi.unserialize(c.pyapi.serialize_object(datetime.date))\n    res = c.pyapi.call_function_objargs(dt_obj, (year_obj, month_obj, day_obj))\n    c.pyapi.decref(year_obj)\n    c.pyapi.decref(month_obj)\n    c.pyapi.decref(day_obj)\n    return res\n\n\n@type_callable(datetime.date)\ndef type_timestamp(context):\n    def typer(year, month, day):\n        # TODO: check types\n        return datetime_date_type\n    return typer\n\n\n@lower_builtin(datetime.date, types.int64, types.int64, types.int64)\ndef impl_ctor_timestamp(context, builder, sig, args):\n    typ = sig.return_type\n    year, month, day = args\n    nopython_date = builder.add(day,\n                                builder.add(builder.shl(year, lir.Constant(lir.IntType(64), 32)),\n                                            builder.shl(month, lir.Constant(lir.IntType(64), 16))))\n    return nopython_date\n\n# ------------------------------------------------------------------------\n\nclass PandasTimestampType(types.Type):\n    def __init__(self):\n        super(PandasTimestampType, self).__init__(\n            name=\'PandasTimestampType()\')\n\n\npandas_timestamp_type = PandasTimestampType()\n\n\n@typeof_impl.register(pd.Timestamp)\ndef typeof_pd_timestamp(val, c):\n    return pandas_timestamp_type\n\n\nts_field_typ = types.int64\n\n\n@register_model(PandasTimestampType)\nclass PandasTimestampModel(models.StructModel):\n    def __init__(self, dmm, fe_type):\n        members = [\n            (\'year\', ts_field_typ),\n            (\'month\', ts_field_typ),\n            (\'day\', ts_field_typ),\n            (\'hour\', ts_field_typ),\n            (\'minute\', ts_field_typ),\n            (\'second\', ts_field_typ),\n            (\'microsecond\', ts_field_typ),\n            (\'nanosecond\', ts_field_typ),\n        ]\n        models.StructModel.__init__(self, dmm, fe_type, members)\n\n\nmake_attribute_wrapper(PandasTimestampType, \'year\', \'year\')\nmake_attribute_wrapper(PandasTimestampType, \'month\', \'month\')\nmake_attribute_wrapper(PandasTimestampType, \'day\', \'day\')\nmake_attribute_wrapper(PandasTimestampType, \'hour\', \'hour\')\nmake_attribute_wrapper(PandasTimestampType, \'minute\', \'minute\')\nmake_attribute_wrapper(PandasTimestampType, \'second\', \'second\')\nmake_attribute_wrapper(PandasTimestampType, \'microsecond\', \'microsecond\')\nmake_attribute_wrapper(PandasTimestampType, \'nanosecond\', \'nanosecond\')\n\n\n@overload_method(PandasTimestampType, \'date\')\ndef overload_pd_timestamp_date(ptt):\n    def pd_timestamp_date_impl(ptt):\n        return datetime.date(ptt.year, ptt.month, ptt.day)\n    return pd_timestamp_date_impl\n\n\n@unbox(PandasTimestampType)\ndef unbox_pandas_timestamp(typ, val, c):\n    year_obj = c.pyapi.object_getattr_string(val, ""year"")\n    month_obj = c.pyapi.object_getattr_string(val, ""month"")\n    day_obj = c.pyapi.object_getattr_string(val, ""day"")\n    hour_obj = c.pyapi.object_getattr_string(val, ""hour"")\n    minute_obj = c.pyapi.object_getattr_string(val, ""minute"")\n    second_obj = c.pyapi.object_getattr_string(val, ""second"")\n    microsecond_obj = c.pyapi.object_getattr_string(val, ""microsecond"")\n    nanosecond_obj = c.pyapi.object_getattr_string(val, ""nanosecond"")\n\n    pd_timestamp = cgutils.create_struct_proxy(typ)(c.context, c.builder)\n    pd_timestamp.year = c.pyapi.long_as_longlong(year_obj)\n    pd_timestamp.month = c.pyapi.long_as_longlong(month_obj)\n    pd_timestamp.day = c.pyapi.long_as_longlong(day_obj)\n    pd_timestamp.hour = c.pyapi.long_as_longlong(hour_obj)\n    pd_timestamp.minute = c.pyapi.long_as_longlong(minute_obj)\n    pd_timestamp.second = c.pyapi.long_as_longlong(second_obj)\n    pd_timestamp.microsecond = c.pyapi.long_as_longlong(microsecond_obj)\n    pd_timestamp.nanosecond = c.pyapi.long_as_longlong(nanosecond_obj)\n\n    c.pyapi.decref(year_obj)\n    c.pyapi.decref(month_obj)\n    c.pyapi.decref(day_obj)\n    c.pyapi.decref(hour_obj)\n    c.pyapi.decref(minute_obj)\n    c.pyapi.decref(second_obj)\n    c.pyapi.decref(microsecond_obj)\n    c.pyapi.decref(nanosecond_obj)\n\n    is_error = cgutils.is_not_null(c.builder, c.pyapi.err_occurred())\n    return NativeValue(pd_timestamp._getvalue(), is_error=is_error)\n\n\n@type_callable(pd.Timestamp)\ndef type_timestamp(context):\n    def typer(datetime_type):\n        # TODO: check types\n        return pandas_timestamp_type\n    return typer\n\n\n@type_callable(datetime.datetime)\ndef type_timestamp(context):\n    def typer(year, month, day):  # how to handle optional hour, minute, second, us, ns?\n        # TODO: check types\n        return pandas_timestamp_type\n    return typer\n\n\n@lower_builtin(pd.Timestamp, pandas_timestamp_type)\ndef impl_ctor_ts_ts(context, builder, sig, args):\n    typ = sig.return_type\n    rhs = args[0]\n    ts = cgutils.create_struct_proxy(typ)(context, builder)\n    rhsproxy = cgutils.create_struct_proxy(typ)(context, builder)\n    rhsproxy._setvalue(rhs)\n    cgutils.copy_struct(ts, rhsproxy)\n    return ts._getvalue()\n\n#              , types.int64, types.int64, types.int64, types.int64, types.int64)\n@lower_builtin(datetime.datetime, types.int64, types.int64, types.int64)\n@lower_builtin(datetime.datetime, types.IntegerLiteral, types.IntegerLiteral, types.IntegerLiteral)\ndef impl_ctor_datetime(context, builder, sig, args):\n    typ = sig.return_type\n    year, month, day = args\n    ts = cgutils.create_struct_proxy(typ)(context, builder)\n    ts.year = year\n    ts.month = month\n    ts.day = day\n    ts.hour = lir.Constant(lir.IntType(64), 0)\n    ts.minute = lir.Constant(lir.IntType(64), 0)\n    ts.second = lir.Constant(lir.IntType(64), 0)\n    ts.microsecond = lir.Constant(lir.IntType(64), 0)\n    ts.nanosecond = lir.Constant(lir.IntType(64), 0)\n    return ts._getvalue()\n'"
sdc/hiframes/rolling.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\nimport pandas as pd\nimport sdc\nimport numba\nfrom numba.core.ir_utils import guard, find_const\n\n\nsupported_rolling_funcs = (\'sum\', \'mean\', \'var\', \'std\', \'count\', \'median\',\n                           \'min\', \'max\', \'cov\', \'corr\', \'apply\')\n\n\ndef get_rolling_setup_args(func_ir, rhs, get_consts=True):\n    """"""\n    Handle Series rolling calls like:\n        r = df.column.rolling(3)\n    """"""\n    center = False\n    on = None\n    kws = dict(rhs.kws)\n    if rhs.args:\n        window = rhs.args[0]\n    elif \'window\' in kws:\n        window = kws[\'window\']\n    else:  # pragma: no cover\n        raise ValueError(""window argument to rolling() required"")\n    if get_consts:\n        window_const = guard(find_const, func_ir, window)\n        window = window_const if window_const is not None else window\n    if \'center\' in kws:\n        center = kws[\'center\']\n        if get_consts:\n            center_const = guard(find_const, func_ir, center)\n            center = center_const if center_const is not None else center\n    if \'on\' in kws:\n        on = guard(find_const, func_ir, kws[\'on\'])\n        if on is None:\n            raise ValueError(""\'on\' argument to rolling() should be constant string"")\n    # convert string offset window statically to nanos\n    # TODO: support dynamic conversion\n    # TODO: support other offsets types (time delta, etc.)\n    if on is not None:\n        window = guard(find_const, func_ir, window)\n        if not isinstance(window, str):\n            raise ValueError(""window argument to rolling should be constant""\n                             ""string in the offset case (variable window)"")\n        window = pd.tseries.frequencies.to_offset(window).nanos\n    return window, center, on\n'"
sdc/hiframes/sort.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\nimport numba\nimport sdc\nimport sdc.timsort\n\nfrom sdc.str_arr_ext import (to_string_list, cp_str_list_to_array)\n\n\n# TODO: fix cache issue\n@numba.njit(no_cpython_wrapper=True, cache=False)\ndef local_sort(key_arrs, data, ascending=True):\n    # convert StringArray to list(string) to enable swapping in sort\n    l_key_arrs = to_string_list(key_arrs)\n    l_data = to_string_list(data)\n    n_out = len(key_arrs[0])\n    sdc.timsort.sort(l_key_arrs, 0, n_out, l_data)\n    if not ascending:\n        sdc.timsort.reverseRange(l_key_arrs, 0, n_out, l_data)\n    cp_str_list_to_array(key_arrs, l_key_arrs)\n    cp_str_list_to_array(data, l_data)\n'"
sdc/io/__init__.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n'"
sdc/io/csv_ext.py,4,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\nimport contextlib\nimport functools\n\nimport llvmlite.binding as ll\nfrom llvmlite import ir as lir\nfrom .. import hio\nfrom collections import defaultdict\nimport numba\nfrom numba.core import typeinfer, ir, ir_utils, types\nfrom numba.core.typing.templates import signature\nfrom numba.extending import overload, intrinsic, register_model, models, box\nfrom numba.core.ir_utils import (visit_vars_inner, replace_vars_inner,\n                            compile_to_numba_ir, replace_arg_nodes)\nfrom numba.core import analysis\nfrom numba.parfors import array_analysis\nimport sdc\nfrom sdc import distributed, distributed_analysis\nfrom sdc.utilities.utils import (debug_prints, alloc_arr_tup, empty_like_type,\n                                 _numba_to_c_type_map)\nfrom sdc.distributed_analysis import Distribution\nfrom sdc.str_ext import string_type\nfrom sdc.str_arr_ext import (string_array_type, to_string_list,\n                              cp_str_list_to_array, str_list_to_array,\n                              get_offset_ptr, get_data_ptr, convert_len_arr_to_offset,\n                              pre_alloc_string_array, num_total_chars,\n                              getitem_str_offset, copy_str_arr_slice)\nfrom sdc.timsort import copyElement_tup, getitem_arr_tup\nfrom sdc import objmode\nimport pandas as pd\nimport numpy as np\n\nfrom sdc.types import CategoricalDtypeType, Categorical\n\nimport pyarrow\nimport pyarrow.csv\n\n\nclass CsvReader(ir.Stmt):\n    def __init__(self, file_name, df_out, sep, df_colnames, out_vars, out_types, usecols, loc, skiprows=0):\n        self.file_name = file_name\n        self.df_out = df_out\n        self.sep = sep\n        self.df_colnames = df_colnames\n        self.out_vars = out_vars\n        self.out_types = out_types\n        self.usecols = usecols\n        self.loc = loc\n        self.skiprows = skiprows\n\n    def __repr__(self):  # pragma: no cover\n        # TODO\n        return ""{} = ReadCsv()"".format(self.df_out)\n\n\ndef csv_array_analysis(csv_node, equiv_set, typemap, array_analysis):\n    post = []\n    # empty csv nodes should be deleted in remove dead\n    assert len(csv_node.out_vars) > 0, ""empty csv in array analysis""\n\n    # create correlations for output arrays\n    # arrays of output df have same size in first dimension\n    # gen size variable for an output column\n    all_shapes = []\n    for col_var in csv_node.out_vars:\n        typ = typemap[col_var.name]\n        # TODO: string_series_type also?\n        if typ == string_array_type:\n            continue\n        (shape, c_post) = array_analysis._gen_shape_call(\n            equiv_set, col_var, typ.ndim, None)\n        equiv_set.insert_equiv(col_var, shape)\n        post.extend(c_post)\n        all_shapes.append(shape[0])\n        equiv_set.define(col_var, {})\n\n    if len(all_shapes) > 1:\n        equiv_set.insert_equiv(*all_shapes)\n\n    return [], post\n\n\narray_analysis.array_analysis_extensions[CsvReader] = csv_array_analysis\n\n\ndef csv_distributed_analysis(csv_node, array_dists):\n    for v in csv_node.out_vars:\n        if v.name not in array_dists:\n            array_dists[v.name] = Distribution.OneD\n\n    return\n\n\ndistributed_analysis.distributed_analysis_extensions[CsvReader] = csv_distributed_analysis\n\n\ndef csv_typeinfer(csv_node, typeinferer):\n    for col_var, typ in zip(csv_node.out_vars, csv_node.out_types):\n        typeinferer.lock_type(col_var.name, typ, loc=csv_node.loc)\n    return\n\n\ntypeinfer.typeinfer_extensions[CsvReader] = csv_typeinfer\n\n\ndef visit_vars_csv(csv_node, callback, cbdata):\n    if debug_prints():  # pragma: no cover\n        print(""visiting csv vars for:"", csv_node)\n        print(""cbdata: "", sorted(cbdata.items()))\n\n    # update output_vars\n    new_out_vars = []\n    for col_var in csv_node.out_vars:\n        new_var = visit_vars_inner(col_var, callback, cbdata)\n        new_out_vars.append(new_var)\n\n    csv_node.out_vars = new_out_vars\n    csv_node.file_name = visit_vars_inner(csv_node.file_name, callback, cbdata)\n    return\n\n\n# add call to visit csv variable\nir_utils.visit_vars_extensions[CsvReader] = visit_vars_csv\n\n\ndef remove_dead_csv(csv_node, lives, arg_aliases, alias_map, func_ir, typemap):\n    # TODO\n    new_df_colnames = []\n    new_out_vars = []\n    new_out_types = []\n    new_usecols = []\n\n    for i, col_var in enumerate(csv_node.out_vars):\n        if col_var.name in lives:\n            new_df_colnames.append(csv_node.df_colnames[i])\n            new_out_vars.append(csv_node.out_vars[i])\n            new_out_types.append(csv_node.out_types[i])\n            new_usecols.append(csv_node.usecols[i])\n\n    csv_node.df_colnames = new_df_colnames\n    csv_node.out_vars = new_out_vars\n    csv_node.out_types = new_out_types\n    csv_node.usecols = new_usecols\n\n    if len(csv_node.out_vars) == 0:\n        return None\n\n    return csv_node\n\n\nir_utils.remove_dead_extensions[CsvReader] = remove_dead_csv\n\n\ndef csv_usedefs(csv_node, use_set=None, def_set=None):\n    if use_set is None:\n        use_set = set()\n    if def_set is None:\n        def_set = set()\n\n    # output columns are defined\n    def_set.update({v.name for v in csv_node.out_vars})\n    use_set.add(csv_node.file_name.name)\n\n    return analysis._use_defs_result(usemap=use_set, defmap=def_set)\n\n\nanalysis.ir_extension_usedefs[CsvReader] = csv_usedefs\n\n\ndef get_copies_csv(csv_node, typemap):\n    # csv doesn\'t generate copies, it just kills the output columns\n    kill_set = set(v.name for v in csv_node.out_vars)\n    return set(), kill_set\n\n\nir_utils.copy_propagate_extensions[CsvReader] = get_copies_csv\n\n\ndef apply_copies_csv(csv_node, var_dict, name_var_table, typemap, calltypes, save_copies):\n    """"""apply copy propagate in csv node""""""\n\n    # update output_vars\n    new_out_vars = []\n    for col_var in csv_node.out_vars:\n        new_var = replace_vars_inner(col_var, var_dict)\n        new_out_vars.append(new_var)\n\n    csv_node.out_vars = new_out_vars\n    csv_node.file_name = replace_vars_inner(csv_node.file_name, var_dict)\n    return\n\n\nir_utils.apply_copy_propagate_extensions[CsvReader] = apply_copies_csv\n\n\ndef build_csv_definitions(csv_node, definitions=None):\n    if definitions is None:\n        definitions = defaultdict(list)\n\n    for col_var in csv_node.out_vars:\n        definitions[col_var.name].append(csv_node)\n\n    return definitions\n\n\nir_utils.build_defs_extensions[CsvReader] = build_csv_definitions\n\n\ndef csv_distributed_run(csv_node, array_dists, typemap, calltypes, typingctx, targetctx, dist_pass):\n    parallel = False\n\n    n_cols = len(csv_node.out_vars)\n    # TODO: rebalance if output distributions are 1D instead of 1D_Var\n    # get column variables\n    arg_names = "", "".join(""arr"" + str(i) for i in range(n_cols))\n    func_text = ""def csv_impl(fname):\\n""\n    func_text += ""    ({},) = _csv_reader_py(fname)\\n"".format(arg_names)\n    # print(func_text)\n\n    loc_vars = {}\n    exec(func_text, {}, loc_vars)\n    csv_impl = loc_vars[\'csv_impl\']\n\n    csv_reader_py = _gen_csv_reader_py(\n        csv_node.df_colnames, csv_node.out_types, csv_node.usecols,\n        csv_node.sep, typingctx, targetctx, parallel, csv_node.skiprows)\n\n    f_block = compile_to_numba_ir(csv_impl,\n                                  {\'_csv_reader_py\': csv_reader_py},\n                                  typingctx, (string_type,),\n                                  typemap, calltypes).blocks.popitem()[1]\n    replace_arg_nodes(f_block, [csv_node.file_name])\n    nodes = f_block.body[:-3]\n    for i in range(len(csv_node.out_vars)):\n        nodes[-len(csv_node.out_vars) + i].target = csv_node.out_vars[i]\n\n    # get global array sizes by calling allreduce on chunk lens\n    # TODO: get global size from C\n    for arr in csv_node.out_vars:\n        def f(A):\n            return sdc.distributed_api.dist_reduce(len(A), np.int32(_op))\n        f_block = compile_to_numba_ir(\n            f, {\'sdc\': sdc, \'np\': np,\n                \'_op\': sdc.distributed_api.Reduce_Type.Sum.value},\n            typingctx, (typemap[arr.name],), typemap, calltypes).blocks.popitem()[1]\n        replace_arg_nodes(f_block, [arr])\n        nodes += f_block.body[:-2]\n        size_var = nodes[-1].target\n        dist_pass._array_sizes[arr.name] = [size_var]\n        out, start_var, end_var = dist_pass._gen_1D_div(\n            size_var, arr.scope, csv_node.loc, ""$alloc"", ""get_node_portion"",\n            sdc.distributed_api.get_node_portion)\n        dist_pass._array_starts[arr.name] = [start_var]\n        dist_pass._array_counts[arr.name] = [end_var]\n        nodes += out\n\n    return nodes\n\n\ndistributed.distributed_run_extensions[CsvReader] = csv_distributed_run\n\n\nclass StreamReaderType(types.Opaque):\n    def __init__(self):\n        super(StreamReaderType, self).__init__(name=\'StreamReaderType\')\n\n\nstream_reader_type = StreamReaderType()\nregister_model(StreamReaderType)(models.OpaqueModel)\n\n\n@box(StreamReaderType)\ndef box_stream_reader(typ, val, c):\n    return val\n\n\ndef _get_dtype_str(t):\n    dtype = t.dtype\n\n    if isinstance(t, Categorical):\n        # return categorical representation\n        # for some reason pandas and pyarrow read_csv() return CategoricalDtype with\n        # ordered=False in case when dtype is with ordered=None\n        return str(t).replace(\'ordered=None\', \'ordered=False\')\n\n    if dtype == types.NPDatetime(\'ns\'):\n        dtype = \'NPDatetime(""ns"")\'\n    if t == string_array_type:\n        # HACK: add string_array_type to numba.types\n        # FIXME: fix after Numba #3372 is resolved\n        types.string_array_type = string_array_type\n        return \'string_array_type\'\n    return \'{}[::1]\'.format(dtype)\n\n\ndef _get_pd_dtype_str(t):\n    dtype = t.dtype\n    if isinstance(t, Categorical):\n        return \'pd.{}\'.format(t.pd_dtype)\n    if dtype == types.NPDatetime(\'ns\'):\n        dtype = \'str\'\n    if t == string_array_type:\n        return \'str\'\n    return \'np.{}\'.format(dtype)\n\n\n# XXX: temporary fix pending Numba\'s #3378\n# keep the compiled functions around to make sure GC doesn\'t delete them and\n# the reference to the dynamic function inside them\n# (numba/lowering.py:self.context.add_dynamic_addr ...)\ncompiled_funcs = []\n\n\n# TODO: move to hpat.common\ndef to_varname(string):\n    """"""Converts string to correct Python variable name.\n    Replaces unavailable symbols with _ and insert _ if string starts with digit.\n    """"""\n    import re\n    return re.sub(r\'\\W|^(?=\\d)\',\'_\', string)\n\n\n@contextlib.contextmanager\ndef pyarrow_cpu_count(cpu_count=pyarrow.cpu_count()):\n    old_cpu_count = pyarrow.cpu_count()\n    pyarrow.set_cpu_count(cpu_count)\n    try:\n        yield\n    finally:\n        pyarrow.set_cpu_count(old_cpu_count)\n\n\ndef pyarrow_cpu_count_equal_numba_num_treads(func):\n    """"""Decorator. Set pyarrow cpu_count the same as NUMBA_NUM_THREADS.""""""\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        with pyarrow_cpu_count(numba.config.NUMBA_NUM_THREADS):\n            return func(*args, **kwargs)\n    return wrapper\n\n\n@pyarrow_cpu_count_equal_numba_num_treads\ndef pandas_read_csv(\n        filepath_or_buffer,\n        sep=\',\',\n        delimiter=None,\n        # Column and Index Locations and Names\n        header=""infer"",\n        names=None,\n        index_col=None,\n        usecols=None,\n        squeeze=False,\n        prefix=None,\n        mangle_dupe_cols=True,\n        # General Parsing Configuration\n        dtype=None,\n        engine=None,\n        converters=None,\n        true_values=None,\n        false_values=None,\n        skipinitialspace=False,\n        skiprows=None,\n        skipfooter=0,\n        nrows=None,\n        # NA and Missing Data Handling\n        na_values=None,\n        keep_default_na=True,\n        na_filter=True,\n        verbose=False,\n        skip_blank_lines=True,\n        # Datetime Handling\n        parse_dates=False,\n        infer_datetime_format=False,\n        keep_date_col=False,\n        date_parser=None,\n        dayfirst=False,\n        cache_dates=True,\n        # Iteration\n        iterator=False,\n        chunksize=None,\n        # Quoting, Compression, and File Format\n        compression=""infer"",\n        thousands=None,\n        decimal=b""."",\n        lineterminator=None,\n        quotechar=\'""\',\n        # quoting=csv.QUOTE_MINIMAL,  # not supported\n        doublequote=True,\n        escapechar=None,\n        comment=None,\n        encoding=None,\n        dialect=None,\n        # Error Handling\n        error_bad_lines=True,\n        warn_bad_lines=True,\n        # Internal\n        delim_whitespace=False,\n        # low_memory=_c_parser_defaults[""low_memory""],  # not supported\n        memory_map=False,\n        float_precision=None,\n):\n    """"""Implements pandas.read_csv via pyarrow.csv.read_csv.\n    This function has the same interface as pandas.read_csv.\n    """"""\n\n    if delimiter is None:\n        delimiter = sep\n\n    autogenerate_column_names = bool(names)\n\n    include_columns = None\n\n    if usecols:\n        if type(usecols[0]) == str:\n            if names:\n                include_columns = [f\'f{names.index(col)}\' for col in usecols]\n            else:\n                include_columns = usecols\n        elif type(usecols[0]) == int:\n            include_columns = [f\'f{i}\' for i in usecols]\n        else:\n            # usecols should be all str or int\n            assert False\n\n    # try:\n    #     keys = [k for k, v in dtype.items() if isinstance(v, pd.CategoricalDtype)]\n    #     if keys:\n    #         for k in keys:\n    #             del dtype[k]\n    #         names_list = list(names)\n    #         categories = [f""f{names_list.index(k)}"" for k in keys]\n    # except: pass\n\n    categories = []\n\n    if dtype:\n        if names:\n            names_list = list(names)\n            if isinstance(dtype, dict):\n                column_types = {}\n                for k, v in dtype.items():\n                    column_name = ""f{}"".format(names_list.index(k))\n                    if isinstance(v, pd.CategoricalDtype):\n                        categories.append(column_name)\n                        column_type = pyarrow.string()\n                    else:\n                        column_type = pyarrow.from_numpy_dtype(v)\n                    column_types[column_name] = column_type\n            else:\n                pa_dtype = pyarrow.from_numpy_dtype(dtype)\n                column_types = {f""f{names_list.index(k)}"": pa_dtype for k in names}\n        elif usecols:\n            if isinstance(dtype, dict):\n                column_types = {k: pyarrow.from_numpy_dtype(v) for k, v in dtype.items()}\n            else:\n                column_types = {k: pyarrow.from_numpy_dtype(dtype) for k in usecols}\n        else:\n            if isinstance(dtype, dict):\n                column_types = {k: pyarrow.from_numpy_dtype(v) for k, v in dtype.items()}\n            else:\n                column_types = pyarrow.from_numpy_dtype(dtype)\n    else:\n        column_types = None\n\n    try:\n        for column in parse_dates:\n            name = f""f{column}""\n            # TODO: Try to help pyarrow infer date type - set DateType.\n            # dtype[name] = pyarrow.from_numpy_dtype(np.datetime64) # string\n            del column_types[name]\n    except: pass\n\n    parse_options = pyarrow.csv.ParseOptions(\n        delimiter=delimiter,\n    )\n\n    read_options = pyarrow.csv.ReadOptions(\n        skip_rows=skiprows,\n        # column_names=column_names,\n        autogenerate_column_names=autogenerate_column_names,\n    )\n\n    convert_options = pyarrow.csv.ConvertOptions(\n        column_types=column_types,\n        strings_can_be_null=True,\n        include_columns=include_columns,\n    )\n\n    table = pyarrow.csv.read_csv(\n        filepath_or_buffer,\n        read_options=read_options,\n        parse_options=parse_options,\n        convert_options=convert_options,\n    )\n\n    dataframe = table.to_pandas(\n        # categories=categories or None,\n    )\n\n    if names:\n        if usecols and len(names) != len(usecols):\n            if isinstance(usecols[0], int):\n                dataframe.columns = [names[col] for col in usecols]\n            elif isinstance(usecols[0], str):\n                dataframe.columns = [name for name in names if name in usecols]\n        else:\n            dataframe.columns = names\n\n    # fix when PyArrow will support predicted categories\n    if isinstance(dtype, dict):\n        for column_name, column_type in dtype.items():\n            if isinstance(column_type, pd.CategoricalDtype):\n                dataframe[column_name] = dataframe[column_name].astype(column_type)\n\n    return dataframe\n\n\ndef _gen_csv_reader_py_pyarrow(col_names, col_typs, usecols, sep, typingctx, targetctx, parallel, skiprows):\n    func_text, func_name = _gen_csv_reader_py_pyarrow_func_text(col_names, col_typs, usecols, sep, skiprows)\n    csv_reader_py = _gen_csv_reader_py_pyarrow_py_func(func_text, func_name)\n    return _gen_csv_reader_py_pyarrow_jit_func(csv_reader_py)\n\n\ndef _gen_csv_reader_py_pyarrow_func_text_core(col_names, col_typs, dtype_present, usecols, signature=None):\n    # TODO: support non-numpy types like strings\n    date_inds = "", "".join(str(i) for i, t in enumerate(col_typs) if t.dtype == types.NPDatetime(\'ns\'))\n    return_columns = usecols if usecols and isinstance(usecols[0], str) else col_names\n    nb_objmode_vars = "", "".join([\n        \'{}=""{}""\'.format(to_varname(cname), _get_dtype_str(t))\n        for cname, t in zip(return_columns, col_typs)\n    ])\n    pd_dtype_strs = "", "".join([\n        ""\'{}\': {}"".format(cname, _get_pd_dtype_str(t))\n        for cname, t in zip(return_columns, col_typs)\n    ])\n\n    if signature is None:\n        signature = ""filepath_or_buffer""\n    func_text = ""def csv_reader_py({}):\\n"".format(signature)\n    func_text += ""  with objmode({}):\\n"".format(nb_objmode_vars)\n    func_text += ""    df = pandas_read_csv(filepath_or_buffer,\\n""\n\n    # pyarrow reads unnamed header as "" "", pandas reads it as ""Unnamed: N""\n    # during inference from file names should be raplaced with ""Unnamed: N""\n    # passing names to pyarrow means that one row is header and should be skipped\n    if col_names and any(map(lambda x: x.startswith(\'Unnamed: \'), col_names)):\n        func_text += ""        names={},\\n"".format(col_names)\n        func_text += ""        skiprows=(skiprows and skiprows + 1) or 1,\\n""\n    else:\n        func_text += ""        names=names,\\n""\n        func_text += ""        skiprows=skiprows,\\n""\n\n    func_text += ""        parse_dates=[{}],\\n"".format(date_inds)\n\n    # Python objects (e.g. str, np.float) could not be jitted and passed to objmode\n    # so they are hardcoded to function\n    # func_text += ""        dtype={{{}}},\\n"".format(pd_dtype_strs) if dtype_present else \\\n    #              ""        dtype=dtype,\\n""\n    # dtype is hardcoded because datetime should be read as string\n    func_text += ""        dtype={{{}}},\\n"".format(pd_dtype_strs)\n\n    func_text += ""        usecols=usecols,\\n""\n    func_text += ""        sep=sep,\\n""\n    func_text += ""        delimiter=delimiter,\\n""\n    func_text += ""    )\\n""\n    for cname in return_columns:\n        func_text += ""    {} = df[\'{}\'].values\\n"".format(to_varname(cname), cname)\n        # func_text += ""    print({})\\n"".format(cname)\n    return func_text, \'csv_reader_py\'\n\n\ndef _gen_csv_reader_py_pyarrow_func_text(col_names, col_typs, usecols):\n    func_text, func_name = _gen_csv_reader_py_pyarrow_func_text_core(col_names, col_typs, usecols)\n\n    func_text += ""  return ({},)\\n"".format("", "".join(to_varname(c) for c in col_names))\n\n    return func_text, func_name\n\n\ndef _gen_csv_reader_py_pyarrow_func_text_dataframe(col_names, col_typs, dtype_present, usecols, signature):\n    func_text, func_name = _gen_csv_reader_py_pyarrow_func_text_core(\n        col_names, col_typs, dtype_present, usecols, signature)\n    return_columns = usecols if usecols and isinstance(usecols[0], str) else col_names\n\n    func_text += ""  return sdc.hiframes.pd_dataframe_ext.init_dataframe({}, None, {})\\n"".format(\n        "", "".join(to_varname(c) for c in return_columns),\n        "", "".join(""\'{}\'"".format(c) for c in return_columns)\n    )\n\n    return func_text, func_name\n\n\ndef _gen_csv_reader_py_pyarrow_py_func(func_text, func_name):\n    locals = {}\n    exec(func_text, globals(), locals)\n    func = locals[func_name]\n    return func\n\n\ndef _gen_csv_reader_py_pyarrow_jit_func(csv_reader_py):\n    # TODO: no_cpython_wrapper=True crashes for some reason\n    jit_func = numba.njit(csv_reader_py)\n    compiled_funcs.append(jit_func)\n    return jit_func\n\n\n_gen_csv_reader_py = _gen_csv_reader_py_pyarrow\n'"
sdc/io/np_io.py,6,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\nimport numpy as np\nimport numba\nimport sdc\nfrom numba import types\nfrom numba.core import cgutils\nfrom numba.np.arrayobj import make_array\nfrom numba.extending import overload, intrinsic, overload_method\nfrom sdc.str_ext import string_type\n\nfrom numba.core.ir_utils import (compile_to_numba_ir, replace_arg_nodes,\n                            find_callname, guard)\n\n_get_file_size = types.ExternalFunction(""get_file_size"", types.int64(types.voidptr))\n_file_read = types.ExternalFunction(""file_read"", types.void(types.voidptr, types.voidptr, types.intp))\n_file_read_parallel = types.ExternalFunction(\n    ""file_read_parallel"", types.void(\n        types.voidptr, types.voidptr, types.intp, types.intp))\n\nfile_write = types.ExternalFunction(""file_write"", types.void(types.voidptr, types.voidptr, types.intp))\n\n_file_write_parallel = types.ExternalFunction(\n    ""file_write_parallel"",\n    types.void(\n        types.voidptr,\n        types.voidptr,\n        types.intp,\n        types.intp,\n        types.intp))\n\n\ndef _handle_np_fromfile(assign, lhs, rhs):\n    """"""translate np.fromfile() to native\n    """"""\n    # TODO: dtype in kws\n    if len(rhs.args) != 2:  # pragma: no cover\n        raise ValueError(\n            ""np.fromfile(): file name and dtype expected"")\n\n    # FIXME: import here since hio has hdf5 which might not be available\n    from .. import hio\n    from .. import transport_seq as transport\n\n    import llvmlite.binding as ll\n    ll.add_symbol(\'get_file_size\', transport.get_file_size)\n    ll.add_symbol(\'file_read\', hio.file_read)\n    ll.add_symbol(\'file_read_parallel\', transport.file_read_parallel)\n    _fname = rhs.args[0]\n    _dtype = rhs.args[1]\n\n    def fromfile_impl(fname, dtype):\n        size = get_file_size(fname)\n        dtype_size = get_dtype_size(dtype)\n        A = np.empty(size // dtype_size, dtype=dtype)\n        file_read(fname, A, size)\n        read_arr = A\n\n    f_block = compile_to_numba_ir(fromfile_impl,\n                                  {\'np\': np,\n                                   \'get_file_size\': get_file_size,\n                                   \'file_read\': file_read,\n                                   \'get_dtype_size\': get_dtype_size}).blocks.popitem()[1]\n    replace_arg_nodes(f_block, [_fname, _dtype])\n    nodes = f_block.body[:-3]  # remove none return\n    nodes[-1].target = lhs\n    return nodes\n\n\n@intrinsic\ndef get_dtype_size(typingctx, dtype=None):\n    assert isinstance(dtype, types.DTypeSpec)\n\n    def codegen(context, builder, sig, args):\n        num_bytes = context.get_abi_sizeof(context.get_data_type(dtype.dtype))\n        return context.get_constant(types.intp, num_bytes)\n    return types.intp(dtype), codegen\n\n\n@overload_method(types.Array, \'tofile\')\ndef tofile_overload(arr, fname):\n    # FIXME: import here since hio has hdf5 which might not be available\n    from .. import hio\n    from .. import transport_seq as transport\n\n    import llvmlite.binding as ll\n    ll.add_symbol(\'file_write\', hio.file_write)\n    ll.add_symbol(\'file_write_parallel\', transport.file_write_parallel)\n    # TODO: fix Numba to convert literal\n    if fname == string_type or isinstance(fname, types.StringLiteral):\n        def tofile_impl(arr, fname):\n            A = np.ascontiguousarray(arr)\n            dtype_size = get_dtype_size(A.dtype)\n            file_write(fname._data, A.ctypes, dtype_size * A.size)\n\n        return tofile_impl\n\n# from llvmlite import ir as lir\n# @intrinsic\n# def print_array_ptr(typingctx, arr_ty):\n#     assert isinstance(arr_ty, types.Array)\n#     def codegen(context, builder, sig, args):\n#         out = make_array(sig.args[0])(context, builder, args[0])\n#         cgutils.printf(builder, ""%p "", out.data)\n#         cgutils.printf(builder, ""%lf "", builder.bitcast(out.data, lir.IntType(64).as_pointer()))\n#         return context.get_dummy_value()\n#     return types.void(arr_ty), codegen\n\n\ndef file_write_parallel(fname, arr, start, count):\n    pass\n\n# TODO: fix A.ctype inlined case\n@overload(file_write_parallel)\ndef file_write_parallel_overload(fname, arr, start, count):\n    if fname == string_type:  # avoid str literal\n        def _impl(fname, arr, start, count):\n            A = np.ascontiguousarray(arr)\n            dtype_size = get_dtype_size(A.dtype)\n            elem_size = dtype_size * sdc.distributed_lower.get_tuple_prod(A.shape[1:])\n            # sdc.cprint(start, count, elem_size)\n            _file_write_parallel(fname._data, A.ctypes, start, count, elem_size)\n        return _impl\n\n\ndef file_read_parallel(fname, arr, start, count):\n    return\n\n\n@overload(file_read_parallel)\ndef file_read_parallel_overload(fname, arr, start, count):\n    if fname == string_type:\n        def _impl(fname, arr, start, count):\n            dtype_size = get_dtype_size(arr.dtype)\n            _file_read_parallel(fname._data, arr.ctypes, start * dtype_size, count * dtype_size)\n        return _impl\n\n\ndef file_read(fname, arr, size):\n    return\n\n\n@overload(file_read)\ndef file_read_overload(fname, arr, size):\n    if fname == string_type:\n        return lambda fname, arr, size: _file_read(fname._data, arr.ctypes, size)\n\n\ndef get_file_size(fname):\n    return 0\n\n\n@overload(get_file_size)\ndef get_file_size_overload(fname):\n    if fname == string_type:\n        return lambda fname: _get_file_size(fname._data)\n'"
sdc/io/parquet_pio.py,4,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\nfrom sdc.config import _has_pyarrow\nimport llvmlite.binding as ll\nfrom llvmlite import ir as lir\nfrom numba.np.arrayobj import make_array\nfrom numba.core.imputils import lower_builtin\nfrom numba.core import cgutils\nimport numba\nfrom numba import config, types\nfrom numba.core import ir, ir_utils\nfrom numba.core.ir_utils import (mk_unique_var, replace_vars_inner, find_topo_order,\n                            dprint_func_ir, remove_dead, mk_alloc, remove_dels,\n                            get_name_var_table, replace_var_names,\n                            add_offset_to_labels, get_ir_of_code,\n                            compile_to_numba_ir, replace_arg_nodes,\n                            find_callname, guard, require, get_definition)\n\nfrom numba.core.typing.templates import infer_global, AbstractTemplate\nfrom numba.core.typing import signature\nfrom numba.core.imputils import impl_ret_new_ref, impl_ret_borrowed\nimport numpy as np\nimport sdc\nfrom sdc.str_ext import string_type, unicode_to_char_ptr\nfrom sdc.str_arr_ext import StringArray, StringArrayPayloadType, construct_string_array\nfrom sdc.str_arr_ext import string_array_type\nfrom sdc.utilities.utils import unliteral_all\n\n\n# from parquet/types.h\n# boolean, int32, int64, int96, float, double, byte\n# XXX arrow converts int96 timestamp to int64\n_type_to_pq_dtype_number = {\'bool_\': 0, \'int32\': 1, \'int64\': 2,\n                            \'int96\': 3, \'float32\': 4, \'float64\': 5,\n                            repr(types.NPDatetime(\'ns\')): 3, \'int8\': 6}\n\n\ndef read_parquet():\n    return 0\n\n\ndef read_parquet_str():\n    return 0\n\n\ndef read_parquet_str_parallel():\n    return 0\n\n\ndef read_parquet_parallel():\n    return 0\n\n\ndef get_column_size_parquet():\n    return 0\n\n\ndef remove_parquet(rhs, lives, call_list):\n    # the call is dead if the read array is dead\n    if call_list == [read_parquet] and rhs.args[2].name not in lives:\n        return True\n    if call_list == [get_column_size_parquet]:\n        return True\n    if call_list == [read_parquet_str]:\n        return True\n    return False\n\n\nnumba.core.ir_utils.remove_call_handlers.append(remove_parquet)\n\n\nclass ParquetHandler(object):\n    """"""analyze and transform parquet IO calls""""""\n\n    def __init__(self, func_ir, typingctx, args, _locals, _reverse_copies):\n        self.func_ir = func_ir\n        self.typingctx = typingctx\n        self.args = args\n        self.locals = _locals\n        self.reverse_copies = _reverse_copies\n\n    def gen_parquet_read(self, file_name, lhs):\n        scope = file_name.scope\n        loc = file_name.loc\n\n        table_types = None\n        # lhs is temporary and will possibly be assigned to user variable\n        assert lhs.name.startswith(\'$\')\n        if lhs.name in self.reverse_copies and self.reverse_copies[lhs.name] in self.locals:\n            table_types = self.locals[self.reverse_copies[lhs.name]]\n            self.locals.pop(self.reverse_copies[lhs.name])\n\n        convert_types = {}\n        # user-specified type conversion\n        if lhs.name in self.reverse_copies and (self.reverse_copies[lhs.name] + \':convert\') in self.locals:\n            convert_types = self.locals[self.reverse_copies[lhs.name] + \':convert\']\n            self.locals.pop(self.reverse_copies[lhs.name] + \':convert\')\n\n        if table_types is None:\n            fname_def = guard(get_definition, self.func_ir, file_name)\n            if (not isinstance(fname_def, (ir.Const, ir.Global, ir.FreeVar))\n                    or not isinstance(fname_def.value, str)):\n                raise ValueError(""Parquet schema not available"")\n            file_name_str = fname_def.value\n            col_names, col_types = parquet_file_schema(file_name_str)\n            # remove Pandas index if exists\n            # TODO: handle index properly when indices are supported\n            _rm_pd_index(col_names, col_types)\n        else:\n            col_names = list(table_types.keys())\n            col_types = list(table_types.values())\n\n        out_nodes = []\n        # get arrow readers once\n\n        def init_arrow_readers(fname):\n            arrow_readers = get_arrow_readers(unicode_to_char_ptr(fname))\n\n        f_block = compile_to_numba_ir(init_arrow_readers,\n                                      {\'get_arrow_readers\': _get_arrow_readers,\n                                       \'unicode_to_char_ptr\': unicode_to_char_ptr,\n                                       }).blocks.popitem()[1]\n\n        replace_arg_nodes(f_block, [file_name])\n        out_nodes += f_block.body[:-3]\n        arrow_readers_var = out_nodes[-1].target\n\n        col_arrs = []\n        for i, cname in enumerate(col_names):\n            # get column type from schema\n            c_type = col_types[i]\n            if cname in convert_types:\n                c_type = convert_types[cname].dtype\n\n            # create a variable for column and assign type\n            varname = mk_unique_var(cname)\n            #self.locals[varname] = c_type\n            cvar = ir.Var(scope, varname, loc)\n            col_arrs.append(cvar)\n\n            out_nodes += get_column_read_nodes(c_type, cvar, arrow_readers_var, i)\n\n        # delete arrow readers\n        def cleanup_arrow_readers(readers):\n            s = del_arrow_readers(readers)\n\n        f_block = compile_to_numba_ir(cleanup_arrow_readers,\n                                      {\'del_arrow_readers\': _del_arrow_readers,\n                                       }).blocks.popitem()[1]\n        replace_arg_nodes(f_block, [arrow_readers_var])\n        out_nodes += f_block.body[:-3]\n        return col_names, col_arrs, out_nodes\n\n\ndef get_column_read_nodes(c_type, cvar, arrow_readers_var, i):\n\n    loc = cvar.loc\n\n    func_text = \'def f(arrow_readers):\\n\'\n    func_text += \'  col_size = get_column_size_parquet(arrow_readers, {})\\n\'.format(i)\n    # generate strings differently\n    if c_type == string_type:\n        # pass size for easier allocation and distributed analysis\n        func_text += \'  column = read_parquet_str(arrow_readers, {}, col_size)\\n\'.format(\n            i)\n    else:\n        el_type = get_element_type(c_type)\n        if el_type == repr(types.NPDatetime(\'ns\')):\n            func_text += \'  column_tmp = np.empty(col_size, dtype=np.int64)\\n\'\n            # TODO: fix alloc\n            func_text += \'  column = sdc.hiframes.api.ts_series_to_arr_typ(column_tmp)\\n\'\n        else:\n            func_text += \'  column = np.empty(col_size, dtype=np.{})\\n\'.format(\n                el_type)\n        func_text += \'  status = read_parquet(arrow_readers, {}, column, np.int32({}))\\n\'.format(\n            i, _type_to_pq_dtype_number[el_type])\n\n    loc_vars = {}\n    exec(func_text, {\'sdc\': sdc, \'np\': np}, loc_vars)\n    size_func = loc_vars[\'f\']\n    _, f_block = compile_to_numba_ir(size_func,\n                                     {\'get_column_size_parquet\': get_column_size_parquet,\n                                      \'read_parquet\': read_parquet,\n                                      \'read_parquet_str\': read_parquet_str,\n                                      \'np\': np,\n                                      \'sdc\': sdc,\n                                      \'StringArray\': StringArray}).blocks.popitem()\n\n    replace_arg_nodes(f_block, [arrow_readers_var])\n    out_nodes = f_block.body[:-3]\n    for stmt in reversed(out_nodes):\n        if stmt.target.name.startswith(""column""):\n            assign = ir.Assign(stmt.target, cvar, loc)\n            break\n\n    out_nodes.append(assign)\n    return out_nodes\n\n\ndef get_element_type(dtype):\n    out = repr(dtype)\n    if out == \'bool\':\n        out = \'bool_\'\n    return out\n\n\ndef _get_numba_typ_from_pa_typ(pa_typ):\n    import pyarrow as pa\n    _typ_map = {\n        # boolean\n        pa.bool_(): types.bool_,\n        # signed int types\n        pa.int8(): types.int8,\n        pa.int16(): types.int16,\n        pa.int32(): types.int32,\n        pa.int64(): types.int64,\n        # unsigned int types\n        pa.uint8(): types.uint8,\n        pa.uint16(): types.uint16,\n        pa.uint32(): types.uint32,\n        pa.uint64(): types.uint64,\n        # float types (TODO: float16?)\n        pa.float32(): types.float32,\n        pa.float64(): types.float64,\n        # String\n        pa.string(): string_type,\n        # date\n        pa.date32(): types.NPDatetime(\'ns\'),\n        pa.date64(): types.NPDatetime(\'ns\'),\n        # time (TODO: time32, time64, ...)\n        pa.timestamp(\'ns\'): types.NPDatetime(\'ns\'),\n        pa.timestamp(\'us\'): types.NPDatetime(\'ns\'),\n        pa.timestamp(\'ms\'): types.NPDatetime(\'ns\'),\n        pa.timestamp(\'s\'): types.NPDatetime(\'ns\'),\n    }\n    if pa_typ not in _typ_map:\n        raise ValueError(""Arrow data type {} not supported yet"".format(pa_typ))\n    return _typ_map[pa_typ]\n\n\ndef parquet_file_schema(file_name):\n    import pyarrow.parquet as pq\n    col_names = []\n    col_types = []\n\n    pq_dataset = pq.ParquetDataset(file_name)\n    col_names = pq_dataset.schema.names\n    pa_schema = pq_dataset.schema.to_arrow_schema()\n\n    col_types = [_get_numba_typ_from_pa_typ(pa_schema.field_by_name(c).type)\n                 for c in col_names]\n    # TODO: close file?\n    return col_names, col_types\n\n\ndef _rm_pd_index(col_names, col_types):\n    """"""remove pandas index if found in columns\n    """"""\n    try:\n        pd_index_loc = col_names.index(\'__index_level_0__\')\n        del col_names[pd_index_loc]\n        del col_types[pd_index_loc]\n    except ValueError:\n        pass\n\n\n_get_arrow_readers = types.ExternalFunction(""get_arrow_readers"", types.Opaque(\'arrow_reader\')(types.voidptr))\n_del_arrow_readers = types.ExternalFunction(""del_arrow_readers"", types.void(types.Opaque(\'arrow_reader\')))\n\n\n@infer_global(get_column_size_parquet)\nclass SizeParquetInfer(AbstractTemplate):\n    def generic(self, args, kws):\n        assert not kws\n        assert len(args) == 2\n        return signature(types.intp, args[0], types.unliteral(args[1]))\n\n\n@infer_global(read_parquet)\nclass ReadParquetInfer(AbstractTemplate):\n    def generic(self, args, kws):\n        assert not kws\n        assert len(args) == 4\n        if args[2] == types.intp:  # string read call, returns string array\n            return signature(string_array_type, *unliteral_all(args))\n        # array_ty = types.Array(ndim=1, layout=\'C\', dtype=args[2])\n        return signature(types.int64, *unliteral_all(args))\n\n\n@infer_global(read_parquet_str)\nclass ReadParquetStrInfer(AbstractTemplate):\n    def generic(self, args, kws):\n        assert not kws\n        assert len(args) == 3\n        return signature(string_array_type, *unliteral_all(args))\n\n\n@infer_global(read_parquet_str_parallel)\nclass ReadParquetStrParallelInfer(AbstractTemplate):\n    def generic(self, args, kws):\n        assert not kws\n        assert len(args) == 4\n        return signature(string_array_type, *unliteral_all(args))\n\n\n@infer_global(read_parquet_parallel)\nclass ReadParallelParquetInfer(AbstractTemplate):\n    def generic(self, args, kws):\n        assert not kws\n        assert len(args) == 6\n        # array_ty = types.Array(ndim=1, layout=\'C\', dtype=args[2])\n        return signature(types.int32, *unliteral_all(args))\n\n\n# if _has_pyarrow:\n#     from .. import parquet_cpp\n#     ll.add_symbol(\'get_arrow_readers\', parquet_cpp.get_arrow_readers)\n#     ll.add_symbol(\'del_arrow_readers\', parquet_cpp.del_arrow_readers)\n#     ll.add_symbol(\'pq_read\', parquet_cpp.read)\n#     ll.add_symbol(\'pq_read_parallel\', parquet_cpp.read_parallel)\n#     ll.add_symbol(\'pq_get_size\', parquet_cpp.get_size)\n#     ll.add_symbol(\'pq_read_string\', parquet_cpp.read_string)\n#     ll.add_symbol(\'pq_read_string_parallel\', parquet_cpp.read_string_parallel)\n\n\n@lower_builtin(get_column_size_parquet, types.Opaque(\'arrow_reader\'), types.intp)\ndef pq_size_lower(context, builder, sig, args):\n    fnty = lir.FunctionType(lir.IntType(64),\n                            [lir.IntType(8).as_pointer(), lir.IntType(64)])\n    fn = builder.module.get_or_insert_function(fnty, name=""pq_get_size"")\n    return builder.call(fn, args)\n\n\n@lower_builtin(read_parquet, types.Opaque(\'arrow_reader\'), types.intp, types.Array, types.int32)\ndef pq_read_lower(context, builder, sig, args):\n    fnty = lir.FunctionType(lir.IntType(64),\n                            [lir.IntType(8).as_pointer(), lir.IntType(64),\n                             lir.IntType(8).as_pointer()], lir.IntType(32))\n    out_array = make_array(sig.args[2])(context, builder, args[2])\n\n    fn = builder.module.get_or_insert_function(fnty, name=""pq_read"")\n    return builder.call(fn, [args[0], args[1],\n                             builder.bitcast(\n                                 out_array.data, lir.IntType(8).as_pointer()),\n                             args[3]])\n\n\n@lower_builtin(\n    read_parquet_parallel,\n    types.Opaque(\'arrow_reader\'),\n    types.intp,\n    types.Array,\n    types.int32,\n    types.intp,\n    types.intp)\ndef pq_read_parallel_lower(context, builder, sig, args):\n    fnty = lir.FunctionType(lir.IntType(32),\n                            [lir.IntType(8).as_pointer(), lir.IntType(64),\n                             lir.IntType(8).as_pointer(),\n                             lir.IntType(32), lir.IntType(64), lir.IntType(64)])\n    out_array = make_array(sig.args[2])(context, builder, args[2])\n\n    fn = builder.module.get_or_insert_function(fnty, name=""pq_read_parallel"")\n    return builder.call(fn, [args[0], args[1],\n                             builder.bitcast(\n                                 out_array.data, lir.IntType(8).as_pointer()),\n                             args[3], args[4], args[5]])\n\n# read strings\n\n\n@lower_builtin(read_parquet_str, types.Opaque(\'arrow_reader\'), types.intp, types.intp)\ndef pq_read_string_lower(context, builder, sig, args):\n\n    typ = sig.return_type\n    dtype = StringArrayPayloadType()\n    meminfo, meminfo_data_ptr = construct_string_array(context, builder)\n    string_array = context.make_helper(builder, typ)\n\n    str_arr_payload = cgutils.create_struct_proxy(dtype)(context, builder)\n    string_array.num_items = args[2]\n\n    fnty = lir.FunctionType(lir.IntType(32),\n                            [lir.IntType(8).as_pointer(), lir.IntType(64),\n                             lir.IntType(32).as_pointer().as_pointer(),\n                             lir.IntType(8).as_pointer().as_pointer(),\n                             lir.IntType(8).as_pointer().as_pointer()])\n\n    fn = builder.module.get_or_insert_function(fnty, name=""pq_read_string"")\n    res = builder.call(fn, [args[0], args[1],\n                            str_arr_payload._get_ptr_by_name(\'offsets\'),\n                            str_arr_payload._get_ptr_by_name(\'data\'),\n                            str_arr_payload._get_ptr_by_name(\'null_bitmap\')])\n    builder.store(str_arr_payload._getvalue(), meminfo_data_ptr)\n\n    string_array.meminfo = meminfo\n    string_array.offsets = str_arr_payload.offsets\n    string_array.data = str_arr_payload.data\n    string_array.null_bitmap = str_arr_payload.null_bitmap\n    string_array.num_total_chars = builder.zext(builder.load(\n        builder.gep(string_array.offsets, [string_array.num_items])), lir.IntType(64))\n    ret = string_array._getvalue()\n    return impl_ret_new_ref(context, builder, typ, ret)\n\n\n@lower_builtin(read_parquet_str_parallel, types.Opaque(\'arrow_reader\'), types.intp, types.intp, types.intp)\ndef pq_read_string_parallel_lower(context, builder, sig, args):\n    typ = sig.return_type\n    dtype = StringArrayPayloadType()\n    meminfo, meminfo_data_ptr = construct_string_array(context, builder)\n    str_arr_payload = cgutils.create_struct_proxy(dtype)(context, builder)\n    string_array = context.make_helper(builder, typ)\n    string_array.num_items = args[3]\n\n    fnty = lir.FunctionType(lir.IntType(32),\n                            [lir.IntType(8).as_pointer(), lir.IntType(64),\n                             lir.IntType(32).as_pointer().as_pointer(),\n                             lir.IntType(8).as_pointer().as_pointer(),\n                             lir.IntType(8).as_pointer().as_pointer(),\n                             lir.IntType(64), lir.IntType(64)])\n\n    fn = builder.module.get_or_insert_function(\n        fnty, name=""pq_read_string_parallel"")\n    res = builder.call(fn, [args[0], args[1],\n                            str_arr_payload._get_ptr_by_name(\'offsets\'),\n                            str_arr_payload._get_ptr_by_name(\'data\'),\n                            str_arr_payload._get_ptr_by_name(\'null_bitmap\'),\n                            args[2],\n                            args[3]])\n\n    builder.store(str_arr_payload._getvalue(), meminfo_data_ptr)\n\n    string_array.meminfo = meminfo\n    string_array.offsets = str_arr_payload.offsets\n    string_array.data = str_arr_payload.data\n    string_array.null_bitmap = str_arr_payload.null_bitmap\n    string_array.num_total_chars = builder.zext(builder.load(\n        builder.gep(string_array.offsets, [string_array.num_items])), lir.IntType(64))\n    ret = string_array._getvalue()\n    return impl_ret_new_ref(context, builder, typ, ret)\n'"
sdc/rewrites/__init__.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n'"
sdc/rewrites/dataframe_constructor.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\nfrom numba.core.rewrites import (register_rewrite, Rewrite)\nfrom numba.core.ir_utils import (guard, find_callname)\nfrom numba.core.ir import (Expr)\nfrom numba.extending import overload\n\nfrom pandas import DataFrame\n\nfrom sdc.rewrites.ir_utils import (find_operations, is_dict,\n                                   get_tuple_items, get_dict_items, remove_unused_recursively,\n                                   get_call_parameters,\n                                   declare_constant,\n                                   import_function, make_call,\n                                   insert_before)\nfrom sdc.hiframes.pd_dataframe_ext import (init_dataframe, DataFrameType)\n\nfrom sdc.hiframes.api import fix_df_array\n\nfrom sdc.config import config_pipeline_hpat_default\n\nif not config_pipeline_hpat_default:\n    @register_rewrite(\'before-inference\')\n    class RewriteDataFrame(Rewrite):\n        """"""\n        Searches for calls of pandas.DataFrame and replace it with calls of init_dataframe.\n        """"""\n\n        _pandas_dataframe = (\'DataFrame\', \'pandas\')\n\n        _df_arg_list = (\'data\', \'index\', \'columns\', \'dtype\', \'copy\')\n\n        def __init__(self, pipeline):\n            super().__init__(pipeline)\n\n            self._reset()\n\n        def match(self, func_ir, block, typemap, calltypes):\n            self._reset()\n\n            self._block = block\n            self._func_ir = func_ir\n            self._calls_to_rewrite = set()\n\n            for stmt in find_operations(block=block, op_name=\'call\'):\n                expr = stmt.value\n                fdef = guard(find_callname, func_ir, expr)\n                if fdef == self._pandas_dataframe:\n                    args = get_call_parameters(call=expr, arg_names=self._df_arg_list)\n\n                    if self._match_dict_case(args, func_ir):\n                        self._calls_to_rewrite.add(stmt)\n                    else:\n                        pass  # Forward this case to pd_dataframe_overload which will handle it\n\n            return len(self._calls_to_rewrite) > 0\n\n        def apply(self):\n            init_df_stmt = import_function(init_dataframe, self._block, self._func_ir)\n\n            for stmt in self._calls_to_rewrite:\n                args = get_call_parameters(call=stmt.value, arg_names=self._df_arg_list)\n\n                old_data = args[\'data\']\n\n                args[\'data\'], args[\'columns\'] = self._extract_dict_args(args, self._func_ir)\n\n                self._replace_call(stmt, init_df_stmt.target, args, self._block, self._func_ir)\n\n                remove_unused_recursively(old_data, self._block, self._func_ir)\n\n            return self._block\n\n        def _reset(self):\n            self._block = None\n            self._func_ir = None\n            self._calls_to_rewrite = None\n\n        @staticmethod\n        def _match_dict_case(args, func_ir):\n            if \'data\' in args and is_dict(args[\'data\'], func_ir) and \'columns\' not in args:\n                return True\n\n            return False\n\n        @staticmethod\n        def _extract_tuple_args(args, block, func_ir):\n            data_args = get_tuple_items(args[\'data\'], block, func_ir) if \'data\' in args else None\n            columns_args = get_tuple_items(args[\'columns\'], block, func_ir) if \'columns\' in args else None\n\n            return data_args, columns_args\n\n        @staticmethod\n        def _extract_dict_args(args, func_ir):\n            dict_items = get_dict_items(args[\'data\'], func_ir)\n\n            data_args = [item[1] for item in dict_items]\n            columns_args = [item[0] for item in dict_items]\n\n            return data_args, columns_args\n\n        @staticmethod\n        def _replace_call(stmt, new_call, args, block, func_ir):\n            func = stmt.value\n\n            data_args = args[\'data\']\n            columns_args = args[\'columns\']\n\n            index_args = args.get(\'index\')\n\n            if index_args is None:\n                none_stmt = declare_constant(None, block, func_ir, stmt.loc)\n                index_args = [none_stmt.target]\n            else:\n                index_args = RewriteDataFrame._replace_with_arrays([index_args], stmt, block, func_ir)\n\n            data_args = RewriteDataFrame._replace_with_arrays(data_args, stmt, block, func_ir)\n            all_args = data_args + index_args + columns_args\n\n            call = Expr.call(new_call, all_args, {}, func.loc)\n\n            stmt.value = call\n\n        @staticmethod\n        def _replace_with_arrays(args, stmt, block, func_ir):\n            new_args = []\n\n            for var in args:\n                call_stmt = make_call(fix_df_array, [var], {}, block, func_ir, var.loc)\n                insert_before(block, call_stmt, stmt)\n                new_args.append(call_stmt.target)\n\n            return new_args\n\n    @overload(DataFrame)\n    def pd_dataframe_overload(data, index=None, columns=None, dtype=None, copy=False):\n        """"""\n        Intel Scalable Dataframe Compiler User Guide\n        ********************************************\n        Pandas API: pandas.DataFrame\n\n        Limitations\n        -----------\n        - Parameters `dtype` and `copy` are currently unsupported by Intel Scalable Dataframe Compiler.\n        """"""\n\n        ty_checker = TypeChecker(\'Method DataFrame\')\n        ty_checker.check(self, DataFrameType)\n\n        if not isinstance(data, dict):\n            ty_checker.raise_exc(pat, \'dict\', \'data\')\n\n        if not isinstance(index, (types.Ommited, types.Array, StringArray, types.NoneType)) and index is not None:\n            ty_checker.raise_exc(na, \'array-like\', \'index\')\n\n        if not isinstance(columns, (types.Ommited, types.NoneType)) and columns is not None:\n            ty_checker.raise_exc(na, \'None\', \'columns\')\n\n        if not isinstance(dtype, (types.Ommited, types.NoneType)) and dtype is not None:\n            ty_checker.raise_exc(na, \'None\', \'dtype\')\n\n        if not isinstance(copy, (types.Ommited, types.NoneType)) and columns is not False:\n            ty_checker.raise_exc(na, \'False\', \'copy\')\n\n        return None\n'"
sdc/rewrites/dataframe_getitem_attribute.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nfrom numba.core.ir import Assign, Const, Expr, Var\nfrom numba.core.ir_utils import mk_unique_var\nfrom numba.core.rewrites import register_rewrite, Rewrite\nfrom numba.core.types import StringLiteral\nfrom numba.core.typing import signature\n\nfrom sdc.config import config_pipeline_hpat_default\nfrom sdc.hiframes.pd_dataframe_type import DataFrameType\n\n\nif not config_pipeline_hpat_default:\n    @register_rewrite(\'after-inference\')\n    class RewriteDataFrameGetItemAttr(Rewrite):\n        """"""\n        Search for calls of df.attr and replace it with calls of df[\'attr\']:\n        $0.2 = getattr(value=df, attr=A) -> $const0.0 = const(str, A)\n                                            $0.2 = static_getitem(value=df, index=A, index_var=$const0.0)\n        """"""\n\n        def match(self, func_ir, block, typemap, calltypes):\n            self.func_ir = func_ir\n            self.block = block\n            self.typemap = typemap\n            self.calltypes = calltypes\n            self.getattrs = getattrs = set()\n            for expr in block.find_exprs(op=\'getattr\'):\n                obj = typemap[expr.value.name]\n                if not isinstance(obj, DataFrameType):\n                    continue\n                if expr.attr in obj.columns:\n                    getattrs.add(expr)\n\n            return len(getattrs) > 0\n\n        def apply(self):\n            new_block = self.block.copy()\n            new_block.clear()\n            for inst in self.block.body:\n                if isinstance(inst, Assign) and inst.value in self.getattrs:\n                    const_assign = self._assign_const(inst)\n                    new_block.append(const_assign)\n\n                    inst = self._assign_getitem(inst, index=const_assign.target)\n\n                new_block.append(inst)\n\n            return new_block\n\n        def _mk_unique_var(self, prefix):\n            """"""Make unique var name checking self.func_ir._definitions""""""\n            name = mk_unique_var(prefix)\n            while name in self.func_ir._definitions:\n                name = mk_unique_var(prefix)\n\n            return name\n\n        def _assign_const(self, inst, prefix=\'$const0\'):\n            """"""Create constant from attribute of the instruction.""""""\n            const_node = Const(inst.value.attr, inst.loc)\n            unique_var_name = self._mk_unique_var(prefix)\n            const_var = Var(inst.target.scope, unique_var_name, inst.loc)\n\n            self.func_ir._definitions[const_var.name] = [const_node]\n            self.typemap[const_var.name] = StringLiteral(inst.value.attr)\n\n            return Assign(const_node, const_var, inst.loc)\n\n        def _assign_getitem(self, inst, index):\n            """"""Create getitem instruction from the getattr instruction.""""""\n            new_expr = Expr.getitem(inst.value.value, index, inst.loc)\n            new_inst = Assign(value=new_expr, target=inst.target, loc=inst.loc)\n\n            self.func_ir._definitions[inst.target] = [new_expr]\n            self.calltypes[new_expr] = signature(\n                self.typemap[inst.target.name],\n                self.typemap[new_expr.value.name],\n                self.typemap[new_expr.index.name]\n            )\n\n            return new_inst\n'"
sdc/rewrites/ir_utils.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nfrom sys import modules\n\nfrom types import FunctionType\n\nfrom numba.core.ir import (Const, Global, Var, FreeVar,\n                      Expr, Assign, Del,\n                      unknown_loc)\nfrom numba.core.ir_utils import (guard, find_const, mk_unique_var)\nfrom numba.extending import _Intrinsic\n\n\ndef filter_block_statements(block, stmt_types):\n    """"""\n    Filters given block returning statments of specific type\n    """"""\n\n    for stmt in block.body:\n        if isinstance(stmt, stmt_types):\n            yield stmt\n\n\ndef find_operations(block, op_name):\n    """"""\n    Filter given block returning statements with specific expressions\n    """"""\n\n    for assign in filter_block_statements(block, Assign):\n        rhs = assign.value\n        if isinstance(rhs, Expr) and rhs.op == op_name:\n            yield assign\n\n\ndef find_declarations(block, var_types=None):\n    """"""\n    Filter given block returning statements with variables assignments of specific type\n    """"""\n\n    all_var_types = (Const, Global, Var, FreeVar)\n    if var_types is None:\n        var_types = all_var_types\n\n    if isinstance(var_types, tuple):\n        assert all(typ in all_var_types for typ in var_types)\n    else:\n        assert var_types in all_var_types\n\n    for assign in filter_block_statements(block, (Assign)):\n        rhs = assign.value\n        if isinstance(rhs, var_types):\n            yield assign\n\n\ndef make_var_name(prefix=None, name=None):\n    """"""\n    Creates variable name.\n    If prefix is given it would be extended with postfix to create unique name.\n    If name is given returning this name.\n    Either prefix or name could be not None at the same time\n    """"""\n\n    assert prefix is None or name is None\n    var_name = None\n    if name is not None:\n        var_name = name\n    else:\n        if prefix is None:\n            prefix = \'$_var_\'\n\n        var_name = mk_unique_var(prefix)\n\n    return var_name\n\n\ndef _new_definition(func_ir, var, value, loc):\n    func_ir._definitions[var.name] = [value]\n    return Assign(value=value, target=var, loc=loc)\n\n\ndef make_assign(expr, scope, func_ir, loc=unknown_loc, prefix=None, name=None):\n    """"""\n    Creates variable, assign statement and add variable to the function definition section\n    """"""\n\n    var_name = make_var_name(prefix=prefix, name=name)\n    var = Var(scope, var_name, loc)\n    return _new_definition(func_ir, var, expr, loc)\n\n\ndef declare_constant(value, block, func_ir, loc=unknown_loc, prefix=None, name=None):\n    """"""\n    Creates variable and constant object with given value. Assign created variable to the constant\n    """"""\n\n    if prefix is None and name is None:\n        prefix = \'$_const_\'\n\n    stmt = make_assign(Const(value, loc, True), block.scope, func_ir, loc, prefix=prefix, name=name)\n\n    block.prepend(stmt)\n\n    return stmt\n\n\ndef declare_global(global_name, value, block, func_ir, loc=unknown_loc, prefix=None, name=None):\n    """"""\n    Creates variable and global object. Assign created variable to the global\n    """"""\n\n    if prefix is None and name is None:\n        prefix = \'$_global_\' + global_name\n\n    stmt = make_assign(Global(global_name, value, loc), block.scope, func_ir, loc, prefix=prefix, name=name)\n\n    block.prepend(stmt)\n\n    return stmt\n\n\ndef make_getattr(attr, var, block, func_ir, loc=unknown_loc, prefix=None, name=None):\n    """"""\n    Creates variable and assign to it provided attribute.\n    """"""\n\n    if prefix is None and name is None:\n        prefix = \'$_attr_\' + attr\n    stmt_ = make_assign(Expr.getattr(var, attr, loc), block.scope, func_ir, loc, prefix=prefix, name=name)\n\n    assign = block.find_variable_assignment(var.name)\n    if assign is not None:\n        block.insert_after(stmt_, assign)\n    else:\n        block.prepend(assign)\n\n    return stmt_\n\n\ndef is_dict(var, func_ir):\n    """"""\n    Checks if variable is a dictionary\n    """"""\n\n    try:\n        rhs = func_ir.get_definition(var)\n        if isinstance(rhs, Expr):\n            return rhs.op == \'build_map\'\n    except Exception:\n        pass\n\n    return False\n\n\ndef get_dict_items(var, func_ir):\n    """"""\n    Returns dictionary items\n    """"""\n\n    assert is_dict(var, func_ir)\n    rhs = func_ir.get_definition(var)\n\n    return rhs.items\n\n\ndef is_list(var, func_ir):\n    """"""\n    Checks if variable is a list\n    """"""\n\n    try:\n        rhs = func_ir.get_definition(var)\n        if isinstance(rhs, Expr):\n            return rhs.op == \'build_list\'\n    except Exception:\n        pass\n\n    return False\n\n\ndef is_tuple(var, func_ir):\n    """"""\n    Checks if variable is either constant or non-constant tuple\n    """"""\n\n    val = guard(find_const, func_ir, var)\n    if val is not None:\n        return isinstance(val, tuple)\n\n    try:\n        rhs = func_ir.get_definition(var)\n        if isinstance(rhs, Expr):\n            return rhs.op == \'build_tuple\'\n    except Exception:\n        pass\n\n    return False\n\n\ndef get_tuple_items(var, block, func_ir):\n    """"""\n    Returns tuple items. If tuple is constant creates and returns constants\n    """"""\n\n    def wrap_into_var(value, block, func_ir, loc):\n        stmt = declare_constant(value, block, func_ir, loc)\n\n        return stmt.target\n\n    val = guard(find_const, func_ir, var)\n    if val is not None:\n        if isinstance(val, tuple):\n            return [wrap_into_var(v, block, func_ir, var.loc) for v in val]\n\n        return None\n\n    try:\n        rhs = func_ir.get_definition(var)\n        if isinstance(rhs, Expr):\n            if rhs.op == \'build_tuple\':\n                return list(rhs.items)\n    except Exception:\n        pass\n\n    return None\n\n\ndef find_usage(var, block):\n    """"""\n    Filters given block by statements with given variable\n    """"""\n    assert isinstance(var, Var)\n\n    # TODO handle usage outside of given block\n    for stmt in block.body:\n        if var in stmt.list_vars() or (isinstance(stmt, Del) and stmt.value == var.name):\n            yield stmt\n\n\ndef _remove_unused_internal(var, block, func_ir):\n    """"""\n    Search given block for variable usages. If it is used only in one assignment and one Del - remove this variable\n    """"""\n\n    usage_list = []\n    use_count = 0\n\n    for stmt in find_usage(var, block):\n        usage_list.append(stmt)\n        if isinstance(stmt, Assign) and var == stmt.target:\n            continue\n        elif isinstance(stmt, Del):\n            continue\n        else:\n            use_count += 1\n\n    if use_count == 0:\n        for stmt in usage_list:\n            block.remove(stmt)\n\n        del func_ir._definitions[var.name]\n\n        return True, usage_list\n\n    return False, None\n\n\ndef remove_unused(var, block, func_ir):\n    """"""\n    Search given block for variable usages. If it is used only in one assignment and one Del - remove this variable\n    """"""\n\n    result, _ = _remove_unused_internal(var, block, func_ir)\n\n    return result\n\n\ndef remove_var(var, block, func_ir):\n    """"""\n    Remove all statements with given variable from the block\n    """"""\n\n    for stmt in find_usage(var, block):\n        block.remove(stmt)\n\n    del func_ir._definitions[var.name]\n\n\ndef remove_unused_recursively(var, block, func_ir):\n    """"""\n    Search given block for variable usages.\n    If it is used only in one assignment and one Del - remove it and all it\'s aliases\n    """"""\n\n    result, usage = _remove_unused_internal(var, block, func_ir)\n\n    while usage:\n        for stmt in usage:\n            if isinstance(stmt, Del):\n                continue\n\n            if isinstance(stmt.value, Var):\n                result, usage = _remove_unused_internal(stmt.value, block, func_ir)\n            else:\n                usage = None\n                break\n\n    return result\n\n\ndef get_call_parameters(call, arg_names):\n    """"""\n    Extracts call parameters into dict\n    """"""\n\n    params = dict(zip(arg_names, call.args))\n    params.update(call.kws)\n\n    return params\n\n\ndef import_object(object_name, module_name, block, func_ir, prefix=None, name=None):\n    """"""\n    Imports specific object from a specific module. Do nothing if object is already imported in the block\n    """"""\n\n    def get_module_stmt(module_name, block, func_ir):\n        module = modules[module_name]\n        module_stmt = None\n\n        for stmt in find_declarations(block, Global):\n            rhs = stmt.value\n            if rhs.value == module:\n                module_stmt = stmt\n\n        if module_stmt is None:\n            stmt = declare_global(module_name, module, block, func_ir)\n            module_stmt = stmt\n\n        assert module_stmt is not None\n\n        return module_stmt\n\n    def find_object_stmt(object_name, block):\n        object_stmt = None\n\n        for stmt in find_operations(block, \'getattr\'):\n            expr = stmt.value\n            if expr.attr == object_name:\n                object_stmt = stmt\n\n        return object_stmt\n\n    object_stmt = find_object_stmt(object_name, block)\n\n    if object_stmt is None:\n        module_stmt = get_module_stmt(module_name, block, func_ir)\n        object_stmt = make_getattr(object_name, module_stmt.target, block, func_ir, prefix=prefix, name=name)\n\n    assert object_stmt is not None\n\n    return object_stmt\n\n\ndef import_function(func, block, func_ir, prefix=None, name=None):\n    """"""\n    Creates variable and imports function. Assign created variable to the function\n    """"""\n\n    assert isinstance(func, (FunctionType, _Intrinsic))\n\n    if isinstance(func, _Intrinsic):\n        func = func._defn\n\n    func_name = func.__name__\n    module_name = func.__module__\n\n    return import_object(func_name, module_name, block, func_ir, prefix=prefix, name=name)\n\n\ndef make_call(func, args, kwargs, block, func_ir, loc=unknown_loc, prefix=None, name=None):\n    """"""\n    Creates variable and call expression with given functions object, intrinsic or function variable.\n    Returns assignment statement of created variable to call expression\n    """"""\n    assert isinstance(func, (FunctionType, _Intrinsic, Var))\n    func_var = func\n\n    if isinstance(func, (FunctionType, _Intrinsic)):\n        func_var = import_function(func, block, func_ir).target\n\n    return make_assign(Expr.call(func_var, args, kwargs, loc), block.scope, func_ir, loc, prefix, name)\n\n\ndef insert_before(block, stmt, other):\n    """"""\n    Inserts given statement before another statement in the given block\n    """"""\n\n    index = block.body.index(other)\n    block.body.insert(index, stmt)\n'"
sdc/rewrites/read_csv_consts.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nfrom numba.core.rewrites import register_rewrite, Rewrite\nfrom numba.core.ir_utils import guard, get_definition\nfrom numba import errors\nfrom numba.core import ir\n\nfrom sdc.rewrites.ir_utils import find_operations\n\nimport pandas as pd\n\n\n@register_rewrite(\'before-inference\')\nclass RewriteReadCsv(Rewrite):\n    """"""\n    Searches for calls to Pandas read_csv() and replace its arguments with tuples.\n    """"""\n\n    _read_csv_const_args = (\'names\', \'dtype\', \'usecols\')\n\n    def match(self, func_ir, block, typemap, calltypes):\n        # TODO: check that vars are used only in read_csv\n\n        self.block = block\n        self.args = args = []\n\n        # Find all assignments with a right-hand read_csv() call\n        for inst in find_operations(block=block, op_name=\'call\'):\n            expr = inst.value\n            try:\n                callee = func_ir.infer_constant(expr.func)\n            except errors.ConstantInferenceError:\n                continue\n            if callee is not pd.read_csv:\n                continue\n            # collect arguments with list, set and dict\n            # in order to replace with tuple\n            for key, var in expr.kws:\n                if key in self._read_csv_const_args:\n                    arg_def = guard(get_definition, func_ir, var)\n                    ops = [\'build_list\', \'build_set\', \'build_map\']\n                    if arg_def.op in ops:\n                        args.append(arg_def)\n\n        return len(args) > 0\n\n    def apply(self):\n        """"""\n        Replace list, set and dict expressions with tuple.\n        """"""\n        block = self.block\n        for inst in block.body:\n            if isinstance(inst, ir.Assign) and inst.value in self.args:\n                if inst.value.op == \'build_map\':\n                    inst.value.items = sum(map(list, inst.value.items), [])\n                inst.value.op = \'build_tuple\'\n        return block\n'"
sdc/tests/__init__.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\nfrom sdc.tests.test_basic import *\nfrom sdc.tests.test_series import *\nfrom sdc.tests.test_dataframe import *\nfrom sdc.tests.test_hiframes import *\nfrom .categorical import *\n\n# from sdc.tests.test_d4p import *\nfrom sdc.tests.test_date import *\nfrom sdc.tests.test_strings import *\n\nfrom sdc.tests.test_groupby import *\nfrom sdc.tests.test_join import *\nfrom sdc.tests.test_rolling import *\n\nfrom sdc.tests.test_ml import *\n\nfrom sdc.tests.test_io import *\n\nfrom sdc.tests.test_hpat_jit import *\nfrom sdc.tests.test_indexes import *\n\nfrom sdc.tests.test_sdc_numpy import *\nfrom sdc.tests.test_prange_utils import *\n\n# performance tests\nimport sdc.tests.tests_perf\n'"
sdc/tests/gen_test_data.py,2,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport os\nimport numpy as np\nimport pyarrow as pa\n# import pyarrow.parquet as pq\nimport pandas as pd\n\n\nclass ParquetGenerator:\n    GEN_KDE_PQ_CALLED = False\n    GEN_PQ_TEST_CALLED = False\n\n    @classmethod\n    def gen_kde_pq(cls, file_name=\'kde.parquet\', N=101):\n        if not cls.GEN_KDE_PQ_CALLED:\n            df = pd.DataFrame({\'points\': np.random.random(N)})\n            table = pa.Table.from_pandas(df)\n            row_group_size = 128\n            pq.write_table(table, file_name, row_group_size)\n            cls.GEN_KDE_PQ_CALLED = True\n\n    @classmethod\n    def gen_pq_test(cls):\n        if not cls.GEN_PQ_TEST_CALLED:\n            df = pd.DataFrame(\n                {\n                    \'one\': [-1, np.nan, 2.5, 3., 4., 6., 10.0],\n                    \'two\': [\'foo\', \'bar\', \'baz\', \'foo\', \'bar\', \'baz\', \'foo\'],\n                    \'three\': [True, False, True, True, True, False, False],\n                    # float without NA\n                    \'four\': [-1, 5.1, 2.5, 3., 4., 6., 11.0],\n                    # str with NA\n                    \'five\': [\'foo\', \'bar\', \'baz\', None, \'bar\', \'baz\', \'foo\'],\n                }\n            )\n            table = pa.Table.from_pandas(df)\n            pq.write_table(table, \'example.parquet\')\n            pq.write_table(table, \'example2.parquet\', row_group_size=2)\n            cls.GEN_PQ_TEST_CALLED = True\n\n\ndef generate_spark_data():\n    # test datetime64, spark dates\n    dt1 = pd.DatetimeIndex([\'2017-03-03 03:23\',\n                            \'1990-10-23\', \'1993-07-02 10:33:01\'])\n    df = pd.DataFrame({\'DT64\': dt1, \'DATE\': dt1.copy()})\n    df.to_parquet(\'pandas_dt.pq\')\n\n    import os\n    import shutil\n    import tarfile\n\n    if os.path.exists(\'sdf_dt.pq\'):\n        shutil.rmtree(\'sdf_dt.pq\')\n\n    sdf_dt_archive = os.path.join(os.path.dirname(os.path.abspath(__file__)), \'sdf_dt.pq.bz2\')\n    tar = tarfile.open(sdf_dt_archive, ""r:bz2"")\n    tar.extractall(\'.\')\n    tar.close()\n\n\ndef generate_csv_data():\n    # CSV reader test\n    data = (""0,2.3,4.6,A\\n""\n            ""1,2.3,4.6,B\\n""\n            ""2,2.3,4.6,\\n""\n            ""4,2.3,4.6,D\\n"")\n\n    with open(""csv_data1.csv"", ""w"") as f:\n        f.write(data)\n\n    with open(""csv_data2.csv"", ""w"") as f:\n        # the same types as csv_data1.csv\n        f.write((\n            ""4,2.1,4.1,D\\n""\n            ""3,2.2,4.2,\\n""\n            ""2,2.3,4.3,B\\n""\n            ""1,2.4,4.4,A\\n""\n        ))\n\n    with open(""csv_data_infer1.csv"", ""w"") as f:\n        f.write(\'A,B,C,D\\n\' + data)\n\n    with open(""csv_data_infer_no_column_name.csv"", ""w"") as f:\n        f.write(\',,C,D\\n\' + data)\n\n    with open(""csv_data_infer_sep.csv"", ""w"") as f:\n        f.write((\'A,B,C,D\\n\' + data).replace(\',\', \';\'))\n\n    data = (""0,2.3,2015-01-03,47736\\n""\n            ""1,2.3,1966-11-13,47736\\n""\n            ""2,2.3,1998-05-21,47736\\n""\n            ""4,2.3,2018-07-11,47736\\n"")\n\n    with open(""csv_data_date1.csv"", ""w"") as f:\n        f.write(data)\n\n    # generated data for parallel merge_asof testing\n    df1 = pd.DataFrame({\'time\': pd.DatetimeIndex(\n        [\'2017-01-03\', \'2017-01-06\', \'2017-02-15\', \'2017-02-21\']),\n        \'B\': [4, 5, 9, 6]})\n    df2 = pd.DataFrame({\'time\': pd.DatetimeIndex(\n        [\'2017-01-01\', \'2017-01-14\', \'2017-01-16\', \'2017-02-23\', \'2017-02-23\',\n         \'2017-02-25\']), \'A\': [2, 3, 7, 8, 9, 10]})\n    df1.to_parquet(""asof1.pq"")\n    df2.to_parquet(""asof2.pq"")\n\n\ndef generate_other_data():\n    df = pd.DataFrame({\'A\': [\'bc\'] + [""a""] * 3 + [""bc""] * 3 + [\'a\'], \'B\': [-8, 1, 2, 3, 1, 5, 6, 7]})\n    df.to_parquet(""groupby3.pq"")\n\n    df = pd.DataFrame({""A"": [""foo"", ""foo"", ""foo"", ""foo"", ""foo"",\n                             ""bar"", ""bar"", ""bar"", ""bar""],\n                       ""B"": [""one"", ""one"", ""one"", ""two"", ""two"",\n                             ""one"", ""one"", ""two"", ""two""],\n                       ""C"": [""small"", ""large"", ""large"", ""small"",\n                             ""small"", ""large"", ""small"", ""small"",\n                             ""large""],\n                       ""D"": [1, 2, 2, 6, 3, 4, 5, 6, 9]})\n    df.to_parquet(""pivot2.pq"")\n\n    # generated data for parallel merge_asof testing\n    df1 = pd.DataFrame({\'time\': pd.DatetimeIndex(\n        [\'2017-01-03\', \'2017-01-06\', \'2017-02-15\', \'2017-02-21\']),\n        \'B\': [4, 5, 9, 6]})\n    df2 = pd.DataFrame({\'time\': pd.DatetimeIndex(\n        [\'2017-01-01\', \'2017-01-14\', \'2017-01-16\', \'2017-02-23\', \'2017-02-23\',\n         \'2017-02-25\']), \'A\': [2, 3, 7, 8, 9, 10]})\n    df1.to_parquet(""asof1.pq"")\n    df2.to_parquet(""asof2.pq"")\n\n\nif __name__ == ""__main__"":\n    print(f\'generation phase in {os.getcwd()}\')\n    # ParquetGenerator.gen_kde_pq()\n    # ParquetGenerator.gen_pq_test()\n    # generate_spark_data()\n    generate_csv_data()\n    # generate_other_data()\n'"
sdc/tests/test_base.py,0,"b'import warnings\nimport unittest\n\n\nclass TestCase(unittest.TestCase):\n    """"""Base class for all tests""""""\n\n    def numba_jit(self, *args, **kwargs):\n        import numba\n        if \'nopython\' in kwargs:\n            warnings.warn(\'nopython is set to True and is ignored\', RuntimeWarning)\n        if \'parallel\' in kwargs:\n            warnings.warn(\'parallel is set to True and is ignored\', RuntimeWarning)\n        kwargs.update({\'nopython\': True, \'parallel\': True})\n        return numba.jit(*args, **kwargs)\n\n    def sdc_jit(self, *args, **kwargs):\n        import sdc\n        return sdc.jit(*args, **kwargs)\n\n    def jit(self, *args, **kwargs):\n        from sdc import config\n        if config.config_pipeline_hpat_default:\n            return self.sdc_jit(*args, **kwargs)\n        else:\n            return self.numba_jit(*args, **kwargs)\n'"
sdc/tests/test_basic.py,73,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport itertools\nimport numba\nimport numpy as np\nimport pandas as pd\nimport random\nimport unittest\nfrom numba import types\n\nimport sdc\nfrom sdc.tests.test_base import TestCase\nfrom sdc.tests.test_utils import (check_numba_version,\n                                  count_array_OneD_Vars,\n                                  count_array_OneDs,\n                                  count_array_REPs,\n                                  count_parfor_OneDs,\n                                  count_parfor_REPs,\n                                  dist_IR_contains,\n                                  get_rank,\n                                  get_start_end,\n                                  skip_numba_jit,\n                                  skip_sdc_jit)\n\n\ndef get_np_state_ptr():\n    return numba._helperlib.rnd_get_np_state_ptr()\n\n\ndef _copy_py_state(r, ptr):\n    """"""\n    Copy state of Python random *r* to Numba state *ptr*.\n    """"""\n    mt = r.getstate()[1]\n    ints, index = mt[:-1], mt[-1]\n    numba._helperlib.rnd_set_state(ptr, (index, list(ints)))\n    return ints, index\n\n\nclass BaseTest(TestCase):\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.rank = self.jit(lambda: sdc.distributed_api.get_rank())()\n        self.num_ranks = self.jit(lambda: sdc.distributed_api.get_size())()\n\n    def _rank_begin(self, arr_len):\n        f = self.jit(\n            lambda arr_len, num_ranks, rank: sdc.distributed_api.get_start(\n                arr_len, np.int32(num_ranks), np.int32(rank)))\n        return f(arr_len, self.num_ranks, self.rank)\n\n    def _rank_end(self, arr_len):\n        f = self.jit(\n            lambda arr_len, num_ranks, rank: sdc.distributed_api.get_end(\n                arr_len, np.int32(num_ranks), np.int32(rank)))\n        return f(arr_len, self.num_ranks, self.rank)\n\n    def _rank_bounds(self, arr_len):\n        return self._rank_begin(arr_len), self._rank_end(arr_len)\n\n    def _follow_cpython(self, ptr, seed=2):\n        r = random.Random(seed)\n        _copy_py_state(r, ptr)\n        return r\n\n\nclass TestBasic(BaseTest):\n\n    @skip_numba_jit(""hang with numba.jit. ok with sdc.jit"")\n    def test_getitem(self):\n        def test_impl(N):\n            A = np.ones(N)\n            B = np.ones(N) > .5\n            C = A[B]\n            return C.sum()\n\n        hpat_func = self.jit(test_impl)\n        n = 128\n        np.testing.assert_allclose(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit(""Failed in nopython mode pipeline (step: Preprocessing for parfors)"")\n    def test_setitem1(self):\n        def test_impl(N):\n            A = np.arange(10) + 1.0\n            A[0] = 30\n            return A.sum()\n\n        hpat_func = self.jit(test_impl)\n        n = 128\n        np.testing.assert_allclose(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit(""Failed in nopython mode pipeline (step: Preprocessing for parfors)"")\n    def test_setitem2(self):\n        def test_impl(N):\n            A = np.arange(10) + 1.0\n            A[0:4] = 30\n            return A.sum()\n\n        hpat_func = self.jit(test_impl)\n        n = 128\n        np.testing.assert_allclose(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit(""hang with numba.jit. ok with sdc.jit"")\n    def test_astype(self):\n        def test_impl(N):\n            return np.ones(N).astype(np.int32).sum()\n\n        hpat_func = self.jit(test_impl)\n        n = 128\n        np.testing.assert_allclose(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    def test_shape(self):\n        def test_impl(N):\n            return np.ones(N).shape[0]\n\n        hpat_func = self.jit(test_impl)\n        n = 128\n        np.testing.assert_allclose(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n        # def test_impl(N):\n        #     return np.ones((N, 3, 4)).shape\n        #\n        # hpat_func = self.jit(test_impl)\n        # n = 128\n        # np.testing.assert_allclose(hpat_func(n), test_impl(n))\n        # self.assertEqual(count_array_REPs(), 0)\n        # self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit(""hang with numba.jit. ok with sdc.jit"")\n    def test_inplace_binop(self):\n        def test_impl(N):\n            A = np.ones(N)\n            B = np.ones(N)\n            B += A\n            return B.sum()\n\n        hpat_func = self.jit(test_impl)\n        n = 128\n        np.testing.assert_allclose(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit(""hang with numba.jit. ok with sdc.jit"")\n    def test_getitem_multidim(self):\n        def test_impl(N):\n            A = np.ones((N, 3))\n            B = np.ones(N) > .5\n            C = A[B, 2]\n            return C.sum()\n\n        hpat_func = self.jit(test_impl)\n        n = 128\n        np.testing.assert_allclose(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit(""Failed in nopython mode pipeline (step: Preprocessing for parfors)"")\n    def test_whole_slice(self):\n        def test_impl(N):\n            X = np.ones((N, 4))\n            X[:, 3] = (X[:, 3]) / (np.max(X[:, 3]) - np.min(X[:, 3]))\n            return X.sum()\n\n        hpat_func = self.jit(test_impl)\n        n = 128\n        np.testing.assert_allclose(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit(""hang with numba.jit. ok with sdc.jit"")\n    @skip_sdc_jit(\'Not implemented in sequential transport layer\')\n    def test_strided_getitem(self):\n        def test_impl(N):\n            A = np.ones(N)\n            B = A[::7]\n            return B.sum()\n\n        hpat_func = self.jit(test_impl)\n        n = 128\n        np.testing.assert_allclose(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    def test_assert(self):\n        # make sure assert in an inlined function works\n\n        def g(a):\n            assert a == 0\n\n        hpat_g = self.jit(g)\n\n        def f():\n            hpat_g(0)\n\n        hpat_f = self.jit(f)\n        hpat_f()\n\n    @skip_numba_jit\n    def test_inline_locals(self):\n        # make sure locals in inlined function works\n        @self.jit(locals={\'B\': types.float64[:]})\n        def g(S):\n            B = pd.to_numeric(S, errors=\'coerce\')\n            return B\n\n        def f():\n            return g(pd.Series([\'1.2\']))\n\n        pd.testing.assert_series_equal(self.jit(f)(), f())\n\n    @skip_numba_jit(""Failed in nopython mode pipeline (step: Preprocessing for parfors)"")\n    def test_reduce(self):\n        import sys\n        dtypes = [\'float32\', \'float64\', \'int32\', \'int64\']\n        funcs = [\'sum\', \'prod\', \'min\', \'max\', \'argmin\', \'argmax\']\n        for (dtype, func) in itertools.product(dtypes, funcs):\n            # loc allreduce doesn\'t support int64 on windows\n            if (sys.platform.startswith(\'win\')\n                    and dtype == \'int64\'\n                    and func in [\'argmin\', \'argmax\']):\n                continue\n            func_text = """"""def f(n):\n                A = np.arange(0, n, 1, np.{})\n                return A.{}()\n            """""".format(dtype, func)\n            loc_vars = {}\n            exec(func_text, {\'np\': np}, loc_vars)\n            test_impl = loc_vars[\'f\']\n\n            hpat_func = self.jit(test_impl)\n            n = 21  # XXX arange() on float32 has overflow issues on large n\n            np.testing.assert_almost_equal(hpat_func(n), test_impl(n))\n            self.assertEqual(count_array_REPs(), 0)\n            self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit\n    def test_reduce2(self):\n        import sys\n        dtypes = [\'float32\', \'float64\', \'int32\', \'int64\']\n        funcs = [\'sum\', \'prod\', \'min\', \'max\', \'argmin\', \'argmax\']\n        for (dtype, func) in itertools.product(dtypes, funcs):\n            # loc allreduce doesn\'t support int64 on windows\n            if (sys.platform.startswith(\'win\')\n                    and dtype == \'int64\'\n                    and func in [\'argmin\', \'argmax\']):\n                continue\n            func_text = """"""def f(A):\n                return A.{}()\n            """""".format(func)\n            loc_vars = {}\n            exec(func_text, {\'np\': np}, loc_vars)\n            test_impl = loc_vars[\'f\']\n\n            hpat_func = self.jit(locals={\'A:input\': \'distributed\'})(test_impl)\n            n = 21\n            start, end = get_start_end(n)\n            np.random.seed(0)\n            A = np.random.randint(0, 10, n).astype(dtype)\n            np.testing.assert_almost_equal(\n                hpat_func(A[start:end]), test_impl(A), decimal=3)\n            self.assertEqual(count_array_REPs(), 0)\n            self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit\n    @skip_sdc_jit(\'Not implemented in sequential transport layer\')\n    def test_reduce_filter1(self):\n        import sys\n        dtypes = [\'float32\', \'float64\', \'int32\', \'int64\']\n        funcs = [\'sum\', \'prod\', \'min\', \'max\', \'argmin\', \'argmax\']\n        for (dtype, func) in itertools.product(dtypes, funcs):\n            # loc allreduce doesn\'t support int64 on windows\n            if (sys.platform.startswith(\'win\')\n                    and dtype == \'int64\'\n                    and func in [\'argmin\', \'argmax\']):\n                continue\n            func_text = """"""def f(A):\n                A = A[A>5]\n                return A.{}()\n            """""".format(func)\n            loc_vars = {}\n            exec(func_text, {\'np\': np}, loc_vars)\n            test_impl = loc_vars[\'f\']\n\n            hpat_func = self.jit(locals={\'A:input\': \'distributed\'})(test_impl)\n            n = 21\n            start, end = get_start_end(n)\n            np.random.seed(0)\n            A = np.random.randint(0, 10, n).astype(dtype)\n            np.testing.assert_almost_equal(\n                hpat_func(A[start:end]), test_impl(A), decimal=3,\n                err_msg=""{} on {}"".format(func, dtype))\n            self.assertEqual(count_array_REPs(), 0)\n            self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit\n    @skip_sdc_jit(\'Not implemented in sequential transport layer\')\n    def test_array_reduce(self):\n        binops = [\'+=\', \'*=\', \'+=\', \'*=\', \'|=\', \'|=\']\n        dtypes = [\'np.float32\', \'np.float32\', \'np.float64\', \'np.float64\', \'np.int32\', \'np.int64\']\n        for (op, typ) in zip(binops, dtypes):\n            func_text = """"""def f(n):\n                  A = np.arange(0, 10, 1, {})\n                  B = np.arange(0 +  3, 10 + 3, 1, {})\n                  for i in numba.prange(n):\n                      A {} B\n                  return A\n            """""".format(typ, typ, op)\n            loc_vars = {}\n            exec(func_text, {\'np\': np, \'numba\': numba}, loc_vars)\n            test_impl = loc_vars[\'f\']\n\n            hpat_func = self.jit(test_impl)\n            n = 128\n            np.testing.assert_allclose(hpat_func(n), test_impl(n))\n            self.assertEqual(count_array_OneDs(), 0)\n            self.assertEqual(count_parfor_OneDs(), 1)\n\n    @unittest.expectedFailure  # https://github.com/numba/numba/issues/4690\n    def test_dist_return(self):\n        def test_impl(N):\n            A = np.arange(N)\n            return A\n\n        hpat_func = self.jit(locals={\'A:return\': \'distributed\'})(test_impl)\n        n = 128\n        dist_sum = self.jit(\n            lambda a: sdc.distributed_api.dist_reduce(\n                a, np.int32(sdc.distributed_api.Reduce_Type.Sum.value)))\n        dist_sum(1)  # run to compile\n        np.testing.assert_allclose(\n            dist_sum(hpat_func(n).sum()), test_impl(n).sum())\n        self.assertEqual(count_array_OneDs(), 1)\n        self.assertEqual(count_parfor_OneDs(), 1)\n\n    @unittest.expectedFailure  # https://github.com/numba/numba/issues/4690\n    def test_dist_return_tuple(self):\n        def test_impl(N):\n            A = np.arange(N)\n            B = np.arange(N) + 1.5\n            return A, B\n\n        hpat_func = self.jit(locals={\'A:return\': \'distributed\',\n                                     \'B:return\': \'distributed\'})(test_impl)\n        n = 128\n        dist_sum = self.jit(\n            lambda a: sdc.distributed_api.dist_reduce(\n                a, np.int32(sdc.distributed_api.Reduce_Type.Sum.value)))\n        dist_sum(1.0)  # run to compile\n        np.testing.assert_allclose(\n            dist_sum((hpat_func(n)[0] + hpat_func(n)[1]).sum()), (test_impl(n)[0] + test_impl(n)[1]).sum())\n        self.assertEqual(count_array_OneDs(), 2)\n        self.assertEqual(count_parfor_OneDs(), 2)\n\n    @skip_numba_jit\n    def test_dist_input(self):\n        def test_impl(A):\n            return len(A)\n\n        hpat_func = self.jit(distributed=[\'A\'])(test_impl)\n        n = 128\n        arr = np.ones(n)\n        np.testing.assert_allclose(hpat_func(arr) / self.num_ranks, test_impl(arr))\n        self.assertEqual(count_array_OneDs(), 1)\n\n    @skip_sdc_jit(\'Not implemented in sequential transport layer\')\n    @unittest.expectedFailure  # https://github.com/numba/numba/issues/4690\n    def test_rebalance(self):\n        def test_impl(N):\n            A = np.arange(n)\n            B = A[A > 10]\n            C = sdc.distributed_api.rebalance_array(B)\n            return C.sum()\n\n        try:\n            sdc.distributed_analysis.auto_rebalance = True\n            hpat_func = self.jit(test_impl)\n            n = 128\n            np.testing.assert_allclose(hpat_func(n), test_impl(n))\n            self.assertEqual(count_array_OneDs(), 3)\n            self.assertEqual(count_parfor_OneDs(), 2)\n        finally:\n            sdc.distributed_analysis.auto_rebalance = False\n\n    @skip_sdc_jit(\'Not implemented in sequential transport layer\')\n    @unittest.expectedFailure  # https://github.com/numba/numba/issues/4690\n    def test_rebalance_loop(self):\n        def test_impl(N):\n            A = np.arange(n)\n            B = A[A > 10]\n            s = 0\n            for i in range(3):\n                s += B.sum()\n            return s\n\n        try:\n            sdc.distributed_analysis.auto_rebalance = True\n            hpat_func = self.jit(test_impl)\n            n = 128\n            np.testing.assert_allclose(hpat_func(n), test_impl(n))\n            self.assertEqual(count_array_OneDs(), 4)\n            self.assertEqual(count_parfor_OneDs(), 2)\n            self.assertIn(\'allgather\', list(hpat_func.inspect_llvm().values())[0])\n        finally:\n            sdc.distributed_analysis.auto_rebalance = False\n\n    @skip_numba_jit(""Failed in nopython mode pipeline (step: Preprocessing for parfors)"")\n    def test_transpose(self):\n        def test_impl(n):\n            A = np.ones((30, 40, 50))\n            B = A.transpose((0, 2, 1))\n            C = A.transpose(0, 2, 1)\n            return B.sum() + C.sum()\n\n        hpat_func = self.jit(test_impl)\n        n = 128\n        np.testing.assert_allclose(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @unittest.skip(""Numba\'s perfmute generation needs to use np seed properly"")\n    def test_permuted_array_indexing(self):\n\n        # Since Numba uses Python\'s PRNG for producing random numbers in NumPy,\n        # we cannot compare against NumPy.  Therefore, we implement permutation\n        # in Python.\n        def python_permutation(n, r):\n            arr = np.arange(n)\n            r.shuffle(arr)\n            return arr\n\n        def test_one_dim(arr_len):\n            A = np.arange(arr_len)\n            B = np.copy(A)\n            P = np.random.permutation(arr_len)\n            A, B = A[P], B[P]\n            return A, B\n\n        # Implementation that uses Python\'s PRNG for producing a permutation.\n        # We test against this function.\n        def python_one_dim(arr_len, r):\n            A = np.arange(arr_len)\n            B = np.copy(A)\n            P = python_permutation(arr_len, r)\n            A, B = A[P], B[P]\n            return A, B\n\n        # Ideally, in above *_impl functions we should just call\n        # np.random.seed() and they should produce the same sequence of random\n        # numbers.  However, since Numba\'s PRNG uses NumPy\'s initialization\n        # method for initializing PRNG, we cannot just set seed.  Instead, we\n        # resort to this hack that generates a Python Random object with a fixed\n        # seed and copies the state to Numba\'s internal NumPy PRNG state.  For\n        # details please see https://github.com/numba/numba/issues/2782.\n        r = self._follow_cpython(get_np_state_ptr())\n\n        hpat_func1 = self.jit(locals={\'A:return\': \'distributed\',\n                                      \'B:return\': \'distributed\'})(test_one_dim)\n\n        # Test one-dimensional array indexing.\n        for arr_len in [11, 111, 128, 120]:\n            hpat_A, hpat_B = hpat_func1(arr_len)\n            python_A, python_B = python_one_dim(arr_len, r)\n            rank_bounds = self._rank_bounds(arr_len)\n            np.testing.assert_allclose(hpat_A, python_A[slice(*rank_bounds)])\n            np.testing.assert_allclose(hpat_B, python_B[slice(*rank_bounds)])\n\n        # Test two-dimensional array indexing.  Like in one-dimensional case\n        # above, in addition to NumPy version that is compiled by Numba, we\n        # implement a Python version.\n        def test_two_dim(arr_len):\n            first_dim = arr_len // 2\n            A = np.arange(arr_len).reshape(first_dim, 2)\n            B = np.copy(A)\n            P = np.random.permutation(first_dim)\n            A, B = A[P], B[P]\n            return A, B\n\n        def python_two_dim(arr_len, r):\n            first_dim = arr_len // 2\n            A = np.arange(arr_len).reshape(first_dim, 2)\n            B = np.copy(A)\n            P = python_permutation(first_dim, r)\n            A, B = A[P], B[P]\n            return A, B\n\n        hpat_func2 = self.jit(locals={\'A:return\': \'distributed\',\n                                      \'B:return\': \'distributed\'})(test_two_dim)\n\n        for arr_len in [18, 66, 128]:\n            hpat_A, hpat_B = hpat_func2(arr_len)\n            python_A, python_B = python_two_dim(arr_len, r)\n            rank_bounds = self._rank_bounds(arr_len // 2)\n            np.testing.assert_allclose(hpat_A, python_A[slice(*rank_bounds)])\n            np.testing.assert_allclose(hpat_B, python_B[slice(*rank_bounds)])\n\n        # Test that the indexed array is not modified if it is not being\n        # assigned to.\n        def test_rhs(arr_len):\n            A = np.arange(arr_len)\n            B = np.copy(A)\n            P = np.random.permutation(arr_len)\n            C = A[P]\n            return A, B, C\n\n        hpat_func3 = self.jit(locals={\'A:return\': \'distributed\',\n                                      \'B:return\': \'distributed\',\n                                      \'C:return\': \'distributed\'})(test_rhs)\n\n        for arr_len in [15, 23, 26]:\n            A, B, _ = hpat_func3(arr_len)\n            np.testing.assert_allclose(A, B)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
sdc/tests/test_d4p.py,4,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\ntry:\n    import daal4py as d4p\nexcept ImportError:\n    print(\'Ignoring daal4py tests.\')\nelse:\n    import numba\n    import numpy as np\n    import pandas as pd\n    import unittest\n    from math import sqrt\n\n    import sdc\n    from sdc.tests.test_base import TestCase\n    from sdc.tests.test_utils import (TestCase,\n                                      count_array_OneD_Vars,\n                                      count_array_OneDs,\n                                      count_array_REPs,\n                                      count_parfor_OneD_Vars,\n                                      count_parfor_OneDs,\n                                      count_parfor_REPs,\n                                      dist_IR_contains)\n\n\n    class TestD4P(TestCase):\n        def test_logistic_regression(self):\n            \'\'\'\n            Testing logistic regression including\n               * result and model boxing/unboxing\n               * optional and required arguments passing\n            \'\'\'\n            def train_impl(n, d):\n                X = np.ones((n, d), dtype=np.double) + .5\n                Y = np.ones((n, 1), dtype=np.double)\n                algo = d4p.logistic_regression_training(2,\n                                                        penaltyL1=0.1,\n                                                        penaltyL2=0.1,\n                                                        interceptFlag=True)\n                return algo.compute(X, Y)\n\n            def prdct_impl(n, d, model):\n                w = np.ones((n, d), dtype=np.double) - 22.5\n                algo = d4p.logistic_regression_prediction(\n                    2,\n                    resultsToCompute=""computeClassesLabels|computeClassesProbabilities|computeClassesLogProbabilities""\n                )\n                return algo.compute(w, model)\n\n            train_hpat = self.jit(train_impl)\n            prdct_hpat = self.jit(prdct_impl)\n            n = 11\n            d = 4\n            pred_impl = prdct_impl(n, d, train_impl(n, d).model).prediction\n            pred_hpat = prdct_hpat(n, d, train_hpat(n, d).model).prediction\n\n            np.testing.assert_allclose(pred_impl, pred_hpat)\n\n    if __name__ == ""__main__"":\n        unittest.main()\n'"
sdc/tests/test_dataframe.py,277,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numba\nimport numpy as np\nimport pandas as pd\nimport platform\nimport random\nimport string\nimport unittest\nfrom itertools import permutations, product\nfrom numba import types\nfrom numba.core.config import IS_32BITS\nfrom numba import literal_unroll\nfrom numba.core.errors import TypingError\nfrom pandas.core.indexing import IndexingError\n\nimport sdc\nfrom sdc.datatypes.common_functions import SDCLimitation\nfrom sdc.tests.gen_test_data import ParquetGenerator\nfrom sdc.tests.test_base import TestCase\nfrom sdc.tests.test_utils import (check_numba_version,\n                                  count_array_OneDs,\n                                  count_array_REPs,\n                                  count_parfor_OneDs,\n                                  count_parfor_REPs,\n                                  dist_IR_contains,\n                                  gen_df, gen_df_int_cols,\n                                  get_start_end,\n                                  skip_numba_jit,\n                                  skip_sdc_jit,\n                                  test_global_input_data_float64,\n                                  test_global_input_data_unicode_kind4)\n\n\n@sdc.jit\ndef inner_get_column(df):\n    # df2 = df[[\'A\', \'C\']]\n    # df2[\'D\'] = np.ones(3)\n    return df.A\n\n\nCOL_IND = 0\n\n\nclass TestDataFrame(TestCase):\n\n    # TODO: Data generator for DataFrames\n\n    def test_create1(self):\n        def test_impl(A, B):\n            df = pd.DataFrame({\'A\': A, \'B\': B})\n            return df\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        A = np.ones(n)\n        B = np.random.ranf(n)\n        pd.testing.assert_frame_equal(hpat_func(A, B), test_impl(A, B))\n\n    def test_create2(self):\n        def test_impl():\n            df = pd.DataFrame({\'A\': [1, 2, 3]})\n            return (df.A == 1).sum()\n        hpat_func = self.jit(test_impl)\n\n        self.assertEqual(hpat_func(), test_impl())\n\n    def test_create3(self):\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.arange(n)})\n            return (df.A == 2).sum()\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        self.assertEqual(hpat_func(n), test_impl(n))\n\n    def test_create_str(self):\n        def test_impl():\n            df = pd.DataFrame({\'A\': [\'a\', \'b\', \'c\']})\n            return (df.A == \'a\').sum()\n        hpat_func = self.jit(test_impl)\n\n        self.assertEqual(hpat_func(), test_impl())\n\n    def test_create_with_series1(self):\n        def test_impl(n):\n            A = pd.Series(np.ones(n, dtype=np.int64))\n            B = pd.Series(np.zeros(n, dtype=np.float64))\n            df = pd.DataFrame({\'A\': A, \'B\': B})\n            return df\n\n        hpat_func = sdc.jit(test_impl)\n        n = 11\n        pd.testing.assert_frame_equal(hpat_func(n), test_impl(n))\n\n    def test_create_with_series2(self):\n        # test creating dataframe from passed series\n        def test_impl(A):\n            df = pd.DataFrame({\'A\': A})\n            return (df.A == 2).sum()\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        df = pd.DataFrame({\'A\': np.arange(n)})\n        self.assertEqual(hpat_func(df.A), test_impl(df.A))\n\n    @skip_sdc_jit\n    def test_create_string_index(self):\n        def test_impl(a):\n            data = {\'A\': [\'a\', \'b\'], \'B\': [2, 3]}\n            df = pd.DataFrame(data=data, index=[\'A\', \'B\'])\n            return df\n\n        hpat_func = sdc.jit(test_impl)\n        pd.testing.assert_frame_equal(hpat_func(True), test_impl(True))\n\n    def test_create_cond1(self):\n        def test_impl(A, B, c):\n            if c:\n                df = pd.DataFrame({\'A\': A})\n            else:\n                df = pd.DataFrame({\'A\': B})\n            return df\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        A = np.ones(n)\n        B = np.arange(n) + 1.0\n        c = 0\n        pd.testing.assert_frame_equal(hpat_func(A, B, c), test_impl(A, B, c))\n        c = 2\n        pd.testing.assert_frame_equal(hpat_func(A, B, c), test_impl(A, B, c))\n\n    @unittest.skip(\'Implement feature to create DataFrame without column names\')\n    def test_create_without_column_names(self):\n        def test_impl():\n            df = pd.DataFrame([100, 200, 300, 400, 200, 100])\n            return df\n\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_frame_equal(hpat_func(), test_impl())\n\n    def test_pass_df1(self):\n        def test_impl(df):\n            return (df.A == 2).sum()\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        df = pd.DataFrame({\'A\': np.arange(n)})\n        self.assertEqual(hpat_func(df), test_impl(df))\n\n    def test_pass_df_str(self):\n        def test_impl(df):\n            return (df.A == \'a\').sum()\n        hpat_func = self.jit(test_impl)\n\n        df = pd.DataFrame({\'A\': [\'a\', \'b\', \'c\']})\n        self.assertEqual(hpat_func(df), test_impl(df))\n\n    def test_unbox1(self):\n        def test_impl(df):\n            return df\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        df = pd.DataFrame({\'A\': np.arange(n), \'B\': np.random.ranf(n)})\n        pd.testing.assert_frame_equal(hpat_func(df), test_impl(df))\n\n    @unittest.skip(""needs properly refcounted dataframes"")\n    def test_unbox2(self):\n        def test_impl(df, cond):\n            n = len(df)\n            if cond:\n                df[\'A\'] = np.arange(n) + 2.0\n            return df.A\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        df = pd.DataFrame({\'A\': np.ones(n), \'B\': np.random.ranf(n)})\n        pd.testing.assert_series_equal(hpat_func(df.copy(), True), test_impl(df.copy(), True))\n        pd.testing.assert_series_equal(hpat_func(df.copy(), False), test_impl(df.copy(), False))\n\n    @unittest.skip(\'Implement feature to create DataFrame without column names\')\n    def test_unbox_without_column_names(self):\n        def test_impl(df):\n            return df\n\n        df = pd.DataFrame([100, 200, 300, 400, 200, 100])\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_frame_equal(hpat_func(df), test_impl(df))\n\n    @skip_numba_jit\n    def test_box1(self):\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.ones(n), \'B\': np.arange(n)})\n            return df\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        do_check = False if platform.system() == \'Windows\' and not IS_32BITS else True\n        pd.testing.assert_frame_equal(hpat_func(n), test_impl(n), check_dtype=do_check)\n\n    def test_box2(self):\n        def test_impl():\n            df = pd.DataFrame({\'A\': [1, 2, 3], \'B\': [\'a\', \'bb\', \'ccc\']})\n            return df\n\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_frame_equal(hpat_func(), test_impl())\n\n    @skip_sdc_jit(""pending df filter support"")\n    def test_box3(self):\n        def test_impl(df):\n            df = df[df.A != \'dd\']\n            return df\n\n        hpat_func = self.jit(test_impl)\n        df = pd.DataFrame({\'A\': [\'aa\', \'bb\', \'cc\']})\n        pd.testing.assert_frame_equal(hpat_func(df), test_impl(df))\n\n    @skip_numba_jit\n    def test_box_categorical(self):\n        def test_impl(df):\n            df[\'A\'] = df[\'A\'] + 1\n            return df\n\n        hpat_func = self.jit(test_impl)\n        df = pd.DataFrame({\'A\': [1, 2, 3],\n                           \'B\': pd.Series([\'N\', \'Y\', \'Y\'],\n                                          dtype=pd.api.types.CategoricalDtype([\'N\', \'Y\']))})\n        pd.testing.assert_frame_equal(hpat_func(df.copy(deep=True)), test_impl(df))\n\n    @unittest.expectedFailure  # https://github.com/numba/numba/issues/4690\n    def test_box_dist_return(self):\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.ones(n), \'B\': np.arange(n)})\n            return df\n\n        hpat_func = self.jit(distributed={\'df\'})(test_impl)\n        n = 11\n        hres, res = hpat_func(n), test_impl(n)\n        self.assertEqual(count_array_OneDs(), 3)\n        self.assertEqual(count_parfor_OneDs(), 2)\n        dist_sum = self.jit(\n            lambda a: sdc.distributed_api.dist_reduce(\n                a, np.int32(sdc.distributed_api.Reduce_Type.Sum.value)))\n        dist_sum(1)  # run to compile\n        np.testing.assert_allclose(dist_sum(hres.A.sum()), res.A.sum())\n        np.testing.assert_allclose(dist_sum(hres.B.sum()), res.B.sum())\n\n    @skip_numba_jit\n    def test_len1(self):\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.ones(n, np.int64), \'B\': np.random.ranf(n)})\n            return len(df)\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        self.assertEqual(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit\n    def test_shape1(self):\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.ones(n, np.int64), \'B\': np.random.ranf(n)})\n            return df.shape\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        self.assertEqual(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit\n    def test_column_getitem1(self):\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.ones(n), \'B\': np.random.ranf(n)})\n            Ac = df[\'A\'].values\n            return Ac.sum()\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        self.assertEqual(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n        self.assertEqual(count_parfor_OneDs(), 1)\n\n    @skip_numba_jit\n    def test_column_list_getitem1(self):\n        def test_impl(df):\n            return df[[\'A\', \'C\']]\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        df = pd.DataFrame(\n            {\'A\': np.arange(n), \'B\': np.ones(n), \'C\': np.random.ranf(n)})\n        pd.testing.assert_frame_equal(hpat_func(df), test_impl(df))\n\n    def test_filter1(self):\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.arange(n) + n, \'B\': np.arange(n)**2})\n            df1 = df[df.A > .5]\n            return df1.B.sum()\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        self.assertEqual(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit(\'np.sum of Series unsupported\')\n    def test_filter2(self):\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.arange(n) + n, \'B\': np.arange(n)**2})\n            df1 = df.loc[df.A > .5]\n            return np.sum(df1.B)\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        self.assertEqual(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit(\'np.sum of Series unsupported\')\n    def test_filter3(self):\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.arange(n) + n, \'B\': np.arange(n)**2})\n            df1 = df.iloc[(df.A > .5).values]\n            return np.sum(df1.B)\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        self.assertEqual(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit\n    def test_iloc1(self):\n        def test_impl(df, n):\n            return df.iloc[1:n].B.values\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        df = pd.DataFrame({\'A\': np.arange(n), \'B\': np.arange(n)**2})\n        np.testing.assert_array_equal(hpat_func(df, n), test_impl(df, n))\n\n    @skip_numba_jit\n    def test_iloc2(self):\n        def test_impl(df, n):\n            return df.iloc[np.array([1, 4, 9])].B.values\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        df = pd.DataFrame({\'A\': np.arange(n), \'B\': np.arange(n)**2})\n        np.testing.assert_array_equal(hpat_func(df, n), test_impl(df, n))\n\n    @skip_numba_jit\n    def test_iloc3(self):\n        def test_impl(df):\n            return df.iloc[:, 1].values\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        df = pd.DataFrame({\'A\': np.arange(n), \'B\': np.arange(n)**2})\n        np.testing.assert_array_equal(hpat_func(df), test_impl(df))\n\n    @unittest.skip(""TODO: support A[[1,2,3]] in Numba"")\n    def test_iloc4(self):\n        def test_impl(df, n):\n            return df.iloc[[1, 4, 9]].B.values\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        df = pd.DataFrame({\'A\': np.arange(n), \'B\': np.arange(n)**2})\n        np.testing.assert_array_equal(hpat_func(df, n), test_impl(df, n))\n\n    @skip_numba_jit\n    def test_iloc5(self):\n        # test iloc with global value\n        def test_impl(df):\n            return df.iloc[:, COL_IND].values\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        df = pd.DataFrame({\'A\': np.arange(n), \'B\': np.arange(n)**2})\n        np.testing.assert_array_equal(hpat_func(df), test_impl(df))\n\n    @skip_numba_jit\n    def test_loc1(self):\n        def test_impl(df):\n            return df.loc[:, \'B\'].values\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        df = pd.DataFrame({\'A\': np.arange(n), \'B\': np.arange(n)**2})\n        np.testing.assert_array_equal(hpat_func(df), test_impl(df))\n\n    @skip_numba_jit\n    def test_iat1(self):\n        def test_impl(n):\n            df = pd.DataFrame({\'B\': np.ones(n), \'A\': np.arange(n) + n})\n            return df.iat[3, 1]\n        hpat_func = self.jit(test_impl)\n        n = 11\n        self.assertEqual(hpat_func(n), test_impl(n))\n\n    @skip_numba_jit\n    def test_iat2(self):\n        def test_impl(df):\n            return df.iat[3, 1]\n        hpat_func = self.jit(test_impl)\n        n = 11\n        df = pd.DataFrame({\'B\': np.ones(n), \'A\': np.arange(n) + n})\n        self.assertEqual(hpat_func(df), test_impl(df))\n\n    @skip_numba_jit\n    def test_iat3(self):\n        def test_impl(df, n):\n            return df.iat[n - 1, 1]\n        hpat_func = self.jit(test_impl)\n        n = 11\n        df = pd.DataFrame({\'B\': np.ones(n), \'A\': np.arange(n) + n})\n        self.assertEqual(hpat_func(df, n), test_impl(df, n))\n\n    @skip_numba_jit\n    def test_iat_set1(self):\n        def test_impl(df, n):\n            df.iat[n - 1, 1] = n**2\n            return df.A  # return the column to check column aliasing\n        hpat_func = self.jit(test_impl)\n        n = 11\n        df = pd.DataFrame({\'B\': np.ones(n), \'A\': np.arange(n) + n})\n        df2 = df.copy()\n        pd.testing.assert_series_equal(hpat_func(df, n), test_impl(df2, n))\n\n    @skip_numba_jit\n    def test_iat_set2(self):\n        def test_impl(df, n):\n            df.iat[n - 1, 1] = n**2\n            return df  # check df aliasing/boxing\n        hpat_func = self.jit(test_impl)\n        n = 11\n        df = pd.DataFrame({\'B\': np.ones(n), \'A\': np.arange(n) + n})\n        df2 = df.copy()\n        pd.testing.assert_frame_equal(hpat_func(df, n), test_impl(df2, n))\n\n    @skip_numba_jit\n    def test_set_column1(self):\n        # set existing column\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.ones(n, np.int64), \'B\': np.arange(n) + 3.0})\n            df[\'A\'] = np.arange(n)\n            return df\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        do_check = False if platform.system() == \'Windows\' and not IS_32BITS else True\n        pd.testing.assert_frame_equal(hpat_func(n), test_impl(n), check_dtype=do_check)\n\n    @skip_numba_jit\n    def test_set_column_reflect4(self):\n        # set existing column\n        def test_impl(df, n):\n            df[\'A\'] = np.arange(n)\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        df1 = pd.DataFrame({\'A\': np.ones(n, np.int64), \'B\': np.arange(n) + 3.0})\n        df2 = df1.copy()\n        hpat_func(df1, n)\n        test_impl(df2, n)\n        do_check = False if platform.system() == \'Windows\' and not IS_32BITS else True\n        pd.testing.assert_frame_equal(df1, df2, check_dtype=do_check)\n\n    @skip_numba_jit\n    def test_set_column_new_type1(self):\n        # set existing column with a new type\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.ones(n), \'B\': np.arange(n) + 3.0})\n            df[\'A\'] = np.arange(n)\n            return df\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        do_check = False if platform.system() == \'Windows\' and not IS_32BITS else True\n        pd.testing.assert_frame_equal(hpat_func(n), test_impl(n), check_dtype=do_check)\n\n    @skip_numba_jit\n    def test_set_column2(self):\n        # create new column\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.ones(n), \'B\': np.arange(n) + 1.0})\n            df[\'C\'] = np.arange(n)\n            return df\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        do_check = False if platform.system() == \'Windows\' and not IS_32BITS else True\n        pd.testing.assert_frame_equal(hpat_func(n), test_impl(n), check_dtype=do_check)\n\n    @skip_numba_jit\n    def test_set_column_reflect3(self):\n        # create new column\n        def test_impl(df, n):\n            df[\'C\'] = np.arange(n)\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        df1 = pd.DataFrame({\'A\': np.ones(n, np.int64), \'B\': np.arange(n) + 3.0})\n        df2 = df1.copy()\n        hpat_func(df1, n)\n        test_impl(df2, n)\n        do_check = False if platform.system() == \'Windows\' and not IS_32BITS else True\n        pd.testing.assert_frame_equal(df1, df2, check_dtype=do_check)\n\n    @skip_numba_jit\n    def test_set_column_bool1(self):\n        def test_impl(df):\n            df[\'C\'] = df[\'A\'][df[\'B\']]\n\n        hpat_func = self.jit(test_impl)\n        df = pd.DataFrame({\'A\': [1, 2, 3], \'B\': [True, False, True]})\n        df2 = df.copy()\n        test_impl(df2)\n        hpat_func(df)\n        pd.testing.assert_series_equal(df.C, df2.C)\n\n    @skip_numba_jit\n    def test_set_column_reflect1(self):\n        def test_impl(df, arr):\n            df[\'C\'] = arr\n            return df.C.sum()\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        arr = np.random.ranf(n)\n        df = pd.DataFrame({\'A\': np.ones(n), \'B\': np.random.ranf(n)})\n        hpat_func(df, arr)\n        self.assertIn(\'C\', df)\n        np.testing.assert_almost_equal(df.C.values, arr)\n\n    @skip_numba_jit\n    def test_set_column_reflect2(self):\n        def test_impl(df, arr):\n            df[\'C\'] = arr\n            return df.C.sum()\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        arr = np.random.ranf(n)\n        df = pd.DataFrame({\'A\': np.ones(n), \'B\': np.random.ranf(n)})\n        df2 = df.copy()\n        np.testing.assert_almost_equal(hpat_func(df, arr), test_impl(df2, arr))\n\n    def _test_df_set_column(self, all_data, key, value):\n        def gen_test_impl(value, do_jit=False):\n            if isinstance(value, pd.Series):\n                def test_impl(df, key, value):\n                    if do_jit == True:  # noqa\n                        return df._set_column(key, value.values)\n                    else:\n                        df[key] = value.values\n            else:\n                def test_impl(df, key, value):\n                    if do_jit == True:  # noqa\n                        return df._set_column(key, value)\n                    else:\n                        df[key] = value\n\n            return test_impl\n\n        test_impl = gen_test_impl(value)\n        sdc_func = self.jit(gen_test_impl(value, do_jit=True))\n\n        for data in all_data:\n            with self.subTest(data=data):\n                df1 = pd.DataFrame(data)\n                df2 = df1.copy(deep=True)\n                test_impl(df1, key, value)\n                result_ref = df1  # in pandas setitem modifies original DF\n                result_jit = sdc_func(df2, key, value)\n                pd.testing.assert_frame_equal(result_jit, result_ref)\n\n    def _test_df_set_column_exception_invalid_length(self, df, key, value):\n        def test_impl(df, key, value):\n            return df._set_column(key, value)\n\n        sdc_func = self.jit(test_impl)\n\n        with self.assertRaises(ValueError) as raises:\n            sdc_func(df, key, value)\n        msg = \'Length of values does not match length of index\'\n        self.assertIn(msg, str(raises.exception))\n\n    def _test_df_set_column_exception_empty_columns(self, df, key, value):\n        def test_impl(df, key, value):\n            return df._set_column(key, value)\n\n        sdc_func = self.jit(test_impl)\n\n        with self.assertRaises(SDCLimitation) as raises:\n            sdc_func(df, key, value)\n        msg = \'Could not set item for DataFrame with empty columns\'\n        self.assertIn(msg, str(raises.exception))\n\n    def test_df_add_column(self):\n        all_data = [{\'A\': [0, 1, 2], \'C\': [0., np.nan, np.inf]}, {}]\n        key, value = \'B\', np.array([1., -1., 0.])\n\n        self._test_df_set_column(all_data, key, value)\n\n    def test_df_add_column_str(self):\n        all_data = [{\'A\': [0, 1, 2], \'C\': [0., np.nan, np.inf]}, {}]\n        key, value = \'B\', pd.Series(test_global_input_data_unicode_kind4)\n\n        self._test_df_set_column(all_data, key, value)\n\n    def test_df_add_column_exception_invalid_length(self):\n        df = pd.DataFrame({\'A\': [0, 1, 2], \'C\': [3., 4., 5.]})\n        key, value = \'B\', np.array([1., np.nan, -1., 0.])\n        self._test_df_set_column_exception_invalid_length(df, key, value)\n\n        df = pd.DataFrame({\'A\': []})\n        self._test_df_set_column_exception_empty_columns(df, key, value)\n\n    def test_df_replace_column(self):\n        all_data = [{\'A\': [0, 1, 2], \'C\': [0., np.nan, np.inf]}]\n        key, value = \'A\', np.array([1., -1., 0.])\n\n        self._test_df_set_column(all_data, key, value)\n\n    def test_df_replace_column_str(self):\n        all_data = [{\'A\': [0, 1, 2], \'C\': [0., np.nan, np.inf]}]\n        key, value = \'A\', pd.Series(test_global_input_data_unicode_kind4)\n\n        self._test_df_set_column(all_data, key, value)\n\n    def test_df_replace_column_exception_invalid_length(self):\n        df = pd.DataFrame({\'A\': [0, 1, 2], \'C\': [3., 4., 5.]})\n        key, value = \'A\', np.array([1., np.nan, -1., 0.])\n        self._test_df_set_column_exception_invalid_length(df, key, value)\n\n        df = pd.DataFrame({\'A\': []})\n        self._test_df_set_column_exception_empty_columns(df, key, value)\n\n    def _test_df_values_unboxing(self, df):\n        def test_impl(df):\n            return df.values\n\n        sdc_func = self.jit(test_impl)\n        np.testing.assert_array_equal(sdc_func(df), test_impl(df))\n\n    def test_df_values_unboxing(self):\n        values_to_test = [[1, 2, 3, 4, 5],\n                          [.1, .2, .3, .4, .5],\n                          [np.nan, np.inf, .0, .1, -1.]]\n        n = 5\n        np.random.seed(0)\n        A = np.ones(n)\n        B = np.random.ranf(n)\n\n        for values in values_to_test:\n            with self.subTest(values=values):\n                df = pd.DataFrame({\'A\': A, \'B\': B, \'C D E\': values})\n                self._test_df_values_unboxing(df)\n\n    def test_df_values(self):\n        def test_impl(n, values):\n            df = pd.DataFrame({\'A\': np.ones(n), \'B\': np.arange(n), \'C\': values})\n            return df.values\n\n        sdc_func = self.jit(test_impl)\n        n = 5\n        values_to_test = [[1, 2, 3, 4, 5],\n                          [.1, .2, .3, .4, .5],\n                          [np.nan, np.inf, .0, .1, -1.]]\n\n        for values in values_to_test:\n            with self.subTest(values=values):\n                np.testing.assert_array_equal(sdc_func(n, values), test_impl(n, values))\n\n    @skip_numba_jit\n    def test_df_values_parallel1(self):\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.ones(n), \'B\': np.arange(n)})\n            return df.values.sum()\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        np.testing.assert_array_equal(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    def _test_df_index(self, df):\n        def test_impl(df):\n            return df.index\n\n        sdc_func = self.jit(test_impl)\n        np.testing.assert_array_equal(sdc_func(df), test_impl(df))\n\n    @skip_sdc_jit\n    def test_index_attribute(self):\n        index_to_test = [[1, 2, 3, 4, 5],\n                         [.1, .2, .3, .4, .5],\n                         [\'a\', \'b\', \'c\', \'d\', \'e\']]\n        n = 5\n        np.random.seed(0)\n        A = np.ones(n)\n        B = np.random.ranf(n)\n\n        for index in index_to_test:\n            with self.subTest(index=index):\n                df = pd.DataFrame({\'A\': A, \'B\': B}, index=index)\n                self._test_df_index(df)\n\n    @skip_sdc_jit\n    def test_index_attribute_empty(self):\n        n = 5\n        np.random.seed(0)\n        A = np.ones(n)\n        B = np.random.ranf(n)\n        df = pd.DataFrame({\'A\': A, \'B\': B})\n\n        self._test_df_index(df)\n\n    @skip_sdc_jit\n    def test_index_attribute_empty_df(self):\n        df = pd.DataFrame()\n        self._test_df_index(df)\n\n    def test_index_attribute_no_unboxing(self):\n        def test_impl(n, index):\n            np.random.seed(0)\n            df = pd.DataFrame({\n                \'A\': np.ones(n),\n                \'B\': np.random.ranf(n)\n            }, index=index)\n            return df.index\n\n        sdc_impl = self.jit(test_impl)\n        index_to_test = [\n            [1, 2, 3, 4, 5],\n            [.1, .2, .3, .4, .5],\n            [\'a\', \'b\', \'c\', \'d\', \'e\']\n        ]\n        for index in index_to_test:\n            with self.subTest(index=index):\n                n = len(index)\n                jit_result = sdc_impl(n, index)\n                ref_result = test_impl(n, index)\n                np.testing.assert_array_equal(jit_result, ref_result)\n\n    def test_index_attribute_default_no_unboxing(self):\n        def test_impl(n):\n            np.random.seed(0)\n            df = pd.DataFrame({\n                \'A\': np.ones(n),\n                \'B\': np.random.ranf(n)\n            })\n            return df.index\n\n        sdc_impl = self.jit(test_impl)\n        np.testing.assert_array_equal(sdc_impl(10), test_impl(10))\n\n    @skip_sdc_jit\n    @skip_numba_jit\n    def test_df_apply(self):\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.arange(n), \'B\': np.arange(n)})\n            B = df.apply(lambda r: r.A + r.B, axis=1)\n            return df.B.sum()\n\n        n = 121\n        hpat_func = self.jit(test_impl)\n        np.testing.assert_almost_equal(hpat_func(n), test_impl(n))\n\n    @skip_sdc_jit\n    @skip_numba_jit\n    def test_df_apply_branch(self):\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.arange(n), \'B\': np.arange(n)})\n            B = df.apply(lambda r: r.A < 10 and r.B > 20, axis=1)\n            return df.B.sum()\n\n        n = 121\n        hpat_func = self.jit(test_impl)\n        np.testing.assert_almost_equal(hpat_func(n), test_impl(n))\n\n    @skip_numba_jit\n    @skip_sdc_jit(\'Not implemented in sequential transport layer\')\n    def test_df_describe(self):\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.arange(0, n, 1, np.float32),\n                               \'B\': np.arange(n)})\n            #df.A[0:1] = np.nan\n            return df.describe()\n\n        hpat_func = self.jit(test_impl)\n        n = 1001\n        hpat_func(n)\n        # XXX: test actual output\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit\n    def test_sort_values(self):\n        def test_impl(df):\n            df.sort_values(\'A\', inplace=True)\n            return df.B.values\n\n        n = 1211\n        np.random.seed(2)\n        df = pd.DataFrame({\'A\': np.random.ranf(n), \'B\': np.arange(n), \'C\': np.random.ranf(n)})\n        hpat_func = self.jit(test_impl)\n        np.testing.assert_almost_equal(hpat_func(df.copy()), test_impl(df))\n\n    @skip_numba_jit\n    def test_sort_values_copy(self):\n        def test_impl(df):\n            df2 = df.sort_values(\'A\')\n            return df2.B.values\n\n        n = 1211\n        np.random.seed(2)\n        df = pd.DataFrame({\'A\': np.random.ranf(n), \'B\': np.arange(n), \'C\': np.random.ranf(n)})\n        hpat_func = self.jit(test_impl)\n        np.testing.assert_almost_equal(hpat_func(df.copy()), test_impl(df))\n\n    @skip_numba_jit\n    def test_sort_values_single_col(self):\n        def test_impl(df):\n            df.sort_values(\'A\', inplace=True)\n            return df.A.values\n\n        n = 1211\n        np.random.seed(2)\n        df = pd.DataFrame({\'A\': np.random.ranf(n)})\n        hpat_func = self.jit(test_impl)\n        np.testing.assert_almost_equal(hpat_func(df.copy()), test_impl(df))\n\n    @skip_numba_jit\n    def test_sort_values_single_col_str(self):\n        def test_impl(df):\n            df.sort_values(\'A\', inplace=True)\n            return df.A.values\n\n        n = 1211\n        random.seed(2)\n        str_vals = []\n\n        for _ in range(n):\n            k = random.randint(1, 30)\n            val = \'\'.join(random.choices(string.ascii_uppercase + string.digits, k=k))\n            str_vals.append(val)\n        df = pd.DataFrame({\'A\': str_vals})\n        hpat_func = self.jit(test_impl)\n        self.assertTrue((hpat_func(df.copy()) == test_impl(df)).all())\n\n    @skip_numba_jit\n    def test_sort_values_str(self):\n        def test_impl(df):\n            df.sort_values(\'A\', inplace=True)\n            return df.B.values\n\n        n = 1211\n        random.seed(2)\n        str_vals = []\n        str_vals2 = []\n\n        for i in range(n):\n            k = random.randint(1, 30)\n            val = \'\'.join(random.choices(string.ascii_uppercase + string.digits, k=k))\n            str_vals.append(val)\n            val = \'\'.join(random.choices(string.ascii_uppercase + string.digits, k=k))\n            str_vals2.append(val)\n\n        df = pd.DataFrame({\'A\': str_vals, \'B\': str_vals2})\n        # use mergesort for stability, in str generation equal keys are more probable\n        sorted_df = df.sort_values(\'A\', inplace=False, kind=\'mergesort\')\n        hpat_func = self.jit(test_impl)\n        self.assertTrue((hpat_func(df) == sorted_df.B.values).all())\n\n    @skip_numba_jit\n    def test_sort_parallel_single_col(self):\n        # create `kde.parquet` file\n        ParquetGenerator.gen_kde_pq()\n\n        # TODO: better parallel sort test\n        def test_impl():\n            df = pd.read_parquet(\'kde.parquet\')\n            df.sort_values(\'points\', inplace=True)\n            res = df.points.values\n            return res\n\n        hpat_func = self.jit(locals={\'res:return\': \'distributed\'})(test_impl)\n\n        save_min_samples = sdc.hiframes.sort.MIN_SAMPLES\n        try:\n            sdc.hiframes.sort.MIN_SAMPLES = 10\n            res = hpat_func()\n            self.assertTrue((np.diff(res) >= 0).all())\n        finally:\n            # restore global val\n            sdc.hiframes.sort.MIN_SAMPLES = save_min_samples\n\n    @skip_numba_jit\n    def test_df_isna1(self):\n        \'\'\'Verify DataFrame.isna implementation for various types of data\'\'\'\n        def test_impl(df):\n            return df.isna()\n        hpat_func = self.jit(test_impl)\n\n        # TODO: add column with datetime values when test_series_datetime_isna1 is fixed\n        df = pd.DataFrame({\'A\': [1.0, 2.0, np.nan, 1.0],\n                           \'B\': [np.inf, 5, np.nan, 6],\n                           \'C\': [\'aa\', \'b\', None, \'ccc\'],\n                           \'D\': [None, \'dd\', \'\', None]})\n        pd.testing.assert_frame_equal(hpat_func(df), test_impl(df))\n\n    def test_df_isna(self):\n        def test_impl(df):\n            return df.isna()\n\n        sdc_func = sdc.jit(test_impl)\n        indexes = [[3, 4, 2, 6, 1], [\'a\', \'b\', \'c\', \'d\', \'e\'], None]\n\n        for idx in indexes:\n            df = pd.DataFrame({""A"": [3.2, np.nan, 7.0, 3.3, np.nan],\n                               ""B"": [3, 4, 1, 0, 222],\n                               ""C"": [True, True, False, False, True],\n                               ""D"": [\'a\', \'dd\', \'c\', \'12\', None]}, index=idx)\n            with self.subTest(index=idx):\n                pd.testing.assert_frame_equal(sdc_func(df), test_impl(df))\n\n    def test_df_isna_no_unboxing(self):\n        def test_impl():\n            df = pd.DataFrame({\n                ""A"": [3.2, np.nan, 7.0, 3.3, np.nan],\n                ""B"": [3, 4, 1, 0, 222],\n                ""C"": [True, True, False, False, True],\n                ""D"": [\'a\', \'dd\', \'c\', \'12\', None]\n            }, index=[3, 4, 2, 6, 1])\n            return df.isna()\n\n        sdc_func = sdc.jit(test_impl)\n        pd.testing.assert_frame_equal(sdc_func(), test_impl())\n\n    @unittest.skip(\'DF with column named ""bool"" Segmentation fault\')\n    def test_df_bool(self):\n        def test_impl(df):\n            return df.isna()\n\n        sdc_func = sdc.jit(test_impl)\n        df = pd.DataFrame({""bool"": [True, True, False, False, True]}, index=None)\n        pd.testing.assert_frame_equal(sdc_func(df), test_impl(df))\n\n    @skip_numba_jit\n    def test_df_astype_str1(self):\n        \'\'\'Verifies DataFrame.astype implementation converting various types to string\'\'\'\n        def test_impl(df):\n            return df.astype(str)\n        hpat_func = self.jit(test_impl)\n\n        # TODO: add column with float values when test_series_astype_float_to_str1 is fixed\n        df = pd.DataFrame({\'A\': [-1, 2, 11, 5, 0, -7],\n                           \'B\': [\'aa\', \'bb\', \'cc\', \'dd\', \'\', \'fff\']\n                           })\n        pd.testing.assert_frame_equal(hpat_func(df), test_impl(df))\n\n    @skip_numba_jit\n    def test_df_astype_float1(self):\n        \'\'\'Verifies DataFrame.astype implementation converting various types to float\'\'\'\n        def test_impl(df):\n            return df.astype(np.float64)\n        hpat_func = self.jit(test_impl)\n\n        # TODO: uncomment column with string values when test_series_astype_str_to_float64 is fixed\n        df = pd.DataFrame({\'A\': [-1, 2, 11, 5, 0, -7],\n                           #                   \'B\': [\'3.24\', \'1E+05\', \'-1\', \'-1.3E-01\', \'nan\', \'inf\'],\n                           \'C\': [3.24, 1E+05, -1, -1.3E-01, np.nan, np.inf]\n                           })\n        pd.testing.assert_frame_equal(hpat_func(df), test_impl(df))\n\n    @skip_numba_jit\n    def test_df_astype_int1(self):\n        \'\'\'Verifies DataFrame.astype implementation converting various types to int\'\'\'\n        def test_impl(df):\n            return df.astype(np.int32)\n        hpat_func = self.jit(test_impl)\n\n        n = 6\n        # TODO: uncomment column with string values when test_series_astype_str_to_int32 is fixed\n        df = pd.DataFrame({\'A\': np.ones(n, dtype=np.int64),\n                           \'B\': np.arange(n, dtype=np.int32),\n                           #                   \'C\': [\'-1\', \'2\', \'3\', \'0\', \'-7\', \'99\'],\n                           \'D\': np.arange(float(n), dtype=np.float32)\n                           })\n        pd.testing.assert_frame_equal(hpat_func(df), test_impl(df))\n\n    @skip_numba_jit\n    def test_sort_parallel(self):\n        # create `kde.parquet` file\n        ParquetGenerator.gen_kde_pq()\n\n        # TODO: better parallel sort test\n        def test_impl():\n            df = pd.read_parquet(\'kde.parquet\')\n            df[\'A\'] = df.points.astype(np.float64)\n            df.sort_values(\'points\', inplace=True)\n            res = df.A.values\n            return res\n\n        hpat_func = self.jit(locals={\'res:return\': \'distributed\'})(test_impl)\n\n        save_min_samples = sdc.hiframes.sort.MIN_SAMPLES\n        try:\n            sdc.hiframes.sort.MIN_SAMPLES = 10\n            res = hpat_func()\n            self.assertTrue((np.diff(res) >= 0).all())\n        finally:\n            # restore global val\n            sdc.hiframes.sort.MIN_SAMPLES = save_min_samples\n\n    @skip_numba_jit\n    def test_itertuples(self):\n        def test_impl(df):\n            res = 0.0\n            for r in df.itertuples():\n                res += r[1]\n            return res\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        df = pd.DataFrame({\'A\': np.arange(n), \'B\': np.ones(n, np.int64)})\n        self.assertEqual(hpat_func(df), test_impl(df))\n\n    @skip_numba_jit\n    def test_itertuples_str(self):\n        def test_impl(df):\n            res = """"\n            for r in df.itertuples():\n                res += r[1]\n            return res\n\n        hpat_func = self.jit(test_impl)\n        n = 3\n        df = pd.DataFrame({\'A\': [\'aa\', \'bb\', \'cc\'], \'B\': np.ones(n, np.int64)})\n        self.assertEqual(hpat_func(df), test_impl(df))\n\n    @skip_numba_jit\n    def test_itertuples_order(self):\n        def test_impl(n):\n            res = 0.0\n            df = pd.DataFrame({\'B\': np.arange(n), \'A\': np.ones(n, np.int64)})\n            for r in df.itertuples():\n                res += r[1]\n            return res\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        self.assertEqual(hpat_func(n), test_impl(n))\n\n    @skip_numba_jit\n    def test_itertuples_analysis(self):\n        """"""tests array analysis handling of generated tuples, shapes going\n        through blocks and getting used in an array dimension\n        """"""\n        def test_impl(n):\n            res = 0\n            df = pd.DataFrame({\'B\': np.arange(n), \'A\': np.ones(n, np.int64)})\n            for r in df.itertuples():\n                if r[1] == 2:\n                    A = np.ones(r[1])\n                    res += len(A)\n            return res\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        self.assertEqual(hpat_func(n), test_impl(n))\n\n    @skip_numba_jit\n    @unittest.skipIf(platform.system() == \'Windows\', ""Attribute \'dtype\' are different int64 and int32"")\n    def test_df_head1(self):\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.ones(n), \'B\': np.arange(n)})\n            return df.head(3)\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        pd.testing.assert_frame_equal(hpat_func(n), test_impl(n))\n\n    def test_df_head_unbox(self):\n        def test_impl(df, n):\n            return df.head(n)\n        sdc_func = sdc.jit(test_impl)\n        for n in [-3, 0, 3, 5, None]:\n            for idx in [[3, 4, 2, 6, 1], None]:\n                df = pd.DataFrame({""float"": [3.2, 4.4, 7.0, 3.3, 1.0],\n                                   ""int"": [3, 4, 1, 0, 222],\n                                   ""string"": [\'a\', \'dd\', \'c\', \'12\', \'ddf\']}, index=idx)\n                with self.subTest(n=n, index=idx):\n                    pd.testing.assert_frame_equal(sdc_func(df, n), test_impl(df, n))\n\n    def test_df_iloc_slice(self):\n        def test_impl(df, n, k):\n            return df.iloc[n:k]\n        sdc_func = sdc.jit(test_impl)\n        cases_idx = [[3, 4, 2, 6, 1], None]\n        cases_n = [-10, 0, 8, None]\n        for idx in cases_idx:\n            df = pd.DataFrame({""A"": [3.2, 4.4, 7.0, 3.3, 1.0],\n                               ""B"": [5.5, np.nan, 3, 0, 7.7],\n                               ""C"": [3, 4, 1, 0, 222]}, index=idx)\n            for n, k in product(cases_n, cases_n[::-1]):\n                with self.subTest(index=idx, n=n, k=k):\n                    pd.testing.assert_frame_equal(sdc_func(df, n, k), test_impl(df, n, k))\n\n    def test_df_iloc_slice_no_unboxing(self):\n        def test_impl(n, k):\n            df = pd.DataFrame({\n                \'A\': [3.2, 4.4, 7.0, 3.3, 1.0],\n                \'B\': [5.5, np.nan, 3, 0, 7.7],\n                \'C\': [3, 4, 1, 0, 222],\n            }, index=[3, 4, 2, 6, 1])\n            return df.iloc[n:k]\n\n        sdc_func = sdc.jit(test_impl)\n        cases_n = [-10, 0, 8, None]\n        for n, k in product(cases_n, cases_n[::-1]):\n            with self.subTest(n=n, k=k):\n                pd.testing.assert_frame_equal(sdc_func(n, k), test_impl(n, k))\n\n    def test_df_iloc_values(self):\n        def test_impl(df, n):\n            return df.iloc[n, 1]\n        sdc_func = sdc.jit(test_impl)\n        cases_idx = [[3, 4, 2, 6, 1], None]\n        cases_n = [1, 0, 2]\n        for idx in cases_idx:\n            df = pd.DataFrame({""A"": [3.2, 4.4, 7.0, 3.3, 1.0],\n                               ""B"": [5.5, np.nan, 3, 0, 7.7],\n                               ""C"": [3, 4, 1, 0, 222]}, index=idx)\n            for n in cases_n:\n                with self.subTest(index=idx, n=n):\n                    if not (np.isnan(sdc_func(df, n)) and np.isnan(test_impl(df, n))):\n                        self.assertEqual(sdc_func(df, n), test_impl(df, n))\n\n    def test_df_iloc_values_no_unboxing(self):\n        def test_impl(n):\n            df = pd.DataFrame({\n                \'A\': [3.2, 4.4, 7.0, 3.3, 1.0],\n                \'B\': [5.5, np.nan, 3, 0, 7.7],\n                \'C\': [3, 4, 1, 0, 222],\n            }, index=[3, 4, 2, 6, 1])\n            return df.iloc[n, 1]\n\n        sdc_func = sdc.jit(test_impl)\n        for n in [1, 0, 2]:\n            with self.subTest(n=n):\n                if not (np.isnan(sdc_func(n)) and np.isnan(test_impl(n))):\n                    self.assertEqual(sdc_func(n), test_impl(n))\n\n    def test_df_iloc_value_error(self):\n        def int_impl(df):\n            return df.iloc[11]\n\n        def list_impl(df):\n            return df.iloc[[7, 14]]\n\n        def list_bool_impl(df):\n            return df.iloc[[True, False]]\n\n        msg1 = \'Index is out of bounds for axis\'\n        msg2 = \'Item wrong length\'\n        df = pd.DataFrame({""A"": [3.2, 4.4, 7.0, 3.3, 1.0],\n                           ""B"": [5.5, np.nan, 3, 0, 7.7],\n                           ""C"": [3, 4, 1, 0, 222]})\n\n        impls = [(int_impl, msg1), (list_impl, msg1), (list_bool_impl, msg2)]\n        for impl, msg in impls:\n            with self.subTest(case=impl, msg=msg):\n                func = self.jit(impl)\n                with self.assertRaises(IndexingError) as raises:\n                    func(df)\n                self.assertIn(msg, str(raises.exception))\n\n    def test_df_iloc_int(self):\n        def test_impl(df, n):\n            return df.iloc[n]\n        sdc_func = sdc.jit(test_impl)\n        cases_idx = [[3, 4, 2, 6, 1], None]\n        cases_n = [0, 1, 2]\n        for idx in cases_idx:\n            df = pd.DataFrame({""A"": [3.2, 4.4, 7.0, 3.3, 1.0],\n                               ""B"": [5.5, np.nan, 3, 0, 7.7],\n                               ""C"": [3, 4, 1, 0, 222]}, index=idx)\n            for n in cases_n:\n                with self.subTest(index=idx, n=n):\n                    pd.testing.assert_series_equal(sdc_func(df, n), test_impl(df, n), check_names=False)\n\n    def test_df_iloc_int_no_unboxing(self):\n        def test_impl(n):\n            df = pd.DataFrame({\n                \'A\': [3.2, 4.4, 7.0, 3.3, 1.0],\n                \'B\': [5.5, np.nan, 3, 0, 7.7],\n                \'C\': [3, 4, 1, 0, 222],\n            }, index=[3, 4, 2, 6, 1])\n            return df.iloc[n]\n\n        sdc_func = sdc.jit(test_impl)\n        for n in [0, 1, 2]:\n            with self.subTest(n=n):\n                pd.testing.assert_series_equal(sdc_func(n), test_impl(n), check_names=False)\n\n    def test_df_iloc_list(self):\n        def test_impl(df, n):\n            return df.iloc[n]\n        sdc_func = sdc.jit(test_impl)\n        cases_idx = [[3, 4, 2, 6, 1], None]\n        cases_n = [[0, 1], [2, 0]]\n        for idx in cases_idx:\n            df = pd.DataFrame({""A"": [3.2, 4.4, 7.0, 3.3, 1.0],\n                               ""B"": [5.5, np.nan, 3, 0, 7.7],\n                               ""C"": [3, 4, 1, 0, 222]}, index=idx)\n            for n in cases_n:\n                with self.subTest(index=idx, n=n):\n                    pd.testing.assert_frame_equal(sdc_func(df, n), test_impl(df, n))\n\n    def test_df_iloc_list_no_unboxing(self):\n        def test_impl(n):\n            df = pd.DataFrame({\n                \'A\': [3.2, 4.4, 7.0, 3.3, 1.0],\n                \'B\': [5.5, np.nan, 3, 0, 7.7],\n                \'C\': [3, 4, 1, 0, 222]\n            }, index=[3, 4, 2, 6, 1])\n            return df.iloc[n]\n\n        sdc_func = sdc.jit(test_impl)\n        for n in [[0, 1], [2, 0]]:\n            with self.subTest(n=n):\n                pd.testing.assert_frame_equal(sdc_func(n), test_impl(n))\n\n    def test_df_iloc_list_bool(self):\n        def test_impl(df, n):\n            return df.iloc[n]\n        sdc_func = sdc.jit(test_impl)\n        cases_idx = [[3, 4, 2, 6, 1], None]\n        cases_n = [[True, False, True, False, True]]\n        for idx in cases_idx:\n            df = pd.DataFrame({""A"": [3.2, 4.4, 7.0, 3.3, 1.0],\n                               ""B"": [5.5, np.nan, 3, 0, 7.7],\n                               ""C"": [3, 4, 1, 0, 222]}, index=idx)\n            for n in cases_n:\n                with self.subTest(index=idx, n=n):\n                    pd.testing.assert_frame_equal(sdc_func(df, n), test_impl(df, n))\n\n    def test_df_iloc_list_bool_no_unboxing(self):\n        def test_impl(n):\n            df = pd.DataFrame({\n                \'A\': [3.2, 4.4, 7.0, 3.3, 1.0],\n                \'B\': [5.5, np.nan, 3, 0, 7.7],\n                \'C\': [3, 4, 1, 0, 222]\n            }, index=[3, 4, 2, 6, 1])\n            return df.iloc[n]\n\n        sdc_func = sdc.jit(test_impl)\n        for n in [[True, False, True, False, True]]:\n            with self.subTest(n=n):\n                pd.testing.assert_frame_equal(sdc_func(n), test_impl(n))\n\n    def test_df_iat(self):\n        def test_impl(df):\n            return df.iat[0, 1]\n        sdc_func = sdc.jit(test_impl)\n        idx = [3, 4, 2, 6, 1]\n        df = pd.DataFrame({""A"": [3.2, 4.4, 7.0, 3.3, 1.0],\n                           ""B"": [3, 4, 1, 0, 222],\n                           ""C"": [\'a\', \'dd\', \'c\', \'12\', \'ddf\']}, index=idx)\n        self.assertEqual(sdc_func(df), test_impl(df))\n\n    def test_df_iat_no_unboxing(self):\n        def test_impl():\n            df = pd.DataFrame({\n                \'A\': [3.2, 4.4, 7.0, 3.3, 1.0],\n                \'B\': [3, 4, 1, 0, 222],\n                \'C\': [\'a\', \'dd\', \'c\', \'12\', \'ddf\']\n            }, index=[3, 4, 2, 6, 1])\n            return df.iat[0, 1]\n\n        sdc_func = sdc.jit(test_impl)\n        self.assertEqual(sdc_func(), test_impl())\n\n    def test_df_iat_value_error(self):\n        def test_impl(df):\n            return df.iat[1, 22]\n        sdc_func = sdc.jit(test_impl)\n        df = pd.DataFrame({""A"": [3.2, 4.4, 7.0, 3.3, 1.0],\n                           ""B"": [3, 4, 1, 0, 222],\n                           ""C"": [\'a\', \'dd\', \'c\', \'12\', \'ddf\']})\n\n        with self.assertRaises(TypingError) as raises:\n            sdc_func(df)\n        msg = \'Index is out of bounds for axis\'\n        self.assertIn(msg, str(raises.exception))\n\n    def test_df_at(self):\n        def test_impl(df, n):\n            return df.at[n, \'C\']\n\n        sdc_func = sdc.jit(test_impl)\n        idx = [3, 0, 1, 2, 0]\n        n_cases = [0, 2]\n        df = pd.DataFrame({""A"": [3.2, 4.4, 7.0, 3.3, 1.0],\n                           ""B"": [3, 4, 1, 0, 222],\n                           ""C"": [\'a\', \'dd\', \'c\', \'12\', \'ddf\']}, index=idx)\n        for n in n_cases:\n            np.testing.assert_array_equal(sdc_func(df, n), test_impl(df, n))\n\n    def test_df_at_no_unboxing(self):\n        def test_impl(n):\n            df = pd.DataFrame({\n                \'A\': [3.2, 4.4, 7.0, 3.3, 1.0],\n                \'B\': [3, 4, 1, 0, 222],\n                \'C\': [\'a\', \'dd\', \'c\', \'12\', \'ddf\']\n            }, index=[3, 0, 1, 2, 0])\n            return df.at[n, \'C\']\n\n        sdc_func = sdc.jit(test_impl)\n        for n in [0, 2]:\n            np.testing.assert_array_equal(sdc_func(n), test_impl(n))\n\n    def test_df_at_type(self):\n        def test_impl(df, n, k):\n            return df.at[n, ""B""]\n\n        sdc_func = sdc.jit(test_impl)\n        idx = [\'3\', \'4\', \'1\', \'2\', \'0\']\n        n_cases = [\'2\', \'3\']\n        df = pd.DataFrame({""A"": [3.2, 4.4, 7.0, 3.3, 1.0],\n                           ""B"": [3, 4, 1, 0, 222],\n                           ""C"": [\'a\', \'dd\', \'c\', \'12\', \'ddf\']}, index=idx)\n        for n in n_cases:\n            self.assertEqual(sdc_func(df, n, ""B""), test_impl(df, n, ""B""))\n\n    def test_df_at_value_error(self):\n        def test_impl(df):\n            return df.at[5, \'C\']\n        sdc_func = sdc.jit(test_impl)\n        idx = [3, 4, 1, 2, 0]\n        df = pd.DataFrame({""A"": [3.2, 4.4, 7.0, 3.3, 1.0],\n                           ""B"": [3, 4, 1, 0, 222],\n                           ""C"": [3, 4, 2, 6, 1]}, index=idx)\n\n        with self.assertRaises(ValueError) as raises:\n            sdc_func(df)\n        msg = \'Index is not in the Series\'\n        self.assertIn(msg, str(raises.exception))\n\n    def test_df_loc(self):\n        def test_impl(df):\n            return df.loc[4]\n\n        sdc_func = sdc.jit(test_impl)\n        idx = [3, 4, 1, 4, 0]\n        df = pd.DataFrame({""A"": [3.2, 4.4, 7.0, 3.3, 1.0],\n                           ""B"": [3, 4, 1, 0, 222],\n                           ""C"": [3.1, 8.4, 7.1, 3.2, 1]}, index=idx)\n        pd.testing.assert_frame_equal(sdc_func(df), test_impl(df))\n\n    def test_df_loc_no_unboxing(self):\n        def test_impl():\n            df = pd.DataFrame({\n                \'A\': [3.2, 4.4, 7.0, 3.3, 1.0],\n                \'B\': [3, 4, 1, 0, 222],\n                \'C\': [3.1, 8.4, 7.1, 3.2, 1]\n            }, index=[3, 4, 1, 4, 0])\n            return df.loc[4]\n\n        sdc_func = sdc.jit(test_impl)\n        pd.testing.assert_frame_equal(sdc_func(), test_impl())\n\n    @unittest.skip(""SDC Dataframe.loc[] always return Dataframe"")\n    def test_df_loc_str(self):\n        def test_impl(df):\n            return df.loc[\'c\']\n\n        sdc_func = sdc.jit(test_impl)\n        idx = [\'a\', \'b\', \'c\', \'\xd1\x81\', \'e\']\n        df = pd.DataFrame({""A"": [\'3.2\', \'4.4\', \'7.0\', \'3.3\', \'1.0\'],\n                           ""B"": [\'3\', \'4\', \'1\', \'0\', \'222\'],\n                           ""C"": [\'3.1\', \'8.4\', \'7.1\', \'3.2\', \'1\']}, index=idx)\n        pd.testing.assert_frame_equal(sdc_func(df), test_impl(df))\n\n    @unittest.skip(""SDC Dataframe.loc[] always return Dataframe"")\n    def test_df_loc_no_idx(self):\n        def test_impl(df):\n            return df.loc[2]\n\n        sdc_func = sdc.jit(test_impl)\n        df = pd.DataFrame({""A"": [3.2, 4.4, 7.0, 3.3, 1.0],\n                           ""B"": [3, 4, 1, 0, 222],\n                           ""C"": [3.1, 8.4, 7.1, 3.2, 1]})\n        pd.testing.assert_frame_equal(sdc_func(df), test_impl(df))\n\n    def test_df_head(self):\n        def get_func(n):\n            def impl(a):\n                return a.head(n)\n\n            return impl\n\n        cases_n = [-3, 0, 3, 5, None]\n        cases_index = [[3, 4, 2, 6, 1], None]\n        for n in cases_n:\n            for idx in cases_index:\n                ref_impl = get_func(n)\n                sdc_impl = get_func(n)\n                sdc_func = self.jit(sdc_impl)\n                with self.subTest(n=n, index=idx):\n                    df = pd.DataFrame(\n                        {""float"": [3.2, 4.4, 7.0, 3.3, 1.0],\n                         ""int"": [3, 4, 1, 0, 222],\n                         ""string"": [\'a\', \'dd\', \'c\', \'12\', \'ddf\']},\n                        index=[3, 4, 2, 6, 1]\n                    )\n                    pd.testing.assert_frame_equal(sdc_func(df), ref_impl(df))\n\n    def test_df_head_no_unboxing(self):\n        def test_impl(n):\n            df = pd.DataFrame({\n                \'float\': [3.2, 4.4, 7.0, 3.3, 1.0],\n                \'int\': [3, 4, 1, 0, 222],\n                \'string\': [\'a\', \'dd\', \'c\', \'12\', \'ddf\']\n            }, index=[3, 4, 2, 6, 1])\n            return df.head(n)\n\n        sdc_impl = self.jit(test_impl)\n        for n in [-3, 0, 3, 5, None]:\n            with self.subTest(n=n):\n                pd.testing.assert_frame_equal(sdc_impl(n), test_impl(n))\n\n    def test_df_copy(self):\n        def test_impl(df, deep):\n            return df.copy(deep=deep)\n\n        sdc_func = sdc.jit(test_impl)\n        indexes = [[3, 4, 2, 6, 1], [\'a\', \'b\', \'c\', \'d\', \'e\'], None]\n        cases_deep = [None, True, False]\n\n        for idx in indexes:\n            df = pd.DataFrame({""A"": [3.2, np.nan, 7.0, 3.3, np.nan],\n                               ""B"": [3, 4, 1, 0, 222],\n                               ""C"": [True, True, False, False, True],\n                               ""D"": [\'a\', \'dd\', \'c\', \'12\', None]}, index=idx)\n            for deep in cases_deep:\n                with self.subTest(index=idx, deep=deep):\n                    pd.testing.assert_frame_equal(sdc_func(df, deep), test_impl(df, deep))\n\n    def test_df_copy_no_unboxing(self):\n        def test_impl(idx, deep):\n            df = pd.DataFrame({\n                \'A\': [3.2, np.nan, 7.0, 3.3, np.nan],\n                \'B\': [3, 4, 1, 0, 222],\n                \'C\': [True, True, False, False, True],\n                \'D\': [\'a\', \'dd\', \'c\', \'12\', None]\n            }, index=idx)\n            return df.copy(deep=deep)\n\n        sdc_impl = sdc.jit(test_impl)\n        indexes = [[3, 4, 2, 6, 1], [\'a\', \'b\', \'c\', \'d\', \'e\']]\n        cases_deep = [None, True, False]\n        for idx, deep in product(indexes, cases_deep):\n            with self.subTest(index=idx, deep=deep):\n                jit_result = sdc_impl(idx, deep)\n                ref_result = test_impl(idx, deep)\n                pd.testing.assert_frame_equal(jit_result, ref_result)\n\n    @unittest.expectedFailure\n    def test_df_copy_no_unboxing_none_index_error(self):\n        def test_impl():\n            df = pd.DataFrame({\n                \'A\': [3.2, np.nan, 7.0, 3.3, np.nan],\n                \'B\': [3, 4, 1, 0, 222],\n                \'C\': [True, True, False, False, True],\n                \'D\': [\'a\', \'dd\', \'c\', \'12\', None]\n            }, index=None)\n            return df.copy(deep=True)\n\n        sdc_impl = sdc.jit(test_impl)\n        pd.testing.assert_frame_equal(sdc_impl(), test_impl())\n\n    def test_pct_change1(self):\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.arange(n) + 1.0, \'B\': np.arange(n) + 1})\n            return df.pct_change(3)\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        pd.testing.assert_frame_equal(hpat_func(n), test_impl(n))\n\n    def test_mean1(self):\n        # TODO: non-numeric columns should be ignored automatically\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.arange(n) + 1.0, \'B\': np.arange(n) + 1})\n            return df.mean()\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        pd.testing.assert_series_equal(hpat_func(n), test_impl(n))\n\n    def test_median1(self):\n        # TODO: non-numeric columns should be ignored automatically\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': 2 ** np.arange(n), \'B D\': np.arange(n) + 1.0})\n            return df.median()\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        pd.testing.assert_series_equal(hpat_func(n), test_impl(n))\n\n    def test_std1(self):\n        # TODO: non-numeric columns should be ignored automatically\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.arange(n) + 1.0, \'B\': np.arange(n) + 1})\n            return df.std()\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        pd.testing.assert_series_equal(hpat_func(n), test_impl(n))\n\n    def test_var1(self):\n        # TODO: non-numeric columns should be ignored automatically\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.arange(n) + 1.0, \'B\': np.arange(n) + 1})\n            return df.var()\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        pd.testing.assert_series_equal(hpat_func(n), test_impl(n))\n\n    def test_max1(self):\n        # TODO: non-numeric columns should be ignored automatically\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.arange(n) + 1.0, \'B\': np.arange(n) + 1})\n            return df.max()\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        pd.testing.assert_series_equal(hpat_func(n), test_impl(n))\n\n    def test_min1(self):\n        # TODO: non-numeric columns should be ignored automatically\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.arange(n) + 1.0, \'B\': np.arange(n) + 1})\n            return df.min()\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        pd.testing.assert_series_equal(hpat_func(n), test_impl(n))\n\n    def test_sum1(self):\n        # TODO: non-numeric columns should be ignored automatically\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.arange(n) + 1.0, \'B\': np.arange(n) + 1})\n            return df.sum()\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        pd.testing.assert_series_equal(hpat_func(n), test_impl(n))\n\n    def test_prod1(self):\n        # TODO: non-numeric columns should be ignored automatically\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.arange(n) + 1.0, \'B\': np.arange(n) + 1})\n            return df.prod()\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        pd.testing.assert_series_equal(hpat_func(n), test_impl(n))\n\n    def test_count(self):\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.arange(n), \'B\': np.arange(n)})\n            return df.count()\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        pd.testing.assert_series_equal(hpat_func(n), test_impl(n))\n\n    def test_count1(self):\n        # TODO: non-numeric columns should be ignored automatically\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.arange(n) + 1.0, \'B\': np.arange(n) + 1})\n            return df.count()\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        pd.testing.assert_series_equal(hpat_func(n), test_impl(n))\n\n    @skip_numba_jit\n    def test_df_fillna1(self):\n        def test_impl(df):\n            return df.fillna(5.0)\n\n        df = pd.DataFrame({\'A\': [1.0, 2.0, np.nan, 1.0]})\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_frame_equal(hpat_func(df), test_impl(df))\n\n    @skip_numba_jit\n    def test_df_fillna_str1(self):\n        def test_impl(df):\n            return df.fillna(""dd"")\n\n        df = pd.DataFrame({\'A\': [\'aa\', \'b\', None, \'ccc\']})\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_frame_equal(hpat_func(df), test_impl(df))\n\n    @skip_numba_jit\n    def test_df_fillna_inplace1(self):\n        def test_impl(A):\n            A.fillna(11.0, inplace=True)\n            return A\n\n        df = pd.DataFrame({\'A\': [1.0, 2.0, np.nan, 1.0]})\n        df2 = df.copy()\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_frame_equal(hpat_func(df), test_impl(df2))\n\n    def test_df_reset_index_drop(self):\n        def test_impl(df, drop):\n            return df.reset_index(drop=drop)\n\n        df = pd.DataFrame({\'A\': [1.0, 2.0, np.nan, 1.0], \'B\': np.arange(4.0)})\n        hpat_func = self.jit(test_impl)\n\n        for drop in [True, False]:\n            with self.subTest(drop=drop):\n                with self.assertRaises(Exception) as raises:\n                    hpat_func(df, drop)\n                msg = \'drop is only supported as a literal\'\n                self.assertIn(msg, str(raises.exception))\n\n    def test_df_reset_index_drop_literal_index_int(self):\n        def gen_test_impl(drop):\n            def test_impl(df):\n                if drop == False:  # noqa\n                    return df.reset_index(drop=False)\n                else:\n                    return df.reset_index(drop=True)\n            return test_impl\n\n        df = pd.DataFrame({\n            \'A\': [1.0, 2.0, np.nan, 1.0],\n            \'B\': np.arange(4.0)\n        }, index=[5, 8, 4, 6])\n        for drop in [True, False]:\n            with self.subTest(drop=drop):\n                test_impl = gen_test_impl(drop)\n                hpat_func = self.jit(test_impl)\n                pd.testing.assert_frame_equal(hpat_func(df), test_impl(df))\n\n    def test_df_reset_index_drop_literal_index_int_no_unboxing(self):\n        def gen_test_impl(drop):\n            def test_impl():\n                df = pd.DataFrame({\n                    \'A\': [1.0, 2.0, np.nan, 1.0],\n                    \'B\': np.arange(4.0)\n                }, index=[5, 8, 4, 6])\n                if drop == False:  # noqa\n                    return df.reset_index(drop=False)\n                else:\n                    return df.reset_index(drop=True)\n            return test_impl\n\n        for drop in [True, False]:\n            with self.subTest(drop=drop):\n                test_impl = gen_test_impl(drop)\n                hpat_func = self.jit(test_impl)\n                pd.testing.assert_frame_equal(hpat_func(), test_impl())\n\n    def test_df_reset_index_drop_default_index_int(self):\n        def test_impl(df):\n            return df.reset_index()\n\n        df = pd.DataFrame({\'A\': [1.0, 2.0, np.nan, 1.0],\n                           \'B\': np.arange(4.0)}, index=[5, 8, 4, 6])\n        hpat_func = self.jit(test_impl)\n\n        pd.testing.assert_frame_equal(hpat_func(df), test_impl(df))\n\n    def test_df_reset_index_drop_default_index_int_no_unboxing(self):\n        def test_impl():\n            df = pd.DataFrame({\n                \'A\': [1.0, 2.0, np.nan, 1.0],\n                \'B\': np.arange(4.0)\n            }, index=[5, 8, 4, 6])\n            return df.reset_index()\n\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_frame_equal(hpat_func(), test_impl())\n\n    @skip_numba_jit\n    def test_df_reset_index_empty_df(self):\n        def test_impl(df):\n            return df.reset_index()\n\n        df = pd.DataFrame({})\n        hpat_func = self.jit(test_impl)\n\n        pd.testing.assert_frame_equal(hpat_func(df), test_impl(df))\n\n    @skip_numba_jit\n    def test_df_reset_index_inplace1(self):\n        def test_impl():\n            df = pd.DataFrame({\'A\': [1.0, 2.0, np.nan, 1.0]})\n            df.reset_index(drop=True, inplace=True)\n            return df\n\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_frame_equal(hpat_func(), test_impl())\n\n    @skip_numba_jit\n    def test_df_dropna1(self):\n        def test_impl(df):\n            return df.dropna()\n\n        df = pd.DataFrame({\'A\': [1.0, 2.0, np.nan, 1.0], \'B\': [4, 5, 6, 7]})\n        hpat_func = self.jit(test_impl)\n        out = test_impl(df).reset_index(drop=True)\n        h_out = hpat_func(df)\n        pd.testing.assert_frame_equal(out, h_out)\n\n    @skip_numba_jit\n    def test_df_dropna2(self):\n        def test_impl(df):\n            return df.dropna()\n\n        df = pd.DataFrame({\'A\': [1.0, 2.0, np.nan, 1.0]})\n        hpat_func = self.jit(test_impl)\n        out = test_impl(df).reset_index(drop=True)\n        h_out = hpat_func(df)\n        pd.testing.assert_frame_equal(out, h_out)\n\n    @skip_numba_jit\n    def test_df_dropna_inplace1(self):\n        # TODO: fix error when no df is returned\n        def test_impl(df):\n            df.dropna(inplace=True)\n            return df\n\n        df = pd.DataFrame({\'A\': [1.0, 2.0, np.nan, 1.0], \'B\': [4, 5, 6, 7]})\n        df2 = df.copy()\n        hpat_func = self.jit(test_impl)\n        out = test_impl(df).reset_index(drop=True)\n        h_out = hpat_func(df2)\n        pd.testing.assert_frame_equal(out, h_out)\n\n    @skip_numba_jit\n    def test_df_dropna_str1(self):\n        def test_impl(df):\n            return df.dropna()\n\n        df = pd.DataFrame({\'A\': [1.0, 2.0, 4.0, 1.0], \'B\': [\'aa\', \'b\', None, \'ccc\']})\n        hpat_func = self.jit(test_impl)\n        out = test_impl(df).reset_index(drop=True)\n        h_out = hpat_func(df)\n        pd.testing.assert_frame_equal(out, h_out)\n\n    def test_df_drop_one_column_unboxing(self):\n        def test_impl(df):\n            return df.drop(columns=\'C D\')\n\n        index_to_test = [[1, 2, 3, 4],\n                         [.1, .2, .3, .4],\n                         None,\n                         [\'a\', \'b\', \'c\', \'d\']]\n\n        sdc_func = self.jit(test_impl)\n\n        for index in index_to_test:\n            with self.subTest(index=index):\n                df = pd.DataFrame({\'A\': [1.0, 2.0, np.nan, 1.0], \'B\': [4, 5, 6, 7], \'C D\': [1.0, 2.0, np.nan, 1.0]},\n                                  index=index)\n                pd.testing.assert_frame_equal(sdc_func(df), test_impl(df))\n\n    def test_df_drop_one_column(self):\n        def test_impl(index):\n            df = pd.DataFrame({\n                \'A\': [1.0, 2.0, np.nan, 1.0],\n                \'B\': [4, 5, 6, 7],\n                \'C\': [1.0, 2.0, np.nan, 1.0]\n            }, index=index)\n            return df.drop(columns=\'A\')\n\n        sdc_func = self.jit(test_impl)\n        for index in [[1, 2, 3, 4], [.1, .2, .3, .4], [\'a\', \'b\', \'c\', \'d\']]:\n            with self.subTest(index=index):\n                pd.testing.assert_frame_equal(sdc_func(index), test_impl(index))\n\n    def test_df_drop_tuple_column_unboxing(self):\n        def gen_test_impl(do_jit=False):\n            def test_impl(df):\n                if do_jit == True:  # noqa\n                    return df.drop(columns=(\'A\', \'C\'))\n                else:\n                    return df.drop(columns=[\'A\', \'C\'])\n\n            return test_impl\n\n        index_to_test = [[1, 2, 3, 4],\n                         [.1, .2, .3, .4],\n                         None,\n                         [\'a\', \'b\', \'c\', \'d\']]\n\n        test_impl = gen_test_impl()\n        sdc_func = self.jit(gen_test_impl(do_jit=True))\n\n        for index in index_to_test:\n            with self.subTest(index=index):\n                df = pd.DataFrame({\'A\': [1.0, 2.0, np.nan, 1.0], \'B\': [4, 5, 6, 7], \'C\': [1.0, 2.0, np.nan, 1.0]},\n                                  index=index)\n                pd.testing.assert_frame_equal(sdc_func(df), test_impl(df))\n\n    def test_df_drop_tuple_column(self):\n        def gen_test_impl(do_jit=False):\n            def test_impl(index):\n                df = pd.DataFrame({\'A\': [1.0, 2.0, np.nan, 1.0], \'B\': [4, 5, 6, 7], \'C\': [1.0, 2.0, np.nan, 1.0]},\n                                  index=index)\n                if do_jit == True:  # noqa\n                    return df.drop(columns=(\'A\', \'C\'))\n                else:\n                    return df.drop(columns=[\'A\', \'C\'])\n\n            return test_impl\n\n        index_to_test = [[1, 2, 3, 4],\n                         [.1, .2, .3, .4],\n                         [\'a\', \'b\', \'c\', \'d\']]\n\n        test_impl = gen_test_impl()\n        sdc_func = self.jit(gen_test_impl(do_jit=True))\n\n        for index in index_to_test:\n            with self.subTest(index=index):\n                pd.testing.assert_frame_equal(sdc_func(index), test_impl(index))\n\n    @unittest.skip(""ValueError when return empty dataframe"")\n    def test_df_drop_tuple_columns_all(self):\n        def gen_test_impl(do_jit=False):\n            def test_impl(df):\n                if do_jit == True:  # noqa\n                    return df.drop(columns=(\'A\', \'B\', \'C\'))\n                else:\n                    return df.drop(columns=[\'A\', \'B\', \'C\'])\n\n            return test_impl\n\n        index_to_test = [[1, 2, 3, 4],\n                         [.1, .2, .3, .4],\n                         None,\n                         [\'a\', \'b\', \'c\', \'d\']]\n\n        test_impl = gen_test_impl()\n        sdc_func = self.jit(gen_test_impl(do_jit=True))\n\n        for index in index_to_test:\n            with self.subTest(index=index):\n                df = pd.DataFrame({\'A\': [1.0, 2.0, np.nan, 1.0], \'B\': [4, 5, 6, 7], \'C\': [1.0, 2.0, np.nan, 1.0]},\n                                  index=index)\n\n                pd.testing.assert_frame_equal(sdc_func(df), test_impl(df))\n\n    def test_df_drop_by_column_errors_ignore(self):\n        def test_impl(df):\n            return df.drop(columns=\'M\', errors=\'ignore\')\n\n        df = pd.DataFrame({\'A\': [1.0, 2.0, np.nan, 1.0], \'B\': [4, 5, 6, 7], \'C\': [1.0, 2.0, np.nan, 1.0]})\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_frame_equal(hpat_func(df), test_impl(df))\n\n    @skip_numba_jit\n    def test_df_drop_inplace2(self):\n        # test droping after setting the column\n        def test_impl(df):\n            df2 = df[[\'A\', \'B\']]\n            df2[\'D\'] = np.ones(3)\n            df2.drop(columns=[\'D\'], inplace=True)\n            return df2\n\n        df = pd.DataFrame({\'A\': [1, 2, 3], \'B\': [2, 3, 4]})\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_frame_equal(hpat_func(df), test_impl(df))\n\n    @skip_numba_jit\n    def test_df_drop_inplace1(self):\n        def test_impl(df):\n            df.drop(\'A\', axis=1, inplace=True)\n            return df\n\n        df = pd.DataFrame({\'A\': [1.0, 2.0, np.nan, 1.0], \'B\': [4, 5, 6, 7]})\n        df2 = df.copy()\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_frame_equal(hpat_func(df), test_impl(df2))\n\n    def _test_df_getitem_str_literal_idx(self, df):\n        def test_impl(df):\n            return df[\'A\']\n\n        sdc_func = self.jit(test_impl)\n        pd.testing.assert_series_equal(sdc_func(df), test_impl(df))\n\n    def _test_df_getitem_unicode_idx(self, df, idx):\n        def test_impl(df, idx):\n            return df[idx]\n\n        sdc_func = self.jit(test_impl)\n        pd.testing.assert_series_equal(sdc_func(df, idx), test_impl(df, idx))\n\n    def _test_df_getitem_slice_idx(self, df):\n        def test_impl(df):\n            return df[1:3]\n\n        sdc_func = self.jit(test_impl)\n        pd.testing.assert_frame_equal(sdc_func(df), test_impl(df))\n\n    def _test_df_getitem_unbox_slice_idx(self, df, start, end):\n        def test_impl(df, start, end):\n            return df[start:end]\n\n        sdc_func = self.jit(test_impl)\n        jit_result = sdc_func(df, start, end)\n        ref_result = test_impl(df, start, end)\n        pd.testing.assert_frame_equal(jit_result, ref_result)\n\n    def _test_df_getitem_tuple_idx(self, df):\n        def gen_test_impl(do_jit=False):\n            def test_impl(df):\n                if do_jit == True:  # noqa\n                    return df[(\'A\', \'C\')]\n                else:\n                    return df[[\'A\', \'C\']]\n\n            return test_impl\n\n        test_impl = gen_test_impl()\n        sdc_func = self.jit(gen_test_impl(do_jit=True))\n\n        pd.testing.assert_frame_equal(sdc_func(df), test_impl(df))\n\n    def _test_df_getitem_bool_series_idx(self, df):\n        def test_impl(df):\n            return df[df[\'A\'] == -1.]\n\n        sdc_func = self.jit(test_impl)\n        pd.testing.assert_frame_equal(sdc_func(df), test_impl(df))\n\n    def _test_df_getitem_bool_series_even_idx(self, df):\n        def test_impl(df, series):\n            return df[series]\n\n        s = pd.Series([False, True] * 5)\n\n        sdc_func = self.jit(test_impl)\n        pd.testing.assert_frame_equal(sdc_func(df, s), test_impl(df, s))\n\n    def _test_df_getitem_bool_array_even_idx(self, df):\n        def test_impl(df, arr):\n            return df[arr]\n\n        arr = np.array([i % 2 for i in range(len(df))], dtype=np.bool_)\n\n        sdc_func = self.jit(test_impl)\n        pd.testing.assert_frame_equal(sdc_func(df, arr), test_impl(df, arr))\n\n    def test_df_getitem_bool_array_even_idx_no_unboxing(self):\n        def test_impl(arr):\n            df = pd.DataFrame({\n                \'A\': [1., -1., 0.1, -0.1],\n                \'B\': list(range(4)),\n                \'C\': [1., np.nan, np.inf, np.inf],\n            })\n            return df[arr]\n\n        arr = np.array([i % 2 for i in range(4)], dtype=np.bool_)\n\n        sdc_func = self.jit(test_impl)\n        pd.testing.assert_frame_equal(sdc_func(arr), test_impl(arr))\n\n    @skip_sdc_jit(\'DF.getitem unsupported exceptions\')\n    def test_df_getitem_str_literal_idx_exception_key_error(self):\n        def test_impl(df):\n            return df[\'ABC\']\n\n        sdc_func = self.jit(test_impl)\n\n        for df in [gen_df(test_global_input_data_float64), pd.DataFrame()]:\n            with self.subTest(df=df):\n                with self.assertRaises(KeyError):\n                    sdc_func(df)\n\n    @skip_sdc_jit(\'DF.getitem unsupported exceptions\')\n    def test_df_getitem_unicode_idx_exception_key_error(self):\n        def test_impl(df, idx):\n            return df[idx]\n\n        sdc_func = self.jit(test_impl)\n\n        for df in [gen_df(test_global_input_data_float64), pd.DataFrame()]:\n            with self.subTest(df=df):\n                with self.assertRaises(KeyError):\n                    sdc_func(df, \'ABC\')\n\n    @skip_sdc_jit(\'DF.getitem unsupported exceptions\')\n    def test_df_getitem_tuple_idx_exception_key_error(self):\n        sdc_func = self.jit(lambda df: df[(\'A\', \'Z\')])\n\n        for df in [gen_df(test_global_input_data_float64), pd.DataFrame()]:\n            with self.subTest(df=df):\n                with self.assertRaises(KeyError):\n                    sdc_func(df)\n\n    @skip_sdc_jit(\'DF.getitem unsupported exceptions\')\n    def test_df_getitem_bool_array_idx_exception_value_error(self):\n        sdc_func = self.jit(lambda df, arr: df[arr])\n\n        for df in [gen_df(test_global_input_data_float64), pd.DataFrame()]:\n            arr = np.array([i % 2 for i in range(len(df) + 1)], dtype=np.bool_)\n            with self.subTest(df=df, arr=arr):\n                with self.assertRaises(ValueError) as raises:\n                    sdc_func(df, arr)\n                self.assertIn(\'Item wrong length\', str(raises.exception))\n\n    @skip_sdc_jit(\'DF.getitem unsupported Series name\')\n    def test_df_getitem_idx(self):\n        dfs = [gen_df(test_global_input_data_float64),\n               gen_df(test_global_input_data_float64, with_index=True),\n               pd.DataFrame({\'A\': [], \'B\': [], \'C\': []})]\n        for df in dfs:\n            with self.subTest(df=df):\n                self._test_df_getitem_str_literal_idx(df)\n                self._test_df_getitem_unicode_idx(df, \'A\')\n                self._test_df_getitem_slice_idx(df)\n                self._test_df_getitem_unbox_slice_idx(df, 1, 3)\n                self._test_df_getitem_tuple_idx(df)\n                self._test_df_getitem_bool_series_idx(df)\n\n    def test_df_getitem_str_literal_idx_no_unboxing(self):\n        def test_impl():\n            df = pd.DataFrame({\n                \'A\': [1., -1., 0.1, -0.1],\n                \'B\': list(range(4)),\n                \'C\': [1., np.nan, np.inf, np.inf],\n            })\n            return df[\'A\']\n\n        sdc_func = self.jit(test_impl)\n        pd.testing.assert_series_equal(sdc_func(), test_impl())\n\n    def test_df_getitem_unicode_idx_no_unboxing(self):\n        def test_impl(idx):\n            df = pd.DataFrame({\n                \'A\': [1., -1., 0.1, -0.1],\n                \'B\': list(range(4)),\n                \'C\': [1., np.nan, np.inf, np.inf],\n            })\n            return df[idx]\n\n        sdc_func = self.jit(test_impl)\n        pd.testing.assert_series_equal(sdc_func(\'A\'), test_impl(\'A\'))\n\n    def test_df_getitem_slice_idx_no_unboxing(self):\n        def test_impl():\n            df = pd.DataFrame({\n                \'A\': [1., -1., 0.1, -0.1],\n                \'B\': list(range(4)),\n                \'C\': [1., np.nan, np.inf, np.inf],\n            })\n            return df[1:3]\n\n        sdc_func = self.jit(test_impl)\n        pd.testing.assert_frame_equal(sdc_func(), test_impl())\n\n    def test_df_getitem_unbox_slice_idx_no_unboxing(self):\n        def test_impl(start, end):\n            df = pd.DataFrame({\n                \'A\': [1., -1., 0.1, -0.1],\n                \'B\': list(range(4)),\n                \'C\': [1., np.nan, np.inf, np.inf],\n            })\n            return df[start:end]\n\n        sdc_func = self.jit(test_impl)\n        pd.testing.assert_frame_equal(sdc_func(1, 3), test_impl(1, 3))\n\n    def test_df_getitem_tuple_idx_no_unboxing(self):\n        def gen_test_impl(do_jit=False):\n            def test_impl():\n                df = pd.DataFrame({\n                    \'A\': [1., -1., 0.1, -0.1],\n                    \'B\': list(range(4)),\n                    \'C\': [1., np.nan, np.inf, np.inf],\n                })\n                if do_jit == True:  # noqa\n                    return df[(\'A\', \'C\')]\n                else:\n                    return df[[\'A\', \'C\']]\n\n            return test_impl\n\n        test_impl = gen_test_impl()\n        sdc_func = self.jit(gen_test_impl(do_jit=True))\n        pd.testing.assert_frame_equal(sdc_func(), test_impl())\n\n    def test_df_getitem_bool_series_idx_no_unboxing(self):\n        def test_impl():\n            df = pd.DataFrame({\n                \'A\': [1., -1., 0.1, -0.1],\n                \'B\': list(range(4)),\n                \'C\': [1., np.nan, np.inf, np.inf],\n            })\n            return df[df[\'A\'] == -1.]\n\n        sdc_func = self.jit(test_impl)\n        pd.testing.assert_frame_equal(sdc_func(), test_impl())\n\n    @skip_sdc_jit(\'DF.getitem unsupported Series name\')\n    def test_df_getitem_idx_no_index(self):\n        dfs = [gen_df(test_global_input_data_float64), pd.DataFrame({\'A\': []})]\n        for df in dfs:\n            with self.subTest(df=df):\n                self._test_df_getitem_bool_series_even_idx(df)\n                self._test_df_getitem_bool_array_even_idx(df)\n\n    @skip_sdc_jit(\'DF.getitem unsupported Series name\')\n    def test_df_getitem_idx_multiple_types(self):\n        int_data = [-1, 1, 0]\n        float_data = [0.1, 0., -0.1]\n        str_data = [\'ascii\', \'12345\', \'1234567890\']\n        for a, b, c in permutations([int_data, float_data, str_data], 3):\n            df = pd.DataFrame({\'A\': a, \'B\': b, \'C\': c})\n            with self.subTest(df=df):\n                self._test_df_getitem_str_literal_idx(df)\n                self._test_df_getitem_unicode_idx(df, \'A\')\n                self._test_df_getitem_slice_idx(df)\n                self._test_df_getitem_unbox_slice_idx(df, 1, 3)\n                self._test_df_getitem_tuple_idx(df)\n                self._test_df_getitem_bool_series_even_idx(df)\n                self._test_df_getitem_bool_array_even_idx(df)\n\n    def test_df_getitem_bool_series_even_idx_with_index(self):\n        df = gen_df(test_global_input_data_float64, with_index=True)\n        self._test_df_getitem_bool_series_even_idx(df)\n\n    @unittest.skip(\'DF.getitem unsupported integer columns\')\n    def test_df_getitem_int_literal_idx(self):\n        def test_impl(df):\n            return df[1]\n\n        sdc_func = self.jit(test_impl)\n        df = gen_df_int_cols(test_global_input_data_float64)\n\n        pd.testing.assert_series_equal(sdc_func(df), test_impl(df))\n\n    def test_df_getitem_attr(self):\n        def test_impl(df):\n            return df.A\n\n        sdc_func = self.jit(test_impl)\n        df = gen_df(test_global_input_data_float64)\n\n        pd.testing.assert_series_equal(sdc_func(df), test_impl(df))\n\n    @skip_numba_jit\n    def test_isin_df1(self):\n        def test_impl(df, df2):\n            return df.isin(df2)\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        df = pd.DataFrame({\'A\': np.arange(n), \'B\': np.arange(n)**2})\n        df2 = pd.DataFrame({\'A\': np.arange(n), \'C\': np.arange(n)**2})\n        df2.A[n // 2:] = n\n        pd.testing.assert_frame_equal(hpat_func(df, df2), test_impl(df, df2))\n\n    @unittest.skip(""needs dict typing in Numba"")\n    def test_isin_dict1(self):\n        def test_impl(df):\n            vals = {\'A\': [2, 3, 4], \'C\': [4, 5, 6]}\n            return df.isin(vals)\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        df = pd.DataFrame({\'A\': np.arange(n), \'B\': np.arange(n)**2})\n        pd.testing.assert_frame_equal(hpat_func(df), test_impl(df))\n\n    @skip_numba_jit\n    def test_isin_list1(self):\n        def test_impl(df):\n            vals = [2, 3, 4]\n            return df.isin(vals)\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        df = pd.DataFrame({\'A\': np.arange(n), \'B\': np.arange(n)**2})\n        pd.testing.assert_frame_equal(hpat_func(df), test_impl(df))\n\n    def test_append_df_same_cols_no_index(self):\n        def test_impl(df, df2):\n            return df.append(df2, ignore_index=True)\n\n        sdc_func = self.jit(test_impl)\n        n = 11\n        df = pd.DataFrame({\'A\': np.arange(n), \'B\': np.arange(n)**2})\n        df2 = pd.DataFrame({\'A\': np.arange(n), \'B\': np.arange(n)**2})\n        df2.A[n // 2:] = n\n        pd.testing.assert_frame_equal(sdc_func(df, df2), test_impl(df, df2))\n\n    def test_append_df_same_cols_no_index_no_unboxing(self):\n        def test_impl():\n            n = 11\n            df = pd.DataFrame({\'A\': np.arange(n), \'B\': np.arange(n)**2})\n            df2 = pd.DataFrame({\'A\': np.arange(n), \'B\': np.arange(n)**2})\n            df2.A[n // 2:] = n\n            return df.append(df2, ignore_index=True)\n\n        sdc_impl = self.jit(test_impl)\n\n        kwargs = {}\n        if platform.system() == \'Windows\':\n            # Attribute ""dtype"" are different on windows int64 vs int32\n            kwargs[\'check_dtype\'] = False\n\n        pd.testing.assert_frame_equal(sdc_impl(), test_impl(), **kwargs)\n\n    def test_append_df_same_cols_index_default(self):\n        def test_impl(df, df2):\n            return df.append(df2)\n\n        sdc_func = self.jit(test_impl)\n        n = 11\n        df = pd.DataFrame({\'A\': np.arange(n), \'B\': np.arange(n) ** 2}, index=np.arange(n) ** 4)\n        df2 = pd.DataFrame({\'A\': np.arange(n), \'B\': np.arange(n) ** 2}, index=np.arange(n) ** 8)\n        df2.A[n // 2:] = n\n\n        pd.testing.assert_frame_equal(sdc_func(df, df2), test_impl(df, df2))\n\n    def test_append_df_diff_cols_index_ignore_false(self):\n        def test_impl(df, df2):\n            return df.append(df2, ignore_index=False)\n\n        sdc_func = self.jit(test_impl)\n        n1 = 11\n        n2 = n1 * 2\n        df = pd.DataFrame({\'A\': np.arange(n1), \'B\': np.arange(n1)**2}, index=np.arange(n1) ** 4)\n        df2 = pd.DataFrame({\'C\': np.arange(n2), \'D\': np.arange(n2)**2, \'E S D\': np.arange(n2) + 100},\n                           index=np.arange(n2) ** 8)\n\n        pd.testing.assert_frame_equal(sdc_func(df, df2), test_impl(df, df2))\n\n    def test_append_df_diff_cols_index_ignore_false_no_unboxing(self):\n        def test_impl():\n            n1 = 11\n            n2 = n1 * 2\n            df = pd.DataFrame({\n                \'A\': np.arange(n1), \'B\': np.arange(n1) ** 2\n            }, index=np.arange(n1) ** 2)\n            df2 = pd.DataFrame({\n                \'C\': np.arange(n2), \'D\': np.arange(n2) ** 2,\n                \'E S D\': np.arange(n2) + 100\n            }, index=np.arange(n2) ** 4)\n            return df.append(df2, ignore_index=False)\n\n        sdc_func = self.jit(test_impl)\n        res_jit = sdc_func()\n        res_ref = test_impl()\n        pd.testing.assert_frame_equal(res_jit, res_ref)\n\n    def test_append_df_diff_cols_index_ignore_index(self):\n        def test_impl(df, df2):\n            return df.append(df2, ignore_index=True)\n\n        sdc_func = self.jit(test_impl)\n        n1 = 11\n        n2 = n1 * 2\n        df = pd.DataFrame({\'A\': np.arange(n1), \'B\': np.arange(n1)**2}, index=np.arange(n1) ** 4)\n        df2 = pd.DataFrame({\'C\': np.arange(n2), \'D\': np.arange(n2)**2, \'E S D\': np.arange(n2) + 100},\n                           index=np.arange(n2) ** 8)\n\n        pd.testing.assert_frame_equal(sdc_func(df, df2), test_impl(df, df2))\n\n    def test_append_df_diff_cols_no_index(self):\n        def test_impl(df, df2):\n            return df.append(df2)\n\n        sdc_func = self.jit(test_impl)\n        n1 = 4\n        n2 = n1 * 2\n        df = pd.DataFrame({\'A\': np.arange(n1), \'B\': np.arange(n1)**2})\n        df2 = pd.DataFrame({\'C\': np.arange(n2), \'D\': np.arange(n2)**2, \'E S D\': np.arange(n2) + 100})\n\n        pd.testing.assert_frame_equal(sdc_func(df, df2), test_impl(df, df2))\n\n    def test_append_df_cross_cols_no_index(self):\n        def test_impl(df, df2):\n            return df.append(df2, ignore_index=True)\n\n        sdc_func = self.jit(test_impl)\n        n1 = 11\n        n2 = n1 * 2\n        df = pd.DataFrame({\'A\': np.arange(n1), \'B\': np.arange(n1)**2})\n        df2 = pd.DataFrame({\'A\': np.arange(n2), \'D\': np.arange(n2)**2, \'E S D\': np.arange(n2) + 100})\n\n        pd.testing.assert_frame_equal(sdc_func(df, df2), test_impl(df, df2))\n\n    def test_append_df_exception_incomparable_index_type(self):\n        def test_impl(df, df2):\n            return df.append(df2, ignore_index=False)\n\n        sdc_func = self.jit(test_impl)\n\n        n1 = 2\n        n2 = n1 * 2\n        df = pd.DataFrame({\'A\': np.arange(n1), \'B\': np.arange(n1) ** 2}, index=[\'a\', \'b\'])\n        df2 = pd.DataFrame({\'A\': np.arange(n2), \'D\': np.arange(n2) ** 2, \'E S D\': np.arange(n2) + 100},\n                           index=np.arange(n2))\n\n        with self.assertRaises(SDCLimitation) as raises:\n            sdc_func(df, df2)\n\n        msg = ""Indexes of dataframes are expected to have comparable (both Numeric or String) types "" \\\n              ""if parameter ignore_index is set to False.""\n        self.assertIn(msg, str(raises.exception))\n\n    @skip_sdc_jit\n    def test_append_df_diff_types_no_index(self):\n        def test_impl(df, df2):\n            return df.append(df2, ignore_index=True)\n\n        hpat_func = self.jit(test_impl)\n\n        df = pd.DataFrame({\'A\': [\'cat\', \'dog\', np.nan], \'B\': [.2, .3, np.nan]})\n        df2 = pd.DataFrame({\'C\': [5, 6, 7, 8]*64, \'D\': [\'a\', \'b\', np.nan, \'\']*64})\n\n        pd.testing.assert_frame_equal(hpat_func(df, df2), test_impl(df, df2))\n\n    @skip_numba_jit(\'Unsupported functionality df.append([df2, df3])\')\n    def test_append_no_index(self):\n        def test_impl(df, df2, df3):\n            return df.append([df2, df3], ignore_index=True)\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        df = pd.DataFrame({\'A\': np.arange(n), \'B\': np.arange(n)**2})\n        df2 = pd.DataFrame({\'A\': np.arange(n), \'B\': np.arange(n)**2})\n        df2.A[n // 2:] = n\n        df3 = pd.DataFrame({\'A\': np.arange(n), \'B\': np.arange(n)**2})\n        pd.testing.assert_frame_equal(\n            hpat_func(df, df2, df3), test_impl(df, df2, df3))\n\n    @skip_numba_jit\n    def test_concat_columns1(self):\n        def test_impl(S1, S2):\n            return pd.concat([S1, S2], axis=1)\n\n        hpat_func = self.jit(test_impl)\n        S1 = pd.Series([4, 5])\n        S2 = pd.Series([6., 7.])\n        # TODO: support int as column name\n        pd.testing.assert_frame_equal(\n            hpat_func(S1, S2),\n            test_impl(S1, S2).rename(columns={0: \'0\', 1: \'1\'}))\n\n    @skip_numba_jit\n    def test_var_rename(self):\n        # tests df variable replacement in hiframes_untyped where inlining\n        # can cause extra assignments and definition handling errors\n        # TODO: inline freevar\n        def test_impl():\n            df = pd.DataFrame({\'A\': [1, 2, 3], \'B\': [2, 3, 4]})\n            # TODO: df[\'C\'] = [5,6,7]\n            df[\'C\'] = np.ones(3)\n            return inner_get_column(df)\n\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_series_equal(hpat_func(), test_impl(), check_names=False)\n\n    @unittest.skip(""Implement getting columns attribute"")\n    def test_dataframe_columns_attribute(self):\n        def test_impl():\n            df = pd.DataFrame({\'A\': [1, 2, 3], \'B\': [2, 3, 4]})\n            return df.columns\n\n        hpat_func = self.jit(test_impl)\n        np.testing.assert_array_equal(hpat_func(), test_impl())\n\n    @unittest.skip(""Implement getting columns attribute"")\n    def test_dataframe_columns_iterator(self):\n        def test_impl():\n            df = pd.DataFrame({\'A\': [1, 2, 3], \'B\': [2, 3, 4]})\n            return [column for column in df.columns]\n\n        hpat_func = self.jit(test_impl)\n        np.testing.assert_array_equal(hpat_func(), test_impl())\n\n    @unittest.skip(""Implement set_index for DataFrame"")\n    def test_dataframe_set_index(self):\n        def test_impl():\n            df = pd.DataFrame({\'month\': [1, 4, 7, 10],\n                               \'year\': [2012, 2014, 2013, 2014],\n                               \'sale\': [55, 40, 84, 31]})\n            return df.set_index(\'month\')\n\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_frame_equal(hpat_func(), test_impl())\n\n    @unittest.skip(""Implement sort_index for DataFrame"")\n    def test_dataframe_sort_index(self):\n        def test_impl():\n            df = pd.DataFrame({\'A\': [1, 2, 3, 4, 5]}, index=[100, 29, 234, 1, 150])\n            return df.sort_index()\n\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_frame_equal(hpat_func(), test_impl())\n\n    @unittest.skip(""Implement iterrows for DataFrame"")\n    def test_dataframe_iterrows(self):\n        def test_impl(df):\n            return [row for _, row in df.iterrows()]\n\n        df = pd.DataFrame({\'A\': [1, 2, 3], \'B\': [0.2, 0.5, 0.001], \'C\': [\'a\', \'bb\', \'ccc\']})\n        hpat_func = self.jit(test_impl)\n        np.testing.assert_array_equal(hpat_func(df), test_impl(df))\n\n    @unittest.skip(""Support parameter axis=1"")\n    def test_dataframe_axis_param(self):\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.arange(n), \'B\': np.arange(n)})\n            return df.sum(axis=1)\n\n        n = 100\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_series_equal(hpat_func(n), test_impl(n))\n\n    def test_min_dataframe_default(self):\n        def test_impl(df):\n            return df.min()\n\n        sdc_func = sdc.jit(test_impl)\n        df = pd.DataFrame({\n            ""A"": [12, 4, 5, 44, 1],\n            ""B"": [5.0, np.nan, 9, 2, -1],\n            # unsupported\n            # ""C"": [\'a\', \'aa\', \'d\', \'cc\', None],\n            # ""D"": [True, True, False, True, True]\n        })\n        pd.testing.assert_series_equal(sdc_func(df), test_impl(df))\n\n    @skip_sdc_jit\n    def test_median_default(self):\n        def test_impl(df):\n            return df.median()\n\n        hpat_func = sdc.jit(test_impl)\n        df = pd.DataFrame({""A"": [.2, .0, .6, .2],\n                           ""B"": [.5, .6, .7, .8],\n                           ""C"": [2, 0, 6, 2],\n                           ""D"": [.2, .1, np.nan, .5],\n                           ""E"": [-1, np.nan, 1, np.inf],\n                           ""F"": [np.nan, np.nan, np.inf, np.nan]})\n        pd.testing.assert_series_equal(hpat_func(df), test_impl(df))\n\n    def test_mean_default(self):\n        def test_impl(df):\n            return df.mean()\n\n        hpat_func = sdc.jit(test_impl)\n        df = pd.DataFrame({""A"": [.2, .0, .6, .2],\n                           ""B"": [.5, .6, .7, .8],\n                           ""C"": [2, 0, 6, 2],\n                           ""D"": [.2, .1, np.nan, .5],\n                           ""E"": [-1, np.nan, 1, np.inf],\n                           ""F H"": [np.nan, np.nan, np.inf, np.nan]})\n        pd.testing.assert_series_equal(hpat_func(df), test_impl(df))\n\n    def test_std_default(self):\n        def test_impl(df):\n            return df.std()\n\n        hpat_func = sdc.jit(test_impl)\n        df = pd.DataFrame({""A"": [.2, .0, .6, .2],\n                           ""B"": [.5, .6, .7, .8],\n                           ""C"": [2, 0, 6, 2],\n                           ""D"": [.2, .1, np.nan, .5],\n                           ""E"": [-1, np.nan, 1, np.inf],\n                           ""F H"": [np.nan, np.nan, np.inf, np.nan]})\n        pd.testing.assert_series_equal(hpat_func(df), test_impl(df))\n\n    def test_var_default(self):\n        def test_impl(df):\n            return df.var()\n\n        hpat_func = sdc.jit(test_impl)\n        df = pd.DataFrame({""A"": [.2, .0, .6, .2],\n                           ""B"": [.5, .6, .7, .8],\n                           ""C"": [2, 0, 6, 2],\n                           ""D"": [.2, .1, np.nan, .5],\n                           ""E"": [-1, np.nan, 1, np.inf],\n                           ""F H"": [np.nan, np.nan, np.inf, np.nan]})\n        pd.testing.assert_series_equal(hpat_func(df), test_impl(df))\n\n    def test_max_default(self):\n        def test_impl(df):\n            return df.max()\n\n        hpat_func = sdc.jit(test_impl)\n        df = pd.DataFrame({""A"": [.2, .0, .6, .2],\n                           ""B"": [.5, .6, .7, .8],\n                           ""C"": [2, 0, 6, 2],\n                           ""D"": [.2, .1, np.nan, .5],\n                           ""E"": [-1, np.nan, 1, np.inf],\n                           ""F H"": [np.nan, np.nan, np.inf, np.nan]})\n        pd.testing.assert_series_equal(hpat_func(df), test_impl(df))\n\n    @skip_sdc_jit\n    def test_min_default(self):\n        def test_impl(df):\n            return df.min()\n\n        hpat_func = sdc.jit(test_impl)\n        df = pd.DataFrame({""A"": [.2, .0, .6, .2],\n                           ""B"": [.5, .6, .7, .8],\n                           ""C"": [2, 0, 6, 2],\n                           ""D"": [.2, .1, np.nan, .5],\n                           ""E"": [-1, np.nan, 1, np.inf],\n                           ""F H"": [np.nan, np.nan, np.inf, np.nan]})\n        pd.testing.assert_series_equal(hpat_func(df), test_impl(df))\n\n    def test_sum_default(self):\n        def test_impl(df):\n            return df.sum()\n\n        hpat_func = sdc.jit(test_impl)\n        df = pd.DataFrame({""A"": [.2, .0, .6, .2],\n                           ""B"": [.5, .6, .7, .8],\n                           ""C"": [2, 0, 6, 2],\n                           ""D"": [.2, .1, np.nan, .5],\n                           ""E"": [-1, np.nan, 1, np.inf],\n                           ""F H"": [np.nan, np.nan, np.inf, np.nan]})\n        pd.testing.assert_series_equal(hpat_func(df), test_impl(df))\n\n    def test_prod_default(self):\n        def test_impl(df):\n            return df.prod()\n\n        hpat_func = sdc.jit(test_impl)\n        df = pd.DataFrame({""A"": [.2, .0, .6, .2],\n                           ""B"": [.5, .6, .7, .8],\n                           ""C"": [2, 0, 6, 2],\n                           ""D"": [.2, .1, np.nan, .5],\n                           ""E"": [-1, np.nan, 1, np.inf],\n                           ""F H"": [np.nan, np.nan, np.inf, np.nan]})\n        pd.testing.assert_series_equal(hpat_func(df), test_impl(df))\n\n    def test_count2_default(self):\n        def test_impl(df):\n            return df.count()\n\n        hpat_func = sdc.jit(test_impl)\n        df = pd.DataFrame({""A"": [.2, .0, .6, .2],\n                           ""B"": [.5, .6, .7, .8],\n                           ""C"": [2, 0, 6, 2],\n                           ""D"": [.2, .1, np.nan, .5],\n                           ""E"": [-1, np.nan, 1, np.inf],\n                           ""F H"": [np.nan, np.nan, np.inf, np.nan]})\n        pd.testing.assert_series_equal(hpat_func(df), test_impl(df))\n\n    @skip_sdc_jit\n    def test_pct_change(self):\n        def test_impl(df):\n            return df.pct_change()\n\n        hpat_func = sdc.jit(test_impl)\n        df = pd.DataFrame({""A"": [14, 4, 5, 4, 1, 55],\n                           ""B"": [5, 2, None, 3, 2, 32],\n                           ""C"": [20, 20, 7, 21, 8, None],\n                           ""D"": [14, None, 6, 2, 6, 4]})\n        pd.testing.assert_frame_equal(hpat_func(df), test_impl(df))\n\n    @skip_sdc_jit\n    def test_pct_change_with_parameters_limit_and_freq(self):\n        def test_impl(df, limit, freq):\n            return df.pct_change(limit=limit, freq=freq)\n\n        hpat_func = sdc.jit(test_impl)\n        df = pd.DataFrame({""A"": [14, 4, 5, 4, 1, 55],\n                           ""B"": [5, 2, None, 3, 2, 32],\n                           ""C"": [20, 20, 7, 21, 8, None],\n                           ""D"": [14, None, 6, 2, 6, 4]})\n        pd.testing.assert_frame_equal(hpat_func(df, None, None), test_impl(df, None, None))\n\n    @skip_sdc_jit\n    def test_pct_change_with_parametrs(self):\n        def test_impl(df, periods, method):\n            return df.pct_change(periods=periods, fill_method=method, limit=None, freq=None)\n\n        hpat_func = sdc.jit(test_impl)\n        df = pd.DataFrame({""A"": [.2, .0, .6, .2],\n                           ""B"": [.5, .6, .7, .8],\n                           ""C"": [2, 0, 6, 2],\n                           ""D"": [.2, .1, np.nan, .5],\n                           ""E"": [-1, np.nan, 1, np.inf],\n                           ""F"": [np.nan, np.nan, np.inf, np.nan]})\n        all_periods = [0, 1, 2, 5, 10, -1, -2, -5]\n        methods = [None, \'pad\', \'ffill\', \'backfill\', \'bfill\']\n        for periods, method in product(all_periods, methods):\n            with self.subTest(periods=periods, method=method):\n                result_ref = test_impl(df, periods, method)\n                result = hpat_func(df, periods, method)\n                pd.testing.assert_frame_equal(result, result_ref)\n\n    def test_list_convert(self):\n        def test_impl():\n            df = pd.DataFrame({\'one\': np.array([-1, np.nan, 2.5]),\n                               \'two\': [\'foo\', \'bar\', \'baz\'],\n                               \'three\': [True, False, True]})\n            return df.one.values, df.two.values, df.three.values\n        hpat_func = self.jit(test_impl)\n\n        one, two, three = hpat_func()\n        self.assertTrue(isinstance(one, np.ndarray))\n        self.assertTrue(isinstance(two, np.ndarray))\n        self.assertTrue(isinstance(three, np.ndarray))\n\n    def test_df_len(self):\n        def test_impl(df):\n            return len(df)\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        df = pd.DataFrame({\'A\': np.arange(n), \'B\': np.arange(n) ** 2})\n        self.assertEqual(hpat_func(df), test_impl(df))\n\n    @unittest.skip(""Literal unrol is broken by inline get_dataframe_data"")\n    def test_df_iterate_over_columns1(self):\n        """""" Verifies iteration over df columns using literal tuple of column indices. """"""\n        from sdc.hiframes.pd_dataframe_ext import get_dataframe_data\n        from sdc.hiframes.api import get_nan_mask\n\n        @self.jit\n        def jitted_func():\n            df = pd.DataFrame({\n                        \'A\': [\'a\', \'b\', None, \'a\', \'\', None, \'b\'],\n                        \'B\': [\'a\', \'b\', \'d\', \'a\', \'\', \'c\', \'b\'],\n                        \'C\': [np.nan, 1, 2, 1, np.nan, 2, 1],\n                        \'D\': [1, 2, 9, 5, 2, 1, 0]\n            })\n\n            # tuple of literals has to be created in a jitted function, otherwise\n            # col_id won\'t be literal and unboxing in get_dataframe_data won\'t compile\n            column_ids = (0, 1, 2, 3)\n            res_nan_mask = np.zeros(len(df), dtype=np.bool_)\n            for col_id in literal_unroll(column_ids):\n                res_nan_mask += get_nan_mask(get_dataframe_data(df, col_id))\n            return res_nan_mask\n\n        # expected is a boolean mask of df rows that have None values\n        expected = np.asarray([True, False, True, False, True, True, False])\n        result = jitted_func()\n        np.testing.assert_array_equal(result, expected)\n\n    def test_df_create_str_with_none(self):\n        """""" Verifies creation of a dataframe with a string column from a list of Optional values. """"""\n        def test_impl():\n            df = pd.DataFrame({\n                \'A\': [\'a\', \'b\', None, \'a\', \'\', None, \'b\'],\n                \'B\': [\'a\', \'b\', \'d\', \'a\', \'\', \'c\', \'b\'],\n                \'C\': [np.nan, 1, 2, 1, np.nan, 2, 1]\n            })\n            return df[\'A\'].isna()\n\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_series_equal(hpat_func(), test_impl())\n\n    def test_df_iterate_over_columns2(self):\n        """""" Verifies iteration over unboxed df columns using literal unroll. """"""\n        from sdc.hiframes.api import get_nan_mask\n\n        @self.jit\n        def jitted_func():\n            cols = (\'A\', \'B\', \'C\', \'D\')\n            df = pd.DataFrame({\n                \'A\': [\'a\', \'b\', None, \'a\', \'\', None, \'b\'],\n                \'B\': [\'a\', \'b\', \'d\', \'a\', \'\', \'c\', \'b\'],\n                \'C\': [np.nan, 1, 2, 1, np.nan, 2, 1],\n                \'D\': [1, 2, 9, 5, 2, 1, 0]\n            })\n            res_nan_mask = np.zeros(len(df), dtype=np.bool_)\n            for col in literal_unroll(cols):\n                res_nan_mask += get_nan_mask(df[col].values)\n            return res_nan_mask\n\n        # expected is a boolean mask of df rows that have None values\n        expected = np.asarray([True, False, True, False, True, True, False])\n        result = jitted_func()\n        np.testing.assert_array_equal(result, expected)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
sdc/tests/test_date.py,24,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numba\nimport numpy as np\nimport pandas as pd\nimport unittest\nfrom math import sqrt\n\nimport sdc\nfrom sdc.tests.test_base import TestCase\nfrom sdc.tests.test_utils import (count_array_OneD_Vars,\n                                  count_array_OneDs,\n                                  count_array_REPs,\n                                  count_parfor_OneD_Vars,\n                                  count_parfor_OneDs,\n                                  count_parfor_REPs,\n                                  dist_IR_contains,\n                                  skip_numba_jit,\n                                  skip_sdc_jit)\n\nfrom datetime import datetime\nimport random\n\n\nclass TestDate(TestCase):\n    @unittest.skip(""needs support for boxing/unboxing DatetimeIndex"")\n    def test_datetime_index_in(self):\n        def test_impl(dti):\n            return dti\n\n        hpat_func = self.jit(test_impl)\n        df = self._gen_str_date_df()\n        dti = pd.DatetimeIndex(df[\'str_date\'])\n        np.testing.assert_array_equal(hpat_func(dti).values, test_impl(dti).values)\n\n    @skip_numba_jit\n    def test_datetime_index(self):\n        def test_impl(df):\n            return pd.DatetimeIndex(df[\'str_date\']).values\n\n        hpat_func = self.jit(test_impl)\n        df = self._gen_str_date_df()\n        np.testing.assert_array_equal(hpat_func(df), test_impl(df))\n\n    @skip_numba_jit\n    def test_datetime_index_kw(self):\n        def test_impl(df):\n            return pd.DatetimeIndex(data=df[\'str_date\']).values\n\n        hpat_func = self.jit(test_impl)\n        df = self._gen_str_date_df()\n        np.testing.assert_array_equal(hpat_func(df), test_impl(df))\n\n    def test_datetime_arg(self):\n        def test_impl(A):\n            return A\n\n        hpat_func = self.jit(test_impl)\n        df = self._gen_str_date_df()\n        A = pd.DatetimeIndex(df[\'str_date\']).to_series()\n        np.testing.assert_array_equal(hpat_func(A), test_impl(A))\n\n    @skip_numba_jit\n    def test_datetime_getitem(self):\n        def test_impl(A):\n            return A[0]\n\n        hpat_func = self.jit(test_impl)\n        df = self._gen_str_date_df()\n        A = pd.DatetimeIndex(df[\'str_date\']).to_series()\n        self.assertEqual(hpat_func(A), test_impl(A))\n\n    @skip_numba_jit\n    def test_ts_map(self):\n        def test_impl(A):\n            return A.map(lambda x: x.hour)\n\n        hpat_func = self.jit(test_impl)\n        df = self._gen_str_date_df()\n        A = pd.DatetimeIndex(df[\'str_date\']).to_series()\n        np.testing.assert_array_equal(hpat_func(A), test_impl(A))\n\n    @skip_numba_jit\n    def test_ts_map_date(self):\n        def test_impl(A):\n            return A.map(lambda x: x.date())[0]\n\n        hpat_func = self.jit(test_impl)\n        df = self._gen_str_date_df()\n        A = pd.DatetimeIndex(df[\'str_date\']).to_series()\n        np.testing.assert_array_equal(hpat_func(A), test_impl(A))\n\n    @skip_sdc_jit\n    @skip_numba_jit\n    def test_ts_map_date2(self):\n        def test_impl(df):\n            return df.apply(lambda row: row.dt_ind.date(), axis=1)[0]\n\n        hpat_func = self.jit(test_impl)\n        df = self._gen_str_date_df()\n        df[\'dt_ind\'] = pd.DatetimeIndex(df[\'str_date\'])\n        np.testing.assert_array_equal(hpat_func(df), test_impl(df))\n\n    @skip_numba_jit\n    def test_ts_map_date_set(self):\n        def test_impl(df):\n            df[\'hpat_date\'] = df.dt_ind.map(lambda x: x.date())\n\n        hpat_func = self.jit(test_impl)\n        df = self._gen_str_date_df()\n        df[\'dt_ind\'] = pd.DatetimeIndex(df[\'str_date\'])\n        hpat_func(df)\n        df[\'pd_date\'] = df.dt_ind.map(lambda x: x.date())\n        np.testing.assert_array_equal(df[\'hpat_date\'], df[\'pd_date\'])\n\n    @skip_numba_jit\n    @unittest.expectedFailure\n    def test_date_series_unbox(self):\n        def test_impl(A):\n            return A[0]\n\n        hpat_func = self.jit(test_impl)\n        df = self._gen_str_date_df()\n        A = pd.DatetimeIndex(df[\'str_date\']).to_series().map(lambda x: x.date())\n        self.assertEqual(hpat_func(A), test_impl(A))\n\n    @skip_numba_jit\n    @unittest.expectedFailure\n    def test_date_series_unbox2(self):\n        def test_impl(A):\n            return A[0]\n\n        hpat_func = self.jit(test_impl)\n        df = self._gen_str_date_df()\n        A = pd.DatetimeIndex(df[\'str_date\']).map(lambda x: x.date())\n        self.assertEqual(hpat_func(A), test_impl(A))\n\n    @skip_numba_jit\n    def test_datetime_index_set(self):\n        def test_impl(df):\n            df[\'sdc\'] = pd.DatetimeIndex(df[\'str_date\']).values\n\n        hpat_func = self.jit(test_impl)\n        df = self._gen_str_date_df()\n        hpat_func(df)\n        df[\'std\'] = pd.DatetimeIndex(df[\'str_date\'])\n        allequal = (df[\'std\'].equals(df[\'sdc\']))\n        self.assertTrue(allequal)\n\n    def test_timestamp(self):\n        def test_impl():\n            dt = datetime(2017, 4, 26)\n            ts = pd.Timestamp(dt)\n            return ts.day + ts.hour + ts.microsecond + ts.month + ts.nanosecond + ts.second + ts.year\n\n        hpat_func = self.jit(test_impl)\n        self.assertEqual(hpat_func(), test_impl())\n\n    def test_extract(self):\n        def test_impl(s):\n            return s.month\n\n        hpat_func = self.jit(test_impl)\n        ts = pd.Timestamp(datetime(2017, 4, 26).isoformat())\n        month = hpat_func(ts)\n        self.assertEqual(month, 4)\n\n    def test_timestamp_date(self):\n        def test_impl(s):\n            return s.date()\n\n        hpat_func = self.jit(test_impl)\n        ts = pd.Timestamp(datetime(2017, 4, 26).isoformat())\n        self.assertEqual(hpat_func(ts), test_impl(ts))\n\n    @skip_numba_jit\n    def test_datetimeindex_str_comp(self):\n        def test_impl(df):\n            return (df.A >= \'2011-10-23\').values\n\n        df = pd.DataFrame({\'A\': pd.DatetimeIndex([\'2015-01-03\', \'2010-10-11\'])})\n        hpat_func = self.jit(test_impl)\n        np.testing.assert_array_equal(hpat_func(df), test_impl(df))\n\n    @skip_numba_jit\n    def test_datetimeindex_str_comp2(self):\n        def test_impl(df):\n            return (\'2011-10-23\' <= df.A).values\n\n        df = pd.DataFrame({\'A\': pd.DatetimeIndex([\'2015-01-03\', \'2010-10-11\'])})\n        hpat_func = self.jit(test_impl)\n        np.testing.assert_array_equal(hpat_func(df), test_impl(df))\n\n    @skip_numba_jit\n    def test_datetime_index_df(self):\n        def test_impl(df):\n            df = pd.DataFrame({\'A\': pd.DatetimeIndex(df[\'str_date\'])})\n            return df.A\n\n        hpat_func = self.jit(test_impl)\n        df = self._gen_str_date_df()\n        np.testing.assert_array_equal(hpat_func(df), test_impl(df))\n\n    @skip_numba_jit\n    def test_datetime_index_date(self):\n        def test_impl(df):\n            return pd.DatetimeIndex(df[\'str_date\']).date\n\n        hpat_func = self.jit(test_impl)\n        df = self._gen_str_date_df()\n        np.testing.assert_array_equal(hpat_func(df), test_impl(df))\n\n    @skip_numba_jit\n    def test_datetime_index_max(self):\n        def test_impl(df):\n            return pd.DatetimeIndex(df[\'str_date\']).max()\n\n        hpat_func = self.jit(test_impl)\n        df = self._gen_str_date_df()\n        self.assertEqual(hpat_func(df), test_impl(df))\n\n    @skip_numba_jit\n    def test_datetime_index_min(self):\n        def test_impl(df):\n            return pd.DatetimeIndex(df[\'str_date\']).min()\n\n        hpat_func = self.jit(test_impl)\n        df = self._gen_str_date_df()\n        self.assertEqual(hpat_func(df), test_impl(df))\n\n    @skip_numba_jit\n    def test_datetime_index_timedelta_days(self):\n        def test_impl(df):\n            s = pd.DatetimeIndex(df[\'str_date\'])\n            t = s - s.min()\n            return t.days\n\n        hpat_func = self.jit(test_impl)\n        df = self._gen_str_date_df()\n        np.testing.assert_array_equal(hpat_func(df), test_impl(df))\n\n    @skip_numba_jit\n    def test_datetime_index_timedelta_seconds(self):\n        def test_impl(df):\n            s = pd.DatetimeIndex(df[\'str_date\'])\n            t = s - s.min()\n            return t.seconds\n\n        hpat_func = self.jit(test_impl)\n        df = self._gen_str_date_df()\n        np.testing.assert_array_equal(hpat_func(df), test_impl(df))\n\n    @skip_numba_jit\n    def test_datetime_index_timedelta_microseconds(self):\n        def test_impl(df):\n            s = pd.DatetimeIndex(df[\'str_date\'])\n            t = s - s.min()\n            return t.microseconds\n\n        hpat_func = self.jit(test_impl)\n        df = self._gen_str_date_df()\n        np.testing.assert_array_equal(hpat_func(df), test_impl(df))\n\n    @skip_numba_jit\n    def test_datetime_index_timedelta_nanoseconds(self):\n        def test_impl(df):\n            s = pd.DatetimeIndex(df[\'str_date\'])\n            t = s - s.min()\n            return t.nanoseconds\n\n        hpat_func = self.jit(test_impl)\n        df = self._gen_str_date_df()\n        np.testing.assert_array_equal(hpat_func(df), test_impl(df))\n\n    @skip_numba_jit\n    def test_datetime_index_ret(self):\n        def test_impl(df):\n            return pd.DatetimeIndex(df[\'str_date\'])\n\n        hpat_func = self.jit(test_impl)\n        df = self._gen_str_date_df()\n        pd.testing.assert_index_equal(hpat_func(df), test_impl(df),\n                                      check_names=False)\n\n    @skip_numba_jit\n    def test_datetime_index_year(self):\n        def test_impl(df):\n            return pd.DatetimeIndex(df[\'str_date\']).year\n\n        hpat_func = self.jit(test_impl)\n        df = self._gen_str_date_df()\n        np.testing.assert_array_equal(hpat_func(df), test_impl(df))\n\n    @skip_numba_jit\n    def test_datetime_index_month(self):\n        def test_impl(df):\n            return pd.DatetimeIndex(df[\'str_date\']).month\n\n        hpat_func = self.jit(test_impl)\n        df = self._gen_str_date_df()\n        np.testing.assert_array_equal(hpat_func(df), test_impl(df))\n\n    @skip_numba_jit\n    def test_datetime_index_day(self):\n        def test_impl(df):\n            return pd.DatetimeIndex(df[\'str_date\']).day\n\n        hpat_func = self.jit(test_impl)\n        df = self._gen_str_date_df()\n        np.testing.assert_array_equal(hpat_func(df), test_impl(df))\n\n    @skip_numba_jit\n    def test_datetime_index_hour(self):\n        def test_impl(df):\n            return pd.DatetimeIndex(df[\'str_date\']).hour\n\n        hpat_func = self.jit(test_impl)\n        df = self._gen_str_date_df()\n        np.testing.assert_array_equal(hpat_func(df), test_impl(df))\n\n    @skip_numba_jit\n    def test_datetime_index_minute(self):\n        def test_impl(df):\n            return pd.DatetimeIndex(df[\'str_date\']).minute\n\n        hpat_func = self.jit(test_impl)\n        df = self._gen_str_date_df()\n        np.testing.assert_array_equal(hpat_func(df), test_impl(df))\n\n    @skip_numba_jit\n    def test_datetime_index_second(self):\n        def test_impl(df):\n            return pd.DatetimeIndex(df[\'str_date\']).second\n\n        hpat_func = self.jit(test_impl)\n        df = self._gen_str_date_df()\n        np.testing.assert_array_equal(hpat_func(df), test_impl(df))\n\n    @skip_numba_jit\n    def test_datetime_index_microsecond(self):\n        def test_impl(df):\n            return pd.DatetimeIndex(df[\'str_date\']).microsecond\n\n        hpat_func = self.jit(test_impl)\n        df = self._gen_str_date_df()\n        np.testing.assert_array_equal(hpat_func(df), test_impl(df))\n\n    @skip_numba_jit\n    def test_datetime_index_nanosecond(self):\n        def test_impl(df):\n            return pd.DatetimeIndex(df[\'str_date\']).nanosecond\n\n        hpat_func = self.jit(test_impl)\n        df = self._gen_str_date_df()\n        np.testing.assert_array_equal(hpat_func(df), test_impl(df))\n\n    @skip_numba_jit\n    def test_datetime_series_dt_date(self):\n        def test_impl(A):\n            return A.dt.date\n\n        hpat_func = self.jit(test_impl)\n        df = self._gen_str_date_df()\n        A = pd.DatetimeIndex(df[\'str_date\']).to_series()\n        # TODO: fix index and name\n        pd.testing.assert_series_equal(\n            hpat_func(A), test_impl(A).reset_index(drop=True),\n            check_names=False)\n\n    @skip_numba_jit\n    def test_datetime_series_dt_year(self):\n        def test_impl(A):\n            return A.dt.year\n\n        hpat_func = self.jit(test_impl)\n        df = self._gen_str_date_df()\n        A = pd.DatetimeIndex(df[\'str_date\']).to_series()\n        # TODO: fix index and name\n        pd.testing.assert_series_equal(\n            hpat_func(A), test_impl(A).reset_index(drop=True),\n            check_names=False)\n\n    def _gen_str_date_df(self):\n        rows = 10\n        data = []\n        for row in range(rows):\n            data.append(datetime(2017, random.randint(1, 12), random.randint(1, 28)).isoformat())\n        return pd.DataFrame({\'str_date\': data})\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
sdc/tests/test_groupby.py,55,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numba\nimport numpy as np\nimport pandas as pd\nimport platform\nimport pyarrow.parquet as pq\nimport unittest\nfrom itertools import product\n\nimport sdc\nfrom sdc.tests.test_base import TestCase\nfrom sdc.tests.test_utils import (count_array_OneDs,\n                                  count_array_REPs,\n                                  count_parfor_OneDs,\n                                  count_parfor_REPs,\n                                  dist_IR_contains,\n                                  get_start_end,\n                                  skip_numba_jit,\n                                  skip_sdc_jit,\n                                  sdc_limitation)\nfrom sdc.tests.test_series import gen_frand_array\n\n\n_pivot_df1 = pd.DataFrame({""A"": [""foo"", ""foo"", ""foo"", ""foo"", ""foo"",\n                                 ""bar"", ""bar"", ""bar"", ""bar""],\n                           ""B"": [""one"", ""one"", ""one"", ""two"", ""two"",\n                                 ""one"", ""one"", ""two"", ""two""],\n                           ""C"": [""small"", ""large"", ""large"", ""small"",\n                                 ""small"", ""large"", ""small"", ""small"",\n                                 ""large""],\n                           ""D"": [1, 2, 2, 6, 3, 4, 5, 6, 9]})\n\n_default_df_numeric_data = {\n                    \'A\': [2, 1, 2, 1, 2, 2, 1, 0, 3, 1, 3],\n                    \'B\': np.arange(11, dtype=np.intp),\n                    \'C\': np.arange(11, dtype=np.float_),\n                    \'D\': [np.nan, 2., -1.3, np.nan, 3.5, 0, 10, 0.42, np.nan, -2.5, 23],\n                    \'E\': [np.inf, 2., -1.3, -np.inf, 3.5, 0, 10, 0.42, np.nan, -2.5, 23]\n    }\n\nclass TestGroupBy(TestCase):\n\n    @skip_sdc_jit(\'Fails with old-pipeline from the start\')\n    @sdc_limitation\n    def test_dataframe_groupby_index_name(self):\n        """"""SDC indexes do not have names, so index created from a named Series looses it\'s name.""""""\n        def test_impl(df):\n            return df.groupby(\'A\').min()\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        df = pd.DataFrame({\n                    \'A\': [2, 1, 1, 1, 2, 2, 1, 0, 3, 1, 3],\n                    \'B\': np.arange(n, dtype=np.intp)\n        })\n        result = hpat_func(df)\n        result_ref = test_impl(df)\n        pd.testing.assert_frame_equal(result, result_ref)\n\n    @skip_sdc_jit(\'Fails with old-pipeline from the start\')\n    def test_dataframe_groupby_by_all_dtypes(self):\n        def test_impl(df):\n            return df.groupby(\'A\').count()\n        hpat_func = self.jit(test_impl)\n\n        dtype_to_column_data = {\n                \'int\': [2, 1, 1, 1, 2, 2, 1, 0, 3, 1, 3],\n                \'float\': [2, 1, 1, 1, 2, 2, 1, 3, np.nan, 1, np.nan],\n                \'string\': [\'b\', \'a\', \'a\', \'a\', \'b\', \'b\', \'a\', \' \', None, \'a\', None]\n        }\n        df = pd.DataFrame(_default_df_numeric_data)\n        for dtype, col_data in dtype_to_column_data.items():\n            with self.subTest(by_dtype=dtype, by_data=col_data):\n                df[\'A\'] = col_data\n                result = hpat_func(df)\n                result_ref = test_impl(df)\n                # TODO: implement index classes, as current indexes do not have names\n                pd.testing.assert_frame_equal(result, result_ref, check_names=False)\n\n    @skip_sdc_jit(\'Fails with old-pipeline from the start\')\n    def test_dataframe_groupby_sort(self):\n        def test_impl(df, param):\n            return df.groupby(\'A\', sort=param).min()\n        hpat_func = self.jit(test_impl)\n\n        n, m = 1000, 20\n        np.random.seed(0)\n        df = pd.DataFrame({\n                    \'A\': np.random.choice(np.arange(m), n),\n                    \'B\': np.arange(n, dtype=np.intp),\n                    \'C\': np.arange(n, dtype=np.float_),\n                    \'D\': gen_frand_array(n, nancount=n // 2),\n        })\n\n        for value in [True, False]:\n            with self.subTest(sort=value):\n                result = hpat_func(df, value) if value else hpat_func(df, value).sort_index()\n                result_ref = test_impl(df, value) if value else hpat_func(df, value).sort_index()\n                # TODO: implement index classes, as current indexes do not have names\n                pd.testing.assert_frame_equal(result, result_ref, check_names=False)\n\n    @skip_sdc_jit(\'Fails with old-pipeline from the start\')\n    def test_dataframe_groupby_count(self):\n        def test_impl(df):\n            return df.groupby(\'A\').count()\n        hpat_func = self.jit(test_impl)\n\n        df = pd.DataFrame(_default_df_numeric_data)\n        result = hpat_func(df)\n        result_ref = test_impl(df)\n        # TODO: implement index classes, as current indexes do not have names\n        pd.testing.assert_frame_equal(result, result_ref, check_names=False)\n\n    def test_dataframe_groupby_count_no_unboxing(self):\n        def test_impl():\n            df = pd.DataFrame({\n                \'A\': [2, 1, 2, 1, 2, 2, 1, 0, 3, 1, 3],\n                \'B\': np.arange(11),\n                \'C\': [np.nan, 2., -1.3, np.nan, 3.5, 0, 10, 0.42, np.nan, -2.5, 23],\n                \'D\': [np.inf, 2., -1.3, -np.inf, 3.5, 0, 10, 0.42, np.nan, -2.5, 23]\n            })\n            return df.groupby(\'A\').count()\n\n        sdc_impl = self.jit(test_impl)\n\n        result_jit = sdc_impl()\n        result_ref = test_impl()\n        # TODO: implement index classes, as current indexes do not have names\n        pd.testing.assert_frame_equal(result_jit, result_ref, check_names=False)\n\n\n    @skip_sdc_jit(\'Fails with old-pipeline from the start\')\n    def test_dataframe_groupby_max(self):\n        def test_impl(df):\n            return df.groupby(\'A\').max()\n        hpat_func = self.jit(test_impl)\n\n        df = pd.DataFrame(_default_df_numeric_data)\n        result = hpat_func(df)\n        result_ref = test_impl(df)\n        # TODO: implement index classes, as current indexes do not have names\n        pd.testing.assert_frame_equal(result, result_ref, check_names=False)\n\n    def test_dataframe_groupby_max_no_unboxing(self):\n        def test_impl():\n            df = pd.DataFrame({\n                \'A\': [2, 1, 2, 1, 2, 2, 1, 0, 3, 1, 3],\n                \'B\': np.arange(11),\n                \'C\': [np.nan, 2., -1.3, np.nan, 3.5, 0, 10, 0.42, np.nan, -2.5, 23],\n                \'D\': [np.inf, 2., -1.3, -np.inf, 3.5, 0, 10, 0.42, np.nan, -2.5, 23]\n            })\n            return df.groupby(\'A\').max()\n\n        sdc_impl = self.jit(test_impl)\n\n        # TODO: implement index classes, as current indexes do not have names\n        kwargs = {\'check_names\': False}\n        if platform.system() == \'Windows\':\n            # Attribute ""dtype"" are different on windows int64 vs int32\n            kwargs[\'check_dtype\'] = False\n\n        pd.testing.assert_frame_equal(sdc_impl(), test_impl(), **kwargs)\n\n    @skip_sdc_jit(\'Fails with old-pipeline from the start\')\n    def test_dataframe_groupby_min(self):\n        def test_impl(df):\n            return df.groupby(\'A\').min()\n        hpat_func = self.jit(test_impl)\n\n        df = pd.DataFrame(_default_df_numeric_data)\n        result = hpat_func(df)\n        result_ref = test_impl(df)\n        # TODO: implement index classes, as current indexes do not have names\n        pd.testing.assert_frame_equal(result, result_ref, check_names=False)\n\n    def test_dataframe_groupby_min_no_unboxing(self):\n        def test_impl():\n            df = pd.DataFrame({\n                \'A\': [2, 1, 2, 1, 2, 2, 1, 0, 3, 1, 3],\n                \'B\': np.arange(11),\n                \'C\': [np.nan, 2., -1.3, np.nan, 3.5, 0, 10, 0.42, np.nan, -2.5, 23],\n                \'D\': [np.inf, 2., -1.3, -np.inf, 3.5, 0, 10, 0.42, np.nan, -2.5, 23]\n            })\n            return df.groupby(\'A\').min()\n\n        sdc_impl = self.jit(test_impl)\n\n        # TODO: implement index classes, as current indexes do not have names\n        kwargs = {\'check_names\': False}\n        if platform.system() == \'Windows\':\n            # Attribute ""dtype"" are different on windows int64 vs int32\n            kwargs[\'check_dtype\'] = False\n\n        pd.testing.assert_frame_equal(sdc_impl(), test_impl(), **kwargs)\n\n    @skip_sdc_jit(\'Fails with old-pipeline from the start\')\n    def test_dataframe_groupby_mean(self):\n        def test_impl(df):\n            return df.groupby(\'A\').mean()\n        hpat_func = self.jit(test_impl)\n\n        df = pd.DataFrame(_default_df_numeric_data)\n        result = hpat_func(df)\n        result_ref = test_impl(df)\n        # TODO: implement index classes, as current indexes do not have names\n        pd.testing.assert_frame_equal(result, result_ref, check_names=False)\n\n    def test_dataframe_groupby_mean_no_unboxing(self):\n        def test_impl():\n            df = pd.DataFrame({\n                \'A\': [2, 1, 2, 1, 2, 2, 1, 0, 3, 1, 3],\n                \'B\': np.arange(11),\n                \'C\': [np.nan, 2., -1.3, np.nan, 3.5, 0, 10, 0.42, np.nan, -2.5, 23],\n                \'D\': [np.inf, 2., -1.3, -np.inf, 3.5, 0, 10, 0.42, np.nan, -2.5, 23]\n            })\n            return df.groupby(\'A\').mean()\n\n        sdc_impl = self.jit(test_impl)\n\n        result_jit = sdc_impl()\n        result_ref = test_impl()\n        # TODO: implement index classes, as current indexes do not have names\n        pd.testing.assert_frame_equal(result_jit, result_ref, check_names=False)\n\n    @skip_sdc_jit(\'Fails with old-pipeline from the start\')\n    def test_dataframe_groupby_median(self):\n        def test_impl(df):\n            return df.groupby(\'A\').median()\n        hpat_func = self.jit(test_impl)\n\n        df = pd.DataFrame(_default_df_numeric_data)\n        result = hpat_func(df)\n        result_ref = test_impl(df)\n        # TODO: implement index classes, as current indexes do not have names\n        pd.testing.assert_frame_equal(result, result_ref, check_names=False)\n\n    def test_dataframe_groupby_median_no_unboxing(self):\n        def test_impl():\n            df = pd.DataFrame({\n                \'A\': [2, 1, 2, 1, 2, 2, 1, 0, 3, 1, 3],\n                \'B\': np.arange(11),\n                \'C\': [np.nan, 2., -1.3, np.nan, 3.5, 0, 10, 0.42, np.nan, -2.5, 23],\n                \'D\': [np.inf, 2., -1.3, -np.inf, 3.5, 0, 10, 0.42, np.nan, -2.5, 23]\n            })\n            return df.groupby(\'A\').median()\n\n        sdc_impl = self.jit(test_impl)\n\n        result_jit = sdc_impl()\n        result_ref = test_impl()\n        # TODO: implement index classes, as current indexes do not have names\n        pd.testing.assert_frame_equal(result_jit, result_ref, check_names=False)\n\n    @skip_sdc_jit(\'Fails with old-pipeline from the start\')\n    @unittest.expectedFailure   # pandas groupby.median returns unstable dtype (int or float) unlike series.median\n    def test_dataframe_groupby_median_result_dtype(self):\n        def test_impl(df):\n            return df.groupby(\'A\').median()\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        df = pd.DataFrame({\n                    \'A\': [2, 1, 1, 1, 2, 2, 1, 0, 3, 1, 3],\n                    \'B\': np.arange(n, dtype=np.intp)\n        })\n        result = hpat_func(df)\n        result_ref = test_impl(df)\n        # TODO: implement index classes, as current indexes do not have names\n        pd.testing.assert_frame_equal(result, result_ref, check_names=False)\n\n    @skip_sdc_jit(\'Fails with old-pipeline from the start\')\n    def test_dataframe_groupby_prod(self):\n        def test_impl(df):\n            return df.groupby(\'A\').prod()\n        hpat_func = self.jit(test_impl)\n\n        df = pd.DataFrame(_default_df_numeric_data)\n        result = hpat_func(df)\n        result_ref = test_impl(df)\n        # TODO: implement index classes, as current indexes do not have names\n        pd.testing.assert_frame_equal(result, result_ref, check_names=False)\n\n    def test_dataframe_groupby_prod_no_unboxing(self):\n        def test_impl():\n            df = pd.DataFrame({\n                \'A\': [2, 1, 2, 1, 2, 2, 1, 0, 3, 1, 3],\n                \'B\': np.arange(11),\n                \'C\': [np.nan, 2., -1.3, np.nan, 3.5, 0, 10, 0.42, np.nan, -2.5, 23],\n                \'D\': [np.inf, 2., -1.3, -np.inf, 3.5, 0, 10, 0.42, np.nan, -2.5, 23]\n            })\n            return df.groupby(\'A\').prod()\n\n        sdc_impl = self.jit(test_impl)\n\n        # TODO: implement index classes, as current indexes do not have names\n        kwargs = {\'check_names\': False}\n        if platform.system() == \'Windows\':\n            # Attribute ""dtype"" are different on windows int64 vs int32\n            kwargs[\'check_dtype\'] = False\n\n        pd.testing.assert_frame_equal(sdc_impl(), test_impl(), **kwargs)\n\n    @skip_sdc_jit(\'Fails with old-pipeline from the start\')\n    @skip_numba_jit(""BUG: SDC impl of Series.sum returns float64 on as series of ints"")\n    def test_dataframe_groupby_sum(self):\n        def test_impl(df):\n            return df.groupby(\'A\').sum()\n        hpat_func = self.jit(test_impl)\n\n        df = pd.DataFrame(_default_df_numeric_data)\n        result = hpat_func(df)\n        result_ref = test_impl(df)\n        # TODO: implement index classes, as current indexes do not have names\n        pd.testing.assert_frame_equal(result, result_ref, check_names=False)\n\n    def test_dataframe_groupby_sum_no_unboxing(self):\n        def test_impl():\n            df = pd.DataFrame({\n                \'A\': [2, 1, 2, 1, 2, 2, 1, 0, 3, 1, 3],\n                \'B\': np.arange(11),\n                \'C\': [np.nan, 2., -1.3, np.nan, 3.5, 0, 10, 0.42, np.nan, -2.5, 23],\n                \'D\': [np.inf, 2., -1.3, -np.inf, 3.5, 0, 10, 0.42, np.nan, -2.5, 23]\n            })\n            return df.groupby(\'A\').sum()\n\n        sdc_impl = self.jit(test_impl)\n\n        # TODO: implement index classes, as current indexes do not have names\n        # Attribute ""dtype"" are different int64 vs int32\n        kwargs = {\'check_names\': False, \'check_dtype\': False}\n        pd.testing.assert_frame_equal(sdc_impl(), test_impl(), **kwargs)\n\n    @skip_sdc_jit(\'Fails with old-pipeline from the start\')\n    def test_dataframe_groupby_std(self):\n        def test_impl(df):\n            return df.groupby(\'A\').std()\n        hpat_func = self.jit(test_impl)\n\n        df = pd.DataFrame(_default_df_numeric_data)\n        result = hpat_func(df)\n        result_ref = test_impl(df)\n        # TODO: implement index classes, as current indexes do not have names\n        pd.testing.assert_frame_equal(result, result_ref, check_names=False)\n\n    def test_dataframe_groupby_std_no_unboxing(self):\n        def test_impl():\n            df = pd.DataFrame({\n                \'A\': [2, 1, 2, 1, 2, 2, 1, 0, 3, 1, 3],\n                \'B\': np.arange(11),\n                \'C\': [np.nan, 2., -1.3, np.nan, 3.5, 0, 10, 0.42, np.nan, -2.5, 23],\n                \'D\': [np.inf, 2., -1.3, -np.inf, 3.5, 0, 10, 0.42, np.nan, -2.5, 23]\n            })\n            return df.groupby(\'A\').std()\n\n        sdc_impl = self.jit(test_impl)\n\n        result_jit = sdc_impl()\n        result_ref = test_impl()\n        # TODO: implement index classes, as current indexes do not have names\n        pd.testing.assert_frame_equal(result_jit, result_ref, check_names=False)\n\n    @skip_sdc_jit(\'Fails with old-pipeline from the start\')\n    def test_dataframe_groupby_var(self):\n        def test_impl(df):\n            return df.groupby(\'A\').var()\n        hpat_func = self.jit(test_impl)\n\n        df = pd.DataFrame(_default_df_numeric_data)\n        result = hpat_func(df)\n        result_ref = test_impl(df)\n        # TODO: implement index classes, as current indexes do not have names\n        pd.testing.assert_frame_equal(result, result_ref, check_names=False)\n\n    def test_dataframe_groupby_var_no_unboxing(self):\n        def test_impl():\n            df = pd.DataFrame({\n                \'A\': [2, 1, 2, 1, 2, 2, 1, 0, 3, 1, 3],\n                \'B\': np.arange(11),\n                \'C\': [np.nan, 2., -1.3, np.nan, 3.5, 0, 10, 0.42, np.nan, -2.5, 23],\n                \'D\': [np.inf, 2., -1.3, -np.inf, 3.5, 0, 10, 0.42, np.nan, -2.5, 23]\n            })\n            return df.groupby(\'A\').var()\n\n        sdc_impl = self.jit(test_impl)\n\n        result_jit = sdc_impl()\n        result_ref = test_impl()\n        # TODO: implement index classes, as current indexes do not have names\n        pd.testing.assert_frame_equal(result_jit, result_ref, check_names=False)\n\n    @skip_sdc_jit\n    @skip_numba_jit\n    def test_agg_seq(self):\n        def test_impl(df):\n            A = df.groupby(\'A\')[\'B\'].agg(lambda x: x.max() - x.min())\n            return A.values\n\n        hpat_func = self.jit(test_impl)\n        df = pd.DataFrame({\'A\': [2, 1, 1, 1, 2, 2, 1], \'B\': [-8, 2, 3, 1, 5, 6, 7]})\n        # np.testing.assert_array_equal(hpat_func(df), test_impl(df))\n        self.assertEqual(set(hpat_func(df)), set(test_impl(df)))\n\n    @skip_numba_jit(""BUG: SDC impl of Series.sum returns float64 on as series of ints"")\n    def test_agg_seq_sum(self):\n        def test_impl(df):\n            return df.groupby(\'A\')[\'B\'].sum()\n\n        hpat_func = self.jit(test_impl)\n        df = pd.DataFrame({\'A\': [2, 1, 1, 1, 2, 2, 1], \'B\': [-8, 2, 3, 1, 5, 6, 7]})\n        result = hpat_func(df)\n        result_ref = test_impl(df)\n        pd.testing.assert_frame_equal(result, result_ref, check_names=False)\n\n    @skip_sdc_jit(""Old-style implementation returns ndarray, not a Series"")\n    def test_agg_seq_count(self):\n        def test_impl(df):\n            return df.groupby(\'A\')[\'B\'].count()\n\n        hpat_func = self.jit(test_impl)\n        df = pd.DataFrame({\'A\': [2, 1, 1, 1, 2, 2, 1], \'B\': [-8, 2, 3, 1, 5, 6, 7]})\n        result = hpat_func(df)\n        result_ref = test_impl(df)\n        pd.testing.assert_series_equal(result, result_ref, check_names=False)\n\n    @skip_sdc_jit(""Old-style implementation returns ndarray, not a Series"")\n    def test_agg_seq_mean(self):\n        def test_impl(df):\n            return df.groupby(\'A\')[\'B\'].mean()\n\n        hpat_func = self.jit(test_impl)\n        df = pd.DataFrame({\'A\': [2, 1, 1, 1, 2, 2, 1], \'B\': [-8, 2, 3, 1, 5, 6, 7]})\n        result = hpat_func(df)\n        result_ref = test_impl(df)\n        pd.testing.assert_series_equal(result, result_ref, check_names=False)\n\n    @skip_sdc_jit(""Old-style implementation returns ndarray, not a Series"")\n    def test_agg_seq_median(self):\n        def test_impl(df):\n            return df.groupby(\'A\')[\'B\'].median()\n\n        hpat_func = self.jit(test_impl)\n        df = pd.DataFrame({\'A\': [2, 1, 1, 1, 2, 2, 1], \'B\': [-8, 2, 3, 1, 5, 6, 7]})\n        result = hpat_func(df)\n        result_ref = test_impl(df)\n        pd.testing.assert_series_equal(result, result_ref, check_names=False)\n\n    @skip_sdc_jit(""Old-style implementation returns ndarray, not a Series"")\n    def test_agg_seq_min(self):\n        def test_impl(df):\n            return df.groupby(\'A\')[\'B\'].min()\n\n        hpat_func = self.jit(test_impl)\n        df = pd.DataFrame({\'A\': [2, 1, 1, 1, 2, 2, 1], \'B\': [-8, 2, 3, 1, 5, 6, 7]})\n        result = hpat_func(df)\n        result_ref = test_impl(df)\n        pd.testing.assert_series_equal(result, result_ref, check_names=False)\n\n    @skip_numba_jit\n    def test_agg_seq_min_date(self):\n        def test_impl(df):\n            df2 = df.groupby(\'A\', as_index=False).min()\n            return df2\n\n        hpat_func = self.jit(test_impl)\n        df = pd.DataFrame({\'A\': [2, 1, 1, 1, 2, 2, 1], \'B\': pd.date_range(\'2019-1-3\', \'2019-1-9\')})\n        self.assertEqual(set(hpat_func(df)), set(test_impl(df)))\n\n    @skip_sdc_jit(""Old-style implementation returns ndarray, not a Series"")\n    def test_agg_seq_max(self):\n        def test_impl(df):\n            return df.groupby(\'A\')[\'B\'].max()\n\n        hpat_func = self.jit(test_impl)\n        df = pd.DataFrame({\'A\': [2, 1, 1, 1, 2, 2, 1], \'B\': [-8, 2, 3, 1, 5, 6, 7]})\n        result = hpat_func(df)\n        result_ref = test_impl(df)\n        pd.testing.assert_series_equal(result, result_ref, check_names=False)\n\n    @skip_numba_jit\n    def test_agg_seq_as_index(self):\n        def test_impl(df):\n            df2 = df.groupby(\'A\', as_index=False).mean()\n            return df2.A.values\n\n        hpat_func = self.jit(test_impl)\n        df = pd.DataFrame({\'A\': [2, 1, 1, 1, 2, 2, 1], \'B\': [-8, 2, 3, 1, 5, 6, 7]})\n        self.assertEqual(set(hpat_func(df)), set(test_impl(df)))\n\n    @skip_sdc_jit(""Old-style implementation returns ndarray, not a Series"")\n    def test_agg_seq_prod(self):\n        def test_impl(df):\n            return df.groupby(\'A\')[\'B\'].prod()\n\n        hpat_func = self.jit(test_impl)\n        df = pd.DataFrame({\'A\': [2, 1, 1, 1, 2, 2, 1], \'B\': [-8, 2, 3, 1, 5, 6, 7]})\n        result = hpat_func(df)\n        result_ref = test_impl(df)\n        pd.testing.assert_series_equal(result, result_ref, check_names=False)\n\n    @skip_sdc_jit\n    def test_agg_seq_var(self):\n        def test_impl(df):\n            return df.groupby(\'A\')[\'B\'].var()\n\n        hpat_func = self.jit(test_impl)\n        df = pd.DataFrame({\'A\': [2, 1, 1, 1, 2, 2, 1], \'B\': [-8, 2, 3, 1, 5, 6, 7]})\n        result = hpat_func(df)\n        result_ref = test_impl(df)\n        pd.testing.assert_series_equal(result, result_ref, check_names=False)\n\n    @skip_sdc_jit\n    def test_agg_seq_std(self):\n        def test_impl(df):\n            return df.groupby(\'A\')[\'B\'].std()\n\n        hpat_func = self.jit(test_impl)\n        df = pd.DataFrame({\'A\': [2, 1, 1, 1, 2, 2, 1], \'B\': [-8, 2, 3, 1, 5, 6, 7]})\n        result = hpat_func(df)\n        result_ref = test_impl(df)\n        pd.testing.assert_series_equal(result, result_ref, check_names=False)\n\n    @skip_numba_jit\n    def test_agg_multikey_seq(self):\n        def test_impl(df):\n            A = df.groupby([\'A\', \'C\'])[\'B\'].sum()\n            return A.values\n\n        hpat_func = self.jit(test_impl)\n        df = pd.DataFrame({\'A\': [2, 1, 1, 1, 2, 2, 1], \'B\': [-8, 2, 3, 1, 5, 6, 7],\n                           \'C\': [3, 5, 6, 5, 4, 4, 3]})\n        self.assertEqual(set(hpat_func(df)), set(test_impl(df)))\n\n    @skip_sdc_jit\n    @skip_numba_jit\n    def test_agg_multikey_parallel(self):\n        def test_impl(in_A, in_B, in_C):\n            df = pd.DataFrame({\'A\': in_A, \'B\': in_B, \'C\': in_C})\n            A = df.groupby([\'A\', \'C\'])[\'B\'].sum()\n            return A.sum()\n\n        hpat_func = self.jit(locals={\'in_A:input\': \'distributed\',\n                                     \'in_B:input\': \'distributed\',\n                                     \'in_C:input\': \'distributed\'})(test_impl)\n        df = pd.DataFrame({\'A\': [2, 1, 1, 1, 2, 2, 1], \'B\': [-8, 2, 3, 1, 5, 6, 7],\n                           \'C\': [3, 5, 6, 5, 4, 4, 3]})\n        start, end = get_start_end(len(df))\n        h_A = df.A.values[start:end]\n        h_B = df.B.values[start:end]\n        h_C = df.C.values[start:end]\n        p_A = df.A.values\n        p_B = df.B.values\n        p_C = df.C.values\n        h_res = hpat_func(h_A, h_B, h_C)\n        p_res = test_impl(p_A, p_B, p_C)\n        self.assertEqual(h_res, p_res)\n\n    @skip_sdc_jit\n    @skip_numba_jit\n    def test_agg_parallel(self):\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.ones(n, np.int64), \'B\': np.arange(n)})\n            A = df.groupby(\'A\')[\'B\'].agg(lambda x: x.max() - x.min())\n            return A.sum()\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        self.assertEqual(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit\n    def test_agg_parallel_sum(self):\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.ones(n, np.int64), \'B\': np.arange(n)})\n            A = df.groupby(\'A\')[\'B\'].sum()\n            return A.sum()\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        self.assertEqual(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit\n    def test_agg_parallel_count(self):\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.ones(n, np.int64), \'B\': np.arange(n)})\n            A = df.groupby(\'A\')[\'B\'].count()\n            return A.sum()\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        self.assertEqual(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit\n    def test_agg_parallel_mean(self):\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.ones(n, np.int64), \'B\': np.arange(n)})\n            A = df.groupby(\'A\')[\'B\'].mean()\n            return A.sum()\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        self.assertEqual(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit\n    def test_agg_parallel_min(self):\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.ones(n, np.int64), \'B\': np.arange(n)})\n            A = df.groupby(\'A\')[\'B\'].min()\n            return A.sum()\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        self.assertEqual(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit\n    def test_agg_parallel_max(self):\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.ones(n, np.int64), \'B\': np.arange(n)})\n            A = df.groupby(\'A\')[\'B\'].max()\n            return A.sum()\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        self.assertEqual(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_sdc_jit\n    @skip_numba_jit\n    def test_agg_parallel_var(self):\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.ones(n, np.int64), \'B\': np.arange(n)})\n            A = df.groupby(\'A\')[\'B\'].var()\n            return A.sum()\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        self.assertEqual(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_sdc_jit\n    @skip_numba_jit\n    def test_agg_parallel_std(self):\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.ones(n, np.int64), \'B\': np.arange(n)})\n            A = df.groupby(\'A\')[\'B\'].std()\n            return A.sum()\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        self.assertEqual(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @unittest.skip(\'AssertionError - fix needed\\n\'\n                   \'16 != 20\\n\')\n    def test_agg_parallel_str(self):\n        def test_impl():\n            df = pq.read_table(""groupby3.pq"").to_pandas()\n            A = df.groupby(\'A\')[\'B\'].agg(lambda x: x.max() - x.min())\n            return A.sum()\n\n        hpat_func = self.jit(test_impl)\n        self.assertEqual(hpat_func(), test_impl())\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit\n    def test_agg_parallel_all_col(self):\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.ones(n, np.int64), \'B\': np.arange(n)})\n            df2 = df.groupby(\'A\').max()\n            return df2.B.sum()\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        self.assertEqual(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit\n    def test_agg_parallel_as_index(self):\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.ones(n, np.int64), \'B\': np.arange(n)})\n            df2 = df.groupby(\'A\', as_index=False).max()\n            return df2.A.sum()\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        self.assertEqual(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_sdc_jit\n    @skip_numba_jit\n    def test_muti_hiframes_node_filter_agg(self):\n        def test_impl(df, cond):\n            df2 = df[cond]\n            c = df2.groupby(\'A\')[\'B\'].count()\n            return df2.C, c\n\n        hpat_func = self.jit(test_impl)\n        df = pd.DataFrame({\'A\': [2, 1, 1, 1, 2, 2, 1], \'B\': [-8, 2, 3, 1, 5, 6, 7], \'C\': [2, 3, -1, 1, 2, 3, -1]})\n        cond = df.A > 1\n        res = test_impl(df, cond)\n        h_res = hpat_func(df, cond)\n        self.assertEqual(set(res[1]), set(h_res[1]))\n        np.testing.assert_array_equal(res[0], h_res[0])\n\n    @skip_sdc_jit\n    @skip_numba_jit\n    def test_agg_seq_str(self):\n        def test_impl(df):\n            A = df.groupby(\'A\')[\'B\'].agg(lambda x: (x == \'aa\').sum())\n            return A.values\n\n        hpat_func = self.jit(test_impl)\n        df = pd.DataFrame({\'A\': [\'aa\', \'b\', \'b\', \'b\', \'aa\', \'aa\', \'b\'],\n                           \'B\': [\'ccc\', \'a\', \'bb\', \'aa\', \'dd\', \'ggg\', \'rr\']})\n        # np.testing.assert_array_equal(hpat_func(df), test_impl(df))\n        self.assertEqual(set(hpat_func(df)), set(test_impl(df)))\n\n    @skip_numba_jit\n    def test_agg_seq_count_str(self):\n        def test_impl(df):\n            A = df.groupby(\'A\')[\'B\'].count()\n            return A.values\n\n        hpat_func = self.jit(test_impl)\n        df = pd.DataFrame({\'A\': [\'aa\', \'b\', \'b\', \'b\', \'aa\', \'aa\', \'b\'],\n                           \'B\': [\'ccc\', \'a\', \'bb\', \'aa\', \'dd\', \'ggg\', \'rr\']})\n        # np.testing.assert_array_equal(hpat_func(df), test_impl(df))\n        self.assertEqual(set(hpat_func(df)), set(test_impl(df)))\n\n    @skip_numba_jit\n    def test_pivot(self):\n        def test_impl(df):\n            pt = df.pivot_table(index=\'A\', columns=\'C\', values=\'D\', aggfunc=\'sum\')\n            return (pt.small.values, pt.large.values)\n\n        hpat_func = self.jit(pivots={\'pt\': [\'small\', \'large\']})(test_impl)\n        self.assertEqual(\n            set(hpat_func(_pivot_df1)[0]), set(test_impl(_pivot_df1)[0]))\n        self.assertEqual(\n            set(hpat_func(_pivot_df1)[1]), set(test_impl(_pivot_df1)[1]))\n\n    @skip_numba_jit\n    def test_pivot_parallel(self):\n        def test_impl():\n            df = pd.read_parquet(""pivot2.pq"")\n            pt = df.pivot_table(index=\'A\', columns=\'C\', values=\'D\', aggfunc=\'sum\')\n            res = pt.small.values.sum()\n            return res\n\n        hpat_func = self.jit(\n            pivots={\'pt\': [\'small\', \'large\']})(test_impl)\n        self.assertEqual(hpat_func(), test_impl())\n\n    @skip_numba_jit\n    def test_crosstab1(self):\n        def test_impl(df):\n            pt = pd.crosstab(df.A, df.C)\n            return (pt.small.values, pt.large.values)\n\n        hpat_func = self.jit(pivots={\'pt\': [\'small\', \'large\']})(test_impl)\n        self.assertEqual(\n            set(hpat_func(_pivot_df1)[0]), set(test_impl(_pivot_df1)[0]))\n        self.assertEqual(\n            set(hpat_func(_pivot_df1)[1]), set(test_impl(_pivot_df1)[1]))\n\n    @skip_numba_jit\n    def test_crosstab_parallel1(self):\n        def test_impl():\n            df = pd.read_parquet(""pivot2.pq"")\n            pt = pd.crosstab(df.A, df.C)\n            res = pt.small.values.sum()\n            return res\n\n        hpat_func = self.jit(\n            pivots={\'pt\': [\'small\', \'large\']})(test_impl)\n        self.assertEqual(hpat_func(), test_impl())\n\n    @unittest.skip(""Implement groupby(lambda) for DataFrame"")\n    def test_groupby_lambda(self):\n        def test_impl(df):\n            group = df.groupby(lambda x: x % 2 == 0)\n            return group.count()\n\n        df = pd.DataFrame({\'A\': [2, 1, 1, 1, 2, 2, 1], \'B\': [-8, 2, 3, 1, 5, 6, 7]})\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_frame_equal(hpat_func(df), test_impl(df))\n\n    def test_dataframe_groupby_getitem_literal_tuple(self):\n        def test_impl(df):\n            return df.groupby(\'A\')[\'B\', \'C\'].count()\n        hpat_func = self.jit(test_impl)\n\n        df = pd.DataFrame(_default_df_numeric_data)\n        result = hpat_func(df)\n        result_ref = test_impl(df)\n        # TODO: implement index classes, as current indexes do not have names\n        pd.testing.assert_frame_equal(result, result_ref, check_names=False)\n\n    def test_dataframe_groupby_getitem_literal_str(self):\n        def test_impl(df):\n            return df.groupby(\'C\')[\'B\'].count()\n        hpat_func = self.jit(test_impl)\n\n        df = pd.DataFrame(_default_df_numeric_data)\n        result = hpat_func(df)\n        result_ref = test_impl(df)\n        # TODO: implement index classes, as current indexes do not have names\n        pd.testing.assert_series_equal(result, result_ref, check_names=False)\n\n    def test_dataframe_groupby_getitem_unicode_str(self):\n        def test_impl(df, col_name):\n            return df.groupby(\'A\')[col_name].count()\n        hpat_func = self.jit(test_impl)\n\n        df = pd.DataFrame(_default_df_numeric_data)\n        col_name = \'C\'\n        # pandas returns groupby.generic.SeriesGroupBy object in this case, hence align result_ref\n        result = hpat_func(df, col_name)\n        result_ref = test_impl(df, col_name)\n        # TODO: implement index classes, as current indexes do not have names\n        pd.testing.assert_series_equal(result, result_ref, check_names=False)\n\n    def test_dataframe_groupby_getitem_repeated(self):\n        def test_impl(df):\n            return df.groupby(\'A\')[\'B\', \'C\'][\'D\']\n        hpat_func = self.jit(test_impl)\n\n        df = pd.DataFrame(_default_df_numeric_data)\n        with self.assertRaises(Exception) as context:\n            test_impl(df)\n        pandas_exception = context.exception\n\n        self.assertRaises(type(pandas_exception), hpat_func, df)\n\n    def test_series_groupby_by_array(self):\n        def test_impl(A, data):\n            return A.groupby(data).count()\n        hpat_func = self.jit(test_impl)\n\n        data_to_test = [\n                    [True, False, False, True, False, False, True, False, True, True, False],\n                    [2, 1, 1, 1, 2, 2, 1, 0, 3, 1, 3],\n                    [2, 1, 1, 1, 2, 2, 1, 3, np.nan, 1, np.nan],\n                    [\'b\', \'a\', \'a\', \'a\', \'b\', \'b\', \'a\', \' \', None, \'a\', None]\n        ]\n        for series_data, arr_data in product(data_to_test, data_to_test):\n            S = pd.Series(series_data)\n            by_arr = np.asarray(arr_data)\n\n            # arrays of dtype object cannot be jitted, so skip group by string data for now\n            if by_arr.dtype.name == \'object\':\n                continue\n            with self.subTest(series_data=series_data, by_arr=by_arr):\n                result = hpat_func(S, by_arr)\n                result_ref = test_impl(S, by_arr)\n                pd.testing.assert_series_equal(result, result_ref)\n\n    @unittest.skip(""getiter for this type is not implemented yet"")\n    def test_series_groupby_iterator_int(self):\n        def test_impl():\n            A = pd.Series([13, 11, 21, 13, 13, 51, 42, 21])\n            grouped = A.groupby(A)\n            return [i for i in grouped]\n\n        hpat_func = self.jit(test_impl)\n\n        ref_result = test_impl()\n        result = hpat_func()\n        np.testing.assert_array_equal(result, ref_result)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
sdc/tests/test_hiframes.py,62,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport itertools\nimport numba\nimport numpy as np\nimport os\nimport pandas as pd\nimport pyarrow.parquet as pq\nimport random\nimport string\nimport unittest\nfrom numba import types\n\nimport sdc\nfrom sdc import hiframes\nfrom sdc.str_arr_ext import StringArray\nfrom sdc.tests.test_base import TestCase\nfrom sdc.tests.test_utils import (count_array_OneDs,\n                                  count_array_REPs,\n                                  count_parfor_OneDs,\n                                  count_parfor_REPs,\n                                  dist_IR_contains,\n                                  get_start_end,\n                                  skip_numba_jit,\n                                  skip_sdc_jit)\n\n\nclass TestHiFrames(TestCase):\n\n    @skip_numba_jit\n    def test_column_list_select2(self):\n        # make sure SDC copies the columns like Pandas does\n        def test_impl(df):\n            df2 = df[[\'A\']]\n            df2[\'A\'] += 10\n            return df2.A, df.A\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        df = pd.DataFrame(\n            {\'A\': np.arange(n), \'B\': np.ones(n), \'C\': np.random.ranf(n)})\n        np.testing.assert_array_equal(hpat_func(df.copy())[1], test_impl(df)[1])\n\n    @skip_numba_jit\n    def test_pd_DataFrame_from_series_par(self):\n        def test_impl(n):\n            S1 = pd.Series(np.ones(n))\n            S2 = pd.Series(np.random.ranf(n))\n            df = pd.DataFrame({\'A\': S1, \'B\': S2})\n            return df.A.sum()\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        self.assertEqual(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n        self.assertEqual(count_parfor_OneDs(), 1)\n\n    @skip_numba_jit\n    def test_getitem_bool_series(self):\n        def test_impl(df):\n            return df[\'A\'][df[\'B\']].values\n\n        hpat_func = self.jit(test_impl)\n        df = pd.DataFrame({\'A\': [1, 2, 3], \'B\': [True, False, True]})\n        np.testing.assert_array_equal(test_impl(df), hpat_func(df))\n\n    @skip_numba_jit\n    def test_fillna(self):\n        def test_impl():\n            A = np.array([1., 2., 3.])\n            A[0] = np.nan\n            df = pd.DataFrame({\'A\': A})\n            B = df.A.fillna(5.0)\n            return B.sum()\n\n        hpat_func = self.jit(test_impl)\n        self.assertEqual(hpat_func(), test_impl())\n\n    @skip_numba_jit\n    def test_fillna_inplace(self):\n        def test_impl():\n            A = np.array([1., 2., 3.])\n            A[0] = np.nan\n            df = pd.DataFrame({\'A\': A})\n            df.A.fillna(5.0, inplace=True)\n            return df.A.sum()\n\n        hpat_func = self.jit(test_impl)\n        self.assertEqual(hpat_func(), test_impl())\n\n    @skip_numba_jit\n    def test_column_mean(self):\n        def test_impl():\n            A = np.array([1., 2., 3.])\n            A[0] = np.nan\n            df = pd.DataFrame({\'A\': A})\n            return df.A.mean()\n\n        hpat_func = self.jit(test_impl)\n        self.assertEqual(hpat_func(), test_impl())\n\n    @skip_numba_jit\n    def test_column_var(self):\n        def test_impl():\n            A = np.array([1., 2., 3.])\n            A[0] = 4.0\n            df = pd.DataFrame({\'A\': A})\n            return df.A.var()\n\n        hpat_func = self.jit(test_impl)\n        np.testing.assert_almost_equal(hpat_func(), test_impl())\n\n    @skip_numba_jit\n    def test_column_std(self):\n        def test_impl():\n            A = np.array([1., 2., 3.])\n            A[0] = 4.0\n            df = pd.DataFrame({\'A\': A})\n            return df.A.std()\n\n        hpat_func = self.jit(test_impl)\n        np.testing.assert_almost_equal(hpat_func(), test_impl())\n\n    @skip_numba_jit\n    def test_column_map(self):\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.arange(n)})\n            df[\'B\'] = df.A.map(lambda a: 2 * a)\n            return df.B.sum()\n\n        n = 121\n        hpat_func = self.jit(test_impl)\n        np.testing.assert_almost_equal(hpat_func(n), test_impl(n))\n\n    @skip_numba_jit\n    def test_column_map_arg(self):\n        def test_impl(df):\n            df[\'B\'] = df.A.map(lambda a: 2 * a)\n            return\n\n        n = 121\n        df1 = pd.DataFrame({\'A\': np.arange(n)})\n        df2 = pd.DataFrame({\'A\': np.arange(n)})\n        hpat_func = self.jit(test_impl)\n        hpat_func(df1)\n        self.assertTrue(hasattr(df1, \'B\'))\n        test_impl(df2)\n        np.testing.assert_equal(df1.B.values, df2.B.values)\n\n    @skip_numba_jit\n    @skip_sdc_jit(\'Not implemented in sequential transport layer\')\n    def test_cumsum(self):\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.ones(n), \'B\': np.random.ranf(n)})\n            Ac = df.A.cumsum()\n            return Ac.sum()\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        self.assertEqual(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_array_OneDs(), 2)\n        self.assertEqual(count_parfor_REPs(), 0)\n        self.assertEqual(count_parfor_OneDs(), 2)\n        self.assertTrue(dist_IR_contains(\'dist_cumsum\'))\n\n    @skip_numba_jit\n    @skip_sdc_jit(\'Not implemented in sequential transport layer\')\n    def test_column_distribution(self):\n        # make sure all column calls are distributed\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.ones(n), \'B\': np.random.ranf(n)})\n            df.A.fillna(5.0, inplace=True)\n            DF = df.A.fillna(5.0)\n            s = DF.sum()\n            m = df.A.mean()\n            v = df.A.var()\n            t = df.A.std()\n            Ac = df.A.cumsum()\n            return Ac.sum() + s + m + v + t\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        self.assertEqual(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n        self.assertTrue(dist_IR_contains(\'dist_cumsum\'))\n\n    @skip_numba_jit\n    @skip_sdc_jit(\'Not implemented in sequential transport layer\')\n    def test_quantile_parallel(self):\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.arange(0, n, 1, np.float64)})\n            return df.A.quantile(.25)\n\n        hpat_func = self.jit(test_impl)\n        n = 1001\n        np.testing.assert_almost_equal(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @unittest.skip(\'Error - fix needed\\n\'\n                   \'NUMA_PES=3 build\')\n    def test_quantile_parallel_float_nan(self):\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.arange(0, n, 1, np.float32)})\n            df.A[0:100] = np.nan\n            df.A[200:331] = np.nan\n            return df.A.quantile(.25)\n\n        hpat_func = self.jit(test_impl)\n        n = 1001\n        np.testing.assert_almost_equal(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @unittest.skip(\'Error - fix needed\\n\'\n                   \'NUMA_PES=3 build\')\n    def test_quantile_parallel_int(self):\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.arange(0, n, 1, np.int32)})\n            return df.A.quantile(.25)\n\n        hpat_func = self.jit(test_impl)\n        n = 1001\n        np.testing.assert_almost_equal(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @unittest.skip(\'Error - fix needed\\n\'\n                   \'NUMA_PES=3 build\')\n    def test_quantile_sequential(self):\n        def test_impl(A):\n            df = pd.DataFrame({\'A\': A})\n            return df.A.quantile(.25)\n\n        hpat_func = self.jit(test_impl)\n        n = 1001\n        A = np.arange(0, n, 1, np.float64)\n        np.testing.assert_almost_equal(hpat_func(A), test_impl(A))\n\n    @skip_numba_jit\n    def test_nunique(self):\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.arange(n)})\n            df.A[2] = 0\n            return df.A.nunique()\n\n        hpat_func = self.jit(test_impl)\n        n = 1001\n        np.testing.assert_almost_equal(hpat_func(n), test_impl(n))\n        # test compile again for overload related issues\n        hpat_func = self.jit(test_impl)\n        np.testing.assert_almost_equal(hpat_func(n), test_impl(n))\n\n    @skip_numba_jit\n    def test_nunique_parallel(self):\n        # TODO: test without file\n        def test_impl():\n            df = pq.read_table(\'example.parquet\').to_pandas()\n            return df.four.nunique()\n\n        hpat_func = self.jit(test_impl)\n        self.assertEqual(hpat_func(), test_impl())\n        self.assertEqual(count_array_REPs(), 0)\n        # test compile again for overload related issues\n        hpat_func = self.jit(test_impl)\n        self.assertEqual(hpat_func(), test_impl())\n        self.assertEqual(count_array_REPs(), 0)\n\n    @skip_numba_jit\n    def test_nunique_str(self):\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': [\'aa\', \'bb\', \'aa\', \'cc\', \'cc\']})\n            return df.A.nunique()\n\n        hpat_func = self.jit(test_impl)\n        n = 1001\n        np.testing.assert_almost_equal(hpat_func(n), test_impl(n))\n        # test compile again for overload related issues\n        hpat_func = self.jit(test_impl)\n        np.testing.assert_almost_equal(hpat_func(n), test_impl(n))\n\n    @unittest.skip(\'AssertionError - fix needed\\n\'\n                   \'5 != 3\\n\')\n    def test_nunique_str_parallel(self):\n        # TODO: test without file\n        def test_impl():\n            df = pq.read_table(\'example.parquet\').to_pandas()\n            return df.two.nunique()\n\n        hpat_func = self.jit(test_impl)\n        self.assertEqual(hpat_func(), test_impl())\n        self.assertEqual(count_array_REPs(), 0)\n        # test compile again for overload related issues\n        hpat_func = self.jit(test_impl)\n        self.assertEqual(hpat_func(), test_impl())\n        self.assertEqual(count_array_REPs(), 0)\n\n    @skip_numba_jit\n    def test_unique_parallel(self):\n        # TODO: test without file\n        def test_impl():\n            df = pq.read_table(\'example.parquet\').to_pandas()\n            return (df.four.unique() == 3.0).sum()\n\n        hpat_func = self.jit(test_impl)\n        self.assertEqual(hpat_func(), test_impl())\n        self.assertEqual(count_array_REPs(), 0)\n\n    @unittest.skip(\'AssertionError - fix needed\\n\'\n                   \'2 != 1\\n\')\n    def test_unique_str_parallel(self):\n        # TODO: test without file\n        def test_impl():\n            df = pq.read_table(\'example.parquet\').to_pandas()\n            return (df.two.unique() == \'foo\').sum()\n\n        hpat_func = self.jit(test_impl)\n        self.assertEqual(hpat_func(), test_impl())\n        self.assertEqual(count_array_REPs(), 0)\n\n    @skip_numba_jit\n    @skip_sdc_jit(\'Not implemented in sequential transport layer\')\n    def test_describe(self):\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.arange(0, n, 1, np.float64)})\n            return df.A.describe()\n\n        hpat_func = self.jit(test_impl)\n        n = 1001\n        hpat_func(n)\n        # XXX: test actual output\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit\n    def test_str_contains_regex(self):\n        def test_impl():\n            A = StringArray([\'ABC\', \'BB\', \'ADEF\'])\n            df = pd.DataFrame({\'A\': A})\n            B = df.A.str.contains(\'AB*\', regex=True)\n            return B.sum()\n\n        hpat_func = self.jit(test_impl)\n        self.assertEqual(hpat_func(), 2)\n\n    @skip_numba_jit\n    def test_str_contains_noregex(self):\n        def test_impl():\n            A = StringArray([\'ABC\', \'BB\', \'ADEF\'])\n            df = pd.DataFrame({\'A\': A})\n            B = df.A.str.contains(\'BB\', regex=False)\n            return B.sum()\n\n        hpat_func = self.jit(test_impl)\n        self.assertEqual(hpat_func(), 1)\n\n    @skip_numba_jit\n    def test_str_replace_regex(self):\n        def test_impl(df):\n            return df.A.str.replace(\'AB*\', \'EE\', regex=True)\n\n        df = pd.DataFrame({\'A\': [\'ABCC\', \'CABBD\']})\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_series_equal(\n            hpat_func(df), test_impl(df), check_names=False)\n\n    @skip_numba_jit\n    def test_str_replace_noregex(self):\n        def test_impl(df):\n            return df.A.str.replace(\'AB\', \'EE\', regex=False)\n\n        df = pd.DataFrame({\'A\': [\'ABCC\', \'CABBD\']})\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_series_equal(\n            hpat_func(df), test_impl(df), check_names=False)\n\n    @skip_numba_jit\n    def test_str_replace_regex_parallel(self):\n        def test_impl(df):\n            B = df.A.str.replace(\'AB*\', \'EE\', regex=True)\n            return B\n\n        n = 5\n        A = [\'ABCC\', \'CABBD\', \'CCD\', \'CCDAABB\', \'ED\']\n        start, end = get_start_end(n)\n        df = pd.DataFrame({\'A\': A[start:end]})\n        hpat_func = self.jit(distributed={\'df\', \'B\'})(test_impl)\n        pd.testing.assert_series_equal(\n            hpat_func(df), test_impl(df), check_names=False)\n        self.assertEqual(count_array_REPs(), 3)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit\n    def test_str_split(self):\n        def test_impl(df):\n            return df.A.str.split(\',\')\n\n        df = pd.DataFrame({\'A\': [\'AB,CC\', \'C,ABB,D\', \'G\', \'\', \'g,f\']})\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_series_equal(\n            hpat_func(df), test_impl(df), check_names=False)\n\n    @skip_numba_jit\n    def test_str_split_default(self):\n        def test_impl(df):\n            return df.A.str.split()\n\n        df = pd.DataFrame({\'A\': [\'AB CC\', \'C ABB D\', \'G \', \' \', \'g\\t f\']})\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_series_equal(\n            hpat_func(df), test_impl(df), check_names=False)\n\n    @skip_numba_jit\n    def test_str_split_filter(self):\n        def test_impl(df):\n            B = df.A.str.split(\',\')\n            df2 = pd.DataFrame({\'B\': B})\n            return df2[df2.B.str.len() > 1]\n\n        df = pd.DataFrame({\'A\': [\'AB,CC\', \'C,ABB,D\', \'G\', \'\', \'g,f\']})\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_frame_equal(\n            hpat_func(df), test_impl(df).reset_index(drop=True))\n\n    @skip_numba_jit\n    def test_str_split_box_df(self):\n        def test_impl(df):\n            return pd.DataFrame({\'B\': df.A.str.split(\',\')})\n\n        df = pd.DataFrame({\'A\': [\'AB,CC\', \'C,ABB,D\']})\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_series_equal(\n            hpat_func(df).B, test_impl(df).B, check_names=False)\n\n    @skip_numba_jit\n    def test_str_split_unbox_df(self):\n        def test_impl(df):\n            return df.A.iloc[0]\n\n        df = pd.DataFrame({\'A\': [\'AB,CC\', \'C,ABB,D\']})\n        df2 = pd.DataFrame({\'A\': df.A.str.split(\',\')})\n        hpat_func = self.jit(test_impl)\n        self.assertEqual(hpat_func(df2), test_impl(df2))\n\n    @unittest.skip(\'Getitem Series with list values not implement\')\n    def test_str_split_bool_index(self):\n        def test_impl(df):\n            C = df.A.str.split(\',\')\n            return C[df.B == \'aa\']\n\n        df = pd.DataFrame({\'A\': [\'AB,CC\', \'C,ABB,D\'], \'B\': [\'aa\', \'bb\']})\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_series_equal(\n            hpat_func(df), test_impl(df), check_names=False)\n\n    @skip_numba_jit\n    def test_str_split_parallel(self):\n        def test_impl(df):\n            B = df.A.str.split(\',\')\n            return B\n\n        n = 5\n        start, end = get_start_end(n)\n        A = [\'AB,CC\', \'C,ABB,D\', \'CAD\', \'CA,D\', \'AA,,D\']\n        df = pd.DataFrame({\'A\': A[start:end]})\n        hpat_func = self.jit(distributed={\'df\', \'B\'})(test_impl)\n        pd.testing.assert_series_equal(\n            hpat_func(df), test_impl(df), check_names=False)\n        self.assertEqual(count_array_REPs(), 3)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit\n    def test_str_get(self):\n        def test_impl(df):\n            B = df.A.str.split(\',\')\n            return B.str.get(1)\n\n        df = pd.DataFrame({\'A\': [\'AB,CC\', \'C,ABB,D\']})\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_series_equal(\n            hpat_func(df), test_impl(df), check_names=False)\n\n    @skip_numba_jit\n    def test_str_split(self):\n        def test_impl(df):\n            return df.A.str.split(\',\')\n\n        df = pd.DataFrame({\'A\': [\'AB,CC\', \'C,ABB,D\']})\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_series_equal(hpat_func(df), test_impl(df), check_names=False)\n\n    @skip_numba_jit\n    def test_str_get_parallel(self):\n        def test_impl(df):\n            A = df.A.str.split(\',\')\n            B = A.str.get(1)\n            return B\n\n        n = 5\n        start, end = get_start_end(n)\n        A = [\'AB,CC\', \'C,ABB,D\', \'CAD,F\', \'CA,D\', \'AA,,D\']\n        df = pd.DataFrame({\'A\': A[start:end]})\n        hpat_func = self.jit(distributed={\'df\', \'B\'})(test_impl)\n        pd.testing.assert_series_equal(\n            hpat_func(df), test_impl(df), check_names=False)\n        self.assertEqual(count_array_REPs(), 3)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit\n    def test_str_get_to_numeric(self):\n        def test_impl(df):\n            B = df.A.str.split(\',\')\n            C = pd.to_numeric(B.str.get(1), errors=\'coerce\')\n            return C\n\n        df = pd.DataFrame({\'A\': [\'AB,12\', \'C,321,D\']})\n        hpat_func = self.jit(locals={\'C\': types.int64[:]})(test_impl)\n        pd.testing.assert_series_equal(\n            hpat_func(df), test_impl(df), check_names=False)\n\n    @skip_numba_jit\n    def test_str_flatten(self):\n        def test_impl(df):\n            A = df.A.str.split(\',\')\n            return pd.Series(list(itertools.chain(*A)))\n\n        df = pd.DataFrame({\'A\': [\'AB,CC\', \'C,ABB,D\']})\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_series_equal(\n            hpat_func(df), test_impl(df), check_names=False)\n\n    @skip_numba_jit\n    def test_str_flatten_parallel(self):\n        def test_impl(df):\n            A = df.A.str.split(\',\')\n            B = pd.Series(list(itertools.chain(*A)))\n            return B\n\n        n = 5\n        start, end = get_start_end(n)\n        A = [\'AB,CC\', \'C,ABB,D\', \'CAD\', \'CA,D\', \'AA,,D\']\n        df = pd.DataFrame({\'A\': A[start:end]})\n        hpat_func = self.jit(distributed={\'df\', \'B\'})(test_impl)\n        pd.testing.assert_series_equal(\n            hpat_func(df), test_impl(df), check_names=False)\n        self.assertEqual(count_array_REPs(), 3)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit\n    def test_to_numeric(self):\n        def test_impl(df):\n            B = pd.to_numeric(df.A, errors=\'coerce\')\n            return B\n\n        df = pd.DataFrame({\'A\': [\'123.1\', \'331.2\']})\n        hpat_func = self.jit(locals={\'B\': types.float64[:]})(test_impl)\n        pd.testing.assert_series_equal(\n            hpat_func(df), test_impl(df), check_names=False)\n\n    @skip_numba_jit\n    def test_1D_Var_len(self):\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.arange(n), \'B\': np.arange(n) + 1.0})\n            df1 = df[df.A > 5]\n            return len(df1.B)\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        self.assertEqual(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit\n    def test_rolling1(self):\n        # size 3 without unroll\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.arange(n), \'B\': np.random.ranf(n)})\n            Ac = df.A.rolling(3).sum()\n            return Ac.sum()\n\n        hpat_func = self.jit(test_impl)\n        n = 121\n        self.assertEqual(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n        # size 7 with unroll\n\n        def test_impl_2(n):\n            df = pd.DataFrame({\'A\': np.arange(n) + 1.0, \'B\': np.random.ranf(n)})\n            Ac = df.A.rolling(7).sum()\n            return Ac.sum()\n\n        hpat_func = self.jit(test_impl)\n        n = 121\n        self.assertEqual(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit\n    def test_rolling2(self):\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.ones(n), \'B\': np.random.ranf(n)})\n            df[\'moving average\'] = df.A.rolling(window=5, center=True).mean()\n            return df[\'moving average\'].sum()\n\n        hpat_func = self.jit(test_impl)\n        n = 121\n        self.assertEqual(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit\n    def test_rolling3(self):\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.ones(n), \'B\': np.random.ranf(n)})\n            Ac = df.A.rolling(3, center=True).apply(lambda a: a[0] + 2 * a[1] + a[2])\n            return Ac.sum()\n\n        hpat_func = self.jit(test_impl)\n        n = 121\n        self.assertEqual(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @unittest.skip(\'Error - fix needed\\n\'\n                   \'NUMA_PES=3 build\')\n    def test_shift1(self):\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.arange(n) + 1.0, \'B\': np.random.ranf(n)})\n            Ac = df.A.shift(1)\n            return Ac.sum()\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        self.assertEqual(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @unittest.skip(\'Error - fix needed\\n\'\n                   \'NUMA_PES=3 build\')\n    def test_shift2(self):\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.arange(n) + 1.0, \'B\': np.random.ranf(n)})\n            Ac = df.A.pct_change(1)\n            return Ac.sum()\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        np.testing.assert_almost_equal(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit\n    def test_df_input(self):\n        def test_impl(df):\n            return df.B.sum()\n\n        n = 121\n        df = pd.DataFrame({\'A\': np.ones(n), \'B\': np.random.ranf(n)})\n        hpat_func = self.jit(test_impl)\n        np.testing.assert_almost_equal(hpat_func(df), test_impl(df))\n\n    @skip_numba_jit\n    def test_df_input2(self):\n        def test_impl(df):\n            C = df.B == \'two\'\n            return C.sum()\n\n        n = 11\n        df = pd.DataFrame({\'A\': np.random.ranf(3 * n), \'B\': [\'one\', \'two\', \'three\'] * n})\n        hpat_func = self.jit(test_impl)\n        np.testing.assert_almost_equal(hpat_func(df), test_impl(df))\n\n    @skip_numba_jit\n    def test_df_input_dist1(self):\n        def test_impl(df):\n            return df.B.sum()\n\n        n = 121\n        A = [3, 4, 5, 6, 1]\n        B = [5, 6, 2, 1, 3]\n        n = 5\n        start, end = get_start_end(n)\n        df = pd.DataFrame({\'A\': A, \'B\': B})\n        df_h = pd.DataFrame({\'A\': A[start:end], \'B\': B[start:end]})\n        hpat_func = self.jit(distributed={\'df\'})(test_impl)\n        np.testing.assert_almost_equal(hpat_func(df_h), test_impl(df))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit\n    def test_concat(self):\n        def test_impl(n):\n            df1 = pd.DataFrame({\'key1\': np.arange(n), \'A\': np.arange(n) + 1.0})\n            df2 = pd.DataFrame({\'key2\': n - np.arange(n), \'A\': n + np.arange(n) + 1.0})\n            df3 = pd.concat([df1, df2])\n            return df3.A.sum() + df3.key2.sum()\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        self.assertEqual(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n        n = 11111\n        self.assertEqual(hpat_func(n), test_impl(n))\n\n    @skip_numba_jit\n    def test_concat_str(self):\n        def test_impl():\n            df1 = pq.read_table(\'example.parquet\').to_pandas()\n            df2 = pq.read_table(\'example.parquet\').to_pandas()\n            A3 = pd.concat([df1, df2])\n            return (A3.two == \'foo\').sum()\n\n        hpat_func = self.jit(test_impl)\n        self.assertEqual(hpat_func(), test_impl())\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit\n    def test_concat_series(self):\n        def test_impl(n):\n            df1 = pd.DataFrame({\'key1\': np.arange(n), \'A\': np.arange(n) + 1.0})\n            df2 = pd.DataFrame({\'key2\': n - np.arange(n), \'A\': n + np.arange(n) + 1.0})\n            A3 = pd.concat([df1.A, df2.A])\n            return A3.sum()\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        self.assertEqual(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n        n = 11111\n        self.assertEqual(hpat_func(n), test_impl(n))\n\n    @skip_numba_jit\n    def test_concat_series_str(self):\n        def test_impl():\n            df1 = pq.read_table(\'example.parquet\').to_pandas()\n            df2 = pq.read_table(\'example.parquet\').to_pandas()\n            A3 = pd.concat([df1.two, df2.two])\n            return (A3 == \'foo\').sum()\n\n        hpat_func = self.jit(test_impl)\n        self.assertEqual(hpat_func(), test_impl())\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit\n    @unittest.skipIf(int(os.getenv(\'SDC_NP_MPI\', \'0\')) > 1, \'Test hangs on NP=2 and NP=3 on all platforms\')\n    def test_intraday(self):\n        def test_impl(nsyms):\n            max_num_days = 100\n            all_res = 0.0\n            for i in sdc.prange(nsyms):\n                s_open = 20 * np.ones(max_num_days)\n                s_low = 28 * np.ones(max_num_days)\n                s_close = 19 * np.ones(max_num_days)\n                df = pd.DataFrame({\'Open\': s_open, \'Low\': s_low, \'Close\': s_close})\n                df[\'Stdev\'] = df[\'Close\'].rolling(window=90).std()\n                df[\'Moving Average\'] = df[\'Close\'].rolling(window=20).mean()\n                df[\'Criteria1\'] = (df[\'Open\'] - df[\'Low\'].shift(1)) < -df[\'Stdev\']\n                df[\'Criteria2\'] = df[\'Open\'] > df[\'Moving Average\']\n                df[\'BUY\'] = df[\'Criteria1\'] & df[\'Criteria2\']\n                df[\'Pct Change\'] = (df[\'Close\'] - df[\'Open\']) / df[\'Open\']\n                df[\'Rets\'] = df[\'Pct Change\'][df[\'BUY\']]\n                all_res += df[\'Rets\'].mean()\n            return all_res\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        self.assertEqual(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_OneDs(), 0)\n        self.assertEqual(count_parfor_OneDs(), 1)\n\n    @skip_numba_jit\n    def test_var_dist1(self):\n        def test_impl(A, B):\n            df = pd.DataFrame({\'A\': A, \'B\': B})\n            df2 = df.groupby(\'A\', as_index=False)[\'B\'].sum()\n            # TODO: fix handling of df setitem to force match of array dists\n            # probably with a new node that is appended to the end of basic block\n            # df2[\'C\'] = np.full(len(df2.B), 3, np.int8)\n            # TODO: full_like for Series\n            df2[\'C\'] = np.full_like(df2.B.values, 3, np.int8)\n            return df2\n\n        A = np.array([1, 1, 2, 3])\n        B = np.array([3, 4, 5, 6])\n        hpat_func = self.jit(locals={\'A:input\': \'distributed\',\n                                     \'B:input\': \'distributed\', \'df2:return\': \'distributed\'})(test_impl)\n        start, end = get_start_end(len(A))\n        df2 = hpat_func(A[start:end], B[start:end])\n        # TODO:\n        # pd.testing.assert_frame_equal(\n        #     hpat_func(A[start:end], B[start:end]), test_impl(A, B))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
sdc/tests/test_hpat_jit.py,23,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numba\nimport numpy as np\nimport pandas as pd\nimport platform\nimport unittest\nfrom collections import defaultdict\nfrom numba.typed import Dict\n\nimport sdc\nfrom sdc import *\nfrom sdc.tests.test_base import TestCase\nfrom sdc.tests.test_utils import skip_numba_jit\n\n\nclass TestHpatJitIssues(TestCase):\n\n    @unittest.skip(""Dict is not supported as class member"")\n    def test_class_with_dict(self):\n        @jitclass([(\'d\', Dict)])\n        class ClassWithDict:\n            def __init__(self):\n                self.d = Dict.empty(key_type=int32, value_type=int32)\n\n        @numba.njit\n        def test_impl():\n            c = ClassWithDict()\n\n            c.d[0] = 1\n\n            return c.d[0]\n\n        test_impl()\n\n    @unittest.skip(""Type infer from __init__ is not supported"")\n    def test_class_from_init(self):\n        @jitclass()\n        class ClassWithInt:\n            def __init__(self):\n                self.i = 0\n\n        @numba.njit\n        def test_impl():\n            c = ClassWithInt()\n\n            print(c.i)\n\n        test_impl()\n\n    @unittest.skip(""list.sort with lambda is not supported"")\n    def test_list_sort_lambda(self):\n        @numba.njit\n        def sort_with_list_and_lambda():\n            data = [5, 4, 3, 2, 1, 0]\n\n            data.sort(key=lambda x: x)\n\n            return data\n\n        sort_with_list_and_lambda()\n\n    @unittest.skip(""list.sort with key is not supported"")\n    def test_list_sort_with_func(self):\n        @numba.njit\n        def key_func(x):\n            return x\n\n        @numba.njit\n        def sort_with_list():\n            data = [5, 4, 3, 2, 1, 0]\n\n            data.sort(key=key_func)\n\n            return data\n\n        sort_with_list()\n\n    @unittest.skip(""sorted with lambda is not supported"")\n    def test_sorted_lambda(self):\n        @numba.njit\n        def sorted_with_list():\n            data = [5, 4, 3, 2, 1, 0]\n\n            sorted(data, key=lambda x: x)\n\n            return data\n\n        sorted_with_list()\n\n    @unittest.skip(""sorted with key is not supported"")\n    def test_sorted_with_func(self):\n        @numba.njit\n        def key_func(x):\n            return x\n\n        @numba.njit\n        def sorted_with_list():\n            data = [5, 4, 3, 2, 1, 0]\n\n            sorted(data, key=key_func)\n\n            return data\n\n        sorted_with_list()\n\n    @unittest.skip(""iterate over tuple is not supported"")\n    def test_iterate_over_tuple(self):\n        @numba.njit\n        def func_iterate_over_tuple():\n            t = (\'1\', 1, 1.)\n\n            for i in t:\n                print(i)\n\n        func_iterate_over_tuple()\n\n    @unittest.skip(""try/except is not supported"")\n    def test_with_try_except(self):\n        @numba.njit\n        def func_with_try_except():\n            try:\n                return 0\n            except BaseException:\n                return 1\n\n        func_with_try_except()\n\n    @unittest.skip(""raise is not supported"")\n    def test_with_raise(self):\n        @numba.njit\n        def func_with_raise(b):\n            if b:\n                return b\n            else:\n                raise ""error""\n\n        func_with_raise(True)\n\n    @unittest.skip(""defaultdict is not supported"")\n    def test_default_dict(self):\n        @numba.njit\n        def func_with_dict():\n            d = defaultdict(int)\n\n            return d[\'a\']\n\n        func_with_dict()\n\n    @unittest.skip(\'TODO: needs different integer typing in Numba\\n\'\n                   \'AssertionError - Attribute ""dtype"" are different\\n\'\n                   \'[left]:  int64\\n\'\n                   \'[right]: int32\\n\')\n    def test_series_binop_int_casting(self):\n        def test_impl(A):\n            res = A + 42\n            return res.dtype\n        hpat_func = self.jit(test_impl)\n\n        A = np.ones(1, dtype=\'int32\')\n        self.assertEqual(hpat_func(A), test_impl(A))\n\n    @unittest.skip(\'AssertionError - fix needed\\n\'\n                   \'Attribute ""dtype"" are different\\n\'\n                   \'[left]:  int64\\n\'\n                   \'[right]: int32\\n\')\n    def test_box1_issue(self):\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.ones(n), \'B\': np.arange(n)})\n            return df\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        pd.testing.assert_frame_equal(hpat_func(n), test_impl(n))\n\n    @unittest.skip(\'AssertionError - fix needed\\n\'\n                   \'Attribute ""dtype"" are different\\n\'\n                   \'[left]:  int64\\n\'\n                   \'[right]: int32\\n\')\n    def test_set_column1_issue(self):\n        # set existing column\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.ones(n, np.int64), \'B\': np.arange(n) + 3.0})\n            df[\'A\'] = np.arange(n)\n            return df\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        pd.testing.assert_frame_equal(hpat_func(n), test_impl(n))\n\n    @unittest.skip(\'AssertionError - fix needed\\n\'\n                   \'Attribute ""dtype"" are different\\n\'\n                   \'[left]:  int64\\n\'\n                   \'[right]: int32\\n\')\n    def test_set_column_reflect4(self):\n        # set existing column\n        def test_impl(df, n):\n            df[\'A\'] = np.arange(n)\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        df1 = pd.DataFrame({\'A\': np.ones(n, np.int64), \'B\': np.arange(n) + 3.0})\n        df2 = df1.copy()\n        hpat_func(df1, n)\n        test_impl(df2, n)\n        pd.testing.assert_frame_equal(df1, df2)\n\n    @unittest.skip(\'AssertionError - fix needed\\n\'\n                   \'Attribute ""dtype"" are different\\n\'\n                   \'[left]:  int64\\n\'\n                   \'[right]: int32\\n\')\n    def test_set_column_new_type1(self):\n        # set existing column with a new type\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.ones(n), \'B\': np.arange(n) + 3.0})\n            df[\'A\'] = np.arange(n)\n            return df\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        pd.testing.assert_frame_equal(hpat_func(n), test_impl(n))\n\n    @unittest.skip(\'AssertionError - fix needed\\n\'\n                   \'Attribute ""dtype"" are different\\n\'\n                   \'[left]:  int64\\n\'\n                   \'[right]: int32\\n\')\n    def test_set_column2(self):\n        # create new column\n        def test_impl(n):\n            df = pd.DataFrame({\'A\': np.ones(n), \'B\': np.arange(n) + 1.0})\n            df[\'C\'] = np.arange(n)\n            return df\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        pd.testing.assert_frame_equal(hpat_func(n), test_impl(n))\n\n    @unittest.skip(\'AssertionError - fix needed\\n\'\n                   \'Attribute ""dtype"" are different\\n\'\n                   \'[left]:  int64\\n\'\n                   \'[right]: int32\\n\')\n    def test_set_column_reflect3(self):\n        # create new column\n        def test_impl(df, n):\n            df[\'C\'] = np.arange(n)\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        df1 = pd.DataFrame({\'A\': np.ones(n, np.int64), \'B\': np.arange(n) + 3.0})\n        df2 = df1.copy()\n        hpat_func(df1, n)\n        test_impl(df2, n)\n        pd.testing.assert_frame_equal(df1, df2)\n\n    @unittest.skip(\'AssertionError - fix needed\\n\'\n                   \'Attribute ""dtype"" are different\\n\'\n                   \'[left]:  int64\\n\'\n                   \'[right]: int32\\n\')\n    def test_series_op2_issue(self):\n        arithmetic_binops = (\'+\', \'-\', \'*\', \'/\', \'//\', \'%\', \'**\')\n\n        for operator in arithmetic_binops:\n            test_impl = _make_func_use_binop1(operator)\n            hpat_func = self.jit(test_impl)\n\n            n = 11\n            df = pd.DataFrame({\'A\': np.arange(1, n)})\n            pd.testing.assert_series_equal(hpat_func(df.A, 1), test_impl(df.A, 1), check_names=False)\n\n    @unittest.skip(\'AssertionError - fix needed\\n\'\n                   \'Attribute ""dtype"" are different\\n\'\n                   \'[left]:  int64\\n\'\n                   \'[right]: int32\\n\')\n    def test_series_op5_integer_scalar_issue(self):\n        arithmetic_methods = (\'add\', \'sub\', \'mul\', \'div\', \'truediv\', \'floordiv\', \'mod\', \'pow\')\n\n        for method in arithmetic_methods:\n            test_impl = _make_func_use_method_arg1(method)\n            hpat_func = self.jit(test_impl)\n\n            n = 11\n            operand_series = pd.Series(np.arange(1, n))\n            operand_scalar = 10\n            pd.testing.assert_series_equal(\n                hpat_func(operand_series, operand_scalar),\n                test_impl(operand_series, operand_scalar),\n                check_names=False)\n\n    @unittest.skip(\'AssertionError - fix needed\\n\'\n                   \'Attribute ""dtype"" are different\\n\'\n                   \'[left]:  int64\\n\'\n                   \'[right]: int32\\n\')\n    def test_series_fusion1_issue(self):\n        def test_impl(A, B):\n            return A + B + 1\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        A = pd.Series(np.arange(n))\n        B = pd.Series(np.arange(n)**2)\n        pd.testing.assert_series_equal(hpat_func(A, B), test_impl(A, B))\n        self.assertEqual(count_parfor_REPs(), 1)\n\n    @unittest.skip(\'AssertionError - fix needed\\n\'\n                   \'Attribute ""dtype"" are different\\n\'\n                   \'[left]:  int64\\n\'\n                   \'[right]: int32\\n\')\n    def test_series_fusion2_issue(self):\n        # make sure getting data var avoids incorrect single def assumption\n        def test_impl(A, B):\n            S = B + 2\n            if A[0] == 0:\n                S = A + 1\n            return S + B\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        A = pd.Series(np.arange(n))\n        B = pd.Series(np.arange(n)**2)\n        pd.testing.assert_series_equal(hpat_func(A, B), test_impl(A, B))\n        self.assertEqual(count_parfor_REPs(), 3)\n\n    @skip_numba_jit\n    @unittest.skipIf(platform.system() == \'Windows\',\n                     \'AssertionError: Attributes are different\'\n                     \'Attribute ""dtype"" are different\'\n                     \'[left]:  int64\'\n                     \'[right]: int32\')\n    def test_csv1_issue(self):\n        def test_impl():\n            return pd.read_csv(""csv_data1.csv"",\n                               names=[\'A\', \'B\', \'C\', \'D\'],\n                               dtype={\'A\': np.int, \'B\': np.float, \'C\': np.float, \'D\': str},\n                               )\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_frame_equal(hpat_func(), test_impl())\n\n    @skip_numba_jit\n    @unittest.skipIf(platform.system() == \'Windows\',\n                     \'AssertionError: Attributes are different\'\n                     \'Attribute ""dtype"" are different\'\n                     \'[left]:  int64\'\n                     \'[right]: int32\')\n    def test_csv_keys1_issue(self):\n        def test_impl():\n            dtype = {\'A\': np.int, \'B\': np.float, \'C\': np.float, \'D\': str}\n            return pd.read_csv(""csv_data1.csv"",\n                               names=dtype.keys(),\n                               dtype=dtype,\n                               )\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_frame_equal(hpat_func(), test_impl())\n\n    @skip_numba_jit\n    @unittest.skipIf(platform.system() == \'Windows\',\n                     \'AssertionError: Attributes are different\'\n                     \'Attribute ""dtype"" are different\'\n                     \'[left]:  int64\'\n                     \'[right]: int32\')\n    def test_csv_const_dtype1_issue(self):\n        def test_impl():\n            dtype = {\'A\': \'int\', \'B\': \'float64\', \'C\': \'float\', \'D\': \'str\'}\n            return pd.read_csv(""csv_data1.csv"",\n                               names=dtype.keys(),\n                               dtype=dtype,\n                               )\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_frame_equal(hpat_func(), test_impl())\n\n    @skip_numba_jit\n    @unittest.skipIf(platform.system() == \'Windows\',\n                     \'AssertionError: Attributes are different\'\n                     \'Attribute ""dtype"" are different\'\n                     \'[left]:  int64\'\n                     \'[right]: int32\')\n    def test_csv_skip1_issue(self):\n        def test_impl():\n            return pd.read_csv(""csv_data1.csv"",\n                               names=[\'A\', \'B\', \'C\', \'D\'],\n                               dtype={\'A\': np.int, \'B\': np.float, \'C\': np.float, \'D\': str},\n                               skiprows=2,\n                               )\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_frame_equal(hpat_func(), test_impl())\n\n    @skip_numba_jit\n    @unittest.skipIf(platform.system() == \'Windows\',\n                     \'AssertionError: Attributes are different\'\n                     \'Attribute ""dtype"" are different\'\n                     \'[left]:  int64\'\n                     \'[right]: int32\')\n    def test_csv_date1_issue(self):\n        def test_impl():\n            return pd.read_csv(""csv_data_date1.csv"",\n                               names=[\'A\', \'B\', \'C\', \'D\'],\n                               dtype={\'A\': np.int, \'B\': np.float, \'C\': str, \'D\': np.int},\n                               parse_dates=[2])\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_frame_equal(hpat_func(), test_impl())\n\n    @skip_numba_jit\n    @unittest.skipIf(platform.system() == \'Windows\',\n                     \'AssertionError: Attributes are different\'\n                     \'Attribute ""dtype"" are different\'\n                     \'[left]:  int64\'\n                     \'[right]: int32\')\n    def test_csv_str1_issue(self):\n        def test_impl():\n            return pd.read_csv(""csv_data_date1.csv"",\n                               names=[\'A\', \'B\', \'C\', \'D\'],\n                               dtype={\'A\': np.int, \'B\': np.float, \'C\': str, \'D\': np.int})\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_frame_equal(hpat_func(), test_impl())\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
sdc/tests/test_indexes.py,4,"b'# -*- coding: utf-8 -*-\n# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numpy as np\nimport pandas as pd\nimport unittest\n\nfrom itertools import (combinations_with_replacement, product, filterfalse)\n\nfrom sdc.tests.test_base import TestCase\nfrom sdc.utilities.sdc_typing_utils import kwsparams2list\nfrom sdc.tests.test_series import _make_func_from_text\nfrom numba.errors import TypingError\n\n\ntest_global_index_names = [None, \'abc\', \'index\']\ntest_global_range_member_values = [1, 2, 10, -5, 0, None]\n\n\ndef _generate_valid_range_params():\n\n    def valid_params_predicate(range_params):\n        # if step is zero or all start/stop/step are None range is invalid\n        return (range_params[-1] == 0\n                or all(map(lambda x: x is None, range_params)))\n\n    return filterfalse(\n        valid_params_predicate,\n        combinations_with_replacement(test_global_range_member_values, 3)\n    )\n\n\nclass TestRangeIndex(TestCase):\n\n    def test_range_index_create_and_box(self):\n        def test_impl(start, stop, step, name):\n            return pd.RangeIndex(start, stop, step, name=name)\n        sdc_func = self.jit(test_impl)\n\n        for params in _generate_valid_range_params():\n            start, stop, step = params\n            for name in test_global_index_names:\n                with self.subTest(start=start, stop=stop, step=step, name=name):\n                    result = sdc_func(start, stop, step, name)\n                    result_ref = test_impl(start, stop, step, name)\n                    pd.testing.assert_index_equal(result, result_ref)\n\n    def test_range_index_unbox_and_box(self):\n        def test_impl(index):\n            return index\n        sdc_func = self.jit(test_impl)\n\n        for params in _generate_valid_range_params():\n            start, stop, step = params\n            for name in test_global_index_names:\n                index = pd.RangeIndex(start, stop, step, name=name)\n                with self.subTest(index=index):\n                    result = sdc_func(index)\n                    result_ref = test_impl(index)\n                    pd.testing.assert_index_equal(result, result_ref)\n\n    @unittest.skip(""TODO: support boxing/unboxing and parent ref for Python ranges in Numba"")\n    def test_range_index_unbox_data_id_check(self):\n        def test_impl(index):\n            return index\n        sdc_func = self.jit(test_impl)\n\n        index = pd.RangeIndex(11, name=\'abc\')\n        result = sdc_func(index)\n        result_ref = test_impl(index)\n        self.assertIs(index._range, result_ref._range)\n        self.assertIs(result._range, result_ref._range)\n\n    @unittest.skip(""TODO: add support for integers as floats in ctor"")\n    def test_range_index_create_from_floats(self):\n        def test_impl(*args):\n            return pd.RangeIndex(*args)\n        sdc_func = self.jit(test_impl)\n\n        start, stop, step = 3.0, 15.0, 2.0\n        result = sdc_func(start, stop, step)\n        result_ref = test_impl(start, stop, step)\n        pd.testing.assert_index_equal(result, result_ref)\n\n    def test_range_index_create_invalid1(self):\n        def test_impl(start, stop, step):\n            return pd.RangeIndex(start, stop, step)\n        sdc_func = self.jit(test_impl)\n\n        # zero step is not allowed by pandas\n        start, stop, step = 3, 5, 0\n        with self.assertRaises(Exception) as context:\n            test_impl(start, stop, step)\n        pandas_exception = context.exception\n\n        with self.assertRaises(type(pandas_exception)) as context:\n            sdc_func(start, stop, step)\n        sdc_exception = context.exception\n        self.assertIn(str(sdc_exception), str(pandas_exception))\n\n    def test_range_index_create_invalid2(self):\n        def test_impl():\n            return pd.RangeIndex(name=\'index\')\n        sdc_func = self.jit(test_impl)\n\n        # all start, stop and step cannot be None at the same time\n        with self.assertRaises(Exception) as context:\n            test_impl()\n        pandas_exception = context.exception\n\n        with self.assertRaises(type(pandas_exception)) as context:\n            sdc_func()\n        sdc_exception = context.exception\n        self.assertIn(str(sdc_exception), str(pandas_exception))\n\n    def test_range_index_create_defaults(self):\n        func_lines = [\n            \'def test_impl():\',\n            \'  return pd.RangeIndex({})\'\n        ]\n        test_impl_text = \'\\n\'.join(func_lines)\n\n        # use non default values for all parameters except one (tested)\n        non_default_params = {\'start\': 2, \'stop\': 7, \'step\': 2, \'name\': ""\'index\'""}\n        for arg in non_default_params.keys():\n            with self.subTest(omitted=arg):\n                kwargs = {key: val for key, val in non_default_params.items() if key != arg}\n                func_text = test_impl_text.format(\', \'.join(kwsparams2list(kwargs)))\n                test_impl = _make_func_from_text(func_text, global_vars={\'pd\': pd})\n                sdc_func = self.jit(test_impl)\n                result = sdc_func()\n                result_ref = test_impl()\n                pd.testing.assert_index_equal(result, result_ref)\n\n    def test_range_index_create_param_copy(self):\n        def test_impl(stop, value):\n            return pd.RangeIndex(stop, copy=value)\n        sdc_func = self.jit(test_impl)\n\n        with self.assertRaises(TypingError) as raises:\n            sdc_func(11, False)\n        self.assertIn(""SDCLimitation: pd.RangeIndex(). Unsupported parameter"",\n                      str(raises.exception))\n\n    def test_range_index_create_param_name_literal_str(self):\n        def test_impl(stop):\n            return pd.RangeIndex(stop, name=\'index\')\n        sdc_func = self.jit(test_impl)\n\n        n = 11\n        result = sdc_func(n)\n        result_ref = test_impl(n)\n        pd.testing.assert_index_equal(result, result_ref)\n\n    def test_range_index_create_param_dtype(self):\n        def test_impl(stop, dtype):\n            return pd.RangeIndex(stop, dtype=dtype)\n        sdc_func = self.jit(test_impl)\n\n        n = 11\n        supported_dtypes = [None, np.int64, \'int64\']\n        for dtype in supported_dtypes:\n            with self.subTest(dtype=dtype):\n                result = sdc_func(n, dtype)\n                result_ref = test_impl(n, dtype)\n                pd.testing.assert_index_equal(result, result_ref)\n\n    def test_range_index_create_param_dtype_invalid(self):\n        def test_impl(stop, dtype):\n            return pd.RangeIndex(stop, dtype=dtype)\n        sdc_func = self.jit(test_impl)\n\n        n = 11\n        invalid_dtypes = [\'float\', np.int32, \'int32\']\n        for dtype in invalid_dtypes:\n            with self.subTest(dtype=dtype):\n                with self.assertRaises(Exception) as context:\n                    test_impl(n, dtype)\n                pandas_exception = context.exception\n\n                with self.assertRaises(type(pandas_exception)) as context:\n                    sdc_func(n, dtype)\n                sdc_exception = context.exception\n                self.assertIn(str(sdc_exception), str(pandas_exception))\n\n    def test_range_index_attribute_start(self):\n        def test_impl(*args):\n            index = pd.RangeIndex(*args)\n            return index.start\n        sdc_func = self.jit(test_impl)\n\n        for params in _generate_valid_range_params():\n            start, stop, step = params\n            with self.subTest(start=start, stop=stop, step=step):\n                result = sdc_func(*params)\n                result_ref = test_impl(*params)\n                self.assertEqual(result, result_ref)\n\n    def test_range_index_attribute_stop(self):\n        def test_impl(*args):\n            index = pd.RangeIndex(*args)\n            return index.stop\n        sdc_func = self.jit(test_impl)\n\n        for params in _generate_valid_range_params():\n            start, stop, step = params\n            with self.subTest(start=start, stop=stop, step=step):\n                result = sdc_func(*params)\n                result_ref = test_impl(*params)\n                self.assertEqual(result, result_ref)\n\n    def test_range_index_attribute_step(self):\n        def test_impl(*args):\n            index = pd.RangeIndex(*args)\n            return index.step\n        sdc_func = self.jit(test_impl)\n\n        for params in _generate_valid_range_params():\n            start, stop, step = params\n            with self.subTest(start=start, stop=stop, step=step):\n                result = sdc_func(*params)\n                result_ref = test_impl(*params)\n                self.assertEqual(result, result_ref)\n\n    def test_range_index_attribute_dtype(self):\n        def test_impl(index):\n            return index.dtype\n        sdc_func = self.jit(test_impl)\n\n        index = pd.RangeIndex(11)\n        result = sdc_func(index)\n        result_ref = test_impl(index)\n        self.assertEqual(result, result_ref)\n\n    def test_range_index_attribute_name(self):\n        def test_impl(index):\n            return index.name\n        sdc_func = self.jit(test_impl)\n\n        n = 11\n        for name in test_global_index_names:\n            with self.subTest(name=name):\n                index = pd.RangeIndex(n, name=name)\n                result = sdc_func(index)\n                result_ref = test_impl(index)\n                self.assertEqual(result, result_ref)\n\n    def test_range_index_len(self):\n        def test_impl(*args):\n            index = pd.RangeIndex(*args)\n            return len(index)\n        sdc_func = self.jit(test_impl)\n\n        for params in _generate_valid_range_params():\n            start, stop, step = params\n            with self.subTest(start=start, stop=stop, step=step):\n                result = sdc_func(*params)\n                result_ref = test_impl(*params)\n                self.assertEqual(result, result_ref)\n\n    def test_range_index_attribute_values(self):\n        def test_impl(index):\n            return index.values\n        sdc_func = self.jit(test_impl)\n\n        for params in _generate_valid_range_params():\n            index = pd.RangeIndex(*params)\n            with self.subTest(index=index):\n                result = sdc_func(index)\n                result_ref = test_impl(index)\n                np.testing.assert_array_equal(result, result_ref)\n\n    def test_range_index_contains(self):\n        def test_impl(index, value):\n            return value in index\n        sdc_func = self.jit(test_impl)\n\n        index = pd.RangeIndex(1, 11, 2)\n        values_to_test = [-5, 15, 1, 11, 5, 6]\n        for value in values_to_test:\n            with self.subTest(value=value):\n                result = sdc_func(index, value)\n                result_ref = test_impl(index, value)\n                np.testing.assert_array_equal(result, result_ref)\n\n    def test_range_index_copy(self):\n        def test_impl(index, new_name):\n            return index.copy(name=new_name)\n        sdc_func = self.jit(test_impl)\n\n        for params in _generate_valid_range_params():\n            start, stop, step = params\n            for name, new_name in product(test_global_index_names, repeat=2):\n                index = pd.RangeIndex(start, stop, step, name=name)\n                with self.subTest(index=index, new_name=new_name):\n                    result = sdc_func(index, new_name)\n                    result_ref = test_impl(index, new_name)\n                    pd.testing.assert_index_equal(result, result_ref)\n\n    def test_range_index_getitem_scalar(self):\n        def test_impl(index, idx):\n            return index[idx]\n        sdc_func = self.jit(test_impl)\n\n        for params in _generate_valid_range_params():\n            index = pd.RangeIndex(*params)\n            n = len(index)\n            if not n:  # test only non-empty ranges\n                continue\n            values_to_test = [-n, n // 2, n - 1]\n            for idx in values_to_test:\n                with self.subTest(index=index, idx=idx):\n                    result = sdc_func(index, idx)\n                    result_ref = test_impl(index, idx)\n                    self.assertEqual(result, result_ref)\n\n    def test_range_index_getitem_scalar_idx_bounds(self):\n        def test_impl(index, idx):\n            return index[idx]\n        sdc_func = self.jit(test_impl)\n\n        n = 11\n        index = pd.RangeIndex(n, name=\'abc\')\n        values_to_test = [-(n + 1), n]\n        for idx in values_to_test:\n            with self.subTest(idx=idx):\n                with self.assertRaises(Exception) as context:\n                    test_impl(index, idx)\n                pandas_exception = context.exception\n\n                with self.assertRaises(type(pandas_exception)) as context:\n                    sdc_func(index, idx)\n                sdc_exception = context.exception\n                self.assertIsInstance(sdc_exception, type(pandas_exception))\n                self.assertIn(""out of bounds"", str(sdc_exception))\n\n    def test_range_index_getitem_slice(self):\n        def test_impl(index, idx):\n            return index[idx]\n        sdc_func = self.jit(test_impl)\n\n        index_len = 17\n        start_values, step_values = [0, 5, -5], [1, 2, 7]\n        slices_params = combinations_with_replacement(\n            [None, 0, -1, index_len // 2, index_len, index_len - 3, index_len + 3, -(index_len + 3)],\n            2\n        )\n\n        for start, step, slice_step in product(start_values, step_values, step_values):\n            stop = start + index_len\n            for slice_start, slice_stop in slices_params:\n                idx = slice(slice_start, slice_stop, slice_step)\n                index = pd.RangeIndex(start, stop, step, name=\'abc\')\n                with self.subTest(index=index, idx=idx):\n                    result = sdc_func(index, idx)\n                    result_ref = test_impl(index, idx)\n                    pd.testing.assert_index_equal(result, result_ref)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
sdc/tests/test_io.py,44,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numpy as np\nimport os\nimport pandas as pd\nimport platform\nimport pyarrow.parquet as pq\nimport unittest\nimport numba\nfrom numba.core.config import IS_32BITS\nfrom pandas import CategoricalDtype\n\nimport sdc\nfrom sdc.io.csv_ext import pandas_read_csv as pd_read_csv\nfrom sdc.tests.test_base import TestCase\nfrom sdc.tests.test_utils import (count_array_OneDs,\n                                  count_array_REPs,\n                                  count_parfor_OneDs,\n                                  count_parfor_REPs,\n                                  dist_IR_contains,\n                                  get_rank,\n                                  get_start_end,\n                                  skip_numba_jit,\n                                  skip_sdc_jit)\n\n\nkde_file = \'kde.parquet\'\n\n\nclass TestIO(TestCase):\n\n    def setUp(self):\n        if get_rank() == 0:\n            # test_csv_cat1\n            data = (""2,B,SA\\n""\n                    ""3,A,SBC\\n""\n                    ""4,C,S123\\n""\n                    ""5,B,BCD\\n"")\n\n            with open(""csv_data_cat1.csv"", ""w"") as f:\n                f.write(data)\n\n            # test_csv_single_dtype1\n            data = (""2,4.1\\n""\n                    ""3,3.4\\n""\n                    ""4,1.3\\n""\n                    ""5,1.1\\n"")\n\n            with open(""csv_data_dtype1.csv"", ""w"") as f:\n                f.write(data)\n\n            # test_np_io1\n            n = 111\n            A = np.random.ranf(n)\n            A.tofile(""np_file1.dat"")\n\n\nclass TestParquet(TestIO):\n\n    @skip_numba_jit\n    def test_pq_read(self):\n        def test_impl():\n            t = pq.read_table(\'kde.parquet\')\n            df = t.to_pandas()\n            X = df[\'points\']\n            return X.sum()\n\n        hpat_func = self.jit(test_impl)\n        np.testing.assert_almost_equal(hpat_func(), test_impl())\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit\n    def test_pq_read_global_str1(self):\n        def test_impl():\n            df = pd.read_parquet(kde_file)\n            X = df[\'points\']\n            return X.sum()\n\n        hpat_func = self.jit(test_impl)\n        np.testing.assert_almost_equal(hpat_func(), test_impl())\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit\n    def test_pq_read_freevar_str1(self):\n        kde_file2 = \'kde.parquet\'\n\n        def test_impl():\n            df = pd.read_parquet(kde_file2)\n            X = df[\'points\']\n            return X.sum()\n\n        hpat_func = self.jit(test_impl)\n        np.testing.assert_almost_equal(hpat_func(), test_impl())\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit\n    def test_pd_read_parquet(self):\n        def test_impl():\n            df = pd.read_parquet(\'kde.parquet\')\n            X = df[\'points\']\n            return X.sum()\n\n        hpat_func = self.jit(test_impl)\n        np.testing.assert_almost_equal(hpat_func(), test_impl())\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit\n    def test_pq_str(self):\n        def test_impl():\n            df = pq.read_table(\'example.parquet\').to_pandas()\n            A = df.two.values == \'foo\'\n            return A.sum()\n\n        hpat_func = self.jit(test_impl)\n        np.testing.assert_almost_equal(hpat_func(), test_impl())\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit\n    def test_pq_str_with_nan_seq(self):\n        def test_impl():\n            df = pq.read_table(\'example.parquet\').to_pandas()\n            A = df.five.values == \'foo\'\n            return A\n\n        hpat_func = self.jit(test_impl)\n        np.testing.assert_almost_equal(hpat_func(), test_impl())\n\n    @skip_numba_jit\n    def test_pq_str_with_nan_par(self):\n        def test_impl():\n            df = pq.read_table(\'example.parquet\').to_pandas()\n            A = df.five.values == \'foo\'\n            return A.sum()\n\n        hpat_func = self.jit(test_impl)\n        np.testing.assert_almost_equal(hpat_func(), test_impl())\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit\n    def test_pq_str_with_nan_par_multigroup(self):\n        def test_impl():\n            df = pq.read_table(\'example2.parquet\').to_pandas()\n            A = df.five.values == \'foo\'\n            return A.sum()\n\n        hpat_func = self.jit(test_impl)\n        np.testing.assert_almost_equal(hpat_func(), test_impl())\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit\n    def test_pq_bool(self):\n        def test_impl():\n            df = pq.read_table(\'example.parquet\').to_pandas()\n            return df.three.sum()\n\n        hpat_func = self.jit(test_impl)\n        np.testing.assert_almost_equal(hpat_func(), test_impl())\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit\n    def test_pq_nan(self):\n        def test_impl():\n            df = pq.read_table(\'example.parquet\').to_pandas()\n            return df.one.sum()\n\n        hpat_func = self.jit(test_impl)\n        np.testing.assert_almost_equal(hpat_func(), test_impl())\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit\n    def test_pq_float_no_nan(self):\n        def test_impl():\n            df = pq.read_table(\'example.parquet\').to_pandas()\n            return df.four.sum()\n\n        hpat_func = self.jit(test_impl)\n        np.testing.assert_almost_equal(hpat_func(), test_impl())\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit\n    def test_pq_pandas_date(self):\n        def test_impl():\n            df = pd.read_parquet(\'pandas_dt.pq\')\n            return pd.DataFrame({\'DT64\': df.DT64, \'col2\': df.DATE})\n\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_frame_equal(hpat_func(), test_impl())\n\n    @skip_sdc_jit(\'Error: Attribute ""dtype"" are different\\n\'\n                  \'[left]:  datetime64[ns]\\n\'\n                  \'[right]: object\')\n    @skip_numba_jit\n    def test_pq_spark_date(self):\n        def test_impl():\n            df = pd.read_parquet(\'sdf_dt.pq\')\n            return pd.DataFrame({\'DT64\': df.DT64, \'col2\': df.DATE})\n\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_frame_equal(hpat_func(), test_impl())\n\n\nclass TestCSV(TestIO):\n\n    def test_pyarrow(self):\n        tests = [\n            ""csv_keys1"",\n            ""csv_const_dtype1"",\n            ""csv_infer_file_default"",\n            ""csv_infer_file_sep"",\n            ""csv_infer_file_delimiter"",\n            ""csv_infer_file_names"",\n            ""csv_infer_parallel1"",\n            ""csv_skip1"",\n            ""csv_infer_skip1"",\n            ""csv_infer_skip_parallel1"",\n            ""csv_rm_dead1"",\n            ""csv_date1"",\n            ""csv_str1"",\n            ""csv_parallel1"",\n            ""csv_str_parallel1"",\n            ""csv_usecols1"",\n            ""csv_cat1"",\n            ""csv_cat2"",\n            ""csv_single_dtype1"",\n        ]\n        for test in tests:\n            with self.subTest(test=test):\n                test = getattr(self, f""pd_{test}"")\n                pd_val = test(use_pyarrow=False)()\n                pa_val = test(use_pyarrow=True)()\n                if isinstance(pd_val, pd.Series):\n                    pd.testing.assert_series_equal(pa_val, pd_val,\n                        check_categorical=False\n                    )\n                elif isinstance(pd_val, pd.DataFrame):\n                    pd.testing.assert_frame_equal(pa_val, pd_val,\n                        check_categorical=False\n                    )\n                elif isinstance(pd_val, np.ndarray):\n                    np.testing.assert_array_equal(pa_val, pd_val)\n                elif isinstance(pd_val, tuple):\n                    self.assertEqual(pa_val, pd_val)\n                else:\n                    self.fail(f""Unknown Pandas type: {type(pd_val)}"")\n\n    def _int_type(self):\n        # TODO: w/a for Numba issue with int typing rules infering intp for integers literals\n        # unlike NumPy which uses int32 by default - causes dtype mismatch on Windows 64 bit\n        if platform.system() == \'Windows\' and not IS_32BITS:\n            return np.intp\n        else:\n            return np.int\n\n    def _int_type_str(self):\n        return np.dtype(self._int_type()).name\n\n    def _read_csv(self, use_pyarrow=False):\n        return pd_read_csv if use_pyarrow else pd.read_csv\n\n    # inference errors\n\n    def test_csv_infer_error(self):\n        read_csv = self._read_csv()\n\n        def pyfunc(fname):\n            return read_csv(fname)\n\n        cfunc = self.jit(pyfunc)\n\n        with self.assertRaises(numba.core.errors.TypingError) as cm:\n            cfunc(""csv_data1.csv"")\n\n        self.assertIn(""Cannot infer resulting DataFrame"", cm.exception.msg)\n\n    # inference from parameters\n\n    def test_csv_infer_params_default(self):\n        read_csv = self._read_csv()\n        int_type = self._int_type()\n\n        def pyfunc(fname):\n            names = [\'A\', \'B\', \'C\', \'D\']\n            dtype = {\'A\': int_type, \'B\': np.float, \'C\': \'float\', \'D\': str}\n            return read_csv(fname, names=names, dtype=dtype)\n\n        cfunc = self.jit(pyfunc)\n\n        for fname in [""csv_data1.csv"", ""csv_data2.csv""]:\n            with self.subTest(fname=fname):\n                pd.testing.assert_frame_equal(cfunc(fname), pyfunc(fname))\n\n    def test_csv_infer_params_usecols_names(self):\n        read_csv = self._read_csv()\n        int_type = self._int_type()\n\n        def pyfunc(fname):\n            names = [\'A\', \'B\', \'C\', \'D\']\n            dtype = {\'A\': int_type, \'B\': np.float, \'C\': np.float, \'D\': str}\n            usecols = [\'B\', \'D\']\n            return read_csv(fname, names=names, dtype=dtype, usecols=usecols)\n\n        fname = ""csv_data1.csv""\n        cfunc = self.jit(pyfunc)\n        pd.testing.assert_frame_equal(cfunc(fname), pyfunc(fname))\n\n    def test_csv_infer_params_usecols_no_names(self):\n        read_csv = self._read_csv()\n        int_type = self._int_type()\n\n        def pyfunc(fname):\n            dtype = {\'B\': np.float, \'D\': str}\n            usecols = [\'B\', \'D\']\n            return read_csv(fname, dtype=dtype, usecols=usecols)\n\n        fname = ""csv_data_infer1.csv""\n        cfunc = self.jit(pyfunc)\n        pd.testing.assert_frame_equal(cfunc(fname), pyfunc(fname))\n\n    def pd_csv_keys1(self, use_pyarrow=False):\n        read_csv = self._read_csv(use_pyarrow)\n        int_type = self._int_type()\n\n        def test_impl():\n            dtype = {\'A\': int_type, \'B\': np.float, \'C\': np.float, \'D\': str}\n            return read_csv(""csv_data1.csv"",\n                            names=dtype.keys(),\n                            dtype=dtype,\n                            )\n\n        return test_impl\n\n    @skip_numba_jit\n    def test_csv_keys1(self):\n        test_impl = self.pd_csv_keys1()\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_frame_equal(hpat_func(), test_impl())\n\n    def pd_csv_const_dtype1(self, use_pyarrow=False):\n        read_csv = self._read_csv(use_pyarrow)\n        int_type = self._int_type_str()\n\n        def test_impl():\n            dtype = {\'A\': int_type, \'B\': \'float64\', \'C\': \'float\', \'D\': \'str\'}\n            return read_csv(""csv_data1.csv"",\n                            names=dtype.keys(),\n                            dtype=dtype,\n                            )\n\n        return test_impl\n\n    @skip_numba_jit\n    def test_csv_const_dtype1(self):\n        test_impl = self.pd_csv_const_dtype1()\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_frame_equal(hpat_func(), test_impl())\n\n    # inference from file\n\n    def pd_csv_infer_file_default(self, file_name=""csv_data_infer1.csv"", use_pyarrow=False):\n        read_csv = self._read_csv(use_pyarrow)\n\n        def test_impl():\n            return read_csv(file_name)\n\n        return test_impl\n\n    def test_csv_infer_file_default(self):\n        def test(file_name):\n            test_impl = self.pd_csv_infer_file_default(file_name)\n            hpat_func = self.jit(test_impl)\n            pd.testing.assert_frame_equal(hpat_func(), test_impl())\n\n        for file_name in [""csv_data_infer1.csv"", ""csv_data_infer_no_column_name.csv""]:\n            with self.subTest(file_name=file_name):\n                test(file_name)\n\n    def pd_csv_infer_file_sep(self, use_pyarrow=False):\n        read_csv = self._read_csv(use_pyarrow)\n\n        def test_impl():\n            return read_csv(""csv_data_infer_sep.csv"", sep=\';\')\n\n        return test_impl\n\n    def test_csv_infer_file_sep(self):\n        test_impl = self.pd_csv_infer_file_sep()\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_frame_equal(hpat_func(), test_impl())\n\n    def pd_csv_infer_file_delimiter(self, use_pyarrow=False):\n        read_csv = self._read_csv(use_pyarrow)\n\n        def test_impl():\n            return read_csv(""csv_data_infer_sep.csv"", delimiter=\';\')\n\n        return test_impl\n\n    def test_csv_infer_file_delimiter(self):\n        test_impl = self.pd_csv_infer_file_delimiter()\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_frame_equal(hpat_func(), test_impl())\n\n    def pd_csv_infer_file_names(self, file_name=""csv_data1.csv"", use_pyarrow=False):\n        read_csv = self._read_csv(use_pyarrow)\n\n        def test_impl():\n            return read_csv(file_name, names=[\'A\', \'B\', \'C\', \'D\'])\n\n        return test_impl\n\n    def test_csv_infer_file_names(self):\n        def test(file_name):\n            test_impl = self.pd_csv_infer_file_names(file_name)\n            hpat_func = self.jit(test_impl)\n            pd.testing.assert_frame_equal(hpat_func(), test_impl())\n\n        for file_name in [""csv_data1.csv"", ""csv_data_infer1.csv""]:\n            with self.subTest(file_name=file_name):\n                test(file_name)\n\n    def pd_csv_infer_file_usecols(self, file_name=""csv_data_infer1.csv"", use_pyarrow=False):\n        read_csv = self._read_csv(use_pyarrow)\n\n        def test_impl():\n            return read_csv(file_name, usecols=[\'B\', \'D\'])\n\n        return test_impl\n\n    def test_csv_infer_file_usecols(self):\n        def test(file_name):\n            test_impl = self.pd_csv_infer_file_usecols(file_name)\n            hpat_func = self.jit(test_impl)\n            pd.testing.assert_frame_equal(hpat_func(), test_impl())\n\n        for file_name in [""csv_data_infer1.csv""]:\n            with self.subTest(file_name=file_name):\n                test(file_name)\n\n    def pd_csv_infer_file_names_usecols(self, file_name=""csv_data1.csv"", use_pyarrow=False):\n        read_csv = self._read_csv(use_pyarrow)\n\n        def test_impl():\n            return read_csv(file_name, names=[\'A\', \'B\', \'C\', \'D\'], usecols=[\'B\', \'D\'])\n\n        return test_impl\n\n    def test_csv_infer_file_names_usecols(self):\n        def test(file_name):\n            test_impl = self.pd_csv_infer_file_names_usecols(file_name)\n            hpat_func = self.jit(test_impl)\n            pd.testing.assert_frame_equal(hpat_func(), test_impl())\n\n        for file_name in [""csv_data1.csv"", ""csv_data_infer1.csv""]:\n            with self.subTest(file_name=file_name):\n                test(file_name)\n\n    def pd_csv_infer_parallel1(self, use_pyarrow=False):\n        read_csv = self._read_csv(use_pyarrow)\n\n        def test_impl():\n            df = read_csv(""csv_data_infer1.csv"")\n            return df.A.sum(), df.B.sum(), df.C.sum()\n\n        return test_impl\n\n    @skip_numba_jit\n    def test_csv_infer_parallel1(self):\n        test_impl = self.pd_csv_infer_parallel1()\n        hpat_func = self.jit(test_impl)\n        self.assertEqual(hpat_func(), test_impl())\n\n    def pd_csv_skip1(self, use_pyarrow=False):\n        read_csv = self._read_csv(use_pyarrow)\n        int_type = self._int_type()\n\n        def test_impl():\n            return read_csv(""csv_data1.csv"",\n                            names=[\'A\', \'B\', \'C\', \'D\'],\n                            dtype={\'A\': int_type, \'B\': np.float, \'C\': np.float, \'D\': str},\n                            skiprows=2,\n                            )\n\n        return test_impl\n\n    @skip_numba_jit\n    def test_csv_skip1(self):\n        test_impl = self.pd_csv_skip1()\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_frame_equal(hpat_func(), test_impl())\n\n    def pd_csv_infer_skip1(self, use_pyarrow=False):\n        read_csv = self._read_csv(use_pyarrow)\n\n        def test_impl():\n            return read_csv(""csv_data_infer1.csv"", skiprows=2)\n\n        return test_impl\n\n    def test_csv_infer_skip1(self):\n        test_impl = self.pd_csv_infer_skip1()\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_frame_equal(hpat_func(), test_impl())\n\n    def pd_csv_infer_skip_parallel1(self, use_pyarrow=False):\n        read_csv = self._read_csv(use_pyarrow)\n\n        def test_impl():\n            df = read_csv(""csv_data_infer1.csv"", skiprows=2,\n                          names=[\'A\', \'B\', \'C\', \'D\'])\n            return df.A.sum(), df.B.sum(), df.C.sum()\n\n        return test_impl\n\n    @skip_numba_jit\n    def test_csv_infer_skip_parallel1(self):\n        test_impl = self.pd_csv_infer_skip_parallel1()\n        hpat_func = self.jit(test_impl)\n        self.assertEqual(hpat_func(), test_impl())\n\n    def pd_csv_rm_dead1(self, use_pyarrow=False):\n        read_csv = self._read_csv(use_pyarrow)\n\n        def test_impl():\n            df = read_csv(""csv_data1.csv"",\n                          names=[\'A\', \'B\', \'C\', \'D\'],\n                          dtype={\'A\': np.int, \'B\': np.float, \'C\': np.float, \'D\': str},)\n            return df.B.values\n\n        return test_impl\n\n    @skip_numba_jit\n    def test_csv_rm_dead1(self):\n        test_impl = self.pd_csv_rm_dead1()\n        hpat_func = self.jit(test_impl)\n        np.testing.assert_array_equal(hpat_func(), test_impl())\n\n    def pd_csv_date1(self, use_pyarrow=False):\n        read_csv = self._read_csv(use_pyarrow)\n        int_type = self._int_type()\n\n        def test_impl():\n            return read_csv(""csv_data_date1.csv"",\n                            names=[\'A\', \'B\', \'C\', \'D\'],\n                            dtype={\'A\': int_type, \'B\': np.float, \'C\': str, \'D\': np.int64},\n                            parse_dates=[2])\n\n        return test_impl\n\n    @skip_numba_jit\n    def test_csv_date1(self):\n        test_impl = self.pd_csv_date1()\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_frame_equal(hpat_func(), test_impl())\n\n    def pd_csv_str1(self, use_pyarrow=False):\n        read_csv = self._read_csv(use_pyarrow)\n        int_type = self._int_type()\n\n        def test_impl():\n            return read_csv(""csv_data_date1.csv"",\n                            names=[\'A\', \'B\', \'C\', \'D\'],\n                            dtype={\'A\': int_type, \'B\': np.float, \'C\': str, \'D\': np.int64})\n\n        return test_impl\n\n    def test_csv_str1(self):\n        test_impl = self.pd_csv_str1()\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_frame_equal(hpat_func(), test_impl())\n\n    def pd_csv_parallel1(self, use_pyarrow=False):\n        read_csv = self._read_csv(use_pyarrow)\n\n        def test_impl():\n            df = read_csv(""csv_data1.csv"",\n                          names=[\'A\', \'B\', \'C\', \'D\'],\n                          dtype={\'A\': np.int, \'B\': np.float, \'C\': np.float, \'D\': str})\n            return (df.A.sum(), df.B.sum(), df.C.sum())\n\n        return test_impl\n\n    @skip_numba_jit\n    def test_csv_parallel1(self):\n        test_impl = self.pd_csv_parallel1()\n        hpat_func = self.jit(test_impl)\n        self.assertEqual(hpat_func(), test_impl())\n\n    def pd_csv_str_parallel1(self, use_pyarrow=False):\n        read_csv = self._read_csv(use_pyarrow)\n\n        def test_impl():\n            df = read_csv(""csv_data_date1.csv"",\n                          names=[\'A\', \'B\', \'C\', \'D\'],\n                          dtype={\'A\': np.int, \'B\': np.float, \'C\': str, \'D\': np.int})\n            return (df.A.sum(), df.B.sum(), (df.C == \'1966-11-13\').sum(), df.D.sum())\n\n        return test_impl\n\n    @skip_numba_jit\n    def test_csv_str_parallel1(self):\n        test_impl = self.pd_csv_str_parallel1()\n        hpat_func = self.jit(locals={\'df:return\': \'distributed\'})(test_impl)\n        self.assertEqual(hpat_func(), test_impl())\n\n    def pd_csv_usecols1(self, use_pyarrow=False):\n        read_csv = self._read_csv(use_pyarrow)\n\n        def test_impl():\n            return read_csv(""csv_data1.csv"",\n                            names=[\'C\'],\n                            dtype={\'C\': np.float},\n                            usecols=[2],\n                            )\n\n        return test_impl\n\n    @skip_numba_jit\n    def test_csv_usecols1(self):\n        test_impl = self.pd_csv_usecols1()\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_frame_equal(hpat_func(), test_impl())\n\n    def pd_csv_cat1(self, use_pyarrow=False):\n        read_csv = self._read_csv(use_pyarrow)\n\n        def test_impl():\n            names = [\'C1\', \'C2\', \'C3\']\n            ct_dtype = CategoricalDtype([\'A\', \'B\', \'C\'])\n            dtypes = {\'C1\': np.int, \'C2\': ct_dtype, \'C3\': str}\n            df = read_csv(""csv_data_cat1.csv"", names=names, dtype=dtypes)\n            return df\n\n        return test_impl\n\n    def test_csv_cat1(self):\n        test_impl = self.pd_csv_cat1()\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_frame_equal(hpat_func(), test_impl(), check_names=False)\n\n    def pd_csv_cat2(self, use_pyarrow=False):\n        read_csv = self._read_csv(use_pyarrow)\n        int_type = self._int_type()\n\n        def test_impl():\n            ct_dtype = CategoricalDtype([\'A\', \'B\', \'C\', \'D\'])\n            df = read_csv(""csv_data_cat1.csv"",\n                          names=[\'C1\', \'C2\', \'C3\'],\n                          dtype={\'C1\': int_type, \'C2\': ct_dtype, \'C3\': str},\n                          )\n            return df\n\n        return test_impl\n\n    def test_csv_cat2(self):\n        test_impl = self.pd_csv_cat2()\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_frame_equal(hpat_func(), test_impl())\n\n    def pd_csv_single_dtype1(self, use_pyarrow=False):\n        read_csv = self._read_csv(use_pyarrow)\n\n        def test_impl():\n            df = read_csv(""csv_data_dtype1.csv"",\n                          names=[\'C1\', \'C2\'],\n                          dtype=np.float64,\n                          )\n            return df\n\n        return test_impl\n\n    @skip_numba_jit\n    def test_csv_single_dtype1(self):\n        test_impl = self.pd_csv_single_dtype1()\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_frame_equal(hpat_func(), test_impl())\n\n    @skip_numba_jit\n    @skip_sdc_jit(\'TypeError: to_csv() takes from 1 to 20 positional arguments but 21 were given)\\n\'\n                  \'Notice: Not seen with Pandas 0.24.2\')\n    def test_write_csv1(self):\n        def test_impl(df, fname):\n            df.to_csv(fname)\n\n        hpat_func = self.jit(test_impl)\n        n = 111\n        df = pd.DataFrame({\'A\': np.arange(n)})\n        hp_fname = \'test_write_csv1_sdc.csv\'\n        pd_fname = \'test_write_csv1_pd.csv\'\n        hpat_func(df, hp_fname)\n        test_impl(df, pd_fname)\n        # TODO: delete files\n        pd.testing.assert_frame_equal(pd.read_csv(hp_fname), pd.read_csv(pd_fname))\n\n    @skip_numba_jit\n    @skip_sdc_jit(\'AttributeError: Failed in hpat mode pipeline (step: convert to distributed)\\n\'\n                  \'module \\\'sdc.hio\\\' has no attribute \\\'file_write_parallel\\\'\')\n    def test_write_csv_parallel1(self):\n        def test_impl(n, fname):\n            df = pd.DataFrame({\'A\': np.arange(n)})\n            df.to_csv(fname)\n\n        hpat_func = self.jit(test_impl)\n        n = 111\n        hp_fname = \'test_write_csv1_hpat_par.csv\'\n        pd_fname = \'test_write_csv1_pd_par.csv\'\n        hpat_func(n, hp_fname)\n        test_impl(n, pd_fname)\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n        # TODO: delete files\n        if get_rank() == 0:\n            pd.testing.assert_frame_equal(\n                pd.read_csv(hp_fname), pd.read_csv(pd_fname))\n\n\nclass TestNumpy(TestIO):\n\n    @skip_numba_jit\n    def test_np_io1(self):\n        def test_impl():\n            A = np.fromfile(""np_file1.dat"", np.float64)\n            return A\n\n        hpat_func = self.jit(test_impl)\n        np.testing.assert_almost_equal(hpat_func(), test_impl())\n\n    @skip_numba_jit\n    @skip_sdc_jit(\'Not implemented in sequential transport layer\')\n    def test_np_io2(self):\n        # parallel version\n        def test_impl():\n            A = np.fromfile(""np_file1.dat"", np.float64)\n            return A.sum()\n\n        hpat_func = self.jit(test_impl)\n        np.testing.assert_almost_equal(hpat_func(), test_impl())\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    def test_np_io3(self):\n        def test_impl(A):\n            if get_rank() == 0:\n                A.tofile(""np_file_3.dat"")\n\n        hpat_func = self.jit(test_impl)\n        n = 111\n        A = np.random.ranf(n)\n        hpat_func(A)\n        if get_rank() == 0:\n            B = np.fromfile(""np_file_3.dat"", np.float64)\n            np.testing.assert_almost_equal(A, B)\n\n    @skip_numba_jit(""AssertionError: Failed in nopython mode pipeline (step: Preprocessing for parfors)"")\n    def test_np_io4(self):\n        # parallel version\n        def test_impl(n):\n            A = np.arange(n)\n            A.tofile(""np_file_3.dat"")\n\n        hpat_func = self.jit(test_impl)\n        n = 111\n        A = np.arange(n)\n        hpat_func(n)\n        B = np.fromfile(""np_file_3.dat"", np.int64)\n        np.testing.assert_almost_equal(A, B)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
sdc/tests/test_join.py,26,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numba\nimport numpy as np\nimport pandas as pd\nimport platform\nimport pyarrow.parquet as pq\nimport random\nimport string\nimport unittest\nfrom pandas.api.types import CategoricalDtype\n\nimport sdc\nfrom sdc.str_arr_ext import StringArray\nfrom sdc.tests.test_base import TestCase\nfrom sdc.tests.test_utils import (count_array_OneDs,\n                                  count_array_REPs,\n                                  count_parfor_OneDs,\n                                  count_parfor_REPs,\n                                  dist_IR_contains,\n                                  get_start_end,\n                                  skip_numba_jit)\n\n\nclass TestJoin(TestCase):\n\n    @skip_numba_jit\n    def test_join1(self):\n        def test_impl(n):\n            df1 = pd.DataFrame({\'key1\': np.arange(n) + 3, \'A\': np.arange(n) + 1.0})\n            df2 = pd.DataFrame({\'key2\': 2 * np.arange(n) + 1, \'B\': n + np.arange(n) + 1.0})\n            df3 = pd.merge(df1, df2, left_on=\'key1\', right_on=\'key2\')\n            return df3.B.sum()\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        self.assertEqual(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n        n = 11111\n        self.assertEqual(hpat_func(n), test_impl(n))\n\n    @skip_numba_jit\n    def test_join1_seq(self):\n        def test_impl(df1, df2):\n            df3 = df1.merge(df2, left_on=\'key1\', right_on=\'key2\')\n            return df3\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        df1 = pd.DataFrame({\'key1\': np.arange(n) + 3, \'A\': np.arange(n) + 1.0})\n        df2 = pd.DataFrame({\'key2\': 2 * np.arange(n) + 1, \'B\': n + np.arange(n) + 1.0})\n        pd.testing.assert_frame_equal(hpat_func(df1, df2), test_impl(df1, df2))\n        n = 11111\n        df1 = pd.DataFrame({\'key1\': np.arange(n) + 3, \'A\': np.arange(n) + 1.0})\n        df2 = pd.DataFrame({\'key2\': 2 * np.arange(n) + 1, \'B\': n + np.arange(n) + 1.0})\n        pd.testing.assert_frame_equal(hpat_func(df1, df2), test_impl(df1, df2))\n\n    @skip_numba_jit\n    def test_join1_seq_str(self):\n        def test_impl():\n            df1 = pd.DataFrame({\'key1\': [\'foo\', \'bar\', \'baz\']})\n            df2 = pd.DataFrame({\'key2\': [\'baz\', \'bar\', \'baz\'], \'B\': [\'b\', \'zzz\', \'ss\']})\n            df3 = pd.merge(df1, df2, left_on=\'key1\', right_on=\'key2\')\n            return df3.B\n\n        hpat_func = self.jit(test_impl)\n        self.assertEqual(set(hpat_func()), set(test_impl()))\n\n    @skip_numba_jit\n    def test_join1_seq_str_na(self):\n        # test setting NA in string data column\n        def test_impl():\n            df1 = pd.DataFrame({\'key1\': [\'foo\', \'bar\', \'baz\']})\n            df2 = pd.DataFrame({\'key2\': [\'baz\', \'bar\', \'baz\'], \'B\': [\'b\', \'zzz\', \'ss\']})\n            df3 = df1.merge(df2, left_on=\'key1\', right_on=\'key2\', how=\'left\')\n            return df3.B\n\n        hpat_func = self.jit(test_impl)\n        self.assertEqual(set(hpat_func()), set(test_impl()))\n\n    @skip_numba_jit\n    def test_join_mutil_seq1(self):\n        def test_impl(df1, df2):\n            return df1.merge(df2, on=[\'A\', \'B\'])\n\n        hpat_func = self.jit(test_impl)\n        df1 = pd.DataFrame({\'A\': [3, 1, 1, 3, 4],\n                            \'B\': [1, 2, 3, 2, 3],\n                            \'C\': [7, 8, 9, 4, 5]})\n\n        df2 = pd.DataFrame({\'A\': [2, 1, 4, 4, 3],\n                            \'B\': [1, 3, 2, 3, 2],\n                            \'D\': [1, 2, 3, 4, 8]})\n\n        pd.testing.assert_frame_equal(hpat_func(df1, df2), test_impl(df1, df2))\n\n    @skip_numba_jit\n    def test_join_mutil_parallel1(self):\n        def test_impl(A1, B1, C1, A2, B2, D2):\n            df1 = pd.DataFrame({\'A\': A1, \'B\': B1, \'C\': C1})\n            df2 = pd.DataFrame({\'A\': A2, \'B\': B2, \'D\': D2})\n            df3 = df1.merge(df2, on=[\'A\', \'B\'])\n            return df3.C.sum() + df3.D.sum()\n\n        hpat_func = self.jit(locals={\n            \'A1:input\': \'distributed\',\n            \'B1:input\': \'distributed\',\n            \'C1:input\': \'distributed\',\n            \'A2:input\': \'distributed\',\n            \'B2:input\': \'distributed\',\n            \'D2:input\': \'distributed\', })(test_impl)\n        df1 = pd.DataFrame({\'A\': [3, 1, 1, 3, 4],\n                            \'B\': [1, 2, 3, 2, 3],\n                            \'C\': [7, 8, 9, 4, 5]})\n\n        df2 = pd.DataFrame({\'A\': [2, 1, 4, 4, 3],\n                            \'B\': [1, 3, 2, 3, 2],\n                            \'D\': [1, 2, 3, 4, 8]})\n\n        start, end = get_start_end(len(df1))\n        h_A1 = df1.A.values[start:end]\n        h_B1 = df1.B.values[start:end]\n        h_C1 = df1.C.values[start:end]\n        h_A2 = df2.A.values[start:end]\n        h_B2 = df2.B.values[start:end]\n        h_D2 = df2.D.values[start:end]\n        p_A1 = df1.A.values\n        p_B1 = df1.B.values\n        p_C1 = df1.C.values\n        p_A2 = df2.A.values\n        p_B2 = df2.B.values\n        p_D2 = df2.D.values\n        h_res = hpat_func(h_A1, h_B1, h_C1, h_A2, h_B2, h_D2)\n        p_res = test_impl(p_A1, p_B1, p_C1, p_A2, p_B2, p_D2)\n        self.assertEqual(h_res, p_res)\n\n    @skip_numba_jit\n    def test_join_left_parallel1(self):\n        """"""\n        """"""\n        def test_impl(A1, B1, C1, A2, B2, D2):\n            df1 = pd.DataFrame({\'A\': A1, \'B\': B1, \'C\': C1})\n            df2 = pd.DataFrame({\'A\': A2, \'B\': B2, \'D\': D2})\n            df3 = df1.merge(df2, on=(\'A\', \'B\'))\n            return df3.C.sum() + df3.D.sum()\n\n        hpat_func = self.jit(locals={\n            \'A1:input\': \'distributed\',\n            \'B1:input\': \'distributed\',\n            \'C1:input\': \'distributed\', })(test_impl)\n        df1 = pd.DataFrame({\'A\': [3, 1, 1, 3, 4],\n                            \'B\': [1, 2, 3, 2, 3],\n                            \'C\': [7, 8, 9, 4, 5]})\n\n        df2 = pd.DataFrame({\'A\': [2, 1, 4, 4, 3],\n                            \'B\': [1, 3, 2, 3, 2],\n                            \'D\': [1, 2, 3, 4, 8]})\n\n        start, end = get_start_end(len(df1))\n        h_A1 = df1.A.values[start:end]\n        h_B1 = df1.B.values[start:end]\n        h_C1 = df1.C.values[start:end]\n        h_A2 = df2.A.values\n        h_B2 = df2.B.values\n        h_D2 = df2.D.values\n        p_A1 = df1.A.values\n        p_B1 = df1.B.values\n        p_C1 = df1.C.values\n        p_A2 = df2.A.values\n        p_B2 = df2.B.values\n        p_D2 = df2.D.values\n        h_res = hpat_func(h_A1, h_B1, h_C1, h_A2, h_B2, h_D2)\n        p_res = test_impl(p_A1, p_B1, p_C1, p_A2, p_B2, p_D2)\n        self.assertEqual(h_res, p_res)\n        self.assertEqual(count_array_OneDs(), 3)\n\n    @skip_numba_jit\n    def test_join_datetime_seq1(self):\n        def test_impl(df1, df2):\n            return pd.merge(df1, df2, on=\'time\')\n\n        hpat_func = self.jit(test_impl)\n        df1 = pd.DataFrame(\n            {\'time\': pd.DatetimeIndex(\n                [\'2017-01-03\', \'2017-01-06\', \'2017-02-21\']), \'B\': [4, 5, 6]})\n        df2 = pd.DataFrame(\n            {\'time\': pd.DatetimeIndex(\n                [\'2017-01-01\', \'2017-01-06\', \'2017-01-03\']), \'A\': [7, 8, 9]})\n        pd.testing.assert_frame_equal(hpat_func(df1, df2), test_impl(df1, df2))\n\n    @unittest.skip(""Method max(). Currently function supports only numeric values. Given data type: datetime64[ns]"")\n    def test_join_datetime_parallel1(self):\n        def test_impl(df1, df2):\n            df3 = pd.merge(df1, df2, on=\'time\')\n            return (df3.A.sum(), df3.time.max(), df3.B.sum())\n\n        hpat_func = self.jit(distributed=[\'df1\', \'df2\'])(test_impl)\n        df1 = pd.DataFrame(\n            {\'time\': pd.DatetimeIndex(\n                [\'2017-01-03\', \'2017-01-06\', \'2017-02-21\']), \'B\': [4, 5, 6]})\n        df2 = pd.DataFrame(\n            {\'time\': pd.DatetimeIndex(\n                [\'2017-01-01\', \'2017-01-06\', \'2017-01-03\']), \'A\': [7, 8, 9]})\n        start1, end1 = get_start_end(len(df1))\n        start2, end2 = get_start_end(len(df2))\n        self.assertEqual(\n            hpat_func(df1.iloc[start1:end1], df2.iloc[start2:end2]),\n            test_impl(df1, df2))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit\n    def test_merge_asof_seq1(self):\n        def test_impl(df1, df2):\n            return pd.merge_asof(df1, df2, on=\'time\')\n\n        hpat_func = self.jit(test_impl)\n        df1 = pd.DataFrame(\n            {\'time\': pd.DatetimeIndex(\n                [\'2017-01-03\', \'2017-01-06\', \'2017-02-21\']), \'B\': [4, 5, 6]})\n        df2 = pd.DataFrame(\n            {\'time\': pd.DatetimeIndex(\n                [\'2017-01-01\', \'2017-01-02\', \'2017-01-04\', \'2017-02-23\',\n                 \'2017-02-25\']), \'A\': [2, 3, 7, 8, 9]})\n        pd.testing.assert_frame_equal(hpat_func(df1, df2), test_impl(df1, df2))\n\n    @unittest.skip(""Method max(). Currently function supports only numeric values. Given data type: datetime64[ns]"")\n    def test_merge_asof_parallel1(self):\n        def test_impl():\n            df1 = pd.read_parquet(\'asof1.pq\')\n            df2 = pd.read_parquet(\'asof2.pq\')\n            df3 = pd.merge_asof(df1, df2, on=\'time\')\n            return (df3.A.sum(), df3.time.max(), df3.B.sum())\n\n        hpat_func = self.jit(test_impl)\n        self.assertEqual(hpat_func(), test_impl())\n\n    @skip_numba_jit\n    def test_join_left_seq1(self):\n        def test_impl(df1, df2):\n            return pd.merge(df1, df2, how=\'left\', on=\'key\')\n\n        hpat_func = self.jit(test_impl)\n        df1 = pd.DataFrame(\n            {\'key\': [2, 3, 5, 1, 2, 8], \'A\': np.array([4, 6, 3, 9, 9, -1], np.float)})\n        df2 = pd.DataFrame(\n            {\'key\': [1, 2, 9, 3, 2], \'B\': np.array([1, 7, 2, 6, 5], np.float)})\n        h_res = hpat_func(df1, df2)\n        res = test_impl(df1, df2)\n        np.testing.assert_array_equal(h_res.key.values, res.key.values)\n        # converting arrays to sets since order of values can be different\n        self.assertEqual(set(h_res.A.values), set(res.A.values))\n        self.assertEqual(\n            set(h_res.B.dropna().values), set(res.B.dropna().values))\n\n    @skip_numba_jit\n    def test_join_left_seq2(self):\n        def test_impl(df1, df2):\n            return pd.merge(df1, df2, how=\'left\', on=\'key\')\n\n        hpat_func = self.jit(test_impl)\n        # test left run where a key is repeated on left but not right side\n        df1 = pd.DataFrame(\n            {\'key\': [2, 3, 5, 3, 2, 8], \'A\': np.array([4, 6, 3, 9, 9, -1], np.float)})\n        df2 = pd.DataFrame(\n            {\'key\': [1, 2, 9, 3, 10], \'B\': np.array([1, 7, 2, 6, 5], np.float)})\n        h_res = hpat_func(df1, df2)\n        res = test_impl(df1, df2)\n        np.testing.assert_array_equal(h_res.key.values, res.key.values)\n        # converting arrays to sets since order of values can be different\n        self.assertEqual(set(h_res.A.values), set(res.A.values))\n        self.assertEqual(\n            set(h_res.B.dropna().values), set(res.B.dropna().values))\n\n    @skip_numba_jit\n    def test_join_right_seq1(self):\n        def test_impl(df1, df2):\n            return pd.merge(df1, df2, how=\'right\', on=\'key\')\n\n        hpat_func = self.jit(test_impl)\n        df1 = pd.DataFrame(\n            {\'key\': [2, 3, 5, 1, 2, 8], \'A\': np.array([4, 6, 3, 9, 9, -1], np.float)})\n        df2 = pd.DataFrame(\n            {\'key\': [1, 2, 9, 3, 2], \'B\': np.array([1, 7, 2, 6, 5], np.float)})\n        h_res = hpat_func(df1, df2)\n        res = test_impl(df1, df2)\n        self.assertEqual(set(h_res.key.values), set(res.key.values))\n        # converting arrays to sets since order of values can be different\n        self.assertEqual(set(h_res.B.values), set(res.B.values))\n        self.assertEqual(\n            set(h_res.A.dropna().values), set(res.A.dropna().values))\n\n    @skip_numba_jit\n    def test_join_outer_seq1(self):\n        def test_impl(df1, df2):\n            return pd.merge(df1, df2, how=\'outer\', on=\'key\')\n\n        hpat_func = self.jit(test_impl)\n        df1 = pd.DataFrame(\n            {\'key\': [2, 3, 5, 1, 2, 8], \'A\': np.array([4, 6, 3, 9, 9, -1], np.float)})\n        df2 = pd.DataFrame(\n            {\'key\': [1, 2, 9, 3, 2], \'B\': np.array([1, 7, 2, 6, 5], np.float)})\n        h_res = hpat_func(df1, df2)\n        res = test_impl(df1, df2)\n        self.assertEqual(set(h_res.key.values), set(res.key.values))\n        # converting arrays to sets since order of values can be different\n        self.assertEqual(\n            set(h_res.B.dropna().values), set(res.B.dropna().values))\n        self.assertEqual(\n            set(h_res.A.dropna().values), set(res.A.dropna().values))\n\n    @skip_numba_jit\n    def test_join1_seq_key_change1(self):\n        # make sure const list typing doesn\'t replace const key values\n        def test_impl(df1, df2, df3, df4):\n            o1 = df1.merge(df2, on=[\'A\'])\n            o2 = df3.merge(df4, on=[\'B\'])\n            return o1, o2\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        df1 = pd.DataFrame({\'A\': np.arange(n) + 3, \'AA\': np.arange(n) + 1.0})\n        df2 = pd.DataFrame({\'A\': 2 * np.arange(n) + 1, \'AAA\': n + np.arange(n) + 1.0})\n        df3 = pd.DataFrame({\'B\': 2 * np.arange(n) + 1, \'BB\': n + np.arange(n) + 1.0})\n        df4 = pd.DataFrame({\'B\': 2 * np.arange(n) + 1, \'BBB\': n + np.arange(n) + 1.0})\n        pd.testing.assert_frame_equal(hpat_func(df1, df2, df3, df4)[1], test_impl(df1, df2, df3, df4)[1])\n\n    @skip_numba_jit\n    @unittest.skipIf(platform.system() == \'Windows\', ""error on windows"")\n    def test_join_cat1(self):\n        def test_impl():\n            ct_dtype = CategoricalDtype([\'A\', \'B\', \'C\'])\n            dtypes = {\'C1\': np.int, \'C2\': ct_dtype, \'C3\': str}\n            df1 = pd.read_csv(""csv_data_cat1.csv"",\n                              names=[\'C1\', \'C2\', \'C3\'],\n                              dtype=dtypes,\n                              )\n            n = len(df1)\n            df2 = pd.DataFrame({\'C1\': 2 * np.arange(n) + 1, \'AAA\': n + np.arange(n) + 1.0})\n            df3 = df1.merge(df2, on=\'C1\')\n            return df3\n\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_frame_equal(hpat_func(), test_impl())\n\n    @skip_numba_jit\n    @unittest.skipIf(platform.system() == \'Windows\', ""error on windows"")\n    def test_join_cat2(self):\n        # test setting NaN in categorical array\n        def test_impl():\n            ct_dtype = CategoricalDtype([\'A\', \'B\', \'C\'])\n            dtypes = {\'C1\': np.int, \'C2\': ct_dtype, \'C3\': str}\n            df1 = pd.read_csv(""csv_data_cat1.csv"",\n                              names=[\'C1\', \'C2\', \'C3\'],\n                              dtype=dtypes,\n                              )\n            n = len(df1)\n            df2 = pd.DataFrame({\'C1\': 2 * np.arange(n) + 1, \'AAA\': n + np.arange(n) + 1.0})\n            df3 = df1.merge(df2, on=\'C1\', how=\'right\')\n            return df3\n\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_frame_equal(\n            hpat_func().sort_values(\'C1\').reset_index(drop=True),\n            test_impl().sort_values(\'C1\').reset_index(drop=True))\n\n    @skip_numba_jit\n    @unittest.skipIf(platform.system() == \'Windows\', ""error on windows"")\n    def test_join_cat_parallel1(self):\n        # TODO: cat as keys\n        def test_impl():\n            ct_dtype = CategoricalDtype([\'A\', \'B\', \'C\'])\n            dtypes = {\'C1\': np.int, \'C2\': ct_dtype, \'C3\': str}\n            df1 = pd.read_csv(""csv_data_cat1.csv"",\n                              names=[\'C1\', \'C2\', \'C3\'],\n                              dtype=dtypes,\n                              )\n            n = len(df1)\n            df2 = pd.DataFrame({\'C1\': 2 * np.arange(n) + 1, \'AAA\': n + np.arange(n) + 1.0})\n            df3 = df1.merge(df2, on=\'C1\')\n            return df3\n\n        hpat_func = self.jit(distributed=[\'df3\'])(test_impl)\n        # TODO: check results\n        self.assertTrue((hpat_func().columns == test_impl().columns).all())\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
sdc/tests/test_ml.py,29,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numba\nimport numpy as np\nimport pandas as pd\nimport unittest\nfrom math import sqrt\n\nimport sdc\nfrom sdc.tests.test_base import TestCase\nfrom sdc.tests.test_utils import (check_numba_version,\n                                  count_array_OneD_Vars,\n                                  count_array_OneDs,\n                                  count_array_REPs,\n                                  count_parfor_OneD_Vars,\n                                  count_parfor_OneDs,\n                                  count_parfor_REPs,\n                                  dist_IR_contains,\n                                  skip_numba_jit,\n                                  skip_sdc_jit)\n\n\nclass TestML(TestCase):\n\n    @skip_numba_jit\n    @skip_sdc_jit(\'Not implemented in sequential transport layer\')\n    def test_logistic_regression(self):\n        def test_impl(n, d):\n            iterations = 3\n            X = np.ones((n, d)) + .5\n            Y = np.ones(n)\n            D = X.shape[1]\n            w = np.ones(D) - 0.5\n            for i in range(iterations):\n                w -= np.dot(((1.0 / (1.0 + np.exp(-Y * np.dot(X, w))) - 1.0) * Y), X)\n            return w\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        d = 4\n        np.testing.assert_allclose(hpat_func(n, d), test_impl(n, d))\n        self.assertEqual(count_array_OneDs(), 3)\n        self.assertEqual(count_parfor_OneDs(), 3)\n\n    @skip_numba_jit\n    @skip_sdc_jit(\'Not implemented in sequential transport layer\')\n    def test_logistic_regression_acc(self):\n        def test_impl(N, D):\n            iterations = 3\n            g = 2 * np.ones(D) - 1\n            X = 2 * np.ones((N, D)) - 1\n            Y = ((np.dot(X, g) > 0.0) == (np.ones(N) > .90)) + .0\n\n            w = 2 * np.ones(D) - 1\n            for i in range(iterations):\n                w -= np.dot(((1.0 / (1.0 + np.exp(-Y * np.dot(X, w))) - 1.0) * Y), X)\n                R = np.dot(X, w) > 0.0\n                accuracy = np.sum(R == Y) / N\n            return accuracy\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        d = 4\n        np.testing.assert_approx_equal(hpat_func(n, d), test_impl(n, d))\n        self.assertEqual(count_array_OneDs(), 3)\n        self.assertEqual(count_parfor_OneDs(), 4)\n\n    @skip_numba_jit\n    @skip_sdc_jit(\'Not implemented in sequential transport layer\')\n    def test_linear_regression(self):\n        def test_impl(N, D):\n            p = 2\n            iterations = 3\n            X = np.ones((N, D)) + .5\n            Y = np.ones((N, p))\n            alphaN = 0.01 / N\n            w = np.zeros((D, p))\n            for i in range(iterations):\n                w -= alphaN * np.dot(X.T, np.dot(X, w) - Y)\n            return w\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        d = 4\n        np.testing.assert_allclose(hpat_func(n, d), test_impl(n, d))\n        self.assertEqual(count_array_OneDs(), 5)\n        self.assertEqual(count_parfor_OneDs(), 3)\n\n    @skip_numba_jit\n    def test_kde(self):\n        def test_impl(n):\n            X = np.ones(n)\n            b = 0.5\n            points = np.array([-1.0, 2.0, 5.0])\n            N = points.shape[0]\n            exps = 0\n            for i in sdc.prange(n):\n                p = X[i]\n                d = (-(p - points)**2) / (2 * b**2)\n                m = np.min(d)\n                exps += m - np.log(b * N) + np.log(np.sum(np.exp(d - m)))\n            return exps\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        np.testing.assert_approx_equal(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_OneDs(), 1)\n        self.assertEqual(count_parfor_OneDs(), 2)\n\n    @unittest.expectedFailure  # https://github.com/numba/numba/issues/4690\n    def test_kmeans(self):\n        def test_impl(numCenter, numIter, N, D):\n            A = np.ones((N, D))\n            centroids = np.zeros((numCenter, D))\n\n            for l in range(numIter):\n                dist = np.array([[sqrt(np.sum((A[i, :] - centroids[j, :])**2))\n                                  for j in range(numCenter)] for i in range(N)])\n                labels = np.array([dist[i, :].argmin() for i in range(N)])\n\n                centroids = np.array([[np.sum(A[labels == i, j]) / np.sum(labels == i)\n                                       for j in range(D)] for i in range(numCenter)])\n\n            return centroids\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        np.testing.assert_allclose(hpat_func(1, 1, n, 2), test_impl(1, 1, n, 2))\n        self.assertEqual(count_array_OneDs(), 4)\n        self.assertEqual(count_array_OneD_Vars(), 1)\n        self.assertEqual(count_parfor_OneDs(), 5)\n        self.assertEqual(count_parfor_OneD_Vars(), 1)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
sdc/tests/test_prange_utils.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nfrom sdc.tests.test_base import TestCase\n\nfrom sdc.utilities.prange_utils import get_chunks, Chunk\n\n\nclass ChunkTest(TestCase):\n\n    def _get_chunks_data(self):\n        yield (5, 5), [\n            Chunk(start=0, stop=1),\n            Chunk(start=1, stop=2),\n            Chunk(start=2, stop=3),\n            Chunk(start=3, stop=4),\n            Chunk(start=4, stop=5),\n        ]\n        yield (5, 6), [\n            Chunk(start=0, stop=1),\n            Chunk(start=1, stop=2),\n            Chunk(start=2, stop=3),\n            Chunk(start=3, stop=4),\n            Chunk(start=4, stop=5),\n        ]\n        yield (9, 5), [\n            Chunk(start=0, stop=2),\n            Chunk(start=2, stop=4),\n            Chunk(start=4, stop=6),\n            Chunk(start=6, stop=8),\n            Chunk(start=8, stop=9),\n        ]\n        yield (9, 4), [\n            Chunk(start=0, stop=3),\n            Chunk(start=3, stop=5),\n            Chunk(start=5, stop=7),\n            Chunk(start=7, stop=9),\n        ]\n        yield (9, 2), [\n            Chunk(start=0, stop=5),\n            Chunk(start=5, stop=9),\n        ]\n        yield (9, 3), [\n            Chunk(start=0, stop=3),\n            Chunk(start=3, stop=6),\n            Chunk(start=6, stop=9),\n        ]\n        yield (0, 5), []\n        yield (5, 0), []\n\n    def _check_get_chunks(self, args, expected_chunks):\n        pyfunc = get_chunks\n        cfunc = self.jit(pyfunc)\n\n        self.assertEqual(pyfunc(*args), expected_chunks)\n        self.assertEqual(cfunc(*args), expected_chunks)\n\n    def test_get_chunks(self):\n        for args, expected_chunks in self._get_chunks_data():\n            with self.subTest(args=args):\n                self._check_get_chunks(args, expected_chunks)\n'"
sdc/tests/test_rolling.py,113,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport itertools\nimport os\nimport platform\nimport string\nimport unittest\nfrom copy import deepcopy\nfrom itertools import product\n\nimport numpy as np\nimport pandas as pd\n\nfrom numba.core.errors import TypingError\nfrom sdc.hiframes.rolling import supported_rolling_funcs\nfrom sdc.tests.test_base import TestCase\nfrom sdc.tests.test_series import gen_frand_array\nfrom sdc.tests.test_utils import (count_array_REPs, count_parfor_REPs,\n                                  skip_numba_jit, skip_sdc_jit,\n                                  test_global_input_data_float64)\n\n\nLONG_TEST = (int(os.environ[\'SDC_LONG_ROLLING_TEST\']) != 0\n             if \'SDC_LONG_ROLLING_TEST\' in os.environ else False)\n\ntest_funcs = (\'mean\', \'max\',)\nif LONG_TEST:\n    # all functions except apply, cov, corr\n    test_funcs = supported_rolling_funcs[:-3]\n\n\ndef rolling_std_usecase(obj, window, min_periods, ddof):\n    return obj.rolling(window, min_periods).std(ddof)\n\n\ndef rolling_var_usecase(obj, window, min_periods, ddof):\n    return obj.rolling(window, min_periods).var(ddof)\n\n\nclass TestRolling(TestCase):\n\n    @skip_numba_jit\n    def test_series_rolling1(self):\n        def test_impl(S):\n            return S.rolling(3).sum()\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([1.0, 2., 3., 4., 5.])\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    @skip_numba_jit\n    def test_fixed1(self):\n        # test sequentially with manually created dfs\n        wins = (3,)\n        if LONG_TEST:\n            wins = (2, 3, 5)\n        centers = (False, True)\n\n        for func_name in test_funcs:\n            func_text = ""def test_impl(df, w, c):\\n  return df.rolling(w, center=c).{}()\\n"".format(func_name)\n            loc_vars = {}\n            exec(func_text, {}, loc_vars)\n            test_impl = loc_vars[\'test_impl\']\n            hpat_func = self.jit(test_impl)\n\n            for args in itertools.product(wins, centers):\n                df = pd.DataFrame({\'B\': [0, 1, 2, np.nan, 4]})\n                pd.testing.assert_frame_equal(hpat_func(df, *args), test_impl(df, *args))\n                df = pd.DataFrame({\'B\': [0, 1, 2, -2, 4]})\n                pd.testing.assert_frame_equal(hpat_func(df, *args), test_impl(df, *args))\n\n    @skip_numba_jit\n    def test_fixed2(self):\n        # test sequentially with generated dfs\n        sizes = (121,)\n        wins = (3,)\n        if LONG_TEST:\n            sizes = (1, 2, 10, 11, 121, 1000)\n            wins = (2, 3, 5)\n        centers = (False, True)\n        for func_name in test_funcs:\n            func_text = ""def test_impl(df, w, c):\\n  return df.rolling(w, center=c).{}()\\n"".format(func_name)\n            loc_vars = {}\n            exec(func_text, {}, loc_vars)\n            test_impl = loc_vars[\'test_impl\']\n            hpat_func = self.jit(test_impl)\n            for n, w, c in itertools.product(sizes, wins, centers):\n                df = pd.DataFrame({\'B\': np.arange(n)})\n                pd.testing.assert_frame_equal(hpat_func(df, w, c), test_impl(df, w, c))\n\n    @skip_numba_jit\n    def test_fixed_apply1(self):\n        # test sequentially with manually created dfs\n        def test_impl(df, w, c):\n            return df.rolling(w, center=c).apply(lambda a: a.sum())\n        hpat_func = self.jit(test_impl)\n        wins = (3,)\n        if LONG_TEST:\n            wins = (2, 3, 5)\n        centers = (False, True)\n        for args in itertools.product(wins, centers):\n            df = pd.DataFrame({\'B\': [0, 1, 2, np.nan, 4]})\n            pd.testing.assert_frame_equal(hpat_func(df, *args), test_impl(df, *args))\n            df = pd.DataFrame({\'B\': [0, 1, 2, -2, 4]})\n            pd.testing.assert_frame_equal(hpat_func(df, *args), test_impl(df, *args))\n\n    @skip_numba_jit\n    def test_fixed_apply2(self):\n        # test sequentially with generated dfs\n        def test_impl(df, w, c):\n            return df.rolling(w, center=c).apply(lambda a: a.sum())\n        hpat_func = self.jit(test_impl)\n        sizes = (121,)\n        wins = (3,)\n        if LONG_TEST:\n            sizes = (1, 2, 10, 11, 121, 1000)\n            wins = (2, 3, 5)\n        centers = (False, True)\n        for n, w, c in itertools.product(sizes, wins, centers):\n            df = pd.DataFrame({\'B\': np.arange(n)})\n            pd.testing.assert_frame_equal(hpat_func(df, w, c), test_impl(df, w, c))\n\n    @skip_numba_jit\n    def test_fixed_parallel1(self):\n        def test_impl(n, w, center):\n            df = pd.DataFrame({\'B\': np.arange(n)})\n            R = df.rolling(w, center=center).sum()\n            return R.B.sum()\n\n        hpat_func = self.jit(test_impl)\n        sizes = (121,)\n        wins = (5,)\n        if LONG_TEST:\n            sizes = (1, 2, 10, 11, 121, 1000)\n            wins = (2, 4, 5, 10, 11)\n        centers = (False, True)\n        for args in itertools.product(sizes, wins, centers):\n            self.assertEqual(hpat_func(*args), test_impl(*args),\n                             ""rolling fixed window with {}"".format(args))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit\n    def test_fixed_parallel_apply1(self):\n        def test_impl(n, w, center):\n            df = pd.DataFrame({\'B\': np.arange(n)})\n            R = df.rolling(w, center=center).apply(lambda a: a.sum())\n            return R.B.sum()\n\n        hpat_func = self.jit(test_impl)\n        sizes = (121,)\n        wins = (5,)\n        if LONG_TEST:\n            sizes = (1, 2, 10, 11, 121, 1000)\n            wins = (2, 4, 5, 10, 11)\n        centers = (False, True)\n        for args in itertools.product(sizes, wins, centers):\n            self.assertEqual(hpat_func(*args), test_impl(*args),\n                             ""rolling fixed window with {}"".format(args))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit\n    def test_variable1(self):\n        # test sequentially with manually created dfs\n        df1 = pd.DataFrame({\'B\': [0, 1, 2, np.nan, 4],\n                            \'time\': [pd.Timestamp(\'20130101 09:00:00\'),\n                                     pd.Timestamp(\'20130101 09:00:02\'),\n                                     pd.Timestamp(\'20130101 09:00:03\'),\n                                     pd.Timestamp(\'20130101 09:00:05\'),\n                                     pd.Timestamp(\'20130101 09:00:06\')]})\n        df2 = pd.DataFrame({\'B\': [0, 1, 2, -2, 4],\n                            \'time\': [pd.Timestamp(\'20130101 09:00:01\'),\n                                     pd.Timestamp(\'20130101 09:00:02\'),\n                                     pd.Timestamp(\'20130101 09:00:03\'),\n                                     pd.Timestamp(\'20130101 09:00:04\'),\n                                     pd.Timestamp(\'20130101 09:00:09\')]})\n        wins = (\'2s\',)\n        if LONG_TEST:\n            wins = (\'1s\', \'2s\', \'3s\', \'4s\')\n        # all functions except apply\n        for w, func_name in itertools.product(wins, test_funcs):\n            func_text = ""def test_impl(df):\\n  return df.rolling(\'{}\', on=\'time\').{}()\\n"".format(w, func_name)\n            loc_vars = {}\n            exec(func_text, {}, loc_vars)\n            test_impl = loc_vars[\'test_impl\']\n            hpat_func = self.jit(test_impl)\n            # XXX: skipping min/max for this test since the behavior of Pandas\n            # is inconsistent: it assigns NaN to last output instead of 4!\n            if func_name not in (\'min\', \'max\'):\n                pd.testing.assert_frame_equal(hpat_func(df1), test_impl(df1))\n            pd.testing.assert_frame_equal(hpat_func(df2), test_impl(df2))\n\n    @skip_numba_jit\n    def test_variable2(self):\n        # test sequentially with generated dfs\n        wins = (\'2s\',)\n        sizes = (121,)\n        if LONG_TEST:\n            wins = (\'1s\', \'2s\', \'3s\', \'4s\')\n            sizes = (1, 2, 10, 11, 121, 1000)\n        # all functions except apply\n        for w, func_name in itertools.product(wins, test_funcs):\n            func_text = ""def test_impl(df):\\n  return df.rolling(\'{}\', on=\'time\').{}()\\n"".format(w, func_name)\n            loc_vars = {}\n            exec(func_text, {}, loc_vars)\n            test_impl = loc_vars[\'test_impl\']\n            hpat_func = self.jit(test_impl)\n            for n in sizes:\n                time = pd.date_range(start=\'1/1/2018\', periods=n, freq=\'s\')\n                df = pd.DataFrame({\'B\': np.arange(n), \'time\': time})\n                pd.testing.assert_frame_equal(hpat_func(df), test_impl(df))\n\n    @skip_numba_jit\n    def test_variable_apply1(self):\n        # test sequentially with manually created dfs\n        df1 = pd.DataFrame({\'B\': [0, 1, 2, np.nan, 4],\n                            \'time\': [pd.Timestamp(\'20130101 09:00:00\'),\n                                     pd.Timestamp(\'20130101 09:00:02\'),\n                                     pd.Timestamp(\'20130101 09:00:03\'),\n                                     pd.Timestamp(\'20130101 09:00:05\'),\n                                     pd.Timestamp(\'20130101 09:00:06\')]})\n        df2 = pd.DataFrame({\'B\': [0, 1, 2, -2, 4],\n                            \'time\': [pd.Timestamp(\'20130101 09:00:01\'),\n                                     pd.Timestamp(\'20130101 09:00:02\'),\n                                     pd.Timestamp(\'20130101 09:00:03\'),\n                                     pd.Timestamp(\'20130101 09:00:04\'),\n                                     pd.Timestamp(\'20130101 09:00:09\')]})\n        wins = (\'2s\',)\n        if LONG_TEST:\n            wins = (\'1s\', \'2s\', \'3s\', \'4s\')\n        # all functions except apply\n        for w in wins:\n            func_text = ""def test_impl(df):\\n  return df.rolling(\'{}\', on=\'time\').apply(lambda a: a.sum())\\n"".format(w)\n            loc_vars = {}\n            exec(func_text, {}, loc_vars)\n            test_impl = loc_vars[\'test_impl\']\n            hpat_func = self.jit(test_impl)\n            pd.testing.assert_frame_equal(hpat_func(df1), test_impl(df1))\n            pd.testing.assert_frame_equal(hpat_func(df2), test_impl(df2))\n\n    @skip_numba_jit\n    def test_variable_apply2(self):\n        # test sequentially with generated dfs\n        wins = (\'2s\',)\n        sizes = (121,)\n        if LONG_TEST:\n            wins = (\'1s\', \'2s\', \'3s\', \'4s\')\n            # TODO: this crashes on Travis (3 process config) with size 1\n            sizes = (2, 10, 11, 121, 1000)\n        # all functions except apply\n        for w in wins:\n            func_text = ""def test_impl(df):\\n  return df.rolling(\'{}\', on=\'time\').apply(lambda a: a.sum())\\n"".format(w)\n            loc_vars = {}\n            exec(func_text, {}, loc_vars)\n            test_impl = loc_vars[\'test_impl\']\n            hpat_func = self.jit(test_impl)\n            for n in sizes:\n                time = pd.date_range(start=\'1/1/2018\', periods=n, freq=\'s\')\n                df = pd.DataFrame({\'B\': np.arange(n), \'time\': time})\n                pd.testing.assert_frame_equal(hpat_func(df), test_impl(df))\n\n    @skip_numba_jit\n    @unittest.skipIf(platform.system() == \'Windows\', ""ValueError: time must be monotonic"")\n    def test_variable_parallel1(self):\n        wins = (\'2s\',)\n        sizes = (121,)\n        if LONG_TEST:\n            wins = (\'1s\', \'2s\', \'3s\', \'4s\')\n            # XXX: Pandas returns time = [np.nan] for size==1 for some reason\n            sizes = (2, 10, 11, 121, 1000)\n        # all functions except apply\n        for w, func_name in itertools.product(wins, test_funcs):\n            func_text = ""def test_impl(n):\\n""\n            func_text += ""  df = pd.DataFrame({\'B\': np.arange(n), \'time\': ""\n            func_text += ""    pd.DatetimeIndex(np.arange(n) * 1000000000)})\\n""\n            func_text += ""  res = df.rolling(\'{}\', on=\'time\').{}()\\n"".format(w, func_name)\n            func_text += ""  return res.B.sum()\\n""\n            loc_vars = {}\n            exec(func_text, {\'pd\': pd, \'np\': np}, loc_vars)\n            test_impl = loc_vars[\'test_impl\']\n            hpat_func = self.jit(test_impl)\n            for n in sizes:\n                np.testing.assert_almost_equal(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit\n    @unittest.skipIf(platform.system() == \'Windows\', ""ValueError: time must be monotonic"")\n    def test_variable_apply_parallel1(self):\n        wins = (\'2s\',)\n        sizes = (121,)\n        if LONG_TEST:\n            wins = (\'1s\', \'2s\', \'3s\', \'4s\')\n            # XXX: Pandas returns time = [np.nan] for size==1 for some reason\n            sizes = (2, 10, 11, 121, 1000)\n        # all functions except apply\n        for w in wins:\n            func_text = ""def test_impl(n):\\n""\n            func_text += ""  df = pd.DataFrame({\'B\': np.arange(n), \'time\': ""\n            func_text += ""    pd.DatetimeIndex(np.arange(n) * 1000000000)})\\n""\n            func_text += ""  res = df.rolling(\'{}\', on=\'time\').apply(lambda a: a.sum())\\n"".format(w)\n            func_text += ""  return res.B.sum()\\n""\n            loc_vars = {}\n            exec(func_text, {\'pd\': pd, \'np\': np}, loc_vars)\n            test_impl = loc_vars[\'test_impl\']\n            hpat_func = self.jit(test_impl)\n            for n in sizes:\n                np.testing.assert_almost_equal(hpat_func(n), test_impl(n))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_numba_jit\n    def test_series_fixed1(self):\n        # test series rolling functions\n        # all functions except apply\n        S1 = pd.Series([0, 1, 2, np.nan, 4])\n        S2 = pd.Series([0, 1, 2, -2, 4])\n        wins = (3,)\n        if LONG_TEST:\n            wins = (2, 3, 5)\n        centers = (False, True)\n        for func_name in test_funcs:\n            func_text = ""def test_impl(S, w, c):\\n  return S.rolling(w, center=c).{}()\\n"".format(func_name)\n            loc_vars = {}\n            exec(func_text, {}, loc_vars)\n            test_impl = loc_vars[\'test_impl\']\n            hpat_func = self.jit(test_impl)\n            for args in itertools.product(wins, centers):\n                pd.testing.assert_series_equal(hpat_func(S1, *args), test_impl(S1, *args))\n                pd.testing.assert_series_equal(hpat_func(S2, *args), test_impl(S2, *args))\n        # test apply\n\n        def apply_test_impl(S, w, c):\n            return S.rolling(w, center=c).apply(lambda a: a.sum())\n        hpat_func = self.jit(apply_test_impl)\n        for args in itertools.product(wins, centers):\n            pd.testing.assert_series_equal(hpat_func(S1, *args), apply_test_impl(S1, *args))\n            pd.testing.assert_series_equal(hpat_func(S2, *args), apply_test_impl(S2, *args))\n\n    @skip_numba_jit\n    def test_series_cov1(self):\n        # test series rolling functions\n        # all functions except apply\n        S1 = pd.Series([0, 1, 2, np.nan, 4])\n        S2 = pd.Series([0, 1, 2, -2, 4])\n        wins = (3,)\n        if LONG_TEST:\n            wins = (2, 3, 5)\n        centers = (False, True)\n\n        def test_impl(S, S2, w, c):\n            return S.rolling(w, center=c).cov(S2)\n        hpat_func = self.jit(test_impl)\n        for args in itertools.product([S1, S2], [S1, S2], wins, centers):\n            pd.testing.assert_series_equal(hpat_func(*args), test_impl(*args))\n            pd.testing.assert_series_equal(hpat_func(*args), test_impl(*args))\n\n        def test_impl2(S, S2, w, c):\n            return S.rolling(w, center=c).corr(S2)\n        hpat_func = self.jit(test_impl2)\n        for args in itertools.product([S1, S2], [S1, S2], wins, centers):\n            pd.testing.assert_series_equal(hpat_func(*args), test_impl2(*args))\n            pd.testing.assert_series_equal(hpat_func(*args), test_impl2(*args))\n\n    @skip_numba_jit\n    def test_df_cov1(self):\n        # test series rolling functions\n        # all functions except apply\n        df1 = pd.DataFrame({\'A\': [0, 1, 2, np.nan, 4], \'B\': np.ones(5)})\n        df2 = pd.DataFrame({\'A\': [0, 1, 2, -2, 4], \'C\': np.ones(5)})\n        wins = (3,)\n        if LONG_TEST:\n            wins = (2, 3, 5)\n        centers = (False, True)\n\n        def test_impl(df, df2, w, c):\n            return df.rolling(w, center=c).cov(df2)\n        hpat_func = self.jit(test_impl)\n        for args in itertools.product([df1, df2], [df1, df2], wins, centers):\n            pd.testing.assert_frame_equal(hpat_func(*args), test_impl(*args))\n            pd.testing.assert_frame_equal(hpat_func(*args), test_impl(*args))\n\n        def test_impl2(df, df2, w, c):\n            return df.rolling(w, center=c).corr(df2)\n        hpat_func = self.jit(test_impl2)\n        for args in itertools.product([df1, df2], [df1, df2], wins, centers):\n            pd.testing.assert_frame_equal(hpat_func(*args), test_impl2(*args))\n            pd.testing.assert_frame_equal(hpat_func(*args), test_impl2(*args))\n\n    def _get_assert_equal(self, obj):\n        if isinstance(obj, pd.Series):\n            return pd.testing.assert_series_equal\n        elif isinstance(obj, pd.DataFrame):\n            return pd.testing.assert_frame_equal\n        elif isinstance(obj, np.ndarray):\n            return np.testing.assert_array_equal\n\n        return self.assertEqual\n\n    def _test_rolling_unsupported_values(self, obj):\n        def test_impl(obj, window, min_periods, center,\n                      win_type, on, axis, closed):\n            return obj.rolling(window, min_periods, center,\n                               win_type, on, axis, closed).min()\n\n        hpat_func = self.jit(test_impl)\n\n        with self.assertRaises(ValueError) as raises:\n            hpat_func(obj, -1, None, False, None, None, 0, None)\n        self.assertIn(\'window must be non-negative\', str(raises.exception))\n\n        with self.assertRaises(ValueError) as raises:\n            hpat_func(obj, 1, -1, False, None, None, 0, None)\n        self.assertIn(\'min_periods must be >= 0\', str(raises.exception))\n\n        with self.assertRaises(ValueError) as raises:\n            hpat_func(obj, 1, 2, False, None, None, 0, None)\n        self.assertIn(\'min_periods must be <= window\', str(raises.exception))\n\n        with self.assertRaises(ValueError) as raises:\n            hpat_func(obj, 1, 2, False, None, None, 0, None)\n        self.assertIn(\'min_periods must be <= window\', str(raises.exception))\n\n        msg_tmpl = \'Method rolling(). The object {}\\n expected: {}\'\n\n        with self.assertRaises(ValueError) as raises:\n            hpat_func(obj, 1, None, True, None, None, 0, None)\n        msg = msg_tmpl.format(\'center\', \'False\')\n        self.assertIn(msg, str(raises.exception))\n\n        with self.assertRaises(ValueError) as raises:\n            hpat_func(obj, 1, None, False, \'None\', None, 0, None)\n        msg = msg_tmpl.format(\'win_type\', \'None\')\n        self.assertIn(msg, str(raises.exception))\n\n        with self.assertRaises(ValueError) as raises:\n            hpat_func(obj, 1, None, False, None, \'None\', 0, None)\n        msg = msg_tmpl.format(\'on\', \'None\')\n        self.assertIn(msg, str(raises.exception))\n\n        with self.assertRaises(ValueError) as raises:\n            hpat_func(obj, 1, None, False, None, None, 1, None)\n        msg = msg_tmpl.format(\'axis\', \'0\')\n        self.assertIn(msg, str(raises.exception))\n\n        with self.assertRaises(ValueError) as raises:\n            hpat_func(obj, 1, None, False, None, None, 0, \'None\')\n        msg = msg_tmpl.format(\'closed\', \'None\')\n        self.assertIn(msg, str(raises.exception))\n\n    def _test_rolling_unsupported_types(self, obj):\n        def test_impl(obj, window, min_periods, center,\n                      win_type, on, axis, closed):\n            return obj.rolling(window, min_periods, center,\n                               win_type, on, axis, closed).min()\n\n        hpat_func = self.jit(test_impl)\n        msg_tmpl = \'Method rolling(). The object {}\\n given: {}\\n expected: {}\'\n\n        with self.assertRaises(TypingError) as raises:\n            hpat_func(obj, \'1\', None, False, None, None, 0, None)\n        msg = msg_tmpl.format(\'window\', \'unicode_type\', \'int\')\n        self.assertIn(msg, str(raises.exception))\n\n        with self.assertRaises(TypingError) as raises:\n            hpat_func(obj, 1, \'1\', False, None, None, 0, None)\n        msg = msg_tmpl.format(\'min_periods\', \'unicode_type\', \'None, int\')\n        self.assertIn(msg, str(raises.exception))\n\n        with self.assertRaises(TypingError) as raises:\n            hpat_func(obj, 1, None, 0, None, None, 0, None)\n        msg = msg_tmpl.format(\'center\', \'int64\', \'bool\')\n        self.assertIn(msg, str(raises.exception))\n\n        with self.assertRaises(TypingError) as raises:\n            hpat_func(obj, 1, None, False, -1, None, 0, None)\n        msg = msg_tmpl.format(\'win_type\', \'int64\', \'str\')\n        self.assertIn(msg, str(raises.exception))\n\n        with self.assertRaises(TypingError) as raises:\n            hpat_func(obj, 1, None, False, None, -1, 0, None)\n        msg = msg_tmpl.format(\'on\', \'int64\', \'str\')\n        self.assertIn(msg, str(raises.exception))\n\n        with self.assertRaises(TypingError) as raises:\n            hpat_func(obj, 1, None, False, None, None, None, None)\n        msg = msg_tmpl.format(\'axis\', \'none\', \'int, str\')\n        self.assertIn(msg, str(raises.exception))\n\n        with self.assertRaises(TypingError) as raises:\n            hpat_func(obj, 1, None, False, None, None, 0, -1)\n        msg = msg_tmpl.format(\'closed\', \'int64\', \'str\')\n        self.assertIn(msg, str(raises.exception))\n\n    def _test_rolling_apply_mean(self, obj):\n        def test_impl(obj, window, min_periods):\n            def func(x):\n                if len(x) == 0:\n                    return np.nan\n                return x.mean()\n\n            return obj.rolling(window, min_periods).apply(func)\n\n        hpat_func = self.jit(test_impl)\n        assert_equal = self._get_assert_equal(obj)\n\n        for window in range(0, len(obj) + 3, 2):\n            for min_periods in range(0, window + 1, 2):\n                with self.subTest(obj=obj, window=window,\n                                  min_periods=min_periods):\n                    jit_result = hpat_func(obj, window, min_periods)\n                    ref_result = test_impl(obj, window, min_periods)\n                    assert_equal(jit_result, ref_result)\n\n    def _test_rolling_apply_unsupported_types(self, obj):\n        def test_impl(obj, raw):\n            def func(x):\n                if len(x) == 0:\n                    return np.nan\n                return np.median(x)\n\n            return obj.rolling(3).apply(func, raw=raw)\n\n        hpat_func = self.jit(test_impl)\n\n        with self.assertRaises(TypingError) as raises:\n            hpat_func(obj, 1)\n        msg = \'Method rolling.apply(). The object raw\\n given: int64\\n expected: bool\'\n        self.assertIn(msg, str(raises.exception))\n\n    def _test_rolling_apply_args(self, obj):\n        def test_impl(obj, window, min_periods, q):\n            def func(x, q):\n                if len(x) == 0:\n                    return np.nan\n                return np.quantile(x, q)\n\n            return obj.rolling(window, min_periods).apply(func, raw=None, args=(q,))\n\n        hpat_func = self.jit(test_impl)\n        assert_equal = self._get_assert_equal(obj)\n\n        for window in range(0, len(obj) + 3, 2):\n            for min_periods in range(0, window + 1, 2):\n                for q in [0.25, 0.5, 0.75]:\n                    with self.subTest(obj=obj, window=window,\n                                      min_periods=min_periods, q=q):\n                        jit_result = hpat_func(obj, window, min_periods, q)\n                        ref_result = test_impl(obj, window, min_periods, q)\n                        assert_equal(jit_result, ref_result)\n\n    def _test_rolling_corr(self, obj, other):\n        def test_impl(obj, window, min_periods, other):\n            return obj.rolling(window, min_periods).corr(other)\n\n        hpat_func = self.jit(test_impl)\n        assert_equal = self._get_assert_equal(obj)\n\n        for window in range(0, len(obj) + 3, 2):\n            for min_periods in range(0, window, 2):\n                with self.subTest(obj=obj, other=other,\n                                  window=window, min_periods=min_periods):\n                    jit_result = hpat_func(obj, window, min_periods, other)\n                    ref_result = test_impl(obj, window, min_periods, other)\n                    assert_equal(jit_result, ref_result)\n\n    def _test_rolling_corr_with_no_other(self, obj):\n        def test_impl(obj, window, min_periods):\n            return obj.rolling(window, min_periods).corr(pairwise=False)\n\n        hpat_func = self.jit(test_impl)\n        assert_equal = self._get_assert_equal(obj)\n\n        for window in range(0, len(obj) + 3, 2):\n            for min_periods in range(0, window, 2):\n                with self.subTest(obj=obj, window=window,\n                                  min_periods=min_periods):\n                    jit_result = hpat_func(obj, window, min_periods)\n                    ref_result = test_impl(obj, window, min_periods)\n                    assert_equal(jit_result, ref_result)\n\n    def _test_rolling_corr_unsupported_types(self, obj):\n        def test_impl(obj, pairwise):\n            return obj.rolling(3, 3).corr(pairwise=pairwise)\n\n        hpat_func = self.jit(test_impl)\n\n        with self.assertRaises(TypingError) as raises:\n            hpat_func(obj, 1)\n        msg = \'Method rolling.corr(). The object pairwise\\n given: int64\\n expected: bool\'\n        self.assertIn(msg, str(raises.exception))\n\n    def _test_rolling_count(self, obj):\n        def test_impl(obj, window, min_periods):\n            return obj.rolling(window, min_periods).count()\n\n        hpat_func = self.jit(test_impl)\n        assert_equal = self._get_assert_equal(obj)\n\n        for window in range(0, len(obj) + 3, 2):\n            for min_periods in range(0, window + 1, 2):\n                with self.subTest(obj=obj, window=window,\n                                  min_periods=min_periods):\n                    jit_result = hpat_func(obj, window, min_periods)\n                    ref_result = test_impl(obj, window, min_periods)\n                    assert_equal(jit_result, ref_result)\n\n    def _test_rolling_cov(self, obj, other):\n        def test_impl(obj, window, min_periods, other, ddof):\n            return obj.rolling(window, min_periods).cov(other, ddof=ddof)\n\n        hpat_func = self.jit(test_impl)\n        assert_equal = self._get_assert_equal(obj)\n\n        for window in range(0, len(obj) + 3, 2):\n            for min_periods, ddof in product(range(0, window, 2), [0, 1]):\n                with self.subTest(obj=obj, other=other, window=window,\n                                  min_periods=min_periods, ddof=ddof):\n                    jit_result = hpat_func(obj, window, min_periods, other, ddof)\n                    ref_result = test_impl(obj, window, min_periods, other, ddof)\n                    assert_equal(jit_result, ref_result)\n\n    def _test_rolling_cov_with_no_other(self, obj):\n        def test_impl(obj, window, min_periods):\n            return obj.rolling(window, min_periods).cov(pairwise=False)\n\n        hpat_func = self.jit(test_impl)\n        assert_equal = self._get_assert_equal(obj)\n\n        for window in range(0, len(obj) + 3, 2):\n            for min_periods in range(0, window, 2):\n                with self.subTest(obj=obj, window=window,\n                                  min_periods=min_periods):\n                    jit_result = hpat_func(obj, window, min_periods)\n                    ref_result = test_impl(obj, window, min_periods)\n                    assert_equal(jit_result, ref_result)\n\n    def _test_rolling_cov_unsupported_types(self, obj):\n        def test_impl(obj, pairwise, ddof):\n            return obj.rolling(3, 3).cov(pairwise=pairwise, ddof=ddof)\n\n        hpat_func = self.jit(test_impl)\n\n        msg_tmpl = \'Method rolling.cov(). The object {}\\n given: {}\\n expected: {}\'\n\n        with self.assertRaises(TypingError) as raises:\n            hpat_func(obj, 1, 1)\n        msg = msg_tmpl.format(\'pairwise\', \'int64\', \'bool\')\n        self.assertIn(msg, str(raises.exception))\n\n        with self.assertRaises(TypingError) as raises:\n            hpat_func(obj, None, \'1\')\n        msg = msg_tmpl.format(\'ddof\', \'unicode_type\', \'int\')\n        self.assertIn(msg, str(raises.exception))\n\n    def _test_rolling_kurt(self, obj):\n        def test_impl(obj, window, min_periods):\n            return obj.rolling(window, min_periods).kurt()\n\n        hpat_func = self.jit(test_impl)\n        assert_equal = self._get_assert_equal(obj)\n\n        for window in range(4, len(obj) + 1):\n            for min_periods in range(window + 1):\n                with self.subTest(obj=obj, window=window,\n                                  min_periods=min_periods):\n                    ref_result = test_impl(obj, window, min_periods)\n                    jit_result = hpat_func(obj, window, min_periods)\n                    assert_equal(jit_result, ref_result)\n\n    def _test_rolling_max(self, obj):\n        def test_impl(obj, window, min_periods):\n            return obj.rolling(window, min_periods).max()\n\n        hpat_func = self.jit(test_impl)\n        assert_equal = self._get_assert_equal(obj)\n\n        # python implementation crashes if window = 0, jit works correctly\n        for window in range(1, len(obj) + 2):\n            for min_periods in range(window + 1):\n                with self.subTest(obj=obj, window=window,\n                                  min_periods=min_periods):\n                    jit_result = hpat_func(obj, window, min_periods)\n                    ref_result = test_impl(obj, window, min_periods)\n                    assert_equal(jit_result, ref_result)\n\n    def _test_rolling_mean(self, obj):\n        def test_impl(obj, window, min_periods):\n            return obj.rolling(window, min_periods).mean()\n\n        hpat_func = self.jit(test_impl)\n        assert_equal = self._get_assert_equal(obj)\n\n        for window in range(len(obj) + 2):\n            for min_periods in range(window):\n                with self.subTest(obj=obj, window=window,\n                                  min_periods=min_periods):\n                    jit_result = hpat_func(obj, window, min_periods)\n                    ref_result = test_impl(obj, window, min_periods)\n                    assert_equal(jit_result, ref_result)\n\n    def _test_rolling_median(self, obj):\n        def test_impl(obj, window, min_periods):\n            return obj.rolling(window, min_periods).median()\n\n        hpat_func = self.jit(test_impl)\n        assert_equal = self._get_assert_equal(obj)\n\n        for window in range(0, len(obj) + 3, 2):\n            for min_periods in range(0, window + 1, 2):\n                with self.subTest(obj=obj, window=window,\n                                  min_periods=min_periods):\n                    jit_result = hpat_func(obj, window, min_periods)\n                    ref_result = test_impl(obj, window, min_periods)\n                    assert_equal(jit_result, ref_result)\n\n    def _test_rolling_min(self, obj):\n        def test_impl(obj, window, min_periods):\n            return obj.rolling(window, min_periods).min()\n\n        hpat_func = self.jit(test_impl)\n        assert_equal = self._get_assert_equal(obj)\n\n        # python implementation crashes if window = 0, jit works correctly\n        for window in range(1, len(obj) + 2):\n            for min_periods in range(window + 1):\n                with self.subTest(obj=obj, window=window,\n                                  min_periods=min_periods):\n                    jit_result = hpat_func(obj, window, min_periods)\n                    ref_result = test_impl(obj, window, min_periods)\n                    assert_equal(jit_result, ref_result)\n\n    def _test_rolling_quantile(self, obj):\n        def test_impl(obj, window, min_periods, quantile):\n            return obj.rolling(window, min_periods).quantile(quantile)\n\n        hpat_func = self.jit(test_impl)\n        assert_equal = self._get_assert_equal(obj)\n        quantiles = [0, 0.25, 0.5, 0.75, 1]\n\n        for window in range(0, len(obj) + 3, 2):\n            for min_periods, q in product(range(0, window, 2), quantiles):\n                with self.subTest(obj=obj, window=window,\n                                  min_periods=min_periods, quantiles=q):\n                    jit_result = hpat_func(obj, window, min_periods, q)\n                    ref_result = test_impl(obj, window, min_periods, q)\n                    assert_equal(jit_result, ref_result)\n\n    def _test_rolling_quantile_exception_unsupported_types(self, obj):\n        def test_impl(obj, quantile, interpolation):\n            return obj.rolling(3, 2).quantile(quantile, interpolation)\n\n        hpat_func = self.jit(test_impl)\n\n        msg_tmpl = \'Method rolling.quantile(). The object {}\\n given: {}\\n expected: {}\'\n\n        with self.assertRaises(TypingError) as raises:\n            hpat_func(obj, \'0.5\', \'linear\')\n        msg = msg_tmpl.format(\'quantile\', \'unicode_type\', \'float\')\n        self.assertIn(msg, str(raises.exception))\n\n        with self.assertRaises(TypingError) as raises:\n            hpat_func(obj, 0.5, None)\n        msg = msg_tmpl.format(\'interpolation\', \'none\', \'str\')\n        self.assertIn(msg, str(raises.exception))\n\n    def _test_rolling_quantile_exception_unsupported_values(self, obj):\n        def test_impl(obj, quantile, interpolation):\n            return obj.rolling(3, 2).quantile(quantile, interpolation)\n\n        hpat_func = self.jit(test_impl)\n\n        with self.assertRaises(ValueError) as raises:\n            hpat_func(obj, 2, \'linear\')\n        self.assertIn(\'quantile value not in [0, 1]\', str(raises.exception))\n\n        with self.assertRaises(ValueError) as raises:\n            hpat_func(obj, 0.5, \'lower\')\n        self.assertIn(\'interpolation value not ""linear""\', str(raises.exception))\n\n    def _test_rolling_skew(self, obj):\n        def test_impl(obj, window, min_periods):\n            return obj.rolling(window, min_periods).skew()\n\n        hpat_func = self.jit(test_impl)\n        assert_equal = self._get_assert_equal(obj)\n\n        for window in range(3, len(obj) + 1):\n            for min_periods in range(window + 1):\n                with self.subTest(obj=obj, window=window,\n                                  min_periods=min_periods):\n                    ref_result = test_impl(obj, window, min_periods)\n                    jit_result = hpat_func(obj, window, min_periods)\n                    assert_equal(jit_result, ref_result)\n\n    def _test_rolling_std(self, obj):\n        test_impl = rolling_std_usecase\n        hpat_func = self.jit(test_impl)\n        assert_equal = self._get_assert_equal(obj)\n\n        for window in range(0, len(obj) + 3, 2):\n            for min_periods, ddof in product(range(0, window, 2), [0, 1]):\n                with self.subTest(obj=obj, window=window,\n                                  min_periods=min_periods, ddof=ddof):\n                    jit_result = hpat_func(obj, window, min_periods, ddof)\n                    ref_result = test_impl(obj, window, min_periods, ddof)\n                    assert_equal(jit_result, ref_result)\n\n    def _test_rolling_std_exception_unsupported_ddof(self, obj):\n        test_impl = rolling_std_usecase\n        hpat_func = self.jit(test_impl)\n\n        window, min_periods, invalid_ddof = 3, 2, \'1\'\n        with self.assertRaises(TypingError) as raises:\n            hpat_func(obj, window, min_periods, invalid_ddof)\n        msg = \'Method rolling.std(). The object ddof\\n given: unicode_type\\n expected: int\'\n        self.assertIn(msg, str(raises.exception))\n\n    def _test_rolling_sum(self, obj):\n        def test_impl(obj, window, min_periods):\n            return obj.rolling(window, min_periods).sum()\n\n        hpat_func = self.jit(test_impl)\n        assert_equal = self._get_assert_equal(obj)\n\n        for window in range(len(obj) + 2):\n            for min_periods in range(window):\n                with self.subTest(obj=obj, window=window,\n                                  min_periods=min_periods):\n                    jit_result = hpat_func(obj, window, min_periods)\n                    ref_result = test_impl(obj, window, min_periods)\n                    assert_equal(jit_result, ref_result)\n\n    def _test_rolling_var(self, obj):\n        test_impl = rolling_var_usecase\n        hpat_func = self.jit(test_impl)\n        assert_equal = self._get_assert_equal(obj)\n\n        for window in range(0, len(obj) + 3, 2):\n            for min_periods, ddof in product(range(0, window, 2), [0, 1]):\n                with self.subTest(obj=obj, window=window,\n                                  min_periods=min_periods, ddof=ddof):\n                    jit_result = hpat_func(obj, window, min_periods, ddof)\n                    ref_result = test_impl(obj, window, min_periods, ddof)\n                    assert_equal(jit_result, ref_result)\n\n    def _test_rolling_var_exception_unsupported_ddof(self, obj):\n        test_impl = rolling_var_usecase\n        hpat_func = self.jit(test_impl)\n\n        window, min_periods, invalid_ddof = 3, 2, \'1\'\n        with self.assertRaises(TypingError) as raises:\n            hpat_func(obj, window, min_periods, invalid_ddof)\n        msg = \'Method rolling.var(). The object ddof\\n given: unicode_type\\n expected: int\'\n        self.assertIn(msg, str(raises.exception))\n\n    @skip_sdc_jit(\'DataFrame.rolling.min() unsupported exceptions\')\n    def test_df_rolling_unsupported_values(self):\n        all_data = test_global_input_data_float64\n        length = min(len(d) for d in all_data)\n        data = {n: d[:length] for n, d in zip(string.ascii_uppercase, all_data)}\n        df = pd.DataFrame(data)\n\n        self._test_rolling_unsupported_values(df)\n\n    @skip_sdc_jit(\'DataFrame.rolling.min() unsupported exceptions\')\n    def test_df_rolling_unsupported_types(self):\n        all_data = test_global_input_data_float64\n        length = min(len(d) for d in all_data)\n        data = {n: d[:length] for n, d in zip(string.ascii_uppercase, all_data)}\n        df = pd.DataFrame(data)\n\n        self._test_rolling_unsupported_types(df)\n\n    @skip_sdc_jit(\'DataFrame.rolling.apply() unsupported\')\n    def test_df_rolling_apply_mean(self):\n        all_data = [\n            list(range(10)), [1., -1., 0., 0.1, -0.1],\n            [1., np.inf, np.inf, -1., 0., np.inf, np.NINF, np.NINF],\n            [np.nan, np.inf, np.inf, np.nan, np.nan, np.nan, np.NINF, np.NZERO]\n        ]\n        length = min(len(d) for d in all_data)\n        data = {n: d[:length] for n, d in zip(string.ascii_uppercase, all_data)}\n        df = pd.DataFrame(data)\n\n        self._test_rolling_apply_mean(df)\n\n    def test_df_rolling_apply_mean_no_unboxing(self):\n        def test_impl(window, min_periods):\n            def func(x):\n                if len(x) == 0:\n                    return np.nan\n                return x.mean()\n\n            df = pd.DataFrame({\n                \'A\': [0, 1, 2, 3, 4],\n                \'B\': [1., -1., 0., 0.1, -0.1],\n                \'C\': [1., np.inf, np.inf, -1., 0.],\n                \'D\': [np.nan, np.inf, np.inf, np.nan, np.nan],\n            })\n            return df.rolling(window, min_periods).apply(func)\n\n        hpat_func = self.jit(test_impl)\n        for window in range(0, 8, 2):\n            for min_periods in range(0, window + 1, 2):\n                with self.subTest(window=window, min_periods=min_periods):\n                    jit_result = hpat_func(window, min_periods)\n                    ref_result = test_impl(window, min_periods)\n                    pd.testing.assert_frame_equal(jit_result, ref_result)\n\n    @skip_sdc_jit(\'DataFrame.rolling.apply() unsupported exceptions\')\n    def test_df_rolling_apply_unsupported_types(self):\n        all_data = [[1., -1., 0., 0.1, -0.1], [-1., 1., 0., -0.1, 0.1]]\n        length = min(len(d) for d in all_data)\n        data = {n: d[:length] for n, d in zip(string.ascii_uppercase, all_data)}\n        df = pd.DataFrame(data)\n\n        self._test_rolling_apply_unsupported_types(df)\n\n    @unittest.skip(\'DataFrame.rolling.apply() unsupported args\')\n    def test_df_rolling_apply_args(self):\n        all_data = [\n            list(range(10)), [1., -1., 0., 0.1, -0.1],\n            [1., np.inf, np.inf, -1., 0., np.inf, np.NINF, np.NINF],\n            [np.nan, np.inf, np.inf, np.nan, np.nan, np.nan, np.NINF, np.NZERO]\n        ]\n        length = min(len(d) for d in all_data)\n        data = {n: d[:length] for n, d in zip(string.ascii_uppercase, all_data)}\n        df = pd.DataFrame(data)\n\n        self._test_rolling_apply_args(df)\n\n    @skip_sdc_jit(\'DataFrame.rolling.corr() unsupported\')\n    def test_df_rolling_corr(self):\n        all_data = [\n            list(range(10)), [1., -1., 0., 0.1, -0.1],\n            [1., np.inf, np.inf, -1., 0., np.inf, np.NINF, np.NINF],\n            [np.nan, np.inf, np.inf, np.nan, np.nan, np.nan, np.NINF, np.NZERO]\n        ]\n        length = min(len(d) for d in all_data)\n        data = {n: d[:length] for n, d in zip(string.ascii_uppercase, all_data)}\n        df = pd.DataFrame(data)\n        for d in all_data:\n            other = pd.Series(d)\n            self._test_rolling_corr(df, other)\n\n        other_all_data = deepcopy(all_data) + [list(range(10))[::-1]]\n        other_all_data[1] = [-1., 1., 0., -0.1, 0.1, 0.]\n        other_length = min(len(d) for d in other_all_data)\n        other_data = {n: d[:other_length] for n, d in zip(string.ascii_uppercase, other_all_data)}\n        other = pd.DataFrame(other_data)\n\n        self._test_rolling_corr(df, other)\n\n    def test_df_rolling_corr_no_unboxing(self):\n        def test_impl(window, min_periods):\n            df = pd.DataFrame({\n                \'A\': [0, 1, 2, 3, 4],\n                \'B\': [1., -1., 0., 0.1, -0.1],\n                \'C\': [1., np.inf, np.inf, -1., 0.],\n                \'D\': [np.nan, np.inf, np.inf, np.nan, np.nan],\n            })\n            other = pd.DataFrame({\n                \'A\': [0, 1, 2, 3, 4, 5],\n                \'B\': [-1., 1., 0., -0.1, 0.1, 0.],\n                \'C\': [1., np.inf, np.inf, -1., 0., np.inf],\n                \'D\': [np.nan, np.inf, np.inf, np.nan, np.nan, np.nan],\n                \'E\': [9, 8, 7, 6, 5, 4],\n            })\n            return df.rolling(window, min_periods).corr(other)\n\n        hpat_func = self.jit(test_impl)\n        for window in range(0, 8, 2):\n            for min_periods in range(0, window, 2):\n                with self.subTest(window=window, min_periods=min_periods):\n                    jit_result = hpat_func(window, min_periods)\n                    ref_result = test_impl(window, min_periods)\n                    pd.testing.assert_frame_equal(jit_result, ref_result)\n\n    @skip_sdc_jit(\'DataFrame.rolling.corr() unsupported\')\n    def test_df_rolling_corr_no_other(self):\n        all_data = [\n            list(range(10)), [1., -1., 0., 0.1, -0.1],\n            [1., np.inf, np.inf, -1., 0., np.inf, np.NINF, np.NINF],\n            [np.nan, np.inf, np.inf, np.nan, np.nan, np.nan, np.NINF, np.NZERO]\n        ]\n        length = min(len(d) for d in all_data)\n        data = {n: d[:length] for n, d in zip(string.ascii_uppercase, all_data)}\n        df = pd.DataFrame(data)\n\n        self._test_rolling_corr_with_no_other(df)\n\n    @skip_sdc_jit(\'DataFrame.rolling.corr() unsupported exceptions\')\n    def test_df_rolling_corr_unsupported_types(self):\n        all_data = [[1., -1., 0., 0.1, -0.1], [-1., 1., 0., -0.1, 0.1]]\n        length = min(len(d) for d in all_data)\n        data = {n: d[:length] for n, d in zip(string.ascii_uppercase, all_data)}\n        df = pd.DataFrame(data)\n\n        self._test_rolling_corr_unsupported_types(df)\n\n    @skip_sdc_jit(\'DataFrame.rolling.corr() unsupported exceptions\')\n    def test_df_rolling_corr_unsupported_values(self):\n        def test_impl(df, other, pairwise):\n            return df.rolling(3, 3).corr(other=other, pairwise=pairwise)\n\n        hpat_func = self.jit(test_impl)\n        msg_tmpl = \'Method rolling.corr(). The object pairwise\\n expected: {}\'\n\n        df = pd.DataFrame({\'A\': [1., -1., 0., 0.1, -0.1],\n                           \'B\': [-1., 1., 0., -0.1, 0.1]})\n        for pairwise in [None, True]:\n            with self.assertRaises(ValueError) as raises:\n                hpat_func(df, None, pairwise)\n            self.assertIn(msg_tmpl.format(\'False\'), str(raises.exception))\n\n        other = pd.DataFrame({\'A\': [-1., 1., 0., -0.1, 0.1],\n                              \'C\': [1., -1., 0., 0.1, -0.1]})\n        with self.assertRaises(ValueError) as raises:\n            hpat_func(df, other, True)\n        self.assertIn(msg_tmpl.format(\'False, None\'), str(raises.exception))\n\n    @skip_sdc_jit(\'DataFrame.rolling.count() unsupported\')\n    def test_df_rolling_count(self):\n        all_data = test_global_input_data_float64\n        length = min(len(d) for d in all_data)\n        data = {n: d[:length] for n, d in zip(string.ascii_uppercase, all_data)}\n        df = pd.DataFrame(data)\n\n        self._test_rolling_count(df)\n\n    def test_df_rolling_count_no_unboxing(self):\n        def test_impl(window, min_periods):\n            df = pd.DataFrame({\n                \'A\': [0, 1, 2, 3, 4],\n                \'B\': [1., -1., 0., 0.1, -0.1],\n                \'C\': [1., np.inf, np.inf, -1., 0.],\n                \'D\': [np.nan, np.inf, np.inf, np.nan, np.nan],\n            })\n            return df.rolling(window, min_periods).count()\n\n        hpat_func = self.jit(test_impl)\n        for window in range(0, 8, 2):\n            for min_periods in range(0, window + 1, 2):\n                with self.subTest(window=window, min_periods=min_periods):\n                    jit_result = hpat_func(window, min_periods)\n                    ref_result = test_impl(window, min_periods)\n                    pd.testing.assert_frame_equal(jit_result, ref_result)\n\n    @skip_sdc_jit(\'DataFrame.rolling.cov() unsupported\')\n    def test_df_rolling_cov(self):\n        all_data = [\n            list(range(10)), [1., -1., 0., 0.1, -0.1],\n            [1., np.inf, np.inf, -1., 0., np.inf, np.NINF, np.NINF],\n            [np.nan, np.inf, np.inf, np.nan, np.nan, np.nan, np.NINF, np.NZERO]\n        ]\n        length = min(len(d) for d in all_data)\n        data = {n: d[:length] for n, d in zip(string.ascii_uppercase, all_data)}\n        df = pd.DataFrame(data)\n        for d in all_data:\n            other = pd.Series(d)\n            self._test_rolling_cov(df, other)\n\n        other_all_data = deepcopy(all_data) + [list(range(10))[::-1]]\n        other_all_data[1] = [-1., 1., 0., -0.1, 0.1]\n        other_length = min(len(d) for d in other_all_data)\n        other_data = {n: d[:other_length] for n, d in zip(string.ascii_uppercase, other_all_data)}\n        other = pd.DataFrame(other_data)\n\n        self._test_rolling_cov(df, other)\n\n    def test_df_rolling_cov_no_unboxing(self):\n        def test_impl(window, min_periods, ddof):\n            df = pd.DataFrame({\n                \'A\': [0, 1, 2, 3, 4],\n                \'B\': [1., -1., 0., 0.1, -0.1],\n                \'C\': [1., np.inf, np.inf, -1., 0.],\n                \'D\': [np.nan, np.inf, np.inf, np.nan, np.nan],\n            })\n            other = pd.DataFrame({\n                \'A\': [0, 1, 2, 3, 4],\n                \'B\': [-1., 1., 0., -0.1, 0.1],\n                \'C\': [1., np.inf, np.inf, -1., 0.],\n                \'D\': [np.nan, np.inf, np.inf, np.nan, np.nan],\n                \'E\': [9, 8, 7, 6, 5],\n            })\n            return df.rolling(window, min_periods).cov(other, ddof=ddof)\n\n        hpat_func = self.jit(test_impl)\n        for window in range(0, 8, 2):\n            for min_periods, ddof in product(range(0, window, 2), [0, 1]):\n                with self.subTest(window=window, min_periods=min_periods,\n                                  ddof=ddof):\n                    jit_result = hpat_func(window, min_periods, ddof)\n                    ref_result = test_impl(window, min_periods, ddof)\n                    pd.testing.assert_frame_equal(jit_result, ref_result)\n\n    @skip_sdc_jit(\'DataFrame.rolling.cov() unsupported\')\n    def test_df_rolling_cov_no_other(self):\n        all_data = [\n            list(range(10)), [1., -1., 0., 0.1, -0.1],\n            [1., np.inf, np.inf, -1., 0., np.inf, np.NINF, np.NINF],\n            [np.nan, np.inf, np.inf, np.nan, np.nan, np.nan, np.NINF, np.NZERO]\n        ]\n        length = min(len(d) for d in all_data)\n        data = {n: d[:length] for n, d in zip(string.ascii_uppercase, all_data)}\n        df = pd.DataFrame(data)\n\n        self._test_rolling_cov_with_no_other(df)\n\n    @skip_sdc_jit(\'DataFrame.rolling.cov() unsupported exceptions\')\n    def test_df_rolling_cov_unsupported_types(self):\n        all_data = [[1., -1., 0., 0.1, -0.1], [-1., 1., 0., -0.1, 0.1]]\n        length = min(len(d) for d in all_data)\n        data = {n: d[:length] for n, d in zip(string.ascii_uppercase, all_data)}\n        df = pd.DataFrame(data)\n\n        self._test_rolling_cov_unsupported_types(df)\n\n    @skip_sdc_jit(\'DataFrame.rolling.cov() unsupported exceptions\')\n    def test_df_rolling_cov_unsupported_values(self):\n        def test_impl(df, other, pairwise):\n            return df.rolling(3, 3).cov(other=other, pairwise=pairwise)\n\n        hpat_func = self.jit(test_impl)\n        msg_tmpl = \'Method rolling.cov(). The object pairwise\\n expected: {}\'\n\n        df = pd.DataFrame({\'A\': [1., -1., 0., 0.1, -0.1],\n                           \'B\': [-1., 1., 0., -0.1, 0.1]})\n        for pairwise in [None, True]:\n            with self.assertRaises(ValueError) as raises:\n                hpat_func(df, None, pairwise)\n            self.assertIn(msg_tmpl.format(\'False\'), str(raises.exception))\n\n        other = pd.DataFrame({\'A\': [-1., 1., 0., -0.1, 0.1],\n                              \'C\': [1., -1., 0., 0.1, -0.1]})\n        with self.assertRaises(ValueError) as raises:\n            hpat_func(df, other, True)\n        self.assertIn(msg_tmpl.format(\'False, None\'), str(raises.exception))\n\n    @skip_sdc_jit(\'Series.rolling.cov() unsupported Series index\')\n    @unittest.expectedFailure\n    def test_df_rolling_cov_issue_floating_point_rounding(self):\n        """"""\n            Cover issue of different float rounding in Python and SDC/Numba:\n\n            s = np.Series([1., -1., 0., 0.1, -0.1])\n            s.rolling(2, 0).mean()\n\n            Python:                  SDC/Numba:\n            0    1.000000e+00        0    1.00\n            1    0.000000e+00        1    0.00\n            2   -5.000000e-01        2   -0.50\n            3    5.000000e-02        3    0.05\n            4   -1.387779e-17        4    0.00\n            dtype: float64           dtype: float64\n\n            BTW: cov uses mean inside itself\n        """"""\n        def test_impl(df, window, min_periods, other, ddof):\n            return df.rolling(window, min_periods).cov(other, ddof=ddof)\n\n        hpat_func = self.jit(test_impl)\n\n        df = pd.DataFrame({\'A\': [1., -1., 0., 0.1, -0.1]})\n        other = pd.DataFrame({\'A\': [-1., 1., 0., -0.1, 0.1, 0.]})\n\n        jit_result = hpat_func(df, 2, 0, other, 1)\n        ref_result = test_impl(df, 2, 0, other, 1)\n        pd.testing.assert_frame_equal(jit_result, ref_result)\n\n    @skip_sdc_jit(\'DataFrame.rolling.kurt() unsupported\')\n    def test_df_rolling_kurt(self):\n        all_data = test_global_input_data_float64\n        length = min(len(d) for d in all_data)\n        data = {n: d[:length] for n, d in zip(string.ascii_uppercase, all_data)}\n        df = pd.DataFrame(data)\n\n        self._test_rolling_kurt(df)\n\n    def test_df_rolling_kurt_no_unboxing(self):\n        def test_impl(window, min_periods):\n            df = pd.DataFrame({\n                \'A\': [0, 1, 2, 3, 4],\n                \'B\': [1., -1., 0., 0.1, -0.1],\n                \'C\': [1., np.inf, np.inf, -1., 0.],\n                \'D\': [np.nan, np.inf, np.inf, np.nan, np.nan],\n            })\n            return df.rolling(window, min_periods).kurt()\n\n        hpat_func = self.jit(test_impl)\n        for window in range(4, 6):\n            for min_periods in range(window + 1):\n                with self.subTest(window=window, min_periods=min_periods):\n                    jit_result = hpat_func(window, min_periods)\n                    ref_result = test_impl(window, min_periods)\n                    pd.testing.assert_frame_equal(jit_result, ref_result)\n\n    @skip_sdc_jit(\'DataFrame.rolling.max() unsupported\')\n    def test_df_rolling_max(self):\n        all_data = test_global_input_data_float64\n        length = min(len(d) for d in all_data)\n        data = {n: d[:length] for n, d in zip(string.ascii_uppercase, all_data)}\n        df = pd.DataFrame(data)\n\n        self._test_rolling_max(df)\n\n    def test_df_rolling_max_no_unboxing(self):\n        def test_impl(window, min_periods):\n            df = pd.DataFrame({\n                \'A\': [0, 1, 2, 3, 4],\n                \'B\': [1., -1., 0., 0.1, -0.1],\n                \'C\': [1., np.inf, np.inf, -1., 0.],\n                \'D\': [np.nan, np.inf, np.inf, np.nan, np.nan],\n            })\n            return df.rolling(window, min_periods).max()\n\n        hpat_func = self.jit(test_impl)\n        for window in range(1, 7):\n            for min_periods in range(window + 1):\n                with self.subTest(window=window, min_periods=min_periods):\n                    jit_result = hpat_func(window, min_periods)\n                    ref_result = test_impl(window, min_periods)\n                    pd.testing.assert_frame_equal(jit_result, ref_result)\n\n    @skip_sdc_jit(\'DataFrame.rolling.mean() unsupported\')\n    def test_df_rolling_mean(self):\n        all_data = [\n            list(range(10)), [1., -1., 0., 0.1, -0.1],\n            [1., np.inf, np.inf, -1., 0., np.inf, np.NINF, np.NINF],\n            [np.nan, np.inf, np.inf, np.nan, np.nan, np.nan, np.NINF, np.NZERO]\n        ]\n        length = min(len(d) for d in all_data)\n        data = {n: d[:length] for n, d in zip(string.ascii_uppercase, all_data)}\n        df = pd.DataFrame(data)\n\n        self._test_rolling_mean(df)\n\n    def test_df_rolling_mean_no_unboxing(self):\n        def test_impl(window, min_periods):\n            df = pd.DataFrame({\n                \'A\': [0, 1, 2, 3, 4],\n                \'B\': [1., -1., 0., 0.1, -0.1],\n                \'C\': [1., np.inf, np.inf, -1., 0.],\n                \'D\': [np.nan, np.inf, np.inf, np.nan, np.nan],\n            })\n            return df.rolling(window, min_periods).mean()\n\n        hpat_func = self.jit(test_impl)\n        for window in range(7):\n            for min_periods in range(window):\n                with self.subTest(window=window, min_periods=min_periods):\n                    jit_result = hpat_func(window, min_periods)\n                    ref_result = test_impl(window, min_periods)\n                    pd.testing.assert_frame_equal(jit_result, ref_result)\n\n    @skip_sdc_jit(\'DataFrame.rolling.median() unsupported\')\n    def test_df_rolling_median(self):\n        all_data = test_global_input_data_float64\n        length = min(len(d) for d in all_data)\n        data = {n: d[:length] for n, d in zip(string.ascii_uppercase, all_data)}\n        df = pd.DataFrame(data)\n\n        self._test_rolling_median(df)\n\n    def test_df_rolling_median_no_unboxing(self):\n        def test_impl(window, min_periods):\n            df = pd.DataFrame({\n                \'A\': [0, 1, 2, 3, 4],\n                \'B\': [1., -1., 0., 0.1, -0.1],\n                \'C\': [1., np.inf, np.inf, -1., 0.],\n                \'D\': [np.nan, np.inf, np.inf, np.nan, np.nan],\n            })\n            return df.rolling(window, min_periods).median()\n\n        hpat_func = self.jit(test_impl)\n        for window in range(0, 8, 2):\n            for min_periods in range(0, window + 1, 2):\n                with self.subTest(window=window, min_periods=min_periods):\n                    jit_result = hpat_func(window, min_periods)\n                    ref_result = test_impl(window, min_periods)\n                    pd.testing.assert_frame_equal(jit_result, ref_result)\n\n    @skip_sdc_jit(\'DataFrame.rolling.min() unsupported\')\n    def test_df_rolling_min(self):\n        all_data = test_global_input_data_float64\n        length = min(len(d) for d in all_data)\n        data = {n: d[:length] for n, d in zip(string.ascii_uppercase, all_data)}\n        df = pd.DataFrame(data)\n\n        self._test_rolling_min(df)\n\n    def test_df_rolling_min_no_unboxing(self):\n        def test_impl(window, min_periods):\n            df = pd.DataFrame({\n                \'A\': [0, 1, 2, 3, 4],\n                \'B\': [1., -1., 0., 0.1, -0.1],\n                \'C\': [1., np.inf, np.inf, -1., 0.],\n                \'D\': [np.nan, np.inf, np.inf, np.nan, np.nan],\n            })\n            return df.rolling(window, min_periods).min()\n\n        hpat_func = self.jit(test_impl)\n        for window in range(1, 7):\n            for min_periods in range(window + 1):\n                with self.subTest(window=window, min_periods=min_periods):\n                    jit_result = hpat_func(window, min_periods)\n                    ref_result = test_impl(window, min_periods)\n                    pd.testing.assert_frame_equal(jit_result, ref_result)\n\n    @unittest.expectedFailure\n    @unittest.skipIf(platform.system() == \'Darwin\', \'Segmentation fault on Mac\')\n    @skip_sdc_jit(\'DataFrame.rolling.min() unsupported\')\n    def test_df_rolling_min_exception_many_columns(self):\n        def test_impl(df):\n            return df.rolling(3).min()\n\n        hpat_func = self.jit(test_impl)\n\n        # more than 19 columns raise SystemError: CPUDispatcher() returned a result with an error set\n        all_data = test_global_input_data_float64 * 5\n        length = min(len(d) for d in all_data)\n        data = {n: d[:length] for n, d in zip(string.ascii_uppercase, all_data)}\n        df = pd.DataFrame(data)\n\n        pd.testing.assert_frame_equal(hpat_func(df), test_impl(df))\n\n    @skip_sdc_jit(\'DataFrame.rolling.quantile() unsupported\')\n    def test_df_rolling_quantile(self):\n        all_data = [\n            list(range(10)), [1., -1., 0., 0.1, -0.1],\n            [1., np.inf, np.inf, -1., 0., np.inf, np.NINF, np.NINF],\n            [np.nan, np.inf, np.inf, np.nan, np.nan, np.nan, np.NINF, np.NZERO]\n        ]\n        length = min(len(d) for d in all_data)\n        data = {n: d[:length] for n, d in zip(string.ascii_uppercase, all_data)}\n        df = pd.DataFrame(data)\n\n        self._test_rolling_quantile(df)\n\n    def test_df_rolling_quantile_no_unboxing(self):\n        def test_impl(window, min_periods, quantile):\n            df = pd.DataFrame({\n                \'A\': [0, 1, 2, 3, 4],\n                \'B\': [1., -1., 0., 0.1, -0.1],\n                \'C\': [1., np.inf, np.inf, -1., 0.],\n                \'D\': [np.nan, np.inf, np.inf, np.nan, np.nan],\n            })\n            return df.rolling(window, min_periods).quantile(quantile)\n\n        hpat_func = self.jit(test_impl)\n        quantiles = [0, 0.25, 0.5, 0.75, 1]\n        for window in range(0, 8, 2):\n            for min_periods, q in product(range(0, window, 2), quantiles):\n                with self.subTest(window=window, min_periods=min_periods,\n                                  quantiles=q):\n                    jit_result = hpat_func(window, min_periods, q)\n                    ref_result = test_impl(window, min_periods, q)\n                    pd.testing.assert_frame_equal(jit_result, ref_result)\n\n    @skip_sdc_jit(\'DataFrame.rolling.quantile() unsupported exceptions\')\n    def test_df_rolling_quantile_exception_unsupported_types(self):\n        all_data = [[1., -1., 0., 0.1, -0.1], [-1., 1., 0., -0.1, 0.1]]\n        length = min(len(d) for d in all_data)\n        data = {n: d[:length] for n, d in zip(string.ascii_uppercase, all_data)}\n        df = pd.DataFrame(data)\n\n        self._test_rolling_quantile_exception_unsupported_types(df)\n\n    @skip_sdc_jit(\'DataFrame.rolling.quantile() unsupported exceptions\')\n    def test_df_rolling_quantile_exception_unsupported_values(self):\n        all_data = [[1., -1., 0., 0.1, -0.1], [-1., 1., 0., -0.1, 0.1]]\n        length = min(len(d) for d in all_data)\n        data = {n: d[:length] for n, d in zip(string.ascii_uppercase, all_data)}\n        df = pd.DataFrame(data)\n\n        self._test_rolling_quantile_exception_unsupported_values(df)\n\n    @skip_sdc_jit(\'DataFrame.rolling.skew() unsupported\')\n    def test_df_rolling_skew(self):\n        all_data = test_global_input_data_float64\n        length = min(len(d) for d in all_data)\n        data = {n: d[:length] for n, d in zip(string.ascii_uppercase, all_data)}\n        df = pd.DataFrame(data)\n\n        self._test_rolling_skew(df)\n\n    def test_df_rolling_skew_no_unboxing(self):\n        def test_impl(window, min_periods):\n            df = pd.DataFrame({\n                \'A\': [0, 1, 2, 3, 4],\n                \'B\': [1., -1., 0., 0.1, -0.1],\n                \'C\': [1., np.inf, np.inf, -1., 0.],\n                \'D\': [np.nan, np.inf, np.inf, np.nan, np.nan],\n            })\n            return df.rolling(window, min_periods).skew()\n\n        hpat_func = self.jit(test_impl)\n        for window in range(3, 6):\n            for min_periods in range(window + 1):\n                with self.subTest(window=window, min_periods=min_periods):\n                    jit_result = hpat_func(window, min_periods)\n                    ref_result = test_impl(window, min_periods)\n                    pd.testing.assert_frame_equal(jit_result, ref_result)\n\n    @skip_sdc_jit(\'DataFrame.rolling.std() unsupported\')\n    def test_df_rolling_std(self):\n        all_data = [\n            list(range(10)), [1., -1., 0., 0.1, -0.1],\n            [1., np.inf, np.inf, -1., 0., np.inf, np.NINF, np.NINF],\n            [np.nan, np.inf, np.inf, np.nan, np.nan, np.nan, np.NINF, np.NZERO]\n        ]\n        length = min(len(d) for d in all_data)\n        data = {n: d[:length] for n, d in zip(string.ascii_uppercase, all_data)}\n        df = pd.DataFrame(data)\n\n        self._test_rolling_std(df)\n\n    def test_df_rolling_std_no_unboxing(self):\n        def test_impl(window, min_periods, ddof):\n            df = pd.DataFrame({\n                \'A\': [0, 1, 2, 3, 4],\n                \'B\': [1., -1., 0., 0.1, -0.1],\n                \'C\': [1., np.inf, np.inf, -1., 0.],\n                \'D\': [np.nan, np.inf, np.inf, np.nan, np.nan],\n            })\n            return df.rolling(window, min_periods).std(ddof)\n\n        hpat_func = self.jit(test_impl)\n        for window in range(0, 8, 2):\n            for min_periods, ddof in product(range(0, window, 2), [0, 1]):\n                with self.subTest(window=window, min_periods=min_periods,\n                                  ddof=ddof):\n                    jit_result = hpat_func(window, min_periods, ddof)\n                    ref_result = test_impl(window, min_periods, ddof)\n                    pd.testing.assert_frame_equal(jit_result, ref_result)\n\n    @skip_sdc_jit(\'DataFrame.rolling.std() unsupported exceptions\')\n    def test_df_rolling_std_exception_unsupported_ddof(self):\n        all_data = [[1., -1., 0., 0.1, -0.1], [-1., 1., 0., -0.1, 0.1]]\n        length = min(len(d) for d in all_data)\n        data = {n: d[:length] for n, d in zip(string.ascii_uppercase, all_data)}\n        df = pd.DataFrame(data)\n\n        self._test_rolling_std_exception_unsupported_ddof(df)\n\n    @skip_sdc_jit(\'DataFrame.rolling.sum() unsupported\')\n    def test_df_rolling_sum(self):\n        all_data = [\n            list(range(10)), [1., -1., 0., 0.1, -0.1],\n            [1., np.inf, np.inf, -1., 0., np.inf, np.NINF, np.NINF],\n            [np.nan, np.inf, np.inf, np.nan, np.nan, np.nan, np.NINF, np.NZERO]\n        ]\n        length = min(len(d) for d in all_data)\n        data = {n: d[:length] for n, d in zip(string.ascii_uppercase, all_data)}\n        df = pd.DataFrame(data)\n\n        self._test_rolling_sum(df)\n\n    def test_df_rolling_sum_no_unboxing(self):\n        def test_impl(window, min_periods):\n            df = pd.DataFrame({\n                \'A\': [0, 1, 2, 3, 4],\n                \'B\': [1., -1., 0., 0.1, -0.1],\n                \'C\': [1., np.inf, np.inf, -1., 0.],\n                \'D\': [np.nan, np.inf, np.inf, np.nan, np.nan],\n            })\n            return df.rolling(window, min_periods).sum()\n\n        hpat_func = self.jit(test_impl)\n        for window in range(7):\n            for min_periods in range(window):\n                with self.subTest(window=window, min_periods=min_periods):\n                    jit_result = hpat_func(window, min_periods)\n                    ref_result = test_impl(window, min_periods)\n                    pd.testing.assert_frame_equal(jit_result, ref_result)\n\n    @skip_sdc_jit(\'DataFrame.rolling.var() unsupported\')\n    def test_df_rolling_var(self):\n        all_data = [\n            list(range(10)), [1., -1., 0., 0.1, -0.1],\n            [1., np.inf, np.inf, -1., 0., np.inf, np.NINF, np.NINF],\n            [np.nan, np.inf, np.inf, np.nan, np.nan, np.nan, np.NINF, np.NZERO]\n        ]\n        length = min(len(d) for d in all_data)\n        data = {n: d[:length] for n, d in zip(string.ascii_uppercase, all_data)}\n        df = pd.DataFrame(data)\n\n        self._test_rolling_var(df)\n\n    def test_df_rolling_var_no_unboxing(self):\n        def test_impl(window, min_periods, ddof):\n            df = pd.DataFrame({\n                \'A\': [0, 1, 2, 3, 4],\n                \'B\': [1., -1., 0., 0.1, -0.1],\n                \'C\': [1., np.inf, np.inf, -1., 0.],\n                \'D\': [np.nan, np.inf, np.inf, np.nan, np.nan],\n            })\n            return df.rolling(window, min_periods).var(ddof)\n\n        hpat_func = self.jit(test_impl)\n        for window in range(0, 8, 2):\n            for min_periods, ddof in product(range(0, window, 2), [0, 1]):\n                with self.subTest(window=window, min_periods=min_periods,\n                                  ddof=ddof):\n                    jit_result = hpat_func(window, min_periods, ddof)\n                    ref_result = test_impl(window, min_periods, ddof)\n                    pd.testing.assert_frame_equal(jit_result, ref_result)\n\n    @skip_sdc_jit(\'DataFrame.rolling.var() unsupported exceptions\')\n    def test_df_rolling_var_exception_unsupported_ddof(self):\n        all_data = [[1., -1., 0., 0.1, -0.1], [-1., 1., 0., -0.1, 0.1]]\n        length = min(len(d) for d in all_data)\n        data = {n: d[:length] for n, d in zip(string.ascii_uppercase, all_data)}\n        df = pd.DataFrame(data)\n\n        self._test_rolling_var_exception_unsupported_ddof(df)\n\n    @skip_sdc_jit(\'Series.rolling.min() unsupported exceptions\')\n    def test_series_rolling_unsupported_values(self):\n        series = pd.Series(test_global_input_data_float64[0])\n        self._test_rolling_unsupported_values(series)\n\n    @skip_sdc_jit(\'Series.rolling.min() unsupported exceptions\')\n    def test_series_rolling_unsupported_types(self):\n        series = pd.Series(test_global_input_data_float64[0])\n        self._test_rolling_unsupported_types(series)\n\n    @skip_sdc_jit(\'Series.rolling.apply() unsupported Series index\')\n    def test_series_rolling_apply_mean(self):\n        all_data = [\n            list(range(10)), [1., -1., 0., 0.1, -0.1],\n            [1., np.inf, np.inf, -1., 0., np.inf, np.NINF, np.NINF],\n            [np.nan, np.inf, np.inf, np.nan, np.nan, np.nan, np.NINF, np.NZERO]\n        ]\n        indices = [list(range(len(data)))[::-1] for data in all_data]\n        for data, index in zip(all_data, indices):\n            series = pd.Series(data, index, name=\'A\')\n            self._test_rolling_apply_mean(series)\n\n    @skip_sdc_jit(\'Series.rolling.apply() unsupported exceptions\')\n    def test_series_rolling_apply_unsupported_types(self):\n        series = pd.Series([1., -1., 0., 0.1, -0.1])\n        self._test_rolling_apply_unsupported_types(series)\n\n    @unittest.skip(\'Series.rolling.apply() unsupported args\')\n    def test_series_rolling_apply_args(self):\n        all_data = [\n            list(range(10)), [1., -1., 0., 0.1, -0.1],\n            [1., np.inf, np.inf, -1., 0., np.inf, np.NINF, np.NINF],\n            [np.nan, np.inf, np.inf, np.nan, np.nan, np.nan, np.NINF, np.NZERO]\n        ]\n        indices = [list(range(len(data)))[::-1] for data in all_data]\n        for data, index in zip(all_data, indices):\n            series = pd.Series(data, index, name=\'A\')\n            self._test_rolling_apply_args(series)\n\n    @skip_sdc_jit(\'Series.rolling.corr() unsupported Series index\')\n    def test_series_rolling_corr(self):\n        all_data = [\n            list(range(10)), [1., -1., 0., 0.1, -0.1],\n            [-1., 1., 0., -0.1, 0.1, 0.],\n            [1., np.inf, np.inf, -1., 0., np.inf, np.NINF, np.NINF],\n            [np.nan, np.inf, np.inf, np.nan, np.nan, np.nan, np.NINF, np.NZERO]\n        ]\n        for main_data, other_data in product(all_data, all_data):\n            series = pd.Series(main_data)\n            other = pd.Series(other_data)\n            self._test_rolling_corr(series, other)\n\n    @skip_sdc_jit(\'Series.rolling.corr() unsupported Series index\')\n    def test_series_rolling_corr_diff_length(self):\n        def test_impl(series, window, other):\n            return series.rolling(window).corr(other)\n\n        hpat_func = self.jit(test_impl)\n\n        series = pd.Series([1., -1., 0., 0.1, -0.1])\n        other = pd.Series(gen_frand_array(40))\n        window = 5\n        jit_result = hpat_func(series, window, other)\n        ref_result = test_impl(series, window, other)\n        pd.testing.assert_series_equal(jit_result, ref_result)\n\n    @skip_sdc_jit(\'Series.rolling.corr() unsupported Series index\')\n    def test_series_rolling_corr_with_no_other(self):\n        all_data = [\n            list(range(10)), [1., -1., 0., 0.1, -0.1],\n            [1., np.inf, np.inf, -1., 0., np.inf, np.NINF, np.NINF],\n            [np.nan, np.inf, np.inf, np.nan, np.nan, np.nan, np.NINF, np.NZERO]\n        ]\n        for data in all_data:\n            series = pd.Series(data)\n            self._test_rolling_corr_with_no_other(series)\n\n    @skip_sdc_jit(\'Series.rolling.corr() unsupported exceptions\')\n    def test_series_rolling_corr_unsupported_types(self):\n        series = pd.Series([1., -1., 0., 0.1, -0.1])\n        self._test_rolling_corr_unsupported_types(series)\n\n    @skip_sdc_jit(\'Series.rolling.corr() unsupported Series index\')\n    @unittest.expectedFailure  # https://jira.devtools.intel.com/browse/SAT-2377\n    def test_series_rolling_corr_index(self):\n        def test_impl(S1, S2):\n            return S1.rolling(window=3).corr(S2)\n\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        np.random.seed(0)\n        index_values = np.arange(n)\n\n        np.random.shuffle(index_values)\n        S1 = pd.Series(np.arange(n), index=index_values, name=\'A\')\n        np.random.shuffle(index_values)\n        S2 = pd.Series(2 * np.arange(n) - 5, index=index_values, name=\'B\')\n\n        result = hpat_func(S1, S2)\n        result_ref = test_impl(S1, S2)\n        pd.testing.assert_series_equal(result, result_ref)\n\n    @skip_sdc_jit(\'Series.rolling.count() unsupported Series index\')\n    def test_series_rolling_count(self):\n        all_data = test_global_input_data_float64\n        indices = [list(range(len(data)))[::-1] for data in all_data]\n        for data, index in zip(all_data, indices):\n            series = pd.Series(data, index, name=\'A\')\n            self._test_rolling_count(series)\n\n    @skip_sdc_jit(\'Series.rolling.cov() unsupported Series index\')\n    def test_series_rolling_cov(self):\n        all_data = [\n            list(range(5)), [1., -1., 0., 0.1, -0.1],\n            [1., np.inf, np.inf, -1., 0., np.inf, np.NINF, np.NINF],\n            [np.nan, np.inf, np.inf, np.nan, np.nan, np.nan, np.NINF, np.NZERO]\n        ]\n        for main_data, other_data in product(all_data, all_data):\n            series = pd.Series(main_data)\n            other = pd.Series(other_data)\n            self._test_rolling_cov(series, other)\n\n    @skip_sdc_jit(\'Series.rolling.cov() unsupported Series index\')\n    def test_series_rolling_cov_diff_length(self):\n        def test_impl(series, window, other):\n            return series.rolling(window).cov(other)\n\n        hpat_func = self.jit(test_impl)\n\n        series = pd.Series([1., -1., 0., 0.1, -0.1])\n        other = pd.Series(gen_frand_array(40))\n        window = 5\n        jit_result = hpat_func(series, window, other)\n        ref_result = test_impl(series, window, other)\n        pd.testing.assert_series_equal(jit_result, ref_result)\n\n    @skip_sdc_jit(\'Series.rolling.cov() unsupported Series index\')\n    def test_series_rolling_cov_no_other(self):\n        all_data = [\n            list(range(5)), [1., -1., 0., 0.1, -0.1],\n            [1., np.inf, np.inf, -1., 0., np.inf, np.NINF, np.NINF],\n            [np.nan, np.inf, np.inf, np.nan, np.nan, np.nan, np.NINF, np.NZERO]\n        ]\n        for data in all_data:\n            series = pd.Series(data)\n            self._test_rolling_cov_with_no_other(series)\n\n    @skip_sdc_jit(\'Series.rolling.cov() unsupported Series index\')\n    @unittest.expectedFailure\n    def test_series_rolling_cov_issue_floating_point_rounding(self):\n        """"""Cover issue of different float rounding in Python and SDC/Numba""""""\n        def test_impl(series, window, min_periods, other, ddof):\n            return series.rolling(window, min_periods).cov(other, ddof=ddof)\n\n        hpat_func = self.jit(test_impl)\n\n        series = pd.Series(list(range(10)))\n        other = pd.Series([1., -1., 0., 0.1, -0.1])\n        jit_result = hpat_func(series, 6, 0, other, 1)\n        ref_result = test_impl(series, 6, 0, other, 1)\n        pd.testing.assert_series_equal(jit_result, ref_result)\n\n    @skip_sdc_jit(\'Series.rolling.cov() unsupported exceptions\')\n    def test_series_rolling_cov_unsupported_types(self):\n        series = pd.Series([1., -1., 0., 0.1, -0.1])\n        self._test_rolling_cov_unsupported_types(series)\n\n    @skip_sdc_jit(\'Series.rolling.kurt() unsupported Series index\')\n    def test_series_rolling_kurt(self):\n        all_data = test_global_input_data_float64\n        indices = [list(range(len(data)))[::-1] for data in all_data]\n        for data, index in zip(all_data, indices):\n            series = pd.Series(data, index, name=\'A\')\n            self._test_rolling_kurt(series)\n\n    @skip_sdc_jit(\'Series.rolling.max() unsupported Series index\')\n    def test_series_rolling_max(self):\n        all_data = test_global_input_data_float64\n        indices = [list(range(len(data)))[::-1] for data in all_data]\n        for data, index in zip(all_data, indices):\n            series = pd.Series(data, index, name=\'A\')\n            self._test_rolling_max(series)\n\n    @skip_sdc_jit(\'Series.rolling.mean() unsupported Series index\')\n    def test_series_rolling_mean(self):\n        all_data = [\n            list(range(10)), [1., -1., 0., 0.1, -0.1],\n            [1., np.inf, np.inf, -1., 0., np.inf, np.NINF, np.NINF],\n            [np.nan, np.inf, np.inf, np.nan, np.nan, np.nan, np.NINF, np.NZERO]\n        ]\n        indices = [list(range(len(data)))[::-1] for data in all_data]\n        for data, index in zip(all_data, indices):\n            series = pd.Series(data, index, name=\'A\')\n            self._test_rolling_mean(series)\n\n    @skip_sdc_jit(\'Series.rolling.median() unsupported Series index\')\n    def test_series_rolling_median(self):\n        all_data = test_global_input_data_float64\n        indices = [list(range(len(data)))[::-1] for data in all_data]\n        for data, index in zip(all_data, indices):\n            series = pd.Series(data, index, name=\'A\')\n            self._test_rolling_median(series)\n\n    @skip_sdc_jit(\'Series.rolling.min() unsupported Series index\')\n    def test_series_rolling_min(self):\n        all_data = test_global_input_data_float64\n        indices = [list(range(len(data)))[::-1] for data in all_data]\n        for data, index in zip(all_data, indices):\n            series = pd.Series(data, index, name=\'A\')\n            self._test_rolling_min(series)\n\n    @skip_sdc_jit(\'Series.rolling.quantile() unsupported Series index\')\n    def test_series_rolling_quantile(self):\n        all_data = [\n            list(range(10)), [1., -1., 0., 0.1, -0.1],\n            [1., np.inf, np.inf, -1., 0., np.inf, np.NINF, np.NINF],\n            [np.nan, np.inf, np.inf, np.nan, np.nan, np.nan, np.NINF, np.NZERO]\n        ]\n        indices = [list(range(len(data)))[::-1] for data in all_data]\n        for data, index in zip(all_data, indices):\n            series = pd.Series(data, index, name=\'A\')\n            self._test_rolling_quantile(series)\n\n    @skip_sdc_jit(\'Series.rolling.quantile() unsupported exceptions\')\n    def test_series_rolling_quantile_exception_unsupported_types(self):\n        series = pd.Series([1., -1., 0., 0.1, -0.1])\n        self._test_rolling_quantile_exception_unsupported_types(series)\n\n    @skip_sdc_jit(\'Series.rolling.quantile() unsupported exceptions\')\n    def test_series_rolling_quantile_exception_unsupported_values(self):\n        series = pd.Series([1., -1., 0., 0.1, -0.1])\n        self._test_rolling_quantile_exception_unsupported_values(series)\n\n    @skip_sdc_jit(\'Series.rolling.skew() unsupported Series index\')\n    def test_series_rolling_skew(self):\n        all_data = test_global_input_data_float64\n        indices = [list(range(len(data)))[::-1] for data in all_data]\n        for data, index in zip(all_data, indices):\n            series = pd.Series(data, index, name=\'A\')\n            self._test_rolling_skew(series)\n\n    @skip_sdc_jit(\'Series.rolling.std() unsupported Series index\')\n    def test_series_rolling_std(self):\n        all_data = [\n            list(range(10)), [1., -1., 0., 0.1, -0.1],\n            [1., np.inf, np.inf, -1., 0., np.inf, np.NINF, np.NINF],\n            [np.nan, np.inf, np.inf, np.nan, np.nan, np.nan, np.NINF, np.NZERO]\n        ]\n        indices = [list(range(len(data)))[::-1] for data in all_data]\n        for data, index in zip(all_data, indices):\n            series = pd.Series(data, index, name=\'A\')\n            self._test_rolling_std(series)\n\n    @skip_sdc_jit(\'Series.rolling.std() unsupported exceptions\')\n    def test_series_rolling_std_exception_unsupported_ddof(self):\n        series = pd.Series([1., -1., 0., 0.1, -0.1])\n        self._test_rolling_std_exception_unsupported_ddof(series)\n\n    @skip_sdc_jit(\'Series.rolling.sum() unsupported Series index\')\n    def test_series_rolling_sum(self):\n        all_data = [\n            list(range(10)), [1., -1., 0., 0.1, -0.1],\n            [1., np.inf, np.inf, -1., 0., np.inf, np.NINF, np.NINF],\n            [np.nan, np.inf, np.inf, np.nan, np.nan, np.nan, np.NINF, np.NZERO]\n        ]\n        indices = [list(range(len(data)))[::-1] for data in all_data]\n        for data, index in zip(all_data, indices):\n            series = pd.Series(data, index, name=\'A\')\n            self._test_rolling_sum(series)\n\n    @skip_sdc_jit(\'Series.rolling.var() unsupported Series index\')\n    def test_series_rolling_var(self):\n        all_data = [\n            list(range(10)), [1., -1., 0., 0.1, -0.1],\n            [1., np.inf, np.inf, -1., 0., np.inf, np.NINF, np.NINF],\n            [np.nan, np.inf, np.inf, np.nan, np.nan, np.nan, np.NINF, np.NZERO]\n        ]\n        indices = [list(range(len(data)))[::-1] for data in all_data]\n        for data, index in zip(all_data, indices):\n            series = pd.Series(data, index, name=\'A\')\n            self._test_rolling_var(series)\n\n    @skip_sdc_jit(\'Series.rolling.var() unsupported exceptions\')\n    def test_series_rolling_var_exception_unsupported_ddof(self):\n        series = pd.Series([1., -1., 0., 0.1, -0.1])\n        self._test_rolling_var_exception_unsupported_ddof(series)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
sdc/tests/test_sdc_numpy.py,102,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\n# -*- coding: utf-8 -*-\nimport numpy as np\nimport pandas as pd\nimport sdc\nimport unittest\n\nfrom sdc.str_arr_ext import StringArray\nfrom sdc.str_ext import std_str_to_unicode, unicode_to_std_str\nfrom sdc.tests.test_base import TestCase\nfrom sdc.tests.test_utils import skip_numba_jit\nfrom sdc.functions import numpy_like\nfrom sdc.functions import sort\n\n\nclass TestArrays(TestCase):\n\n    def test_astype_to_num(self):\n        def ref_impl(a, t):\n            return a.astype(t)\n\n        def sdc_impl(a, t):\n            return numpy_like.astype(a, t)\n\n        sdc_func = self.jit(sdc_impl)\n\n        cases = [[5, 2, 0, 333, -4], [3.3, 5.4, np.nan]]\n        cases_type = [np.float64, np.int64, \'float64\', \'int64\']\n        for case in cases:\n            a = np.array(case)\n            for type_ in cases_type:\n                with self.subTest(data=case, type=type_):\n                    np.testing.assert_array_equal(sdc_func(a, type_), ref_impl(a, type_))\n\n    def test_astype_to_float(self):\n        def ref_impl(a):\n            return a.astype(\'float64\')\n\n        def sdc_impl(a):\n            return numpy_like.astype(a, \'float64\')\n\n        sdc_func = self.jit(sdc_impl)\n\n        cases = [[2, 3, 0], [4., 5.6, np.nan]]\n        for case in cases:\n            a = np.array(case)\n            with self.subTest(data=case):\n                np.testing.assert_array_equal(sdc_func(a), ref_impl(a))\n\n    def test_astype_to_int(self):\n        def ref_impl(a):\n            return a.astype(np.int64)\n\n        def sdc_impl(a):\n            return numpy_like.astype(a, np.int64)\n\n        sdc_func = self.jit(sdc_impl)\n\n        cases = [[2, 3, 0], [4., 5.6, np.nan]]\n        for case in cases:\n            a = np.array(case)\n            with self.subTest(data=case):\n                np.testing.assert_array_equal(sdc_func(a), ref_impl(a))\n\n    def test_astype_int_to_str(self):\n        def ref_impl(a):\n            return a.astype(str)\n\n        def sdc_impl(a):\n            return numpy_like.astype(a, str)\n\n        sdc_func = self.jit(sdc_impl)\n\n        a = np.array([2, 3, 0])\n        np.testing.assert_array_equal(sdc_func(a), ref_impl(a))\n\n    @unittest.skip(\'Numba converts float to string with incorrect precision\')\n    def test_astype_float_to_str(self):\n        def ref_impl(a):\n            return a.astype(str)\n\n        def sdc_impl(a):\n            return numpy_like.astype(a, str)\n\n        sdc_func = self.jit(sdc_impl)\n\n        a = np.array([4., 5.6, np.nan])\n        np.testing.assert_array_equal(sdc_func(a), ref_impl(a))\n\n    def test_astype_num_to_str(self):\n        def ref_impl(a):\n            return a.astype(\'str\')\n\n        def sdc_impl(a):\n            return numpy_like.astype(a, \'str\')\n\n        sdc_func = self.jit(sdc_impl)\n\n        a = np.array([5, 2, 0, 333, -4])\n        np.testing.assert_array_equal(sdc_func(a), ref_impl(a))\n\n    @unittest.skip(\'Needs Numba astype impl support converting unicode_type to other type\')\n    def test_astype_str_to_num(self):\n        def ref_impl(a, t):\n            return a.astype(t)\n\n        def sdc_impl(a, t):\n            return numpy_like.astype(a, t)\n\n        sdc_func = self.jit(sdc_impl)\n\n        cases = [[\'a\', \'cc\', \'d\'], [\'3.3\', \'5\', \'.4\'], [\'\xc2\xa1Y\', \'t\xc3\xba qui\xc3\xa9n \', \'te crees\']]\n        cases_type = [np.float64, np.int64]\n        for case in cases:\n            a = np.array(case)\n            for type_ in cases_type:\n                with self.subTest(data=case, type=type_):\n                    np.testing.assert_array_equal(sdc_func(a, type_), ref_impl(a, type_))\n\n    def test_isnan(self):\n        def ref_impl(a):\n            return np.isnan(a)\n\n        def sdc_impl(a):\n            return numpy_like.isnan(a)\n\n        sdc_func = self.jit(sdc_impl)\n\n        cases = [[5, 2, 0, 333, -4], [3.3, 5.4, np.nan, 7.9, np.nan]]\n        for case in cases:\n            a = np.array(case)\n            with self.subTest(data=case):\n                np.testing.assert_array_equal(sdc_func(a), ref_impl(a))\n\n    @unittest.skip(\'Needs provide String Array boxing\')\n    def test_isnan_str(self):\n        def ref_impl(a):\n            return np.isnan(a)\n\n        def sdc_impl(a):\n            return numpy_like.isnan(a)\n\n        sdc_func = self.jit(sdc_impl)\n\n        cases = [[\'a\', \'cc\', np.nan], [\'se\', None, \'vvv\']]\n        for case in cases:\n            a = np.array(case)\n            with self.subTest(data=case):\n                np.testing.assert_array_equal(sdc_func(a), ref_impl(a))\n\n    def test_notnan(self):\n        def ref_impl(a):\n            return np.invert(np.isnan(a))\n\n        def sdc_impl(a):\n            return numpy_like.notnan(a)\n\n        sdc_func = self.jit(sdc_impl)\n\n        cases = [[5, 2, 0, 333, -4], [3.3, 5.4, np.nan, 7.9, np.nan]]\n        for case in cases:\n            a = np.array(case)\n            with self.subTest(data=case):\n                np.testing.assert_array_equal(sdc_func(a), ref_impl(a))\n\n    def test_copy(self):\n        def ref_impl(a):\n            return np.copy(a)\n\n        def sdc_impl(a):\n            return numpy_like.copy(a)\n\n        sdc_func = self.jit(sdc_impl)\n\n        cases = [[5, 2, 0, 333, -4], [3.3, 5.4, np.nan, 7.9, np.nan], [True, False, True], [\'a\', \'vv\', \'o12oo\']]\n        for case in cases:\n            a = np.array(case)\n            with self.subTest(data=case):\n                np.testing.assert_array_equal(sdc_func(a), ref_impl(a))\n\n    def test_copy_int(self):\n        def ref_impl():\n            a = np.array([5, 2, 0, 333, -4])\n            return np.copy(a)\n\n        def sdc_impl():\n            a = np.array([5, 2, 0, 333, -4])\n            return numpy_like.copy(a)\n\n        sdc_func = self.jit(sdc_impl)\n        np.testing.assert_array_equal(sdc_func(), ref_impl())\n\n    def test_copy_bool(self):\n        def ref_impl():\n            a = np.array([True, False, True])\n            return np.copy(a)\n\n        def sdc_impl():\n            a = np.array([True, False, True])\n            return numpy_like.copy(a)\n\n        sdc_func = self.jit(sdc_impl)\n        np.testing.assert_array_equal(sdc_func(), ref_impl())\n\n    @unittest.skip(""Numba doesn\'t have string array"")\n    def test_copy_str(self):\n        def ref_impl():\n            a = np.array([\'a\', \'vv\', \'o12oo\'])\n            return np.copy(a)\n\n        def sdc_impl():\n            a = np.array([\'a\', \'vv\', \'o12oo\'])\n            return numpy_like.copy(a)\n\n        sdc_func = self.jit(sdc_impl)\n        np.testing.assert_array_equal(sdc_func(), ref_impl())\n\n    def test_argmin(self):\n        def ref_impl(a):\n            return np.argmin(a)\n\n        def sdc_impl(a):\n            return numpy_like.argmin(a)\n\n        sdc_func = self.jit(sdc_impl)\n\n        cases = [[5, 2, 0, 333, -4], [3.3, 5.4, np.nan, 7.9, np.nan]]\n        for case in cases:\n            a = np.array(case)\n            with self.subTest(data=case):\n                np.testing.assert_array_equal(sdc_func(a), ref_impl(a))\n\n    def test_argmax(self):\n        def ref_impl(a):\n            return np.argmax(a)\n\n        def sdc_impl(a):\n            return numpy_like.argmax(a)\n\n        sdc_func = self.jit(sdc_impl)\n\n        cases = [[np.nan, np.nan, np.inf, np.nan], [5, 2, 0, 333, -4], [3.3, 5.4, np.nan, 7.9, np.nan]]\n        for case in cases:\n            a = np.array(case)\n            with self.subTest(data=case):\n                np.testing.assert_array_equal(sdc_func(a), ref_impl(a))\n\n    def test_nanargmin(self):\n        def ref_impl(a):\n            return np.nanargmin(a)\n\n        def sdc_impl(a):\n            return numpy_like.nanargmin(a)\n\n        sdc_func = self.jit(sdc_impl)\n\n        cases = [[5, 2, 0, 333, -4], [3.3, 5.4, np.nan, 7.9, np.nan]]\n        for case in cases:\n            a = np.array(case)\n            with self.subTest(data=case):\n                np.testing.assert_array_equal(sdc_func(a), ref_impl(a))\n\n    def test_nanargmax(self):\n        def ref_impl(a):\n            return np.nanargmax(a)\n\n        def sdc_impl(a):\n            return numpy_like.nanargmax(a)\n\n        sdc_func = self.jit(sdc_impl)\n\n        cases = [[np.nan, np.nan, np.inf, np.nan], [5, 2, -9, 333, -4], [3.3, 5.4, np.nan, 7.9]]\n        for case in cases:\n            a = np.array(case)\n            with self.subTest(data=case):\n                np.testing.assert_array_equal(sdc_func(a), ref_impl(a))\n\n    def test_sort(self):\n        np.random.seed(0)\n\n        def ref_impl(a):\n            return np.sort(a)\n\n        def sdc_impl(a):\n            sort.parallel_sort(a)\n            return a\n\n        sdc_func = self.jit(sdc_impl)\n\n        float_array = np.random.ranf(10**2)\n        int_arryay = np.random.randint(0, 127, 10**2)\n\n        float_cases = [\'float32\', \'float64\']\n        for case in float_cases:\n            array0 = float_array.astype(case)\n            array1 = np.copy(array0)\n            with self.subTest(data=case):\n                np.testing.assert_array_equal(ref_impl(array0), sdc_func(array1))\n\n        int_cases = [\'int8\', \'uint8\', \'int16\', \'uint16\', \'int32\', \'uint32\', \'int64\', \'uint64\']\n        for case in int_cases:\n            array0 = int_arryay.astype(case)\n            array1 = np.copy(array0)\n            with self.subTest(data=case):\n                np.testing.assert_array_equal(ref_impl(array0), sdc_func(array1))\n\n    def test_stable_sort(self):\n        np.random.seed(0)\n\n        def ref_impl(a):\n            return np.sort(a)\n\n        def sdc_impl(a):\n            sort.parallel_stable_sort(a)\n            return a\n\n        sdc_func = self.jit(sdc_impl)\n\n        float_array = np.random.ranf(10**2)\n        int_arryay = np.random.randint(0, 127, 10**2)\n\n        float_cases = [\'float32\', \'float64\']\n        for case in float_cases:\n            array0 = float_array.astype(case)\n            array1 = np.copy(array0)\n            with self.subTest(data=case):\n                np.testing.assert_array_equal(ref_impl(array0), sdc_func(array1))\n\n        int_cases = [\'int8\', \'uint8\', \'int16\', \'uint16\', \'int32\', \'uint32\', \'int64\', \'uint64\']\n        for case in int_cases:\n            array0 = int_arryay.astype(case)\n            array1 = np.copy(array0)\n            with self.subTest(data=case):\n                np.testing.assert_array_equal(ref_impl(array0), sdc_func(array1))\n\n\nclass TestArrayReductions(TestCase):\n\n    def check_reduction_basic(self, pyfunc, alt_pyfunc, all_nans=True, comparator=None):\n        if not comparator:\n            comparator = np.testing.assert_array_equal\n\n        alt_cfunc = self.jit(alt_pyfunc)\n\n        def cases():\n            yield np.array([5, 2, 0, 333, -4])\n            yield np.array([3.3, 5.4, np.nan, 7.9, np.nan])\n            yield np.float64([1.0, 2.0, 0.0, -0.0, 1.0, -1.5])\n            yield np.float64([-0.0, -1.5])\n            yield np.float64([-1.5, 2.5, \'inf\'])\n            yield np.float64([-1.5, 2.5, \'-inf\'])\n            yield np.float64([-1.5, 2.5, \'inf\', \'-inf\'])\n            yield np.float64([\'nan\', -1.5, 2.5, \'nan\', 3.0])\n            yield np.float64([\'nan\', -1.5, 2.5, \'nan\', \'inf\', \'-inf\', 3.0])\n            if all_nans:\n                # Only NaNs\n                yield np.float64([\'nan\', \'nan\'])\n\n        for case in cases():\n            with self.subTest(data=case):\n                comparator(alt_cfunc(case), pyfunc(case))\n\n    def test_nanmean(self):\n        def ref_impl(a):\n            return np.nanmean(a)\n\n        def sdc_impl(a):\n            return numpy_like.nanmean(a)\n\n        self.check_reduction_basic(ref_impl, sdc_impl)\n\n    def test_nanmin(self):\n        def ref_impl(a):\n            return np.nanmin(a)\n\n        def sdc_impl(a):\n            return numpy_like.nanmin(a)\n\n        self.check_reduction_basic(ref_impl, sdc_impl)\n\n    def test_nanmax(self):\n        def ref_impl(a):\n            return np.nanmax(a)\n\n        def sdc_impl(a):\n            return numpy_like.nanmax(a)\n\n        self.check_reduction_basic(ref_impl, sdc_impl)\n\n    def test_nanprod(self):\n        def ref_impl(a):\n            return np.nanprod(a)\n\n        def sdc_impl(a):\n            return numpy_like.nanprod(a)\n\n        self.check_reduction_basic(ref_impl, sdc_impl)\n\n    def test_nansum(self):\n        def ref_impl(a):\n            return np.nansum(a)\n\n        def sdc_impl(a):\n            return numpy_like.nansum(a)\n\n        self.check_reduction_basic(ref_impl, sdc_impl)\n\n    def test_nanvar(self):\n        def ref_impl(a):\n            return np.nanvar(a)\n\n        def sdc_impl(a):\n            return numpy_like.nanvar(a)\n\n        self.check_reduction_basic(ref_impl, sdc_impl,\n                                   comparator=np.testing.assert_array_almost_equal)\n\n    def test_sum(self):\n        def ref_impl(a):\n            return np.sum(a)\n\n        def sdc_impl(a):\n            return numpy_like.sum(a)\n\n        self.check_reduction_basic(ref_impl, sdc_impl)\n\n    def test_cumsum(self):\n        def ref_impl(a):\n            return np.cumsum(a)\n\n        def sdc_impl(a):\n            return numpy_like.cumsum(a)\n\n        self.check_reduction_basic(ref_impl, sdc_impl)\n\n    def test_nancumsum(self):\n        def ref_impl(a):\n            return np.nancumsum(a)\n\n        def sdc_impl(a):\n            return numpy_like.nancumsum(a)\n\n        self.check_reduction_basic(ref_impl, sdc_impl)\n'"
sdc/tests/test_series.py,641,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\n# -*- coding: utf-8 -*-\nimport numpy as np\nimport pandas as pd\nimport platform\nimport pyarrow.parquet as pq\nimport sdc\nimport string\nimport unittest\nfrom itertools import combinations, combinations_with_replacement, islice, permutations, product\nimport numba\nfrom numba import types\nfrom numba.core.config import IS_32BITS\nfrom numba.core.errors import TypingError\nfrom numba import literally\n\nfrom sdc.tests.test_series_apply import TestSeries_apply\nfrom sdc.tests.test_series_map import TestSeries_map\nfrom sdc.tests.test_base import TestCase\nfrom sdc.tests.test_utils import (count_array_OneDs,\n                                  count_array_REPs,\n                                  count_parfor_REPs,\n                                  get_start_end,\n                                  sdc_limitation,\n                                  skip_inline,\n                                  skip_numba_jit,\n                                  skip_parallel,\n                                  skip_sdc_jit,\n                                  create_series_from_values,\n                                  take_k_elements)\nfrom sdc.tests.gen_test_data import ParquetGenerator\n\nfrom sdc.tests.test_utils import test_global_input_data_unicode_kind1\nfrom sdc.datatypes.common_functions import SDCLimitation\n\n\n_cov_corr_series = [(pd.Series(x), pd.Series(y)) for x, y in [\n    (\n        [np.nan, -2., 3., 9.1],\n        [np.nan, -2., 3., 5.0],\n    ),\n    # TODO(quasilyte): more intricate data for complex-typed series.\n    # Some arguments make assert_almost_equal fail.\n    # Functions that yield mismaching results:\n    # _column_corr_impl and _column_cov_impl.\n    (\n        [complex(-2., 1.0), complex(3.0, 1.0)],\n        [complex(-3., 1.0), complex(2.0, 1.0)],\n    ),\n    (\n        [complex(-2.0, 1.0), complex(3.0, 1.0)],\n        [1.0, -2.0],\n    ),\n    (\n        [1.0, -4.5],\n        [complex(-4.5, 1.0), complex(3.0, 1.0)],\n    ),\n]]\n\nmin_float64 = np.finfo(\'float64\').min\nmax_float64 = np.finfo(\'float64\').max\n\ntest_global_input_data_float64 = [\n    [11., 35.2, -24., 0., np.NZERO, np.NINF, np.PZERO, min_float64],\n    [1., np.nan, -1., 0., min_float64, max_float64, max_float64, min_float64],\n    [np.nan, np.inf, np.inf, np.nan, np.nan, np.nan, np.NINF, np.NZERO]\n]\n\nmin_int64 = np.iinfo(\'int64\').min\nmax_int64 = np.iinfo(\'int64\').max\nmax_uint64 = np.iinfo(\'uint64\').max\n\ntest_global_input_data_signed_integer64 = [\n    [1, -1, 0],\n    [min_int64, max_int64, max_int64, min_int64],\n]\n\ntest_global_input_data_integer64 = test_global_input_data_signed_integer64 + [[max_uint64, max_uint64]]\n\ntest_global_input_data_numeric = test_global_input_data_integer64 + test_global_input_data_float64\n\ntest_global_input_data_unicode_kind4 = [\n    \'ascii\',\n    \'12345\',\n    \'1234567890\',\n    \'\xc2\xa1Y t\xc3\xba qui\xc3\xa9n te crees?\',\n    \'\xf0\x9f\x90\x8d\xe2\x9a\xa1\',\n    \'\xe5\xa4\xa7\xe5\xa4\x84\xe7\x9d\x80\xe7\x9c\xbc\xef\xbc\x8c\xe5\xb0\x8f\xe5\xa4\x84\xe7\x9d\x80\xe6\x89\x8b\xe3\x80\x82\',\n]\n\ndef gen_srand_array(size, nchars=8):\n    """"""Generate array of strings of specified size based on [a-zA-Z] + [0-9]""""""\n    accepted_chars = list(string.ascii_letters + string.digits)\n    rands_chars = np.array(accepted_chars, dtype=(np.str_, 1))\n\n    np.random.seed(100)\n    return np.random.choice(rands_chars, size=nchars * size).view((np.str_, nchars))\n\n\ndef gen_frand_array(size, min=-100, max=100, nancount=0):\n    """"""Generate array of float of specified size based on [-100-100]""""""\n    np.random.seed(100)\n    res = (max - min) * np.random.sample(size) + min\n    if nancount:\n        res[np.random.choice(np.arange(size), nancount)] = np.nan\n    return res\n\n\ndef gen_strlist(size, nchars=8, accepted_chars=None):\n    """"""Generate list of strings of specified size based on accepted_chars""""""\n    if not accepted_chars:\n        accepted_chars = string.ascii_letters + string.digits\n    generated_chars = islice(permutations(accepted_chars, nchars), size)\n\n    return [\'\'.join(chars) for chars in generated_chars]\n\n\ndef series_values_from_argsort_result(series, argsorted):\n    """"""\n        Rearranges series values according to pandas argsort result.\n        Used in tests to verify correct work of Series.argsort implementation for unstable sortings.\n    """"""\n    argsort_indices = argsorted.values\n    result = np.empty_like(series.values)\n    # pandas argsort returns -1 in positions of NaN elements\n    nan_values_mask = argsort_indices == -1\n    if np.any(nan_values_mask):\n        result[nan_values_mask] = np.nan\n\n    # pandas argsort returns indexes in series values after all nans were dropped from it\n    # hence drop the NaN values, rearrange the rest with argsort result and assign them back to their positions\n    series_notna_values = series.dropna().values\n    result[~nan_values_mask] = series_notna_values.take(argsort_indices[~nan_values_mask])\n\n    return result\n\n\n#   Restores a series and checks the correct arrangement of indices,\n#   taking into account the same elements for unstable sortings\n#   Example: pd.Series([15, 3, 7, 3, 1],[2, 4, 6, 8, 10])\n#   Result can be pd.Series([1, 3, 3, 7, 15],[10, 4, 8, 6, 2]) or pd.Series([1, 3, 3, 7, 15],[10, 8, 4, 6, 2])\n#   if indices correct - return 0; wrong - return 1\ndef restore_series_sort_values(series, my_result_index, ascending):\n    value_dict = {}\n    nan_list = []\n    data = np.copy(series.data)\n    index = np.copy(series.index)\n    for value in range(len(data)):\n        # if np.isnan(data[value]):\n        if series.isna()[index[value]]:\n            nan_list.append(index[value])\n        if data[value] in value_dict:\n            value_dict[data[value]].append(index[value])\n        else:\n            value_dict[data[value]] = [index[value]]\n    na = series.isna().sum()\n    sort = np.argsort(data)\n    result = np.copy(my_result_index)\n    if not ascending:\n        sort[:len(result)-na] = sort[:len(result)-na][::-1]\n    for i in range(len(result)-na):\n        check = 0\n        for j in value_dict[data[sort[i]]]:\n            if j == result[i]:\n                check = 1\n        if check == 0:\n            return 1\n    for i in range(len(result)-na, len(result)):\n        check = 0\n        for j in nan_list:\n            if result[i] == j:\n                check = 1\n        if check == 0:\n            return 1\n    return 0\n\n\ndef _make_func_from_text(func_text, func_name=\'test_impl\', global_vars={}):\n    loc_vars = {}\n    exec(func_text, global_vars, loc_vars)\n    test_impl = loc_vars[func_name]\n    return test_impl\n\n\ndef _make_func_use_binop1(operator):\n    func_text = ""def test_impl(A, B):\\n""\n    func_text += ""   return A {} B\\n"".format(operator)\n    return _make_func_from_text(func_text)\n\n\ndef _make_func_use_binop2(operator):\n    func_text = ""def test_impl(A, B):\\n""\n    func_text += ""   A {} B\\n"".format(operator)\n    func_text += ""   return A\\n""\n    return _make_func_from_text(func_text)\n\n\ndef _make_func_use_method_arg1(method):\n    func_text = ""def test_impl(A, B):\\n""\n    func_text += ""   return A.{}(B)\\n"".format(method)\n    return _make_func_from_text(func_text)\n\n\ndef ljust_usecase(series, width):\n    return series.str.ljust(width)\n\n\ndef ljust_with_fillchar_usecase(series, width, fillchar):\n    return series.str.ljust(width, fillchar)\n\n\ndef rjust_usecase(series, width):\n    return series.str.rjust(width)\n\n\ndef rjust_with_fillchar_usecase(series, width, fillchar):\n    return series.str.rjust(width, fillchar)\n\n\ndef istitle_usecase(series):\n    return series.str.istitle()\n\n\ndef isspace_usecase(series):\n    return series.str.isspace()\n\n\ndef isalpha_usecase(series):\n    return series.str.isalpha()\n\n\ndef islower_usecase(series):\n    return series.str.islower()\n\n\ndef isalnum_usecase(series):\n    return series.str.isalnum()\n\n\ndef isnumeric_usecase(series):\n    return series.str.isnumeric()\n\n\ndef isdigit_usecase(series):\n    return series.str.isdigit()\n\n\ndef isdecimal_usecase(series):\n    return series.str.isdecimal()\n\n\ndef isupper_usecase(series):\n    return series.str.isupper()\n\n\ndef lower_usecase(series):\n    return series.str.lower()\n\n\ndef upper_usecase(series):\n    return series.str.upper()\n\n\ndef strip_usecase(series, to_strip=None):\n    return series.str.strip(to_strip)\n\n\ndef lstrip_usecase(series, to_strip=None):\n    return series.str.lstrip(to_strip)\n\n\ndef rstrip_usecase(series, to_strip=None):\n    return series.str.rstrip(to_strip)\n\n\ndef contains_usecase(series, pat, case=True, flags=0, na=None, regex=True):\n    return series.str.contains(pat, case, flags, na, regex)\n\n\nclass TestSeries(\n    TestSeries_apply,\n    TestSeries_map,\n    TestCase\n):\n\n    @unittest.skip(\'Feature request: implement Series::ctor with list(list(type))\')\n    def test_create_list_list_unicode(self):\n        def test_impl():\n            S = pd.Series([\n                          [\'abc\', \'defg\', \'ijk\'],\n                          [\'lmn\', \'opq\', \'rstuvwxyz\']\n                          ])\n            return S\n        hpat_func = self.jit(test_impl)\n\n        result_ref = test_impl()\n        result = hpat_func()\n        pd.testing.assert_series_equal(result, result_ref)\n\n    @unittest.skip(\'Feature request: implement Series::ctor with list(list(type))\')\n    def test_create_list_list_integer(self):\n        def test_impl():\n            S = pd.Series([\n                          [123, 456, -789],\n                          [-112233, 445566, 778899]\n                          ])\n            return S\n        hpat_func = self.jit(test_impl)\n\n        result_ref = test_impl()\n        result = hpat_func()\n        pd.testing.assert_series_equal(result, result_ref)\n\n    @unittest.skip(\'Feature request: implement Series::ctor with list(list(type))\')\n    def test_create_list_list_float(self):\n        def test_impl():\n            S = pd.Series([\n                          [1.23, -4.56, 7.89],\n                          [11.2233, 44.5566, -778.899]\n                          ])\n            return S\n        hpat_func = self.jit(test_impl)\n\n        result_ref = test_impl()\n        result = hpat_func()\n        pd.testing.assert_series_equal(result, result_ref)\n\n    def test_create_series1(self):\n        def test_impl():\n            A = pd.Series([1, 2, 3])\n            return A\n        hpat_func = self.jit(test_impl)\n\n        pd.testing.assert_series_equal(hpat_func(), test_impl())\n\n    def test_create_series_index1(self):\n        # create and box an indexed Series\n        def test_impl():\n            A = pd.Series([1, 2, 3], [\'A\', \'C\', \'B\'])\n            return A\n        hpat_func = self.jit(test_impl)\n\n        pd.testing.assert_series_equal(hpat_func(), test_impl())\n\n    def test_create_series_index2(self):\n        def test_impl():\n            A = pd.Series([1, 2, 3], index=[2, 1, 0])\n            return A\n        hpat_func = self.jit(test_impl)\n\n        pd.testing.assert_series_equal(hpat_func(), test_impl())\n\n    def test_create_series_index3(self):\n        def test_impl():\n            A = pd.Series([1, 2, 3], index=[\'A\', \'C\', \'B\'], name=\'A\')\n            return A\n        hpat_func = self.jit(test_impl)\n\n        pd.testing.assert_series_equal(hpat_func(), test_impl())\n\n    def test_create_series_index4(self):\n        def test_impl(name):\n            A = pd.Series([1, 2, 3], index=[\'A\', \'C\', \'B\'], name=name)\n            return A\n        hpat_func = self.jit(test_impl)\n\n        pd.testing.assert_series_equal(hpat_func(\'A\'), test_impl(\'A\'))\n\n    @skip_numba_jit\n    def test_pass_series1(self):\n        # TODO: check to make sure it is series type\n        def test_impl(A):\n            return (A == 2).sum()\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        S = pd.Series(np.arange(n), name=\'A\')\n        self.assertEqual(hpat_func(S), test_impl(S))\n\n    @skip_numba_jit\n    def test_pass_series_str(self):\n        def test_impl(A):\n            return (A == \'a\').sum()\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([\'a\', \'b\', \'c\'], name=\'A\')\n        self.assertEqual(hpat_func(S), test_impl(S))\n\n    def test_pass_series_index1(self):\n        def test_impl(A):\n            return A\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([3, 5, 6], [\'a\', \'b\', \'c\'], name=\'A\')\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    def test_series_getattr_size(self):\n        def test_impl(S):\n            return S.size\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        for S, expected in [\n            (pd.Series(), 0),\n            (pd.Series([]), 0),\n            (pd.Series(np.arange(n)), n),\n            (pd.Series([np.nan, 1, 2]), 3),\n            (pd.Series([\'1\', \'2\', \'3\']), 3),\n        ]:\n            with self.subTest(S=S, expected=expected):\n                self.assertEqual(hpat_func(S), expected)\n                self.assertEqual(hpat_func(S), test_impl(S))\n\n    def test_series_argsort1(self):\n        def test_impl(A):\n            return A.argsort()\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        np.random.seed(0)\n        A = pd.Series(np.random.ranf(n))\n        pd.testing.assert_series_equal(hpat_func(A), test_impl(A))\n\n    @skip_sdc_jit(""Fails to compile with latest Numba"")\n    def test_series_argsort2(self):\n        def test_impl(S):\n            return S.argsort()\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([1, -1, 0, 2, np.nan], [1, 2, 3, 4, 5])\n        pd.testing.assert_series_equal(test_impl(S), hpat_func(S))\n\n    @skip_sdc_jit(""Fails to compile with latest Numba"")\n    def test_series_argsort_full(self):\n        def test_impl(series, kind):\n            return series.argsort(axis=0, kind=kind, order=None)\n\n        hpat_func = self.jit(test_impl)\n\n        all_data = test_global_input_data_numeric\n\n        for data in all_data:\n            S = pd.Series(data * 3)\n            for kind in [\'quicksort\', \'mergesort\']:\n                result = test_impl(S, kind=kind)\n                result_ref = hpat_func(S, kind=kind)\n                if kind == \'mergesort\':\n                    pd.testing.assert_series_equal(result, result_ref)\n                else:\n                    # for non-stable sorting check that values of restored series are equal\n                    np.testing.assert_array_equal(\n                        series_values_from_argsort_result(S, result),\n                        series_values_from_argsort_result(S, result_ref)\n                    )\n\n    @skip_sdc_jit(""Fails to compile with latest Numba"")\n    def test_series_argsort_full_idx(self):\n        def test_impl(series, kind):\n            return series.argsort(axis=0, kind=kind, order=None)\n\n        hpat_func = self.jit(test_impl)\n\n        all_data = test_global_input_data_numeric\n\n        for data in all_data:\n            data = data * 3\n            for index in [gen_srand_array(len(data)), gen_frand_array(len(data)), range(len(data))]:\n                S = pd.Series(data, index)\n                for kind in [\'quicksort\', \'mergesort\']:\n                    result = test_impl(S, kind=kind)\n                    result_ref = hpat_func(S, kind=kind)\n                    if kind == \'mergesort\':\n                        pd.testing.assert_series_equal(result, result_ref)\n                    else:\n                        # for non-stable sorting check that values of restored series are equal\n                        np.testing.assert_array_equal(\n                            series_values_from_argsort_result(S, result),\n                            series_values_from_argsort_result(S, result_ref)\n                        )\n\n    @skip_sdc_jit(""Fails to compile with latest Numba"")\n    def test_series_attr6(self):\n        def test_impl(A):\n            return A.take([2, 3]).values\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        df = pd.DataFrame({\'A\': np.arange(n)})\n        np.testing.assert_array_equal(hpat_func(df.A), test_impl(df.A))\n\n    def test_series_attr7(self):\n        def test_impl(A):\n            return A.astype(np.float64)\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        df = pd.DataFrame({\'A\': np.arange(n)})\n        np.testing.assert_array_equal(hpat_func(df.A), test_impl(df.A))\n\n    def test_series_getattr_ndim(self):\n        """"""Verifies getting Series attribute ndim is supported""""""\n        def test_impl(S):\n            return S.ndim\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        S = pd.Series(np.arange(n))\n        self.assertEqual(hpat_func(S), test_impl(S))\n\n    def test_series_getattr_T(self):\n        """"""Verifies getting Series attribute T is supported""""""\n        def test_impl(S):\n            return S.T\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        S = pd.Series(np.arange(n))\n        np.testing.assert_array_equal(hpat_func(S), test_impl(S))\n\n    def test_series_copy_str1(self):\n        def test_impl(A):\n            return A.copy()\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([\'aa\', \'bb\', \'cc\'])\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    def test_series_copy_int1(self):\n        def test_impl(A):\n            return A.copy()\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([1, 2, 3])\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    def test_series_copy_deep(self):\n        def test_impl(A, deep):\n            return A.copy(deep=deep)\n        hpat_func = self.jit(test_impl)\n\n        for S in [\n            pd.Series([1, 2]),\n            pd.Series([1, 2], index=[""a"", ""b""]),\n            pd.Series([1, 2], name=\'A\'),\n            pd.Series([1, 2], index=[""a"", ""b""], name=\'A\'),\n        ]:\n            with self.subTest(S=S):\n                for deep in (True, False):\n                    with self.subTest(deep=deep):\n                        actual = hpat_func(S, deep)\n                        expected = test_impl(S, deep)\n\n                        pd.testing.assert_series_equal(actual, expected)\n\n                        self.assertEqual(actual.values is S.values, expected.values is S.values)\n                        self.assertEqual(actual.values is S.values, not deep)\n\n                        # Shallow copy of index is not supported yet\n                        if deep:\n                            self.assertEqual(actual.index is S.index, expected.index is S.index)\n                            self.assertEqual(actual.index is S.index, not deep)\n\n    @skip_sdc_jit(\'Series.corr() parameter ""min_periods"" unsupported\')\n    def test_series_corr(self):\n        def test_series_corr_impl(s1, s2, min_periods=None):\n            return s1.corr(s2, min_periods=min_periods)\n\n        hpat_func = self.jit(test_series_corr_impl)\n        test_input_data1 = [[.2, .0, .6, .2],\n                            [.2, .0, .6, .2, .5, .6, .7, .8],\n                            [],\n                            [2, 0, 6, 2],\n                            [.2, .1, np.nan, .5, .3],\n                            [-1, np.nan, 1, np.inf]]\n        test_input_data2 = [[.3, .6, .0, .1],\n                            [.3, .6, .0, .1, .8],\n                            [],\n                            [3, 6, 0, 1],\n                            [.3, .2, .9, .6, np.nan],\n                            [np.nan, np.nan, np.inf, np.nan]]\n        for input_data1 in test_input_data1:\n            for input_data2 in test_input_data2:\n                s1 = pd.Series(input_data1)\n                s2 = pd.Series(input_data2)\n                for period in [None, 2, 1, 8, -4]:\n                    result_ref = test_series_corr_impl(s1, s2, min_periods=period)\n                    result = hpat_func(s1, s2, min_periods=period)\n                    np.testing.assert_allclose(result, result_ref)\n\n    @skip_sdc_jit(\'Series.corr() parameter ""min_periods"" unsupported\')\n    def test_series_corr_unsupported_dtype(self):\n        def test_series_corr_impl(s1, s2, min_periods=None):\n            return s1.corr(s2, min_periods=min_periods)\n\n        hpat_func = self.jit(test_series_corr_impl)\n        s1 = pd.Series([.2, .0, .6, .2])\n        s2 = pd.Series([\'abcdefgh\', \'a\', \'abcdefg\', \'ab\', \'abcdef\', \'abc\'])\n        s3 = pd.Series([\'aaaaa\', \'bbbb\', \'ccc\', \'dd\', \'e\'])\n        s4 = pd.Series([.3, .6, .0, .1])\n\n        with self.assertRaises(TypingError) as raises:\n            hpat_func(s1, s2, min_periods=5)\n        msg = \'Method corr(). The object other.data\'\n        self.assertIn(msg, str(raises.exception))\n\n        with self.assertRaises(TypingError) as raises:\n            hpat_func(s3, s4, min_periods=5)\n        msg = \'Method corr(). The object self.data\'\n        self.assertIn(msg, str(raises.exception))\n\n    @skip_sdc_jit(\'Series.corr() parameter ""min_periods"" unsupported\')\n    def test_series_corr_unsupported_period(self):\n        def test_series_corr_impl(s1, s2, min_periods=None):\n            return s1.corr(s2, min_periods=min_periods)\n\n        hpat_func = self.jit(test_series_corr_impl)\n        s1 = pd.Series([.2, .0, .6, .2])\n        s2 = pd.Series([.3, .6, .0, .1])\n\n        with self.assertRaises(TypingError) as raises:\n            hpat_func(s1, s2, min_periods=\'aaaa\')\n        msg = \'Method corr(). The object min_periods\'\n        self.assertIn(msg, str(raises.exception))\n\n        with self.assertRaises(TypingError) as raises:\n            hpat_func(s1, s2, min_periods=0.5)\n        msg = \'Method corr(). The object min_periods\'\n        self.assertIn(msg, str(raises.exception))\n\n    @skip_parallel\n    @skip_inline\n    def test_series_astype_int_to_str1(self):\n        """"""Verifies Series.astype implementation with function \'str\' as argument\n           converts integer series to series of strings\n        """"""\n        def test_impl(S):\n            return S.astype(str)\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        S = pd.Series(np.arange(n))\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    @skip_parallel\n    @skip_inline\n    def test_series_astype_int_to_str2(self):\n        """"""Verifies Series.astype implementation with a string literal dtype argument\n           converts integer series to series of strings\n        """"""\n        def test_impl(S):\n            return S.astype(\'str\')\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        S = pd.Series(np.arange(n))\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    @skip_parallel\n    @skip_inline\n    def test_series_astype_str_to_str1(self):\n        """"""Verifies Series.astype implementation with function \'str\' as argument\n           handles string series not changing it\n        """"""\n        def test_impl(S):\n            return S.astype(str)\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([\'aa\', \'bb\', \'cc\'])\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    @skip_parallel\n    @skip_inline\n    def test_series_astype_str_to_str2(self):\n        """"""Verifies Series.astype implementation with a string literal dtype argument\n           handles string series not changing it\n        """"""\n        def test_impl(S):\n            return S.astype(\'str\')\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([\'aa\', \'bb\', \'cc\'])\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    @skip_parallel\n    @skip_inline\n    def test_series_astype_str_to_str_index_str(self):\n        """"""Verifies Series.astype implementation with function \'str\' as argument\n           handles string series not changing it\n        """"""\n\n        def test_impl(S):\n            return S.astype(str)\n\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([\'aa\', \'bb\', \'cc\'], index=[\'d\', \'e\', \'f\'])\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    @skip_parallel\n    @skip_inline\n    def test_series_astype_str_to_str_index_int(self):\n        """"""Verifies Series.astype implementation with function \'str\' as argument\n           handles string series not changing it\n        """"""\n\n        def test_impl(S):\n            return S.astype(str)\n\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([\'aa\', \'bb\', \'cc\'], index=[1, 2, 3])\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    @unittest.skip(\'TODO: requires str(datetime64) support in Numba\')\n    def test_series_astype_dt_to_str1(self):\n        """"""Verifies Series.astype implementation with function \'str\' as argument\n           converts datetime series to series of strings\n        """"""\n        def test_impl(A):\n            return A.astype(str)\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([pd.Timestamp(\'20130101 09:00:00\'),\n                       pd.Timestamp(\'20130101 09:00:02\'),\n                       pd.Timestamp(\'20130101 09:00:03\')\n                       ])\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    @unittest.skip(\'AssertionError: Series are different\'\n                   \'[left]:  [0.000000, 1.000000, 2.000000, 3.000000, ...\'\n                   \'[right]:  [0.0, 1.0, 2.0, 3.0, ...\'\n                   \'TODO: needs alignment to NumPy on Numba side\')\n    def test_series_astype_float_to_str1(self):\n        """"""Verifies Series.astype implementation with function \'str\' as argument\n           converts float series to series of strings\n        """"""\n        def test_impl(A):\n            return A.astype(str)\n        hpat_func = self.jit(test_impl)\n\n        n = 11.0\n        S = pd.Series(np.arange(n))\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    def test_series_astype_int32_to_int64(self):\n        """"""Verifies Series.astype implementation with NumPy dtype argument\n           converts series with dtype=int32 to series with dtype=int64\n        """"""\n        def test_impl(A):\n            return A.astype(np.int64)\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        S = pd.Series(np.arange(n), dtype=np.int32)\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    def test_series_astype_int_to_float64(self):\n        """"""Verifies Series.astype implementation with NumPy dtype argument\n           converts named integer series to series of float\n        """"""\n        def test_impl(A):\n            return A.astype(np.float64)\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        S = pd.Series(np.arange(n), name=\'A\')\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    def test_series_astype_float_to_int32(self):\n        """"""Verifies Series.astype implementation with NumPy dtype argument\n           converts float series to series of integers\n        """"""\n        def test_impl(A):\n            return A.astype(np.int32)\n        hpat_func = self.jit(test_impl)\n\n        n = 11.0\n        S = pd.Series(np.arange(n))\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    def test_series_astype_literal_dtype1(self):\n        """"""Verifies Series.astype implementation with a string literal dtype argument\n           converts float series to series of integers\n        """"""\n        def test_impl(A):\n            return A.astype(\'int32\')\n        hpat_func = self.jit(test_impl)\n\n        n = 11.0\n        S = pd.Series(np.arange(n))\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    @unittest.skip(\'TODO: needs Numba astype impl support converting unicode_type to int\')\n    def test_series_astype_str_to_int32(self):\n        """"""Verifies Series.astype implementation with NumPy dtype argument\n           converts series of strings to series of integers\n        """"""\n        import numba\n\n        def test_impl(A):\n            return A.astype(np.int32)\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        S = pd.Series([str(x) for x in np.arange(n) - n // 2])\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    @unittest.skip(\'TODO: needs Numba astype impl support converting unicode_type to float\')\n    def test_series_astype_str_to_float64(self):\n        """"""Verifies Series.astype implementation with NumPy dtype argument\n           converts series of strings to series of float\n        """"""\n        def test_impl(A):\n            return A.astype(np.float64)\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([\'3.24\', \'1E+05\', \'-1\', \'-1.3E-01\', \'nan\', \'inf\'])\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    @skip_parallel\n    @skip_inline\n    def test_series_astype_str_index_str(self):\n        """"""Verifies Series.astype implementation with function \'str\' as argument\n           handles string series not changing it\n        """"""\n\n        def test_impl(S):\n            return S.astype(str)\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([\'aa\', \'bb\', \'cc\'], index=[\'a\', \'b\', \'c\'])\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    @skip_parallel\n    @skip_inline\n    def test_series_astype_str_index_int(self):\n        """"""Verifies Series.astype implementation with function \'str\' as argument\n           handles string series not changing it\n        """"""\n\n        def test_impl(S):\n            return S.astype(str)\n\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([\'aa\', \'bb\', \'cc\'], index=[2, 3, 5])\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    def test_series_astype_errors_ignore_return_self_str(self):\n        """"""Verifies Series.astype implementation return self object on error\n           if errors=\'ignore\' is passed in arguments\n        """"""\n\n        def test_impl(S):\n            return S.astype(np.float64, errors=\'ignore\')\n\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([\'aa\', \'bb\', \'cc\'], index=[2, 3, 5])\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    @skip_numba_jit(\'TODO: implement np.call on Series in new-pipeline\')\n    def test_np_call_on_series1(self):\n        def test_impl(A):\n            return np.min(A)\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        S = pd.Series(np.arange(n), name=\'A\')\n        np.testing.assert_array_equal(hpat_func(S), test_impl(S))\n\n    def test_series_getattr_values(self):\n        def test_impl(A):\n            return A.values\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        S = pd.Series(np.arange(n), name=\'A\')\n        np.testing.assert_array_equal(hpat_func(S), test_impl(S))\n\n    def test_series_values1(self):\n        def test_impl(A):\n            return (A == 2).values\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        S = pd.Series(np.arange(n), name=\'A\')\n        np.testing.assert_array_equal(hpat_func(S), test_impl(S))\n\n    def test_series_getattr_shape1(self):\n        def test_impl(A):\n            return A.shape\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        S = pd.Series(np.arange(n), name=\'A\')\n        self.assertEqual(hpat_func(S), test_impl(S))\n\n    def test_series_static_setitem(self):\n        def test_impl(A):\n            A[0] = 2\n            return (A == 2).sum()\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        S1 = pd.Series(np.arange(n), name=\'A\')\n        S2 = S1.copy()\n        self.assertEqual(hpat_func(S1), test_impl(S2))\n\n    def test_series_setitem1(self):\n        def test_impl(A, i):\n            A[i] = 2\n            return (A == 2).sum()\n        hpat_func = self.jit(test_impl)\n\n        n, i = 11, 0\n        S1 = pd.Series(np.arange(n), name=\'A\')\n        S2 = S1.copy()\n        self.assertEqual(hpat_func(S1, i), test_impl(S2, i))\n\n    def test_series_setitem2(self):\n        def test_impl(A, i):\n            A[i] = 100\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        S1 = pd.Series(np.arange(n), name=\'A\')\n        S2 = S1.copy()\n        hpat_func(S1, 0)\n        test_impl(S2, 0)\n        pd.testing.assert_series_equal(S1, S2)\n\n    @skip_sdc_jit(""enable after remove dead in hiframes is removed"")\n    @skip_numba_jit(""Assertion Error. Effects of set are not observed due to dead code elimination""\n                    ""TODO: investigate how to support this in Numba"")\n    def test_series_setitem3(self):\n        def test_impl(A, i):\n            S = pd.Series(A)\n            S[i] = 100\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        A = np.arange(n)\n        A1 = A.copy()\n        A2 = A\n        hpat_func(A1, 0)\n        test_impl(A2, 0)\n        np.testing.assert_array_equal(A1, A2)\n\n    def test_series_setitem_with_filter1(self):\n        def test_impl(A):\n            A[A > 3] = 100\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        S1 = pd.Series(np.arange(n))\n        S2 = S1.copy()\n        hpat_func(S1)\n        test_impl(S2)\n        pd.testing.assert_series_equal(S1, S2)\n\n    @skip_sdc_jit(\'cannot assign slice from input of different size\')\n    @skip_numba_jit(""Series.setitem misses specialization for OptionalType"")\n    def test_series_setitem_with_filter2(self):\n        def test_impl(A, B):\n            A[A > 3] = B[A > 3]\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        A1 = pd.Series(np.arange(n), name=\'A\')\n        B = pd.Series(np.arange(n)**2, name=\'B\')\n        A2 = A1.copy()\n        hpat_func(A1, B)\n        test_impl(A2, B)\n        pd.testing.assert_series_equal(A1, A2)\n\n    @skip_sdc_jit(""Fails to compile with latest Numba"")\n    def test_series_static_getitem(self):\n        def test_impl(A):\n            return A[1]\n        hpat_func = self.jit(test_impl)\n\n        A = pd.Series([1, 3, 5], [\'1\', \'4\', \'2\'], name=\'A\')\n        self.assertEqual(hpat_func(A), test_impl(A))\n\n    @skip_sdc_jit(\'Not impl in old style\')\n    def test_series_getitem_idx_int1(self):\n        def test_impl(A, i):\n            return A[i]\n        hpat_func = self.jit(test_impl)\n\n        n, i = 11, 0\n        S = pd.Series(np.arange(n), name=\'A\')\n        # SDC and pandas results differ due to type limitation requirements:\n        # SDC returns Series of one element, whereas pandas returns scalar, hence we align result_ref\n        result = hpat_func(S, i)\n        result_ref = pd.Series(test_impl(S, i), dtype=S.dtype, name=\'A\')\n        pd.testing.assert_series_equal(result, result_ref)\n        self.assertEqual(len(result), 1)\n\n    @skip_sdc_jit(\'Not impl in old style\')\n    def test_series_getitem_idx_int2(self):\n        def test_impl(A, i):\n            return A[i]\n        hpat_func = self.jit(test_impl)\n\n        n, i = 11, 0\n        S = pd.Series(np.arange(n), name=\'A\')\n        # SDC and pandas results differ due to type limitation requirements:\n        # SDC returns Series of one element, whereas pandas returns scalar, hence we align result\n        result = hpat_func(S, i).data[0]\n        result_ref = test_impl(S, i)\n        self.assertEqual(result, result_ref)\n\n    @skip_sdc_jit(\'Not impl in old style\')\n    def test_series_getitem_idx_int3(self):\n        def test_impl(A, i):\n            return A[i]\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([\'aa\', \'bb\', \'cc\'], name=\'A\')\n        # SDC and pandas results differ due to type limitation requirements:\n        # SDC returns Series of one element, whereas pandas returns scalar, hence we align result_ref\n        result = hpat_func(S, 0)\n        result_ref = pd.Series(test_impl(S, 0), name=S.name)\n        pd.testing.assert_series_equal(result, result_ref)\n\n    @skip_sdc_jit(\'Not impl in old style\')\n    def test_series_iat1(self):\n        def test_impl(A):\n            return A.iat[3]\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        S = pd.Series(np.arange(n)**2, name=\'A\')\n        self.assertEqual(hpat_func(S), test_impl(S))\n\n    @skip_sdc_jit(\'Not impl in old style\')\n    @skip_numba_jit(\'TODO: implement setitem for SeriesGetitemAccessorType\')\n    def test_series_iat2(self):\n        def test_impl(A):\n            A.iat[3] = 1\n            return A\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        S = pd.Series(np.arange(n)**2, name=\'A\')\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    @skip_sdc_jit(\'Not impl in old style\')\n    def test_series_getitem_idx_series(self):\n        def test_impl(A, B):\n            return A[B]\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([1, 2, 3, 4, 5], [6, 0, 8, 0, 0], name=\'A\')\n        S2 = pd.Series([8, 6, 0], [12, 11, 14])\n        pd.testing.assert_series_equal(hpat_func(S, S2), test_impl(S, S2))\n\n    @skip_sdc_jit(\'Not impl in old style\')\n    def test_series_getitem_idx_series_noidx(self):\n        def test_impl(A, B):\n            return A[B]\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([1, 2, 3, 4, 5], name=\'A\')\n        S2 = pd.Series([3, 2, 0])\n        pd.testing.assert_series_equal(hpat_func(S, S2), test_impl(S, S2))\n\n    @skip_sdc_jit(\'Getitem Series with str index not implement in old-style\')\n    def test_series_getitem_idx_series_index_str(self):\n        def test_impl(A, B):\n            return A[B]\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([\'1\', \'2\', \'3\', \'4\', \'5\'], [\'6\', \'7\', \'8\', \'9\', \'0\'], name=\'A\')\n        S2 = pd.Series([\'8\', \'6\', \'0\'], [\'12\', \'11\', \'14\'])\n        pd.testing.assert_series_equal(hpat_func(S, S2), test_impl(S, S2))\n\n    @skip_sdc_jit(\'Not impl in old style\')\n    @skip_numba_jit(""TODO: support named Series indexes"")\n    def test_series_getitem_idx_series_named(self):\n        def test_impl(A, B):\n            return A[B]\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([\'1\', \'2\', \'3\', \'4\', \'5\'], [\'6\', \'7\', \'8\', \'9\', \'0\'], name=\'A\')\n        S2 = pd.Series([\'8\', \'6\', \'0\'], [\'12\', \'11\', \'14\'], name=\'B\')\n        pd.testing.assert_series_equal(hpat_func(S, S2), test_impl(S, S2))\n\n    def test_series_iloc1(self):\n        def test_impl(A):\n            return A.iloc[3]\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        S = pd.Series(np.arange(n)**2)\n        self.assertEqual(hpat_func(S), test_impl(S))\n\n    @skip_sdc_jit(\'Not impl in old style\')\n    def test_series_loc_return_ser(self):\n        def test_impl(A):\n            return A.loc[3]\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([2, 4, 6, 5, 7], [1, 3, 5, 3, 3])\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    @skip_sdc_jit(\'Not impl in old style\')\n    def test_series_getitem_idx_int4(self):\n        def test_impl(S, key):\n            return S[key]\n\n        jit_impl = self.jit(test_impl)\n\n        keys = [2, 2, 3]\n        indices = [[2, 3, 5], [2, 3, 5], [2, 3, 5]]\n        for key, index in zip(keys, indices):\n            S = pd.Series([11, 22, 33], index, name=\'A\')\n            np.testing.assert_array_equal(jit_impl(S, key).data, np.array(test_impl(S, key)))\n\n    @skip_sdc_jit(\'Not impl in old style\')\n    def test_series_getitem_duplicate_index(self):\n        def test_impl(A):\n            return A[3]\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([2, 4, 6, 33, 7], [1, 3, 5, 3, 3], name=\'A\')\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    @skip_sdc_jit(\'Not impl in old style\')\n    def test_series_getitem_idx_int_slice(self):\n        def test_impl(S, start, end):\n            return S[start:end]\n\n        jit_impl = self.jit(test_impl)\n\n        starts = [0, 0]\n        ends = [2, 2]\n        indices = [[2, 3, 5], [\'2\', \'3\', \'5\'], [\'2\', \'3\', \'5\']]\n        for start, end, index in zip(starts, ends, indices):\n            S = pd.Series([11, 22, 33], index, name=\'A\')\n            ref_result = test_impl(S, start, end)\n            jit_result = jit_impl(S, start, end)\n            pd.testing.assert_series_equal(jit_result, ref_result)\n\n    @skip_sdc_jit(\'Not impl in old style\')\n    @skip_numba_jit(\'TODO: implement String slice support\')\n    def test_series_getitem_idx_str_slice(self):\n        def test_impl(A):\n            return A[\'1\':\'7\']\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([\'1\', \'4\', \'6\', \'33\', \'7\'], [\'1\', \'3\', \'5\', \'4\', \'7\'], name=\'A\')\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    def test_series_at(self):\n        def test_impl(S, key):\n            return S.at[key]\n\n        jit_impl = self.jit(test_impl)\n\n        keys = [\'2\', \'2\']\n        all_data = [[11, 22, 33], [11, 22, 33]]\n        indices = [[\'2\', \'22\', \'0\'], [\'2\', \'3\', \'5\']]\n        for key, data, index in zip(keys, all_data, indices):\n            S = pd.Series(data, index, name=\'A\')\n            self.assertEqual(jit_impl(S, key), test_impl(S, key))\n\n    def test_series_at_array(self):\n        def test_impl(A):\n            return A.at[3]\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([2, 4, 6, 12, 0], [1, 3, 5, 3, 3], name=\'A\')\n        np.testing.assert_array_equal(hpat_func(S), test_impl(S))\n\n    @skip_sdc_jit(\'Not impl in old style\')\n    def test_series_loc(self):\n        def test_impl(S, key):\n            return S.loc[key]\n\n        jit_impl = self.jit(test_impl)\n\n        keys = [2, 2, 2]\n        all_data = [[11, 22, 33], [11, 22, 33], [11, 22, 33]]\n        indices = [[2, 3, 5], [2, 2, 2], [2, 4, 15]]\n        for key, data, index in zip(keys, all_data, indices):\n            S = pd.Series(data, index, name=\'A\')\n            np.testing.assert_array_equal(jit_impl(S, key).data, np.array(test_impl(S, key)))\n\n    @skip_sdc_jit(\'Not impl in old style\')\n    def test_series_loc_str(self):\n        def test_impl(A):\n            return A.loc[\'1\']\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([2, 4, 6], [\'1\', \'3\', \'5\'], name=\'A\')\n        np.testing.assert_array_equal(hpat_func(S), test_impl(S))\n\n    @skip_sdc_jit(\'Not impl in old style\')\n    def test_series_loc_array(self):\n        def test_impl(A, n):\n            return A.loc[n]\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([1, 2, 4, 8, 6, 0], [1, 2, 4, 0, 6, 0], name=\'A\')\n        n = [0, 4, 2]\n        cases = [n, np.array(n)]\n        for n in cases:\n            pd.testing.assert_series_equal(hpat_func(S, n), test_impl(S, n))\n\n    @skip_sdc_jit(\'Not impl in old style\')\n    def test_series_at_str(self):\n        def test_impl(A):\n            return A.at[\'1\']\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([2, 4, 6], [\'1\', \'3\', \'5\'], name=\'A\')\n        np.testing.assert_array_equal(hpat_func(S), test_impl(S))\n\n    @skip_sdc_jit(\'Not impl in old style\')\n    def test_series_loc_slice_nonidx(self):\n        def test_impl(A):\n            return A.loc[1:3]\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([2, 4, 6, 6, 3], name=\'A\')\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    @unittest.skip(\'Slice string index not impl\')\n    def test_series_loc_slice_empty(self):\n        def test_impl(A):\n            return A.loc[\'301\':\'-4\']\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([2, 4, 6, 6, 3], [\'-22\', \'-5\', \'-2\', \'300\', \'40000\'], name=\'A\')\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    @skip_sdc_jit(\'Not impl in old style\')\n    def test_series_iloc2(self):\n        def test_impl(A):\n            return A.iloc[3:8]\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        S = pd.Series(np.arange(n)**2, name=\'A\')\n        pd.testing.assert_series_equal(\n            hpat_func(S), test_impl(S))\n\n    @skip_sdc_jit(\'Not impl in old style\')\n    def test_series_slice_loc_start(self):\n        def test_impl(A, n):\n            return A.loc[n:]\n        hpat_func = self.jit(test_impl)\n\n        all_data = [[1, 3, 5, 13, 22], [1, 3, 3, 13, 22], [22, 13, 5, 3, 1], [100, 3, 1, -3, -3]]\n        key = [1, 3, 18]\n        for index in all_data:\n            for n in key:\n                with self.subTest(index=index, start=n):\n                    S = pd.Series([2, 4, 6, 6, 3], index, name=\'A\')\n                    pd.testing.assert_series_equal(hpat_func(S, n), test_impl(S, n))\n\n    """"""\n        For a pandas series: S = pd.Series([2, 4, 6, 6, 3], index=[100, 3, 0, -3, -3])\n        pandas implementation of loc with slice returns different results if slice has start defined,\n        and if it\'s omitted:\n            >>>S.loc[:-3]\n             100    2\n             3      4\n             0      6\n            -3      6\n            -3      3\n            dtype: int64\n            >>>S.loc[0:-3]\n             0    6\n            -3    6\n            -3    3\n            dtype: int64\n        Current Numba SliceType implementation doesn\'t allow to distinguish these cases.\n    """"""\n    @unittest.expectedFailure  # add reference to Numba issue!\n    def test_series_slice_loc_stop(self):\n        def test_impl(A, n):\n            return A.loc[:n]\n        hpat_func = self.jit(test_impl)\n\n        all_data = [[1, 3, 5, 13, 22], [1, 3, 3, 13, 22], [22, 13, 5, 3, 1], [100, 3, 0, -3, -3]]\n        key = [1, 3, 18]\n        for index in all_data:\n            for n in key:\n                with self.subTest(index=index, stop=n):\n                    S = pd.Series([2, 4, 6, 6, 3], index, name=\'A\')\n                    pd.testing.assert_series_equal(hpat_func(S, n), test_impl(S, n))\n\n    @skip_sdc_jit(\'Not impl in old style\')\n    def test_series_slice_loc_start_stop(self):\n        def test_impl(A, n, k):\n            return A.loc[n:k]\n        hpat_func = self.jit(test_impl)\n\n        all_data = [[1, 3, 5, 13, 22], [1, 3, 3, 13, 22], [22, 13, 5, 3, 1], [100, 3, 0, -3, -3]]\n        key = [-100, 1, 3, 18, 22, 100]\n        for index in all_data:\n            for data_left, data_right in combinations_with_replacement(key, 2):\n                with self.subTest(index=index, left=data_left, right=data_right):\n                    S = pd.Series([2, 4, 6, 6, 3], index, name=\'A\')\n                    pd.testing.assert_series_equal(hpat_func(S, data_left, data_right),\n                                                   test_impl(S, data_left, data_right))\n\n    @skip_parallel\n    @skip_sdc_jit(\'Old-style implementation of operators doesn\\\'t support comparing Series of different lengths\')\n    def test_series_op1_integer(self):\n        """"""Verifies using all various Series arithmetic binary operators on two integer Series with default indexes""""""\n        n = 11\n        np.random.seed(0)\n        data_to_test = [np.arange(-5, -5 + n, dtype=np.int32),\n                        np.ones(n + 3, dtype=np.int32),\n                        np.random.randint(-5, 5, n + 7)]\n\n        arithmetic_binops = (\'+\', \'-\', \'*\', \'/\', \'//\', \'%\', \'**\')\n        for operator in arithmetic_binops:\n            test_impl = _make_func_use_binop1(operator)\n            hpat_func = self.jit(test_impl)\n            for data_left, data_right in combinations_with_replacement(data_to_test, 2):\n                # integers to negative powers are not allowed\n                if (operator == \'**\' and np.any(data_right < 0)):\n                    data_right = np.abs(data_right)\n\n                with self.subTest(left=data_left, right=data_right, operator=operator):\n                    S1 = pd.Series(data_left)\n                    S2 = pd.Series(data_right)\n                    # check_dtype=False because SDC implementation always returns float64 Series\n                    pd.testing.assert_series_equal(hpat_func(S1, S2), test_impl(S1, S2), check_dtype=False)\n\n    @skip_parallel\n    @skip_sdc_jit(\'Old-style implementation of operators doesn\\\'t support division/modulo/etc by zero\')\n    def test_series_op2_integer(self):\n        """"""Verifies using all various Series arithmetic binary operators\n           on an integer Series with default index and a scalar value""""""\n        n = 11\n        np.random.seed(0)\n        data_to_test = [np.arange(-5, -5 + n, dtype=np.int32),\n                        np.ones(n + 3, dtype=np.int32),\n                        np.random.randint(-5, 5, n + 7)]\n        scalar_values = [1, -1, 0, 3, 7, -5]\n\n        arithmetic_binops = (\'+\', \'-\', \'*\', \'/\', \'//\', \'%\', \'**\')\n        for operator in arithmetic_binops:\n            test_impl = _make_func_use_binop1(operator)\n            hpat_func = self.jit(test_impl)\n            for data_left in data_to_test:\n                for scalar in scalar_values:\n                    # integers to negative powers are not allowed\n                    if (operator == \'**\' and scalar < 0):\n                        scalar = abs(scalar)\n\n                    with self.subTest(left=data_left, right=scalar, operator=operator):\n                        S = pd.Series(data_left)\n                        # check_dtype=False because SDC implementation always returns float64 Series\n                        pd.testing.assert_series_equal(hpat_func(S, scalar), test_impl(S, scalar), check_dtype=False)\n\n    @skip_parallel\n    @skip_sdc_jit(\'Old-style implementation of operators doesn\\\'t support comparing Series of different lengths\')\n    def test_series_op1_float(self):\n        """"""Verifies using all various Series arithmetic binary operators on two float Series with default indexes""""""\n        n = 11\n        np.random.seed(0)\n        data_to_test = [np.arange(-5, -5 + n, dtype=np.float32),\n                        np.ones(n + 3, dtype=np.float32),\n                        np.random.ranf(n + 7)]\n\n        arithmetic_binops = (\'+\', \'-\', \'*\', \'/\', \'//\', \'%\', \'**\')\n        for operator in arithmetic_binops:\n            test_impl = _make_func_use_binop1(operator)\n            hpat_func = self.jit(test_impl)\n            for data_left, data_right in combinations_with_replacement(data_to_test, 2):\n                with self.subTest(left=data_left, right=data_right, operator=operator):\n                    S1 = pd.Series(data_left)\n                    S2 = pd.Series(data_right)\n                    # check_dtype=False because SDC implementation always returns float64 Series\n                    pd.testing.assert_series_equal(hpat_func(S1, S2), test_impl(S1, S2), check_dtype=False)\n\n    @skip_parallel\n    @skip_sdc_jit(\'Old-style implementation of operators doesn\\\'t support division/modulo/etc by zero\')\n    def test_series_op2_float(self):\n        """"""Verifies using all various Series arithmetic binary operators\n           on a float Series with default index and a scalar value""""""\n        n = 11\n        np.random.seed(0)\n        data_to_test = [np.arange(-5, -5 + n, dtype=np.float32),\n                        np.ones(n + 3, dtype=np.float32),\n                        np.random.ranf(n + 7)]\n        scalar_values = [1., -1., 0., -0., 7., -5.]\n\n        arithmetic_binops = (\'+\', \'-\', \'*\', \'/\', \'//\', \'%\', \'**\')\n        for operator in arithmetic_binops:\n            test_impl = _make_func_use_binop1(operator)\n            hpat_func = self.jit(test_impl)\n            for data_left in data_to_test:\n                for scalar in scalar_values:\n                    with self.subTest(left=data_left, right=scalar, operator=operator):\n                        S = pd.Series(data_left)\n                        pd.testing.assert_series_equal(hpat_func(S, scalar), test_impl(S, scalar), check_dtype=False)\n\n    @skip_parallel\n    @skip_numba_jit(\'Not implemented in new-pipeline yet\')\n    def test_series_op3(self):\n        arithmetic_binops = (\'+=\', \'-=\', \'*=\', \'/=\', \'//=\', \'%=\', \'**=\')\n\n        for operator in arithmetic_binops:\n            test_impl = _make_func_use_binop2(operator)\n            hpat_func = self.jit(test_impl)\n\n            # TODO: extend to test arithmetic operations between numeric Series of different dtypes\n            n = 11\n            A1 = pd.Series(np.arange(1, n, dtype=np.float64), name=\'A\')\n            A2 = A1.copy(deep=True)\n            B = pd.Series(np.ones(n - 1), name=\'B\')\n            hpat_func(A1, B)\n            test_impl(A2, B)\n            pd.testing.assert_series_equal(A1, A2)\n\n    @skip_parallel\n    @skip_numba_jit(\'Not implemented in new-pipeline yet\')\n    def test_series_op4(self):\n        arithmetic_binops = (\'+=\', \'-=\', \'*=\', \'/=\', \'//=\', \'%=\', \'**=\')\n\n        for operator in arithmetic_binops:\n            test_impl = _make_func_use_binop2(operator)\n            hpat_func = self.jit(test_impl)\n\n            # TODO: extend to test arithmetic operations between numeric Series of different dtypes\n            n = 11\n            S1 = pd.Series(np.arange(1, n, dtype=np.float64), name=\'A\')\n            S2 = S1.copy(deep=True)\n            hpat_func(S1, 1)\n            test_impl(S2, 1)\n            pd.testing.assert_series_equal(S1, S2)\n\n    @skip_parallel\n    def test_series_op5(self):\n        arithmetic_methods = (\'add\', \'sub\', \'mul\', \'div\', \'truediv\', \'floordiv\', \'mod\', \'pow\')\n\n        for method in arithmetic_methods:\n            test_impl = _make_func_use_method_arg1(method)\n            hpat_func = self.jit(test_impl)\n\n            n = 11\n            S1 = pd.Series(np.arange(1, n), name=\'A\')\n            S2 = pd.Series(np.ones(n - 1), name=\'B\')\n            pd.testing.assert_series_equal(hpat_func(S1, S2), test_impl(S1, S2))\n\n    # SDC operator methods returns only float Series\n    @skip_parallel\n    def test_series_op5_integer_scalar(self):\n        arithmetic_methods = (\'add\', \'sub\', \'mul\', \'div\', \'truediv\', \'floordiv\', \'mod\', \'pow\')\n\n        for method in arithmetic_methods:\n            test_impl = _make_func_use_method_arg1(method)\n            hpat_func = self.jit(test_impl)\n\n            n = 11\n            if platform.system() == \'Windows\' and not IS_32BITS:\n                operand_series = pd.Series(np.arange(1, n, dtype=np.int64))\n            else:\n                operand_series = pd.Series(np.arange(1, n))\n            operand_scalar = 5\n            pd.testing.assert_series_equal(\n                hpat_func(operand_series, operand_scalar),\n                test_impl(operand_series, operand_scalar),\n                check_names=False, check_dtype=False)\n\n    @skip_parallel\n    def test_series_op5_float_scalar(self):\n        arithmetic_methods = (\'add\', \'sub\', \'mul\', \'div\', \'truediv\', \'floordiv\', \'mod\', \'pow\')\n\n        for method in arithmetic_methods:\n            test_impl = _make_func_use_method_arg1(method)\n            hpat_func = self.jit(test_impl)\n\n            n = 11\n            operand_series = pd.Series(np.arange(1, n))\n            operand_scalar = .5\n            pd.testing.assert_series_equal(\n                hpat_func(operand_series, operand_scalar),\n                test_impl(operand_series, operand_scalar),\n                check_names=False)\n\n    @skip_parallel\n    @skip_numba_jit\n    def test_series_op6(self):\n        def test_impl(A):\n            return -A\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        A = pd.Series(np.arange(n))\n        pd.testing.assert_series_equal(hpat_func(A), test_impl(A))\n\n    @skip_parallel\n    @skip_sdc_jit(\'Old-style implementation of operators doesn\\\'t support Series indexes\')\n    def test_series_op7(self):\n        """"""Verifies using all various Series comparison binary operators on two integer Series with various indexes""""""\n        n = 11\n        data_left = [1, 2, -1, 3, 4, 2, -3, 5, 6, 6, 0]\n        data_right = [3, 2, -2, 1, 4, 1, -5, 6, 6, 3, -1]\n        dtype_to_index = {\'None\': None,\n                          \'int\': np.arange(n, dtype=\'int\'),\n                          \'float\': np.arange(n, dtype=\'float\'),\n                          \'string\': [\'aa\', \'aa\', \'\', \'\', \'b\', \'b\', \'cccc\', None, \'dd\', \'ddd\', None]}\n\n        comparison_binops = (\'<\', \'>\', \'<=\', \'>=\', \'!=\', \'==\')\n        for operator in comparison_binops:\n            test_impl = _make_func_use_binop1(operator)\n            hpat_func = self.jit(test_impl)\n            for dtype, index_data in dtype_to_index.items():\n                with self.subTest(operator=operator, index_dtype=dtype, index=index_data):\n                    A = pd.Series(data_left, index=index_data)\n                    B = pd.Series(data_right, index=index_data)\n                    pd.testing.assert_series_equal(hpat_func(A, B), test_impl(A, B))\n\n    @skip_parallel\n    @skip_sdc_jit(\'Old-style implementation of operators doesn\\\'t support comparing to inf\')\n    def test_series_op7_scalar(self):\n        """"""Verifies using all various Series comparison binary operators on an integer Series and scalar values""""""\n        S = pd.Series([1, 2, -1, 3, 4, 2, -3, 5, 6, 6, 0])\n\n        scalar_values = [2, 2.0, -3, np.inf, -np.inf, np.PZERO, np.NZERO]\n        comparison_binops = (\'<\', \'>\', \'<=\', \'>=\', \'!=\', \'==\')\n        for operator in comparison_binops:\n            test_impl = _make_func_use_binop1(operator)\n            hpat_func = self.jit(test_impl)\n            for scalar in scalar_values:\n                with self.subTest(left=S, right=scalar, operator=operator):\n                    pd.testing.assert_series_equal(hpat_func(S, scalar), test_impl(S, scalar))\n\n    @skip_parallel\n    def test_series_op8(self):\n        comparison_methods = (\'lt\', \'gt\', \'le\', \'ge\', \'ne\', \'eq\')\n\n        for method in comparison_methods:\n            test_impl = _make_func_use_method_arg1(method)\n            hpat_func = self.jit(test_impl)\n\n            n = 11\n            A = pd.Series(np.arange(n))\n            B = pd.Series(np.arange(n)**2)\n            pd.testing.assert_series_equal(hpat_func(A, B), test_impl(A, B), check_names=False)\n\n    @skip_parallel\n    @unittest.skipIf(platform.system() == \'Windows\', ""Attribute dtype are different: int64, int32"")\n    def test_series_op8_integer_scalar(self):\n        comparison_methods = (\'lt\', \'gt\', \'le\', \'ge\', \'eq\', \'ne\')\n\n        for method in comparison_methods:\n            test_impl = _make_func_use_method_arg1(method)\n            hpat_func = self.jit(test_impl)\n\n            n = 11\n            operand_series = pd.Series(np.arange(1, n))\n            operand_scalar = 10\n            pd.testing.assert_series_equal(\n                hpat_func(operand_series, operand_scalar),\n                test_impl(operand_series, operand_scalar),\n                check_names=False)\n\n    @skip_parallel\n    def test_series_op8_float_scalar(self):\n        comparison_methods = (\'lt\', \'gt\', \'le\', \'ge\', \'eq\', \'ne\')\n\n        for method in comparison_methods:\n            test_impl = _make_func_use_method_arg1(method)\n            hpat_func = self.jit(test_impl)\n\n            n = 11\n            operand_series = pd.Series(np.arange(1, n))\n            operand_scalar = .5\n            pd.testing.assert_series_equal(\n                hpat_func(operand_series, operand_scalar),\n                test_impl(operand_series, operand_scalar),\n                check_names=False)\n\n    @skip_parallel\n    @skip_numba_jit\n    def test_series_inplace_binop_array(self):\n        def test_impl(A, B):\n            A += B\n            return A\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        A = np.arange(n)**2.0  # TODO: use 2 for test int casting\n        B = pd.Series(np.ones(n))\n        np.testing.assert_array_equal(hpat_func(A.copy(), B), test_impl(A, B))\n\n    if sdc.config.config_pipeline_hpat_default:\n        def test_series_fusion1(self):\n            def test_impl(A, B):\n                return A + B + 1\n            hpat_func = self.jit(test_impl)\n\n            n = 11\n            if platform.system() == \'Windows\' and not IS_32BITS:\n                A = pd.Series(np.arange(n), dtype=np.int64)\n                B = pd.Series(np.arange(n)**2, dtype=np.int64)\n            else:\n                A = pd.Series(np.arange(n))\n                B = pd.Series(np.arange(n)**2)\n            pd.testing.assert_series_equal(hpat_func(A, B), test_impl(A, B))\n            self.assertEqual(count_parfor_REPs(), 1)\n    else:\n        @skip_numba_jit(\'Functionally test passes, but in old-style it checked fusion of parfors.\\n\'\n                        \'TODO: implement the same checks in new-pipeline\')\n        def test_series_fusion1(self):\n            def test_impl(A, B):\n                return A + B + 1\n            hpat_func = self.jit(test_impl)\n\n            n = 11\n            A = pd.Series(np.arange(n))\n            B = pd.Series(np.arange(n)**2)\n            pd.testing.assert_series_equal(hpat_func(A, B), test_impl(A, B), check_dtype=False)\n            # self.assertEqual(count_parfor_REPs(), 1)\n\n    if sdc.config.config_pipeline_hpat_default:\n        def test_series_fusion2(self):\n            # make sure getting data var avoids incorrect single def assumption\n            def test_impl(A, B):\n                S = B + 2\n                if A[0] == 0:\n                    S = A + 1\n                return S + B\n            hpat_func = self.jit(test_impl)\n\n            n = 11\n            if platform.system() == \'Windows\' and not IS_32BITS:\n                A = pd.Series(np.arange(n), dtype=np.int64)\n                B = pd.Series(np.arange(n)**2, dtype=np.int64)\n            else:\n                A = pd.Series(np.arange(n))\n                B = pd.Series(np.arange(n)**2)\n            pd.testing.assert_series_equal(hpat_func(A, B), test_impl(A, B))\n            self.assertEqual(count_parfor_REPs(), 3)\n    else:\n        @skip_numba_jit(\'Functionally test passes, but in old-style it checked fusion of parfors.\\n\'\n                        \'TODO: implement the same checks in new-pipeline\')\n        def test_series_fusion2(self):\n            def test_impl(A, B):\n                S = B + 2\n                if A.iat[0] == 0:\n                    S = A + 1\n                return S + B\n            hpat_func = self.jit(test_impl)\n\n            n = 11\n            A = pd.Series(np.arange(n))\n            B = pd.Series(np.arange(n)**2)\n            pd.testing.assert_series_equal(hpat_func(A, B), test_impl(A, B), check_dtype=False)\n            # self.assertEqual(count_parfor_REPs(), 3)\n\n    def test_series_len(self):\n        def test_impl(A):\n            return len(A)\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        S = pd.Series(np.arange(n), name=\'A\')\n        self.assertEqual(hpat_func(S), test_impl(S))\n\n    def test_series_box(self):\n        def test_impl():\n            A = pd.Series([1, 2, 3])\n            return A\n        hpat_func = self.jit(test_impl)\n\n        pd.testing.assert_series_equal(hpat_func(), test_impl())\n\n    def test_series_box2(self):\n        def test_impl():\n            A = pd.Series([\'1\', \'2\', \'3\'])\n            return A\n        hpat_func = self.jit(test_impl)\n\n        pd.testing.assert_series_equal(hpat_func(), test_impl())\n\n    def test_series_list_str_unbox1(self):\n        def test_impl(A):\n            return A.iloc[0]\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([[\'aa\', \'b\'], [\'ccc\'], []])\n        np.testing.assert_array_equal(hpat_func(S), test_impl(S))\n\n        # call twice to test potential refcount errors\n        np.testing.assert_array_equal(hpat_func(S), test_impl(S))\n\n    @skip_sdc_jit(\'Not impl in old style\')\n    def test_series_iloc_array(self):\n        def test_impl(A, n):\n            return A.iloc[n]\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([1, 2, 4, 8, 6, 0], [1, 2, 4, 8, 6, 0])\n        n = np.array([0, 4, 2])\n        pd.testing.assert_series_equal(hpat_func(S, n), test_impl(S, n))\n\n    @skip_sdc_jit(\'Not impl in old style\')\n    def test_series_iloc_callable(self):\n        def test_impl(S):\n            return S.iloc[(lambda a: abs(4 - a))]\n        hpat_func = self.jit(test_impl)\n        S = pd.Series([0, 6, 4, 7, 8], [0, 6, 66, 6, 8])\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    def test_np_typ_call_replace(self):\n        # calltype replacement is tricky for np.typ() calls since variable\n        # type can\'t provide calltype\n        def test_impl(i):\n            return np.int32(i)\n        hpat_func = self.jit(test_impl)\n\n        self.assertEqual(hpat_func(1), test_impl(1))\n\n    @skip_numba_jit(\'TODO: implement np.call on Series in new-pipeline\')\n    def test_series_ufunc1(self):\n        def test_impl(A):\n            return np.isinf(A).values\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        S = pd.Series(np.arange(n), name=\'A\')\n        np.testing.assert_array_equal(hpat_func(S), test_impl(S))\n\n    @skip_numba_jit(\'TODO: implement np.call on Series in new-pipeline\')\n    @skip_sdc_jit(""needs empty_like typing fix in npydecl.py"")\n    def test_series_empty_like(self):\n        def test_impl(A):\n            return np.empty_like(A)\n        hpat_func = self.jit(test_impl)\n        n = 11\n        S = pd.Series(np.arange(n), name=\'A\')\n        result = hpat_func(S)\n        self.assertTrue(isinstance(result, np.ndarray))\n        self.assertTrue(result.dtype is S.dtype)\n\n    @skip_sdc_jit(\'No support of axis argument in old-style Series.fillna() impl\')\n    def test_series_fillna_axis1(self):\n        """"""Verifies Series.fillna() implementation handles \'index\' as axis argument""""""\n        def test_impl(S):\n            return S.fillna(5.0, axis=\'index\')\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([1.0, 2.0, np.nan, 1.0, np.inf])\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    @skip_sdc_jit(\'No support of axis argument in old-style Series.fillna() impl\')\n    def test_series_fillna_axis2(self):\n        """"""Verifies Series.fillna() implementation handles 0 as axis argument""""""\n        def test_impl(S):\n            return S.fillna(5.0, axis=0)\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([1.0, 2.0, np.nan, 1.0, np.inf])\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    @skip_sdc_jit(\'No support of axis argument in old-style Series.fillna() impl\')\n    def test_series_fillna_axis3(self):\n        """"""Verifies Series.fillna() implementation handles correct non-literal axis argument""""""\n        def test_impl(S, axis):\n            return S.fillna(5.0, axis=axis)\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([1.0, 2.0, np.nan, 1.0, np.inf])\n        for axis in [0, \'index\']:\n            pd.testing.assert_series_equal(hpat_func(S, axis), test_impl(S, axis))\n\n    @skip_sdc_jit(\'BUG: old-style fillna impl returns series without index\')\n    def test_series_fillna_float(self):\n        """"""Verifies Series.fillna() applied to a named float Series with default index""""""\n        def test_impl(S):\n            return S.fillna(5.0)\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([1.0, 2.0, np.nan, 1.0, np.inf], name=\'A\')\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    @skip_sdc_jit(\'BUG: old-style fillna impl returns series without index\')\n    def test_series_fillna_float_index1(self):\n        """"""Verifies Series.fillna() implementation for float series with default index""""""\n        def test_impl(S):\n            return S.fillna(5.0)\n        hpat_func = self.jit(test_impl)\n\n        for data in test_global_input_data_float64:\n            S = pd.Series(data)\n            pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    @skip_sdc_jit(\'BUG: old-style fillna impl returns series without index\')\n    def test_series_fillna_float_index2(self):\n        """"""Verifies Series.fillna() implementation for float series with string index""""""\n        def test_impl(S):\n            return S.fillna(5.0)\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([1.0, 2.0, np.nan, 1.0, np.inf], [\'a\', \'b\', \'c\', \'d\', \'e\'])\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    @skip_sdc_jit(\'BUG: old-style fillna impl returns series without index\')\n    def test_series_fillna_float_index3(self):\n        def test_impl(S):\n            return S.fillna(5.0)\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([1.0, 2.0, np.nan, 1.0, np.inf], index=[1, 2, 5, 7, 10])\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    @skip_parallel\n    @skip_inline\n    @skip_sdc_jit(\'BUG: old-style fillna impl returns series without index\')\n    def test_series_fillna_str_index1(self):\n        """"""Verifies Series.fillna() applied to a named string Series with default index""""""\n        def test_impl(S):\n            return S.fillna(""dd"")\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([\'aa\', \'b\', None, \'cccd\', \'\'], name=\'A\')\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    @skip_parallel\n    @skip_inline\n    @skip_sdc_jit(\'BUG: old-style fillna impl returns series without index\')\n    def test_series_fillna_str_index2(self):\n        """"""Verifies Series.fillna() implementation for series of strings with string index""""""\n        def test_impl(S):\n            return S.fillna(""dd"")\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([\'aa\', \'b\', None, \'cccd\', \'\'], [\'a\', \'b\', \'c\', \'d\', \'e\'])\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    @skip_parallel\n    @skip_inline\n    @skip_sdc_jit(\'BUG: old-style fillna impl returns series without index\')\n    def test_series_fillna_str_index3(self):\n        def test_impl(S):\n            return S.fillna(""dd"")\n\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([\'aa\', \'b\', None, \'cccd\', \'\'], index=[1, 2, 5, 7, 10])\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    @skip_parallel\n    @skip_sdc_jit(\'BUG: old-style fillna impl returns series without index\')\n    def test_series_fillna_float_inplace1(self):\n        """"""Verifies Series.fillna() implementation for float series with default index and inplace argument True""""""\n        def test_impl(S):\n            S.fillna(5.0, inplace=True)\n            return S\n        hpat_func = self.jit(test_impl)\n\n        S1 = pd.Series([1.0, 2.0, np.nan, 1.0, np.inf])\n        S2 = S1.copy()\n        pd.testing.assert_series_equal(hpat_func(S1), test_impl(S2))\n\n    @skip_parallel\n    @unittest.skip(\'TODO: add reflection support and check method return value\')\n    def test_series_fillna_float_inplace2(self):\n        """"""Verifies Series.fillna(inplace=True) results are reflected back in the original float series""""""\n        def test_impl(S):\n            return S.fillna(inplace=True)\n        hpat_func = self.jit(test_impl)\n\n        S1 = pd.Series([1.0, 2.0, np.nan, 1.0, np.inf])\n        S2 = S1.copy()\n        self.assertIsNone(hpat_func(S1))\n        self.assertIsNone(test_impl(S2))\n        pd.testing.assert_series_equal(S1, S2)\n\n    @skip_parallel\n    def test_series_fillna_float_inplace3(self):\n        """"""Verifies Series.fillna() implementation correcly handles omitted inplace argument as default False""""""\n        def test_impl(S):\n            return S.fillna(5.0)\n        hpat_func = self.jit(test_impl)\n\n        S1 = pd.Series([1.0, 2.0, np.nan, 1.0, np.inf])\n        S2 = S1.copy()\n        pd.testing.assert_series_equal(hpat_func(S1), test_impl(S1))\n        pd.testing.assert_series_equal(S1, S2)\n\n    @skip_parallel\n    def test_series_fillna_inplace_non_literal(self):\n        """"""Verifies Series.fillna() implementation handles only Boolean literals as inplace argument""""""\n        def test_impl(S, param):\n            S.fillna(5.0, inplace=param)\n            return S\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([1.0, 2.0, np.nan, 1.0, np.inf])\n        expected = ValueError if sdc.config.config_pipeline_hpat_default else TypingError\n        self.assertRaises(expected, hpat_func, S, True)\n\n    @skip_parallel\n    @skip_numba_jit(\'TODO: investigate why Numba types inplace as bool (non-literal value)\')\n    def test_series_fillna_str_inplace1(self):\n        """"""Verifies Series.fillna() implementation for series of strings\n           with default index and inplace argument True\n        """"""\n        def test_impl(S):\n            S.fillna(""dd"", inplace=True)\n            return S\n        hpat_func = self.jit(test_impl)\n\n        S1 = pd.Series([\'aa\', \'b\', None, \'cccd\', \'\'])\n        S2 = S1.copy()\n        pd.testing.assert_series_equal(hpat_func(S1), test_impl(S2))\n\n    @unittest.skip(\'TODO (both): support StringArrayType reflection\'\n                   \'TODO (new-style): investigate why Numba infers inplace type as bool (non-literal value)\')\n    def test_series_fillna_str_inplace2(self):\n        """"""Verifies Series.fillna(inplace=True) results are reflected back in the original string series""""""\n        def test_impl(S):\n            return S.fillna(""dd"", inplace=True)\n        hpat_func = self.jit(test_impl)\n\n        S1 = pd.Series([\'aa\', \'b\', None, \'cccd\', \'\'])\n        S2 = S1.copy()\n        self.assertIsNone(hpat_func(S1))\n        self.assertIsNone(test_impl(S2))\n        pd.testing.assert_series_equal(S1, S2)\n\n    @skip_parallel\n    @skip_numba_jit(\'TODO: investigate why Numba types inplace as bool (non-literal value)\')\n    def test_series_fillna_str_inplace_empty1(self):\n        def test_impl(A):\n            A.fillna("""", inplace=True)\n            return A\n        hpat_func = self.jit(test_impl)\n\n        S1 = pd.Series([\'aa\', \'b\', None, \'cccd\', \'\'])\n        S2 = S1.copy()\n        pd.testing.assert_series_equal(hpat_func(S1), test_impl(S2))\n\n    @unittest.skip(\'AssertionError: Series are different\\n\'\n                   \'Series length are different\\n\'\n                   \'[left]:  [NaT, 1970-12-01T00:00:00.000000000, 2012-07-25T00:00:00.000000000]\\n\'\n                   \'[right]: [2020-05-03T00:00:00.000000000, 1970-12-01T00:00:00.000000000, 2012-07-25T00:00:00.000000000]\')\n    def test_series_fillna_dt_no_index1(self):\n        """"""Verifies Series.fillna() implementation for datetime series and np.datetime64 value""""""\n        def test_impl(S, value):\n            return S.fillna(value)\n        hpat_func = self.jit(test_impl)\n\n        value = np.datetime64(\'2020-05-03\', \'ns\')\n        S = pd.Series([pd.NaT, pd.Timestamp(\'1970-12-01\'), pd.Timestamp(\'2012-07-25\'), None])\n        pd.testing.assert_series_equal(hpat_func(S, value), test_impl(S, value))\n\n    @unittest.skip(\'TODO: change unboxing of pd.Timestamp Series or support conversion between PandasTimestampType and datetime64\')\n    def test_series_fillna_dt_no_index2(self):\n        """"""Verifies Series.fillna() implementation for datetime series and pd.Timestamp value""""""\n        def test_impl(S):\n            value = pd.Timestamp(\'2020-05-03\')\n            return S.fillna(value)\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([pd.NaT, pd.Timestamp(\'1970-12-01\'), pd.Timestamp(\'2012-07-25\')])\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    @skip_parallel\n    def test_series_fillna_bool_no_index1(self):\n        """"""Verifies Series.fillna() implementation for bool series with default index""""""\n        def test_impl(S):\n            return S.fillna(True)\n        hpat_func = self.jit(test_impl)\n\n        S1 = pd.Series([True, False, False, True])\n        S2 = S1.copy()\n        pd.testing.assert_series_equal(hpat_func(S1), test_impl(S2))\n\n    @skip_parallel\n    @skip_sdc_jit(\'BUG: old-style fillna impl returns series without index\')\n    def test_series_fillna_int_no_index1(self):\n        """"""Verifies Series.fillna() implementation for integer series with default index""""""\n        def test_impl(S):\n            return S.fillna(7)\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        S1 = pd.Series(np.arange(n, dtype=np.int64))\n        S2 = S1.copy()\n        pd.testing.assert_series_equal(hpat_func(S1), test_impl(S2))\n\n    @skip_sdc_jit(\'No support of axis argument in old-style Series.dropna() impl\')\n    def test_series_dropna_axis1(self):\n        """"""Verifies Series.dropna() implementation handles \'index\' as axis argument""""""\n        def test_impl(S):\n            return S.dropna(axis=\'index\')\n        hpat_func = self.jit(test_impl)\n\n        S1 = pd.Series([1.0, 2.0, np.nan, 1.0, np.inf])\n        S2 = S1.copy()\n        pd.testing.assert_series_equal(hpat_func(S1), test_impl(S2))\n\n    @skip_sdc_jit(\'No support of axis argument in old-style Series.dropna() impl\')\n    def test_series_dropna_axis2(self):\n        """"""Verifies Series.dropna() implementation handles 0 as axis argument""""""\n        def test_impl(S):\n            return S.dropna(axis=0)\n        hpat_func = self.jit(test_impl)\n\n        S1 = pd.Series([1.0, 2.0, np.nan, 1.0, np.inf])\n        S2 = S1.copy()\n        pd.testing.assert_series_equal(hpat_func(S1), test_impl(S2))\n\n    @skip_sdc_jit(\'No support of axis argument in old-style Series.dropna() impl\')\n    def test_series_dropna_axis3(self):\n        """"""Verifies Series.dropna() implementation handles correct non-literal axis argument""""""\n        def test_impl(S, axis):\n            return S.dropna(axis=axis)\n        hpat_func = self.jit(test_impl)\n\n        S1 = pd.Series([1.0, 2.0, np.nan, 1.0, np.inf])\n        S2 = S1.copy()\n        for axis in [0, \'index\']:\n            pd.testing.assert_series_equal(hpat_func(S1, axis), test_impl(S2, axis))\n\n    @skip_sdc_jit(\'BUG: old-style dropna impl returns series without index\')\n    def test_series_dropna_float_index1(self):\n        """"""Verifies Series.dropna() implementation for float series with default index""""""\n        def test_impl(S):\n            return S.dropna()\n        hpat_func = self.jit(test_impl)\n\n        for data in test_global_input_data_float64:\n            S1 = pd.Series(data)\n            S2 = S1.copy()\n            pd.testing.assert_series_equal(hpat_func(S1), test_impl(S2))\n\n    @skip_sdc_jit(\'BUG: old-style dropna impl returns series without index\')\n    def test_series_dropna_float_index2(self):\n        """"""Verifies Series.dropna() implementation for float series with string index""""""\n        def test_impl(S):\n            return S.dropna()\n        hpat_func = self.jit(test_impl)\n\n        S1 = pd.Series([1.0, 2.0, np.nan, 1.0, np.inf], [\'a\', \'b\', \'c\', \'d\', \'e\'])\n        S2 = S1.copy()\n        pd.testing.assert_series_equal(hpat_func(S1), test_impl(S2))\n\n    @skip_sdc_jit(\'BUG: old-style dropna impl returns series without index\')\n    def test_series_dropna_str_index1(self):\n        """"""Verifies Series.dropna() implementation for series of strings with default index""""""\n        def test_impl(S):\n            return S.dropna()\n        hpat_func = self.jit(test_impl)\n\n        S1 = pd.Series([\'aa\', \'b\', None, \'cccd\', \'\'])\n        S2 = S1.copy()\n        pd.testing.assert_series_equal(hpat_func(S1), test_impl(S2))\n\n    @skip_sdc_jit(\'BUG: old-style dropna impl returns series without index\')\n    def test_series_dropna_str_index2(self):\n        """"""Verifies Series.dropna() implementation for series of strings with string index""""""\n        def test_impl(S):\n            return S.dropna()\n        hpat_func = self.jit(test_impl)\n\n        S1 = pd.Series([\'aa\', \'b\', None, \'cccd\', \'\'], [\'a\', \'b\', \'c\', \'d\', \'e\'])\n        S2 = S1.copy()\n        pd.testing.assert_series_equal(hpat_func(S1), test_impl(S2))\n\n    @skip_sdc_jit(\'BUG: old-style dropna impl returns series without index\')\n    def test_series_dropna_str_index3(self):\n        def test_impl(S):\n            return S.dropna()\n\n        hpat_func = self.jit(test_impl)\n\n        S1 = pd.Series([\'aa\', \'b\', None, \'cccd\', \'\'], index=[1, 2, 5, 7, 10])\n        S2 = S1.copy()\n        pd.testing.assert_series_equal(hpat_func(S1), test_impl(S2))\n\n    @unittest.skip(\'BUG: old-style dropna impl returns series without index, in new-style inplace is unsupported\')\n    def test_series_dropna_float_inplace_no_index1(self):\n        """"""Verifies Series.dropna() implementation for float series with default index and inplace argument True""""""\n        def test_impl(S):\n            S.dropna(inplace=True)\n            return S\n        hpat_func = self.jit(test_impl)\n\n        S1 = pd.Series([1.0, 2.0, np.nan, 1.0, np.inf])\n        S2 = S1.copy()\n        pd.testing.assert_series_equal(hpat_func(S1), test_impl(S2))\n\n    @unittest.skip(\'TODO: add reflection support and check method return value\')\n    def test_series_dropna_float_inplace_no_index2(self):\n        """"""Verifies Series.dropna(inplace=True) results are reflected back in the original float series""""""\n        def test_impl(S):\n            return S.dropna(inplace=True)\n        hpat_func = self.jit(test_impl)\n\n        S1 = pd.Series([1.0, 2.0, np.nan, 1.0, np.inf])\n        S2 = S1.copy()\n        self.assertIsNone(hpat_func(S1))\n        self.assertIsNone(test_impl(S2))\n        pd.testing.assert_series_equal(S1, S2)\n\n    @unittest.skip(\'BUG: old-style dropna impl returns series without index, in new-style inplace is unsupported\')\n    def test_series_dropna_str_inplace_no_index1(self):\n        """"""Verifies Series.dropna() implementation for series of strings\n           with default index and inplace argument True\n        """"""\n        def test_impl(S):\n            S.dropna(inplace=True)\n            return S\n        hpat_func = self.jit(test_impl)\n\n        S1 = pd.Series([\'aa\', \'b\', None, \'cccd\', \'\'])\n        S2 = S1.copy()\n        pd.testing.assert_series_equal(hpat_func(S1), test_impl(S2))\n\n    @unittest.skip(\'TODO: add reflection support and check method return value\')\n    def test_series_dropna_str_inplace_no_index2(self):\n        """"""Verifies Series.dropna(inplace=True) results are reflected back in the original string series""""""\n        def test_impl(S):\n            return S.dropna(inplace=True)\n        hpat_func = self.jit(test_impl)\n\n        S1 = pd.Series([\'aa\', \'b\', None, \'cccd\', \'\'])\n        S2 = S1.copy()\n        self.assertIsNone(hpat_func(S1))\n        self.assertIsNone(test_impl(S2))\n        pd.testing.assert_series_equal(S1, S2)\n\n    @skip_numba_jit\n    def test_series_dropna_str_parallel1(self):\n        """"""Verifies Series.dropna() distributed work for series of strings with default index""""""\n        def test_impl(A):\n            B = A.dropna()\n            return (B == \'gg\').sum()\n        hpat_func = self.jit(distributed=[\'A\'])(test_impl)\n\n        S1 = pd.Series([\'aa\', \'b\', None, \'ccc\', \'dd\', \'gg\'])\n        start, end = get_start_end(len(S1))\n        # TODO: gatherv\n        self.assertEqual(hpat_func(S1[start:end]), test_impl(S1))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n        self.assertTrue(count_array_OneDs() > 0)\n\n    @unittest.skip(\'AssertionError: Series are different\\n\'\n                   \'Series length are different\\n\'\n                   \'[left]:  3, Int64Index([0, 1, 2], dtype=\\\'int64\\\')\\n\'\n                   \'[right]: 2, Int64Index([1, 2], dtype=\\\'int64\\\')\')\n    def test_series_dropna_dt_no_index1(self):\n        """"""Verifies Series.dropna() implementation for datetime series with default index""""""\n        def test_impl(S):\n            return S.dropna()\n        hpat_func = self.jit(test_impl)\n\n        S1 = pd.Series([pd.NaT, pd.Timestamp(\'1970-12-01\'), pd.Timestamp(\'2012-07-25\')])\n        S2 = S1.copy()\n        pd.testing.assert_series_equal(hpat_func(S1), test_impl(S2))\n\n    def test_series_dropna_bool_no_index1(self):\n        """"""Verifies Series.dropna() implementation for bool series with default index""""""\n        def test_impl(S):\n            return S.dropna()\n        hpat_func = self.jit(test_impl)\n\n        S1 = pd.Series([True, False, False, True])\n        S2 = S1.copy()\n        pd.testing.assert_series_equal(hpat_func(S1), test_impl(S2))\n\n    @skip_sdc_jit(\'BUG: old-style dropna impl returns series without index\')\n    def test_series_dropna_int_no_index1(self):\n        """"""Verifies Series.dropna() implementation for integer series with default index""""""\n        def test_impl(S):\n            return S.dropna()\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        S1 = pd.Series(np.arange(n, dtype=np.int64))\n        S2 = S1.copy()\n        pd.testing.assert_series_equal(hpat_func(S1), test_impl(S2))\n\n    def test_series_rename_str_noidx(self):\n        def test_impl(A):\n            return A.rename(\'B\')\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([1.0, 2.0, np.nan, 1.0], name=\'A\')\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    def test_series_rename_str_noidx_noname(self):\n        def test_impl(S):\n            return S.rename(\'Name\')\n        jit_func = self.jit(test_impl)\n\n        S = pd.Series([1, 2, 3])\n        pd.testing.assert_series_equal(jit_func(S), test_impl(S))\n\n    def test_series_rename_str_idx(self):\n        def test_impl(S):\n            return S.rename(\'Name\')\n        jit_func = self.jit(test_impl)\n\n        S = pd.Series([1, 2, 3], index=[\'a\', \'b\', \'c\'])\n        pd.testing.assert_series_equal(jit_func(S), test_impl(S))\n\n    def test_series_rename_no_name_str_noidx(self):\n        def test_impl(S):\n            return S.rename()\n        jit_func = self.jit(test_impl)\n\n        S = pd.Series([1, 2, 3], name=\'Name\')\n        pd.testing.assert_series_equal(jit_func(S), test_impl(S))\n\n    def test_series_rename_no_name_str_idx(self):\n        def test_impl(S):\n            return S.rename()\n        jit_func = self.jit(test_impl)\n\n        S = pd.Series([1, 2, 3], index=[\'a\', \'b\', \'c\'], name=\'Name\')\n        pd.testing.assert_series_equal(jit_func(S), test_impl(S))\n\n    def test_series_rename_str_noidx_no_copy(self):\n        def test_impl(S):\n            return S.rename(\'Another Name\', copy=False)\n        jit_func = self.jit(test_impl)\n\n        S = pd.Series([1, 2, 3], name=\'Name\')\n        pd.testing.assert_series_equal(jit_func(S), test_impl(S))\n\n    def test_series_rename_str_idx_no_copy(self):\n        def test_impl(S):\n            return S.rename(\'Another Name\', copy=False)\n        jit_func = self.jit(test_impl)\n\n        S = pd.Series([1, 2, 3], index=[\'a\', \'b\', \'c\'], name=\'Name\')\n        pd.testing.assert_series_equal(jit_func(S), test_impl(S))\n\n    @skip_sdc_jit(""Requires full scalar types (not only str) support as Series name"")\n    @skip_numba_jit(""Requires full scalar types (not only str) support as Series name"")\n    def test_series_rename_int_noidx(self):\n        def test_impl(S):\n            return S.rename(1)\n        jit_func = self.jit(test_impl)\n\n        S = pd.Series([1, 2, 3], name=\'Name\')\n        pd.testing.assert_series_equal(jit_func(S), test_impl(S))\n\n    @skip_sdc_jit(""Requires full scalar types (not only str) support as Series name"")\n    @skip_numba_jit(""Requires full scalar types (not only str) support as Series name"")\n    def test_series_rename_int_idx(self):\n        def test_impl(S):\n            return S.rename(1)\n        jit_func = self.jit(test_impl)\n\n        S = pd.Series([1, 2, 3], index=[\'a\', \'b\', \'c\'], name=\'Name\')\n        pd.testing.assert_series_equal(jit_func(S), test_impl(S))\n\n    @skip_sdc_jit(""Requires full scalar types (not only str) support as Series name"")\n    @skip_numba_jit(""Requires full scalar types (not only str) support as Series name"")\n    def test_series_rename_float_noidx(self):\n        def test_impl(S):\n            return S.rename(1.1)\n        jit_func = self.jit(test_impl)\n\n        S = pd.Series([1, 2, 3], name=\'Name\')\n        pd.testing.assert_series_equal(jit_func(S), test_impl(S))\n\n    @skip_sdc_jit(""Requires full scalar types (not only str) support as Series name"")\n    @skip_numba_jit(""Requires full scalar types (not only str) support as Series name"")\n    def test_series_rename_float_idx(self):\n        def test_impl(S):\n            return S.rename(1.1)\n        jit_func = self.jit(test_impl)\n\n        S = pd.Series([1, 2, 3], index=[\'a\', \'b\', \'c\'], name=\'Name\')\n        pd.testing.assert_series_equal(jit_func(S), test_impl(S))\n\n    def test_series_sum_default(self):\n        def test_impl(S):\n            return S.sum()\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([1., 2., 3.])\n        self.assertEqual(hpat_func(S), test_impl(S))\n\n    def test_series_sum_bool(self):\n        def test_impl(S):\n            return S.sum()\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([True, True, False])\n        self.assertEqual(hpat_func(S), test_impl(S))\n\n    def test_series_sum_nan(self):\n        def test_impl(S):\n            return S.sum()\n        hpat_func = self.jit(test_impl)\n\n        # column with NA\n        S = pd.Series([np.nan, 2., 3.])\n        self.assertEqual(hpat_func(S), test_impl(S))\n\n        # all NA case should produce 0\n        S = pd.Series([np.nan, np.nan])\n        self.assertEqual(hpat_func(S), test_impl(S))\n\n    @skip_sdc_jit(""Old style Series.sum() does not support parameters"")\n    def test_series_sum_skipna_false(self):\n        def test_impl(S):\n            return S.sum(skipna=False)\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([np.nan, 2., 3.])\n        self.assertEqual(np.isnan(hpat_func(S)), np.isnan(test_impl(S)))\n\n    def test_series_sum2(self):\n        def test_impl(S):\n            return (S + S).sum()\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([np.nan, 2., 3.])\n        self.assertEqual(hpat_func(S), test_impl(S))\n\n        S = pd.Series([np.nan, np.nan])\n        self.assertEqual(hpat_func(S), test_impl(S))\n\n    def test_series_prod(self):\n        def test_impl(S, skipna):\n            return S.prod(skipna=skipna)\n        hpat_func = self.jit(test_impl)\n\n        data_samples = [\n            [6, 6, 2, 1, 3, 3, 2, 1, 2],\n            [1.1, 0.3, 2.1, 1, 3, 0.3, 2.1, 1.1, 2.2],\n            [6, 6.1, 2.2, 1, 3, 3, 2.2, 1, 2],\n            [6, 6, np.nan, 2, np.nan, 1, 3, 3, np.inf, 2, 1, 2, np.inf],\n            [1.1, 0.3, np.nan, 1.0, np.inf, 0.3, 2.1, np.nan, 2.2, np.inf],\n            [1.1, 0.3, np.nan, 1, np.inf, 0, 1.1, np.nan, 2.2, np.inf, 2, 2],\n            [np.nan, np.nan, np.nan],\n            [np.nan, np.nan, np.inf],\n        ]\n\n        for data in data_samples:\n            S = pd.Series(data)\n\n            for skipna_var in [True, False]:\n                actual = hpat_func(S, skipna=skipna_var)\n                expected = test_impl(S, skipna=skipna_var)\n\n                if np.isnan(actual) or np.isnan(expected):\n                    # con not compare Nan != Nan directly\n                    self.assertEqual(np.isnan(actual), np.isnan(expected))\n                else:\n                    self.assertAlmostEqual(actual, expected)\n\n    def test_series_prod_skipna_default(self):\n        def test_impl(S):\n            return S.prod()\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([np.nan, 2, 3.])\n        self.assertEqual(hpat_func(S), test_impl(S))\n\n    def test_series_count1(self):\n        def test_impl(S):\n            return S.count()\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([np.nan, 2., 3.])\n        self.assertEqual(hpat_func(S), test_impl(S))\n\n        S = pd.Series([np.nan, np.nan])\n        self.assertEqual(hpat_func(S), test_impl(S))\n\n        S = pd.Series([\'aa\', \'bb\', np.nan])\n        self.assertEqual(hpat_func(S), test_impl(S))\n\n    def _mean_data_samples(self):\n        yield [6, 6, 2, 1, 3, 3, 2, 1, 2]\n        yield [1.1, 0.3, 2.1, 1, 3, 0.3, 2.1, 1.1, 2.2]\n        yield [6, 6.1, 2.2, 1, 3, 3, 2.2, 1, 2]\n        yield [6, 6, np.nan, 2, np.nan, 1, 3, 3, np.inf, 2, 1, 2, np.inf]\n        yield [1.1, 0.3, np.nan, 1.0, np.inf, 0.3, 2.1, np.nan, 2.2, np.inf]\n        yield [1.1, 0.3, np.nan, 1, np.inf, 0, 1.1, np.nan, 2.2, np.inf, 2, 2]\n        yield [np.nan, np.nan, np.nan]\n        yield [np.nan, np.nan, np.inf]\n\n    def _check_mean(self, pyfunc, *args):\n        cfunc = self.jit(pyfunc)\n\n        actual = cfunc(*args)\n        expected = pyfunc(*args)\n        if np.isnan(actual) or np.isnan(expected):\n            self.assertEqual(np.isnan(actual), np.isnan(expected))\n        else:\n            self.assertEqual(actual, expected)\n\n    def test_series_mean(self):\n        def test_impl(S):\n            return S.mean()\n\n        for data in self._mean_data_samples():\n            with self.subTest(data=data):\n                S = pd.Series(data)\n                self._check_mean(test_impl, S)\n\n    @skip_sdc_jit(""Series.mean() any parameters unsupported"")\n    def test_series_mean_skipna(self):\n        def test_impl(S, skipna):\n            return S.mean(skipna=skipna)\n\n        for skipna in [True, False]:\n            for data in self._mean_data_samples():\n                S = pd.Series(data)\n                self._check_mean(test_impl, S, skipna)\n\n    def test_series_var1(self):\n        def test_impl(S):\n            return S.var()\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([np.nan, 2., 3.])\n        self.assertEqual(hpat_func(S), test_impl(S))\n\n    def test_series_min(self):\n        def test_impl(S):\n            return S.min()\n        hpat_func = self.jit(test_impl)\n\n        # TODO type_min/type_max\n        for input_data in [\n                [np.nan, 2., np.nan, 3., np.inf, 1, -1000],\n                [8, 31, 1123, -1024],\n                [2., 3., 1, -1000, np.inf],\n                [np.nan, np.nan, np.inf, np.nan],\n            ]:\n            S = pd.Series(input_data)\n\n            result_ref = test_impl(S)\n            result = hpat_func(S)\n            self.assertEqual(result, result_ref)\n\n    @skip_sdc_jit(""Series.min() any parameters unsupported"")\n    def test_series_min_param(self):\n        def test_impl(S, param_skipna):\n            return S.min(skipna=param_skipna)\n\n        hpat_func = self.jit(test_impl)\n\n        for input_data, param_skipna in [([np.nan, 2., np.nan, 3., 1, -1000, np.inf], True),\n                                         ([2., 3., 1, np.inf, -1000], False)]:\n            S = pd.Series(input_data)\n\n            result_ref = test_impl(S, param_skipna)\n            result = hpat_func(S, param_skipna)\n            self.assertEqual(result, result_ref)\n\n    @unittest.expectedFailure\n    def test_series_min_param_fail(self):\n        def test_impl(S, param_skipna):\n            return S.min(skipna=param_skipna)\n\n        hpat_func = self.jit(test_impl)\n\n        cases = [\n            ([2., 3., 1, np.inf, -1000, np.nan], False),  # min == np.nan\n        ]\n\n        for input_data, param_skipna in cases:\n            S = pd.Series(input_data)\n\n            result_ref = test_impl(S, param_skipna)\n            result = hpat_func(S, param_skipna)\n            self.assertEqual(result, result_ref)\n\n    def test_series_max(self):\n        def test_impl(S):\n            return S.max()\n        hpat_func = self.jit(test_impl)\n\n        # TODO type_min/type_max\n        for input_data in [\n                [np.nan, 2., np.nan, 3., np.inf, 1, -1000],\n                [8, 31, 1123, -1024],\n                [2., 3., 1, -1000, np.inf],\n                [np.inf, np.inf, np.inf, np.inf],\n                [np.inf, np.nan, np.nan, np.nan],\n\n                [np.nan, np.nan, np.nan, np.nan],\n                [np.nan, 1.0, np.nan, np.nan],\n                [np.nan, 1.0, 1.0, np.nan],\n\n                [np.nan, np.nan, 1.0, np.nan],\n                [np.nan, np.nan, 1.0, np.nan, np.nan],\n\n                [np.nan, np.nan, np.inf, np.nan],\n                [np.nan, np.nan, np.inf, np.nan, np.nan],\n\n                [np.nan, np.nan, np.nan, np.inf],\n                np.arange(11),\n            ]:\n            with self.subTest(data=input_data):\n                S = pd.Series(input_data)\n\n                result_ref = test_impl(S)\n                result = hpat_func(S)\n                np.testing.assert_equal(result, result_ref)\n\n    @skip_sdc_jit(""Series.max() any parameters unsupported"")\n    def test_series_max_param(self):\n        def test_impl(S, param_skipna):\n            return S.max(skipna=param_skipna)\n\n        hpat_func = self.jit(test_impl)\n\n        for input_data, param_skipna in [([np.nan, 2., np.nan, 3., 1, -1000, np.inf], True),\n                                         ([2., 3., 1, np.inf, -1000], False)]:\n            S = pd.Series(input_data)\n\n            result_ref = test_impl(S, param_skipna)\n            result = hpat_func(S, param_skipna)\n            self.assertEqual(result, result_ref)\n\n    @skip_sdc_jit(\'Old-style value_counts implementation doesn\\\'t handle numpy.nan values\')\n    def test_series_value_counts_number(self):\n        def test_impl(S):\n            return S.value_counts()\n\n        input_data = [test_global_input_data_integer64, test_global_input_data_float64]\n        extras = [[1, 2, 3, 1, 1, 3], [0.1, 0., 0.1, 0.1]]\n\n        hpat_func = self.jit(test_impl)\n\n        for data_to_test, extra in zip(input_data, extras):\n            for d in data_to_test:\n                data = d + extra\n                with self.subTest(series_data=data):\n                    S = pd.Series(data)\n                    # use sort_index() due to possible different order of values with the same counts in results\n                    result_ref = test_impl(S).sort_index()\n                    result = hpat_func(S).sort_index()\n                    pd.testing.assert_series_equal(result, result_ref)\n\n    @skip_sdc_jit(\'Fails to compile with latest Numba\')\n    def test_series_value_counts_boolean(self):\n        def test_impl(S):\n            return S.value_counts()\n\n        input_data = [True, False, True, True, False]\n\n        sdc_func = self.jit(test_impl)\n\n        S = pd.Series(input_data)\n        result_ref = test_impl(S)\n        result = sdc_func(S)\n        pd.testing.assert_series_equal(result, result_ref)\n\n    @skip_sdc_jit(\'Bug in old-style value_counts implementation for ascending param support\')\n    def test_series_value_counts_sort(self):\n        def test_impl(S, value):\n            return S.value_counts(sort=True, ascending=value)\n\n        hpat_func = self.jit(test_impl)\n\n        data = [1, 0, 0, 1, 1, -1, 0, -1, 0]\n\n        for ascending in (False, True):\n            with self.subTest(ascending=ascending):\n                S = pd.Series(data)\n                # to test sorting of result series works correctly do not use sort_index() on results!\n                # instead ensure that there are no elements with the same frequency in the data\n                result_ref = test_impl(S, ascending)\n                result = hpat_func(S, ascending)\n                pd.testing.assert_series_equal(result, result_ref)\n\n    @skip_sdc_jit(\'Old-style value_counts implementation doesn\\\'t handle numpy.nan values\')\n    def test_series_value_counts_numeric_dropna_false(self):\n        def test_impl(S):\n            return S.value_counts(dropna=False)\n\n        data_to_test = [[1, 2, 3, 1, 1, 3],\n                        [1, 2, 3, np.nan, 1, 3, np.nan, np.inf],\n                        [0.1, 3., np.nan, 3., 0.1, 3., np.nan, np.inf, 0.1, 0.1]]\n\n        hpat_func = self.jit(test_impl)\n\n        for data in data_to_test:\n            with self.subTest(series_data=data):\n                S = pd.Series(data)\n                pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    @skip_sdc_jit(\'Old-style value_counts implementation doesn\\\'t handle None values in string series\')\n    def test_series_value_counts_str_dropna_false(self):\n        def test_impl(S):\n            return S.value_counts(dropna=False)\n\n        data_to_test = [[\'a\', \'\', \'a\', \'\', \'b\', None, \'a\', \'\', None, \'b\'],\n                        [\'dog\', None, \'NaN\', \'\', \'cat\', None, \'cat\', None, \'dog\', \'\'],\n                        [\'dog\', \'NaN\', \'\', \'cat\', \'cat\', \'dog\', \'\']]\n\n        hpat_func = self.jit(test_impl)\n\n        for data in data_to_test:\n            with self.subTest(series_data=data):\n                S = pd.Series(data)\n                # use sort_index() due to possible different order of values with the same counts in results\n                result_ref = test_impl(S).sort_index()\n                result = hpat_func(S).sort_index()\n                pd.testing.assert_series_equal(result, result_ref)\n\n    @skip_sdc_jit(\'Old-style value_counts implementation doesn\\\'t handle sort argument\')\n    def test_series_value_counts_str_sort(self):\n        def test_impl(S, ascending):\n            return S.value_counts(sort=True, ascending=ascending)\n\n        data_to_test = [[\'a\', \'\', \'a\', \'\', \'b\', None, \'a\', \'\', \'a\', \'b\'],\n                        [\'dog\', \'cat\', \'cat\', \'cat\', \'dog\']]\n\n        hpat_func = self.jit(test_impl)\n\n        for data in data_to_test:\n            for ascending in (True, False):\n                with self.subTest(series_data=data, ascending=ascending):\n                    S = pd.Series(data)\n                    # to test sorting of result series works correctly do not use sort_index() on results!\n                    # instead ensure that there are no elements with the same frequency in the data\n                    result_ref = test_impl(S, ascending)\n                    result = hpat_func(S, ascending)\n                    pd.testing.assert_series_equal(result, result_ref)\n\n    @skip_sdc_jit(""Fails to compile with latest Numba"")\n    def test_series_value_counts_index(self):\n        def test_impl(S):\n            return S.value_counts()\n\n        hpat_func = self.jit(test_impl)\n\n        for data in test_global_input_data_integer64:\n            with self.subTest(series_data=data):\n                index = np.arange(start=1, stop=len(data) + 1)\n                S = pd.Series(data, index=index)\n                pd.testing.assert_series_equal(hpat_func(S).sort_index(), test_impl(S).sort_index())\n\n    def test_series_value_counts_no_unboxing(self):\n        def test_impl():\n            S = pd.Series([1, 2, 3, 1, 1, 3])\n            return S.value_counts()\n\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_series_equal(hpat_func(), test_impl())\n\n    @skip_numba_jit\n    def test_series_dist_input1(self):\n        """"""Verify distribution of a Series without index""""""\n        def test_impl(S):\n            return S.max()\n        hpat_func = self.jit(distributed={\'S\'})(test_impl)\n\n        n = 111\n        S = pd.Series(np.arange(n))\n        start, end = get_start_end(n)\n        self.assertEqual(hpat_func(S[start:end]), test_impl(S))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @skip_sdc_jit(""Fails to compile with latest Numba"")\n    @skip_numba_jit\n    def test_series_dist_input2(self):\n        """"""Verify distribution of a Series with integer index""""""\n        def test_impl(S):\n            return S.max()\n        hpat_func = self.jit(distributed={\'S\'})(test_impl)\n\n        n = 111\n        S = pd.Series(np.arange(n), 1 + np.arange(n))\n        start, end = get_start_end(n)\n        self.assertEqual(hpat_func(S[start:end]), test_impl(S[start:end]))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    @unittest.skip(""Passed if run single"")\n    def test_series_dist_input3(self):\n        """"""Verify distribution of a Series with string index""""""\n        def test_impl(S):\n            return S.max()\n        hpat_func = self.jit(distributed={\'S\'})(test_impl)\n\n        n = 111\n        S = pd.Series(np.arange(n), [\'abc{}\'.format(id) for id in range(n)])\n        start, end = get_start_end(n)\n        self.assertEqual(hpat_func(S[start:end]), test_impl(S))\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n\n    def test_series_tuple_input1(self):\n        def test_impl(s_tup):\n            return s_tup[0].max()\n        hpat_func = self.jit(test_impl)\n\n        n = 111\n        S = pd.Series(np.arange(n))\n        S2 = pd.Series(np.arange(n) + 1.0)\n        s_tup = (S, 1, S2)\n        self.assertEqual(hpat_func(s_tup), test_impl(s_tup))\n\n    @unittest.skip(""pending handling of build_tuple in dist pass"")\n    def test_series_tuple_input_dist1(self):\n        def test_impl(s_tup):\n            return s_tup[0].max()\n        hpat_func = self.jit(locals={\'s_tup:input\': \'distributed\'})(test_impl)\n\n        n = 111\n        S = pd.Series(np.arange(n))\n        S2 = pd.Series(np.arange(n) + 1.0)\n        start, end = get_start_end(n)\n        s_tup = (S, 1, S2)\n        h_s_tup = (S[start:end], 1, S2[start:end])\n        self.assertEqual(hpat_func(h_s_tup), test_impl(s_tup))\n\n    @skip_numba_jit\n    def test_series_concat1(self):\n        def test_impl(S1, S2):\n            return pd.concat([S1, S2]).values\n        hpat_func = self.jit(test_impl)\n\n        S1 = pd.Series([1.0, 2., 3., 4., 5.])\n        S2 = pd.Series([6., 7.])\n        np.testing.assert_array_equal(hpat_func(S1, S2), test_impl(S1, S2))\n\n    @skip_numba_jit\n    def test_series_combine(self):\n        def test_impl(S1, S2):\n            return S1.combine(S2, lambda a, b: 2 * a + b)\n        hpat_func = self.jit(test_impl)\n\n        S1 = pd.Series([1.0, 2., 3., 4., 5.])\n        S2 = pd.Series([6.0, 21., 3.6, 5.])\n        pd.testing.assert_series_equal(hpat_func(S1, S2), test_impl(S1, S2))\n\n    @skip_numba_jit\n    def test_series_combine_float3264(self):\n        def test_impl(S1, S2):\n            return S1.combine(S2, lambda a, b: 2 * a + b)\n        hpat_func = self.jit(test_impl)\n\n        S1 = pd.Series([np.float64(1), np.float64(2),\n                        np.float64(3), np.float64(4), np.float64(5)])\n        S2 = pd.Series([np.float32(1), np.float32(2),\n                        np.float32(3), np.float32(4), np.float32(5)])\n        pd.testing.assert_series_equal(hpat_func(S1, S2), test_impl(S1, S2))\n\n    @skip_numba_jit\n    def test_series_combine_assert1(self):\n        def test_impl(S1, S2):\n            return S1.combine(S2, lambda a, b: 2 * a + b)\n        hpat_func = self.jit(test_impl)\n\n        S1 = pd.Series([1, 2, 3])\n        S2 = pd.Series([6., 21., 3., 5.])\n        with self.assertRaises(AssertionError):\n            hpat_func(S1, S2)\n\n    @skip_numba_jit\n    def test_series_combine_assert2(self):\n        def test_impl(S1, S2):\n            return S1.combine(S2, lambda a, b: 2 * a + b)\n        hpat_func = self.jit(test_impl)\n\n        S1 = pd.Series([6., 21., 3., 5.])\n        S2 = pd.Series([1, 2, 3])\n        with self.assertRaises(AssertionError):\n            hpat_func(S1, S2)\n\n    @skip_numba_jit\n    def test_series_combine_integer(self):\n        def test_impl(S1, S2):\n            return S1.combine(S2, lambda a, b: 2 * a + b, 16)\n        hpat_func = self.jit(test_impl)\n\n        S1 = pd.Series([1, 2, 3, 4, 5])\n        S2 = pd.Series([6, 21, 3, 5])\n        pd.testing.assert_series_equal(hpat_func(S1, S2), test_impl(S1, S2))\n\n    @skip_numba_jit\n    def test_series_combine_different_types(self):\n        def test_impl(S1, S2):\n            return S1.combine(S2, lambda a, b: 2 * a + b)\n        hpat_func = self.jit(test_impl)\n\n        S1 = pd.Series([6.1, 21.2, 3.3, 5.4, 6.7])\n        S2 = pd.Series([1, 2, 3, 4, 5])\n        pd.testing.assert_series_equal(hpat_func(S1, S2), test_impl(S1, S2))\n\n    @skip_numba_jit\n    def test_series_combine_integer_samelen(self):\n        def test_impl(S1, S2):\n            return S1.combine(S2, lambda a, b: 2 * a + b)\n        hpat_func = self.jit(test_impl)\n\n        S1 = pd.Series([1, 2, 3, 4, 5])\n        S2 = pd.Series([6, 21, 17, -5, 4])\n        pd.testing.assert_series_equal(hpat_func(S1, S2), test_impl(S1, S2))\n\n    @skip_numba_jit\n    def test_series_combine_samelen(self):\n        def test_impl(S1, S2):\n            return S1.combine(S2, lambda a, b: 2 * a + b)\n        hpat_func = self.jit(test_impl)\n\n        S1 = pd.Series([1.0, 2., 3., 4., 5.])\n        S2 = pd.Series([6.0, 21., 3.6, 5., 0.0])\n        pd.testing.assert_series_equal(hpat_func(S1, S2), test_impl(S1, S2))\n\n    @skip_numba_jit\n    def test_series_combine_value(self):\n        def test_impl(S1, S2):\n            return S1.combine(S2, lambda a, b: 2 * a + b, 1237.56)\n        hpat_func = self.jit(test_impl)\n\n        S1 = pd.Series([1.0, 2., 3., 4., 5.])\n        S2 = pd.Series([6.0, 21., 3.6, 5.])\n        pd.testing.assert_series_equal(hpat_func(S1, S2), test_impl(S1, S2))\n\n    @skip_numba_jit\n    def test_series_combine_value_samelen(self):\n        def test_impl(S1, S2):\n            return S1.combine(S2, lambda a, b: 2 * a + b, 1237.56)\n        hpat_func = self.jit(test_impl)\n\n        S1 = pd.Series([1.0, 2., 3., 4., 5.])\n        S2 = pd.Series([6.0, 21., 3.6, 5., 0.0])\n        pd.testing.assert_series_equal(hpat_func(S1, S2), test_impl(S1, S2))\n\n    def test_series_abs1(self):\n        def test_impl(S):\n            return S.abs()\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([np.nan, -2., 3., 0.5E-01, 0xFF, 0o7, 0b101])\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    def test_series_corr1(self):\n        def test_impl(s1, s2):\n            return s1.corr(s2)\n        hpat_func = self.jit(test_impl)\n\n        for pair in _cov_corr_series:\n            s1, s2 = pair\n            with self.subTest(s1=s1.values, s2=s2.values):\n                result = hpat_func(s1, s2)\n                result_ref = test_impl(s1, s2)\n                np.testing.assert_almost_equal(result, result_ref)\n\n    def test_series_str_center_default_fillchar(self):\n        def test_impl(series, width):\n            return series.str.center(width)\n\n        hpat_func = self.jit(test_impl)\n\n        data = test_global_input_data_unicode_kind1\n        series = pd.Series(data)\n        width = max(len(s) for s in data) + 10\n\n        pd.testing.assert_series_equal(hpat_func(series, width),\n                                       test_impl(series, width))\n\n    def test_series_str_center(self):\n        def test_impl(series, width, fillchar):\n            return series.str.center(width, fillchar)\n\n        hpat_func = self.jit(test_impl)\n\n        data = test_global_input_data_unicode_kind1\n        data_lengths = [len(s) for s in data]\n        widths = [max(data_lengths) + 10, min(data_lengths)]\n\n        for index in [None, list(range(len(data)))[::-1], data[::-1]]:\n            series = pd.Series(data, index, name=\'A\')\n            for width, fillchar in product(widths, [\'\\t\']):\n                jit_result = hpat_func(series, width, fillchar)\n                ref_result = test_impl(series, width, fillchar)\n                pd.testing.assert_series_equal(jit_result, ref_result)\n\n    def test_series_str_center_exception_unsupported_fillchar(self):\n        def test_impl(series, width, fillchar):\n            return series.str.center(width, fillchar)\n\n        hpat_func = self.jit(test_impl)\n\n        data = test_global_input_data_unicode_kind1\n        series = pd.Series(data)\n        width = max(len(s) for s in data) + 10\n\n        with self.assertRaises(TypingError) as raises:\n            hpat_func(series, width, 10)\n        msg_tmpl = \'Method center(). The object fillchar\\n {}\'\n        msg = msg_tmpl.format(\'given: int64\\n expected: str\')\n        self.assertIn(msg, str(raises.exception))\n\n    @unittest.expectedFailure  # https://jira.devtools.intel.com/browse/SAT-2348\n    def test_series_str_center_exception_unsupported_kind4(self):\n        def test_impl(series, width):\n            return series.str.center(width)\n\n        hpat_func = self.jit(test_impl)\n\n        data = test_global_input_data_unicode_kind4\n        series = pd.Series(data)\n        width = max(len(s) for s in data) + 10\n\n        jit_result = hpat_func(series, width)\n        ref_result = test_impl(series, width)\n        pd.testing.assert_series_equal(jit_result, ref_result)\n\n    def test_series_str_center_with_none(self):\n        def test_impl(series, width, fillchar):\n            return series.str.center(width, fillchar)\n\n        cfunc = self.jit(test_impl)\n        idx = [\'City 1\', \'City 2\', \'City 3\', \'City 4\', \'City 5\', \'City 6\', \'City 7\', \'City 8\']\n        s = pd.Series([\'New_York\', \'Lisbon\', np.nan, \'Tokyo\', \'Paris\', None, \'Munich\', None], index=idx)\n        pd.testing.assert_series_equal(cfunc(s, width=13, fillchar=\'*\'), test_impl(s, width=13, fillchar=\'*\'))\n\n    def test_series_str_endswith(self):\n        def test_impl(series, pat):\n            return series.str.endswith(pat)\n\n        hpat_func = self.jit(test_impl)\n\n        data = test_global_input_data_unicode_kind4\n        pats = [\'\'] + [s[-min(len(s) for s in data):] for s in data] + data\n        indices = [None, list(range(len(data)))[::-1], data[::-1]]\n        names = [None, \'A\']\n        for index, name in product(indices, names):\n            series = pd.Series(data, index, name=name)\n            for pat in pats:\n                pd.testing.assert_series_equal(hpat_func(series, pat),\n                                               test_impl(series, pat))\n\n    def test_series_str_endswith_exception_unsupported_na(self):\n        def test_impl(series, pat, na):\n            return series.str.endswith(pat, na)\n\n        hpat_func = self.jit(test_impl)\n\n        series = pd.Series(test_global_input_data_unicode_kind4)\n        msg_tmpl = \'Method endswith(). The object na\\n {}\'\n\n        with self.assertRaises(TypingError) as raises:\n            hpat_func(series, \'\', \'None\')\n        msg = msg_tmpl.format(\'given: unicode_type\\n expected: bool\')\n        self.assertIn(msg, str(raises.exception))\n\n        with self.assertRaises(ValueError) as raises:\n            hpat_func(series, \'\', False)\n        msg = msg_tmpl.format(\'expected: None\')\n        self.assertIn(msg, str(raises.exception))\n\n    def test_series_str_find(self):\n        def test_impl(series, sub):\n            return series.str.find(sub)\n        hpat_func = self.jit(test_impl)\n\n        data = test_global_input_data_unicode_kind4\n        subs = [\'\'] + [s[:min(len(s) for s in data)] for s in data] + data\n        indices = [None, list(range(len(data)))[::-1], data[::-1]]\n        names = [None, \'A\']\n        for index, name in product(indices, names):\n            series = pd.Series(data, index, name=name)\n            for sub in subs:\n                pd.testing.assert_series_equal(hpat_func(series, sub),\n                                               test_impl(series, sub))\n\n    def test_series_str_find_exception_unsupported_start(self):\n        def test_impl(series, sub, start):\n            return series.str.find(sub, start)\n        hpat_func = self.jit(test_impl)\n\n        series = pd.Series(test_global_input_data_unicode_kind4)\n        msg_tmpl = \'Method find(). The object start\\n {}\'\n\n        with self.assertRaises(TypingError) as raises:\n            hpat_func(series, \'\', \'0\')\n        msg = msg_tmpl.format(\'given: unicode_type\\n expected: None, int\')\n        self.assertIn(msg, str(raises.exception))\n\n        with self.assertRaises(ValueError) as raises:\n            hpat_func(series, \'\', 1)\n        msg = msg_tmpl.format(\'expected: 0\')\n        self.assertIn(msg, str(raises.exception))\n\n    def test_series_str_find_exception_unsupported_end(self):\n        def test_impl(series, sub, start, end):\n            return series.str.find(sub, start, end)\n        hpat_func = self.jit(test_impl)\n\n        series = pd.Series(test_global_input_data_unicode_kind4)\n        msg_tmpl = \'Method find(). The object end\\n {}\'\n\n        with self.assertRaises(TypingError) as raises:\n            hpat_func(series, \'\', 0, \'None\')\n        msg = msg_tmpl.format(\'given: unicode_type\\n expected: None, int\')\n        self.assertIn(msg, str(raises.exception))\n\n        with self.assertRaises(ValueError) as raises:\n            hpat_func(series, \'\', 0, 0)\n        msg = msg_tmpl.format(\'expected: None\')\n        self.assertIn(msg, str(raises.exception))\n\n    def test_series_str_len1(self):\n        def test_impl(S):\n            return S.str.len()\n        hpat_func = self.jit(test_impl)\n\n        data = [\'aa\', \'abc\', \'c\', \'cccd\']\n        indices = [None, [1, 3, 2, 0], data]\n        names = [None, \'A\']\n        for index, name in product(indices, names):\n            S = pd.Series(data, index, name=name)\n            pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    def test_series_str_just_default_fillchar(self):\n        data = test_global_input_data_unicode_kind1\n        series = pd.Series(data)\n        width = max(len(s) for s in data) + 5\n\n        pyfuncs = [ljust_usecase, rjust_usecase]\n        for pyfunc in pyfuncs:\n            cfunc = self.jit(pyfunc)\n            pd.testing.assert_series_equal(cfunc(series, width),\n                                           pyfunc(series, width))\n\n    def test_series_str_just(self):\n        data = test_global_input_data_unicode_kind1\n        data_lengths = [len(s) for s in data]\n        widths = [max(data_lengths) + 5, min(data_lengths)]\n\n        pyfuncs = [ljust_with_fillchar_usecase, rjust_with_fillchar_usecase]\n        for index in [None, list(range(len(data)))[::-1], data[::-1]]:\n            series = pd.Series(data, index, name=\'A\')\n            for width, fillchar in product(widths, [\'\\t\']):\n                for pyfunc in pyfuncs:\n                    cfunc = self.jit(pyfunc)\n                    jit_result = cfunc(series, width, fillchar)\n                    ref_result = pyfunc(series, width, fillchar)\n                    pd.testing.assert_series_equal(jit_result, ref_result)\n\n    def test_series_str_just_exception_unsupported_fillchar(self):\n        data = test_global_input_data_unicode_kind1\n        series = pd.Series(data)\n        width = max(len(s) for s in data) + 5\n        msg_tmpl = \'Method {}(). The object fillchar\\n given: int64\\n expected: str\'\n\n        pyfuncs = [(\'ljust\', ljust_with_fillchar_usecase),\n                   (\'rjust\', rjust_with_fillchar_usecase)]\n        for name, pyfunc in pyfuncs:\n            cfunc = self.jit(pyfunc)\n            with self.assertRaises(TypingError) as raises:\n                cfunc(series, width, 5)\n            self.assertIn(msg_tmpl.format(name), str(raises.exception))\n\n    @unittest.expectedFailure  # https://jira.devtools.intel.com/browse/SAT-2348\n    def test_series_str_just_exception_unsupported_kind4(self):\n        data = test_global_input_data_unicode_kind4\n        series = pd.Series(data)\n        width = max(len(s) for s in data) + 5\n\n        for pyfunc in [ljust_usecase, rjust_usecase]:\n            cfunc = self.jit(pyfunc)\n            jit_result = cfunc(series, width)\n            ref_result = pyfunc(series, width)\n            pd.testing.assert_series_equal(jit_result, ref_result)\n\n    def test_series_str_ljust_with_none(self):\n        def test_impl(series, width, fillchar):\n            return series.str.ljust(width, fillchar)\n\n        cfunc = self.jit(test_impl)\n        idx = [\'City 1\', \'City 2\', \'City 3\', \'City 4\', \'City 5\', \'City 6\', \'City 7\', \'City 8\']\n        s = pd.Series([\'New_York\', \'Lisbon\', np.nan, \'Tokyo\', \'Paris\', None, \'Munich\', None], index=idx)\n        pd.testing.assert_series_equal(cfunc(s, width=13, fillchar=\'*\'), test_impl(s, width=13, fillchar=\'*\'))\n\n\n    def test_series_str_rjust_with_none(self):\n        def test_impl(series, width, fillchar):\n            return series.str.rjust(width, fillchar)\n\n        cfunc = self.jit(test_impl)\n        idx = [\'City 1\', \'City 2\', \'City 3\', \'City 4\', \'City 5\', \'City 6\', \'City 7\', \'City 8\']\n        s = pd.Series([\'New_York\', \'Lisbon\', np.nan, \'Tokyo\', \'Paris\', None, \'Munich\', None], index=idx)\n        pd.testing.assert_series_equal(cfunc(s, width=13, fillchar=\'*\'), test_impl(s, width=13, fillchar=\'*\'))\n\n    def test_series_str_startswith(self):\n        def test_impl(series, pat):\n            return series.str.startswith(pat)\n\n        hpat_func = self.jit(test_impl)\n\n        data = test_global_input_data_unicode_kind4\n        pats = [\'\'] + [s[:min(len(s) for s in data)] for s in data] + data\n        indices = [None, list(range(len(data)))[::-1], data[::-1]]\n        names = [None, \'A\']\n        for index, name in product(indices, names):\n            series = pd.Series(data, index, name=name)\n            for pat in pats:\n                pd.testing.assert_series_equal(hpat_func(series, pat),\n                                               test_impl(series, pat))\n\n    def test_series_str_startswith_exception_unsupported_na(self):\n        def test_impl(series, pat, na):\n            return series.str.startswith(pat, na)\n\n        hpat_func = self.jit(test_impl)\n\n        series = pd.Series(test_global_input_data_unicode_kind4)\n        msg_tmpl = \'Method startswith(). The object na\\n {}\'\n\n        with self.assertRaises(TypingError) as raises:\n            hpat_func(series, \'\', \'None\')\n        msg = msg_tmpl.format(\'given: unicode_type\\n expected: bool\')\n        self.assertIn(msg, str(raises.exception))\n\n        with self.assertRaises(ValueError) as raises:\n            hpat_func(series, \'\', False)\n        msg = msg_tmpl.format(\'expected: None\')\n        self.assertIn(msg, str(raises.exception))\n\n    def test_series_str_zfill(self):\n        def test_impl(series, width):\n            return series.str.zfill(width)\n\n        hpat_func = self.jit(test_impl)\n\n        data = test_global_input_data_unicode_kind1\n        data_lengths = [len(s) for s in data]\n\n        for index in [None, list(range(len(data)))[::-1], data[::-1]]:\n            series = pd.Series(data, index, name=\'A\')\n            for width in [max(data_lengths) + 5, min(data_lengths)]:\n                jit_result = hpat_func(series, width)\n                ref_result = test_impl(series, width)\n                pd.testing.assert_series_equal(jit_result, ref_result)\n\n    @unittest.expectedFailure\n    def test_series_str_zfill_limitation(self):\n        def test_impl(series, width):\n            return series.str.zfill(width)\n\n        cfunc = self.jit(test_impl)\n        s = pd.Series([\'-1\', \'1\', \'1000\', np.nan])\n        jit_result = cfunc(s, 3)\n        ref_result = test_impl(s, 3)\n        pd.testing.assert_series_equal(jit_result, ref_result)\n\n    def test_series_str_zfill_with_none(self):\n        def test_impl(series, width):\n            return series.str.zfill(width)\n\n        cfunc = self.jit(test_impl)\n        s = pd.Series([\'1\', \'1000\', np.nan])\n        jit_result = cfunc(s, 3)\n        ref_result = test_impl(s, 3)\n        pd.testing.assert_series_equal(jit_result, ref_result)\n\n    @unittest.expectedFailure  # https://jira.devtools.intel.com/browse/SAT-2348\n    def test_series_str_zfill_exception_unsupported_kind4(self):\n        def test_impl(series, width):\n            return series.str.zfill(width)\n\n        hpat_func = self.jit(test_impl)\n\n        data = test_global_input_data_unicode_kind4\n        series = pd.Series(data)\n        width = max(len(s) for s in data) + 5\n\n        jit_result = hpat_func(series, width)\n        ref_result = test_impl(series, width)\n        pd.testing.assert_series_equal(jit_result, ref_result)\n\n    def test_series_str2str(self):\n        common_methods = [\'lower\', \'upper\', \'isupper\']\n        sdc_methods = [\'capitalize\', \'swapcase\', \'title\',\n                       \'lstrip\', \'rstrip\', \'strip\']\n        str2str_methods = common_methods[:]\n\n        data = [\' \\tbbCD\\t \', \'ABC\', \' mCDm\\t\', \'abc\']\n        indices = [None]\n        names = [None, \'A\']\n        if sdc.config.config_pipeline_hpat_default:\n            str2str_methods += sdc_methods\n        else:\n            indices += [[1, 3, 2, 0], data]\n\n        for method in str2str_methods:\n            func_lines = [\'def test_impl(S):\',\n                          \'  return S.str.{}()\'.format(method)]\n            func_text = \'\\n\'.join(func_lines)\n            test_impl = _make_func_from_text(func_text)\n            hpat_func = self.jit(test_impl)\n\n            check_names = method in common_methods\n            for index, name in product(indices, names):\n                S = pd.Series(data, index, name=name)\n                pd.testing.assert_series_equal(hpat_func(S), test_impl(S),\n                                               check_names=check_names)\n\n    def test_series_capitalize_str(self):\n        def test_impl(S):\n            return S.str.capitalize()\n\n        sdc_func = self.jit(test_impl)\n        test_data = [test_global_input_data_unicode_kind4,\n                     [\'lower\', None, \'CAPITALS\', None, \'this is a sentence\', \'SwApCaSe\', None]]\n        for data in test_data:\n            with self.subTest(data=data):\n                s = pd.Series(data)\n                pd.testing.assert_series_equal(sdc_func(s), test_impl(s))\n\n    def test_series_title_str(self):\n        def test_impl(S):\n            return S.str.title()\n\n        sdc_func = self.jit(test_impl)\n        test_data = [test_global_input_data_unicode_kind4,\n                     [\'lower\', None, \'CAPITALS\', None, \'this is a sentence\', \'SwApCaSe\', None]]\n        for data in test_data:\n            with self.subTest(data=data):\n                s = pd.Series(data)\n                pd.testing.assert_series_equal(sdc_func(s), test_impl(s))\n\n    def test_series_upper_str(self):\n        sdc_func = self.jit(upper_usecase)\n        test_data = [test_global_input_data_unicode_kind4,\n                     [\'lower\', None, \'CAPITALS\', None, \'this is a sentence\', \'SwApCaSe\', None]]\n        for data in test_data:\n            with self.subTest(data=data):\n                s = pd.Series(data)\n                pd.testing.assert_series_equal(sdc_func(s), upper_usecase(s))\n\n    def test_series_swapcase_str(self):\n        def test_impl(S):\n            return S.str.swapcase()\n\n        sdc_func = self.jit(test_impl)\n        test_data = [test_global_input_data_unicode_kind4,\n                     [\'lower\', None, \'CAPITALS\', None, \'this is a sentence\', \'SwApCaSe\', None]]\n        for data in test_data:\n            with self.subTest(data=data):\n                s = pd.Series(data)\n                pd.testing.assert_series_equal(sdc_func(s), test_impl(s))\n\n    def test_series_casefold_str(self):\n        def test_impl(S):\n            return S.str.casefold()\n\n        sdc_func = self.jit(test_impl)\n        test_data = [test_global_input_data_unicode_kind4,\n                     [\'lower\', None, \'CAPITALS\', None, \'this is a sentence\', \'SwApCaSe\', None]]\n        for data in test_data:\n            with self.subTest(data=data):\n                s = pd.Series(data)\n                pd.testing.assert_series_equal(sdc_func(s), test_impl(s))\n\n    @sdc_limitation\n    def test_series_append_same_names(self):\n        """"""SDC discards name""""""\n        def test_impl():\n            s1 = pd.Series(data=[0, 1, 2], name=\'A\')\n            s2 = pd.Series(data=[3, 4, 5], name=\'A\')\n            return s1.append(s2)\n\n        sdc_func = self.jit(test_impl)\n        pd.testing.assert_series_equal(sdc_func(), test_impl())\n\n    @skip_sdc_jit(""Old-style append implementation doesn\'t handle ignore_index argument"")\n    def test_series_append_single_ignore_index(self):\n        """"""Verify Series.append() concatenates Series with other single Series ignoring indexes""""""\n        def test_impl(S, other):\n            return S.append(other, ignore_index=True)\n        hpat_func = self.jit(test_impl)\n\n        dtype_to_data = {\'float\': [[-2., 3., 9.1, np.nan], [-2., 5.0, np.inf, 0, -1]],\n                         \'string\': [[\'a\', None, \'bbbb\', \'\'], [\'dd\', None, \'\', \'e\', \'ttt\']]}\n\n        for dtype, data_list in dtype_to_data.items():\n            with self.subTest(series_dtype=dtype, concatenated_data=data_list):\n                S1, S2 = [pd.Series(data) for data in data_list]\n                pd.testing.assert_series_equal(hpat_func(S1, S2), test_impl(S1, S2))\n\n    @skip_sdc_jit(""Old-style append implementation doesn\'t handle ignore_index argument"")\n    def test_series_append_list_ignore_index(self):\n        """"""Verify Series.append() concatenates Series with list of other Series ignoring indexes""""""\n        def test_impl(S1, S2, S3):\n            return S1.append([S2, S3], ignore_index=True)\n        hpat_func = self.jit(test_impl)\n\n        dtype_to_data = {\'float\': [[-2., 3., 9.1], [-2., 5.0], [1.0]],\n                         \'string\': [[\'a\', None, \'\'], [\'d\', None], [\'\']]}\n\n        for dtype, data_list in dtype_to_data.items():\n            with self.subTest(series_dtype=dtype, concatenated_data=data_list):\n                S1, S2, S3 = [pd.Series(data) for data in data_list]\n                pd.testing.assert_series_equal(hpat_func(S1, S2, S3), test_impl(S1, S2, S3))\n\n    @unittest.skip(\'BUG: Pandas 0.25.1 Series.append() doesn\\\'t support tuple as appending values\')\n    def test_series_append_tuple_ignore_index(self):\n        """"""Verify Series.append() concatenates Series with tuple of other Series ignoring indexes""""""\n        def test_impl(S1, S2, S3):\n            return S1.append((S2, S3, ), ignore_index=True)\n        hpat_func = self.jit(test_impl)\n\n        dtype_to_data = {\'float\': [[-2., 3., 9.1], [-2., 5.0], [1.0]]}\n        if not sdc.config.config_pipeline_hpat_default:\n            dtype_to_data[\'string\'] = [[\'a\', None, \'\'], [\'d\', None], [\'\']]\n\n        for dtype, data_list in dtype_to_data.items():\n            with self.subTest(series_dtype=dtype, concatenated_data=data_list):\n                S1, S2, S3 = [pd.Series(data) for data in data_list]\n                pd.testing.assert_series_equal(hpat_func(S1, S2, S3), test_impl(S1, S2, S3))\n\n    @skip_sdc_jit(""BUG: old-style append implementation doesn\'t handle series index"")\n    def test_series_append_single_index_default(self):\n        """"""Verify Series.append() concatenates Series with other single Series respecting default indexes""""""\n        def test_impl(S, other):\n            return S.append(other)\n        hpat_func = self.jit(test_impl)\n\n        dtype_to_data = {\'float\': [[-2., 3., 9.1], [-2., 5.0]]}\n        if not sdc.config.config_pipeline_hpat_default:\n            dtype_to_data[\'string\'] = [[\'a\', None, \'bbbb\', \'\'], [\'dd\', None, \'\', \'e\']]\n\n        for dtype, data_list in dtype_to_data.items():\n            with self.subTest(series_dtype=dtype, concatenated_data=data_list):\n                S1, S2 = [pd.Series(data) for data in data_list]\n                pd.testing.assert_series_equal(hpat_func(S1, S2), test_impl(S1, S2))\n\n    @skip_sdc_jit(""BUG: old-style append implementation doesn\'t handle series index"")\n    def test_series_append_list_index_default(self):\n        """"""Verify Series.append() concatenates Series with list of other Series respecting default indexes""""""\n        def test_impl(S1, S2, S3):\n            return S1.append([S2, S3])\n        hpat_func = self.jit(test_impl)\n\n        dtype_to_data = {\'float\': [[-2., 3., 9.1], [-2., 5.0], [1.0]]}\n        if not sdc.config.config_pipeline_hpat_default:\n            dtype_to_data[\'string\'] = [[\'a\', \'b\', \'q\'], [\'d\', \'e\'], [\'s\']]\n\n        for dtype, data_list in dtype_to_data.items():\n            with self.subTest(series_dtype=dtype, concatenated_data=data_list):\n                S1, S2, S3 = [pd.Series(data) for data in data_list]\n                pd.testing.assert_series_equal(hpat_func(S1, S2, S3), test_impl(S1, S2, S3))\n\n    @unittest.skip(\'BUG: Pandas 0.25.1 Series.append() doesn\\\'t support tuple as appending values\')\n    def test_series_append_tuple_index_default(self):\n        """"""Verify Series.append() concatenates Series with tuple of other Series respecting default indexes""""""\n        def test_impl(S1, S2, S3):\n            return S1.append((S2, S3, ))\n        hpat_func = self.jit(test_impl)\n\n        dtype_to_data = {\'float\': [[-2., 3., 9.1], [-2., 5.0], [1.0]]}\n        if not sdc.config.config_pipeline_hpat_default:\n            dtype_to_data[\'string\'] = [[\'a\', \'b\', \'q\'], [\'d\', \'e\'], [\'s\']]\n\n        for dtype, data_list in dtype_to_data.items():\n            with self.subTest(series_dtype=dtype, concatenated_data=data_list):\n                S1, S2, S3 = [pd.Series(data) for data in data_list]\n                pd.testing.assert_series_equal(hpat_func(S1, S2, S3), test_impl(S1, S2, S3))\n\n    @skip_sdc_jit(""BUG: old-style append implementation doesn\'t handle series index"")\n    def test_series_append_single_index_int(self):\n        """"""Verify Series.append() concatenates Series with other single Series respecting integer indexes""""""\n        def test_impl(S, other):\n            return S.append(other)\n        hpat_func = self.jit(test_impl)\n\n        dtype_to_data = {\'float\': [[-2., 3., 9.1, np.nan], [-2., 5.0, np.inf, 0, -1]]}\n        if not sdc.config.config_pipeline_hpat_default:\n            dtype_to_data[\'string\'] = [[\'a\', None, \'bbbb\', \'\'], [\'dd\', None, \'\', \'e\', \'ttt\']]\n        indexes = [[1, 2, 3, 4], [7, 8, 11, 3, 4]]\n\n        for dtype, data_list in dtype_to_data.items():\n            with self.subTest(series_dtype=dtype, concatenated_data=data_list):\n                S1, S2 = [pd.Series(data, index=indexes[i]) for i, data in enumerate(data_list)]\n                pd.testing.assert_series_equal(hpat_func(S1, S2), test_impl(S1, S2))\n\n    @skip_sdc_jit(""BUG: old-style append implementation doesn\'t handle series index"")\n    def test_series_append_list_index_int(self):\n        """"""Verify Series.append() concatenates Series with list of other Series respecting integer indexes""""""\n        def test_impl(S1, S2, S3):\n            return S1.append([S2, S3])\n        hpat_func = self.jit(test_impl)\n\n        dtype_to_data = {\'float\': [[-2., 3., 9.1, np.nan], [-2., 5.0, np.inf, 0], [-1.0]]}\n        if not sdc.config.config_pipeline_hpat_default:\n            dtype_to_data[\'string\'] = [[\'a\', None, \'bbbb\', \'\'], [\'dd\', None, \'\', \'e\'], [\'ttt\']]\n        indexes = [[1, 2, 3, 4], [7, 8, 11, 3], [4]]\n\n        for dtype, data_list in dtype_to_data.items():\n            with self.subTest(series_dtype=dtype, concatenated_data=data_list):\n                S1, S2, S3 = [pd.Series(data, index=indexes[i]) for i, data in enumerate(data_list)]\n                pd.testing.assert_series_equal(hpat_func(S1, S2, S3), test_impl(S1, S2, S3))\n\n    @unittest.skip(\'BUG: Pandas 0.25.1 Series.append() doesn\\\'t support tuple as appending values\')\n    def test_series_append_tuple_index_int(self):\n        """"""Verify Series.append() concatenates Series with tuple of other Series respecting integer indexes""""""\n        def test_impl(S1, S2, S3):\n            return S1.append((S2, S3, ))\n        hpat_func = self.jit(test_impl)\n\n        dtype_to_data = {\'float\': [[-2., 3., 9.1, np.nan], [-2., 5.0, np.inf, 0], [-1.0]]}\n        if not sdc.config.config_pipeline_hpat_default:\n            dtype_to_data[\'string\'] = [[\'a\', None, \'bbbb\', \'\'], [\'dd\', None, \'\', \'e\'], [\'ttt\']]\n        indexes = [[1, 2, 3, 4], [7, 8, 11, 3], [4]]\n\n        for dtype, data_list in dtype_to_data.items():\n            with self.subTest(series_dtype=dtype, concatenated_data=data_list):\n                S1, S2, S3 = [pd.Series(data, index=indexes[i]) for i, data in enumerate(data_list)]\n                pd.testing.assert_series_equal(hpat_func(S1, S2, S3), test_impl(S1, S2, S3))\n\n    @skip_sdc_jit(""BUG: old-style append implementation doesn\'t handle series index"")\n    def test_series_append_single_index_str(self):\n        """"""Verify Series.append() concatenates Series with other single Series respecting string indexes""""""\n        def test_impl(S, other):\n            return S.append(other)\n        hpat_func = self.jit(test_impl)\n\n        dtype_to_data = {\'float\': [[-2., 3., 9.1, np.nan], [-2., 5.0, np.inf, 0, -1.0]]}\n        if not sdc.config.config_pipeline_hpat_default:\n            dtype_to_data[\'string\'] = [[\'a\', None, \'bbbb\', \'\'], [\'dd\', None, \'\', \'e\', \'ttt\']]\n        indexes = [[\'a\', \'bb\', \'ccc\', \'dddd\'], [\'a1\', \'a2\', \'a3\', \'a4\', \'a5\']]\n\n        for dtype, data_list in dtype_to_data.items():\n            with self.subTest(series_dtype=dtype, concatenated_data=data_list):\n                S1, S2 = [pd.Series(data, index=indexes[i]) for i, data in enumerate(data_list)]\n                pd.testing.assert_series_equal(hpat_func(S1, S2), test_impl(S1, S2))\n\n    @skip_sdc_jit(""BUG: old-style append implementation doesn\'t handle series index"")\n    def test_series_append_list_index_str(self):\n        """"""Verify Series.append() concatenates Series with list of other Series respecting string indexes""""""\n        def test_impl(S1, S2, S3):\n            return S1.append([S2, S3])\n        hpat_func = self.jit(test_impl)\n\n        dtype_to_data = {\'float\': [[-2., 3., 9.1, np.nan], [-2., 5.0, np.inf, 0], [-1.0]]}\n        if not sdc.config.config_pipeline_hpat_default:\n            dtype_to_data[\'string\'] = [[\'a\', None, \'bbbb\', \'\'], [\'dd\', None, \'\', \'e\'], [\'ttt\']]\n        indexes = [[\'a\', \'bb\', \'ccc\', \'dddd\'], [\'q\', \'t\', \'a\', \'x\'], [\'dd\']]\n\n        for dtype, data_list in dtype_to_data.items():\n            with self.subTest(series_dtype=dtype, concatenated_data=data_list):\n                S1, S2, S3 = [pd.Series(data, index=indexes[i]) for i, data in enumerate(data_list)]\n                pd.testing.assert_series_equal(hpat_func(S1, S2, S3), test_impl(S1, S2, S3))\n\n    @unittest.skip(\'BUG: Pandas 0.25.1 Series.append() doesn\\\'t support tuple as appending values\')\n    def test_series_append_tuple_index_str(self):\n        """"""Verify Series.append() concatenates Series with tuple of other Series respecting string indexes""""""\n        def test_impl(S1, S2, S3):\n            return S1.append((S2, S3, ))\n        hpat_func = self.jit(test_impl)\n\n        dtype_to_data = {\'float\': [[-2., 3., 9.1, np.nan], [-2., 5.0, np.inf, 0], [-1.0]]}\n        if not sdc.config.config_pipeline_hpat_default:\n            dtype_to_data[\'string\'] = [[\'a\', None, \'bbbb\', \'\'], [\'dd\', None, \'\', \'e\'], [\'ttt\']]\n        indexes = [[\'a\', \'bb\', \'ccc\', \'dddd\'], [\'q\', \'t\', \'a\', \'x\'], [\'dd\']]\n\n        for dtype, data_list in dtype_to_data.items():\n            with self.subTest(series_dtype=dtype, concatenated_data=data_list):\n                S1, S2, S3 = [pd.Series(data, index=indexes[i]) for i, data in enumerate(data_list)]\n                pd.testing.assert_series_equal(hpat_func(S1, S2, S3), test_impl(S1, S2, S3))\n\n    @skip_sdc_jit(""Old-style append implementation doesn\'t handle ignore_index argument"")\n    def test_series_append_ignore_index_literal(self):\n        """"""Verify Series.append() implementation handles ignore_index argument as Boolean literal""""""\n        def test_impl(S, other):\n            return S.append(other, ignore_index=False)\n        hpat_func = self.jit(test_impl)\n\n        S1 = pd.Series([-2., 3., 9.1], [\'a1\', \'b1\', \'c1\'])\n        S2 = pd.Series([-2., 5.0], [\'a2\', \'b2\'])\n        pd.testing.assert_series_equal(hpat_func(S1, S2), test_impl(S1, S2))\n\n    @skip_sdc_jit(""Old-style append implementation doesn\'t handle ignore_index argument"")\n    def test_series_append_ignore_index_non_literal(self):\n        """"""Verify Series.append() implementation raises if ignore_index argument is not a Boolean literal""""""\n        def test_impl(S, other, param):\n            return S.append(other, ignore_index=param)\n        hpat_func = self.jit(test_impl)\n\n        ignore_index = True\n        S1 = pd.Series([-2., 3., 9.1], [\'a1\', \'b1\', \'c1\'])\n        S2 = pd.Series([-2., 5.0], [\'a2\', \'b2\'])\n        with self.assertRaises(TypingError) as raises:\n            hpat_func(S1, S2, ignore_index)\n        msg = \'Method append(). The object ignore_index\\n given: bool\\n expected: literal Boolean constant\\n\'\n        self.assertIn(msg.format(types.bool_), str(raises.exception))\n\n    @skip_sdc_jit(""BUG: old-style append implementation doesn\'t handle series index"")\n    def test_series_append_single_dtype_promotion(self):\n        """"""Verify Series.append() implementation handles appending single Series with different dtypes""""""\n        def test_impl(S, other):\n            return S.append(other)\n        hpat_func = self.jit(test_impl)\n\n        S1 = pd.Series([-2., 3., 9.1], [\'a1\', \'b1\', \'c1\'])\n        S2 = pd.Series([-2, 5], [\'a2\', \'b2\'])\n        pd.testing.assert_series_equal(hpat_func(S1, S2), test_impl(S1, S2))\n\n    @skip_sdc_jit(""BUG: old-style append implementation doesn\'t handle series index"")\n    def test_series_append_list_dtype_promotion(self):\n        """"""Verify Series.append() implementation handles appending list of Series with different dtypes""""""\n        def test_impl(S1, S2, S3):\n            return S1.append([S2, S3])\n        hpat_func = self.jit(test_impl)\n\n        S1 = pd.Series([-2, 3, 9])\n        S2 = pd.Series([-2., 5.0])\n        S3 = pd.Series([1.0])\n        pd.testing.assert_series_equal(hpat_func(S1, S2, S3),\n                                       test_impl(S1, S2, S3))\n\n    def sdc_join_series_indexes(self):\n        def test_impl(S1, S2):\n            return S1.add(S2)\n\n        sdc_func = self.jit(test_impl)\n\n        data = [0, 1, 2, 3, 4]\n        index1 = [3.3, 5.4, np.nan, 7.9, np.nan]\n        index2 = [3, 4, 3, 9, 2]\n        S1 = pd.Series(data, index1)\n        S2 = pd.Series(data, index2)\n        pd.testing.assert_series_equal(sdc_func(S1, S2), test_impl(S1, S2))\n\n    # SDC operator methods returns only float Series\n    def test_series_add(self):\n        def test_impl(S1, S2, value):\n            return S1.add(S2, fill_value=value)\n\n        sdc_func = self.jit(test_impl)\n\n        cases_data = [[0, 1, 2, 3, 4], [5, 2, 0, 333, -4], [3.3, 5.4, np.nan, 7.9, np.nan]]\n        cases_index = [[0, 1, 2, 3, 4], [3, 4, 3, 9, 2], None]\n        cases_value = [None, 4, 5.5]\n        for data, index, value in product(cases_data, cases_index, cases_value):\n            with self.subTest(data=data, index=index, value=value):\n                S1 = pd.Series(data, index)\n                S2 = pd.Series(index, data)\n                pd.testing.assert_series_equal(sdc_func(S1, S2, value), test_impl(S1, S2, value), check_dtype=False)\n\n    # SDC operator methods returns only float Series\n    def test_series_add_scalar(self):\n        def test_impl(S1, S2, value):\n            return S1.add(S2, fill_value=value)\n\n        sdc_func = self.jit(test_impl)\n\n        cases_data = [[0, 1, 2, 3, 4], [5, 2, 0, 333, -4], [3.3, 5.4, np.nan, 7.9, np.nan]]\n        cases_index = [[0, 1, 2, 3, 4], [3, 4, 3, 9, 2], None]\n        cases_scalar = [0, 1, 5.5, np.nan]\n        cases_value = [None, 4, 5.5]\n        for data, index, scalar, value in product(cases_data, cases_index, cases_scalar, cases_value):\n            with self.subTest(data=data, index=index, scalar=scalar, value=value):\n                S1 = pd.Series(data, index)\n                pd.testing.assert_series_equal(sdc_func(S1, scalar, value), test_impl(S1, scalar, value),\n                                               check_dtype=False)\n\n    def test_series_lt_fill_value(self):\n        def test_impl(S1, S2, value):\n            return S1.lt(S2, fill_value=value)\n\n        sdc_func = self.jit(test_impl)\n\n        cases_data = [[0, 1, 2, 3, 4], [5, 2, 0, 333, -4], [3.3, 5.4, np.nan, 7.9, np.nan]]\n        cases_index = [[3, 4, 3, 9, 2], None]\n        cases_value = [None, 4, 5.5]\n        for data, index, value in product(cases_data, cases_index, cases_value):\n            with self.subTest(data=data, index=index, value=value):\n                S1 = pd.Series(data, index)\n                S2 = pd.Series(data[::-1], index)\n                pd.testing.assert_series_equal(sdc_func(S1, S2, value), test_impl(S1, S2, value))\n\n    def test_series_lt_scalar_fill_value(self):\n        def test_impl(S1, S2, value):\n            return S1.lt(S2, fill_value=value)\n\n        sdc_func = self.jit(test_impl)\n\n        cases_data = [[0, 1, 2, 3, 4], [5, 2, 0, 333, -4], [3.3, 5.4, np.nan, 7.9, np.nan]]\n        cases_index = [[0, 1, 2, 3, 4], [3, 4, 3, 9, 2], None]\n        cases_scalar = [0, 1, 5.5, np.nan]\n        cases_value = [None, 4, 5.5]\n        for data, index, scalar, value in product(cases_data, cases_index, cases_scalar, cases_value):\n            with self.subTest(data=data, index=index, scalar=scalar, value=value):\n                S1 = pd.Series(data, index)\n                pd.testing.assert_series_equal(sdc_func(S1, scalar, value), test_impl(S1, scalar, value))\n\n    def test_series_isin_list1(self):\n        def test_impl(S, values):\n            return S.isin(values)\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        S = pd.Series(np.arange(n))\n        values = [1, 2, 5, 7, 8]\n        pd.testing.assert_series_equal(hpat_func(S, values), test_impl(S, values))\n\n    def test_series_isin_index(self):\n        def test_impl(S, values):\n            return S.isin(values)\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        data = np.arange(n)\n        index = [item + 1 for item in data]\n        S = pd.Series(data=data, index=index)\n        values = [1, 2, 5, 7, 8]\n        pd.testing.assert_series_equal(hpat_func(S, values), test_impl(S, values))\n\n    def test_series_isin_name(self):\n        def test_impl(S, values):\n            return S.isin(values)\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        S = pd.Series(np.arange(n), name=\'A\')\n        values = [1, 2, 5, 7, 8]\n        pd.testing.assert_series_equal(hpat_func(S, values), test_impl(S, values))\n\n    def test_series_isin_list2(self):\n        def test_impl(S, values):\n            return S.isin(values)\n        hpat_func = self.jit(test_impl)\n\n        n = 11.0\n        S = pd.Series(np.arange(n))\n        values = [1., 2., 5., 7., 8.]\n        pd.testing.assert_series_equal(hpat_func(S, values), test_impl(S, values))\n\n    def test_series_isin_list3(self):\n        def test_impl(S, values):\n            return S.isin(values)\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([\'a\', \'b\', \'q\', \'w\', \'c\', \'d\', \'e\', \'r\'])\n        values = [\'a\', \'q\', \'c\', \'d\', \'e\']\n        pd.testing.assert_series_equal(hpat_func(S, values), test_impl(S, values))\n\n    def test_series_isin_set1(self):\n        def test_impl(S, values):\n            return S.isin(values)\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        S = pd.Series(np.arange(n))\n        values = {1, 2, 5, 7, 8}\n        pd.testing.assert_series_equal(hpat_func(S, values), test_impl(S, values))\n\n    def test_series_isin_set2(self):\n        def test_impl(S, values):\n            return S.isin(values)\n        hpat_func = self.jit(test_impl)\n\n        n = 11.0\n        S = pd.Series(np.arange(n))\n        values = {1., 2., 5., 7., 8.}\n        pd.testing.assert_series_equal(hpat_func(S, values), test_impl(S, values))\n\n    @unittest.skip(\'TODO: requires hashable unicode strings in Numba\')\n    def test_series_isin_set3(self):\n        def test_impl(S, values):\n            return S.isin(values)\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([\'a\', \'b\', \'c\', \'d\', \'e\'] * 2)\n        values = {\'b\', \'c\', \'e\'}\n        pd.testing.assert_series_equal(hpat_func(S, values), test_impl(S, values))\n\n    def test_series_isna(self):\n        def test_impl(S):\n            return S.isna()\n\n        jit_func = self.jit(test_impl)\n\n        datas = [[0, 1, 2, 3], [0., 1., np.inf, np.nan], [\'a\', None, \'b\', \'c\'], [True, True, False, False]]\n        indices = [None, [3, 2, 1, 0], [\'a\', \'b\', \'c\', \'d\']]\n        names = [None, \'A\']\n\n        for data, index, name in product(datas, indices, names):\n            with self.subTest(data=data, index=index, name=name):\n                series = pd.Series(data=data, index=index, name=name)\n                jit_result = jit_func(series)\n                ref_result = test_impl(series)\n                pd.testing.assert_series_equal(jit_result, ref_result)\n\n    @skip_sdc_jit(\'Index and name are not supported\')\n    def test_series_isnull(self):\n        def test_impl(S):\n            return S.isnull()\n\n        jit_func = self.jit(test_impl)\n\n        datas = [[0, 1, 2, 3], [0., 1., np.inf, np.nan], [\'a\', None, \'b\', \'c\'], [True, True, False, False]]\n        indices = [None, [3, 2, 1, 0], [\'a\', \'b\', \'c\', \'d\']]\n        names = [None, \'A\']\n\n        for data, index, name in product(datas, indices, names):\n            with self.subTest(data=data, index=index, name=name):\n                series = pd.Series(data=data, index=index, name=name)\n                jit_result = jit_func(series)\n                ref_result = test_impl(series)\n                pd.testing.assert_series_equal(jit_result, ref_result)\n\n    def test_series_isnull1(self):\n        def test_impl(S):\n            return S.isnull()\n        hpat_func = self.jit(test_impl)\n\n        # column with NA\n        S = pd.Series([np.nan, 2., 3.])\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    def test_series_notna(self):\n        def test_impl(S):\n            return S.notna()\n\n        jit_func = self.jit(test_impl)\n\n        datas = [[0, 1, 2, 3], [0., 1., np.inf, np.nan], [\'a\', None, \'b\', \'c\'], [True, True, False, False]]\n        indices = [None, [3, 2, 1, 0], [\'a\', \'b\', \'c\', \'d\']]\n        names = [None, \'A\']\n\n        for data, index, name in product(datas, indices, names):\n            with self.subTest(data=data, index=index, name=name):\n                series = pd.Series(data=data, index=index, name=name)\n                jit_result = jit_func(series)\n                ref_result = test_impl(series)\n                pd.testing.assert_series_equal(jit_result, ref_result)\n\n    @unittest.skip(\'AssertionError: Series are different\')\n    def test_series_dt_isna1(self):\n        def test_impl(S):\n            return S.isna()\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([pd.NaT, pd.Timestamp(\'1970-12-01\'), pd.Timestamp(\'2012-07-25\')])\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    def test_series_nlargest(self):\n        def test_impl():\n            series = pd.Series([1., np.nan, -1., 0., min_float64, max_float64])\n            return series.nlargest(4)\n        hpat_func = self.jit(test_impl)\n\n        if sdc.config.config_pipeline_hpat_default:\n            np.testing.assert_array_equal(test_impl(), hpat_func())\n        else:\n            pd.testing.assert_series_equal(test_impl(), hpat_func())\n\n    def test_series_nlargest_unboxing(self):\n        def test_impl(series, n):\n            return series.nlargest(n)\n        hpat_func = self.jit(test_impl)\n\n        for data in test_global_input_data_numeric + [[]]:\n            series = pd.Series(data * 3)\n            for n in range(-1, 10):\n                ref_result = test_impl(series, n)\n                jit_result = hpat_func(series, n)\n                if sdc.config.config_pipeline_hpat_default:\n                    np.testing.assert_array_equal(ref_result, jit_result)\n                else:\n                    pd.testing.assert_series_equal(ref_result, jit_result)\n\n    @skip_numba_jit(\'Series.nlargest() parallelism unsupported and parquet not supported\')\n    def test_series_nlargest_parallel(self):\n        # create `kde.parquet` file\n        ParquetGenerator.gen_kde_pq()\n\n        def test_impl():\n            df = pq.read_table(\'kde.parquet\').to_pandas()\n            S = df.points\n            return S.nlargest(4)\n        hpat_func = self.jit(test_impl)\n\n        if sdc.config.config_pipeline_hpat_default:\n            np.testing.assert_array_equal(test_impl(), hpat_func())\n        else:\n            pd.testing.assert_series_equal(test_impl(), hpat_func())\n        self.assertEqual(count_parfor_REPs(), 0)\n        self.assertTrue(count_array_OneDs() > 0)\n\n    @skip_sdc_jit(\'Series.nlargest() parameter keep unsupported\')\n    def test_series_nlargest_full(self):\n        def test_impl(series, n, keep):\n            return series.nlargest(n, keep)\n        hpat_func = self.jit(test_impl)\n\n        keep = \'first\'\n        for data in test_global_input_data_numeric + [[]]:\n            series = pd.Series(data * 3)\n            for n in range(-1, 10):\n                ref_result = test_impl(series, n, keep)\n                jit_result = hpat_func(series, n, keep)\n                pd.testing.assert_series_equal(ref_result, jit_result)\n\n    def test_series_nlargest_index(self):\n        def test_impl(series, n):\n            return series.nlargest(n)\n        hpat_func = self.jit(test_impl)\n\n        # TODO: check data == [] after index is fixed\n        for data in test_global_input_data_numeric:\n            data_duplicated = data * 3\n            # TODO: add integer index not equal to range after index is fixed\n            indexes = [range(len(data_duplicated))]\n            if not sdc.config.config_pipeline_hpat_default:\n                indexes.append(gen_strlist(len(data_duplicated)))\n\n            for index in indexes:\n                series = pd.Series(data_duplicated, index)\n                for n in range(-1, 10):\n                    ref_result = test_impl(series, n)\n                    jit_result = hpat_func(series, n)\n                    if sdc.config.config_pipeline_hpat_default:\n                        np.testing.assert_array_equal(ref_result, jit_result)\n                    else:\n                        pd.testing.assert_series_equal(ref_result, jit_result)\n\n    @skip_sdc_jit(\'Series.nlargest() does not raise an exception\')\n    def test_series_nlargest_typing(self):\n        _func_name = \'Method nlargest().\'\n\n        def test_impl(series, n, keep):\n            return series.nlargest(n, keep)\n        hpat_func = self.jit(test_impl)\n\n        series = pd.Series(test_global_input_data_float64[0])\n        for n, ntype in [(True, types.boolean), (None, types.none),\n                         (0.1, \'float64\'), (\'n\', types.unicode_type)]:\n            with self.assertRaises(TypingError) as raises:\n                hpat_func(series, n=n, keep=\'first\')\n            msg = \'{} The object n\\n given: {}\\n expected: int\'\n            self.assertIn(msg.format(_func_name, ntype), str(raises.exception))\n\n        for keep, dtype in [(True, types.boolean), (None, types.none),\n                            (0.1, \'float64\'), (1, \'int64\')]:\n            with self.assertRaises(TypingError) as raises:\n                hpat_func(series, n=5, keep=keep)\n            msg = \'{} The object keep\\n given: {}\\n expected: str\'\n            self.assertIn(msg.format(_func_name, dtype), str(raises.exception))\n\n    @skip_sdc_jit(\'Series.nlargest() does not raise an exception\')\n    def test_series_nlargest_unsupported(self):\n        msg = ""Method nlargest(). Unsupported parameter. Given \'keep\' != \'first\'""\n\n        def test_impl(series, n, keep):\n            return series.nlargest(n, keep)\n        hpat_func = self.jit(test_impl)\n\n        series = pd.Series(test_global_input_data_float64[0])\n        for keep in [\'last\', \'all\', \'\']:\n            with self.assertRaises(ValueError) as raises:\n                hpat_func(series, n=5, keep=keep)\n            self.assertIn(msg, str(raises.exception))\n\n        with self.assertRaises(ValueError) as raises:\n            hpat_func(series, n=5, keep=\'last\')\n        self.assertIn(msg, str(raises.exception))\n\n    def test_series_nsmallest(self):\n        def test_impl():\n            series = pd.Series([1., np.nan, -1., 0., min_float64, max_float64])\n            return series.nsmallest(4)\n        hpat_func = self.jit(test_impl)\n\n        if sdc.config.config_pipeline_hpat_default:\n            np.testing.assert_array_equal(test_impl(), hpat_func())\n        else:\n            pd.testing.assert_series_equal(test_impl(), hpat_func())\n\n    def test_series_nsmallest_unboxing(self):\n        def test_impl(series, n):\n            return series.nsmallest(n)\n        hpat_func = self.jit(test_impl)\n\n        for data in test_global_input_data_numeric + [[]]:\n            series = pd.Series(data * 3)\n            for n in range(-1, 10):\n                ref_result = test_impl(series, n)\n                jit_result = hpat_func(series, n)\n                if sdc.config.config_pipeline_hpat_default:\n                    np.testing.assert_array_equal(ref_result, jit_result)\n                else:\n                    pd.testing.assert_series_equal(ref_result, jit_result)\n\n    @skip_numba_jit(\'Series.nsmallest() parallelism unsupported and parquet not supported\')\n    def test_series_nsmallest_parallel(self):\n        # create `kde.parquet` file\n        ParquetGenerator.gen_kde_pq()\n\n        def test_impl():\n            df = pq.read_table(\'kde.parquet\').to_pandas()\n            S = df.points\n            return S.nsmallest(4)\n        hpat_func = self.jit(test_impl)\n\n        if sdc.config.config_pipeline_hpat_default:\n            np.testing.assert_array_equal(test_impl(), hpat_func())\n        else:\n            pd.testing.assert_series_equal(test_impl(), hpat_func())\n        self.assertEqual(count_parfor_REPs(), 0)\n        self.assertTrue(count_array_OneDs() > 0)\n\n    @skip_sdc_jit(\'Series.nsmallest() parameter keep unsupported\')\n    def test_series_nsmallest_full(self):\n        def test_impl(series, n, keep):\n            return series.nsmallest(n, keep)\n        hpat_func = self.jit(test_impl)\n\n        keep = \'first\'\n        for data in test_global_input_data_numeric + [[]]:\n            series = pd.Series(data * 3)\n            for n in range(-1, 10):\n                ref_result = test_impl(series, n, keep)\n                jit_result = hpat_func(series, n, keep)\n                pd.testing.assert_series_equal(ref_result, jit_result)\n\n    def test_series_nsmallest_index(self):\n        def test_impl(series, n):\n            return series.nsmallest(n)\n        hpat_func = self.jit(test_impl)\n\n        # TODO: check data == [] after index is fixed\n        for data in test_global_input_data_numeric:\n            data_duplicated = data * 3\n            # TODO: add integer index not equal to range after index is fixed\n            indexes = [range(len(data_duplicated))]\n            if not sdc.config.config_pipeline_hpat_default:\n                indexes.append(gen_strlist(len(data_duplicated)))\n\n            for index in indexes:\n                series = pd.Series(data_duplicated, index)\n                for n in range(-1, 10):\n                    ref_result = test_impl(series, n)\n                    jit_result = hpat_func(series, n)\n                    if sdc.config.config_pipeline_hpat_default:\n                        np.testing.assert_array_equal(ref_result, jit_result)\n                    else:\n                        pd.testing.assert_series_equal(ref_result, jit_result)\n\n    @skip_sdc_jit(\'Series.nsmallest() does not raise an exception\')\n    def test_series_nsmallest_typing(self):\n        _func_name = \'Method nsmallest().\'\n\n        def test_impl(series, n, keep):\n            return series.nsmallest(n, keep)\n        hpat_func = self.jit(test_impl)\n\n        series = pd.Series(test_global_input_data_float64[0])\n        for n, ntype in [(True, types.boolean), (None, types.none),\n                         (0.1, \'float64\'), (\'n\', types.unicode_type)]:\n            with self.assertRaises(TypingError) as raises:\n                hpat_func(series, n=n, keep=\'first\')\n            msg = \'{} The object n\\n given: {}\\n expected: int\'\n            self.assertIn(msg.format(_func_name, ntype), str(raises.exception))\n\n        for keep, dtype in [(True, types.boolean), (None, types.none),\n                            (0.1, \'float64\'), (1, \'int64\')]:\n            with self.assertRaises(TypingError) as raises:\n                hpat_func(series, n=5, keep=keep)\n            msg = \'{} The object keep\\n given: {}\\n expected: str\'\n            self.assertIn(msg.format(_func_name, dtype), str(raises.exception))\n\n    @skip_sdc_jit(\'Series.nsmallest() does not raise an exception\')\n    def test_series_nsmallest_unsupported(self):\n        msg = ""Method nsmallest(). Unsupported parameter. Given \'keep\' != \'first\'""\n\n        def test_impl(series, n, keep):\n            return series.nsmallest(n, keep)\n        hpat_func = self.jit(test_impl)\n\n        series = pd.Series(test_global_input_data_float64[0])\n        for keep in [\'last\', \'all\', \'\']:\n            with self.assertRaises(ValueError) as raises:\n                hpat_func(series, n=5, keep=keep)\n            self.assertIn(msg, str(raises.exception))\n\n        with self.assertRaises(ValueError) as raises:\n            hpat_func(series, n=5, keep=\'last\')\n        self.assertIn(msg, str(raises.exception))\n\n    def test_series_head1(self):\n        def test_impl(S):\n            return S.head(4)\n        hpat_func = self.jit(test_impl)\n\n        m = 100\n        np.random.seed(0)\n        S = pd.Series(np.random.randint(-30, 30, m))\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    def test_series_head_named(self):\n        def test_impl(S):\n            return S.head(4)\n        hpat_func = self.jit(test_impl)\n\n        m = 100\n        np.random.seed(0)\n        S = pd.Series(data=np.random.randint(-30, 30, m), name=\'A\')\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    def test_series_head_default1(self):\n        """"""Verifies default head method for non-distributed pass of Series with no index""""""\n        def test_impl(S):\n            return S.head()\n        hpat_func = self.jit(test_impl)\n\n        m = 100\n        np.random.seed(0)\n        S = pd.Series(np.random.randint(-30, 30, m))\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    def test_series_head_index1(self):\n        """"""Verifies head method for Series with integer index created inside jitted function""""""\n        def test_impl():\n            S = pd.Series([6, 9, 2, 3, 6, 4, 5], [8, 1, 6, 0, 9, 1, 3])\n            return S.head(3)\n        hpat_func = self.jit(test_impl)\n\n        pd.testing.assert_series_equal(hpat_func(), test_impl())\n\n    def test_series_head_index_named(self):\n        """"""Verifies head method for Series with integer index created inside jitted function""""""\n        def test_impl():\n            S = pd.Series([6, 9, 2, 3, 6, 4, 5], [8, 1, 6, 0, 9, 1, 3], name=\'A\')\n            return S.head(3)\n        hpat_func = self.jit(test_impl)\n\n        pd.testing.assert_series_equal(hpat_func(), test_impl())\n\n    def test_series_head_index2(self):\n        """"""Verifies head method for Series with string index created inside jitted function""""""\n        def test_impl():\n            S = pd.Series([6, 9, 2, 3, 6, 4, 5], [\'a\', \'ab\', \'abc\', \'c\', \'f\', \'hh\', \'\'])\n            return S.head(3)\n        hpat_func = self.jit(test_impl)\n\n        pd.testing.assert_series_equal(hpat_func(), test_impl())\n\n    @skip_sdc_jit(""Fails to compile with latest Numba"")\n    def test_series_head_index3(self):\n        """"""Verifies head method for non-distributed pass of Series with integer index""""""\n        def test_impl(S):\n            return S.head(3)\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([6, 9, 2, 3, 6, 4, 5], [8, 1, 6, 0, 9, 1, 3])\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    @skip_sdc_jit(\'Breaks other tests\')\n    def test_series_head_index4(self):\n        """"""Verifies head method for non-distributed pass of Series with string index""""""\n        def test_impl(S):\n            return S.head(3)\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([6, 9, 2, 4, 6, 4, 5], [\'a\', \'ab\', \'abc\', \'c\', \'f\', \'hh\', \'\'])\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    @skip_numba_jit\n    def test_series_head_parallel1(self):\n        """"""Verifies head method for distributed Series with string data and no index""""""\n        def test_impl(S):\n            return S.head(7)\n\n        hpat_func = self.jit(distributed={\'S\'})(test_impl)\n\n        # need to test different lenghts, as head\'s size is fixed and implementation\n        # depends on relation of size of the data per processor to output data size\n        for n in range(1, 5):\n            S = pd.Series([\'a\', \'ab\', \'abc\', \'c\', \'f\', \'hh\', \'\'] * n)\n            start, end = get_start_end(len(S))\n            pd.testing.assert_series_equal(hpat_func(S[start:end]), test_impl(S[start:end]))\n            self.assertTrue(count_array_OneDs() > 0)\n\n    @skip_numba_jit\n    @unittest.expectedFailure\n    def test_series_head_index_parallel1(self):\n        """"""Verifies head method for distributed Series with integer index""""""\n        def test_impl(S):\n            return S.head(3)\n        hpat_func = self.jit(distributed={\'S\'})(test_impl)\n\n        S = pd.Series([6, 9, 2, 3, 6, 4, 5], [8, 1, 6, 0, 9, 1, 3])\n        start, end = get_start_end(len(S))\n        pd.testing.assert_series_equal(hpat_func(S[start:end]), test_impl(S[start:end]))\n        self.assertTrue(count_array_OneDs() > 0)\n\n    @unittest.skip(""Passed if run single"")\n    def test_series_head_index_parallel2(self):\n        """"""Verifies head method for distributed Series with string index""""""\n        def test_impl(S):\n            return S.head(3)\n        hpat_func = self.jit(distributed={\'S\'})(test_impl)\n\n        S = pd.Series([6, 9, 2, 3, 6, 4, 5], [\'a\', \'ab\', \'abc\', \'c\', \'f\', \'hh\', \'\'])\n        start, end = get_start_end(len(S))\n        pd.testing.assert_series_equal(hpat_func(S[start:end]), test_impl(S))\n        self.assertTrue(count_array_OneDs() > 0)\n\n    def test_series_head_noidx_float(self):\n        def test_impl(S, n):\n            return S.head(n)\n        hpat_func = self.jit(test_impl)\n        for input_data in test_global_input_data_float64:\n            S = pd.Series(input_data)\n            for n in [-1, 0, 2, 3]:\n                result_ref = test_impl(S, n)\n                result_jit = hpat_func(S, n)\n                pd.testing.assert_series_equal(result_jit, result_ref)\n\n    def test_series_head_noidx_int(self):\n        def test_impl(S, n):\n            return S.head(n)\n        hpat_func = self.jit(test_impl)\n        for input_data in test_global_input_data_integer64:\n            S = pd.Series(input_data)\n            for n in [-1, 0, 2, 3]:\n                result_ref = test_impl(S, n)\n                result_jit = hpat_func(S, n)\n                pd.testing.assert_series_equal(result_jit, result_ref)\n\n    def test_series_head_noidx_num(self):\n        def test_impl(S, n):\n            return S.head(n)\n        hpat_func = self.jit(test_impl)\n        for input_data in test_global_input_data_numeric:\n            S = pd.Series(input_data)\n            for n in [-1, 0, 2, 3]:\n                result_ref = test_impl(S, n)\n                result_jit = hpat_func(S, n)\n                pd.testing.assert_series_equal(result_jit, result_ref)\n\n    @unittest.skip(""Old implementation not work with n negative and data str"")\n    def test_series_head_noidx_str(self):\n        def test_impl(S, n):\n            return S.head(n)\n        hpat_func = self.jit(test_impl)\n        input_data = test_global_input_data_unicode_kind4\n        S = pd.Series(input_data)\n        for n in [-1, 0, 2, 3]:\n            result_ref = test_impl(S, n)\n            result_jit = hpat_func(S, n)\n            pd.testing.assert_series_equal(result_jit, result_ref)\n\n    @unittest.skip(""Broke another three tests"")\n    def test_series_head_idx(self):\n        def test_impl(S):\n            return S.head()\n\n        def test_impl_param(S, n):\n            return S.head(n)\n\n        hpat_func = self.jit(test_impl)\n\n        data_test = [[6, 6, 2, 1, 3, 3, 2, 1, 2],\n                     [1.1, 0.3, 2.1, 1, 3, 0.3, 2.1, 1.1, 2.2],\n                     [6, 6.1, 2.2, 1, 3, 0, 2.2, 1, 2],\n                     [\'as\', \'b\', \'abb\', \'sss\', \'ytr65\', \'\', \'qw\', \'a\', \'b\'],\n                     [6, 6, 2, 1, 3, np.inf, np.nan, np.nan, np.nan],\n                     [3., 5.3, np.nan, np.nan, np.inf, np.inf, 4.4, 3.7, 8.9]\n                     ]\n\n        for input_data in data_test:\n            for index_data in data_test:\n                S = pd.Series(input_data, index_data)\n\n                result_ref = test_impl(S)\n                result = hpat_func(S)\n                pd.testing.assert_series_equal(result, result_ref)\n\n                hpat_func_param1 = self.jit(test_impl_param)\n\n                for param1 in [1, 3, 7]:\n                    result_param1_ref = test_impl_param(S, param1)\n                    result_param1 = hpat_func_param1(S, param1)\n                    pd.testing.assert_series_equal(result_param1, result_param1_ref)\n\n    def test_series_median1(self):\n        """"""Verifies median implementation for float and integer series of random data""""""\n        def test_impl(S):\n            return S.median()\n        hpat_func = self.jit(test_impl)\n\n        m = 100\n        np.random.seed(0)\n        S = pd.Series(np.random.randint(-30, 30, m))\n        self.assertEqual(hpat_func(S), test_impl(S))\n\n        S = pd.Series(np.random.ranf(m))\n        self.assertEqual(hpat_func(S), test_impl(S))\n\n        # odd size\n        m = 101\n        S = pd.Series(np.random.randint(-30, 30, m))\n        self.assertEqual(hpat_func(S), test_impl(S))\n\n        S = pd.Series(np.random.ranf(m))\n        self.assertEqual(hpat_func(S), test_impl(S))\n\n    @skip_sdc_jit(""BUG: old-style median implementation doesn\'t filter NaNs"")\n    def test_series_median_skipna_default1(self):\n        """"""Verifies median implementation with default skipna=True argument on a series with NA values""""""\n        def test_impl(S):\n            return S.median()\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([2., 3., 5., np.nan, 5., 6., 7.])\n        self.assertEqual(hpat_func(S), test_impl(S))\n\n    @skip_sdc_jit(""Skipna argument is not supported in old-style"")\n    def test_series_median_skipna_false1(self):\n        """"""Verifies median implementation with skipna=False on a series with NA values""""""\n        def test_impl(S):\n            return S.median(skipna=False)\n        hpat_func = self.jit(test_impl)\n\n        # np.inf is not NaN, so verify that a correct number is returned\n        S1 = pd.Series([2., 3., 5., np.inf, 5., 6., 7.])\n        self.assertEqual(hpat_func(S1), test_impl(S1))\n\n        # TODO: both return values are \'nan\', but SDC\'s is not np.nan, hence checking with\n        # assertIs() doesn\'t work - check if it\'s Numba relatated\n        S2 = pd.Series([2., 3., 5., np.nan, 5., 6., 7.])\n        self.assertEqual(np.isnan(hpat_func(S2)), np.isnan(test_impl(S2)))\n\n    @skip_numba_jit\n    def test_series_median_parallel1(self):\n        # create `kde.parquet` file\n        ParquetGenerator.gen_kde_pq()\n\n        def test_impl():\n            df = pq.read_table(\'kde.parquet\').to_pandas()\n            S = df.points\n            return S.median()\n        hpat_func = self.jit(test_impl)\n\n        self.assertEqual(hpat_func(), test_impl())\n        self.assertEqual(count_array_REPs(), 0)\n        self.assertEqual(count_parfor_REPs(), 0)\n        self.assertTrue(count_array_OneDs() > 0)\n\n    @skip_numba_jit\n    def test_series_argsort_parallel(self):\n        # create `kde.parquet` file\n        ParquetGenerator.gen_kde_pq()\n\n        def test_impl():\n            df = pq.read_table(\'kde.parquet\').to_pandas()\n            S = df.points\n            return S.argsort().values\n        hpat_func = self.jit(test_impl)\n\n        np.testing.assert_array_equal(hpat_func(), test_impl())\n\n    def test_series_idxmin1(self):\n        def test_impl(a):\n            return a.idxmin()\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        np.random.seed(0)\n        s = pd.Series(np.random.ranf(n))\n        np.testing.assert_array_equal(hpat_func(s), test_impl(s))\n\n    def test_series_idxmin_str(self):\n        def test_impl(s):\n            return s.idxmin()\n        hpat_func = self.jit(test_impl)\n\n        s = pd.Series([8, 6, 34, np.nan], [\'a\', \'ab\', \'abc\', \'c\'])\n        self.assertEqual(hpat_func(s), test_impl(s))\n\n    @unittest.skip(""Skipna is not implemented"")\n    def test_series_idxmin_str_idx(self):\n        def test_impl(s):\n            return s.idxmin(skipna=False)\n\n        hpat_func = self.jit(test_impl)\n\n        s = pd.Series([8, 6, 34, np.nan], [\'a\', \'ab\', \'abc\', \'c\'])\n        self.assertEqual(hpat_func(s), test_impl(s))\n\n    def test_series_idxmin_no(self):\n        def test_impl(s):\n            return s.idxmin()\n        hpat_func = self.jit(test_impl)\n\n        s = pd.Series([8, 6, 34, np.nan])\n        self.assertEqual(hpat_func(s), test_impl(s))\n\n    def test_series_idxmin_int(self):\n        def test_impl(s):\n            return s.idxmin()\n        hpat_func = self.jit(test_impl)\n\n        s = pd.Series([1, 2, 3], [4, 45, 14])\n        self.assertEqual(hpat_func(s), test_impl(s))\n\n    def test_series_idxmin_noidx(self):\n        def test_impl(s):\n            return s.idxmin()\n\n        hpat_func = self.jit(test_impl)\n\n        data_test = [[6, 6, 2, 1, 3, 3, 2, 1, 2],\n                     [1.1, 0.3, 2.1, 1, 3, 0.3, 2.1, 1.1, 2.2],\n                     [6, 6.1, 2.2, 1, 3, 0, 2.2, 1, 2],\n                     [6, 6, 2, 1, 3, np.inf, np.nan, np.nan, np.nan],\n                     [3., 5.3, np.nan, np.nan, np.inf, np.inf, 4.4, 3.7, 8.9]\n                     ]\n\n        for input_data in data_test:\n            s = pd.Series(input_data)\n\n            result_ref = test_impl(s)\n            result = hpat_func(s)\n            self.assertEqual(result, result_ref)\n\n    def test_series_idxmin_idx(self):\n        def test_impl(s):\n            return s.idxmin()\n\n        hpat_func = self.jit(test_impl)\n\n        data_test = [[6, 6, 2, 1, 3, 3, 2, 1, 2],\n                     [1.1, 0.3, 2.1, 1, 3, 0.3, 2.1, 1.1, 2.2],\n                     [6, 6.1, 2.2, 1, 3, 0, 2.2, 1, 2],\n                     [6, 6, 2, 1, 3, -np.inf, np.nan, np.inf, np.nan],\n                     [3., 5.3, np.nan, np.nan, np.inf, np.inf, 4.4, 3.7, 8.9]\n                     ]\n\n        for input_data in data_test:\n            for index_data in data_test:\n                s = pd.Series(input_data, index_data)\n                result_ref = test_impl(s)\n                result = hpat_func(s)\n                if np.isnan(result) or np.isnan(result_ref):\n                    self.assertEqual(np.isnan(result), np.isnan(result_ref))\n                else:\n                    self.assertEqual(result, result_ref)\n\n    def test_series_idxmax1(self):\n        def test_impl(a):\n            return a.idxmax()\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        np.random.seed(0)\n        s = pd.Series(np.random.ranf(n))\n        np.testing.assert_array_equal(hpat_func(s), test_impl(s))\n\n    @unittest.skip(""Skipna is not implemented"")\n    def test_series_idxmax_str_idx(self):\n        def test_impl(s):\n            return s.idxmax(skipna=False)\n\n        hpat_func = self.jit(test_impl)\n\n        s = pd.Series([8, 6, 34, np.nan], [\'a\', \'ab\', \'abc\', \'c\'])\n        self.assertEqual(hpat_func(s), test_impl(s))\n\n    def test_series_idxmax_noidx(self):\n        def test_impl(s):\n            return s.idxmax()\n\n        hpat_func = self.jit(test_impl)\n\n        data_test = [[6, 6, 2, 1, 3, 3, 2, 1, 2],\n                     [1.1, 0.3, 2.1, 1, 3, 0.3, 2.1, 1.1, 2.2],\n                     [6, 6.1, 2.2, 1, 3, 0, 2.2, 1, 2],\n                     [6, 6, 2, 1, 3, np.inf, np.nan, np.inf, np.nan],\n                     [3., 5.3, np.nan, np.nan, np.inf, np.inf, 4.4, 3.7, 8.9]\n                     ]\n\n        for input_data in data_test:\n            s = pd.Series(input_data)\n\n            result_ref = test_impl(s)\n            result = hpat_func(s)\n            self.assertEqual(result, result_ref)\n\n    def test_series_idxmax_idx(self):\n        def test_impl(s):\n            return s.idxmax()\n\n        hpat_func = self.jit(test_impl)\n\n        data_test = [[6, 6, 2, 1, 3, 3, 2, 1, 2],\n                     [1.1, 0.3, 2.1, 1, 3, 0.3, 2.1, 1.1, 2.2],\n                     [6, 6.1, 2.2, 1, 3, 0, 2.2, 1, 2],\n                     [6, 6, 2, 1, 3, np.nan, np.nan, np.nan, np.nan],\n                     [3., 5.3, np.nan, np.nan, np.inf, np.inf, 4.4, 3.7, 8.9]\n                     ]\n\n        for input_data in data_test:\n            for index_data in data_test:\n                s = pd.Series(input_data, index_data)\n                result_ref = test_impl(s)\n                result = hpat_func(s)\n                if np.isnan(result) or np.isnan(result_ref):\n                    self.assertEqual(np.isnan(result), np.isnan(result_ref))\n                else:\n                    self.assertEqual(result, result_ref)\n\n    @skip_sdc_jit(\'Old-style impl returns array but not Series\')\n    def test_series_sort_values_default(self):\n        """"""Verifies Series.sort_values method with default parameters\n            on a named Series of different dtypes and default index""""""\n        def test_impl(A):\n            return A.sort_values()\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        np.random.seed(0)\n        # using sequences of unique values because default sorting algorithm is not stable\n        data_to_test = [\n            [1, -0., 0.2, -3.7, np.inf, np.nan, -1.0, 2/3, 21.2, -np.inf, 9.99],\n            np.arange(-10, 20, 1),\n            np.unique(np.random.ranf(n)),\n            np.unique(np.random.randint(0, 100, n)),\n            [\'ac\', \'c\', \'cb\', \'ca\', None, \'da\', \'cc\', \'ddd\', \'d\']\n        ]\n        for data in data_to_test:\n            with self.subTest(series_data=data):\n                S = pd.Series(data, name=\'A\')\n                pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    @skip_sdc_jit(\'Old-style impl returns array but not Series\')\n    def test_series_sort_values_ascending(self):\n        """"""Verifies Series.sort_values method handles parameter \'ascending\' as a literal and non-literal value""""""\n        def test_impl(S, param_value):\n            return S.sort_values(ascending=param_value)\n\n        def test_impl_literal(S):\n            return S.sort_values(ascending=False)\n\n        hpat_func1 = self.jit(test_impl)\n        hpat_func2 = self.jit(test_impl_literal)\n\n        S = pd.Series([\'ac\', \'c\', \'cb\', \'ca\', None, \'da\', \'cc\', \'ddd\', \'d\'])\n        for ascending in (False, True):\n            with self.subTest(literal_value=\'no\', ascending=ascending):\n                pd.testing.assert_series_equal(hpat_func1(S, ascending), test_impl(S, ascending))\n\n        with self.subTest(literal_value=\'yes\'):\n            pd.testing.assert_series_equal(hpat_func2(S), test_impl_literal(S))\n\n    @skip_sdc_jit(\'Old-style impl returns array but not Series\')\n    def test_series_sort_values_invalid_axis(self):\n        """"""Verifies Series.sort_values method raises with invalid value of parameter \'axis\'""""""\n        def test_impl(S, param_value):\n            return S.sort_values(axis=param_value)\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([\'ac\', \'c\', \'cb\', \'ca\', None, \'da\', \'cc\', \'ddd\', \'d\'])\n        unsupported_values = [1, \'columns\', \'abcde\']\n        for axis in unsupported_values:\n            with self.assertRaises(Exception) as context:\n                test_impl(S, axis)\n            pandas_exception = context.exception\n\n            self.assertRaises(type(pandas_exception), hpat_func, S, axis)\n\n    @skip_sdc_jit(\'Old-style impl returns array but not Series\')\n    @skip_numba_jit(\'TODO: inplace sorting is not implemented yet\')\n    def test_series_sort_values_inplace(self):\n        def test_impl(S):\n            S.sort_values(inplace=True)\n            return S\n        hpat_func = self.jit(test_impl)\n\n        S1 = pd.Series([\'ac\', \'c\', \'cb\', \'ca\', None, \'da\', \'cc\', \'ddd\', \'d\'])\n        S2 = S1.copy()\n        pd.testing.assert_series_equal(hpat_func(S1), test_impl(S2))\n\n    @skip_sdc_jit(\'Old-style impl returns array but not Series\')\n    def test_series_sort_values_kind(self):\n        """"""Verifies Series.sort_values method support of parameter \'kind\'\n           on a unnamed Series of different dtypes and default index""""""\n        def test_impl_literal_kind(A, param_value):\n            # FIXME: use literally(kind) because, numpy.argsort is supported by Numba with literal kind value only\n            # and literally when applied inside sort_values overload_method impl is not working due to some bug in Numba\n            return A.sort_values(kind=literally(param_value))\n        hpat_func1 = self.jit(test_impl_literal_kind)\n\n        def test_impl_non_literal_kind(A, param_value):\n            return A.sort_values(kind=param_value)\n        hpat_func2 = self.jit(test_impl_non_literal_kind)\n\n        # using sequences of unique values because default sorting algorithm is not stable\n        n = 11\n        np.random.seed(0)\n        data_to_test = [\n            [1, -0., 0.2, -3.7, np.inf, np.nan, -1.0, 2/3, 21.2, -np.inf, 9.99],\n            np.arange(-10, 20, 1),\n            np.unique(np.random.ranf(n)),\n            np.unique(np.random.randint(0, 100, n)),\n            [\'ac\', \'c\', \'cb\', \'ca\', None, \'da\', \'cc\', \'ddd\', \'d\']\n        ]\n        kind_values = [\'quicksort\', \'mergesort\']\n        for data in data_to_test:\n            S = pd.Series(data=data)\n            for kind in kind_values:\n                with self.subTest(series_data=data, kind=kind):\n                    pd.testing.assert_series_equal(hpat_func1(S, kind), test_impl_literal_kind(S, kind))\n\n        kind = None\n        with self.subTest(series_data=data, kind=kind):\n            pd.testing.assert_series_equal(hpat_func2(S, kind), test_impl_non_literal_kind(S, kind))\n\n    @skip_sdc_jit(\'Old-style impl returns array but not Series\')\n    def test_series_sort_values_na_position(self):\n        """"""Verifies Series.sort_values method support of parameter \'na_position\'\n           on a unnamed Series of different dtypes and default index""""""\n        def test_impl(S, param_value):\n            # kind=mergesort is used for sort stability\n            return S.sort_values(kind=\'mergesort\', na_position=param_value)\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        np.random.seed(0)\n        data_to_test = [\n            [1, -0., 0.2, -3.7, np.inf, np.nan, -1.0, 2/3, 21.2, -np.inf, 9.99],\n            np.arange(-10, 20, 1),\n            np.random.ranf(n),\n            np.random.randint(0, 100, n),\n            [\'ac\', \'c\', None, \'\', \'cb\', \'ca\', None, \'da\', \'cc\', \'ddd\', \'\', \'\', \'d\']\n        ]\n        na_position_values = [\'first\', \'last\']\n        for data in data_to_test:\n            S = pd.Series(data=data)\n            for na_position in na_position_values:\n                with self.subTest(series_data=data, na_position=na_position):\n                    pd.testing.assert_series_equal(hpat_func(S, na_position), test_impl(S, na_position))\n\n    @skip_sdc_jit(\'Old-style impl returns array but not Series\')\n    def test_series_sort_values_index(self):\n        """"""Verifies Series.sort_values method with default parameters\n           on an unnamed integer Series and different indexes""""""\n        def test_impl(S):\n            # kind=mergesort is used for sort stability\n            return S.sort_values(kind=\'mergesort\')\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        np.random.seed(0)\n        data = np.random.randint(0, 100, n)\n        use_indexes = [\'id1\', None, \'\', \'abc\', \'a\', \'dd\', \'d1\', \'23\']\n        dtype_to_index = {\'None\': None,\n                          \'int\': np.arange(n, dtype=\'int\'),\n                          \'float\': np.arange(n, dtype=\'float\'),\n                          \'string\': [use_indexes[i] for i in np.random.randint(0, len(use_indexes), n)]}\n\n        for dtype, index_data in dtype_to_index.items():\n            with self.subTest(index_dtype=dtype, index=index_data):\n                S = pd.Series(data, index=index_data)\n                pd.testing.assert_series_equal(hpat_func(S), test_impl(S), check_names=False)\n\n    @skip_parallel\n    @skip_sdc_jit(\'Old-style impl returns array but not Series\')\n    def test_series_sort_values_full(self):\n        def test_impl(series, ascending, kind):\n            return series.sort_values(axis=0, ascending=ascending, kind=literally(kind), na_position=\'last\')\n\n        hpat_func = self.jit(test_impl)\n\n        all_data = test_global_input_data_numeric + [test_global_input_data_unicode_kind1]\n\n        for data in all_data:\n            series = pd.Series(data * 3)\n            for ascending in [True, False]:\n                for kind in [\'quicksort\', \'mergesort\']:\n                    ref_result = test_impl(series, ascending, kind=kind)\n                    jit_result = hpat_func(series, ascending, kind=kind)\n                    ref = restore_series_sort_values(series, ref_result.index, ascending)\n                    jit = restore_series_sort_values(series, jit_result.index, ascending)\n                    if kind == \'mergesort\':\n                        pd.testing.assert_series_equal(ref_result, jit_result)\n                    else:\n                        np.testing.assert_array_equal(ref_result.data, jit_result.data)\n                        self.assertEqual(ref, jit)\n\n    @skip_sdc_jit(\'Old-style impl returns array but not Series\')\n    def test_series_sort_values_full_unicode4(self):\n        def test_impl(series, ascending, kind):\n            return series.sort_values(axis=0, ascending=ascending, kind=literally(kind), na_position=\'last\')\n\n        hpat_func = self.jit(test_impl)\n\n        all_data = [test_global_input_data_unicode_kind1]\n\n        for data in all_data:\n            series = pd.Series(data * 3)\n            for ascending in [True, False]:\n                for kind in [\'quicksort\', \'mergesort\']:\n                    ref_result = test_impl(series, ascending, kind=kind)\n                    jit_result = hpat_func(series, ascending, kind=kind)\n                    ref = restore_series_sort_values(series, ref_result.index, ascending)\n                    jit = restore_series_sort_values(series, jit_result.index, ascending)\n                    if kind == \'mergesort\':\n                        pd.testing.assert_series_equal(ref_result, jit_result)\n                    else:\n                        np.testing.assert_array_equal(ref_result.data, jit_result.data)\n                        self.assertEqual(ref, jit)\n\n    @skip_parallel\n    @skip_sdc_jit(\'Old-style impl returns array but not Series\')\n    def test_series_sort_values_full_idx(self):\n        def test_impl(series, ascending, kind):\n            return series.sort_values(axis=0, ascending=ascending, kind=literally(kind), na_position=\'last\')\n\n        hpat_func = self.jit(test_impl)\n\n        all_data = test_global_input_data_numeric + [test_global_input_data_unicode_kind1]\n\n        for data in all_data:\n            data = data * 3\n            for index in [gen_srand_array(len(data)), gen_frand_array(len(data)), range(len(data))]:\n                for ascending in [True, False]:\n                    for kind in [\'quicksort\', \'mergesort\']:\n                        series = pd.Series(data, index)\n                        ref_result = test_impl(series, ascending, kind=kind)\n                        jit_result = hpat_func(series, ascending, kind=kind)\n                        ref = restore_series_sort_values(series, ref_result.index, ascending)\n                        jit = restore_series_sort_values(series, jit_result.index, ascending)\n                        if kind == \'mergesort\':\n                            pd.testing.assert_series_equal(ref_result, jit_result)\n                        else:\n                            np.testing.assert_array_equal(ref_result.data, jit_result.data)\n                            self.assertEqual(ref, jit)\n\n    @skip_numba_jit\n    def test_series_sort_values_parallel1(self):\n        # create `kde.parquet` file\n        ParquetGenerator.gen_kde_pq()\n\n        def test_impl():\n            df = pq.read_table(\'kde.parquet\').to_pandas()\n            S = df.points\n            return S.sort_values()\n        hpat_func = self.jit(test_impl)\n\n        np.testing.assert_array_equal(hpat_func(), test_impl())\n\n    def test_series_shift(self):\n        def pyfunc():\n            series = pd.Series([1.0, np.nan, -1.0, 0.0, 5e-324])\n            return series.shift()\n\n        cfunc = self.jit(pyfunc)\n        pd.testing.assert_series_equal(cfunc(), pyfunc())\n\n    def test_series_shift_name(self):\n        def pyfunc():\n            series = pd.Series([1.0, np.nan, -1.0, 0.0, 5e-324], name=\'A\')\n            return series.shift()\n\n        cfunc = self.jit(pyfunc)\n        pd.testing.assert_series_equal(cfunc(), pyfunc())\n\n    def test_series_shift_unboxing(self):\n        def pyfunc(series):\n            return series.shift()\n\n        cfunc = self.jit(pyfunc)\n        for data in test_global_input_data_float64:\n            series = pd.Series(data)\n            pd.testing.assert_series_equal(cfunc(series), pyfunc(series))\n\n    def test_series_shift_full(self):\n        def pyfunc(series, periods, freq, axis, fill_value):\n            return series.shift(periods=periods, freq=freq, axis=axis, fill_value=fill_value)\n\n        cfunc = self.jit(pyfunc)\n        freq = None\n        axis = 0\n        datas = test_global_input_data_signed_integer64 + test_global_input_data_float64\n        for data in datas:\n            for periods in [1, 2, -1]:\n                for fill_value in [-1, 0, 9.1, np.nan, -3.3, None]:\n                    with self.subTest(data=data, periods=periods, fill_value=fill_value):\n                        series = pd.Series(data)\n                        jit_result = cfunc(series, periods, freq, axis, fill_value)\n                        ref_result = pyfunc(series, periods, freq, axis, fill_value)\n                        pd.testing.assert_series_equal(jit_result, ref_result)\n\n    @sdc_limitation\n    def test_series_shift_0period(self):\n        """"""SDC implementation always changes dtype to float. Even in case of period = 0""""""\n        def pyfunc():\n            series = pd.Series([6, 4, 3])\n            return series.shift(periods=0)\n\n        cfunc = self.jit(pyfunc)\n        ref_result = pyfunc()\n        pd.testing.assert_series_equal(cfunc(), ref_result)\n\n    def test_series_shift_0period_sdc(self):\n        def pyfunc():\n            series = pd.Series([6, 4, 3])\n            return series.shift(periods=0)\n\n        cfunc = self.jit(pyfunc)\n        ref_result = pyfunc()\n        pd.testing.assert_series_equal(cfunc(), ref_result, check_dtype=False)\n\n    @sdc_limitation\n    def test_series_shift_uint_int(self):\n        """"""SDC assumes fill_value is int and unifies unsigned int and int to float. Even if fill_value is positive""""""\n        def pyfunc():\n            series = pd.Series([max_uint64])\n            return series.shift(fill_value=0)\n\n        cfunc = self.jit(pyfunc)\n        ref_result = pyfunc()\n        pd.testing.assert_series_equal(cfunc(), ref_result)\n\n    def test_series_shift_uint_int_sdc(self):\n        def pyfunc():\n            series = pd.Series([max_uint64])\n            return series.shift(fill_value=0)\n\n        cfunc = self.jit(pyfunc)\n        ref_result = pyfunc()\n        pd.testing.assert_series_equal(cfunc(), ref_result, check_dtype=False)\n\n    def test_series_shift_str(self):\n        def pyfunc(series):\n            return series.shift()\n\n        cfunc = self.jit(pyfunc)\n        series = pd.Series(test_global_input_data_unicode_kind4)\n        with self.assertRaises(TypingError) as raises:\n            cfunc(series)\n        msg = \'Method shift(). The object self.data.dtype\\n given: unicode_type\\n expected: number\\n\'\n        self.assertIn(msg.format(types.unicode_type), str(raises.exception))\n\n    def test_series_shift_fill_str(self):\n        def pyfunc(series, fill_value):\n            return series.shift(fill_value=fill_value)\n\n        cfunc = self.jit(pyfunc)\n        series = pd.Series(test_global_input_data_float64[0])\n        with self.assertRaises(TypingError) as raises:\n            cfunc(series, fill_value=\'unicode\')\n        msg = \'Method shift(). The object fill_value\\n given: unicode_type\\n expected: number\\n\'\n        self.assertIn(msg.format(types.unicode_type), str(raises.exception))\n\n    def test_series_shift_unsupported_params(self):\n        def pyfunc(series, freq, axis):\n            return series.shift(freq=freq, axis=axis)\n\n        cfunc = self.jit(pyfunc)\n        series = pd.Series(test_global_input_data_float64[0])\n        with self.assertRaises(TypingError) as raises:\n            cfunc(series, freq=\'12H\', axis=0)\n        msg = \'Method shift(). The object freq\\n given: unicode_type\\n expected: None\\n\'\n        self.assertIn(msg.format(types.unicode_type), str(raises.exception))\n\n        with self.assertRaises(TypingError) as raises:\n            cfunc(series, freq=None, axis=1)\n        msg = \'Method shift(). Unsupported parameters. Given axis != 0\'\n        self.assertIn(msg, str(raises.exception))\n\n    def test_series_shift_index_str(self):\n        def test_impl(S):\n            return S.shift()\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([np.nan, 2., 3., 5., np.nan, 6., 7.], index=[\'a\', \'b\', \'c\', \'d\', \'e\', \'f\', \'g\'])\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    def test_series_shift_index_int(self):\n        def test_impl(S):\n            return S.shift()\n\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([np.nan, 2., 3., 5., np.nan, 6., 7.], index=[1, 2, 3, 4, 5, 6, 7])\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    def test_series_getattr_index1(self):\n        def test_impl():\n            A = pd.Series([1, 2, 3], index=[\'A\', \'C\', \'B\'])\n            return A.index\n\n        hpat_func = self.jit(test_impl)\n        np.testing.assert_array_equal(hpat_func(), test_impl())\n\n    def test_series_getattr_index2(self):\n        def test_impl():\n            A = pd.Series([1, 2, 3], index=[0, 1, 2])\n            return A.index\n\n        hpat_func = self.jit(test_impl)\n        np.testing.assert_array_equal(hpat_func(), test_impl())\n\n    def test_series_getattr_index3(self):\n        def test_impl():\n            A = pd.Series([1, 2, 3])\n            return A.index\n\n        hpat_func = self.jit(test_impl)\n        np.testing.assert_array_equal(hpat_func(), test_impl())\n\n    def test_series_take_index_default(self):\n        def pyfunc():\n            series = pd.Series([1.0, 13.0, 9.0, -1.0, 7.0])\n            indices = [1, 3]\n            return series.take(indices)\n\n        cfunc = self.jit(pyfunc)\n        ref_result = pyfunc()\n        result = cfunc()\n        pd.testing.assert_series_equal(ref_result, result)\n\n    def test_series_take_index_default_unboxing(self):\n        def pyfunc(series, indices):\n            return series.take(indices)\n\n        cfunc = self.jit(pyfunc)\n        series = pd.Series([1.0, 13.0, 9.0, -1.0, 7.0])\n        indices = [1, 3]\n        ref_result = pyfunc(series, indices)\n        result = cfunc(series, indices)\n        pd.testing.assert_series_equal(ref_result, result)\n\n    def test_series_take_index_int(self):\n        def pyfunc():\n            series = pd.Series([1.0, 13.0, 9.0, -1.0, 7.0], index=[3, 0, 4, 2, 1])\n            indices = [1, 3]\n            return series.take(indices)\n\n        cfunc = self.jit(pyfunc)\n        ref_result = pyfunc()\n        result = cfunc()\n        pd.testing.assert_series_equal(ref_result, result)\n\n    def test_series_take_index_int_unboxing(self):\n        def pyfunc(series, indices):\n            return series.take(indices)\n\n        cfunc = self.jit(pyfunc)\n        series = pd.Series([1.0, 13.0, 9.0, -1.0, 7.0], index=[3, 0, 4, 2, 1])\n        indices = [1, 3]\n        ref_result = pyfunc(series, indices)\n        result = cfunc(series, indices)\n        pd.testing.assert_series_equal(ref_result, result)\n\n    def test_series_take_index_str(self):\n        def pyfunc():\n            series = pd.Series([1.0, 13.0, 9.0, -1.0, 7.0], index=[\'test\', \'series\', \'take\', \'str\', \'index\'])\n            indices = [1, 3]\n            return series.take(indices)\n\n        cfunc = self.jit(pyfunc)\n        ref_result = pyfunc()\n        result = cfunc()\n        pd.testing.assert_series_equal(ref_result, result)\n\n    def test_series_take_index_str_unboxing(self):\n        def pyfunc(series, indices):\n            return series.take(indices)\n\n        cfunc = self.jit(pyfunc)\n        series = pd.Series([1.0, 13.0, 9.0, -1.0, 7.0], index=[\'test\', \'series\', \'take\', \'str\', \'index\'])\n        indices = [1, 3]\n        ref_result = pyfunc(series, indices)\n        result = cfunc(series, indices)\n        pd.testing.assert_series_equal(ref_result, result)\n\n    def test_series_iterator_int(self):\n        def test_impl(A):\n            return [i for i in A]\n\n        A = pd.Series([3, 2, 1, 5, 4])\n        hpat_func = self.jit(test_impl)\n        np.testing.assert_array_equal(hpat_func(A), test_impl(A))\n\n    def test_series_iterator_float(self):\n        def test_impl(A):\n            return [i for i in A]\n\n        A = pd.Series([0.3, 0.2222, 0.1756, 0.005, 0.4])\n        hpat_func = self.jit(test_impl)\n        np.testing.assert_array_equal(hpat_func(A), test_impl(A))\n\n    def test_series_iterator_boolean(self):\n        def test_impl(A):\n            return [i for i in A]\n\n        A = pd.Series([True, False])\n        hpat_func = self.jit(test_impl)\n        np.testing.assert_array_equal(hpat_func(A), test_impl(A))\n\n    def test_series_iterator_string(self):\n        def test_impl(A):\n            return [i for i in A]\n\n        A = pd.Series([\'a\', \'ab\', \'abc\', \'\', \'dddd\'])\n        hpat_func = self.jit(test_impl)\n        np.testing.assert_array_equal(hpat_func(A), test_impl(A))\n\n    def test_series_iterator_one_value(self):\n        def test_impl(A):\n            return [i for i in A]\n\n        A = pd.Series([5])\n        hpat_func = self.jit(test_impl)\n        np.testing.assert_array_equal(hpat_func(A), test_impl(A))\n\n    def test_series_iterator_no_param(self):\n        def test_impl():\n            A = pd.Series([3, 2, 1, 5, 4])\n            return [i for i in A]\n\n        hpat_func = self.jit(test_impl)\n        np.testing.assert_array_equal(hpat_func(), test_impl())\n\n    def test_series_iterator_empty(self):\n        def test_impl(A):\n            return [i for i in A]\n\n        A = pd.Series([np.int64(x) for x in range(0)])\n        hpat_func = self.jit(test_impl)\n        np.testing.assert_array_equal(hpat_func(A), test_impl(A))\n\n    def test_series_getattr_default_index(self):\n        def test_impl():\n            A = pd.Series([3, 2, 1, 5, 4])\n            return A.index\n\n        hpat_func = self.jit(test_impl)\n        np.testing.assert_array_equal(hpat_func(), test_impl())\n\n    @unittest.skip(""Implement drop_duplicates for Series"")\n    def test_series_drop_duplicates(self):\n        def test_impl():\n            A = pd.Series([\'lama\', \'cow\', \'lama\', \'beetle\', \'lama\', \'hippo\'])\n            return A.drop_duplicates()\n\n        hpat_func = self.jit(test_impl)\n        pd.testing.assert_series_equal(hpat_func(), test_impl())\n\n    @skip_sdc_jit(\'Not implemented in sequential transport layer\')\n    def test_series_quantile(self):\n        def test_impl():\n            a = pd.Series([1, 2.5, .5, 3, 5])\n            return a.quantile()\n\n        hpat_func = self.jit(test_impl)\n        np.testing.assert_equal(hpat_func(), test_impl())\n\n    @skip_sdc_jit(""Series.quantile() parameter as a list unsupported"")\n    def test_series_quantile_q_vector(self):\n        def test_series_quantile_q_vector_impl(S, param1):\n            return S.quantile(param1)\n\n        s = pd.Series(np.random.ranf(100))\n        hpat_func = self.jit(test_series_quantile_q_vector_impl)\n\n        param1 = [0.0, 0.25, 0.5, 0.75, 1.0]\n        result_ref = test_series_quantile_q_vector_impl(s, param1)\n        result = hpat_func(s, param1)\n        np.testing.assert_equal(result, result_ref)\n\n    @unittest.skip(""Implement unique without sorting like in pandas"")\n    def test_unique(self):\n        def test_impl(S):\n            return S.unique()\n\n        hpat_func = self.jit(test_impl)\n        S = pd.Series([2, 1, 3, 3])\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    def test_unique_sorted(self):\n        def test_impl(S):\n            return S.unique()\n\n        hpat_func = self.jit(test_impl)\n        n = 11\n        S = pd.Series(np.arange(n))\n        S[2] = 0\n        np.testing.assert_array_equal(hpat_func(S), test_impl(S))\n\n    def test_unique_str(self):\n        def test_impl():\n            data = pd.Series([\'aa\', \'aa\', \'b\', \'b\', \'cccc\', \'dd\', \'ddd\', \'dd\'])\n            return data.unique()\n\n        hpat_func = self.jit(test_impl)\n\n        # since the orider of the elements are diffrent - check count of elements only\n        ref_result = test_impl().size\n        result = hpat_func().size\n        np.testing.assert_array_equal(ref_result, result)\n\n    def test_series_std(self):\n        def pyfunc():\n            series = pd.Series([1.0, np.nan, -1.0, 0.0, 5e-324])\n            return series.std()\n\n        cfunc = self.jit(pyfunc)\n        ref_result = pyfunc()\n        result = cfunc()\n        np.testing.assert_equal(ref_result, result)\n\n    @skip_sdc_jit(\'Series.std() parameters ""skipna"" and ""ddof"" unsupported\')\n    def test_series_std_unboxing(self):\n        def pyfunc(series, skipna, ddof):\n            return series.std(skipna=skipna, ddof=ddof)\n\n        cfunc = self.jit(pyfunc)\n        for data in test_global_input_data_numeric + [[]]:\n            series = pd.Series(data)\n            for ddof in [0, 1]:\n                for skipna in [True, False]:\n                    ref_result = pyfunc(series, skipna=skipna, ddof=ddof)\n                    result = cfunc(series, skipna=skipna, ddof=ddof)\n                    np.testing.assert_equal(ref_result, result)\n\n    @skip_sdc_jit(\'Series.std() strings as input data unsupported\')\n    def test_series_std_str(self):\n        def pyfunc(series):\n            return series.std()\n\n        cfunc = self.jit(pyfunc)\n        series = pd.Series(test_global_input_data_unicode_kind4)\n        with self.assertRaises(TypingError) as raises:\n            cfunc(series)\n        msg = \'Method std(). The object self.data\\n given: StringArrayType()\\n expected: number\\n\'\n        self.assertIn(msg.format(types.unicode_type), str(raises.exception))\n\n    @skip_sdc_jit(\'Series.std() parameters ""axis"", ""level"", ""numeric_only"" unsupported\')\n    def test_series_std_unsupported_params(self):\n        def pyfunc(series, axis, level, numeric_only):\n            return series.std(axis=axis, level=level, numeric_only=numeric_only)\n\n        cfunc = self.jit(pyfunc)\n        series = pd.Series(test_global_input_data_float64[0])\n        with self.assertRaises(TypingError) as raises:\n            cfunc(series, axis=1, level=None, numeric_only=None)\n        msg = \'Method std(). The object axis\\n given: int64\\n expected: None\\n\'\n        self.assertIn(msg, str(raises.exception))\n\n        with self.assertRaises(TypingError) as raises:\n            cfunc(series, axis=None, level=1, numeric_only=None)\n        msg = \'Method std(). The object level\\n given: int64\\n expected: None\\n\'\n        self.assertIn(msg, str(raises.exception))\n\n        with self.assertRaises(TypingError) as raises:\n            cfunc(series, axis=None, level=None, numeric_only=True)\n        msg = \'Method std(). The object numeric_only\\n given: bool\\n expected: None\\n\'\n        self.assertIn(msg, str(raises.exception))\n\n    def test_series_nunique(self):\n        def test_series_nunique_impl(s):\n            return s.nunique()\n\n        def test_series_nunique_param1_impl(s, dropna):\n            return s.nunique(dropna)\n\n        hpat_func = self.jit(test_series_nunique_impl)\n\n        the_same_string = ""the same string""\n        test_input_data = []\n        data_simple = [[6, 6, 2, 1, 3, 3, 2, 1, 2],\n                       [1.1, 0.3, 2.1, 1, 3, 0.3, 2.1, 1.1, 2.2],\n                       [6, 6.1, 2.2, 1, 3, 3, 2.2, 1, 2],\n                       [\'aa\', \'aa\', \'b\', \'b\', \'cccc\', \'dd\', \'ddd\', \'dd\'],\n                       [\'aa\', \'copy aa\', the_same_string, \'b\', \'b\', \'cccc\', the_same_string,\n                        \'dd\', \'ddd\', \'dd\', \'copy aa\', \'copy aa\'],\n                       []\n                       ]\n\n        data_extra = [[6, 6, np.nan, 2, np.nan, 1, 3, 3, np.inf, 2, 1, 2, np.inf],\n                      [1.1, 0.3, np.nan, 1.0, np.inf, 0.3, 2.1, np.nan, 2.2, np.inf],\n                      [1.1, 0.3, np.nan, 1, np.inf, 0, 1.1, np.nan, 2.2, np.inf, 2, 2],\n                      [\'aa\', np.nan, \'b\', \'b\', \'cccc\', np.nan, \'ddd\', \'dd\'],\n                      [np.nan, \'copy aa\', the_same_string, \'b\', \'b\', \'cccc\', the_same_string,\n                       \'dd\', \'ddd\', \'dd\', \'copy aa\', \'copy aa\'],\n                      [np.nan, np.nan, np.nan],\n                      [np.nan, np.nan, np.inf],\n                      ]\n\n        if sdc.config.config_pipeline_hpat_default:\n            """"""\n            SDC pipeline Series.nunique() does not support numpy.nan\n            """"""\n\n            test_input_data = data_simple\n        else:\n            test_input_data = data_simple + data_extra\n\n        for input_data in test_input_data:\n            s = pd.Series(input_data)\n\n            result_ref = test_series_nunique_impl(s)\n            result = hpat_func(s)\n            self.assertEqual(result, result_ref)\n\n            if not sdc.config.config_pipeline_hpat_default:\n                """"""\n                SDC pipeline does not support parameter to Series.nunique(dropna=True)\n                """"""\n\n                hpat_func_param1 = self.jit(test_series_nunique_param1_impl)\n\n                for param1 in [True, False]:\n                    result_param1_ref = test_series_nunique_param1_impl(s, param1)\n                    result_param1 = hpat_func_param1(s, param1)\n                    self.assertEqual(result_param1, result_param1_ref)\n\n    def test_series_var(self):\n        def pyfunc():\n            series = pd.Series([1.0, np.nan, -1.0, 0.0, 5e-324])\n            return series.var()\n\n        cfunc = self.jit(pyfunc)\n        np.testing.assert_equal(pyfunc(), cfunc())\n\n    @skip_sdc_jit(\'Series.var() data [max_uint64, max_uint64] unsupported\')\n    def test_series_var_unboxing(self):\n        def pyfunc(series):\n            return series.var()\n\n        cfunc = self.jit(pyfunc)\n        for data in test_global_input_data_numeric + [[]]:\n            series = pd.Series(data)\n            np.testing.assert_equal(pyfunc(series), cfunc(series))\n\n    @skip_sdc_jit(\'Series.var() parameters ""ddof"" and ""skipna"" unsupported\')\n    def test_series_var_full(self):\n        def pyfunc(series, skipna, ddof):\n            return series.var(skipna=skipna, ddof=ddof)\n\n        cfunc = self.jit(pyfunc)\n        for data in test_global_input_data_numeric + [[]]:\n            series = pd.Series(data)\n            for ddof in [0, 1]:\n                for skipna in [True, False]:\n                    ref_result = pyfunc(series, skipna=skipna, ddof=ddof)\n                    result = cfunc(series, skipna=skipna, ddof=ddof)\n                    np.testing.assert_equal(ref_result, result)\n\n    @skip_sdc_jit(\'Series.var() strings as input data unsupported\')\n    def test_series_var_str(self):\n        def pyfunc(series):\n            return series.var()\n\n        cfunc = self.jit(pyfunc)\n        series = pd.Series(test_global_input_data_unicode_kind4)\n        with self.assertRaises(TypingError) as raises:\n            cfunc(series)\n        msg = \'Method var(). The object self.data\\n given: StringArrayType()\\n expected: number\\n\'\n        self.assertIn(msg, str(raises.exception))\n\n    @skip_sdc_jit(\'Series.var() parameters ""axis"", ""level"", ""numeric_only"" unsupported\')\n    def test_series_var_unsupported_params(self):\n        def pyfunc(series, axis, level, numeric_only):\n            return series.var(axis=axis, level=level, numeric_only=numeric_only)\n\n        cfunc = self.jit(pyfunc)\n        series = pd.Series(test_global_input_data_float64[0])\n        with self.assertRaises(TypingError) as raises:\n            cfunc(series, axis=1, level=None, numeric_only=None)\n        msg = \'Method var(). The object axis\\n given: int64\\n expected: None\\n\'\n        self.assertIn(msg, str(raises.exception))\n\n        with self.assertRaises(TypingError) as raises:\n            cfunc(series, axis=None, level=1, numeric_only=None)\n        msg = \'Method var(). The object level\\n given: int64\\n expected: None\\n\'\n        self.assertIn(msg, str(raises.exception))\n\n        with self.assertRaises(TypingError) as raises:\n            cfunc(series, axis=None, level=None, numeric_only=True)\n        msg = \'Method var(). The object numeric_only\\n given: bool\\n expected: None\\n\'\n        self.assertIn(msg, str(raises.exception))\n\n    def test_series_count(self):\n        def test_series_count_impl(S):\n            return S.count()\n\n        hpat_func = self.jit(test_series_count_impl)\n\n        the_same_string = ""the same string""\n        test_input_data = [[6, 6, 2, 1, 3, 3, 2, 1, 2],\n                           [1.1, 0.3, 2.1, 1, 3, 0.3, 2.1, 1.1, 2.2],\n                           [6, 6.1, 2.2, 1, 3, 3, 2.2, 1, 2],\n                           [\'aa\', \'aa\', \'b\', \'b\', \'cccc\', \'dd\', \'ddd\', \'dd\'],\n                           [\'aa\', None, \'\', \'\', None, \'cccc\', \'dd\', \'ddd\', None, \'dd\'],\n                           [\'aa\', \'copy aa\', the_same_string, \'b\', \'b\', \'cccc\', the_same_string, \'dd\', \'ddd\', \'dd\',\n                            \'copy aa\', \'copy aa\'],\n                           [],\n                           [6, 6, np.nan, 2, np.nan, 1, 3, 3, np.inf, 2, 1, 2, np.inf],\n                           [1.1, 0.3, np.nan, 1.0, np.inf, 0.3, 2.1, np.nan, 2.2, np.inf],\n                           [1.1, 0.3, np.nan, 1, np.inf, 0, 1.1, np.nan, 2.2, np.inf, 2, 2],\n                           [np.nan, np.nan, np.nan],\n                           [np.nan, np.nan, np.inf]\n                           ]\n\n        for input_data in test_input_data:\n            S = pd.Series(input_data)\n\n            result_ref = test_series_count_impl(S)\n            result = hpat_func(S)\n            self.assertEqual(result, result_ref)\n\n    @skip_sdc_jit(\'Series.cumsum() np.nan as input data unsupported\')\n    def test_series_cumsum(self):\n        def test_impl():\n            series = pd.Series([1.0, np.nan, -1.0, 0.0, 5e-324])\n            return series.cumsum()\n        hpat_func = self.jit(test_impl)\n\n        result = hpat_func()\n        result_ref = test_impl()\n        pd.testing.assert_series_equal(result, result_ref)\n\n    def _gen_cumulative_data_skip(self):\n        yield [min_int64, max_int64, max_int64, min_int64]\n        yield [max_uint64, max_uint64]\n\n    def _gen_cumulative_data(self):\n        for case in test_global_input_data_numeric:\n            if case not in self._gen_cumulative_data_skip():\n                yield case\n        yield [1.0, np.nan, -1.0, 0.0, 5e-324]\n        yield []\n\n    def _check_cumulative(self, pyfunc, generator, **kwds):\n        cfunc = self.jit(pyfunc)\n\n        for data in generator():\n            with self.subTest(series_data=data):\n                S = pd.Series(data)\n                pd.testing.assert_series_equal(cfunc(S, **kwds), pyfunc(S, **kwds))\n\n    @skip_sdc_jit(\'Series.cumsum() np.nan as input data unsupported\')\n    def test_series_cumsum_unboxing(self):\n        def test_impl(s):\n            return s.cumsum()\n\n        self._check_cumulative(test_impl, self._gen_cumulative_data)\n\n    def _cumsum_full_usecase(self):\n        def test_impl(s, axis, skipna):\n            return s.cumsum(axis=axis, skipna=skipna)\n        return test_impl\n\n    @skip_sdc_jit(\'Series.cumsum() parameters ""axis"", ""skipna"" unsupported\')\n    def test_series_cumsum_full(self):\n        axis = None\n        for skipna in [True, False]:\n            with self.subTest(skipna=skipna):\n                self._check_cumulative(self._cumsum_full_usecase(),\n                                       self._gen_cumulative_data, axis=axis, skipna=skipna)\n\n    @unittest.expectedFailure\n    def test_series_cumsum_expectedFailure(self):\n        axis = None\n        for skipna in [True, False]:\n            with self.subTest(skipna=skipna):\n                self._check_cumulative(self._cumsum_full_usecase(),\n                                       self._gen_cumulative_data_skip, axis=axis, skipna=skipna)\n\n    @skip_sdc_jit(\'Series.cumsum() strings as input data unsupported\')\n    def test_series_cumsum_str(self):\n        def test_impl(s):\n            return s.cumsum()\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series(test_global_input_data_unicode_kind4)\n        with self.assertRaises(TypingError) as raises:\n            hpat_func(S)\n        msg = \'TypingError: Method cumsum(). The object self.data.dtype\\n given: unicode_type\\n expected: numeric\\n\'\n        self.assertIn(msg, str(raises.exception))\n\n    @skip_sdc_jit(\'Series.cumsum() parameter ""axis"" unsupported\')\n    def test_series_cumsum_unsupported_axis(self):\n        def test_impl(s, axis):\n            return s.cumsum(axis=axis)\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series(test_global_input_data_float64[0])\n        for axis in [0, 1]:\n            with self.subTest(axis=axis):\n                with self.assertRaises(TypingError) as raises:\n                    hpat_func(S, axis=axis)\n                msg = \'TypingError: Method cumsum(). The object axis\\n given: int64\\n expected: None\\n\'\n                self.assertIn(msg, str(raises.exception))\n\n    def test_series_cov1(self):\n        def test_impl(s1, s2):\n            return s1.cov(s2)\n        hpat_func = self.jit(test_impl)\n\n        for pair in _cov_corr_series:\n            s1, s2 = pair\n            np.testing.assert_almost_equal(\n                hpat_func(s1, s2), test_impl(s1, s2),\n                err_msg=\'s1={}\\ns2={}\'.format(s1, s2))\n\n    @skip_sdc_jit(\'Series.cov() parameter ""min_periods"" unsupported\')\n    def test_series_cov(self):\n        def test_series_cov_impl(s1, s2, min_periods=None):\n            return s1.cov(s2, min_periods)\n\n        hpat_func = self.jit(test_series_cov_impl)\n        test_input_data1 = [[.2, .0, .6, .2],\n                            [.2, .0, .6, .2, .5, .6, .7, .8],\n                            [],\n                            [2, 0, 6, 2],\n                            [.2, .1, np.nan, .5, .3],\n                            [-1, np.nan, 1, np.inf]]\n        test_input_data2 = [[.3, .6, .0, .1],\n                            [.3, .6, .0, .1, .8],\n                            [],\n                            [3, 6, 0, 1],\n                            [.3, .2, .9, .6, np.nan],\n                            [np.nan, np.nan, np.inf, np.nan]]\n        for input_data1 in test_input_data1:\n            for input_data2 in test_input_data2:\n                s1 = pd.Series(input_data1)\n                s2 = pd.Series(input_data2)\n                for period in [None, 2, 1, 8, -4]:\n                    with self.subTest(input_data1=input_data1, input_data2=input_data2, min_periods=period):\n                        result_ref = test_series_cov_impl(s1, s2, min_periods=period)\n                        result = hpat_func(s1, s2, min_periods=period)\n                        np.testing.assert_allclose(result, result_ref)\n\n    @skip_sdc_jit(\'Series.cov() parameter ""min_periods"" unsupported\')\n    def test_series_cov_unsupported_dtype(self):\n        def test_series_cov_impl(s1, s2, min_periods=None):\n            return s1.cov(s2, min_periods=min_periods)\n\n        hpat_func = self.jit(test_series_cov_impl)\n        s1 = pd.Series([.2, .0, .6, .2])\n        s2 = pd.Series([\'abcdefgh\', \'a\', \'abcdefg\', \'ab\', \'abcdef\', \'abc\'])\n        s3 = pd.Series([\'aaaaa\', \'bbbb\', \'ccc\', \'dd\', \'e\'])\n        s4 = pd.Series([.3, .6, .0, .1])\n\n        with self.assertRaises(TypingError) as raises:\n            hpat_func(s1, s2, min_periods=5)\n        msg = \'Method cov(). The object other.data\'\n        self.assertIn(msg, str(raises.exception))\n\n        with self.assertRaises(TypingError) as raises:\n            hpat_func(s3, s4, min_periods=5)\n        msg = \'Method cov(). The object self.data\'\n        self.assertIn(msg, str(raises.exception))\n\n    @skip_sdc_jit(\'Series.cov() parameter ""min_periods"" unsupported\')\n    def test_series_cov_unsupported_period(self):\n        def test_series_cov_impl(s1, s2, min_periods=None):\n            return s1.cov(s2, min_periods)\n\n        hpat_func = self.jit(test_series_cov_impl)\n        s1 = pd.Series([.2, .0, .6, .2])\n        s2 = pd.Series([.3, .6, .0, .1])\n\n        with self.assertRaises(TypingError) as raises:\n            hpat_func(s1, s2, min_periods=\'aaaa\')\n        msg = \'Method cov(). The object min_periods\'\n        self.assertIn(msg, str(raises.exception))\n\n        with self.assertRaises(TypingError) as raises:\n            hpat_func(s1, s2, min_periods=0.5)\n        msg = \'Method cov(). The object min_periods\'\n        self.assertIn(msg, str(raises.exception))\n\n    @skip_numba_jit\n    @skip_sdc_jit(\'Series.pct_change unsupported some Series\')\n    def test_series_pct_change(self):\n        def test_series_pct_change_impl(S, periods, method):\n            return S.pct_change(periods=periods, fill_method=method, limit=None, freq=None)\n\n        hpat_func = self.jit(test_series_pct_change_impl)\n        test_input_data = [\n            [],\n            [np.nan, np.nan, np.nan],\n            [np.nan, np.nan, np.inf],\n            [0] * 8,\n            [0, 0, 0, np.nan, np.nan, 0, 0, np.nan, np.inf, 0, 0, np.inf, np.inf],\n            [1.1, 0.3, np.nan, 1, np.inf, 0, 1.1, np.nan, 2.2, np.inf, 2, 2],\n            [1, 2, 3, 4, np.nan, np.inf, 0, 0, np.nan, np.nan]\n        ]\n        for input_data in test_input_data:\n            S = pd.Series(input_data)\n            for periods in [0, 1, 2, 5, 10, -1, -2, -5]:\n                for method in [None, \'pad\', \'ffill\', \'backfill\', \'bfill\']:\n                    result_ref = test_series_pct_change_impl(S, periods, method)\n                    result = hpat_func(S, periods, method)\n                    pd.testing.assert_series_equal(result, result_ref)\n\n    @skip_sdc_jit(\'Series.pct_change() strings as input data unsupported\')\n    def test_series_pct_change_str(self):\n        def test_series_pct_change_impl(S):\n            return S.pct_change(periods=1, fill_method=\'pad\', limit=None, freq=None)\n\n        hpat_func = self.jit(test_series_pct_change_impl)\n        S = pd.Series(test_global_input_data_unicode_kind4)\n\n        with self.assertRaises(TypingError) as raises:\n            hpat_func(S)\n        msg = \'Method pct_change(). The object self.data\'\n        self.assertIn(msg, str(raises.exception))\n\n    @skip_sdc_jit(\'Series.pct_change() does not raise an exception\')\n    def test_series_pct_change_not_supported(self):\n        def test_series_pct_change_impl(S, periods=1, fill_method=\'pad\', limit=None, freq=None):\n            return S.pct_change(periods=periods, fill_method=fill_method, limit=limit, freq=freq)\n\n        hpat_func = self.jit(test_series_pct_change_impl)\n        S = pd.Series([0, 0, 0, np.nan, np.nan, 0, 0, np.nan, np.inf, 0, 0, np.inf, np.inf])\n        with self.assertRaises(ValueError) as raises:\n            hpat_func(S, fill_method=\'ababa\')\n        msg = \'Method pct_change(). Unsupported parameter. The function uses fill_method pad (ffill) or backfill (bfill) or None.\'\n        self.assertIn(msg, str(raises.exception))\n\n        with self.assertRaises(TypingError) as raises:\n            hpat_func(S, limit=5)\n        msg = \'Method pct_change(). The object limit\'\n        self.assertIn(msg, str(raises.exception))\n\n        with self.assertRaises(TypingError) as raises:\n            hpat_func(S, freq=5)\n        msg = \'Method pct_change(). The object freq\'\n        self.assertIn(msg, str(raises.exception))\n\n        with self.assertRaises(TypingError) as raises:\n            hpat_func(S, fill_method=1.6)\n        msg = \'Method pct_change(). The object fill_method\'\n        self.assertIn(msg, str(raises.exception))\n\n        with self.assertRaises(TypingError) as raises:\n            hpat_func(S, periods=1.6)\n        msg = \'Method pct_change(). The object periods\'\n        self.assertIn(msg, str(raises.exception))\n\n    def test_series_setitem_value_int(self):\n        def test_impl(S, val):\n            S[3] = val\n            return S\n\n        hpat_func = self.jit(test_impl)\n        data_to_test = [[0, 1, 2, 3, 4]]\n\n        for data in data_to_test:\n            S1 = pd.Series(data)\n            S2 = S1.copy(deep=True)\n            value = 50\n            result_ref = test_impl(S1, value)\n            result = hpat_func(S2, value)\n            pd.testing.assert_series_equal(result_ref, result)\n\n    def test_series_setitem_value_float(self):\n        def test_impl(S, val):\n            S[3] = val\n            return S\n\n        hpat_func = self.jit(test_impl)\n        data_to_test = [[0, 0, 0, np.nan, np.nan, 0, 0, np.nan, np.inf, 0, 0, np.inf, np.inf],\n                        [1.1, 0.3, np.nan, 1, np.inf, 0, 1.1, np.nan, 2.2, np.inf, 2, 2],\n                        [1, 2, 3, 4, np.nan, np.inf, 0, 0, np.nan, np.nan]]\n\n        for data in data_to_test:\n            S1 = pd.Series(data)\n            S2 = S1.copy(deep=True)\n            value = np.nan\n            result_ref = test_impl(S1, value)\n            result = hpat_func(S2, value)\n            pd.testing.assert_series_equal(result_ref, result)\n\n    @skip_numba_jit(\'Requires fully functional StringArray setitem\')\n    @skip_sdc_jit\n    def test_series_setitem_value_string(self):\n        def test_impl(S, val):\n            S[2] = val\n            return S\n        hpat_func = self.jit(test_impl)\n\n        data_to_test = [[\'a\', \'\', \'abc\', \'\', \'b\', None, \'a\', \'\', None, \'b\'],\n                        [\'dog\', None, \'NaN\', \'\', \'cat\', None, \'cat\', None, \'dog\', \'\'],\n                        [\'dog\', \'NaN\', \'cat\', \'\', \'cat\', \'dog\', \'\']]\n\n        for data in data_to_test:\n            S1 = pd.Series(data)\n            S2 = S1.copy(deep=True)\n            # the length of value is greater than S[idx] length!\n            value = \'Hello, world!\'\n            result_ref = test_impl(S1, value)\n            result = hpat_func(S2, value)\n            pd.testing.assert_series_equal(result, result_ref)\n\n    def test_series_setitem_idx_int_slice_all_dtypes(self):\n        def test_impl(S, idx, val):\n            S[idx] = val\n            return S\n        hpat_func = self.jit(test_impl)\n\n        dtype_to_data = {\n            \'int\':    [[0, 1, 2, 3, 4]],\n            \'float\':  [[0, 0, 0, np.nan, np.nan, 0, 0, np.nan, np.inf, 0, 0, np.inf, np.inf],\n                       [1.1, 0.3, np.nan, 1, np.inf, 0, 1.1, np.nan, 2.2, np.inf, 2, 2],\n                       [1, 2, 3, 4, np.nan, np.inf, 0, 0, np.nan, np.nan]],\n            \'string\': [[\'a\', \'\', \'a\', \'\', \'b\', None, \'a\', \'\', None, \'b\'],\n                       [\'dog\', None, \'NaN\', \'\', \'cat\', None, \'cat\', None, \'dog\', \'\'],\n                       [\'dog\', \'NaN\', \'\', \'cat\', \'cat\', \'dog\', \'\']]\n        }\n        dtype_to_values = {\n            \'int\':    50,\n            \'float\':  np.nan,\n            \'string\': \'bird\'\n        }\n\n        idx = slice(2, None)\n        for dtype, all_data in dtype_to_data.items():\n            # FIXME: setitem for StringArray type has no overload for slice idx\n            if dtype == \'string\':\n                continue\n            value = dtype_to_values[dtype]\n            for series_data in all_data:\n                S1 = pd.Series(series_data)\n                S2 = S1.copy(deep=True)\n                with self.subTest(series=S1, value=value):\n                    hpat_func(S1, idx, value)\n                    test_impl(S2, idx, value)\n                    pd.testing.assert_series_equal(S1, S2)\n\n    def test_series_setitem_idx_int_series_all_dtypes(self):\n        def test_impl(S, idx, val):\n            S[idx] = val\n            return S\n        hpat_func = self.jit(test_impl)\n\n        dtype_to_data = {\n            \'int\':    [[0, 1, 2, 3, 4]],\n            \'float\':  [[0, 0, 0, np.nan, np.nan, 0, 0, np.nan, np.inf, 0, 0, np.inf, np.inf],\n                       [1.1, 0.3, np.nan, 1, np.inf, 0, 1.1, np.nan, 2.2, np.inf, 2, 2],\n                       [1, 2, 3, 4, np.nan, np.inf, 0, 0, np.nan, np.nan]],\n            \'string\': [[\'a\', \'\', \'a\', \'\', \'b\', None, \'a\', \'\', None, \'b\'],\n                       [\'dog\', None, \'NaN\', \'\', \'cat\', None, \'cat\', None, \'dog\', \'\'],\n                       [\'dog\', \'NaN\', \'\', \'cat\', \'cat\', \'dog\', \'\']]\n        }\n        dtype_to_values = {\n            \'int\':    50,\n            \'float\':  np.nan,\n            \'string\': \'bird\'\n        }\n\n        idx = pd.Series([0, 2, 4])\n        for dtype, all_data in dtype_to_data.items():\n            # FIXME: setitem for StringArray type has no overload for slice idx\n            if dtype == \'string\':\n                continue\n            value = dtype_to_values[dtype]\n            for series_data in all_data:\n                S1 = pd.Series(series_data)\n                S2 = S1.copy(deep=True)\n                with self.subTest(series=S1, value=value):\n                    hpat_func(S1, idx, value)\n                    test_impl(S2, idx, value)\n                    pd.testing.assert_series_equal(S1, S2)\n\n    @skip_sdc_jit(""Not supported in old-style"")\n    def test_series_setitem_unsupported(self):\n        def test_impl(S, idx, value):\n            S[idx] = value\n            return S\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([0, 1, 2, 3, 4, 5])\n\n        idx, value = 5, \'ababa\'\n        msg_tmpl = \'Operator setitem(). The value and Series data must be comparable. Given: self.dtype={}, value={}\'\n        with self.assertRaises(TypingError) as raises:\n            hpat_func(S, idx, value)\n        msg = msg_tmpl.format(S.dtype, \'unicode_type\')\n        self.assertIn(msg, str(raises.exception))\n\n        idx, value = \'3\', 101\n        msg_tmpl = \'Operator setitem(). The idx is not comparable to Series index, not a Boolean or integer indexer\' \\\n                   + \' or a Slice. Given: self.index={}, idx={}\'\n        with self.assertRaises(TypingError) as raises:\n            hpat_func(S, idx, value)\n        msg = msg_tmpl.format(\'none\', \'unicode_type\')\n        self.assertIn(msg, str(raises.exception))\n\n    @skip_sdc_jit(\'Arithmetic operations on Series with non-default indexes are not supported in old-style\')\n    def test_series_operator_add_numeric_scalar(self):\n        """"""Verifies Series.operator.add implementation for numeric series and scalar second operand""""""\n        def test_impl(A, B):\n            return A + B\n        hpat_func = self.jit(test_impl)\n\n        n = 7\n        dtype_to_index = {\'None\': None,\n                          \'int\': np.arange(n, dtype=\'int\'),\n                          \'float\': np.arange(n, dtype=\'float\'),\n                          \'string\': [\'aa\', \'aa\', \'b\', \'b\', \'cccc\', \'dd\', \'ddd\']}\n\n        int_scalar = 24\n        for dtype, index_data in dtype_to_index.items():\n            with self.subTest(index_dtype=dtype, index=index_data):\n                if platform.system() == \'Windows\' and not IS_32BITS:\n                    A = pd.Series(np.arange(n, dtype=np.int64), index=index_data)\n                else:\n                    A = pd.Series(np.arange(n), index=index_data)\n                result = hpat_func(A, int_scalar)\n                result_ref = test_impl(A, int_scalar)\n                pd.testing.assert_series_equal(result, result_ref, check_dtype=False, check_names=False)\n\n        float_scalar = 24.0\n        for dtype, index_data in dtype_to_index.items():\n            with self.subTest(index_dtype=dtype, index=index_data):\n                if platform.system() == \'Windows\' and not IS_32BITS:\n                    A = pd.Series(np.arange(n, dtype=np.int64), index=index_data)\n                else:\n                    A = pd.Series(np.arange(n), index=index_data)\n                ref_result = test_impl(A, float_scalar)\n                result = hpat_func(A, float_scalar)\n                pd.testing.assert_series_equal(result, ref_result, check_dtype=False, check_names=False)\n\n    def test_series_operator_add_numeric_same_index_default(self):\n        """"""Verifies implementation of Series.operator.add between two numeric Series\n        with default indexes and same size""""""\n        def test_impl(A, B):\n            return A + B\n        hpat_func = self.jit(test_impl)\n\n        n = 7\n        dtypes_to_test = (np.int32, np.int64, np.float32, np.float64)\n        for dtype_left, dtype_right in combinations(dtypes_to_test, 2):\n            with self.subTest(left_series_dtype=dtype_left, right_series_dtype=dtype_right):\n                A = pd.Series(np.arange(n), dtype=dtype_left)\n                B = pd.Series(np.arange(n)**2, dtype=dtype_right)\n                pd.testing.assert_series_equal(hpat_func(A, B), test_impl(A, B), check_dtype=False)\n\n    @skip_numba_jit\n    @skip_sdc_jit(""TODO: find out why pandas aligning series indexes produces Int64Index when common dtype is float\\n""\n                  ""AssertionError: Series.index are different\\n""\n                  ""Series.index classes are not equivalent\\n""\n                  ""[left]:  Float64Index([0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0], dtype=\'float64\')\\n""\n                  ""[right]: Int64Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=\'int64\')\\n"")\n    def test_series_operator_add_numeric_same_index_numeric(self):\n        """"""Verifies implementation of Series.operator.add between two numeric Series\n           with the same numeric indexes of different dtypes""""""\n        def test_impl(A, B):\n            return A + B\n        hpat_func = self.jit(test_impl)\n\n        n = 7\n        dtypes_to_test = (np.int32, np.int64, np.float32, np.float64)\n        for dtype_left, dtype_right in combinations(dtypes_to_test, 2):\n            with self.subTest(left_series_dtype=dtype_left, right_series_dtype=dtype_right):\n                A = pd.Series(np.arange(n), index=np.arange(n, dtype=dtype_left))\n                B = pd.Series(np.arange(n)**2, index=np.arange(n, dtype=dtype_right))\n                pd.testing.assert_series_equal(hpat_func(A, B), test_impl(A, B), check_dtype=False)\n\n    @skip_parallel\n    @skip_sdc_jit(\'Arithmetic operations on Series with non-default indexes are not supported in old-style\')\n    def test_series_operator_add_numeric_same_index_numeric_fixme(self):\n        """""" Same as test_series_operator_add_same_index_numeric but with w/a for the problem.\n        Can be deleted when the latter is fixed """"""\n        def test_impl(A, B):\n            return A + B\n        hpat_func = self.jit(test_impl)\n\n        n = 7\n        index_dtypes_to_test = (np.int32, np.int64, np.float32, np.float64)\n        for dtype_left, dtype_right in combinations(index_dtypes_to_test, 2):\n            # FIXME: skip the sub-test if one of the dtypes is float and the other is integer\n            if not (np.issubdtype(dtype_left, np.integer) and np.issubdtype(dtype_right, np.integer)\n                    or np.issubdtype(dtype_left, np.float) and np.issubdtype(dtype_right, np.float)):\n                continue\n\n            with self.subTest(left_series_dtype=dtype_left, right_series_dtype=dtype_right):\n                A = pd.Series(np.arange(n), index=np.arange(n, dtype=dtype_left))\n                B = pd.Series(np.arange(n)**2, index=np.arange(n, dtype=dtype_right))\n                pd.testing.assert_series_equal(hpat_func(A, B), test_impl(A, B), check_dtype=False)\n\n    @skip_parallel\n    @skip_sdc_jit(\'Arithmetic operations on Series with non-default indexes are not supported in old-style\')\n    def test_series_operator_add_numeric_same_index_str(self):\n        """"""Verifies implementation of Series.operator.add between two numeric Series with the same string indexes""""""\n        def test_impl(A, B):\n            return A + B\n        hpat_func = self.jit(test_impl)\n\n        n = 7\n        A = pd.Series(np.arange(n), index=[\'a\', \'c\', \'e\', \'c\', \'b\', \'a\', \'o\'])\n        B = pd.Series(np.arange(n)**2, index=[\'a\', \'c\', \'e\', \'c\', \'b\', \'a\', \'o\'])\n        pd.testing.assert_series_equal(hpat_func(A, B), test_impl(A, B), check_dtype=False, check_names=False)\n\n    @skip_parallel\n    @skip_sdc_jit(\'Arithmetic operations on Series with non-default indexes are not supported in old-style\')\n    def test_series_operator_add_numeric_align_index_int(self):\n        """"""Verifies implementation of Series.operator.add between two numeric Series with non-equal integer indexes""""""\n        def test_impl(A, B):\n            return A + B\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        index_A = [0, 1, 1, 2, 3, 3, 3, 4, 6, 8, 9]\n        index_B = [0, 1, 1, 3, 4, 4, 5, 5, 6, 6, 9]\n        np.random.shuffle(index_A)\n        np.random.shuffle(index_B)\n        A = pd.Series(np.arange(n), index=index_A)\n        B = pd.Series(np.arange(n)**2, index=index_B)\n        pd.testing.assert_series_equal(hpat_func(A, B), test_impl(A, B), check_dtype=False, check_names=False)\n\n    @skip_parallel\n    @skip_sdc_jit(\'Arithmetic operations on Series with non-default indexes are not supported in old-style\')\n    def test_series_operator_add_numeric_align_index_str(self):\n        """"""Verifies implementation of Series.operator.add between two numeric Series with non-equal string indexes""""""\n        def test_impl(A, B):\n            return A + B\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        index_A = [\'\', \'\', \'aa\', \'aa\', \'ae\', \'ae\', \'b\', \'ccc\', \'cccc\', \'oo\', \'s\']\n        index_B = [\'\', \'\', \'aa\', \'aa\', \'cc\', \'cccc\', \'e\', \'f\', \'h\', \'oo\', \'s\']\n        np.random.shuffle(index_A)\n        np.random.shuffle(index_B)\n        A = pd.Series(np.arange(n), index=index_A)\n        B = pd.Series(np.arange(n)**2, index=index_B)\n        pd.testing.assert_series_equal(hpat_func(A, B), test_impl(A, B), check_dtype=False, check_names=False)\n\n    @skip_numba_jit(\'TODO: fix Series.sort_values to handle both None and \'\' in string series\')\n    @skip_sdc_jit(\'Arithmetic operations on Series with non-default indexes are not supported in old-style\')\n    def test_series_operator_add_numeric_align_index_str_fixme(self):\n        """"""Same as test_series_operator_add_align_index_str but with None values in string indexes""""""\n        def test_impl(A, B):\n            return A + B\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        index_A = [\'\', \'\', \'aa\', \'aa\', \'ae\', \'b\', \'ccc\', \'cccc\', \'oo\', None, None]\n        index_B = [\'\', \'\', \'aa\', \'aa\', \'cccc\', \'f\', \'h\', \'oo\', \'s\', None, None]\n        np.random.shuffle(index_A)\n        np.random.shuffle(index_B)\n        A = pd.Series(np.arange(n), index=index_A)\n        B = pd.Series(np.arange(n)**2, index=index_B)\n        pd.testing.assert_series_equal(hpat_func(A, B), test_impl(A, B), check_dtype=False, check_names=False)\n\n    @skip_parallel\n    @skip_sdc_jit(\'Arithmetic operations on Series with non-default indexes are not supported in old-style\')\n    def test_series_operator_add_numeric_align_index_other_dtype(self):\n        """"""Verifies implementation of Series.operator.add between two numeric Series\n        with non-equal integer indexes of different dtypes""""""\n        def test_impl(A, B):\n            return A + B\n        hpat_func = self.jit(test_impl)\n\n        n = 7\n        A = pd.Series(np.arange(3*n), index=np.arange(-n, 2*n, 1, dtype=np.int64))\n        B = pd.Series(np.arange(3*n)**2, index=np.arange(0, 3*n, 1, dtype=np.float64))\n        pd.testing.assert_series_equal(hpat_func(A, B), test_impl(A, B), check_dtype=False, check_names=False)\n\n    @skip_sdc_jit(\'Arithmetic operations on Series with different sizes are not supported in old-style\')\n    def test_series_operator_add_numeric_diff_series_sizes(self):\n        """"""Verifies implementation of Series.operator.add between two numeric Series with different sizes""""""\n        def test_impl(A, B):\n            return A + B\n        hpat_func = self.jit(test_impl)\n\n        size_A, size_B = 7, 25\n        A = pd.Series(np.arange(size_A))\n        B = pd.Series(np.arange(size_B)**2)\n        pd.testing.assert_series_equal(hpat_func(A, B), test_impl(A, B), check_dtype=False, check_names=False)\n\n    @skip_parallel\n    @skip_sdc_jit(\'Arithmetic operations on Series requiring alignment of indexes are not supported in old-style\')\n    def test_series_operator_add_align_index_int_capacity(self):\n        """"""Verifies implementation of Series.operator.add and alignment of numeric indexes of large size""""""\n        def test_impl(A, B):\n            return A + B\n        hpat_func = self.jit(test_impl)\n\n        n = 20000\n        np.random.seed(0)\n        index1 = np.random.randint(-30, 30, n)\n        index2 = np.random.randint(-30, 30, n)\n        A = pd.Series(np.random.ranf(n), index=index1)\n        B = pd.Series(np.random.ranf(n), index=index2)\n        pd.testing.assert_series_equal(hpat_func(A, B), test_impl(A, B), check_dtype=False, check_names=False)\n\n    @skip_sdc_jit(\'Arithmetic operations on Series requiring alignment of indexes are not supported in old-style\')\n    def test_series_operator_add_align_index_str_capacity(self):\n        """"""Verifies implementation of Series.operator.add and alignment of string indexes of large size""""""\n        def test_impl(A, B):\n            return A + B\n        hpat_func = self.jit(test_impl)\n\n        n = 2000\n        np.random.seed(0)\n        valid_ids = [\'\', \'aaa\', \'a\', \'b\', \'ccc\', \'ef\', \'ff\', \'fff\', \'fa\', \'dddd\']\n        index1 = [valid_ids[i] for i in np.random.randint(0, len(valid_ids), n)]\n        index2 = [valid_ids[i] for i in np.random.randint(0, len(valid_ids), n)]\n        A = pd.Series(np.random.ranf(n), index=index1)\n        B = pd.Series(np.random.ranf(n), index=index2)\n        pd.testing.assert_series_equal(hpat_func(A, B), test_impl(A, B), check_dtype=False, check_names=False)\n\n    @skip_sdc_jit\n    def test_series_operator_add_str_same_index_default(self):\n        """"""Verifies implementation of Series.operator.add between two string Series\n        with default indexes and same size""""""\n        def test_impl(A, B):\n            return A + B\n        hpat_func = self.jit(test_impl)\n\n        A = pd.Series([\'a\', \'\', \'ae\', \'b\', \'cccc\', \'oo\', None])\n        B = pd.Series([\'b\', \'aa\', \'\', \'b\', \'o\', None, \'oo\'])\n        pd.testing.assert_series_equal(hpat_func(A, B), test_impl(A, B), check_dtype=False, check_names=False)\n\n    @skip_parallel\n    @skip_sdc_jit(\'Arithmetic operations on Series with non-default indexes are not supported in old-style\')\n    def test_series_operator_add_str_align_index_int(self):\n        """"""Verifies implementation of Series.operator.add between two string Series with non-equal integer indexes""""""\n        def test_impl(A, B):\n            return A + B\n        hpat_func = self.jit(test_impl)\n\n        np.random.seed(0)\n        index_A = [0, 1, 1, 2, 3, 3, 3, 4, 6, 8, 9]\n        index_B = [0, 1, 1, 3, 4, 4, 5, 5, 6, 6, 9]\n        np.random.shuffle(index_A)\n        np.random.shuffle(index_B)\n        data = [\'\', \'\', \'aa\', \'aa\', None, \'ae\', \'b\', \'ccc\', \'cccc\', None, \'oo\']\n        A = pd.Series(data, index=index_A)\n        B = pd.Series(data, index=index_B)\n        pd.testing.assert_series_equal(hpat_func(A, B), test_impl(A, B), check_dtype=False, check_names=False)\n\n    def test_series_operator_add_result_name1(self):\n        """"""Verifies name of the Series resulting from appying Series.operator.add to different arguments""""""\n        def test_impl(A, B):\n            return A + B\n        hpat_func = self.jit(test_impl)\n\n        n = 7\n        series_names = [\'A\', \'\', None, \'B\']\n        for left_name, right_name in combinations(series_names, 2):\n            S1 = pd.Series(np.arange(n), name=left_name)\n            S2 = pd.Series(np.arange(n, 0, -1), name=right_name)\n            with self.subTest(left_series_name=left_name, right_series_name=right_name):\n                # check_dtype=False because SDC implementation always returns float64 Series\n                pd.testing.assert_series_equal(hpat_func(S1, S2), test_impl(S1, S2), check_dtype=False)\n\n        # also verify case when second operator is scalar\n        scalar = 3.0\n        S1 = pd.Series(np.arange(n), name=\'A\')\n        pd.testing.assert_series_equal(hpat_func(S1, S2), test_impl(S1, S2), check_dtype=False)\n\n    @unittest.expectedFailure\n    def test_series_operator_add_result_name2(self):\n        """"""Verifies implementation of Series.operator.add differs from Pandas\n           in returning unnamed Series when both operands are named Series with the same name""""""\n        def test_impl(A, B):\n            return A + B\n        hpat_func = self.jit(test_impl)\n\n        n = 7\n        S1 = pd.Series(np.arange(n), name=\'A\')\n        S2 = pd.Series(np.arange(n, 0, -1), name=\'A\')\n        result = hpat_func(S1, S2)\n        result_ref = test_impl(S1, S2)\n        # check_dtype=False because SDC implementation always returns float64 Series\n        pd.testing.assert_series_equal(result, result_ref, check_dtype=False)\n\n    @unittest.expectedFailure\n    def test_series_operator_add_series_dtype_promotion(self):\n        """"""Verifies implementation of Series.operator.add differs from Pandas\n           in dtype of resulting Series that is fixed to float64""""""\n        def test_impl(A, B):\n            return A + B\n        hpat_func = self.jit(test_impl)\n\n        n = 7\n        dtypes_to_test = (np.int32, np.int64, np.float32, np.float64)\n        for dtype_left, dtype_right in combinations(dtypes_to_test, 2):\n            with self.subTest(left_series_dtype=dtype_left, right_series_dtype=dtype_right):\n                A = pd.Series(np.array(np.arange(n), dtype=dtype_left))\n                B = pd.Series(np.array(np.arange(n)**2, dtype=dtype_right))\n                pd.testing.assert_series_equal(hpat_func(A, B), test_impl(A, B))\n\n    @skip_sdc_jit(\'Old-style implementation of operators doesn\\\'t support Series indexes\')\n    def test_series_operator_lt_index_mismatch1(self):\n        """"""Verifies correct exception is raised when comparing Series with non equal integer indexes""""""\n        def test_impl(A, B):\n            return A < B\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        np.random.seed(0)\n        index1 = np.arange(n)\n        index2 = np.copy(index1)\n        np.random.shuffle(index2)\n        A = pd.Series([1, 2, -1, 3, 4, 2, -3, 5, 6, 6, 0], index=index1)\n        B = pd.Series([3, 2, -2, 1, 4, 1, -5, 6, 6, 3, -1], index=index2)\n\n        with self.assertRaises(Exception) as context:\n            test_impl(A, B)\n        exception_ref = context.exception\n\n        self.assertRaises(type(exception_ref), hpat_func, A, B)\n\n    @skip_sdc_jit(\'Old-style implementation of operators doesn\\\'t support comparing Series of different lengths\')\n    def test_series_operator_lt_index_mismatch2(self):\n        """"""Verifies correct exception is raised when comparing Series of different size with default indexes""""""\n        def test_impl(A, B):\n            return A < B\n        hpat_func = self.jit(test_impl)\n\n        A = pd.Series([1, 2, -1, 3, 4, 2])\n        B = pd.Series([3, 2, -2, 1, 4, 1, -5, 6, 6, 3, -1])\n\n        with self.assertRaises(Exception) as context:\n            test_impl(A, B)\n        exception_ref = context.exception\n\n        self.assertRaises(type(exception_ref), hpat_func, A, B)\n\n    @skip_numba_jit(\'Numba propagates different exception:\\n\'\n                    \'numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)\\n\'\n                    \'Internal error at <numba.core.typeinfer.IntrinsicCallConstraint ...\\n\'\n                    \'\\\'Signature\\\' object is not iterable\')\n    @skip_sdc_jit(\'Typing checks not implemented for Series operators in old-style\')\n    def test_series_operator_lt_index_mismatch3(self):\n        """"""Verifies correct exception is raised when comparing two Series with non-comparable indexes""""""\n        def test_impl(A, B):\n            return A < B\n        hpat_func = self.jit(test_impl)\n\n        S1 = pd.Series([1, 2, -1, 3, 4, 2])\n        S2 = pd.Series([\'a\', \'b\', \'\', None, \'2\', \'ccc\'])\n\n        with self.assertRaises(TypingError) as raises:\n            hpat_func(S1, S2)\n        msg = \'Operator lt(). Not supported for series with not-comparable indexes.\'\n        self.assertIn(msg, str(raises.exception))\n\n    @skip_sdc_jit(\'Comparison operations on Series with non-default indexes are not supported in old-style\')\n    @skip_numba_jit(""TODO: find out why pandas aligning series indexes produces Int64Index when common dtype is float\\n""\n                    ""AssertionError: Series.index are different\\n""\n                    ""Series.index classes are not equivalent\\n""\n                    ""[left]:  Float64Index([0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0], dtype=\'float64\')\\n""\n                    ""[right]: Int64Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=\'int64\')\\n"")\n    def test_series_operator_lt_index_dtype_promotion(self):\n        """"""Verifies implementation of Series.operator.lt between two numeric Series\n           with the same numeric indexes of different dtypes""""""\n        def test_impl(A, B):\n            return A < B\n        hpat_func = self.jit(test_impl)\n\n        n = 7\n        index_dtypes_to_test = (np.int32, np.int64, np.float32, np.float64)\n        for dtype_left, dtype_right in combinations(index_dtypes_to_test, 2):\n            with self.subTest(left_series_dtype=dtype_left, right_series_dtype=dtype_right):\n                A = pd.Series(np.arange(n), index=np.arange(n, dtype=dtype_left))\n                B = pd.Series(np.arange(n)**2, index=np.arange(n, dtype=dtype_right))\n                pd.testing.assert_series_equal(hpat_func(A, B), test_impl(A, B))\n\n    @skip_sdc_jit(\'Comparison operations on Series with non-default indexes are not supported in old-style\')\n    def test_series_operator_lt_index_dtype_promotion_fixme(self):\n        """""" Same as test_series_operator_lt_index_dtype_promotion but with w/a for the problem.\n        Can be deleted when the latter is fixed """"""\n        def test_impl(A, B):\n            return A < B\n        hpat_func = self.jit(test_impl)\n\n        n = 7\n        index_dtypes_to_test = (np.int32, np.int64, np.float32, np.float64)\n        for dtype_left, dtype_right in combinations(index_dtypes_to_test, 2):\n            # FIXME: skip the sub-test if one of the dtypes is float and the other is integer\n            if not (np.issubdtype(dtype_left, np.integer) and np.issubdtype(dtype_right, np.integer)\n                    or np.issubdtype(dtype_left, np.float) and np.issubdtype(dtype_right, np.float)):\n                continue\n\n            with self.subTest(left_series_dtype=dtype_left, right_series_dtype=dtype_right):\n                A = pd.Series(np.arange(n), index=np.arange(n, dtype=dtype_left))\n                B = pd.Series(np.arange(n)**2, index=np.arange(n, dtype=dtype_right))\n                pd.testing.assert_series_equal(hpat_func(A, B), test_impl(A, B))\n\n    @skip_numba_jit(\'Numba propagates different exception:\\n\'\n                    \'numba.core.errors.TypingError: Failed in nopython mode pipeline (step: nopython frontend)\\n\'\n                    \'Internal error at <numba.core.typeinfer.IntrinsicCallConstraint ...\\n\'\n                    \'\\\'Signature\\\' object is not iterable\')\n    @skip_sdc_jit(\'Typing checks not implemented for Series operators in old-style\')\n    def test_series_operator_lt_unsupported_dtypes(self):\n        """"""Verifies correct exception is raised when comparing two Series with non-comparable dtypes""""""\n        def test_impl(A, B):\n            return A < B\n        hpat_func = self.jit(test_impl)\n\n        S1 = pd.Series([1, 2, -1, 3, 4, 2])\n        S2 = pd.Series([\'a\', \'b\', \'\', None, \'2\', \'ccc\'])\n\n        with self.assertRaises(TypingError) as raises:\n            hpat_func(S1, S2)\n        msg = \'Operator lt(). Not supported for series with not-comparable data.\'\n        self.assertIn(msg, str(raises.exception))\n\n    @skip_sdc_jit\n    def test_series_operator_lt_str(self):\n        """"""Verifies implementation of Series.operator.lt between two string Series with default indexes""""""\n        def test_impl(A, B):\n            return A < B\n        hpat_func = self.jit(test_impl)\n\n        A = pd.Series([\'a\', \'\', \'ae\', \'b\', \'cccc\', \'oo\', None])\n        B = pd.Series([\'b\', \'aa\', \'\', \'b\', \'o\', None, \'oo\'])\n        pd.testing.assert_series_equal(hpat_func(A, B), test_impl(A, B))\n\n    @skip_sdc_jit(""Series.str.istitle is not supported yet"")\n    def test_series_istitle_str(self):\n        series = pd.Series([\'Cat\', \'dog\', \'Bird\'])\n\n        cfunc = self.jit(istitle_usecase)\n        pd.testing.assert_series_equal(cfunc(series), istitle_usecase(series))\n\n    @skip_sdc_jit(""Series.str.istitle is not supported yet"")\n    @skip_numba_jit(""Not work with None and np.nan"")\n    def test_series_istitle_str_fixme(self):\n        series = pd.Series([\'Cat\', \'dog\', \'Bird\', None, np.nan])\n\n        cfunc = self.jit(istitle_usecase)\n        pd.testing.assert_series_equal(cfunc(series), istitle_usecase(series))\n\n    @skip_sdc_jit(""Series.str.isspace is not supported yet"")\n    def test_series_isspace_str(self):\n        series = [[\'\', \'  \', \'    \', \'           \'],\n                  [\'\', \' c \', \'  b \', \'     a     \'],\n                  [\'aaaaaa\', \'bb\', \'c\', \'  d\']\n                  ]\n\n        cfunc = self.jit(isspace_usecase)\n        for ser in series:\n            S = pd.Series(ser)\n            pd.testing.assert_series_equal(cfunc(S), isspace_usecase(S))\n\n    @skip_sdc_jit(""Series.str.isalpha is not supported yet"")\n    def test_series_isalpha_str(self):\n        series = [[\'leopard\', \'Golden Eagle\', \'SNAKE\', \'\'],\n                  [\'Hello world!\', \'hello 123\', \'mynameisPeter\'],\n                  [\'one\', \'one1\', \'1\', \'\']\n                  ]\n\n        cfunc = self.jit(isalpha_usecase)\n        for ser in series:\n            S = pd.Series(ser)\n            pd.testing.assert_series_equal(cfunc(S), isalpha_usecase(S))\n\n    @skip_sdc_jit(""Series.str.islower is not supported yet"")\n    def test_series_islower_str(self):\n        series = [[\'leopard\', \'Golden Eagle\', \'SNAKE\', \'\'],\n                  [\'Hello world!\', \'hello 123\', \'mynameisPeter\']\n                  ]\n\n        cfunc = self.jit(islower_usecase)\n        for ser in series:\n            S = pd.Series(ser)\n            pd.testing.assert_series_equal(cfunc(S), islower_usecase(S))\n\n    def test_series_lower_str(self):\n        all_data = [[\'leopard\', None, \'Golden Eagle\', np.nan, \'SNAKE\', \'\'],\n                    [\'Hello world!\', np.nan, \'hello 123\', None, \'mynameisPeter\']\n                    ]\n\n        cfunc = self.jit(lower_usecase)\n        for data in all_data:\n            s = pd.Series(data)\n            pd.testing.assert_series_equal(cfunc(s), lower_usecase(s))\n\n    def test_series_strip_str(self):\n        s = pd.Series([\'1. Ant.  \', None, \'2. Bee!\\n\', np.nan, \'3. Cat?\\t\'])\n        cfunc = self.jit(strip_usecase)\n        for to_strip in [None, \'123.\', \'.!? \\n\\t\', \'123.!? \\n\\t\']:\n            pd.testing.assert_series_equal(cfunc(s, to_strip), strip_usecase(s, to_strip))\n\n    def test_series_lstrip_str(self):\n        s = pd.Series([\'1. Ant.  \', None, \'2. Bee!\\n\', np.nan, \'3. Cat?\\t\'])\n        cfunc = self.jit(lstrip_usecase)\n        for to_strip in [None, \'123.\', \'.!? \\n\\t\', \'123.!? \\n\\t\']:\n            pd.testing.assert_series_equal(cfunc(s, to_strip), lstrip_usecase(s, to_strip))\n\n    def test_series_rstrip_str(self):\n        s = pd.Series([\'1. Ant.  \', None, \'2. Bee!\\n\', np.nan, \'3. Cat?\\t\'])\n        cfunc = self.jit(rstrip_usecase)\n        for to_strip in [None, \'123.\', \'.!? \\n\\t\', \'123.!? \\n\\t\']:\n            pd.testing.assert_series_equal(cfunc(s, to_strip), rstrip_usecase(s, to_strip))\n\n    @skip_sdc_jit(""Series.str.isalnum is not supported yet"")\n    def test_series_isalnum_str(self):\n        cfunc = self.jit(isalnum_usecase)\n        test_data = [test_global_input_data_unicode_kind1, test_global_input_data_unicode_kind4]\n        for data in test_data:\n            S = pd.Series(data)\n            pd.testing.assert_series_equal(cfunc(S), isalnum_usecase(S))\n\n    @skip_sdc_jit(""Series.str.isnumeric is not supported yet"")\n    def test_series_isnumeric_str(self):\n        cfunc = self.jit(isnumeric_usecase)\n        test_data = [test_global_input_data_unicode_kind1, test_global_input_data_unicode_kind4]\n        for data in test_data:\n            S = pd.Series(data)\n            pd.testing.assert_series_equal(cfunc(S), isnumeric_usecase(S))\n\n    @skip_sdc_jit(""Series.str.isdigit is not supported yet"")\n    def test_series_isdigit_str(self):\n        cfunc = self.jit(isdigit_usecase)\n        test_data = [test_global_input_data_unicode_kind1, test_global_input_data_unicode_kind4]\n        for data in test_data:\n            S = pd.Series(data)\n            pd.testing.assert_series_equal(cfunc(S), isdigit_usecase(S))\n\n    @skip_sdc_jit(""Series.str.isdecimal is not supported yet"")\n    def test_series_isdecimal_str(self):\n        cfunc = self.jit(isdecimal_usecase)\n        test_data = [test_global_input_data_unicode_kind1, test_global_input_data_unicode_kind4]\n        for data in test_data:\n            S = pd.Series(data)\n            pd.testing.assert_series_equal(cfunc(S), isdecimal_usecase(S))\n\n    @skip_sdc_jit(""Series.str.isupper is not supported yet"")\n    def test_series_isupper_str(self):\n        cfunc = self.jit(isupper_usecase)\n        test_data = [test_global_input_data_unicode_kind1, test_global_input_data_unicode_kind4]\n        for data in test_data:\n            s = pd.Series(data)\n            pd.testing.assert_series_equal(cfunc(s), isupper_usecase(s))\n\n    def test_series_contains(self):\n        hpat_func = self.jit(contains_usecase)\n        s = pd.Series([\'Mouse\', \'dog\', \'house and parrot\', \'23\'])\n        for pat in [\'og\', \'Og\', \'OG\', \'o\']:\n            for case in [True, False]:\n                with self.subTest(pat=pat, case=case):\n                    pd.testing.assert_series_equal(hpat_func(s, pat, case), contains_usecase(s, pat, case))\n\n    def test_series_contains_with_na_flags_regex(self):\n        hpat_func = self.jit(contains_usecase)\n        s = pd.Series([\'Mouse\', \'dog\', \'house and parrot\', \'23\'])\n        pat = \'og\'\n        pd.testing.assert_series_equal(hpat_func(s, pat, flags=0, na=None, regex=True),\n                                       contains_usecase(s, pat, flags=0, na=None, regex=True))\n\n    def test_series_contains_unsupported(self):\n        hpat_func = self.jit(contains_usecase)\n        s = pd.Series([\'Mouse\', \'dog\', \'house and parrot\', \'23\'])\n        pat = \'og\'\n\n        with self.assertRaises(SDCLimitation) as raises:\n            hpat_func(s, pat, flags=1)\n        msg = ""Method contains(). Unsupported parameter. Given \'flags\' != 0""\n        self.assertIn(msg, str(raises.exception))\n\n        with self.assertRaises(TypingError) as raises:\n            hpat_func(s, pat, na=0)\n        msg = \'Method contains(). The object na\\n given: int64\\n expected: none\'\n        self.assertIn(msg, str(raises.exception))\n\n        with self.assertRaises(SDCLimitation) as raises:\n            hpat_func(s, pat, regex=False)\n        msg = ""Method contains(). Unsupported parameter. Given \'regex\' is False""\n        self.assertIn(msg, str(raises.exception))\n\n    @skip_sdc_jit(\'Old-style implementation returns string, but not series\')\n    def test_series_describe_numeric(self):\n        def test_impl(A):\n            return A.describe()\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        S = pd.Series(np.arange(n))\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    @skip_sdc_jit(\'Old-style implementation doesn\\\'t support pecentiles argument\')\n    def test_series_describe_numeric_percentiles(self):\n        def test_impl(A, values):\n            return A.describe(percentiles=values)\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        S = pd.Series(np.arange(n))\n        supported_values = [\n            [0.323, 0.778, 0.1, 0.01, 0.2],\n            [0.001, 0.002],\n            [0.001, 0.5, 0.002],\n            [0.9999, 0.0001],\n            (0.323, 0.778, 0.1, 0.01, 0.2),\n            np.array([0, 1.0]),\n            np.array([0.323, 0.778, 0.1, 0.01, 0.2]),\n            None,\n        ]\n        for percentiles in supported_values:\n            with self.subTest(percentiles=percentiles):\n                pd.testing.assert_series_equal(hpat_func(S, percentiles), test_impl(S, percentiles))\n\n    @skip_sdc_jit(\'Old-style implementation for string series is not supported\')\n    def test_series_describe_str(self):\n        def test_impl(A):\n            return A.describe()\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([\'a\', \'dd\', None, \'bbbb\', \'dd\', \'\', \'dd\', \'\', \'dd\'])\n        # SDC implementation returns series of string, hence conversion of reference result is needed\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S).astype(str))\n\n    @skip_sdc_jit(\'Old-style implementation for datetime series is not supported\')\n    @skip_numba_jit(\'Series.describe is not implemented for datatime Series due to Numba limitations\\n\'\n                    \'Requires dropna for pd.Timestamp (depends on Numba isnat) to be implemented\')\n    def test_series_describe_dt(self):\n        def test_impl(A):\n            return A.describe()\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([pd.Timestamp(\'1970-12-01 03:02:35\'),\n                       pd.NaT,\n                       pd.Timestamp(\'1970-03-03 12:34:59\'),\n                       pd.Timestamp(\'1970-12-01 03:02:35\'),\n                       pd.Timestamp(\'2012-07-25\'),\n                       None])\n        # SDC implementation returns series of string, hence conversion of reference result is needed\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S).astype(str))\n\n    @skip_sdc_jit(\'Old-style implementation doesn\\\'t support pecentiles argument\')\n    def test_series_describe_unsupported_percentiles(self):\n        def test_impl(A, values):\n            return A.describe(percentiles=values)\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        S = pd.Series(np.arange(n))\n        unsupported_values = [0.5, \'0.77\', True, (\'a\', \'b\'), [\'0.5\', \'0.7\'], np.arange(0.1, 0.5, 0.1).astype(str)]\n        for percentiles in unsupported_values:\n            with self.assertRaises(TypingError) as raises:\n                hpat_func(S, percentiles)\n            msg = \'Method describe(). The object percentiles\'\n            self.assertIn(msg, str(raises.exception))\n\n    @skip_sdc_jit(\'Old-style implementation doesn\\\'t support pecentiles argument\')\n    def test_series_describe_invalid_percentiles(self):\n        def test_impl(A, values):\n            return A.describe(percentiles=values)\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        S = pd.Series(np.arange(n))\n        unsupported_values = [\n            [0.5, 0.7, 1.1],\n            [-0.5, 0.7, 1.1],\n            [0.5, 0.7, 0.2, 0.7]\n        ]\n        for percentiles in unsupported_values:\n            with self.assertRaises(Exception) as context:\n                test_impl(S, percentiles)\n            pandas_exception = context.exception\n\n            self.assertRaises(type(pandas_exception), hpat_func, S, percentiles)\n\n    @skip_sdc_jit(\'Arithmetic operations on string series not implemented in old-pipeline\')\n    def test_series_operator_add_str_scalar(self):\n        def test_impl(A, B):\n            return A + B\n        hpat_func = self.jit(test_impl)\n\n        series_data = [\'a\', \'\', \'ae\', \'b\', \'cccc\', \'oo\', None]\n        S = pd.Series(series_data)\n        values_to_test = [\' \', \'wq\', \'\', \'23\']\n        for scalar in values_to_test:\n            with self.subTest(left=series_data, right=scalar):\n                result_ref = test_impl(S, scalar)\n                result = hpat_func(S, scalar)\n                pd.testing.assert_series_equal(result, result_ref)\n\n            with self.subTest(left=scalar, right=series_data):\n                result_ref = test_impl(scalar, S)\n                result = hpat_func(scalar, S)\n                pd.testing.assert_series_equal(result, result_ref)\n\n    @skip_sdc_jit(\'Arithmetic operations on string series not implemented in old-pipeline\')\n    def test_series_operator_add_str_unsupported(self):\n        def test_impl(A, B):\n            return A + B\n        hpat_func = self.jit(test_impl)\n\n        n = 7\n        series_data = [\'a\', \'\', \'ae\', \'b\', \'cccc\', \'oo\', None]\n        S = pd.Series(series_data)\n        other_operands = [\n            1,\n            3.0,\n            pd.Series(np.arange(n)),\n            pd.Series([True, False, False, True, False, True, True]),\n        ]\n\n        for operand in other_operands:\n            with self.subTest(right=operand):\n                with self.assertRaises(TypingError) as raises:\n                    hpat_func(S, operand)\n                expected_msg = \'Operator add(). Not supported for not-comparable operands.\'\n                self.assertIn(expected_msg, str(raises.exception))\n\n    @skip_sdc_jit(\'Arithmetic operations on string series not implemented in old-pipeline\')\n    def test_series_operator_mul_str_scalar(self):\n        def test_impl(A, B):\n            return A * B\n        hpat_func = self.jit(test_impl)\n\n        series_data = [\'a\', \'\', \'ae\', \'b\', \' \', \'cccc\', \'oo\', None]\n        S = pd.Series(series_data)\n        values_to_test = [-1, 0, 2, 5]\n        for scalar in values_to_test:\n            with self.subTest(left=series_data, right=scalar):\n                result_ref = test_impl(S, scalar)\n                result = hpat_func(S, scalar)\n                pd.testing.assert_series_equal(result, result_ref)\n\n            with self.subTest(left=scalar, right=series_data):\n                result_ref = test_impl(scalar, S)\n                result = hpat_func(scalar, S)\n                pd.testing.assert_series_equal(result, result_ref)\n\n    @skip_sdc_jit\n    def test_series_operator_mul_str_same_index_default(self):\n        """"""Verifies implementation of Series.operator.add between two string Series\n        with default indexes and same size""""""\n        def test_impl(A, B):\n            return A * B\n        hpat_func = self.jit(test_impl)\n\n        A = pd.Series([\'a\', \'\', \'ae\', \'b\', \'cccc\', \'oo\', None])\n        B = pd.Series([-1, 2, 0, 5, 3, -5, 4])\n        pd.testing.assert_series_equal(hpat_func(A, B), test_impl(A, B))\n\n    @skip_parallel\n    @skip_sdc_jit(\'Arithmetic operations on Series with non-default indexes are not supported in old-style\')\n    def test_series_operator_mul_str_align_index_int1(self):\n        """""" Verifies implementation of Series.operator.add between two string Series\n            with integer indexes containg same unique values (so alignment doesn\'t produce NaNs) """"""\n        def test_impl(A, B):\n            return A * B\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        np.random.seed(0)\n        shuffled_data = np.arange(n, dtype=np.int)\n        np.random.shuffle(shuffled_data)\n        index_A = shuffled_data\n        np.random.shuffle(shuffled_data)\n        index_B = shuffled_data\n        str_series_values = [\'\', \'\', \'aa\', \'aa\', None, \'ae\', \'b\', \'ccc\', \'cccc\', None, \'oo\']\n        int_series_values = np.random.randint(-5, 5, n)\n\n        A = pd.Series(str_series_values, index=index_A)\n        B = pd.Series(int_series_values, index=index_B)\n        for swap_operands in (False, True):\n            if swap_operands:\n                A, B = B, A\n            with self.subTest(left=A, right=B):\n                result = hpat_func(A, B)\n                result_ref = test_impl(A, B)\n                pd.testing.assert_series_equal(result, result_ref)\n\n    @unittest.expectedFailure   # pandas can\'t calculate this due to adding NaNs to int series during alignment\n    def test_series_operator_mul_str_align_index_int2(self):\n        """""" Verifies implementation of Series.operator.add between two string Series\n            with integer indexes that cannot be aligned without NaNs """"""\n        def test_impl(A, B):\n            return A * B\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        np.random.seed(0)\n        index_A = [0, 1, 1, 2, 3, 3, 3, 4, 6, 8, 9]\n        index_B = [0, 1, 1, 3, 4, 4, 5, 5, 6, 6, 9]\n        np.random.shuffle(index_A)\n        np.random.shuffle(index_B)\n        str_series_values = [\'\', \'\', \'aa\', \'aa\', None, \'ae\', \'b\', \'ccc\', \'cccc\', None, \'oo\']\n        int_series_values = np.random.randint(-5, 5, n)\n\n        A = pd.Series(str_series_values, index=index_A)\n        B = pd.Series(int_series_values, index=index_B)\n        for swap_operands in (False, True):\n            if swap_operands:\n                A, B = B, A\n            with self.subTest(left=A, right=B):\n                result = hpat_func(A, B)\n                result_ref = test_impl(A, B)\n                pd.testing.assert_series_equal(result, result_ref)\n\n    @skip_sdc_jit(\'Arithmetic operations on string series not implemented in old-pipeline\')\n    def test_series_operator_mul_str_unsupported(self):\n        def test_impl(A, B):\n            return A * B\n        hpat_func = self.jit(test_impl)\n\n        n = 7\n        series_data = [\'a\', \'\', \'ae\', \'b\', \'cccc\', \'oo\', None]\n        S = pd.Series(series_data)\n        other_operands = [\n            \'abc\',\n            3.0,\n            pd.Series(series_data),\n            pd.Series([True, False, False, True, False, True, True]),\n        ]\n\n        for operand in other_operands:\n            with self.subTest(right=operand):\n                with self.assertRaises(TypingError) as raises:\n                    hpat_func(S, operand)\n                expected_msg = \'Operator mul(). Not supported between operands of types:\'\n                self.assertIn(expected_msg, str(raises.exception))\n\n    @skip_sdc_jit(""StringArray reflection was not implemented in old-pipeline"")\n    @skip_numba_jit(""TODO: support StringArray reflection"")\n    def test_series_setitem_str_reflection(self):\n        """""" Verifies that changes made to string Series passed into a jitted function\n            are propagated back to the native python object.\n        """"""\n        def test_impl(S, idx, val):\n            S[idx] = val\n            return S\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([\'cat\', \'\', \'bbb\', \'\', \'a\', None, \'a\', \'\', None, \'b\'])\n        idx, value = 0, \'dog\'\n        result = hpat_func(S, idx, value)\n        pd.testing.assert_series_equal(result, S)\n\n    def _test_series_setitem(self, all_data, indexes, idxs, values, dtype=None):\n        """""" Common function used by setitem tests to compile and run setitem on provided data""""""\n        def test_impl(A, i, value):\n            A[i] = value\n        hpat_func = self.jit(test_impl)\n\n        for series_data in all_data:\n            for series_index in indexes:\n                S = pd.Series(series_data, series_index, dtype=dtype, name=\'A\')\n                for idx, value in product(idxs, values):\n                    with self.subTest(series=S, idx=idx, value=value):\n                        S1 = S.copy(deep=True)\n                        S2 = S.copy(deep=True)\n                        hpat_func(S1, idx, value)\n                        test_impl(S2, idx, value)\n                        pd.testing.assert_series_equal(S1, S2)\n\n    @skip_sdc_jit(\'Not implemented in old-pipeline\')\n    @skip_numba_jit(\'Requires StringArray support of operator.eq\')\n    def test_series_setitem_idx_str_scalar(self):\n        """""" Verifies Series.setitem for scalar string idx operand and integer Series with index of matching dtype""""""\n\n        series_data = np.arange(5)\n        series_index = [\'a\', \'a\', \'c\', \'d\', \'a\']\n        idx = \'a\'\n        values_to_test = [-100,\n                          np.array([5, 55, 555]),\n                          pd.Series([5, 55, 555])]\n\n        self._test_series_setitem([series_data], [series_index], [idx], values_to_test)\n\n    @skip_sdc_jit(\'Not implemented in old-pipeline\')\n    def test_series_setitem_idx_int_scalar1(self):\n        """""" Verifies Series.setitem for scalar integer idx operand and integer Series with index of matching dtype""""""\n\n        series_data = np.arange(5)\n        series_index = [8, 8, 9, 8, 5]\n        idx = 8\n        values_to_test = [-100,\n                          np.array([5, 55, 555]),\n                          pd.Series([5, 55, 555])]\n\n        self._test_series_setitem([series_data], [series_index], [idx], values_to_test)\n\n    @skip_sdc_jit(\'Not implemented in old-pipeline\')\n    def test_series_setitem_idx_int_scalar2(self):\n        """""" Verifies Series.setitem for scalar integer idx operand and integer Series with\n            index of non matching dtype (i.e. set along positions, not index)""""""\n\n        n = 11\n        series_data = np.arange(n)\n        series_indexes = [\n                            None,\n                            np.arange(n, dtype=np.float),\n                            gen_strlist(n, 2, \'abcd123 \')\n        ]\n        idx = 8\n        value = -100\n        self._test_series_setitem([series_data], series_indexes, [idx], [value])\n\n    @skip_sdc_jit(\'Not implemented in old-pipeline\')\n    @skip_numba_jit(\'TODO: support replacing data in Series with new array\')\n    def test_series_setitem_idx_int_scalar_non_existing(self):\n        """""" Verifies adding new element to an integer Series by using Series.setitem with\n            scalar integer idx not present in index """"""\n        def test_impl(A, i, value):\n            A[i] = value\n        hpat_func = self.jit(test_impl)\n\n        idx, value = 0, -100\n        S1 = pd.Series(np.arange(5), index=[8, 8, 9, 8, 5])\n        S2 = S1.copy(deep=True)\n        hpat_func(S1, idx, value)\n        test_impl(S2, idx, value)\n        pd.testing.assert_series_equal(S1, S2)\n\n    @skip_sdc_jit(\'Not implemented in old-pipeline\')\n    def test_series_setitem_idx_str_series(self):\n        """""" Verifies Series.setitem for idx operand of type pandas.Series and string dtype called on\n            integer Series with index of matching dtype and scalar and non scalar assigned values """"""\n\n        n, k = 11, 4\n        series_data = np.arange(n)\n        series_index = gen_strlist(n, 2, \'abcd123 \')\n\n        idx = create_series_from_values(k, series_index, seed=0)\n        assigned_values = -10 + np.arange(k) * (-1)\n        values_to_test = [-100,\n                          np.array(assigned_values),\n                          pd.Series(assigned_values)]\n        self._test_series_setitem([series_data], [series_index], [idx], values_to_test, np.intp)\n\n    @skip_sdc_jit(\'Not implemented in old-pipeline\')\n    def test_series_setitem_idx_float_series(self):\n        """""" Verifies Series.setitem for idx operand of type pandas.Series and float dtype called on\n            integer Series with index of matching dtype and scalar and non scalar assigned values """"""\n\n        n, k = 11, 4\n        series_data = np.arange(n)\n        series_index = np.arange(n, dtype=np.float)\n\n        idx = create_series_from_values(k, series_index, seed=0)\n        assigned_values = -10 + np.arange(k) * (-1)\n        values_to_test = [\n                            -100,\n                            np.array(assigned_values),\n                            pd.Series(assigned_values)\n        ]\n        self._test_series_setitem([series_data], [series_index], [idx], values_to_test)\n\n    @skip_sdc_jit(\'Not implemented in old-pipeline\')\n    def test_series_setitem_idx_int_series1(self):\n        """""" Verifies Series.setitem for idx operand of type pandas.Series and integer dtype called on\n            integer Series with index of matching dtype and scalar and non scalar assigned values """"""\n        def test_impl(A, i, value):\n            A[i] = value\n        hpat_func = self.jit(test_impl)\n\n        n, k = 11, 4\n        series_data = np.arange(n)\n        series_index = np.arange(n)\n\n        idx = create_series_from_values(k, series_index, seed=0)\n        assigned_values = -10 + np.arange(k) * (-1)\n        values_to_test = [-100,\n                          np.array(assigned_values),\n                          pd.Series(assigned_values)]\n        self._test_series_setitem([series_data], [series_index], [idx], values_to_test)\n\n    @skip_sdc_jit(\'Not implemented in old-pipeline\')\n    def test_series_setitem_idx_int_series2(self):\n        """""" Verifies Series.setitem for idx operand of type pandas.Series and integer dtype called on\n            integer Series with index of non-matching dtype and scalar and non scalar assigned values """"""\n\n        n, k = 11, 4\n        series_data = np.arange(n)\n        series_index = gen_strlist(n, 2, \'abcd123 \')\n\n        idx = create_series_from_values(k, np.arange(n), seed=0)\n        assigned_values = -10 + np.arange(k) * (-1)\n        values_to_test = [-100,\n                          np.array(assigned_values),\n                          pd.Series(assigned_values)]\n        self._test_series_setitem([series_data], [series_index], [idx], values_to_test)\n\n    @skip_sdc_jit(\'Not implemented in old-pipeline\')\n    def test_series_setitem_idx_int_series3(self):\n        """""" Verifies negative case of using Series.setitem with idx operand of type pandas.Series\n            and integer dtype called on integer Series with index that has duplicate values """"""\n        def test_impl(A, i, value):\n            A[i] = value\n        hpat_func = self.jit(test_impl)\n\n        value = pd.Series(-10 + np.arange(5) * (-1))\n        idx = pd.Series([8, 5, 9])\n        S = pd.Series(np.arange(5), index=[8, 8, 9, 8, 5])\n\n        # pandas raises it\'s own exception - pandas.core.indexes.base.InvalidIndexError\n        # SDC implementation currently raises ValueError, so assert for the correct message only\n        with self.assertRaises(ValueError) as raises:\n            hpat_func(S, idx, value)\n        msg = \'Reindexing only valid with uniquely valued Index objects\'\n        self.assertIn(msg, str(raises.exception))\n\n    @skip_sdc_jit(\'Not implemented in old-pipeline\')\n    def test_series_setitem_idx_int_series4(self):\n        """""" Verifies negative case of using Series.setitem with idx operand of type pandas.Series\n            and integer dtype called on integer Series with index not containg some values in idx """"""\n        def test_impl(A, i, value):\n            A[i] = value\n        hpat_func = self.jit(test_impl)\n\n        value = -100\n        idx = pd.Series([1, 5, 77])\n        S1 = pd.Series(np.arange(5), index=[1, 4, 3, 2, 5])\n        S2 = S1.copy(deep=True)\n\n        with self.assertRaises(Exception) as context:\n            test_impl(S2, idx, value)\n        pandas_exception = context.exception\n\n        self.assertRaises(type(pandas_exception), hpat_func, S1, idx, value)\n\n    @skip_sdc_jit(\'Not implemented in old-pipeline\')\n    def test_series_setitem_idx_int_array1(self):\n        """""" Verifies Series.setitem for idx operand of type numpy.ndarray and integer dtype called on\n            integer Series with integer index and scalar and non scalar assigned values """"""\n\n        n, k = 11, 4\n        series_data = np.arange(n)\n        series_index = np.arange(n)\n\n        np.random.seed(0)\n        idx = take_k_elements(k, series_index, seed=0)\n        assigned_values = -10 + np.arange(k) * (-1)\n        values_to_test = [\n                            -100,\n                            np.array(assigned_values),\n                            pd.Series(assigned_values)\n        ]\n        self._test_series_setitem([series_data], [series_index], [idx], values_to_test)\n\n    @skip_sdc_jit(\'Not implemented in old-pipeline\')\n    def test_series_setitem_idx_int_array2(self):\n        """""" Verifies Series.setitem for idx operand of type numpy.ndarray and integer dtype called on\n            integer Series with string index and scalar and non scalar assigned values """"""\n\n        n, k = 11, 4\n        series_data = np.arange(n)\n        series_index = gen_strlist(n, 2, \'abcd123 \')\n\n        idx = take_k_elements(k, np.arange(n), seed=0)\n        assigned_values = -10 + np.arange(k) * (-1)\n        values_to_test = [\n                            -100,\n                            np.array(assigned_values),\n                            pd.Series(assigned_values)\n        ]\n        self._test_series_setitem([series_data], [series_index], [idx], values_to_test)\n\n    @skip_sdc_jit(\'Not implemented in old-pipeline\')\n    def test_series_setitem_idx_int_slice1(self):\n        """""" Verifies that Series.setitem for int slice as idx operand called on integer Series\n            with index of matching dtype assigns vector value along positions (but not along index) """"""\n        def test_impl(A, i, value):\n            A[i] = value\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series(np.arange(7), index=[2, 5, 1, 3, 6, 0, 4])\n        slices_to_test = [(4, ), (None, 4), (1, 4), (2, 7, 3), (None, -4), (-4, ), (None, ), (None, None, 2)]\n\n        for slice_members in slices_to_test:\n            idx = slice(*slice_members)\n            k = len(np.arange(len(S))[idx])\n            assigned_values = -10 + np.arange(k) * (-1)\n            value = pd.Series(assigned_values)\n            with self.subTest(value=value):\n                S1 = S.copy(deep=True)\n                S2 = S.copy(deep=True)\n                hpat_func(S1, idx, value)\n                test_impl(S2, idx, value)\n                pd.testing.assert_series_equal(S1, S2)\n\n    @skip_sdc_jit(\'Not implemented in old-pipeline\')\n    def test_series_setitem_idx_int_slice2(self):\n        """""" Verifies that Series.setitem for int slice as idx operand called on integer Series\n            with index of matching dtype assigns scalar value along positions (but not along index) """"""\n        def test_impl(A, idx, value):\n            A[idx] = value\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        dtype_to_data = {\n            \'int\':   np.arange(n),\n            \'float\': np.arange(n, dtype=np.float_)\n        }\n        dtype_to_values = {\n            \'int\':   [100, -100],\n            \'float\': [np.nan, np.inf, 1.25, np.PZERO, -2]\n        }\n        series_indexes = [\n             None,\n             np.arange(n, dtype=\'int\'),\n             gen_strlist(n, 2, \'abcd123 \')\n        ]\n\n        slices_to_test = [(4, ), (None, 4), (1, 4), (2, 7, 3), (None, -4), (-4, ), (None, ), (None, None, 2)]\n        for dtype, series_data in dtype_to_data.items():\n            for value in dtype_to_values[dtype]:\n                for index in series_indexes:\n                    S = pd.Series(series_data, index=index)\n                    for slice_members in slices_to_test:\n                        idx = slice(*slice_members)\n                        S1 = S.copy(deep=True)\n                        S2 = S.copy(deep=True)\n                        with self.subTest(series=S1, idx=idx, value=value):\n                            hpat_func(S1, idx, value)\n                            test_impl(S2, idx, value)\n                            pd.testing.assert_series_equal(S1, S2)\n\n    @unittest.expectedFailure   # Fails due to incorrect behavior of pandas (doesn\'t set anything)\n    def test_series_setitem_idx_int_slice2_fixme(self):\n        """""" The same as test_series_setitem_idx_int_slice2, but for float series index.\n            Fails because pandas doesn\'t make assignment.\n        """"""\n        def test_impl(A, idx, value):\n            A[idx] = value\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        value = 1.25\n        series_indexes = [np.arange(n, dtype=\'float\')]\n        slices_to_test = [(4, ), (None, 4), (1, 4), (2, 7, 3), (None, -4), (-4, ), (None, ), (None, None, 2)]\n\n        for index in series_indexes:\n            S = pd.Series(np.arange(n, dtype=np.float_), index=index)\n            for slice_members in slices_to_test:\n                idx = slice(*slice_members)\n                S1 = S.copy(deep=True)\n                S2 = S.copy(deep=True)\n                with self.subTest(series=S1, idx=idx, value=value):\n                    hpat_func(S1, idx, value)\n                    test_impl(S2, idx, value)\n                    pd.testing.assert_series_equal(S1, S2)\n\n    @skip_sdc_jit(\'Not implemented in old-pipeline\')\n    def test_series_setitem_idx_int_scalar_no_dtype_change(self):\n        """""" Verifies that setting float value to an element of integer Series via scalar integer index\n            converts the value and keeps origin Series dtype unchanged """"""\n        def test_impl(A, i, value):\n            A[i] = value\n        hpat_func = self.jit(test_impl)\n\n        idx, value = 3, -100.25\n        S1 = pd.Series(np.arange(5), index=[5, 3, 1, 3, 2], dtype=\'int\')\n        S2 = S1.copy(deep=True)\n        hpat_func(S1, idx, value)\n        test_impl(S2, idx, value)\n        pd.testing.assert_series_equal(S1, S2)\n\n    @skip_sdc_jit(\'Not implemented in old-pipeline\')\n    @skip_numba_jit(\'TODO: support changing Series.dtype\')\n    def test_series_setitem_idx_int_slice_dtype_change(self):\n        """""" Verifies that setting float value to an element of integer Series with default index via integer slice\n            does not trim the value but promotes Series dtype """"""\n        def test_impl(A, i, value):\n            A[i] = value\n        hpat_func = self.jit(test_impl)\n\n        idx, value = slice(1, 3), -100.25\n        S1 = pd.Series(np.arange(5), index=[5, 3, 1, 3, 2], dtype=\'int\')\n        S2 = S1.copy(deep=True)\n        hpat_func(S1, idx, value)\n        test_impl(S2, idx, value)\n        pd.testing.assert_series_equal(S1, S2)\n\n    @skip_sdc_jit(\'Not implemented in old-pipeline\')\n    def test_series_setitem_idx_bool_series1(self):\n        """""" Verifies Series.setitem assigning scalar and non scalar values\n            via mask indicated by a Boolean pandas.Series with integer index """"""\n\n        n, k = 11, 4\n        np.random.seed(0)\n        series_data = np.arange(n)\n        series_index = np.arange(n)\n\n        # create a bool Series with the same len as S and True values at exactly k positions\n        idx = pd.Series(np.zeros(n, dtype=np.bool))\n        idx[take_k_elements(k, np.arange(n))] = True\n        values_index = take_k_elements(k, series_index)\n\n        assigned_values = -10 + np.arange(k) * (-1)\n        values_to_test = [\n                            -100,\n                            np.array(assigned_values),\n                            pd.Series(assigned_values),\n                            pd.Series(assigned_values, index=values_index),\n                            pd.Series(assigned_values, index=values_index.astype(\'float\'))\n        ]\n        self._test_series_setitem([series_data], [series_index], [idx], values_to_test, dtype=np.float)\n\n    @skip_sdc_jit(\'Not implemented in old-pipeline\')\n    def test_series_setitem_idx_bool_series2(self):\n        """""" Verifies Series.setitem assigning scalar and non scalar values\n            via mask indicated by a Boolean pandas.Series with string index """"""\n\n        n, k = 11, 4\n        np.random.seed(0)\n        series_data = np.arange(n)\n        series_index = gen_strlist(n, 2, \'abcd123 \')\n\n        # create a bool Series with the same len as S, reordered values from original series index\n        # as index and True values at exactly k positions\n        idx = pd.Series(np.zeros(n, dtype=np.bool), index=take_k_elements(n, series_index))\n        idx[take_k_elements(k, np.arange(n))] = True\n        values_index = take_k_elements(k, series_index)\n\n        assigned_values = -10 + np.arange(k) * (-1)\n        values_to_test = [\n                            -100,\n                            np.array(assigned_values),\n                            pd.Series(assigned_values, index=values_index),\n        ]\n        self._test_series_setitem([series_data], [series_index], [idx], values_to_test, dtype=np.float)\n\n    @skip_sdc_jit(\'Not implemented in old-pipeline\')\n    def test_series_setitem_idx_bool_array1(self):\n        """""" Verifies Series.setitem for idx operand of type numpy.ndarray and Boolean dtype called on\n            integer Series with default index and scalar and non scalar assigned values. Due to no duplicates\n            in idx.index or S.index the assignment is made along provided indexes. """"""\n\n        n, k = 11, 4\n        np.random.seed(0)\n        series_data = np.arange(n)\n        series_index = np.arange(n)\n\n        # create a bool array with the same len as S and True values at exactly k positions\n        idx = np.zeros(n, dtype=np.bool)\n        idx[take_k_elements(k, np.arange(n))] = True\n        values_index = take_k_elements(k, series_index)\n\n        assigned_values = -10 + np.arange(k) * (-1)\n        values_to_test = [\n                            -100,\n                            np.array(assigned_values),\n                            pd.Series(assigned_values),\n                            pd.Series(assigned_values, index=values_index),\n                            pd.Series(assigned_values, index=values_index.astype(\'float\'))\n        ]\n        self._test_series_setitem([series_data], [series_index], [idx], values_to_test, dtype=np.float)\n\n    @skip_sdc_jit(\'Not implemented in old-pipeline\')\n    def test_series_setitem_idx_bool_array2(self):\n        """""" Verifies Series.setitem for idx operand of type numpy.ndarray and Boolean dtype called on\n            integer Series with default index and scalar and non scalar assigned values. Due to duplicates\n            in idx.index and S.index the assignment is made along Series positions (but not index). """"""\n\n        n, k = 7, 4\n        series_data = np.arange(n)\n        series_index = [6, 2, 3, 2, 7, 5, 1]\n\n        idx = np.asarray([False, True, True, False, False, True, True])\n        assigned_values = -10 + np.arange(k) * (-1)\n        values_to_test = [\n                            pd.Series(assigned_values),\n                            pd.Series(assigned_values, index=[1, 6, 3, 5])\n        ]\n        self._test_series_setitem([series_data], [series_index], [idx], values_to_test, dtype=np.float)\n\n    @skip_sdc_jit(\'Not implemented in old-pipeline\')\n    def test_series_setitem_idx_bool_array3(self):\n        """""" Verifies Series.setitem for idx operand of type numpy.ndarray and Boolean dtype called on\n            integer Series with string index and scalar and non scalar assigned values. Due to no duplicates\n            in idx.index or S.index the assignment is made along provided indexes. """"""\n\n        n, k = 11, 4\n        np.random.seed(0)\n        series_data = np.arange(n)\n        series_index = gen_strlist(n, 2, \'abcd123 \')\n\n        # create a bool array with the same len as S and True values at exactly k positions\n        idx = np.zeros(n, dtype=np.bool)\n        idx[take_k_elements(k, np.arange(n))] = True\n        values_index = take_k_elements(k, series_index)\n\n        assigned_values = -10 + np.arange(k) * (-1)\n        values_to_test = [\n                            -100,\n                            np.array(assigned_values),\n                            pd.Series(assigned_values, index=values_index)\n        ]\n        self._test_series_setitem([series_data], [series_index], [idx], values_to_test, dtype=np.float)\n\n    def _test_series_getitem(self, all_data, indexes, idxs, dtype=None):\n        """""" Common function used by getitem tests to compile and run getitem on provided data""""""\n        def test_impl(A, idx):\n            return A[idx]\n        hpat_func = self.jit(test_impl)\n\n        for series_data in all_data:\n            for series_index in indexes:\n                S = pd.Series(series_data, series_index, dtype=dtype, name=\'A\')\n                for idx in idxs:\n                    with self.subTest(series=S, idx=idx):\n                        result = hpat_func(S, idx)\n                        result_ref = test_impl(S, idx)\n                        pd.testing.assert_series_equal(result, result_ref)\n\n    @skip_sdc_jit(\'Not implemented in old-pipeline\')\n    def test_series_getitem_idx_bool_array1(self):\n        """""" Verifies Series.getitem by mask indicated by a Boolean array on Series of various dtypes and indexes """"""\n\n        n = 11\n        np.random.seed(0)\n        data_to_test = [\n            np.arange(n),\n            np.arange(n, dtype=\'float\'),\n            np.random.choice([True, False], n),\n            gen_strlist(n, 2, \'abcd123 \')\n        ]\n        idxs_to_test = [\n            None,\n            np.arange(n),\n            np.arange(n, dtype=\'float\'),\n            gen_strlist(n, 2, \'abcd123 \')\n        ]\n\n        idx = np.random.choice([True, False], n)\n        self._test_series_getitem(data_to_test, idxs_to_test, [idx])\n\n    @skip_sdc_jit(\'Not implemented in old-pipeline\')\n    def test_series_getitem_idx_bool_array2(self):\n        """""" Verifies negative case of using Series.getitem by Boolean array indexer\n        on a Series with different length than the indexer """"""\n        def test_impl(A, idx):\n            return A[idx]\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        np.random.seed(0)\n        S = pd.Series(np.arange(n))\n        idxs_to_test = [\n            np.random.choice([True, False], n - 3),\n            np.random.choice([True, False], n + 3)\n        ]\n\n        for idx in idxs_to_test:\n            with self.subTest(idx=idx):\n                with self.assertRaises(Exception) as context:\n                    test_impl(S, idx)\n                pandas_exception = context.exception\n\n                with self.assertRaises(type(pandas_exception)) as context:\n                    hpat_func(S, idx)\n                sdc_exception = context.exception\n                self.assertIn(str(sdc_exception), str(pandas_exception))\n\n    @skip_sdc_jit(\'Not implemented in old-pipeline\')\n    def test_series_getitem_idx_bool_list(self):\n        """""" Verifies Series.getitem by mask indicated by a Boolean list on Series of various dtypes and indexes """"""\n\n        n = 11\n        np.random.seed(0)\n\n        data_to_test = [\n            np.arange(n),\n            np.arange(n, dtype=\'float\'),\n            np.random.choice([True, False], n),\n            gen_strlist(n, 2, \'abcd123 \')\n        ]\n        idxs_to_test = [\n            None,\n            np.arange(n),\n            np.arange(n, dtype=\'float\'),\n            gen_strlist(n, 2, \'abcd123 \')\n        ]\n\n        idx = list(np.random.choice([True, False], n))\n        self._test_series_getitem(data_to_test, idxs_to_test, [idx])\n\n    @skip_sdc_jit(\'Not implemented in old-pipeline\')\n    def test_series_getitem_idx_bool_series1(self):\n        """""" Verifies Series.getitem by mask indicated by a Boolean Series on Series of various dtypes\n        when both Series and indexer have default indexes """"""\n\n        n, k = 21, 13\n        np.random.seed(0)\n\n        data_to_test = [\n            np.arange(n),\n            np.arange(n, dtype=\'float\'),\n            np.random.choice([True, False], n),\n            gen_strlist(n, 2, \'abcd123 \')\n        ]\n\n        idxs_to_test = []\n        for s in (n, 2*n):\n            idx = pd.Series(np.zeros(s, dtype=np.bool), index=None)\n            idx[take_k_elements(k, np.arange(s))] = True\n            idxs_to_test.append(idx)\n\n        self._test_series_getitem(data_to_test, [None], idxs_to_test)\n\n    @skip_sdc_jit(\'Not implemented in old-pipeline\')\n    def test_series_getitem_idx_bool_series2(self):\n        """""" Verifies negative case of using Series.getitem with Boolean Series indexer idx with default index\n        on a Series with default index but wider range of index values """"""\n        def test_impl(A, idx):\n            return A[idx]\n        hpat_func = self.jit(test_impl)\n\n        n, k = 21, 13\n        np.random.seed(0)\n\n        S = pd.Series(np.arange(n))\n        idx = pd.Series(np.zeros(n - 3, dtype=np.bool), index=None)\n        idx[take_k_elements(k, np.arange(k))] = True\n\n        with self.assertRaises(Exception) as context:\n            test_impl(S, idx)\n        pandas_exception = context.exception\n\n        with self.assertRaises(type(pandas_exception)) as context:\n            hpat_func(S, idx)\n        sdc_exception = context.exception\n        self.assertIn(str(sdc_exception), str(pandas_exception))\n\n    @skip_sdc_jit(\'Not implemented in old-pipeline\')\n    def test_series_getitem_idx_bool_series3(self):\n        """""" Verifies Series.getitem by mask indicated by a Boolean Series with the same object as index """"""\n        def test_impl(A, mask, index):\n            S = pd.Series(A, index)\n            idx = pd.Series(mask, S.index)\n            return S[idx]\n        hpat_func = self.jit(test_impl)\n\n        n = 11\n        np.random.seed(0)\n\n        idxs_to_test = [\n            np.arange(n),\n            np.arange(n, dtype=\'float\'),\n            gen_strlist(n, 2, \'abcd123 \')\n        ]\n        series_data = np.arange(n)\n        mask = np.random.choice([True, False], n)\n        for index in idxs_to_test:\n            with self.subTest(series_index=index):\n                result = hpat_func(series_data, mask, index)\n                result_ref = test_impl(series_data, mask, index)\n                pd.testing.assert_series_equal(result, result_ref)\n\n    @skip_sdc_jit(\'Not implemented in old-pipeline\')\n    def test_series_getitem_idx_bool_series_reindex(self):\n        """""" Verifies Series.getitem with reindexing by mask indicated by a Boolean Series\n        on Series with various types of indexes """"""\n\n        def test_impl(A, idx):\n            return A[idx]\n        hpat_func = self.jit(test_impl)\n\n        n, k = 21, 13\n        np.random.seed(0)\n\n        idx_indexes_to_test = {\n            \'default\': None,\n            \'int\': np.arange(n),\n            \'float\': np.arange(n, dtype=\'float\'),\n            \'str\': gen_strlist(n, 2, \'abcd123 \')\n        }\n\n        idx_data = np.random.choice([True, False], n)\n        for idx_index in idx_indexes_to_test.values():\n            idx = pd.Series(idx_data, idx_index)\n            # create a series with index values in idx_index\n            idx_values = idx_index if idx_index is not None else np.arange(k)\n            series_index = np.random.choice(idx_values, k)\n            S = pd.Series(np.arange(k), index=series_index)\n            with self.subTest(series=S, idx=idx):\n                result = hpat_func(S, idx)\n                result_ref = test_impl(S, idx)\n                pd.testing.assert_series_equal(result, result_ref)\n\n    @skip_sdc_jit(\'Not implemented in old-pipeline\')\n    def test_series_getitem_idx_bool_series_restrictions1(self):\n        """""" Verifies negative case of using Series.getitem with Boolean indexer with duplicate index values """"""\n        def test_impl(A, idx):\n            return A[idx]\n        hpat_func = self.jit(test_impl)\n\n        n = 7\n        np.random.seed(0)\n\n        S = pd.Series(np.arange(n))\n        idx_data = [True, False, False, True, False, False, True]\n        idx = pd.Series(idx_data, index=[0, 1, 2, 3, 4, 5, 0])\n\n        with self.assertRaises(Exception) as context:\n            test_impl(S, idx)\n        pandas_exception = context.exception\n\n        with self.assertRaises(type(pandas_exception)) as context:\n            hpat_func(S, idx)\n        sdc_exception = context.exception\n        self.assertIn(str(sdc_exception), str(pandas_exception))\n\n    @skip_sdc_jit(\'Not implemented in old-pipeline\')\n    def test_series_getitem_idx_bool_series_restrictions2(self):\n        """""" Verifies negative case of using Series.getitem with Boolean indexer\n        on a Series with some indices not present in the indexer (reindexing failure) """"""\n        def test_impl(A, idx):\n            return A[idx]\n        hpat_func = self.jit(test_impl)\n\n        n = 7\n        np.random.seed(0)\n\n        S = pd.Series(np.arange(n), index=[5, 3, 1, 2, 6, 4, 0])\n        idx_data = [True, False, True, True, False]\n        idx = pd.Series(idx_data, index=[4, 3, 2, 1, 0])\n\n        with self.assertRaises(Exception) as context:\n            test_impl(S, idx)\n        pandas_exception = context.exception\n\n        with self.assertRaises(type(pandas_exception)) as context:\n            hpat_func(S, idx)\n        sdc_exception = context.exception\n        self.assertIn(str(sdc_exception), str(pandas_exception))\n\n    @skip_sdc_jit(\'Not implemented in old-pipeline\')\n    def test_series_getitem_idx_bool_series_restrictions3(self):\n        """""" Verifies negative case of using Series.getitem with Boolean indexer\n        on a Series with index of different type that in the indexer """"""\n        def test_impl(A, idx):\n            return A[idx]\n        hpat_func = self.jit(test_impl)\n\n        n = 7\n        np.random.seed(0)\n\n        incompatible_indexes = [\n            np.arange(n),\n            gen_strlist(n, 2, \'abcd123 \')\n        ]\n        for index1, index2 in combinations(incompatible_indexes, 2):\n            S = pd.Series(np.arange(n), index=index1)\n            idx = pd.Series(np.random.choice([True, False], n), index=index2)\n            with self.subTest(series_index=index1, idx_index=index2):\n                with self.assertRaises(TypingError) as raises:\n                    hpat_func(S, idx)\n                msg = \'The index of boolean indexer is not comparable to Series index.\'\n                self.assertIn(msg, str(raises.exception))\n\n    def test_series_skew(self):\n        def test_impl(series, axis, skipna):\n            return series.skew(axis=axis, skipna=skipna)\n\n        hpat_func = self.jit(test_impl)\n        test_data = [[6, 6, 2, 1, 3, 3, 2, 1, 2],\n                     [1.1, 0.3, 2.1, 1, 3, 0.3, 2.1, 1.1, 2.2],\n                     [6, 6.1, 2.2, 1, 3, 3, 2.2, 1, 2],\n                     [],\n                     [6, 6, np.nan, 2, np.nan, 1, 3, 3, np.inf, 2, 1, 2, np.inf],\n                     [1.1, 0.3, np.nan, 1.0, np.inf, 0.3, 2.1, np.nan, 2.2, np.inf],\n                     [1.1, 0.3, np.nan, 1, np.inf, 0, 1.1, np.nan, 2.2, np.inf, 2, 2],\n                     [np.nan, np.nan, np.nan],\n                     [np.nan, np.nan, np.inf],\n                     [np.inf, 0, np.inf, 1, 2, 3, 4, 5]\n                     ]\n        all_test_data = test_data + test_global_input_data_float64\n        for data in all_test_data:\n            with self.subTest(data=data):\n                s = pd.Series(data)\n                for axis in [0, None]:\n                    with self.subTest(axis=axis):\n                        for skipna in [None, False, True]:\n                            with self.subTest(skipna=skipna):\n                                res1 = test_impl(s, axis, skipna)\n                                res2 = hpat_func(s, axis, skipna)\n                                np.testing.assert_allclose(res1, res2)\n\n    def test_series_skew_default(self):\n        def test_impl():\n            s = pd.Series([np.nan, -2., 3., 9.1])\n            return s.skew()\n\n        hpat_func = self.jit(test_impl)\n        np.testing.assert_allclose(test_impl(), hpat_func())\n\n    def test_series_skew_not_supported(self):\n        def test_impl(series, axis=None, skipna=None, level=None, numeric_only=None):\n            return series.skew(axis=axis, skipna=skipna, level=level, numeric_only=numeric_only)\n\n        hpat_func = self.jit(test_impl)\n        s = pd.Series([1.1, 0.3, np.nan, 1, np.inf, 0, 1.1, np.nan, 2.2, np.inf, 2, 2])\n        with self.assertRaises(TypingError) as raises:\n            hpat_func(s, axis=0.75)\n        msg = \'TypingError: Method Series.skew() The object axis\\n given: float64\\n expected: int64\'\n        self.assertIn(msg, str(raises.exception))\n\n        with self.assertRaises(TypingError) as raises:\n            hpat_func(s, skipna=0)\n        msg = \'TypingError: Method Series.skew() The object skipna\\n given: int64\\n expected: bool\'\n        self.assertIn(msg, str(raises.exception))\n\n        with self.assertRaises(TypingError) as raises:\n            hpat_func(s, level=0)\n        msg = \'TypingError: Method Series.skew() The object level\\n given: int64\\n expected: None\'\n        self.assertIn(msg, str(raises.exception))\n\n        with self.assertRaises(TypingError) as raises:\n            hpat_func(s, numeric_only=0)\n        msg = \'TypingError: Method Series.skew() The object numeric_only\\n given: int64\\n expected: None\'\n        self.assertIn(msg, str(raises.exception))\n\n        with self.assertRaises(ValueError) as raises:\n            hpat_func(s, axis=5)\n        msg = \'Parameter axis must be only 0 or None.\'\n        self.assertIn(msg, str(raises.exception))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
sdc/tests/test_series_apply.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numba\nimport numpy as np\nimport pandas as pd\n\nfrom sdc.tests.test_base import TestCase\nfrom sdc.tests.test_utils import skip_numba_jit, skip_sdc_jit\n\n\nDATA = [1.0, 2., 3., 4., 5.]\nINDEX = [5, 4, 3, 2, 1]\nNAME = ""sname""\n\n\ndef series_apply_square_usecase(S):\n\n    def square(x):\n        return x ** 2\n\n    return S.apply(square)\n\n\nclass TestSeries_apply(object):\n\n    def test_series_apply(self):\n        test_impl = series_apply_square_usecase\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series(DATA)\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    def test_series_apply_convert_type(self):\n        def test_impl(S):\n            def to_int(x):\n                return int(x)\n            return S.apply(to_int)\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series(DATA)\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    @skip_sdc_jit(""Series.index values are different"")\n    def test_series_apply_index(self):\n        test_impl = series_apply_square_usecase\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series(DATA, INDEX)\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    @skip_sdc_jit(\'Attribute ""name"" are different\')\n    def test_series_apply_name(self):\n        test_impl = series_apply_square_usecase\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series(DATA, name=NAME)\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    def test_series_apply_lambda(self):\n        def test_impl(S):\n            return S.apply(lambda a: 2 * a)\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series(DATA)\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    @skip_sdc_jit(""\'Var\' object has no attribute \'py_func\'"")\n    def test_series_apply_np(self):\n        def test_impl(S):\n            return S.apply(np.log)\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series(DATA)\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    @skip_sdc_jit(""\'function\' object has no attribute \'py_func\'"")\n    def test_series_apply_register_jitable(self):\n        @numba.extending.register_jitable\n        def square(x):\n            return x ** 2\n\n        def test_impl(S):\n            return S.apply(square)\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series(DATA)\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    @skip_numba_jit(""\'args\' in apply is not supported"")\n    @skip_sdc_jit(""\'args\' in apply is not supported"")\n    def test_series_apply_args(self):\n        @numba.extending.register_jitable\n        def subtract_custom_value(x, custom_value):\n            return x - custom_value\n\n        def test_impl(S):\n            return S.apply(subtract_custom_value, args=(5,))\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series(DATA)\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    @skip_numba_jit(""\'kwds\' in apply is not supported"")\n    @skip_sdc_jit(""\'kwds\' in apply is not supported"")\n    def test_series_apply_kwds(self):\n        @numba.extending.register_jitable\n        def add_custom_values(x, **kwargs):\n            for month in kwargs:\n                x += kwargs[month]\n            return x\n\n        def test_impl(S):\n            return S.apply(add_custom_values, june=30, july=20, august=25)\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series(DATA)\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n\nclass _Test(TestSeries_apply, TestCase):\n    pass\n'"
sdc/tests/test_series_map.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\n\nfrom sdc.tests.test_base import TestCase\nfrom sdc.tests.test_utils import skip_numba_jit, skip_sdc_jit\n\n\nGLOBAL_VAL = 2\n\n\nclass TestSeries_map(object):\n\n    def test_series_map1(self):\n        def test_impl(S):\n            return S.map(lambda a: 2 * a)\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([1.0, 2., 3., 4., 5.])\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    def test_series_map_global1(self):\n        def test_impl(S):\n            return S.map(lambda a: a + GLOBAL_VAL)\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([1.0, 2., 3., 4., 5.])\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    @skip_numba_jit\n    def test_series_map_tup1(self):\n        def test_impl(S):\n            return S.map(lambda a: (a, 2 * a))\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([1.0, 2., 3., 4., 5.])\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    @skip_numba_jit\n    def test_series_map_tup_map1(self):\n        def test_impl(S):\n            A = S.map(lambda a: (a, 2 * a))\n            return A.map(lambda a: a[1])\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([1.0, 2., 3., 4., 5.])\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n    @skip_sdc_jit\n    def test_series_map_dict(self):\n        def test_impl(S):\n            return S.map({2.: 42., 4.: 3.14})\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([1., 2., 3., 4., 5.])\n        pd.testing.assert_series_equal(hpat_func(S), test_impl(S))\n\n\nclass _Test(TestSeries_map, TestCase):\n    pass\n'"
sdc/tests/test_strings.py,4,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\n# -*- coding: utf-8 -*-\n\nimport gc\nimport glob\nimport numpy as np\nimport pandas as pd\nimport platform\nimport pyarrow.parquet as pq\nimport re\nimport sdc\nimport unittest\n\nfrom sdc.str_arr_ext import StringArray\nfrom sdc.str_ext import std_str_to_unicode, unicode_to_std_str\nfrom sdc.tests.gen_test_data import ParquetGenerator\nfrom sdc.tests.test_base import TestCase\nfrom sdc.tests.test_utils import skip_numba_jit\n\n\nclass TestStrings(TestCase):\n\n    def test_pass_return(self):\n        def test_impl(_str):\n            return _str\n        hpat_func = self.jit(test_impl)\n\n        # pass single string and return\n        arg = \'test_str\'\n        self.assertEqual(hpat_func(arg), test_impl(arg))\n\n        # pass string list and return\n        arg = [\'test_str1\', \'test_str2\']\n        self.assertEqual(hpat_func(arg), test_impl(arg))\n\n    def test_const(self):\n        def test_impl():\n            return \'test_str\'\n        hpat_func = self.jit(test_impl)\n        self.assertEqual(hpat_func(), test_impl())\n\n    @skip_numba_jit\n    def test_str2str(self):\n        str2str_methods = [\'capitalize\', \'casefold\', \'lower\', \'lstrip\',\n                           \'rstrip\', \'strip\', \'swapcase\', \'title\', \'upper\']\n        for method in str2str_methods:\n            func_text = ""def test_impl(_str):\\n""\n            func_text += ""  return _str.{}()\\n"".format(method)\n            loc_vars = {}\n            exec(func_text, {}, loc_vars)\n            test_impl = loc_vars[\'test_impl\']\n            hpat_func = self.jit(test_impl)\n\n            arg = \' \\tbbCD\\t \'\n            self.assertEqual(hpat_func(arg), test_impl(arg))\n\n    def test_equality(self):\n        arg = \'test_str\'\n\n        def test_impl(_str):\n            return (_str == \'test_str\')\n        hpat_func = self.jit(test_impl)\n\n        self.assertEqual(hpat_func(arg), test_impl(arg))\n\n        def test_impl(_str):\n            return (_str != \'test_str\')\n        hpat_func = self.jit(test_impl)\n\n        self.assertEqual(hpat_func(arg), test_impl(arg))\n\n    def test_concat(self):\n        def test_impl(_str):\n            return (_str + \'test_str\')\n        hpat_func = self.jit(test_impl)\n\n        arg = \'a_\'\n        self.assertEqual(hpat_func(arg), test_impl(arg))\n\n    def test_split(self):\n        def test_impl(_str):\n            return _str.split(\'/\')\n        hpat_func = self.jit(test_impl)\n\n        arg = \'aa/bb/cc\'\n        self.assertEqual(hpat_func(arg), test_impl(arg))\n\n    def test_replace(self):\n        def test_impl(_str):\n            return _str.replace(\'/\', \';\')\n        hpat_func = self.jit(test_impl)\n\n        arg = \'aa/bb/cc\'\n        self.assertEqual(hpat_func(arg), test_impl(arg))\n\n    def test_getitem_int(self):\n        def test_impl(_str):\n            return _str[3]\n        hpat_func = self.jit(test_impl)\n\n        arg = \'aa/bb/cc\'\n        self.assertEqual(hpat_func(arg), test_impl(arg))\n\n    def test_string_int_cast(self):\n        def test_impl(_str):\n            return int(_str)\n        hpat_func = self.jit(test_impl)\n\n        arg = \'12\'\n        self.assertEqual(hpat_func(arg), test_impl(arg))\n\n    def test_string_float_cast(self):\n        def test_impl(_str):\n            return float(_str)\n        hpat_func = self.jit(test_impl)\n\n        arg = \'12.2\'\n        self.assertEqual(hpat_func(arg), test_impl(arg))\n\n    def test_string_str_cast(self):\n        def test_impl(a):\n            return str(a)\n        hpat_func = self.jit(test_impl)\n\n        for arg in [np.int32(45), 43, np.float32(1.4), 4.5]:\n            py_res = test_impl(arg)\n            h_res = hpat_func(arg)\n            # XXX: use startswith since hpat output can have extra characters\n            self.assertTrue(h_res.startswith(py_res))\n\n    def test_re_sub(self):\n        def test_impl(_str):\n            p = re.compile(\'ab*\')\n            return p.sub(\'ff\', _str)\n        hpat_func = self.jit(test_impl)\n\n        arg = \'aabbcc\'\n        self.assertEqual(hpat_func(arg), test_impl(arg))\n\n    def test_regex_std(self):\n        def test_impl(_str, _pat):\n            return sdc.str_ext.contains_regex(\n                _str, sdc.str_ext.compile_regex(_pat))\n        hpat_func = self.jit(test_impl)\n\n        self.assertEqual(hpat_func(\'What does the fox say\',\n                                   r\'d.*(the |fox ){2}\'), True)\n        self.assertEqual(hpat_func(\'What does the fox say\', r\'[kz]u*\'), False)\n\n    def test_replace_regex_std(self):\n        def test_impl(_str, pat, val):\n            s = unicode_to_std_str(_str)\n            e = sdc.str_ext.compile_regex(unicode_to_std_str(pat))\n            val = unicode_to_std_str(val)\n            out = sdc.str_ext.str_replace_regex(s, e, val)\n            return std_str_to_unicode(out)\n        hpat_func = self.jit(test_impl)\n\n        _str = \'What does the fox say\'\n        pat = r\'d.*(the |fox ){2}\'\n        val = \'does the cat \'\n        self.assertEqual(\n            hpat_func(_str, pat, val),\n            _str.replace(re.compile(pat).search(_str).group(), val)\n        )\n\n    # string array tests\n    def test_string_array_constructor(self):\n        # create StringArray and return as list of strings\n        def test_impl():\n            return StringArray([\'ABC\', \'BB\', \'CDEF\'])\n        hpat_func = self.jit(test_impl)\n\n        self.assertTrue(np.array_equal(hpat_func(), [\'ABC\', \'BB\', \'CDEF\']))\n\n    @skip_numba_jit\n    def test_string_array_comp(self):\n        def test_impl():\n            A = StringArray([\'ABC\', \'BB\', \'CDEF\'])\n            B = A == \'ABC\'\n            return B.sum()\n        hpat_func = self.jit(test_impl)\n\n        self.assertEqual(hpat_func(), 1)\n\n    @skip_numba_jit\n    def test_string_series(self):\n        def test_impl(ds):\n            rs = ds == \'one\'\n            return ds, rs\n        hpat_func = self.jit(test_impl)\n\n        df = pd.DataFrame(\n            {\n                \'A\': [1, 2, 3] * 33,\n                \'B\': [\'one\', \'two\', \'three\'] * 33\n            }\n        )\n        ds, rs = hpat_func(df.B)\n        gc.collect()\n        self.assertTrue(isinstance(ds, pd.Series) and isinstance(rs, pd.Series))\n        self.assertTrue(ds[0] == \'one\' and ds[2] == \'three\' and rs[0] and not rs[2])\n\n    @skip_numba_jit\n    def test_string_array_bool_getitem(self):\n        def test_impl():\n            A = StringArray([\'ABC\', \'BB\', \'CDEF\'])\n            B = A == \'ABC\'\n            C = A[B]\n            return len(C) == 1 and C[0] == \'ABC\'\n        hpat_func = self.jit(test_impl)\n\n        self.assertEqual(hpat_func(), True)\n\n    @skip_numba_jit\n    def test_string_NA_box(self):\n        # create `example.parquet` file\n        ParquetGenerator.gen_pq_test()\n\n        def test_impl():\n            df = pq.read_table(\'example.parquet\').to_pandas()\n            return df.five\n        hpat_func = self.jit(test_impl)\n\n        # XXX just checking isna() since Pandas uses None in this case\n        # instead of nan for some reason\n        np.testing.assert_array_equal(hpat_func().isna(), test_impl().isna())\n\n    # test utf8 decode\n    @skip_numba_jit\n    def test_decode_empty1(self):\n        def test_impl(S):\n            return S[0]\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([\'\'])\n        self.assertEqual(hpat_func(S), test_impl(S))\n\n    @skip_numba_jit\n    def test_decode_single_ascii_char1(self):\n        def test_impl(S):\n            return S[0]\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([\'A\'])\n        self.assertEqual(hpat_func(S)[0], test_impl(S))\n\n    @skip_numba_jit\n    def test_decode_ascii1(self):\n        def test_impl(S):\n            return S[0]\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([\'Abc12\', \'bcd\', \'345\'])\n        self.assertEqual(hpat_func(S), test_impl(S))\n\n    @unittest.skip(""non ascii decode not implement"")\n    def test_decode_unicode1(self):\n        def test_impl(S):\n            return S[0]\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([""\xc2\xa1Y t\xc3\xba qui\xc3\xa9n te crees?"", ""\xf0\x9f\x90\x8d\xe2\x9a\xa1"", ""\xe5\xa4\xa7\xe5\xa4\x84\xe7\x9d\x80\xe7\x9c\xbc\xef\xbc\x8c\xe5\xb0\x8f\xe5\xa4\x84\xe7\x9d\x80\xe6\x89\x8b\xe3\x80\x82""])\n        self.assertEqual(hpat_func(S), test_impl(S))\n\n    @unittest.skip(""non ascii decode not implement"")\n    def test_decode_unicode2(self):\n        # test strings that start with ascii\n        def test_impl(S):\n            return S[0], S[1], S[2]\n        hpat_func = self.jit(test_impl)\n\n        S = pd.Series([\'abc\xc2\xa1Y t\xc3\xba qui\xc3\xa9n te crees?\',\n                       \'dd2\xf0\x9f\x90\x8d\xe2\x9a\xa1\', \'22 \xe5\xa4\xa7\xe5\xa4\x84\xe7\x9d\x80\xe7\x9c\xbc\xef\xbc\x8c\xe5\xb0\x8f\xe5\xa4\x84\xe7\x9d\x80\xe6\x89\x8b\xe3\x80\x82\', ])\n        self.assertEqual(hpat_func(S), test_impl(S))\n\n    def test_encode_unicode1(self):\n        def test_impl():\n            return pd.Series([\'\xc2\xa1Y t\xc3\xba qui\xc3\xa9n te crees?\',\n                              \'\xf0\x9f\x90\x8d\xe2\x9a\xa1\', \'\xe5\xa4\xa7\xe5\xa4\x84\xe7\x9d\x80\xe7\x9c\xbc\xef\xbc\x8c\xe5\xb0\x8f\xe5\xa4\x84\xe7\x9d\x80\xe6\x89\x8b\xe3\x80\x82\', ])\n        hpat_func = self.jit(test_impl)\n\n        pd.testing.assert_series_equal(hpat_func(), test_impl())\n\n    @unittest.skip(""TODO: explore np array of strings"")\n    def test_box_np_arr_string(self):\n        def test_impl(A):\n            return A[0]\n        hpat_func = self.jit(test_impl)\n\n        A = np.array([\'AA\', \'B\'])\n        self.assertEqual(hpat_func(A), test_impl(A))\n\n    @unittest.skip(""No glob support on windows yet. Segfault on Linux if no files found by pattern"")\n    def test_glob(self):\n        def test_impl():\n            glob.glob(""*py"")\n        hpat_func = self.jit(test_impl)\n\n        self.assertEqual(hpat_func(), test_impl())\n\n    def test_set_string(self):\n        def test_impl():\n            s = sdc.set_ext.init_set_string()\n            s.add(\'ff\')\n            for v in s:\n                pass\n            return v\n        hpat_func = self.jit(test_impl)\n\n        self.assertEqual(hpat_func(), test_impl())\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
sdc/tests/test_utils.py,11,"b'# -*- coding: utf-8 -*-\n# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport string\nimport unittest\n\nimport numba\nimport numpy as np\nimport pandas\n\nimport sdc\nfrom sdc.config import config_inline_overloads, config_use_parallel_overloads\n\n\ntest_global_input_data_unicode_kind4 = [\n    \'\xc2\xa1Y t\xc3\xba qui\xc3\xa9n te crees?\',\n    \'\xf0\x9f\x90\x8d\xe2\x9a\xa1\',\n    \'\xe5\xa4\xa7\xe5\xa4\x84 \xe7\x9d\x80\xe7\x9c\xbc\xef\xbc\x8cc\xe5\xb0\x8f\xe5\xa4\x84\xe7\x9d\x80\xe6\x89\x8bc\xe3\x80\x82\xe5\xa4\xa7\xe5\xa4\xa7c\xe5\xa4\xa7\xe5\xa4\x84\',\n]\n\ntest_global_input_data_unicode_kind1 = [\n    \'ascii\',\n    \'12345\',\n    \'1234567890\',\n]\n\nmin_float64 = np.finfo(\'float64\').min\nmax_float64 = np.finfo(\'float64\').max\n\ntest_global_input_data_float64 = [\n    [1., -1., 0.1, min_float64, max_float64, max_float64, min_float64, -0.1],\n    [1., np.nan, -1., 0., min_float64, max_float64, max_float64, min_float64],\n    [1., np.inf, np.inf, -1., 0., np.inf, np.NINF, np.NINF],\n    [np.nan, np.inf, np.inf, np.nan, np.nan, np.nan, np.NINF, np.NZERO],\n]\n\n\ndef gen_int_df_index(length):\n    """"""Generate random integer index for DataFrame""""""\n    arr = np.arange(length)\n    np.random.seed(0)\n    np.random.shuffle(arr)\n\n    return arr\n\n\ndef gen_df(input_data, with_index=False):\n    """"""Generate DataFrame based on list of data like a [[1, 2, 3], [4, 5, 6]]""""""\n    length = min(len(d) for d in input_data)\n    data = {n: d[:length] for n, d in zip(string.ascii_uppercase, input_data)}\n\n    index = None\n    if with_index:\n        index = gen_int_df_index(length)\n\n    return pandas.DataFrame(data, index=index)\n\n\ndef gen_df_int_cols(input_data, with_index=False):\n    """"""Generate DataFrame based on list of data like a [[1, 2, 3], [4, 5, 6]]""""""\n    length = min(len(d) for d in input_data)\n    data = {n: d[:length] for n, d in enumerate(input_data)}\n\n    index = None\n    if with_index:\n        index = gen_int_df_index(length)\n\n    return pandas.DataFrame(data, index=index)\n\n\ndef count_array_REPs():\n    if sdc.config.config_pipeline_hpat_default:\n        from sdc.distributed import Distribution\n        vals = sdc.distributed.dist_analysis.array_dists.values()\n        return sum([v == Distribution.REP for v in vals])\n    else:\n        return 0\n\n\ndef count_parfor_REPs():\n    if sdc.config.config_pipeline_hpat_default:\n        from sdc.distributed import Distribution\n        vals = sdc.distributed.dist_analysis.parfor_dists.values()\n        return sum([v == Distribution.REP for v in vals])\n    else:\n        return 0\n\n\ndef count_parfor_OneDs():\n    from sdc.distributed import Distribution\n    vals = sdc.distributed.dist_analysis.parfor_dists.values()\n    return sum([v == Distribution.OneD for v in vals])\n\n\ndef count_array_OneDs():\n    from sdc.distributed import Distribution\n    vals = sdc.distributed.dist_analysis.array_dists.values()\n    return sum([v == Distribution.OneD for v in vals])\n\n\ndef count_parfor_OneD_Vars():\n    from sdc.distributed import Distribution\n    vals = sdc.distributed.dist_analysis.parfor_dists.values()\n    return sum([v == Distribution.OneD_Var for v in vals])\n\n\ndef count_array_OneD_Vars():\n    from sdc.distributed import Distribution\n    vals = sdc.distributed.dist_analysis.array_dists.values()\n    return sum([v == Distribution.OneD_Var for v in vals])\n\n\ndef dist_IR_contains(*args):\n    return sum([(s in sdc.distributed.fir_text) for s in args])\n\n\n@sdc.jit\ndef get_rank():\n    return sdc.distributed_api.get_rank()\n\n\n@sdc.jit\ndef get_start_end(n):\n    rank = sdc.distributed_api.get_rank()\n    n_pes = sdc.distributed_api.get_size()\n    start = sdc.distributed_api.get_start(n, n_pes, rank)\n    end = sdc.distributed_api.get_end(n, n_pes, rank)\n    return start, end\n\n\ndef check_numba_version(version):\n    return numba.__version__ == version\n\n\ndef msg_and_func(msg_or_func=None):\n    if msg_or_func is None:\n        # No signature, no function\n        func = None\n        msg = None\n    elif isinstance(msg_or_func, str):\n        # A message is passed\n        func = None\n        msg = msg_or_func\n    else:\n        # A function is passed\n        func = msg_or_func\n        msg = None\n    return msg, func\n\n\ndef skip_numba_jit(msg_or_func=None):\n    msg, func = msg_and_func(msg_or_func)\n    wrapper = unittest.skipUnless(sdc.config.config_pipeline_hpat_default, msg or ""numba pipeline not supported"")\n    if sdc.config.test_expected_failure:\n        wrapper = unittest.expectedFailure\n    # wrapper = lambda f: f  # disable skipping\n    return wrapper(func) if func else wrapper\n\n\ndef skip_sdc_jit(msg_or_func=None):\n    msg, func = msg_and_func(msg_or_func)\n    wrapper = unittest.skipIf(sdc.config.config_pipeline_hpat_default, msg or ""sdc pipeline not supported"")\n    if sdc.config.test_expected_failure:\n        wrapper = unittest.expectedFailure\n    # wrapper = lambda f: f  # disable skipping\n    return wrapper(func) if func else wrapper\n\n\ndef sdc_limitation(func):\n    return unittest.expectedFailure(func)\n\n\ndef skip_parallel(msg_or_func):\n    msg, func = msg_and_func(msg_or_func)\n    wrapper = unittest.skipIf(config_use_parallel_overloads, msg or ""fails in parallel mode"")\n    if sdc.config.test_expected_failure:\n        wrapper = unittest.expectedFailure\n    # wrapper = lambda f: f  # disable skipping\n    return wrapper(func) if func else wrapper\n\n\ndef skip_inline(msg_or_func):\n    msg, func = msg_and_func(msg_or_func)\n    wrapper = unittest.skipIf(config_inline_overloads, msg or ""fails in inline mode"")\n    if sdc.config.test_expected_failure:\n        wrapper = unittest.expectedFailure\n    # wrapper = lambda f: f  # disable skipping\n    return wrapper(func) if func else wrapper\n\n\ndef take_k_elements(k, data, repeat=False, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n    return np.random.choice(np.asarray(data), k, replace=repeat)\n\n\ndef create_series_from_values(size, data_values, index_values=None, name=None, unique=True, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n\n    min_size = min(size, len(data_values))\n    if index_values:\n        min_size = min(min_size, len(index_values))\n    repeat = False if unique and min_size == size else True\n\n    series_data = take_k_elements(size, data_values, repeat)\n    series_index = take_k_elements(size, index_values, repeat) if index_values else None\n\n    return pandas.Series(series_data, series_index, name)\n'"
sdc/utilities/__init__.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n'"
sdc/utilities/prange_utils.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\nimport numba\nimport sdc\n\nfrom typing import NamedTuple\nfrom sdc.utilities.utils import sdc_register_jitable\n\n\nclass Chunk(NamedTuple):\n    start: int\n    stop: int\n\n\n@sdc_register_jitable\ndef get_pool_size():\n    if sdc.config.config_use_parallel_overloads:\n        return numba.config.NUMBA_NUM_THREADS\n\n    return 1\n\n\n@sdc_register_jitable\ndef get_chunks(size, pool_size):\n    chunks = []\n\n    if size < 1 or pool_size < 1:\n        return chunks\n\n    pool_size = min(pool_size, size)\n    chunk_size = size // pool_size\n    overload_size = size % pool_size\n\n    for i in range(pool_size):\n        start = i * chunk_size + min(i, overload_size)\n        stop = (i + 1) * chunk_size + min(i + 1, overload_size)\n        chunks.append(Chunk(start, stop))\n\n    return chunks\n\n\n@sdc_register_jitable\ndef parallel_chunks(size):\n    return get_chunks(size, get_pool_size())\n'"
sdc/utilities/sdc_typing_utils.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\n\n| This file contains SDC utility functions related to typing compilation phase\n\n""""""\n\nimport numpy\nimport numba\nimport sdc\n\nfrom numba import types\nfrom numba.core.errors import TypingError\nfrom numba.np import numpy_support\n\nfrom sdc.str_arr_type import string_array_type\n\n\nclass TypeChecker:\n    """"""\n        Validate object type and raise TypingError if the type is invalid, e.g.:\n            Method nsmallest(). The object n\n             given: bool\n             expected: int\n    """"""\n    msg_template = \'{} The object {}\\n given: {}\\n expected: {}\'\n\n    def __init__(self, func_name):\n        """"""\n        Parameters\n        ----------\n        func_name: :obj:`str`\n            name of the function where types checking\n        """"""\n        self.func_name = func_name\n\n    def raise_exc(self, data, expected_types, name=\'\'):\n        """"""\n        Raise exception with unified message\n        Parameters\n        ----------\n        data: :obj:`any`\n            real type of the data\n        expected_types: :obj:`str`\n            expected types inserting directly to the exception\n        name: :obj:`str`\n            name of the parameter\n        """"""\n        msg = self.msg_template.format(self.func_name, name, data, expected_types)\n        raise TypingError(msg)\n\n    def check(self, data, accepted_type, name=\'\'):\n        """"""\n        Check data type belongs to specified type\n        Parameters\n        ----------\n        data: :obj:`any`\n            real type of the data\n        accepted_type: :obj:`type`\n            accepted type\n        name: :obj:`str`\n            name of the parameter\n        """"""\n        if not isinstance(data, accepted_type):\n            self.raise_exc(data, accepted_type.__name__, name=name)\n\n\ndef kwsparams2list(params):\n    """"""Convert parameters dict to a list of string of a format \'key=value\'""""""\n    return [\'{}={}\'.format(k, v) for k, v in params.items()]\n\n\ndef sigparams2list(param_names, defaults):\n    """"""Creates a list of strings of a format \'key=value\' from parameter names and default values""""""\n    return [(f\'{param}\' if param not in defaults else f\'{param}={defaults[param]}\') for param in param_names]\n\n\ndef has_literal_value(var, value):\n    """"""Used during typing to check that variable var is a Numba literal value equal to value""""""\n\n    if not isinstance(var, types.Literal):\n        return False\n\n    if value is None or isinstance(value, type(bool)):\n        return var.literal_value is value\n    else:\n        return var.literal_value == value\n\n\ndef has_python_value(var, value):\n    """"""Used during typing to check that variable var was resolved as Python type and has specific value""""""\n\n    if not isinstance(var, type(value)):\n        return False\n\n    if value is None or isinstance(value, type(bool)):\n        return var is value\n    else:\n        return var == value\n\n\ndef check_is_numeric_array(type_var):\n    """"""Used during typing to check that type_var is a numeric numpy arrays""""""\n    return check_is_array_of_dtype(type_var, types.Number)\n\n\ndef check_index_is_numeric(ty_series):\n    """"""Used during typing to check that series has numeric index""""""\n    return check_is_numeric_array(ty_series.index)\n\n\ndef check_types_comparable(ty_left, ty_right):\n    """"""Used during typing to check that specified types can be compared""""""\n\n    if hasattr(ty_left, \'dtype\'):\n        ty_left = ty_left.dtype\n\n    if hasattr(ty_right, \'dtype\'):\n        ty_right = ty_right.dtype\n\n    # add the rest of supported types here\n    if isinstance(ty_left, types.Number):\n        return isinstance(ty_right, types.Number)\n    if isinstance(ty_left, types.UnicodeType):\n        return isinstance(ty_right, types.UnicodeType)\n    if isinstance(ty_left, types.Boolean):\n        return isinstance(ty_right, types.Boolean)\n\n    return False\n\n\ndef check_arrays_comparable(ty_left, ty_right):\n    """"""Used during typing to check that underlying arrays of specified types can be compared""""""\n    return ((ty_left == string_array_type and ty_right == string_array_type)\n            or (check_is_numeric_array(ty_left) and check_is_numeric_array(ty_right)))\n\n\ndef check_is_array_of_dtype(type_var, dtype):\n    """"""Used during typing to check that type_var is a numeric numpy array of specific dtype""""""\n    return isinstance(type_var, types.Array) and isinstance(type_var.dtype, dtype)\n\n\ndef find_common_dtype_from_numpy_dtypes(array_types, scalar_types):\n    """"""Used to find common numba dtype for a sequences of numba dtypes each representing some numpy dtype""""""\n    np_array_dtypes = [numpy_support.as_dtype(dtype) for dtype in array_types]\n    np_scalar_dtypes = [numpy_support.as_dtype(dtype) for dtype in scalar_types]\n    np_common_dtype = numpy.find_common_type(np_array_dtypes, np_scalar_dtypes)\n    numba_common_dtype = numpy_support.from_dtype(np_common_dtype)\n\n    return numba_common_dtype\n\n\ndef gen_impl_generator(codegen, impl_name):\n    """"""Generate generator of an implementation""""""\n    def _df_impl_generator(*args, **kwargs):\n        func_text, global_vars = codegen(*args, **kwargs)\n\n        loc_vars = {}\n        exec(func_text, global_vars, loc_vars)\n        _impl = loc_vars[impl_name]\n\n        return _impl\n\n    return _df_impl_generator\n'"
sdc/utilities/utils.py,13,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\nfrom sdc import hstr_ext\nimport llvmlite.binding as ll\nfrom llvmlite import ir as lir\nfrom collections import namedtuple\nimport operator\nimport numba\nfrom numba import types\nfrom numba.core import cgutils\nfrom numba.core import ir_utils, ir\nfrom numba.core.ir_utils import (guard, get_definition, find_callname, require,\n                            add_offset_to_labels, find_topo_order, find_const)\nfrom numba.core.typing import signature\nfrom numba.core.typing.templates import infer_global, AbstractTemplate\nfrom numba.core.imputils import lower_builtin\nfrom numba.extending import overload, intrinsic, lower_cast\nfrom numba.np import numpy_support\nimport numpy as np\nimport sdc\nfrom sdc.str_ext import string_type, list_string_array_type\nfrom sdc.str_arr_ext import string_array_type, num_total_chars, pre_alloc_string_array\nfrom sdc.config import (config_use_parallel_overloads, config_inline_overloads)\nfrom enum import Enum\nimport types as pytypes\nfrom numba.extending import overload, overload_method, overload_attribute\nfrom numba.extending import register_jitable, register_model\n\n\n# int values for types to pass to C code\n# XXX: _hpat_common.h\nclass CTypeEnum(Enum):\n    Int8 = 0\n    UInt8 = 1\n    Int32 = 2\n    UInt32 = 3\n    Int64 = 4\n    UInt64 = 7\n    Float32 = 5\n    Float64 = 6\n    Int16 = 8\n    UInt16 = 9\n\n\n_numba_to_c_type_map = {\n    types.int8: CTypeEnum.Int8.value,\n    types.uint8: CTypeEnum.UInt8.value,\n    types.int32: CTypeEnum.Int32.value,\n    types.uint32: CTypeEnum.UInt32.value,\n    types.int64: CTypeEnum.Int64.value,\n    types.uint64: CTypeEnum.UInt64.value,\n    types.float32: CTypeEnum.Float32.value,\n    types.float64: CTypeEnum.Float64.value,\n    types.NPDatetime(\'ns\'): CTypeEnum.UInt64.value,\n    # XXX: Numpy\'s bool array uses a byte for each value but regular booleans\n    # are not bytes\n    # TODO: handle boolean scalars properly\n    types.bool_: CTypeEnum.UInt8.value,\n    types.int16: CTypeEnum.Int16.value,\n    types.uint16: CTypeEnum.UInt16.value,\n}\n\n\ndef min_dtype_int_val(dtype):\n    numpy_dtype = numpy_support.as_dtype(dtype)\n    return np.iinfo(numpy_dtype).min\n\n\ndef max_dtype_int_val(dtype):\n    numpy_dtype = numpy_support.as_dtype(dtype)\n    return np.iinfo(numpy_dtype).max\n\n\ndef min_dtype_float_val(dtype):\n    numpy_dtype = numpy_support.as_dtype(dtype)\n    return np.finfo(numpy_dtype).min\n\n\ndef max_dtype_float_val(dtype):\n    numpy_dtype = numpy_support.as_dtype(dtype)\n    return np.finfo(numpy_dtype).max\n\n\n# silence Numba error messages for now\n# TODO: customize through @sdc.jit\nnumba.core.errors.error_extras = {\n    \'unsupported_error\': \'\',\n    \'typing\': \'\',\n    \'reportable\': \'\',\n    \'interpreter\': \'\',\n    \'constant_inference\': \'\'}\n\n# sentinel value representing non-constant values\n\n\nclass NotConstant:\n    pass\n\n\nNOT_CONSTANT = NotConstant()\n\nReplaceFunc = namedtuple(""ReplaceFunc"",\n                         [""func"", ""arg_types"", ""args"", ""glbls"", ""pre_nodes""])\n\nnp_alloc_callnames = (\'empty\', \'zeros\', \'ones\', \'full\')\n\n\ndef unliteral_all(args):\n    return tuple(types.unliteral(a) for a in args)\n\n# TODO: move to Numba\n\n\nclass BooleanLiteral(types.Literal, types.Boolean):\n\n    def can_convert_to(self, typingctx, other):\n        # similar to IntegerLiteral\n        conv = typingctx.can_convert(self.literal_type, other)\n        if conv is not None:\n            return max(conv, types.Conversion.promote)\n\n\ntypes.Literal.ctor_map[bool] = BooleanLiteral\n\nregister_model(BooleanLiteral)(numba.extending.models.BooleanModel)\n\n\n@lower_cast(BooleanLiteral, types.Boolean)\ndef literal_bool_cast(context, builder, fromty, toty, val):\n    lit = context.get_constant_generic(\n        builder,\n        fromty.literal_type,\n        fromty.literal_value,\n    )\n    return context.cast(builder, lit, fromty.literal_type, toty)\n\n\ndef get_constant(func_ir, var, default=NOT_CONSTANT):\n    def_node = guard(get_definition, func_ir, var)\n    if def_node is None:\n        return default\n    if isinstance(def_node, ir.Const):\n        return def_node.value\n    # call recursively if variable assignment\n    if isinstance(def_node, ir.Var):\n        return get_constant(func_ir, def_node, default)\n    return default\n\n\ndef inline_new_blocks(func_ir, block, i, callee_blocks, work_list=None):\n    # adopted from inline_closure_call\n    scope = block.scope\n    instr = block.body[i]\n\n    # 1. relabel callee_ir by adding an offset\n    callee_blocks = add_offset_to_labels(callee_blocks, ir_utils._max_label + 1)\n    callee_blocks = ir_utils.simplify_CFG(callee_blocks)\n    max_label = max(callee_blocks.keys())\n    #    reset globals in ir_utils before we use it\n    ir_utils._max_label = max_label\n    topo_order = find_topo_order(callee_blocks)\n\n    # 5. split caller blocks into two\n    new_blocks = []\n    new_block = ir.Block(scope, block.loc)\n    new_block.body = block.body[i + 1:]\n    new_label = ir_utils.next_label()\n    func_ir.blocks[new_label] = new_block\n    new_blocks.append((new_label, new_block))\n    block.body = block.body[:i]\n    min_label = topo_order[0]\n    block.body.append(ir.Jump(min_label, instr.loc))\n\n    # 6. replace Return with assignment to LHS\n    numba.core.inline_closurecall._replace_returns(callee_blocks, instr.target, new_label)\n    #    remove the old definition of instr.target too\n    if (instr.target.name in func_ir._definitions):\n        func_ir._definitions[instr.target.name] = []\n\n    # 7. insert all new blocks, and add back definitions\n    for label in topo_order:\n        # block scope must point to parent\'s\n        block = callee_blocks[label]\n        block.scope = scope\n        numba.core.inline_closurecall._add_definitions(func_ir, block)\n        func_ir.blocks[label] = block\n        new_blocks.append((label, block))\n\n    if work_list is not None:\n        for block in new_blocks:\n            work_list.append(block)\n    return callee_blocks\n\n\ndef is_alloc_call(func_var, call_table):\n    """"""\n    return true if func_var represents an array creation call\n    """"""\n    assert func_var in call_table\n    call_list = call_table[func_var]\n    return ((len(call_list) == 2 and call_list[1] == np\n             and call_list[0] in [\'empty\', \'zeros\', \'ones\', \'full\'])\n            or call_list == [numba.unsafe.ndarray.empty_inferred])\n\n\ndef is_alloc_callname(func_name, mod_name):\n    """"""\n    return true if function represents an array creation call\n    """"""\n    return isinstance(mod_name, str) and ((mod_name == \'numpy\'\n                                           and func_name in np_alloc_callnames)\n                                          or (func_name == \'empty_inferred\'\n                                              and mod_name in (\'numba.extending\', \'numba.unsafe.ndarray\'))\n                                          or (func_name == \'pre_alloc_string_array\'\n                                              and mod_name == \'sdc.str_arr_ext\')\n                                          or (func_name in (\'alloc_str_list\', \'alloc_list_list_str\')\n                                              and mod_name == \'sdc.str_ext\'))\n\n\ndef find_build_tuple(func_ir, var):\n    """"""Check if a variable is constructed via build_tuple\n    and return the sequence or raise GuardException otherwise.\n    """"""\n    # variable or variable name\n    require(isinstance(var, (ir.Var, str)))\n    var_def = get_definition(func_ir, var)\n    require(isinstance(var_def, ir.Expr))\n    require(var_def.op == \'build_tuple\')\n    return var_def.items\n\n\ndef cprint(*s):\n    print(*s)\n\n\n@infer_global(cprint)\nclass CprintInfer(AbstractTemplate):\n    def generic(self, args, kws):\n        assert not kws\n        return signature(types.none, *unliteral_all(args))\n\n\ntyp_to_format = {\n    types.int32: \'d\',\n    types.uint32: \'u\',\n    types.int64: \'lld\',\n    types.uint64: \'llu\',\n    types.float32: \'f\',\n    types.float64: \'lf\',\n}\n\nll.add_symbol(\'print_str\', hstr_ext.print_str)\nll.add_symbol(\'print_char\', hstr_ext.print_char)\n\n\n@lower_builtin(cprint, types.VarArg(types.Any))\ndef cprint_lower(context, builder, sig, args):\n    from sdc.str_ext import string_type, char_type\n\n    for i, val in enumerate(args):\n        typ = sig.args[i]\n        if typ == string_type:\n            fnty = lir.FunctionType(\n                lir.VoidType(), [lir.IntType(8).as_pointer()])\n            fn = builder.module.get_or_insert_function(fnty, name=""print_str"")\n            builder.call(fn, [val])\n            cgutils.printf(builder, "" "")\n            continue\n        if typ == char_type:\n            fnty = lir.FunctionType(\n                lir.VoidType(), [lir.IntType(8)])\n            fn = builder.module.get_or_insert_function(fnty, name=""print_char"")\n            builder.call(fn, [val])\n            cgutils.printf(builder, "" "")\n            continue\n        if isinstance(typ, types.ArrayCTypes):\n            cgutils.printf(builder, ""%p "", val)\n            continue\n        format_str = typ_to_format[typ]\n        cgutils.printf(builder, ""%{} "".format(format_str), val)\n    cgutils.printf(builder, ""\\n"")\n    return context.get_dummy_value()\n\n\ndef print_dist(d):\n    from sdc.distributed_analysis import Distribution\n    if d == Distribution.REP:\n        return ""REP""\n    if d == Distribution.OneD:\n        return ""1D_Block""\n    if d == Distribution.OneD_Var:\n        return ""1D_Block_Var""\n    if d == Distribution.Thread:\n        return ""Multi-thread""\n    if d == Distribution.TwoD:\n        return ""2D_Block""\n\n\ndef distribution_report():\n    import sdc.distributed\n    if sdc.distributed.dist_analysis is None:\n        return\n    print(""Array distributions:"")\n    for arr, dist in sdc.distributed.dist_analysis.array_dists.items():\n        print(""   {0:20} {1}"".format(arr, print_dist(dist)))\n    print(""\\nParfor distributions:"")\n    for p, dist in sdc.distributed.dist_analysis.parfor_dists.items():\n        print(""   {0:<20} {1}"".format(p, print_dist(dist)))\n\n\ndef is_whole_slice(typemap, func_ir, var, accept_stride=False):\n    """""" return True if var can be determined to be a whole slice """"""\n    require(typemap[var.name] == types.slice2_type\n            or (accept_stride and typemap[var.name] == types.slice3_type))\n    call_expr = get_definition(func_ir, var)\n    require(isinstance(call_expr, ir.Expr) and call_expr.op == \'call\')\n    assert (len(call_expr.args) == 2\n            or (accept_stride and len(call_expr.args) == 3))\n    assert find_callname(func_ir, call_expr) == (\'slice\', \'builtins\')\n    arg0_def = get_definition(func_ir, call_expr.args[0])\n    arg1_def = get_definition(func_ir, call_expr.args[1])\n    require(isinstance(arg0_def, ir.Const) and arg0_def.value is None)\n    require(isinstance(arg1_def, ir.Const) and arg1_def.value is None)\n    return True\n\n\ndef is_const_slice(typemap, func_ir, var, accept_stride=False):\n    """""" return True if var can be determined to be a constant size slice """"""\n    require(typemap[var.name] == types.slice2_type\n            or (accept_stride and typemap[var.name] == types.slice3_type))\n    call_expr = get_definition(func_ir, var)\n    require(isinstance(call_expr, ir.Expr) and call_expr.op == \'call\')\n    assert (len(call_expr.args) == 2\n            or (accept_stride and len(call_expr.args) == 3))\n    assert find_callname(func_ir, call_expr) == (\'slice\', \'builtins\')\n    arg0_def = get_definition(func_ir, call_expr.args[0])\n    require(isinstance(arg0_def, ir.Const) and arg0_def.value is None)\n    size_const = find_const(func_ir, call_expr.args[1])\n    require(isinstance(size_const, int))\n    return True\n\n\ndef get_slice_step(typemap, func_ir, var):\n    require(typemap[var.name] == types.slice3_type)\n    call_expr = get_definition(func_ir, var)\n    require(isinstance(call_expr, ir.Expr) and call_expr.op == \'call\')\n    assert len(call_expr.args) == 3\n    return call_expr.args[2]\n\n\ndef is_array(typemap, varname):\n    return (varname in typemap\n            and (is_np_array(typemap, varname)\n                 or typemap[varname] in (string_array_type, list_string_array_type,\n                                         sdc.hiframes.split_impl.string_array_split_view_type)\n                 or isinstance(typemap[varname], sdc.hiframes.pd_series_ext.SeriesType)))\n\n\ndef is_np_array(typemap, varname):\n    return (varname in typemap\n            and isinstance(typemap[varname], types.Array))\n\n\ndef is_array_container(typemap, varname):\n    return (varname in typemap\n            and isinstance(typemap[varname], (types.List, types.Set))\n            and (isinstance(typemap[varname].dtype, types.Array)\n                 or typemap[varname].dtype == string_array_type\n                 or isinstance(typemap[varname].dtype,\n                               sdc.hiframes.pd_series_ext.SeriesType)))\n\n\n# converts an iterable to array, similar to np.array, but can support\n# other things like StringArray\n# TODO: other types like datetime?\ndef to_array(A):\n    return np.array(A)\n\n\n@overload(to_array)\ndef to_array_overload(A):\n    # try regular np.array and return it if it works\n    def to_array_impl(A):\n        return np.array(A)\n    try:\n        numba.njit(to_array_impl).get_call_template((A,), {})\n        return to_array_impl\n    except BaseException:\n        pass  # should be handled elsewhere (e.g. Set)\n\n\ndef empty_like_type(n, arr):\n    return np.empty(n, arr.dtype)\n\n\n@overload(empty_like_type)\ndef empty_like_type_overload(n, arr):\n    if isinstance(arr, sdc.hiframes.pd_categorical_ext.CategoricalArray):\n        from sdc.hiframes.pd_categorical_ext import fix_cat_array_type\n        return lambda n, arr: fix_cat_array_type(np.empty(n, arr.dtype))\n    if isinstance(arr, types.Array):\n        return lambda n, arr: np.empty(n, arr.dtype)\n    if isinstance(arr, types.List) and arr.dtype == string_type:\n        def empty_like_type_str_list(n, arr):\n            return [\'\'] * n\n        return empty_like_type_str_list\n\n    # string array buffer for join\n    assert arr == string_array_type\n\n    def empty_like_type_str_arr(n, arr):\n        # average character heuristic\n        avg_chars = 20  # heuristic\n        if len(arr) != 0:\n            avg_chars = num_total_chars(arr) // len(arr)\n        return pre_alloc_string_array(n, n * avg_chars)\n\n    return empty_like_type_str_arr\n\n\ndef alloc_arr_tup(n, arr_tup, init_vals=()):\n    arrs = []\n    for in_arr in arr_tup:\n        arrs.append(np.empty(n, in_arr.dtype))\n    return tuple(arrs)\n\n\n@overload(alloc_arr_tup)\ndef alloc_arr_tup_overload(n, data, init_vals=()):\n    count = data.count\n\n    allocs = \',\'.join([""empty_like_type(n, data[{}])"".format(i)\n                       for i in range(count)])\n\n    if init_vals is not ():\n        # TODO check for numeric value\n        allocs = \',\'.join([""np.full(n, init_vals[{}], data[{}].dtype)"".format(i, i)\n                           for i in range(count)])\n\n    func_text = ""def f(n, data, init_vals=()):\\n""\n    func_text += ""  return ({}{})\\n"".format(allocs,\n                                            "","" if count == 1 else """")  # single value needs comma to become tuple\n\n    loc_vars = {}\n    exec(func_text, {\'empty_like_type\': empty_like_type, \'np\': np}, loc_vars)\n    alloc_impl = loc_vars[\'f\']\n    return alloc_impl\n\n\n@intrinsic\ndef get_ctypes_ptr(typingctx, ctypes_typ=None):\n    assert isinstance(ctypes_typ, types.ArrayCTypes)\n\n    def codegen(context, builder, sig, args):\n        in_carr, = args\n        ctinfo = context.make_helper(builder, sig.args[0], in_carr)\n        return ctinfo.data\n\n    return types.voidptr(ctypes_typ), codegen\n\n\ndef remove_return_from_block(last_block):\n    # remove const none, cast, return nodes\n    assert isinstance(last_block.body[-1], ir.Return)\n    last_block.body.pop()\n    assert (isinstance(last_block.body[-1], ir.Assign)\n            and isinstance(last_block.body[-1].value, ir.Expr)\n            and last_block.body[-1].value.op == \'cast\')\n    last_block.body.pop()\n    if (isinstance(last_block.body[-1], ir.Assign)\n            and isinstance(last_block.body[-1].value, ir.Const)\n            and last_block.body[-1].value.value is None):\n        last_block.body.pop()\n\n\ndef include_new_blocks(blocks, new_blocks, label, new_body, remove_non_return=True, work_list=None, func_ir=None):\n    inner_blocks = add_offset_to_labels(new_blocks, ir_utils._max_label + 1)\n    blocks.update(inner_blocks)\n    ir_utils._max_label = max(blocks.keys())\n    scope = blocks[label].scope\n    loc = blocks[label].loc\n    inner_topo_order = find_topo_order(inner_blocks)\n    inner_first_label = inner_topo_order[0]\n    inner_last_label = inner_topo_order[-1]\n    if remove_non_return:\n        remove_return_from_block(inner_blocks[inner_last_label])\n    new_body.append(ir.Jump(inner_first_label, loc))\n    blocks[label].body = new_body\n    label = ir_utils.next_label()\n    blocks[label] = ir.Block(scope, loc)\n    if remove_non_return:\n        inner_blocks[inner_last_label].body.append(ir.Jump(label, loc))\n    # new_body.clear()\n    if work_list is not None:\n        topo_order = find_topo_order(inner_blocks)\n        for _label in topo_order:\n            block = inner_blocks[_label]\n            block.scope = scope\n            numba.core.inline_closurecall._add_definitions(func_ir, block)\n            work_list.append((_label, block))\n    return label\n\n\ndef find_str_const(func_ir, var):\n    """"""Check if a variable can be inferred as a string constant, and return\n    the constant value, or raise GuardException otherwise.\n    """"""\n    require(isinstance(var, ir.Var))\n    var_def = get_definition(func_ir, var)\n    if isinstance(var_def, ir.Const):\n        val = var_def.value\n        require(isinstance(val, str))\n        return val\n\n    # only add supported (s1+s2), TODO: extend to other expressions\n    require(isinstance(var_def, ir.Expr) and var_def.op == \'binop\'\n            and var_def.fn == operator.add)\n    arg1 = find_str_const(func_ir, var_def.lhs)\n    arg2 = find_str_const(func_ir, var_def.rhs)\n    return arg1 + arg2\n\n\ndef gen_getitem(out_var, in_var, ind, calltypes, nodes):\n    loc = out_var.loc\n    getitem = ir.Expr.static_getitem(in_var, ind, None, loc)\n    calltypes[getitem] = None\n    nodes.append(ir.Assign(getitem, out_var, loc))\n\n\ndef sanitize_varname(varname):\n    return varname.replace(\'$\', \'_\').replace(\'.\', \'_\')\n\n\ndef is_call_assign(stmt):\n    return (isinstance(stmt, ir.Assign)\n            and isinstance(stmt.value, ir.Expr)\n            and stmt.value.op == \'call\')\n\n\ndef is_call(expr):\n    return (isinstance(expr, ir.Expr)\n            and expr.op == \'call\')\n\n\ndef is_var_assign(inst):\n    return isinstance(inst, ir.Assign) and isinstance(inst.value, ir.Var)\n\n\ndef is_assign(inst):\n    return isinstance(inst, ir.Assign)\n\n\ndef dump_node_list(node_list):\n    for n in node_list:\n        print(""   "", n)\n\n\ndef debug_prints():\n    return numba.config.DEBUG_ARRAY_OPT == 1\n\n\ndef update_globals(func, glbls):\n    if isinstance(func, pytypes.FunctionType):\n        func.__globals__.update(glbls)\n\n\ndef update_jit_options(jit_options, parallel, config_flag):\n    jit_options = jit_options.copy()\n\n    if parallel is not None:\n        if \'parallel\' not in jit_options:\n            jit_options.update({\'parallel\': parallel})\n        else:\n            raise ValueError(\'Either jit_options ""parallel"" or parallel parameter could be specified at the same time\')\n\n    if \'parallel\' not in jit_options:\n        jit_options = jit_options.copy()\n        jit_options.update({\'parallel\': config_use_parallel_overloads})\n\n    return jit_options\n\n\ndef sdc_overload(func, jit_options={}, parallel=None, strict=True, inline=None):\n    jit_options = update_jit_options(jit_options, parallel, config_use_parallel_overloads)\n\n    if inline is None:\n        inline = \'always\' if config_inline_overloads else \'never\'\n\n    return overload(func, jit_options=jit_options, strict=strict, inline=inline)\n\n\ndef patched_register_jitable(*args, **kwargs):\n    """"""\n    register_jitable patched according to this:\n    https://github.com/numba/numba/issues/5142#issuecomment-579704346\n    """"""\n    def wrap(fn):\n        # It is just a wrapper for @overload\n        inline = kwargs.pop(\'inline\', \'never\')\n        @overload(fn, jit_options=kwargs, inline=inline, strict=False)\n        def ov_wrap(*args, **kwargs):\n            return fn\n        return fn\n\n    if kwargs:\n        return wrap\n    else:\n        return wrap(*args)\n\n\ndef sdc_register_jitable(*args, **kwargs):\n    updated_kwargs = kwargs.copy()\n    updated_kwargs[\'parallel\'] = updated_kwargs.get(\'parallel\', config_use_parallel_overloads)\n    updated_kwargs[\'inline\'] = updated_kwargs.get(\'inline\', \'always\' if config_inline_overloads else \'never\')\n\n    def wrap(fn):\n        return patched_register_jitable(**updated_kwargs)(fn)\n\n    if kwargs:\n        return wrap\n    else:\n        return wrap(*args)\n\n\ndef sdc_overload_method(typ, name, jit_options={}, parallel=None, strict=True, inline=None):\n    jit_options = update_jit_options(jit_options, parallel, config_use_parallel_overloads)\n\n    if inline is None:\n        inline = \'always\' if config_inline_overloads else \'never\'\n\n    return overload_method(typ, name, jit_options=jit_options, strict=strict, inline=inline)\n\n\ndef sdc_overload_attribute(typ, name, jit_options={}, parallel=None, strict=True, inline=None):\n    jit_options = update_jit_options(jit_options, parallel, config_use_parallel_overloads)\n\n    if inline is None:\n        inline = \'always\' if config_inline_overloads else \'never\'\n\n    return overload_attribute(typ, name, jit_options=jit_options, strict=strict, inline=inline)\n'"
docs/source/buildscripts/__init__.py,0,"b'# -*- coding: utf-8 -*-\n# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n'"
docs/source/buildscripts/apiref_generator.py,0,"b'# -*- coding: utf-8 -*-\n# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas\nfrom sdc_object_utils import init_pandas_structure, init_sdc_structure, init_pandas_sdc_dict\nfrom sdc_object_utils import get_sdc_object_by_pandas_name, get_obj\nfrom sdc_object_utils import get_class_methods, get_class_attributes, get_fully_qualified_name\nfrom sdc_doc_utils import is_sdc_user_guide_header, get_indent, reindent, get_short_description\nfrom sdc_doc_utils import split_in_sections, get_docstring, create_heading_str, cut_sdc_dev_guide\nimport os\n\n\nAPIREF_TEMPLATE_FNAMES = [\n    \'./_templates/_api_ref.pandas.io_templ.rst\',\n    \'./_templates/_api_ref.pandas.series_templ.rst\',\n    \'./_templates/_api_ref.pandas.dataframe_templ.rst\',\n    \'./_templates/_api_ref.pandas.window_templ.rst\',\n    \'./_templates/_api_ref.pandas.groupby_templ.rst\',\n]\n\n\nAPIREF_REL_PATH = \'./_api_ref/\'\n\n\ndef reformat(text):\n    """"""\n    Wrapper function that includes series of transformations of the ``text`` to fix Pandas docstrings which\n    cause Sphinx to generate warnings.\n\n    :param text: Original text with warnings\n    :return: Modified text that fixes warnings\n    """"""\n    text = reformat_replace_star_list_with_dash_list(text)  # Must be called before :func:`reformat_asterisks`\n    text = reformat_asterisks(text)  # Fix for * and ** symbols\n    text = reformat_explicit_markup(text)  # Fix for explicit markup without a blank line\n    text = reformat_bullet_list(text)  # Fix bullet list indentation issues\n    text = reformat_remove_unresolved_references(text)  # Fix unresolved references after removal of References sections\n    return reformat_remove_multiple_blank_lines(text)\n\n\ndef reformat_remove_unresolved_references(text):\n    """"""\n    Fixes unresolved references after removing References sections.\n\n    Searches for pattern [numeric]_ in the text and removes it. Intel SDC references do not use [numeric]_ pattern\n\n    :param text: Original text\n    :return: Reformatted text\n    """"""\n    new_text = \'\'\n    while len(text) > 0:\n        idx = text.find(\'[\')\n\n        if idx >= 0:\n            new_text += text[0:idx]\n            idx1 = idx+1\n            while idx1 < len(text) and text[idx1].isnumeric():\n                # Iterating through numeric characters\n                idx1 += 1\n\n            if idx1+1 < len(text):\n                # There are at least two more symbols after numeric ones in the text\n                if text[idx1:idx1+2] != \']_\':\n                    new_text += text[idx:idx1+2]\n\n                if idx1+2 < len(text):\n                    text = text[idx1+2:]  # Remove reference\n                else:\n                    text = \'\'\n            else:\n                new_text += text[idx:]\n                text = \'\'\n        else:\n            new_text += text\n            text = \'\'\n    return new_text\n\n\ndef reformat_replace_star_list_with_dash_list(text):\n    """"""\n    Replaces bullet lists starting with `*` with the lists starting with `-`\n\n    :param text: Original text\n    :return: New text without `*` bullet lists\n    """"""\n    lines = text.split(\'\\n\')\n    new_text = \'\'\n    for line in lines:\n        if line.strip().startswith(\'* \'):\n            line = line.replace(\'* \', \'- \', 1)\n\n        new_text += line + \'\\n\'\n\n    return new_text\n\n\ndef reformat_remove_multiple_blank_lines(text):\n    """"""\n    Removes redundant blank lines\n\n    After multiple passes of the text reformatting there could be redundant blank lines between sections.\n    This pass is intended for removal of consecutive blank lines and keeping just one blank line between sections\n\n    :param text: Original text\n    :return: Text with removed redundant blank lines\n    """"""\n\n    len_changed = True\n\n    while len_changed:\n        new_text = text.replace(\'\\n\\n\\n\', \'\\n\\n\')\n        len_changed = len(new_text) < len(text)\n        text = new_text\n\n    return new_text\n\n\ndef reformat_bullet_list(text):\n    lines = text.split(\'\\n\')\n    new_text = \'\'\n    bullet_indent = -1\n    while len(lines) > 0:\n        line = lines[0]\n        if line.strip().startswith(\'- \'):\n            # Here if met new bullet\n            bullet_indent = get_indent(line)  # We need to know indent to identify multi-line bullets\n            new_text += line + \'\\n\'\n        elif line.strip() == \'\':\n            bullet_indent = -1  # We finished parsing multi-line bullet\n            new_text += \'\\n\'\n        else:\n            if bullet_indent >= 0:\n                # Here if we\'re parsing multi-line bullet\n                new_text += reindent(line, bullet_indent + 4) + \'\\n\'\n            else:\n                # Here if we are not in bullet list\n                new_text += line + \'\\n\'\n        lines.pop(0)\n\n    return new_text\n\n\ndef reformat_explicit_markup(text):\n    """"""\n    Fixes Pandas docstring warning about explicit markup not followed by a blank line.\n\n    Parses the text and finds ``\'.. \'`` strings by adding a blank line next after.\n\n    :param text: Original text with warnings\n    :return: Modified text that fixes warnings\n    """"""\n    lines = text.split(\'\\n\')\n    new_text = \'\'\n    while len(lines) > 0:\n        line = lines[0]\n\n        if line.strip().startswith(\'.. versionchanged\') or line.strip().startswith(\'.. versionadded\') or \\\n                line.strip().startswith(\'.. deprecated\') or line.strip().startswith(\'.. _\'):\n            new_text += line + \'\\n\'\n            # Here if found explicit markup\n            if len(lines) > 1:\n                # Here if there is at least one line after explicit markup\n                if lines[1].strip != \'\':\n                    # Here if there is no empty line after explicit markup. Add new line then\n                    new_text += \'\\n\'\n                lines.pop(0)\n        elif line.strip().startswith(\'.. note\') or line.strip().startswith(\'.. warning\'):\n            new_text += line.strip() + \'\\n\'\n            if len(lines) > 1:\n                # Here if there is at least one line after explicit markup\n                if lines[1].strip() == \'\':\n                    # Here if there is empty line after explicit markup. Remove new line then\n                    lines.pop(1)\n        elif line.strip().startswith(\'.. [\'):\n            new_text += \'\\n\'  # Remove references\n        else:\n            new_text += line + \'\\n\'\n        lines.pop(0)\n    return new_text\n\n\ndef reformat_asterisks(text):\n    """"""\n    Fixes Pandas docstring warning about using * and ** without ending \\* and \\*\\*.\n\n    The fix distinguishes single * and ** by adding \\\\ to them. No changes for *italic* and **bold** usages.\n\n    :param text: Original text with warnings\n    :return: Modified text that fixes warnings\n    """"""\n    lines = text.split(\'\\n\')\n    new_text = \'\'\n    for line in lines:\n        idx = 0  # Starting parsing position within the ``line``\n        while idx < len(line):  # Parsing until end of string reached\n            idx1 = line.find(\'*\', idx)\n            if idx1 >= idx:\n                # There is at least one asterisk in the line\n                idx2 = line.find(\'*\', idx1+1)\n\n                if idx2 == -1:\n                    # Only one single asterisk in the line - Reformat to `\\*`\n                    line = line.replace(\'*\', \'\\\\*\')\n                    idx = len(line)  # Parsed the line. Go to another line\n                elif idx2 == idx1+1:\n                    # First double asterisk met in the line\n                    idx2 = line.find(\'**\', idx1+2)\n                    if idx2 == -1:\n                        # Only one double asterisk in the line Reformat to `\\*\\*`. But there could be more asterisks\n                        line = line.replace(\'**\', \'\\\\*\\\\*\')\n                        idx = idx1+4\n                    else:\n                        # At least two double asterisks in the line\n                        idx = idx2+2  # Deal with remaining asterisks on the next ``while`` loop iteration\n                else:\n                    # There is another asterisk apart from the first asterisk\n                    if idx2+1 < len(line):\n                        # There is at least one more symbol in the line after second asterisk\n                        if line[idx2+1] == \'*\':\n                            # Situation when double asterisk is met after the first single asterisk - Reformat to `\\*`\n                            line = line.replace(\'*\', \'\\\\*\', 1)  # Replace the first single asterisk\n                            idx = idx2  # Handle double asterisk on the next ``while`` iteration\n                        else:\n                            # Two asterisks met in the line to italize characters between them\n                            idx = idx2+1\n                    else:\n                        # Second asterisk was the last symbol in the line\n                        idx = len(line)\n            else:\n                # No asterisks in the line\n                idx = len(line)\n        new_text += line + \'\\n\'\n\n    return new_text\n\n\ndef reformat_pandas_params(title, text):\n    """"""\n    Re-formats ``text`` written in NumPy style documenting Parameters, Returns, Raises sections into\n    explicit `:<param>:` style.\n\n    Algorithm searches for the pattern:\n    `<alpha_numeric_value> : <text>`\n         `<text continued with indent>`\n         `<text continued with indent>`\n    Reformat to the following:\n    `:<alpha_numeric_value>:`\n         `<text>`\n         `<text continued with indent>`\n         `<text continued with indent>`\n\n\n    :param title:\n    :param text:\n    :return: Reformatted text\n    """"""\n\n    # Internal function. Returns correct markup for :param <param>:, :return:, and :raises <exception>:\n    def _get_param_text(title, param):\n        title = title.strip()\n        if title == \'Parameters\':\n            return \':param \' + param + \':\'\n        elif title == \'Return\' or title == \'Returns\':\n            return \':return:\'\n        elif title == \'Raises\':\n            return \':raises:\'\n\n    # Internal function. Returns correct markup for Parameters section\n    def _reformat_parameters(title, text):\n        lines = text.split(\'\\n\')\n        new_text = \'\'\n\n        if len(lines) == 0:\n            return new_text\n\n        indent = get_indent(text)\n        param = \'\'\n        description = \'\'\n        while len(lines) > 0:\n            line = lines[0]\n            line = line.strip()\n            idx = line.find(\' : \')\n            if idx >= 0 & line[0:idx].isalnum() and line[0:idx].isidentifier():\n                # Check if previous parameter existed. If so, need to add it to reformatted text\n                if param != \'\':\n                    new_text += _get_param_text(title, param) + \'\\n\' + reindent(description, indent+4) + \'\\n\'\n\n                # Found parameter. Extract the description (can be multi-line)\n                param = line[0:idx]\n                description = line[idx+3:] + \'\\n\'\n                lines.pop(0)\n            else:\n                # There is no parameter description starting in this line.\n                # Check if it is continuation of parameter description from previous lines\n                if param != \'\':\n                    # It is continuation of multi-line parameter description\n                    description += reindent(line, indent+4) + \'\\n\'\n                else:\n                    # This is not the description of parameter. Copy as is\n                    new_text += reindent(line, indent) + \'\\n\'\n                lines.pop(0)\n\n        if param != \'\' and description != \'\':\n            new_text += _get_param_text(title, param) + \'\\n\' + reindent(description, indent+4) + \'\\n\'\n        return new_text\n\n    # Internal function. Returns correct markup for Raises section\n    def _reformat_raises(title, text):\n        lines = text.split(\'\\n\')\n        new_text = \'\'\n\n        if len(lines) == 0:\n            return new_text\n\n        indent = get_indent(text)\n        param = \'\'\n        description = \'\'\n        while len(lines) > 0:\n            line = lines[0]\n            line = line.strip()\n\n            # Check if it is continuation of parameter description from previous lines\n            if param != \'\':\n                # It is continuation of multi-line parameter description\n                description += reindent(line, indent + 8) + \'\\n\'\n            else:\n                # This is the first line of ``raises`` description\n                param = _get_param_text(title, \'\') + \'\\n\' + reindent(line, indent + 4)\n                new_text += param + \'\\n\'\n            lines.pop(0)\n\n        if param != \'\' and description != \'\':\n            new_text += reindent(description, indent + 8) + \'\\n\'\n        return new_text + \'\\n\'\n\n    # Internal function. Returns correct markup for Returns section\n    def _reformat_returns(title, text):\n        lines = text.split(\'\\n\')\n        new_text = \'\'\n\n        if len(lines) == 0:\n            return new_text\n\n        indent = get_indent(text)\n        param = \'\'\n        description = \'\'\n        while len(lines) > 0:\n            line = lines[0]\n            line = line.strip()\n\n            # Check if it is continuation of parameter description from previous lines\n            if param != \'\':\n                # It is continuation of multi-line parameter description\n                description += reindent(line, indent + 4) + \'\\n\'\n            else:\n                # This is the first line of ``return`` description\n                param = _get_param_text(title, \'\') + \' \' + line\n                new_text += reindent(param, indent) + \'\\n\'\n            lines.pop(0)\n\n        if param != \'\' and description != \'\':\n            new_text += reindent(description, indent + 4) + \'\\n\'\n        return new_text + \'\\n\'\n\n    if title.strip() == \'Parameters\':\n        return _reformat_parameters(title, text)\n    elif title.strip() == \'Returns\' or title.strip() == \'Return\':\n        return _reformat_returns(title, text)\n    elif title.strip() == \'Raises\':\n        return _reformat_raises(title, text)\n    else:\n        return text\n\n\ndef generate_simple_object_doc(pandas_name, short_doc_flag=False, doc_from_pandas_flag=True, add_sdc_sections=True,\n                               unsupported_warning=True, reformat_pandas=True):\n    """"""\n    Generates documentation for Pandas object obj according to flags.\n\n    For complex objects such as modules and classes the function does not go to sub-objects,\n    i.e. to class attributes and sub-modules of the module.\n\n    :param pandas_name: Pandas object for which documentation to be generated.\n    :param short_doc_flag: Flag to indicate that only short description for the object is needed.\n    :param doc_from_pandas_flag: Flag to indicate that the documentation must be taken from Pandas docstring.\n           This docstring can be extended with Intel SDC specific sections. These are See Also, Examples,\n           Notes, Warning, Limitations, etc. if ``add_sdc_sections`` flag is set.\n    :param add_sdc_sections: Flag to indicate that extra sections of the documentation need to be taken from Intel SDC.\n           If ``doc_from_pandas_flag==False`` then the description section is taken from Intel SDC too. Otherwise\n           Intel SDC description section will be cut and Pandas API description will be used instead.\n    :param unsupported_warning: Flag, if ``True`` includes warning message if corresponding Intel SDC object is not\n           found. This indicates that given SDC method is unsupported.\n    :param reformat_pandas: Flag, if ``True`` re-formats Parameters section to :param: style. Needed to work around\n           Sphinx generator issues for Pandas Parameters section written in NumPy style\n    :return: Generated docstring.\n    """"""\n\n    doc = \'\'\n    pandas_obj = get_obj(pandas_name)\n    if pandas_obj is None:\n        return doc  # Empty documentation for no-object\n\n    # Here if additional sections from Intel SDC object needs to be added to pandas_obj docstring\n    sdc_obj = get_sdc_object_by_pandas_name(pandas_name)\n\n    if pandas_obj == 0:\n        # there is no Pandas documentation, documentation is fully generated from SDC docstring\n        doc_from_pandas_flag = False\n        if short_doc_flag:\n            doc = get_short_description(sdc_obj, sdc_header_flag=True)\n            return reformat(doc)\n\n    if doc_from_pandas_flag:  # Check if documentation needs to be generated from Pandas docstring\n        if short_doc_flag:  # Check if only short description is needed\n            doc = get_short_description(pandas_obj)  # Short description is requested\n        else:\n            # Exclude Examples, Notes, See Also, References sections\n            sections = split_in_sections(reindent(get_docstring(pandas_obj), 0))\n            while len(sections) > 0:\n                title, text = sections[0]\n                if title.strip() == \'\':  # Description sections\n                    doc += text + \'\\n\\n\'\n                    sections.pop(0)\n                elif title.strip() == \'Examples\':  # Exclude Examples section\n                    sections.pop(0)\n                elif title.strip() == \'Notes\':  # Exclude Notes section (may be too specific to Pandas)\n                    sections.pop(0)\n                elif title.strip().lower() == \'see also\':  # Exclude See Also section (may be too specific to Pandas)\n                    sections.pop(0)\n                elif title.strip() == \'References\':  # Exclude References section (may be too specific to Pandas)\n                    sections.pop(0)\n                elif title.strip() == \'Parameters\' or title.strip() == \'Raises\' or title.strip() == \'Return\' or \\\n                        title.strip() == \'Returns\':\n                    if reformat_pandas:\n                        doc += reformat_pandas_params(title, text)\n                        sections.pop(0)\n                    else:\n                        doc += create_heading_str(title) + \'\\n\\n\' + text + \'\\n\\n\'\n                        sections.pop(0)\n                else:\n                    doc += create_heading_str(title) + \'\\n\\n\' + text + \'\\n\\n\'\n                    sections.pop(0)\n\n    if not add_sdc_sections:\n        if reformat_pandas:\n            return reformat(doc)\n        else:\n            return doc\n\n    if sdc_obj is None:\n        if unsupported_warning:\n            if reformat_pandas:\n                doc = reformat(doc)\n\n            if short_doc_flag:\n                return doc + \' **Unsupported by Intel SDC**.\'\n            else:\n                return doc + \'\\n\\n.. warning::\\n    This feature is currently unsupported \' \\\n                                      \'by Intel Scalable Dataframe Compiler\\n\\n\'\n\n    if not short_doc_flag:\n        sdc_doc = get_docstring(sdc_obj)\n        sdc_doc = cut_sdc_dev_guide(sdc_doc)\n\n        # Cut description section from ``sdc_doc``\n        if is_sdc_user_guide_header(sdc_doc[0]):  # First section is SDC User Guide header\n            sdc_doc.pop(0)\n\n        if doc_from_pandas_flag:\n            # Ignore description from Intel SDC, keep Pandas description only\n            while len(sdc_doc) > 0:\n                title, text = sdc_doc[0]\n                if title.strip() != \'\':\n                    break\n                sdc_doc.pop(0)\n\n        indent = get_indent(doc)\n        for title, text in sdc_doc:\n            if title.strip() == \'\':\n                if pandas_obj == 0:\n                    doc += reindent(text, indent) + \'\\n\'\n                else:\n                    doc += \'\\n\' + reindent(text, indent)\n            else:\n                doc += \'\\n\' + reindent(create_heading_str(title), indent) + \'\\n\' + \\\n                       reindent(text, indent) + \'\\n\'\n\n    return reformat(doc)\n\n\ndef get_rst_filename(obj_name):\n    """"""\n    Returns rst file name by respective object name.\n\n    :param obj_name: String, object name for which file name is constructed\n    :return: String, rst file name for the object being documented\n    """"""\n    file_name = obj_name.replace(\'.\', \'/\')\n    file_name = APIREF_REL_PATH + file_name + \'.rst\'\n    return file_name\n\n\ndef open_file_for_write(file_name):\n    """"""\n    Opens file ``filename`` for writing. If necessary, creates file directories on the path.\n\n    :param file_name: Absolute or relative path that includes file name being created.\n    :return: File descriptor created.\n    """"""\n    directory = os.path.dirname(file_name)\n\n    if len(directory) > 0 and not os.path.exists(directory):\n        os.makedirs(directory)\n\n    return open(file_name, \'w\', encoding=\'utf-8\')\n\n\ndef write_rst(file_name, docstring):\n    """"""\n    Writes ``docstring`` into the file ``file_name``.\n\n    :param file_name: String, name of the file including relative or absolute path\n    :param docstring: String, docstring to be written in the file\n    """"""\n    file = open_file_for_write(file_name)\n    file.write(docstring)\n    file.close()\n\n\ndef write_simple_object_rst_file(pandas_name, short_doc_flag=False, doc_from_pandas_flag=True, add_sdc_sections=True):\n    """"""\n    Writes Pandas object ``pandas_name`` (e.g. \'pandas.Series.at\') into rst file.\n\n    RST file has the name derived from ``pandas_name`` (e.g. \'pandas.Series.at.rst\'). Additional flags are used\n    to control look and feel of the resulting content of the file. See :func:`generate_simple_object_doc` function\n    for details about these flags.\n\n    :param pandas_name: String, the name of Pandas object\n    :param short_doc_flag: Flag, if ``True``, write short description of the object only\n    :param doc_from_pandas_flag: Flag, if ``True``, derive the description from Pandas docstring for the object.\n    :param add_sdc_sections: Flag, if ``True``, extend the docstring with respective Intel SDC sections (if any)\n    """"""\n    doc = generate_simple_object_doc(pandas_name, short_doc_flag, doc_from_pandas_flag, add_sdc_sections)\n    if doc is None or doc == \'\':\n        return\n\n    fname = get_rst_filename(pandas_name)\n    write_rst(fname, doc)\n\n\ndef parse_templ_rst(fname_templ):\n    """"""\n    Parses input template rst file and outputs the final rst file\n    Template document must have the following structure:\n\n    Heading or subheading\n    *********************\n\n    Any text (if any)\n\n    Another heading or subheading\n    -----------------------------\n\n    Any text (if any)\n\n    .. currentmodule:: <module name>\n\n    .. sdc_toctree\n    <api1>\n    <api2>\n    <api3>\n    ...\n\n    Any text (if any)\n\n    Any text (if any)\n\n    Another heading or subheading\n    -----------------------------\n\n    Any text (if any)\n    ...\n\n    :param fname_templ:\n    """"""\n    path, fname_out = os.path.split(fname_templ)\n    fname_out = fname_out.replace(\'_templ\', \'\')\n    fname_out = fname_out.replace(\'_\', \'\', 1)\n    fout = open_file_for_write(APIREF_REL_PATH + fname_out)\n    with open(fname_templ, \'r\', encoding=\'utf-8\') as fin:\n        doc = fin.readlines()\n\n        while len(doc) > 0:\n            # Parsing lines until ``.. sdc_toctree`` section is met\n            while len(doc) > 0 and not doc[0].startswith(\'.. sdc_toctree\'):\n                line = doc[0]\n                if line.startswith(\'.. currentmodule::\'):\n                    current_module_name = line[19:].strip()\n                fout.write(line)\n                doc.pop(0)\n\n            if len(doc) == 0:\n                return\n\n            doc.pop(0)  # Skipping ``.. sdc_toctree``\n\n            # Parsing the list of APIs\n            while len(doc) > 0 and doc[0].strip() != \'\':\n                line = doc[0]\n                indent = get_indent(line)\n                line = line.strip()\n                full_name = current_module_name + \'.\' + line\n                short_description = generate_simple_object_doc(full_name, short_doc_flag=True).strip()\n                new_line = reindent(\':ref:`\', indent) + line + \' <\' + full_name + \'>`\\n\' + \\\n                    reindent(short_description, indent+4) + \'\\n\'\n                fout.write(new_line)\n                doc.pop(0)\n\n                full_description = generate_simple_object_doc(full_name, short_doc_flag=False)\n                f = open_file_for_write(APIREF_REL_PATH + full_name + \'.rst\')\n                f.write(\'.. _\' + full_name + \':\\n\\n:orphan:\\n\\n\')\n                f.write(create_heading_str(full_name, \'*\') + \'\\n\\n\')\n                f.write(full_description)\n                f.close()\n\n            if len(doc) == 0:\n                return\n\n    fout.close()\n\n\ndef write_class_rst_files(cls, short_doc_flag=False, doc_from_pandas_flag=True, add_sdc_sections=True):\n    # Currenlty not in use. Should be used for auto-documenting class methods and attributes.\n\n    for method_name, method_object in get_class_methods(cls):\n        write_simple_object_rst_file(get_fully_qualified_name(cls) + \'.\' + method_name,\n                                     short_doc_flag, doc_from_pandas_flag, add_sdc_sections)\n\n    for attr_name, attr_object in get_class_attributes(cls):\n        write_simple_object_rst_file(get_fully_qualified_name(cls) + \'.\' + attr_name,\n                                     short_doc_flag, doc_from_pandas_flag, add_sdc_sections)\n\n\ndef generate_api_reference():\n    """"""\n    Master function for API Reference generation.\n\n    This function initializes all required data structures, and parses required templates for\n    Final RST files generation that looks and feels like Pandas API Reference.\n    """"""\n    init_pandas_structure()\n    init_sdc_structure()\n    init_pandas_sdc_dict()\n\n    for templ_fname in APIREF_TEMPLATE_FNAMES:\n        parse_templ_rst(templ_fname)\n\n\nif __name__ == ""__main__"":\n    generate_api_reference()\n'"
docs/source/buildscripts/examples_generator.py,0,"b'# -*- coding: utf-8 -*-\n# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\nfrom pathlib import Path\n\nfrom sdc_object_utils import get_sdc_object_by_pandas_name, init_sdc_structure\nfrom sdc_doc_utils import get_docstring, reindent, split_in_sections\nfrom apiref_generator import (APIREF_TEMPLATE_FNAMES, reformat)\n\n\nEXAMPLES_REL_PATH = Path(\'.\') / \'_examples\'\n\n\ndef get_obj_examples(pandas_name):\n    """"""\n    Get list of examples for Pandas object.\n\n    :param pandas_name: Pandas object for which documentation to be generated.\n    :return: Generated docstring.\n    """"""\n    sdc_obj = get_sdc_object_by_pandas_name(pandas_name)\n    sdc_doc = get_docstring(sdc_obj)\n    sections = split_in_sections(reindent(sdc_doc, 0))\n    sections_as_dict = {title.strip(): text for title, text in sections}\n    example_section = sections_as_dict.get(\'Examples\')\n    if not example_section:\n        return None\n\n    examples = []\n    section_names = [\'literalinclude\', \'command-output\']\n    for subsection in example_section.strip().split(\'\\n\\n\'):\n        subsection = subsection.strip()\n        if any(subsection.startswith(f\'.. {name}\') for name in section_names):\n            # remove a directory level from path to examples\n            examples.append(subsection.replace(\' ../\', \' \'))\n\n    return reformat(\'\\n\\n\'.join(examples))\n\n\ndef get_tmpl_examples(fname_templ):\n    """"""Get all examples based on input template rst file""""""\n    tmpl_examples = []\n    with open(fname_templ, encoding=\'utf-8\') as fin:\n        doc = fin.readlines()\n\n        while len(doc) > 0:\n            # Parsing lines until ``.. sdc_toctree`` section is met\n            while len(doc) > 0 and not doc[0].startswith(\'.. sdc_toctree\'):\n                line = doc[0]\n                if line.startswith(\'.. currentmodule::\'):\n                    current_module_name = line[19:].strip()\n                doc.pop(0)\n\n            if len(doc) == 0:\n                break\n\n            doc.pop(0)  # Skipping ``.. sdc_toctree``\n\n            # Parsing the list of APIs\n            while len(doc) > 0 and doc[0].strip() != \'\':\n                line = doc[0]\n                line = line.strip()\n                full_name = current_module_name + \'.\' + line\n                doc.pop(0)\n\n                obj_examples = get_obj_examples(full_name)\n                if obj_examples:\n                    tmpl_examples.append(obj_examples)\n\n            if len(doc) == 0:\n                break\n\n    return tmpl_examples\n\n\ndef generate_examples():\n    """"""\n    Master function for examples list generation.\n\n    This function initializes SDC data structures, and parses required templates for\n    Final RST file generation that lists all the examples.\n    """"""\n    init_sdc_structure()\n\n    all_examples = []\n    for templ_fname in APIREF_TEMPLATE_FNAMES:\n        all_examples += get_tmpl_examples(templ_fname)\n\n    if not EXAMPLES_REL_PATH.exists():\n        EXAMPLES_REL_PATH.mkdir(parents=True, exist_ok=True)\n\n    examples_rst_path = EXAMPLES_REL_PATH / \'examples.rst\'\n    with examples_rst_path.open(\'w\', encoding=\'utf-8\') as fd:\n        for examples in all_examples:\n            fd.write(examples + \'\\n\')\n\n\nif __name__ == ""__main__"":\n    generate_examples()\n'"
docs/source/buildscripts/module_info.py,0,"b'# -*- coding: utf-8 -*-\n# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\nfrom inspect import getmembers, ismodule, isclass, isfunction\nimport logging\nimport sys\n\n\n# -- Debug logging --------------------------------------------------------------------------------------------------\nENABLE_LOGGING = False\n\n\n# Logging information about attribute parsing\ndef _attribute_logging(s):\n    if ENABLE_LOGGING:\n        logging.debug(\'[ATTRIBUTE]\' + s)\n    return\n\n\n# Logging information about method parsing\ndef _method_logging(s):\n    if ENABLE_LOGGING:\n        logging.debug(\'[METHOD]\' + s)\n    return\n\n\n# Logging information about function parsing\ndef _function_logging(s):\n    if ENABLE_LOGGING:\n        logging.debug(\'[FUNCTION]\' + s)\n    return\n\n\n# Logging information about class parsing\ndef _class_logging(s):\n    if ENABLE_LOGGING:\n        logging.debug(\'[CLASS]\' + s)\n    return\n\n\n# Logging information about module parsing\ndef _module_logging(s):\n    if ENABLE_LOGGING:\n        logging.debug(\'[MODULE]\' + s)\n    return\n\n\n# -- Returns all classes and respective methods of the module -------------------------------------------------------\ndef get_submodules_of(module, inspected, module_list, skip_module_test, skip_class_test,\n                      skip_method_test, skip_attribute_test, skip_function_test):\n\n    # Returns True if the mod module will not be included in API Reference\n    def _skip_module(mod):\n        mod_name = mod.__name__  # Get new submodule name\n        sk_mod = False\n\n        if mod in inspected:  # Ignore already traversed modules\n            sk_mod = True\n            _module_logging(\'`\' + mod_name + \'` already traversed. Ignoring\')\n            return sk_mod\n\n        if \'._\' in mod_name or mod_name.startswith(\'_\'):  # Ignore internal module\n            sk_mod = True\n            _module_logging(\'`\' + mod_name + \'` is internal (starts with _). Ignoring\')\n            return sk_mod\n\n        if skip_module_test(mod, mod_name):\n            sk_mod = True\n            return sk_mod\n\n        return sk_mod\n\n    # Returns True if the cls class will not be included in API Reference\n    def _skip_class(cls):\n        sk_class = False\n        class_name = repr(cls)[8:-2]  # Get full class name\n\n        if \'._\' in class_name:  # We are interested only in public classes\n            sk_class = True\n            _class_logging(\'`\' + class_name + \'` is internal. Ignoring\')\n            return sk_class\n\n        if skip_class_test(cls, class_name):\n            sk_class = True\n            return sk_class\n\n        return sk_class\n\n    # Returns True if the method method_name will not be included in API Reference\n    def _skip_method(cls, method_name):\n        sk_method = False\n\n        if method_name.startswith(\'_\'):  # Ignore internal methods\n            sk_method = True\n            _method_logging(\'`\' + method_name + \'` is internal (starts with __). Ignoring\')\n            return sk_method\n\n        if skip_method_test(cls, method_name):\n            sk_method = True\n            return sk_method\n\n        return sk_method\n\n    # Returns True if the method method_name will not be included in API Reference\n    def _skip_function(function, function_name):\n        sk_function = False\n\n        if function_name.startswith(\'_\'):  # Ignore internal function\n            sk_function = True\n            _function_logging(\'`\' + function_name + \'` is internal (starts with __). Ignoring\')\n            return sk_function\n\n        if skip_function_test(function, function_name):\n            sk_function = True\n            return sk_function\n\n        return sk_function\n\n    # Returns True if the attribute attr_name will not be included in API Reference\n    def _skip_attribute(cls, attr_name):\n        sk_attr = False\n\n        if attr_name.startswith(\'_\'):  # Ignore internal methods\n            sk_attr = True\n            _attribute_logging(\'`\' + attr_name + \'` is internal (starts with __). Ignoring\')\n            return sk_attr\n\n        if skip_attribute_test(cls, attr_name):\n            sk_attr = True\n            return sk_attr\n\n        return sk_attr\n\n    # Creates the list of methods for the class\n    def _generate_class_methods(cls):\n        meths = [func for func in dir(cls) if callable(getattr(cls, func)) and not _skip_method(cls, func)]\n        for meth in meths:\n            _method_logging(\'Adding method `\' + meth + \'` to the list\')\n        return meths\n\n    # Creates the list of class\'s attributes\n    def _generate_class_attributes(cls):\n        attrs = [func for func in dir(cls) if not callable(getattr(cls, func)) and not _skip_attribute(cls, func)]\n        for att in attrs:\n            _attribute_logging(\'Adding attribute `\' + att + \'` to the list\')\n        return attrs\n\n    # -- get_classes_of() implementation begins\n    if _skip_module(module):\n        return\n\n    inspected.add(module)  # Add module to the set of traversed modules\n    module_name = module.__name__\n    module_list.append({\'module_name\': module_name, \'module_object\': module, \'classes\': []})\n\n    _module_logging(\'********************** Inspecting module `\' + module_name + \'`\')\n\n    class_list = []\n    module_list[-1][\'classes\'] = class_list\n    function_list = []\n    module_list[-1][\'functions\'] = function_list\n\n    # Traverses the mod module classes and submodules\n    for (name, obj) in getmembers(module):  # Iterate through members of the submodule\n        if isclass(obj):  # We are interested in objects, which are classes\n            if not _skip_class(obj):\n                _class_logging(\'********************** Inspecting class `\' + name + \'`\')\n                methods = _generate_class_methods(obj)  # Inspect methods of the class of interest only\n                attributes = _generate_class_attributes(obj)  # Inspect attributes of the class of interest only\n                class_list.append({\'class_name\': name, \'class_object\': obj, \'class_methods\': methods,\n                                   \'class_attributes\': attributes})\n\n        if isfunction(obj):  # We are interested in objects, which are functions\n            if not _skip_function(obj, name):\n                function_list.append({\'function_name\': name, \'function_object\': obj})\n\n        if ismodule(obj):\n            if not _skip_module(obj):\n                get_submodules_of(obj, inspected, module_list, skip_module_test, skip_class_test,\n                                  skip_method_test, skip_attribute_test, skip_function_test)\n\n    return\n\n\n# -- Returns all classes and respective methods of the module -------------------------------------------------------\ndef print_modules_classes_methods_attributes(modules):\n    for the_module in modules:  # modules is the list, each element represents dictionary characterizing the sub-module\n        print(the_module[\'module_name\'])\n        print(\'  FUNCTIONS:\')\n        for the_function in the_module[\'functions\']:\n            print(\'  - \' + the_function[\'function_name\'])\n\n        print(\'  CLASSES:\')\n        for the_class in the_module[\'classes\']:\n            print(\'  - \' + the_class[\'class_name\'])\n            print(\'    METHODS:\')\n            for the_method in the_class[\'class_methods\']:\n                print(\'      \' + the_method)\n            print(\'    ATTRIBUTES:\')\n            for the_attribute in the_class[\'class_attributes\']:\n                print(\'      \' + the_attribute)\n    return\n\n\n# -- Trimming docstring  --------------------------------------------------------------------------------------------\ndef trim(docstring):\n    # Copyright 2015: Mirantis Inc.\n    # All Rights Reserved.\n    #\n    #    Licensed under the Apache License, Version 2.0 (the ""License""); you may\n    #    not use this file except in compliance with the License. You may obtain\n    #    a copy of the License at\n    #\n    #         http://www.apache.org/licenses/LICENSE-2.0\n    #\n    #    Unless required by applicable law or agreed to in writing, software\n    #    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT\n    #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n    #    License for the specific language governing permissions and limitations\n    #    under the License.\n\n    """"""trim function from PEP-257""""""\n    if not docstring:\n        return """"\n    # Convert tabs to spaces (following the normal Python rules)\n    # and split into a list of lines:\n    lines = docstring.expandtabs().splitlines()\n    # Determine minimum indentation (first line doesn\'t count):\n    indent = sys.maxsize\n    for line in lines[1:]:\n        stripped = line.lstrip()\n        if stripped:\n            indent = min(indent, len(line) - len(stripped))\n    # Remove indentation (first line is special):\n    trimmed = [lines[0].strip()]\n    if indent < sys.maxsize:\n        for line in lines[1:]:\n            trimmed.append(line[indent:].rstrip())\n    # Strip off trailing and leading blank lines:\n    while trimmed and not trimmed[-1]:\n        trimmed.pop()\n    while trimmed and not trimmed[0]:\n        trimmed.pop(0)\n\n    # Current code/unittests expects a line return at\n    # end of multiline docstrings\n    # workaround expected behavior from unittests\n    if ""\\n"" in docstring:\n        trimmed.append("""")\n\n    # Return a single string:\n    return ""\\n"".join(trimmed)\n\n\n# -- String formatting ----------------------------------------------------------------------------------------------\ndef reindent(string):\n    # Copyright 2015: Mirantis Inc.\n    # All Rights Reserved.\n    #\n    #    Licensed under the Apache License, Version 2.0 (the ""License""); you may\n    #    not use this file except in compliance with the License. You may obtain\n    #    a copy of the License at\n    #\n    #         http://www.apache.org/licenses/LICENSE-2.0\n    #\n    #    Unless required by applicable law or agreed to in writing, software\n    #    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT\n    #    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n    #    License for the specific language governing permissions and limitations\n    #    under the License.\n\n    return ""\\n"".join(l.strip() for l in string.strip().split(""\\n""))\n\n\n# -- These symbols can be used to underline section title -----------------------------------------------------------\nUNDERLINE_SYMBOLS = [\'~\', \'#\', \'@\', \'^\', \'*\', \'-\', \'_\', \'+\', \'=\']\n\n\n# -- Split section into section title and remaining text ------------------------------------------------------------\ndef split_title(section):\n    def _is_section_title(title_line, underscore_line):\n        n = len(title_line)\n        for c in UNDERLINE_SYMBOLS:\n            s = c * n\n            if underscore_line.startswith(s):\n                return True\n\n        return False\n\n    if section.startswith(\'\\n\'):\n        section = section.replace(\'\\n\', \'\', 1)\n\n    lines = section.split(\'\\n\', 2)\n    if len(lines) > 2:\n        # Only sections with number of lines>2 can start with a title\n        if _is_section_title(lines[0].strip(), lines[1].strip()):\n            return lines[0], lines[2]\n        else:\n            return \'\', section\n    else:\n        return \'\', section\n\n\n# -- Parse docstring by forming the list of sections, where each section is dictionary with title and text ----------\ndef split_in_sections(doc, sdc_header_section_flag=False):\n    sections = doc.split(\'\\n\\n\')\n    titled_sections = []\n\n    # For SDC API Reference documentation the topmost section gives Pandas API name\n    if sdc_header_section_flag:\n        section = sections[0]\n        title, text = split_title(section)\n        titled_sections.append({\'title\': title, \'text\': text})\n        sections.pop(0)\n\n    # Special processing for short and long description sections, if any\n    section = sections[0]\n    title, text = split_title(section)\n    while title.strip() == \'\':\n        titled_sections.append({\'title\': title, \'text\': text})\n        sections.pop(0)\n        if len(sections) > 0:\n            section = sections[0]\n            title, text = split_title(section)\n        else:\n            break\n\n    # Other sections. Merge those which are just separated by blank lines\n    for i in range(len(sections)):\n        section = sections[i]\n        title, text = split_title(section)\n        if title.strip() == \'\':\n            titled_sections[-1][\'text\'] += \'\\n\\n\' + text\n        else:\n            titled_sections.append({\'title\': title, \'text\': text})\n\n    return titled_sections\n\n\ndef get_function_doc(func, sdc_header_flag=False):\n    doc = func.__doc__\n\n    if doc is None:\n        doc = \'\'\n\n    titled_sections = split_in_sections(doc, sdc_header_flag)\n    return titled_sections\n\n\ndef get_function_short_description(func, sdc_header_flag=False):\n    titled_sections = get_function_doc(func, sdc_header_flag)\n    if sdc_header_flag: # Ignore the first section\n        titled_sections.pop(0)\n    short_description = titled_sections[0][\'text\']\n\n    # Make it single line in case it is multi-line\n    lines = short_description.split(\'\\n\')\n    lines = [s.strip()+\' \' for s in lines]\n    short_description = \'\'.join(lines)\n\n    return short_description\n\n\ndef create_header_str(s, underlying_symbol=\'*\'):\n    n = len(s)\n    return s + \'\\n\' + underlying_symbol*n\n\n\ndef get_function(func_name, modules):\n    """"""\n    Searches for the function func_name in the modules list. Name can or cannot be given fully qualified\n\n    :param func_name: string, the function name being searched\n    :param modules: the list of modules created by :func:`get_submodules_of`\n    :return: function object or None\n    """"""\n\n    # Check if fully qualified name given\n    if func_name.find(\'.\') != -1:\n        split_name = func_name.rsplit(\'.\', 1)\n        func_name = split_name[-1]\n        module_name = split_name[-2]\n\n        the_module = next((e for e in modules if e[\'module_name\'] == module_name), None)\n        try:\n            if the_module:\n                return getattr(the_module[\'module_object\'], func_name)\n            else:\n                return None\n        except AttributeError:\n            return None\n    else:\n        for the_module in modules:\n            for func_dict in the_module[\'functions\']:\n                if func_name == func_dict[\'function_name\']:\n                    return func_dict[\'function_object\']\n\n    return None\n\ndef get_method_attr(name, modules):\n    """"""\n    Searches for the method/attribute name in the modules list. Name is fully qualified\n\n    :param name: string, the method/attribute being searched\n    :param modules: the list of modules created by :func:`get_submodules_of`\n    :return: method/attribute object or None\n    """"""\n    split_name = name.rsplit(\'.\', 2)\n    name = split_name[-1]\n    class_name = split_name[-2]\n    module_name = split_name[-3]\n\n    the_module = next((e for e in modules if e[\'module_name\'] == module_name), None)\n    the_class = next((e for e in the_module[\'classes\'] if e[\'class_name\'] == class_name), None)\n    try:\n        return getattr(the_class[\'class_object\'], name)\n    except AttributeError:\n        return None\n'"
docs/source/buildscripts/pandas_info.py,0,"b'# -*- coding: utf-8 -*-\n# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n\nimport pandas\nimport logging\nfrom datetime import datetime\nfrom module_info import get_submodules_of, print_modules_classes_methods_attributes, ENABLE_LOGGING\n\n# -- Debug logging --------------------------------------------------------------------------------------------------\nlog_file_name = \'../build/pandas_info.log\'\n\n# -- Submodules, classes, and methods to be excluded from API Reference ---------------------------------------------\nexclude_modules = [\n    \'pandas.compat\',  # This is PRIVATE submodule\n    \'pandas.util\',    # This is PRIVATE submodule\n    \'pandas.api.extensions\',  # This is extension for library developers extending Pandas. Not current interest to SDC\n    \'pandas.testing\',   # Utility functions for testing. Not a priority for SDC\n    \'pandas.plotting\',  # Plotting functions. Not a priority for compiling with SDC\n    \'pandas.errors\',  # Error handling functionality. Not a priority for SDC\n    \'pandas.api.types\',  # Not a priority for SDC\n    \'pandas.io.formats.style\',  # Helps to style dataframes with HTML and CSS. Not a priority for SDC\n    \'pandas.arrays\',  # Array extensions for Numpy. We do not explicitly cover in SDC documentation now\n    \'pandas.tseries\',  # SDC does not yet support Time Series objects\n    \'pandas.core.dtypes.dtypes\',\n]\n\nexclude_classes = [\n]\n\nexclude_methods = [\n]\n\nexclude_attributes = [\n]\n\nexclude_functions = [\n]\n\n\n# -- Implements custom skip functions for the parser ----------------------------------------------------------------\ndef _skip_pandas_module(mod, mod_name):\n    for excl_mname in exclude_modules:\n        if mod_name.startswith(excl_mname):\n            return True\n    return not mod_name.startswith(\'pandas\')\n\n\ndef _skip_pandas_class(cls, cls_name):\n    return cls_name in exclude_classes\n\n\ndef _skip_pandas_method(cls, method_name):\n    # Exclude the method if in the exclude_methods list\n    if method_name in exclude_methods:  # Explicit exclusion of the method\n        return True\n\n    #  Exclude the method without docstring\n    try:\n        doc = getattr(cls, method_name).__doc__\n        return len(doc) < 1\n    except AttributeError:\n        return True\n    except TypeError:\n        return True\n\n\ndef _skip_pandas_function(func, function_name):\n    # Exclude the function if in the exclude_functions list\n    if function_name in exclude_functions:  # Explicit exclusion of the method\n        return True\n\n    #  Exclude the function without docstring\n    try:\n        doc = func.__doc__\n        return len(doc) < 1\n    except AttributeError:\n        return True\n    except TypeError:\n        return True\n\n\ndef _skip_pandas_attribute(cls, attr_name):\n    # Exclude the attribute if in the exclude_methods list\n    if attr_name in exclude_attributes:  # Explicit exclusion of the attribute\n        return True\n\n    #  Exclude the attribute without docstring\n    try:\n        doc = getattr(cls, attr_name).__doc__\n        return len(doc) < 1\n    except AttributeError:\n        return True\n    except TypeError:\n        return True\n\n\ndef get_pandas_modules():\n    inspected_modules = set()\n    modules = []\n    get_submodules_of(pandas, inspected_modules, modules, _skip_pandas_module, _skip_pandas_class,\n                      _skip_pandas_method, _skip_pandas_attribute, _skip_pandas_function)\n    return modules\n\n\ndef init_pandas_logging():\n    if ENABLE_LOGGING:\n        logging.basicConfig(filename=log_file_name, level=logging.DEBUG)\n        logging.debug(\'****************** STARTING THE LOG *************************\')\n        logging.debug(datetime.now().strftime(""%d/%m/%Y %H:%M:%S""))\n\n\nif __name__ == ""__main__"":\n    # Initialize logging\n    init_pandas_logging()\n\n    # Execute parser for Pandas\n    modules = get_pandas_modules()\n\n    # You may uncomment this line in case you want to print out generated methods and attributes\n    print_modules_classes_methods_attributes(modules)\n'"
docs/source/buildscripts/sdc2pd_name.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport os\nimport glob\nimport sdc  # TODO: Rename hpat module name to sdc\n\n# *****************************************************************************************************\n# ***                                     PARSER CONFIGURATION                                      ***\n# *****************************************************************************************************\n\n# Exclude these *.py files from parsing that changes SDC internal function names to Pandas-like\nexclude_files_list = [\n    \'__init__.py\',\n    \'hpat_pandas_dataframe_types.py\',\n    \'hpat_pandas_seriesgroupby_types.py\',\n]\n\n\n# This dictionary is used to substitute SDC internal lower class name with respective Pandas mixed-case name\nCLASSNAME_SUBSTITUTOR = {\n    \'dataframe\': \'DataFrame\',\n    \'series\': \'Series\',\n    \'seriesgroupby\': \'SeriesGroupBy\',\n    \'timestamp\': \'Timestamp\',\n    \'timedelta\': \'Timedelta\',\n    \'period\': \'Period\',\n    \'interval\': \'Interval\',\n    \'index\': \'Index\',\n    \'rangeindex\': \'RangeIndex\',\n    \'categoricalindex\': \'CategoricalIndex\',\n    \'intervalindex\': \'IntervalIndex\',\n    \'multiindex\': \'MultiIndex\',\n    \'datetimeindex\': \'DatetimeIndex\',\n    \'timedeltaindex\': \'TimedeltaIndex\',\n    \'periodindex\': \'PeriodIndex\',\n    \'dateoffset\': \'DateOffset\',\n    \'businessday\': \'BusinessDay\',\n    \'businesshour\': \'BusinessHour\',\n    \'custombusinessday\': \'CustomBusinessDay\',\n    \'custombusinesshour\': \'CustomBusinessHour\',\n    \'monthoffset\': \'MonthOffset\',\n    \'monthend\': \'MonthEnd\',\n    \'monthbegin\': \'MonthBegin\',\n    \'businessmonthend\': \'BusinessMonthEnd\',\n    \'businessmonthbegin\': \'BusinessMonthBegin\',\n    \'custombusinessmonthend\': \'CustomBusinessMonthEnd\',\n    \'custombusinessmonthbegin\': \'CustomBusinessMonthBegin\',\n    \'semimonthoffset\': \'SemiMonthOffset\',\n    \'semimonthend\': \'SemiMonthEnd\',\n    \'semimonthbegin\': \'SemiMonthBegin\',\n    \'week\': \'Week\',\n    \'weekofmonth\': \'WeekOfMonth\',\n    \'lastweekofmonth\': \'LastWeekOfMonth\',\n    \'quarteroffset\': \'QuarterOffset\',\n    \'bquarterend\': \'BQuarterEnd\',\n    \'bquarterbegin\': \'BQuarterBegin\',\n    \'quarterend\': \'QuarterEnd\',\n    \'quarterbegin\': \'QuarterBegin\',\n    \'yearoffset\': \'YearOffset\',\n    \'byearend\': \'BYearEnd\',\n    \'byearbegin\': \'BYearBegin\',\n    \'yearend\': \'YearEnd\',\n    \'yearbegin\': \'YearBegin\',\n    \'fy5253\': \'FY5253\',\n    \'fy5253quarter\': \'FY5253Quarter\',\n    \'easter\': \'Easter\',\n    \'tick\': \'Tick\',\n    \'day\': \'Day\',\n    \'hour\': \'Hour\',\n    \'minute\': \'Minute\',\n    \'second\': \'Second\',\n    \'milli\': \'Milli\',\n    \'micro\': \'Micro\',\n    \'nano\': \'Nano\',\n    \'bday\': \'BDay\',\n    \'bmonthend\': \'BMonthEnd\',\n    \'bmonthbegin\': \'BMonthBegin\',\n    \'cbmonthend\': \'CBMonthEnd\',\n    \'cbmonthbegin\': \'CBMonthBegin\',\n    \'cday\': \'CDay\',\n    \'rolling\': \'Rolling\',\n    \'expanding\': \'Expanding\',\n    \'ewm\': \'EWM\',\n    \'groupby\': \'GroupBy\',\n    \'dataframegroupby\': \'DataFrameGroupBy\',\n    \'resampler\': \'Resampler\',\n}\n\n\n# This is main parsing functions that changes the content of the SDC internal names to Pandas-like\n# Note that input fname is the absolute path to the file being parsed\n# The parser assumes SDC filename has the structure:\n#    sdc_pandas_<classname>_functions\n# The parser assumes for a given file of above structure internal names follow the naming scheme:\n#    sdc_pandas_<classname>_<function name>\ndef parse_file(fname):\n\n    # Constructing the SDC function basename\n    basename = os.path.basename(fname)  # Extracting file name from the full path\n    print(\'Parsing \' + basename + \'...\')\n    split_basename = basename.split(\'_\')  # Splitting to get the file name structure\n    func_name_def_sdc_base = \'def sdc_\' + split_basename[1] + \\\n                    \'_\' + split_basename[2] + \'_\'  # Construct the function basename\n    func_name_def_hpat_base = \'def hpat_\' + split_basename[1] + \\\n                    \'_\' + split_basename[2] + \'_\'  # Construct the function basename\n    pd_func_name_def_base = \'def \' + split_basename[1] + \'.\' + CLASSNAME_SUBSTITUTOR[split_basename[2]] + \'.\'\n\n    # Loading file content\n    with open(fname, \'r\') as fn:\n        content = fn.read()\n\n        content = content.replace(func_name_def_sdc_base, pd_func_name_def_base)\n        content = content.replace(func_name_def_hpat_base, pd_func_name_def_base)\n        print(content)\n\n\n#  Get path to SDC module installed\nsdc_path = os.path.dirname(sdc.__file__)  # TODO: Change hpat to sdc\nsdc_datatypes_path = os.path.join(sdc_path, ""datatypes"")\nsdc_datatypes_pathname_mask = os.path.join(sdc_datatypes_path, ""*.py"")\n\n#  Parse all Python files from the SDC installation directory\nprint(\'Parsing *.py files in \' + sdc_datatypes_path + \':\')\n\npyfiles_list = glob.glob(sdc_datatypes_pathname_mask)  # Get all *.py file names in the SDC package dir\n\n# Exclude files from exclude_files list\nexclude_files_list = [os.path.join(sdc_datatypes_path, fname) for fname in exclude_files_list]  # Abs. path name\n\npyfiles_list = [fname for fname in pyfiles_list if fname not in exclude_files_list] # File list with exclusions\n\n# ************************************************ PARSING SDC FILES ***********************************\nfor fname in pyfiles_list:\n    parse_file(fname)\n'"
docs/source/buildscripts/sdc_build_doc.py,0,"b'# -*- coding: utf-8 -*-\n# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport os\nimport sys\nfrom setuptools import Command\n\n\n# Sphinx User\'s Documentation Build\n\nclass SDCBuildDoc(Command):\n    description = \'Builds Intel SDC documentation\'\n    user_options = [\n        (\'format\', \'f\', \'Documentation output format (html, pdf)\'),\n        (\'doctype\', \'t\', \'Documentation type (user, dev)\'),\n    ]\n\n    @staticmethod\n    def _remove_cwd_from_syspath():\n        # Remove current working directory from PYTHONPATH to avoid importing SDC from sources vs.\n        # from installed SDC package\n        cwd = [\'\', \'.\', \'./\', os.getcwd()]\n        sys.path = [p for p in sys.path if p not in cwd]\n\n    def initialize_options(self):\n        self.format = \'html\'\n        self.doctype = \'user\'\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        self.sdc_build_doc_command.finalize_options()\n        self.sdc_build_doc_command.run()\n\n    def __init__(self, dist):\n        super(SDCBuildDoc, self).__init__(dist)\n        self.format = \'html\'\n        self.doctype = \'user\'\n        try:\n            from sphinx.setup_command import BuildDoc\n        except ImportError:\n            raise ImportError(\'Cannot import Sphinx. \'\n                              \'Sphinx is the expected dependency for Intel SDC documentation build\')\n\n        self._remove_cwd_from_syspath()\n        self.sdc_build_doc_command = BuildDoc(dist)\n        self.sdc_build_doc_command.initialize_options()\n\n\n# Sphinx Developer\'s Documentation Build\n\n#class build_devdoc(build.build):\n#    description = ""Build developer\'s documentation""\n#\n#    def run(self):\n#        spawn([\'rm\', \'-rf\', \'docs/_builddev\'])\n#        spawn([\'sphinx-build\', \'-b\', \'html\', \'-d\', \'docs/_builddev/docstrees\',\n#               \'-j1\', \'docs/devsource\', \'-t\', \'developer\', \'docs/_builddev/html\'])\n'"
docs/source/buildscripts/sdc_doc_utils.py,0,"b'# -*- coding: utf-8 -*-\n# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nUNDERLINE_CHARS = [\'-\', \'`\', \':\', \'~\', \'^\', \'_\', \'*\', \'+\', \'#\', \'<\', \'>\']  # Characters that can underline title\n\nSDC_USR_GUIDE_HEADING_STR = \'Intel Scalable Dataframe Compiler User Guide\'\n\nSDC_USER_GUIDE_PANDAS_STR = \'Pandas API:\'\n\nSDC_DEV_GUIDE_HEADING_STR = \'Intel Scalable Dataframe Compiler Developer Guide\'\n\n\ndef get_indent(text):\n    """"""\n    Returns indentation for a given ``text``.\n\n    :param text: String, can be multi-line. Only first non-empty line is used to determine the indentation\n    :return: Indentation (the number of whitespace characters)\n    """"""\n    lines = text.split(\'\\n\')\n    while len(lines) > 0 and lines[0] == \'\':\n        lines.pop(0)\n\n    if len(lines) == 0:\n        return 0  # Text was empty, indentation for empty text is 0\n\n    n_stripped = len(lines[0].lstrip())  # Length of the string after stripping whitespaces on the left\n    return len(lines[0]) - n_stripped\n\n\ndef reindent(old_text, new_indent):\n    """"""\n    Perform re-indentation of the text ``old_text`` with new indent ``new_indent``.\n\n    :param old_text: Multi-line string for which re-indentation is performed\n    :param new_indent: New indent\n    :return: New multi-line text\n    """"""\n\n    if old_text == \'\':\n        return \' \'*new_indent\n\n    old_indent = get_indent(old_text)\n    lines = old_text.split(\'\\n\')\n    new_text = \'\'\n    for line in lines:\n        if line.strip() == \'\':\n            new_text += \'\\n\'\n        else:\n            line = line[old_indent:]\n            new_text += \' \'*new_indent + line + \'\\n\'\n\n    # If ``old_text`` has no ``\'\\n\'`` in the end, remove it too from the ``new_text``\n    if old_text[-1] != \'\\n\':\n        new_text = new_text[:-1]\n\n    return new_text\n\n\ndef create_heading_str(title, underlying_symbol=\'-\'):\n    """"""\n    Creates heading string for a given ``title``. Second line under title is decorated with ``underlying_symbol``\n\n    Heading is created taking into account of ``title`` indentation.\n\n    :param title:\n    :param underlying_symbol:\n    :return: resulting heading string\n    """"""\n    indent = get_indent(title)\n    n = len(title.strip())\n    return title + \'\\n\' + \' \'*indent + underlying_symbol*n\n\n\ndef get_docstring(obj):\n    """"""\n    Returns docstring for a given object or empty string if no-object is provided or there is no docstring for it.\n\n    :param obj: Object for which the docstring to be provided\n    :return: Docstring\n    """"""\n    if obj is None:\n        return \'\'\n\n    doc = obj.__doc__\n    if doc is None:\n        return \'\'\n    else:\n        return doc\n\n\ndef is_section_title(line, underline):\n    """"""\n    Checks whether line and consecutive underline form valid section title.\n\n    .. note::\n        Function expects leading and trailing whitespaces removed for both strings prior to the call.\n\n    :param line: String, title text\n    :param underline: String, underlying characters\n    :return: True if line and underline form valid section title\n    """"""\n\n    if line is None:\n        return False\n\n    if underline is None:\n        return False\n\n    if line == \'\':\n        return False\n\n    if underline == \'\':\n        return False\n\n    n = len(line)\n    for c in UNDERLINE_CHARS:\n        s = c * n\n        if underline == s:\n            return True\n\n    return False\n\n\ndef is_sdc_user_guide_header(sdc_header):\n    """"""\n    Checks whether a given title-text tuple forms valid Intel SDC header for User Guide.\n\n    The header is expected to be 4 lines long, where the first three lines are of the form:\n        Intel Scalable Dataframe Compiler User Guide\n        ********************************************\n        Pandas API: <pandas API name>\n    The fourth line must be empty\n\n    :param sdc_header: Tuple (title, text)\n    :return: True if sdc_header forms valid Intel SDC User Guide docstring header\n    """"""\n    title, text = sdc_header\n    return title.strip() == SDC_USR_GUIDE_HEADING_STR and text.strip().startswith(SDC_USER_GUIDE_PANDAS_STR)\n\n\ndef is_sdc_dev_guide_header(sdc_header):\n    """"""\n    Checks whether a given title-text tuple forms valid Intel SDC header for Developer Guide.\n\n    The header is expected to be 3 lines long, where the first two lines are of the form:\n        Intel Scalable Dataframe Compiler Developer Guide\n        *************************************************\n    The third line must be empty\n\n    :param sdc_header: Tuple (title, text)\n    :return: True if sdc_header forms valid Intel SDC Developer Guide docstring header\n    """"""\n    title, text = sdc_header\n    return title.strip() == SDC_DEV_GUIDE_HEADING_STR\n\n\ndef extract_pandas_name_from(text):\n    """"""\n    Extracts Pandas API from ``text``.\n\n    This function is used in conjunction with :func:`split_title`, which returns the tuple (title, text).\n    The ``title`` must contain valid Intel SDC header. The ``text`` is expected to be in the form\n    ``Pandas API: *fully qualified Pandas name*``\n\n    :param text:\n    :return: Pandas API name as a string\n    """"""\n    line = text.strip().split(\'\\n\', 1)[0]  # Pandas API is in the first line. Ignore whitespaces\n    return line.replace(SDC_USER_GUIDE_PANDAS_STR, \'\').strip()  # Name begins right after ``Pandas API:``\n\n\ndef split_title(section):\n    """"""\n    Split section into title and remaining text.\n\n    :param section: String, documented section\n    :return: Tuple (title, text)\n    """"""\n\n    if section is None:\n        return \'\', \'\'\n\n    section = section.lstrip(\'\\n\')  # Remove leading empty lines\n\n    lines = section.split(\'\\n\', 2)\n    if len(lines) > 1:\n        # Only sections with number of lines >= 2 can be a title\n        if is_section_title(lines[0].strip(), lines[1].strip()):\n            if len(lines) > 2:\n                return lines[0], lines[2]  # First line is title, second is underline, remaining is text\n            else:\n                return lines[0], \'\'  # First line is title, second line is underline, but the text is empty string\n        else:\n            return \'\', section  # First two lines do not form valid heading\n    else:\n        return \'\', section  # When section is less than 3 lines we consider it having no title\n\n\ndef _merge_paragraphs_within_section(sections):\n    """"""\n    Internal utility function that merges paragraphs into a single section.\n\n    This function call is required after initial splitting of the docstring into sections.  The initial split\n    is based on the presence of ``\'\\n\\n\'``, which separates sections and paragraphs. The difference between\n    section and paragraph is that section starts with the title of the form:\n\n        This is title\n        -------------\n        This is the first paragraph. It may be multi-line.\n        This is the second line of the paragraph.\n\n        This is another multi-line paragraph.\n        This is the second line of the paragraph.\n\n    Special treatment is required for Intel SDC header section and the following description section. Intel SDC\n    header section must the the first one in the docstring. It consists of exactly 3 lines:\n\n        Intel Scalable Dataframe Compiler User Guide\n        ********************************************\n        Pandas API: *pandas_api_fully_qualified_name*\n\n    Right after the Intel SDC header section the description section (if any) goes. It generally consists of two\n    or more paragraphs. The first paragraph represents short description, which is typically single line.\n    The following paragraphs provide full description. In rare cases documentation does not have description section,\n    and this must be treated accordingly.\n\n\n    :param sections: List of tuples ``(title, text)``.\n    :return: Reformatted list of tuples ``(title, text)`, where paragraphs belonging to one section are merged in\n        single ``text`` item.\n    """"""\n    if len(sections) == 0:\n        return sections\n\n    merged_sections = []\n    # Check if the very first section is Intel SDC header\n    section_title, section_text = sections[0]\n    if is_sdc_user_guide_header((section_title, section_text)):\n        merged_sections.append(sections[0])\n        sections.pop(0)\n\n    # Check if the next section is the short description\n    section_title, section_text = sections[0]\n    if section_title.strip() == \'\':\n        merged_sections.append(sections[0])\n        sections.pop(0)\n\n    if len(sections) == 0:\n        return merged_sections\n\n    # Merge next sections with empty title into a single section representing full description\n    section_title, section_text = sections[0]\n    if section_title.strip() == \'\':\n        sections.pop(0)\n        while len(sections) > 0:\n            title, text = sections[0]\n            if title.strip() == \'\':\n                section_text += \'\\n\\n\' + text\n                sections.pop(0)\n            else:\n                break\n        merged_sections.append((section_title, section_text))\n\n    # Now merge paragraphs of remaining titled sections\n    while len(sections) > 0:\n        section_title, section_text = sections[0]\n        sections.pop(0)\n        while len(sections) > 0:\n            title, text = sections[0]\n            if title.strip() == \'\':\n                section_text += \'\\n\\n\' + text\n                sections.pop(0)\n            else:\n                break\n        merged_sections.append((section_title, section_text))\n\n    return merged_sections\n\n\ndef split_in_sections(doc):\n    """"""\n    Splits the doc string into sections\n\n    Each section is separated by empty line. Sections can start with headers or without. Each header follows NumPy\n    style:\n\n        Section Title\n        -------------\n\n    Other permitted characters can be used to underline section title\n\n    :param doc: Docstring to be split into sections\n    :return: List, sections of the doc. Each section is a tuple of strings (title, text)\n\n    :seealso: NumPy style `example\n        <https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_numpy.html#example-numpy>`_\n    """"""\n    doc = reindent(doc, 0)\n    sections = doc.split(\'\\n\\n\')  # Sections are separated by empty lines\n    titled_sections = []\n\n    while len(sections) > 0:\n        title, text = split_title(sections[0])\n        sections.pop(0)\n        titled_sections.append((title, text))\n\n    return _merge_paragraphs_within_section(titled_sections)\n\n\ndef get_short_description(obj, sdc_header_flag=False):\n    """"""\n    Returns short description for a given object obj\n\n    :param obj: Object for which short description needs to be returned\n    :param sdc_header_flag: Flag indicating that the first three lines must be considered as Intel SDC header\n    :return: String, short description\n    :raises: NameError, when ``sdc_header_flag==True`` and no Intel SDC header section found.\n        The header is expected to be 4 lines long, where the first three lines are of the form:\n            Intel Scalable Dataframe Compiler User Guide\n            ********************************************\n            Pandas API: <pandas API name>\n        The fourth line must be empty\n\n    """"""\n    doc = get_docstring(obj)\n    if doc == \'\':\n        return doc\n\n    sections = split_in_sections(doc)  # tuple (title, text)\n\n    if sdc_header_flag:\n        if len(sections) > 1:  # There must be at least one more section after Intel SDC header section\n            if not is_sdc_user_guide_header(sections[0]):\n                raise NameError(\'No Intel SDC header section found\')\n\n            sections.pop(0)  # Ignore Intel SDC header section\n\n    if len(sections) == 0:\n        return \'\'  # Docstring has no sections, i.e. short description is absent\n\n    title, text = sections[0]  # Short description is the first section of the docstring\n    text = text.strip()\n    lines = text.split(\'\\n\')\n    lines = [line.strip() for line in lines]\n    lines = \' \'.join(lines)\n\n    return lines\n\n\ndef cut_sdc_dev_guide(doc):\n    """"""\n    Removes Intel SDC Developer Guide related sections from the docstring.\n\n    It is assumed that Developer Guide docstring follows the User Guide related sections of the docstring.\n    Everything after section the titled *Intel Scalable Dataframe Compiler Developer Guide* is cut\n\n    :param doc: Docstring that includes User Guide and the following Developer Guide sections\n    :return: Docstring with the cut Developer Guide sections\n    """"""\n    sections = split_in_sections(doc)    # tuple (title, text)\n    trimmed_sections = []\n\n    while len(sections) > 0:\n        if is_sdc_dev_guide_header(sections[0]):\n            break\n        trimmed_sections.append(sections[0])\n        sections.pop(0)\n\n    return trimmed_sections\n'"
docs/source/buildscripts/sdc_info.py,0,"b'# -*- coding: utf-8 -*-\n# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport logging\nimport sdc\nfrom datetime import datetime\nfrom module_info import get_submodules_of, print_modules_classes_methods_attributes, ENABLE_LOGGING, trim\nfrom module_info import get_function, get_function_doc\n\n# -- String for pattern matching that indicates that the docstring belongs to Intel SDC API Reference ---------------\nSDC_USR_GUIDE_HEADING_STR = \\\n    \'Intel Scalable Dataframe Compiler User Guide********************************************\'\nSDC_DEV_GUIDE_HEADING_STR = \\\n    \'Intel Scalablle Dataframe Compiler Developer Guide**************************************************\'\n\n# -- Debug logging --------------------------------------------------------------------------------------------------\nlog_file_name = \'../build/sdc_info.log\'\n\n\n# -- Submodules, classes, and methods to be excluded from API Reference ---------------------------------------------\nexclude_modules = [\n    \'sdc.chiframes\',\n    \'sdc.compiler\',\n    \'sdc.config\',\n    \'sdc.io.pio\',\n    \'sdc.io.pio_api\',\n    \'sdc.io.pio_lower\',\n    \'sdc.utilities.utils\',\n    \'sdc.utilities.sdc_typing_utils\',\n    \'sdc.hstr_ext\',\n    \'sdc.datatypes.common_functions\',\n    \'sdc.datatypes.hpat_pandas_dataframe_pass\',\n    \'sdc.decorators\',\n    \'sdc.dict_ext\',\n    \'sdc.hdict_ext\',\n    \'sdc.distributed\',\n    \'sdc.distributed_api\',\n    \'sdc.transport_seq\',\n    \'sdc.distributed_lower\',\n    \'sdc.hdist\',\n    \'sdc.distributed_analysis\',\n    \'sdc.hdatetime_ext\',\n    \'sdc.hiframes\',\n    \'sdc.io.csv_ext\',\n    \'sdc.hio\',\n    \'sdc.hiframes.join\',\n    \'sdc.io.parquet_pio\',\n    \'sdc.parquet_cpp\',\n    \'sdc.shuffle_utils\',\n    \'sdc.str_arr_ext\',\n    \'sdc.str_ext\',\n    \'sdc.timsort\',\n]\n\nexclude_classes = [\n]\n\nexclude_methods = [\n]\n\nexclude_attributes = [\n]\n\nexclude_functions = [\n]\n\n\n# -- Implements custom skip functions for the parser ----------------------------------------------------------------\ndef _skip_sdc_module(mod, mod_name):\n    return mod_name in exclude_modules or (not mod_name.startswith(\'sdc\') and not mod_name.startswith(\'hpat\'))\n\n\ndef _skip_sdc_class(cls, cls_name):\n    return True  # Exclude all classes\n#    return cls_name in exclude_classes  # Explicit exclusion of the class\n\n\ndef _skip_sdc_method(cls, method_name):\n    # Exclude the method if in the exclude_methods list\n    if method_name in exclude_methods:\n        return True\n\n    # Exclude the method without docstring\n    try:\n        doc = getattr(cls, method_name).__doc__\n        if len(doc) < 1:\n            return True\n    except AttributeError:\n        return True\n    except TypeError:\n        return True\n\n    # Exclude the method that does have docstring aimed for API Reference\n    return not doc.startswith(SDC_USR_GUIDE_HEADING_STR)\n\n\ndef _skip_sdc_function(func, function_name):\n    # Exclude the function if in the exclude_methods list\n    if function_name in exclude_functions:\n        return True\n\n    # Exclude the function without docstring\n    try:\n        doc = func.__doc__\n        if len(doc) < 1:\n            return True\n    except AttributeError:\n        return True\n    except TypeError:\n        return True\n\n    # Include the function that has docstring aimed for API Reference\n    doc = \'\'.join(trim(doc).splitlines())\n    return not doc.startswith(SDC_USR_GUIDE_HEADING_STR)\n\n\ndef _skip_sdc_attribute(cls, attr_name):\n    # Exclude the attribute if in the exclude_methods list\n    if attr_name in exclude_attributes:\n        return True\n\n    #  Exclude the attribute without docstring\n    try:\n        doc = getattr(cls, attr_name).__doc__\n        if len(doc) < 1:\n            return True\n    except AttributeError:\n        return True\n    except TypeError:\n        return True\n\n    # Include the attribute that has docstring aimed for API Reference\n    doc = \'\'.join(trim(doc).splitlines())\n    return not doc.startswith(SDC_USR_GUIDE_HEADING_STR)\n\n\ndef get_sdc_modules():\n    inspected_modules = set()\n    modules = []\n    get_submodules_of(sdc, inspected_modules, modules, _skip_sdc_module, _skip_sdc_class,\n                      _skip_sdc_method, _skip_sdc_attribute, _skip_sdc_function)\n    return modules\n\n\ndef init_sdc_logging():\n    if ENABLE_LOGGING:\n        logging.basicConfig(filename=log_file_name, level=logging.DEBUG)\n        logging.debug(\'****************** STARTING THE LOG *************************\')\n        logging.debug(datetime.now().strftime(""%d/%m/%Y %H:%M:%S""))\n\n\nif __name__ == ""__main__"":\n    # Initialize logging\n    init_sdc_logging()\n\n    # Execute parser for SDC\n\n    # You may uncomment this line in case you want to print out generated methods and attributes\n    # print_modules_classes_methods_attributes(modules)\n    modules = get_sdc_modules()\n\n    func = get_function(\'hpat_pandas_series_at\', modules)\n    if func:\n        titled_sections = get_function_doc(func)\n        print(titled_sections)\n'"
docs/source/buildscripts/sdc_object_utils.py,0,"b'# -*- coding: utf-8 -*-\n# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nfrom inspect import getmembers, ismodule, isclass, isfunction\nimport sys\nimport pandas\nimport sdc\nfrom sdc_doc_utils import is_sdc_user_guide_header, get_docstring, split_title, extract_pandas_name_from\n\n# -- Pandas submodules to be excluded from API Reference ---------------------------------------------\nexclude_pandas_submodules = [\n    \'pandas.compat\',  # This is PRIVATE submodule\n    \'pandas.util\',    # This is PRIVATE submodule\n    \'pandas.api.extensions\',  # This is extension for library developers extending Pandas\n    \'pandas.testing\',   # Utility functions for testing. Not a priority for SDC\n    \'pandas.plotting\',  # Plotting functions. Not a priority for compiling with SDC\n    \'pandas.errors\',  # Error handling functionality. Not a priority for SDC\n    \'pandas.api.types\',  # Not a priority for SDC\n    \'pandas.io.formats.style\',  # Helps to style dataframes with HTML and CSS. Not a priority for SDC\n    \'pandas.arrays\',  # Array extensions for Numpy. We do not explicitly cover in SDC documentation now\n    \'pandas.tseries\',  # SDC does not yet support Time Series objects\n    \'pandas.core.dtypes.dtypes\',\n]\n\n# -- Intel SDC submodules to be excluded from API Reference -------------------------------------------\nexclude_sdc_submodules = [\n    \'sdc.chiframes\',\n    \'sdc.compiler\',\n    \'sdc.config\',\n    \'sdc.io.pio\',\n    \'sdc.io.pio_api\',\n    \'sdc.io.pio_lower\',\n    \'sdc.utilities.utils\',\n    \'sdc.utilities.sdc_typing_utils\',\n    \'sdc.hstr_ext\',\n    \'sdc.datatypes.common_functions\',\n    \'sdc.datatypes.hpat_pandas_dataframe_pass\',\n    \'sdc.decorators\',\n    \'sdc.dict_ext\',\n    \'sdc.hdict_ext\',\n    \'sdc.distributed\',\n    \'sdc.distributed_api\',\n    \'sdc.transport_seq\',\n    \'sdc.distributed_lower\',\n    \'sdc.hdist\',\n    \'sdc.distributed_analysis\',\n    \'sdc.hdatetime_ext\',\n    \'sdc.io.csv_ext\',\n    \'sdc.hio\',\n    \'sdc.hiframes.join\',\n    \'sdc.io.parquet_pio\',\n    \'sdc.parquet_cpp\',\n    \'sdc.shuffle_utils\',\n    \'sdc.str_arr_ext\',\n    \'sdc.str_ext\',\n    \'sdc.timsort\',\n]\n\npandas_modules = dict()  # Dictionary of pandas submodules and their classes and functions\nsdc_modules = dict()  # Dictionary of Intel SDC submodules and their classes and functions\npandas_sdc_dict = dict()  # Dictionary {<pandas_obj>: <sdc_obj>} that maps Pandas API to respective Intel SDC API\n\n\ndef get_sdc_object(pandas_obj):\n    """"""\n    Returns corresponding Intel SDC object for a given Pandas object pandas_obj.\n\n    :param pandas_obj: Pandas object to be matched with Intel SDC object\n    :return: Intel SDC object corresponding to pandas_obj\n    """"""\n    if pandas_obj in pandas_sdc_dict:\n        return pandas_sdc_dict[pandas_obj]\n    else:\n        return None  # There is no match in Intel SDC to pandas_obj\n\n\ndef get_sdc_object_by_pandas_name(pandas_name):\n    """"""\n    Returns corresponding Intel SDC object for a given Pandas object given as string ``pandas_name``.\n\n    This function is needed because :func:`get_sdc_object` cannot uniquely match Intel SDC and Pandas objects.\n    For example, the same Pandas object represents :meth:`Series.get` and :meth:`DataFrame.get` methods. As a result\n    that :func:`get_sdc_object` will return **some** SDC object that matches respective Pandas object. If you need\n    unique match between Pandas and Intel SDC use :func:`get_sdc_object_by_pandas_name` function instead. (Which\n    should be the case for majority usecases).\n\n    :param pandas_name: Pandas object to be matched with Intel SDC object\n    :return: Intel SDC object corresponding to Pandas object having ``pandas_name`` name\n    """"""\n    if pandas_name in pandas_sdc_dict:\n        return pandas_sdc_dict[pandas_name]\n    else:\n        return None  # There is no match in Intel SDC to pandas_obj\n\n\ndef init_pandas_sdc_dict():\n    """"""\n    Initializes global dictionary that performs mapping between Pandas objects and SDC objects.\n\n    To function correctly this function must be called after initialization of ``sdc_modules`` and ``pandas_modules``\n    lists by :func:`init_sdc_structure` and :func:`init_pandas_structure`` functions respectively.\n    """"""\n\n    def _map_sdc_to_pandas(sdc_obj):\n        if isfunction(sdc_obj):\n            doc = get_docstring(sdc_obj)\n\n            # The very first section of Intel SDC documentation is expected to start with\n            # the User Guide header followed by the name of respective Pandas API.\n            # The following code extracts respective Pandas API\n            title, text = split_title(doc)\n            if is_sdc_user_guide_header((title, text)):\n                pandas_name = extract_pandas_name_from(text)\n                pandas_obj = get_obj(pandas_name)\n                pandas_sdc_dict[pandas_obj] = sdc_obj\n                pandas_sdc_dict[pandas_name] = sdc_obj\n        return False\n\n    global pandas_sdc_dict\n    pandas_sdc_dict = {}\n\n    traverse(sdc_modules, _map_sdc_to_pandas, True)\n\n\ndef get_obj(obj_name):\n    """"""\n    Retrieves object corresponding to fully qualified name obj_name.\n\n    The fully qualified name starts with the imported module name visible by sys.modules followed by\n    submodules and then classes and finally by class attributes\n    :param obj_name: Fully qualified object name string\n    :return: If found, returns the object corresponding to obj_name. Otherwise raises exception\n    :raises AttributeError: If submodule or attribute does not exists\n    """"""\n    split_name = obj_name.split(\'.\')\n    split_obj = sys.modules[split_name[0]]\n\n    # Iterate through submodules\n    while ismodule(split_obj) and len(split_name) > 1:\n        split_name.pop(0)\n        not_found = True\n        for (name, obj) in getmembers(split_obj):  # Go through members of split_obj\n            if split_name[0] == name:\n                not_found = False\n                break\n\n        if not_found:\n            raise AttributeError(\'Member `\' + split_name[0] + \'` for `\' + obj_name + \'` does not exists\')\n\n        split_obj = obj\n\n    split_name.pop(0)\n    for name in split_name:\n        split_obj = getattr(split_obj, name, 0)\n\n    return split_obj\n\n\ndef get_class_methods(cls):\n    """"""\n    Returns the list of class methods, accessible by both names and as objects.\n\n    Function ignores internal methods starting with ``_``.\n\n    :param cls: The class object\n    :return: List of class methods, each item is the tuple ``(method_name, method_object)``\n    """"""\n    return [(func, getattr(cls, func)) for func in dir(cls)\n            if callable(getattr(cls, func)) and not func.startswith(\'_\')]\n\n\ndef get_class_attributes(cls):\n    """"""\n    Returns the list of class attributes, accessible by both names and as objects.\n\n    Function ignores internal attributes starting with ``_``.\n\n    :param cls: The class object\n    :return: List of class attributes, each item is the tuple ``(attribute_name, attribute_object)``\n    """"""\n    return [(func, getattr(cls, func)) for func in dir(cls)\n            if not callable(getattr(cls, func)) and not func.startswith(\'_\')]\n\n\ndef get_fully_qualified_name(cls):\n    """"""\n    Returns fully qualified name of the class.\n\n    :param cls: The class object\n    :return: String, fully qualified name\n    """"""\n    return repr(cls)[8:-2]\n\n\ndef init_module_structure(module_obj, the_module, inspected, skip_test):\n    """"""\n    Initializes hierarchical structure ``the_module``.\n\n    :param module_obj: Module object being traversed.\n    :param the_module: Dictionary ``{\'module_obj\': module_obj, \'submodules\': submodules,\n        \'classes\': classes, \'functions\': functions}``. The ``submodules`` is the list of\n        submodules that belong to ``module_obj``. Each submodule has the same structure as ``the_module``.\n        The ``classes`` is the list of classes that belong to ``module_obj``.\n        The functions is the list of functions that belong ``to module_obj``.\n    :param inspected: Set of already traversed module objects. This set is needed to avoid circular traversal of\n        the same module, which may be returned by by ``getmembers`` function multiple times.\n    :param skip_test: Function that takes module object as an argument and returns True if this object\n        needs to be included in the module structure hierarchy or skipped if False. This function is used as\n        a mechanism to customize the structure of modules, classes, and functions. This in turn minimizes following\n        structure traversal costs.\n    """"""\n\n    # Returns True if the mod module needs to be ignored\n    def _is_skip_module(mod):\n        mod_name = mod.__name__\n        return \'._\' in mod_name or mod_name.startswith(\'_\')\n\n    # Returns True if the class cls needs to be ignored\n    def _is_skip_class(cls):\n        class_name = get_fully_qualified_name(cls)\n        return \'._\' in class_name\n\n    # Returns True if the object obj needs to be ignored\n    def _is_internal(obj):\n        obj_name = obj.__name__\n        return obj_name.startswith(\'_\')\n\n    # ************  The init_module_structure implementation starts here  *******************************************\n    if _is_skip_module(module_obj) or module_obj in inspected or skip_test(module_obj):\n        return\n\n    inspected.add(module_obj)\n\n    # Traverse submodules, classes, and functions\n    submodules = []\n    classes = []\n    functions = []\n    for (name, obj) in getmembers(module_obj):  # Iterate through members of the submodule\n        if skip_test(obj):\n            continue  # Customizable test for skipping objects as needed\n\n        if ismodule(obj) and obj not in inspected and not _is_skip_module(obj):\n            the_submodule = dict()\n            init_module_structure(obj, the_submodule, inspected, skip_test)\n            submodules.append(the_submodule)\n\n        if isclass(obj) and not _is_skip_class(obj):\n            classes.append(obj)\n\n        if isfunction(obj) and not _is_internal(obj):\n            functions.append(obj)\n\n    the_module[\'module_obj\'] = module_obj\n    the_module[\'submodules\'] = submodules\n    the_module[\'classes\'] = classes\n    the_module[\'functions\'] = functions\n\n\ndef _print_module(the_module, print_submodules_flag=True):\n    """"""\n    Recursively prints ``the_module`` content. Internal utility function for debugging purposes\n\n    :param the_module: Dictionary ``{\'module_obj\': module_obj, \'submodules\': submodules,\n        \'classes\': classes, \'functions\': functions}``. The ``submodules`` is the list of\n        submodules that belong to ``module_obj``. Each submodule has the same structure as ``the_module``.\n        The ``classes`` is the list of classes that belong to ``module_obj``.\n        The functions is the list of functions that belong ``to module_obj``.\n    """"""\n    print(the_module[\'module_obj\'].__name__)\n\n    print(\'  CLASSES:\')\n    for the_class in the_module[\'classes\']:\n        print(\'  - \' + the_class.__name__)\n\n    print(\'  FUNCTIONS:\')\n    for the_func in the_module[\'functions\']:\n        print(\'  - \' + the_func.__name__)\n\n    if print_submodules_flag:\n        print(\'  SUBMODULES:\')\n        for submodule in the_module[\'submodules\']:\n            _print_module(submodule, print_submodules_flag)\n\n\ndef traverse(the_module, do_action, traverse_submodules_flag=True):\n    """"""\n    Traverses ``the_module`` and performs action :func:`do_action` on each of the objects of the structure.\n\n    :param the_module: Dictionary ``{\'module_obj\': module_obj, \'submodules\': submodules,\n        \'classes\': classes, \'functions\': functions}``. The ``submodules`` is the list of\n        submodules that belong to ``module_obj``. Each submodule has the same structure as ``the_module``.\n        The ``classes`` is the list of classes that belong to ``module_obj``.\n        The functions is the list of functions that belong to ``module_obj``.\n    :param do_action: Function that takes one parameter ``module_obj`` as input. It returns ``True`` if\n        traversal needs to be stopped.\n    :param traverse_submodules_flag: True if function must recursively traverse submodules too\n    :return: Returns tuple ``(the_module, obj)`` where ``obj`` is the object identified by :func:`do_action` and\n        ``the_module`` is the corresponding dictionary structure to which the object belongs. It returns ``None``\n        if no object has been identified by the :func:`do_action`\n    """"""\n    if do_action(the_module[\'module_obj\']):\n        return the_module, the_module[\'module_obj\']\n\n    # Traverse classes of the_module\n    for the_class in the_module[\'classes\']:\n        if do_action(the_class):\n            return the_module, the_class\n\n    # Traverse functions of the_module\n    for the_func in the_module[\'functions\']:\n        if do_action(the_func):\n            return the_module, the_func\n\n    # Recursively traverse submodules of the_module\n    if traverse_submodules_flag:\n        for submodule in the_module[\'submodules\']:\n            the_tuple = traverse(submodule, do_action, traverse_submodules_flag)\n            if the_tuple is not None:\n                return the_tuple\n\n    return None\n\n\ndef get_pandas_module_structure(pandas_obj):\n    """"""\n    Returns corresponding ``the_module`` dictionary structure to which ``pandas_obj`` belongs to.\n\n    This function is typically used in conjunction with :func:`traverse`\n\n    :param pandas_obj:\n    :return: ``the_module`` dictionary structure\n    """"""\n\n    def _find(obj):\n        return obj == pandas_obj\n\n    the_module, the_object = traverse(pandas_modules, _find)\n    return the_module\n\n\ndef init_pandas_structure():\n    """"""\n    Initializes ``pandas_modules`` global dictionary representing the structure of Pandas.\n    """"""\n\n    # Test that allows to ignore certain Pandas submodules, classes, or attributes\n    def _skip_pandas_test(obj):\n        if ismodule(obj):\n            name = obj.__name__\n            for mod_name in exclude_pandas_submodules:\n                if name.startswith(mod_name):\n                    return True\n            return not name.startswith(\'pandas\')\n\n    global pandas_modules\n    pandas_modules = dict()\n    inspected_mods = set()\n    init_module_structure(pandas, pandas_modules, inspected_mods, _skip_pandas_test)\n\n\ndef init_sdc_structure():\n    """"""\n    Initializes ``sdc_modules`` global dictionary representing the structure of Intel SDC.\n    """"""\n\n    # Test that allows to ignore certain Intel SDC submodules, classes, or attributes\n    def _skip_sdc_test(obj):\n        if ismodule(obj):\n            name = obj.__name__\n            for mod_name in exclude_sdc_submodules:\n                if name.startswith(mod_name):\n                    return True\n            return not name.startswith(\'sdc\') and not name.startswith(\'hpat\')\n\n    global sdc_modules\n    sdc_modules = dict()\n    inspected_mods = set()\n    init_module_structure(sdc, sdc_modules, inspected_mods, _skip_sdc_test)\n\n\nif __name__ == ""__main__"":\n    init_pandas_structure()\n    _print_module(pandas_modules)\n\n    init_sdc_structure()\n    _print_module(sdc_modules)\n\n    init_pandas_sdc_dict()\n    print(pandas_sdc_dict)\n'"
docs/source/buildscripts/user_guide_gen.py,0,"b'# -*- coding: utf-8 -*-\n# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nfrom module_info import get_function, get_method_attr, get_function_doc, get_function_short_description\nfrom module_info import create_header_str\nfrom pandas_info import get_pandas_modules, init_pandas_logging\nfrom sdc_info import get_sdc_modules, init_sdc_logging\nfrom texttable import Texttable\nimport os\n\nPANDAS_API_STR = \'Pandas API: \'  # This substring prepends Pandas API name in the documentation\nAPIREF_RELPATH = r\'./_api_ref/\'  # Relative path to API Reference folder\nRST_MODULES = {\n    \'api_reference.rst\': [\'pandas\'],\n    \'io.rst\': [\'pandas.io.api\', \'pandas.io.clipboards\', \'pandas.io.common\', \'pandas.io.excel\',\n               \'pandas.io.feather_format\', \'pandas.io.formats.console\', \'pandas.io.formats.format\',\n               \'pandas.io.formats.printing\', \'pandas.io.gbq\', \'pandas.io.html\', \'pandas.io.json\',\n               \'pandas.io.msgpack\', \'pandas.io.msgpack.exceptions\', \'pandas.io.packers\', \'pandas.io.parquet\',\n               \'pandas.io.parsers\', \'pandas.io.pickle\', \'pandas.io.pytables\', \'pandas.io.sas\',\n               \'pandas.io.sas.sasreader\', \'pandas.io.spss\', \'pandas.io.sql\', \'pandas.io.stata\'],\n    \'series.rst\': [\'pandas.Series\'],\n    \'dataframe.rst\': [\'pandas.DataFrame\'],\n    \'\'\n    \'general_functions.rst\': [],\n}\n\npandas_modules = []  # List of Pandas submodules along with its functions and classes\nsdc_modules = []  # List of Intel SDC submodules along with its functions and classes\n\n\ndef generate_module_doc(the_module):\n    module_doc = None\n    module_name = the_module[\'module_name\']\n\n    # First, look up if there is RST file documenting particular module\n    for rst in RST_MODULES:\n        for mod in RST_MODULES[rst]:\n            if mod == module_name:\n                return module_doc  # If there is a documentation for a given module then just return\n\n    # If there is no RST file then we create the documentation based on module\'s docstring\n    module_obj = the_module[\'module_object\']\n    module_description = get_function_short_description(module_obj).strip()\n    if module_description is None:\n        module_description = \'\'\n\n    module_doc = module_description + \'\\n\\nFor details please refer to Pandas API Reference for :py:mod:`\' + \\\n        module_name + \'`\\n\\n\'\n    return module_doc\n\n\ndef generate_api_index_for_module(the_module):\n    module_description = generate_module_doc(the_module)\n    if module_description is None:\n        module_description = \'\'\n    module_doc = \'\'\n\n    module_header_flag = False\n    # Document functions first, if any\n    tab = Texttable()\n    for func in the_module[\'functions\']:  # Iterate through the module functions\n        name = func[\'function_name\']\n        obj = getattr(the_module[\'module_object\'], name)  # Retrieve the function object\n        description = get_function_short_description(obj).strip()\n        tab.add_rows([[name, description]], header=False)\n\n    module_name = \'\'\n    func_doc = tab.draw()\n    if func_doc and func_doc != \'\':  # If the function list is not empty then add module name to the document\n        module_name = the_module[\'module_name\']\n        module_doc += create_header_str(module_name, \'~\') + \'\\n\\n\' + module_description + \'\\n\\n\' + \\\n            create_header_str(\'Functions:\', \'-\') + \\\n            \'\\n\\n\' + func_doc + \'\\n\\n\'\n        module_header_flag = True\n\n    # Document classes\n    classes_header_flag = False\n    for the_class in the_module[\'classes\']:  # Iterate through the module classes\n        tab.reset()\n        class_name = the_class[\'class_name\']\n        class_obj = the_class[\'class_object\']\n        class_description = class_obj.__doc__\n        if not class_description:\n            class_description = \'\'\n        class_doc = \'\'\n        class_header_flag = False\n\n        # Document class attributes first, if any\n        for attr in the_class[\'class_attributes\']:  # Iterate through the class attributes\n            name = attr\n            obj = getattr(the_class[\'class_object\'], name)  # Retrieve the attribute object\n            description = get_function_short_description(obj).strip()\n            tab.add_rows([[name, description]], header=False)\n\n        attr_doc = tab.draw()\n        if attr_doc and attr_doc != \'\':  # If the attribute list is not empty then add class name to the document\n            class_header_flag = True\n            class_doc += create_header_str(class_name, \'^\') + \'\\n\\n\' + class_description + \'\\n\\n\' + \\\n                create_header_str(\'Attributes:\', \'+\') + \\\n                \'\\n\\n\' + attr_doc + \'\\n\\n\'\n\n        # Document class methods, if any\n        for method in the_class[\'class_methods\']:  # Iterate through the class methods\n            name = method\n            obj = getattr(the_class[\'class_object\'], name)  # Retrieve the method object\n            description = get_function_short_description(obj).strip()\n            tab.add_rows([[name, description]], header=False)\n\n        method_doc = tab.draw()\n        if method_doc and method_doc != \'\':  # If the method list is not empty then add class name to the document\n            if not class_header_flag:\n                class_doc += create_header_str(class_name, \'^\') + \'\\n\\n\' + class_description + \'\\n\\n\' + \\\n                             create_header_str(\'Methods:\', \'+\') + \\\n                             \'\\n\\n\' + method_doc + \'\\n\\n\'\n                class_header_flag = True\n            else:\n                class_doc += create_header_str(\'Methods:\', \'+\') + \\\n                             \'\\n\\n\' + method_doc + \'\\n\\n\'\n\n        if not module_header_flag:  # There is no module header yet\n            if class_header_flag:  # There were methods/attributes for the class\n                module_doc += create_header_str(module_name, \'~\') + \'\\n\\n\' + module_description + \'\\n\\n\' + \\\n                              create_header_str(\'Classes:\', \'-\') + \\\n                              \'\\n\\n\' + class_doc + \'\\n\\n\'\n                module_header_flag = True\n                classes_header_flag = True\n        else:  # The module header has been added\n            if class_header_flag:  # There are new methods/attributes for the class\n                if not classes_header_flag:  # First class of the module description\n                    module_doc += create_header_str(\'Classes:\', \'-\') + \'\\n\\n\'\n                module_doc += \'\\n\\n\' + class_doc + \'\\n\\n\'\n    return module_doc\n\n\ndef get_module_rst_fname(the_module):\n    file_name = the_module[\'module_name\']\n    file_name = file_name.replace(\'.\', \'/\')\n    file_name = APIREF_RELPATH + file_name + \'.rst\'\n    return file_name\n\n\ndef generate_api_index():\n    doc = \'.. _apireference::\\n\\nAPI Reference\\n*************\\n\\n\' \\\n          \'.. toctree::\\n   :maxdepth: 1\\n\\n\'\n\n    for the_module in pandas_modules:  # Iterate through pandas_modules\n        module_doc = generate_api_index_for_module(the_module)\n        if len(module_doc) > 0:\n            file_name = get_module_rst_fname(the_module)\n            write_rst(file_name, module_doc)\n            doc += \'   \' + file_name + \'\\n\'\n    return doc\n\n\ndef generate_sdc_object_doc(sdc_func):\n    sdc_titled_sections = get_function_doc(sdc_func, True)\n    sdc_see_also_text = next((sec[\'text\'] for sec in sdc_titled_sections\n                              if sec[\'title\'].lower().strip() == \'see also\'), \'\')\n    sdc_limitations_text = next((sec[\'text\'] for sec in sdc_titled_sections\n                                 if sec[\'title\'].lower().strip() == \'limitations\'), \'\')\n    sdc_examples_text = next((sec[\'text\'] for sec in sdc_titled_sections\n                              if sec[\'title\'].lower().strip() == \'examples\'), \'\')\n\n    # Get respective Pandas API name\n    pandas_name = sdc_titled_sections[0][\'text\'].strip()\n    pandas_name = pandas_name.replace(PANDAS_API_STR, \'\')\n    pandas_name = pandas_name.replace(\'\\n\', \'\')\n\n    # Find respective Pandas API\n    doc_object = get_method_attr(pandas_name, pandas_modules)\n    if not doc_object:\n        doc_object = get_function(pandas_name, pandas_modules)\n    if not doc_object:\n        raise NameError(\'Pandas API:\' + pandas_name + \'does not exist\')\n\n    # Extract Pandas API docstring as the list of sections\n    pandas_titled_sections = []\n    if doc_object:\n        pandas_titled_sections = get_function_doc(doc_object, False)\n\n    # Form final docstring which is a combination of Pandas docstring for the description, Parameters section,\n    # Raises section, Returns section. See Also, Limitations and Examples sections (if any) are taken from SDC docstring\n    short_description_section = pandas_titled_sections[0][\'text\'] + \'\\n\\n\'\n    pandas_titled_sections.pop(0)\n\n    long_description_section = \'\'\n    while pandas_titled_sections[0][\'title\'] == \'\':\n        long_description_section += pandas_titled_sections[0][\'text\'] + \'\\n\\n\'\n        pandas_titled_sections.pop(0)\n\n    raises_section = parameters_section = returns_section = see_also_section = \\\n        limitations_section = examples_section = \'\'\n    for section in pandas_titled_sections:\n        title = section[\'title\'].lower().strip()\n        if title == \'raises\':\n            raises_section = \'Raises\\n------\\n\\n\' + section[\'text\'] + \'\\n\\n\'\n        elif title == \'parameters\':\n            parameters_section = \'Parameters\\n----------\\n\\n\' + section[\'text\'] + \'\\n\\n\'\n        elif title == \'return\' or title == \'returns\':\n            returns_section = \'Returns\\n-------\\n\\n\' + section[\'text\'] + \'\\n\\n\'\n\n    if sdc_see_also_text:\n        see_also_section = \'\\n.. seealso::\\n\\n\' + sdc_see_also_text + \'\\n\\n\'\n\n    if sdc_limitations_text:\n        limitations_section = \'Limitations\\n-----------\\n\\n\' + sdc_limitations_text + \'\\n\\n\'\n\n    if sdc_examples_text:\n        examples_section = \'Examples\\n-----------\\n\\n\' + sdc_examples_text + \'\\n\\n\'\n\n    rst_label = pandas_name.replace(\'.\', \'_\')\n\n    n = len(pandas_name)\n    docstring = \\\n        \'.. _\' + rst_label + \':\\n\\n\' + \\\n        pandas_name + \'\\n\' + \'*\'*n + \'\\n\' + \\\n        short_description_section + \\\n        long_description_section + \\\n        parameters_section + \\\n        returns_section + \\\n        raises_section +  \\\n        limitations_section +  \\\n        examples_section +  \\\n        see_also_section\n\n    file_name = rst_label + \'.rst\'\n\n    return file_name, docstring\n\n\ndef write_rst(file_name, docstring):\n    directory = os.path.dirname(file_name)\n\n    if len(directory) > 0 and not os.path.exists(directory):\n        os.makedirs(directory)\n\n    file = open(file_name, \'w\')\n    file.write(docstring)\n    file.close()\n\n\nif __name__ == ""__main__"":\n    init_pandas_logging()\n    pandas_modules = get_pandas_modules()\n\n    init_sdc_logging()\n    sdc_modules = get_sdc_modules()\n\n    for the_module in sdc_modules:\n        if the_module[\'module_name\'] == \'sdc.datatypes.hpat_pandas_series_functions\':\n            for func in the_module[\'functions\']:\n                file_name, doc = generate_sdc_object_doc(func[\'function_object\'])\n                write_rst(APIREF_RELPATH + file_name, doc)\n\n    doc = generate_api_index()\n    write_rst(\'apireference.rst\', doc)\n'"
examples/dataframe/getitem/df_getitem.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\nExpected result:\n0    0\n1    1\n2    2\n3    3\n4    4\nName: A, dtype: int64\n""""""\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef dataframe_getitem():\n    df = pd.DataFrame({\'A\': [0, 1, 2, 3, 4],\n                       \'B\': [1, 2, 3, 4, 5],\n                       \'C\': [2, 3, 4, 5, 6]})\n\n    return df[\'A\']\n\n\nprint(dataframe_getitem())\n'"
examples/dataframe/getitem/df_getitem_array.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\nExpected result:\n   A  B  C\n1  1  2  3\n4  4  5  6\n""""""\n\nimport pandas as pd\nimport numpy as np\nfrom numba import njit\n\n\n@njit\ndef dataframe_getitem():\n    df = pd.DataFrame({\'A\': [0, 1, 2, 3, 4],\n                       \'B\': [1, 2, 3, 4, 5],\n                       \'C\': [2, 3, 4, 5, 6]})\n    arr = np.array([False, True, False, False, True])\n\n    return df[arr]\n\n\nprint(dataframe_getitem())\n'"
examples/dataframe/getitem/df_getitem_attr.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\nExpected result:\n0    2\n1    3\n2    4\n3    5\n4    6\nName: C, dtype: int64\n""""""\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef dataframe_getitem():\n    df = pd.DataFrame({\'A\': [0, 1, 2, 3, 4],\n                       \'B\': [1, 2, 3, 4, 5],\n                       \'C\': [2, 3, 4, 5, 6]})\n\n    return df.C\n\n\nprint(dataframe_getitem())\n'"
examples/dataframe/getitem/df_getitem_series.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\nExpected result:\n   A  B  C\n0  0  1  2\n2  2  3  4\n""""""\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef dataframe_getitem():\n    df = pd.DataFrame({\'A\': [0, 1, 2, 3, 4],\n                       \'B\': [1, 2, 3, 4, 5],\n                       \'C\': [2, 3, 4, 5, 6]})\n    val = pd.Series([True, False, True, False, False])\n\n    return df[val]\n\n\nprint(dataframe_getitem())\n'"
examples/dataframe/getitem/df_getitem_slice.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\nExpected result:\n   A  B  C\n1  1  2  3\n2  2  3  4\n""""""\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef dataframe_getitem():\n    df = pd.DataFrame({\'A\': [0, 1, 2, 3, 4],\n                       \'B\': [1, 2, 3, 4, 5],\n                       \'C\': [2, 3, 4, 5, 6]})\n\n    return df[1:3]\n\n\nprint(dataframe_getitem())\n'"
examples/dataframe/getitem/df_getitem_tuple.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\nExpected result:\n   A  C\n0  0  2\n1  1  3\n2  2  4\n3  3  5\n4  4  6\n""""""\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef dataframe_getitem():\n    df = pd.DataFrame({\'A\': [0, 1, 2, 3, 4],\n                       \'B\': [1, 2, 3, 4, 5],\n                       \'C\': [2, 3, 4, 5, 6]})\n\n    return df[(\'A\', \'C\')]\n\n\nprint(dataframe_getitem())\n'"
examples/dataframe/groupby/dataframe_groupby_count.py,2,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nimport numpy as np\nfrom numba import njit\n\n\n@njit\ndef df_groupby_count():\n    df = pd.DataFrame({\'A\': [1, 2, 3, 1, 2, 3, 3, 3, 2],\n                       \'B\': [0, 1, np.nan, np.nan, 2, 4, 3, 2, np.inf],\n                       \'C\': [np.nan, 2, 3, np.nan, 5, 6, 7, 8, 9]})\n    out_df = df.groupby(\'A\').count()\n\n    # Expect DataFrame of\n    # {\'B\': [1, 3, 3], \'C\': [0, 3, 4} with index=[1, 2, 3]\n    return out_df\n\n\nprint(df_groupby_count())\n'"
examples/dataframe/groupby/dataframe_groupby_max.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef df_groupby_max():\n    df = pd.DataFrame({\'A\': [1, 2, 3, 1, 2, 3, 3, 3, 2],\n                       \'B\': [0, 1, 5, 0, 2, 4, 3, 2, 3],\n                       \'C\': [1, 2, 3, 4, 5, 6, 7, 8, 9]})\n    out_df = df.groupby(\'A\').max()\n\n    # Expect DataFrame of\n    # {\'B\': [0, 3, 5], \'C\': [4, 9, 8} with index=[1, 2, 3]\n    return out_df\n\n\nprint(df_groupby_max())\n'"
examples/dataframe/groupby/dataframe_groupby_mean.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef df_groupby_mean():\n    df = pd.DataFrame({\'A\': [1, 2, 3, 1, 2, 3, 3, 3, 2],\n                       \'B\': [0, 1, 5, 0, 2, 4, 3, 2, 3],\n                       \'C\': [1, 2, 3, 4, 5, 6, 7, 8, 9]})\n    out_df = df.groupby(\'A\').mean()\n\n    # Expect DataFrame of\n    # {\'B\': [0.0, 2.0, 3.5], \'C\': [2.500000, 5.333333, 6.000000} with index=[1, 2, 3]\n    return out_df\n\n\nprint(df_groupby_mean())\n'"
examples/dataframe/groupby/dataframe_groupby_median.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef df_groupby_median():\n    df = pd.DataFrame({\'A\': [1, 2, 3, 1, 2, 3, 3, 3, 2],\n                       \'B\': [0, 1, 5, 0, 3, 4, 3, 2, 4],\n                       \'C\': [1, 2, 3, 4, 5, 6, 7, 8, 9]})\n    out_df = df.groupby(\'A\').median()\n\n    # Expect DataFrame of\n    # {\'B\': [0.0, 3.0, 3.5], \'C\': [2.5, 5.0, 6.5} with index=[1, 2, 3]\n    return out_df\n\n\nprint(df_groupby_median())\n'"
examples/dataframe/groupby/dataframe_groupby_min.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef df_groupby_min():\n    df = pd.DataFrame({\'A\': [1, 2, 3, 1, 2, 3, 3, 3, 2],\n                       \'B\': [0, 1, 5, 0, 2, 4, 3, 2, 3],\n                       \'C\': [1, 2, 3, 4, 5, 6, 7, 8, 9]})\n    out_df = df.groupby(\'A\').min()\n\n    # Expect DataFrame of\n    # {\'B\': [0, 1, 2], \'C\': [1, 2, 3} with index=[1, 2, 3]\n    return out_df\n\n\nprint(df_groupby_min())\n'"
examples/dataframe/groupby/dataframe_groupby_prod.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef df_groupby_prod():\n    df = pd.DataFrame({\'A\': [1, 2, 3, 1, 2, 3, 3, 3, 2],\n                       \'B\': [0, 1, 5, 0, 2, 4, 3, 2, 3],\n                       \'C\': [1, 2, 3, 4, 5, 6, 7, 8, 9]})\n    out_df = df.groupby(\'A\').prod()\n\n    # Expect DataFrame of\n    # {\'B\': [0, 6, 120], \'C\': [4, 90, 1008} with index=[1, 2, 3]\n    return out_df\n\n\nprint(df_groupby_prod())\n'"
examples/dataframe/groupby/dataframe_groupby_std.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef df_groupby_std():\n    df = pd.DataFrame({\'A\': [1, 2, 3, 1, 2, 3, 3, 3, 2],\n                       \'B\': [0, 1, 5, 0, 2, 4, 3, 2, 3],\n                       \'C\': [1, 2, 3, 4, 5, 6, 7, 8, 9]})\n    out_df = df.groupby(\'A\').std()\n\n    # Expect DataFrame of\n    # {\'B\': [0.000000, 1.000000, 1.290994], \'C\': [2.121320, 3.511885, 2.160247} with index=[1, 2, 3]\n    return out_df\n\n\nprint(df_groupby_std())\n'"
examples/dataframe/groupby/dataframe_groupby_sum.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef df_groupby_sum():\n    df = pd.DataFrame({\'A\': [1, 2, 3, 1, 2, 3, 3, 3, 2],\n                       \'B\': [0, 1, 5, 0, 2, 4, 3, 2, 3],\n                       \'C\': [1, 2, 3, 4, 5, 6, 7, 8, 9]})\n    out_df = df.groupby(\'A\').sum()\n\n    # Expect DataFrame of\n    # {\'B\': [0, 6, 14], \'C\': [5, 16, 24} with index=[1, 2, 3]\n    return out_df\n\n\nprint(df_groupby_sum())\n'"
examples/dataframe/groupby/dataframe_groupby_var.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef df_groupby_var():\n    df = pd.DataFrame({\'A\': [1, 2, 3, 1, 2, 3, 3, 3, 2],\n                       \'B\': [0, 1, 5, 0, 2, 4, 3, 2, 3],\n                       \'C\': [1, 2, 3, 4, 5, 6, 7, 8, 9]})\n    out_df = df.groupby(\'A\').var()\n\n    # Expect DataFrame of\n    # {\'B\': [0.000000, 1.000000, 1.666667], \'C\': [4.500000, 12.333333, 4.666667} with index=[1, 2, 3]\n    return out_df\n\n\nprint(df_groupby_var())\n'"
examples/dataframe/rolling/dataframe_rolling_apply.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef df_rolling_apply():\n    df = pd.DataFrame({\'A\': [4, 3, 5, 2, 6], \'B\': [-4, -3, -5, -2, -6]})\n\n    def get_median(x):\n        return np.median(x)\n\n    out_df = df.rolling(3).apply(get_median)\n\n    # Expect DataFrame of\n    # {\'A\': [NaN, NaN, 4.0, 3.0, 5.0], \'B\': [NaN, NaN, -4.0, -3.0, -5.0]}\n    return out_df\n\n\nprint(df_rolling_apply())\n'"
examples/dataframe/rolling/dataframe_rolling_corr.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef df_rolling_corr():\n    df = pd.DataFrame({\'A\': [3, 3, 3, 5, 8], \'B\': [-3, -3, -3, -5, -8]})\n    other = pd.DataFrame({\'A\': [3, 4, 4, 4, 8], \'B\': [-3, -4, -4, -4, -8]})\n    out_df = df.rolling(4).corr(other)\n\n    # Expect DataFrame of\n    # {\'A\': [NaN, NaN, NaN, 0.333333, 0.916949],\n    #  \'B\': [NaN, NaN, NaN, 0.333333, 0.916949]}\n    return out_df\n\n\nprint(df_rolling_corr())\n'"
examples/dataframe/rolling/dataframe_rolling_count.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef df_rolling_count():\n    df = pd.DataFrame({\'A\': [4, 3, 2, np.nan, 6], \'B\': [4, np.nan, 2, np.nan, 6]})\n    out_df = df.rolling(3).count()\n\n    # Expect DataFrame of\n    # {\'A\': [1.0, 2.0, 3.0, 2.0, 2.0], \'B\': [1.0, 1.0, 2.0, 1.0, 2.0]}\n    return out_df\n\n\nprint(df_rolling_count())\n'"
examples/dataframe/rolling/dataframe_rolling_cov.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef df_rolling_cov():\n    df = pd.DataFrame({\'A\': [3, 3, 3, 5, 8], \'B\': [-3, -3, -3, -5, -8]})\n    other = pd.DataFrame({\'A\': [3, 4, 4, 4, 8], \'B\': [-3, -4, -4, -4, -8]})\n    out_df = df.rolling(4).cov(other)\n\n    # Expect DataFrame of\n    # {\'A\': [NaN, NaN, NaN, 0.166667, 4.333333],\n    #  \'B\': [NaN, NaN, NaN, 0.166667, 4.333333]}\n    return out_df\n\n\nprint(df_rolling_cov())\n'"
examples/dataframe/rolling/dataframe_rolling_kurt.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef df_rolling_kurt():\n    df = pd.DataFrame({\'A\': [4, 3, 5, 2, 6], \'B\': [-4, -3, -5, -2, -6]})\n    out_df = df.rolling(4).kurt()\n\n    # Expect DataFrame of\n    # {\'A\': [NaN, NaN, NaN, -1.2, -3.3], \'B\': [NaN, NaN, NaN, -1.2, -3.3]}\n    return out_df\n\n\nprint(df_rolling_kurt())\n'"
examples/dataframe/rolling/dataframe_rolling_max.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef df_rolling_max():\n    df = pd.DataFrame({\'A\': [4, 3, 5, 2, 6], \'B\': [-4, -3, -5, -2, -6]})\n    out_df = df.rolling(3).max()\n\n    # Expect DataFrame of\n    # {\'A\': [NaN, NaN, 5.0, 5.0, 6.0], \'B\': [NaN, NaN, -3.0, -2.0, -2.0]}\n    return out_df\n\n\nprint(df_rolling_max())\n'"
examples/dataframe/rolling/dataframe_rolling_mean.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef df_rolling_mean():\n    df = pd.DataFrame({\'A\': [4, 3, 5, 2, 6], \'B\': [-4, -3, -5, -2, -6]})\n    out_df = df.rolling(3).mean()\n\n    # Expect DataFrame of\n    # {\'A\': [NaN, NaN, 4.000000, 3.333333, 4.333333],\n    #  \'B\': [NaN, NaN, -4.000000, -3.333333, -4.333333]}\n    return out_df\n\n\nprint(df_rolling_mean())\n'"
examples/dataframe/rolling/dataframe_rolling_median.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef df_rolling_median():\n    df = pd.DataFrame({\'A\': [4, 3, 5, 2, 6], \'B\': [-4, -3, -5, -2, -6]})\n    out_df = df.rolling(3).median()\n\n    # Expect DataFrame of\n    # {\'A\': [NaN, NaN, 4.0, 3.0, 5.0], \'B\': [NaN, NaN, -4.0, -3.0, -5.0]}\n    return out_df\n\n\nprint(df_rolling_median())\n'"
examples/dataframe/rolling/dataframe_rolling_min.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef df_rolling_min():\n    df = pd.DataFrame({\'A\': [4, 3, 5, 2, 6], \'B\': [-4, -3, -5, -2, -6]})\n    out_df = df.rolling(3).min()\n\n    # Expect DataFrame of\n    # {\'A\': [NaN, NaN, 3.0, 2.0, 2.0], \'B\': [NaN, NaN, -5.0, -5.0, -6.0]}\n    return out_df\n\n\nprint(df_rolling_min())\n'"
examples/dataframe/rolling/dataframe_rolling_quantile.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef df_rolling_quantile():\n    df = pd.DataFrame({\'A\': [4, 3, 5, 2, 6], \'B\': [-4, -3, -5, -2, -6]})\n    out_df = df.rolling(3).quantile(0.25)\n\n    # Expect DataFrame of\n    # {\'A\': [NaN, NaN, 3.5, 2.5, 3.5], \'B\': [NaN, NaN, -4.5, -4.0, -5.5]}\n    return out_df\n\n\nprint(df_rolling_quantile())\n'"
examples/dataframe/rolling/dataframe_rolling_skew.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef df_rolling_skew():\n    df = pd.DataFrame({\'A\': [4, 3, 5, 2, 6], \'B\': [-4, -3, -5, -2, -6]})\n    out_df = df.rolling(3).skew()\n\n    # Expect DataFrame of\n    # {\'A\': [NaN, NaN, 0.000000, 0.935220, -1.293343],\n    #  \'B\': [NaN, NaN, 0.000000, -0.935220, 1.293343]}\n    return out_df\n\n\nprint(df_rolling_skew())\n'"
examples/dataframe/rolling/dataframe_rolling_std.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef df_rolling_std():\n    df = pd.DataFrame({\'A\': [4, 3, 5, 2, 6], \'B\': [-4, -3, -5, -2, -6]})\n    out_df = df.rolling(3).std()\n\n    # Expect DataFrame of\n    # {\'A\': [NaN, NaN, 1.000000, 1.527525, 2.081666],\n    #  \'B\': [NaN, NaN, 1.000000, 1.527525, 2.081666]}\n    return out_df\n\n\nprint(df_rolling_std())\n'"
examples/dataframe/rolling/dataframe_rolling_sum.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef df_rolling_sum():\n    df = pd.DataFrame({\'A\': [4, 3, 5, 2, 6], \'B\': [-4, -3, -5, -2, -6]})\n    out_df = df.rolling(3).sum()\n\n    # Expect DataFrame of\n    # {\'A\': [NaN, NaN, 12.0, 10.0, 13.0], \'B\': [NaN, NaN, -12.0, -10.0, -13.0]}\n    return out_df\n\n\nprint(df_rolling_sum())\n'"
examples/dataframe/rolling/dataframe_rolling_var.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef df_rolling_var():\n    df = pd.DataFrame({\'A\': [4, 3, 5, 2, 6], \'B\': [-4, -3, -5, -2, -6]})\n    out_df = df.rolling(3).var()\n\n    # Expect DataFrame of\n    # {\'A\': [NaN, NaN, 1.000000, 2.333333, 4.333333],\n    #  \'B\': [NaN, NaN, 1.000000, 2.333333, 4.333333]}\n    return out_df\n\n\nprint(df_rolling_var())\n'"
examples/dataframe/setitem/df_set_existing_column.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\nExpected result:\n     A    B  C\n0 -2.0 -1.0  0\n1 -1.0  1.0  1\n2  0.0  0.0  2\n3  1.0  0.1  3\n4  2.0 -0.1  4\n""""""\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef dataframe_set_existing_column():\n    df = pd.DataFrame({\'A\': [-2., -1., 0., 1., 2.],\n                       \'B\': [2., 1., 0., -1., -2.],\n                       \'C\': [0, 1, 2, 3, 4]})\n\n    return df._set_column(\'B\', [-1., 1., 0., 0.1, -0.1])\n\n\nprint(dataframe_set_existing_column())\n'"
examples/dataframe/setitem/df_set_new_column.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\nExpected result:\n     A  C    B\n0 -2.0  0 -1.0\n1 -1.0  1  1.0\n2  0.0  2  0.0\n3  1.0  3  0.1\n4  2.0  4 -0.1\n""""""\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef dataframe_set_new_column():\n    df = pd.DataFrame({\'A\': [-2., -1., 0., 1., 2.],\n                       \'C\': [0, 1, 2, 3, 4]})\n\n    return df._set_column(\'B\', [-1., 1., 0., 0.1, -0.1])\n\n\nprint(dataframe_set_new_column())\n'"
examples/series/rolling/series_rolling_apply.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_rolling_apply():\n    series = pd.Series([4, 3, 5, 2, 6])  # Series of 4, 3, 5, 2, 6\n\n    def get_median(x):\n        return np.median(x)\n\n    out_series = series.rolling(3).apply(get_median)\n\n    return out_series  # Expect series of NaN, NaN, 4.0, 3.0, 5.0\n\n\nprint(series_rolling_apply())\n'"
examples/series/rolling/series_rolling_corr.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_rolling_corr():\n    series = pd.Series([3, 3, 3, 5, 8])  # Series of 3, 3, 3, 5, 8\n    other = pd.Series([3, 4, 4, 4, 8])  # Series of 3, 4, 4, 4, 8\n    out_series = series.rolling(4).corr(other)\n\n    return out_series  # Expect series of NaN, NaN, NaN, 0.333333, 0.916949\n\n\nprint(series_rolling_corr())\n'"
examples/series/rolling/series_rolling_count.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_rolling_count():\n    series = pd.Series([4, 3, 2, np.nan, 6])  # Series of 4, 3, 2, np.nan, 6\n    out_series = series.rolling(3).count()\n\n    return out_series  # Expect series of 1.0, 2.0, 3.0, 2.0, 2.0\n\n\nprint(series_rolling_count())\n'"
examples/series/rolling/series_rolling_cov.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_rolling_cov():\n    series = pd.Series([3, 3, 3, 5, 8])  # Series of 3, 3, 3, 5, 8\n    other = pd.Series([3, 4, 4, 4, 8])  # Series of 3, 4, 4, 4, 8\n    out_series = series.rolling(4).cov(other)\n\n    return out_series  # Expect series of NaN, NaN, NaN, 0.166667, 4.333333\n\n\nprint(series_rolling_cov())\n'"
examples/series/rolling/series_rolling_kurt.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_rolling_kurt():\n    series = pd.Series([4, 3, 5, 2, 6])  # Series of 4, 3, 5, 2, 6\n    out_series = series.rolling(4).kurt()\n\n    return out_series  # Expect series of NaN, NaN, NaN, -1.2, -3.3\n\n\nprint(series_rolling_kurt())\n'"
examples/series/rolling/series_rolling_max.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_rolling_max():\n    series = pd.Series([4, 3, 5, 2, 6])  # Series of 4, 3, 5, 2, 6\n    out_series = series.rolling(3).max()\n\n    return out_series  # Expect series of NaN, NaN, 5.0, 5.0, 6.0\n\n\nprint(series_rolling_max())\n'"
examples/series/rolling/series_rolling_mean.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_rolling_mean():\n    series = pd.Series([4, 3, 5, 2, 6])  # Series of 4, 3, 5, 2, 6\n    out_series = series.rolling(3).mean()\n\n    return out_series  # Expect series of NaN, NaN, 4.000000, 3.333333, 4.333333\n\n\nprint(series_rolling_mean())\n'"
examples/series/rolling/series_rolling_median.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_rolling_median():\n    series = pd.Series([4, 3, 5, 2, 6])  # Series of 4, 3, 5, 2, 6\n    out_series = series.rolling(3).median()\n\n    return out_series  # Expect series of NaN, NaN, 4.0, 3.0, 5.0\n\n\nprint(series_rolling_median())\n'"
examples/series/rolling/series_rolling_min.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_rolling_min():\n    series = pd.Series([4, 3, 5, 2, 6])  # Series of 4, 3, 5, 2, 6\n    out_series = series.rolling(3).min()\n\n    return out_series  # Expect series of NaN, NaN, 3.0, 2.0, 2.0\n\n\nprint(series_rolling_min())\n'"
examples/series/rolling/series_rolling_quantile.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_rolling_quantile():\n    series = pd.Series([4, 3, 5, 2, 6])  # Series of 4, 3, 5, 2, 6\n    out_series = series.rolling(3).quantile(0.25)\n\n    return out_series  # Expect series of NaN, NaN, 3.5, 2.5, 3.5\n\n\nprint(series_rolling_quantile())\n'"
examples/series/rolling/series_rolling_skew.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_rolling_skew():\n    series = pd.Series([4, 3, 5, 2, 6])  # Series of 4, 3, 5, 2, 6\n    out_series = series.rolling(3).skew()\n\n    return out_series  # Expect series of NaN, NaN, 0.000000, 0.935220, -1.293343\n\n\nprint(series_rolling_skew())\n'"
examples/series/rolling/series_rolling_std.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_rolling_std():\n    series = pd.Series([4, 3, 5, 2, 6])  # Series of 4, 3, 5, 2, 6\n    out_series = series.rolling(3).std()\n\n    return out_series  # Expect series of NaN, NaN, 1.000000, 1.527525, 2.081666\n\n\nprint(series_rolling_std())\n'"
examples/series/rolling/series_rolling_sum.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_rolling_sum():\n    series = pd.Series([4, 3, 5, 2, 6])  # Series of 4, 3, 5, 2, 6\n    out_series = series.rolling(3).sum()\n\n    return out_series  # Expect series of NaN, NaN, 12.0, 10.0, 13.0\n\n\nprint(series_rolling_sum())\n'"
examples/series/rolling/series_rolling_var.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_rolling_var():\n    series = pd.Series([4, 3, 5, 2, 6])  # Series of 4, 3, 5, 2, 6\n    out_series = series.rolling(3).var()\n\n    return out_series  # Expect series of NaN, NaN, 1.000000, 2.333333, 4.333333\n\n\nprint(series_rolling_var())\n'"
examples/series/series_at/series_at_multiple_result.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_at_many_idx():\n    series = pd.Series([5, 4, 3, 2, 1], index=[0, 2, 0, 6, 0])\n\n    return series.at[0]  # Expect array: [5 3 1]\n\n\nprint(series_at_many_idx())\n'"
examples/series/series_at/series_at_single_result.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_at_value():\n    series = pd.Series([5, 4, 3, 2, 1], index=[0, 2, 4, 6, 8])\n\n    return series.at[4]  # Expect array: [3]\n\n\nprint(series_at_value())\n'"
examples/series/series_getitem/series_getitem_bool_array.py,2,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\nExpected Series:\n0    10\n2     8\n3     7\n5     5\n7     3\n8     2\ndtype: int64\n""""""\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_getitem_array():\n    series = pd.Series(np.arange(10, 0, -1))  # Series of 10, 9, ..., 1\n    array = np.array([True, False, True, True, False] * 2)\n\n    return series[array]  # Accessing series by array\n\n\nprint(series_getitem_array())\n'"
examples/series/series_getitem/series_getitem_scalar_multiple_result.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\nExpected Series\n0    5\n0    3\n0    1\ndtype: int64\n""""""\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_getitem_scalar_many_idx():\n    series = pd.Series([5, 4, 3, 2, 1], index=[0, 2, 0, 6, 0])\n\n    return series[0]\n\n\nprint(series_getitem_scalar_many_idx())\n'"
examples/series/series_getitem/series_getitem_scalar_single_result.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\nExpected Series:\n0    10\ndtype: int64\n""""""\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_getitem_scalar():\n    series = pd.Series(np.arange(10, 0, -1))  # Series of 10, 9, ..., 1\n\n    return series[0]  # Accessing series by scalar index\n\n\nprint(series_getitem_scalar())\n'"
examples/series/series_getitem/series_getitem_series.py,2,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\nExpected Series:\n1    9\n6    4\n7    3\n8    2\n9    1\ndtype: int64\n""""""\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_getitem_series():\n    series = pd.Series(np.arange(10, 0, -1))  # Series of 10, 9, ..., 1\n    indices = pd.Series(np.asarray([1, 6, 7, 8, 9]))\n\n    return series[indices]  # Accessing series by series\n\n\nprint(series_getitem_series())\n'"
examples/series/series_getitem/series_getitem_slice.py,1,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\nExpected Series:\n3    7\n4    6\n5    5\n6    4\ndtype: int64\n""""""\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_getitem_slice():\n    series = pd.Series(np.arange(10, 0, -1))  # Series of 10, 9, ..., 1\n\n    return series[3:7]  # Accessing series by slice index\n\n\nprint(series_getitem_slice())\n'"
examples/series/series_iloc/series_iloc_slice.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\nExpected Series:\n0    3\n1    2\ndtype: int64\n""""""\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_iloc_slice():\n    series = pd.Series([5, 4, 3, 2, 1], index=[0, 2, 4, 6, 8])\n\n    return series.iloc[2:4]\n\n\nprint(series_iloc_slice())\n'"
examples/series/series_iloc/series_iloc_value.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_iloc_value():\n    series = pd.Series([5, 4, 3, 2, 1], index=[0, 2, 4, 6, 8])\n\n    return series.iloc[4]  # Expect value: 1\n\n\nprint(series_iloc_value())\n'"
examples/series/series_loc/series_loc_multiple_result.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\nExpected Series:\n0    5\n0    3\n0    1\ndtype: int64\n""""""\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_loc_many_idx():\n    series = pd.Series([5, 4, 3, 2, 1], index=[0, 2, 0, 6, 0])\n\n    return series.loc[0]\n\n\nprint(series_loc_many_idx())\n'"
examples/series/series_loc/series_loc_single_result.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\nExpected Series:\n4    3\ndtype: int64\n""""""\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_loc_value():\n    series = pd.Series([5, 4, 3, 2, 1], index=[0, 2, 4, 6, 8])\n\n    return series.loc[4]\n\n\nprint(series_loc_value())\n'"
examples/series/series_loc/series_loc_slice.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\nExpected Series:\n1    4\n2    3\n3    2\ndtype: int64\n""""""\nimport numpy as np\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_loc_slice():\n    series = pd.Series([5, 4, 3, 2, 1])\n\n    return series.loc[1:3]\n\n\nprint(series_loc_slice())\n'"
examples/series/str/series_str_capitalize.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_str_capitalize():\n    series = pd.Series([\'lower\', \'CAPITALS\', \'this is a sentence\', \'SwApCaSe\'])\n    out_series = series.str.capitalize()\n\n    return out_series  # Expect series of \'Lower\', \'Capitals\', \'This is a sentence\', \'Swapcase\'\n\n\nprint(series_str_capitalize())\n'"
examples/series/str/series_str_casefold.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_str_casefold():\n    series = pd.Series([\'lower\', \'CAPITALS\', \'this is a sentence\', \'SwApCaSe\'])\n    out_series = series.str.casefold()\n\n    return out_series  # Expect series of \'lower\', \'capitals\', \'this is a sentence\', \'swapcase\'\n\n\nprint(series_str_casefold())\n'"
examples/series/str/series_str_center.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_str_center():\n    series = pd.Series([\'dog\', \'foo\', \'bar\'])  # Series of \'dog\', \'foo\', \'bar\'\n    out_series = series.str.center(5, \'*\')\n\n    return out_series  # Expect series of \'*dog*\', \'*foo*\', \'*bar*\'\n\n\nprint(series_str_center())\n'"
examples/series/str/series_str_contains.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_str_contains():\n    series = pd.Series([\'dog\', \'foo\', \'bar\'])\n\n    return series.str.contains(\'o\')  # Expect series of True, True, False\n\n\nprint(series_str_contains())\n'"
examples/series/str/series_str_endswith.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_str_endswith():\n    series = pd.Series([\'foo\', \'bar\', \'foobar\'])  # Series of \'foo\', \'bar\', \'foobar\'\n    out_series = series.str.endswith(\'bar\')\n\n    return out_series  # Expect series of False, True, True\n\n\nprint(series_str_endswith())\n'"
examples/series/str/series_str_find.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_str_find():\n    series = pd.Series([\'foo\', \'bar\', \'foobar\'])  # Series of \'foo\', \'bar\', \'foobar\'\n    out_series = series.str.find(\'bar\')\n\n    return out_series  # Expect series of -1, 0, 3\n\n\nprint(series_str_find())\n'"
examples/series/str/series_str_isalnum.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_str_isalnum():\n    series = pd.Series([\'leopard\', \'Golden Eagle\', \'SNAKE\', \'\'])\n    out_series = series.str.isalnum()\n\n    return out_series  # Expect series of True, False, True, False\n\n\nprint(series_str_isalnum())\n'"
examples/series/str/series_str_isalpha.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_str_isalpha():\n    series = pd.Series([\'leopard\', \'Golden Eagle\', \'SNAKE\', \'\'])\n    out_series = series.str.isalpha()\n\n    return out_series  # Expect series of True, False, True, False\n\n\nprint(series_str_isalpha())\n'"
examples/series/str/series_str_isdecimal.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_str_isdecimal():\n    series = pd.Series([\'23\', \'\xc2\xb3\', \'\xe2\x85\x95\', \'\'])\n    out_series = series.str.isdecimal()\n\n    return out_series  # Expect series of True, False, False, False\n\n\nprint(series_str_isdecimal())\n'"
examples/series/str/series_str_isdigit.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_str_isdigit():\n    series = pd.Series([\'23\', \'\xc2\xb3\', \'\xe2\x85\x95\', \'\'])\n    out_series = series.str.isdigit()\n\n    return out_series  # Expect series of True, True, False, False\n\n\nprint(series_str_isdigit())\n'"
examples/series/str/series_str_islower.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_str_islower():\n    series = pd.Series([\'leopard\', \'Golden Eagle\', \'SNAKE\', \'\'])\n    out_series = series.str.islower()\n\n    return out_series  # Expect series of True, False, False, False\n\n\nprint(series_str_islower())\n'"
examples/series/str/series_str_isnumeric.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_str_isnumeric():\n    series = pd.Series([\'one\', \'one1\', \'1\', \'\'])\n    out_series = series.str.isnumeric()\n\n    return out_series  # Expect series of False, False, True, False\n\n\nprint(series_str_isnumeric())\n'"
examples/series/str/series_str_isspace.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_str_isspace():\n    series = pd.Series([\' \', \' c \', \'  b \', \'     a     \'])\n    out_series = series.str.isspace()\n\n    return out_series  # Expect series of True, False, False, False\n\n\nprint(series_str_isspace())\n'"
examples/series/str/series_str_istitle.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_str_istitle():\n    series = pd.Series([\'Cat\', \'dog\', \'Bird\'])\n    out_series = series.str.istitle()\n\n    return out_series  # Expect series of True, False, True\n\n\nprint(series_str_istitle())\n'"
examples/series/str/series_str_isupper.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_str_isupper():\n    series = pd.Series([\'FOO\', \'BAr\', \'FooBar\'])  # Series of \'FOO\', \'BAr\', \'FooBar\'\n    out_series = series.str.isupper()\n\n    return out_series  # Expect series of True, False, False\n\n\nprint(series_str_isupper())\n'"
examples/series/str/series_str_len.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_str_len():\n    series = pd.Series([\'foo\', \'bar\', \'foobar\'])  # Series of \'foo\', \'bar\', \'foobar\'\n    out_series = series.str.len()\n\n    return out_series  # Expect series of 3, 3, 6\n\n\nprint(series_str_len())\n'"
examples/series/str/series_str_ljust.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_str_ljust():\n    series = pd.Series([\'dog\', \'foo\', \'bar\'])  # Series of \'dog\', \'foo\', \'bar\'\n    out_series = series.str.ljust(5, \'*\')\n\n    return out_series  # Expect series of \'dog**\', \'foo**\', \'bar**\'\n\n\nprint(series_str_ljust())\n'"
examples/series/str/series_str_lower.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_str_lower():\n    series = pd.Series([\'DOG\', \'foo\', \'BaR\'])\n    out_series = series.str.lower()\n\n    return out_series\n\n\nprint(series_str_lower())\n'"
examples/series/str/series_str_lstrip.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_str_lstrip():\n    series = pd.Series([\'1. Ant.  \', \'2. Bee!\\n\', \'3. Cat?\\t\'])\n\n    return series.str.lstrip(\'123.\')\n\n\nprint(series_str_lstrip())\n'"
examples/series/str/series_str_rjust.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_str_rjust():\n    series = pd.Series([\'dog\', \'foo\', \'bar\'])  # Series of \'dog\', \'foo\', \'bar\'\n    out_series = series.str.rjust(5, \'*\')\n\n    return out_series  # Expect series of \'**dog\', \'**foo\', \'**bar\'\n\n\nprint(series_str_rjust())\n'"
examples/series/str/series_str_rstrip.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_str_rstrip():\n    series = pd.Series([\'1. Ant.  \', \'2. Bee!\\n\', \'3. Cat?\\t\'])\n\n    return series.str.rstrip(\'.!? \\n\\t\')\n\n\nprint(series_str_rstrip())\n'"
examples/series/str/series_str_startswith.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_str_startswith():\n    series = pd.Series([\'foo\', \'bar\', \'foobar\'])  # Series of \'foo\', \'bar\', \'foobar\'\n    out_series = series.str.startswith(\'foo\')\n\n    return out_series  # Expect series of True, False, True\n\n\nprint(series_str_startswith())\n'"
examples/series/str/series_str_strip.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_str_strip():\n    series = pd.Series([\'1. Ant.  \', \'2. Bee!\\n\', \'3. Cat?\\t\'])\n\n    return series.str.strip(\'123.!? \\n\\t\')\n\n\nprint(series_str_strip())\n'"
examples/series/str/series_str_swapcase.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_str_swapcase():\n    series = pd.Series([\'lower\', \'CAPITALS\', \'this is a sentence\', \'SwApCaSe\'])\n    out_series = series.str.swapcase()\n\n    return out_series  # Expect series of \'LOWER\', \'capitals\', \'THIS IS A SENTENCE\', \'sWaPcAsE\'\n\n\nprint(series_str_swapcase())\n'"
examples/series/str/series_str_title.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_str_title():\n    series = pd.Series([\'lower\', \'CAPITALS\', \'this is a sentence\', \'SwApCaSe\'])\n    out_series = series.str.title()\n\n    return out_series  # Expect series of \'Lower\', \'Capitals\', \'This Is A Sentence\', \'Swapcase\'\n\n\nprint(series_str_title())\n'"
examples/series/str/series_str_upper.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_str_upper():\n    series = pd.Series([\'lower\', \'CAPITALS\', \'this is a sentence\', \'SwApCaSe\'])\n\n    return series.str.upper()\n\n\nprint(series_str_upper())\n'"
examples/series/str/series_str_zfill.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom numba import njit\n\n\n@njit\ndef series_str_zfill():\n    series = pd.Series([\'dog\', \'foo\', \'bar\'])  # Series of \'dog\', \'foo\', \'bar\'\n    out_series = series.str.zfill(5)\n\n    return out_series  # Expect series of \'00dog\', \'00foo\', \'00bar\'\n\n\nprint(series_str_zfill())\n'"
sdc/datatypes/categorical/__init__.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n'"
sdc/datatypes/categorical/boxing.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nfrom numba.extending import box, unbox, NativeValue\nfrom numba.core import boxing\nfrom numba.core.imputils import lower_constant\nfrom numba.np import arrayobj\nfrom numba import types\n\nfrom . import pandas_support\nfrom .types import (\n    CategoricalDtypeType,\n    Categorical,\n)\n\n\n@box(CategoricalDtypeType)\ndef box_CategoricalDtype(typ, val, c):\n    pd_dtype = pandas_support.as_dtype(typ)\n    return c.pyapi.unserialize(c.pyapi.serialize_object(pd_dtype))\n\n\n@unbox(CategoricalDtypeType)\ndef unbox_CategoricalDtype(typ, val, c):\n    return NativeValue(c.context.get_dummy_value())\n\n\n@box(Categorical)\ndef box_Categorical(typ, val, c):\n    pandas_module_name = c.context.insert_const_string(c.builder.module, ""pandas"")\n    pandas_module = c.pyapi.import_module_noblock(pandas_module_name)\n\n    constructor = c.pyapi.object_getattr_string(pandas_module, ""Categorical"")\n\n    empty_list = c.pyapi.list_new(c.context.get_constant(types.intp, 0))\n    args = c.pyapi.tuple_pack([empty_list])\n    categorical = c.pyapi.call(constructor, args)\n\n    dtype = box_CategoricalDtype(typ.pd_dtype, val, c)\n    c.pyapi.object_setattr_string(categorical, ""_dtype"", dtype)\n\n    codes = boxing.box_array(typ.codes, val, c)\n    c.pyapi.object_setattr_string(categorical, ""_codes"", codes)\n\n    c.pyapi.decref(codes)\n    c.pyapi.decref(dtype)\n    c.pyapi.decref(args)\n    c.pyapi.decref(empty_list)\n    c.pyapi.decref(constructor)\n    c.pyapi.decref(pandas_module)\n    return categorical\n\n\n@unbox(Categorical)\ndef unbox_Categorical(typ, val, c):\n    codes = c.pyapi.object_getattr_string(val, ""codes"")\n    native_value = boxing.unbox_array(typ.codes, codes, c)\n    c.pyapi.decref(codes)\n    return native_value\n\n\n@lower_constant(Categorical)\ndef constant_Categorical(context, builder, ty, pyval):\n    """"""\n    Create a constant Categorical.\n    """"""\n    return arrayobj.constant_array(context, builder, ty.codes, pyval.codes)\n'"
sdc/datatypes/categorical/functions.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nfrom sdc.utilities.utils import sdc_overload_attribute\n\nfrom .types import CategoricalDtypeType\n\n\n@sdc_overload_attribute(CategoricalDtypeType, \'ordered\')\ndef pd_CategoricalDtype_categories_overload(self):\n    ordered = self.ordered\n\n    def impl(self):\n        return ordered\n    return impl\n'"
sdc/datatypes/categorical/init.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\nInit Numba extension for Pandas Categorical.\n""""""\n\nfrom . import types\nfrom . import typeof\nfrom . import models\nfrom . import boxing\nfrom . import pdimpl\nfrom . import rewrites\nfrom . import functions\n\nimport numba\n\n\n# register new types in numba.types for using in objmode\nsetattr(numba.types, ""CategoricalDtype"", types.CategoricalDtypeType)\nsetattr(numba.types, ""Categorical"", types.Categorical)\n'"
sdc/datatypes/categorical/models.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nfrom numba.extending import models\nfrom numba.extending import register_model\n\nfrom .types import (\n    CategoricalDtypeType,\n    Categorical,\n)\n\n\nregister_model(CategoricalDtypeType)(models.OpaqueModel)\nregister_model(Categorical)(models.ArrayModel)\n'"
sdc/datatypes/categorical/pandas_support.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\n\nfrom numba import types\n\nfrom .types import CategoricalDtypeType\n\n\ndef from_dtype(pdtype):\n    """"""\n    Return a Numba Type instance corresponding to the given Pandas *dtype*.\n    NotImplementedError is raised if unsupported Pandas dtypes.\n    """"""\n    # TODO: use issubclass\n    if isinstance(pdtype, pd.CategoricalDtype):\n        if pdtype.categories is None:\n            categories = None\n        else:\n            categories = list(pdtype.categories)\n        return CategoricalDtypeType(categories=categories,\n                                    ordered=pdtype.ordered)\n\n    raise NotImplementedError(""%r cannot be represented as a Numba type""\n                              % (pdtype,))\n\n\ndef as_dtype(nbtype):\n    """"""\n    Return a Pandas *dtype* instance corresponding to the given Numba type.\n    NotImplementedError is raised if no correspondence is known.\n    """"""\n    nbtype = types.unliteral(nbtype)\n    if isinstance(nbtype, CategoricalDtypeType):\n        return pd.CategoricalDtype(categories=nbtype.categories,\n                                   ordered=nbtype.ordered)\n\n    raise NotImplementedError(""%r cannot be represented as a Pandas dtype""\n                              % (nbtype,))\n'"
sdc/datatypes/categorical/pdimpl.py,3,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\n\nfrom numba.extending import overload\nfrom numba.extending import intrinsic\nfrom numba.extending import type_callable\nfrom numba.extending import lower_builtin\nfrom numba import types\nfrom numba import typeof\nfrom numba import objmode\n\nfrom .types import (\n    CategoricalDtypeType,\n    Categorical,\n)\n\nfrom . import pandas_support\n\n\n# Possible alternative implementations:\n# 1. @overload + @intrinsic\n# 2. @type_callable + @lower_builtin\n# They are equivalent. Who is defined firts - has higher priority.\n\n\ndef _reconstruct_CategoricalDtype(dtype):\n    if isinstance(dtype, types.Literal):\n        return dtype.literal_value\n\n    if isinstance(dtype, CategoricalDtypeType):\n        return pandas_support.as_dtype(dtype)\n\n    raise NotImplementedError()\n\n\n@overload(pd.CategoricalDtype)\ndef _CategoricalDtype(categories=None, ordered=None):\n    """"""\n    Implementation of constructor for pandas CategoricalDtype.\n    """"""\n    if isinstance(ordered, types.Literal):\n        ordered_const = ordered.literal_value\n    else:\n        ordered_const = ordered\n\n    def impl(categories=None, ordered=None):\n        return _CategoricalDtype_intrinsic(categories, ordered_const)\n    return impl\n\n\n@intrinsic\ndef _CategoricalDtype_intrinsic(typingctx, categories, ordered):\n    """"""\n    Creates CategoricalDtype object.\n\n    Assertions:\n        categories - Tuple of literal values or None\n        ordered - literal Bool\n    """"""\n    if isinstance(categories, types.NoneType):\n        categories_list = None\n    if isinstance(categories, types.Tuple):\n        categories_list = [c.literal_value for c in categories]\n\n    if isinstance(ordered, types.NoneType):\n        ordered_value = None\n    if isinstance(ordered, types.Literal):\n        ordered_value = ordered.literal_value\n\n    return_type = CategoricalDtypeType(categories_list, ordered_value)\n    sig = return_type(categories, ordered)\n\n    def codegen(context, builder, signature, args):\n        # All CategoricalDtype objects are dummy values in LLVM.\n        # They only exist in the type level.\n        return context.get_dummy_value()\n\n    return sig, codegen\n\n\n# TODO: move to tools\ndef is_categoricaldtype(dtype):\n    if isinstance(dtype, types.Literal) and dtype.literal_value == \'category\':\n        return True\n\n    if isinstance(dtype, CategoricalDtypeType):\n        return True\n\n    return False\n\n\n# @type_callable(pd.CategoricalDtype)\n# def type_CategoricalDtype_constructor(context):\n#     def typer(categories, ordered):\n#         # TODO: check all Literal in categories\n#         if isinstance(categories, types.Tuple) and isinstance(ordered, types.Literal):\n#             categories_list = [c.literal_value for c in categories]\n#             return CategoricalDtypeType(categories_list, ordered.literal_value)\n\n#     return typer\n\n\n# @lower_builtin(pd.CategoricalDtype, types.Any, types.Any)\n# def _CategoricalDtype_constructor(context, builder, sig, args):\n#     # All CategoricalDtype objects are dummy values in LLVM.\n#     # They only exist in the type level.\n#     return context.get_dummy_value()\n\n\n# @type_callable(pd.CategoricalDtype)\n# def type_CategoricalDtype_constructor(context):\n#     def typer(categories):\n#         # TODO: check all Literal in categories\n#         if isinstance(categories, types.Tuple):\n#             categories_list = [c.literal_value for c in categories]\n#             return CategoricalDtypeType(categories_list)\n\n#     return typer\n\n\n# @lower_builtin(pd.CategoricalDtype, types.Any)\n# def _CategoricalDtype_constructor(context, builder, sig, args):\n#     # All CategoricalDtype objects are dummy values in LLVM.\n#     # They only exist in the type level.\n#     return context.get_dummy_value()\n\n\n# TODO: use dtype too\ndef _reconstruct_Categorical(values):\n    values_list = [v.literal_value for v in values]\n    return pd.Categorical(values=values_list)\n\n\n@overload(pd.Categorical)\ndef _Categorical(values, categories=None, ordered=None, dtype=None, fastpath=False):\n    """"""\n    Implementation of constructor for pandas Categorical via objmode.\n    """"""\n    # TODO: support other parameters (only values now)\n\n    ty = typeof(_reconstruct_Categorical(values))\n\n    from textwrap import dedent\n    text = dedent(f""""""\n    def impl(values, categories=None, ordered=None, dtype=None, fastpath=False):\n        with objmode(categorical=""{ty}""):\n            categorical = pd.Categorical(values, categories, ordered, dtype, fastpath)\n        return categorical\n    """""")\n    globals, locals = {\'objmode\': objmode, \'pd\': pd}, {}\n    exec(text, globals, locals)\n    impl = locals[\'impl\']\n    return impl\n\n\n# @type_callable(pd.Categorical)\n# def type_Categorical_constructor(context):\n#     """"""\n#     Similar to @infer_global(np.array).\n#     """"""\n#     def typer(values, categories=None, ordered=None, dtype=None, fastpath=False):\n#         # from numba.core.typing import npydecl\n#         # codes = npydecl.NpArray(context).generic()(values)\n#         categorical = _reconstruct_Categorical(values)\n#         return typeof(categorical)\n\n#     return typer\n\n\n# @lower_builtin(pd.Categorical, types.Any)\n# # @lower_builtin(np.Categorical, types.Any, types.DTypeSpec)\n# def pd_Categorical(context, builder, sig, args):\n#     """"""\n#     Similar to @lower_builtin(np.array, ...).\n#     """"""\n#     from numba.np import arrayobj\n#     codes = sig.return_type.codes\n#     return arrayobj.np_array(context, builder, sig.replace(return_type=codes), args)\n\n\n# via intrinsic\n# @overload(pd.Categorical)\n# def _Categorical(values, categories=None, ordered=None, dtype=None, fastpath=False):\n#     """"""\n#     Implementation of constructor for pandas Categorical.\n#     """"""\n#     def impl(values, categories=None, ordered=None, dtype=None, fastpath=False):\n#         return _Categorical_intrinsic(values, categories, ordered, dtype, fastpath)\n#     return impl\n\n\n# @intrinsic\n# def _Categorical_intrinsic(typingctx, values, categories, ordered, dtype, fastpath):\n#     """"""\n#     Creates Categorical object.\n#     """"""\n#     if isinstance(values, types.Tuple):\n#         values_list = [v.literal_value for v in values]\n#         categorical = pd.Categorical(values=values_list)\n#         return_type = typeof(categorical)\n\n#         def codegen(context, builder, signature, args):\n#             [values] = args\n#             # TODO: can not recall similar function\n#             native_value = boxing.unbox_array(typ.codes, codes, c)\n#             return native_value\n\n#     sig = return_type(values, categories, ordered, dtype, fastpath)\n#     return sig, codegen\n\n#     # return_type = Categorical(dtype=CategoricalDtypeType(), codes=types.Array(types.int8, 1, \'C\'))\n#     # sig = return_type(values)\n\n#     # def codegen(context, builder, signature, args):\n#     #     return context.get_dummy_value()\n\n#     # return sig, codegen\n'"
sdc/datatypes/categorical/rewrites.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom sdc.datatypes.common.rewriteutils import register_tuplify\n\n\nregister_tuplify(pd.CategoricalDtype, \'categories\')\nregister_tuplify(pd.Categorical, \'values\')\n'"
sdc/datatypes/categorical/typeof.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\npandas.CategoricalDtype\n""""""\n\nimport pandas as pd\n\nfrom numba.extending import typeof_impl\nfrom numba.np import numpy_support\nfrom numba import typeof\n\nfrom . import pandas_support\nfrom .types import Categorical\n\n\n@typeof_impl.register(pd.CategoricalDtype)\ndef _typeof_CategoricalDtype(val, c):\n    return pandas_support.from_dtype(val)\n\n\n@typeof_impl.register(pd.Categorical)\ndef _typeof_Categorical(val, c):\n    try:\n        dtype = pandas_support.from_dtype(val.dtype)\n    except NotImplementedError:\n        raise ValueError(""Unsupported Categorical dtype: %s"" % (val.dtype,))\n    codes = typeof(val.codes)\n    return Categorical(dtype=dtype, codes=codes)\n'"
sdc/datatypes/categorical/types.py,3,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\nNumba types for support pandas Categorical.\n""""""\n\nfrom numba import types\n\nimport numpy as np\n\n\n__all__ = [\n    \'CategoricalDtypeType\',\n    \'Categorical\',\n]\n\n\n# TODO: consider renaming to CategoricalDtype b/c Categorical - not CategoricalType\nclass CategoricalDtypeType(types.Opaque):\n    """"""\n    Numba type for pandas CategoricalDtype.\n\n    Contains:\n        categories -> array-like\n        ordered -> bool\n    """"""\n    def __init__(self, categories=None, ordered=None):\n        self.categories = categories\n        self.ordered = ordered\n        super().__init__(name=self.__repr__())\n\n    def __repr__(self):\n        return \'CategoricalDtype(categories={}, ordered={})\'.format(\n            self.categories, self.ordered)\n\n    def __len__(self):\n        return len(self.categories) if self.categories else 0\n\n    @property\n    def dtype(self):\n        # TODO: take dtype from categories array\n        return types.int64\n\n    def int_type(self):\n        """"""\n        Return minimal int type to fit all categories.\n        """"""\n        dtype = types.int64\n        n_cats = len(self.categories)\n        if n_cats < np.iinfo(np.int8).max:\n            dtype = types.int8\n        elif n_cats < np.iinfo(np.int16).max:\n            dtype = types.int16\n        elif n_cats < np.iinfo(np.int32).max:\n            dtype = types.int32\n        return dtype\n\n\n# TODO: make ArrayCompatible. It will make reuse Array boxing, unboxing.\nclass Categorical(types.Type):\n    """"""\n    Numba type for pandas Categorical.\n\n    Contains:\n        codes -> array-like\n        dtype -> CategoricalDtypeType\n    """"""\n    def __init__(self, dtype, codes=None):\n        assert(isinstance(dtype, CategoricalDtypeType))\n        self.pd_dtype = dtype\n        self.codes = codes or types.Array(self.pd_dtype.int_type(), ndim=1, layout=\'C\')\n        # TODO: store dtype for categories values and use it for dtype\n        super().__init__(name=self.__repr__())\n\n    def __repr__(self):\n        def Array__repr__(array):\n            return ""Array({}, {}, {})"".format(\n                self.codes.dtype.__repr__(),\n                self.codes.ndim.__repr__(),\n                self.codes.layout.__repr__()\n            )\n\n        dtype = self.pd_dtype.__repr__()\n        codes = Array__repr__(self.codes)\n        return \'Categorical(dtype={}, codes={})\'.format(dtype, codes)\n\n    @property\n    def categories(self):\n        return self.pd_dtype.categories\n\n    # Properties for model\n\n    @property\n    def ndim(self):\n        return self.codes.ndim\n\n    @property\n    def dtype(self):\n        return self.codes.dtype\n'"
sdc/datatypes/common/__init__.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n'"
sdc/datatypes/common/rewriteutils.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nfrom numba.core.rewrites import register_rewrite, Rewrite\nfrom numba import errors\nfrom numba.core import ir\nfrom numba.core.ir_utils import guard, get_definition\n\n\nclass TuplifyArgs(Rewrite):\n    """"""\n    Base rewrite calls to *callee*. Replaces *arg* from list and set to tuple.\n\n    Redefine callee and arg in subclass.\n    """"""\n\n    # need to be defined in subclasses\n    callee = None\n    arg = None\n    expr_checker = None\n\n    def match_expr(self, expr, func_ir, block, typemap, calltypes):\n        """"""For extended checks in supbclasses.""""""\n        if self.expr_checker:\n            return self.expr_checker(expr, func_ir, block, typemap, calltypes)\n        return True\n\n    def match(self, func_ir, block, typemap, calltypes):\n        self.args = args = []\n        self.block = block\n        for inst in block.find_insts(ir.Assign):\n            if isinstance(inst.value, ir.Expr) and inst.value.op == \'call\':\n                expr = inst.value\n                try:\n                    callee = func_ir.infer_constant(expr.func)\n                except errors.ConstantInferenceError:\n                    continue\n                if callee is self.callee:\n                    if not self.match_expr(expr, func_ir, block, typemap, calltypes):\n                        continue\n\n                    arg_var = None\n                    if len(expr.args):\n                        arg_var = expr.args[0]\n                    elif len(expr.kws) and expr.kws[0][0] == self.arg:\n                        arg_var = expr.kws[0][1]\n                    if arg_var:\n                        arg_var_def = guard(get_definition, func_ir, arg_var)\n                        if arg_var_def and arg_var_def.op in (\'build_list\', \'build_set\'):\n                            args.append(arg_var_def)\n        return len(args) > 0\n\n    def apply(self):\n        """"""\n        Replace list expression with tuple.\n        """"""\n        block = self.block\n        for inst in block.body:\n            if isinstance(inst, ir.Assign) and inst.value in self.args:\n                inst.value.op = \'build_tuple\'\n        return block\n\n\ndef register_tuplify(_callee, _arg, _expr_checker=None):\n    @register_rewrite(\'before-inference\')\n    class Tuplifier(TuplifyArgs):\n        callee = _callee\n        arg = _arg\n        expr_checker = _expr_checker\n'"
sdc/datatypes/pandas_series_functions/__init__.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n'"
sdc/datatypes/pandas_series_functions/apply.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numpy\nimport pandas\nfrom numba import prange, types\nfrom numba.core.registry import cpu_target\n\nfrom sdc.hiframes.pd_series_ext import SeriesType\nfrom sdc.utilities.utils import sdc_overload_method\n\nfrom sdc.utilities.sdc_typing_utils import TypeChecker\n\n\n@sdc_overload_method(SeriesType, \'apply\')\ndef hpat_pandas_series_apply(self, func, convert_dtype=True, args=()):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.apply\n\n    Limitations\n    -----------\n    - Parameters ``convert_dtype`` and ``args`` are currently unsupported by Intel Scalable Dataframe Compiler.\n    - ``function`` returning a Series object is currently unsupported by Intel Scalable Dataframe Compiler.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_apply.py\n        :language: python\n        :lines: 33-\n        :caption: Square the values by defining a function and passing it as an argument to `apply()`.\n        :name: ex_series_apply\n\n    .. command-output:: python ./series/series_apply.py\n        :cwd: ../../../examples\n\n    .. literalinclude:: ../../../examples/series/series_apply_lambda.py\n        :language: python\n        :lines: 33-\n        :caption: Square the values by passing an anonymous function as an argument to `apply()`.\n        :name: ex_series_apply_lambda\n\n    .. command-output:: python ./series/series_apply_lambda.py\n        :cwd: ../../../examples\n\n    .. literalinclude:: ../../../examples/series/series_apply_log.py\n        :language: python\n        :lines: 33-\n        :caption: Use a function from the Numpy library.\n        :name: ex_series_apply_log\n\n    .. command-output:: python ./series/series_apply_log.py\n        :cwd: ../../../examples\n\n    .. seealso::\n\n        :ref:`Series.map <pandas.Series.map>`\n            For element-wise operations.\n        :ref:`Series.agg <pandas.Series.agg>`\n            Only perform aggregating type operations.\n        :ref:`Series.transform <pandas.transform>`\n            Only perform transforming type operations.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n\n    .. only:: developer\n        Test: python -m sdc.runtests sdc.tests.test_series_apply\n    """"""\n\n    ty_checker = TypeChecker(""Method apply()."")\n    ty_checker.check(self, SeriesType)\n    ty_checker.check(func, types.Callable)\n\n    func_args = [self.dtype]\n    if not isinstance(args, types.Omitted):\n        func_args.extend(args)\n\n    sig = func.get_call_type(cpu_target.typing_context, func_args, {})\n    output_type = sig.return_type\n\n    def impl(self, func, convert_dtype=True, args=()):\n        input_arr = self._data\n        length = len(input_arr)\n\n        output_arr = numpy.empty(length, dtype=output_type)\n\n        for i in prange(length):\n            # Numba issue https://github.com/numba/numba/issues/5065\n            # output_arr[i] = func(input_arr[i], *args)\n            output_arr[i] = func(input_arr[i])\n\n        return pandas.Series(output_arr, index=self._index, name=self._name)\n\n    return impl\n'"
sdc/datatypes/pandas_series_functions/map.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numpy\nimport pandas\nfrom numba import prange, types\nfrom numba.core.registry import cpu_target\n\nfrom sdc.hiframes.pd_series_ext import SeriesType\nfrom sdc.utilities.utils import sdc_overload_method\n\nfrom sdc.utilities.sdc_typing_utils import TypeChecker\n\n\n@sdc_overload_method(SeriesType, \'map\')\ndef hpat_pandas_series_map(self, arg, na_action=None):\n    """"""\n    Intel Scalable Dataframe Compiler User Guide\n    ********************************************\n\n    Pandas API: pandas.Series.map\n\n    Limitations\n    -----------\n    - Series data types String is currently unsupported by Intel Scalable Dataframe Compiler.\n    - ``arg`` as Series is currently unsupported by Intel Scalable Dataframe Compiler.\n    - ``arg`` as function should return scalar. Other types \\\n        are currently unsupported by Intel Scalable Dataframe Compiler.\n    - ``na_action`` is currently unsupported by Intel Scalable Dataframe Compiler.\n\n    Examples\n    --------\n    .. literalinclude:: ../../../examples/series/series_map.py\n       :language: python\n       :lines: 36-\n       :caption: `map()` accepts a function.\n       :name: ex_series_map\n\n    .. command-output:: python ./series/series_map.py\n       :cwd: ../../../examples\n\n    .. seealso::\n\n        :ref:`Series.map <pandas.Series.apply>`\n            For applying more complex functions on a Series.\n        :ref:`DataFrame.apply <pandas.DataFrame.apply>`\n            Apply a function row-/column-wise.\n        :ref:`DataFrame.applymap <pandas.DataFrame.applymap>`\n            Apply a function elementwise on a whole DataFrame.\n\n    Intel Scalable Dataframe Compiler Developer Guide\n    *************************************************\n\n    .. only:: developer\n        Test: python -m sdc.runtests sdc.tests.test_series -k map\n    """"""\n\n    ty_checker = TypeChecker(""Method map()."")\n    ty_checker.check(self, SeriesType)\n\n    if isinstance(arg, types.Callable):\n        sig = arg.get_call_type(cpu_target.typing_context, [self.dtype], {})\n        output_type = sig.return_type\n\n        def impl(self, arg, na_action=None):\n            input_arr = self._data\n            length = len(input_arr)\n\n            output_arr = numpy.empty(length, dtype=output_type)\n\n            for i in prange(length):\n                output_arr[i] = arg(input_arr[i])\n\n            return pandas.Series(output_arr, index=self._index, name=self._name)\n\n        return impl\n\n    if isinstance(arg, types.DictType):\n        output_type = self.dtype\n\n        def impl(self, arg, na_action=None):\n            input_arr = self._data\n            length = len(input_arr)\n\n            output_arr = numpy.empty(length, dtype=output_type)\n\n            for i in prange(length):\n                output_arr[i] = arg.get(input_arr[i], numpy.nan)\n\n            return pandas.Series(output_arr, index=self._index, name=self._name)\n\n        return impl\n'"
sdc/datatypes/series/__init__.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n'"
sdc/datatypes/series/boxing.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nfrom numba.core.imputils import lower_constant\nfrom numba.core import cgutils\n\nfrom .types import SeriesType\n\n\n@lower_constant(SeriesType)\ndef constant_Series(context, builder, ty, pyval):\n    """"""\n    Create a constant Series.\n\n    See @unbox(SeriesType)\n    """"""\n    series = cgutils.create_struct_proxy(ty)(context, builder)\n    series.data = _constant_Series_data(context, builder, ty, pyval)\n    # TODO: index and name\n    return series._getvalue()\n\n\ndef _constant_Series_data(context, builder, ty, pyval):\n    """"""\n    Create a constant for Series data.\n\n    Mostly reuses constant creation for pandas arrays.\n    """"""\n\n    from ..categorical.types import CategoricalDtypeType\n\n    if isinstance(ty.dtype, CategoricalDtypeType):\n        from ..categorical.boxing import constant_Categorical\n        return constant_Categorical(context, builder, ty.data, pyval.array)\n\n    raise NotImplementedError()\n'"
sdc/datatypes/series/init.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\nInit Numba extension for Pandas Series.\n""""""\n\nfrom . import types\nfrom . import boxing\nfrom . import pdimpl\nfrom . import rewrites\n\nimport numba\n\n\n# register Series in numba.types for using in objmode\nsetattr(numba.types, \'series\', types.SeriesType)\n'"
sdc/datatypes/series/pdimpl.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\n\nfrom numba import typeof\nfrom numba import types\nfrom numba import objmode\n\nfrom ..categorical.pdimpl import _reconstruct_CategoricalDtype\n\n\ndef _reconstruct_Series(data, dtype):\n    values_list = [v.literal_value for v in data]\n    dtype = _reconstruct_CategoricalDtype(dtype)\n    return pd.Series(data=values_list, dtype=dtype)\n\n\ndef _Series_category(data=None, index=None, dtype=None, name=None, copy=False, fastpath=False):\n    """"""\n    Implementation of constructor for pandas Series via objmode.\n    """"""\n    # TODO: support other parameters (only data and dtype now)\n\n    ty = typeof(_reconstruct_Series(data, dtype))\n\n    from textwrap import dedent\n    text = dedent(f""""""\n    def impl(data=None, index=None, dtype=None, name=None, copy=False, fastpath=False):\n        with objmode(series=""{ty}""):\n            series = pd.Series(data, index, dtype, name, copy, fastpath)\n        return series\n    """""")\n    globals, locals = {\'objmode\': objmode, \'pd\': pd}, {}\n    exec(text, globals, locals)\n    impl = locals[\'impl\']\n    return impl\n'"
sdc/datatypes/series/rewrites.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport pandas as pd\nfrom sdc.datatypes.common.rewriteutils import register_tuplify\n\nfrom numba.core import ir\nfrom numba.core.ir_utils import guard, get_definition\n\n\ndef check_dtype_is_categorical(self, expr, func_ir, block, typemap, calltypes):\n    dtype_var = None\n    for name, var in expr.kws:\n        if name == \'dtype\':\n            dtype_var = var\n    if not dtype_var:\n        return False\n\n    dtype_var_def = guard(get_definition, func_ir, dtype_var)\n    is_alias = isinstance(dtype_var_def, ir.Const) and dtype_var_def.value == \'category\'\n    is_categoricaldtype = (hasattr(dtype_var_def, \'func\') and\n                           func_ir.infer_constant(dtype_var_def.func) == pd.CategoricalDtype)\n    if not (is_alias or is_categoricaldtype):\n        return False\n\n    return True\n\n\ndef expr_checker(self, expr, func_ir, block, typemap, calltypes):\n    return check_dtype_is_categorical(self, expr, func_ir, block, typemap, calltypes)\n\n\nregister_tuplify(pd.Series, \'data\', expr_checker)\n'"
sdc/datatypes/series/types.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\n""""""\nNumba types for support pandas Series.\n""""""\n\n__all__ = [\n    \'SeriesType\',\n]\n\nfrom sdc.hiframes.pd_series_type import SeriesType\n'"
sdc/extensions/indexes/__init__.py,0,"b'# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n'"
sdc/extensions/indexes/range_index_ext.py,1,"b'# -*- coding: utf-8 -*-\n# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numba\nimport numpy as np\nimport operator\nimport pandas as pd\n\nfrom numba import types\nfrom numba.core import cgutils\nfrom numba.extending import (typeof_impl, NativeValue, intrinsic, box, unbox, lower_builtin)\nfrom numba.core.typing.templates import signature\nfrom numba.core.imputils import impl_ret_new_ref, impl_ret_untracked, iterator_impl\n\nfrom sdc.datatypes.range_index_type import RangeIndexType, RangeIndexDataType\nfrom sdc.datatypes.common_functions import SDCLimitation\nfrom sdc.utilities.utils import sdc_overload, sdc_overload_attribute, sdc_overload_method\nfrom sdc.utilities.sdc_typing_utils import TypeChecker\n\n\ndef _check_dtype_param_type(dtype):\n    """""" Returns True is dtype is a valid type for dtype parameter and False otherwise.\n        Used in RangeIndex ctor and other methods that take dtype parameter. """"""\n\n    valid_dtype_types = (types.NoneType, types.Omitted, types.UnicodeType, types.NumberClass)\n    return isinstance(dtype, valid_dtype_types) or dtype is None\n\n\n@intrinsic\ndef init_range_index(typingctx, data, name=None):\n\n    name = types.none if name is None else name\n    is_named = False if name is types.none else True\n\n    def codegen(context, builder, sig, args):\n        data_val, name_val = args\n        # create series struct and store values\n        range_index = cgutils.create_struct_proxy(\n            sig.return_type)(context, builder)\n\n        range_index.data = data_val\n\n        if is_named:\n            if isinstance(name, types.StringLiteral):\n                range_index.name = numba.unicode.make_string_from_constant(\n                    context, builder, types.unicode_type, name.literal_value)\n            else:\n                range_index.name = name_val\n\n        if context.enable_nrt:\n            context.nrt.incref(builder, sig.args[0], data_val)\n            if is_named:\n                context.nrt.incref(builder, sig.args[1], name_val)\n\n        return range_index._getvalue()\n\n    ret_typ = RangeIndexType(is_named)\n    sig = signature(ret_typ, data, name)\n    return sig, codegen\n\n\n@sdc_overload(pd.RangeIndex)\ndef pd_range_index_overload(start=None, stop=None, step=None, dtype=None, copy=False, name=None, fastpath=None):\n\n    _func_name = \'pd.RangeIndex().\'\n    ty_checker = TypeChecker(_func_name)\n\n    if not (isinstance(copy, types.Omitted) or copy is False):\n        raise SDCLimitation(f""{_func_name} Unsupported parameter. Given \'copy\': {copy}"")\n\n    if not (isinstance(copy, types.Omitted) or fastpath is None):\n        raise SDCLimitation(f""{_func_name} Unsupported parameter. Given \'fastpath\': {fastpath}"")\n\n    dtype_is_np_int64 = dtype is types.NumberClass(types.int64)\n    if not _check_dtype_param_type(dtype):\n        ty_checker.raise_exc(dtype, \'int64 dtype\', \'dtype\')\n\n    # TODO: support ensure_python_int from pandas.core.dtype.common to handle integers as float params\n    if not (isinstance(start, (types.NoneType, types.Omitted, types.Integer)) or start is None):\n        ty_checker.raise_exc(start, \'number or none\', \'start\')\n    if not (isinstance(stop, (types.NoneType, types.Omitted, types.Integer)) or stop is None):\n        ty_checker.raise_exc(stop, \'number or none\', \'stop\')\n    if not (isinstance(step, (types.NoneType, types.Omitted, types.Integer)) or step is None):\n        ty_checker.raise_exc(step, \'number or none\', \'step\')\n\n    if not (isinstance(name, (types.NoneType, types.Omitted, types.StringLiteral, types.UnicodeType)) or name is None):\n        ty_checker.raise_exc(name, \'string or none\', \'name\')\n\n    if ((isinstance(start, (types.NoneType, types.Omitted)) or start is None)\n            and (isinstance(stop, (types.NoneType, types.Omitted)) or stop is None)\n            and (isinstance(step, (types.NoneType, types.Omitted)) or step is None)):\n        def pd_range_index_ctor_dummy_impl(\n                start=None, stop=None, step=None, dtype=None, copy=False, name=None, fastpath=None):\n            raise TypeError(""RangeIndex(...) must be called with integers"")\n\n        return pd_range_index_ctor_dummy_impl\n\n    def pd_range_index_ctor_impl(start=None, stop=None, step=None, dtype=None, copy=False, name=None, fastpath=None):\n\n        if not (dtype is None\n                or dtype == \'int64\'\n                or dtype_is_np_int64):\n            raise TypeError(""Invalid to pass a non-int64 dtype to RangeIndex"")\n\n        _start = types.int64(start) if start is not None else types.int64(0)\n\n        if stop is None:\n            _start, _stop = types.int64(0), types.int64(start)\n        else:\n            _stop = types.int64(stop)\n\n        _step = types.int64(step) if step is not None else types.int64(1)\n        if _step == 0:\n            raise ValueError(""Step must not be zero"")\n\n        return init_range_index(range(_start, _stop, _step), name)\n\n    return pd_range_index_ctor_impl\n\n\n@typeof_impl.register(pd.RangeIndex)\ndef typeof_range_index(val, c):\n    is_named = val.name is not None\n    return RangeIndexType(is_named=is_named)\n\n\n@box(RangeIndexType)\ndef box_range_index(typ, val, c):\n\n    mod_name = c.context.insert_const_string(c.builder.module, ""pandas"")\n    pd_class_obj = c.pyapi.import_module_noblock(mod_name)\n\n    range_index = numba.cgutils.create_struct_proxy(\n        typ)(c.context, c.builder, val)\n    range_index_data = numba.cgutils.create_struct_proxy(\n        RangeIndexDataType)(c.context, c.builder, range_index.data)\n\n    start = c.pyapi.from_native_value(types.int64, range_index_data.start)\n    stop = c.pyapi.from_native_value(types.int64, range_index_data.stop)\n    step = c.pyapi.from_native_value(types.int64, range_index_data.step)\n\n    # dtype and copy params are not stored so use default values\n    dtype = c.pyapi.make_none()\n    copy = c.pyapi.bool_from_bool(\n        c.context.get_constant(types.bool_, False)\n    )\n\n    if typ.is_named:\n        name = c.pyapi.from_native_value(types.unicode_type, range_index.name)\n    else:\n        name = c.pyapi.make_none()\n\n    res = c.pyapi.call_method(pd_class_obj, ""RangeIndex"", (start, stop, step, dtype, copy, name))\n\n    c.pyapi.decref(start)\n    c.pyapi.decref(stop)\n    c.pyapi.decref(step)\n    c.pyapi.decref(dtype)\n    c.pyapi.decref(copy)\n    c.pyapi.decref(name)\n    c.pyapi.decref(pd_class_obj)\n    return res\n\n\n@unbox(RangeIndexType)\ndef unbox_range_index(typ, val, c):\n    start_obj = c.pyapi.object_getattr_string(val, ""start"")\n    stop_obj = c.pyapi.object_getattr_string(val, ""stop"")\n    step_obj = c.pyapi.object_getattr_string(val, ""step"")\n\n    # TODO: support range unboxing with reference to parent in Numba?\n    range_index = cgutils.create_struct_proxy(typ)(c.context, c.builder)\n    range_index_data = cgutils.create_struct_proxy(RangeIndexDataType)(c.context, c.builder)\n    range_index_data.start = c.pyapi.long_as_longlong(start_obj)\n    range_index_data.stop = c.pyapi.long_as_longlong(stop_obj)\n    range_index_data.step = c.pyapi.long_as_longlong(step_obj)\n    range_index.data = range_index_data._getvalue()\n\n    if typ.is_named:\n        name_obj = c.pyapi.object_getattr_string(val, ""name"")\n        range_index.name = numba.unicode.unbox_unicode_str(\n            types.unicode_type, name_obj, c).value\n        c.pyapi.decref(name_obj)\n\n    c.pyapi.decref(start_obj)\n    c.pyapi.decref(stop_obj)\n    c.pyapi.decref(step_obj)\n    is_error = cgutils.is_not_null(c.builder, c.pyapi.err_occurred())\n    return NativeValue(range_index._getvalue(), is_error=is_error)\n\n\n@sdc_overload_attribute(RangeIndexType, \'start\')\ndef pd_range_index_start_overload(self):\n    if not isinstance(self, RangeIndexType):\n        return None\n\n    def pd_range_index_start_impl(self):\n        return self._data.start\n\n    return pd_range_index_start_impl\n\n\n@sdc_overload_attribute(RangeIndexType, \'stop\')\ndef pd_range_index_stop_overload(self):\n    if not isinstance(self, RangeIndexType):\n        return None\n\n    def pd_range_index_stop_impl(self):\n        return self._data.stop\n\n    return pd_range_index_stop_impl\n\n\n@sdc_overload_attribute(RangeIndexType, \'step\')\ndef pd_range_index_step_overload(self):\n    if not isinstance(self, RangeIndexType):\n        return None\n\n    def pd_range_index_step_impl(self):\n        return self._data.step\n\n    return pd_range_index_step_impl\n\n\n@sdc_overload_attribute(RangeIndexType, \'name\')\ndef pd_range_index_name_overload(self):\n    if not isinstance(self, RangeIndexType):\n        return None\n\n    is_named_index = self.is_named\n\n    def pd_range_index_name_impl(self):\n        if is_named_index == True:  # noqa\n            return self._name\n        else:\n            return None\n\n    return pd_range_index_name_impl\n\n\n@sdc_overload_attribute(RangeIndexType, \'dtype\')\ndef pd_range_index_dtype_overload(self):\n    if not isinstance(self, RangeIndexType):\n        return None\n\n    range_index_dtype = self.dtype\n\n    def pd_range_index_dtype_impl(self):\n        return range_index_dtype\n\n    return pd_range_index_dtype_impl\n\n\n@sdc_overload_attribute(RangeIndexType, \'values\')\ndef pd_range_index_values_overload(self):\n    if not isinstance(self, RangeIndexType):\n        return None\n\n    def pd_range_index_values_impl(self):\n        return np.arange(self.start, self.stop, self.step)\n\n    return pd_range_index_values_impl\n\n\n@sdc_overload(len)\ndef pd_range_index_len_overload(self):\n    if not isinstance(self, RangeIndexType):\n        return None\n\n    def pd_range_index_len_impl(self):\n        return len(self._data)\n\n    return pd_range_index_len_impl\n\n\n@sdc_overload(operator.contains)\ndef pd_range_index_contains_overload(self, val):\n    if not isinstance(self, RangeIndexType):\n        return None\n\n    def pd_range_index_contains_impl(self, val):\n        return val in self._data\n\n    return pd_range_index_contains_impl\n\n\n@sdc_overload_method(RangeIndexType, \'copy\')\ndef pd_range_index_copy_overload(self, name=None, deep=False, dtype=None):\n    if not isinstance(self, RangeIndexType):\n        return None\n\n    _func_name = \'Method copy().\'\n    ty_checker = TypeChecker(_func_name)\n\n    if not (isinstance(name, (types.NoneType, types.Omitted, types.UnicodeType)) or name is None):\n        ty_checker.raise_exc(name, \'string or none\', \'name\')\n\n    if not (isinstance(deep, (types.Omitted, types.Boolean)) or deep is False):\n        ty_checker.raise_exc(deep, \'boolean\', \'deep\')\n\n    if not _check_dtype_param_type(dtype):\n        ty_checker.raise_exc(dtype, \'int64 dtype\', \'dtype\')\n\n    name_is_none = isinstance(name, (types.NoneType, types.Omitted)) or name is None\n    keep_name = name_is_none and self.is_named\n    def pd_range_index_copy_impl(self, name=None, deep=False, dtype=None):\n\n        _name = self._name if keep_name == True else name  # noqa\n        return init_range_index(self._data, _name)\n\n    return pd_range_index_copy_impl\n\n\n@sdc_overload(operator.getitem)\ndef pd_range_index_getitem_overload(self, idx):\n    if not isinstance(self, RangeIndexType):\n        return None\n\n    _func_name = \'Operator getitem().\'\n    ty_checker = TypeChecker(_func_name)\n\n    # TO-DO: extend getitem to support other indexers (Arrays, Lists, etc)\n    # for Arrays and Lists it requires Int64Index class as return value\n    if not isinstance(idx, (types.Integer, types.SliceType)):\n        ty_checker.raise_exc(idx, \'integer\', \'idx\')\n\n    if isinstance(idx, types.Integer):\n        def pd_range_index_getitem_impl(self, idx):\n            range_len = len(self._data)\n            idx = (range_len + idx) if idx < 0 else idx\n            if (idx < 0 or idx >= range_len):\n                raise IndexError(""RangeIndex.getitem: index is out of bounds"")\n            return self.start + self.step * idx\n\n        return pd_range_index_getitem_impl\n\n    if isinstance(idx, types.SliceType):\n        def pd_range_index_getitem_impl(self, idx):\n            fix_start, fix_stop, fix_step = idx.indices(len(self._data))\n            return pd.RangeIndex(\n                self.start + self.step * fix_start,\n                self.start + self.step * fix_stop,\n                self.step * fix_step,\n                name=self._name\n            )\n\n        return pd_range_index_getitem_impl\n'"
sdc/tests/categorical/__init__.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nfrom . import test_categorical\nfrom . import test_categoricaldtype\nfrom . import test_series_category\n'"
sdc/tests/categorical/test_categorical.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nfrom sdc.tests.test_base import TestCase\n\nimport pandas as pd\nimport numba as nb\nfrom numba import types\n\nfrom sdc.datatypes.categorical.types import (\n    Categorical,\n    CategoricalDtypeType,\n)\n\n\nclass CategoricalTest(TestCase):\n\n    def _pd_value(self):\n        return pd.Categorical(values=[1, 2, 3, 2, 1])\n\n    def test_typeof(self):\n        pd_value = self._pd_value()\n        nb_type = nb.typeof(pd_value)\n\n        assert(isinstance(nb_type, Categorical))\n        assert(nb_type.pd_dtype == CategoricalDtypeType(categories=[1, 2, 3], ordered=False))\n        assert(nb_type.codes == types.Array(dtype=types.int8, ndim=1, layout=\'C\', readonly=True))\n\n    def test_unboxing(self):\n        @nb.njit\n        def func(c):\n            pass\n\n        pd_value = self._pd_value()\n        func(pd_value)\n\n    def test_boxing(self):\n        @nb.njit\n        def func(c):\n            return c\n\n        pd_value = self._pd_value()\n        boxed = func(pd_value)\n        assert(boxed.equals(pd_value))\n\n    def test_lowering(self):\n        pd_value = self._pd_value()\n\n        @nb.njit\n        def func():\n            return pd_value\n\n        boxed = func()\n        assert(boxed.equals(pd_value))\n\n    def test_constructor(self):\n        @nb.njit\n        def func():\n            return pd.Categorical(values=(1, 2, 3, 2, 1))\n\n        boxed = func()\n        assert(boxed.equals(self._pd_value()))\n\n    def test_constructor_values_list(self):\n        @nb.njit\n        def func():\n            return pd.Categorical(values=[1, 2, 3, 2, 1])\n\n        boxed = func()\n        assert(boxed.equals(self._pd_value()))\n'"
sdc/tests/categorical/test_categoricaldtype.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nfrom sdc.tests.test_base import TestCase\n\nimport pandas as pd\nimport numba as nb\n\nfrom sdc.datatypes.categorical.types import CategoricalDtypeType\n\n\nclass CategoricalDtypeTest(TestCase):\n\n    def _pd_dtype(self, ordered=True):\n        return pd.CategoricalDtype(categories=[\'b\', \'a\'], ordered=ordered)\n\n    def test_typeof(self):\n        pd_dtype = self._pd_dtype()\n        nb_dtype = nb.typeof(pd_dtype)\n\n        assert(isinstance(nb_dtype, CategoricalDtypeType))\n        assert(nb_dtype.categories == list(pd_dtype.categories))\n        assert(nb_dtype.ordered == pd_dtype.ordered)\n\n    def test_unboxing(self):\n        @nb.njit\n        def func(c):\n            pass\n\n        pd_dtype = self._pd_dtype()\n        func(pd_dtype)\n\n    def test_boxing(self):\n        @nb.njit\n        def func(c):\n            return c\n\n        pd_dtype = self._pd_dtype()\n        boxed = func(pd_dtype)\n        assert(boxed == pd_dtype)\n\n    def test_lowering(self):\n        pd_dtype = self._pd_dtype()\n\n        @nb.njit\n        def func():\n            return pd_dtype\n\n        boxed = func()\n        assert(boxed == pd_dtype)\n\n    def test_constructor(self):\n        @nb.njit\n        def func():\n            return pd.CategoricalDtype(categories=(\'b\', \'a\'), ordered=True)\n\n        boxed = func()\n        assert(boxed == self._pd_dtype())\n\n    def test_constructor_categories_list(self):\n        @nb.njit\n        def func():\n            return pd.CategoricalDtype(categories=[\'b\', \'a\'], ordered=True)\n\n        boxed = func()\n        assert(boxed == self._pd_dtype())\n\n    def test_constructor_categories_set(self):\n        @nb.njit\n        def func():\n            return pd.CategoricalDtype(categories={\'b\', \'a\'}, ordered=True)\n\n        boxed = func()\n        assert(boxed == self._pd_dtype())\n\n    def test_constructor_no_order(self):\n        @nb.njit\n        def func():\n            return pd.CategoricalDtype(categories=(\'b\', \'a\'))\n\n        boxed = func()\n        assert(boxed == self._pd_dtype(ordered=False))\n\n    def test_constructor_no_categories(self):\n        @nb.njit\n        def func():\n            return pd.CategoricalDtype()\n\n        boxed = func()\n        expected = pd.CategoricalDtype(ordered=None)\n        assert(boxed == expected)\n        assert(boxed.categories == expected.categories)\n        assert(boxed.ordered == expected.ordered)\n\n    def test_attribute_ordered(self):\n        @nb.njit\n        def func(c):\n            return c.ordered\n\n        pd_dtype = self._pd_dtype()\n        ordered = func(pd_dtype)\n        assert(ordered == pd_dtype.ordered)\n\n\nif __name__ == ""__main__"":\n    import unittest\n    unittest.main()\n'"
sdc/tests/categorical/test_series_category.py,0,"b'# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nfrom sdc.tests.test_base import TestCase\n\nimport pandas as pd\nimport numba as nb\nfrom numba import types\n\nfrom sdc.types import (\n    SeriesType,\n    CategoricalDtypeType,\n    Categorical,\n)\n\n\nclass SeriesCategoryTest(TestCase):\n    """"""\n    Test for pandas Series with CategoricalDtype.\n    """"""\n\n    def _pd_value(self):\n        return pd.Series(data=[1, 2, 3, 2, 1], dtype=\'category\')\n\n    def test_typeof(self):\n        pd_value = self._pd_value()\n        nb_type = nb.typeof(pd_value)\n\n        assert(isinstance(nb_type, SeriesType))\n        assert(nb_type.dtype == CategoricalDtypeType(categories=[1, 2, 3], ordered=False))\n        assert(nb_type.index == types.none)\n        assert(nb_type.data == Categorical(CategoricalDtypeType(categories=[1, 2, 3], ordered=False)))\n\n    def test_unboxing(self):\n        @nb.njit\n        def func(c):\n            pass\n\n        pd_value = self._pd_value()\n        func(pd_value)\n\n    def test_boxing(self):\n        @nb.njit\n        def func(c):\n            return c\n\n        pd_value = self._pd_value()\n        boxed = func(pd_value)\n        assert(boxed.equals(pd_value))\n\n    def test_lowering(self):\n        pd_value = self._pd_value()\n\n        @nb.njit\n        def func():\n            return pd_value\n\n        boxed = func()\n        assert(boxed.equals(pd_value))\n\n    def test_constructor(self):\n        @nb.njit\n        def func():\n            return pd.Series(data=(1, 2, 3, 2, 1), dtype=\'category\')\n\n        boxed = func()\n        assert(boxed.equals(self._pd_value()))\n\n    def test_constructor_list(self):\n        @nb.njit\n        def func():\n            return pd.Series(data=[1, 2, 3, 2, 1], dtype=\'category\')\n\n        boxed = func()\n        assert(boxed.equals(self._pd_value()))\n\n    def test_constructor_CategoricalDtype(self):\n        @nb.njit\n        def func():\n            return pd.Series(data=(1, 2, 3, 2, 1), dtype=pd.CategoricalDtype(categories=(1, 2, 3)))\n\n        boxed = func()\n        assert(boxed.equals(self._pd_value()))\n\n    def test_constructor_CategoricalDtype_list(self):\n        @nb.njit\n        def func():\n            return pd.Series(data=[1, 2, 3, 2, 1], dtype=pd.CategoricalDtype(categories=[1, 2, 3]))\n\n        boxed = func()\n        assert(boxed.equals(self._pd_value()))\n'"
sdc/tests/tests_perf/__init__.py,0,b'from sdc.tests.tests_perf.test_perf_df import *\nfrom sdc.tests.tests_perf.test_perf_df_rolling import *\nfrom sdc.tests.tests_perf.test_perf_df_groupby import *\nfrom sdc.tests.tests_perf.test_perf_numpy import *\nfrom sdc.tests.tests_perf.test_perf_read_csv import *\nfrom sdc.tests.tests_perf.test_perf_series import *\nfrom sdc.tests.tests_perf.test_perf_series_operators import *\nfrom sdc.tests.tests_perf.test_perf_series_rolling import *\nfrom sdc.tests.tests_perf.test_perf_series_str import *\nfrom sdc.tests.tests_perf.test_perf_unicode import *\n'
sdc/tests/tests_perf/data_generator.py,14,"b'import numpy as np\nimport pandas as pd\nimport string\n\nfrom sdc.tests.tests_perf.test_perf_utils import *\nfrom sdc.tests.test_series import gen_strlist\n\n\ndef gen_str_data(data_num, data_length, input_data, data_width):\n    data = []\n    full_input_data_length = data_width\n\n    data.append(perf_data_gen_fixed_len(input_data, full_input_data_length, data_length))\n    for i in range(data_num - 1):\n        np.random.seed(i)\n        data.append(np.random.choice(input_data, data_length))\n\n    return data\n\n\ndef gen_series_fixed_str(data_num, data_length, input_data, data_width):\n    all_data = gen_str_data(data_num, data_length, input_data, data_width)\n    results = []\n    for data in all_data:\n        test_data = pd.Series(data)\n        results.append(test_data)\n\n    return results\n\n\ndef gen_arr_from_input(data_length, input_data, random=True, repeat=True, seed=None):\n    if seed is not None:\n        np.random.seed(seed)\n\n    if random:\n        return np.random.choice(input_data, data_length, replace=repeat)\n    else:\n        return np.asarray(multiply_oneds_data(input_data, data_length))\n\n\ndef gen_arr_of_dtype(data_length, dtype=\'float\', random=True, limits=None, nunique=1000, input_data=None, seed=None):\n    """"""\n    data_length: result array length,\n    dtype: dtype of generated array,\n    limits: a tuple of (min, max) limits for numeric arrays,\n    nunique: number of unique values in generated array,\n    input_data: 1D sequence of values used for generation of array data,\n    seed: seed to initialize random state\n    """"""\n\n    if seed is not None:\n        np.random.seed(seed)\n\n    # prefer generation based on input data if it\'s provided\n    if input_data is not None:\n        return gen_arr_from_input(data_length, input_data, random=random)\n\n    if dtype == \'float\':\n        return np.random.ranf(data_length)\n    if dtype == \'int\':\n        default_limits = (np.iinfo(dtype).min, np.iinfo(dtype).max)\n        min_value, max_value = limits or default_limits\n        return np.random.randint(min_value, max_value, data_length)\n    if dtype == \'str\':\n        default_strings = gen_strlist(nunique)\n        return np.random.choice(default_strings, data_length)\n    if dtype == \'bool\':\n        return np.random.choice([True, False], data_length)\n\n    return None\n\n\ndef gen_unique_values(data_length, dtype=\'int\', seed=None):\n    """"""\n    data_length: result length of array of unique values,\n    dtype: dtype of generated array,\n    seed: seed to initialize random state\n    """"""\n\n    if dtype in (\'float\', \'int\'):\n        values = np.arange(data_length, dtype=dtype)\n    if dtype == \'str\':\n        values = gen_strlist(data_length)\n\n    return gen_arr_from_input(data_length, values, repeat=False, seed=seed)\n\n\ndef gen_series(data_length, dtype=\'float\', random=True, limits=None, nunique=1000, input_data=None, seed=None):\n    """"""\n    data_length: result series length,\n    dtype: dtype of generated series,\n    limits: a tuple of (min, max) limits for numeric series,\n    nunique: number of unique values in generated series,\n    input_data: 1D sequence of values used for generation of series data,\n    seed: seed to initialize random state\n    """"""\n\n    if seed is not None:\n        np.random.seed(seed)\n\n    # prefer generation based on input data if it\'s provided\n    if input_data is not None:\n        series_data = gen_arr_from_input(data_length, input_data, random=random)\n    else:\n        series_data = gen_arr_of_dtype(data_length, dtype=dtype, limits=limits, nunique=nunique)\n\n    # TODO: support index generation\n    return pd.Series(series_data)\n\n\ndef gen_df(data_length,\n           columns=3,\n           col_names=None,\n           dtype=\'float\',\n           random=True,\n           limits=None,\n           nunique=1000,\n           input_data=None,\n           index_gen=None,\n           seed=None):\n    """"""\n    data_length: result series length,\n    dtype: dtype of generated series,\n    limits: a tuple of (min, max) limits for numeric series,\n    nunique: number of unique values in generated series,\n    input_data: 2D sequence of values used for generation of dataframe columns,\n    index_gen: callable that will generate index of needed size,\n    seed: seed to initialize random state\n    """"""\n\n    if seed is not None:\n        np.random.seed(seed)\n\n    col_names = string.ascii_uppercase if col_names is None else col_names\n    all_data = []\n    for i in range(columns):\n        # prefer generation based on input data if it\'s provided\n        if (input_data is not None and i < len(input_data)):\n            col_data = gen_arr_from_input(data_length, input_data[i], random=random)\n        else:\n            col_data = gen_arr_of_dtype(data_length, dtype=dtype, limits=limits, nunique=nunique)\n        all_data.append(col_data)\n\n    index_data = index_gen(data_length) if index_gen is not None else None\n    return pd.DataFrame(dict(zip(col_names, all_data)), index=index_data)\n'"
sdc/tests/tests_perf/gen_csv.py,0,"b'# -*- coding: utf-8 -*-\n# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport csv\n\n\ndef generate(rows, headers, providers, file_name):\n    """"""Generate CSV file.\n    rows: rows count\n    headers: list of column names\n    providers: list of functions whic provide values for corresponding column\n    file_name:\n    """"""\n\n    assert len(headers) == len(providers)\n\n    with open(file_name, \'wt\') as f:\n        writer = csv.DictWriter(f, fieldnames=headers)\n        writer.writeheader()\n        for i in range(rows):\n            writer.writerow({k: p() for k, p in zip(headers, providers)})\n\n\ndef md5(filename):\n    """"""Return MD5 sum of the file.""""""\n    import hashlib\n    hash_md5 = hashlib.md5()\n    with open(filename, ""rb"") as f:\n        for chunk in iter(lambda: f.read(4096), b""""):\n            hash_md5.update(chunk)\n    return hash_md5.hexdigest()\n\n\ndef csv_file_name(rows=10**5, columns=10, seed=0):\n    """"""Return file name for given parameters.""""""\n    return f""data_{rows}_{columns}_{seed}.csv""\n\n\ndef generate_csv(rows=10**5, columns=10, seed=0):\n    """"""Generate CSV file and return file name.""""""\n    import random\n\n    md5_sums = {\n        (10**6, 10, 0): ""6fa2a115dfeaee4f574106b513ad79e6""\n    }\n\n    file_name = csv_file_name(rows, columns, seed)\n\n    try:\n        if md5_sums.get((rows, columns, seed)) == md5(file_name):\n            return file_name\n    except:\n        pass\n\n    r = random.Random(seed)\n    generate(rows,\n        [f""f{c}"" for c in range(columns)],\n        [lambda: r.uniform(-1.0, 1.0) for _ in range(columns)],\n        file_name\n    )\n\n    md5_sums[(rows, columns, seed)] = md5(file_name)\n\n    return file_name\n'"
sdc/tests/tests_perf/generator.py,0,"b'import time\nimport numpy as np\nfrom typing import NamedTuple\nfrom sdc.io.csv_ext import to_varname\nfrom sdc.tests.test_utils import *\n\n\nclass CallExpression(NamedTuple):\n    """"""\n    code: function or method call as a string\n    type_: type of function performed (Python, Numba, SDC)\n    jitted: option indicating whether to jit call\n    """"""\n    code: str\n    type_: str\n    jitted: bool\n\n\nclass TestCase(NamedTuple):\n    """"""\n    name: name of the API item, e.g. method, operator\n    size: size of the generated data for tests\n    params: method parameters in format \'par1, par2, ...\'\n    call_expr: call expression as a string, e.g. \'(A+B).sum()\' where A, B are Series or DF\n    usecase_params: input parameters for usecase in format \'par1, par2, ...\', e.g. \'data, other\'\n    data_num: total number of generated data, e.g. 2 (data, other)\n    input_data: input data for generating test data\n    skip: flag for skipping a test\n    check_skipna: flag for checking a function with both parameters skipna=True and skipna=False\n    """"""\n    name: str\n    size: list\n    params: str = \'\'\n    call_expr: str = None\n    usecase_params: str = None\n    input_data: list = None\n    data_num: int = 1\n    data_gens: tuple = None\n    skip: bool = False\n    check_skipna: bool = False\n\n\ndef to_varname_without_excess_underscores(string):\n    """"""Removing excess underscores from the string.""""""\n    return \'_\'.join(i for i in to_varname(string).split(\'_\') if i)\n\n\ndef skipna_cases(cases):\n    """"""Generator. Replaces a test case containing check_skipna=True\n    with two cases containing parameters skipna=True and skipna=False\n    """"""\n    for case in cases:\n        if case.check_skipna:\n            for skipna in [True, False]:\n                params = case.params\n                if params:\n                    params += \', \'\n                params += f\'skipna={skipna}\'\n                yield case._replace(params=params)\n        else:\n            yield case\n\n\ndef generate_test_cases(cases, class_add, typ, prefix=\'\'):\n    for test_case in skipna_cases(cases):\n        test_name_parts = [\'test\', typ, prefix, test_case.name, gen_params_wo_data(test_case)]\n        test_name = to_varname_without_excess_underscores(\'_\'.join(test_name_parts))\n\n        setattr(class_add, test_name, gen_test(test_case, prefix))\n\n\ndef gen_params_wo_data(test_case):\n    """"""Generate API item parameters without parameters with data, e.g. without parameter other""""""\n    if test_case.data_gens is None:\n        extra_data_num = test_case.data_num - 1\n    else:\n        extra_data_num = len(test_case.data_gens)\n    method_params = test_case.params.split(\', \')[extra_data_num:]\n\n    return \', \'.join(method_params)\n\n\ndef gen_usecase_params(test_case):\n    """"""Generate usecase parameters based on method parameters and number of extra generated data""""""\n    if test_case.data_gens is None:\n        extra_data_num = test_case.data_num - 1\n    else:\n        extra_data_num = len(test_case.data_gens)\n    extra_usecase_params = test_case.params.split(\', \')[:extra_data_num]\n    usecase_params_parts = [\'data\'] + extra_usecase_params\n\n    return \', \'.join(usecase_params_parts)\n\n\ndef gen_call_expr(test_case, prefix):\n    """"""Generate call expression based on method name and parameters and method prefix, e.g. str""""""\n    prefix_as_list = [prefix] if prefix else []\n    call_expr_parts = [\'data\'] + prefix_as_list + [\'{}({})\'.format(test_case.name, test_case.params)]\n\n    return \'.\'.join(call_expr_parts)\n\n\ndef gen_test(test_case, prefix):\n    usecase = gen_usecase(test_case, prefix)\n\n    test_name = test_case.name\n    if test_case.params:\n        test_name += f\'({test_case.params})\'\n\n    def func(self):\n        self._test_case(usecase, name=test_name, total_data_length=test_case.size,\n                        data_num=test_case.data_num, data_gens=test_case.data_gens,\n                        input_data=test_case.input_data)\n\n    if test_case.skip:\n        func = skip_numba_jit(func)\n\n    return func\n\n\ndef create_func(usecase_params, call_expr):\n    func_name = \'func\'\n\n    func_text = f""""""\ndef {func_name}({usecase_params}):\n  start_time = time.time()\n  res = {call_expr}\n  finish_time = time.time()\n  return finish_time - start_time, res\n""""""\n\n    loc_vars = {}\n    exec(func_text, globals(), loc_vars)\n    func = loc_vars[func_name]\n\n    return func\n\n\ndef gen_usecase(test_case, prefix):\n\n    usecase_params = test_case.usecase_params\n    call_expr = test_case.call_expr\n    if call_expr is None:\n        if usecase_params is None:\n            usecase_params = gen_usecase_params(test_case)\n        call_expr = gen_call_expr(test_case, prefix)\n\n    if isinstance(call_expr, list):\n        results = []\n        for ce in call_expr:\n            results.append({\n                \'func\': create_func(usecase_params, ce.code),\n                \'type_\': ce.type_,\n                \'jitted\': ce.jitted\n            })\n\n        return results\n\n    func = create_func(usecase_params, call_expr)\n    return func\n'"
sdc/tests/tests_perf/test_perf_base.py,0,"b'import os\nimport unittest\nimport numba\n\nfrom sdc.tests.test_base import TestCase\nfrom sdc.tests.tests_perf.test_perf_utils import *\n\n\nclass TestBase(TestCase):\n    iter_number = 5\n    results_class = TestResults\n\n    @classmethod\n    def create_test_results(cls):\n        drivers = []\n        if is_true(os.environ.get(\'SDC_TEST_PERF_EXCEL\', True)):\n            drivers.append(ExcelResultsDriver(\'perf_results.xlsx\'))\n        if is_true(os.environ.get(\'SDC_TEST_PERF_CSV\', False)):\n            drivers.append(CSVResultsDriver(\'perf_results.csv\'))\n\n        results = cls.results_class(drivers)\n\n        if is_true(os.environ.get(\'LOAD_PREV_RESULTS\')):\n            results.load()\n\n        return results\n\n    @classmethod\n    def setUpClass(cls):\n        cls.test_results = cls.create_test_results()\n\n        cls.total_data_length = []\n        cls.num_threads = int(os.environ.get(\'NUMBA_NUM_THREADS\', config.NUMBA_NUM_THREADS))\n        cls.threading_layer = os.environ.get(\'NUMBA_THREADING_LAYER\', config.THREADING_LAYER)\n\n    @classmethod\n    def tearDownClass(cls):\n        # TODO: https://jira.devtools.intel.com/browse/SAT-2371\n        cls.test_results.print()\n        cls.test_results.dump()\n\n    def _test_jitted(self, pyfunc, record, *args, **kwargs):\n        # compilation time\n        record[""compile_results""] = calc_compilation(pyfunc, *args, **kwargs)\n\n        cfunc = self.jit(pyfunc)\n\n        # execution and boxing time\n        record[""test_results""], record[""boxing_results""] = \\\n            get_times(cfunc, *args, **kwargs)\n\n    def _test_python(self, pyfunc, record, *args, **kwargs):\n        record[""test_results""], _ = \\\n            get_times(pyfunc, *args, **kwargs)\n\n    def _test_jit(self, pyfunc, base, *args):\n        record = base.copy()\n        record[""test_type""] = \'SDC\'\n        self._test_jitted(pyfunc, record, *args)\n        self.test_results.add(**record)\n\n    def _test_py(self, pyfunc, base, *args):\n        skip = \'NUMBA_NUM_THREADS\' in os.environ and config.NUMBA_NUM_THREADS > 1\n        if skip:\n            return\n\n        record = base.copy()\n        record[""test_type""] = \'Python\'\n        self._test_python(pyfunc, record, *args)\n        self.test_results.add(**record)\n'"
sdc/tests/tests_perf/test_perf_df.py,0,"b'# -*- coding: utf-8 -*-\n# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numpy as np\nimport numba\nimport sdc\n\nfrom functools import partial\n\nfrom sdc.tests.tests_perf.test_perf_base import TestBase\nfrom sdc.tests.tests_perf.test_perf_utils import calc_compilation, get_times, perf_data_gen_fixed_len\nfrom sdc.tests.test_utils import test_global_input_data_float64\nfrom .generator import generate_test_cases\nfrom .generator import TestCase as TC\nfrom .data_generator import gen_df, gen_series, gen_arr_of_dtype, gen_unique_values\n\n\n# python -m sdc.runtests sdc.tests.tests_perf.test_perf_df.TestDataFrameMethods.test_df_{method_name}\nclass TestDataFrameMethods(TestBase):\n    @classmethod\n    def setUpClass(cls):\n        super().setUpClass()\n\n    def _test_case(self, pyfunc, name, total_data_length, input_data=None, data_num=1, data_gens=None):\n        test_name = \'DataFrame.{}\'.format(name)\n\n        data_num = len(data_gens) if data_gens is not None else data_num\n        default_data_gens = [gen_df] * data_num\n        data_gens = data_gens or default_data_gens\n        default_input_data = [None] * data_num\n        input_data = input_data or default_input_data\n\n        for data_length in total_data_length:\n            base = {\n                ""test_name"": test_name,\n                ""data_size"": data_length,\n            }\n\n            args = tuple(gen(data_length, input_data=input_data[i]) for i, gen in enumerate(data_gens))\n            self._test_jit(pyfunc, base, *args)\n            self._test_py(pyfunc, base, *args)\n\n\ncases = [\n    TC(name=\'append\', size=[10 ** 7], params=\'other\', data_num=2),\n    TC(name=\'copy\', size=[10 ** 7], params=\'deep=True\'),\n    TC(name=\'copy\', size=[10 ** 7], params=\'deep=False\'),\n    TC(name=\'count\', size=[10 ** 7]),\n    TC(name=\'drop\', size=[10 ** 8], params=\'columns=""A""\'),\n    TC(name=\'isna\', size=[10 ** 6 * 6]),\n    TC(name=\'max\', size=[10 ** 7], check_skipna=True),\n    TC(name=\'mean\', size=[10 ** 7], check_skipna=True),\n    TC(name=\'median\', size=[10 ** 7], check_skipna=True),\n    TC(name=\'min\', size=[10 ** 7], check_skipna=True),\n    TC(name=\'pct_change\', size=[10 ** 7]),\n    TC(name=\'prod\', size=[10 ** 7], check_skipna=True),\n    TC(name=\'reset_index\', size=[10 ** 7], params=\'drop=False\'),\n    TC(name=\'std\', size=[10 ** 7], check_skipna=True),\n    TC(name=\'sum\', size=[10 ** 7], check_skipna=True),\n    TC(name=\'var\', size=[10 ** 7], check_skipna=True),\n    TC(name=\'getitem_idx_bool_series\', size=[10 ** 7], call_expr=\'df[idx]\', usecase_params=\'df, idx\',\n       data_gens=(gen_df, partial(gen_series, dtype=\'bool\', random=False)),\n       input_data=[None, [True, False, False, True, False, True]]),\n    TC(name=\'getitem_idx_bool_array\', size=[10 ** 7], call_expr=\'df[idx]\', usecase_params=\'df, idx\',\n       data_gens=(gen_df, partial(gen_arr_of_dtype, dtype=\'bool\', random=False)),\n       input_data=[None, [True, False, False, True, False, True]]),\n    TC(name=\'getitem_filter_by_value\', size=[10 ** 7], call_expr=\'df[df.A > 0]\', usecase_params=\'df\',\n       data_gens=(partial(gen_df, index_gen=gen_unique_values), )),\n]\n\ngenerate_test_cases(cases, TestDataFrameMethods, \'df\')\n'"
sdc/tests/tests_perf/test_perf_df_groupby.py,0,"b'# -*- coding: utf-8 -*-\n# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\nimport string\nimport time\n\nimport numba\nimport numpy\nimport pandas\n\nfrom sdc.tests.tests_perf.test_perf_base import TestBase\nfrom sdc.tests.test_series import gen_strlist, gen_frand_array\nfrom sdc.utilities.sdc_typing_utils import kwsparams2list\n\n\nmin_groupby_int = -50\nmax_groupby_int = 50\nn_groups_default = 200\n\n\ngroupby_usecase_tmpl = """"""\ndef df_groupby_{method_name}_usecase(data):\n    start_time = time.time()\n    res = data.groupby({groupby_params}).{method_name}({method_params})\n    end_time = time.time()\n    return end_time - start_time, res\n""""""\n\n\ndef get_groupby_params(**kwargs):\n    """"""Generate supported groupby parameters""""""\n\n    # only supported groupby parameters are here\n    df_params_defaults = {\n        \'by\': ""\'A\'"",\n        \'sort\': \'True\'\n    }\n    groupby_params = {k: kwargs.get(k, df_params_defaults[k]) for k in df_params_defaults}\n    return \', \'.join(kwsparams2list(groupby_params))\n\n\ndef gen_df_groupby_usecase(method_name, groupby_params=None, method_params=\'\'):\n    """"""Generate df groupby method use case""""""\n\n    groupby_params = {} if groupby_params is None else groupby_params\n    groupby_params = get_groupby_params(**groupby_params)\n\n    func_text = groupby_usecase_tmpl.format(**{\n        \'method_name\': method_name,\n        \'groupby_params\': groupby_params,\n        \'method_params\': method_params\n    })\n\n    global_vars = {\'np\': numpy, \'time\': time}\n    loc_vars = {}\n    exec(func_text, global_vars, loc_vars)\n    _df_groupby_usecase = loc_vars[f\'df_groupby_{method_name}_usecase\']\n\n    return _df_groupby_usecase\n\n\n# python -m sdc.runtests sdc.tests.tests_perf.test_perf_df_groupby.TestDFGroupByMethods\nclass TestDFGroupByMethods(TestBase):\n    # more than 19 columns raise SystemError: CPUDispatcher() returned a result with an error set\n    max_columns_num = 19\n\n    @classmethod\n    def setUpClass(cls):\n        super().setUpClass()\n        cls.total_data_length = {\n            \'count\': [2 * 10 ** 5],\n            \'max\': [2 * 10 ** 5],\n            \'mean\': [2 * 10 ** 5],\n            \'median\': [2 * 10 ** 5],\n            \'min\': [2 * 10 ** 5],\n            \'prod\': [2 * 10 ** 5],\n            \'std\': [2 * 10 ** 5],\n            \'sum\': [2 * 10 ** 5],\n            \'var\': [2 * 10 ** 5],\n        }\n\n    def _gen_df(self, data, columns_num=10):\n        """"""Generate DataFrame based on input data""""""\n\n        if len(data) < columns_num:\n            data *= columns_num // len(data)\n            data.extend(data[:(columns_num % len(data))])\n        columns_seq = zip(string.ascii_uppercase[:columns_num], data)\n        return pandas.DataFrame(dict(columns_seq))\n\n    def _test_case(self, pyfunc, name, usecase_name=None,\n                   input_data=None,\n                   columns_num=10):\n        """"""\n        Test DataFrame.groupby method\n        :param pyfunc: Python function to test which calls tested method inside\n        :param name: name of the tested method, e.g. min\n        :param input_data: initial data used for generating test data\n        :param columns_num: number of columns in generated DataFrame """"""\n        if columns_num > self.max_columns_num:\n            columns_num = self.max_columns_num\n\n        usecase_name = f\'{name}\' if usecase_name is None else f\'{usecase_name}\'\n        for data_length in self.total_data_length[name]:\n            base = {\n                \'test_name\': f\'DataFrame.groupby.{usecase_name}\',\n                \'data_size\': data_length,\n            }\n\n            numpy.random.seed(0)\n            data = []\n            if input_data is None:\n                data.append(numpy.random.randint(min_groupby_int, max_groupby_int, data_length))\n                for _ in range(1, columns_num):\n                    data.append(numpy.random.ranf(data_length))\n            else:\n                for i in range(columns_num):\n                    if i < len(input_data):\n                        col_data = numpy.random.choice(input_data[i], data_length)\n                    else:\n                        col_data = numpy.random.ranf(data_length)\n                    data.append(col_data)\n\n            test_data = self._gen_df(data, columns_num=columns_num)\n            args = [test_data]\n\n            record = base.copy()\n            record[\'test_type\'] = \'SDC\'\n            self._test_jitted(pyfunc, record, *args)\n            self.test_results.add(**record)\n\n            record = base.copy()\n            record[\'test_type\'] = \'Python\'\n            self._test_python(pyfunc, record, *args)\n            self.test_results.add(**record)\n\n    def _test_df_groupby_method(self, name, usecase_name=None, groupby_params=None, method_params=\'\', input_data=None):\n        usecase = gen_df_groupby_usecase(name, groupby_params=groupby_params, method_params=method_params)\n        self._test_case(usecase, name, usecase_name=usecase_name, input_data=input_data)\n\n    def test_df_groupby_count_sort_false(self):\n        self._test_df_groupby_method(\'count\', groupby_params={\'sort\': \'False\'})\n\n    def test_df_groupby_max_sort_false(self):\n        self._test_df_groupby_method(\'max\', groupby_params={\'sort\': \'False\'})\n\n    def test_df_groupby_mean_sort_false(self):\n        self._test_df_groupby_method(\'mean\', groupby_params={\'sort\': \'False\'})\n\n    def test_df_groupby_median_sort_false(self):\n        self._test_df_groupby_method(\'median\', groupby_params={\'sort\': \'False\'})\n\n    def test_df_groupby_min_sort_false(self):\n        self._test_df_groupby_method(\'min\', groupby_params={\'sort\': \'False\'})\n\n    def test_df_groupby_prod_sort_false(self):\n        self._test_df_groupby_method(\'prod\', groupby_params={\'sort\': \'False\'})\n\n    def test_df_groupby_std_sort_false(self):\n        self._test_df_groupby_method(\'std\', groupby_params={\'sort\': \'False\'})\n\n    def test_df_groupby_sum_sort_false(self):\n        self._test_df_groupby_method(\'sum\', groupby_params={\'sort\': \'False\'})\n\n    def test_df_groupby_var_sort_false(self):\n        self._test_df_groupby_method(\'var\', groupby_params={\'sort\': \'False\'})\n\n    def test_df_groupby_mean_sort_true(self):\n        self._test_df_groupby_method(\'mean\', usecase_name=\'mean_sort_true\')\n\n    def test_df_groupby_mean_by_float_sort_false(self):\n        self._test_df_groupby_method(\'mean\',\n                                     usecase_name=\'by_float_mean\',\n                                     input_data=[gen_frand_array(n_groups_default)],\n                                     groupby_params={\'sort\': \'False\'})\n\n    def test_df_groupby_mean_by_str_sort_false(self):\n        self._test_df_groupby_method(\'mean\',\n                                     usecase_name=\'by_str_mean\',\n                                     input_data=[gen_strlist(n_groups_default, 3, \'abcdef\')],\n                                     groupby_params={\'sort\': \'False\'})\n'"
sdc/tests/tests_perf/test_perf_df_rolling.py,1,"b'# -*- coding: utf-8 -*-\n# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\nimport string\nimport time\n\nimport numpy\n\nfrom sdc.tests.test_utils import test_global_input_data_float64\nfrom sdc.tests.tests_perf.data_generator import gen_df\nfrom sdc.tests.tests_perf.test_perf_base import TestBase\n\n\nrolling_usecase_tmpl = """"""\ndef df_rolling_{method_name}_usecase(data, {extra_usecase_params}):\n    start_time = time.time()\n    for i in range({ncalls}):\n        res = data.rolling({rolling_params}).{method_name}({method_params})\n    end_time = time.time()\n    return end_time - start_time, res\n""""""\n\n\ndef get_rolling_params(window=100, min_periods=None):\n    """"""Generate supported rolling parameters""""""\n    rolling_params = [f\'{window}\']\n    if min_periods:\n        rolling_params.append(f\'min_periods={min_periods}\')\n\n    return \', \'.join(rolling_params)\n\n\ndef gen_df_rolling_usecase(method_name, rolling_params=None,\n                           extra_usecase_params=\'\', method_params=\'\', ncalls=1):\n    """"""Generate df rolling method use case""""""\n    if not rolling_params:\n        rolling_params = get_rolling_params()\n\n    func_text = rolling_usecase_tmpl.format(**{\n        \'method_name\': method_name,\n        \'extra_usecase_params\': extra_usecase_params,\n        \'rolling_params\': rolling_params,\n        \'method_params\': method_params,\n        \'ncalls\': ncalls\n    })\n\n    global_vars = {\'np\': numpy, \'time\': time}\n    loc_vars = {}\n    exec(func_text, global_vars, loc_vars)\n    _df_rolling_usecase = loc_vars[f\'df_rolling_{method_name}_usecase\']\n\n    return _df_rolling_usecase\n\n\n# python -m sdc.runtests sdc.tests.tests_perf.test_perf_df_rolling.TestDFRollingMethods\nclass TestDFRollingMethods(TestBase):\n    # more than 19 columns raise SystemError: CPUDispatcher() returned a result with an error set\n    max_columns_num = 19\n\n    @classmethod\n    def setUpClass(cls):\n        super().setUpClass()\n        cls.map_ncalls_dlength = {\n            \'apply\': (10, [10 ** 4]),\n            \'corr\': (10, [8 * 10 ** 5]),\n            \'count\': (10, [10 ** 6]),\n            \'cov\': (10, [8 * 10 ** 5]),\n            \'kurt\': (10, [4 * 10 ** 5]),\n            \'max\': (10, [4 * 10 ** 5]),\n            \'mean\': (10, [10 ** 6]),\n            \'median\': (10, [10 ** 4]),\n            \'min\': (10, [4 * 10 ** 5]),\n            \'quantile\': (10, [4 * 10 ** 3]),\n            \'skew\': (10, [2 * 10 ** 5]),\n            \'std\': (10, [8 * 10 ** 5]),\n            \'sum\': (10, [10 ** 6]),\n            \'var\': (10, [8 * 10 ** 5]),\n        }\n\n    def _test_case(self, pyfunc, name, total_data_length,\n                   input_data=test_global_input_data_float64,\n                   columns_num=10, extra_data_num=0):\n        """"""\n        Test DataFrame.rolling method\n        :param pyfunc: Python function to test which calls tested method inside\n        :param name: name of the tested method, e.g. min\n        :param total_data_length: length of generating test data\n        :param input_data: initial data used for generating test data\n        :param columns_num: number of columns in generated DataFrame\n        :param extra_data_num: number of additionally generated DataFrames\n        """"""\n        if columns_num > self.max_columns_num:\n            columns_num = self.max_columns_num\n\n        for data_length in total_data_length:\n            base = {\n                \'test_name\': f\'DataFrame.rolling.{name}\',\n                \'data_size\': data_length,\n            }\n            args = [gen_df(data_length, columns=columns_num,\n                           col_names=string.ascii_uppercase[:columns_num],\n                           random=False, input_data=input_data)]\n            for i in range(extra_data_num):\n                extra_data = gen_df(data_length, columns=columns_num,\n                                    col_names=string.ascii_uppercase[:columns_num],\n                                    seed=i)\n                args.append(extra_data)\n\n            self._test_jit(pyfunc, base, *args)\n            self._test_py(pyfunc, base, *args)\n\n    def _test_df_rolling_method(self, name, rolling_params=None,\n                                extra_usecase_params=\'\', method_params=\'\'):\n        ncalls, total_data_length = self.map_ncalls_dlength[name]\n        usecase = gen_df_rolling_usecase(name, rolling_params=rolling_params,\n                                         extra_usecase_params=extra_usecase_params,\n                                         method_params=method_params, ncalls=ncalls)\n        extra_data_num = 0\n        if extra_usecase_params:\n            extra_data_num += len(extra_usecase_params.split(\', \'))\n        self._test_case(usecase, name, total_data_length,\n                        extra_data_num=extra_data_num)\n\n    def test_df_rolling_apply_mean(self):\n        method_params = \'lambda x: np.nan if len(x) == 0 else x.mean()\'\n        self._test_df_rolling_method(\'apply\', method_params=method_params)\n\n    def test_df_rolling_corr(self):\n        self._test_df_rolling_method(\'corr\', extra_usecase_params=\'other\',\n                                     method_params=\'other=other\')\n\n    def test_df_rolling_count(self):\n        self._test_df_rolling_method(\'count\')\n\n    def test_df_rolling_cov(self):\n        self._test_df_rolling_method(\'cov\', extra_usecase_params=\'other\',\n                                     method_params=\'other=other\')\n\n    def test_df_rolling_kurt(self):\n        self._test_df_rolling_method(\'kurt\')\n\n    def test_df_rolling_max(self):\n        self._test_df_rolling_method(\'max\')\n\n    def test_df_rolling_mean(self):\n        self._test_df_rolling_method(\'mean\')\n\n    def test_df_rolling_median(self):\n        self._test_df_rolling_method(\'median\')\n\n    def test_df_rolling_min(self):\n        self._test_df_rolling_method(\'min\')\n\n    def test_df_rolling_quantile(self):\n        self._test_df_rolling_method(\'quantile\', method_params=\'0.25\')\n\n    def test_df_rolling_skew(self):\n        self._test_df_rolling_method(\'skew\')\n\n    def test_df_rolling_std(self):\n        self._test_df_rolling_method(\'std\')\n\n    def test_df_rolling_sum(self):\n        self._test_df_rolling_method(\'sum\')\n\n    def test_df_rolling_var(self):\n        self._test_df_rolling_method(\'var\')\n'"
sdc/tests/tests_perf/test_perf_numpy.py,32,"b'# -*- coding: utf-8 -*-\n# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numpy as np\nimport pandas as pd\nimport random\nimport time\nfrom functools import partial\n\nimport sdc\n\nfrom .test_perf_base import TestBase\nfrom sdc.tests.test_utils import test_global_input_data_float64\nfrom .test_perf_utils import calc_compilation, get_times, perf_data_gen_fixed_len\nfrom .generator import generate_test_cases\nfrom .generator import TestCase as TC\nfrom .generator import CallExpression as CE\nfrom sdc.functions import numpy_like\nfrom .data_generator import gen_arr_of_dtype\n\n\n# python -m sdc.runtests sdc.tests.tests_perf.test_perf_numpy.TestFunctions.test_function_{name}\nclass TestFunctions(TestBase):\n    @classmethod\n    def setUpClass(cls):\n        super().setUpClass()\n\n    def _test_case(self, cases, name, total_data_length, input_data=None, data_num=1, data_gens=None):\n        test_name = \'{}\'.format(name)\n\n        if input_data is None:\n            input_data = [np.asarray(test_global_input_data_float64).flatten()]\n\n        data_num = len(data_gens) if data_gens is not None else data_num\n        default_data_gens = [gen_arr_of_dtype] * data_num\n        data_gens = data_gens or default_data_gens\n        default_input_data = [np.asarray(test_global_input_data_float64).flatten()] + [None] * (data_num - 1)\n        input_data = input_data or default_input_data\n\n        for data_length in total_data_length:\n            base = {\n                ""test_name"": test_name,\n                ""data_size"": data_length,\n            }\n\n            args = tuple(gen(data_length, input_data=input_data[i]) for i, gen in enumerate(data_gens))\n\n            for case in cases:\n                record = base.copy()\n                record[""test_type""] = case[\'type_\']\n                if case[\'jitted\']:\n                    self._test_jitted(case[\'func\'], record, *args)\n                else:\n                    self._test_python(case[\'func\'], record, *args)\n                self.test_results.add(**record)\n\n\ncases = [\n    TC(name=\'astype\', size=[10 ** 7], call_expr=[\n        CE(type_=\'Python\', code=\'data.astype(np.int64)\', jitted=False),\n        CE(type_=\'Numba\', code=\'data.astype(np.int64)\', jitted=True),\n        CE(type_=\'SDC\', code=\'sdc.functions.numpy_like.astype(data, np.int64)\', jitted=True),\n    ], usecase_params=\'data\'),\n    TC(name=\'nanargmin\', size=[10 ** 7], call_expr=[\n        CE(type_=\'Python\', code=\'np.nanargmin(data)\', jitted=False),\n        CE(type_=\'SDC\', code=\'sdc.functions.numpy_like.nanargmin(data)\', jitted=True),\n    ], usecase_params=\'data\'),\n    TC(name=\'nanargmax\', size=[10 ** 7], call_expr=[\n        CE(type_=\'Python\', code=\'np.nanargmax(data)\', jitted=False),\n        CE(type_=\'SDC\', code=\'sdc.functions.numpy_like.nanargmax(data)\', jitted=True),\n    ], usecase_params=\'data\'),\n    TC(name=\'argmax\', size=[10 ** 7], call_expr=[\n        CE(type_=\'Python\', code=\'np.argmax(data)\', jitted=False),\n        CE(type_=\'Numba\', code=\'np.argmax(data)\', jitted=True),\n        CE(type_=\'SDC\', code=\'sdc.functions.numpy_like.argmax(data)\', jitted=True),\n    ], usecase_params=\'data\'),\n    TC(name=\'argmin\', size=[10 ** 7], call_expr=[\n        CE(type_=\'Python\', code=\'np.argmin(data)\', jitted=False),\n        CE(type_=\'Numba\', code=\'np.argmin(data)\', jitted=True),\n        CE(type_=\'SDC\', code=\'sdc.functions.numpy_like.argmin(data)\', jitted=True),\n    ], usecase_params=\'data\'),\n    TC(name=\'copy\', size=[10 ** 7], call_expr=[\n        CE(type_=\'Python\', code=\'np.copy(data)\', jitted=False),\n        CE(type_=\'Numba\', code=\'np.copy(data)\', jitted=True),\n        CE(type_=\'SDC\', code=\'sdc.functions.numpy_like.copy(data)\', jitted=True),\n    ], usecase_params=\'data\'),\n    TC(name=\'cumsum\', size=[10 ** 7], call_expr=[\n        CE(type_=\'Python\', code=\'np.cumsum(data)\', jitted=False),\n        CE(type_=\'Numba\', code=\'np.cumsum(data)\', jitted=True),\n        CE(type_=\'SDC\', code=\'sdc.functions.numpy_like.cumsum(data)\', jitted=True),\n    ], usecase_params=\'data\'),\n    TC(name=\'nancumsum\', size=[10 ** 7], call_expr=[\n        CE(type_=\'Python\', code=\'np.nancumsum(data)\', jitted=False),\n        CE(type_=\'Numba\', code=\'np.nancumsum(data)\', jitted=True),\n        CE(type_=\'SDC\', code=\'sdc.functions.numpy_like.nancumsum(data)\', jitted=True),\n    ], usecase_params=\'data\'),\n    TC(name=\'isnan\', size=[10 ** 7], call_expr=[\n        CE(type_=\'Python\', code=\'np.isnan(data)\', jitted=False),\n        CE(type_=\'Numba\', code=\'np.isnan(data)\', jitted=True),\n        CE(type_=\'SDC\', code=\'sdc.functions.numpy_like.isnan(data)\', jitted=True),\n    ], usecase_params=\'data\'),\n    TC(name=\'nanmean\', size=[10 ** 8], call_expr=[\n        CE(type_=\'Python\', code=\'np.nanmean(data)\', jitted=False),\n        CE(type_=\'Numba\', code=\'np.nanmean(data)\', jitted=True),\n        CE(type_=\'SDC\', code=\'sdc.functions.numpy_like.nanmean(data)\', jitted=True),\n    ], usecase_params=\'data\'),\n    TC(name=\'nansum\', size=[10 ** 7], call_expr=[\n        CE(type_=\'Python\', code=\'np.nansum(data)\', jitted=False),\n        CE(type_=\'SDC\', code=\'sdc.functions.numpy_like.nansum(data)\', jitted=True),\n    ], usecase_params=\'data\'),\n    TC(name=\'nanprod\', size=[10 ** 7], call_expr=[\n        CE(type_=\'Python\', code=\'np.nanprod(data)\', jitted=False),\n        CE(type_=\'Numba\', code=\'np.nanprod(data)\', jitted=True),\n        CE(type_=\'SDC\', code=\'sdc.functions.numpy_like.nanprod(data)\', jitted=True),\n    ], usecase_params=\'data\'),\n    TC(name=\'nanvar\', size=[10 ** 7], call_expr=[\n        CE(type_=\'Python\', code=\'np.nanvar(data)\', jitted=False),\n        CE(type_=\'Numba\', code=\'np.nanvar(data)\', jitted=True),\n        CE(type_=\'SDC\', code=\'sdc.functions.numpy_like.nanvar(data)\', jitted=True),\n    ], usecase_params=\'data\'),\n    TC(name=\'sum\', size=[10 ** 7], call_expr=[\n        CE(type_=\'Python\', code=\'np.sum(data)\', jitted=False),\n        CE(type_=\'Numba\', code=\'np.sum(data)\', jitted=True),\n        CE(type_=\'SDC\', code=\'sdc.functions.numpy_like.sum(data)\', jitted=True),\n    ], usecase_params=\'data\'),\n    TC(name=\'nanmin\', size=[10 ** 7], call_expr=[\n        CE(type_=\'Python\', code=\'np.nanmin(data)\', jitted=False),\n        CE(type_=\'Numba\', code=\'np.nanmin(data)\', jitted=True),\n        CE(type_=\'SDC\', code=\'sdc.functions.numpy_like.nanmin(data)\', jitted=True),\n    ], usecase_params=\'data\'),\n    TC(name=\'nanmax\', size=[10 ** 7], call_expr=[\n        CE(type_=\'Python\', code=\'np.nanmax(data)\', jitted=False),\n        CE(type_=\'Numba\', code=\'np.nanmax(data)\', jitted=True),\n        CE(type_=\'SDC\', code=\'sdc.functions.numpy_like.nanmax(data)\', jitted=True),\n    ], usecase_params=\'data\'),\n]\n\ngenerate_test_cases(cases, TestFunctions, \'function\')\n'"
sdc/tests/tests_perf/test_perf_read_csv.py,0,"b'# -*- coding: utf-8 -*-\n# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport time\n\nimport pandas\nimport pyarrow.csv\nimport sdc\n\nfrom sdc.tests.tests_perf.test_perf_base import TestBase\nfrom sdc.tests.tests_perf.test_perf_utils import calc_compilation, get_times\n\nfrom sdc.tests.tests_perf.gen_csv import generate_csv\n\n\ndef make_func(file_name):\n    """"""Create function for testing.\n    It is necessary because file_name should be constant for jitted function.\n    """"""\n    def _function():\n        start = time.time()\n        df = pandas.read_csv(file_name)\n        return time.time() - start, df\n    return _function\n\n\ndef make_func_pyarrow(file_name):\n    """"""Create function implemented via PyArrow.""""""\n    def _function():\n        start = time.time()\n        df = sdc.io.csv_ext.pandas_read_csv(file_name)\n        return time.time() - start, df\n    return _function\n\n\nclass TestPandasReadCSV(TestBase):\n\n    @classmethod\n    def setUpClass(cls):\n        super().setUpClass()\n        cls.rows = 10**6\n        cls.columns = 10\n        cls.generated_file = generate_csv(cls.rows, cls.columns)\n\n    def _test_case(self, pyfunc, name):\n        base = {\n            ""test_name"": name,\n            ""data_size"": f""[{self.rows},{self.columns}]"",\n        }\n\n        self._test_jit(pyfunc, base)\n        self._test_py(pyfunc, base)\n\n    def test_read_csv(self):\n        self._test_case(make_func(self.generated_file), \'read_csv\')\n\n    def test_read_csv_pyarrow(self):\n        pyfunc = make_func_pyarrow(self.generated_file)\n        name = \'read_csv\'\n\n        base = {\n            ""test_name"": name,\n            ""data_size"": f""[{self.rows},{self.columns}]"",\n        }\n\n        record = base.copy()\n        record[""test_type""] = \'PyArrow\'\n        self._test_python(pyfunc, record)\n        self.test_results.add(**record)\n\n\nif __name__ == ""__main__"":\n    print(""Gererate data files..."")\n    generate_csv(rows=10**6, columns=10)\n    print(""Data files generated!"")\n'"
sdc/tests/tests_perf/test_perf_series.py,2,"b'# -*- coding: utf-8 -*-\n# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numpy as np\nimport pandas as pd\nimport time\nimport random\nfrom functools import partial\n\nimport sdc\n\nfrom .test_perf_base import TestBase\nfrom sdc.tests.test_utils import test_global_input_data_float64\nfrom .test_perf_utils import calc_compilation, get_times, perf_data_gen_fixed_len\nfrom .generator import generate_test_cases\nfrom .generator import TestCase as TC\nfrom .data_generator import gen_series, gen_arr_of_dtype\n\n\n# python -m sdc.runtests sdc.tests.tests_perf.test_perf_series.TestSeriesMethods.test_series_{method_name}\nclass TestSeriesMethods(TestBase):\n    @classmethod\n    def setUpClass(cls):\n        super().setUpClass()\n\n    def _test_case(self, pyfunc, name, total_data_length, input_data=None, data_num=1, data_gens=None):\n        test_name = \'Series.{}\'.format(name)\n\n        data_num = len(data_gens) if data_gens is not None else data_num\n        default_data_gens = [gen_series] * data_num\n        data_gens = data_gens or default_data_gens\n        default_input_data = [np.asarray(test_global_input_data_float64).flatten()] + [None] * (data_num - 1)\n        input_data = input_data or default_input_data\n\n        for data_length in total_data_length:\n            base = {\n                ""test_name"": test_name,\n                ""data_size"": data_length,\n            }\n\n            args = tuple(gen(data_length, input_data=input_data[i]) for i, gen in enumerate(data_gens))\n            self._test_jit(pyfunc, base, *args)\n            self._test_py(pyfunc, base, *args)\n\n\ncases = [\n    TC(name=\'abs\', size=[10 ** 8]),\n    TC(name=\'add\', size=[10 ** 7], params=\'other\',  data_num=2),\n    TC(name=\'add\', size=[10 ** 7], params=\'other, fill_value=3\',  data_num=2),\n    TC(name=\'append\', size=[10 ** 7], params=\'other\', data_num=2),\n    TC(name=\'apply\', size=[10 ** 7], params=\'lambda x: x\'),\n    TC(name=\'argsort\', size=[10 ** 4]),\n    TC(name=\'astype\', size=[10 ** 8], call_expr=\'data.astype(np.int8)\', usecase_params=\'data\',\n       input_data=[test_global_input_data_float64[0]]),\n    TC(name=\'at\', size=[10 ** 7], call_expr=\'data.at[3]\', usecase_params=\'data\'),\n    TC(name=\'chain_add_and_sum\', size=[20 * 10 ** 6, 25 * 10 ** 6, 30 * 10 ** 6], call_expr=\'(A + B).sum()\',\n       usecase_params=\'A, B\', data_num=2),\n    TC(name=\'copy\', size=[10 ** 7]),\n    TC(name=\'corr\',  size=[10 ** 7],params=\'other\', data_num=2),\n    TC(name=\'count\', size=[10 ** 8]),\n    TC(name=\'cov\', size=[10 ** 8], params=\'other\', data_num=2),\n    TC(name=\'cumsum\', size=[10 ** 7], check_skipna=True),\n    TC(name=\'describe\', size=[10 ** 7]),\n    TC(name=\'div\', size=[10 ** 7], params=\'other\', data_num=2),\n    TC(name=\'dropna\', size=[10 ** 7]),\n    TC(name=\'eq\', size=[10 ** 7], params=\'other\', data_num=2),\n    TC(name=\'fillna\', size=[10 ** 7], params=\'-1\'),\n    TC(name=\'floordiv\', size=[10 ** 7], params=\'other\', data_num=2),\n    TC(name=\'ge\', size=[10 ** 7], params=\'other\', data_num=2),\n    TC(name=\'getitem_idx_scalar\', size=[10 ** 7], call_expr=\'data[100000]\', usecase_params=\'data\'),\n    TC(name=\'getitem_idx_bool_series\', size=[10 ** 7], call_expr=\'A[B]\', usecase_params=\'A, B\',\n       data_gens=(gen_series, partial(gen_series, dtype=\'bool\', random=False)),\n       input_data=[None, [True, False, False, True, False, True]]),\n    TC(name=\'getitem_idx_bool_array\', size=[10 ** 7], call_expr=\'A[B]\', usecase_params=\'A, B\',\n       data_gens=(gen_series, partial(gen_arr_of_dtype, dtype=\'bool\', random=False)),\n       input_data=[None, [True, False, False, True, False, True]]),\n   #  TC(name=\'getitem_idx_int_series\', size=[10 ** 7], call_expr=\'A[B]\', usecase_params=\'A, B\',\n   #     data_gens=(gen_series, partial(gen_series, dtype=\'int\', limits=(0, 10 ** 7)))),\n    TC(name=\'gt\',  size=[10 ** 7],params=\'other\', data_num=2),\n    TC(name=\'head\', size=[10 ** 8]),\n    TC(name=\'iat\', size=[10 ** 7], call_expr=\'data.iat[100000]\', usecase_params=\'data\'),\n    TC(name=\'idxmax\', size=[10 ** 8], check_skipna=True),\n    TC(name=\'idxmin\', size=[10 ** 8], check_skipna=True),\n    TC(name=\'iloc\', size=[10 ** 7], call_expr=\'data.iloc[100000]\', usecase_params=\'data\'),\n    TC(name=\'index\', size=[10 ** 7], call_expr=\'data.index\', usecase_params=\'data\'),\n    TC(name=\'isin\', size=[10 ** 7], params=\'values=[0]\'),\n    TC(name=\'isin\', size=[10 ** 7], call_expr=\'data.isin([""a"", ""q"", ""c"", ""q"", ""d"", ""q"", ""e""])\', usecase_params=\'data\',\n       input_data=[[\'a\', \'b\', \'q\', \'w\', \'c\', \'d\', \'e\', \'r\']]),\n    TC(name=\'isna\', size=[10 ** 7]),\n    TC(name=\'isnull\', size=[10 ** 7]),\n    TC(name=\'le\', size=[10 ** 7], params=\'other\', data_num=2),\n    TC(name=\'loc\', size=[10 ** 7], call_expr=\'data.loc[0]\', usecase_params=\'data\'),\n    TC(name=\'lt\', size=[10 ** 7], params=\'other\', data_num=2),\n    TC(name=\'lt\', size=[10 ** 7], params=\'other, fill_value=3\', data_num=2),\n    TC(name=\'map\', size=[10 ** 7], params=\'lambda x: x * 2\'),\n    TC(name=\'map\', size=[10 ** 7], params=\'{2.: 42., 4.: 3.14}\'),\n    TC(name=\'max\', size=[10 ** 8], check_skipna=True),\n    TC(name=\'mean\', size=[10 ** 8], check_skipna=True),\n    TC(name=\'median\', size=[10 ** 8], check_skipna=True),\n    TC(name=\'min\', size=[10 ** 8], check_skipna=True),\n    TC(name=\'mod\', size=[10 ** 7], params=\'other\', data_num=2),\n    TC(name=\'mul\', size=[10 ** 7], params=\'other\', data_num=2),\n    TC(name=\'ndim\', size=[10 ** 7], call_expr=\'data.ndim\', usecase_params=\'data\'),\n    TC(name=\'ne\', size=[10 ** 8], params=\'other\', data_num=2),\n    TC(name=\'nlargest\', size=[10 ** 6]),\n    TC(name=\'notna\', size=[10 ** 7]),\n    TC(name=\'nsmallest\', size=[10 ** 6]),\n    TC(name=\'nunique\', size=[10 ** 7]),\n    TC(name=\'prod\', size=[10 ** 8], check_skipna=True),\n    TC(name=\'pct_change\', size=[10 ** 7], params=\'periods=1, limit=None, freq=None\'),\n    TC(name=\'pow\', size=[10 ** 7], params=\'other\', data_num=2),\n    TC(name=\'quantile\', size=[10 ** 8]),\n    TC(name=\'rename\', size=[10 ** 7], call_expr=\'data.rename(""new_series"")\', usecase_params=\'data\'),\n    # TC(name=\'setitem\', size=[10 ** 7], call_expr=\'data[100000] = 0\', usecase_params=\'data\'),\n    TC(name=\'shape\', size=[10 ** 7], call_expr=\'data.shape\', usecase_params=\'data\'),\n    TC(name=\'shift\', size=[10 ** 8]),\n    TC(name=\'size\', size=[10 ** 7], call_expr=\'data.size\', usecase_params=\'data\'),\n    TC(name=\'skew\', size=[10 ** 8], check_skipna=True),\n    TC(name=\'sort_values\', size=[10 ** 5]),\n    TC(name=\'std\', size=[10 ** 7], check_skipna=True),\n    TC(name=\'sub\', size=[10 ** 7], params=\'other\', data_num=2),\n    TC(name=\'sum\', size=[10 ** 8], check_skipna=True),\n    TC(name=\'take\', size=[10 ** 7], call_expr=\'data.take([0])\', usecase_params=\'data\'),\n    TC(name=\'truediv\', size=[10 ** 7], params=\'other\', data_num=2),\n    TC(name=\'values\', size=[10 ** 7], call_expr=\'data.values\', usecase_params=\'data\'),\n    TC(name=\'value_counts\', size=[10 ** 6]),\n    TC(name=\'var\', size=[10 ** 8], check_skipna=True),\n    TC(name=\'unique\', size=[10 ** 5]),\n]\n\ngenerate_test_cases(cases, TestSeriesMethods, \'series\')\n'"
sdc/tests/tests_perf/test_perf_series_operators.py,1,"b'# -*- coding: utf-8 -*-\n# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport numpy as np\nimport time\nimport random\n\nimport sdc\n\nfrom .test_perf_base import TestBase\nfrom sdc.tests.test_utils import test_global_input_data_float64\nfrom .test_perf_utils import calc_compilation, get_times, perf_data_gen_fixed_len\nfrom .generator import generate_test_cases\nfrom .generator import TestCase as TC\nfrom .data_generator import gen_series\n\n\n""""""\npython -m sdc.runtests\nsdc.tests.tests_perf.test_perf_series_operators.TestSeriesOperatorMethods.test_series_operator_{name}\n""""""\n\n\nclass TestSeriesOperatorMethods(TestBase):\n    @classmethod\n    def setUpClass(cls):\n        super().setUpClass()\n\n    def _test_case(self, pyfunc, name, total_data_length, input_data=None, data_num=1, data_gens=None):\n        test_name = \'Series.{}\'.format(name)\n\n        data_num = len(data_gens) if data_gens is not None else data_num\n        default_data_gens = [gen_series] * data_num\n        data_gens = data_gens or default_data_gens\n        default_input_data = [np.asarray(test_global_input_data_float64).flatten()] + [None] * (data_num - 1)\n        input_data = input_data or default_input_data\n\n        for data_length in total_data_length:\n            base = {\n                ""test_name"": test_name,\n                ""data_size"": data_length,\n            }\n\n            args = tuple(gen(data_length, input_data=input_data[i]) for i, gen in enumerate(data_gens))\n            self._test_jit(pyfunc, base, *args)\n            self._test_py(pyfunc, base, *args)\n\n\ncases = [\n    TC(name=\'operator.add\', size=[10 ** 7], call_expr=\'A + B\', usecase_params=\'A, B\', data_num=2),\n    TC(name=\'operator.eq\', size=[10 ** 7], call_expr=\'A == B\', usecase_params=\'A, B\', data_num=2),\n    TC(name=\'operator.floordiv\', size=[10 ** 7], call_expr=\'A // B\', usecase_params=\'A, B\', data_num=2),\n    TC(name=\'operator.ge\', size=[10 ** 7], call_expr=\'A >= B\', usecase_params=\'A, B\', data_num=2),\n    TC(name=\'operator.gt\', size=[10 ** 7], call_expr=\'A > B\', usecase_params=\'A, B\', data_num=2),\n    TC(name=\'operator.le\', size=[10 ** 7], call_expr=\'A <= B\', usecase_params=\'A, B\', data_num=2),\n    TC(name=\'operator.lt\', size=[10 ** 7], call_expr=\'A < B\', usecase_params=\'A, B\', data_num=2),\n    TC(name=\'operator.mod\', size=[10 ** 7], call_expr=\'A % B\', usecase_params=\'A, B\', data_num=2),\n    TC(name=\'operator.mul\', size=[10 ** 7], call_expr=\'A * B\', usecase_params=\'A, B\', data_num=2),\n    TC(name=\'operator.ne\', size=[10 ** 7], call_expr=\'A != B\', usecase_params=\'A, B\', data_num=2),\n    TC(name=\'operator.pow\', size=[10 ** 7], call_expr=\'A ** B\', usecase_params=\'A, B\', data_num=2),\n    TC(name=\'operator.sub\', size=[10 ** 7], call_expr=\'A - B\', usecase_params=\'A, B\', data_num=2),\n    TC(name=\'operator.truediv\', size=[10 ** 7], call_expr=\'A / B\', usecase_params=\'A, B\', data_num=2),\n]\n\ngenerate_test_cases(cases, TestSeriesOperatorMethods, \'series\')\n'"
sdc/tests/tests_perf/test_perf_series_rolling.py,2,"b'# -*- coding: utf-8 -*-\n# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport time\n\nimport pandas\nimport numpy as np\n\nfrom sdc.tests.test_utils import test_global_input_data_float64\nfrom sdc.tests.tests_perf.test_perf_base import TestBase\nfrom sdc.tests.tests_perf.test_perf_utils import perf_data_gen_fixed_len\nfrom .generator import generate_test_cases\nfrom .generator import TestCase as TC\nfrom .data_generator import gen_series\n\nrolling_usecase_tmpl = """"""\ndef series_rolling_{method_name}_usecase(data, {extra_usecase_params}):\n    start_time = time.time()\n    for i in range({ncalls}):\n        res = data.rolling({rolling_params}).{method_name}({method_params})\n    end_time = time.time()\n    return end_time - start_time, res\n""""""\n\n\ndef get_rolling_params(window=100, min_periods=None):\n    """"""Generate supported rolling parameters""""""\n    rolling_params = [f\'{window}\']\n    if min_periods:\n        rolling_params.append(f\'min_periods={min_periods}\')\n\n    return \', \'.join(rolling_params)\n\n\ndef gen_series_rolling_usecase(method_name, rolling_params=None,\n                               extra_usecase_params=\'\',\n                               method_params=\'\', ncalls=1):\n    """"""Generate series rolling method use case""""""\n    if not rolling_params:\n        rolling_params = get_rolling_params()\n\n    func_text = rolling_usecase_tmpl.format(**{\n        \'method_name\': method_name,\n        \'extra_usecase_params\': extra_usecase_params,\n        \'rolling_params\': rolling_params,\n        \'method_params\': method_params,\n        \'ncalls\': ncalls\n    })\n\n    global_vars = {\'np\': np, \'time\': time}\n    loc_vars = {}\n    exec(func_text, global_vars, loc_vars)\n    _series_rolling_usecase = loc_vars[f\'series_rolling_{method_name}_usecase\']\n\n    return _series_rolling_usecase\n\n\n# python -m sdc.runtests sdc.tests.tests_perf.test_perf_series_rolling.TestSeriesRollingMethods\nclass TestSeriesRollingMethods(TestBase):\n    @classmethod\n    def setUpClass(cls):\n        super().setUpClass()\n        cls.map_ncalls_dlength = {\n            \'corr\': (100, [4 * 10 ** 5]),\n            \'count\': (100, [8 * 10 ** 5]),\n            \'cov\': (100, [4 * 10 ** 5]),\n            \'kurt\': (100, [8 * 10 ** 5]),\n            \'max\': (100, [4 * 10 ** 5]),\n            \'mean\': (100, [8 * 10 ** 5]),\n            \'min\': (100, [4 * 10 ** 5]),\n            \'skew\': (100, [8 * 10 ** 5]),\n            \'sum\': (100, [8 * 10 ** 5]),\n            \'std\': (100, [8 * 10 ** 5]),\n            \'var\': (100, [8 * 10 ** 5]),\n        }\n\n    def _test_case(self, pyfunc, name, total_data_length, input_data=None, data_num=1, data_gens=None):\n        test_name = \'Series.rolling.{}\'.format(name)\n\n        data_num = len(data_gens) if data_gens is not None else data_num\n        default_data_gens = [gen_series] * data_num\n        data_gens = data_gens or default_data_gens\n        default_input_data = [np.asarray(test_global_input_data_float64).flatten()] + [None] * (data_num - 1)\n        input_data = input_data or default_input_data\n\n        for data_length in total_data_length:\n            base = {\n                \'test_name\': test_name,\n                \'data_size\': data_length,\n            }\n\n            args = tuple(gen(data_length, input_data=input_data[i]) for i, gen in enumerate(data_gens))\n            self._test_jit(pyfunc, base, *args)\n            self._test_py(pyfunc, base, *args)\n\n    def _test_series_rolling_method(self, name, rolling_params=None,\n                                    extra_usecase_params=\'\', method_params=\'\'):\n        ncalls, total_data_length = self.map_ncalls_dlength[name]\n        usecase = gen_series_rolling_usecase(name, rolling_params=rolling_params,\n                                             extra_usecase_params=extra_usecase_params,\n                                             method_params=method_params, ncalls=ncalls)\n        data_num = 1\n        if extra_usecase_params:\n            data_num += len(extra_usecase_params.split(\', \'))\n        self._test_case(usecase, name, total_data_length, data_num=data_num)\n\n    def test_series_rolling_corr(self):\n        self._test_series_rolling_method(\'corr\', extra_usecase_params=\'other\',\n                                         method_params=\'other=other\')\n\n    def test_series_rolling_count(self):\n        self._test_series_rolling_method(\'count\')\n\n    def test_series_rolling_cov(self):\n        self._test_series_rolling_method(\'cov\', extra_usecase_params=\'other\',\n                                         method_params=\'other=other\')\n\n    def test_series_rolling_kurt(self):\n        self._test_series_rolling_method(\'kurt\')\n\n    def test_series_rolling_max(self):\n        self._test_series_rolling_method(\'max\')\n\n    def test_series_rolling_mean(self):\n        self._test_series_rolling_method(\'mean\')\n\n    def test_series_rolling_min(self):\n        self._test_series_rolling_method(\'min\')\n\n    def test_series_rolling_skew(self):\n        self._test_series_rolling_method(\'skew\')\n\n    def test_series_rolling_sum(self):\n        self._test_series_rolling_method(\'sum\')\n\n    def test_series_rolling_std(self):\n        self._test_series_rolling_method(\'std\')\n\n    def test_series_rolling_var(self):\n        self._test_series_rolling_method(\'var\')\n\n\ncases = [\n    TC(name=\'apply\', size=[10 ** 7], params=\'func=lambda x: np.nan if len(x) == 0 else x.mean()\'),\n    TC(name=\'median\', size=[10 ** 7]),\n    TC(name=\'quantile\', size=[10 ** 7], params=\'0.2\'),\n    TC(name=\'std\', size=[10 ** 7]),\n]\n\n\ngenerate_test_cases(cases, TestSeriesRollingMethods, \'series\', \'rolling({})\'.format(get_rolling_params()))\n'"
sdc/tests/tests_perf/test_perf_series_str.py,0,"b'# -*- coding: utf-8 -*-\n# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport itertools\nimport os\nimport time\nimport unittest\nfrom contextlib import contextmanager\n\nfrom sdc.tests.test_utils import *\nfrom sdc.tests.tests_perf.test_perf_base import TestBase\nfrom sdc.tests.tests_perf.test_perf_utils import *\nfrom .generator import generate_test_cases\nfrom .generator import TestCase as TC\nfrom .data_generator import gen_series_fixed_str\n\n\ntest_global_input_data_unicode_kind1 = [\n    \'ascii\',\n    \'12345\',\n    \'1234567890\',\n]\n\n\n# python -m sdc.runtests sdc.tests.tests_perf.test_perf_series_str.TestSeriesStringMethods.test_series_str_{method_name}\nclass TestSeriesStringMethods(TestBase):\n    iter_number = 5\n    results_class = TestResultsStr\n\n    @classmethod\n    def setUpClass(cls):\n        super().setUpClass()\n        cls.width = [16, 64, 512, 1024]\n\n    def _test_case(self, pyfunc, name, total_data_length, input_data=None, data_num=1, data_gens=None):\n        test_name = \'Series.str.{}\'.format(name)\n\n        input_data = input_data or test_global_input_data_unicode_kind4\n\n        for data_length, data_width in itertools.product(total_data_length, self.width):\n            base = {\n                ""test_name"": test_name,\n                ""data_size"": data_length,\n                ""data_width"": data_width,\n            }\n\n            args = gen_series_fixed_str(data_num, data_length, input_data, data_width)\n            self._test_jit(pyfunc, base, *args)\n            self._test_py(pyfunc, base, *args)\n\n\ncases = [\n    TC(name=\'capitalize\', size=[10 ** 4, 10 ** 5]),\n    TC(name=\'casefold\', size=[10 ** 4, 10 ** 5]),\n    TC(name=\'center\', params=\'1\', size=[10 ** 4, 10 ** 5],  input_data=test_global_input_data_unicode_kind1),\n    TC(name=\'contains\', params=\'""a""\', size=[10 ** 4, 10 ** 5]),\n    TC(name=\'endswith\', params=\'""e""\', size=[10 ** 4, 10 ** 5]),\n    TC(name=\'find\', params=\'""e""\', size=[10 ** 4, 10 ** 5]),\n    TC(name=\'isalnum\', size=[10 ** 4, 10 ** 5]),\n    TC(name=\'isalpha\', size=[10 ** 4, 10 ** 5]),\n    TC(name=\'isdecimal\', size=[10 ** 4, 10 ** 5]),\n    TC(name=\'isdigit\', size=[10 ** 4, 10 ** 5]),\n    TC(name=\'islower\', size=[10 ** 4, 10 ** 5]),\n    TC(name=\'isnumeric\', size=[10 ** 4, 10 ** 5]),\n    TC(name=\'isspace\', size=[10 ** 4, 10 ** 5]),\n    TC(name=\'istitle\', size=[10 ** 4, 10 ** 5]),\n    TC(name=\'isupper\', size=[10 ** 4, 10 ** 5]),\n    TC(name=\'len\', size=[10 ** 4, 10 ** 5]),\n    TC(name=\'ljust\', params=\'1\', size=[10 ** 4, 10 ** 5], input_data=test_global_input_data_unicode_kind1),\n    TC(name=\'lower\', size=[10 ** 4, 10 ** 5]),\n    TC(name=\'lstrip\', size=[10 ** 4, 10 ** 5],\n       input_data=[\'\\t{}  \'.format(case) for case in test_global_input_data_unicode_kind4]),\n    TC(name=\'rjust\', params=\'1\', size=[10 ** 4, 10 ** 5], input_data=test_global_input_data_unicode_kind1),\n    TC(name=\'rstrip\', size=[10 ** 4, 10 ** 5],\n       input_data=[\'\\t{}  \'.format(case) for case in test_global_input_data_unicode_kind4]),\n    TC(name=\'startswith\', params=\'""e""\', size=[10 ** 4, 10 ** 5]),\n    TC(name=\'strip\', size=[10 ** 4, 10 ** 5],\n       input_data=[\'\\t{}  \'.format(case) for case in test_global_input_data_unicode_kind4]),\n    TC(name=\'swapcase\', size=[10 ** 4, 10 ** 5]),\n    TC(name=\'title\', size=[10 ** 4, 10 ** 5]),\n    TC(name=\'upper\', size=[10 ** 4, 10 ** 5]),\n    TC(name=\'zfill\', params=\'1\', size=[10 ** 4, 10 ** 5], input_data=test_global_input_data_unicode_kind1),\n]\n\ngenerate_test_cases(cases, TestSeriesStringMethods, \'series\', \'str\')\n'"
sdc/tests/tests_perf/test_perf_unicode.py,0,"b'# -*- coding: utf-8 -*-\n# *****************************************************************************\n# Copyright (c) 2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport unittest\nimport os\nimport time\nimport numba\n\nfrom sdc.tests.test_utils import *\nfrom sdc.tests.tests_perf.test_perf_base import TestBase\nfrom sdc.tests.tests_perf.test_perf_utils import *\n\n\ndef usecase_split(input_data):\n    test_iteration_number = 5\n    iter_time = []\n\n    for local_iter in range(test_iteration_number):\n        start_time = time.time()\n\n        for string in input_data:\n            string.split(\'c\')\n\n        finish_time = time.time()\n        local_result = finish_time - start_time\n\n        iter_time.append(local_result)\n\n    return iter_time\n\n\ndef usecase_join(input_data):\n    test_iteration_number = 5\n    iter_time = []\n\n    for local_iter in range(test_iteration_number):\n        start_time = time.time()\n\n        for string in input_data:\n            \'X\'.join([string, string, string, string])\n\n        finish_time = time.time()\n        local_result = finish_time - start_time\n\n        iter_time.append(local_result)\n\n    return iter_time\n\n\ndef usecase_center(input_data):\n    result_str_grow_factor = 1.4\n    test_iteration_number = 5\n    iter_time = []\n\n    for local_iter in range(test_iteration_number):\n        start_time = time.time()\n\n        for string in input_data:\n            # The width must be an Integer\n            new_string_len = int(len(string) * result_str_grow_factor)\n\n            string.center(new_string_len, \'+\')\n\n        finish_time = time.time()\n        local_result = finish_time - start_time\n\n        iter_time.append(local_result)\n\n    return iter_time\n\n\nclass TestStringMethods(TestBase):\n    results_class = TestResultsStr\n\n    @classmethod\n    def setUpClass(cls):\n        super().setUpClass()\n\n        cls.total_data_size_bytes = [1.0E+04]\n        cls.width = [16, 64, 512, 1024]\n\n    def _test_unicode(self, pyfunc, name):\n        hpat_func = numba.njit(pyfunc)\n        for data_size in self.total_data_size_bytes:\n            for data_width in self.width:\n                test_data = perf_data_gen(test_global_input_data_unicode_kind4, data_width, data_size)\n                self.test_results.add(name, \'SDC\', len(test_data), hpat_func(test_data), data_width)\n                self.test_results.add(name, \'Python\', len(test_data), pyfunc(test_data), data_width)\n\n    def test_unicode_split(self):\n        self._test_unicode(usecase_split, \'unicode_split\')\n\n    def test_unicode_join(self):\n        self._test_unicode(usecase_join, \'unicode_join\')\n\n    def test_unicode_center(self):\n        self._test_unicode(usecase_center, \'unicode_center\')\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
sdc/tests/tests_perf/test_perf_utils.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# *****************************************************************************\n# Copyright (c) 2019-2020, Intel Corporation All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#     Redistributions of source code must retain the above copyright notice,\n#     this list of conditions and the following disclaimer.\n#\n#     Redistributions in binary form must reproduce the above copyright notice,\n#     this list of conditions and the following disclaimer in the documentation\n#     and/or other materials provided with the distribution.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n# THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n# PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR\n# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;\n# OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n# WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR\n# OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,\n# EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n# *****************************************************************************\n\nimport gc\nimport logging\nimport sys\nimport sdc\nimport time\nfrom contextlib import contextmanager\nfrom copy import copy\nfrom pathlib import Path\n\nimport pandas\nfrom numba import config\n\n""""""\nUtility functions collection to support performance testing of\nfunctions implemented in the project\n\nData generators:\n    perf_data_gen() generates list of items with fixed length\n\nData handling:\n    add_results() add an experiment timing results to globla storage\n    print_results() print all timing results from global storage\n\n""""""\n\n\ndef setup_logging():\n    """"""Setup logger""""""\n    stream_handler = logging.StreamHandler()\n    stream_handler.setFormatter(logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s\'))\n\n    logger = logging.getLogger(__name__)\n    logger.setLevel(level=logging.INFO)\n    logger.addHandler(stream_handler)\n\n    return logger\n\n\n\ndef is_true(input_string):\n    if isinstance(input_string, str):\n        input_string = input_string.lower()\n    return input_string in [\'yes\', \'y\', \'true\', \'t\', \'1\', True]\n\n\ndef get_size(obj):\n    """"""Sum size of object and its members.""""""\n    size = 0\n    processed_ids = set()\n    objects = [obj]\n    while objects:\n        need_refer = []\n        for obj in objects:\n            if id(obj) in processed_ids:\n                continue\n            processed_ids.add(id(obj))\n            need_refer.append(obj)\n            size += sys.getsizeof(obj)\n        objects = gc.get_referents(*need_refer)\n    return size\n\n\ndef multiply_oneds_data(tmpl, max_len):\n    """"""Multiply specified 1D like data.""""""\n    result = copy(tmpl)\n    while len(result) < max_len:\n        result += tmpl\n\n    # Trim result to max_len\n    return result[:max_len]\n\n\ndef multiply_data(tmpl, max_item_len):\n    """"""Multiply specified 2D like data.""""""\n    result = copy(tmpl)\n    for i, item in enumerate(result):\n        result[i] = multiply_oneds_data(item, max_item_len)\n\n    return result\n\n\ndef perf_data_gen(tmpl, max_item_len, max_bytes_size):\n    """"""\n    Data generator produces 2D like data.\n                  tmpl: list of input template string\n          max_item_len: length (in elements) of resulted string in an element of the result array\n        max_bytes_size: maximum size in bytes of the return data\n\n                return: list of data\n    """"""\n    result = []\n    while get_size(result) < max_bytes_size:\n        result.extend(multiply_data(tmpl, max_item_len))\n\n    # Trim result to max_bytes_size\n    while result and get_size(result) > max_bytes_size:\n        del result[-1]\n\n    return result\n\n\ndef perf_data_gen_fixed_len(tmpl, max_item_len, max_obj_len):\n    """"""\n    Data generator produces 2D like data.\n                  tmpl: list of input template string\n          max_item_len: length (in elements) of resulted string in an element of the result array\n           max_obj_len: maximum length of the return data\n\n                return: list of data\n    """"""\n    result = []\n    while len(result) < max_obj_len:\n        result.extend(multiply_data(tmpl, max_item_len))\n\n    # Trim result to max_obj_len\n    return result[:max_obj_len]\n\n\n@contextmanager\ndef do_jit(f):\n    """"""Context manager to jit function""""""\n    cfunc = sdc.jit(f)\n    try:\n        yield cfunc\n    finally:\n        del cfunc\n\n\ndef calc_time(func, *args, **kwargs):\n    """"""Calculate execution time of specified function.""""""\n    start_time = time.time()\n    func(*args, **kwargs)\n    finish_time = time.time()\n\n    return finish_time - start_time\n\n\ndef calc_compile_time(func, *args, **kwargs):\n    """"""Calculate compile time as difference between first 2 runs.""""""\n    return calc_time(func, *args, **kwargs) - calc_time(func, *args, **kwargs)\n\n\ndef calc_compilation(pyfunc, *args, iter_number=5):\n    """"""Calculate compile time several times.""""""\n    compile_times = []\n    for _ in range(iter_number):\n        with do_jit(pyfunc) as cfunc:\n            compile_time = calc_compile_time(cfunc, *args)\n            compile_times.append(compile_time)\n\n    return compile_times\n\n\ndef get_times(f, *args, iter_number=5):\n    """"""Get time of boxing+unboxing and internal execution""""""\n    exec_times = []\n    boxing_times = []\n\n    # Warming up\n    f(*args)\n\n    for _ in range(iter_number):\n        ext_start = time.time()\n        int_result, _ = f(*args)\n        ext_finish = time.time()\n\n        exec_times.append(int_result)\n        boxing_times.append(max(ext_finish - ext_start - int_result, 0))\n\n    return exec_times, boxing_times\n\n\nclass ResultsDriver:\n    """"""Base class. Load and dump results.""""""\n\n    def __init__(self, file_name, raw_file_name=None):\n        self.file_name = file_name\n        self.raw_file_name = raw_file_name or f\'raw_{file_name}\'\n\n\nclass ExcelResultsDriver(ResultsDriver):\n    # openpyxl need to be installed\n\n    def dump_grouped_data(self, grouped_data, logger=None):\n        try:\n            with pandas.ExcelWriter(self.file_name) as writer:\n                grouped_data.to_excel(writer)\n        except ModuleNotFoundError as e:\n            if logger:\n                msg = \'Could not dump the results to ""%s"": %s\'\n                logger.warning(msg, self.file_name, e)\n\n    def dump_test_results_data(self, test_results_data, logger=None):\n        try:\n            with pandas.ExcelWriter(self.raw_file_name) as writer:\n                test_results_data.to_excel(writer, index=False)\n        except ModuleNotFoundError as e:\n            if logger:\n                msg = \'Could not dump raw results to ""%s"": %s\'\n                logger.warning(msg, self.raw_file_name, e)\n\n    def load(self, logger=None):\n        raw_perf_results_xlsx = Path(self.raw_file_name)\n        if not raw_perf_results_xlsx.exists():\n            return\n        with raw_perf_results_xlsx.open(\'rb\') as fd:\n            # xlrd need to be installed\n            try:\n                return pandas.read_excel(fd)\n            except ModuleNotFoundError as e:\n                if logger:\n                    msg = \'Could not load previous results from %s: %s\'\n                    logger.warning(msg, self.raw_file_name, e)\n\n\nclass CSVResultsDriver(ResultsDriver):\n\n    def dump_grouped_data(self, grouped_data, logger=None):\n        grouped_data.to_csv(self.file_name)\n\n    def dump_test_results_data(self, test_results_data, logger=None):\n        test_results_data.to_csv(self.raw_file_name, index=False)\n\n    def load(self, logger=None):\n        raw_perf_results_csv = Path(self.raw_file_name)\n        if not raw_perf_results_csv.exists():\n            return\n        with raw_perf_results_csv.open(\'rb\') as fd:\n            try:\n                return pandas.read_csv(fd)\n            except ModuleNotFoundError as e:\n                if logger:\n                    msg = \'Could not load previous results from %s: %s\'\n                    logger.warning(msg, self.raw_file_name, e)\n\n\nclass TestResults:\n    index = [\'name\', \'nthreads\', \'type\', \'size\']\n    test_results_data = pandas.DataFrame()\n    logger = setup_logging()\n\n    def __init__(self, drivers=None):\n        self.drivers = drivers or []\n\n    @property\n    def grouped_data(self):\n        """"""\n        Group global storage results\n        Example:\n                                                    median       min       max  compilation(median)  boxing(median)\n            name           type      size  width\n            series_str_len JIT       33174 16     0.005283  0.005190  0.005888             0.163459        0.001801\n                                     6201  64     0.001473  0.001458  0.001886             0.156071        0.000528\n                                     1374  512    0.001087  0.001066  0.001268             0.154500        0.000972\n                                     729   1024   0.000998  0.000993  0.001235             0.155549        0.001002\n                           Reference 33174 16     0.007499  0.007000  0.010999                  NaN        0.000000\n                                     6201  64     0.001998  0.001498  0.002002                  NaN        0.000000\n                                     1374  512    0.000541  0.000500  0.000960                  NaN        0.000000\n                                     729   1024   0.000500  0.000000  0.000502                  NaN        0.000000\n        """"""\n        if self.test_results_data.empty:\n            return pandas.DataFrame()\n\n        time_columns = [\'Time(s)\', \'Compile(s)\', \'Boxing(s)\']\n        index = [column for column in self.test_results_data.columns if column not in time_columns]\n\n        # by default Pandas drop data with NaN in index during groupby\n        # so replace NaN in index with empty string\n        data = self.test_results_data.fillna(value={column: \'\' for column in index})\n\n        grouped = data.groupby(index)\n\n        median_col = grouped[\'Time(s)\'].median()\n        min_col = grouped[\'Time(s)\'].min()\n        max_col = grouped[\'Time(s)\'].max()\n        compilation_col = grouped[\'Compile(s)\'].median(skipna=False)\n        boxing_col = grouped[\'Boxing(s)\'].median(skipna=False)\n\n        test_results_data = data.set_index(index)\n        test_results_data[\'median\'] = median_col\n        test_results_data[\'min\'] = min_col\n        test_results_data[\'max\'] = max_col\n        test_results_data[\'compile\'] = compilation_col\n        test_results_data[\'boxing\'] = boxing_col\n        test_results_data = test_results_data.reset_index()\n\n        columns = [\'median\', \'min\', \'max\', \'compile\', \'boxing\']\n        return test_results_data.groupby(index)[columns].first().sort_values(index)\n\n    def add(self, test_name, test_type, data_size, test_results,\n            boxing_results=None, compile_results=None, num_threads=config.NUMBA_NUM_THREADS):\n        """"""\n        Add performance testing timing results into global storage\n                  test_name: Name of test (1st column in grouped result)\n                  test_type: Type of test (3rd column in grouped result)\n                  data_size: Size of input data (4s column in grouped result)\n               test_results: List of timing results of the experiment\n             boxing_results: List of timing results of the overhead (boxing/unboxing)\n           compilation_time: Timing result of compilation\n                num_threads: Value from NUMBA_NUM_THREADS (2nd column in grouped result)\n        """"""\n        data = {\n            \'name\': test_name,\n            \'nthreads\': num_threads,\n            \'type\': test_type,\n            \'size\': data_size,\n            \'Time(s)\': test_results,\n            \'Compile(s)\': compile_results,\n            \'Boxing(s)\': boxing_results\n        }\n        local_results = pandas.DataFrame(data)\n        self.test_results_data = self.test_results_data.append(local_results, sort=False)\n\n    def print(self):\n        """"""\n        Print performance testing results from global data storage\n        """"""\n        print(""Performance testing results:"")\n        print(self.grouped_data.to_string())\n\n    def dump(self):\n        """"""\n        Dump performance testing results from global data storage to excel\n        """"""\n        for d in self.drivers:\n            d.dump_grouped_data(self.grouped_data, self.logger)\n            d.dump_test_results_data(self.test_results_data, self.logger)\n\n    def load(self):\n        """"""\n        Load existing performance testing results from excel to global data storage\n        """"""\n        for d in self.drivers:\n            test_results_data = d.load(self.logger)\n            if test_results_data is not None:\n                self.test_results_data = test_results_data\n                break\n\n\nclass TestResultsStr(TestResults):\n    index = [\'name\', \'nthreads\', \'type\', \'size\', \'width\']\n\n    def add(self, test_name, test_type, data_size, test_results, data_width=None,\n            boxing_results=None, compile_results=None, num_threads=config.NUMBA_NUM_THREADS):\n        """"""\n        Add performance testing timing results into global storage\n                  test_name: Name of test (1st column in grouped result)\n                  test_type: Type of test (3rd column in grouped result)\n                  data_size: Size of input data (4s column in grouped result)\n               test_results: List of timing results of the experiment\n                 data_width: Scalability attribute for str input data (5s column in grouped result)\n             boxing_results: List of timing results of the overhead (boxing/unboxing)\n           compilation_time: Timing result of compilation\n                num_threads: Value from NUMBA_NUM_THREADS (2nd column in grouped result)\n        """"""\n        data = {\n            \'name\': test_name,\n            \'nthreads\': num_threads,\n            \'type\': test_type,\n            \'size\': data_size,\n            \'width\': data_width,\n            \'Time(s)\': test_results,\n            \'Compile(s)\': compile_results,\n            \'Boxing(s)\': boxing_results\n        }\n        local_results = pandas.DataFrame(data)\n        self.test_results_data = self.test_results_data.append(local_results, sort=False)\n\n\nif __name__ == ""__main__"":\n    data = perf_data_gen([\'Test example\'], 64, 1.0E+03)\n    print(""Result data:"", data)\n'"
